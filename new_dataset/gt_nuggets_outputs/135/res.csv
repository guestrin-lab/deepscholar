qid,query,nuggets,supported_nuggets,partially_supported_nuggets,nuggets_metrics
2511.02458v1,"Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:
We evaluate whether persona-based prompting improves Large Language Model (LLM) performance on macroeconomic forecasting tasks. Using 2,368 economics-related personas from the PersonaHub corpus, we prompt GPT-4o to replicate the ECB Survey of Professional Forecasters across 50 quarterly rounds (2013-2025). We compare the persona-prompted forecasts against the human experts panel, across four target variables (HICP, core HICP, GDP growth, unemployment) and four forecast horizons. We also compare the results against 100 baseline forecasts without persona descriptions to isolate its effect. We report two main findings. Firstly, GPT-4o and human forecasters achieve remarkably similar accuracy levels, with differences that are statistically significant yet practically modest. Our out-of-sample evaluation on 2024-2025 data demonstrates that GPT-4o can maintain competitive forecasting performance on unseen events, though with notable differences compared to the in-sample period. Secondly, our ablation experiment reveals no measurable forecasting advantage from persona descriptions, suggesting these prompt components can be omitted to reduce computational costs without sacrificing accuracy. Our results provide evidence that GPT-4o can achieve competitive forecasting accuracy even on out-of-sample macroeconomic events, if provided with relevant context data, while revealing that diverse prompts produce remarkably homogeneous forecasts compared to human panels.","[{'text': 'LLMs applied to macroeconomic forecasting', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Persona-based prompting in LLMs', 'importance': 'vital', 'assignment': 'support'}, {'text': 'GPT-4o replicates ECB Survey of Professional Forecasters', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': '2,368 personas from PersonaHub corpus used', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Comparison with human expert panel forecasts', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Four target variables: HICP, core HICP, GDP growth, unemployment', 'importance': 'vital', 'assignment': 'not_support'}, {'text': '100 baseline forecasts without persona descriptions', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'GPT-4o and human forecasters have similar accuracy', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Out-of-sample evaluation on 2024-2025 data', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'No measurable advantage from persona descriptions', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Persona components can be omitted to reduce costs', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'Diverse prompts produce homogeneous forecasts', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'Four forecast horizons evaluated', 'importance': 'okay', 'assignment': 'not_support'}, {'text': 'LLMs simulate human survey responses', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Role-play prompting improves LLM performance', 'importance': 'okay', 'assignment': 'support'}, {'text': 'LLMs sensitive to prompt content and structure', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Manual forecaster personas improve forecast accuracy', 'importance': 'okay', 'assignment': 'support'}, {'text': 'LLMs replicate human behaviors in surveys', 'importance': 'okay', 'assignment': 'support'}, {'text': 'LLMs compared to traditional time series methods', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Memorization effects in LLM-based forecasts', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'LLMs applied to macroeconomic forecasting', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Persona-based prompting in LLMs', 'importance': 'vital', 'assignment': 'support'}, {'text': '2,368 personas from PersonaHub corpus used', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Comparison with human expert panel forecasts', 'importance': 'vital', 'assignment': 'support'}, {'text': 'GPT-4o and human forecasters have similar accuracy', 'importance': 'vital', 'assignment': 'support'}, {'text': 'No measurable advantage from persona descriptions', 'importance': 'vital', 'assignment': 'support'}, {'text': 'LLMs simulate human survey responses', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Role-play prompting improves LLM performance', 'importance': 'okay', 'assignment': 'support'}, {'text': 'LLMs sensitive to prompt content and structure', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Manual forecaster personas improve forecast accuracy', 'importance': 'okay', 'assignment': 'support'}, {'text': 'LLMs replicate human behaviors in surveys', 'importance': 'okay', 'assignment': 'support'}, {'text': 'LLMs compared to traditional time series methods', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Memorization effects in LLM-based forecasts', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'LLMs applied to macroeconomic forecasting', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Persona-based prompting in LLMs', 'importance': 'vital', 'assignment': 'support'}, {'text': 'GPT-4o replicates ECB Survey of Professional Forecasters', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': '2,368 personas from PersonaHub corpus used', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Comparison with human expert panel forecasts', 'importance': 'vital', 'assignment': 'support'}, {'text': 'GPT-4o and human forecasters have similar accuracy', 'importance': 'vital', 'assignment': 'support'}, {'text': 'No measurable advantage from persona descriptions', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Persona components can be omitted to reduce costs', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'Diverse prompts produce homogeneous forecasts', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'LLMs simulate human survey responses', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Role-play prompting improves LLM performance', 'importance': 'okay', 'assignment': 'support'}, {'text': 'LLMs sensitive to prompt content and structure', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Manual forecaster personas improve forecast accuracy', 'importance': 'okay', 'assignment': 'support'}, {'text': 'LLMs replicate human behaviors in surveys', 'importance': 'okay', 'assignment': 'support'}, {'text': 'LLMs compared to traditional time series methods', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Memorization effects in LLM-based forecasts', 'importance': 'okay', 'assignment': 'support'}]","{'strict_vital_score': 0.5, 'strict_all_score': 0.65, 'vital_score': 0.625, 'all_score': 0.725}"
