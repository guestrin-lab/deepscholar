{
  "qid": "2511.07272v1",
  "query": "Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:\nOverparameterized fully-connected neural networks have been shown to behave like kernel models when trained with gradient descent, under mild conditions on the width, the learning rate, and the parameter initialization. In the limit of infinitely large widths and small learning rate, the kernel that is obtained allows to represent the output of the learned model with a closed-form solution. This closed-form solution hinges on the invertibility of the limiting kernel, a property that often holds on real-world datasets. In this work, we analyze the sensitivity of large ReLU networks to increasing depths by characterizing the corresponding limiting kernel. Our theoretical results demonstrate that the normalized limiting kernel approaches the matrix of ones. In contrast, they show the corresponding closed-form solution approaches a fixed limit on the sphere. We empirically evaluate the order of magnitude in network depth required to observe this convergent behavior, and we describe the essential properties that enable the generalization of our results to other kernels.",
  "nuggets": [
    {
      "text": "Overparameterized networks behave like kernel models with gradient descent.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Neural tangent kernel (NTK) describes infinitely wide networks' dynamics.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "NTK framework formalizes neural networks' behavior as kernel methods.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Closed-form NTK expressions exist for ReLU activations.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Depth affects NTK sensitivity and network output multiplicity.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Limiting kernel is deterministic in infinite width case.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "NTK extended to CNNs captures learning dynamics.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Loss landscape of overparameterized networks shows near-linearity.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "NTKs often outperform finite-width networks but not CNNs.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Uniform measure on sphere shows depth's limited impact on representation.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "NTK eigenvalues follow power law decay with training set size.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "NTK variance increases with unbounded depth-to-width ratio.",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "supported_nuggets": [
    {
      "text": "Overparameterized networks behave like kernel models with gradient descent.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Neural tangent kernel (NTK) describes infinitely wide networks' dynamics.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "NTK framework formalizes neural networks' behavior as kernel methods.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Closed-form NTK expressions exist for ReLU activations.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Depth affects NTK sensitivity and network output multiplicity.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Limiting kernel is deterministic in infinite width case.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "NTK extended to CNNs captures learning dynamics.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Loss landscape of overparameterized networks shows near-linearity.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "NTKs often outperform finite-width networks but not CNNs.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Uniform measure on sphere shows depth's limited impact on representation.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "NTK eigenvalues follow power law decay with training set size.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "NTK variance increases with unbounded depth-to-width ratio.",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "partially_supported_nuggets": [
    {
      "text": "Overparameterized networks behave like kernel models with gradient descent.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Neural tangent kernel (NTK) describes infinitely wide networks' dynamics.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "NTK framework formalizes neural networks' behavior as kernel methods.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Closed-form NTK expressions exist for ReLU activations.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Depth affects NTK sensitivity and network output multiplicity.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Limiting kernel is deterministic in infinite width case.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "NTK extended to CNNs captures learning dynamics.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Loss landscape of overparameterized networks shows near-linearity.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "NTKs often outperform finite-width networks but not CNNs.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Uniform measure on sphere shows depth's limited impact on representation.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "NTK eigenvalues follow power law decay with training set size.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "NTK variance increases with unbounded depth-to-width ratio.",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "nuggets_metrics": {
    "strict_vital_score": 1.0,
    "strict_all_score": 1.0,
    "vital_score": 1.0,
    "all_score": 1.0
  }
}