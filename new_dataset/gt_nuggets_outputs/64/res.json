{
  "qid": "2511.10059v1",
  "query": "Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:\nCan Multimodal Large Language Models (MLLMs) discern confused objects that are visually present but audio-absent? To study this, we introduce a new benchmark, AV-ConfuseBench, which simulates an ``Audio-Visual Confusion'' scene by modifying the corresponding sound of an object in the video, e.g., mute the sounding object and ask MLLMs Is there a/an muted-object sound''. Experimental results reveal that MLLMs, such as Qwen2.5-Omni and Gemini 2.5, struggle to discriminate non-existent audio due to visually dominated reasoning. Motivated by this observation, we introduce RL-CoMM, a Reinforcement Learning-based Collaborative Multi-MLLM that is built upon the Qwen2.5-Omni foundation. RL-CoMM includes two stages: 1) To alleviate visually dominated ambiguities, we introduce an external model, a Large Audio Language Model (LALM), as the reference model to generate audio-only reasoning. Then, we design a Step-wise Reasoning Reward function that enables MLLMs to self-improve audio-visual reasoning with the audio-only reference. 2) To ensure an accurate answer prediction, we introduce Answer-centered Confidence Optimization to reduce the uncertainty of potential heterogeneous reasoning differences. Extensive experiments on audio-visual question answering and audio-visual hallucination show that RL-CoMM improves the accuracy by 10~30\\% over the baseline model with limited training data. Follow: https://github.com/rikeilong/AVConfusion.",
  "nuggets": [
    {
      "text": "AV-ConfuseBench simulates audio-visual confusion scenes",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "MLLMs struggle with visually dominated reasoning",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "RL-CoMM uses reinforcement learning for MLLMs",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "Large Audio Language Model aids audio-only reasoning",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "Step-wise Reasoning Reward improves audio-visual reasoning",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Answer-centered Confidence Optimization reduces reasoning uncertainty",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "RL-CoMM improves accuracy by 10-30% with limited data",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Supervised Fine-Tuning and RL improve LLM sensitivity",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Omni-LLMs understand both video and audio inputs",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Audio-visual hallucination caused by internal LLM knowledge",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "GRPO algorithm enhances intermediate thinking trajectories",
      "importance": "okay",
      "assignment": "partial_support"
    },
    {
      "text": "Audio-LLMs complement Omni-LLMs thinking knowledge",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "supported_nuggets": [
    {
      "text": "MLLMs struggle with visually dominated reasoning",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Supervised Fine-Tuning and RL improve LLM sensitivity",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Omni-LLMs understand both video and audio inputs",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Audio-visual hallucination caused by internal LLM knowledge",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Audio-LLMs complement Omni-LLMs thinking knowledge",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "partially_supported_nuggets": [
    {
      "text": "MLLMs struggle with visually dominated reasoning",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "RL-CoMM uses reinforcement learning for MLLMs",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "Large Audio Language Model aids audio-only reasoning",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "Supervised Fine-Tuning and RL improve LLM sensitivity",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Omni-LLMs understand both video and audio inputs",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Audio-visual hallucination caused by internal LLM knowledge",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "GRPO algorithm enhances intermediate thinking trajectories",
      "importance": "okay",
      "assignment": "partial_support"
    },
    {
      "text": "Audio-LLMs complement Omni-LLMs thinking knowledge",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "nuggets_metrics": {
    "strict_vital_score": 0.25,
    "strict_all_score": 0.4166666666666667,
    "vital_score": 0.375,
    "all_score": 0.5416666666666666
  }
}