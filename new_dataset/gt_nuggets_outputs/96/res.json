{
  "qid": "2511.09871v1",
  "query": "Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:\nContinual learning methods used to force neural networks to process sequential tasks in isolation, preventing them from leveraging useful inter-task relationships and causing them to repeatedly relearn similar features or overly differentiate them. To address this problem, we propose a fully differentiable, exemplar-free expandable method composed of two complementary memories: One learns common features that can be used across all tasks, and the other combines the shared features to learn discriminative characteristics unique to each sample. Both memories are differentiable so that the network can autonomously learn latent representations for each sample. For each task, the memory adjustment module adaptively prunes critical slots and minimally expands capacity to accommodate new concepts, and orthogonal regularization enforces geometric separation between preserved and newly learned memory components to prevent interference. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet show that the proposed method outperforms 14 state-of-the-art methods for class-incremental learning, achieving final accuracies of 55.13\\%, 37.24\\%, and 30.11\\%, respectively. Additional analysis confirms that, through effective integration and utilization of knowledge, the proposed method can increase average performance across sequential tasks, and it produces feature extraction results closest to the upper bound, thus establishing a new milestone in continual learning.",
  "nuggets": [
    {
      "text": "Proposed method uses two complementary differentiable memories",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Method adaptively prunes and expands memory for new concepts",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Orthogonal regularization prevents interference in memory components",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Outperforms 14 state-of-the-art methods in class-incremental learning",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Method increases average performance across sequential tasks",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Produces feature extraction results closest to the upper bound",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Establishes a new milestone in continual learning",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Regularization-based methods preserve prior knowledge",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "Dynamic expansion approaches allocate new parameters per task",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "EDD integrates knowledge end-to-end through self-organizing memory",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Achieves 55.13% accuracy on CIFAR-10",
      "importance": "okay",
      "assignment": "not_support"
    },
    {
      "text": "Achieves 37.24% accuracy on CIFAR-100",
      "importance": "okay",
      "assignment": "not_support"
    },
    {
      "text": "Achieves 30.11% accuracy on Tiny-ImageNet",
      "importance": "okay",
      "assignment": "not_support"
    },
    {
      "text": "Parameter isolation methods freeze upper layers of neural networks",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Dual memory approaches mimic human cognitive processes",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "supported_nuggets": [
    {
      "text": "Proposed method uses two complementary differentiable memories",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Method adaptively prunes and expands memory for new concepts",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "EDD integrates knowledge end-to-end through self-organizing memory",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Parameter isolation methods freeze upper layers of neural networks",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Dual memory approaches mimic human cognitive processes",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "partially_supported_nuggets": [
    {
      "text": "Proposed method uses two complementary differentiable memories",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Method adaptively prunes and expands memory for new concepts",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Regularization-based methods preserve prior knowledge",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "Dynamic expansion approaches allocate new parameters per task",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "EDD integrates knowledge end-to-end through self-organizing memory",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Parameter isolation methods freeze upper layers of neural networks",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Dual memory approaches mimic human cognitive processes",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "nuggets_metrics": {
    "strict_vital_score": 0.3,
    "strict_all_score": 0.3333333333333333,
    "vital_score": 0.4,
    "all_score": 0.4
  }
}