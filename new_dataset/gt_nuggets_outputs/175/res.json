{
  "qid": "2511.09925v1",
  "query": "Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:\nGradient descent dynamics on the deep matrix factorization problem is extensively studied as a simplified theoretical model for deep neural networks. Although the convergence theory for two-layer matrix factorization is well-established, no global convergence guarantee for general deep matrix factorization under random initialization has been established to date. To address this gap, we provide a polynomial-time global convergence guarantee for randomly initialized gradient descent on four-layer matrix factorization, given certain conditions on the target matrix and a standard balanced regularization term. Our analysis employs new techniques to show saddle-avoidance properties of gradient decent dynamics, and extends previous theories to characterize the change in eigenvalues of layer weights.",
  "nuggets": [
    {
      "text": "Global convergence for two-layer matrix factorization is established.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Polynomial-time global convergence for four-layer matrix factorization.",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Saddle-avoidance properties in gradient descent dynamics.",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Eigenvalue changes in layer weights are characterized.",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Gradient descent dynamics studied in deep matrix factorization.",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "No global convergence for deep networks with N>2 layers.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Asymmetric matrix factorization faces homogeneity issues.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Gradient descent balances layer magnitudes under small initialization.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Identity initialization affects convergence in deep networks.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Regularization term helps in asymmetric matrix factorization.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Deep linear networks analyzed under neural tangent kernel regime.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Infinite-width limit studied in deep linear networks.",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "supported_nuggets": [
    {
      "text": "Global convergence for two-layer matrix factorization is established.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "No global convergence for deep networks with N>2 layers.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Asymmetric matrix factorization faces homogeneity issues.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Gradient descent balances layer magnitudes under small initialization.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Identity initialization affects convergence in deep networks.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Regularization term helps in asymmetric matrix factorization.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Deep linear networks analyzed under neural tangent kernel regime.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Infinite-width limit studied in deep linear networks.",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "partially_supported_nuggets": [
    {
      "text": "Global convergence for two-layer matrix factorization is established.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Gradient descent dynamics studied in deep matrix factorization.",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "No global convergence for deep networks with N>2 layers.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Asymmetric matrix factorization faces homogeneity issues.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Gradient descent balances layer magnitudes under small initialization.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Identity initialization affects convergence in deep networks.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Regularization term helps in asymmetric matrix factorization.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Deep linear networks analyzed under neural tangent kernel regime.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Infinite-width limit studied in deep linear networks.",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "nuggets_metrics": {
    "strict_vital_score": 0.3333333333333333,
    "strict_all_score": 0.6666666666666666,
    "vital_score": 0.4166666666666667,
    "all_score": 0.7083333333333334
  }
}