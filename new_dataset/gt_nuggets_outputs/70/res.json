{
  "qid": "2511.10054v1",
  "query": "Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:\nMixture-of-Experts (MoE) architectures scale language models by activating only a subset of specialized expert networks for each input token, thereby reducing the number of floating-point operations. However, the growing size of modern MoE models causes their full parameter sets to exceed GPU memory capacity; for example, Mixtral-8x7B has 45 billion parameters and requires 87 GB of memory even though only 14 billion parameters are used per token. Existing systems alleviate this limitation by offloading inactive experts to CPU memory, but transferring experts across the PCIe interconnect incurs significant latency (about 10 ms). Prefetching heuristics aim to hide this latency by predicting which experts are needed, but prefetch failures introduce significant stalls and amplify inference latency. In the event of a prefetch failure, prior work offers two primary solutions: either fetch the expert on demand, which incurs a long stall due to the PCIe bottleneck, or drop the expert from the computation, which significantly degrades model accuracy. The critical challenge, therefore, is to maintain both high inference speed and model accuracy when prefetching fails.",
  "nuggets": [
    {
      "text": "Mixture-of-Experts (MoE) scales language models with sparse activations.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Memory footprint of MoE models is a challenge at inference.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "MoE models require low-latency expert swapping.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "MoE-Infinity predicts expert usage to hide transfer latency.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Mixtral-8x7B has 45 billion parameters, needs 87 GB memory.",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Prefetching heuristics aim to hide PCIe transfer latency.",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "Prefetch failures cause stalls and increase inference latency.",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Fetching experts on demand incurs PCIe bottleneck stalls.",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Dropping experts degrades model accuracy significantly.",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Shazeer et al. introduced sparsely-gated MoE layer.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "GShard demonstrated MoE models beyond 600B parameters.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Switch Transformer achieved state-of-the-art with 1.6T parameters.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "DeepSpeed's ZeRO-Infinity offloads model components to CPU memory.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Pre-gated MoE predicts next layer's expert needs.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "EdgeMoE improves cache management with activation frequency.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "EdgeMoE uses mixed-precision quantization to reduce transfer cost.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "AdapMoE limits activated experts based on sensitivity metric.",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "supported_nuggets": [
    {
      "text": "Mixture-of-Experts (MoE) scales language models with sparse activations.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Memory footprint of MoE models is a challenge at inference.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "MoE models require low-latency expert swapping.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "MoE-Infinity predicts expert usage to hide transfer latency.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Shazeer et al. introduced sparsely-gated MoE layer.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "GShard demonstrated MoE models beyond 600B parameters.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Switch Transformer achieved state-of-the-art with 1.6T parameters.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "DeepSpeed's ZeRO-Infinity offloads model components to CPU memory.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Pre-gated MoE predicts next layer's expert needs.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "EdgeMoE improves cache management with activation frequency.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "EdgeMoE uses mixed-precision quantization to reduce transfer cost.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "AdapMoE limits activated experts based on sensitivity metric.",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "partially_supported_nuggets": [
    {
      "text": "Mixture-of-Experts (MoE) scales language models with sparse activations.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Memory footprint of MoE models is a challenge at inference.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "MoE models require low-latency expert swapping.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "MoE-Infinity predicts expert usage to hide transfer latency.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Prefetching heuristics aim to hide PCIe transfer latency.",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "Shazeer et al. introduced sparsely-gated MoE layer.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "GShard demonstrated MoE models beyond 600B parameters.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Switch Transformer achieved state-of-the-art with 1.6T parameters.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "DeepSpeed's ZeRO-Infinity offloads model components to CPU memory.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Pre-gated MoE predicts next layer's expert needs.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "EdgeMoE improves cache management with activation frequency.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "EdgeMoE uses mixed-precision quantization to reduce transfer cost.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "AdapMoE limits activated experts based on sensitivity metric.",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "nuggets_metrics": {
    "strict_vital_score": 0.4444444444444444,
    "strict_all_score": 0.7058823529411765,
    "vital_score": 0.5,
    "all_score": 0.7352941176470589
  }
}