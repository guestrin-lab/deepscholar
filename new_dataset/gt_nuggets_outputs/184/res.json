{
  "qid": "2511.06495v1",
  "query": "Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:\nWe propose and investigate probabilistic guarantees for the adversarial robustness of classification algorithms. While traditional formal verification approaches for robustness are intractable and sampling-based approaches do not provide formal guarantees, our approach is able to efficiently certify a probabilistic relaxation of robustness. The key idea is to sample an $ε$-net and invoke a local robustness oracle on the sample. Remarkably, the size of the sample needed to achieve probably approximately global robustness guarantees is independent of the input dimensionality, the number of classes, and the learning algorithm itself. Our approach can, therefore, be applied even to large neural networks that are beyond the scope of traditional formal verification. Experiments empirically confirm that it characterizes robustness better than state-of-the-art sampling-based approaches and scales better than formal methods.",
  "nuggets": [
    {
      "text": "Probabilistic guarantees for adversarial robustness of classification algorithms",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "Traditional formal verification is intractable for large neural networks",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Sampling-based approaches lack formal guarantees for robustness",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Efficiently certifying probabilistic relaxation of robustness",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "Sample an $ε$-net and use local robustness oracle",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Sample size independent of input dimensionality and classes",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Applicable to large neural networks beyond formal verification",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Characterizes robustness better than state-of-the-art sampling methods",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Scales better than traditional formal verification methods",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Empirical approaches assess and improve neural network robustness",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Formal methods provide provable guarantees for small networks",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Hard to guarantee behavior of large networks",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Approximate and statistical robustness guarantees explored",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Probabilistic relaxation of global robustness notion",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Adversarial examples sensitivity in neural networks",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Gradient-based methods like FGSM and PGD for adversarial examples",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "C&W attack considers distance to original data point",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Global robustness for confident predictions using softmax",
      "importance": "okay",
      "assignment": "partial_support"
    },
    {
      "text": "Margin-based confidence for global robustness",
      "importance": "okay",
      "assignment": "partial_support"
    }
  ],
  "supported_nuggets": [
    {
      "text": "Traditional formal verification is intractable for large neural networks",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Sampling-based approaches lack formal guarantees for robustness",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Sample an $ε$-net and use local robustness oracle",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Applicable to large neural networks beyond formal verification",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Empirical approaches assess and improve neural network robustness",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Formal methods provide provable guarantees for small networks",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Hard to guarantee behavior of large networks",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Approximate and statistical robustness guarantees explored",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Probabilistic relaxation of global robustness notion",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Adversarial examples sensitivity in neural networks",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Gradient-based methods like FGSM and PGD for adversarial examples",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "C&W attack considers distance to original data point",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "partially_supported_nuggets": [
    {
      "text": "Probabilistic guarantees for adversarial robustness of classification algorithms",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "Traditional formal verification is intractable for large neural networks",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Sampling-based approaches lack formal guarantees for robustness",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Efficiently certifying probabilistic relaxation of robustness",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "Sample an $ε$-net and use local robustness oracle",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Applicable to large neural networks beyond formal verification",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Empirical approaches assess and improve neural network robustness",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Formal methods provide provable guarantees for small networks",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Hard to guarantee behavior of large networks",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Approximate and statistical robustness guarantees explored",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Probabilistic relaxation of global robustness notion",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Adversarial examples sensitivity in neural networks",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Gradient-based methods like FGSM and PGD for adversarial examples",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "C&W attack considers distance to original data point",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Global robustness for confident predictions using softmax",
      "importance": "okay",
      "assignment": "partial_support"
    },
    {
      "text": "Margin-based confidence for global robustness",
      "importance": "okay",
      "assignment": "partial_support"
    }
  ],
  "nuggets_metrics": {
    "strict_vital_score": 0.6428571428571429,
    "strict_all_score": 0.631578947368421,
    "vital_score": 0.7142857142857143,
    "all_score": 0.7368421052631579
  }
}