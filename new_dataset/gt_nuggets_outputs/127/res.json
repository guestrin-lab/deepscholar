{
  "qid": "2511.09611v1",
  "query": "Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:\nWhile thinking-aware generation aims to improve performance on complex tasks, we identify a critical failure mode where existing sequential, autoregressive approaches can paradoxically degrade performance due to error propagation. To systematically analyze this issue, we propose ParaBench, a new benchmark designed to evaluate both text and image output modalities. Our analysis using ParaBench reveals that this performance degradation is strongly correlated with poor alignment between the generated reasoning and the final image. To resolve this, we propose a parallel multimodal diffusion framework, MMaDA-Parallel, that enables continuous, bidirectional interaction between text and images throughout the entire denoising trajectory. MMaDA-Parallel is trained with supervised finetuning and then further optimized by Parallel Reinforcement Learning (ParaRL), a novel strategy that applies semantic rewards along the trajectory to enforce cross-modal consistency. Experiments validate that our model significantly improves cross-modal alignment and semantic consistency, achieving a 6.9\\% improvement in Output Alignment on ParaBench compared to the state-of-the-art model, Bagel, establishing a more robust paradigm for thinking-aware image synthesis. Our code is open-sourced at https://github.com/tyfeld/MMaDA-Parallel",
  "nuggets": [
    {
      "text": "MMaDA-Parallel enables bidirectional text-image interaction",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "ParaBench evaluates text and image output modalities",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Error propagation degrades performance in autoregressive models",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "MMaDA-Parallel uses Parallel Reinforcement Learning for optimization",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "6.9% improvement in Output Alignment on ParaBench",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Thinking-aware generation aims to improve complex task performance",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Sequential autoregressive models prone to error accumulation",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "MMaDA-Parallel trained with supervised finetuning",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Bagel integrates chain-of-thought reasoning in image generation",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Chameleon and Mogao explored interleaved text-image generation",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Image-CoT and GoT used CoT reasoning before image synthesis",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "OmniGen2 and IRG introduced reflective reasoning post-generation",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Open-sourced code at github.com/tyfeld/MMaDA-Parallel",
      "importance": "okay",
      "assignment": "not_support"
    }
  ],
  "supported_nuggets": [
    {
      "text": "Thinking-aware generation aims to improve complex task performance",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Sequential autoregressive models prone to error accumulation",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Bagel integrates chain-of-thought reasoning in image generation",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Chameleon and Mogao explored interleaved text-image generation",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Image-CoT and GoT used CoT reasoning before image synthesis",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "OmniGen2 and IRG introduced reflective reasoning post-generation",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "partially_supported_nuggets": [
    {
      "text": "Thinking-aware generation aims to improve complex task performance",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Sequential autoregressive models prone to error accumulation",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Bagel integrates chain-of-thought reasoning in image generation",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Chameleon and Mogao explored interleaved text-image generation",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Image-CoT and GoT used CoT reasoning before image synthesis",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "OmniGen2 and IRG introduced reflective reasoning post-generation",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "nuggets_metrics": {
    "strict_vital_score": 0.25,
    "strict_all_score": 0.46153846153846156,
    "vital_score": 0.25,
    "all_score": 0.46153846153846156
  }
}