{
  "qid": "2511.08238v2",
  "query": "Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:\nVision-language fine-tuning has emerged as an efficient paradigm for constructing multimodal foundation models. While textual context often highlights semantic relationships within an image, existing fine-tuning methods typically overlook this information when aligning vision and language, thus leading to suboptimal performance. Toward solving this problem, we propose a method that can improve multimodal alignment and fusion based on both semantics and relationships.Specifically, we first extract multilevel semantic features from different vision encoder to capture more visual cues of the relationships. Then, we learn to project the vision features to group related semantics, among which are more likely to have relationships. Finally, we fuse the visual features with the textual by using inheritable cross-attention, where we globally remove the redundant visual relationships by discarding visual-language feature pairs with low correlation. We evaluate our proposed method on eight foundation models and two downstream tasks, visual question answering and image captioning, and show that it outperforms all existing methods.",
  "nuggets": [
    {
      "text": "Vision-language fine-tuning enhances multimodal model efficiency",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Textual context highlights semantic relationships in images",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Existing methods often overlook semantic relationships",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Proposed method improves multimodal alignment and fusion",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Extract multilevel semantic features from vision encoders",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Project vision features to group related semantics",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Fuse visual and textual features using inheritable cross-attention",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "Remove redundant visual relationships with low correlation",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Outperforms existing methods in image captioning",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Aligning visual and textual representations is crucial",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Cross-attention-based alignment avoids increasing input length",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Flamingo and BLIP use cross-attention for fusion",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Evaluated on eight foundation models and two tasks",
      "importance": "okay",
      "assignment": "not_support"
    },
    {
      "text": "Outperforms existing methods in visual question answering",
      "importance": "okay",
      "assignment": "not_support"
    },
    {
      "text": "Parameter-efficient fine-tuning reduces updated parameters",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "PEFT methods impact model parameters or inputs",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Adversarial DuAl Prompt Tuning achieves domain alignment",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "MetaPrompt enhances domain generalization with dual-modality prompts",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "CLIP4STR boosts image-text alignment and text recognition",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "MemVP trains lightweight alignment modules for vision-language models",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "UniAdapter and MemVP optimize cross-attention computations",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "supported_nuggets": [
    {
      "text": "Aligning visual and textual representations is crucial",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Cross-attention-based alignment avoids increasing input length",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Flamingo and BLIP use cross-attention for fusion",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Parameter-efficient fine-tuning reduces updated parameters",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "PEFT methods impact model parameters or inputs",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Adversarial DuAl Prompt Tuning achieves domain alignment",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "MetaPrompt enhances domain generalization with dual-modality prompts",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "CLIP4STR boosts image-text alignment and text recognition",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "MemVP trains lightweight alignment modules for vision-language models",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "UniAdapter and MemVP optimize cross-attention computations",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "partially_supported_nuggets": [
    {
      "text": "Fuse visual and textual features using inheritable cross-attention",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "Aligning visual and textual representations is crucial",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Cross-attention-based alignment avoids increasing input length",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Flamingo and BLIP use cross-attention for fusion",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Parameter-efficient fine-tuning reduces updated parameters",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "PEFT methods impact model parameters or inputs",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Adversarial DuAl Prompt Tuning achieves domain alignment",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "MetaPrompt enhances domain generalization with dual-modality prompts",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "CLIP4STR boosts image-text alignment and text recognition",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "MemVP trains lightweight alignment modules for vision-language models",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "UniAdapter and MemVP optimize cross-attention computations",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "nuggets_metrics": {
    "strict_vital_score": 0.25,
    "strict_all_score": 0.47619047619047616,
    "vital_score": 0.2916666666666667,
    "all_score": 0.5
  }
}