{
  "qid": "2511.06854v1",
  "query": "Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:\nIrregularly sampled time series (ISTS), characterized by non-uniform time intervals with natural missingness, are prevalent in real-world applications. Existing approaches for ISTS modeling primarily rely on observed values to impute unobserved ones or infer latent dynamics. However, these methods overlook a critical source of learning signal: the reconstruction error inherently produced during model training. Such error implicitly reflects how well a model captures the underlying data structure and can serve as an informative proxy for unobserved values. To exploit this insight, we propose iTimER, a simple yet effective self-supervised pre-training framework for ISTS representation learning. iTimER models the distribution of reconstruction errors over observed values and generates pseudo-observations for unobserved timestamps through a mixup strategy between sampled errors and the last available observations. This transforms unobserved timestamps into noise-aware training targets, enabling meaningful reconstruction signals. A Wasserstein metric aligns reconstruction error distributions between observed and pseudo-observed regions, while a contrastive learning objective enhances the discriminability of learned representations. Extensive experiments on classification, interpolation, and forecasting tasks demonstrate that iTimER consistently outperforms state-of-the-art methods under the ISTS setting.",
  "nuggets": [
    {
      "text": "iTimER leverages reconstruction error for ISTS representation learning",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Reconstruction error serves as a learning signal in iTimER",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "iTimER uses pseudo-observations for unobserved timestamps",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Wasserstein metric aligns reconstruction error distributions",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Contrastive learning enhances representation discriminability in iTimER",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "iTimER outperforms state-of-the-art ISTS methods",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "ISTS characterized by non-uniform time intervals and missingness",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Existing ISTS methods rely on imputation or latent dynamics",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "Reconstruction error highlights uncertainty and noise",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Masked modeling methods predict dropout values in ISTS",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Recurrent Neural Networks used in ISTS modeling",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Attention mechanisms aggregate observation values in ISTS",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Self-supervised learning derives supervision from data itself",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Neural ODE-based approaches model ISTS in continuous time",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Graph Neural Networks struggle with continuous temporal dependencies",
      "importance": "okay",
      "assignment": "partial_support"
    }
  ],
  "supported_nuggets": [
    {
      "text": "iTimER leverages reconstruction error for ISTS representation learning",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Reconstruction error serves as a learning signal in iTimER",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "iTimER uses pseudo-observations for unobserved timestamps",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Reconstruction error highlights uncertainty and noise",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Masked modeling methods predict dropout values in ISTS",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Recurrent Neural Networks used in ISTS modeling",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Attention mechanisms aggregate observation values in ISTS",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Self-supervised learning derives supervision from data itself",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Neural ODE-based approaches model ISTS in continuous time",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "partially_supported_nuggets": [
    {
      "text": "iTimER leverages reconstruction error for ISTS representation learning",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Reconstruction error serves as a learning signal in iTimER",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "iTimER uses pseudo-observations for unobserved timestamps",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Contrastive learning enhances representation discriminability in iTimER",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "Existing ISTS methods rely on imputation or latent dynamics",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "Reconstruction error highlights uncertainty and noise",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Masked modeling methods predict dropout values in ISTS",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Recurrent Neural Networks used in ISTS modeling",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Attention mechanisms aggregate observation values in ISTS",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Self-supervised learning derives supervision from data itself",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Neural ODE-based approaches model ISTS in continuous time",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Graph Neural Networks struggle with continuous temporal dependencies",
      "importance": "okay",
      "assignment": "partial_support"
    }
  ],
  "nuggets_metrics": {
    "strict_vital_score": 0.5833333333333334,
    "strict_all_score": 0.6,
    "vital_score": 0.6666666666666666,
    "all_score": 0.7
  }
}