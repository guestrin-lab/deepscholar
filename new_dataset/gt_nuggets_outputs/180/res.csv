qid,query,nuggets,supported_nuggets,partially_supported_nuggets,nuggets_metrics
2511.07831v1,"Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:
The sim-to-real gap, where agents trained in a simulator face significant performance degradation during testing, is a fundamental challenge in reinforcement learning. Extansive works adopt the framework of distributionally robust RL, to learn a policy that acts robustly under worst case environment shift. Within this framework, our objective is to devise algorithms that are sample efficient with interactive data collection and large state spaces. By assuming d-rectangularity of environment dynamic shift, we identify a fundamental hardness result for learning in online Markov game, and address it by adopting minimum value assumption. Then, a novel least square value iteration type algorithm, DR-CCE-LSI, with exploration bonus devised specifically for multiple agents, is proposed to find an \episilon-approximate robust Coarse Correlated Equilibrium(CCE). To obtain sample efficient learning, we find that: when the feature mapping function satisfies certain properties, our algorithm, DR-CCE-LSI, is able to achieve ε-approximate CCE with a regret bound of O{dHmin{H,1/min{σ_i}}\sqrt{K}}, where K is the number of interacting episodes, H is the horizon length, d is the feature dimension, and \simga_i represents the uncertainty level of player i. Our work introduces the first sample-efficient algorithm for this setting, matches the best result so far in single agent setting, and achieves minimax optimalsample complexity in terms of the feature dimension d. Meanwhile, we also conduct simulation study to validate the efficacy of our algorithm in learning a robust equilibrium.","[{'text': 'Sim-to-real gap challenges reinforcement learning', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'Distributionally robust RL addresses environment shifts', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Sample-efficient algorithms for large state spaces', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'd-rectangularity assumption in environment dynamics', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'Hardness result in online Markov games', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'Minimum value assumption for robust learning', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'DR-CCE-LSI algorithm for multi-agent systems', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'ε-approximate robust Coarse Correlated Equilibrium', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'First sample-efficient algorithm for this setting', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'Matches best single-agent result', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'Minimax optimal sample complexity in feature dimension', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Simulation study validates algorithm efficacy', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'Robust Markov games underexplored', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Robustness in online linear Markov games unstudied', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Regret bound: O(dHmin{H,1/min{σ_i}}√K)', 'importance': 'okay', 'assignment': 'partial_support'}, {'text': 'Robust online linear MDPs extensively studied', 'importance': 'okay', 'assignment': 'partial_support'}, {'text': 'Variance-weighted ridge regression in robust MDPs', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Q-FTRL algorithm breaks multi-agency curse', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Decentralized learning in linear Markov games', 'importance': 'okay', 'assignment': 'support'}, {'text': 'NQOVI algorithm achieves regret bound', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'Distributionally robust RL addresses environment shifts', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Minimax optimal sample complexity in feature dimension', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Robust Markov games underexplored', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Robustness in online linear Markov games unstudied', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Variance-weighted ridge regression in robust MDPs', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Q-FTRL algorithm breaks multi-agency curse', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Decentralized learning in linear Markov games', 'importance': 'okay', 'assignment': 'support'}, {'text': 'NQOVI algorithm achieves regret bound', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'Distributionally robust RL addresses environment shifts', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Minimax optimal sample complexity in feature dimension', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Robust Markov games underexplored', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Robustness in online linear Markov games unstudied', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Regret bound: O(dHmin{H,1/min{σ_i}}√K)', 'importance': 'okay', 'assignment': 'partial_support'}, {'text': 'Robust online linear MDPs extensively studied', 'importance': 'okay', 'assignment': 'partial_support'}, {'text': 'Variance-weighted ridge regression in robust MDPs', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Q-FTRL algorithm breaks multi-agency curse', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Decentralized learning in linear Markov games', 'importance': 'okay', 'assignment': 'support'}, {'text': 'NQOVI algorithm achieves regret bound', 'importance': 'okay', 'assignment': 'support'}]","{'strict_vital_score': 0.2857142857142857, 'strict_all_score': 0.4, 'vital_score': 0.2857142857142857, 'all_score': 0.45}"
