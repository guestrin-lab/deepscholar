{
  "qid": "2511.10032v1",
  "query": "Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:\nAlignment methods in moral domains seek to elicit moral preferences of human stakeholders and incorporate them into AI. This presupposes moral preferences as static targets, but such preferences often evolve over time. Proper alignment of AI to dynamic human preferences should ideally account for \"legitimate\" changes to moral reasoning, while ignoring changes related to attention deficits, cognitive biases, or other arbitrary factors. However, common AI alignment approaches largely neglect temporal changes in preferences, posing serious challenges to proper alignment, especially in high-stakes applications of AI, e.g., in healthcare domains, where misalignment can jeopardize the trustworthiness of the system and yield serious individual and societal harms. This work investigates the extent to which people's moral preferences change over time, and the impact of such changes on AI alignment. Our study is grounded in the kidney allocation domain, where we elicit responses to pairwise comparisons of hypothetical kidney transplant patients from over 400 participants across 3-5 sessions. We find that, on average, participants change their response to the same scenario presented at different times around 6-20% of the time (exhibiting \"response instability\"). Additionally, we observe significant shifts in several participants' retrofitted decision-making models over time (capturing \"model instability\"). The predictive performance of simple AI models decreases as a function of both response and model instability. Moreover, predictive performance diminishes over time, highlighting the importance of accounting for temporal changes in preferences during training. These findings raise fundamental normative and technical challenges relevant to AI alignment, highlighting the need to better understand the object of alignment (what to align to) when user preferences change significantly over time.",
  "nuggets": [
    {
      "text": "AI alignment methods often assume static human preferences",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Human moral preferences evolve over time",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Temporal changes in preferences challenge AI alignment",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Misalignment in healthcare AI can cause serious harms",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Study investigates moral preference changes in kidney allocation",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "AI model performance decreases with preference instability",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "Importance of accounting for temporal changes in AI training",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Need to understand alignment object when preferences change",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Participants show 6-20% response instability over time",
      "importance": "okay",
      "assignment": "not_support"
    },
    {
      "text": "Significant shifts in decision-making models observed",
      "importance": "okay",
      "assignment": "partial_support"
    }
  ],
  "supported_nuggets": [
    {
      "text": "AI alignment methods often assume static human preferences",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Human moral preferences evolve over time",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Temporal changes in preferences challenge AI alignment",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Misalignment in healthcare AI can cause serious harms",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Study investigates moral preference changes in kidney allocation",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Importance of accounting for temporal changes in AI training",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Need to understand alignment object when preferences change",
      "importance": "vital",
      "assignment": "support"
    }
  ],
  "partially_supported_nuggets": [
    {
      "text": "AI alignment methods often assume static human preferences",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Human moral preferences evolve over time",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Temporal changes in preferences challenge AI alignment",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Misalignment in healthcare AI can cause serious harms",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Study investigates moral preference changes in kidney allocation",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "AI model performance decreases with preference instability",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "Importance of accounting for temporal changes in AI training",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Need to understand alignment object when preferences change",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Significant shifts in decision-making models observed",
      "importance": "okay",
      "assignment": "partial_support"
    }
  ],
  "nuggets_metrics": {
    "strict_vital_score": 0.875,
    "strict_all_score": 0.7,
    "vital_score": 0.9375,
    "all_score": 0.8
  }
}