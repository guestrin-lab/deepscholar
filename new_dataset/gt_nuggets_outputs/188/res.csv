qid,query,nuggets,supported_nuggets,partially_supported_nuggets,nuggets_metrics
2511.09677v1,"Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:
Generative Flow Networks (GFlowNets) are powerful samplers for compositional objects that, by design, sample proportionally to a given non-negative reward. Nonetheless, in practice, they often struggle to explore the reward landscape evenly: trajectories toward easy-to-reach regions dominate training, while hard-to-reach modes receive vanishing or uninformative gradients, leading to poor coverage of high-reward areas. We address this imbalance with Boosted GFlowNets, a method that sequentially trains an ensemble of GFlowNets, each optimizing a residual reward that compensates for the mass already captured by previous models. This residual principle reactivates learning signals in underexplored regions and, under mild assumptions, ensures a monotone non-degradation property: adding boosters cannot worsen the learned distribution and typically improves it. Empirically, Boosted GFlowNets achieve substantially better exploration and sample diversity on multimodal synthetic benchmarks and peptide design tasks, while preserving the stability and simplicity of standard trajectory-balance training.","[{'text': 'GFlowNets struggle with exploration on multimodal targets', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Boosted GFlowNets train ensembles to improve exploration', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Residual rewards compensate for mass captured by previous models', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Boosted GFlowNets ensure monotone non-degradation', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'Boosted GFlowNets improve sample diversity and exploration', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'Boosted GFlowNets redistribute probability mass to underexplored modes', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Auxiliary exploration policies use novelty-seeking trajectories', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Adaptive curricula steer sampling toward under-covered regions', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Loss-design perspectives bias training away from mode collapse', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Boosting Variational Inference constructs mixture approximations', 'importance': 'okay', 'assignment': 'support'}, {'text': 'BVI reduces Kullback–Leibler divergence iteratively', 'importance': 'okay', 'assignment': 'partial_support'}, {'text': 'BGFNs require no auxiliary policies or teacher modules', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'GFlowNets struggle with exploration on multimodal targets', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Boosted GFlowNets train ensembles to improve exploration', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Residual rewards compensate for mass captured by previous models', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Boosted GFlowNets redistribute probability mass to underexplored modes', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Auxiliary exploration policies use novelty-seeking trajectories', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Adaptive curricula steer sampling toward under-covered regions', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Loss-design perspectives bias training away from mode collapse', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Boosting Variational Inference constructs mixture approximations', 'importance': 'okay', 'assignment': 'support'}, {'text': 'BGFNs require no auxiliary policies or teacher modules', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'GFlowNets struggle with exploration on multimodal targets', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Boosted GFlowNets train ensembles to improve exploration', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Residual rewards compensate for mass captured by previous models', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Boosted GFlowNets improve sample diversity and exploration', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'Boosted GFlowNets redistribute probability mass to underexplored modes', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Auxiliary exploration policies use novelty-seeking trajectories', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Adaptive curricula steer sampling toward under-covered regions', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Loss-design perspectives bias training away from mode collapse', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Boosting Variational Inference constructs mixture approximations', 'importance': 'okay', 'assignment': 'support'}, {'text': 'BVI reduces Kullback–Leibler divergence iteratively', 'importance': 'okay', 'assignment': 'partial_support'}, {'text': 'BGFNs require no auxiliary policies or teacher modules', 'importance': 'okay', 'assignment': 'support'}]","{'strict_vital_score': 0.6666666666666666, 'strict_all_score': 0.75, 'vital_score': 0.75, 'all_score': 0.8333333333333334}"
