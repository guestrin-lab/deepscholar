qid,query,nuggets,supported_nuggets,partially_supported_nuggets,nuggets_metrics
2510.26792v1,"Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:
We study the ability of Transformer models to learn sequences generated by Permuted Congruential Generators (PCGs), a widely used family of pseudo-random number generators (PRNGs). PCGs introduce substantial additional difficulty over linear congruential generators (LCGs) by applying a series of bit-wise shifts, XORs, rotations and truncations to the hidden state. We show that Transformers can nevertheless successfully perform in-context prediction on unseen sequences from diverse PCG variants, in tasks that are beyond published classical attacks. In our experiments we scale moduli up to $2^{22}$ using up to $50$ million model parameters and datasets with up to $5$ billion tokens. Surprisingly, we find even when the output is truncated to a single bit, it can be reliably predicted by the model. When multiple distinct PRNGs are presented together during training, the model can jointly learn them, identifying structures from different permutations. We demonstrate a scaling law with modulus $m$: the number of in-context sequence elements required for near-perfect prediction grows as $\sqrt{m}$. For larger moduli, optimization enters extended stagnation phases; in our experiments, learning moduli $m \geq 2^{20}$ requires incorporating training data from smaller moduli, demonstrating a critical necessity for curriculum learning. Finally, we analyze embedding layers and uncover a novel clustering phenomenon: the model spontaneously groups the integer inputs into bitwise rotationally-invariant clusters, revealing how representations can transfer from smaller to larger moduli.","[{'text': 'Transformers learn sequences from Permuted Congruential Generators (PCGs).', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'PCGs add complexity with bit-wise shifts, XORs, rotations, truncations.', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'Transformers predict unseen PCG sequences beyond classical attacks.', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Model learns multiple PRNGs, identifying permutation structures.', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'Scaling law: sequence elements grow as $\\sqrt{m}$ for prediction.', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'Larger moduli require curriculum learning for effective training.', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Transformers learn modular arithmetic, revealing structured representations.', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'Experiments scale moduli up to $2^{22}$ with 50 million parameters.', 'importance': 'okay', 'assignment': 'not_support'}, {'text': 'Model predicts single-bit outputs reliably.', 'importance': 'okay', 'assignment': 'not_support'}, {'text': 'Embedding layers show bitwise rotationally-invariant clustering.', 'importance': 'okay', 'assignment': 'not_support'}, {'text': 'Curriculum learning speeds up training for complex tasks.', 'importance': 'okay', 'assignment': 'partial_support'}, {'text': 'AI systems increasingly attack cryptographic schemes.', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'Transformers predict unseen PCG sequences beyond classical attacks.', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Larger moduli require curriculum learning for effective training.', 'importance': 'vital', 'assignment': 'support'}, {'text': 'AI systems increasingly attack cryptographic schemes.', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'Transformers predict unseen PCG sequences beyond classical attacks.', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Larger moduli require curriculum learning for effective training.', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Transformers learn modular arithmetic, revealing structured representations.', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'Curriculum learning speeds up training for complex tasks.', 'importance': 'okay', 'assignment': 'partial_support'}, {'text': 'AI systems increasingly attack cryptographic schemes.', 'importance': 'okay', 'assignment': 'support'}]","{'strict_vital_score': 0.2857142857142857, 'strict_all_score': 0.25, 'vital_score': 0.35714285714285715, 'all_score': 0.3333333333333333}"
