{
  "qid": "2511.09973v1",
  "query": "Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:\nContrastive pre-trained vision-language models, such as CLIP, demonstrate strong generalization abilities in zero-shot classification by leveraging embeddings extracted from image and text encoders. This paper aims to robustly fine-tune these vision-language models on in-distribution (ID) data without compromising their generalization abilities in out-of-distribution (OOD) and zero-shot settings. Current robust fine-tuning methods tackle this challenge by reusing contrastive learning, which was used in pre-training, for fine-tuning. However, we found that these methods distort the geometric structure of the embeddings, which plays a crucial role in the generalization of vision-language models, resulting in limited OOD and zero-shot performance. To address this, we propose Difference Vector Equalization (DiVE), which preserves the geometric structure during fine-tuning. The idea behind DiVE is to constrain difference vectors, each of which is obtained by subtracting the embeddings extracted from the pre-trained and fine-tuning models for the same data sample. By constraining the difference vectors to be equal across various data samples, we effectively preserve the geometric structure. Therefore, we introduce two losses: average vector loss (AVL) and pairwise vector loss (PVL). AVL preserves the geometric structure globally by constraining difference vectors to be equal to their weighted average. PVL preserves the geometric structure locally by ensuring a consistent multimodal alignment. Our experiments demonstrate that DiVE effectively preserves the geometric structure, achieving strong results across ID, OOD, and zero-shot metrics.",
  "nuggets": [
    {
      "text": "DiVE preserves geometric structure in fine-tuning",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Contrastive learning used in vision-language models",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "CLIP demonstrates strong zero-shot generalization",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Robust fine-tuning methods reuse contrastive learning",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "DiVE introduces average and pairwise vector losses",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Geometric structure crucial for model generalization",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Prompt learning maintains zero-shot performance",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Weight ensembling improves OOD and zero-shot performance",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "LP-FT enhances OOD performance with linear probing",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "FLYP uses contrastive learning for fine-tuning",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "ARF combines FLYP and replay technique",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "CyCLIP constrains cosine similarities in embeddings",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "SnD leverages reference dataset for feature distillation",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "supported_nuggets": [
    {
      "text": "DiVE preserves geometric structure in fine-tuning",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Contrastive learning used in vision-language models",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "CLIP demonstrates strong zero-shot generalization",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "DiVE introduces average and pairwise vector losses",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Geometric structure crucial for model generalization",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Prompt learning maintains zero-shot performance",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Weight ensembling improves OOD and zero-shot performance",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "LP-FT enhances OOD performance with linear probing",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "FLYP uses contrastive learning for fine-tuning",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "ARF combines FLYP and replay technique",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "CyCLIP constrains cosine similarities in embeddings",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "SnD leverages reference dataset for feature distillation",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "partially_supported_nuggets": [
    {
      "text": "DiVE preserves geometric structure in fine-tuning",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Contrastive learning used in vision-language models",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "CLIP demonstrates strong zero-shot generalization",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Robust fine-tuning methods reuse contrastive learning",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "DiVE introduces average and pairwise vector losses",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Geometric structure crucial for model generalization",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Prompt learning maintains zero-shot performance",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Weight ensembling improves OOD and zero-shot performance",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "LP-FT enhances OOD performance with linear probing",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "FLYP uses contrastive learning for fine-tuning",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "ARF combines FLYP and replay technique",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "CyCLIP constrains cosine similarities in embeddings",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "SnD leverages reference dataset for feature distillation",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "nuggets_metrics": {
    "strict_vital_score": 0.875,
    "strict_all_score": 0.9230769230769231,
    "vital_score": 0.9375,
    "all_score": 0.9615384615384616
  }
}