{
  "qid": "2511.09585v1",
  "query": "Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:\nVideo-to-Music generation seeks to generate musically appropriate background music that enhances audiovisual immersion for videos. However, current approaches suffer from two critical limitations: 1) incomplete representation of video details, leading to weak alignment, and 2) inadequate temporal and rhythmic correspondence, particularly in achieving precise beat synchronization. To address the challenges, we propose Video Echoed in Music (VeM), a latent music diffusion that generates high-quality soundtracks with semantic, temporal, and rhythmic alignment for input videos. To capture video details comprehensively, VeM employs a hierarchical video parsing that acts as a music conductor, orchestrating multi-level information across modalities. Modality-specific encoders, coupled with a storyboard-guided cross-attention mechanism (SG-CAtt), integrate semantic cues while maintaining temporal coherence through position and duration encoding. For rhythmic precision, the frame-level transition-beat aligner and adapter (TB-As) dynamically synchronize visual scene transitions with music beats. We further contribute a novel video-music paired dataset sourced from e-commerce advertisements and video-sharing platforms, which imposes stricter transition-beat synchronization requirements. Meanwhile, we introduce novel metrics tailored to the task. Experimental results demonstrate superiority, particularly in semantic relevance and rhythmic precision.",
  "nuggets": [
    {
      "text": "VeM enhances video-to-music alignment with diffusion models.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Hierarchical video parsing acts as a music conductor.",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "SG-CAtt integrates semantic cues with temporal coherence.",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "TB-As synchronizes visual transitions with music beats.",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "VeM introduces a novel video-music paired dataset.",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "VeM achieves semantic, temporal, and rhythmic alignment.",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "Diffusion models are effective for conditional music generation.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "CMT projects visual features onto musical attributes.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Textual abstraction loses fine-grained temporal dynamics.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Motion-centric methods neglect broader domains.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "VidMuse involves long-short-term temporal dependencies.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "MuVi and VidMusician improve local semantic correspondence.",
      "importance": "okay",
      "assignment": "partial_support"
    }
  ],
  "supported_nuggets": [
    {
      "text": "VeM enhances video-to-music alignment with diffusion models.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Diffusion models are effective for conditional music generation.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "CMT projects visual features onto musical attributes.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Textual abstraction loses fine-grained temporal dynamics.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Motion-centric methods neglect broader domains.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "VidMuse involves long-short-term temporal dependencies.",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "partially_supported_nuggets": [
    {
      "text": "VeM enhances video-to-music alignment with diffusion models.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "VeM achieves semantic, temporal, and rhythmic alignment.",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "Diffusion models are effective for conditional music generation.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "CMT projects visual features onto musical attributes.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Textual abstraction loses fine-grained temporal dynamics.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Motion-centric methods neglect broader domains.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "VidMuse involves long-short-term temporal dependencies.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "MuVi and VidMusician improve local semantic correspondence.",
      "importance": "okay",
      "assignment": "partial_support"
    }
  ],
  "nuggets_metrics": {
    "strict_vital_score": 0.16666666666666666,
    "strict_all_score": 0.5,
    "vital_score": 0.25,
    "all_score": 0.5833333333333334
  }
}