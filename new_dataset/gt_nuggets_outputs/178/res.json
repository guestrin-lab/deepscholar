{
  "qid": "2511.08136v1",
  "query": "Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:\nIn this work, we study the problem of offline safe imitation learning (IL). In many real-world settings, online interactions can be risky, and accurately specifying the reward and the safety cost information at each timestep can be difficult. However, it is often feasible to collect trajectories reflecting undesirable or risky behavior, implicitly conveying the behavior the agent should avoid. We refer to these trajectories as non-preferred trajectories. Unlike standard IL, which aims to mimic demonstrations, our agent must also learn to avoid risky behavior using non-preferred trajectories. In this paper, we propose a novel approach, SafeMIL, to learn a parameterized cost that predicts if the state-action pair is risky via \\textit{Multiple Instance Learning}. The learned cost is then used to avoid non-preferred behaviors, resulting in a policy that prioritizes safety. We empirically demonstrate that our approach can learn a safer policy that satisfies cost constraints without degrading the reward performance, thereby outperforming several baselines.",
  "nuggets": [
    {
      "text": "Offline Imitation Learning focuses on replicating actions without safety considerations.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "SafeMIL learns a parameterized cost to predict risky state-action pairs.",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "SafeMIL avoids non-preferred behaviors, prioritizing safety in policies.",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "SafeDICE estimates stationary distribution to learn safe policies.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "T-REX, B-Pref, PEBBLE learn reward functions from ranked trajectories.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "OPRL uses offline-RL to learn policies from ranked trajectories.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "D-REX, SSRR generate trajectories by injecting noise into policies.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "DWBC uses positive-unlabeled learning to train a discriminator network.",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "supported_nuggets": [
    {
      "text": "Offline Imitation Learning focuses on replicating actions without safety considerations.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "SafeDICE estimates stationary distribution to learn safe policies.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "T-REX, B-Pref, PEBBLE learn reward functions from ranked trajectories.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "OPRL uses offline-RL to learn policies from ranked trajectories.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "D-REX, SSRR generate trajectories by injecting noise into policies.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "DWBC uses positive-unlabeled learning to train a discriminator network.",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "partially_supported_nuggets": [
    {
      "text": "Offline Imitation Learning focuses on replicating actions without safety considerations.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "SafeDICE estimates stationary distribution to learn safe policies.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "T-REX, B-Pref, PEBBLE learn reward functions from ranked trajectories.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "OPRL uses offline-RL to learn policies from ranked trajectories.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "D-REX, SSRR generate trajectories by injecting noise into policies.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "DWBC uses positive-unlabeled learning to train a discriminator network.",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "nuggets_metrics": {
    "strict_vital_score": 0.3333333333333333,
    "strict_all_score": 0.75,
    "vital_score": 0.3333333333333333,
    "all_score": 0.75
  }
}