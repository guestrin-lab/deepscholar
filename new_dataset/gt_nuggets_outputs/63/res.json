{
  "qid": "2511.09139v1",
  "query": "Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:\nHundreds of benchmarks dedicated to evaluating large models from multiple perspectives have been presented over the past few years. Albeit substantial efforts, most of them remain closed-ended and are prone to overfitting due to the potential data contamination in the ever-growing training corpus of large models, thereby undermining the credibility of the evaluation. Moreover, the increasing scale and scope of current benchmarks with transient metrics, as well as the heavily human-dependent curation procedure, pose significant challenges for timely maintenance and adaptation to gauge the advancing capabilities of large models. In this paper, we introduce MACEval, a \\Multi-Agent Continual Evaluation network for dynamic evaluation of large models, and define a new set of metrics to quantify performance longitudinally and sustainably. MACEval adopts an interactive and autonomous evaluation mode that employs role assignment, in-process data generation, and evaluation routing through a cascaded agent network. Extensive experiments on 9 open-ended tasks with 23 participating large models demonstrate that MACEval is (1) human-free and automatic, mitigating laborious result processing with inter-agent judgment guided; (2) efficient and economical, reducing a considerable amount of data and overhead to obtain similar results compared to related benchmarks; and (3) flexible and scalable, migrating or integrating existing benchmarks via customized evaluation topologies. We hope that MACEval can broaden future directions of large model evaluation.",
  "nuggets": [
    {
      "text": "MACEval introduces a Multi-Agent Continual Evaluation network.",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Dynamic evaluation addresses data contamination in large models.",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "Benchmarks face overfitting due to data contamination.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "MACEval uses role assignment and in-process data generation.",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "MACEval is human-free and automatic.",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "MACEval reduces data and overhead compared to benchmarks.",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "MACEval is flexible and scalable for benchmark integration.",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Data contamination impacts reliability of model evaluations.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "MACEval employs a cascaded agent network for evaluation.",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "MACEval defines new metrics for longitudinal performance.",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Existing benchmarks are static and labor-intensive.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Dynamic evaluation involves evolving instances and diverse queries.",
      "importance": "okay",
      "assignment": "partial_support"
    },
    {
      "text": "Dynamic evaluation modulates problem difficulty and complexity.",
      "importance": "okay",
      "assignment": "partial_support"
    },
    {
      "text": "MACEval demonstrates efficiency on 9 open-ended tasks.",
      "importance": "okay",
      "assignment": "not_support"
    },
    {
      "text": "MACEval aims to broaden future large model evaluation directions.",
      "importance": "okay",
      "assignment": "not_support"
    }
  ],
  "supported_nuggets": [
    {
      "text": "Benchmarks face overfitting due to data contamination.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Data contamination impacts reliability of model evaluations.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Existing benchmarks are static and labor-intensive.",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "partially_supported_nuggets": [
    {
      "text": "Dynamic evaluation addresses data contamination in large models.",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "Benchmarks face overfitting due to data contamination.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Data contamination impacts reliability of model evaluations.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Existing benchmarks are static and labor-intensive.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Dynamic evaluation involves evolving instances and diverse queries.",
      "importance": "okay",
      "assignment": "partial_support"
    },
    {
      "text": "Dynamic evaluation modulates problem difficulty and complexity.",
      "importance": "okay",
      "assignment": "partial_support"
    }
  ],
  "nuggets_metrics": {
    "strict_vital_score": 0.2,
    "strict_all_score": 0.2,
    "vital_score": 0.25,
    "all_score": 0.3
  }
}