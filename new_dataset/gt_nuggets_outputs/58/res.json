{
  "qid": "2511.10091v1",
  "query": "Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:\nLarge Language Models (LLMs) hold rich implicit knowledge and powerful transferability. In this paper, we explore the combination of LLMs with the human skeleton to perform action classification and description. However, when treating LLM as a recognizer, two questions arise: 1) How can LLMs understand skeleton? 2) How can LLMs distinguish among actions? To address these problems, we introduce a novel paradigm named learning Skeleton representation with visUal-motion knowledGe for Action Recognition (SUGAR). In our pipeline, we first utilize off-the-shelf large-scale video models as a knowledge base to generate visual, motion information related to actions. Then, we propose to supervise skeleton learning through this prior knowledge to yield discrete representations. Finally, we use the LLM with untouched pre-training weights to understand these representations and generate the desired action targets and descriptions. Notably, we present a Temporal Query Projection (TQP) module to continuously model the skeleton signals with long sequences. Experiments on several skeleton-based action classification benchmarks demonstrate the efficacy of our SUGAR. Moreover, experiments on zero-shot scenarios show that SUGAR is more versatile than linear-based methods.",
  "nuggets": [
    {
      "text": "SUGAR combines LLMs with human skeleton for action recognition.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "SUGAR uses visual-motion knowledge for skeleton representation.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Temporal Query Projection models skeleton signals with long sequences.",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "SUGAR outperforms in zero-shot scenarios.",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "Multimodal approaches enhance action recognition with skeleton and visual.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "SUGAR uses off-the-shelf video models for knowledge base.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "SUGAR supervises skeleton learning with prior knowledge.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "SUGAR uses LLMs with untouched pre-training weights.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "SUGAR generates action targets and descriptions.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "LLMs hold rich implicit knowledge and transferability.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Unimodal methods include RGB-based and skeleton-based recognition.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Vision-language models like CLIP, BLIP, ALIGN aid action recognition.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "LLMs can generate text with prompt learning.",
      "importance": "okay",
      "assignment": "partial_support"
    },
    {
      "text": "VQ-VAE and LoRA fine-tune LLMs for action tasks.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Skeleton-based recognition improved with ST-GCN, 2s-AGCN, CTR-GCN.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Multimodal fusion requires massive computation.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Vision-language models co-train multiple modalities for representation.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "ActionCLIP uses CLIP strategy for action recognition.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Linguistic knowledge improves skeleton representation learning.",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "supported_nuggets": [
    {
      "text": "SUGAR combines LLMs with human skeleton for action recognition.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "SUGAR uses visual-motion knowledge for skeleton representation.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Multimodal approaches enhance action recognition with skeleton and visual.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "SUGAR uses off-the-shelf video models for knowledge base.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "SUGAR supervises skeleton learning with prior knowledge.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "SUGAR uses LLMs with untouched pre-training weights.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "SUGAR generates action targets and descriptions.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "LLMs hold rich implicit knowledge and transferability.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Unimodal methods include RGB-based and skeleton-based recognition.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Vision-language models like CLIP, BLIP, ALIGN aid action recognition.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "VQ-VAE and LoRA fine-tune LLMs for action tasks.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Skeleton-based recognition improved with ST-GCN, 2s-AGCN, CTR-GCN.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Multimodal fusion requires massive computation.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Vision-language models co-train multiple modalities for representation.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "ActionCLIP uses CLIP strategy for action recognition.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Linguistic knowledge improves skeleton representation learning.",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "partially_supported_nuggets": [
    {
      "text": "SUGAR combines LLMs with human skeleton for action recognition.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "SUGAR uses visual-motion knowledge for skeleton representation.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "SUGAR outperforms in zero-shot scenarios.",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "Multimodal approaches enhance action recognition with skeleton and visual.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "SUGAR uses off-the-shelf video models for knowledge base.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "SUGAR supervises skeleton learning with prior knowledge.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "SUGAR uses LLMs with untouched pre-training weights.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "SUGAR generates action targets and descriptions.",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "LLMs hold rich implicit knowledge and transferability.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Unimodal methods include RGB-based and skeleton-based recognition.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Vision-language models like CLIP, BLIP, ALIGN aid action recognition.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "LLMs can generate text with prompt learning.",
      "importance": "okay",
      "assignment": "partial_support"
    },
    {
      "text": "VQ-VAE and LoRA fine-tune LLMs for action tasks.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Skeleton-based recognition improved with ST-GCN, 2s-AGCN, CTR-GCN.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Multimodal fusion requires massive computation.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Vision-language models co-train multiple modalities for representation.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "ActionCLIP uses CLIP strategy for action recognition.",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Linguistic knowledge improves skeleton representation learning.",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "nuggets_metrics": {
    "strict_vital_score": 0.7777777777777778,
    "strict_all_score": 0.8421052631578947,
    "vital_score": 0.8333333333333334,
    "all_score": 0.8947368421052632
  }
}