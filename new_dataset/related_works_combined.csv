arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2509.23254v1,http://arxiv.org/abs/2509.23254v1,2025-09-27 11:12:04+00:00,ABConformer: Physics-inspired Sliding Attention for Antibody-Antigen Interface Prediction,"Accurate prediction of antibody-antigen (Ab-Ag) interfaces is critical for vaccine design, immunodiagnostics, and therapeutic antibody development. However, achieving reliable predictions from sequences alone remains a challenge. In this paper, we present ABCONFORMER, a model based on the Conformer backbone that captures both local and global features of a biosequence. To accurately capture Ab-Ag interactions, we introduced the physics-inspired sliding attention, enabling residue-level contact recovery without relying on three-dimensional structural data. ABConformer can accurately predict paratopes and epitopes given the antibody and antigen sequence, and predict pan-epitopes on the antigen without antibody information. In comparison experiments, ABCONFORMER achieves state-of-the-art performance on a recent SARS-CoV-2 Ab-Ag dataset, and surpasses widely used sequence-based methods for antibody-agnostic epitope prediction. Ablation studies further quantify the contribution of each component, demonstrating that, compared to conventional cross-attention, sliding attention significantly enhances the precision of epitope prediction. To facilitate reproducibility, we will release the code under an open-source license upon acceptance.","\label{Related Work}

\textbf{Antibody-specific interface prediction methods.} PECAN integrates graph representation, graph convolution, attention, and transfer learning to model Ab-Ag structural relationships and contextually predict interfaces \citep{PECAN}. Honda’s work introduces convolution encoders, transformer encoders and a cross-transformer encoder into the backbone, achieving a multi-task model that simultaneously predicts antibody paratopes and antigen epitopes \citep{Honda}. Epi-EPMP employs a graph attention network (GAT) with fully connected layers to capture structural cues on antibodies and antigens \citep{EPMP}. PeSTo is a parameter-free geometric transformer that directly encodes protein structures as atomic point clouds, using pairwise geometry and multi-head attention to update atom-level scalar and vector states for binding site prediction \citep{PeSTo}. MIPE uses multi-modal contrastive learning (CL)—intra-modal CL to separate binding and non-binding residues within each modality, and inter-modal CL to align sequence and structure representations—along with multi-head attention layers that compute attention matrices for antibodies and antigens to capture their interaction patterns \citep{MIPE}. DeepInterAware can evaluate Ab-Ag affinity, identify binding sites, and predict the binding free energy changes due to mutations. Its Interaction Interface-aware Learner (IIL) embeds antigens with ESM-2 and antibodies with AbLang~\citep{AbLang}, using bilinear attention and convolution blocks to capture interfaces of Ab-Ag complexes \citep{DeepInterAware}. Epi4Ab encodes antigen sequences with ESM-2 and antibody CDRs with AntiBERTy~\citep{AntiBERTy}, and integrates them with structural features of Ab-Ag into residual interaction graphs, a graph attention network then classifying residues as epitopes, potential epitopes or non-epitopes \citep{Epi4Ab}.

\textbf{Antibody-agnostic epitope prediction methods.} BepiPred-3.0 uses ESM-2 embeddings as input to a feedforward neural network (FFNN) to predict both linear and conformational B-cell epitopes \citep{BepiPred-3.0}. DiscoTope-3.0 uses inverse folding representations from ESM-IF1~\citep{ESM-IF1} and is trained on both predicted and solved structures using a positive-unlabelled ensemble strategy, enabling structure-based B-cell epitope prediction \citep{DiscoTope-3.0}. SEMA-1D 2.0 adds a fully-connected layer on an ensemble of five ESM-2 models, while SEMA-3D 2.0 follows the same design but replaces ESM-2 with pre-trained Structure-aware Protein language models (SaProt) \citep{SaProt, SEMA-2.0}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%","\textbf{Antibody-specific interface prediction methods.} PECAN integrates graph representation, graph convolution, attention, and transfer learning to model Ab-Ag structural relationships and contextually predict interfaces \citep{PECAN}. Honda’s work introduces convolution encoders, transformer encoders and a cross-transformer encoder into the backbone, achieving a multi-task model that simultaneously predicts antibody paratopes and antigen epitopes \citep{Honda}. Epi-EPMP employs a graph attention network (GAT) with fully connected layers to capture structural cues on antibodies and antigens \citep{EPMP}. PeSTo is a parameter-free geometric transformer that directly encodes protein structures as atomic point clouds, using pairwise geometry and multi-head attention to update atom-level scalar and vector states for binding site prediction \citep{PeSTo}. MIPE uses multi-modal contrastive learning (CL)—intra-modal CL to separate binding and non-binding residues within each modality, and inter-modal CL to align sequence and structure representations—along with multi-head attention layers that compute attention matrices for antibodies and antigens to capture their interaction patterns \citep{MIPE}. DeepInterAware can evaluate Ab-Ag affinity, identify binding sites, and predict the binding free energy changes due to mutations. Its Interaction Interface-aware Learner (IIL) embeds antigens with ESM-2 and antibodies with AbLang~\citep{AbLang}, using bilinear attention and convolution blocks to capture interfaces of Ab-Ag complexes \citep{DeepInterAware}. Epi4Ab encodes antigen sequences with ESM-2 and antibody CDRs with AntiBERTy~\citep{AntiBERTy}, and integrates them with structural features of Ab-Ag into residual interaction graphs, a graph attention network then classifying residues as epitopes, potential epitopes or non-epitopes \citep{Epi4Ab}.

\textbf{Antibody-agnostic epitope prediction methods.} BepiPred-3.0 uses ESM-2 embeddings as input to a feedforward neural network (FFNN) to predict both linear and conformational B-cell epitopes \citep{BepiPred-3.0}. DiscoTope-3.0 uses inverse folding representations from ESM-IF1~\citep{ESM-IF1} and is trained on both predicted and solved structures using a positive-unlabelled ensemble strategy, enabling structure-based B-cell epitope prediction \citep{DiscoTope-3.0}. SEMA-1D 2.0 adds a fully-connected layer on an ensemble of five ESM-2 models, while SEMA-3D 2.0 follows the same design but replaces ESM-2 with pre-trained Structure-aware Protein language models (SaProt) \citep{SaProt, SEMA-2.0}.","Antibody-specific interface prediction methods.PECAN integrates graph representation, graph convolution, atten-
tion, and transfer learning to model Ab-Ag structural relationships and contextually predict interfaces [20]. Honda’s
work introduces convolution encoders, transformer encoders and a cross-transformer encoder into the backbone,
achieving a multi-task model that simultaneously predicts antibody paratopes and antigen epitopes [21]. Epi-EPMP
employs a graph attention network (GAT) with fully connected layers to capture structural cues on antibodies and
antigens [22]. PeSTo is a parameter-free geometric transformer that directly encodes protein structures as atomic point
clouds, using pairwise geometry and multi-head attention to update atom-level scalar and vector states for binding
site prediction [23]. MIPE uses multi-modal contrastive learning (CL)—intra-modal CL to separate binding and non-
binding residues within each modality, and inter-modal CL to align sequence and structure representations—along
with multi-head attention layers that compute attention matrices for antibodies and antigens to capture their interac-
tion patterns [25]. DeepInterAware can evaluate Ab-Ag affinity, identify binding sites, and predict the binding free
energy changes due to mutations. Its Interaction Interface-aware Learner (IIL) embeds antigens with ESM-2 and an-
tibodies with AbLang [44], using bilinear attention and convolution blocks to capture interfaces of Ab-Ag complexes
[26]. Epi4Ab encodes antigen sequences with ESM-2 and antibody CDRs with AntiBERTy [45], and integrates them
with structural features of Ab-Ag into residual interaction graphs, a graph attention network then classifying residues
as epitopes, potential epitopes or non-epitopes [27].
Antibody-agnostic epitope prediction methods.BepiPred-3.0 uses ESM-2 embeddings as input to a feedforward
neural network (FFNN) to predict both linear and conformational B-cell epitopes [28]. DiscoTope-3.0 uses inverse
folding representations from ESM-IF1 [46] and is trained on both predicted and solved structures using a positive-
unlabelled ensemble strategy, enabling structure-based B-cell epitope prediction [29]. SEMA-1D 2.0 adds a fully-
connected layer on an ensemble of five ESM-2 models, while SEMA-3D 2.0 follows the same design but replaces
ESM-2 with pre-trained Structure-aware Protein language models (SaProt) [47, 30]."
2510.14143v1,http://arxiv.org/abs/2510.14143v1,2025-10-15 22:22:06+00:00,cubic: CUDA-accelerated 3D Bioimage Computing,"Quantitative analysis of multidimensional biological images is useful for understanding complex cellular phenotypes and accelerating advances in biomedical research. As modern microscopy generates ever-larger 2D and 3D datasets, existing computational approaches are increasingly limited by their scalability, efficiency, and integration with modern scientific computing workflows. Existing bioimage analysis tools often lack application programmable interfaces (APIs), do not support graphics processing unit (GPU) acceleration, lack broad 3D image processing capabilities, and/or have poor interoperability for compute-heavy workflows. Here, we introduce cubic, an open-source Python library that addresses these challenges by augmenting widely used SciPy and scikit-image APIs with GPU-accelerated alternatives from CuPy and RAPIDS cuCIM. cubic's API is device-agnostic and dispatches operations to GPU when data reside on the device and otherwise executes on CPU, seamlessly accelerating a broad range of image processing routines. This approach enables GPU acceleration of existing bioimage analysis workflows, from preprocessing to segmentation and feature extraction for 2D and 3D data. We evaluate cubic both by benchmarking individual operations and by reproducing existing deconvolution and segmentation pipelines, achieving substantial speedups while maintaining algorithmic fidelity. These advances establish a robust foundation for scalable, reproducible bioimage analysis that integrates with the broader Python scientific computing ecosystem, including other GPU-accelerated methods, enabling both interactive exploration and automated high-throughput analysis workflows. cubic is openly available at https://github$.$com/alxndrkalinin/cubic","\label{sec:related_work}

\subsection{Traditional bioimage analysis tools}

The bioimage analysis ecosystem is rich with widely used, mature platforms that have 
shaped the field~\cite{haase2022hitchhiker}.
ImageJ/Fiji~\cite{schneider2012nih,schindelin2012fiji} offer a vast plugin
ecosystem and user-friendly graphical user interface (GUI) for both 2D and some 3D processing tasks.
While widely adopted, it requires scripting macros for large-scale processing, making interoperability 
with the Python scientific computing stack challenging.
CellProfiler~\cite{stirling2021cellprofiler4} is implemented in Python and enables repeatable, modular
pipelines for segmentation and feature extraction, and supports headless batch operation.
However, it remains entirely CPU-bound and can be prohibitively slow for large volumetric images. 
% I thought there's some code that now uses GPU, no? If not, then Anne and I need to change what we say in talks about CellProfiler bringing in (some) advances in deep learning :D
% Ah you clarify this later ""Specialized GPU-accelerated plugins for Imagej/Fiji and CellProfiler exist""
Tools like ilastik and QuPath cater to interactive segmentation and classification tasks.
Ilastik~\cite{kreshuk2019ilastik} provides user-friendly pixel/object classification using machine learning,
with optional GPU acceleration only within deep learning modules.
QuPath~\cite{bankhead2017qupath}, designed for whole-slide pathology, now includes GPU support via
PyTorch~\cite{paszke2019pytorch}, but remains limited in core morphometric pipelines.
While these packages excel in usability and community support, they usually lack large 3D
image analysis capabilities and GPU acceleration.
Moreover, implementation of image analysis operations in interactivity-first tools is usually tightly
coupled to the GUI, making it difficult to assemble robust pipelines that involve other tools
from the modern scientific Python stack~\cite{munoz2025cp_measure}.

More recently, napari~\cite{napari2019,chiu2022napari} has emerged as a Python-native, multi-dimensional image viewer that also provides a headless API and a rich plugin ecosystem exposing SciPy and scikit-image~\cite{witz2024napari} APIs. While napari’s programmatic interface enables both interactive and scriptable workflows within a single framework, it still relies on plugin registration and viewer-centric constructs even when run headlessly and does not aim to natively provide GPU support for available image analysis operations.

OpenCV~\cite{opencv_library} and scikit-image~\cite{vanderwalt2014scikit} are popular libraries for image processing
and computer vision that offer programmatic flexibility and implement a wide range of algorithms.
OpenCV, while providing a Python interface, primarily focuses on 2D image analysis.
scikit-image is built on NumPy~\cite{harris2020array} and SciPy~\cite{virtanen2020scipy},
making it easy to integrate into the scientific Python stack.
For example, CellProfiler v4~\cite{stirling2021cellprofiler4} itself packaged most of its core image-processing routines into scikit-image.
Similarly, many end‐to‐end bioimage analysis pipeline frameworks~\cite{rasse2020opsef,palla2022squidpy,comolet2024scalefex,atgu2024microscopytools,olafsson2025spacr} leverage scikit-image routines under the hood to perform image preprocessing, segmentation, feature extraction, and analysis.
However, scikit-image itself is CPU-bound and does not natively support GPU acceleration.

\subsection{GPU-accelerated frameworks and tools}

Specialized GPU-accelerated plugins for Imagej/Fiji and CellProfiler exist—such as 
AutoDeconJ~\cite{autodeconj} for 3D light‑field deconvolution in ImageJ—but these remain
task-specific and not integrated into the broader scientific computing environment.
OpenCL-based CLIJ/CLIJ2~\cite{haase2020_clij,vorkel2020_gpu_macro} brings hundreds of classical 2D and 3D GPU-accelerated
image operations into ImageJ/Fiji.
However, CLIJ is also not natively Python-based and primarily operates within
the ImageJ/Fiji ecosystem.
clEsperanto and its Python binding pyclesperanto~\cite{pyclesperanto2023} address this issue
by exposing CLIJ2 operations to Python users, enabling GPU-accelerated image processing
in a device-agnostic manner.
However, pyclesperanto implements a limited range of bioimage analysis operations with 
an interface specific to CLIJ2.

Deep learning frameworks such as PyTorch~\cite{paszke2019pytorch} and
TensorFlow~\cite{abadi2016tensorflow} natively support GPUs, but are tailored to
neural network training and inference and do not implement most of the conventional
image processing routines.
Cytokit~\cite{czech2019cytokit} implements TensorFlow-based GPU acceleration for image registration, deconvolution and quality scoring, but still relies on CPU-bound scikit-image, CellProfiler and OpenCV routines for preprocessing, segmentation and feature extraction.
PyTorch-based libraries such as Kornia~\cite{riba2020kornia} and torchvision v2~\cite{torchvision2016} offer GPU-accelerated augmentations, resampling and basic preprocessing, but are primarily designed for deep-learning workflows and lack the full spectrum of segmentation and morphometric operations.

CuPy~\cite{cupy} and RAPIDS cuCIM~\cite{cucim} represent a different approach, providing GPU-accelerated
implementations that closely mirror the APIs of NumPy, SciPy, and scikit-image.
CuPy offers a drop-in replacement for NumPy arrays and lower-level signal and image processing operations,
while cuCIM extends this paradigm to image processing routines from scikit-image.
Unlike device-agnostic solutions, these libraries require explicit data management—users
must be aware of which device their data resides on to call the appropriate CPU or GPU implementation accordingly.

Finally, several napari plugins provide GPU-accelerated routines for specific tasks. For example, the napari-accelerated pixel-and-object-classification plugin~\cite{haase2021apoc} implements OpenCL-based Random Forest pixel and object classifiers. The pycudadecon plugin~\cite{lambert2021pycudadecon} implements a CUDA-based accelerated Richardson–Lucy deconvolution~\cite{biggs1997acceleration}. However, these remain narrowly focused on their particular algorithmic domains.
The napari-pyclesperanto-assistant~\cite{clesperanto2020napari} plugin integrates clEsperanto kernels, while inheriting pyclesperanto’s API and the operation set.
The napari-cupy-image-processing plugin~\cite{haase2021napari} exposes GPU-accelerated signal and image processing rountines from CuPy and a handful of skimage-like operations within the napari ecosystem, but its coverage
remains far narrower than cuCIM’s comprehensive mapping.","\subsection{Traditional bioimage analysis tools}

The bioimage analysis ecosystem is rich with widely used, mature platforms that have 
shaped the field~\cite{haase2022hitchhiker}.
ImageJ/Fiji~\cite{schneider2012nih,schindelin2012fiji} offer a vast plugin
ecosystem and user-friendly graphical user interface (GUI) for both 2D and some 3D processing tasks.
While widely adopted, it requires scripting macros for large-scale processing, making interoperability 
with the Python scientific computing stack challenging.
CellProfiler~\cite{stirling2021cellprofiler4} is implemented in Python and enables repeatable, modular
pipelines for segmentation and feature extraction, and supports headless batch operation.
However, it remains entirely CPU-bound and can be prohibitively slow for large volumetric images. 


Tools like ilastik and QuPath cater to interactive segmentation and classification tasks.
Ilastik~\cite{kreshuk2019ilastik} provides user-friendly pixel/object classification using machine learning,
with optional GPU acceleration only within deep learning modules.
QuPath~\cite{bankhead2017qupath}, designed for whole-slide pathology, now includes GPU support via
PyTorch~\cite{paszke2019pytorch}, but remains limited in core morphometric pipelines.
While these packages excel in usability and community support, they usually lack large 3D
image analysis capabilities and GPU acceleration.
Moreover, implementation of image analysis operations in interactivity-first tools is usually tightly
coupled to the GUI, making it difficult to assemble robust pipelines that involve other tools
from the modern scientific Python stack~\cite{munoz2025cp_measure}.

More recently, napari~\cite{napari2019,chiu2022napari} has emerged as a Python-native, multi-dimensional image viewer that also provides a headless API and a rich plugin ecosystem exposing SciPy and scikit-image~\cite{witz2024napari} APIs. While napari’s programmatic interface enables both interactive and scriptable workflows within a single framework, it still relies on plugin registration and viewer-centric constructs even when run headlessly and does not aim to natively provide GPU support for available image analysis operations.

OpenCV~\cite{opencv_library} and scikit-image~\cite{vanderwalt2014scikit} are popular libraries for image processing
and computer vision that offer programmatic flexibility and implement a wide range of algorithms.
OpenCV, while providing a Python interface, primarily focuses on 2D image analysis.
scikit-image is built on NumPy~\cite{harris2020array} and SciPy~\cite{virtanen2020scipy},
making it easy to integrate into the scientific Python stack.
For example, CellProfiler v4~\cite{stirling2021cellprofiler4} itself packaged most of its core image-processing routines into scikit-image.
Similarly, many end‐to‐end bioimage analysis pipeline frameworks~\cite{rasse2020opsef,palla2022squidpy,comolet2024scalefex,atgu2024microscopytools,olafsson2025spacr} leverage scikit-image routines under the hood to perform image preprocessing, segmentation, feature extraction, and analysis.
However, scikit-image itself is CPU-bound and does not natively support GPU acceleration.

\subsection{GPU-accelerated frameworks and tools}

Specialized GPU-accelerated plugins for Imagej/Fiji and CellProfiler exist—such as 
AutoDeconJ~\cite{autodeconj} for 3D light‑field deconvolution in ImageJ—but these remain
task-specific and not integrated into the broader scientific computing environment.
OpenCL-based CLIJ/CLIJ2~\cite{haase2020_clij,vorkel2020_gpu_macro} brings hundreds of classical 2D and 3D GPU-accelerated
image operations into ImageJ/Fiji.
However, CLIJ is also not natively Python-based and primarily operates within
the ImageJ/Fiji ecosystem.
clEsperanto and its Python binding pyclesperanto~\cite{pyclesperanto2023} address this issue
by exposing CLIJ2 operations to Python users, enabling GPU-accelerated image processing
in a device-agnostic manner.
However, pyclesperanto implements a limited range of bioimage analysis operations with 
an interface specific to CLIJ2.

Deep learning frameworks such as PyTorch~\cite{paszke2019pytorch} and
TensorFlow~\cite{abadi2016tensorflow} natively support GPUs, but are tailored to
neural network training and inference and do not implement most of the conventional
image processing routines.
Cytokit~\cite{czech2019cytokit} implements TensorFlow-based GPU acceleration for image registration, deconvolution and quality scoring, but still relies on CPU-bound scikit-image, CellProfiler and OpenCV routines for preprocessing, segmentation and feature extraction.
PyTorch-based libraries such as Kornia~\cite{riba2020kornia} and torchvision v2~\cite{torchvision2016} offer GPU-accelerated augmentations, resampling and basic preprocessing, but are primarily designed for deep-learning workflows and lack the full spectrum of segmentation and morphometric operations.

CuPy~\cite{cupy} and RAPIDS cuCIM~\cite{cucim} represent a different approach, providing GPU-accelerated
implementations that closely mirror the APIs of NumPy, SciPy, and scikit-image.
CuPy offers a drop-in replacement for NumPy arrays and lower-level signal and image processing operations,
while cuCIM extends this paradigm to image processing routines from scikit-image.
Unlike device-agnostic solutions, these libraries require explicit data management—users
must be aware of which device their data resides on to call the appropriate CPU or GPU implementation accordingly.

Finally, several napari plugins provide GPU-accelerated routines for specific tasks. For example, the napari-accelerated pixel-and-object-classification plugin~\cite{haase2021apoc} implements OpenCL-based Random Forest pixel and object classifiers. The pycudadecon plugin~\cite{lambert2021pycudadecon} implements a CUDA-based accelerated Richardson–Lucy deconvolution~\cite{biggs1997acceleration}. However, these remain narrowly focused on their particular algorithmic domains.
The napari-pyclesperanto-assistant~\cite{clesperanto2020napari} plugin integrates clEsperanto kernels, while inheriting pyclesperanto’s API and the operation set.
The napari-cupy-image-processing plugin~\cite{haase2021napari} exposes GPU-accelerated signal and image processing rountines from CuPy and a handful of skimage-like operations within the napari ecosystem, but its coverage
remains far narrower than cuCIM’s comprehensive mapping.",
2511.06356v1,http://arxiv.org/abs/2511.06356v1,2025-11-09 12:29:16+00:00,Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets,"Chemical reaction prediction remains a fundamental challenge in organic chemistry, where existing machine learning models face two critical limitations: sensitivity to input permutations (molecule/atom orderings) and inadequate modeling of substructural interactions governing reactivity. These shortcomings lead to inconsistent predictions and poor generalization to real-world scenarios. To address these challenges, we propose ReaDISH, a novel reaction prediction model that learns permutation-invariant representations while incorporating interaction-aware features. It introduces two innovations: (1) symmetric difference shingle encoding, which computes molecular shingle differences to capture reaction-specific structural changes while eliminating order sensitivity; and (2) geometry-structure interaction attention, a mechanism that models intra- and inter-molecular interactions at the shingle level. Extensive experiments demonstrate that ReaDISH improves reaction prediction performance across diverse benchmarks. It shows enhanced robustness with an average improvement of 8.76% on R$^2$ under permutation perturbations.","\label{sec:related}
\paragraph{Reaction performance prediction.} 
Chemical reactions involve complex transformations among multiple molecular entities, such as reactants, catalysts, and reagents. Accurate prediction of reaction outcomes requires expressive and chemically informed representations. Early approaches rely heavily on handcrafted features as molecular fingerprint features derived from domain knowledge, to encode atomic and physicochemical properties \cite{rea_BH, rea_SM, rea_MFF_fp, rea_gnn_desc, rea_NiCOlit}. 
Recent advances in deep learning for reaction prediction can be grouped into three main categories: sequence-based, graph-based, and conformer-based models. 
Sequence-based models represent chemical reactions using linear notations such as SMILES and apply neural sequence architectures to learn patterns in tokenized strings. Notable examples include YieldBERT \cite{rea_bert} and YieldBERT-DA \cite{rea_bert_augmentation}, which leverage BERT-based \cite{BERT} encoders; MolFormer \cite{rea_tf}, which adopts a transformer \cite{transformer} architecture; and T5Chem \cite{rea_t5}, which builds on the T5 language model \cite{t5}. 
Graph-based methods encode molecular structures as graphs and extract meaningful representations using graph neural networks (GNNs). Rxn Hypergraph \cite{permutation_3} employs hypergraph attention to capture molecular interactions. UA-GNN \cite{rea_gnn_sum} sums reactant embeddings via GNNs and concatenates them with product representations. UAM \cite{rea_uam} combines multi-view inputs, including graphs, SMILES, and molecular fingerprints, and aggregates their embeddings. LocalTransform \cite{rea_gnn_template} focuses on local structural changes by encoding atom-level differences between reactants and products. 
Conformer-based models incorporate 3D atomic coordinates to learn geometry-aware representations, which can better capture stereoelectronic factors influencing reactivity. ReaMVP \cite{rea_mvp} combines Bi-GRU networks \cite{gru} with multi-view pre-training on conformers and SMILES. YieldFCP \cite{rea_yieldfcp} introduces a cross-modal projector to align conformer and SMILES representations for yield prediction. 
Despite their respective advantages, these methods often overlook the rich intra- and inter-molecular interactions that influence reaction outcomes. In this work, we address this gap by introducing a geometry- and structure-enhanced modeling approach that explicitly incorporates such interactions into the representation learning process.

\paragraph{Molecular substructure learning.} 
Substructures (functional groups, motifs, and fragments) play a pivotal role in determining molecular properties and, by extension, reaction outcomes. Several fragment-based approaches have been proposed to decompose molecules into chemically meaningful parts. RECAP \cite{RECAP} introduces predefined cleavage rules to generate fragments at drug-like bond types, while BRICS \cite{BRICS} focuses on retrosynthetically relevant splits. ReLMole \cite{ReLMole} further generalizes this process using graph-based heuristics to automatically extract relevant substructures. In parallel, fingerprinting methods such as ECFP \cite{ECFP} iteratively encode local atomic environments, capturing circular subgraphs of increasing radii. These subgraphs, referred to as molecular shingles \cite{shingle_0, shingle_1, shingle_2}, serve as a compact and effective representation of chemical structures. DRFP \cite{rea_DRFP} extracts shingles and applies a symmetric difference operation on reactant and product shingle sets to obtain reaction fingerprints. 
Unlike DRFP, our method directly models the interaction-aware symmetric difference via a transformer, allowing it to capture fine-grained transformations at the shingle level.
% Building on this idea, we adopt a shingle-based representation due to its computational efficiency and strong chemical interpretability. This allows our model to analyze molecular structures at a subgraph level while preserving the structural and chemical integrity necessary for reaction modeling.","\paragraph{Reaction performance prediction.} 
Chemical reactions involve complex transformations among multiple molecular entities, such as reactants, catalysts, and reagents. Accurate prediction of reaction outcomes requires expressive and chemically informed representations. Early approaches rely heavily on handcrafted features as molecular fingerprint features derived from domain knowledge, to encode atomic and physicochemical properties \cite{rea_BH, rea_SM, rea_MFF_fp, rea_gnn_desc, rea_NiCOlit}. 
Recent advances in deep learning for reaction prediction can be grouped into three main categories: sequence-based, graph-based, and conformer-based models. 
Sequence-based models represent chemical reactions using linear notations such as SMILES and apply neural sequence architectures to learn patterns in tokenized strings. Notable examples include YieldBERT \cite{rea_bert} and YieldBERT-DA \cite{rea_bert_augmentation}, which leverage BERT-based \cite{BERT} encoders; MolFormer \cite{rea_tf}, which adopts a transformer \cite{transformer} architecture; and T5Chem \cite{rea_t5}, which builds on the T5 language model \cite{t5}. 
Graph-based methods encode molecular structures as graphs and extract meaningful representations using graph neural networks (GNNs). Rxn Hypergraph \cite{permutation_3} employs hypergraph attention to capture molecular interactions. UA-GNN \cite{rea_gnn_sum} sums reactant embeddings via GNNs and concatenates them with product representations. UAM \cite{rea_uam} combines multi-view inputs, including graphs, SMILES, and molecular fingerprints, and aggregates their embeddings. LocalTransform \cite{rea_gnn_template} focuses on local structural changes by encoding atom-level differences between reactants and products. 
Conformer-based models incorporate 3D atomic coordinates to learn geometry-aware representations, which can better capture stereoelectronic factors influencing reactivity. ReaMVP \cite{rea_mvp} combines Bi-GRU networks \cite{gru} with multi-view pre-training on conformers and SMILES. YieldFCP \cite{rea_yieldfcp} introduces a cross-modal projector to align conformer and SMILES representations for yield prediction. 
Despite their respective advantages, these methods often overlook the rich intra- and inter-molecular interactions that influence reaction outcomes. In this work, we address this gap by introducing a geometry- and structure-enhanced modeling approach that explicitly incorporates such interactions into the representation learning process.

\paragraph{Molecular substructure learning.} 
Substructures (functional groups, motifs, and fragments) play a pivotal role in determining molecular properties and, by extension, reaction outcomes. Several fragment-based approaches have been proposed to decompose molecules into chemically meaningful parts. RECAP \cite{RECAP} introduces predefined cleavage rules to generate fragments at drug-like bond types, while BRICS \cite{BRICS} focuses on retrosynthetically relevant splits. ReLMole \cite{ReLMole} further generalizes this process using graph-based heuristics to automatically extract relevant substructures. In parallel, fingerprinting methods such as ECFP \cite{ECFP} iteratively encode local atomic environments, capturing circular subgraphs of increasing radii. These subgraphs, referred to as molecular shingles \cite{shingle_0, shingle_1, shingle_2}, serve as a compact and effective representation of chemical structures. DRFP \cite{rea_DRFP} extracts shingles and applies a symmetric difference operation on reactant and product shingle sets to obtain reaction fingerprints. 
Unlike DRFP, our method directly models the interaction-aware symmetric difference via a transformer, allowing it to capture fine-grained transformations at the shingle level.",
2510.03370v2,http://arxiv.org/abs/2510.03370v2,2025-10-03 07:42:22+00:00,InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions,"Multimodal protein language models deliver strong performance on mutation-effect prediction, but training such models from scratch demands substantial computational resources. In this paper, we propose a fine-tuning framework called InstructPLM-mu and try to answer a question: \textit{Can multimodal fine-tuning of a pretrained, sequence-only protein language model match the performance of models trained end-to-end? } Surprisingly, our experiments show that fine-tuning ESM2 with structural inputs can reach performance comparable to ESM3. To understand how this is achieved, we systematically compare three different feature-fusion designs and fine-tuning recipes. Our results reveal that both the fusion method and the tuning strategy strongly affect final accuracy, indicating that the fine-tuning process is not trivial. We hope this work offers practical guidance for injecting structure into pretrained protein language models and motivates further research on better fusion mechanisms and fine-tuning protocols.","\subsection{Protein mutation prediction} 
Protein mutation prediction is central to understanding protein function and guiding protein engineering. Classical approaches such as Rosetta\citep{alford2017rosetta} and ABACUS2\citep{xiong2020increasing} rely on energy-based scoring, but are hindered by sampling limitations and biases in their underlying potentials. Deep learning has opened new directions: models like ESM-1v\citep{meier2021language} predict mutation effects from large-scale sequence data, while structure-aware methods such as ProSST\citep{li2024prosst} and Pythia\citep{sun2025structure} further improve accuracy by incorporating structural information.

Benchmarking efforts such as ProteinGym~\citep{notin2023proteingym} have underscored the power of pre-trained PLMs in protein modeling. ProteinGym evaluated over 250 deep mutational scanning assays, revealing that PLMs effectively capture evolutionary constraints and generalize across diverse proteins. For instance, AIDO.Protein~\citep{sun2024mixture}, a state-of-the-art PLM with a mixture-of-experts architecture, highlights the potential of PLMs to scale up protein modeling tasks with enhanced computational efficiency. Meanwhile, models like VenusREM~\citep{tan2024retrieval} and S3F~\citep{zhang2024multi} leverage multimodal information—integrating structure, sequence, evolution, and surface features—to precisely model local conformational and energetic changes, which significantly boosts model performance.

\subsection{Multimodal Alignment}

Multimodal alignment aligns heterogeneous features across modalities to enhance cross-modal understanding and specific task performance~\citep{baltruvsaitis2018multimodal}. This field has gained prominence with advances in large language and vision models~\citep{alayrac2022flamingo}. Key challenges in multimodal learning include Feature Fusion and Training Paradigm, which are critical for model performance~\citep{tong2024cambrian,li2024multimodal}.

\textbf{Feature Fusion.}
Previous studies on multimodal alignment largely focus on the way of projection between different modalities. 
For example, MLP projection methods successfully bridge visual features to LLM token spaces~\citep{liu2023visual,liu2024improved,li2024llava}. 
Query-based resampling optimizes computational efficiency through cross-attention compression of visual tokens~\citep{Qwen-VL}.
Architectures with gated or sparse cross-attention layers for deeper multimodal integration~\citep{alayrac2022flamingo,awadalla2023openflamingo}. 
The main goal of these methods is to discuss how to deal with images with different resolutions and scales.
However, recent work highlights that not only the manipulation of multimodal features but also the manner in which these features are fused inside the language model is crucial.
DeepStack~\citep{meng2024deepstack}, for instance, demonstrates that multimodal performance can be enhanced by injecting vision features into multiple layers of the LLM.
Similarly, a recent study systematically examines four different fusion strategies across a broad range of NLP tasks~\citep{lin2025multi}.
Despite these advances, multimodal fusion strategies remain underexplored in the context of protein language models.
Notably, unlike vision–language models, protein structural features can be naturally aggregated at the residue level, which facilitates fine-grained integration of structural signals into sequence representations and opens up opportunities for designing more efficient and biologically informed fusion mechanisms.

% However, unlike vision-language research where alternative alignment mechanisms have been extensively studied~\citep{lin2025multi}, protein multimodal studies still lack a principled comparison of how sequence-structure feature alignment mechanisms affect representation quality and downstream performance. 
% As the 
% Effective multimodal alignment fundamentally depends on feature fusion methodologies~\citep{baltruvsaitis2018multimodal}. MLP projection methods successfully bridge visual features to LLM token spaces~\citep{liu2023visual,liu2024improved,li2024llava}. Query-based resampling optimizes computational efficiency through cross-attention compression of visual tokens~\citep{Qwen-VL}. Architectures with gated or sparse cross-attention layers for deeper multimodal integration~\citep{alayrac2022flamingo,awadalla2023openflamingo}. 
% Unlike vision-language multimodal alignment which bridges abstract semantic concepts, protein sequence and structure modalities exhibit a positional biophysical mapping where each sequence token deterministically maps to a structural token.
% Methods like ESM3~\citep{hayes2025simulating} leverage this inherent predictability through unified multi-track architectures and geometric attention mechanisms, integrating structure information for precise functional inference.
% These methodological choices directly influence cross-modal task performance~\citep{tong2024cambrian,lin2025multi}.

\textbf{Training Paradigm.}
The end-to-end training paradigm jointly optimizes all parameters in a single phase, pursuing global optimization at the cost of high computational demand and potential suboptimal alignment due to limited intermediate refinement~\citep{tong2024cambrian}. In contrast, multi-stage training separately fine-tunes modality-specific modules (e.g., image encoder) before full-model optimization, improving efficiency and final performance~\citep{liu2023visual,wadekar2024evolution,wu2025qwen}. Unified pretraining integrates multimodal inputs within a single framework, typically employing masked or autoregressive objectives to achieve cross-modal fusion~\citep{zhu2504internvl3}. Additionally, strategies such as reinforcement learning have been proposed for supervision-efficient alignment in specific contexts~\citep{sun2023aligning,chu2025sft}.

Inspired by advances in vision-language multimodal alignment, similar sequence-structure alignment strategies are now being adapted for protein modeling~\citep{su2023saprot,li2024prosst,qiu2024instructplm,hayes2025simulating}.
Here, we compare multiple alignment mechanisms between pre-trained sequence and structure modules, evaluating their performance on protein mutation prediction~\citep{notin2023proteingym}. 
This task is central to protein engineering, as accurately predicting the functional effects of mutations (e.g., on stability, binding, and activity) requires high-quantity representation and deep integration of both sequence and structural information~\citep{meier2021language}. 
It thus provides a rigorous test for assessing protein multimodal model quality and their ability to integrate sequence-structure relationships for precise functional inference.





\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/figures_cropped-1.pdf}
    \caption{Comparison of three different multimodal fusion strategies of InstructPLM-mu: cross attention (Left), Channel-wise Concat (Middle), and Token-wise Concat (Right).}
    \label{fig:main}
\end{figure}","\subsection{Protein mutation prediction} 
Protein mutation prediction is central to understanding protein function and guiding protein engineering. Classical approaches such as Rosetta\citep{alford2017rosetta} and ABACUS2\citep{xiong2020increasing} rely on energy-based scoring, but are hindered by sampling limitations and biases in their underlying potentials. Deep learning has opened new directions: models like ESM-1v\citep{meier2021language} predict mutation effects from large-scale sequence data, while structure-aware methods such as ProSST\citep{li2024prosst} and Pythia\citep{sun2025structure} further improve accuracy by incorporating structural information.

Benchmarking efforts such as ProteinGym~\citep{notin2023proteingym} have underscored the power of pre-trained PLMs in protein modeling. ProteinGym evaluated over 250 deep mutational scanning assays, revealing that PLMs effectively capture evolutionary constraints and generalize across diverse proteins. For instance, AIDO.Protein~\citep{sun2024mixture}, a state-of-the-art PLM with a mixture-of-experts architecture, highlights the potential of PLMs to scale up protein modeling tasks with enhanced computational efficiency. Meanwhile, models like VenusREM~\citep{tan2024retrieval} and S3F~\citep{zhang2024multi} leverage multimodal information—integrating structure, sequence, evolution, and surface features—to precisely model local conformational and energetic changes, which significantly boosts model performance.

\subsection{Multimodal Alignment}

Multimodal alignment aligns heterogeneous features across modalities to enhance cross-modal understanding and specific task performance~\citep{baltruvsaitis2018multimodal}. This field has gained prominence with advances in large language and vision models~\citep{alayrac2022flamingo}. Key challenges in multimodal learning include Feature Fusion and Training Paradigm, which are critical for model performance~\citep{tong2024cambrian,li2024multimodal}.

\textbf{Feature Fusion.}
Previous studies on multimodal alignment largely focus on the way of projection between different modalities. 
For example, MLP projection methods successfully bridge visual features to LLM token spaces~\citep{liu2023visual,liu2024improved,li2024llava}. 
Query-based resampling optimizes computational efficiency through cross-attention compression of visual tokens~\citep{Qwen-VL}.
Architectures with gated or sparse cross-attention layers for deeper multimodal integration~\citep{alayrac2022flamingo,awadalla2023openflamingo}. 
The main goal of these methods is to discuss how to deal with images with different resolutions and scales.
However, recent work highlights that not only the manipulation of multimodal features but also the manner in which these features are fused inside the language model is crucial.
DeepStack~\citep{meng2024deepstack}, for instance, demonstrates that multimodal performance can be enhanced by injecting vision features into multiple layers of the LLM.
Similarly, a recent study systematically examines four different fusion strategies across a broad range of NLP tasks~\citep{lin2025multi}.
Despite these advances, multimodal fusion strategies remain underexplored in the context of protein language models.
Notably, unlike vision–language models, protein structural features can be naturally aggregated at the residue level, which facilitates fine-grained integration of structural signals into sequence representations and opens up opportunities for designing more efficient and biologically informed fusion mechanisms.








\textbf{Training Paradigm.}
The end-to-end training paradigm jointly optimizes all parameters in a single phase, pursuing global optimization at the cost of high computational demand and potential suboptimal alignment due to limited intermediate refinement~\citep{tong2024cambrian}. In contrast, multi-stage training separately fine-tunes modality-specific modules (e.g., image encoder) before full-model optimization, improving efficiency and final performance~\citep{liu2023visual,wadekar2024evolution,wu2025qwen}. Unified pretraining integrates multimodal inputs within a single framework, typically employing masked or autoregressive objectives to achieve cross-modal fusion~\citep{zhu2504internvl3}. Additionally, strategies such as reinforcement learning have been proposed for supervision-efficient alignment in specific contexts~\citep{sun2023aligning,chu2025sft}.

Inspired by advances in vision-language multimodal alignment, similar sequence-structure alignment strategies are now being adapted for protein modeling~\citep{su2023saprot,li2024prosst,qiu2024instructplm,hayes2025simulating}.
Here, we compare multiple alignment mechanisms between pre-trained sequence and structure modules, evaluating their performance on protein mutation prediction~\citep{notin2023proteingym}. 
This task is central to protein engineering, as accurately predicting the functional effects of mutations (e.g., on stability, binding, and activity) requires high-quantity representation and deep integration of both sequence and structural information~\citep{meier2021language}. 
It thus provides a rigorous test for assessing protein multimodal model quality and their ability to integrate sequence-structure relationships for precise functional inference.",
2510.00027v2,http://arxiv.org/abs/2510.00027v2,2025-09-25 22:15:10+00:00,Learning Inter-Atomic Potentials without Explicit Equivariance,"Accurate and scalable machine-learned inter-atomic potentials (MLIPs) are essential for molecular simulations ranging from drug discovery to new material design. Current state-of-the-art models enforce roto-translational symmetries through equivariant neural network architectures, a hard-wired inductive bias that can often lead to reduced flexibility, computational efficiency, and scalability. In this work, we introduce TransIP: Transformer-based Inter-Atomic Potentials, a novel training paradigm for interatomic potentials achieving symmetry compliance without explicit architectural constraints. Our approach guides a generic non-equivariant Transformer-based model to learn SO(3)-equivariance by optimizing its representations in the embedding space. Trained on the recent Open Molecules (OMol25) collection, a large and diverse molecular dataset built specifically for MLIPs and covering different types of molecules (including small organics, biomolecular fragments, and electrolyte-like species), TransIP effectively learns symmetry in its latent space, providing low equivariance error. Further, compared to a data augmentation baseline, TransIP achieves 40% to 60% improvement in performance across varying OMol25 dataset sizes. More broadly, our work shows that learned equivariance can be a powerful and efficient alternative to augmentation-based MLIP models.","\label{Related Work}

\textbf{ML Interatomic Potentials.} 
Using machine learning (ML) methods to predict energies and forces of different molecular systems and materials has been an active area of research \citep{Schutt2017SchNet, chmiela2022accurateglobalmachinelearning, Musaelian2023Allegro, Liao2024EquiformerV2, Yang2025EfficientEquivariantMLIP, yuan2025foundation}. 
Due to the intricate 3D structures of atomistic systems, equivariant message-passing neural networks have been an essential backbone in this domain. For example, \citet{gasteiger2022directionalmessagepassingmolecular, klicpera2021gemnet} introduced equivariant directional message passing between pairs of atoms with a spherical harmonics representation.  In contrast, \citet{Batzner2022Equivariant}
developed equivariant convolution with tensor-products and \citet{batatia2023macehigherorderequivariant} built higher-order messages with equivariant graph neural networks \citep{satorras2022en}. Additionally, \citet{pmlr-v202-passaro23a} reduced the computational complexity of SO(3) convolution and replaced it with SO(2) convolutions, which have been used as a backbone for MLIPs \citep{fu2025learning}. More recently, \cite{rhodes2025orbv3atomisticsimulationscale} presented Orb-v3 models with improved computational efficiency, built on Graph Network Simulators \citep{10.5555/3524938.3525722}.

% \textbf{Equivariant models.}

\textbf{Unconstrained ML models.} While current-state-of-the-art MLIP models primarily rely on equivariant GNNs, unconstrained models are actively used in other domains. For example, integrating data augmentation via image transformations has been used in different vision tasks, from classification \citep{ inoue2018dataaugmentationpairingsamples, dosovitskiy2021an, rahat2024dataaugmentationimageclassification} to segmentation \citep{negassi2021smartsamplingaugmentoptimalefficientdata, yu2024diffusionbaseddataaugmentationnuclei}.  
For geometric data, the use of unconstrained models and diffusion Transformers (without explicit equivariance constraints) has been a recent trend in generative tasks, e.g., AlphaFold 3 for biomolecular structure prediction \citep{Abramson2024x} as well as molecular conformation and materials generation \citep{wang2024swallowingbitterpillsimplified, zhang2025symdiff, joshi2025allatom}.  
In contrast, several works have been introduced to overcome the limitations of strictly equivariant GNNs by enforcing symmetry via frame averaging over geometric inputs  \citep{puny2022frameaveraginginvariantequivariant, pmlr-v202-duval23a, lin2024equivariance, huang2024proteinnucleicacidcomplexmodeling, DBLP:conf/icml/DymLS24}; learning canonicalization functions that map inputs to a canonical orientation before prediction \citep{kaba2022equivariance, baker2024an, ma2024canonizationperspectiveinvariantequivariant, ICLR2025_db7534a0}; or learning equivariance through data augmentation with molecule-specific graph-based architectures \citep{qu2024importance, mazitov2025petmadlightweightuniversalinteratomic}. However, in this work, we demonstrate that an unconstrained general-purpose Transformer model can serve as a backbone for MLIPs, which replaces graph-based inductive biases with a scalable latent equivariance objective that implicitly learns equivariant features without explicit equivariance constraints.

% However, in this work, we demonstrate that an unconstrained Transformer can serve as a backbone for MLIPs - simplifying graph-based approaches such as ESCaIP \citep{qu2024importance} - and propose a latent equivariance objective that implicitly learns equivariant features without explicit equivariance constraints. 
% While there are some works that take the direction of scaling graph neural networks via some architectural changes \citep{qu2024importance}, they still exist within the space of achieving equivariance by design. However, in our work, we go beyond the scalability limitations of such architecturally constrained models by developing a paradigm where general Transformer blocks can simply learn symmetry from the data with implicit latent equivariance. This breaks us free from the need to constrain the architecture and allows us to benefit from the scalability of self-attention models.","\textbf{ML Interatomic Potentials.} 
Using machine learning (ML) methods to predict energies and forces of different molecular systems and materials has been an active area of research \citep{Schutt2017SchNet, chmiela2022accurateglobalmachinelearning, Musaelian2023Allegro, Liao2024EquiformerV2, Yang2025EfficientEquivariantMLIP, yuan2025foundation}. 
Due to the intricate 3D structures of atomistic systems, equivariant message-passing neural networks have been an essential backbone in this domain. For example, \citet{gasteiger2022directionalmessagepassingmolecular, klicpera2021gemnet} introduced equivariant directional message passing between pairs of atoms with a spherical harmonics representation.  In contrast, \citet{Batzner2022Equivariant}
developed equivariant convolution with tensor-products and \citet{batatia2023macehigherorderequivariant} built higher-order messages with equivariant graph neural networks \citep{satorras2022en}. Additionally, \citet{pmlr-v202-passaro23a} reduced the computational complexity of SO(3) convolution and replaced it with SO(2) convolutions, which have been used as a backbone for MLIPs \citep{fu2025learning}. More recently, \cite{rhodes2025orbv3atomisticsimulationscale} presented Orb-v3 models with improved computational efficiency, built on Graph Network Simulators \citep{10.5555/3524938.3525722}.



\textbf{Unconstrained ML models.} While current-state-of-the-art MLIP models primarily rely on equivariant GNNs, unconstrained models are actively used in other domains. For example, integrating data augmentation via image transformations has been used in different vision tasks, from classification \citep{ inoue2018dataaugmentationpairingsamples, dosovitskiy2021an, rahat2024dataaugmentationimageclassification} to segmentation \citep{negassi2021smartsamplingaugmentoptimalefficientdata, yu2024diffusionbaseddataaugmentationnuclei}.  
For geometric data, the use of unconstrained models and diffusion Transformers (without explicit equivariance constraints) has been a recent trend in generative tasks, e.g., AlphaFold 3 for biomolecular structure prediction \citep{Abramson2024x} as well as molecular conformation and materials generation \citep{wang2024swallowingbitterpillsimplified, zhang2025symdiff, joshi2025allatom}.  
In contrast, several works have been introduced to overcome the limitations of strictly equivariant GNNs by enforcing symmetry via frame averaging over geometric inputs  \citep{puny2022frameaveraginginvariantequivariant, pmlr-v202-duval23a, lin2024equivariance, huang2024proteinnucleicacidcomplexmodeling, DBLP:conf/icml/DymLS24}; learning canonicalization functions that map inputs to a canonical orientation before prediction \citep{kaba2022equivariance, baker2024an, ma2024canonizationperspectiveinvariantequivariant, ICLR2025_db7534a0}; or learning equivariance through data augmentation with molecule-specific graph-based architectures \citep{qu2024importance, mazitov2025petmadlightweightuniversalinteratomic}. However, in this work, we demonstrate that an unconstrained general-purpose Transformer model can serve as a backbone for MLIPs, which replaces graph-based inductive biases with a scalable latent equivariance objective that implicitly learns equivariant features without explicit equivariance constraints.",
2509.24693v1,http://arxiv.org/abs/2509.24693v1,2025-09-29 12:27:38+00:00,Brain Harmony: A Multimodal Foundation Model Unifying Morphology and Function into 1D Tokens,"We present Brain Harmony (BrainHarmonix), the first multimodal brain foundation model that unifies structural morphology and functional dynamics into compact 1D token representations. The model was pretrained on two of the largest neuroimaging datasets to date, encompassing 64,594 T1-weighted structural MRI 3D volumes (~ 14 million images) and 70,933 functional MRI (fMRI) time series. BrainHarmonix is grounded in two foundational neuroscience principles: structure complements function - structural and functional modalities offer distinct yet synergistic insights into brain organization; function follows structure - brain functional dynamics are shaped by cortical morphology. The modular pretraining process involves single-modality training with geometric pre-alignment followed by modality fusion through shared brain hub tokens. Notably, our dynamics encoder uniquely handles fMRI time series with heterogeneous repetition times (TRs), addressing a major limitation in existing models. BrainHarmonix is also the first to deeply compress high-dimensional neuroimaging signals into unified, continuous 1D tokens, forming a compact latent space of the human brain. BrainHarmonix achieves strong generalization across diverse downstream tasks, including neurodevelopmental and neurodegenerative disorder classification and cognition prediction - consistently outperforming previous approaches. Our models - pretrained on 8 H100 GPUs - aim to catalyze a new era of AI-driven neuroscience powered by large-scale multimodal neuroimaging.","Recent brain foundation models have made significant advances in learning human brain representations. BrainLM \cite{carobrainlm} and Brain-JEPA \cite{dong2024brain} pioneered self-supervised learning for fMRI time series using masked prediction and joint-embedding approaches, respectively. While these models demonstrated promising generalizability through global representations, they suffer from two critical shortcomings: (1) they ignore brain structural information, and (2) due to their standard choice of patch embedding layer in transformers, cannot accommodate heterogeneous TRs common across - or even within - fMRI datasets. BDO \cite{park2025foundational} proposed a brain dynamics model based on stochastic optimal control, however, it focuses exclusively on brain dynamics, similar to Brain-JEPA and BrainLM. In addition, BrainMass \cite{yang2024brainmass} has been proposed as the first foundation model for brain functional connectivity and pretrained on diverse fMRI datasets. However, it focuses exclusively on static functional connectivity without capturing brain structural information or temporal dynamics. On the other hand, BrainMVP \cite{rui2024brainmvp} introduced self-supervised pretraining for 3D volumetric brain imaging that excels at learning correspondence among multi-parametric MRI, but fails to capture brain functional dynamics. This makes it suboptimal for gaining a comprehensive understanding of human brain functional organization, capturing individual differences in behavior, and detecting abnormal alterations associated with neuropsychiatric disorders. To the best of our knowledge, Brain Harmony (BrainHarmonix) addresses these limitations as \emph{the first multimodal foundation model that seamlessly integrates structural morphology with functional dynamics while accommodating variable TR values}. By unifying both modalities into 1D tokens, BrainHarmonix creates a compact and effective representational space that captures the holistic nature of the human brain.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\columnwidth]{Figure-2.png}
    % \vspace{-12pt}
    \caption{\textbf{Pretraining of Brain Harmony (BrainHarmonix).} \textbf{A. Unimodal Encoding (UE):} BrainHarmonix-S ($\varepsilon_{\scriptscriptstyle S}$) learns T1 structure via a Masked Autoencoder (MAE); gray cubes represent visible patches, while purple cubes are masked and reconstructed by the decoder ($\mathcal{D}'_{S}$). BrainHarmonix-F ($\varepsilon_{\scriptscriptstyle F}$) uses the Joint Embedding Predictive Architecture (JEPA) for fMRI, incorporating our Temporal Adaptive Patch Embedding (TAPE) for heterogeneous TRs and geometric harmonics for cortical alignment, with the observation encoder ($\varepsilon'_{\scriptscriptstyle F}$) and predictor ($\mathcal{P}$) following standard JEPA.
\textbf{B. Multimodal Fusion (MF):} The Harmonizer ($\mathcal{H}$) fuses structural and functional latents into 1D tokens (in green), then decoder ($\mathcal{D}_{S}$ \& $\mathcal{D}_{F}$) reconstruct modality-specific latents.}
    \label{fig2}
    \vspace{-10pt}
\end{figure}","Recent brain foundation models have made significant advances in learning human brain representations. BrainLM \cite{carobrainlm} and Brain-JEPA \cite{dong2024brain} pioneered self-supervised learning for fMRI time series using masked prediction and joint-embedding approaches, respectively. While these models demonstrated promising generalizability through global representations, they suffer from two critical shortcomings: (1) they ignore brain structural information, and (2) due to their standard choice of patch embedding layer in transformers, cannot accommodate heterogeneous TRs common across - or even within - fMRI datasets. BDO \cite{park2025foundational} proposed a brain dynamics model based on stochastic optimal control, however, it focuses exclusively on brain dynamics, similar to Brain-JEPA and BrainLM. In addition, BrainMass \cite{yang2024brainmass} has been proposed as the first foundation model for brain functional connectivity and pretrained on diverse fMRI datasets. However, it focuses exclusively on static functional connectivity without capturing brain structural information or temporal dynamics. On the other hand, BrainMVP \cite{rui2024brainmvp} introduced self-supervised pretraining for 3D volumetric brain imaging that excels at learning correspondence among multi-parametric MRI, but fails to capture brain functional dynamics. This makes it suboptimal for gaining a comprehensive understanding of human brain functional organization, capturing individual differences in behavior, and detecting abnormal alterations associated with neuropsychiatric disorders. To the best of our knowledge, Brain Harmony (BrainHarmonix) addresses these limitations as \emph{the first multimodal foundation model that seamlessly integrates structural morphology with functional dynamics while accommodating variable TR values}. By unifying both modalities into 1D tokens, BrainHarmonix creates a compact and effective representational space that captures the holistic nature of the human brain.","Recent brain foundation models have made significant advances in learning human brain representa-
tions. BrainLM [ 4] and Brain-JEPA [ 5] pioneered self-supervised learning for fMRI time series using
masked prediction and joint-embedding approaches, respectively. While these models demonstrated
promising generalizability through global representations, they suffer from two critical shortcomings:
(1) they ignore brain structural information, and (2) due to their standard choice of patch embedding
layer in transformers, cannot accommodate heterogeneous TRs common across - or even within -
fMRI datasets. BDO [ 14] proposed a brain dynamics model based on stochastic optimal control,
however, it focuses exclusively on brain dynamics, similar to Brain-JEPA and BrainLM. In addition,
BrainMass [ 7] has been proposed as the first foundation model for brain functional connectivity and
pretrained on diverse fMRI datasets. However, it focuses exclusively on static functional connectivity
without capturing brain structural information or temporal dynamics. On the other hand, BrainMVP
[6] introduced self-supervised pretraining for 3D volumetric brain imaging that excels at learning
correspondence among multi-parametric MRI, but fails to capture brain functional dynamics. This
makes it suboptimal for gaining a comprehensive understanding of human brain functional organi-
zation, capturing individual differences in behavior, and detecting abnormal alterations associated
with neuropsychiatric disorders. To the best of our knowledge, Brain Harmony (BrainHarmonix)
addresses these limitations asthe first multimodal foundation model that seamlessly integrates struc-
tural morphology with functional dynamics while accommodating variable TR values. By unifying
both modalities into 1D tokens, BrainHarmonix creates a compact and effective representational
space that captures the holistic nature of the human brain."
2510.01480v1,http://arxiv.org/abs/2510.01480v1,2025-10-01 21:45:58+00:00,Pharmacophore-Guided Generative Design of Novel Drug-Like Molecules,"The integration of artificial intelligence (AI) in early-stage drug discovery offers unprecedented opportunities for exploring chemical space and accelerating hit-to-lead optimization. However, docking optimization in generative approaches is computationally expensive and may lead to inaccurate results. Here, we present a novel generative framework that balances pharmacophore similarity to reference compounds with structural diversity from active molecules. The framework allows users to provide custom reference sets, including FDA-approved drugs or clinical candidates, and guides the \textit{de novo} generation of potential therapeutics. We demonstrate its applicability through a case study targeting estrogen receptor modulators and antagonists for breast cancer. The generated compounds maintain high pharmacophoric fidelity to known active molecules while introducing substantial structural novelty, suggesting strong potential for functional innovation and patentability. Comprehensive evaluation of the generated molecules against common drug-like properties confirms the robustness and pharmaceutical relevance of the approach.","Recent advances have proposed various frameworks for pharmacophore-aware molecular generation. Zhu et al. introduced PGMG, a graph-based generative model guided by pharmacophoric constraints, which achieved high validity, novelty, and docking scores \cite{zhu2023pgmg}. Seo and Kim developed PharmacoNet, an automated pipeline for pharmacophore model construction and scoring, which accelerates virtual screening while retaining high accuracy \cite{seo2024pharmaconet}. Yu et al. proposed DiffPhore, a diffusion-based model that learns to generate molecules conditioned on pharmacophoric maps and can predict binding poses without explicit docking \cite{yu2025diffphore}. Moyano-Gómez et al. presented O-LAP, which creates cavity-filling pseudo-ligands to improve docking rescoring and account for protein-ligand shape complementarity \cite{moyano-gomez2024olap}. Alakhdar et al. introduced PharmaDiff, a pharmacophore-conditioned diffusion model that generates molecules satisfying 3D feature constraints with improved docking performance \cite{alakhdar2025pharmadiff}.

While existing methods often optimize docking scores or rely on specific binding pockets, our framework is target-agnostic and docking-independent, using pharmacophore similarity as a proxy for biological relevance. Unlike PGMG and PharmaDiff, it balances scaffold novelty with pharmacophoric fidelity; unlike O-LAP and PharmacoNet, it avoids predefined binding sites, enabling early-stage exploration when structural data is lacking. This allows us to access diverse, patentable chemical space while preserving pharmacophoric patterns linked to activity.","Recent advances have proposed various frameworks for pharmacophore-aware molecular generation. Zhu et al. introduced PGMG, a graph-based generative model guided by pharmacophoric constraints, which achieved high validity, novelty, and docking scores \cite{zhu2023pgmg}. Seo and Kim developed PharmacoNet, an automated pipeline for pharmacophore model construction and scoring, which accelerates virtual screening while retaining high accuracy \cite{seo2024pharmaconet}. Yu et al. proposed DiffPhore, a diffusion-based model that learns to generate molecules conditioned on pharmacophoric maps and can predict binding poses without explicit docking \cite{yu2025diffphore}. Moyano-Gómez et al. presented O-LAP, which creates cavity-filling pseudo-ligands to improve docking rescoring and account for protein-ligand shape complementarity \cite{moyano-gomez2024olap}. Alakhdar et al. introduced PharmaDiff, a pharmacophore-conditioned diffusion model that generates molecules satisfying 3D feature constraints with improved docking performance \cite{alakhdar2025pharmadiff}.

While existing methods often optimize docking scores or rely on specific binding pockets, our framework is target-agnostic and docking-independent, using pharmacophore similarity as a proxy for biological relevance. Unlike PGMG and PharmaDiff, it balances scaffold novelty with pharmacophoric fidelity; unlike O-LAP and PharmacoNet, it avoids predefined binding sites, enabling early-stage exploration when structural data is lacking. This allows us to access diverse, patentable chemical space while preserving pharmacophoric patterns linked to activity.","Recent advances have proposed various frameworks for pharmacophore-aware molecular genera-
tion. Zhu et al. introduced PGMG, a graph-based generative model guided by pharmacophoric
constraints, which achieved high validity, novelty, and docking scores [10]. Seo and Kim developed
PharmacoNet, an automated pipeline for pharmacophore model construction and scoring, which
accelerates virtual screening while retaining high accuracy [11]. Yu et al. proposed DiffPhore, a
diffusion-based model that learns to generate molecules conditioned on pharmacophoric maps and
can predict binding poses without explicit docking [12]. Moyano-Gómez et al. presented O-LAP,
which creates cavity-filling pseudo-ligands to improve docking rescoring and account for protein-
ligand shape complementarity [13]. Alakhdar et al. introduced PharmaDiff, a pharmacophore-
conditioned diffusion model that generates molecules satisfying 3D feature constraints with im-
proved docking performance [14].
While existing methods often optimize docking scores or rely on specific binding pockets, our frame-
work is target-agnostic and docking-independent, using pharmacophore similarity as a proxy for bio-
logical relevance. Unlike PGMG and PharmaDiff, it balances scaffold novelty with pharmacophoric
fidelity; unlike O-LAP and PharmacoNet, it avoids predefined binding sites, enabling early-stage
exploration when structural data is lacking. This allows us to access diverse, patentable chemical
space while preserving pharmacophoric patterns linked to activity."
2510.19870v1,http://arxiv.org/abs/2510.19870v1,2025-10-22 05:55:49+00:00,Transforming Multi-Omics Integration with GANs: Applications in Alzheimer's and Cancer,"Multi-omics data integration is crucial for understanding complex diseases, yet limited sample sizes, noise, and heterogeneity often reduce predictive power. To address these challenges, we introduce Omics-GAN, a Generative Adversarial Network (GAN)-based framework designed to generate high-quality synthetic multi-omics profiles while preserving biological relationships. We evaluated Omics-GAN on three omics types (mRNA, miRNA, and DNA methylation) using the ROSMAP cohort for Alzheimer's disease (AD) and TCGA datasets for colon and liver cancer. A support vector machine (SVM) classifier with repeated 5-fold cross-validation demonstrated that synthetic datasets consistently improved prediction accuracy compared to original omics profiles. The AUC of SVM for mRNA improved from 0.72 to 0.74 in AD, and from 0.68 to 0.72 in liver cancer. Synthetic miRNA enhanced classification in colon cancer from 0.59 to 0.69, while synthetic methylation data improved performance in liver cancer from 0.64 to 0.71. Boxplot analyses confirmed that synthetic data preserved statistical distributions while reducing noise and outliers. Feature selection identified significant genes overlapping with original datasets and revealed additional candidates validated by GO and KEGG enrichment analyses. Finally, molecular docking highlighted potential drug repurposing candidates, including Nilotinib for AD, Atovaquone for liver cancer, and Tecovirimat for colon cancer. Omics-GAN enhances disease prediction, preserves biological fidelity, and accelerates biomarker and drug discovery, offering a scalable strategy for precision medicine applications.","\label{Sec:rew}

In contemporary research, several advanced multi-omics data integration frameworks have been developed, broadly categorized into traditional methods and GAN-based approaches. Traditional methods primarily focus on analyzing multi-omics data or extracting network signatures for distinct phenotypic groups. Conversely, GAN-based methods leverage synthetic data generation to enhance predictive accuracy \cite{ Chakraborty-24, Ballard-24}.

\subsection{Non-generative approaches  }
Multi-Omics Factor Analysis (MOFA is designed to analyze  multi-omics data, identifying coordinated transcriptional and epigenetic changes during cell differentiation. It also captures biological and technical sources of variability. However, MOFA’s linear model structure limits its ability to detect non-linear relationships between features across assays. As a supervised learning framework, iOmicsPASS integrates multi-omics data within the context of biological networks. It extracts network signatures predictive of phenotypic groups, maintaining low prediction error rates. However, this method does not facilitate phenotypic group prediction directly and excludes molecules not represented in user-provided network data, potentially leading to a loss of valuable biological insights \cite{baryshnikova2022iomics, Alam-16a, Alam-16b, Ashad-13, Alam-19}.  Neighborhood-based Multi-Omics Clustering (NEMO) operates by analyzing the local neighborhood of each sample to capture similarity patterns across different omics. Despite its utility, NEMO requires each pair of samples to share at least one common omic for partial data and does not provide direct insights into feature importance nemo. PINSPlus is used for tumor subtype discovery by integrating multiple omics datasets in a single analysis. It identifies both known subtypes and novel subgroups with significant survival differences. The model has demonstrated high performance in validating subtypes using metrics like the Rand Index (RI) and Adjusted Rand Index (ARI) across various mRNA datasets \cite{Ashad-10, Ashad-14T}.

A deep learning approach employing a late integration strategy, MOLI was introduced by Sharifi-Noghabi et al. \cite{huang2019salmon}. The method utilizes modality-specific feedforward neural networks (FNNs) to independently extract features from each omic modality. These features are subsequently combined into a unified multi-omic representation, which serves as input for a classification sub-network aimed at predicting drug responses. Although this approach is straightforward and accounts for the distinct characteristics of each modality, it does not explicitly address potential interactions between the different modalities, which could be a limitation.

SALMON \cite{huang2019salmon} focuses on biological interpretability by either organizing data in biologically meaningful ways or incorporating prior domain knowledge. It uses mRNA-seq and miRNA-seq data to predict Cox regression survival in breast cancer. This is achieved by first performing gene co-expression analysis to create eigengene modules, which reduce the original feature space into biologically meaningful latent features.
Two alternative methods, MiNet \cite{hao2019gene} and DeepOmix \cite{zhao2021deepomix}, incorporate prior biological knowledge into their architectures. MiNet utilizes a neural network design structured to mirror biological systems. It features a multi-omics input layer, followed by a gene layer that links the multi-omics data to corresponding genes, and a pathway layer that connects these genes to their known biological pathways. Similarly, DeepOmix [23] employs a deep neural network (DNN) framework with an input gene layer designed to integrate multi-omics data at the gene level. This is followed by a functional module layer, which leverages biological insights to establish connections between the input gene layer and the functional modules, representing true biological relationships.

MoGCN, introduced by Li et al. \cite{li2022mogcn}, incorporates patient similarity information and employs an intermediate integration strategy. This method integrates multiple modalities into a single representation before classification by using an autoencoder (AE) with multiple encoders and decoders that share a common layer. Peng et al. \cite{peng2022drugresponse} proposed MOFGCN, a knowledge-guided connectivity method designed to predict drug responses in cell lines. This approach constructs a heterogeneous network that incorporates a cell line similarity network, a drug similarity network, and known drug-cell line associations. 

Althubaiti et al. \cite{althubaiti2021deepmocca} developed DeepMOCCA, a model that combines protein-protein interactions (PPIs) with multi-omics data for cancer survival prediction. The model integrates germline and somatic variants, methylation, gene expression, and copy number variants into a graph structure where nodes represent genes and edges denote functional interactions between them.

\subsection{Generative approaches }

Mitra et al. \cite{mitra2020multiview} proposed multi-view neighborhood embedding (MvNE), a method designed to learn a unified probability distribution of samples across multiple omics modalities. This approach generates low-dimensional embeddings that maintain the relationships between samples in the transformed space. 
Zuo et al. \cite{zuo2021deep} introduced the deep cross-omics cycle attention method (DCCA), which takes a unique approach to jointly analyze single-cell multi-omics data for various downstream applications. DCCA begins by encoding each omics modality with separate variational autoencoders (VAEs). It then employs cyclical attention transfer to capture and model associations between the different modalities.

Finally, Generative Adversarial Networks (GANs) have revolutionized multi-omics data integration by overcoming the limitations of traditional methods. By generating synthetic data that mimics real datasets, GANs capture non-linear correlations between omics features, offering versatile applications:  
\begin{itemize}
    \item  Synthetic Data Generation**: GANs are used to generate gene expression data from bulk RNA-seq datasets, capturing complex patterns and relationships.    
\item Biomarker Identification**: Interaction networks derived from multi-omics datasets with GANs enhance the detection of critical biomarkers.  
\item Overcoming Limited Sample Sizes**: GANs generate synthetic data to augment small datasets, improving the reliability and robustness of analyses.  
\end{itemize}
These capabilities have made GANs a powerful tool in advancing multi-omics research.","In contemporary research, several advanced multi-omics data integration frameworks have been developed, broadly categorized into traditional methods and GAN-based approaches. Traditional methods primarily focus on analyzing multi-omics data or extracting network signatures for distinct phenotypic groups. Conversely, GAN-based methods leverage synthetic data generation to enhance predictive accuracy \cite{ Chakraborty-24, Ballard-24}.

\subsection{Non-generative approaches  }
Multi-Omics Factor Analysis (MOFA is designed to analyze  multi-omics data, identifying coordinated transcriptional and epigenetic changes during cell differentiation. It also captures biological and technical sources of variability. However, MOFA’s linear model structure limits its ability to detect non-linear relationships between features across assays. As a supervised learning framework, iOmicsPASS integrates multi-omics data within the context of biological networks. It extracts network signatures predictive of phenotypic groups, maintaining low prediction error rates. However, this method does not facilitate phenotypic group prediction directly and excludes molecules not represented in user-provided network data, potentially leading to a loss of valuable biological insights \cite{baryshnikova2022iomics, Alam-16a, Alam-16b, Ashad-13, Alam-19}.  Neighborhood-based Multi-Omics Clustering (NEMO) operates by analyzing the local neighborhood of each sample to capture similarity patterns across different omics. Despite its utility, NEMO requires each pair of samples to share at least one common omic for partial data and does not provide direct insights into feature importance nemo. PINSPlus is used for tumor subtype discovery by integrating multiple omics datasets in a single analysis. It identifies both known subtypes and novel subgroups with significant survival differences. The model has demonstrated high performance in validating subtypes using metrics like the Rand Index (RI) and Adjusted Rand Index (ARI) across various mRNA datasets \cite{Ashad-10, Ashad-14T}.

A deep learning approach employing a late integration strategy, MOLI was introduced by Sharifi-Noghabi et al. \cite{huang2019salmon}. The method utilizes modality-specific feedforward neural networks (FNNs) to independently extract features from each omic modality. These features are subsequently combined into a unified multi-omic representation, which serves as input for a classification sub-network aimed at predicting drug responses. Although this approach is straightforward and accounts for the distinct characteristics of each modality, it does not explicitly address potential interactions between the different modalities, which could be a limitation.

SALMON \cite{huang2019salmon} focuses on biological interpretability by either organizing data in biologically meaningful ways or incorporating prior domain knowledge. It uses mRNA-seq and miRNA-seq data to predict Cox regression survival in breast cancer. This is achieved by first performing gene co-expression analysis to create eigengene modules, which reduce the original feature space into biologically meaningful latent features.
Two alternative methods, MiNet \cite{hao2019gene} and DeepOmix \cite{zhao2021deepomix}, incorporate prior biological knowledge into their architectures. MiNet utilizes a neural network design structured to mirror biological systems. It features a multi-omics input layer, followed by a gene layer that links the multi-omics data to corresponding genes, and a pathway layer that connects these genes to their known biological pathways. Similarly, DeepOmix [23] employs a deep neural network (DNN) framework with an input gene layer designed to integrate multi-omics data at the gene level. This is followed by a functional module layer, which leverages biological insights to establish connections between the input gene layer and the functional modules, representing true biological relationships.

MoGCN, introduced by Li et al. \cite{li2022mogcn}, incorporates patient similarity information and employs an intermediate integration strategy. This method integrates multiple modalities into a single representation before classification by using an autoencoder (AE) with multiple encoders and decoders that share a common layer. Peng et al. \cite{peng2022drugresponse} proposed MOFGCN, a knowledge-guided connectivity method designed to predict drug responses in cell lines. This approach constructs a heterogeneous network that incorporates a cell line similarity network, a drug similarity network, and known drug-cell line associations. 

Althubaiti et al. \cite{althubaiti2021deepmocca} developed DeepMOCCA, a model that combines protein-protein interactions (PPIs) with multi-omics data for cancer survival prediction. The model integrates germline and somatic variants, methylation, gene expression, and copy number variants into a graph structure where nodes represent genes and edges denote functional interactions between them.

\subsection{Generative approaches }

Mitra et al. \cite{mitra2020multiview} proposed multi-view neighborhood embedding (MvNE), a method designed to learn a unified probability distribution of samples across multiple omics modalities. This approach generates low-dimensional embeddings that maintain the relationships between samples in the transformed space. 
Zuo et al. \cite{zuo2021deep} introduced the deep cross-omics cycle attention method (DCCA), which takes a unique approach to jointly analyze single-cell multi-omics data for various downstream applications. DCCA begins by encoding each omics modality with separate variational autoencoders (VAEs). It then employs cyclical attention transfer to capture and model associations between the different modalities.

Finally, Generative Adversarial Networks (GANs) have revolutionized multi-omics data integration by overcoming the limitations of traditional methods. By generating synthetic data that mimics real datasets, GANs capture non-linear correlations between omics features, offering versatile applications:  
\begin{itemize}
    \item  Synthetic Data Generation**: GANs are used to generate gene expression data from bulk RNA-seq datasets, capturing complex patterns and relationships.    
\item Biomarker Identification**: Interaction networks derived from multi-omics datasets with GANs enhance the detection of critical biomarkers.  
\item Overcoming Limited Sample Sizes**: GANs generate synthetic data to augment small datasets, improving the reliability and robustness of analyses.  
\end{itemize}
These capabilities have made GANs a powerful tool in advancing multi-omics research.","In contemporary research, several advanced multi-omics data integration frame-
works have been developed, broadly categorized into traditional methods and GAN-
based approaches. Traditional methods primarily focus on analyzing multi-omics
data or extracting network signatures for distinct phenotypic groups. Conversely,
GAN-based methods leverage synthetic data generation to enhance predictive
accuracy [16, 22].
2.1.Non-generative approaches.Multi-Omics Factor Analysis (MOFA is de-
signed to analyze multi-omics data, identifying coordinated transcriptional and
epigenetic changes during cell differentiation. It also captures biological and technical
sources of variability. However, MOFAâĂŹs linear model structure limits its ability
to detect non-linear relationships between features across assays. As a supervised
learning framework, iOmicsPASS integrates multi-omics data within the context of
biological networks. It extracts network signatures predictive of phenotypic groups,
maintaining low prediction error rates. However, this method does not facilitate
phenotypic group prediction directly and excludes molecules not represented in
user-provided network data, potentially leading to a loss of valuable biological
insights [ 4,8,9,11,14]. Neighborhood-based Multi-Omics Clustering (NEMO)
operates by analyzing the local neighborhood of each sample to capture similarity
patterns across different omics. Despite its utility, NEMO requires each pair of
samples to share at least one common omic for partial data and does not provide
direct insights into feature importance nemo. PINSPlus is used for tumor subtype
discovery by integrating multiple omics datasets in a single analysis. It identifies
both known subtypes and novel subgroups with significant survival differences. The
model has demonstrated high performance in validating subtypes using metrics
like the Rand Index (RI) and Adjusted Rand Index (ARI) across various mRNA
datasets [2, 7].
A deep learning approach employing a late integration strategy, MOLI was
introduced by Sharifi-Noghabi et al. [ 21]. The method utilizes modality-specific
feedforward neural networks (FNNs) to independently extract features from each
omic modality. These features are subsequently combined into a unified multi-omic
representation, which serves as input for a classification sub-network aimed at
predicting drug responses. Although this approach is straightforward and accounts
for the distinct characteristics of each modality, it does not explicitly address
potential interactions between the different modalities, which could be a limitation.
SALMON [ 21] focuses on biological interpretability by either organizing data
in biologically meaningful ways or incorporating prior domain knowledge. It uses
mRNA-seq and miRNA-seq data to predict Cox regression survival in breast cancer.
This is achieved by first performing gene co-expression analysis to create eigengene
modules, which reduce the original feature space into biologically meaningful latent
features. Two alternative methods, MiNet [ 20] and DeepOmix [ 32], incorporate
prior biological knowledge into their architectures. MiNet utilizes a neural network
design structured to mirror biological systems. It features a multi-omics input layer,
followed by a gene layer that links the multi-omics data to corresponding genes,
and a pathway layer that connects these genes to their known biological pathways.
4
Similarly, DeepOmix [23] employs a deep neural network (DNN) framework with an
input gene layer designed to integrate multi-omics data at the gene level. This is
followed by a functional module layer, which leverages biological insights to establish
connections between the input gene layer and the functional modules, representing
true biological relationships.
MoGCN, introduced by Li et al. [ 24], incorporates patient similarity information
and employs an intermediate integration strategy. This method integrates multiple
modalities into a single representation before classification by using an autoencoder
(AE) with multiple encoders and decoders that share a common layer. Peng et
al. [28] proposed MOFGCN, a knowledge-guided connectivity method designed
to predict drug responses in cell lines. This approach constructs a heterogeneous
network that incorporates a cell line similarity network, a drug similarity network,
and known drug-cell line associations.
Althubaiti et al. [ 12] developed DeepMOCCA, a model that combines protein-
protein interactions (PPIs) with multi-omics data for cancer survival prediction.
The model integrates germline and somatic variants, methylation, gene expression,
and copy number variants into a graph structure where nodes represent genes and
edges denote functional interactions between them.
2.2.Generative approaches.Mitra et al. [ 25] proposed multi-view neighborhood
embedding (MvNE), a method designed to learn a unified probability distribution of
samples across multiple omics modalities. This approach generates low-dimensional
embeddings that maintain the relationships between samples in the transformed
space. Zuo et al. [ 33] introduced the deep cross-omics cycle attention method
(DCCA), which takes a unique approach to jointly analyze single-cell multi-omics
data for various downstream applications. DCCA begins by encoding each omics
modality with separate variational autoencoders (VAEs). It then employs cyclical
attention transfer to capture and model associations between the different modalities.
Finally, Generative Adversarial Networks (GANs) have revolutionized multi-omics
data integration by overcoming the limitations of traditional methods. By generating
synthetic data that mimics real datasets, GANs capture non-linear correlations
between omics features, offering versatile applications:
•Synthetic Data Generation**: GANs are used to generate gene expres-
sion data from bulk RNA-seq datasets, capturing complex patterns and
relationships.
•Biomarker Identification**: Interaction networks derived from multi-omics
datasets with GANs enhance the detection of critical biomarkers.
•Overcoming Limited Sample Sizes**: GANs generate synthetic data to
augment small datasets, improving the reliability and robustness of analyses.
These capabilities have made GANs a powerful tool in advancing multi-omics
research.
3.Materials and methods
3.1.Datasets and Networks.To assess the effectiveness of our approach, we
applied our model to datasets from The Cancer Genome Atlas (TCGA) for liver
and colon cancer, as well as the Religious Orders Study/Memory and Aging Project
(ROSMAP) dataset for Alzheimer’s Disease (AD). Initially, we implemented the
recommended architecture for biomedical classification tasks using the ROSMAP
5
dataset to distinguish between AD patients and normal controls (NC). This analysis
incorporated three distinct omics data types: Omics1 (mRNA expression), Omics2
(DNA methylation), and Omics3 (miRNA expression). The integration of these
omics layers aimed to provide a comprehensive and complementary perspective on
disease mechanisms. The dataset was sourced from the MOGONET study and is
publicly available in the MOGONET ROSMAP Dataset repository. Only samples
with complete data across all three omics types were included in our analysis,
yielding a total of"
2511.03849v2,http://arxiv.org/abs/2511.03849v2,2025-11-05 20:39:29+00:00,Which Similarity-Sensitive Entropy?,"A canonical step in quantifying a system is to measure its entropy. Shannon entropy and other traditional entropy measures capture only the information encoded in the frequencies of a system's elements. Recently, Leinster, Cobbold, and Reeve (LCR) introduced a method that also captures the rich information encoded in the similarities and differences among elements, yielding similarity-sensitive entropy. More recently, the Vendi score (VS) was introduced as an alternative, raising the question of how LCR and VS compare, and which is preferable. Here we address these questions conceptually, analytically, and experimentally, using 53 machine-learning datasets. We show that LCR and VS can differ by orders of magnitude and can capture complementary information about a system, except in limiting cases. We demonstrate that both LCR and VS depend on how similarities are scaled and introduce the concept of ``half distance'' to parameterize this dependence. We prove that VS provides an upper bound on LCR for several values of the Rényi-Hill order parameter and conjecture that this bound holds for all values. We conclude that VS is preferable only when interpreting elements as linear combinations of a more fundamental set of ``ur-elements'' or when the system or dataset possesses a quantum-mechanical character. In the broader circumstance where one seeks simply to capture the rich information encoded by similarity, LCR is favored; nevertheless, for certain half-distances the two methods can complement each other.","Entropies fall into two classes: traditional/similarity‑insensitive and similarity‑sensitive. 
Although one often sees reference to ``the'' entropy (either as a shorthand or referring specifically to Shannon entropy), entropies are actually families of measures; individual members are distinguished by how they weight frequencies and, for S-entropy, by their similarity matrix (see Table~\ref{table:entropies}).

\subsection{Traditional, similarity-insensitive entropy}\label{sec:effective_number_form}

The best known traditional entropy is Shannon entropy: $-\sum_ip_i\log{p_i}$ \cite{shannon1948mathematical}, where $p_i$ is the frequency of unique element $i$. 
Shannon entropy can be thought of as a weighted average of element frequencies, with the weights being the logarithms of the frequencies themselves ($\log p_i$). 
Rényi \cite{renyiMeasuresEntropyInformation1961} generalized this into a family of entropies $H_\alpha$ of order $\alpha$---``deformations'' of Shannon entropy---in which $\alpha>1$ represents greater up-weighting of more-frequent elements relative to Shannon entropy; Shannon entropy itself is $H_{\alpha=1}$. 

Hill showed that exponentiating Rényi entropy yields the ``effective number'' of distinct elements in the system \cite{jost2006entropy}.  Effective numbers, denoted $D_q$, use a parameter $q$ (identical to Rényi’s $\alpha$) such that $q=0$ counts the distinct types (by giving zero consideration to frequencies), while larger $q$ give greater weight to more frequent elements.
$D_1=\exp({H_1})$ is Shannon entropy in effective-number form.
Hill demonstrated that several other familiar statistics correspond to special cases of $D_q$ for certain $q$, including Simpson’s index for $q=2$ and the Berger–Parker index for $q=\infty$ \cite{hill1973diversity}.  
For this reason, Hill's formulation is often described as a unifying framework. $D_q$ are known as the Hill numbers or D numbers (for ``diversity'') and are given by
\begin{equation}\label{Hill}
D_q(\mathbf{p}) = \exp(H_\alpha(\mathbf{p})) =
\begin{cases}
\left( \sum_{i=1}^n p_i^q \right)^{\frac{1}{1-q}}, & q \neq 1, \\[6pt]
\exp\left( -\sum_{i=1}^n p_i \ln p_i \right), & q = 1.
\end{cases}
\end{equation}
where $\mathbf{p}$ denotes the frequency distribution of unique elements.  Other generalizations such as Tsallis entropy \cite{tsallis1988possible} also exist. What these traditional entropies have in common is that they are similarity‑insensitive: they depend solely on $\mathbf{p}$ and ignore any relationships among the elements.

\subsection{Similarity-sensitive entropy (S-entropy)}

\subsubsection{The Leinster-Cobbold-Reeve framework (LCR)}

LCR \cite{leinsterEntropyDiversityAxiomatic2020} extends Hill’s framework by incorporating information about the similarities and differences of the elements within the system, which traditional entropy does not. 
A similarity matrix $Z$ is introduced, with entries $z_{ij}\in[0,1]$ that quantify the similarity between elements $i$ and $j$.
The similarity of each element to itself is set to 1, making $Z$'s diagonal entries 1.
The resulting quantities, denoted $D_q^{Z}$, are the exponentials of similarity‑sensitive versions of the Rényi entropies $H_\alpha^{Z}$ and are given by  
\begin{equation}\label{eq:LCR}
D_{q}^{Z}(\mathbf{p};Z) = \exp(H^Z_\alpha(\mathbf{p};Z)) =
\begin{cases}
\left( \sum_{i=1}^{n} p_{i} (Z \mathbf{p})_{i}^{\,q-1} \right)^{\frac{1}{1-q}}, & q \neq 1, \\[6pt]
\exp\left( -\sum_{i=1}^{n} p_{i} \ln (Z \mathbf{p})_{i} \right), & q = 1.
\end{cases}
\end{equation}
Here $(Z\mathbf{p})_i=\sum_j z_{ij}p_j$ is the frequency‑weighted average similarity of element $i$ to all the elements (including itself).  
When this average is large, element $i$ is considered ordinary, making Eq.~\ref{eq:LCR} interpretable as the average ``ordinariness'' \cite{leinster2012measuring} across all elements. 
$D_q^Z{(\mathbf{p})}$ appears widely in the recent literature, where it is variously known as: 
\begin{itemize}
    \item \textit{phylogenetic diversity} \cite{chao2010phylogenetic} in the special case where $z_{ij}$ forms an ultrametric, typically derived from a phylogenetic tree;
    \item \textit{functional diversity} \cite{chaoAttributediversityApproachFunctional2019} when the similarity pertains to elements' function, for example the binding similarity between pairs of antibodies or TCRs \cite{aroraRepertoirescaleMeasuresAntigen2022};
    \item \textit{attribute diversity} \cite{chaoAttributediversityApproachFunctional2019} when interpreting the system as a set of attribute contributions instead of frequencies; and
    \item \textit{similarity-sensitive} or \textit{similarity-aware diversity} more generally \cite{nguyen2023greylock}.
\end{itemize}

LCR has proven useful for describing many complex systems whose empirical samples are uniform or close to uniform---where unique elements' frequency distribution is flat or nearly so---a regime where traditional entropies become uninformative (Fig. \ref{fig:overview}).  
Representative applications in the life sciences include high‑throughput immunology (immunomes) \cite{aroraRepertoirescaleMeasuresAntigen2022}, microbiome research (metagenomics) \cite{nguyen2023greylock}, and medical imaging \cite{couch2024beyond}
Specifically in ML contexts, where training sets (e.g., image collections) are often composed of unique observations (e.g. images), LCR has been shown to help identify performance predictors beyond simple dataset size or class balance \cite{couch2024beyond}.

\subsubsection{The Vendi score and its ``cousins'' (VS)}\label{sec:VS}

VS constitutes a related but separate class of similarity‑sensitive entropy measures.  
Like LCR, VS entropies are functions of a similarity matrix, but now the matrix has dimensions $n\times n$, where $n$ is the number of observations or \textit{total} elements; for this reason we refer to it as $Z_n$ to distinguish it from the $Z$ used in LCR, which has one row/column per \textit{unique} element. (When all elements are unique, $Z$ and $Z_n$ are the same.) We can define $Z_n$ in terms of $Z$ as
%
\begin{equation}\label{eq: Z_n from Z}
    \left(Z_n \right)_{i,j} = Z_{s(i), s(j)}.
\end{equation}
where $s(i)$ is the unique element of which the $i^{\text{th}}$ overall element is an instance or example.

The original VS is defined as the exponential of the Shannon entropy of the eigenvalues $\lambda_i$ of $Z_n/n$. The division by $n$ normalizes $Z_n$ to unit trace (because as in $Z$, the diagonals of $Z_n$ equal 1):
\begin{equation}\label{eq:VS}
    \mathrm{VS}{(\mathbf{p}_n;Z_n)} = \exp\!\left(-\sum_{i=1}^{n} \lambda_i \log \lambda_i\right)
\end{equation}
VS is to this similarity matrix what the von Neumann entropy is to the quantum density matrix. 
Replacing the Shannon term in Eq.~\ref{eq:VS} with a Rényi entropy of order $q$ yields the so‑called ``cousins'' \cite{pasarkar2023cousins} of the Vendi score:
\begin{equation}
\mathrm{VS}_{q}{(\mathbf{p}_n;Z_n)} = \left( \sum_{i=1}^{n} \lambda_{i}^{q} \right)^{\frac{1}{1-q}}
\end{equation}
$\mathrm{VS}_q$ is a 1-parameter family of scores with $q$ as in $D_q^Z$.

Note that LCR can also be written as a function of $Z_n$ if desired:
%
\begin{eqnarray}
    D_{q}^{Z}(\mathbf{p};Z) = D_{q}^{Z}(\tilde{\mathbf{p}} = \frac{1}{n};Z_n)
\end{eqnarray}
%
where $\tilde{\mathbf{p}}$ is the uniform distribution on the $n$ elements.

\begin{table}[t]
\centering
\scriptsize % adjust to \footnotesize if too small
\renewcommand{\arraystretch}{1.15}

\begin{tabular}{
>{\raggedright\arraybackslash}p{5.3cm}
>{\centering\arraybackslash}p{2.2cm}
>{\centering\arraybackslash}p{1.9cm}
>{\raggedright\arraybackslash}p{4.9cm}
}
\toprule
\textbf{Entropy type and formula} & \textbf{Frequency weighting} & \textbf{Similarity-sensitive?} & \textbf{Notes and applications} \\
\midrule

\textbf{Shannon (Boltzmann--Gibbs)} \newline
\( H_{1}(\mathbf{p})= -\sum_{i=1}^{S} p_i \log p_i \)
& \( q=1 \) & No & Information theory, Ecology, ML loss functions, Thermodynamics \\

\textbf{Rényi entropy (\(\alpha\))} \newline
\( H_{\alpha}(mathbf{p})=\frac{1}{1-\alpha}\log \sum_i p_i^{\alpha} \)
& Any \(q\) & No & Info-theoretic security, Ecology, Fractal analysis, Physics \\

\textbf{Tsallis entropy (\(q\)-entropy)} \newline
\( S_{q}(\mathbf{p})=\frac{1}{q-1}\big(1-\sum_i p_i^{q}\big) \)
& Any \(q\) & No & Non-extensive statistical mechanics, Turbulence, Astrophysics \\

\textbf{Quantum (von Neumann) entropy} \newline
\( S(\rho) = -\operatorname{Tr}(\rho \log \rho) \)
& \( q=1 \) & No & Quantum information, Entanglement, Quantum thermodynamics \\

\midrule

\textbf{LCR} $D_q^Z(\mathbf{p};Z)=$ \newline
$\big(\sum_i p_i(Z\mathbf{p})_i^{q-1}\big)^{1/(1-q)}$ for $q\neq1$ \newline 
$\exp(-\sum_i p_i \log(Z\mathbf{p})_i)$ for $q=1$
& Any \(q\) & Yes & Phylogenetic, attribute, and functional diversity are special cases. Biodiversity, microbiomes, language... \\

\textbf{Vendi score} \newline
\( \mathrm{VS}(\mathbf{p}_n;Z_n) = \exp{\big( -\sum_{i=1}^{n} \lambda_{i} \log \lambda_{i} \big)} \)
& \(q=1\) & Yes & \( \lambda_i \) are eigenvalues of $Z_n=Z/N$ (positive semi-definite). ML diversity, Generative model evaluation, Ecology \\

\textbf{Cousins of the Vendi score} \newline
\( \text{VS}_q(\mathbf{x},\mathbf{k}) = \left(\sum_i \lambda_i^q \right)^{\tfrac{1}{1-q}} \)
& Any \(q\) & Yes & Quantum-inspired stats, Class diversity profiling \\

\midrule

\textbf{Other variants} \newline
Burg: \( \sum_i \log p_i \) \newline
KL: \( D_{KL}(p\|q)=\sum_i p_i \log\frac{p_i}{q_i} \) \newline
CRE: \( -\int_0^\infty \bar{F}(x)\log \bar{F}(x)\,dx \)
& Partly & Partly & Spectral estimation (Burg); Bayesian inference (KL); Survival analysis (CRE) \\

\bottomrule
\end{tabular}

\caption{Summary of major entropy families, their frequency-weighting forms, similarity sensitivity, and main applications.}
\label{table:entropies}
\end{table}","Entropies fall into two classes: traditional/similarity‑insensitive and similarity‑sensitive. 
Although one often sees reference to ``the'' entropy (either as a shorthand or referring specifically to Shannon entropy), entropies are actually families of measures; individual members are distinguished by how they weight frequencies and, for S-entropy, by their similarity matrix (see Table~\ref{table:entropies}).

\subsection{Traditional, similarity-insensitive entropy}
The best known traditional entropy is Shannon entropy: $-\sum_ip_i\log{p_i}$ \cite{shannon1948mathematical}, where $p_i$ is the frequency of unique element $i$. 
Shannon entropy can be thought of as a weighted average of element frequencies, with the weights being the logarithms of the frequencies themselves ($\log p_i$). 
Rényi \cite{renyiMeasuresEntropyInformation1961} generalized this into a family of entropies $H_\alpha$ of order $\alpha$---``deformations'' of Shannon entropy---in which $\alpha>1$ represents greater up-weighting of more-frequent elements relative to Shannon entropy; Shannon entropy itself is $H_{\alpha=1}$. 

Hill showed that exponentiating Rényi entropy yields the ``effective number'' of distinct elements in the system \cite{jost2006entropy}.  Effective numbers, denoted $D_q$, use a parameter $q$ (identical to Rényi’s $\alpha$) such that $q=0$ counts the distinct types (by giving zero consideration to frequencies), while larger $q$ give greater weight to more frequent elements.
$D_1=\exp({H_1})$ is Shannon entropy in effective-number form.
Hill demonstrated that several other familiar statistics correspond to special cases of $D_q$ for certain $q$, including Simpson’s index for $q=2$ and the Berger–Parker index for $q=\infty$ \cite{hill1973diversity}.  
For this reason, Hill's formulation is often described as a unifying framework. $D_q$ are known as the Hill numbers or D numbers (for ``diversity'') and are given by
\begin{equation}D_q(\mathbf{p}) = \exp(H_\alpha(\mathbf{p})) =
\begin{cases}
\left( \sum_{i=1}^n p_i^q \right)^{\frac{1}{1-q}}, & q \neq 1, \\[6pt]
\exp\left( -\sum_{i=1}^n p_i \ln p_i \right), & q = 1.
\end{cases}
\end{equation}
where $\mathbf{p}$ denotes the frequency distribution of unique elements.  Other generalizations such as Tsallis entropy \cite{tsallis1988possible} also exist. What these traditional entropies have in common is that they are similarity‑insensitive: they depend solely on $\mathbf{p}$ and ignore any relationships among the elements.

\subsection{Similarity-sensitive entropy (S-entropy)}

\subsubsection{The Leinster-Cobbold-Reeve framework (LCR)}

LCR \cite{leinsterEntropyDiversityAxiomatic2020} extends Hill’s framework by incorporating information about the similarities and differences of the elements within the system, which traditional entropy does not. 
A similarity matrix $Z$ is introduced, with entries $z_{ij}\in[0,1]$ that quantify the similarity between elements $i$ and $j$.
The similarity of each element to itself is set to 1, making $Z$'s diagonal entries 1.
The resulting quantities, denoted $D_q^{Z}$, are the exponentials of similarity‑sensitive versions of the Rényi entropies $H_\alpha^{Z}$ and are given by  
\begin{equation}D_{q}^{Z}(\mathbf{p};Z) = \exp(H^Z_\alpha(\mathbf{p};Z)) =
\begin{cases}
\left( \sum_{i=1}^{n} p_{i} (Z \mathbf{p})_{i}^{\,q-1} \right)^{\frac{1}{1-q}}, & q \neq 1, \\[6pt]
\exp\left( -\sum_{i=1}^{n} p_{i} \ln (Z \mathbf{p})_{i} \right), & q = 1.
\end{cases}
\end{equation}
Here $(Z\mathbf{p})_i=\sum_j z_{ij}p_j$ is the frequency‑weighted average similarity of element $i$ to all the elements (including itself).  
When this average is large, element $i$ is considered ordinary, making Eq.~\ref{eq:LCR} interpretable as the average ``ordinariness'' \cite{leinster2012measuring} across all elements. 
$D_q^Z{(\mathbf{p})}$ appears widely in the recent literature, where it is variously known as: 
\begin{itemize}
    \item \textit{phylogenetic diversity} \cite{chao2010phylogenetic} in the special case where $z_{ij}$ forms an ultrametric, typically derived from a phylogenetic tree;
    \item \textit{functional diversity} \cite{chaoAttributediversityApproachFunctional2019} when the similarity pertains to elements' function, for example the binding similarity between pairs of antibodies or TCRs \cite{aroraRepertoirescaleMeasuresAntigen2022};
    \item \textit{attribute diversity} \cite{chaoAttributediversityApproachFunctional2019} when interpreting the system as a set of attribute contributions instead of frequencies; and
    \item \textit{similarity-sensitive} or \textit{similarity-aware diversity} more generally \cite{nguyen2023greylock}.
\end{itemize}

LCR has proven useful for describing many complex systems whose empirical samples are uniform or close to uniform---where unique elements' frequency distribution is flat or nearly so---a regime where traditional entropies become uninformative (Fig. \ref{fig:overview}).  
Representative applications in the life sciences include high‑throughput immunology (immunomes) \cite{aroraRepertoirescaleMeasuresAntigen2022}, microbiome research (metagenomics) \cite{nguyen2023greylock}, and medical imaging \cite{couch2024beyond}
Specifically in ML contexts, where training sets (e.g., image collections) are often composed of unique observations (e.g. images), LCR has been shown to help identify performance predictors beyond simple dataset size or class balance \cite{couch2024beyond}.

\subsubsection{The Vendi score and its ``cousins'' (VS)}
VS constitutes a related but separate class of similarity‑sensitive entropy measures.  
Like LCR, VS entropies are functions of a similarity matrix, but now the matrix has dimensions $n\times n$, where $n$ is the number of observations or \textit{total} elements; for this reason we refer to it as $Z_n$ to distinguish it from the $Z$ used in LCR, which has one row/column per \textit{unique} element. (When all elements are unique, $Z$ and $Z_n$ are the same.) We can define $Z_n$ in terms of $Z$ as

\begin{equation}    \left(Z_n \right)_{i,j} = Z_{s(i), s(j)}.
\end{equation}
where $s(i)$ is the unique element of which the $i^{\text{th}}$ overall element is an instance or example.

The original VS is defined as the exponential of the Shannon entropy of the eigenvalues $\lambda_i$ of $Z_n/n$. The division by $n$ normalizes $Z_n$ to unit trace (because as in $Z$, the diagonals of $Z_n$ equal 1):
\begin{equation}    \mathrm{VS}{(\mathbf{p}_n;Z_n)} = \exp\!\left(-\sum_{i=1}^{n} \lambda_i \log \lambda_i\right)
\end{equation}
VS is to this similarity matrix what the von Neumann entropy is to the quantum density matrix. 
Replacing the Shannon term in Eq.~\ref{eq:VS} with a Rényi entropy of order $q$ yields the so‑called ``cousins'' \cite{pasarkar2023cousins} of the Vendi score:
\begin{equation}
\mathrm{VS}_{q}{(\mathbf{p}_n;Z_n)} = \left( \sum_{i=1}^{n} \lambda_{i}^{q} \right)^{\frac{1}{1-q}}
\end{equation}
$\mathrm{VS}_q$ is a 1-parameter family of scores with $q$ as in $D_q^Z$.

Note that LCR can also be written as a function of $Z_n$ if desired:

\begin{eqnarray}
    D_{q}^{Z}(\mathbf{p};Z) = D_{q}^{Z}(\tilde{\mathbf{p}} = \frac{1}{n};Z_n)
\end{eqnarray}

where $\tilde{\mathbf{p}}$ is the uniform distribution on the $n$ elements.

\begin{table}[t]
\centering
\scriptsize 
\renewcommand{\arraystretch}{1.15}

\begin{tabular}{
>{\raggedright\arraybackslash}p{5.3cm}
>{\centering\arraybackslash}p{2.2cm}
>{\centering\arraybackslash}p{1.9cm}
>{\raggedright\arraybackslash}p{4.9cm}
}
\toprule
\textbf{Entropy type and formula} & \textbf{Frequency weighting} & \textbf{Similarity-sensitive?} & \textbf{Notes and applications} \\
\midrule

\textbf{Shannon (Boltzmann--Gibbs)} \newline
\( H_{1}(\mathbf{p})= -\sum_{i=1}^{S} p_i \log p_i \)
& \( q=1 \) & No & Information theory, Ecology, ML loss functions, Thermodynamics \\

\textbf{Rényi entropy (\(\alpha\))} \newline
\( H_{\alpha}(mathbf{p})=\frac{1}{1-\alpha}\log \sum_i p_i^{\alpha} \)
& Any \(q\) & No & Info-theoretic security, Ecology, Fractal analysis, Physics \\

\textbf{Tsallis entropy (\(q\)-entropy)} \newline
\( S_{q}(\mathbf{p})=\frac{1}{q-1}\big(1-\sum_i p_i^{q}\big) \)
& Any \(q\) & No & Non-extensive statistical mechanics, Turbulence, Astrophysics \\

\textbf{Quantum (von Neumann) entropy} \newline
\( S(\rho) = -\operatorname{Tr}(\rho \log \rho) \)
& \( q=1 \) & No & Quantum information, Entanglement, Quantum thermodynamics \\

\midrule

\textbf{LCR} $D_q^Z(\mathbf{p};Z)=$ \newline
$\big(\sum_i p_i(Z\mathbf{p})_i^{q-1}\big)^{1/(1-q)}$ for $q\neq1$ \newline 
$\exp(-\sum_i p_i \log(Z\mathbf{p})_i)$ for $q=1$
& Any \(q\) & Yes & Phylogenetic, attribute, and functional diversity are special cases. Biodiversity, microbiomes, language... \\

\textbf{Vendi score} \newline
\( \mathrm{VS}(\mathbf{p}_n;Z_n) = \exp{\big( -\sum_{i=1}^{n} \lambda_{i} \log \lambda_{i} \big)} \)
& \(q=1\) & Yes & \( \lambda_i \) are eigenvalues of $Z_n=Z/N$ (positive semi-definite). ML diversity, Generative model evaluation, Ecology \\

\textbf{Cousins of the Vendi score} \newline
\( \text{VS}_q(\mathbf{x},\mathbf{k}) = \left(\sum_i \lambda_i^q \right)^{\tfrac{1}{1-q}} \)
& Any \(q\) & Yes & Quantum-inspired stats, Class diversity profiling \\

\midrule

\textbf{Other variants} \newline
Burg: \( \sum_i \log p_i \) \newline
KL: \( D_{KL}(p\|q)=\sum_i p_i \log\frac{p_i}{q_i} \) \newline
CRE: \( -\int_0^\infty \bar{F}(x)\log \bar{F}(x)\,dx \)
& Partly & Partly & Spectral estimation (Burg); Bayesian inference (KL); Survival analysis (CRE) \\

\bottomrule
\end{tabular}

\caption{Summary of major entropy families, their frequency-weighting forms, similarity sensitivity, and main applications.}
\end{table}","2.1 Traditional, similarity-insensitive entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
2.2 Similarity-sensitive entropy (S-entropy) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2.2.1 The Leinster-Cobbold-Reeve framework (LCR) . . . . . . . . . . . . . . . . . . . . 4
2.2.2 The Vendi score and its “cousins” (VS) . . . . . . . . . . . . . . . . . . . . . . . . 4"
2511.05531v1,http://arxiv.org/abs/2511.05531v1,2025-10-29 07:03:18+00:00,Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,"Brain disorders are an umbrella term for a group of neurological and psychiatric conditions that have a major effect on thinking, feeling, and acting. These conditions encompass a wide range of conditions. The illnesses in question pose significant difficulties not only for individuals, but also for healthcare systems all across the world. In this study, we explore the capability of explainable machine learning for classification of people who suffer from brain disorders. This is accomplished by the utilization of brain connection map, also referred as connectome, derived from functional magnetic resonance imaging (fMRI) data. In order to analyze features that are based on the connectome, we investigated several different feature selection procedures. These strategies included the Least Absolute Shrinkage and Selection Operator (LASSO), Relief, and Analysis of Variance (ANOVA), in addition to a logistic regression (LR) classifier. First and foremost, the purpose was to evaluate and contrast the classification accuracy of different feature selection methods in terms of distinguishing healthy controls from diseased individuals. The evaluation of the stability of the traits that were chosen was the second objective. The identification of the regions of the brain that have an effect on the classification was the third main objective. When applied to the UCLA dataset, the LASSO approach, which is our most effective strategy, produced a classification accuracy of 91.85% and a stability index of 0.74, which is greater than the results obtained by other approaches: Relief and ANOVA. These methods are effective in locating trustworthy biomarkers, which adds to the development of connectome-based classification in the context of issues that impact the brain.","\label{sec:related_work}
Over the past decade, numerous studies have investigated the correlation between brain functional connectivity (FC) and various neurological and mental problems. Researchers identified, selected, and interpreted neuro-imaging characteristics from fMRI \cite{Lynall9477}, EEG \cite{hasanzadeh2020graph}, and MEG \cite{Jin2024} for diagnostic and prognostic reasons using machine learning and deep learning. These systems demonstrate that feature selection, graph-based representations, and multi-modal fusion improve brain disease accuracy and interpretability.
%%%%%% General brain disorders  %%%%%%
Qu et al. \cite{chen2021estimation} introduced a deep graphical approach to extract distinguishing features from functional brain networks. A nonlinear network fusion method using message-passing mechanisms was used to assess multimodal brain connectivity \cite{qu2021brain}. Jie et al. \cite{jie2013integration} improved categorization using topological and statistical FC features across neuroimaging tasks. According to the authors in this study \cite{kragel2022temporal}, spontaneous emotional brain states evolve and affect mental health. Tawhid and colleagues introduced GENet \cite{tawhid2024genet}, a generic neural model for EEG-based neurological diseases. Dai et al. \cite{dai2025feature} suggested a transfer learning method that aligns features across domains to enhance the classification of psychiatric disorders. Authors in \cite{qin2025classification} employed a region-selected graph convolutional network (GCN) to dynamically identify disorder-specific patterns instead of relying on fixed brain areas. A multi-scale dynamic graph technique to model FC fluctuations over time was introduced in another study \cite{ma2023multi}.
%%%%%%% AD %%%%%%
Multimodal integration and graph-based learning have been the primary areas of attention in research about the classification of Alzheimer's disease. An approach to learning that is self-paced and makes use of structural and functional imaging was proposed in \cite{hao2022multimodal}. The application of EEG-based graph neural network (GNN) was demonstrated in \cite{klepl2022eeg}, which showed that the selection of the FC metric had a considerable impact on classification performance.
%%%%%%% MCI %%%%%%
Cao and collaborators \cite{cao2023novel} introduced a dynamic connectivity-based approach for more precise detection of mild cognitive impairment (MCI). In a parallel study, \cite{ma2022diagnosis}  presented an ordinal pattern kernel that captures brain signal temporal patterns for MCI classification.

%%%%%%% MDD %%%%%%

Kang et al. \cite{kang2023classifying} identified depressed patients using residual neural networks trained on certain frequency bands and brain areas. Another study \cite{zheng2023attention} presented an attention-based fusion model for depression diagnosis using multimodal MRI data. The work by Hasanzadeh and colleagues \cite{hasanzadeh2020graph} utilized graph theory to study directed FC networks from EEG signals. Guo et al. \cite{guo2020diagnosis} built whole-brain effective connection networks from resting-state fMRI to find reliable depression related patterns.
%%%%%%% SZ %%%%%%
The innovative schizophrenia diagnosis framework, Schizo-Net \cite{grover2023schizo}, uses multimodal deep learning and EEG-based connection biomarkers.
%%%%%%% Stress %%%%%%
A study \cite{huang2022joint} provided a strategy for selecting joint channels and connectivity applied to fNIRS data for stress detection in decision-making scenarios.
%%%%%%% Autism %%%%%% 
The authors \cite{barik2023functional} utilized machine learning algorithms on MEG data to detect autism in children, with a focus on early diagnosis. In order to highlight the temporal dynamics linked to autism, Jamal et al. \cite{jamal2014classification} used supervised learning to analyze synchrostate-derived EEG characteristics.
%%%%%%% Brain age %%%%%%
The recent paper \cite{han2024brain} examined how dynamic FC changes with age could predict brain age. Their connection analysis method may detect early cognitive deterioration.

%%%%%%% feature selection related %%%%%% 

Neuroimaging investigations require feature selection due to the connectome data's high dimensionality.  Ji and Yao \cite{ji2020convolutional} suggested a CNN-graphical Lasso model for sparse, topologically significant features. The authors in \cite{GUTIERREZGOMEZ2020102316} developed a stability-driven framework to find repeatable biomarkers from functional connectomes in schizophrenia, highlighting the necessity for rigorous feature selection. In the same direction, the study in \cite{Esther2015} demonstrated the effectiveness of a feature selection technique based on support vector machines (SVMs) that uses the weight vector to categorize dementia in high-dimensional neuroimaging data.
Jie et al. \cite{jie2018discriminating} used whole-brain FC and FOBA with SVM to distinguish bipolar disorder from serious depression. Using a majority-voting method, Holker and Susan \cite{holker2021quantitative} ranked EEG characteristics that were important for detecting alcohol use disorders. Ali et al. \cite{ali2023correlation} introduced a correlation-filter-based method for selecting informative channels in EEG-fNIRS systems within a hybrid architecture. Using infinite feature selection, authors \cite{obertino2016infinite} were able to uncover changes in connection following a stroke. In the study \cite{pei2020eeg}, the authors utilized a hybrid method using feature fusion and selection to identify workloads based on EEG data.
%%%%%%%%%%%%%%%%%
Functional connection in clinical neuroscience is shown by the wide range of illnesses and methods in the literature. Most of these studies favor classification accuracy over feature stability and reproducibility, which is crucial for reliable biomarkers. Our work assesses the discriminative strength and consistency of selected features across cross-validation folds using Kuncheva and Jaccard indices. This dual goal improves explainable AI brain illness diagnosis interpretability and reliability.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%","Over the past decade, numerous studies have investigated the correlation between brain functional connectivity (FC) and various neurological and mental problems. Researchers identified, selected, and interpreted neuro-imaging characteristics from fMRI \cite{Lynall9477}, EEG \cite{hasanzadeh2020graph}, and MEG \cite{Jin2024} for diagnostic and prognostic reasons using machine learning and deep learning. These systems demonstrate that feature selection, graph-based representations, and multi-modal fusion improve brain disease accuracy and interpretability.

Qu et al. \cite{chen2021estimation} introduced a deep graphical approach to extract distinguishing features from functional brain networks. A nonlinear network fusion method using message-passing mechanisms was used to assess multimodal brain connectivity \cite{qu2021brain}. Jie et al. \cite{jie2013integration} improved categorization using topological and statistical FC features across neuroimaging tasks. According to the authors in this study \cite{kragel2022temporal}, spontaneous emotional brain states evolve and affect mental health. Tawhid and colleagues introduced GENet \cite{tawhid2024genet}, a generic neural model for EEG-based neurological diseases. Dai et al. \cite{dai2025feature} suggested a transfer learning method that aligns features across domains to enhance the classification of psychiatric disorders. Authors in \cite{qin2025classification} employed a region-selected graph convolutional network (GCN) to dynamically identify disorder-specific patterns instead of relying on fixed brain areas. A multi-scale dynamic graph technique to model FC fluctuations over time was introduced in another study \cite{ma2023multi}.

Multimodal integration and graph-based learning have been the primary areas of attention in research about the classification of Alzheimer's disease. An approach to learning that is self-paced and makes use of structural and functional imaging was proposed in \cite{hao2022multimodal}. The application of EEG-based graph neural network (GNN) was demonstrated in \cite{klepl2022eeg}, which showed that the selection of the FC metric had a considerable impact on classification performance.

Cao and collaborators \cite{cao2023novel} introduced a dynamic connectivity-based approach for more precise detection of mild cognitive impairment (MCI). In a parallel study, \cite{ma2022diagnosis}  presented an ordinal pattern kernel that captures brain signal temporal patterns for MCI classification.



Kang et al. \cite{kang2023classifying} identified depressed patients using residual neural networks trained on certain frequency bands and brain areas. Another study \cite{zheng2023attention} presented an attention-based fusion model for depression diagnosis using multimodal MRI data. The work by Hasanzadeh and colleagues \cite{hasanzadeh2020graph} utilized graph theory to study directed FC networks from EEG signals. Guo et al. \cite{guo2020diagnosis} built whole-brain effective connection networks from resting-state fMRI to find reliable depression related patterns.

The innovative schizophrenia diagnosis framework, Schizo-Net \cite{grover2023schizo}, uses multimodal deep learning and EEG-based connection biomarkers.

A study \cite{huang2022joint} provided a strategy for selecting joint channels and connectivity applied to fNIRS data for stress detection in decision-making scenarios.

The authors \cite{barik2023functional} utilized machine learning algorithms on MEG data to detect autism in children, with a focus on early diagnosis. In order to highlight the temporal dynamics linked to autism, Jamal et al. \cite{jamal2014classification} used supervised learning to analyze synchrostate-derived EEG characteristics.

The recent paper \cite{han2024brain} examined how dynamic FC changes with age could predict brain age. Their connection analysis method may detect early cognitive deterioration.



Neuroimaging investigations require feature selection due to the connectome data's high dimensionality.  Ji and Yao \cite{ji2020convolutional} suggested a CNN-graphical Lasso model for sparse, topologically significant features. The authors in \cite{GUTIERREZGOMEZ2020102316} developed a stability-driven framework to find repeatable biomarkers from functional connectomes in schizophrenia, highlighting the necessity for rigorous feature selection. In the same direction, the study in \cite{Esther2015} demonstrated the effectiveness of a feature selection technique based on support vector machines (SVMs) that uses the weight vector to categorize dementia in high-dimensional neuroimaging data.
Jie et al. \cite{jie2018discriminating} used whole-brain FC and FOBA with SVM to distinguish bipolar disorder from serious depression. Using a majority-voting method, Holker and Susan \cite{holker2021quantitative} ranked EEG characteristics that were important for detecting alcohol use disorders. Ali et al. \cite{ali2023correlation} introduced a correlation-filter-based method for selecting informative channels in EEG-fNIRS systems within a hybrid architecture. Using infinite feature selection, authors \cite{obertino2016infinite} were able to uncover changes in connection following a stroke. In the study \cite{pei2020eeg}, the authors utilized a hybrid method using feature fusion and selection to identify workloads based on EEG data.

Functional connection in clinical neuroscience is shown by the wide range of illnesses and methods in the literature. Most of these studies favor classification accuracy over feature stability and reproducibility, which is crucial for reliable biomarkers. Our work assesses the discriminative strength and consistency of selected features across cross-validation folds using Kuncheva and Jaccard indices. This dual goal improves explainable AI brain illness diagnosis interpretability and reliability.",
2509.14037v1,http://arxiv.org/abs/2509.14037v1,2025-09-17 14:38:52+00:00,PhenoGnet: A Graph-Based Contrastive Learning Framework for Disease Similarity Prediction,"Understanding disease similarity is critical for advancing diagnostics, drug discovery, and personalized treatment strategies. We present PhenoGnet, a novel graph-based contrastive learning framework designed to predict disease similarity by integrating gene functional interaction networks with the Human Phenotype Ontology (HPO). PhenoGnet comprises two key components: an intra-view model that separately encodes gene and phenotype graphs using Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs), and a cross view model implemented as a shared weight multilayer perceptron (MLP) that aligns gene and phenotype embeddings through contrastive learning. The model is trained using known gene phenotype associations as positive pairs and randomly sampled unrelated pairs as negatives. Diseases are represented by the mean embeddings of their associated genes and/or phenotypes, and pairwise similarity is computed via cosine similarity. Evaluation on a curated benchmark of 1,100 similar and 866 dissimilar disease pairs demonstrates strong performance, with gene based embeddings achieving an AUCPR of 0.9012 and AUROC of 0.8764, outperforming existing state of the art methods. Notably, PhenoGnet captures latent biological relationships beyond direct overlap, offering a scalable and interpretable solution for disease similarity prediction. These results underscore its potential for enabling downstream applications in rare disease research and precision medicine.","Graph Neural Networks (GNNs), as a specialized branch of deep learning designed for graph data, have demonstrated significant success, and are particularly well-suited for biomedical informatics. With the rapid growth of biological network data, GNNs have shown strong performance in tasks such as disease association prediction, medical image segmentation, and the identification of disease-gene relationships \cite{zhang2021}. However, the effectiveness of GNNs often relies on the availability of task centric labeled data, which can be scarce in some biomedical domains. To address this limitation, Graph Contrastive Learning (GCL) has emerged as a powerful approach for unsupervised or semi-supervised graph representation learning by allowing to learn discriminative embeddings without requiring extensive labels \cite{zhu2023}. 

Building on this, multi-view graph contrastive learning methods have been developed to further mitigate data sparsity and learn representations from multiple graphs by projecting them into a common latent space\cite{hassani2020}. Particularly in biological association prediction scenarios, integrating diverse networks of graph data can enhance model robustness and generalization \cite{kang2024}. Recent studies have demonstrated the utility of multi-view graph contrastive learning for disease similarity prediction. For instance, CoGO \cite{Chen2022} integrates Gene Ontology (GO) and gene interaction networks to generate disease representation embeddings to measure similarity. DisMVC \cite{Wei2024} combines gene interaction networks with miRNA similarity networks to improve similarity prediction. These approaches highlight the potential of leveraging multiple biological networks through contrastive objectives to capture implicit structural and semantic information.","Graph Neural Networks (GNNs), as a specialized branch of deep learning designed for graph data, have demonstrated significant success, and are particularly well-suited for biomedical informatics. With the rapid growth of biological network data, GNNs have shown strong performance in tasks such as disease association prediction, medical image segmentation, and the identification of disease-gene relationships \cite{zhang2021}. However, the effectiveness of GNNs often relies on the availability of task centric labeled data, which can be scarce in some biomedical domains. To address this limitation, Graph Contrastive Learning (GCL) has emerged as a powerful approach for unsupervised or semi-supervised graph representation learning by allowing to learn discriminative embeddings without requiring extensive labels \cite{zhu2023}. 

Building on this, multi-view graph contrastive learning methods have been developed to further mitigate data sparsity and learn representations from multiple graphs by projecting them into a common latent space\cite{hassani2020}. Particularly in biological association prediction scenarios, integrating diverse networks of graph data can enhance model robustness and generalization \cite{kang2024}. Recent studies have demonstrated the utility of multi-view graph contrastive learning for disease similarity prediction. For instance, CoGO \cite{Chen2022} integrates Gene Ontology (GO) and gene interaction networks to generate disease representation embeddings to measure similarity. DisMVC \cite{Wei2024} combines gene interaction networks with miRNA similarity networks to improve similarity prediction. These approaches highlight the potential of leveraging multiple biological networks through contrastive objectives to capture implicit structural and semantic information.",
2510.16080v1,http://arxiv.org/abs/2510.16080v1,2025-10-17 12:26:29+00:00,TriAgent: Automated Biomarker Discovery with Deep Research Grounding for Triage in Acute Care by LLM-Based Multi-Agent Collaboration,"Emergency departments worldwide face rising patient volumes, workforce shortages, and variability in triage decisions that threaten the delivery of timely and accurate care. Current triage methods rely primarily on vital signs, routine laboratory values, and clinicians' judgment, which, while effective, often miss emerging biological signals that could improve risk prediction for infection typing or antibiotic administration in acute conditions. To address this challenge, we introduce TriAgent, a large language model (LLM)-based multi-agent framework that couples automated biomarker discovery with deep research for literature-grounded validation and novelty assessment. TriAgent employs a supervisor research agent to generate research topics and delegate targeted queries to specialized sub-agents for evidence retrieval from various data sources. Findings are synthesized to classify biomarkers as either grounded in existing knowledge or flagged as novel candidates, offering transparent justification and highlighting unexplored pathways in acute care risk stratification. Unlike prior frameworks limited to existing routine clinical biomarkers, TriAgent aims to deliver an end-to-end framework from data analysis to literature grounding to improve transparency, explainability and expand the frontier of potentially actionable clinical biomarkers. Given a user's clinical query and quantitative triage data, TriAgent achieved a topic adherence F1 score of 55.7 +/- 5.0%, surpassing the CoT-ReAct agent by over 10%, and a faithfulness score of 0.42 +/- 0.39, exceeding all baselines by more than 50%. Across experiments, TriAgent consistently outperformed state-of-the-art LLM-based agentic frameworks in biomarker justification and literature-grounded novelty assessment. We share our repo: https://github.com/CellFace/TriAgent.","\subsection{Biomarker Discovery with LLMs}
To overcome the limitations of biomarker discovery with traditional modeling techniques, AI has emerged as an invaluable tool \cite{RN77,RN76}. Recent works show that LLMs can actively drive biomarker discovery while grounding claims in the biomedical literature. An agentic system orchestrates modular tools (retrieval, code execution, databases) with an LLM to automate biomarker discovery and produce enrichment reports with traceable, cited evidence \cite{RN54}. In parallel, a human-augmented LLM workflow that prioritizes candidate transcriptomic biomarkers using multi-step prompts with GPT-4/Claude \cite{RN55}. Their pipeline repeatedly elevated GPX4 as a top erythroid-module candidate, illustrating how LLMs can surface plausible targets that withstand expert review and can be routed to assay development. Complementary systems for literature-based discovery (LBD) restrict LLM answers to retrieved, cited sources and integrate a user’s experimental context \cite{RN57}. 

% put this a few lines BEFORE the page where you want it to appear

\enlargethispage{\baselineskip}
\subsection{Multi-Agent Collaboration for Clinical Reasoning and Biomarker Discovery}
Beyond single-agent workflows, multi-agent systems emulate collaborative clinical reasoning, improving interpretability and grounding. Hierarchical frameworks couple role-specialized LLMs with knowledge graphs, assigning roles such as genomics, proteomics, and clinical interpretation, to dynamically contextualize putative biomarkers \cite{RN56}. General practitioner and specialist agents (genomics, proteomics, clinical interpretation) are assigned to dynamically build a medical knowledge graph to contextualize and cite putative biomarkers. Multi-Agent Conversation (MAC) framework, where three doctor agents and a supervisory agent, all powered by GPT-4, engaged in interactive diagnosis of rare diseases, outperforming single-model and chain-of-thought \cite{RN69} prompting approaches \cite{RN60}. Moreover, MedAgents framework  demonstrates how role-playing agents engage in expert gathering, individual analyses, report summarization, collaborative consultation, and decision-making, simulating multidisciplinary rounds in clinical practice \cite{RN65}. This training-free setup enhances reasoning and interpretability without requiring domain-specific fine-tuning, achieving state-of-the-art performance across various medical benchmarks. Such findings illustrate how structured multi-agent collaboration can surface domain expertise otherwise inaccessible through single agent architectures. Other adaptive agent systems tailor complexity dynamically: one framework assembles specialist or generalist agents depending on case difficulty \cite{RN61}. Similarly, role-specific LLM agents simulate emergency department staff, using self-confidence metrics and retrieval-augmented generation based on the ESI handbook \cite{RN53}. This system significantly improved triage classification and closed the gap to human-level accuracy in real-world clinical datasets. Beyond triage, modular multi agent framework for ICU decision support deploys specialized agents for lab results, vital interpretation, and contextual reasoning, with a validation agent ensuring ethical oversight and transparency \cite{RN62}.
\enlargethispage{\baselineskip}
\subsection{Problem Statement and Our Contribution}
Despite these advances, no prior work couples automated biomarker discovery with literature-grounded deep research for novelty assessment in acute care triage. Existing multi-agent triage frameworks largely focus on following established guidelines with clinically known biomarkers for guideline-aware decisions with retrieval-augmented evidence in emergency settings by leveraging LLM-based single or multi-agent collaborations. Therefore, these systems currently aim to operate and orchestrate the existing clinical protocols rather than discovering novel biomarkers which could potentially provide faster response time and lower cost with higher specificity for acute-care triage. These constraints point to four unmet challenges: i) traditional frameworks do not surface novel biomarkers dynamically, ii) literature grounding is often absent, iii) practical clinical translation is hindered by analytical and workflow variability, and iv) clinical utility remains uncertain without context-specific validation. 

Addressing these gaps requires a unified, explainable system that not only discovers novel biomarkers from acute care clinical patient data but also rigorously validates them against existing biomedical knowledge and shown to be effective on real-world clinical scenarios for triage decisions. We introduce TriAgent, an LLM-based multi-agentic framework built to discover, ground, and explain novel biomarkers poised to improve acute care triage. At its core, TriAgent consists of two complementary systems: i) a data analysis agent that autonomously mines patient data readily available from routine tests to identify candidate biomarkers not traditionally used in triage and ii) a deep research agent for literature-grounding that retrieves and consolidates supporting medical evidence. If the biomarker appears in literature, it is marked as grounded. If not, TriAgent flags it as novel for validation, and generates a structured justification on biological patterns, and reasoning to explain clinical significance.

\begin{figure*}[!t]
  \centering
  \includegraphics[width=\textwidth]{fig1.png}
  \caption{Architecture of TriAgent. TriAgent is a graph-based multi-agent framework for automated biomarker discovery and literature-grounded validation. The Scoping and Data Analysis Agents produce structured briefs (Brief-1, Brief-2); the Research Supervisor Agent coordinates sub-agents for RAG-based searches; the Reporting Agent integrates findings into an auditable report.}
  \label{fig:triagent-arch}
  \Description{Two-column architecture diagram as described in the caption.}
\end{figure*}

\enlargethispage{\baselineskip}","\subsection{Biomarker Discovery with LLMs}
To overcome the limitations of biomarker discovery with traditional modeling techniques, AI has emerged as an invaluable tool \cite{RN77,RN76}. Recent works show that LLMs can actively drive biomarker discovery while grounding claims in the biomedical literature. An agentic system orchestrates modular tools (retrieval, code execution, databases) with an LLM to automate biomarker discovery and produce enrichment reports with traceable, cited evidence \cite{RN54}. In parallel, a human-augmented LLM workflow that prioritizes candidate transcriptomic biomarkers using multi-step prompts with GPT-4/Claude \cite{RN55}. Their pipeline repeatedly elevated GPX4 as a top erythroid-module candidate, illustrating how LLMs can surface plausible targets that withstand expert review and can be routed to assay development. Complementary systems for literature-based discovery (LBD) restrict LLM answers to retrieved, cited sources and integrate a user’s experimental context \cite{RN57}. 



\enlargethispage{\baselineskip}
\subsection{Multi-Agent Collaboration for Clinical Reasoning and Biomarker Discovery}
Beyond single-agent workflows, multi-agent systems emulate collaborative clinical reasoning, improving interpretability and grounding. Hierarchical frameworks couple role-specialized LLMs with knowledge graphs, assigning roles such as genomics, proteomics, and clinical interpretation, to dynamically contextualize putative biomarkers \cite{RN56}. General practitioner and specialist agents (genomics, proteomics, clinical interpretation) are assigned to dynamically build a medical knowledge graph to contextualize and cite putative biomarkers. Multi-Agent Conversation (MAC) framework, where three doctor agents and a supervisory agent, all powered by GPT-4, engaged in interactive diagnosis of rare diseases, outperforming single-model and chain-of-thought \cite{RN69} prompting approaches \cite{RN60}. Moreover, MedAgents framework  demonstrates how role-playing agents engage in expert gathering, individual analyses, report summarization, collaborative consultation, and decision-making, simulating multidisciplinary rounds in clinical practice \cite{RN65}. This training-free setup enhances reasoning and interpretability without requiring domain-specific fine-tuning, achieving state-of-the-art performance across various medical benchmarks. Such findings illustrate how structured multi-agent collaboration can surface domain expertise otherwise inaccessible through single agent architectures. Other adaptive agent systems tailor complexity dynamically: one framework assembles specialist or generalist agents depending on case difficulty \cite{RN61}. Similarly, role-specific LLM agents simulate emergency department staff, using self-confidence metrics and retrieval-augmented generation based on the ESI handbook \cite{RN53}. This system significantly improved triage classification and closed the gap to human-level accuracy in real-world clinical datasets. Beyond triage, modular multi agent framework for ICU decision support deploys specialized agents for lab results, vital interpretation, and contextual reasoning, with a validation agent ensuring ethical oversight and transparency \cite{RN62}.
\enlargethispage{\baselineskip}
\subsection{Problem Statement and Our Contribution}
Despite these advances, no prior work couples automated biomarker discovery with literature-grounded deep research for novelty assessment in acute care triage. Existing multi-agent triage frameworks largely focus on following established guidelines with clinically known biomarkers for guideline-aware decisions with retrieval-augmented evidence in emergency settings by leveraging LLM-based single or multi-agent collaborations. Therefore, these systems currently aim to operate and orchestrate the existing clinical protocols rather than discovering novel biomarkers which could potentially provide faster response time and lower cost with higher specificity for acute-care triage. These constraints point to four unmet challenges: i) traditional frameworks do not surface novel biomarkers dynamically, ii) literature grounding is often absent, iii) practical clinical translation is hindered by analytical and workflow variability, and iv) clinical utility remains uncertain without context-specific validation. 

Addressing these gaps requires a unified, explainable system that not only discovers novel biomarkers from acute care clinical patient data but also rigorously validates them against existing biomedical knowledge and shown to be effective on real-world clinical scenarios for triage decisions. We introduce TriAgent, an LLM-based multi-agentic framework built to discover, ground, and explain novel biomarkers poised to improve acute care triage. At its core, TriAgent consists of two complementary systems: i) a data analysis agent that autonomously mines patient data readily available from routine tests to identify candidate biomarkers not traditionally used in triage and ii) a deep research agent for literature-grounding that retrieves and consolidates supporting medical evidence. If the biomarker appears in literature, it is marked as grounded. If not, TriAgent flags it as novel for validation, and generates a structured justification on biological patterns, and reasoning to explain clinical significance.



\enlargethispage{\baselineskip}","2.1 Biomarker Discovery with LLMs
To overcome the limitations of biomarker discovery with traditional
modeling techniques, AI has emerged as an invaluable tool [ 11,15].
Recent works show that LLMs can actively drive biomarker dis-
covery while grounding claims in the biomedical literature. An
agentic system orchestrates modular tools (retrieval, code execu-
tion, databases) with an LLM to automate biomarker discovery and
produce enrichment reports with traceable, cited evidence [ 17]. In
parallel, a human-augmented LLM workflow that prioritizes can-
didate transcriptomic biomarkers using multi-step prompts with
GPT-4/Claude [ 21]. Their pipeline repeatedly elevated GPX4 as a
top erythroid-module candidate, illustrating how LLMs can surface
plausible targets that withstand expert review and can be routed to
assay development. Complementary systems for literature-based
discovery (LBD) restrict LLM answers to retrieved, cited sources
and integrate a user’s experimental context [3].
2.2 Multi-Agent Collaboration for Clinical
Reasoning and Biomarker Discovery
Beyond single-agent workflows, multi-agent systems emulate col-
laborative clinical reasoning, improving interpretability and ground-
ing. Hierarchical frameworks couple role-specialized LLMs with
knowledge graphs, assigning roles such as genomics, proteomics,
and clinical interpretation, to dynamically contextualize putative
biomarkers [ 27]. General practitioner and specialist agents (ge-
nomics, proteomics, clinical interpretation) are assigned to dynam-
ically build a medical knowledge graph to contextualize and cite
putative biomarkers. Multi-Agent Conversation (MAC) framework,
where three doctor agents and a supervisory agent, all powered
by GPT-4, engaged in interactive diagnosis of rare diseases, out-
performing single-model and chain-of-thought [ 25] prompting ap-
proaches [ 1]. Moreover, MedAgents framework demonstrates how
role-playing agents engage in expert gathering, individual analyses,
report summarization, collaborative consultation, and decision-
making, simulating multidisciplinary rounds in clinical practice
[22]. This training-free setup enhances reasoning and interpretabil-
ity without requiring domain-specific fine-tuning, achieving state-
of-the-art performance across various medical benchmarks. Such
findings illustrate how structured multi-agent collaboration can
surface domain expertise otherwise inaccessible through single
agent architectures. Other adaptive agent systems tailor complex-
ity dynamically: one framework assembles specialist or generalist
agents depending on case difficulty [ 10]. Similarly, role-specific LLM
agents simulate emergency department staff, using self-confidence
metrics and retrieval-augmented generation based on the ESI hand-
book [ 12]. This system significantly improved triage classification
and closed the gap to human-level accuracy in real-world clinical
datasets. Beyond triage, modular multi agent framework for ICU
decision support deploys specialized agents for lab results, vital
interpretation, and contextual reasoning, with a validation agent
ensuring ethical oversight and transparency [2].
2.3 Problem Statement and Our Contribution
Despite these advances, no prior work couples automated biomarker
discovery with literature-grounded deep research for novelty assess-
ment in acute care triage. Existing multi-agent triage frameworkslargely focus on following established guidelines with clinically
known biomarkers for guideline-aware decisions with retrieval-
augmented evidence in emergency settings by leveraging LLM-
based single or multi-agent collaborations. Therefore, these sys-
tems currently aim to operate and orchestrate the existing clinical
protocols rather than discovering novel biomarkers which could
potentially provide faster response time and lower cost with higher
specificity for acute-care triage. These constraints point to four
unmet challenges: i) traditional frameworks do not surface novel
biomarkers dynamically, ii) literature grounding is often absent, iii)
practical clinical translation is hindered by analytical and work-
flow variability, and iv) clinical utility remains uncertain without
context-specific validation.
Addressing these gaps requires a unified, explainable system that
not only discovers novel biomarkers from acute care clinical patient
data but also rigorously validates them against existing biomedical
knowledge and shown to be effective on real-world clinical sce-
narios for triage decisions. We introduce TriAgent, an LLM-based
multi-agentic framework built to discover, ground, and explain
novel biomarkers poised to improve acute care triage. At its core,
TriAgent consists of two complementary systems: i) a data analy-
sis agent that autonomously mines patient data readily available
from routine tests to identify candidate biomarkers not traditionally
used in triage and ii) a deep research agent for literature-grounding
that retrieves and consolidates supporting medical evidence. If the
biomarker appears in literature, it is marked as grounded. If not,
TriAgent flags it as novel for validation, and generates a structured
justification on biological patterns, and reasoning to explain clinical
significance."
2509.13294v1,http://arxiv.org/abs/2509.13294v1,2025-09-16 17:48:58+00:00,Accelerating Protein Molecular Dynamics Simulation with DeepJump,"Unraveling the dynamical motions of biomolecules is essential for bridging their structure and function, yet it remains a major computational challenge. Molecular dynamics (MD) simulation provides a detailed depiction of biomolecular motion, but its high-resolution temporal evolution comes at significant computational cost, limiting its applicability to timescales of biological relevance. Deep learning approaches have emerged as promising solutions to overcome these computational limitations by learning to predict long-timescale dynamics. However, generalizable kinetics models for proteins remain largely unexplored, and the fundamental limits of achievable acceleration while preserving dynamical accuracy are poorly understood. In this work, we fill this gap with DeepJump, an Euclidean-Equivariant Flow Matching-based model for predicting protein conformational dynamics across multiple temporal scales. We train DeepJump on trajectories of the diverse proteins of mdCATH, systematically studying our model's performance in generalizing to long-term dynamics of fast-folding proteins and characterizing the trade-off between computational acceleration and prediction accuracy. We demonstrate the application of DeepJump to ab initio folding, showcasing prediction of folding pathways and native states. Our results demonstrate that DeepJump achieves significant $\approx$1000$\times$ computational acceleration while effectively recovering long-timescale dynamics, providing a stepping stone for enabling routine simulation of proteins.","Diverse deep learning efforts aim to reproduce MD and tackle its fundamental bottlenecks. Machine learning force fields (MLFFs) like ANI \citep{smith2017ani} NequIP \citep{batzner20223} and MACE \citep{batatia2022mace} have demonstrated remarkable accuracy in reproducing potentials while maintaining computational efficiency. However, MLFFs are constrained by the timestep limitations, as they still require integration of the full atomic equations of motion around femtosecond resolution. Recent breakthroughs have instead turned to generative models, with approaches like AlphaFlow \citep{jing2024alphafold} and BioEmu \citep{lewis2025scalable} training over trajectory data to generate Boltzmann ensembles. Still, ensemble models fail to capture the whole picture of dynamics, as trajectories are needed for mechanistic understanding. In further development towards capturing kinetics,
recent models \citep{ito,li2024f3low,jing24generative} enable sampling of dynamics trajectories with large steps, suggesting a pathway for accelerated MD simulation where the large time that is skipped outweighs the cost of evaluating the neural network. In this work, we build upon EquiJump \citep{costa2024equijump} and JAMUN \citep{daigavane2025jamunbridgingsmoothedmolecular}, leveraging equivariant neural networks \citep{geiger2022e3nn} for generating proximal conformational states. We investigate multiple step sizes, as in ITO \citep{ito}, while utilizing the sampling efficiency of Guided Flow Matching \citep{lipman2022flow}, as in F$^3
$low \citep{li2024f3low}. We extend this line of work to investigate acceleration-accuracy trade-offs when reproducing the dynamics of fast-folding proteins, demonstrating generalization of dynamics across both different proteins and conformational phase space, and benchmarking the limits of practical applicability for large-scale protein simulations.","Diverse deep learning efforts aim to reproduce MD and tackle its fundamental bottlenecks. Machine learning force fields (MLFFs) like ANI \citep{smith2017ani} NequIP \citep{batzner20223} and MACE \citep{batatia2022mace} have demonstrated remarkable accuracy in reproducing potentials while maintaining computational efficiency. However, MLFFs are constrained by the timestep limitations, as they still require integration of the full atomic equations of motion around femtosecond resolution. Recent breakthroughs have instead turned to generative models, with approaches like AlphaFlow \citep{jing2024alphafold} and BioEmu \citep{lewis2025scalable} training over trajectory data to generate Boltzmann ensembles. Still, ensemble models fail to capture the whole picture of dynamics, as trajectories are needed for mechanistic understanding. In further development towards capturing kinetics,
recent models \citep{ito,li2024f3low,jing24generative} enable sampling of dynamics trajectories with large steps, suggesting a pathway for accelerated MD simulation where the large time that is skipped outweighs the cost of evaluating the neural network. In this work, we build upon EquiJump \citep{costa2024equijump} and JAMUN \citep{daigavane2025jamunbridgingsmoothedmolecular}, leveraging equivariant neural networks \citep{geiger2022e3nn} for generating proximal conformational states. We investigate multiple step sizes, as in ITO \citep{ito}, while utilizing the sampling efficiency of Guided Flow Matching \citep{lipman2022flow}, as in F$^3
$low \citep{li2024f3low}. We extend this line of work to investigate acceleration-accuracy trade-offs when reproducing the dynamics of fast-folding proteins, demonstrating generalization of dynamics across both different proteins and conformational phase space, and benchmarking the limits of practical applicability for large-scale protein simulations.",
2510.02259v1,http://arxiv.org/abs/2510.02259v1,2025-10-02 17:42:10+00:00,Transformers Discover Molecular Structure Without Graph Priors,"Graph Neural Networks (GNNs) are the dominant architecture for molecular machine learning, particularly for molecular property prediction and machine learning interatomic potentials (MLIPs). GNNs perform message passing on predefined graphs often induced by a fixed radius cutoff or k-nearest neighbor scheme. While this design aligns with the locality present in many molecular tasks, a hard-coded graph can limit expressivity due to the fixed receptive field and slows down inference with sparse graph operations. In this work, we investigate whether pure, unmodified Transformers trained directly on Cartesian coordinates$\unicode{x2013}$without predefined graphs or physical priors$\unicode{x2013}$can approximate molecular energies and forces. As a starting point for our analysis, we demonstrate how to train a Transformer to competitive energy and force mean absolute errors under a matched training compute budget, relative to a state-of-the-art equivariant GNN on the OMol25 dataset. We discover that the Transformer learns physically consistent patterns$\unicode{x2013}$such as attention weights that decay inversely with interatomic distance$\unicode{x2013}$and flexibly adapts them across different molecular environments due to the absence of hard-coded biases. The use of a standard Transformer also unlocks predictable improvements with respect to scaling training resources, consistent with empirical scaling laws observed in other domains. Our results demonstrate that many favorable properties of GNNs can emerge adaptively in Transformers, challenging the necessity of hard-coded graph inductive biases and pointing toward standardized, scalable architectures for molecular modeling.","\label{sec:related}
\vspace{-6pt}

% \tk{Update to include other pure transformer works and why they are bad}

%\ak{Transformers in molecular ML: make it clear these are with priors or hybrid approaches (graph Transformers)}

%\ak{Some sections could be: GNNs for molecular modeling; Transformers in molecular ML (priors/hybrid with graphs); Inductive bias debates; Scaling laws in scientific ML}

%\ak{Background section: molecular property prediction (energy, forces, MLIPs); graph construction in GNNs (cutoff radius, kNN, pros/cons); Transformers basics (self-attention, positional encodings)}


\paragraph{Machine Learning Interatomic Potentials (MLIPs) and Molecular Property Prediction.}

%\ak{should mention how MLIPs have become a very popular application of GNNs}

Machine learning interatomic potentials (MLIPs) are a popular application area for graph neural networks (GNNs). MLIPs are typically trained using supervised learning to predict a molecule-level energy and per-atom force labels, which are generated from reference computational chemistry methods (like Density Functional Theory).\citet{Behler2007} popularized the use of MLIPs as a substitute for expensive computational chemistry calculations, leading to numerous applications of MLIPs for the study of chemical systems \citep{batatia2024macemp, garrison2023applying, Artrith2016}. Early MLIPs were not graph-based, but used handcrafted physical features. More recent MLIPs are graph-based and incorporate physical inductive biases into the architecture \citep{batatia_mace_2022, gasteiger_gemnet_2021, Batzner_2022nequip}, such as rotational equivariance and geometric features. Although recent models have started to discard some of these hard constraints \citep{qu2024importance, neumann2024orbfastscalableneural, mazitov2025petmadlightweightuniversalinteratomic}, they continue to rely on GNNs as the architectural backbone. 

While some GNNs and MLIPs do incorporate attention-based mechanisms \citep{gat_oversmooth, liao2024equiformerv2, qu2024importance}, it is important to note that \textit{they are not using the standard Transformer architecture} \citep{vaswani2023attentionneed} and still operate on a predefined graph structure. While a Transformer can be viewed as operating on a fully connected graph, its attention mechanism is fully connected for every input. In contrast, GNNs typically construct a new graph for each input (e.g., using a radius cutoff). Although some models for other molecular property prediction tasks have begun exploring Transformers, they either use only textual molecular descriptors (e.g., SMILES) \citep{chithrananda2020chembertalargescaleselfsupervisedpretraining} or depend on custom graph embeddings \citep{kim2022puretransformerspowerfulgraph, rampášek2023recipegeneralpowerfulscalable} and modifications of the attention mechanism to include physical inductive biases \citep{zhou2023unimol, eissler2025simplegoofftheshelftransformer}. Broader ML and generative modeling work has also relaxed physical inductive biases \citep{ Abramson2024,joshi2025allatomdiffusiontransformersunified, wang2024swallowingbitterpillsimplified, vadgama2025probingequivariancesymmetrybreaking}, but these approaches have not gained as much traction in MLIPs or molecular property prediction. In this study, we explore an unmodified Transformer without any graph structure or physics-inspired featurization.    

\vspace{-4pt}
\paragraph{Scaling Laws.} Other fields of ML have converged on the Transformer architecture \citep{vaswani2023attentionneed}, including natural language processing \citep{devlin2019bertpretrainingdeepbidirectional, grattafiori2024llama3herdmodels, touvron2023llamaopenefficientfoundation}, computer vision \citep{dosovitskiy2021imageworth16x16wordsvit} and even robotics, where exact physical constraints are known \citep{octomodelteam2024octoopensourcegeneralistrobot, kim2024openvlaopensourcevisionlanguageactionmodel}. A key factor driving this convergence is the discovery of empirical scaling laws, which reveal predictable relationships between validation loss, model size, dataset size, and computational resources \citep{kaplan2020scaling, henighan2020scalinglawsautoregressivegenerative, hoffmann2022trainingcomputeoptimallargelanguage}. These laws have been observed over orders of magnitude in resources \citep{touvron2023llamaopenefficientfoundation, brown2020languagemodelsfewshotlearners}, showing that larger models reliably yield better performance given sufficient data and compute. In the context of MLIPs, \citet{Frey2023} explored scaling in neural network force fields but found significant deviations from consistent power law relationships with the models and datasets available at the time. More recently, \citet{wood2025umafamilyuniversalmodels} found scaling trends on the new OMol25 dataset \citep{levine2025openmolecules2025omol25}, but required a sophisticated mixture-of-experts scheme to train their largest models. To the best of our knowledge, there are no graph-based MLIPs at the scale of models seen in other ML fields in terms of number of parameters.

\begin{wrapfigure}{R}{0.43\textwidth}
  \centering
  \includegraphics[width=0.38\textwidth]{images/model_fig_2.png}
  \caption{\textbf{Graph-Free Transformers Model Design.} Our model encodes both discretized and continuous molecular sequences using unmodified Transformer layers. Placeholder values represent discrete inputs in the continuous sequence.}
  \label{fig:model_fig}
  \vspace{-6pt}
\end{wrapfigure}

\vspace{-4pt}
\paragraph{Challenges With Graph-Based Learning.} Empirical evidence suggests GNNs are hard to scale compared to Transformers. \citet{sriram2022trainingbillionparametergraph} scaled a GemNet model up to 1B parameters but found the best performance was with only about 300M parameters. The MACE architecture also exhibits performance saturation at just two layers deep \citep{batatia_mace_2022, batatia2024macemp, kovács2023maceoff23}. Even recent models designed for scalability only have up to a few hundred million parameters when reporting their best results \citep{neumann2024orbfastscalableneural, qu2024importance}, which is still small in magnitude compared to current models in other fields of ML \citep{touvron2023llamaopenefficientfoundation, brown2020languagemodelsfewshotlearners}.

GNNs have a number of theoretical and practical issues that hinder their scalability. The permutation invariance and graph bottlenecks in message passing schemes can lead to oversmoothing \citep{rusch2023surveyoversmoothinggraphneural} and oversquashing \citep{digiovanni2023oversquashingmessagepassingneural, topping2022understandingoversquashingbottlenecksgraphs} which theoretically limit the expressive power of GNNs at depth and hinder modeling long-range interactions \citep{dwivedi2023longrangegraphbenchmark}. These theoretical limitations also apply to graph-attention mechanisms \citep{gat_oversmooth}. \citet{bechlerspeicher2024gnnregular} showed that the use of GNNs can hurt generalization when the targets are labeled without a graph structure. \citet{kreiman2025understandingmitigatingdistributionshifts} found that GNN-based MLIPs tend to overfit to the graph structures encountered during training and struggle to generalize to new molecular geometries. The reliance on sparse operations across (potentially large) graphs further complicates efficient parallelization of training on modern hardware \citep{sriram2022trainingbillionparametergraph, powergraph}. 






%\paragraph{Post-Training} \tk{Do we need a whole paragraph here in related work? Was thinking it could be useful to preface physicaltiy limitations and potential solutions}","\vspace{-6pt}










\paragraph{Machine Learning Interatomic Potentials (MLIPs) and Molecular Property Prediction.}



Machine learning interatomic potentials (MLIPs) are a popular application area for graph neural networks (GNNs). MLIPs are typically trained using supervised learning to predict a molecule-level energy and per-atom force labels, which are generated from reference computational chemistry methods (like Density Functional Theory).\citet{Behler2007} popularized the use of MLIPs as a substitute for expensive computational chemistry calculations, leading to numerous applications of MLIPs for the study of chemical systems \citep{batatia2024macemp, garrison2023applying, Artrith2016}. Early MLIPs were not graph-based, but used handcrafted physical features. More recent MLIPs are graph-based and incorporate physical inductive biases into the architecture \citep{batatia_mace_2022, gasteiger_gemnet_2021, Batzner_2022nequip}, such as rotational equivariance and geometric features. Although recent models have started to discard some of these hard constraints \citep{qu2024importance, neumann2024orbfastscalableneural, mazitov2025petmadlightweightuniversalinteratomic}, they continue to rely on GNNs as the architectural backbone. 

While some GNNs and MLIPs do incorporate attention-based mechanisms \citep{gat_oversmooth, liao2024equiformerv2, qu2024importance}, it is important to note that \textit{they are not using the standard Transformer architecture} \citep{vaswani2023attentionneed} and still operate on a predefined graph structure. While a Transformer can be viewed as operating on a fully connected graph, its attention mechanism is fully connected for every input. In contrast, GNNs typically construct a new graph for each input (e.g., using a radius cutoff). Although some models for other molecular property prediction tasks have begun exploring Transformers, they either use only textual molecular descriptors (e.g., SMILES) \citep{chithrananda2020chembertalargescaleselfsupervisedpretraining} or depend on custom graph embeddings \citep{kim2022puretransformerspowerfulgraph, rampášek2023recipegeneralpowerfulscalable} and modifications of the attention mechanism to include physical inductive biases \citep{zhou2023unimol, eissler2025simplegoofftheshelftransformer}. Broader ML and generative modeling work has also relaxed physical inductive biases \citep{ Abramson2024,joshi2025allatomdiffusiontransformersunified, wang2024swallowingbitterpillsimplified, vadgama2025probingequivariancesymmetrybreaking}, but these approaches have not gained as much traction in MLIPs or molecular property prediction. In this study, we explore an unmodified Transformer without any graph structure or physics-inspired featurization.    

\vspace{-4pt}
\paragraph{Scaling Laws.} Other fields of ML have converged on the Transformer architecture \citep{vaswani2023attentionneed}, including natural language processing \citep{devlin2019bertpretrainingdeepbidirectional, grattafiori2024llama3herdmodels, touvron2023llamaopenefficientfoundation}, computer vision \citep{dosovitskiy2021imageworth16x16wordsvit} and even robotics, where exact physical constraints are known \citep{octomodelteam2024octoopensourcegeneralistrobot, kim2024openvlaopensourcevisionlanguageactionmodel}. A key factor driving this convergence is the discovery of empirical scaling laws, which reveal predictable relationships between validation loss, model size, dataset size, and computational resources \citep{kaplan2020scaling, henighan2020scalinglawsautoregressivegenerative, hoffmann2022trainingcomputeoptimallargelanguage}. These laws have been observed over orders of magnitude in resources \citep{touvron2023llamaopenefficientfoundation, brown2020languagemodelsfewshotlearners}, showing that larger models reliably yield better performance given sufficient data and compute. In the context of MLIPs, \citet{Frey2023} explored scaling in neural network force fields but found significant deviations from consistent power law relationships with the models and datasets available at the time. More recently, \citet{wood2025umafamilyuniversalmodels} found scaling trends on the new OMol25 dataset \citep{levine2025openmolecules2025omol25}, but required a sophisticated mixture-of-experts scheme to train their largest models. To the best of our knowledge, there are no graph-based MLIPs at the scale of models seen in other ML fields in terms of number of parameters.

\begin{wrapfigure}{R}{0.43\textwidth}
  \centering
  \includegraphics[width=0.38\textwidth]{images/model_fig_2.png}
  \caption{\textbf{Graph-Free Transformers Model Design.} Our model encodes both discretized and continuous molecular sequences using unmodified Transformer layers. Placeholder values represent discrete inputs in the continuous sequence.}
    \vspace{-6pt}
\end{wrapfigure}

\vspace{-4pt}
\paragraph{Challenges With Graph-Based Learning.} Empirical evidence suggests GNNs are hard to scale compared to Transformers. \citet{sriram2022trainingbillionparametergraph} scaled a GemNet model up to 1B parameters but found the best performance was with only about 300M parameters. The MACE architecture also exhibits performance saturation at just two layers deep \citep{batatia_mace_2022, batatia2024macemp, kovács2023maceoff23}. Even recent models designed for scalability only have up to a few hundred million parameters when reporting their best results \citep{neumann2024orbfastscalableneural, qu2024importance}, which is still small in magnitude compared to current models in other fields of ML \citep{touvron2023llamaopenefficientfoundation, brown2020languagemodelsfewshotlearners}.

GNNs have a number of theoretical and practical issues that hinder their scalability. The permutation invariance and graph bottlenecks in message passing schemes can lead to oversmoothing \citep{rusch2023surveyoversmoothinggraphneural} and oversquashing \citep{digiovanni2023oversquashingmessagepassingneural, topping2022understandingoversquashingbottlenecksgraphs} which theoretically limit the expressive power of GNNs at depth and hinder modeling long-range interactions \citep{dwivedi2023longrangegraphbenchmark}. These theoretical limitations also apply to graph-attention mechanisms \citep{gat_oversmooth}. \citet{bechlerspeicher2024gnnregular} showed that the use of GNNs can hurt generalization when the targets are labeled without a graph structure. \citet{kreiman2025understandingmitigatingdistributionshifts} found that GNN-based MLIPs tend to overfit to the graph structures encountered during training and struggle to generalize to new molecular geometries. The reliance on sparse operations across (potentially large) graphs further complicates efficient parallelization of training on modern hardware \citep{sriram2022trainingbillionparametergraph, powergraph}.",
2511.09576v1,http://arxiv.org/abs/2511.09576v1,2025-11-12 06:13:50+00:00,Prostate-VarBench: A Benchmark with Interpretable TabNet Framework for Prostate Cancer Variant Classification,"Variants of Uncertain Significance (VUS) limit the clinical utility of prostate cancer genomics by delaying diagnosis and therapy when evidence for pathogenicity or benignity is incomplete. Progress is further limited by inconsistent annotations across sources and the absence of a prostate-specific benchmark for fair comparison. We introduce Prostate-VarBench, a curated pipeline for creating prostate-specific benchmarks that integrates COSMIC (somatic cancer mutations), ClinVar (expert-curated clinical variants), and TCGA-PRAD (prostate tumor genomics from The Cancer Genome Atlas) into a harmonized dataset of 193,278 variants supporting patient- or gene-aware splits to prevent data leakage. To ensure data integrity, we corrected a Variant Effect Predictor (VEP) issue that merged multiple transcript records, introducing ambiguity in clinical significance fields. We then standardized 56 interpretable features across eight clinically relevant tiers, including population frequency, variant type, and clinical context. AlphaMissense pathogenicity scores were incorporated to enhance missense variant classification and reduce VUS uncertainty. Building on this resource, we trained an interpretable TabNet model to classify variant pathogenicity, whose step-wise sparse masks provide per-case rationales consistent with molecular tumor board review practices. On the held-out test set, the model achieved 89.9% accuracy with balanced class metrics, and the VEP correction yields an 6.5% absolute reduction in VUS.","% \subsection{Traditional Genomic Variant Classification: Limitations and Clinical Gaps}
% Computational pathogenicity prediction has evolved from single-tool approaches (SIFT, PolyPhen-2, CADD) to meta-predictors such as MetaSVM and MetaLR~\citep{Dong2015_MetaSVM_MetaLR}, yet fundamental limitations persist. These tools fail primarily due to (1) \emph{limited feature diversity}—relying on conservation and sequence metrics while ignoring tissue-specific expression and pathway context, (2) \emph{ancestry bias}—training predominantly on European populations, creating systematic misclassification in diverse cohorts~\citep{Popejoy2016GenomicsDiversity}, and (3) \emph{clinical disconnect}—providing binary predictions without therapeutic context essential for treatment decisions (PARP inhibitor eligibility, hormone therapy selection). Clinical databases including ClinVar provide curated annotations, but 30-50\% of prostate cancer variants remain classified as VUS, creating a \$1.25+ billion annual healthcare burden through inconclusive genomic testing~\citep{richards2015standards}. Current ACMG/AMP guidelines~\citep{Richards2015_ACMG}, while improving consistency, lack the interpretability and prostate-specific optimization required for precision oncology workflows.

\subsection{Traditional genomic variant classification—limitations.}
Classical predictors (SIFT, PolyPhen\mbox{-}2, CADD) and meta\mbox{-}predictors (MetaSVM/MetaLR) remain widely used but emphasize conservation/sequence features, show ancestry-related biases, and yield task-agnostic scores that lack therapeutic context~\citep{Ng2003SIFT,Adzhubei2010PolyPhen2,Kircher2014CADD,Dong2015MetaSVM,Popejoy2016GenomicsDiversity}.
In clinical pipelines, VUS remain common—\(\sim41\%\) of tested individuals carry \(\ge 1\) VUS and \(31.7\%\) receive VUS-only results; complicating decisions even under ACMG/AMP standards~\citep{Chen2023JAMANetOpenVUS,Richards2015ACMGAMP}.
These gaps motivate disease-specific, \emph{inherently interpretable} models tailored to prostate cancer genomics.


\subsection{Machine Learning Evolution: Performance vs. Interpretability Trade-offs}  
Recent machine learning approaches demonstrate competitive performance—XGBoost implementations~\citep{Khandakji2022BRCA2XGB} achieve ROC-AUC of 0.93 in prostate cancer variant prediction, while deep CNNs achieve F1-scores exceeding 0.88 across cancer types. However, these gains create critical clinical deployment barriers: (1) \emph{post-hoc explanation dependency}—requiring SHAP or LIME methods that may not reflect actual model reasoning, compromising regulatory approval, (2) \emph{black-box opacity}—preventing molecular tumor board integration where clinicians require transparent rationales~\citep{Tamborero2022_MTBP}, and (3) \emph{generalization challenges}—pan-cancer models lack prostate-specific optimization for androgen receptor pathway analysis and DNA repair deficiency assessment critical for PARP inhibitor selection. Regulatory frameworks demand explainable AI for medical devices~\citep{FDA2025_AIEnabledDSF_Draft,FDA2021_GMLP}, creating an interpretability-performance gap that limits clinical adoption. TabNet~\citep{arik2020tabnetattentiveinterpretabletabular} addresses this trade-off through sequential attention mechanisms providing inherent explainability while maintaining competitive performance.

% \subsection{Critical Research Gap: Interpretable AI for Prostate Cancer Genomics}
% While interpretable architectures like TabNet show promise for genomic applications, significant gaps remain in cancer-specific implementations. \textbf{Comprehensive literature analysis reveals zero publications applying TabNet specifically to prostate cancer genomic variant classification}—representing a significant research opportunity at the intersection of interpretable AI and precision urology. Current prostate cancer AI focuses predominantly on imaging (MRI lesion detection achieving 85\% PI-RADS 5 identification~\citep{MGB2024_AI_TrialScreening}) and pathology (Gleason grading with AUC 0.92~\citep{Bulten2022_PANDAChallenge}), while genomic interpretation remains dependent on traditional, non-interpretable tools. This gap is critical given well-defined therapeutic pathways—PARP inhibitors for BRCA1/2 and homologous recombination deficiency, hormone therapy guided by androgen receptor variants, and immunotherapy selection based on microsatellite instability. % \noindent\textbf{Burden and costs.}
% ACS projects \(\sim313{,}780\) new U.S. prostate cancer diagnoses in 2025~\citep{ACS2025}. Comprehensive genomic profiling is costly (FoundationOne CDx list \(\$5{,}800\); Medicare \(\approx\$3{,}500\); tests \(\$300\) to \(\$10{,}000\)~\citep{FoundationOneCDxSEC,CMS0037U,ACSCostsGenomicTesting}), while VUS are common (\(\sim41\%\) with \(\ge1\) VUS; \(31.7\%\) VUS-only) and should not guide management~\citep{Chen2023JAMANetOpenVUS,Richards2015ACMGAMP}.



% With 250,000+ annual diagnoses and \$5,000-8,000 per genomic test, reducing VUS uncertainty from 30-50\% to actionable classifications could save \$175M+ annually through improved treatment selection and reduced repeat testing. 



% TabNet's validated pan-cancer genomics performance (AUC 0.96 across 105 patients in 7 TCGA studies) demonstrates suitability for prostate-specific applications. The convergence of clinical need, proven methodology, and regulatory requirements for interpretable AI creates an optimal environment for TabNet application to prostate cancer genomics.

% In a pan-cancer, tumor-only WES task trained on 105 TCGA patients across seven studies, TabNet was benchmarked alongside XGBoost and LightGBM; on a TCGA holdout dataset the tabular models achieved ROC–AUC \(>\) 0.94 (and \(>\)0.85 on an external melanoma set), while TabNet preserves native mask-based explanations ~\citep{McLaughlin2023NPJ,Arik2021TabNet}.

\subsection{Critical Research Gap: Interpretable AI for Prostate Cancer Genomics}
While interpretable architectures such as TabNet show promise for genomic applications, we found no prior work applying TabNet specifically to \emph{prostate} cancer variant classification with a clinically grounded feature hierarchy—leaving a clear, disease-specific gap. Current prostate cancer AI efforts concentrate on imaging and pathology, whereas genomics pipelines remain dominated by non-interpretable or post-hoc–explained models. In a pan-cancer, tumor-only WES benchmark spanning seven TCGA cohorts (105 patients), TabNet performed competitively while preserving native mask-based explanations, supporting its suitability for tabular genomics tasks that require built-in interpretability \citep{McLaughlin2023NPJ,Arik2021TabNet}.","\subsection{Traditional genomic variant classification—limitations.}
Classical predictors (SIFT, PolyPhen\mbox{-}2, CADD) and meta\mbox{-}predictors (MetaSVM/MetaLR) remain widely used but emphasize conservation/sequence features, show ancestry-related biases, and yield task-agnostic scores that lack therapeutic context~\citep{Ng2003SIFT,Adzhubei2010PolyPhen2,Kircher2014CADD,Dong2015MetaSVM,Popejoy2016GenomicsDiversity}.
In clinical pipelines, VUS remain common—\(\sim41\
These gaps motivate disease-specific, \emph{inherently interpretable} models tailored to prostate cancer genomics.


\subsection{Machine Learning Evolution: Performance vs. Interpretability Trade-offs}  
Recent machine learning approaches demonstrate competitive performance—XGBoost implementations~\citep{Khandakji2022BRCA2XGB} achieve ROC-AUC of 0.93 in prostate cancer variant prediction, while deep CNNs achieve F1-scores exceeding 0.88 across cancer types. However, these gains create critical clinical deployment barriers: (1) \emph{post-hoc explanation dependency}—requiring SHAP or LIME methods that may not reflect actual model reasoning, compromising regulatory approval, (2) \emph{black-box opacity}—preventing molecular tumor board integration where clinicians require transparent rationales~\citep{Tamborero2022_MTBP}, and (3) \emph{generalization challenges}—pan-cancer models lack prostate-specific optimization for androgen receptor pathway analysis and DNA repair deficiency assessment critical for PARP inhibitor selection. Regulatory frameworks demand explainable AI for medical devices~\citep{FDA2025_AIEnabledDSF_Draft,FDA2021_GMLP}, creating an interpretability-performance gap that limits clinical adoption. TabNet~\citep{arik2020tabnetattentiveinterpretabletabular} addresses this trade-off through sequential attention mechanisms providing inherent explainability while maintaining competitive performance.















\subsection{Critical Research Gap: Interpretable AI for Prostate Cancer Genomics}
While interpretable architectures such as TabNet show promise for genomic applications, we found no prior work applying TabNet specifically to \emph{prostate} cancer variant classification with a clinically grounded feature hierarchy—leaving a clear, disease-specific gap. Current prostate cancer AI efforts concentrate on imaging and pathology, whereas genomics pipelines remain dominated by non-interpretable or post-hoc–explained models. In a pan-cancer, tumor-only WES benchmark spanning seven TCGA cohorts (105 patients), TabNet performed competitively while preserving native mask-based explanations, supporting its suitability for tabular genomics tasks that require built-in interpretability \citep{McLaughlin2023NPJ,Arik2021TabNet}.",
2511.04838v1,http://arxiv.org/abs/2511.04838v1,2025-11-06 21:57:21+00:00,SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression,"In molecular property prediction, the most valuable compounds (e.g., high potency) often occupy sparse regions of the target space. Standard Graph Neural Networks (GNNs) commonly optimize for the average error, underperforming on these uncommon but critical cases, with existing oversampling methods often distorting molecular topology. In this paper, we introduce SPECTRA, a Spectral Target-Aware graph augmentation framework that generates realistic molecular graphs in the spectral domain. SPECTRA (i) reconstructs multi-attribute molecular graphs from SMILES; (ii) aligns molecule pairs via (Fused) Gromov-Wasserstein couplings to obtain node correspondences; (iii) interpolates Laplacian eigenvalues, eigenvectors and node features in a stable share-basis; and (iv) reconstructs edges to synthesize physically plausible intermediates with interpolated targets. A rarity-aware budgeting scheme, derived from a kernel density estimation of labels, concentrates augmentation where data are scarce. Coupled with a spectral GNN using edge-aware Chebyshev convolutions, SPECTRA densifies underrepresented regions without degrading global accuracy. On benchmarks, SPECTRA consistently improves error in relevant target ranges while maintaining competitive overall MAE, and yields interpretable synthetic molecules whose structure reflects the underlying spectral geometry. Our results demonstrate that spectral, geometry-aware augmentation is an effective and efficient strategy for imbalanced molecular property regression.","The challenge of imbalanced distributions in graph learning tasks has received increasing attention, particularly in scientific domains where rare values are critical. Recent research by \citet{almeida2024overcoming} demonstrates that imbalanced learning in drug discovery datasets can be tackled with techniques such as oversampling and loss function manipulation when using Graph Neural Networks (GNNs). Despite these advances, most approaches operate directly in graph space rather than the spectral domain, limiting their ability to maintain global structural constraints. \citet{bo2023survey} published a comprehensive survey on spectral GNNs, highlighting their unique ability to capture global information and provide better expressiveness than spatial approaches. \citet{wang2022powerful} further analyzed the theoretical expressive power of spectral GNNs, proving that they can produce arbitrary graph signals under specific conditions. However, these methods focus on balanced and classification datasets, illustrating the novelty and significance of SPECTRA.

\subsection{Imbalanced Learning}

Class imbalance has traditionally been addressed through resampling strategies, such as under-sampling majority classes or over-sampling minority classes. SMOTE~\citep{chawla2002smote}, for instance, generates synthetic minority samples by interpolating labeled data. Alternative approaches include cost-sensitive learning~\citep{cui2019class,lin2017focal}, which increases the loss weight of minority classes, and posterior re-calibration~\citep{cao2019learning,menon2020long,tian2020posterior}, which encourages larger margins for minority predictions.  

Imbalanced regression introduces additional challenges because the labels are continuous rather than categorical~\citep{ribeiro2020imbalanced}. Several methods from classification have been adapted to this setting. For example, SMOGN~\cite{branco2017smogn} extends SMOTE to regression, while BMSE~\cite{ren2022balanced} adapts logit re-calibration for numerical targets. LDS~\cite{yang2021delving} smooths the label distribution using kernel density estimation, and RankSim~\cite{gong2022ranksim} regularizes the latent space by aligning distances in label and feature space. Other approaches include SERA~\cite{ribeiro_imbalanced_2020}, which proposes a relevance-aware evaluation metric; SGIR~\cite{liu2023semi}, which leverages unlabeled graphs to enrich underrepresented label ranges; and SIRN~\cite{zong2024boosting}, which combines deviation modeling with adaptive pseudo-label selection.  While these methods improve performance in underrepresented regions, they often reduce accuracy in well-represented areas, especially under limited supervision or when relying heavily on pseudo-labeling.


\subsection{Spectral Graph Methods}

Spectral graph theory has applications spanning dimensionality reduction, clustering, and graph signal processing. Recent work in spectral methods includes Specformer~\citep{bo2023specformer}, combining spectral GNNs with transformer architectures to create learnable set-to-set spectral filters, or the work by \citep{li2025largescale} to enhance the scalability of spectral GNNs without decoupling the network architecture, addressing a key limitation in previous approaches.~\citet{yang2024spectral} present a spectral-aware augmentation method that selectively perturbs eigenpairs to preserve task-relevant frequency bands in graph contrastive learning. These advanced spectral methods demonstrate improved performance on various graph learning tasks, but do not specifically target the regression setting or leverage the spectral domain for learning in imbalanced scenarios. 
%\citet{kaya2024spectral} applied spectral graph convolutional neural networks to Alzheimer's disease diagnosis, showing how these representations can capture complex relationships in biomedical data, but did not address the challenge of imbalanced target distributions. 


%\subsection{Manifold Learning for Structured Data}

%Manifold learning principles underpin many approaches to generating synthetic structured data. %Imbalanced learning remains one of the most significant challenges in data science, as highlighted in a 2024 comprehensive survey by \citet{ren2024survey}, which notes that traditional learning algorithms tend to favor more prevalent classes, potentially overlooking less frequent ones that often carry valuable knowledge.
%Recently, \citet{zhong2024knowledge} described how models can be enhanced by incorporating structured knowledge representations and latent manifold embeddings, in the context of knowledge-augmented graph machine learning for drug discovery. Similarly, \citet{baumgartner2023manifold} demonstrated that incorporating manifold information improves synthetic oversampling techniques for high-dimensional spectral data where standard approaches often fail. Our SMH approach differs from these works by explicitly modeling the regression target-to-spectrum mapping and performing manifold learning in the spectral domain, making it particularly suited for scientific applications with imbalanced regression targets.

\subsection{Graph Sampling and Synthesis in Scientific Domains}

Due to domain-specific constraints and validity requirements, scientific applications pose unique challenges for graph-based methods. \citet{yao2024knowledge} provided a comprehensive bibliometric analysis of GNN applications in drug discovery, showing significant growth in this area and highlighting the need for methods to handle the inherent data imbalances in these domains. Similarly, \citet{fan2024reducing} addressed the challenge of overconfident errors in molecular property classification, demonstrating the importance of uncertainty quantification in imbalanced datasets. These approaches focus primarily on classification rather than regression tasks.On regression tasks, a review on GNNs for predicting synergistic drug combinations~\citep{zhang2023review} noted that graph-based models often suffer from imbalanced data distributions, affecting their performance. They emphasized the need for methods to handle such imbalances to improve predictive accuracy effectively.



\subsection{Molecular Generation}

Molecular generation has become a central task in drug discovery, aiming to explore chemical space efficiently while ensuring chemical validity and optimizing for desired properties. Early approaches combined variational autoencoders (VAEs), recurrent neural networks (RNNs), and adversarial models to generate novel chemical structures from latent spaces, as in LatentGAN~\citep{prykhodko2019novo}, which integrated autoencoding with generative adversarial training for de novo molecular design. More recent methods leverage reinforcement learning to incorporate chemical constraints and multi-objective optimization. For example, DeepGraphMolGen~\citep{khemchandani2020deepgraphmolgen} employs Graph Convolutional Policy Networks to generate molecules while simultaneously optimizing for drug-likeness and synthetic accessibility, whereas MORLD~\citep{jeon2020autonomous} integrates reinforcement learning with docking simulations to propose inhibitors directly guided by protein structures. Conditional generative frameworks, such as MGCVAE~\citep{lee2022mgcvae}, enable property-conditioned molecular graph generation, allowing for inverse design tasks like optimizing logP or molar refractivity. Beyond purely graph-based approaches, protein-informed generation methods such as DeepTarget~\citep{chen2023deep} directly construct candidate molecules from amino acid sequences of target proteins, bridging structural biology with generative chemistry. Despite these advances, most existing models focus on validity, novelty, and property optimization, without explicitly addressing the imbalance of molecular property distributions. 

\subsection{SPECTRA Novelty}

Our SPECTRA method introduces a spectral-domain augmentation strategy that explicitly targets underrepresented regions of the label space while preserving global graph structure and chemical validity. Unlike many existing approaches that rely on pseudo-labeling or sacrifice accuracy in well-represented regions, SPECTRA generates new, chemically coherent samples where data are sparse, mitigating imbalance without degrading overall performance. By combining spectral alignment with rare-target–aware sampling and validity-preserving reconstruction, it enables interpretable molecule generation and improves regression accuracy in rare but scientifically critical regimes.","The challenge of imbalanced distributions in graph learning tasks has received increasing attention, particularly in scientific domains where rare values are critical. Recent research by \citet{almeida2024overcoming} demonstrates that imbalanced learning in drug discovery datasets can be tackled with techniques such as oversampling and loss function manipulation when using Graph Neural Networks (GNNs). Despite these advances, most approaches operate directly in graph space rather than the spectral domain, limiting their ability to maintain global structural constraints. \citet{bo2023survey} published a comprehensive survey on spectral GNNs, highlighting their unique ability to capture global information and provide better expressiveness than spatial approaches. \citet{wang2022powerful} further analyzed the theoretical expressive power of spectral GNNs, proving that they can produce arbitrary graph signals under specific conditions. However, these methods focus on balanced and classification datasets, illustrating the novelty and significance of SPECTRA.

\subsection{Imbalanced Learning}

Class imbalance has traditionally been addressed through resampling strategies, such as under-sampling majority classes or over-sampling minority classes. SMOTE~\citep{chawla2002smote}, for instance, generates synthetic minority samples by interpolating labeled data. Alternative approaches include cost-sensitive learning~\citep{cui2019class,lin2017focal}, which increases the loss weight of minority classes, and posterior re-calibration~\citep{cao2019learning,menon2020long,tian2020posterior}, which encourages larger margins for minority predictions.  

Imbalanced regression introduces additional challenges because the labels are continuous rather than categorical~\citep{ribeiro2020imbalanced}. Several methods from classification have been adapted to this setting. For example, SMOGN~\cite{branco2017smogn} extends SMOTE to regression, while BMSE~\cite{ren2022balanced} adapts logit re-calibration for numerical targets. LDS~\cite{yang2021delving} smooths the label distribution using kernel density estimation, and RankSim~\cite{gong2022ranksim} regularizes the latent space by aligning distances in label and feature space. Other approaches include SERA~\cite{ribeiro_imbalanced_2020}, which proposes a relevance-aware evaluation metric; SGIR~\cite{liu2023semi}, which leverages unlabeled graphs to enrich underrepresented label ranges; and SIRN~\cite{zong2024boosting}, which combines deviation modeling with adaptive pseudo-label selection.  While these methods improve performance in underrepresented regions, they often reduce accuracy in well-represented areas, especially under limited supervision or when relying heavily on pseudo-labeling.


\subsection{Spectral Graph Methods}

Spectral graph theory has applications spanning dimensionality reduction, clustering, and graph signal processing. Recent work in spectral methods includes Specformer~\citep{bo2023specformer}, combining spectral GNNs with transformer architectures to create learnable set-to-set spectral filters, or the work by \citep{li2025largescale} to enhance the scalability of spectral GNNs without decoupling the network architecture, addressing a key limitation in previous approaches.~\citet{yang2024spectral} present a spectral-aware augmentation method that selectively perturbs eigenpairs to preserve task-relevant frequency bands in graph contrastive learning. These advanced spectral methods demonstrate improved performance on various graph learning tasks, but do not specifically target the regression setting or leverage the spectral domain for learning in imbalanced scenarios. 








\subsection{Graph Sampling and Synthesis in Scientific Domains}

Due to domain-specific constraints and validity requirements, scientific applications pose unique challenges for graph-based methods. \citet{yao2024knowledge} provided a comprehensive bibliometric analysis of GNN applications in drug discovery, showing significant growth in this area and highlighting the need for methods to handle the inherent data imbalances in these domains. Similarly, \citet{fan2024reducing} addressed the challenge of overconfident errors in molecular property classification, demonstrating the importance of uncertainty quantification in imbalanced datasets. These approaches focus primarily on classification rather than regression tasks.On regression tasks, a review on GNNs for predicting synergistic drug combinations~\citep{zhang2023review} noted that graph-based models often suffer from imbalanced data distributions, affecting their performance. They emphasized the need for methods to handle such imbalances to improve predictive accuracy effectively.



\subsection{Molecular Generation}

Molecular generation has become a central task in drug discovery, aiming to explore chemical space efficiently while ensuring chemical validity and optimizing for desired properties. Early approaches combined variational autoencoders (VAEs), recurrent neural networks (RNNs), and adversarial models to generate novel chemical structures from latent spaces, as in LatentGAN~\citep{prykhodko2019novo}, which integrated autoencoding with generative adversarial training for de novo molecular design. More recent methods leverage reinforcement learning to incorporate chemical constraints and multi-objective optimization. For example, DeepGraphMolGen~\citep{khemchandani2020deepgraphmolgen} employs Graph Convolutional Policy Networks to generate molecules while simultaneously optimizing for drug-likeness and synthetic accessibility, whereas MORLD~\citep{jeon2020autonomous} integrates reinforcement learning with docking simulations to propose inhibitors directly guided by protein structures. Conditional generative frameworks, such as MGCVAE~\citep{lee2022mgcvae}, enable property-conditioned molecular graph generation, allowing for inverse design tasks like optimizing logP or molar refractivity. Beyond purely graph-based approaches, protein-informed generation methods such as DeepTarget~\citep{chen2023deep} directly construct candidate molecules from amino acid sequences of target proteins, bridging structural biology with generative chemistry. Despite these advances, most existing models focus on validity, novelty, and property optimization, without explicitly addressing the imbalance of molecular property distributions. 

\subsection{SPECTRA Novelty}

Our SPECTRA method introduces a spectral-domain augmentation strategy that explicitly targets underrepresented regions of the label space while preserving global graph structure and chemical validity. Unlike many existing approaches that rely on pseudo-labeling or sacrifice accuracy in well-represented regions, SPECTRA generates new, chemically coherent samples where data are sparse, mitigating imbalance without degrading overall performance. By combining spectral alignment with rare-target–aware sampling and validity-preserving reconstruction, it enables interpretable molecule generation and improves regression accuracy in rare but scientifically critical regimes.",
2511.10165v1,http://arxiv.org/abs/2511.10165v1,2025-11-13 10:25:50+00:00,EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,"Accurate exploration of protein conformational ensembles is essential for uncovering function but remains hard because molecular-dynamics (MD) simulations suffer from high computational costs and energy-barrier trapping. This paper presents Energy Preference Optimization (EPO), an online refinement algorithm that turns a pretrained protein ensemble generator into an energy-aware sampler without extra MD trajectories. Specifically, EPO leverages stochastic differential equation sampling to explore the conformational landscape and incorporates a novel energy-ranking mechanism based on list-wise preference optimization. Crucially, EPO introduces a practical upper bound to efficiently approximate the intractable probability of long sampling trajectories in continuous-time generative models, making it easily adaptable to existing pretrained generators. On Tetrapeptides, ATLAS, and Fast-Folding benchmarks, EPO successfully generates diverse and physically realistic ensembles, establishing a new state-of-the-art in nine evaluation metrics. These results demonstrate that energy-only preference signals can efficiently steer generative models toward thermodynamically consistent conformational ensembles, providing an alternative to long MD simulations and widening the applicability of learned potentials in structural biology and drug discovery.","\subsection{Deep Learning Generative Models for Protein Ensemble Generation}
\paragraph{Heuristic Extension of Single-state Predictors.}
Initial computational approaches for generating protein conformational ensembles often relied upon powerful single-state structure prediction methods, such as AlphaFold~\citep{jumper2021highly} and RoseTTAFold~\citep{baek2021accurate}. 
These approaches generated ensembles heuristically, typically by varying input conditions—such as sampling diverse Multiple Sequence Alignments (MSAs)~\citep{af3,schafer2025sequence}—or by clustering outputs from repeated predictions under perturbed conditions~\citep{afcluster,kalakoti2025afsample2}.
Although these heuristic strategies revealed some conformational variability, they lacked explicit probabilistic modeling of the underlying conformational distributions and were inherently limited in capturing rare or transient states essential for protein function~\citep{karplus2005molecular,henzler2007dynamic}.

\paragraph{Distribution Learning from MD Simulation.}
To explicitly model protein conformational distributions, recent research has increasingly adopted deep generative frameworks, primarily leveraging SE(3)-equivariant diffusion models and normalizing flow techniques. 
To align generated ensembles with physically realistic Boltzmann distributions, several approaches have introduced explicit energy guidance, commonly achieved by fine-tuning pretrained generative models with molecular dynamics (MD) simulation data or by incorporating experimentally derived energy potentials during the sampling process~\citep{mdgen, alphaflow, confdiff, bioemu, klein2023timewarp, schreiner2023implicit}.
Recent works like EBA~\cite{lu2025eba} and P2DFlow~\cite{jin2025p2dflow} explicitly incorporate physical energy or Boltzmann-derived factors into optimization, but face the challenges of numerical instability due to the large variance in energy values across conformational landscape.
While these MD-based refinement methods have substantially improved the physical plausibility of generated conformations, their dependence on simulated trajectories inherently introduces limitations, such as finite timescale coverage, inaccuracies in force fields, and sampling biases. 
Consequently, these constraints restrict models from effectively exploring novel, biologically relevant conformational states absent from their training sets.
An alternative research direction integrates physically informed experimental data—such as Nuclear Magnetic Resonance (NMR) or Cryo-Electron Microscopy (Cryo-EM)—to enhance the diversity and realism of conformational samples produced by generative models~\citep{laurents2022alphafold, maddipatla2025inverseproblemsexperimentguidedalphafold, tengcryogen, renphysical, gyawali2025multimodal}. 


\subsection{Reinforcement Learning from Human Feedback}
Aligning large language models (LLMs) with user intent was first operationalized through Reinforcement Learning from Human Feedback (RLHF) \citep{christiano2017deep, stiennon2020learning}.  
In the canonical pipeline, pairwise preference data are used to train a reward model, and the policy is subsequently improved with on-policy algorithms such as PPO \citep{ppo}.  
Despite impressive empirical success, RLHF inherits the sample inefficiency and brittle hyperparameter tuning of reinforcement learning.  
% Several attempts have been made to mitigate these issues without abandoning the RL formalism entirely.  
% Offline variants such as Implicit Q-Learning (IQL) and Advantage-Weighted Regression (AWR) \citep{peng2019awr, kostrikov2021offline, qin2024align} reweight logged trajectories by estimated advantages, thereby avoiding expensive online roll-outs.  
% Constitutional AI \citep{bai2024constitutional} replaces external preference labels with self-critique signals and thus removes part of the human-in-the-loop cost, but still relies on an RLHF-style outer loop.  
% Collectively, these approaches highlight both the versatility and the lingering complexity of RL-centric alignment.

To bypass reward modelling and online exploration altogether, \citeauthor{dpo} proposed Direct Preference Optimization (DPO).  
DPO reframes preference alignment as a purely offline, KL-regularized classification problem whose gradients depend only on logged comparisons, enabling stable training with standard supervised infrastructure.  
Building on the same “likelihood-based” philosophy, several variants have been introduced.  
Most recently, \citeauthor{ethayarajh2024kto} draw on Kahneman--Tversky prospect theory to design KTO, a human-aware loss that directly maximizes subjective utility rather than log-likelihood.
In parallel, \citeauthor{wang2023beyond} present $f$-DPO, which replaces the reverse-KL term in DPO with a family of tractable $f$-divergence constraints (e.g., Jensen–Shannon, forward-KL, $\alpha$-divergences), yielding a supervised objective that balances alignment and diversity and outperforms PPO in divergence efficiency.

Although the original formulation targets discrete text generation, the underlying principle—matching the Boltzmann rational distribution implied by preferences—extends naturally to continuous data.  
\citeauthor{wallace2024diffusion,yang2024using} instantiate this idea in computer vision with Diffusion-DPO, combining a score-based diffusion backbone with a DPO-style loss.
While pairwise comparisons are convenient, many real-world applications supply graded or ranked feedback.
ListDPO~\citep{lipo} augments the DPO objective with list-wise information through a $\lambda$-rank weighting scheme.  
The resulting loss directly optimizes the target ranking metric Normalized Discounted Cumulative Gain (NDCG), yielding consistent gains on retrieval-augmented generation and recommendation tasks.
\citeauthor{gu2024aligning, wang2024training} utilize external rewards for diffusion models but focus on specific properties.
Other methods employ RL~\cite{black2023training, fan2023dpok} or rely on direct backpropagation via a differentiable reward signal~\cite{clark2023directly}.
To the best of our knowledge, we are the first to extend this post-training paradigm to the generation of protein ensembles.


%","\subsection{Deep Learning Generative Models for Protein Ensemble Generation}
\paragraph{Heuristic Extension of Single-state Predictors.}
Initial computational approaches for generating protein conformational ensembles often relied upon powerful single-state structure prediction methods, such as AlphaFold~\citep{jumper2021highly} and RoseTTAFold~\citep{baek2021accurate}. 
These approaches generated ensembles heuristically, typically by varying input conditions—such as sampling diverse Multiple Sequence Alignments (MSAs)~\citep{af3,schafer2025sequence}—or by clustering outputs from repeated predictions under perturbed conditions~\citep{afcluster,kalakoti2025afsample2}.
Although these heuristic strategies revealed some conformational variability, they lacked explicit probabilistic modeling of the underlying conformational distributions and were inherently limited in capturing rare or transient states essential for protein function~\citep{karplus2005molecular,henzler2007dynamic}.

\paragraph{Distribution Learning from MD Simulation.}
To explicitly model protein conformational distributions, recent research has increasingly adopted deep generative frameworks, primarily leveraging SE(3)-equivariant diffusion models and normalizing flow techniques. 
To align generated ensembles with physically realistic Boltzmann distributions, several approaches have introduced explicit energy guidance, commonly achieved by fine-tuning pretrained generative models with molecular dynamics (MD) simulation data or by incorporating experimentally derived energy potentials during the sampling process~\citep{mdgen, alphaflow, confdiff, bioemu, klein2023timewarp, schreiner2023implicit}.
Recent works like EBA~\cite{lu2025eba} and P2DFlow~\cite{jin2025p2dflow} explicitly incorporate physical energy or Boltzmann-derived factors into optimization, but face the challenges of numerical instability due to the large variance in energy values across conformational landscape.
While these MD-based refinement methods have substantially improved the physical plausibility of generated conformations, their dependence on simulated trajectories inherently introduces limitations, such as finite timescale coverage, inaccuracies in force fields, and sampling biases. 
Consequently, these constraints restrict models from effectively exploring novel, biologically relevant conformational states absent from their training sets.
An alternative research direction integrates physically informed experimental data—such as Nuclear Magnetic Resonance (NMR) or Cryo-Electron Microscopy (Cryo-EM)—to enhance the diversity and realism of conformational samples produced by generative models~\citep{laurents2022alphafold, maddipatla2025inverseproblemsexperimentguidedalphafold, tengcryogen, renphysical, gyawali2025multimodal}. 


\subsection{Reinforcement Learning from Human Feedback}
Aligning large language models (LLMs) with user intent was first operationalized through Reinforcement Learning from Human Feedback (RLHF) \citep{christiano2017deep, stiennon2020learning}.  
In the canonical pipeline, pairwise preference data are used to train a reward model, and the policy is subsequently improved with on-policy algorithms such as PPO \citep{ppo}.  
Despite impressive empirical success, RLHF inherits the sample inefficiency and brittle hyperparameter tuning of reinforcement learning.  





To bypass reward modelling and online exploration altogether, \citeauthor{dpo} proposed Direct Preference Optimization (DPO).  
DPO reframes preference alignment as a purely offline, KL-regularized classification problem whose gradients depend only on logged comparisons, enabling stable training with standard supervised infrastructure.  
Building on the same “likelihood-based” philosophy, several variants have been introduced.  
Most recently, \citeauthor{ethayarajh2024kto} draw on Kahneman--Tversky prospect theory to design KTO, a human-aware loss that directly maximizes subjective utility rather than log-likelihood.
In parallel, \citeauthor{wang2023beyond} present $f$-DPO, which replaces the reverse-KL term in DPO with a family of tractable $f$-divergence constraints (e.g., Jensen–Shannon, forward-KL, $\alpha$-divergences), yielding a supervised objective that balances alignment and diversity and outperforms PPO in divergence efficiency.

Although the original formulation targets discrete text generation, the underlying principle—matching the Boltzmann rational distribution implied by preferences—extends naturally to continuous data.  
\citeauthor{wallace2024diffusion,yang2024using} instantiate this idea in computer vision with Diffusion-DPO, combining a score-based diffusion backbone with a DPO-style loss.
While pairwise comparisons are convenient, many real-world applications supply graded or ranked feedback.
ListDPO~\citep{lipo} augments the DPO objective with list-wise information through a $\lambda$-rank weighting scheme.  
The resulting loss directly optimizes the target ranking metric Normalized Discounted Cumulative Gain (NDCG), yielding consistent gains on retrieval-augmented generation and recommendation tasks.
\citeauthor{gu2024aligning, wang2024training} utilize external rewards for diffusion models but focus on specific properties.
Other methods employ RL~\cite{black2023training, fan2023dpok} or rely on direct backpropagation via a differentiable reward signal~\cite{clark2023directly}.
To the best of our knowledge, we are the first to extend this post-training paradigm to the generation of protein ensembles.",
2510.20846v1,http://arxiv.org/abs/2510.20846v1,2025-10-21 01:58:34+00:00,This EEG Looks Like These EEGs: Interpretable Interictal Epileptiform Discharge Detection With ProtoEEG-kNN,"The presence of interictal epileptiform discharges (IEDs) in electroencephalogram (EEG) recordings is a critical biomarker of epilepsy. Even trained neurologists find detecting IEDs difficult, leading many practitioners to turn to machine learning for help. While existing machine learning algorithms can achieve strong accuracy on this task, most models are uninterpretable and cannot justify their conclusions. Absent the ability to understand model reasoning, doctors cannot leverage their expertise to identify incorrect model predictions and intervene accordingly. To improve the human-model interaction, we introduce ProtoEEG-kNN, an inherently interpretable model that follows a simple case-based reasoning process. ProtoEEG-kNN reasons by comparing an EEG to similar EEGs from the training set and visually demonstrates its reasoning both in terms of IED morphology (shape) and spatial distribution (location). We show that ProtoEEG-kNN can achieve state-of-the-art accuracy in IED detection while providing explanations that experts prefer over existing approaches.","% \begin{figure}[b]
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/combined_fig.png}
%     \caption{Our model's architecture. An input is forward propagated through the SpikeNet Backbone $f$ to produce a latent representation. The Global Comparison Layer computes the similarity between the latent input and each of samples in the training set. The final prediction is produced by $\bar{h}$ which is an average of the labels from the top-k most similar neighbors. \textbf{Channelwise Similarity Calculation based on Two Channels.} This figure illustrates how Latent and Handcrafted similarities are combined into a weight similarity score. This score is multiplied by the channel weight $w_{c}(x)$ to obtain a weighted channel similarity. 
%     }
%     \label{fig:combined}
% \end{figure}

There has been a dramatic increase in interest in IED detection using machine learning models \cite{Diniz2024AdvancingED}, resulting in a wide variety of uninterpretable predictive approaches. Generally, IED detection operates at either the channel-level \cite{Geng2021DeepLF, TjepkemaCloostermans2018DeepLF, Frbass2020AnAI} or by analyzing entire EEGs at once \cite{Kural2022AccurateIO, Jing2020DevelopmentOE, Tveit2023AutomatedIO}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.92\textwidth]{figures/combined_figure.pdf}
    %\hline
    %\includegraphics[width=0.7\textwidth]%{figures/MICCAI_ARCHCHANNEL.pdf}
    \caption{\textbf{Top:} ProtoEEG-kNN reasoning. The topographic map (``topoplot'') highlights important channels as calculated by the channel-wise weights ($w_c(x)$), which are also shown in bars to the left of the input channels. From left to right, we show the input sample, the best two matches selected by our model, and the best matches chosen by each of three ablated models. \textbf{Middle:} ProtoEEG-kNN architecture.  An input is passed through the backbone $f$ to produce a embedding. The Global Comparison Layer $\bar{g}$ computes the similarity between the embedding and each sample in the training set. The final prediction produced by $\bar{h}$ is the average label from the top-k most similar neighbors. \textbf{Bottom:} channel-wise similarity. The similarity along each channel is weighted by $w_c(\mathbf{x})$.} %This score is multiplied by the channel weight $w_{c}(x)$ to obtain a weighted channel similarity. }
    \label{fig:figure1}
\end{figure}

In computer vision, a large body of work has emerged around interpretable neural networks, based on the Prototypical Part Network (ProtoPNet) \cite{chen2019this}. ProtoPNet provides an interpretable alternative to traditional neural networks by forming predictions using a series of comparisons to learned prototypical parts. A ProtoPNet can explain its predictions by saying ``this image is of class A because it looks like this prototype from class A''. Of particular interest to this work, Ukai et al. \cite{ukai2022looks} introduce ProtoKNN, which performs kNN-style classification over the vector of prototype similarities. This is different from our kNN approach, in which we use a specialized similarity metric to compute the similarity between an input and each training sample.
% A variety of followup papers have extended the original ProtoPNet to share prototypes among classes \cite{nauta2021neural, rymarczyk2022interpretable, rymarczyk2021protopshare}, enable spatial flexibility within prototypes \cite{Donnelly2021DeformablePA, ma2024interpretable}, form specially structured latent spaces \cite{wang2021interpretable}, and to support transformer backbones \cite{ma2024interpretable, turbe2024protos}.
Several papers have applied ProtoPNet style reasoning to IED detection \cite{Tang2023ProtoEEGNetAI, Gao2023ASD, Gao2024AnIA}. In Gao et al. \cite{Gao2023ASD}, ProtoPNet is applied to IED detection, while Gao et al. \cite{Gao2024AnIA} extends this work to ``multi-scale'' prototypes of varying lengths. However, both are limited to single channel comparisons, thus failing to consider the spatial distribution of spikes, an important factor in how experts identify IEDs \cite{Nascimento2022AQA,Kural2020CriteriaFD}.
In contrast, Tang et al. \cite{Tang2023ProtoEEGNetAI}, represents prototypes as full EEGs, but convolve every channel together, which keeps their model from providing channel-level interpretability. Additionally, Lopez et al. \cite{Lopes2023UsingCS} and Ozcan et al. \cite{Ozcan2019SeizurePI} apply post-hoc methods to explain black-box IED detection models, but these explanations are not necessarily faithful to how a model actually makes decisions, and may be misleading \cite{rudin2019stop, Adebayo2018SanityCF}.","There has been a dramatic increase in interest in IED detection using machine learning models \cite{Diniz2024AdvancingED}, resulting in a wide variety of uninterpretable predictive approaches. Generally, IED detection operates at either the channel-level \cite{Geng2021DeepLF, TjepkemaCloostermans2018DeepLF, Frbass2020AnAI} or by analyzing entire EEGs at once \cite{Kural2022AccurateIO, Jing2020DevelopmentOE, Tveit2023AutomatedIO}.



In computer vision, a large body of work has emerged around interpretable neural networks, based on the Prototypical Part Network (ProtoPNet) \cite{chen2019this}. ProtoPNet provides an interpretable alternative to traditional neural networks by forming predictions using a series of comparisons to learned prototypical parts. A ProtoPNet can explain its predictions by saying ``this image is of class A because it looks like this prototype from class A''. Of particular interest to this work, Ukai et al. \cite{ukai2022looks} introduce ProtoKNN, which performs kNN-style classification over the vector of prototype similarities. This is different from our kNN approach, in which we use a specialized similarity metric to compute the similarity between an input and each training sample.

Several papers have applied ProtoPNet style reasoning to IED detection \cite{Tang2023ProtoEEGNetAI, Gao2023ASD, Gao2024AnIA}. In Gao et al. \cite{Gao2023ASD}, ProtoPNet is applied to IED detection, while Gao et al. \cite{Gao2024AnIA} extends this work to ``multi-scale'' prototypes of varying lengths. However, both are limited to single channel comparisons, thus failing to consider the spatial distribution of spikes, an important factor in how experts identify IEDs \cite{Nascimento2022AQA,Kural2020CriteriaFD}.
In contrast, Tang et al. \cite{Tang2023ProtoEEGNetAI}, represents prototypes as full EEGs, but convolve every channel together, which keeps their model from providing channel-level interpretability. Additionally, Lopez et al. \cite{Lopes2023UsingCS} and Ozcan et al. \cite{Ozcan2019SeizurePI} apply post-hoc methods to explain black-box IED detection models, but these explanations are not necessarily faithful to how a model actually makes decisions, and may be misleading \cite{rudin2019stop, Adebayo2018SanityCF}.",
2510.14455v1,http://arxiv.org/abs/2510.14455v1,2025-10-16 08:55:06+00:00,Coder as Editor: Code-driven Interpretable Molecular Optimization,"Molecular optimization is a central task in drug discovery that requires precise structural reasoning and domain knowledge. While large language models (LLMs) have shown promise in generating high-level editing intentions in natural language, they often struggle to faithfully execute these modifications-particularly when operating on non-intuitive representations like SMILES. We introduce MECo, a framework that bridges reasoning and execution by translating editing actions into executable code. MECo reformulates molecular optimization for LLMs as a cascaded framework: generating human-interpretable editing intentions from a molecule and property goal, followed by translating those intentions into executable structural edits via code generation. Our approach achieves over 98% accuracy in reproducing held-out realistic edits derived from chemical reactions and target-specific compound pairs. On downstream optimization benchmarks spanning physicochemical properties and target activities, MECo substantially improves consistency by 38-86 percentage points to 90%+ and achieves higher success rates over SMILES-based baselines while preserving structural similarity. By aligning intention with execution, MECo enables consistent, controllable and interpretable molecular design, laying the foundation for high-fidelity feedback loops and collaborative human-AI workflows in drug discovery.","Among various molecular representations, SMILES~\citep{weininger1988smiles} has been widely adopted in sequence-based models, including RNNs~\citep{gomez2018automatic,segler2018generating} and Transformers~\citep{schwaller2019molecular}. In contrast, graph-based approaches~\citep{gilmer2017neural,jin2018junction,shi2020graphaf} and 3D-aware models~\citep{schutt2017quantum,satorras2021n} aim to better capture structural validity by operating directly on molecular graphs and spatial coordinates.
Large-scale pretraining further produced molecular language models such as ChemBERTa~\citep{chithrananda2020chemberta}, MolBERT~\citep{fabian2020molecular}, Chemformer~\citep{irwin2022chemformer}, and MolXPT~\citep{liu2023molxpt}, which perform competitively with graph-based approaches. 
However, SMILES poorly align with natural language: small structural edits can cause large string differences, and multiple encodings exist for the same molecule~\citep{merz2020generative}, making them suboptimal for reasoning in LLMs.


LLMs have been investigated as general-purpose optimizers~\citep{yang2023large,meyerson2024language,liu2024large}, and these ideas have recently been extended to molecular domains. 
Prompt-based approaches such as MOLLEO~\citep{wang2024efficient} and ChatDrug~\citep{liu2024conversational} adapt LLMs to propose molecular modifications, either by embedding them in genetic algorithms or by augmenting them with retrieval databases. 
Other methods rely on representation learning, for example by exploiting pretrained LLM embeddings~\citep{rankovic2023bochemian} or by fine-tuning general-purpose models on molecular corpora to improve generation quality~\citep{bedrosian2024small,fang2023domain,kristiadi2024sober}. 
DrugAssist~\citep{ye2025drugassist} further contributed a large-scale MolOpt-Instructions dataset to support instruction-tuned optimization models. LICO~\citep{nguyen2024lico} proposed a semi-synthetic training framework that extends general-purpose LLMs into surrogate models for black-box molecular optimization.
MolReasoner~\citep{zhao2025molreasoner} introduces a two-stage framework that integrates synthetic Chain-of-Thought supervision with reinforcement learning, shifting molecular LLMs from memorization toward interpretable reasoning.
Despite these advances, most existing systems still operate directly in SMILES or graph spaces, which limits their alignment with natural language reasoning and hinders the interpretability of the generated modifications. % Inspired by progress in code LLMs such as CodeT5+~\citep{wang2023codet5+}, StarCoder~\citep{li2023starcoder}, and InCoder~\citep{fried2022incoder}, our work instead treats code as an intermediate representation. 


% By translating natural language rationales into executable cheminformatics programs (e.g., RDKit scripts), we bridge reasoning and execution, yielding interpretable, reproducible, and chemically valid molecular optimization.



% Beyond molecular modeling, LLMs have also been investigated as general-purpose optimizers. 
% A common strategy is to formulate the optimization process as a dialogue between the LLM and past evaluations, where the model proposes new candidates based on observed outcomes~\citep{yang2023large,zhang2023using,ma2023eureka}. 
% Another line of work incorporates LLMs into population-based methods such as evolutionary algorithms, letting the model generate crossover or mutation operations at each iteration~\citep{meyerson2024language,lehman2023evolution,bradley2024openelm}. 
% In parallel, researchers have explored using LLMs to improve specific components of Bayesian optimization, including warm-starting and surrogate modeling~\citep{liu2024large}, while OptFormer~\citep{chen2022towards} introduced a transformer tailored for in-context optimization.


% \textbf{Instruction-aligned molecular editing and generation.}
% Early de novo design treated molecules as strings or graphs and optimized them with generative models.
% String benchmarks such as GuacaMol and MOSES standardized validity/novelty/diversity metrics for SMILES-based models and exposed pitfalls of trivial objectives \citep{brown2019guacamol,polykovskiy2020molecular}.
% On graphs, Junction Tree VAE generated molecules by first producing a scaffold tree and then assembling substructures, while graph-to-graph translation directly learned paired edits ($A{\to}B$) for property improvement \citep{jin2018junction,jin2018learning}.
% Reinforcement-learning approaches (GCPN, MolDQN) further cast molecular editing as sequential graph actions under property rewards \citep{you2018graph,zhou2019optimization}.
% In medicinal chemistry practice, matched molecular pair analysis (MMPA) and platforms like \texttt{mmpdb} mine fragment-substitution rules at scale \citep{dalke2018mmpdb}.
% These strands motivate MECo’s focus on \emph{controllable, local} edits (e.g., fragment/core replacement), while highlighting persistent gaps in aligning exact edit intent and ensuring chemical consistency (aromaticity, valence, symmetry) when generating molecules as free text.

% \textbf{Tool- and code-augmented LLMs for reliable chemistry.}
% Grounding LLMs in executable toolchains markedly improves reliability for chemical reasoning and editing.
% ChemCrow shows that coupling an LLM with RDKit, databases, and synthesis planners yields substantial gains over prompt-only use across design and synthesis tasks \citep{bran2024chemcrow}.
% Cheminformatics backends such as RDKit provide sanitization, substructure queries, atom mapping, and aromatization that enforce valence/locality constraints during edits.
% MECo follows this paradigm: instead of emitting raw SMILES, a \emph{coder} LLM produces executable RDKit code.
% Edits are then executed and verified by the toolkit, making outcomes deterministic, reproducible, and faithful to the specified intent.

% \textbf{Data and evaluation for consistent editing.}
% Reaction corpora from US patents (USPTO) supply atom-mapped transformations that underpin reaction-grounded editing and template mining \citep{lowe2017chemical}.
% For end-to-end optimization under realistic budgets, the PMO benchmark emphasized sample efficiency and showed many algorithms falter when oracle calls are limited \citep{gao2022sample}.
% Robust encodings such as SELFIES reduce syntactic invalidity in string-based generation \citep{krenn2019selfies}, while multimodal alignment (e.g., MoleculeSTM) links textual edit instructions with structures for zero-shot retrieval/editing \citep{liu2023multi}.
% MECo situates within this landscape by (i) deriving realistic edit pairs from reaction data, (ii) translating edit instructions into verifiable code, and (iii) evaluating with budget-aware objectives and edit-consistency checks.","Among various molecular representations, SMILES~\citep{weininger1988smiles} has been widely adopted in sequence-based models, including RNNs~\citep{gomez2018automatic,segler2018generating} and Transformers~\citep{schwaller2019molecular}. In contrast, graph-based approaches~\citep{gilmer2017neural,jin2018junction,shi2020graphaf} and 3D-aware models~\citep{schutt2017quantum,satorras2021n} aim to better capture structural validity by operating directly on molecular graphs and spatial coordinates.
Large-scale pretraining further produced molecular language models such as ChemBERTa~\citep{chithrananda2020chemberta}, MolBERT~\citep{fabian2020molecular}, Chemformer~\citep{irwin2022chemformer}, and MolXPT~\citep{liu2023molxpt}, which perform competitively with graph-based approaches. 
However, SMILES poorly align with natural language: small structural edits can cause large string differences, and multiple encodings exist for the same molecule~\citep{merz2020generative}, making them suboptimal for reasoning in LLMs.


LLMs have been investigated as general-purpose optimizers~\citep{yang2023large,meyerson2024language,liu2024large}, and these ideas have recently been extended to molecular domains. 
Prompt-based approaches such as MOLLEO~\citep{wang2024efficient} and ChatDrug~\citep{liu2024conversational} adapt LLMs to propose molecular modifications, either by embedding them in genetic algorithms or by augmenting them with retrieval databases. 
Other methods rely on representation learning, for example by exploiting pretrained LLM embeddings~\citep{rankovic2023bochemian} or by fine-tuning general-purpose models on molecular corpora to improve generation quality~\citep{bedrosian2024small,fang2023domain,kristiadi2024sober}. 
DrugAssist~\citep{ye2025drugassist} further contributed a large-scale MolOpt-Instructions dataset to support instruction-tuned optimization models. LICO~\citep{nguyen2024lico} proposed a semi-synthetic training framework that extends general-purpose LLMs into surrogate models for black-box molecular optimization.
MolReasoner~\citep{zhao2025molreasoner} introduces a two-stage framework that integrates synthetic Chain-of-Thought supervision with reinforcement learning, shifting molecular LLMs from memorization toward interpretable reasoning.
Despite these advances, most existing systems still operate directly in SMILES or graph spaces, which limits their alignment with natural language reasoning and hinders the interpretability of the generated modifications.",
2510.00351v1,http://arxiv.org/abs/2510.00351v1,2025-09-30 23:29:39+00:00,Flow Autoencoders are Effective Protein Tokenizers,"Protein structure tokenizers enable the creation of multimodal models of protein structure, sequence, and function. Current approaches to protein structure tokenization rely on bespoke components that are invariant to spatial symmetries, but that are challenging to optimize and scale. We present Kanzi, a flow-based tokenizer for tokenization and generation of protein structures. Kanzi consists of a diffusion autoencoder trained with a flow matching loss. We show that this approach simplifies several aspects of protein structure tokenizers: frame-based representations can be replaced with global coordinates, complex losses are replaced with a single flow matching loss, and SE(3)-invariant attention operations can be replaced with standard attention. We find that these changes stabilize the training of parameter-efficient models that outperform existing tokenizers on reconstruction metrics at a fraction of the model size and training cost. An autoregressive model trained with Kanzi outperforms similar generative models that operate over tokens, although it does not yet match the performance of state-of-the-art continuous diffusion models. Code is available here: https://github.com/rdilip/kanzi/.","\textbf{Tokenization.} State-of-the-art generative image models frequently first train an image \emph{tokenizer}, which downsamples continuous image data to either a discrete or a continuous latent~\citep{esser2021taming, van2017neural}. Recent works in machine learning for biology have followed suit by training discrete tokenizers for protein backbone structures, which enable language models to be trained on sequences of tokens derived from the resulting codebooks~\citep{van2022foldseek, steinegger2017mmseqs2, ist-gaujac2024learning, lin2023protokens}. While tokenized protein models have underperformed diffusion models on the task of unconditional structure generation, they enable the construction of multimodal generative models of proteins. ESM3 notably trained a multimodal discrete diffusion model over sequence, structure, secondary structure, and natural language, which was capable of generating novel proteins with specified functions~\citep{esm3-hayes2025simulating, dplm2-ang2024dplm}. Following AlphaFold2, protein tokenizers generally rely on $SE(3)$-invariant architectural components (e.g., invariant point attention) and $SE(3)$-invariant losses (e.g., frame-aligned point error). In contrast with prior work, we use a non-invariant diffusion loss to supervise the tokenizer. 

\textbf{Diffusion and flow matching.} State-of-the-art protein structure generation models rely on diffusion, either discrete or continuous. FrameDiff, RFDiffusion, and Chroma~\citep{framediff-yim2023se, rf-watson2023novo, chroma-ingraham2023illuminating} generate protein backbones using a denoising process over the joint translation-rotation group $SO(3)\ltimes\mathbb{R}^3$, while FrameFlow, FoldFlow, and FoldFlow-2 similarly perform flow matching over the same manifold~\citep{frameflow-yim2023fast, foldflow2-bose2023se, huguet2024sequence}. Genie2 and Proteina are more recent attempts to train diffusion models at scale on the AlphaFold Structure Database (AFDB); these both operate over $C\alpha$ coordinates~\citep{proteina-geffner2025proteina, lin2024out}. The latter does not explicitly encode invariances, a strategy we broadly adopt here. Despite the strong performance of diffusion models, autoregressive models have unique features that are valuable to the structural biology and machine learning communities. Most notably, they can be applied to more use cases where the protein size is not known \emph{a priori}, an important feature for tasks such as motif scaffolding or \emph{in situ} structure prediction in electron tomography images~\citep{yadav2020situ, bunne2024build}.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/fig_architecture_v6.pdf}
    \caption{Architectural overview of \model. (a) \model~ takes a clean structure as input, which is encoded and passed through a quantization bottleneck. The decoder is provided with the quantized latents as in-context conditioning, along with a noised version of the protein structure. The training is supervised by a single diffusion loss that maximizes $p(\x|\hat{\mathbf{c}})$. No auxiliary losses are used. (b) Our decoder follows the standard diffusion transformer (DiT) presentation, with several notable deviations. We share adaLN conditioning across all blocks, and each DiT block is a transformer with pair-biased attention and optional self-conditioning. (c) Our encoder combines raw coordinate information with sequence positional information. Tokens are mixed using a small stack of transformer layers with sliding window attention. Ablations on other encoder variants are described in Section~\ref{sec:ablations} and Appendix~\ref{sec:app:ablations}.}
    \label{fig:architecture}
\end{figure}


To train \model, we use a flow matching objective ~\citep{esser2024scaling, lipman2022flow, lin2024out}. Flow matching interpolates between a source distribution $p_0$ (often a Gaussian) and a target distribution $p_\text{data}$ by integrating along the ODE $d\x_t = \mathbf{v}_\theta(\x_t, t)\,dt$ using a learned vector field $\mathbf{v}_\theta(\x_t, t): \mathbb{R}^d\times [0,1] \rightarrow \mathbb{R}^d$. As the vector field generating the true probability distribution is in general unknown, one uses conditional flow matching, which constructs a conditional probability path between prior samples $\x_0\sim p_0$ and data samples $\x_1\sim p_\text{data}$. Explicitly, a general probability path can be written $\x_t = \alpha_t \x_1 + \sigma_t \epsilon$, with $\epsilon\sim\mathcal{N}(0, 1)$. This induces a true conditional vector field $\mathbf{u}(\x_t|\x_0,\x_1) = \dot\alpha_t \x_1+\dot\sigma_t\x_0$, where the dot denotes the time derivative. This is a target we can regress against; for the standard case of the linear interpolation path, we have $\x_t = (1-t)\x_0 + t \x_1$ and $\mathbf{u}(\x_t|\x_0,\x_1) = \x_1 - \x_0$. For completeness, we present a more thorough derivation of the flow matching formulation in Appendix~\ref{sec:app:flow_matching}.

\textbf{Diffusion autoencoders.} The idea of using a diffusion model as the decoder in tokenizer reconstruction is a recent insight in computer vision but has yet to be explored for protein structure generation~\citep{preechakul2022diffusion}. Two recent works, FlowMo and DiTo~\citep{flowmo-sargent2025flow, chen2025diffusion}, both study this approach and independently demonstrate SOTA performance on ImageNet-1k reconstruction. In these works, the use of a diffusion model eliminates the need for combinations of perceptual and adversarial losses during training, which were critical insights introduced by VQGAN~\citep{esser2021taming}. In our case, given the success of recent models like AlphaFold3, Boltz~\citep{wohlwend2025boltz}, and Proteina in eschewing symmetric architectures for scalability, we hypothesize that diffusion autoencoders could provide a similar advantage for tokenization.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/samples_w_rmsds.pdf}
    \caption{Designable samples generated from an autoregressive model trained on \model~tokens. scRMSDs shown underneath each visualization.}
    \label{fig:placeholder}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/figure2_v4.pdf}
    \caption{Left: Scaling of protein structure tokenizer performance with dataset size and parameter count. We plot the reconstruction accuracy on the CAMEO test set versus the training dataset size. Circle area is the model parameter count. \model~is competitive with the ESM3 tokenizer, despite a 20-fold smaller parameter count and 400-fold smaller training dataset. Right: \model~simplifies the training pipeline, replacing collections of complex, invariant losses with a single, non-invariant flow matching loss.}
    \label{fig:comparison}
\end{figure}
% \begin{table}[t]
%     \footnotesize
%     \centering
%     \captionsetup{skip=4pt} % reduce space between table and caption
%     \setlength{\tabcolsep}{4pt} % tighter column spacing
%     \renewcommand{\arraystretch}{1.05} % tighter row spacing
%     \rowcolors{2}{gray!5}{white}
%   % \begin{tabular}{l l l c c}
%   %       \Xhline{0.6pt}
%   %       \textbf{Model} & \textbf{Loss} & \textbf{Architecture} & \textbf{Sym.} &
%   %       \\
%   %       \midrule
%   %       ESM3       & 
%   %       $\mathcal{L}_{\mathrm{dRMSD}} + \mathcal{L}_{\mathrm{dir}} + \mathcal{L}^{\mathrm{bin}}_{\mathrm{dRMSD}} + \mathcal{L}^{\mathrm{bin}}_{\mathrm{dir}}$ & Geometric attention & SE(3) \\
%   %       DPLM2      & $\mathcal{L}_{\mathrm{FAPE}} + \mathcal{L}_{\mathrm{violation}} + \mathcal{L}_{\mathrm{distogram}}$ & GVP-Transformer + SM & SE(3) \\
%   %       IST       & $\mathcal{L}_{\mathrm{FAPE}}$ & MPNN + SM & E(3) \\
%   %       Protokens  & $\mathcal{L}_{\mathrm{FAPE}} + \mathcal{L}_{\mathrm{violation}}$ & Hyperformer + SM & SE(3) \\
%   %       \rowcolor{blue!10}
%   %       \textbf{Ours} & $\mathcal{L}_{\mathrm{flow}}$ & Standard attention & None \\
%   %       \Xhline{0.6pt}
%   %   \end{tabular}
%   {\scriptsize
%   \begin{tabular}{l l l c c}
%         \Xhline{0.6pt}
%         \textbf{Model} & \textbf{Loss} & \textbf{Architecture} & \textbf{Sym.} &
%         \\
%         \midrule
%         ESM3       & 
%         dRMSD, direction loss, binned dRMSD, binned direction loss & Geometric attention & SE(3) \\
%         DPLM2      & FAPE, violation, distogram & GVP-Transformer + SM & SE(3) \\
%         IST       & FAPE & MPNN + SM & E(3) \\
%         Protokens  & FAPE, violation & Hyperformer + SM & SE(3) \\
%         \rowcolor{blue!10}
%         \textbf{Ours} & Flow & Standard attention & None \\
%         \Xhline{0.6pt}
%     \end{tabular}
% }
%     \caption{Comparison of protein modeling approaches with loss functions, architectures, and symmetry assumptions. SM = AlphaFold2 Structure Module. DPLM-2 uses a GVP-Transformer pretrained on millions of structures. The subscript ``bin"" refers to binned cross-entropy losses over regression targets, which are often needed to stabilize early training.}
%     \label{tab:method-comparison}
% \end{table}","\textbf{Tokenization.} State-of-the-art generative image models frequently first train an image \emph{tokenizer}, which downsamples continuous image data to either a discrete or a continuous latent~\citep{esser2021taming, van2017neural}. Recent works in machine learning for biology have followed suit by training discrete tokenizers for protein backbone structures, which enable language models to be trained on sequences of tokens derived from the resulting codebooks~\citep{van2022foldseek, steinegger2017mmseqs2, ist-gaujac2024learning, lin2023protokens}. While tokenized protein models have underperformed diffusion models on the task of unconditional structure generation, they enable the construction of multimodal generative models of proteins. ESM3 notably trained a multimodal discrete diffusion model over sequence, structure, secondary structure, and natural language, which was capable of generating novel proteins with specified functions~\citep{esm3-hayes2025simulating, dplm2-ang2024dplm}. Following AlphaFold2, protein tokenizers generally rely on $SE(3)$-invariant architectural components (e.g., invariant point attention) and $SE(3)$-invariant losses (e.g., frame-aligned point error). In contrast with prior work, we use a non-invariant diffusion loss to supervise the tokenizer. 

\textbf{Diffusion and flow matching.} State-of-the-art protein structure generation models rely on diffusion, either discrete or continuous. FrameDiff, RFDiffusion, and Chroma~\citep{framediff-yim2023se, rf-watson2023novo, chroma-ingraham2023illuminating} generate protein backbones using a denoising process over the joint translation-rotation group $SO(3)\ltimes\mathbb{R}^3$, while FrameFlow, FoldFlow, and FoldFlow-2 similarly perform flow matching over the same manifold~\citep{frameflow-yim2023fast, foldflow2-bose2023se, huguet2024sequence}. Genie2 and Proteina are more recent attempts to train diffusion models at scale on the AlphaFold Structure Database (AFDB); these both operate over $C\alpha$ coordinates~\citep{proteina-geffner2025proteina, lin2024out}. The latter does not explicitly encode invariances, a strategy we broadly adopt here. Despite the strong performance of diffusion models, autoregressive models have unique features that are valuable to the structural biology and machine learning communities. Most notably, they can be applied to more use cases where the protein size is not known \emph{a priori}, an important feature for tasks such as motif scaffolding or \emph{in situ} structure prediction in electron tomography images~\citep{yadav2020situ, bunne2024build}.





To train \model, we use a flow matching objective ~\citep{esser2024scaling, lipman2022flow, lin2024out}. Flow matching interpolates between a source distribution $p_0$ (often a Gaussian) and a target distribution $p_\text{data}$ by integrating along the ODE $d\x_t = \mathbf{v}_\theta(\x_t, t)\,dt$ using a learned vector field $\mathbf{v}_\theta(\x_t, t): \mathbb{R}^d\times [0,1] \rightarrow \mathbb{R}^d$. As the vector field generating the true probability distribution is in general unknown, one uses conditional flow matching, which constructs a conditional probability path between prior samples $\x_0\sim p_0$ and data samples $\x_1\sim p_\text{data}$. Explicitly, a general probability path can be written $\x_t = \alpha_t \x_1 + \sigma_t \epsilon$, with $\epsilon\sim\mathcal{N}(0, 1)$. This induces a true conditional vector field $\mathbf{u}(\x_t|\x_0,\x_1) = \dot\alpha_t \x_1+\dot\sigma_t\x_0$, where the dot denotes the time derivative. This is a target we can regress against; for the standard case of the linear interpolation path, we have $\x_t = (1-t)\x_0 + t \x_1$ and $\mathbf{u}(\x_t|\x_0,\x_1) = \x_1 - \x_0$. For completeness, we present a more thorough derivation of the flow matching formulation in Appendix~\ref{sec:app:flow_matching}.

\textbf{Diffusion autoencoders.} The idea of using a diffusion model as the decoder in tokenizer reconstruction is a recent insight in computer vision but has yet to be explored for protein structure generation~\citep{preechakul2022diffusion}. Two recent works, FlowMo and DiTo~\citep{flowmo-sargent2025flow, chen2025diffusion}, both study this approach and independently demonstrate SOTA performance on ImageNet-1k reconstruction. In these works, the use of a diffusion model eliminates the need for combinations of perceptual and adversarial losses during training, which were critical insights introduced by VQGAN~\citep{esser2021taming}. In our case, given the success of recent models like AlphaFold3, Boltz~\citep{wohlwend2025boltz}, and Proteina in eschewing symmetric architectures for scalability, we hypothesize that diffusion autoencoders could provide a similar advantage for tokenization.",
2509.24655v2,http://arxiv.org/abs/2509.24655v2,2025-09-29 12:04:15+00:00,HyperHELM: Hyperbolic Hierarchy Encoding for mRNA Language Modeling,"Language models are increasingly applied to biological sequences like proteins and mRNA, yet their default Euclidean geometry may mismatch the hierarchical structures inherent to biological data. While hyperbolic geometry provides a better alternative for accommodating hierarchical data, it has yet to find a way into language modeling for mRNA sequences. In this work, we introduce HyperHELM, a framework that implements masked language model pre-training in hyperbolic space for mRNA sequences. Using a hybrid design with hyperbolic layers atop Euclidean backbone, HyperHELM aligns learned representations with the biological hierarchy defined by the relationship between mRNA and amino acids. Across multiple multi-species datasets, it outperforms Euclidean baselines on 9 out of 10 tasks involving property prediction, with 10% improvement on average, and excels in out-of-distribution generalization to long and low-GC content sequences; for antibody region annotation, it surpasses hierarchy-aware Euclidean models by 3% in annotation accuracy. Our results highlight hyperbolic geometry as an effective inductive bias for hierarchical language modeling of mRNA sequences.","\paragraph{RNA and mRNA Models} RNA and mRNA language models enable diverse downstream tasks in property prediction, annotation, and generation. These include foundation models trained for different RNA regions such as non-coding RNA (RNA-FM~\citep{chen2022interpretable}, and RINALMO~\citep{penic2024rinalmo}), splice sites (SpliceBERT~\citep{chen2023self}) or UTRs (UTR-LM~\citep{chu20245}), as well as methods using transfer learning from DNA and protein models~\citep{prakash2024bridging, mollaysa2025biolangfusion, garau-luis2024multimodal} for mRNA-focused downstream tasks. For mRNA, codon-level models such as CodonBERT~\citep{li2023codonbert} use codon tokenization with MLM to optimize coding-region embeddings, while Helix-mRNA~\citep{wood2025helix} employs nucleotide level tokenization and hybrid attention and state-space architectures for improved sequence resolution and generation. More recent models incorporate domain priors, such as encoding codon symmetries (Equi-mRNA~\citep{yazdani2025equi}), promoting hierarchy in Euclidean space (HELM~\citep{yazdani2025helm}), or linking sequence to a structure~\citep{moskalev2024hyena, xu2025beyond, xu2025harmony, moskalev2025geometric}. Despite these advances, all existing methods are confined to Euclidean spaces. To our knowledge, this is the first work to explore language model pre-training for RNA or mRNA in hyperbolic space.


\paragraph{Hyperbolic learning} The exponential growth of hyperbolic space makes it a suitable domain for learning on data with an inherent hierarchical structure \citep{sarkar2011low, chamberlain2017neural, nickel2017poincare}. This realization has led to a surge in the popularity of hyperbolic learning \citep{peng2021hyperbolic}. Deep hyperbolic architectures have been developed \citep{ganea2018hyperbolic, shimizu2020hyperbolic, chen2021fully} alongside the algorithms for optimizing such networks \citep{bonnabel2013stochastic, becigneul2018riemannian}. As a result, hyperbolic geometry has seen successful applications across many areas of machine learning, such as in computer vision \citep{khrulkov2020hyperbolic, liu2020hyperbolic,long2020searching, ghadimi2021hyperbolic, spengler2023poincare, mettes2024hyperbolic}, graph learning \citep{liu2019hyperbolic, chami2019hyperbolic, zhang2021hyperbolic, yang2022hyperbolic}, Natural Language Processing \citep{tifrea2018poincar,dhingra2018embedding} and multimodal learning \citep{desai2023hyperbolic, pal2024compositional}. These have shown the potential of hyperbolic learning, particularly in scenarios where the data has a clear hierarchical structure. While the structuring of mRNA is highly hierarchical in nature, existing mRNA language modeling approaches do not leverage hyperbolic geometry.

\paragraph{Prototype learning} 
The prototype learning setting \citep{snell2017prototypical} has become a commonly used approach for classification tasks, where each class is represented by a prototype, resembling in some way the perfect instance of its corresponding class. Within hyperbolic learning, prototype learning approaches are mostly distinguishable by their method of obtaining prototypes \citep{mettes2024hyperbolic}. Many works follow the original approach for generating prototypes based on labeled input data \citep{khrulkov2020hyperbolic, gao2021curvature, gao2022hyperbolic, guo2022clipped}. These typically create prototypes by aggregating features of labeled instances of the corresponding class using, for example, the Fréchet mean. Another approach is to use prior knowledge of the label set to generate prototypes. Examples are \citep{ghadimi2021hyperbolic} and \citep{long2020searching}, which create prototypes using a known hierarchy over the labels, or \citep{yu2022skin}, which optimizes prototypes concurrently with their model through the use of known hierarchical relations. Concurrent work by \citep{fonio2025hyperbolic} generates prototypes using maximal separation, not making use of any known hierarchies. While each of these works deals with an image classification setting, we instead focus on masked language modeling. Moreover, unlike our work, none of these works explore the use of recent low-distortion embedding methods for generating prototypes from hierarchies. Lastly, except for the concurrent work by \citep{fonio2025hyperbolic}, these works restrict the use of similarity functions to hyperbolic distances.","\paragraph{RNA and mRNA Models} RNA and mRNA language models enable diverse downstream tasks in property prediction, annotation, and generation. These include foundation models trained for different RNA regions such as non-coding RNA (RNA-FM~\citep{chen2022interpretable}, and RINALMO~\citep{penic2024rinalmo}), splice sites (SpliceBERT~\citep{chen2023self}) or UTRs (UTR-LM~\citep{chu20245}), as well as methods using transfer learning from DNA and protein models~\citep{prakash2024bridging, mollaysa2025biolangfusion, garau-luis2024multimodal} for mRNA-focused downstream tasks. For mRNA, codon-level models such as CodonBERT~\citep{li2023codonbert} use codon tokenization with MLM to optimize coding-region embeddings, while Helix-mRNA~\citep{wood2025helix} employs nucleotide level tokenization and hybrid attention and state-space architectures for improved sequence resolution and generation. More recent models incorporate domain priors, such as encoding codon symmetries (Equi-mRNA~\citep{yazdani2025equi}), promoting hierarchy in Euclidean space (HELM~\citep{yazdani2025helm}), or linking sequence to a structure~\citep{moskalev2024hyena, xu2025beyond, xu2025harmony, moskalev2025geometric}. Despite these advances, all existing methods are confined to Euclidean spaces. To our knowledge, this is the first work to explore language model pre-training for RNA or mRNA in hyperbolic space.


\paragraph{Hyperbolic learning} The exponential growth of hyperbolic space makes it a suitable domain for learning on data with an inherent hierarchical structure \citep{sarkar2011low, chamberlain2017neural, nickel2017poincare}. This realization has led to a surge in the popularity of hyperbolic learning \citep{peng2021hyperbolic}. Deep hyperbolic architectures have been developed \citep{ganea2018hyperbolic, shimizu2020hyperbolic, chen2021fully} alongside the algorithms for optimizing such networks \citep{bonnabel2013stochastic, becigneul2018riemannian}. As a result, hyperbolic geometry has seen successful applications across many areas of machine learning, such as in computer vision \citep{khrulkov2020hyperbolic, liu2020hyperbolic,long2020searching, ghadimi2021hyperbolic, spengler2023poincare, mettes2024hyperbolic}, graph learning \citep{liu2019hyperbolic, chami2019hyperbolic, zhang2021hyperbolic, yang2022hyperbolic}, Natural Language Processing \citep{tifrea2018poincar,dhingra2018embedding} and multimodal learning \citep{desai2023hyperbolic, pal2024compositional}. These have shown the potential of hyperbolic learning, particularly in scenarios where the data has a clear hierarchical structure. While the structuring of mRNA is highly hierarchical in nature, existing mRNA language modeling approaches do not leverage hyperbolic geometry.

\paragraph{Prototype learning} 
The prototype learning setting \citep{snell2017prototypical} has become a commonly used approach for classification tasks, where each class is represented by a prototype, resembling in some way the perfect instance of its corresponding class. Within hyperbolic learning, prototype learning approaches are mostly distinguishable by their method of obtaining prototypes \citep{mettes2024hyperbolic}. Many works follow the original approach for generating prototypes based on labeled input data \citep{khrulkov2020hyperbolic, gao2021curvature, gao2022hyperbolic, guo2022clipped}. These typically create prototypes by aggregating features of labeled instances of the corresponding class using, for example, the Fréchet mean. Another approach is to use prior knowledge of the label set to generate prototypes. Examples are \citep{ghadimi2021hyperbolic} and \citep{long2020searching}, which create prototypes using a known hierarchy over the labels, or \citep{yu2022skin}, which optimizes prototypes concurrently with their model through the use of known hierarchical relations. Concurrent work by \citep{fonio2025hyperbolic} generates prototypes using maximal separation, not making use of any known hierarchies. While each of these works deals with an image classification setting, we instead focus on masked language modeling. Moreover, unlike our work, none of these works explore the use of recent low-distortion embedding methods for generating prototypes from hierarchies. Lastly, except for the concurrent work by \citep{fonio2025hyperbolic}, these works restrict the use of similarity functions to hyperbolic distances.",
2511.09588v1,http://arxiv.org/abs/2511.09588v1,2025-11-12 13:24:53+00:00,Diffusion-Based Quality Control of Medical Image Segmentations across Organs,"Medical image segmentation using deep learning (DL) has enabled the development of automated analysis pipelines for large-scale population studies. However, state-of-the-art DL methods are prone to hallucinations, which can result in anatomically implausible segmentations. With manual correction impractical at scale, automated quality control (QC) techniques have to address the challenge. While promising, existing QC methods are organ-specific, limiting their generalizability and usability beyond their original intended task. To overcome this limitation, we propose no-new Quality Control (nnQC), a robust QC framework based on a diffusion-generative paradigm that self-adapts to any input organ dataset. Central to nnQC is a novel Team of Experts (ToE) architecture, where two specialized experts independently encode 3D spatial awareness, represented by the relative spatial position of an axial slice, and anatomical information derived from visual features from the original image. A weighted conditional module dynamically combines the pair of independent embeddings, or opinions to condition the sampling mechanism within a diffusion process, enabling the generation of a spatially aware pseudo-ground truth for predicting QC scores. Within its framework, nnQC integrates fingerprint adaptation to ensure adaptability across organs, datasets, and imaging modalities. We evaluated nnQC on seven organs using twelve publicly available datasets. Our results demonstrate that nnQC consistently outperforms state-of-the-art methods across all experiments, including cases where segmentation masks are highly degraded or completely missing, confirming its versatility and effectiveness across different organs.","%\revision{This section needs to be expanded}

\subsection{Automatic QC}
Automatic QC methods can be categorized into three main categories: embedded, semi-detached, and independent. Embedded QC methods are integrated within the segmentation model itself, allowing the model to self-evaluate its predicted output~\cite{doi:10.1073/pnas.2216399120,kalkhof2023m3d,qiu2023qcresunet}. Semi-detached methods work separately but are specifically tailored for a particular family of segmentation approaches~\cite{lin2022novel}. In contrast, independent QC methods are fully detached from any segmentation model, which makes them versatile and applicable across various segmentation frameworks~\cite{audelan2019unsupervised,fournel2021medical,galati2021efficient,robinson2018realtime,specktor2025segqc,valindria2017reverse,wang2020deep}. Our focus is on independent QC approaches due to their flexibility and adaptability, as they can be used without being tied to a specific model.

In detached QC, metric-specific approaches typically focus on either classifying segmentation masks using qualitative scores (e.g., good/bad)~\cite{kohlberger2012evaluating}, or regressing quantitative scores, such as the Dice Score\cite{fournel2021medical,liu2019alarm,qiu2023qcresunet,robinson2018realtime}. However, these methods are limited by the difficulty of gathering a sufficiently representative set of annotations covering the full spectrum of varying segmentation qualities~\cite{qiu2023qcresunet,robinson2018realtime}, or their inability to handle unbounded metrics (e.g., the Hausdorff Distance)~\cite{fournel2021medical}.
Reconstruction-based QC techniques~\cite{valindria2017reverse,galati2021efficient,wang2020deep} are detached methods that circumvent the limitations of metric-specific approaches. These techniques generate a pseudo-ground truth (pGT) mask linked to a given image and its corresponding predicted segmentation, allowing for the estimation of any quality scores for the predicted segmentation. 
%Current automatic QC methods are limited by their organ-specific design, restricting their use across anatomical structures and imaging modalities~\cite{audelan2019unsupervised,fournel2021medical,galati2021efficient,karani2021test,liu2019alarm,wang2020deep}.
%

 % Among these, a first set of methods focuses on QC techniques that are either embedded within the segmentation model itself~\cite{doi:10.1073/pnas.2216399120,kalkhof2023m3d,qiu2023qcresunet}, enabling the model to self-evaluate its predicted output, or semi-detached, where the QC module is separate but tailored for a specific family of segmentation methods~\cite{audelan2019unsupervised}. As both techniques are inherently tied to a segmentation model, their applicability to other segmentation frameworks is compromised. In contrast, independent, fully separated QC approaches~\cite{audelan2019unsupervised,fournel2021medical,galati2021efficient,qiu2023qcresunet,robinson2018realtime,specktor2025segqc,valindria2017reverse,wang2020deep}, being model-agnostic, offer greater flexibility across frameworks.

% Within the latter category, a first group of QC methods focuses on predicting either qualitative scores (e.g., good/bad) within a classification framework~\cite{kohlberger2012evaluating} or quantitative scores (typically the Dice Score) through regression-based modeling  \cite{fournel2021medical,liu2019alarm,qiu2023qcresunet,robinson2018realtime}. In practice, these methods are limited by the difficulty of gathering a sufficiently representative set of annotations covering the full spectrum of varying segmentation qualities~\cite{qiu2023qcresunet,robinson2018realtime}, or their inability to handle unbounded metrics, such as the Hausdorff Distance~\cite{fournel2021medical}.

%Reconstruction-based QC techniques~\cite{valindria2017reverse,galati2021efficient,wang2020deep} circumvent the limitations of metric-specific approaches by generating a pseudo-ground truth (pGT) mask associated with an image and its corresponding predicted segmentation, which can be used to estimate quality scores of the predicted segmentation. 
% These registration-based methods assess segmentation quality by measuring the spatial overlap between the predicted mask and a set of reference atlas images, assuming that a high-quality prediction will align well with at least one atlas. However, the strategy depends on accurate image registration, which can be computationally expensive ~\cite{galati2021efficient} and is prone to failure. Most critically, it requires access to annotated ground truth data even at inference time.

%Later methods adopted learning-based approaches that model a manifold of high-quality ground truth segmentations \cite{galati2021efficient, wang2020deep}. While more efficient, these approaches still rely on distance-based retrieval to find the closest point in the learned space to the predicted segmentation. When the prediction is of poor quality and lies far from the normative distribution, this distance-based matching can fail, resulting in pseudo-ground truths (pGTs) that no longer resemble the real ground truth and ultimately leading to unreliable quality estimates.

Early reconstruction-based QC approaches~\cite{valindria2017reverse,Robinson2019Automated}, relied on atlas propagation strategies.
These registration-based methods assess segmentation quality by measuring the spatial overlap between the predicted mask and a set of reference atlas images. The underlying assumption is that a high-quality prediction will align well with at least one of the atlas images. However, the strategy depends on accurate image registration, which can be computationally expensive~\cite{galati2021efficient} and is prone to failure. Moreover, it requires access to annotated ground truth data at inference time.

More recent reconstruction-based-QC approaches also operate under the premise that high-quality segmentations lie in a common space. However, instead of assuming spatial alignment~\cite{valindria2017reverse,Robinson2019Automated} (i.e. Euclidean space), these methods posit that high-quality ground truth masks lie within a learnable latent manifold~\cite{galati2021efficient,wang2020deep}. While more efficient and robust, these methods suffer from two critical limitations. First, because they rely on distance-based retrieval to find the closest sample point to the predicted segmentation in the learned space, issues can arise when the segmentation to be controlled is very bad and is far from the underlying normative distribution of high-quality segmentations. In such cases, this distance-based matching may fail, resulting in pseudo-ground truths (pGTs) that no longer resemble the actual ground truth, ultimately leading to unreliable quality estimates. The latter problem may be exacerbated by the fact that state-of-the-art learning-based QC techniques operate in 2D~\cite{galati2021efficient,wang2020deep,liu2019alarm,fournel2021medical}. Previous studies~\cite{fournel2021medical} have shown that performing QC at the slice level yields better results and provides finer granularity. However, the loss of three-dimensional information, which carries relevant information about the geometrical properties of a segmentation mask, can be detrimental to the sampling process. For example, segmented 2D masks of the heart's left ventricle should appear larger in the basal slices compared to the axial slices. 

In this work, we leverage the advantages of 2D learning-based reconstruction-based QC techniques, while addressing their limitations. We propose a novel sampling strategy that learns to generate high-quality pGTs from a diffusion-based generative model, guided by a rich embedding of visual and spatial cues. By injecting 3D contextual information, the proposed distance-based retrieval with conditional sampling is better suited to recover pGTs from poor segmentations. At the same time, it offers a scalable and model-agnostic QC solution. % applicable across organs, datasets, and imaging modalities.

%\vincenzo{Motivated by the findings in \cite{fournel2021medical}, we feed the nnQC with 2D image–segmentation pairs instead of 3D volumetric pairs, as this choice has been shown to yield better results in QC analysis and provides finer granularity by processing slices individually rather than entire volumes.}

%COMPLETE WHAT DOES YOUR SAMPLING STRATEGY ALLOWS TO DO.}


%Existing QC methods can be: \textit{embedded} in the segmentation method, allowing to self-evaluate the predicted segmentation~\cite{kalkhof2023m3d}; \textit{semi-detached}, where the QC module is separate but tailored for a specific family of segmentation methods~\cite{audelan2019unsupervised}; or \textit{fully separated}. As both embedded and semi-detached QC techniques are inherently tied to the segmentation model they are designed for, this limits their applicability to other segmentation frameworks. Independent QC modules, instead, by being model-agnostic, offer more flexibility.

%Among those, a significant part of the literature focuses on metric-specific approaches, i.e., models trained to predict either qualitative scores (e.g., good/bad) within classification frameworks~\cite{kohlberger2012evaluating} or quantitative scores (e.g., Dice Score) through regression-based modeling \cite{fournel2021medical,liu2019alarm,qiu2023qcresunet}. 
%In practice however, these methods are limited by their need for annotations for training~\cite{fournel2021medical,qiu2023qcresunet}, or their inability to handle unbounded metrics, such as the Hausdorff Distance. 
%In constrast, reconstruction-based QC approaches~\cite{valindria2017reverse,galati2021efficient,wang2020deep} circumvent these limitations by generating a pseudo-ground truth (pGT) mask associated with an image and its corresponding predicted segmentation that can be used to estimate quality scores of the predicted segmentation.
%However, as these models typically learn either image atlases ~\cite{valindria2017reverse,robinson2018realtime} \vincenzo{relying on time-consuming and failure-prone image registration (20 min per case~\cite{galati2021efficient})} or manifolds~\cite{galati2021efficient,wang2020deep}, they are limited by their reliance on distance metrics to identify the point within the normative model that is closest to a predicted segmentation. When a predicted segmentation is particularly poor, it will lie far from the high-quality points in the normative model, thus, generating pGTs that break the assumption of similarity with the real ground truth and rendering the estimated quality scores unreliable. 

\subsection{Image Synthesis}
%\revision{Discuss: VAEs, Diffusion Models}
Image synthesis, powered by generative modeling, is a powerful tool in medical imaging that is used in numerous applications~\cite{pinaya2022brain,tudosiu2024realistic,bercea2023mask,fernandez2024generating,gupta2024topodiffusionnet}. While earlier approaches primarily relied on Variational Autoencoders (VAEs)~\cite{rezende2015variational} and Generative Adversarial Networks (GANs)~\cite{goodfellow2014generative}, recent trends favor diffusion models (DMs) due to their superior training stability (better than GANs) and high-fidelity sample quality~\cite{ho2020denoising,song2020denoising,rombach2021highresolution} (better than VAEs).
However, the high computational demands of DMs, operating in the image space, limit their scalability in medical imaging applications. Latent Diffusion Models (LDMs)~\cite{rombach2021highresolution} overcome this by performing the diffusion process in a learned latent space, typically using a spatially-aware VAE or VAE-GAN. This approach allows LDMs to sample more effectively than traditional VAEs, maintaining essential structural information in a compact space. This is particularly useful for reconstruction-based QC, where severely corrupted masks may deviate from plausible segmentations. LDMs sampling process helps guide the output towards realistic, high-quality reconstructions, avoiding the risk of producing overly smoothed or implausible results~\cite{rombach2021highresolution,valindria2017reverse}.

Only a few previous works have explored the usage of DMs for segmentation mask generation. Fernández et al.~\cite{fernandez2024generating} use a VAE-GAN–based mask generator to condition an LDM for image synthesis. Gupta et al.~\cite{gupta2024topodiffusionnet} propose a DM to generate topologically accurate masks for subsequent image generation. In both scenarios, the generated masks are an intermediate step towards the final goal of image synthesis.

In this work, we build on the LDM framework for image synthesis proposed by~\cite{fernandez2024generating} and we extend it and adapt it to address a slightly different setup. In our case, we aim at generating segmentation masks (i.e. pseudo ground truths) guided by an input segmentation mask, whose quality is to be assessed, and the original input image.

%Recent advancements in generative modeling have increasingly positioned image synthesis as a powerful tool in medical imaging. Its applications are diverse, ranging from conditional image generation~\cite{pinaya2022brain,tudosiu2024realistic} to anomaly detection for reconstructing healthy anatomy from pathological scans~\cite{bercea2023mask}, and segmentation mask generation for downstream tasks such as inpainting or conditional synthesis~\cite{fernandez2024generating,gupta2024topodiffusionnet}. 
%While earlier approaches primarily relied on Variational Autoencoders (VAEs)~\cite{rezende2015variational} and Generative Adversarial Networks (GANs)~\cite{goodfellow2014generative}, recent trends favor diffusion models (DMs) due to their superior training stability and high-fidelity sample quality~\cite{ho2020denoising,song2020denoising,rombach2021highresolution}. Unlike GANs, which are known to suffer from training instability and mode collapse, DMs offer a more robust generative framework. However, the high computational demands of diffusion models operating in original image space limit their scalability in medical imaging applications. Latent Diffusion Models (LDMs)~\cite{rombach2021highresolution} address this issue by performing the diffusion process in a learned latent space, typically obtained via a spatially-aware VAE or VAE-GAN. This results in significantly faster training and inference while maintaining high generation quality. 
%Despite their promise, diffusion models have been underutilized for segmentation mask generation, with only a few recent works exploring this research branch: 
%~\cite{fernandez2024generating,gupta2024topodiffusionnet}.\revision{WHAT IS THE PROBLEM OF VAEs? WHY IS IT STILL BEST TO TRY LDMs? SINCE VIRGINIA AND GUPTA USE THEM IN YOUR SAME WAY, ADD 1-2 SENTENCES ABOUT WHAT THEY PROPOSE (AND -IF APPLIES- IN WHICH WAY YOYU ARE DIFFERENT FROM THEM)}%Quality control and restoration applications remain predominantly grounded in generative frameworks such as VAEs~\cite{larrazabal2020post,liu2019alarm,painchaud2020,wang2020deep} %\vincenzo{that can't properly produce and retrieve the correct corresponding good-quality segmentation in a high-defective scenario, where the segmentation's latent space lies far away from the VAE's learned good-quality manifold. This} highlights the need for more efficient and generalizable synthesis strategies in structured prediction tasks such as segmentation.
%\vincenzo{Fernández et al.~\cite{fernandez2024generating} proposed a VAE-GAN–based mask generator to condition a latent diffusion model for image synthesis, while Gupta et al.~\cite{gupta2024topodiffusionnet} employed a diffusion model to generate topologically accurate masks for subsequent image generation, with both approaches achieving state-of-the-art performance. A possible reconstruction-based QC objective using diffusion modeling, however, differs in two key aspects: 1) the presence of a bias in the mask generation process coming from the input segmentation whose quality is being assessed; 2) the mask generation that is not for the final scope conditioning image synthesis, but to reconstruct high-quality, defect-free masks.}
%\revision{WHAT IS THE PROBLEM OF VAEs? WHY IS IT STILL BEST TO TRY LDMs? STILL NOT THERE.}
%\vincenzo{Additionally, compared to conventional VAEs, LDMs offer a more effective latent sampling mechanism by operating in a learned latent space that is compact yet preserves high-level structural information. This property is particularly advantageous in reconstruction-based QC, where severely corrupted masks may lie far from the manifold of plausible segmentations; in such cases, the LDM’s sampling process facilitates convergence toward realistic, high-quality reconstructions rather than collapsing to overly smoothed or implausible outputs\cite{rombach2021highresolution,valindria2017reverse}.}



\subsection{Generalist and Specialist Frameworks}
%\revision{Discuss methods like MedSAM, etc with a focus on nnQC. We should thing of a better title for the section.
Recent advances in medical image segmentation have led to the emergence of generalist models capable of segmenting a wide range of anatomical structures from different protocols and imaging modalities with minimal manual intervention (e.g., prompts, scribbles, or bounding boxes)~\cite{butoi2023universeg,MedSAM,wong2024scribbleprompt}. 

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/pipeline_final.pdf}%\hspace{-1cm}
    %\caption{The nnQC framework. Given an image-segmentation pair, a preprocessing module leverages dataset-specific \textit{fingerprints} to normalize the inputs. The \textit{Team of Experts} (ToE) extracts opinions used to condition the sampling procedure of a latent diffusion model generating a pseudo-ground truth (\textit{pGT}), \vincenzo{that is ultimately postprocessed by using the previously stored fingerprints}. %The quality score of te input segmentation is obtained by computing a metric \textit{M} between the segmentation and \textit{pGT}.
    \caption{
      The nnQC framework. For a 3D image–segmentation pair, dataset-specific \textit{fingerprints} are extracted and used to preprocess it. Each axial segmentation slice and its corresponding 2D image are passed to the Team of Experts (ToE), which produces conditioning embeddings $c$ for the latent diffusion process. A VAE-GAN maps the 2D segmentation to be quality checked into a latent space of high-quality segmentations from which a DDIM-based Latent Diffusion Model (LDM) generates a pseudo-ground truth ($pGT$). A postprocessing restores the $pGT$ to its original space.
    }
    \label{fig:nnqc_pipeline}
\end{figure*}

Alongside them, specialist models, most notably nnU-Net~\cite{isensee2021nnu}, remain highly competitive, often surpassing generalist models. nnU-Net exemplifies a “one-for-all” model paradigm, where its architecture is self-configuring and retrained from scratch for each new dataset. Despite requiring full retraining, its strong performance, automation of preprocessing and hyperparameter tuning, and ease of use have made it a de facto standard in the field \cite{isensee2024nnu}. Both generalist and specialist approaches have enabled fast deployment at a large scale of medical image segmentation.

In this work, we take inspiration from nnU-Net's self-adaptation strategy, integrating dataset-specific fingerprints and, thus, removing the need for manual tuning. In this way, we address the bottleneck that QC represents at the moment to medical image segmentation pipeliness, by offering a robust, scalable solution to QC that can be easily adapted across organs, datasets, and imaging techniques.","\subsection{Automatic QC}
Automatic QC methods can be categorized into three main categories: embedded, semi-detached, and independent. Embedded QC methods are integrated within the segmentation model itself, allowing the model to self-evaluate its predicted output~\cite{doi:10.1073/pnas.2216399120,kalkhof2023m3d,qiu2023qcresunet}. Semi-detached methods work separately but are specifically tailored for a particular family of segmentation approaches~\cite{lin2022novel}. In contrast, independent QC methods are fully detached from any segmentation model, which makes them versatile and applicable across various segmentation frameworks~\cite{audelan2019unsupervised,fournel2021medical,galati2021efficient,robinson2018realtime,specktor2025segqc,valindria2017reverse,wang2020deep}. Our focus is on independent QC approaches due to their flexibility and adaptability, as they can be used without being tied to a specific model.

In detached QC, metric-specific approaches typically focus on either classifying segmentation masks using qualitative scores (e.g., good/bad)~\cite{kohlberger2012evaluating}, or regressing quantitative scores, such as the Dice Score\cite{fournel2021medical,liu2019alarm,qiu2023qcresunet,robinson2018realtime}. However, these methods are limited by the difficulty of gathering a sufficiently representative set of annotations covering the full spectrum of varying segmentation qualities~\cite{qiu2023qcresunet,robinson2018realtime}, or their inability to handle unbounded metrics (e.g., the Hausdorff Distance)~\cite{fournel2021medical}.
Reconstruction-based QC techniques~\cite{valindria2017reverse,galati2021efficient,wang2020deep} are detached methods that circumvent the limitations of metric-specific approaches. These techniques generate a pseudo-ground truth (pGT) mask linked to a given image and its corresponding predicted segmentation, allowing for the estimation of any quality scores for the predicted segmentation. 



 








Early reconstruction-based QC approaches~\cite{valindria2017reverse,Robinson2019Automated}, relied on atlas propagation strategies.
These registration-based methods assess segmentation quality by measuring the spatial overlap between the predicted mask and a set of reference atlas images. The underlying assumption is that a high-quality prediction will align well with at least one of the atlas images. However, the strategy depends on accurate image registration, which can be computationally expensive~\cite{galati2021efficient} and is prone to failure. Moreover, it requires access to annotated ground truth data at inference time.

More recent reconstruction-based-QC approaches also operate under the premise that high-quality segmentations lie in a common space. However, instead of assuming spatial alignment~\cite{valindria2017reverse,Robinson2019Automated} (i.e. Euclidean space), these methods posit that high-quality ground truth masks lie within a learnable latent manifold~\cite{galati2021efficient,wang2020deep}. While more efficient and robust, these methods suffer from two critical limitations. First, because they rely on distance-based retrieval to find the closest sample point to the predicted segmentation in the learned space, issues can arise when the segmentation to be controlled is very bad and is far from the underlying normative distribution of high-quality segmentations. In such cases, this distance-based matching may fail, resulting in pseudo-ground truths (pGTs) that no longer resemble the actual ground truth, ultimately leading to unreliable quality estimates. The latter problem may be exacerbated by the fact that state-of-the-art learning-based QC techniques operate in 2D~\cite{galati2021efficient,wang2020deep,liu2019alarm,fournel2021medical}. Previous studies~\cite{fournel2021medical} have shown that performing QC at the slice level yields better results and provides finer granularity. However, the loss of three-dimensional information, which carries relevant information about the geometrical properties of a segmentation mask, can be detrimental to the sampling process. For example, segmented 2D masks of the heart's left ventricle should appear larger in the basal slices compared to the axial slices. 

In this work, we leverage the advantages of 2D learning-based reconstruction-based QC techniques, while addressing their limitations. We propose a novel sampling strategy that learns to generate high-quality pGTs from a diffusion-based generative model, guided by a rich embedding of visual and spatial cues. By injecting 3D contextual information, the proposed distance-based retrieval with conditional sampling is better suited to recover pGTs from poor segmentations. At the same time, it offers a scalable and model-agnostic QC solution. 













\subsection{Image Synthesis}

Image synthesis, powered by generative modeling, is a powerful tool in medical imaging that is used in numerous applications~\cite{pinaya2022brain,tudosiu2024realistic,bercea2023mask,fernandez2024generating,gupta2024topodiffusionnet}. While earlier approaches primarily relied on Variational Autoencoders (VAEs)~\cite{rezende2015variational} and Generative Adversarial Networks (GANs)~\cite{goodfellow2014generative}, recent trends favor diffusion models (DMs) due to their superior training stability (better than GANs) and high-fidelity sample quality~\cite{ho2020denoising,song2020denoising,rombach2021highresolution} (better than VAEs).
However, the high computational demands of DMs, operating in the image space, limit their scalability in medical imaging applications. Latent Diffusion Models (LDMs)~\cite{rombach2021highresolution} overcome this by performing the diffusion process in a learned latent space, typically using a spatially-aware VAE or VAE-GAN. This approach allows LDMs to sample more effectively than traditional VAEs, maintaining essential structural information in a compact space. This is particularly useful for reconstruction-based QC, where severely corrupted masks may deviate from plausible segmentations. LDMs sampling process helps guide the output towards realistic, high-quality reconstructions, avoiding the risk of producing overly smoothed or implausible results~\cite{rombach2021highresolution,valindria2017reverse}.

Only a few previous works have explored the usage of DMs for segmentation mask generation. Fernández et al.~\cite{fernandez2024generating} use a VAE-GAN–based mask generator to condition an LDM for image synthesis. Gupta et al.~\cite{gupta2024topodiffusionnet} propose a DM to generate topologically accurate masks for subsequent image generation. In both scenarios, the generated masks are an intermediate step towards the final goal of image synthesis.

In this work, we build on the LDM framework for image synthesis proposed by~\cite{fernandez2024generating} and we extend it and adapt it to address a slightly different setup. In our case, we aim at generating segmentation masks (i.e. pseudo ground truths) guided by an input segmentation mask, whose quality is to be assessed, and the original input image.











\subsection{Generalist and Specialist Frameworks}

Recent advances in medical image segmentation have led to the emergence of generalist models capable of segmenting a wide range of anatomical structures from different protocols and imaging modalities with minimal manual intervention (e.g., prompts, scribbles, or bounding boxes)~\cite{butoi2023universeg,MedSAM,wong2024scribbleprompt}. 



Alongside them, specialist models, most notably nnU-Net~\cite{isensee2021nnu}, remain highly competitive, often surpassing generalist models. nnU-Net exemplifies a “one-for-all” model paradigm, where its architecture is self-configuring and retrained from scratch for each new dataset. Despite requiring full retraining, its strong performance, automation of preprocessing and hyperparameter tuning, and ease of use have made it a de facto standard in the field \cite{isensee2024nnu}. Both generalist and specialist approaches have enabled fast deployment at a large scale of medical image segmentation.

In this work, we take inspiration from nnU-Net's self-adaptation strategy, integrating dataset-specific fingerprints and, thus, removing the need for manual tuning. In this way, we address the bottleneck that QC represents at the moment to medical image segmentation pipeliness, by offering a robust, scalable solution to QC that can be easily adapted across organs, datasets, and imaging techniques.",
2509.15664v1,http://arxiv.org/abs/2509.15664v1,2025-09-19 06:41:45+00:00,siDPT: siRNA Efficacy Prediction via Debiased Preference-Pair Transformer,"Small interfering RNA (siRNA) is a short double-stranded RNA molecule (about 21-23 nucleotides) with the potential to cure diseases by silencing the function of target genes. Due to its well-understood mechanism, many siRNA-based drugs have been evaluated in clinical trials. However, selecting effective binding regions and designing siRNA sequences requires extensive experimentation, making the process costly. As genomic resources and publicly available siRNA datasets continue to grow, data-driven models can be leveraged to better understand siRNA-mRNA interactions. To fully exploit such data, curating high-quality siRNA datasets is essential to minimize experimental errors and noise. We propose siDPT: siRNA efficacy Prediction via Debiased Preference-Pair Transformer, a framework that constructs a preference-pair dataset and designs an siRNA-mRNA interactive transformer with debiased ranking objectives to improve siRNA inhibition prediction and generalization. We evaluate our approach using two public datasets and one newly collected patent dataset. Our model demonstrates substantial improvement in Pearson correlation and strong performance across other metrics.","\label{sec:related}

Early siRNA efficacy prediction relied on handcrafted features, including thermodynamic stability~\cite{naito2009sidirect}, nucleotide composition~\cite{khvorova2003functional}, and positional rules~\cite{katoh2007specific}. With the release of large-scale datasets~\cite{huesken2005design}, machine learning methods became feasible, e.g., i-Score and DSIR (linear regression), SVM-based method~\cite{wang2010predicting}, and ensemble models such as AdaBoost~\cite{monopoli2023asymmetric}.
Deep learning further advanced prediction with neural networks~\cite{han2018sirna, bereczki2025mitigating}, graph neural networks~\cite{la2022graph}, and latent representation learning~\cite{he2017predicting}. Transformer-based models~\cite{liu2024attsioff} and RNA foundation model embeddings (RNA-FM~\cite{chen2022interpretable}, Evo~\cite{brixi2025genome}, mRNA2vec~\cite{zhang2025mrna2vec}, Oligoformer~\cite{bai2024oligoformer}) now represent the state of the art.
However, dataset bias remains underexplored~\cite{long2024sirnadiscovery}, limiting the robustness of existing approaches.
\vspace{-0.5cm}","Early siRNA efficacy prediction relied on handcrafted features, including thermodynamic stability~\cite{naito2009sidirect}, nucleotide composition~\cite{khvorova2003functional}, and positional rules~\cite{katoh2007specific}. With the release of large-scale datasets~\cite{huesken2005design}, machine learning methods became feasible, e.g., i-Score and DSIR (linear regression), SVM-based method~\cite{wang2010predicting}, and ensemble models such as AdaBoost~\cite{monopoli2023asymmetric}.
Deep learning further advanced prediction with neural networks~\cite{han2018sirna, bereczki2025mitigating}, graph neural networks~\cite{la2022graph}, and latent representation learning~\cite{he2017predicting}. Transformer-based models~\cite{liu2024attsioff} and RNA foundation model embeddings (RNA-FM~\cite{chen2022interpretable}, Evo~\cite{brixi2025genome}, mRNA2vec~\cite{zhang2025mrna2vec}, Oligoformer~\cite{bai2024oligoformer}) now represent the state of the art.
However, dataset bias remains underexplored~\cite{long2024sirnadiscovery}, limiting the robustness of existing approaches.
\vspace{-0.5cm}",
2511.02769v1,http://arxiv.org/abs/2511.02769v1,2025-11-04 17:56:00+00:00,STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation,"The chemical space of drug-like molecules is vast, motivating the development of generative models that must learn broad chemical distributions, enable conditional generation by capturing structure-property representations, and provide fast molecular generation. Meeting the objectives depends on modeling choices, including the probabilistic modeling approach, the conditional generative formulation, the architecture, and the molecular input representation. To address the challenges, we present STAR-VAE (Selfies-encoded, Transformer-based, AutoRegressive Variational Auto Encoder), a scalable latent-variable framework with a Transformer encoder and an autoregressive Transformer decoder. It is trained on 79 million drug-like molecules from PubChem, using SELFIES to guarantee syntactic validity. The latent-variable formulation enables conditional generation: a property predictor supplies a conditioning signal that is applied consistently to the latent prior, the inference network, and the decoder. Our contributions are: (i) a Transformer-based latent-variable encoder-decoder model trained on SELFIES representations; (ii) a principled conditional latent-variable formulation for property-guided generation; and (iii) efficient finetuning with low-rank adapters (LoRA) in both encoder and decoder, enabling fast adaptation with limited property and activity data. On the GuacaMol and MOSES benchmarks, our approach matches or exceeds baselines, and latent-space analyses reveal smooth, semantically structured representations that support both unconditional exploration and property-aware generation. On the Tartarus benchmarks, the conditional model shifts docking-score distributions toward stronger predicted binding. These results suggest that a modernized, scale-appropriate VAE remains competitive for molecular generation when paired with principled conditioning and parameter-efficient finetuning.","Generative modeling has emerged as a central paradigm for molecular design, enabling data-driven exploration of the vast chemical space through learned distributions rather than exhaustive enumeration. A wide range of models have been proposed for molecular generation, spanning variational autoencoders, generative adversarial networks~\cite{li2023spotgan, li2024tengan}, normalizing flows~\cite{kuznetsov2021molgrow}, and score and diffusion-based methods~\cite{jo2022score, huang2023conditional, lee2023exploring}. Here, we focus on developments most relevant to our approach: probabilistic latent variable models and Transformer-based molecular generators, which together trace the evolution from early VAEs to modern large-scale molecular foundation models.

\textbf{Variational Autoencoders.} 
Variational autoencoders (VAEs) have significantly influenced molecular generation due to their ability to create smooth latent spaces amenable to interpolation and optimization. Gómez-Bombarelli et al.~\cite{gomez2018automatic} applied a Variational Autoencoder (VAE) framework for automatic chemical design, utilizing SMILES~\cite{weininger1989smiles} strings as their molecular representation. Their implementation featured a convolutional network as the encoder and recurrent neural networks (GRUs) as the decoder; a primary limitation of this approach was its reliance on a text-based SMILES representation, which inherently struggled with generating chemically valid structures without post-validation. GrammarVAE~\cite{kusner2017grammar} addressed this by encoding parse trees as per context-free grammars, guaranteeing syntactic validity during generation. Syntax-Directed VAE (SD-VAE)~\cite{dai2018syntax} extended this to include semantic constraints via attribute grammars. Moving to graph structures, GraphVAE~\cite{simonovsky2018graphvae} introduced a variational autoencoder that directly outputs a probabilistic fully-connected graph of a predefined maximum size to sidestep the hurdles associated with linearizing discrete graph structures for generation. GF-VAE~\cite{ma2021gf} further advanced this direction by coupling a graph-based encoder with a normalizing-flow decoder, enabling exact likelihood estimation and improving training stability on molecular graphs. Junction-Tree VAE~\cite{jin2018junction} took a complementary approach, first generating a scaffold tree of chemical substructures and then assembling a valid molecular graph. Conditional VAEs (CVAEs)~\cite{lim2018molecular,kang2018conditional} extended the paradigm to property-aware generation by incorporating property vectors into both the encoder and decoder during training. 

These models laid the groundwork for molecular VAEs, but typically relied on RNNs or GNNs and modest-sized datasets, leaving open the opportunity to revisit the VAE paradigm with high-capacity Transformer architectures trained at scale.

\textbf{Transformer-based Models.}
While VAEs formalize probabilistic latent-variable modeling, subsequent advances in molecular generation have increasingly leveraged Transformer architectures, favoring large-scale sequence modeling over explicit probabilistic formulations.
Transformer-based models are typically organized into decoder-only and encoder–decoder families, each offering distinct advantages for molecular representation learning and generation.

\textit{Decoder-only models.}
MolGPT~\cite{bagal2021molgpt} adapts the GPT-style autoregressive Transformer for SMILES, generating molecules token by token with high validity and support for property- and scaffold-conditioned sampling. However, it lacks an explicit encoder to structure latent representations, limiting controllable exploration of molecular space. Large-scale autoregressive models such as MolGen~\cite{fang2023domain} further extend this approach using SELFIES representations, two-stage pretraining, and domain-agnostic prefix tuning to mitigate chemical hallucinations. These decoder-only models excel at fluent autoregressive generation and scale efficiently but rely on surface-level conditioning rather than a principled latent structure.

\textit{Encoder–decoder models.}
Chemformer~\cite{irwin2022chemformer} introduced a BART-style encoder-\-decoder pretrained on SMILES with denoising and multitask objectives, enabling both generative (sequence-to-sequence) and discriminative (property prediction) applications. SELF-BART~\cite{priyadarsini2024self} extended this framework to SELFIES, ensuring syntactic validity and unifying property prediction and molecular generation. SELFIES-TED~\cite{priyadarsini2025selfiested} scaled this architecture to billion-token corpora, underscoring the value of robust pretraining for molecular language modeling.
Encoder–decoder models thus provide richer conditioning interfaces than decoder-only systems but remain deterministic transducers rather than probabilistic latent-variable generators. 
In summary, decoder-only Transformers offer fluent generation with limited structured control, whereas encoder–decoder models enable conditioning but rarely integrate a latent-variable formalism. This gap motivates our work, which combines a high-capacity Transformer encoder–decoder with a probabilistic latent framework to unify broad distribution learning, controllable conditional generation, and parameter-efficient finetuning.","Generative modeling has emerged as a central paradigm for molecular design, enabling data-driven exploration of the vast chemical space through learned distributions rather than exhaustive enumeration. A wide range of models have been proposed for molecular generation, spanning variational autoencoders, generative adversarial networks~\cite{li2023spotgan, li2024tengan}, normalizing flows~\cite{kuznetsov2021molgrow}, and score and diffusion-based methods~\cite{jo2022score, huang2023conditional, lee2023exploring}. Here, we focus on developments most relevant to our approach: probabilistic latent variable models and Transformer-based molecular generators, which together trace the evolution from early VAEs to modern large-scale molecular foundation models.

\textbf{Variational Autoencoders.} 
Variational autoencoders (VAEs) have significantly influenced molecular generation due to their ability to create smooth latent spaces amenable to interpolation and optimization. Gómez-Bombarelli et al.~\cite{gomez2018automatic} applied a Variational Autoencoder (VAE) framework for automatic chemical design, utilizing SMILES~\cite{weininger1989smiles} strings as their molecular representation. Their implementation featured a convolutional network as the encoder and recurrent neural networks (GRUs) as the decoder; a primary limitation of this approach was its reliance on a text-based SMILES representation, which inherently struggled with generating chemically valid structures without post-validation. GrammarVAE~\cite{kusner2017grammar} addressed this by encoding parse trees as per context-free grammars, guaranteeing syntactic validity during generation. Syntax-Directed VAE (SD-VAE)~\cite{dai2018syntax} extended this to include semantic constraints via attribute grammars. Moving to graph structures, GraphVAE~\cite{simonovsky2018graphvae} introduced a variational autoencoder that directly outputs a probabilistic fully-connected graph of a predefined maximum size to sidestep the hurdles associated with linearizing discrete graph structures for generation. GF-VAE~\cite{ma2021gf} further advanced this direction by coupling a graph-based encoder with a normalizing-flow decoder, enabling exact likelihood estimation and improving training stability on molecular graphs. Junction-Tree VAE~\cite{jin2018junction} took a complementary approach, first generating a scaffold tree of chemical substructures and then assembling a valid molecular graph. Conditional VAEs (CVAEs)~\cite{lim2018molecular,kang2018conditional} extended the paradigm to property-aware generation by incorporating property vectors into both the encoder and decoder during training. 

These models laid the groundwork for molecular VAEs, but typically relied on RNNs or GNNs and modest-sized datasets, leaving open the opportunity to revisit the VAE paradigm with high-capacity Transformer architectures trained at scale.

\textbf{Transformer-based Models.}
While VAEs formalize probabilistic latent-variable modeling, subsequent advances in molecular generation have increasingly leveraged Transformer architectures, favoring large-scale sequence modeling over explicit probabilistic formulations.
Transformer-based models are typically organized into decoder-only and encoder–decoder families, each offering distinct advantages for molecular representation learning and generation.

\textit{Decoder-only models.}
MolGPT~\cite{bagal2021molgpt} adapts the GPT-style autoregressive Transformer for SMILES, generating molecules token by token with high validity and support for property- and scaffold-conditioned sampling. However, it lacks an explicit encoder to structure latent representations, limiting controllable exploration of molecular space. Large-scale autoregressive models such as MolGen~\cite{fang2023domain} further extend this approach using SELFIES representations, two-stage pretraining, and domain-agnostic prefix tuning to mitigate chemical hallucinations. These decoder-only models excel at fluent autoregressive generation and scale efficiently but rely on surface-level conditioning rather than a principled latent structure.

\textit{Encoder–decoder models.}
Chemformer~\cite{irwin2022chemformer} introduced a BART-style encoder-\-decoder pretrained on SMILES with denoising and multitask objectives, enabling both generative (sequence-to-sequence) and discriminative (property prediction) applications. SELF-BART~\cite{priyadarsini2024self} extended this framework to SELFIES, ensuring syntactic validity and unifying property prediction and molecular generation. SELFIES-TED~\cite{priyadarsini2025selfiested} scaled this architecture to billion-token corpora, underscoring the value of robust pretraining for molecular language modeling.
Encoder–decoder models thus provide richer conditioning interfaces than decoder-only systems but remain deterministic transducers rather than probabilistic latent-variable generators. 
In summary, decoder-only Transformers offer fluent generation with limited structured control, whereas encoder–decoder models enable conditioning but rarely integrate a latent-variable formalism. This gap motivates our work, which combines a high-capacity Transformer encoder–decoder with a probabilistic latent framework to unify broad distribution learning, controllable conditional generation, and parameter-efficient finetuning.",
2509.25872v1,http://arxiv.org/abs/2509.25872v1,2025-09-30 07:07:25+00:00,Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,"Recovering unbiased properties from biased or perturbed simulations is a central challenge in rare-event sampling. Classical Girsanov Reweighting (GR) offers a principled solution by yielding exact pathwise probability ratios between perturbed and reference processes. However, the variance of GR weights grows rapidly with time, rendering it impractical for long-horizon reweighting. We introduce Marginal Girsanov Reweighting (MGR), which mitigates variance explosion by marginalizing over intermediate paths, producing stable and scalable weights for long-timescale dynamics. Experiments demonstrate that MGR (i) accurately recovers kinetic properties from umbrella-sampling trajectories in molecular dynamics, and (ii) enables efficient Bayesian parameter inference for stochastic differential equations with temporally sparse observations.","\label{sec:relatedwork}
\paragraph{Accelerated sampling via perturbations} Across many domains, direct sampling from the target dynamics is computationally prohibitive, which motivates more efficient but biased sampling methods. In molecular dynamics, metadynamics \citep{huber1994local,barducci2008well} and umbrella sampling \citep{torrie1977nonphysical,kastner2011umbrella} add additional energy \citep{grubmuller1995predicting} to reaction coordinates in order to accelerate exploration. \citep{swendsen1986replica,sugita1999replica,wang2011replica} change the temperature of the system to help cross barriers.
% A biased initial state \citep{dellago2008transition,zuckerman2017weighted} can also enhance sampling of rare events. 

In Bayesian inference for stochastic differential equations (SDEs), accelerated sampling ideas appear in likelihood estimation \citep{li2020scalable,ghosh2022differentiable}, where paths are simulated under a convenient reference SDE with sparsely sampled observations  \citep{elerian2001likelihood,roberts2001inference}. This strategy is broadly applied, including in filtering problems \citep{lyons2014series}, life sciences \citep{fuchs2013inference,bunin2017ecological} and pricing in finance \citep{jones1998bayesian,eraker2001mcmc}.

\paragraph{Reweighting of Dynamics and Path Sampling} Reweighting is a necessary technique to recover the original dynamic from the accelerated data \citep{kamenik2022enhanced}. For thermodynamic quantities, methods such as the weighted histogram analysis method (WHAM) \citep{gallicchio2005temperature,souaille2001extension} and the multistate Bennett acceptance ratio (MBAR) \citep{shirts2008statistically}, provide efficient estimators of equilibrium energies and have been applied to multiple windows enhanced sampling. Recent work also \citep{dibak2022temperature,wang2022data,moqvist2025thermodynamic,Invernizzi2022} uses machine learning to estimate the energy differences between different temperatures. 

For kinetics, finite-lag transition densities help characterize long time behavior, where intermediate states can be ignored \citep{wu2017variational,schreiner2023implicit,klein2023timewarp,diez2024boltzmann, diez2025boltzmann}. With enhanced-simulation data, unbiased kinetics can be recovered by reweighting Markov transition counts \citep{prinz2011markov,husic2018markov}, e.g., via transition-based reweighting analysis method (TRAM) \citep{mey2014xtram} and Girsanov reweighting \citep{donati2017girsanov,donati2022review,schafer2024implementation}.

% Girsanov reweighting also has connections with Feynman–Kac correctors (FKCs) \citep{skreta2025feynman}, where both of them can be formulated as a Radon–Nikodym derivative and FKCs have been applied to corrects the intermediate distributions of diffusion generator \citep{song2020score}.

% \paragraph{Transition density} Transition process for finite-time paired data has been widely studied in molecular kinetics \citep{prinz2011markov,wu2017variational,husic2018markov}. Recent work has applied deep learning to construct the Markov State Model (MSM) \citep{mardt2018vampnets,wu2018deep} or learn transfer operator directly \citep{xu2022geodiff,schreiner2023implicit,diez2024boltzmann,klein2023timewarp}. We notice that when the transfer operator is trained on the biased enhanced-simulation data, our model MGR can recover the unbiased kinetics without relying on Metropolis-Hasting \citep{klein2023timewarp}.


% \paragraph{Ratio estimation} Ratio estimation \citep{sugiyama2010density} is a fundamental technique for comparing two distributions. Kernel moment matching, e.g. KMM \citep{gretton2009covariate}, matches all the moments with reproducing kernels, which is effective and computationally efficient. Probabilistic classification \citep{menon2016linking} recasts ratio estimation as posteriors from a binary classifier, showing powerful fitting capability. Featurized classification with normalizing flows \citep{choi2021featurized} further performs classification in a learned latent space, mitigating issues caused by large distributional discrepancies. Path methods \citep{choi2022density,yu2025density} build a bridge between two distributions, and estimate the ratio by calculating the change of time score. However, these approaches typically require unbiased samples from both distributions. Advanced methods for biased datasets and known reference weights require further exploration.
\vspace{-1mm}","\paragraph{Accelerated sampling via perturbations} Across many domains, direct sampling from the target dynamics is computationally prohibitive, which motivates more efficient but biased sampling methods. In molecular dynamics, metadynamics \citep{huber1994local,barducci2008well} and umbrella sampling \citep{torrie1977nonphysical,kastner2011umbrella} add additional energy \citep{grubmuller1995predicting} to reaction coordinates in order to accelerate exploration. \citep{swendsen1986replica,sugita1999replica,wang2011replica} change the temperature of the system to help cross barriers.


In Bayesian inference for stochastic differential equations (SDEs), accelerated sampling ideas appear in likelihood estimation \citep{li2020scalable,ghosh2022differentiable}, where paths are simulated under a convenient reference SDE with sparsely sampled observations  \citep{elerian2001likelihood,roberts2001inference}. This strategy is broadly applied, including in filtering problems \citep{lyons2014series}, life sciences \citep{fuchs2013inference,bunin2017ecological} and pricing in finance \citep{jones1998bayesian,eraker2001mcmc}.

\paragraph{Reweighting of Dynamics and Path Sampling} Reweighting is a necessary technique to recover the original dynamic from the accelerated data \citep{kamenik2022enhanced}. For thermodynamic quantities, methods such as the weighted histogram analysis method (WHAM) \citep{gallicchio2005temperature,souaille2001extension} and the multistate Bennett acceptance ratio (MBAR) \citep{shirts2008statistically}, provide efficient estimators of equilibrium energies and have been applied to multiple windows enhanced sampling. Recent work also \citep{dibak2022temperature,wang2022data,moqvist2025thermodynamic,Invernizzi2022} uses machine learning to estimate the energy differences between different temperatures. 

For kinetics, finite-lag transition densities help characterize long time behavior, where intermediate states can be ignored \citep{wu2017variational,schreiner2023implicit,klein2023timewarp,diez2024boltzmann, diez2025boltzmann}. With enhanced-simulation data, unbiased kinetics can be recovered by reweighting Markov transition counts \citep{prinz2011markov,husic2018markov}, e.g., via transition-based reweighting analysis method (TRAM) \citep{mey2014xtram} and Girsanov reweighting \citep{donati2017girsanov,donati2022review,schafer2024implementation}.







\vspace{-1mm}",
2510.08655v1,http://arxiv.org/abs/2510.08655v1,2025-10-09 09:05:06+00:00,Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,"Rare genetic disease diagnosis faces critical challenges: insufficient patient data, inaccessible full genome sequencing, and the immense number of possible causative genes. These limitations cause prolonged diagnostic journeys, inappropriate treatments, and critical delays, disproportionately affecting patients in resource-limited settings where diagnostic tools are scarce. We propose RareNet, a subgraph-based Graph Neural Network that requires only patient phenotypes to identify the most likely causal gene and retrieve focused patient subgraphs for targeted clinical investigation. RareNet can function as a standalone method or serve as a pre-processing or post-processing filter for other candidate gene prioritization methods, consistently enhancing their performance while potentially enabling explainable insights. Through comprehensive evaluation on two biomedical datasets, we demonstrate competitive and robust causal gene prediction and significant performance gains when integrated with other frameworks. By requiring only phenotypic data, which is readily available in any clinical setting, RareNet democratizes access to sophisticated genetic analysis, offering particular value for underserved populations lacking advanced genomic infrastructure.","\label{sec:related_work}
\paragraph{Rare Disease Prediction.}
Early computational approaches for rare disease diagnosis fall into three categories: genotype-based, phenotype-based, and hybrid methods. While effective in well-resourced settings, these approaches often assume access to high-quality sequencing data, curated candidate gene lists, and expert interpretation-resources that are often unavailable in low-resource clinical environments. Genotype-based tools such as MutationTaster \cite{steinhaus2021mutationtaster2021}, CADD \cite{rentzsch2019cadd}, and M-CAP \cite{jagadeesh2016m} assess variant pathogenicity using genome-wide annotations, but rely on access to well-annotated variants and sequencing infrastructure. Phenotype-driven models like Phenolyzer \cite{yang2015phenolyzer}, Phrank \cite{jagadeesh2019phrank}, and CADA \cite{peng2021cada} use Human Phenotype Ontology (HPO) terms to prioritize genes based on phenotypic similarity. PhenoApt~\cite{cui2020conan} constructs a heterogeneous knowledge graph from resources such as HPO \cite{10.1093/nar/gkaa1043}, OMIM \cite{10.1093/nar/gky1151}, and Orphanet \cite{weinreich2008orphanet}, integrating gene–phenotype and phenotype–phenotype relationships, and uses graph embedding techniques to rank candidate genes based on their similarity to the patient phenotype profile, with support for both clinician-defined and automatically derived phenotype weights. These models, however, often depend on curated gene–phenotype associations and perform poorly when input phenotypes are sparse, noisy, or atypical. Hybrid methods integrate both genomic and phenotypic information. Amelie \cite{birgmeier2020amelie} uses natural language processing to mine the biomedical literature and rank genes from a candidate list based on literature support for gene–phenotype associations. Shepherd \cite{shepherd} employs a graph neural network to combine patient phenotypes with a biomedical knowledge graph and rank genes from a curated candidate list, typically based on rare or pathogenic variants identified through sequencing. Other hybrid frameworks include Exomiser \cite{smedley2015next}, Xrare \cite{li2019xrare}, and AI-MARRVEL \cite{mao2024ai}. However, many of these tools rely on predefined candidate gene lists or Variant Call Format (VCF) files, which require sequencing and expert interpretation-resources that are rarely available in under-resourced settings. Additionally, curated candidate lists introduce biases toward well-characterized genes, limiting generalization to novel or atypical cases.

\paragraph{Knowledge Graphs in Healthcare.}
To move beyond fixed candidate sets, biomedical knowledge graphs (KGs) such as DisGeNET \cite{10.1093/nar/gkz1021}, DRKG \cite{drkg2020}, GenomicKB \cite{10.1093/nar/gkac957}, and PRIME-KG \cite{Chandak2022.05.01.489928} have emerged as comprehensive resources for representing gene–disease–phenotype–drug relationships at scale. KG embedding methods and multimodal frameworks \cite{vilela2023biomedical, ektefaie2023multimodal, galkin2023towards} enable systematic reasoning and support downstream tasks such as disease–gene association prediction and drug repurposing. While these approaches offer a scalable alternative to manual curation, applying KGs directly to patient-level diagnosis remains difficult, particularly in low-resource settings, where both computational capacity and detailed clinical annotations are often limited. Additionally, the scale and heterogeneity of biomedical KGs pose challenges for generating interpretable, patient-specific outputs in real-time clinical workflows.

\paragraph{Subgraph Extraction and KG-Augmented Reasoning.}
Recent advances have sought to improve efficiency and interpretability by extracting concise, context- or patient-specific subgraphs from large knowledge graphs. SubGNN \cite{alsentzer2020subgraph} and PullNet \cite{sun2019pullnet} formalize supervised subgraph prediction and iterative question-specific retrieval, while other methods leverage data augmentation \cite{shen2022improving} or simulate undiagnosed patients \cite{alsentzer2023simulation}. In parallel, hybrid LLM–KG pipelines-such as GNN-RAG \cite{mavromatis2024gnnraggraphneuralretrieval}, G-Retriever \cite{NEURIPS2024_efaf1c97}, MindMap \cite{wen2024mindmapknowledgegraphprompting}, RoG \cite{luo2024reasoninggraphsfaithfulinterpretable}, and GoG \cite{li2023graphreasoningquestionanswering}-retrieve KG subgraphs to support LLM-based reasoning. These methods address challenges such as graph-to-text encoding \cite{fatemi2023talklikegraphencoding, 10387715, 10697304} and hallucination control \cite{Hager2024, galkin2023towards}. However, rare disease diagnosis presents fundamentally different constraints: data is sparse, phenotypes are often noisy or underspecified, and labeled examples are limited. In this setting, existing LLM–KG pipelines, designed for broad, well-resourced domains, struggle to generalize and are prone to hallucinations, making them unreliable for accurate and equitable gene prioritization in real-world clinical use.

\paragraph{Research Gap.}
Existing models often rely on curated gene lists or variant-level inputs, limiting their applicability in low-resource environments. RareNet addresses this gap by operating directly on phenotype-only inputs, without requiring sequencing or expert-curated candidates. It is the first method to jointly extract patient-specific subgraphs from a biomedical knowledge graph, generate candidate gene lists, and prioritize causal genes, opening the paths to interpretable rare disease diagnostics in settings with limited infrastructure.","\paragraph{Rare Disease Prediction.}
Early computational approaches for rare disease diagnosis fall into three categories: genotype-based, phenotype-based, and hybrid methods. While effective in well-resourced settings, these approaches often assume access to high-quality sequencing data, curated candidate gene lists, and expert interpretation-resources that are often unavailable in low-resource clinical environments. Genotype-based tools such as MutationTaster \cite{steinhaus2021mutationtaster2021}, CADD \cite{rentzsch2019cadd}, and M-CAP \cite{jagadeesh2016m} assess variant pathogenicity using genome-wide annotations, but rely on access to well-annotated variants and sequencing infrastructure. Phenotype-driven models like Phenolyzer \cite{yang2015phenolyzer}, Phrank \cite{jagadeesh2019phrank}, and CADA \cite{peng2021cada} use Human Phenotype Ontology (HPO) terms to prioritize genes based on phenotypic similarity. PhenoApt~\cite{cui2020conan} constructs a heterogeneous knowledge graph from resources such as HPO \cite{10.1093/nar/gkaa1043}, OMIM \cite{10.1093/nar/gky1151}, and Orphanet \cite{weinreich2008orphanet}, integrating gene–phenotype and phenotype–phenotype relationships, and uses graph embedding techniques to rank candidate genes based on their similarity to the patient phenotype profile, with support for both clinician-defined and automatically derived phenotype weights. These models, however, often depend on curated gene–phenotype associations and perform poorly when input phenotypes are sparse, noisy, or atypical. Hybrid methods integrate both genomic and phenotypic information. Amelie \cite{birgmeier2020amelie} uses natural language processing to mine the biomedical literature and rank genes from a candidate list based on literature support for gene–phenotype associations. Shepherd \cite{shepherd} employs a graph neural network to combine patient phenotypes with a biomedical knowledge graph and rank genes from a curated candidate list, typically based on rare or pathogenic variants identified through sequencing. Other hybrid frameworks include Exomiser \cite{smedley2015next}, Xrare \cite{li2019xrare}, and AI-MARRVEL \cite{mao2024ai}. However, many of these tools rely on predefined candidate gene lists or Variant Call Format (VCF) files, which require sequencing and expert interpretation-resources that are rarely available in under-resourced settings. Additionally, curated candidate lists introduce biases toward well-characterized genes, limiting generalization to novel or atypical cases.

\paragraph{Knowledge Graphs in Healthcare.}
To move beyond fixed candidate sets, biomedical knowledge graphs (KGs) such as DisGeNET \cite{10.1093/nar/gkz1021}, DRKG \cite{drkg2020}, GenomicKB \cite{10.1093/nar/gkac957}, and PRIME-KG \cite{Chandak2022.05.01.489928} have emerged as comprehensive resources for representing gene–disease–phenotype–drug relationships at scale. KG embedding methods and multimodal frameworks \cite{vilela2023biomedical, ektefaie2023multimodal, galkin2023towards} enable systematic reasoning and support downstream tasks such as disease–gene association prediction and drug repurposing. While these approaches offer a scalable alternative to manual curation, applying KGs directly to patient-level diagnosis remains difficult, particularly in low-resource settings, where both computational capacity and detailed clinical annotations are often limited. Additionally, the scale and heterogeneity of biomedical KGs pose challenges for generating interpretable, patient-specific outputs in real-time clinical workflows.

\paragraph{Subgraph Extraction and KG-Augmented Reasoning.}
Recent advances have sought to improve efficiency and interpretability by extracting concise, context- or patient-specific subgraphs from large knowledge graphs. SubGNN \cite{alsentzer2020subgraph} and PullNet \cite{sun2019pullnet} formalize supervised subgraph prediction and iterative question-specific retrieval, while other methods leverage data augmentation \cite{shen2022improving} or simulate undiagnosed patients \cite{alsentzer2023simulation}. In parallel, hybrid LLM–KG pipelines-such as GNN-RAG \cite{mavromatis2024gnnraggraphneuralretrieval}, G-Retriever \cite{NEURIPS2024_efaf1c97}, MindMap \cite{wen2024mindmapknowledgegraphprompting}, RoG \cite{luo2024reasoninggraphsfaithfulinterpretable}, and GoG \cite{li2023graphreasoningquestionanswering}-retrieve KG subgraphs to support LLM-based reasoning. These methods address challenges such as graph-to-text encoding \cite{fatemi2023talklikegraphencoding, 10387715, 10697304} and hallucination control \cite{Hager2024, galkin2023towards}. However, rare disease diagnosis presents fundamentally different constraints: data is sparse, phenotypes are often noisy or underspecified, and labeled examples are limited. In this setting, existing LLM–KG pipelines, designed for broad, well-resourced domains, struggle to generalize and are prone to hallucinations, making them unreliable for accurate and equitable gene prioritization in real-world clinical use.

\paragraph{Research Gap.}
Existing models often rely on curated gene lists or variant-level inputs, limiting their applicability in low-resource environments. RareNet addresses this gap by operating directly on phenotype-only inputs, without requiring sequencing or expert-curated candidates. It is the first method to jointly extract patient-specific subgraphs from a biomedical knowledge graph, generate candidate gene lists, and prioritize causal genes, opening the paths to interpretable rare disease diagnostics in settings with limited infrastructure.",
2511.09290v1,http://arxiv.org/abs/2511.09290v1,2025-11-12 12:57:19+00:00,Multi-step Predictive Coding Leads To Simplicity Bias,"Predictive coding is a framework for understanding the formation of low-dimensional internal representations mirroring the environment's latent structure. The conditions under which such representations emerge remain unclear. In this work, we investigate how the prediction horizon and network depth shape the solutions of predictive coding tasks. Using a minimal abstract setting inspired by prior work, we show empirically and theoretically that sufficiently deep networks trained with multi-step prediction horizons consistently recover the underlying latent structure, a phenomenon explained through the Ordinary Least Squares estimator structure and biases in learning dynamics. We then extend these insights to nonlinear networks and complex datasets, including piecewise linear functions, MNIST, multiple latent states and higher dimensional state geometries. Our results provide a principled understanding of when and why predictive coding induces structured representations, bridging the gap between empirical observations and theoretical foundations.","Predictive coding has been shown to uncover latent structure in environments. \cite{recanatesi2021predictive} demonstrated that predictive learning can recover low-dimensional latent spaces in discrete, continuous, and angular settings. However, they did not examine when such structure fails to emerge. \cite{levenstein2024sequential} extended this line of work, showing that recurrent networks form continuous attractors under multi-step but not next-step prediction, underscoring the role of prediction horizon. Our study builds on these findings by analyzing how horizon length, network depth, and optimization dynamics bias predictive coding solutions.

Separately, theoretical results on implicit bias in classification show that gradient descent converges to the hard-margin SVM solution for linearly separable data. This has been established for single-layer \cite{soudry2018implicit}, deep linear \cite{ji2018gradient}, and homogeneous networks \cite{lyu2019gradient}. We use these results to characterize the implicit bias of deep linear networks in our abstract predictive coding classification task, and make a connection that was previously overlooked: In deep neural networks performing multiclass classifications, the parameters converge to the hard margin SVM with regularization over the weight matrix rank rather than its $L_2$ norm. 

Finally, while related to Neural Collapse \cite{papyan2020prevalence}, our findings differ: in our work representations collapse toward the latent geometry of the environment rather than a simplex, indicating that the effect arises from the structure of the environment rather than an optimal decoding geometry.","Predictive coding has been shown to uncover latent structure in environments. \cite{recanatesi2021predictive} demonstrated that predictive learning can recover low-dimensional latent spaces in discrete, continuous, and angular settings. However, they did not examine when such structure fails to emerge. \cite{levenstein2024sequential} extended this line of work, showing that recurrent networks form continuous attractors under multi-step but not next-step prediction, underscoring the role of prediction horizon. Our study builds on these findings by analyzing how horizon length, network depth, and optimization dynamics bias predictive coding solutions.

Separately, theoretical results on implicit bias in classification show that gradient descent converges to the hard-margin SVM solution for linearly separable data. This has been established for single-layer \cite{soudry2018implicit}, deep linear \cite{ji2018gradient}, and homogeneous networks \cite{lyu2019gradient}. We use these results to characterize the implicit bias of deep linear networks in our abstract predictive coding classification task, and make a connection that was previously overlooked: In deep neural networks performing multiclass classifications, the parameters converge to the hard margin SVM with regularization over the weight matrix rank rather than its $L_2$ norm. 

Finally, while related to Neural Collapse \cite{papyan2020prevalence}, our findings differ: in our work representations collapse toward the latent geometry of the environment rather than a simplex, indicating that the effect arises from the structure of the environment rather than an optimal decoding geometry.",
2510.24709v1,http://arxiv.org/abs/2510.24709v1,2025-10-28 17:57:05+00:00,Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,"Object binding, the brain's ability to bind the many features that collectively represent an object into a coherent whole, is central to human cognition. It groups low-level perceptual features into high-level object representations, stores those objects efficiently and compositionally in memory, and supports human reasoning about individual object instances. While prior work often imposes object-centric attention (e.g., Slot Attention) explicitly to probe these benefits, it remains unclear whether this ability naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they could: recognizing which patches belong to the same object should be useful for downstream prediction and thus guide attention. Motivated by the quadratic nature of self-attention, we hypothesize that ViTs represent whether two patches belong to the same object, a property we term IsSameObject. We decode IsSameObject from patch embeddings across ViT layers using a similarity probe, which reaches over 90% accuracy. Crucially, this object-binding capability emerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker in ImageNet-supervised models, suggesting that binding is not a trivial architectural artifact, but an ability acquired through specific pretraining objectives. We further discover that IsSameObject is encoded in a low-dimensional subspace on top of object features, and that this signal actively guides attention. Ablating IsSameObject from model activations degrades downstream performance and works against the learning objective, implying that emergent object binding naturally serves the pretraining objective. Our findings challenge the view that ViTs lack object binding and highlight how symbolic knowledge of ""which parts belong together"" emerges naturally in a connectionist system.","%Area of scientific inquiry
%Summary of the relevant papers
%Difference to this paper
\paragraph{Object Binding in Cognitive Science and Neuroscience.}
The object binding problem asks how the brain integrates features that are processed across many distinct cortical areas into coherent object representations~\cite{vdmalsburg1999binding}. The concept of binding\footnote{The term binding was introduced to neuroscience by Christoph von der Malsburg in 1981, inspired by the notion of variable binding in computer science~\cite{feldman2013neural}.} rests on three key hypotheses: First, visual processing is widely understood to be hierarchical, parallel, and distributed across the cortex~\cite{zeki1978functional, livingstone1988segregation, mishkin1983object, felleman1991distributed, grill2014functional}. Second, we perceive the world primarily in terms of objects, rather than as a collection of scattered features~\cite{peters2021capturing, scholl2001objects}. This abstraction is fundamental to both perception and interaction with the world, allowing us to recognize, reason about, and manipulate our environment effectively~\cite{kanizsa1979organization, palmer1977hierarchical, biederman1987recognition}. Third, feature binding requires a mechanism that correctly assigns features, represented in spatially distinct cortical areas, to their corresponding object~\cite{feldman2013neural, von1994correlation, treisman1996binding}. This third hypothesis is where the core of the binding problem lies, and it has been a longstanding point of debate among neuroscientists and cognitive scientists~\cite{scholte2025beyond, robertson2003binding, roskies1999binding}.

Despite their substantial difference, vision transformers (ViTs) share several key computational parallels with the mammalian visual system: they both rely on parallel, distributed and hierarchical processes. More importantly, ViTs do have two of the three architectural and computational elements hypothesized to enable binding in the brain. The explicit position embeddings in ViTs resemble spatial tagging and the spatiotopic organization observed in the ventral stream~\cite{treisman1977focused, robertson2003binding}; and the self-attention mechanism is akin to dynamic tuning and attentional modulation, which are thought to be primary mechanisms for object binding~\cite{robertson2003binding, reynolds1999role, roelfsema2023solving} (although attention is believed to be of recurrent nature in the brain \cite{van2020going, kar2019evidence}). These parallels position ViTs as potential computational models for exploring object binding in both artificial and biological systems.

\paragraph{Object-Centric Learning.} Motivated by how humans naturally reason about individual objects, Object-Centric Learning (OCL~\cite{locatello2020object}) aims to represent a scene as a composition of disentangled object representations. While segmentation only partitions an image into object masks, OCL goes further by encoding each object into its own representation~\cite{greff2019multi}. Unsupervised approaches such as MONet~\cite{burgess2019monet}, IODINE~\cite{greff2019multi}, and especially Slot Attention~\cite{locatello2020object} encode scenes into a small, permutation‑invariant set of “slots” that are iteratively refined, producing robust object representations on both synthetic~\cite{locatello2020object, kipf2021conditional} and real‑world data~\cite{seitzer2022bridging, singh2022simple} and enabling compositional generation and manipulation~\cite{jiang2023object, jung2024learning, kim2023leveraging}. However, slot‑based models impose a fixed slot budget and require multiple refinement iterations that slows inference, and because Slot Attention is bolted onto rather than built into the transformer, scaling and training become harder~\cite{rubinstein2025we}. Other explicit object-centric approaches include Tensor Product Representations~\cite{teh2023towards} and Capsule Networks~\cite{sabour2017dynamic}.

Instead of object-centric approaches that explicitly enforce object-level attention, we propose an alternative view that ViTs may already encode implicit object‑level structure. Prior work has assumed this and attempted to group patches into objects directly from activations or attention maps ViTs, using methods like clustering~\cite{qian2024recasting} or GraphCut ~\cite{wang2023tokencut}. Other studies design self‑supervised objectives (e.g., object “movability”~\cite{bielski2022move} or patch‑level contrastive learning~\cite{ding2022deeply}) and train models to strengthen object‑level grouping. In contrast, our study directly validates this assumption by showing that ViT patch embeddings intrinsically encode whether any two patches belong to the same object. 

\paragraph{Binding in Transformers.}
Binding has received growing recognition in transformer-based machine learning research and binding failures are seen as examples of performance breakdowns in modern applications~\cite{trusca2024object, hu2024token, wang2025reversal, campbell2024understanding}. Diffusion models rely on binding attributes to entities, and failures cause attribute leakage (e.g., both a dog and a cat end up wearing sunglasses and a sun‑hat)~\cite{hu2024token, trusca2024object}. Vision-language models face similar binding challenges, struggling with differentiating multiple objects with feature conjunctions~\cite{campbell2024understanding}. Despite these binding failures, transformers still demonstrate some binding capability, yet the underlying mechanism is not well understood. \citet{feng2023language, dai2024representational} study binding in language models, showing that attributes (e.g., “lives in Shanghai”) are linked to their subjects (e.g., ""Alice"") via a low-dimensional \textit{binding-ID} code that is added to the activation and can be edited to swap or redirect relations. Binding mechanisms in vision transformers remain unexplored, and our study aims to fill this gap.","\paragraph{Object Binding in Cognitive Science and Neuroscience.}
The object binding problem asks how the brain integrates features that are processed across many distinct cortical areas into coherent object representations~\cite{vdmalsburg1999binding}. The concept of binding\footnote{The term binding was introduced to neuroscience by Christoph von der Malsburg in 1981, inspired by the notion of variable binding in computer science~\cite{feldman2013neural}.} rests on three key hypotheses: First, visual processing is widely understood to be hierarchical, parallel, and distributed across the cortex~\cite{zeki1978functional, livingstone1988segregation, mishkin1983object, felleman1991distributed, grill2014functional}. Second, we perceive the world primarily in terms of objects, rather than as a collection of scattered features~\cite{peters2021capturing, scholl2001objects}. This abstraction is fundamental to both perception and interaction with the world, allowing us to recognize, reason about, and manipulate our environment effectively~\cite{kanizsa1979organization, palmer1977hierarchical, biederman1987recognition}. Third, feature binding requires a mechanism that correctly assigns features, represented in spatially distinct cortical areas, to their corresponding object~\cite{feldman2013neural, von1994correlation, treisman1996binding}. This third hypothesis is where the core of the binding problem lies, and it has been a longstanding point of debate among neuroscientists and cognitive scientists~\cite{scholte2025beyond, robertson2003binding, roskies1999binding}.

Despite their substantial difference, vision transformers (ViTs) share several key computational parallels with the mammalian visual system: they both rely on parallel, distributed and hierarchical processes. More importantly, ViTs do have two of the three architectural and computational elements hypothesized to enable binding in the brain. The explicit position embeddings in ViTs resemble spatial tagging and the spatiotopic organization observed in the ventral stream~\cite{treisman1977focused, robertson2003binding}; and the self-attention mechanism is akin to dynamic tuning and attentional modulation, which are thought to be primary mechanisms for object binding~\cite{robertson2003binding, reynolds1999role, roelfsema2023solving} (although attention is believed to be of recurrent nature in the brain \cite{van2020going, kar2019evidence}). These parallels position ViTs as potential computational models for exploring object binding in both artificial and biological systems.

\paragraph{Object-Centric Learning.} Motivated by how humans naturally reason about individual objects, Object-Centric Learning (OCL~\cite{locatello2020object}) aims to represent a scene as a composition of disentangled object representations. While segmentation only partitions an image into object masks, OCL goes further by encoding each object into its own representation~\cite{greff2019multi}. Unsupervised approaches such as MONet~\cite{burgess2019monet}, IODINE~\cite{greff2019multi}, and especially Slot Attention~\cite{locatello2020object} encode scenes into a small, permutation‑invariant set of “slots” that are iteratively refined, producing robust object representations on both synthetic~\cite{locatello2020object, kipf2021conditional} and real‑world data~\cite{seitzer2022bridging, singh2022simple} and enabling compositional generation and manipulation~\cite{jiang2023object, jung2024learning, kim2023leveraging}. However, slot‑based models impose a fixed slot budget and require multiple refinement iterations that slows inference, and because Slot Attention is bolted onto rather than built into the transformer, scaling and training become harder~\cite{rubinstein2025we}. Other explicit object-centric approaches include Tensor Product Representations~\cite{teh2023towards} and Capsule Networks~\cite{sabour2017dynamic}.

Instead of object-centric approaches that explicitly enforce object-level attention, we propose an alternative view that ViTs may already encode implicit object‑level structure. Prior work has assumed this and attempted to group patches into objects directly from activations or attention maps ViTs, using methods like clustering~\cite{qian2024recasting} or GraphCut ~\cite{wang2023tokencut}. Other studies design self‑supervised objectives (e.g., object “movability”~\cite{bielski2022move} or patch‑level contrastive learning~\cite{ding2022deeply}) and train models to strengthen object‑level grouping. In contrast, our study directly validates this assumption by showing that ViT patch embeddings intrinsically encode whether any two patches belong to the same object. 

\paragraph{Binding in Transformers.}
Binding has received growing recognition in transformer-based machine learning research and binding failures are seen as examples of performance breakdowns in modern applications~\cite{trusca2024object, hu2024token, wang2025reversal, campbell2024understanding}. Diffusion models rely on binding attributes to entities, and failures cause attribute leakage (e.g., both a dog and a cat end up wearing sunglasses and a sun‑hat)~\cite{hu2024token, trusca2024object}. Vision-language models face similar binding challenges, struggling with differentiating multiple objects with feature conjunctions~\cite{campbell2024understanding}. Despite these binding failures, transformers still demonstrate some binding capability, yet the underlying mechanism is not well understood. \citet{feng2023language, dai2024representational} study binding in language models, showing that attributes (e.g., “lives in Shanghai”) are linked to their subjects (e.g., ""Alice"") via a low-dimensional \textit{binding-ID} code that is added to the activation and can be edited to swap or redirect relations. Binding mechanisms in vision transformers remain unexplored, and our study aims to fill this gap.",
2511.00977v1,http://arxiv.org/abs/2511.00977v1,2025-11-02 15:41:38+00:00,Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow,"Understanding the evolution of cellular microenvironments in spatiotemporal data is essential for deciphering tissue development and disease progression. While experimental techniques like spatial transcriptomics now enable high-resolution mapping of tissue organization across space and time, current methods that model cellular evolution operate at the single-cell level, overlooking the coordinated development of cellular states in a tissue. We introduce NicheFlow, a flow-based generative model that infers the temporal trajectory of cellular microenvironments across sequential spatial slides. By representing local cell neighborhoods as point clouds, NicheFlow jointly models the evolution of cell states and spatial coordinates using optimal transport and Variational Flow Matching. Our approach successfully recovers both global spatial architecture and local microenvironment composition across diverse spatiotemporal datasets, from embryonic to brain development.","\modelname{} is at the interface between generative models and spatiotemporal transcriptomic data. 

\textbf{FM and single-cell transcriptomics.} We propose a model based on FM, a framework introduced by several seminal works \citep{liu2023flow, lipman2023flow, albergo2023building}. Specifically, we adopt a variational view of the FM objective, following \citet{eijkelboom2024variational}, but extend it to mixed-factorized distributions for point cloud generation. Our method, \modelname{}, combines FM with OT, a pairing that has proven effective in modeling cellular data \citep{tong2024improving, pmlr-v238-tong24a, atanackovic2025meta, klein2024genot}. Unlike these models, however, we focus on point clouds of spatially-resolved transcriptomic profiles. Closest to our approach is Wasserstein FM for point cloud generation \citep{haviv2024wasserstein}, applied to reconstruct cellular niches. Yet, that work does not address joint generation of spatial coordinates and cellular states, nor OT-based temporal trajectory prediction, both of which are central to our contribution.


\textbf{Generative models for spatial transcriptomics.} Generative models have been key to spatial tasks such as gene expression prediction from histology slides \citep{zhu2025diffusion, wan2023integrating}, integration with dissociated single-cell data \citep{wan2023integrating}, spatial imputation \citep{haviv2025covariance, li2024stdiff}, and perturbation \cite{akbarnejad2025mapping}. More recently, LUNA \citep{yu2025tissue} demonstrated strong performance in predicting single-cell spatial coordinates using diffusion models \citep{ho2020denoising} conditioned on transcription data. While related, our model addresses the distinct task of inferring niche trajectories, enabling the simultaneous generation of coordinates and cellular states.

\textbf{Trajectory inference for spatial transcriptomics.} Previous work has explored learning trajectories from spatial slides. \citet{pham2023robust} proposed a graph-based spatiotemporal algorithm for pseudotime inference, while others leveraged tissue-resolved transcriptomics to estimate cell velocity \citep{abdelaal2024sirv, long2025spvelo, shen2025inferring}. Closer to our approach, \citet{klein2025mapping} and \citet{bryan2025accurate} use discrete OT to link cells across time and infer the evolution of cell states from spatially-resolved gene expression. Similarly, DeST-OT \citep{halmos2025dest} aligns spatial slides with semi-relaxed OT couplings, preserving transcriptomic and spatial proximity between ancestor and descendant cells, while SpaTrack \citep{shen2025inferring} uses Fused Gromov-Wasserstein OT \citep{vayer2020fused}, balancing transcriptomic and spatial differences based on spatial autocorrelation of features. Unlike our mini-batch deep learning model, these methods do not operate on entire microenvironments and rely on exact OT at the single-cell level, resulting in limitations in scalability and generalization.","\modelname{} is at the interface between generative models and spatiotemporal transcriptomic data. 

\textbf{FM and single-cell transcriptomics.} We propose a model based on FM, a framework introduced by several seminal works \citep{liu2023flow, lipman2023flow, albergo2023building}. Specifically, we adopt a variational view of the FM objective, following \citet{eijkelboom2024variational}, but extend it to mixed-factorized distributions for point cloud generation. Our method, \modelname{}, combines FM with OT, a pairing that has proven effective in modeling cellular data \citep{tong2024improving, pmlr-v238-tong24a, atanackovic2025meta, klein2024genot}. Unlike these models, however, we focus on point clouds of spatially-resolved transcriptomic profiles. Closest to our approach is Wasserstein FM for point cloud generation \citep{haviv2024wasserstein}, applied to reconstruct cellular niches. Yet, that work does not address joint generation of spatial coordinates and cellular states, nor OT-based temporal trajectory prediction, both of which are central to our contribution.


\textbf{Generative models for spatial transcriptomics.} Generative models have been key to spatial tasks such as gene expression prediction from histology slides \citep{zhu2025diffusion, wan2023integrating}, integration with dissociated single-cell data \citep{wan2023integrating}, spatial imputation \citep{haviv2025covariance, li2024stdiff}, and perturbation \cite{akbarnejad2025mapping}. More recently, LUNA \citep{yu2025tissue} demonstrated strong performance in predicting single-cell spatial coordinates using diffusion models \citep{ho2020denoising} conditioned on transcription data. While related, our model addresses the distinct task of inferring niche trajectories, enabling the simultaneous generation of coordinates and cellular states.

\textbf{Trajectory inference for spatial transcriptomics.} Previous work has explored learning trajectories from spatial slides. \citet{pham2023robust} proposed a graph-based spatiotemporal algorithm for pseudotime inference, while others leveraged tissue-resolved transcriptomics to estimate cell velocity \citep{abdelaal2024sirv, long2025spvelo, shen2025inferring}. Closer to our approach, \citet{klein2025mapping} and \citet{bryan2025accurate} use discrete OT to link cells across time and infer the evolution of cell states from spatially-resolved gene expression. Similarly, DeST-OT \citep{halmos2025dest} aligns spatial slides with semi-relaxed OT couplings, preserving transcriptomic and spatial proximity between ancestor and descendant cells, while SpaTrack \citep{shen2025inferring} uses Fused Gromov-Wasserstein OT \citep{vayer2020fused}, balancing transcriptomic and spatial differences based on spatial autocorrelation of features. Unlike our mini-batch deep learning model, these methods do not operate on entire microenvironments and rely on exact OT at the single-cell level, resulting in limitations in scalability and generalization.",
2510.18808v1,http://arxiv.org/abs/2510.18808v1,2025-10-21 17:04:06+00:00,On Biologically Plausible Learning in Continuous Time,"Biological learning unfolds continuously in time, yet most algorithmic models rely on discrete updates and separate inference and learning phases. We study a continuous-time neural model that unifies several biologically plausible learning algorithms and removes the need for phase separation. Rules including stochastic gradient descent (SGD), feedback alignment (FA), direct feedback alignment (DFA), and Kolen-Pollack (KP) emerge naturally as limiting cases of the dynamics. Simulations show that these continuous-time networks stably learn at biological timescales, even under temporal mismatches and integration noise. Through analysis and simulation, we show that learning depends on temporal overlap: a synapse updates correctly only when its input and the corresponding error signal coincide in time. When inputs are held constant, learning strength declines linearly as the delay between input and error approaches the stimulus duration, explaining observed robustness and failure across network depths. Critically, robust learning requires the synaptic plasticity timescale to exceed the stimulus duration by one to two orders of magnitude. For typical cortical stimuli (tens of milliseconds), this places the functional plasticity window in the few-second range, a testable prediction that identifies seconds-scale eligibility traces as necessary for error-driven learning in biological circuits.","Related families of algorithms, such as contrastive Hebbian learning \citep{xie2003equivalence}, equilibrium propagation \citep{scellier2017equilibrium}, and other energy-based formulations \citep{hopfield1982neural,bengio2015early}, also instantiate continuous-time learning rules. These approaches rely on symmetric connectivity or two-phase equilibrium dynamics, which differ from the heterosynaptic two-signal rules that are our focus here. While our model shares the spirit of framing learning as a dynamical process, we restrict our focus in this work to error propagation style rules (FA, DFA, KP) and their continuous-time realizations. 

Both neural ODEs~\citep{chen2018neural} and our neural differential-equation model cast network computation as a continuous-time dynamical system, replacing discrete layers/updates with ODE flows over time. In standard neural ODEs, parameters are fixed during the forward solve, and gradients are typically recovered by integrating an adjoint ODE backward in time~\citep{chen2018neural} or by differentiating through the solver~\citep{baydin2018automatic}. By contrast, our model couples learning and inference in one forward-in-time system: both neural states and parameters evolve by ODEs, with weights updated online via locally computed and propagated error terms. 

Concurrently, ``self-assembling’’ heterosynaptic circuits specifying four plastic synapses linking forward and feedback streams have been shown to match backpropagation on standard benchmarks~\citep{liao2024self}. A key insight from this line of work is that error-propagation rules such as feedback-style algorithms (FA, DFA, KP) can emerge without requiring explicit weight symmetry, and only using local rules. This further motivates our focus on these algorithms as being attractive candidates for biologically plausible and hardware-realizable learning. 

Biological relevance of feedback-type algorithms has been argued in various works. For example, \cite{lillicrap2020backpropagation} argued how FA may be realizable in the cerebellum, where the Purkinje cells are the trainable neurons, and the one-to-one climbing fibres carry in the feedback error signals. \cite{koplow2025emergence} computationally showed how these algorithms are consistent with the widely observed Hebbian and anti-Hebbian plasiticities in the brain. However, it is not yet clear whether these algorithm can  work in the continuous-time, and if so, whether they work at biologically reasonable timescales. This is a key question we address.","Related families of algorithms, such as contrastive Hebbian learning \citep{xie2003equivalence}, equilibrium propagation \citep{scellier2017equilibrium}, and other energy-based formulations \citep{hopfield1982neural,bengio2015early}, also instantiate continuous-time learning rules. These approaches rely on symmetric connectivity or two-phase equilibrium dynamics, which differ from the heterosynaptic two-signal rules that are our focus here. While our model shares the spirit of framing learning as a dynamical process, we restrict our focus in this work to error propagation style rules (FA, DFA, KP) and their continuous-time realizations. 

Both neural ODEs~\citep{chen2018neural} and our neural differential-equation model cast network computation as a continuous-time dynamical system, replacing discrete layers/updates with ODE flows over time. In standard neural ODEs, parameters are fixed during the forward solve, and gradients are typically recovered by integrating an adjoint ODE backward in time~\citep{chen2018neural} or by differentiating through the solver~\citep{baydin2018automatic}. By contrast, our model couples learning and inference in one forward-in-time system: both neural states and parameters evolve by ODEs, with weights updated online via locally computed and propagated error terms. 

Concurrently, ``self-assembling’’ heterosynaptic circuits specifying four plastic synapses linking forward and feedback streams have been shown to match backpropagation on standard benchmarks~\citep{liao2024self}. A key insight from this line of work is that error-propagation rules such as feedback-style algorithms (FA, DFA, KP) can emerge without requiring explicit weight symmetry, and only using local rules. This further motivates our focus on these algorithms as being attractive candidates for biologically plausible and hardware-realizable learning. 

Biological relevance of feedback-type algorithms has been argued in various works. For example, \cite{lillicrap2020backpropagation} argued how FA may be realizable in the cerebellum, where the Purkinje cells are the trainable neurons, and the one-to-one climbing fibres carry in the feedback error signals. \cite{koplow2025emergence} computationally showed how these algorithms are consistent with the widely observed Hebbian and anti-Hebbian plasiticities in the brain. However, it is not yet clear whether these algorithm can  work in the continuous-time, and if so, whether they work at biologically reasonable timescales. This is a key question we address.",
2510.25976v1,http://arxiv.org/abs/2510.25976v1,2025-10-29 21:21:54+00:00,Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,"Reconstructing images seen by people from their fMRI brain recordings provides a non-invasive window into the human brain. Despite recent progress enabled by diffusion models, current methods often lack faithfulness to the actual seen images. We present ""Brain-IT"", a brain-inspired approach that addresses this challenge through a Brain Interaction Transformer (BIT), allowing effective interactions between clusters of functionally-similar brain-voxels. These functional-clusters are shared by all subjects, serving as building blocks for integrating information both within and across brains. All model components are shared by all clusters & subjects, allowing efficient training with a limited amount of data. To guide the image reconstruction, BIT predicts two complementary localized patch-level image features: (i)high-level semantic features which steer the diffusion model toward the correct semantic content of the image; and (ii)low-level structural features which help to initialize the diffusion process with the correct coarse layout of the image. BIT's design enables direct flow of information from brain-voxel clusters to localized image features. Through these principles, our method achieves image reconstructions from fMRI that faithfully reconstruct the seen images, and surpass current SotA approaches both visually and by standard objective metrics. Moreover, with only 1-hour of fMRI data from a new subject, we achieve results comparable to current methods trained on full 40-hour recordings.","\label{sec:related}
\vspace{-0.22cm}

fMRI-to-image reconstruction is a well-established field, seeing significant progress in the past decade. Early work mapped fMRI to handcrafted image features 
% \deleted[id=N]{using linear models}
\citep{kay2008identifying,naselaris2009bayesian,nishimoto2011reconstructing}, followed by works that mapped fMRI to deep CNN features 
% \deleted[id=N]{from pretrained networks} 
\citep{gucclu2015deep,zhang2018constraint,shen2019deep}. End-to-end methods have emerged \citep{seeliger2018generative,st2018generative,beliy2019voxels}, with later ones
% {and to cope with limited data per subject, many methods}
predicting latent codes of VAEs or GANs 
% rather than pixels 
\citep{han2019variational,lin2019dcnn,mozafari2020reconstructing,qiao2020biggan,ren2021reconstructing}. Recently, diffusion models have improved realism and faithfulness by turning predicted features into high-quality images \citep{chen2023seeing,ozcelik2023natural,takagi2023high}. In parallel, the community has increasingly focused on integrating information across multiple
subjects to improve generalization under limited data per subject \citep{scotti2024mindeye2,gong2025mindtuner,huo2024neuropictor,xia2024umbrae,ferrante2024through,liu2025see,shen2024neuro}.
Next,
% {In this section,}
we review related efforts along 3 key axes: predicting image features from fMRI, leveraging cross-subject information, and predicting intermediate low-level images. 
% \deleted[id=N]{that capture the image structure (e.g. color, shape, and spatial layout)}.




\vspace*{-0.32cm}
\paragraph{Image Features Prediction.}

Current methods predict image features from fMRI, such as semantic CLIP or 
% low-level
VAE latent embeddings. Most use simple linear models or MLPs \citep{takagi2023high,xia2024dream,wang2024unibrain}; others build on top 
% \deleted[id=N]{of these}
with unCLIP diffusion priors \citep{gong2025mindtuner,scotti2023reconstructing}. A key limitation of these methods is that they typically compress all voxels into a single global fMRI embedding via a fully connected layer before predicting image features. Since visual information is distributed across multiple distinct yet interconnected brain regions, such a projection fails to fully exploit  this distributed nature of the brain.
% \added[id=M]{Such \emph{entire-brain} mapping} underutilizes the distributed nature of brain representations, \added[id=M]{and cannot exploit \emph{local} similarities within and across brains.}
%{limiting their ability to effectively extract visual information from brain activity.} 
Two recent works introduce spatial voxel groupings \citep{huo2024neuropictor,shen2024neuro}, which use voxel patches in shared anatomical space. However, they 
%\deleted[id=M]{still}
predict a single \emph{global} image representation, making it difficult to reconstruct localized image information.
Our approach addresses these issues by forming functionally shared voxel clusters which are mapped directly
%and mapping directly from those clusters 
to localized image features, avoiding projection to a global fMRI embedding and yielding more accurate image-feature predictions.

% A wide range of methods predict image features from fMRI, which may be semantic CLIP embeddings or low-level VAE latents. Most use simple linear models or MLPs \citep{takagi2023high,xia2024dream,wang2024unibrain}; others leverage unCLIP diffusion priors \citep{gong2025mindtuner,scotti2023reconstructing}. A key limitation of these approaches is that they typically compress all voxels into a single global fMRI embedding via a fully connected layer before predicting image features. This under-utilizes the distributed nature of brain representations, limiting their ability to effectively extract visual information from the brain activity. Two recent works introduce spatial voxel groupings \citep{huo2024neuropictor,shen2024neuro} which use voxel patches in shared anatomical space, but they predict a single global image representation, making it hard to reconstruct localized image information.
% Our approach addresses these issues by forming functionally shared voxel clusters and being able to map directly from those clusters to lcoalized image feautres,  avoiding early projection to a global fMRI embedding, yielding more accurate image-feature predictions.

% enabling both inter-cluster interactions (self-attention) and brain–feature interactions (cross-attention) with query tokens. 
% A wide range of methods have been proposed to predict feature embeddings from fMRI, starting with simple linear models and later incorporating generative priors that improved visual quality. Despite this progress, many leading approaches still map the entire set of voxels into a single global embedding via a fully connected layer, which is then further transformed before generating images \citep{scotti2024mindeye2,gong2025mindtuner,xia2024dream}. While this design simplifies decoding, it stands in contrast to the distributed organization of visual processing in the brain \citep{kandel2013principles}. Some methods have introduced spatial groupings of voxels \citep{huo2024neuropictor,shen2024neuro}, yet these too ultimately reduce the signal to a global image embedding before transforming it into images. In contrast, Brain-IT predicts localized image features directly from voxel clusters, enabling finer-grained mappings and overcoming the limitations of a global bottleneck. Our approach predicts spatial details more accurately, leading to reconstructions with higher fidelity and more reliable semantic detail. \added[id=N]{*** (i) I think there is not enough about what other methods did; (ii) I think it's very hard to understand the bottom line and what is exactly the differences and what is it global embedding. ***}

% \deleted[id=R]{
% Early methods predicted CLIP embeddings using simple linear models or MLPs, targeting either CLIP text embeddings \citep{takagi2023high,xia2024dream} or CLIP image embeddings \citep{wang2024unibrain} or both \citep{ozcelik2023natural,wang2024mindbridge,shen2024neuro}. Later works improved these pipelines using unCLIP \citep{ramesh2022hierarchical} diffusion priors \citep{lin2022mind,scotti2024mindeye2,gong2025mindtuner,scotti2023reconstructing} or attention mechanisms~\citep{liu2025see} to generate CLIP image embeddings.
% We predict CLIP image embeddings using our novel Brain Interaction Transformer (BIT). BIT employs a dual design that both utilizes individual voxel activations and enables interactions between brain activations by mapping voxels into meaningful functional clusters. This architecture allows efficient brain interaction, while its shared model weights across voxels and subjects facilitate effective data integration, resulting in more accurate and consistent predictions of the embeddings (e.g., CLIP image embeddings).}


\vspace*{-0.32cm}
\paragraph{Cross-Subject Integration.}
Few prior works support multi-subject training~\citep{lin2022mind,ferrante2024through,gong2025mindtuner,huo2024neuropictor,scotti2024mindeye2}.
% }{. 
% Those that do typically use either linear ridge regression \citep{lin2022mind,ferrante2024through,gong2025mindtuner}, adaptive max pooling \citep{wang2024mindbridge}  or an fMRI-masked autoencoder latent space \citep{huo2024neuropictor}.}
A common aspect of these methods is that alignment is predominantly fMRI-centric: they treat each fMRI scan as a single entity and rely on shared embeddings of entire scans across subjects. As a result, they can only exploit shared representations at the scan level, overlooking the more frequent similarities that exist between individual brain-voxels, both within a single subject and across different subjects. Inspired by the 
%Universal Encoder approach for image-to-fMRI encoding
multi-subject \emph{Image-to-fMRI \underline{Encoder}} of
\citep{beliy2024wisdom}, we adopt a voxel-centric model that shares network weights across all brain-voxels, both within and across subjects. By sharing most model components and leveraging voxel-level similarities, our approach integrates data effectively across subjects and adapts efficiently to new ones even with limited data.
% Since voxel-level similarities are more frequent and meaningful than full-scan similarities, our decoding model is able to more effectively integrate data across subjects. Moreover,  by mapping voxels into shared functional clusters that capture similar functionality across individuals, we leverage cross-subject data resulting in more accurate reconstructions. 
% \deleted[id=M]{We further show that our model adapts effectively to new subjects with limited data, demonstrating strong transfer learning  performance and highlighting the strength of our multi-subject integration and shared representations.}


\vspace*{-0.32cm}
\paragraph{Low-Level Image Reconstruction.}
Most existing methods regress directly from fMRI to the latent space of the diffusion model (e.g. VAE latent space) using MLPs or linear layers \citep{scotti2024mindeye2,gong2025mindtuner,ozcelik2023natural}. Alternatively, NeuroPictor \citep{huo2024neuropictor} 
%takes yet another direction, manipulating 
manipulates internal feature maps within the U-Net of Stable Diffusion using features predicted from fMRI.
Unlike these approaches, our method introduces a complementary low-level branch that predicts VGG-based representations inspired by LPIPS \citep{simonyan2014very,zhang2018perceptual} and inverts them through a Deep Image Prior (DIP) framework. 
% \deleted[id=N]{LPIPS features are shown to be aligned with human perception.}
This setup results in more faithful low-level predictions, reconstructing global structure and low-level visual features.","\vspace{-0.22cm}

fMRI-to-image reconstruction is a well-established field, seeing significant progress in the past decade. Early work mapped fMRI to handcrafted image features 

\citep{kay2008identifying,naselaris2009bayesian,nishimoto2011reconstructing}, followed by works that mapped fMRI to deep CNN features 

\citep{gucclu2015deep,zhang2018constraint,shen2019deep}. End-to-end methods have emerged \citep{seeliger2018generative,st2018generative,beliy2019voxels}, with later ones

predicting latent codes of VAEs or GANs 

\citep{han2019variational,lin2019dcnn,mozafari2020reconstructing,qiao2020biggan,ren2021reconstructing}. Recently, diffusion models have improved realism and faithfulness by turning predicted features into high-quality images \citep{chen2023seeing,ozcelik2023natural,takagi2023high}. In parallel, the community has increasingly focused on integrating information across multiple
subjects to improve generalization under limited data per subject \citep{scotti2024mindeye2,gong2025mindtuner,huo2024neuropictor,xia2024umbrae,ferrante2024through,liu2025see,shen2024neuro}.
Next,

we review related efforts along 3 key axes: predicting image features from fMRI, leveraging cross-subject information, and predicting intermediate low-level images. 





\vspace*{-0.32cm}
\paragraph{Image Features Prediction.}

Current methods predict image features from fMRI, such as semantic CLIP or 

VAE latent embeddings. Most use simple linear models or MLPs \citep{takagi2023high,xia2024dream,wang2024unibrain}; others build on top 

with unCLIP diffusion priors \citep{gong2025mindtuner,scotti2023reconstructing}. A key limitation of these methods is that they typically compress all voxels into a single global fMRI embedding via a fully connected layer before predicting image features. Since visual information is distributed across multiple distinct yet interconnected brain regions, such a projection fails to fully exploit  this distributed nature of the brain.


Two recent works introduce spatial voxel groupings \citep{huo2024neuropictor,shen2024neuro}, which use voxel patches in shared anatomical space. However, they 

predict a single \emph{global} image representation, making it difficult to reconstruct localized image information.
Our approach addresses these issues by forming functionally shared voxel clusters which are mapped directly

to localized image features, avoiding projection to a global fMRI embedding and yielding more accurate image-feature predictions.












\vspace*{-0.32cm}
\paragraph{Cross-Subject Integration.}
Few prior works support multi-subject training~\citep{lin2022mind,ferrante2024through,gong2025mindtuner,huo2024neuropictor,scotti2024mindeye2}.


A common aspect of these methods is that alignment is predominantly fMRI-centric: they treat each fMRI scan as a single entity and rely on shared embeddings of entire scans across subjects. As a result, they can only exploit shared representations at the scan level, overlooking the more frequent similarities that exist between individual brain-voxels, both within a single subject and across different subjects. Inspired by the 

multi-subject \emph{Image-to-fMRI \underline{Encoder}} of
\citep{beliy2024wisdom}, we adopt a voxel-centric model that shares network weights across all brain-voxels, both within and across subjects. By sharing most model components and leveraging voxel-level similarities, our approach integrates data effectively across subjects and adapts efficiently to new ones even with limited data.




\vspace*{-0.32cm}
\paragraph{Low-Level Image Reconstruction.}
Most existing methods regress directly from fMRI to the latent space of the diffusion model (e.g. VAE latent space) using MLPs or linear layers \citep{scotti2024mindeye2,gong2025mindtuner,ozcelik2023natural}. Alternatively, NeuroPictor \citep{huo2024neuropictor} 

manipulates internal feature maps within the U-Net of Stable Diffusion using features predicted from fMRI.
Unlike these approaches, our method introduces a complementary low-level branch that predicts VGG-based representations inspired by LPIPS \citep{simonyan2014very,zhang2018perceptual} and inverts them through a Deep Image Prior (DIP) framework. 

This setup results in more faithful low-level predictions, reconstructing global structure and low-level visual features.",
2511.09590v1,http://arxiv.org/abs/2511.09590v1,2025-11-12 13:52:28+00:00,A Graphical Method for Identifying Gene Clusters from RNA Sequencing Data,"The identification of disease-gene associations is instrumental in understanding the mechanisms of diseases and developing novel treatments. Besides identifying genes from RNA-Seq datasets, it is often necessary to identify gene clusters that have relationships with a disease. In this work, we propose a graph-based method for using an RNA-Seq dataset with known genes related to a disease and perform a robust clustering analysis to identify clusters of genes. Our method involves the construction of a gene co-expression network, followed by the computation of gene embeddings leveraging Node2Vec+, an algorithm applying weighted biased random walks and skipgram with negative sampling to compute node embeddings from undirected graphs with weighted edges. Finally, we perform spectral clustering to identify clusters of genes. All processes in our entire method are jointly optimized for stability, robustness, and optimality by applying Tree-structured Parzen Estimator. Our method was applied to an RNA-Seq dataset of known genes that have associations with Age-related Macular Degeneration (AMD). We also performed tests to validate and verify the robustness and statistical significance of our methods due to the stochastic nature of the involved processes. Our results show that our method is capable of generating consistent and robust clustering results. Our method can be seamlessly applied to other RNA-Seq datasets due to our process of joint optimization, ensuring the stability and optimality of the several steps in our method, including the construction of a gene co-expression network, computation of gene embeddings, and clustering of genes. Our work will aid in the discovery of natural structures in the RNA-Seq data, and understanding gene regulation and gene functions not just for AMD but for any disease in general.","\label{related_work}

\begin{figure*}[t]
    \centering
    \includegraphics[trim=40 200 230 30, clip,width=.8\textwidth]{method_figure.pdf}
    \caption{Overview of our method used to process the subset of bulk mRNA-seq dataset comprising 81 AMD genes to generate a co-expression matrix using CS-CORE, generating a co-expression network by thresholding, embedding to a latent space using \texttt{Node2Vec+}, and finally applying spectral clustering.}
    \label{fig:method_overview}
\end{figure*} 

In recent years, ML models have been successfully applied to RNA-Seq datasets for different diseases, such as breast cancer, colorectal cancer, and other types of cancers, and AMD\cite{cheng_machine_2024, cascianelli_machine_2020, taghizadeh_breast_2022, maurya_transcriptome_2021, jha_identifying_2022, ma2025integrating}. Proposed methods and results show success in stratification of breast cancer into molecular subtypes using multiclass logistic regression \cite{cascianelli_machine_2020}, ML-based identification of differentially expressed genes and independently modulated gene sets \cite{wang_rna-seq_2018, rychel_machine_2020}, identification of transcriptomic features that are commonly shared between cancer types \cite{jha_identifying_2022}, identification of diagnostic biomarkers for colorectal cancer \cite{maurya_transcriptome_2021}, breast cancer prediction with transcriptome profiling \cite{taghizadeh_breast_2022}, and prediction of gene regulatory networks \cite{mochida_statistical_2018}. A wide variety of both supervised and unsupervised classical ML algorithms, such as support vector machines (SVM), Extreme Gradient Boosting (XGBoost), self-organizing maps (SOM), different dimensionality reduction algorithms, such as t-distributed Stochastic Neighbor Embedding (tSNE) and PCA, and clustering algorithms, such as k-means and hierarchical clustering, have been used for the analysis of RNA-Seq datasets \cite{cheng_machine_2024, petegrosso_machine_2019}. The use of graph-based DL and ML algorithms, such as knowledge graphs \cite{torgano_rna_2025} and Node2Vec \cite{grover2016node2vec}, for transcriptomics is also being used in recent years for research in different diseases, including melanoma \cite{wang_single-cell_2021}. 


% \subsection{Statistical GWAS}
% GWAS studies have been performed across many conditions using various computational methods, from statistical methods to more advanced machine/deep learning algorithms \cite{fu2014funseq2, kelemen2025performance}. A common statistical GWAS method is the use of statistical hypothesis testing on expression data between the positive and the control groups \cite{narita2020clustering}. An example study uses k-means clustering in order to find subgroups of homogeneous disease etiologies with a large sample of autism spectrum disorder patients using phenotypic variables in the Simons Simple Collection dataset. They then used these sub-clusters to conduct statistical hypothesis testing between the genetic expression data of positive patients' sub-clusters and the control group \cite{narita2020clustering}. Other General GWAS statistical studies have been applied to areas such as single-nucleotide polymorphism (SNP) detection classification in areas such as cancer research \cite{fu2014funseq2}. For example, FunSeq2 uses a statistically based weighted scoring framework to identify difficult-to-identify genetic mutations in non-coding areas of cancerous tissues' genomes \cite{fu2014funseq2}. The scoring of these genetic mutations into importance allows research to prioritize when to target a mutation in therapeutic efforts \cite{fu2014funseq2}. Other examples of statistical methods are principal component analysis (PCA) on GWAS studies \cite{price2006principal}. Price et al. used PCA to detect and correct expression differences caused by ancestral differences across two different samples (control vs case samples) \cite{price2006principal}. Similarly, statistically based linear mixture models have been used to account for the ancestral differences across samples \cite{dandine2016use}. 

% \subsection{Machine Learning GWAS}
% Classical machine learning GWAS methods include various models such as random forest, logistic regression, and linear regression \cite{goldstein2010application}. A random forest model was used to train and find single-nucleotide polymorphisms (SNPs) in a large genome-wide association dataset \cite{goldstein2010application}. Another study used a large-scale parallel version of a random forest model, which takes genetic expression as input and predicts phenotypical outputs, input feature importance scores, and genetic similarity matrices to identify important SNPs \cite{wang2013random}. Linear regression models have been studied to determine the polygenic risk scores on a certain disease, given a genetic input dataset \cite{kelemen2025performance}. Other machine learning models used for GWAS include support vector machines, random forest, K-Nearest Neighbor, and logistic regression used to prioritize genes relevant to colorectal cancer \cite{kafaie2019network}. Finally, statistical and machine learning methods were used in the GWAS age-related macular degeneration (AMD) study \cite{ma2025integrating}. The statistical methods used for feature selection were first employed to identify important genetic expressions. ANOVA, AUC, and Kruskal using mlr3filters were used to identify genes that allowed identification between different stages of AMD patients, as cited in \cite{ma2025integrating}. Their selected features were then used to fit multiple machine learning models, including neural network, logistic regression, eXtreme Gradient Boosting (XGB), and random forest, leading to models capable of predicting AMD from the transcriptomics dataset \cite{ma2025integrating}. The final finding illuminated AMD-specific genes that distinguished age-related macular degeneration (AMD) patients from baseline control patients \cite{ma2025integrating}.

% \subsection{Deep Learning GWAS}
% Deep learning applications have included graphical and feed-forward based methods on supervised, semi-supervised, and unsupervised learning in numerous GWAS applications \cite{azadifar2022novel}. One such example is the use of a graph convolutional neural network (GCNN) trained to predict disease candidate genes \cite{azadifar2022novel}. This method used the Gene Ontology database to embed the genes into feature vectors and constructs edges using known protein-protein interactions between the transcribed genes' proteins. Finally, the graph is passed through a GNN where each node predicts a binary label if it is a disease-related candidate gene \cite{azadifar2022novel}. Another graphical convolutional neural network uses gene regulatory expression sequence as the input to detect positive spikes in transcription factors that indicate the progression from cognitive impairment to Alzheimer's Disease \cite{rohini2024intelligent}. Finally, neural networks have also been used to embed genetic genes represented as nodes into a latent dimension, allowing them to be used on downstream tasks \cite{ghandikota2022gene2gauss}. A GCNN model used multiple lung transcriptomic datasets to study idiopathic pulmonary fibrosis \cite{ghandikota2022gene2gauss}. The paper constructed co-expression graphical presentations of the genes in the dataset and passed them through the GCNN, which is trained to predict the $\mu$ and $\Sigma$ parameters of a Gaussian distribution. Network maximizes the similarity between the Gaussian parameters of other genes connected to each output gene embedding while pushing non-connected genes' Gaussian parameters away \cite{ghandikota2022gene2gauss}. This study focuses on a new style of genetic embedding with similar objectives to those previously described.","In recent years, ML models have been successfully applied to RNA-Seq datasets for different diseases, such as breast cancer, colorectal cancer, and other types of cancers, and AMD\cite{cheng_machine_2024, cascianelli_machine_2020, taghizadeh_breast_2022, maurya_transcriptome_2021, jha_identifying_2022, ma2025integrating}. Proposed methods and results show success in stratification of breast cancer into molecular subtypes using multiclass logistic regression \cite{cascianelli_machine_2020}, ML-based identification of differentially expressed genes and independently modulated gene sets \cite{wang_rna-seq_2018, rychel_machine_2020}, identification of transcriptomic features that are commonly shared between cancer types \cite{jha_identifying_2022}, identification of diagnostic biomarkers for colorectal cancer \cite{maurya_transcriptome_2021}, breast cancer prediction with transcriptome profiling \cite{taghizadeh_breast_2022}, and prediction of gene regulatory networks \cite{mochida_statistical_2018}. A wide variety of both supervised and unsupervised classical ML algorithms, such as support vector machines (SVM), Extreme Gradient Boosting (XGBoost), self-organizing maps (SOM), different dimensionality reduction algorithms, such as t-distributed Stochastic Neighbor Embedding (tSNE) and PCA, and clustering algorithms, such as k-means and hierarchical clustering, have been used for the analysis of RNA-Seq datasets \cite{cheng_machine_2024, petegrosso_machine_2019}. The use of graph-based DL and ML algorithms, such as knowledge graphs \cite{torgano_rna_2025} and Node2Vec \cite{grover2016node2vec}, for transcriptomics is also being used in recent years for research in different diseases, including melanoma \cite{wang_single-cell_2021}.",
2510.25132v1,http://arxiv.org/abs/2510.25132v1,2025-10-29 03:22:32+00:00,EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation,"Designing enzyme backbones with substrate-specific functionality is a critical challenge in computational protein engineering. Current generative models excel in protein design but face limitations in binding data, substrate-specific control, and flexibility for de novo enzyme backbone generation. To address this, we introduce EnzyBind, a dataset with 11,100 experimentally validated enzyme-substrate pairs specifically curated from PDBbind. Building on this, we propose EnzyControl, a method that enables functional and substrate-specific control in enzyme backbone generation. Our approach generates enzyme backbones conditioned on MSA-annotated catalytic sites and their corresponding substrates, which are automatically extracted from curated enzyme-substrate data. At the core of EnzyControl is EnzyAdapter, a lightweight, modular component integrated into a pretrained motif-scaffolding model, allowing it to become substrate-aware. A two-stage training paradigm further refines the model's ability to generate accurate and functional enzyme structures. Experiments show that our EnzyControl achieves the best performance across structural and functional metrics on EnzyBind and EnzyBench benchmarks, with particularly notable improvements of 13\% in designability and 13\% in catalytic efficiency compared to the baseline models. The code is released at https://github.com/Vecteur-libre/EnzyControl.","\textbf{Enzyme Design Applications.}\quad Designing effective enzymes remains a core challenge, particularly in identifying active sites and optimizing functional properties. Common strategies fall into three main categories: semi-rational design \cite{smi1,smi2,smi3}, rational design \cite{rat1,rat2,rat3}, and de novo design \cite{enzygen}. Semi-rational design leverages known structures and experimental data to guide site-directed mutagenesis near the catalytic site \cite{smi4,smi5}, with residues selected based on structural insights and prior knowledge \cite{smi6}. Rational design relies more heavily on computational modeling \cite{rat4}, using tools such as molecular dynamics and quantum mechanical simulations to explore enzyme–substrate interactions and reaction mechanisms \cite{rat5,rat6}. Both approaches aim to improve or repurpose natural enzymes. In contrast, de novo design constructs entirely new enzymes by embedding catalytic motifs into synthetic scaffolds, often simplifying structure to focus on function \cite{ahern2025atom}. EnzyControl bridges rational and de novo design with a modular adapter for substrate-aware enzyme backbone generation, and leverages MSA to improve the efficiency of active site annotation.

\textbf{Methods for Motif-Scaffolding Task.}\quad The task is to create proteins with functional properties conferred through a prespecified arrangement of residues known as a motif. The problem is to design the remainder of the protein, called the scaffold, that harbors the motif. Early approaches to the motif-scaffolding problem primarily relied on assembling scaffolds from pre-defined protein fragments. These methods are limited by their dependence on finding suitable matches within the Protein Data Bank (PDB) and often struggle to accommodate slight structural mismatches between the scaffold and the motif \cite{silva2016motif,yang2021bottom,sesterhenn2020novo,linsky2020novo}. More recent models, such as RFdiffusion \cite{rfdiffusion} and FrameFlow \cite{frameflow}, represent a shift toward generative modeling. These approaches use diffusion or flow matching models \cite{mf1,mf2,mf3} conditioned on the motif’s structure and/or sequence, while generating only the surrounding scaffold. However, they cannot incorporate additional design constraints such as substrate specificity. Our method addresses this limitation by enabling scaffold generation conditioned on a broader range of inputs, expanding the applicability of motif-scaffolding.","\textbf{Enzyme Design Applications.}\quad Designing effective enzymes remains a core challenge, particularly in identifying active sites and optimizing functional properties. Common strategies fall into three main categories: semi-rational design \cite{smi1,smi2,smi3}, rational design \cite{rat1,rat2,rat3}, and de novo design \cite{enzygen}. Semi-rational design leverages known structures and experimental data to guide site-directed mutagenesis near the catalytic site \cite{smi4,smi5}, with residues selected based on structural insights and prior knowledge \cite{smi6}. Rational design relies more heavily on computational modeling \cite{rat4}, using tools such as molecular dynamics and quantum mechanical simulations to explore enzyme–substrate interactions and reaction mechanisms \cite{rat5,rat6}. Both approaches aim to improve or repurpose natural enzymes. In contrast, de novo design constructs entirely new enzymes by embedding catalytic motifs into synthetic scaffolds, often simplifying structure to focus on function \cite{ahern2025atom}. EnzyControl bridges rational and de novo design with a modular adapter for substrate-aware enzyme backbone generation, and leverages MSA to improve the efficiency of active site annotation.

\textbf{Methods for Motif-Scaffolding Task.}\quad The task is to create proteins with functional properties conferred through a prespecified arrangement of residues known as a motif. The problem is to design the remainder of the protein, called the scaffold, that harbors the motif. Early approaches to the motif-scaffolding problem primarily relied on assembling scaffolds from pre-defined protein fragments. These methods are limited by their dependence on finding suitable matches within the Protein Data Bank (PDB) and often struggle to accommodate slight structural mismatches between the scaffold and the motif \cite{silva2016motif,yang2021bottom,sesterhenn2020novo,linsky2020novo}. More recent models, such as RFdiffusion \cite{rfdiffusion} and FrameFlow \cite{frameflow}, represent a shift toward generative modeling. These approaches use diffusion or flow matching models \cite{mf1,mf2,mf3} conditioned on the motif’s structure and/or sequence, while generating only the surrounding scaffold. However, they cannot incorporate additional design constraints such as substrate specificity. Our method addresses this limitation by enabling scaffold generation conditioned on a broader range of inputs, expanding the applicability of motif-scaffolding.",
2510.18037v2,http://arxiv.org/abs/2510.18037v2,2025-10-20 19:19:29+00:00,Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity,"Neural activity forecasting is central to understanding neural systems and enabling closed-loop control. While deep learning has recently advanced the state-of-the-art in the time series forecasting literature, its application to neural activity forecasting remains limited. To bridge this gap, we systematically evaluated eight probabilistic deep learning models, including two foundation models, that have demonstrated strong performance on general forecasting benchmarks. We compared them against four classical statistical models and two baseline methods on spontaneous neural activity recorded from mouse cortex via widefield imaging. Across prediction horizons, several deep learning models consistently outperformed classical approaches, with the best model producing informative forecasts up to 1.5 seconds into the future. Our findings point toward future control applications and open new avenues for probing the intrinsic temporal structure of neural activity.","Forecasting has long been an important approach in studying neural dynamics. 
Many earlier models, while not explicitly trained to optimize forecasting accuracy, are generative in nature and therefore can produce one-step-ahead or multi-step-ahead forecasts in an autoregressive fashion (e.g. \cite{curto2009simple,glaser2020recurrent,sani2021modeling}). As forecasting was only a secondary objective in these works, evaluations were generally restricted to a fixed prediction horizon, without systematic assessment across multiple horizons, and lack comparisons against forecasting baselines, such as the average of past activity or models capable of making direct-multi-step forecasts. Another major line of forecasting studies in neuroscience is motivated by brain-machine interface applications, where predicting neural activity is closely tied to predicting behavior. In this setting, models have been trained to solely predict behavior \cite{azabou2023unified,ryoo2025generalizable,azabou2025multisession} or to jointly predict neural activity and behavior \cite{sani2024dissociative,vahidi2025braid}. However, since such experiments often involve highly structured tasks (e.g., a monkey reaching to target), the resulting neural dynamics tend to be stereotyped (e.g., with rotational structure\cite{pandarinath2018inferring}), making them easier to forecast compared to less structured scenarios such as spontaneous activity. Forecasting accuracy has also been combined with other performance measures for both training and evaluation, for example in \cite{pei2021neural,zhang2024towards}.

More recently, a growing number of models have begun explicitly adopting forecasting accuracy as their primary training objective. For example, \cite{li2023amag} proposed a graph neural network based model for multi-channel neural activity forecasting; \cite{wagenmaker2024active} used a low-rank linear autoregressive model to predict neural responses to holographic photostimulation; \cite{antoniades2024neuroformer} developed a transformer-based model leveraging multi-modal inputs to autoregressively predict neural responses to visual stimuli; \cite{filipe2025one} introduced a diffusion-based model for joint forecasting of neural activity and behavior across sessions and subjects; \cite{immer2025forecasting} proposed a convolutional neural network based video model to directly forecast future frames of neural imaging videos; and \cite{duan2025poco} demonstrated forecasting of spontaneous neural activity across multiple sessions and subjects with a forecaster in the form of multilayer perceptron. While presented as neuroscience-specific applications, these models are fundamentally instances of general time series forecasting methods. Nevertheless, only a few (\cite{immer2025forecasting, duan2025poco}) have compared their performance against models from the broader time series forecasting literature, and even in those cases, the comparisons were limited. For example no recent foundation models (e.g., \cite{ansari2024chronos,woo2024unified}) were included. Moreover, all of these models produce only point forecasts. However, given the measurement and intrinsic noise in neural data \cite{faisal2008noise}, probabilistic forecasting \cite{gneiting2014probabilistic} -- which provides prediction intervals to quantify uncertainty -- is particularly important, but remains unexplored in the context of neural time series forecasting. 


To fill these gaps, we systematically benchmarked common baseline methods, classical statistical approaches, and state-of-the-art deep learning models from the probabilistic time series forecasting literature on spontaneous mice neural activity. By drawing on the broader forecasting literature, we aim to identify strong model backbones that future neuroscience-specific applications can build upon for more accurate and uncertainty-aware neural activity predictions. 


% While some earlier models did not use forecasting accuracy as the training objective, they are generative models so they have been evaluated for forecasting (e.g. \cite{curto2009simple,glaser2020recurrent,sani2021modeling}). However, as forecasting is only a side objective,
% typically they only considered forecasting some fixed steps into the future and the model's ability to forecast longer or shorter is not explored. Another important line of neural forecasting is rooted in brain-machine interaction (BCI). There forecasting neural activity is also often associated with behavior prediction, with models trained solely on behavior prediction accuracy\cite{azabou2023unified,ryoo2025generalizable,azabou2025multisession} or combined neural activity and behavior forecasting accuracy\cite{sani2024dissociative,vahidi2025braid}. However, as structured behavior is performed (e.g. monkey reaching task) in these experiments, neural activity also exhibits stereotypical dynamics (e.g. rotational), and thus neural activity may be easier to forecast than in a less structured setting (e.g. spontaneous). Forecasting accuracy has also been combined with other metrics and used for model training and evalution, for example, \cite{pei2021neural,zhang2024towards}.

% Lately, as perhaps driven by successful generative models for text and videos, more models using forecasting as the main training objective start to emerge. \cite{li2023amag} proposed a graph neural network based model to forecast neural activity recorded from multiple channels while accounting for their interactions. \cite{antoniades2024neuroformer} proposed a transformer-based model that leverages multiple modalities to autoregressively predict neural response to stimuli. \cite{filipe2025one} proposed a diffusion based model to jointly forecast neural activity and behavior across multiple sessions and subjects. \cite{immer2025forecasting} proposed a convolution neural network based video model to directly predict future frames of activity recording video, without the need to extract time series of individual neurons or regions from the video first. \cite{duan2025poco} proposed a MLP-based model with a population encoder to forecast activity of neural population across multiple sessions and subjects. Despite their neuroscience-specific designs, in terms of forecasting accuracy except\cite{duan2025poco} none of the models compare themselves to state-of-the-art models from the time series forecasting field, and none of them compared to the time series foundation models (e.g. \cite{ansari2024chronos, woo2024unified}). Moreover, all of the models focused on generating point forecasts. However, given the noisy nature of neural data, both measurement noise and intrinsic noise\cite{faisal2008noise}, probabilistic forecasting\cite{gneiting2014probabilistic}, which generates a prediction interval, is particularly important, but yet unexplored for neural forecasting. 


%%%%%%% older
% Forecasting plays an important role in guiding the development of new models for neural dynamics. Some models were solely trained to optimize neural activity forecasting accuracy (e.g. \cite{li2023amag,duan2025poco}), some models were trained by the forecasting loss combined with other losses, such as reconstruction \cite{zhang2024towards} or behavior prediction \cite{sani2024dissociative,vahidi2025braid}. Some models did not use forecasting loss when fitting, but were generative and were evaluated for forecasting afterwards (e.g. \cite{curto2009simple,glaser2020recurrent,sani2021modeling}). Some models were trained and evaluated on behavior forecasting accuracy directly (e.g. \cite{azabou2023unified,ryoo2025generalizable,azabou2025multisession}). Forecasting accuracy has also been used among the evaluation metrics in popular benchmarks\cite{pei2021neural}.
% However, some aspects remain unexplored. First, most of them only considered a fixed number of forecasting steps, and did not study how forecastability varies with prediction length. Second, most of them are tested on scenarios where a stereotypical behavior is performed (e.g. monkey reaching task). In these cases neural activity also exhibits stereotypical dynamics, and thus how well neural activity can be forecast in a less structured setting (e.g. spontaneous) remain unclear. Finally, none of these models compared their forecasting accuracy with the popular models other than the linear autoregressive model from the time series forecasting domain. Lately more works on incorporating advances from time series forecasting domain to neural activity forecasting have emerged. \cite{pankka2025enhanced} reported improved forecasts of resting-state EEG signals using WaveNet, a deep neural network based model, compared to traditional linear autoregressive models. \cite{lueckmann2025zapbench} built the first benchmark on neural time series forecasting from calcium activity recorded from a larval zebrafish brain. Through some initial comparison between one linear and four nonlinear models, they observed that when long history context is employed to generate forecasts, nonlinear deep learning based models, especially one that works with activity recording video directly\cite{immer2025forecasting},  generally outperform the linear model. \cite{duan2025poco} proposed a MLP-based model with a population encoder to forecast the activity of a neural population and can be trained across multiple sessions and subjects. However, they did not compare to the state-of-art forecasting models, and did not perform probabilistic forecasting. 
% \cite{antoniades2024neuroformer} proposed a transformer-based model that leverages multiple modalities to autoregressively predict neural response to stimuli. \cite{filipe2025one} proposed a diffusion based model to jointly forecast neural activity and behavior across multiple sessions and subjects.  





% The first and the only, at the time of writing and to the best of our knowledge, benchmark on neural time series forecasting was presented in \cite{lueckmann2025zapbench}, which compared linear and four nonlinear models for forecasting calcium activity recorded from a larval zebrafish brain. It was observed that when long history context is employed to generate forecasts, nonlinear deep learning based models generally outperform the linear model. Similarly, \cite{pankka2025enhanced} reported improved forecasts of resting-state EEG signals using deep neural network compared to traditional linear autoregressive models. 


% which are in fact reported to be promising for neural activity forecasting\cite{pankka2025enhanced}. \cite{lueckmann2025zapbench} presents the first benchmark focused on neural activity forecasting. 

% \cite{curto2009simple} modeled neural activity as a dynamical system using the classical FitzHugh-Nagumo equations, and showed that model parameters fitted on the spontaneous activity before stimulus onset can be used to predict the subsequent stimulus-evoked response. \cite{glaser2020recurrent} proposed a dynamical systems model which leverages recurrence, switching, and interaction between neural populations, to characterize and forecast neural activity with higher accuracy. \cite{sani2021modeling} proposed a dynamic linear state space model to jointly model the temporal evolution of neural activity and behavior. \cite{li2023amag} proposed a graph neural network based model to forecast neural activity recorded from multiple channels while accounting for their interactions. 
 

% Understanding dynamics: Studying neural system as dynamical system (Curto, Glaser (mp-rSLDS), LFADS, Golub, Bing DMD, Laura) has led to new insights. However, these works typically focus on the dynamical structures such as fixed points and attractors underlying repeated trials. An important aspect of dynamics, the forecastability of the system, has been not studied enough. Notably, Curto and Glaser did touch on forecastability. Glaser showed that including multi-region interaction improved the predicted log likehood of future data. Curto These works did not compare to other models.   and assessed model forecasts in terms of the amount of external drive needed to achieve the observed response. 

% Forecast accuracy is often seen among the metrics for model evaluation (universal translator, neural latents benchmark, zebrafish benchmark). However, except in the zebrafish benchmark, forecasting accuracy is typically used as a secondary metrics with a fixed forecasting horizon, and these works did not systematically investigate how models are doing across different forecast horizons. Some argue that neural data is too chaotic and there are too many unseen inputs, and thus it does not make sense to forecast. However, we believe a characterization of the forecastable time scale is important, especially with the new forecasting models. Some models are trained with forecasting as the main objective, such as AMAG. However, it was trained on neural recording from monkeys doing stereotypical reaching task, so the dynamics are stereotypical and easier to predict. Moreover, it did not study multiple forecasting horizons and did not compare to other time series forecasting models. 


% With the increase in neural data and advances in dynamical systems, many insights of neural activity has been gained by studying neuronal systems as dynamical systems. Dynamical structures, such as attractors, have been discovered in neural activity. However, little is known about the forecastability of neural time series (except one recent work). Understanding the forecastability of neural activity not only provides insights into the time scales underlying brain functions, but is also of great significance for designing effective controller of brain activity. Meanwhile, the field of time series forecasting is being revolutionized with deep learning models. We systematically study the forecastability of neural time series, using both classical statistical models and state of the art deep learning based models.","Forecasting has long been an important approach in studying neural dynamics. 
Many earlier models, while not explicitly trained to optimize forecasting accuracy, are generative in nature and therefore can produce one-step-ahead or multi-step-ahead forecasts in an autoregressive fashion (e.g. \cite{curto2009simple,glaser2020recurrent,sani2021modeling}). As forecasting was only a secondary objective in these works, evaluations were generally restricted to a fixed prediction horizon, without systematic assessment across multiple horizons, and lack comparisons against forecasting baselines, such as the average of past activity or models capable of making direct-multi-step forecasts. Another major line of forecasting studies in neuroscience is motivated by brain-machine interface applications, where predicting neural activity is closely tied to predicting behavior. In this setting, models have been trained to solely predict behavior \cite{azabou2023unified,ryoo2025generalizable,azabou2025multisession} or to jointly predict neural activity and behavior \cite{sani2024dissociative,vahidi2025braid}. However, since such experiments often involve highly structured tasks (e.g., a monkey reaching to target), the resulting neural dynamics tend to be stereotyped (e.g., with rotational structure\cite{pandarinath2018inferring}), making them easier to forecast compared to less structured scenarios such as spontaneous activity. Forecasting accuracy has also been combined with other performance measures for both training and evaluation, for example in \cite{pei2021neural,zhang2024towards}.

More recently, a growing number of models have begun explicitly adopting forecasting accuracy as their primary training objective. For example, \cite{li2023amag} proposed a graph neural network based model for multi-channel neural activity forecasting; \cite{wagenmaker2024active} used a low-rank linear autoregressive model to predict neural responses to holographic photostimulation; \cite{antoniades2024neuroformer} developed a transformer-based model leveraging multi-modal inputs to autoregressively predict neural responses to visual stimuli; \cite{filipe2025one} introduced a diffusion-based model for joint forecasting of neural activity and behavior across sessions and subjects; \cite{immer2025forecasting} proposed a convolutional neural network based video model to directly forecast future frames of neural imaging videos; and \cite{duan2025poco} demonstrated forecasting of spontaneous neural activity across multiple sessions and subjects with a forecaster in the form of multilayer perceptron. While presented as neuroscience-specific applications, these models are fundamentally instances of general time series forecasting methods. Nevertheless, only a few (\cite{immer2025forecasting, duan2025poco}) have compared their performance against models from the broader time series forecasting literature, and even in those cases, the comparisons were limited. For example no recent foundation models (e.g., \cite{ansari2024chronos,woo2024unified}) were included. Moreover, all of these models produce only point forecasts. However, given the measurement and intrinsic noise in neural data \cite{faisal2008noise}, probabilistic forecasting \cite{gneiting2014probabilistic} -- which provides prediction intervals to quantify uncertainty -- is particularly important, but remains unexplored in the context of neural time series forecasting. 


To fill these gaps, we systematically benchmarked common baseline methods, classical statistical approaches, and state-of-the-art deep learning models from the probabilistic time series forecasting literature on spontaneous mice neural activity. By drawing on the broader forecasting literature, we aim to identify strong model backbones that future neuroscience-specific applications can build upon for more accurate and uncertainty-aware neural activity predictions.",
2509.19586v1,http://arxiv.org/abs/2509.19586v1,2025-09-23 21:23:36+00:00,A Foundation Chemical Language Model for Comprehensive Fragment-Based Drug Discovery,"We introduce FragAtlas-62M, a specialized foundation model trained on the largest fragment dataset to date. Built on the complete ZINC-22 fragment subset comprising over 62 million molecules, it achieves unprecedented coverage of fragment chemical space. Our GPT-2 based model (42.7M parameters) generates 99.90% chemically valid fragments. Validation across 12 descriptors and three fingerprint methods shows generated fragments closely match the training distribution (all effect sizes < 0.4). The model retains 53.6% of known ZINC fragments while producing 22% novel structures with practical relevance. We release FragAtlas-62M with training code, preprocessed data, documentation, and model weights to accelerate adoption.","\label{sec:related_work}

\subsection{Chemical Language Models}

Several chemical language models directly relevant to molecular generation have been proposed. MolGPT \cite{bagal2021molgpt} adapts GPT architectures for molecule generation, ChemBERTa \cite{chithrananda2020chemberta} applies BERT-style pretraining to molecular representations, and FragGPT \cite{yue2024unlocking} explores unordered chemical language for molecular design. These models primarily target full-sized drug molecules rather than fragments.

Recent work has also examined how non-canonical or invalid SMILES can act as data augmentation, improving model robustness \cite{skinnider2024invalid}; FragAtlas-62M preserves non-canonical SMILES during training and enforces validity only at evaluation to capture this benefit.

\subsection{Fragment-Based Drug Discovery}

Fragment-based drug discovery (FBDD) has emerged as a promising approach in medicinal chemistry \cite{xu2025fragment, keseru2016design}, but computational tools specifically designed for fragment generation remain limited. FBDD offers several advantages over traditional high-throughput screening, including more efficient exploration of chemical space and higher hit rates for challenging targets. The ZINC database \cite{tingle2023zinc} is a key resource, providing commercially accessible molecules from billion-scale libraries, including fragment collections available for experimental validation.

While computational methods have been applied to fragment-based approaches \cite{bian2018computational}, they have largely focused on virtual screening or fragment linking rather than de novo generation. Some studies have explored using generative models with fragments as building blocks \cite{powers2023geometric, podda2020deep}, fragment-based sequential translation \cite{chen2021fragment}, or fragment retrieval augmentation \cite{lee2024molecule}. Frag2Seq \cite{fu2024fragment} offers a promising approach that combines fragment tokenization with geometric information for structure-based design, while FragGPT \cite{yue2024unlocking} leverages unordered chemical language for comprehensive molecular design. However, none of these approaches have developed foundation models that comprehensively address the specific needs of fragment-based approaches at the scale of the entire ZINC fragment database.

\begin{figure}[t]
\centering
\FloatBarrier
\begin{tabular}{@{}c@{}ccc@{}}
& \small{(A) QED} & \small{(B) LogP} & \small{(C) Molecular Weight} \\
\raisebox{1.5cm}{\rotatebox{90}{\textbf{Density}}} &
\includegraphics[width=0.30\textwidth]{imgs/individual_plots/QED_distribution.png} &
\includegraphics[width=0.30\textwidth]{imgs/individual_plots/LogP_distribution.png} &
\includegraphics[width=0.30\textwidth]{imgs/individual_plots/MW_distribution.png} \\
& \small{(D) H-Bond Donors} & \small{(E) Heavy Atom Count} & \small{(F) TPSA} \\
&
\includegraphics[width=0.30\textwidth]{imgs/individual_plots/HBD_distribution.png} &
\includegraphics[width=0.30\textwidth]{imgs/individual_plots/HeavyAtomCount_distribution.png} &
\includegraphics[width=0.30\textwidth]{imgs/individual_plots/TPSA_distribution.png}
\end{tabular}
\caption{Distribution plots for six molecular properties comparing generated (blue) and training (orange) molecules: (A) QED, (B) LogP, (C) Molecular weight, (D) H-bond donors, (E) Heavy atom count, (F) TPSA.}
\label{fig:property_distributions}
\end{figure}","\subsection{Chemical Language Models}

Several chemical language models directly relevant to molecular generation have been proposed. MolGPT \cite{bagal2021molgpt} adapts GPT architectures for molecule generation, ChemBERTa \cite{chithrananda2020chemberta} applies BERT-style pretraining to molecular representations, and FragGPT \cite{yue2024unlocking} explores unordered chemical language for molecular design. These models primarily target full-sized drug molecules rather than fragments.

Recent work has also examined how non-canonical or invalid SMILES can act as data augmentation, improving model robustness \cite{skinnider2024invalid}; FragAtlas-62M preserves non-canonical SMILES during training and enforces validity only at evaluation to capture this benefit.

\subsection{Fragment-Based Drug Discovery}

Fragment-based drug discovery (FBDD) has emerged as a promising approach in medicinal chemistry \cite{xu2025fragment, keseru2016design}, but computational tools specifically designed for fragment generation remain limited. FBDD offers several advantages over traditional high-throughput screening, including more efficient exploration of chemical space and higher hit rates for challenging targets. The ZINC database \cite{tingle2023zinc} is a key resource, providing commercially accessible molecules from billion-scale libraries, including fragment collections available for experimental validation.

While computational methods have been applied to fragment-based approaches \cite{bian2018computational}, they have largely focused on virtual screening or fragment linking rather than de novo generation. Some studies have explored using generative models with fragments as building blocks \cite{powers2023geometric, podda2020deep}, fragment-based sequential translation \cite{chen2021fragment}, or fragment retrieval augmentation \cite{lee2024molecule}. Frag2Seq \cite{fu2024fragment} offers a promising approach that combines fragment tokenization with geometric information for structure-based design, while FragGPT \cite{yue2024unlocking} leverages unordered chemical language for comprehensive molecular design. However, none of these approaches have developed foundation models that comprehensively address the specific needs of fragment-based approaches at the scale of the entire ZINC fragment database.",
2510.21142v1,http://arxiv.org/abs/2510.21142v1,2025-10-24 04:26:41+00:00,In Silico Mapping of Visual Categorical Selectivity Across the Whole Brain,"A fine-grained account of functional selectivity in the cortex is essential for understanding how visual information is processed and represented in the brain. Classical studies using designed experiments have identified multiple category-selective regions; however, these approaches rely on preconceived hypotheses about categories. Subsequent data-driven discovery methods have sought to address this limitation but are often limited by simple, typically linear encoding models. We propose an in silico approach for data-driven discovery of novel category-selectivity hypotheses based on an encoder-decoder transformer model. The architecture incorporates a brain-region to image-feature cross-attention mechanism, enabling nonlinear mappings between high-dimensional deep network features and semantic patterns encoded in the brain activity. We further introduce a method to characterize the selectivity of individual parcels by leveraging diffusion-based image generative models and large-scale datasets to synthesize and select images that maximally activate each parcel. Our approach reveals regions with complex, compositional selectivity involving diverse semantic concepts, which we validate in silico both within and across subjects. Using a brain encoder as a ""digital twin"" offers a powerful, data-driven framework for generating and testing hypotheses about visual selectivity in the human brain - hypotheses that can guide future fMRI experiments. Our code is available at: https://kriegeskorte-lab.github.io/in-silico-mapping/ .","\paragraph{Semantic mapping.} Our work builds upon a growing body of computational modeling and machine learning research that investigates how semantic information is represented in the higher visual cortex \citep{Kriegeskorte2008matching}. Some approaches leverage large image datasets to build decoders \citep{doerig2022semantic,ferrante2023brain,takagi2022high,kay2008identifying,scotti2024mindeye2,yeung2024neural,chen2022seeing,liu2023brainclip} or models for generating optimal stimuli \citep{Cerdas2024.10.29.620889, luo_brain_2023,luo2024brainscuba,gu_neurogen_2022,ratan2021computational,matsuyama_lavca_2025}, while others use cross-domain (e.g. vision-language) mapping \citep{huth_continuous_2012,huth_natural_2016,kell_task-optimized_2018,barros-loscertales_reading_2012,nierhaus_content_2023,okumura_semantic_2024,pereira_toward_2018,simanova_modality-independent_2014, horikawa_neural_2020}. These studies face a key challenge of dataset size, since the collection of neural data is often expensive and time-consuming. Our work seeks to address this constraint by using encoders trained on large-scale datasets to perform in silico mapping. This allows us to expand the set of concepts that can be probed, beyond those stimuli shown to the subjects. 

\paragraph{Brain encoding models.} Highlighting its importance, several community-driven efforts have sought to benchmark models predicting brain responses \citep{schrimpf2018brain,turishcheva2024dynamic,gifford2023algonauts}. With the release of increasingly large neural datasets each year, researchers have introduced novel architectures and methodologies to improve the accuracy of brain encoding model, including leveraging multiple datasets and pretrained networks \citep{adeli_predicting_2023,Li2022.11.06.515387,yang2024brain,beliy2025wisdomcrowdbrainsuniversal,Yamins2016,qian2024fmripte,Freteault_alignment_2025,kriegeskorte2015deep,tang_jerry_brain_encoding,azabou2023unifiedscalableframeworkneural}. While our paper uses a cutting-edge encoding model \citep{adeli_transformer_2025}, our pipeline is ultimately encoder-agnostic, and can use any encoder that is image-computable. As researchers build better brain encoders, we expect that the space of hypotheses our pipeline could generate and their accuracy will only grow.

\paragraph{Brain-optimized stimuli.} Previous studies have introduced encoding model-based stimulus selection and empirically validated the superstimuli in non-human primates and mice \citep{bashivan2019neural, walker_neural_2020, pierzchlewicz2023energy, franke2025dual}. In this work, we extend this general approach of stimulus optimization for studying neural populations to the fMRI domain, revealing high-level human-specific selectivity beyond the visual cortex.","\paragraph{Semantic mapping.} Our work builds upon a growing body of computational modeling and machine learning research that investigates how semantic information is represented in the higher visual cortex \citep{Kriegeskorte2008matching}. Some approaches leverage large image datasets to build decoders \citep{doerig2022semantic,ferrante2023brain,takagi2022high,kay2008identifying,scotti2024mindeye2,yeung2024neural,chen2022seeing,liu2023brainclip} or models for generating optimal stimuli \citep{Cerdas2024.10.29.620889, luo_brain_2023,luo2024brainscuba,gu_neurogen_2022,ratan2021computational,matsuyama_lavca_2025}, while others use cross-domain (e.g. vision-language) mapping \citep{huth_continuous_2012,huth_natural_2016,kell_task-optimized_2018,barros-loscertales_reading_2012,nierhaus_content_2023,okumura_semantic_2024,pereira_toward_2018,simanova_modality-independent_2014, horikawa_neural_2020}. These studies face a key challenge of dataset size, since the collection of neural data is often expensive and time-consuming. Our work seeks to address this constraint by using encoders trained on large-scale datasets to perform in silico mapping. This allows us to expand the set of concepts that can be probed, beyond those stimuli shown to the subjects. 

\paragraph{Brain encoding models.} Highlighting its importance, several community-driven efforts have sought to benchmark models predicting brain responses \citep{schrimpf2018brain,turishcheva2024dynamic,gifford2023algonauts}. With the release of increasingly large neural datasets each year, researchers have introduced novel architectures and methodologies to improve the accuracy of brain encoding model, including leveraging multiple datasets and pretrained networks \citep{adeli_predicting_2023,Li2022.11.06.515387,yang2024brain,beliy2025wisdomcrowdbrainsuniversal,Yamins2016,qian2024fmripte,Freteault_alignment_2025,kriegeskorte2015deep,tang_jerry_brain_encoding,azabou2023unifiedscalableframeworkneural}. While our paper uses a cutting-edge encoding model \citep{adeli_transformer_2025}, our pipeline is ultimately encoder-agnostic, and can use any encoder that is image-computable. As researchers build better brain encoders, we expect that the space of hypotheses our pipeline could generate and their accuracy will only grow.

\paragraph{Brain-optimized stimuli.} Previous studies have introduced encoding model-based stimulus selection and empirically validated the superstimuli in non-human primates and mice \citep{bashivan2019neural, walker_neural_2020, pierzchlewicz2023energy, franke2025dual}. In this work, we extend this general approach of stimulus optimization for studying neural populations to the fMRI domain, revealing high-level human-specific selectivity beyond the visual cortex.",
2510.25807v1,http://arxiv.org/abs/2510.25807v1,2025-10-29 08:52:55+00:00,Discovering Interpretable Biological Concepts in Single-cell RNA-seq Foundation Models,"Single-cell RNA-seq foundation models achieve strong performance on downstream tasks but remain black boxes, limiting their utility for biological discovery. Recent work has shown that sparse dictionary learning can extract concepts from deep learning models, with promising applications in biomedical imaging and protein models. However, interpreting biological concepts remains challenging, as biological sequences are not inherently human-interpretable. We introduce a novel concept-based interpretability framework for single-cell RNA-seq models with a focus on concept interpretation and evaluation. We propose an attribution method with counterfactual perturbations that identifies genes that influence concept activation, moving beyond correlational approaches like differential expression analysis. We then provide two complementary interpretation approaches: an expert-driven analysis facilitated by an interactive interface and an ontology-driven method with attribution-based biological pathway enrichment. Applying our framework to two well-known single-cell RNA-seq models from the literature, we interpret concepts extracted by Top-K Sparse Auto-Encoders trained on two immune cell datasets. With a domain expert in immunology, we show that concepts improve interpretability compared to individual neurons while preserving the richness and informativeness of the latent representations. This work provides a principled framework for interpreting what biological knowledge foundation models have encoded, paving the way for their use for hypothesis generation and discovery.","\label{sec:related-work}

\paragraph{Explainability of scRNA-seq models} Interpretability in scRNA-seq models often relies on pathway enrichment methods \citep{maleki2020gene}, which interpret the model's mechanism through the lens of existing biological knowledge encoded in curated ontologies such as Reactome and Gene Ontology  \citep{fabregat2016reactome, ashburner2000gene}. Some approaches incorporate prior biological knowledge directly into model architecture, designing models in which certain components correspond to known pathways \citep{bourgeais2022graphgonet, rybakov2020learning, lotfollahi2023biologically, gut2021pmvae, zarlengatabcbm}. Although this strategy yields models that are interpretable by design, it also constrains them to existing knowledge, thereby limiting their potential for discovery. In contrast, post-hoc explainability aims to interpret models after training. Attribution methods are frequently used to identify the genes that contribute most to specific predictions  \citep{yap2021verifying, usman2025explainable, chereda2021explaining}. For comprehensive overview, \cite{zhou2023xai} and \cite{conard2023spectrum} review explainable and interpretable machine learning methods for biology.

\paragraph{Sparse dictionary learning for interpretability of deep learning models in biology} Sparse dictionary learning has recently shown great potential for decomposing the latent space of deep learning models into sparse and interpretable features. Following its success in language model \citep{sharkey2022taking, huben2023sparse} and vision models \citep{fel2023craft}, this methodology has been extended to deep learning models for biology. Sparse Auto-Encoders (SAEs) have successfully uncovered meaningful concepts encoded by protein language models, such as generic and family-specific features  \citep{Adams2025FromMI}, or binding sites and structural motifs \citep{simon2024interplm}. SAEs have also been applied to histopathology models, where they discovered interpretable concepts related to cellular and tissue characteristics, and geometric structures \citep{lelearning}. Alongside our work, \cite{Schuster2024CanSA} trained a Sparse Auto-Encoder on the cell embeddings from a scRNA-seq generative model and used pathway enrichment to map scRNA-seq concepts to known pathways. We introduce different interpretation methods that go beyond correlational approaches and conduct a user study. Additionally, we assess the stability and usefulness of the resulting concepts, which are necessary for practical utility.","\paragraph{Explainability of scRNA-seq models} Interpretability in scRNA-seq models often relies on pathway enrichment methods \citep{maleki2020gene}, which interpret the model's mechanism through the lens of existing biological knowledge encoded in curated ontologies such as Reactome and Gene Ontology  \citep{fabregat2016reactome, ashburner2000gene}. Some approaches incorporate prior biological knowledge directly into model architecture, designing models in which certain components correspond to known pathways \citep{bourgeais2022graphgonet, rybakov2020learning, lotfollahi2023biologically, gut2021pmvae, zarlengatabcbm}. Although this strategy yields models that are interpretable by design, it also constrains them to existing knowledge, thereby limiting their potential for discovery. In contrast, post-hoc explainability aims to interpret models after training. Attribution methods are frequently used to identify the genes that contribute most to specific predictions  \citep{yap2021verifying, usman2025explainable, chereda2021explaining}. For comprehensive overview, \cite{zhou2023xai} and \cite{conard2023spectrum} review explainable and interpretable machine learning methods for biology.

\paragraph{Sparse dictionary learning for interpretability of deep learning models in biology} Sparse dictionary learning has recently shown great potential for decomposing the latent space of deep learning models into sparse and interpretable features. Following its success in language model \citep{sharkey2022taking, huben2023sparse} and vision models \citep{fel2023craft}, this methodology has been extended to deep learning models for biology. Sparse Auto-Encoders (SAEs) have successfully uncovered meaningful concepts encoded by protein language models, such as generic and family-specific features  \citep{Adams2025FromMI}, or binding sites and structural motifs \citep{simon2024interplm}. SAEs have also been applied to histopathology models, where they discovered interpretable concepts related to cellular and tissue characteristics, and geometric structures \citep{lelearning}. Alongside our work, \cite{Schuster2024CanSA} trained a Sparse Auto-Encoder on the cell embeddings from a scRNA-seq generative model and used pathway enrichment to map scRNA-seq concepts to known pathways. We introduce different interpretation methods that go beyond correlational approaches and conduct a user study. Additionally, we assess the stability and usefulness of the resulting concepts, which are necessary for practical utility.",
2510.24670v2,http://arxiv.org/abs/2510.24670v2,2025-10-28 17:36:51+00:00,Pearl: A Foundation Model for Placing Every Atom in the Right Location,"Accurately predicting the three-dimensional structures of protein-ligand complexes remains a fundamental challenge in computational drug discovery that limits the pace and success of therapeutic design. Deep learning methods have recently shown strong potential as structural prediction tools, achieving promising accuracy across diverse biomolecular systems. However, their performance and utility are constrained by scarce experimental data, inefficient architectures, physically invalid poses, and the limited ability to exploit auxiliary information available at inference. To address these issues, we introduce Pearl (Placing Every Atom in the Right Location), a foundation model for protein-ligand cofolding at scale. Pearl addresses these challenges with three key innovations: (1) training recipes that include large-scale synthetic data to overcome data scarcity; (2) architectures that incorporate an SO(3)-equivariant diffusion module to inherently respect 3D rotational symmetries, improving generalization and sample efficiency, and (3) controllable inference, including a generalized multi-chain templating system supporting both protein and non-polymeric components as well as dual unconditional/conditional modes. Pearl establishes a new state-of-the-art performance in protein-ligand cofolding. On the key metric of generating accurate (RMSD < 2 Å) and physically valid poses, Pearl surpasses AlphaFold 3 and other open source baselines on the public Runs N' Poses and PoseBusters benchmarks, delivering 14.5% and 14.2% improvements, respectively, over the next best model. In the pocket-conditional cofolding regime, Pearl delivers $3.6\times$ improvement on a proprietary set of challenging, real-world drug targets at the more rigorous RMSD < 1 Å threshold. Finally, we demonstrate that model performance correlates directly with synthetic dataset size used in training.","\label{sec:related-work}

The computational prediction of protein-ligand binding poses has long been a cornerstone of SBDD, and it was traditionally carried out by physics-based ligand docking \cite{docking-review} (\eg,
FlexX~\cite{rarey_fast_1996},
Surflex-Dock~\cite{surflex},
FRED~\cite{mcgann_gaussian_2003},
GOLD~\cite{gold},
Glide~\cite{friesner_glide_2004},
MOE~DOCK~\cite{corbeil_variability_2012},
DOCK~\cite{venkatraman_flexible_2012},
Autodock Vina~\cite{AutoDockVina}).
These methods, which treat the pocket as (semi-)rigid, are either unable to treat induced fit---the protein conformational changes that may occur upon binding~\cite{inducedfit1,inducedfit2,miller2021inducedfitdocking}---or can only account for minor, local rearrangements of the binding pocket.
While there exist deep learning approaches for molecular docking \cite{corso2022diffdock,lee_genmol_2025}, including diffusion-based generative models, they do not jointly fold the protein and ligand, nor do they outperform classical docking methods~\cite{buttenschoen2024posebusters}.

A paradigm shift occurred with the advent of end-to-end folding models, most notably when \alphafold2 achieved near-experimental accuracy on the CASP14 challenge on protein monomers, starting from sequence and evolutionary information alone~\cite{af2,casp14}.
RoseTTAFold extended this approach to complexes with multiple protein chains~\cite{rosettafold2021}.
\alphafold3 (AF3) \cite{abramson2024accurate} generalized protein folding models to nearly all molecule types in the Protein Data Bank (PDB)~\cite{wwpdb_stats_2025}, inspiring a new generation of \emph{cofolding} models, such as
\chai1~\cite{chai2024chai},
\boltz1(x)~\cite{wohlwend2024boltz1},
\boltz2~\cite{passaro2025boltz2},
HelixFold3~\cite{liu2024helixfold3},
NeuralPlexer3~\cite{qiao2024neuralplexer3}, and
\protenix~\cite{bytedance2025protenix}.
Modern cofolding architectures typically share a blueprint, consisting of a trunk module (\eg, AF3's Pairformer) and a generative structure module, which are often denoising diffusion probabilistic models~\cite{diffusion}.
While \pearl is inspired by these approaches, it also altogether reflects a novel approach to cofolding.
\pearl incorporates geometric deep learning principles \cite{bronstein2017geometric} among other architectural, pretraining, and training improvements.
These innovations, together with \textbackslash{}pearl's unique inference-time controllability, enable state-of-the-art usability for practical small molecule drug discovery, including an enhanced ability to exploit known binding pockets and prior liganded structures.

A persistent bottleneck for all cofolding systems is their reliance upon relatively limited and biased training data from the PDB~\cite{burley2025updated}, which can lead to poor generalization to novel structures, instead relying on \emph{memorization} \cite{vskrinjar2025have}.
To mitigate this challenge, models have incorporated strategies like training on ``distillation data''~\cite{Melnyk2025} from other models \cite{ahdritz2024openfold} or integrating molecular dynamics and biochemical assay data~\cite{passaro2025boltz2}.
In contrast, \pearl's training corpus is augmented with a large-scale, dataset of synthetically generated protein-ligand complexes, exposing the model to wider chemical diversity.

Finally, controllability, \eg, the ability of users to incorporate structural priors during inference via templates, can improve the practical utility of cofolding models.
AF3 established per-protein-chain templates as the standard for cofolding models.
While scalable, this strategy can miss cross-chain interactions~\cite{abramson2024accurate} and more sophisticated approaches have emerged.
For example, later open source models introduced external experimental restraints and multimeric templates for protein complexes~\cite{chai2024chai,passaro2025boltz2}.
\pearl further generalizes multi-chain templates to additionally encompass non-polymeric components, such as cofactors and related ligands.","The computational prediction of protein-ligand binding poses has long been a cornerstone of SBDD, and it was traditionally carried out by physics-based ligand docking \cite{docking-review} (\eg,
FlexX~\cite{rarey_fast_1996},
Surflex-Dock~\cite{surflex},
FRED~\cite{mcgann_gaussian_2003},
GOLD~\cite{gold},
Glide~\cite{friesner_glide_2004},
MOE~DOCK~\cite{corbeil_variability_2012},
DOCK~\cite{venkatraman_flexible_2012},
Autodock Vina~\cite{AutoDockVina}).
These methods, which treat the pocket as (semi-)rigid, are either unable to treat induced fit---the protein conformational changes that may occur upon binding~\cite{inducedfit1,inducedfit2,miller2021inducedfitdocking}---or can only account for minor, local rearrangements of the binding pocket.
While there exist deep learning approaches for molecular docking \cite{corso2022diffdock,lee_genmol_2025}, including diffusion-based generative models, they do not jointly fold the protein and ligand, nor do they outperform classical docking methods~\cite{buttenschoen2024posebusters}.

A paradigm shift occurred with the advent of end-to-end folding models, most notably when \alphafold2 achieved near-experimental accuracy on the CASP14 challenge on protein monomers, starting from sequence and evolutionary information alone~\cite{af2,casp14}.
RoseTTAFold extended this approach to complexes with multiple protein chains~\cite{rosettafold2021}.
\alphafold3 (AF3) \cite{abramson2024accurate} generalized protein folding models to nearly all molecule types in the Protein Data Bank (PDB)~\cite{wwpdb_stats_2025}, inspiring a new generation of \emph{cofolding} models, such as
\chai1~\cite{chai2024chai},
\boltz1(x)~\cite{wohlwend2024boltz1},
\boltz2~\cite{passaro2025boltz2},
HelixFold3~\cite{liu2024helixfold3},
NeuralPlexer3~\cite{qiao2024neuralplexer3}, and
\protenix~\cite{bytedance2025protenix}.
Modern cofolding architectures typically share a blueprint, consisting of a trunk module (\eg, AF3's Pairformer) and a generative structure module, which are often denoising diffusion probabilistic models~\cite{diffusion}.
While \pearl is inspired by these approaches, it also altogether reflects a novel approach to cofolding.
\pearl incorporates geometric deep learning principles \cite{bronstein2017geometric} among other architectural, pretraining, and training improvements.
These innovations, together with \textbackslash{}pearl's unique inference-time controllability, enable state-of-the-art usability for practical small molecule drug discovery, including an enhanced ability to exploit known binding pockets and prior liganded structures.

A persistent bottleneck for all cofolding systems is their reliance upon relatively limited and biased training data from the PDB~\cite{burley2025updated}, which can lead to poor generalization to novel structures, instead relying on \emph{memorization} \cite{vskrinjar2025have}.
To mitigate this challenge, models have incorporated strategies like training on ``distillation data''~\cite{Melnyk2025} from other models \cite{ahdritz2024openfold} or integrating molecular dynamics and biochemical assay data~\cite{passaro2025boltz2}.
In contrast, \pearl's training corpus is augmented with a large-scale, dataset of synthetically generated protein-ligand complexes, exposing the model to wider chemical diversity.

Finally, controllability, \eg, the ability of users to incorporate structural priors during inference via templates, can improve the practical utility of cofolding models.
AF3 established per-protein-chain templates as the standard for cofolding models.
While scalable, this strategy can miss cross-chain interactions~\cite{abramson2024accurate} and more sophisticated approaches have emerged.
For example, later open source models introduced external experimental restraints and multimeric templates for protein complexes~\cite{chai2024chai,passaro2025boltz2}.
\pearl further generalizes multi-chain templates to additionally encompass non-polymeric components, such as cofactors and related ligands.",
2509.25479v2,http://arxiv.org/abs/2509.25479v2,2025-09-29 20:33:32+00:00,Discontinuous Epitope Fragments as Sufficient Target Templates for Efficient Binder Design,"Recent advances in structure-based protein design have accelerated de novo binder generation, yet interfaces on large domains or spanning multiple domains remain challenging due to high computational cost and declining success with increasing target size. We hypothesized that protein folding neural networks (PFNNs) operate in a ``local-first'' manner, prioritizing local interactions while displaying limited sensitivity to global foldability. Guided by this hypothesis, we propose an epitope-only strategy that retains only the discontinuous surface residues surrounding the binding site. Compared to intact-domain workflows, this approach improves in silico success rates by up to 80% and reduces the average time per successful design by up to forty-fold, enabling binder design against previously intractable targets such as ClpP and ALS3. Building on this foundation, we further developed a tailored pipeline that incorporates a Monte Carlo-based evolution step to overcome local minima and a position-specific biased inverse folding step to refine sequence patterns. Together, these advances not only establish a generalizable framework for efficient binder design against structurally large and otherwise inaccessible targets, but also support the broader ``local-first'' hypothesis as a guiding principle for PFNN-based design.","\subsection{Protein Folding Neural Networks (PFNNs)} 
The recent revolution in protein structure prediction was triggered by AlphaFold2 (AF2), a neural network that achieved unprecedented accuracy \cite{jumperHighlyAccurateProtein2021}. 
By integrating multiple sequence alignment (MSA) and structural template features with a novel invariant point attention module, AF2 and its analog RoseTTAFold \cite{baekAccuratePredictionProtein2021} enabled near-experimental accuracy in monomer folding for the first time. 
ESMFold further demonstrated that comparable accuracy can be achieved without querying MSAs, by leveraging a large-scale protein language model, while offering much faster inference \cite{linEvolutionaryscalePredictionAtomiclevel2023}. 
While multimer folding was initially pursued through finetuning AF2 on complex structures \cite{evansProteinComplexPrediction2022}, it was soon demonstrated that the original AF2 parameters trained only on monomers are already sufficient for multimer prediction \cite{gaoAF2ComplexPredictsDirect2022}. 
More recently, all-atom PFNNs such as AF3 \cite{abramsonAccurateStructurePrediction2024}, RF3 \cite{corleyAcceleratingBiomolecularModeling2025}, Chai-1 \cite{ChaidiscoveryChailab2024}, and Boltz-2 \cite{passaroBoltz2AccurateEfficient2025} have extended modeling to DNA, RNA, ligands, and ions, thereby enabling design and evaluation across a broader chemical space. 

\subsection{PFNN-derived Binder Evaluation} 
Beyond structure prediction, PFNNs also output a series of confidence scores, including point-wise (pLDDT), pair-wise (pAE), and global (pTM) measures. 
These scores have demonstrated state-of-the-art discriminative power for assessing structural quality and have been widely applied to evaluate designed proteins \cite{roneyStateoftheartEstimationProtein2022}. 
Nevertheless, their ability to distinguish true protein–protein interactions (PPIs) from artifacts remains limited: a retrospective study reported accuracies below 10\% \cite{bennettImprovingNovoProtein2023}. 
Moreover, this limitation persists even in the latest all-atom PFNNs, despite their substantially longer inference times \cite{manshourComprehensiveEvaluationAlphaFoldMultimer2024}. 
For these reasons, we focused our evaluations on AF2-like models, where inference is both efficient and well-validated. 

\subsection{PFNN-derived Binder Design} 
Several approaches have sought to repurpose PFNNs as generative models. 
One prominent direction is fine-tuning as diffusion models, exemplified by RFDiffusion \cite{watsonNovoDesignProtein2023}. 
Diffusion-based methods have proven effective for backbone generation, but co-design of sequence and structure remains challenging in this framework. 
In contrast, hallucination strategies allow simultaneous optimization of sequence and structure without additional training. 
By iteratively updating random sequences through gradient-based optimization of PFNN-derived confidence scores and geometric constraints, hallucination can directly produce tailored binder candidates \cite{anishchenkoNovoProteinDesign2021,goverdeNovoProteinDesign2023}. 

BindCraft (BC) represents the most systematic application of hallucination to binder design and has achieved remarkable \textit{in vitro} success \cite{pacesaBindCraftOneshotDesign2024}. 
BC employs AF2-multimer for hallucination \cite{evansProteinComplexPrediction2022} and AF2-monomer for refolding validation \cite{jumperHighlyAccurateProtein2021}, supplemented by several optimization and filtering steps. 
First, to alleviate the local minima that arise from gradient-based optimization, BC introduces a semi-greedy mutation procedure. 
Second, to address developability issues that often plague hallucinated binders \cite{raybouldFiveComputationalDevelopability2019,goverdeNovoProteinDesign2023}, BC applies ProteinMPNN-based redesign to refill non-interface residues. 
Finally, a suite of physics-based filters is used to eliminate designs with undesirable features, such as an excessive number of unsatisfied hydrogen-bond donors. 
While highly successful, these steps also reveal limitations: semi-greedy mutation can stagnate in suboptimal solutions, MPNN redesign is applied only in a global manner without fine-grained control of local sequence features, and efficiency remains limited when tackling large or multi-domain targets. 
These limitations motivate the methodological refinements explored in this work. 
% \subsection{Protein Folding Neural Networks (PFNNs)}
% Revolution in the field of protein structure prediction has been triggered by AlphaFold2 (AF2), a neural network with unprecedented accuracy\cite{jumperHighlyAccurateProtein2021}. 
% By integrating multiple sequence alignment (MSA) and structural template features with a novel invariant point attention module, AF2 and its analog, RoseTTAFold\cite{baekAccuratePredictionProtein2021}, has enabled near‑experimental accuracy in monomer folding for the first time. 
% ESMFold, leveraging a transformer‑based protein language model without querying MSAs, achieves comparable accuracy with much faster inference \cite{linEvolutionaryscalePredictionAtomiclevel2023}. 
% While folding of protein multimer is achieved by finetuning of AF2 on multimer conformation \cite{evansProteinComplexPrediction2022}, it was soon proved that parameter of original AF2 trained on monomer-only is already sufficient for such task \cite{gaoAF2ComplexPredictsDirect2022}. 
% The latest all-atom PFNNs like AF3, RF3, Chai‑1, and Boltz‑2 have enabled multimeric modeling of DNA, RNA, ligands, and ions, allowing design and evaluation in a wider chemical space \cite{abramsonAccurateStructurePrediction2024, ChaidiscoveryChailab2024, passaroBoltz2AccurateEfficient2025, corleyAcceleratingBiomolecularModeling2025}.

% \subsection{PFNN-derived Binder Evaluation}
% Besides predicted structures, PFNNs also produce a series of point-wise (predicted Local Distance Difference Test score, pLDDT), pair-wise (predicted Align Error, pAE) and global (predicted Template Modeling score, pTM) confidence scores. 
% These confidence scores have been observed to have state-of-the-art discriminative power for structure quality and have been applied to evaluate designed proteins \cite{roneyStateoftheartEstimationProtein2022}. 

% Despite tremendous progress compared to previous physical-based scoring, the ability of existing PFNNs to distinguish the true protein-protein interaction (PPI) from artifacts is still limited, \textless10\% according to a retrospective study \cite{bennettImprovingNovoProtein2023}. 
% Moreover, the performance is not necessarily improved in latest all-atom models \cite{manshourComprehensiveEvaluationAlphaFoldMultimer2024}, despite longer inference time. 
% Therefore, we stuck to AF2-like models for evaluation.

% \subsection{PFNN-derived Binder Design}
% PFNNs have been converted into generative models by several approaches. 
% Fine-tuning of PFNNs as diffusion models has been exemplified by RFDiffusion \cite{watsonNovoDesignProtein2023}. 
% However, diffusion is limited to backbone generation, and co-design of sequence and structure remains challenging in this framework. 
% On the other hand, hallucination can co-design sequence and structure without additional training. 
% By gradient-based optimization the predicted binding conformation by confidence scores and geometric constraint, random-initialized sequence can be iteratively finetuned into tailored binder. \cite{anishchenkoNovoProteinDesign2021,goverdeNovoProteinDesign2023}. 
% % Hallucination is a well-established strategy to re-purpose PFNN for binder design. 

% As an exemplified instance of hallucination, BindCraft(BC) achieved remarkable \textit{in vitro} success \cite{pacesaBindCraftOneshotDesign2024} by hallucination on AF2-multimer (\cite{evansProteinComplexPrediction2022}) and refolding validation on AF2-monomer \cite{jumperHighlyAccurateProtein2021}. 
% Besides hallucination, BindCraft incorporate a series of steps to optimize and filter hallucination outcomes. 
% To overcome the local minima incurred by gradient-based optimization, BC will perform a series of semi-greedy mutation. 
% To optimize the developability issue of hallucinated binders \cite{raybouldFiveComputationalDevelopability2019, goverdeNovoProteinDesign2023}, a ProteinMPNN-based optimization is performed to refill the residues outside binding interface. 
% Lastly, a series of physical-based metrics are calculated, to filtered out binders with undesired properties like too many unsatisfied hydrogen bond (H-Bond) donors.


% MSA is known to be vital for folding accuracy \cite{elofssonProgressProteinStructure2023}, assumably by providing co-evolution patterns correlated with remote contacts \cite{morcosDirectcouplingAnalysisResidue2011}. 
% While a deep MSA with accurate alignment and high sequence variety is considered beneficial for PFNN, a rich body of literature to manipulate MSA revealed unexpected behaviors. 
% Learned MSA with low-quality can conversely improve folding quality \cite{pettiEndtoendLearningMultiple2023}. 
% Sub-sampling of MSA can capture alternative conformations \cite{wayment-steelePredictingMultipleConformations2024, nunez-francoAlphaFold2PredictsAlternative2024}. 
 

% Besides MSA, global contacts can also be provided by template structure. In the scenario of binder design, target structure as template could ensure high-fidelity folding without computation-intensive calculation of MSA, and re-fold validation with designed conformation as ``initial guess'' can retain the binding pose without impairing the distinguishability of confidence scores \cite{bennettImprovingNovoProtein2023}. 

% While all these observation underscore how heavily PFNN relies on given global constraint, it's also demonstrated that PFNN can accurately rank structure candidates without MSA/templates \cite{roneyStateoftheartEstimationProtein2022}.
% It's thus deduced that PFNNs did learn a potent biophysical energy function and leverage global constraint only to narrow its searching space. 

% A recently study shows \textit{ab initio} folding with AF2 could sensitively capture contacts between residues already in vicinity, while remote contacts emerge after extensive iteration, indicating AF2 focuses more on local fitness \cite{changAlphaFold2KnowsProtein2024}. 
% Hallucinated protein, which reflects the preferred patterns of PFNN, also displayed 
% densely packed core and higher melting temperature (which compromised the solubility), indicating over-optimization on local interactions\cite{goverdeNovoProteinDesign2023}. 
% Such preference is re-confirmed by a radical case, where AF3 adopted a overlapped backbone with severe clashes, only to accommodate two alternative binders--both could form ideal local contacts--in the same binding sites \cite{jimenez-osesStructurePredictionAlternate2025}. 

% The ``local first'' mechanism was first proposed to describe the \textit{ab initio} folding behavior of AF2 \cite{changAlphaFold2KnowsProtein2024}. 
% Hereby we extend it to describe the learned energy function of PFNNs, which focus primarily on the local interaction, and has limited understanding on global fitness. 
% , indicating current PFNN are still far way from true understanding of physical folding process. 
% On the other hand, , indicating a has been learned . 
% PFNN leverage prior constraints to solve the global search problem. 





% It thus remains debatable to which extend PFNNs understand the physics underlying protein folding, or do they merely decode co-evolution patterns. 


% Despite remarkable progress, current PFNNs
% In particular, they relies heavily on prior global constraints to determine the overall geometry. 
% Such constraints can be either the encoded in the MSA (or pre-trained language model in ESM) or direct template from homologue structures. 



 % emerged from sub-sampling .

% Among them, co-evolution is more exploited, with application on optimizing folding accuracy and discovering alternative conformations \cite{pettiEndtoendLearningMultiple2023,wayment-}. 



% \subsection{Protein Folding Neural Networks (PFNNs)}
% Powerful neural network-based methods have revolutionized the field of protein structure prediction, laying the foundation for the efficient structure-based design of functional proteins. 
% By integrating multiple sequence alignment (MSA) and structural template features with a novel invariant point attention module, AlphaFold2 (AF2) and RoseTTA Fold enabled near‑experimental accuracy in monomer folding for the first time \cite{jumperHighlyAccurateProtein2021,baekAccuratePredictionProtein2021}. 

% In AF3 medium accuracy is not as good as AF2-multimer.
% \cite{manshourComprehensiveEvaluationAlphaFoldMultimer2024}
% % The new AlphaFold3 (AF3) expands capabilities beyond proteins to include DNA, RNA, ligands, and ions, achieving significantly better biomolecular interaction accuracy\cite{abramsonAccurateStructurePrediction2024}. 
% % Novel models such as Chai‑1, Boltz‑2, and RosettaFold3 represent the latest lineage-stage tools, further enriching the methodological landscape, though their detailed architectures and performance remain under peer‑review \cite{ChaidiscoveryChailab2024,
% % passaroBoltz2AccurateEfficient2025,
% % corleyAcceleratingBiomolecularModeling2025}. 



% \subsubsection{Global Constraint of PFNNs and its Application}\label{intro:templates}


% However, in the field of \textit{de novo} protein design where evolution background is absent, templates serves an indispensable role. 
% Passing known target structure as template could ensure refolding with high fidelity, even when it's multi-conformational or with missing residues\cite{pettiEndtoendLearningMultiple2023}. 
% Using designed binding pose as ``initial guess'' for novel binders could retain the designed conformation while keeping the predictability of confidence scores\cite{wayment-steelePredictingMultipleConformations2024}. 

% % In contrast, structural template feature gains less attention. 
% % Although it's vital for AF2's accuracy, as passing previous predictions as template could improve accuracy iteratively, very few attempts have been reported to leverage this feature. 


% % partially attributed to PFNNs' dependence on , which is not available for \textit{de novo} designed proteins.
% % Co-evolution pattern is suggested to facilitate folding via constraining remote contact and narrowing down the search space of the energy surface learned by PFNN \cite{changAlphaFold2KnowsProtein2024}. 
% % The modification of MSA has been proven to be potent in optimizing prediction accuracy and discovering alternative conformations , recapitulating this function.

% % Meanwhile, structural template features, as another input track for global constraint, is gaining less attention compared to MSA. 
% % In original AF2, this feature is provided as homologue structure at initialization to accelerate convergence and as previous predictions during iterations to . 
% % In the field of protein design, structural templates have been leveraged for faithful folding of target structure and evaluation of designed binder with ``initial guess'' \cite{pacesaBindCraftOneshotDesign2024, bennettImprovingNovoProtein2023}. 

% \subsection{PFNN-derived Protein Design}
% PFNNs have been converted into generative models by several approaches. 
% Fine-tuning of PFNNs as diffusion models has been exemplified by RFDiffusion \cite{watsonNovoDesignProtein2023}. 
% However, diffusion is limited to backbone generation, and co-design of sequence and structure remains challenging in this framework. 
% On the other hand, hallucination leverages backpropagation to iteratively refine sequences until achieving high model confidence scores, and can co-design sequence and structure without additional training \cite{anishchenkoNovoProteinDesign2021,goverdeNovoProteinDesign2023}. 
% One of the widely applied hallucination pipelines, BindCraft (BC) \cite{pacesaBindCraftOneshotDesign2024}, is adopted as the baseline for my current implementation.

% However, sequence generated by hallucination is known to suffer from poor developability (e.g. solubility) \cite{raybouldFiveComputationalDevelopability2019,bryantPeptideBinderDesign2023} . 
% Compared to diffusion, it is also computationally intensive and can hardly be scaled up. 
% % In our previous design tasks, BC hallucination has consistently failed for certain targets. 


% \subsubsection{Challenging Targets in \textit{de novo} Binder Design}\label{intro:challenging_target}
% Challenging targets refer to those with extremely low \textit{in silico} success rate. 
% In our previous design tasks, both BC and RFD has consistently failed for certain targets. 
% Although previous research never acknowledged their existence explicitly, they have consistently dealt with them with brute-force strategy like increasing sampling size and lower filtering threshold. 

% Underlying mechanism of challenging target can intricate. While some are intractable under current framework, e.g., the poor consistence between different PFNNs on certain targets (Fig. \ref{fig:bb_success_cor}, we've noticed two correlated features might be solvable: target size and flexibility. 

% Dropped performance as input size grows has been a recognized issue in deep-learning (DL) based generation \cite{levySameTaskMore2024}. 
% This challenge is especially a bottleneck for binder design task, as computational cost increases quadratically, while design/fold performance reduces drastically with increased input target size \cite{changAlphaFold2KnowsProtein2024, bryantPeptideBinderDesign2023}. 
% Previously, this challenge is tacitly sidestepped by trimming the input target to small domain (usually less than 300 aa) with binding epitope\cite{watsonNovoDesignProtein2023,pacesaBindCraftOneshotDesign2024,lvNovoDesignMiniprotein2024,ragotteDesignedMiniproteinsPotently2025}. 
% However, for functional interface on huge domain or across domain, binder design remains challenging. 

% In fact, current design algorithms is robust to missing residues, a pitfall prevalent in experimental structures. 
% As long as the missing region is distant from binding area and inferred as disordered loop, it's believed safe to ignore missing region by padding their positions with null token and take the known region as templates \cite{watsonNovoDesignProtein2023, pacesaBindCraftOneshotDesign2024}. 
% However, physical fold-ability of input target as a perquisite has never been questioned. Wether local structure of binding surface (epitope), consisting of scattered peptide segments can be used for binder design, and what's the effect on sampling speed and success rate, is never studied. 

% Meanwhile, current PFNN's ability to distinguish true protein-protein interaction (PPI) from artifacts is still limited, \textless10\% according to a retrospective study \cite{bennettImprovingNovoProtein2023}. 
% For PPI mediated by intrinsic disordered region (IDR), the modeling ability is just slight above random guess \cite{zhangAssessmentProteinComplex2025}. 
% Although diffusion-based protocol has been applied to IDR-mediated PPI design , free peptide binder\cite{liuDiffusingProteinBinders2025}), they either treating flexible PPI as rigid structure \cite{bennettAtomicallyAccurateNovo2025}, or rooted on false believe that IDR ``lacks in preferred secondary structure'' \cite{wuDesignIntrinsicallyDisordered2025}. 
% Even valid design cam emerge from extensive design and screening, it's hard to believe a behavior can not be predicted accurately so far can really be designed with a derivative framework. 

% \subsection{Key Components of BC Pipeline}\label{intro:bc}
% Since my current progress focuses on modification of BC, more introduction will be given on other key components of this pipeline besides hallucination.
% \subsubsection{Mutation-based Evolution to Escape Local Minima} \label{intro:mc}
% Hallucination implemented in ColabDesign leveraged a stochastic gradient descendant(SGD)-like optimizer and thus easily to be trapped in local minima. 
% To mitigate this issue, BC pipeline performs semi-greedy(SM) based evolution after hallucination. 
% By iteratively introducing random mutations and accepting them when quality score increases, sampling trajectory may be pushed outside the hallucinated local minima and explore wider sequential space. 
% Obviously, SM is still prone to local minima. 

% Markov chain Monte Carlo (MC) methods like Metropolis–Hastings is a classic method solve this issue. 
% By accepting attenuated mutations by a small chances proportional to decreased loss, MC allows for a wider searching space with potential better samples. 
% Meanwhile, as random mutations can be aimless, the hallucinated per-residue AA distribution hallucination might guide the evolution and achieve higher efficiency. 
% However, neither of them was explored before.

% \subsubsection{MPNN-based Optimization for Developability}\label{intro:mpnn}
% Meanwhile, hallucinated sequence are known to suffer from poor developability \cite{goverdeNovoProteinDesign2023}. 
% To mitigated this issue, BC would optimize binder sequence outside interface with ProteinMPNN inverse folding \cite{dauparasRobustDeepLearning2022}. 
% ProteinMPNN infers a predicted positional specific logits with all $C_\alpha$ coordinates and aa types of unmasked residues. 
% By sampling from the logits auto-regressively in a randomized order, better binder can be generated. 

% Extra bias on the logits has been proved as a cheap but effective solution to optimize aa distribution\cite{goverdeComputationalDesignSoluble2024}. 
% In BC, bias is introduced to exclude cysteine from design and expedite expression. 
% However, previous work only apply bias globally by applying even bias on every site, while local features vital for design success requires dedicate per-residue control. 
% For instance, unoccupied polar residue on PPI would weaken binding energy by \textasciitilde{}3 kcal/mol and reduced affinity by \textasciitilde160 folds\cite{fershtHydrogenBondingBiological1985}. 
% Also, too many charged residue on non-PPI surface could extremify isoelectric points (pI) and lead to self-aggregation and non-specific binding \cite{guptaAntibodiesWeaklyBasic2022}. 
% Original BC pipelines simply discard designs with undesired local patterns, a wasteful behavior as qualified designs are already sparse when tackling difficult targets.","\subsection{Protein Folding Neural Networks (PFNNs)} 
The recent revolution in protein structure prediction was triggered by AlphaFold2 (AF2), a neural network that achieved unprecedented accuracy \cite{jumperHighlyAccurateProtein2021}. 
By integrating multiple sequence alignment (MSA) and structural template features with a novel invariant point attention module, AF2 and its analog RoseTTAFold \cite{baekAccuratePredictionProtein2021} enabled near-experimental accuracy in monomer folding for the first time. 
ESMFold further demonstrated that comparable accuracy can be achieved without querying MSAs, by leveraging a large-scale protein language model, while offering much faster inference \cite{linEvolutionaryscalePredictionAtomiclevel2023}. 
While multimer folding was initially pursued through finetuning AF2 on complex structures \cite{evansProteinComplexPrediction2022}, it was soon demonstrated that the original AF2 parameters trained only on monomers are already sufficient for multimer prediction \cite{gaoAF2ComplexPredictsDirect2022}. 
More recently, all-atom PFNNs such as AF3 \cite{abramsonAccurateStructurePrediction2024}, RF3 \cite{corleyAcceleratingBiomolecularModeling2025}, Chai-1 \cite{ChaidiscoveryChailab2024}, and Boltz-2 \cite{passaroBoltz2AccurateEfficient2025} have extended modeling to DNA, RNA, ligands, and ions, thereby enabling design and evaluation across a broader chemical space. 

\subsection{PFNN-derived Binder Evaluation} 
Beyond structure prediction, PFNNs also output a series of confidence scores, including point-wise (pLDDT), pair-wise (pAE), and global (pTM) measures. 
These scores have demonstrated state-of-the-art discriminative power for assessing structural quality and have been widely applied to evaluate designed proteins \cite{roneyStateoftheartEstimationProtein2022}. 
Nevertheless, their ability to distinguish true protein–protein interactions (PPIs) from artifacts remains limited: a retrospective study reported accuracies below 10\
Moreover, this limitation persists even in the latest all-atom PFNNs, despite their substantially longer inference times \cite{manshourComprehensiveEvaluationAlphaFoldMultimer2024}. 
For these reasons, we focused our evaluations on AF2-like models, where inference is both efficient and well-validated. 

\subsection{PFNN-derived Binder Design} 
Several approaches have sought to repurpose PFNNs as generative models. 
One prominent direction is fine-tuning as diffusion models, exemplified by RFDiffusion \cite{watsonNovoDesignProtein2023}. 
Diffusion-based methods have proven effective for backbone generation, but co-design of sequence and structure remains challenging in this framework. 
In contrast, hallucination strategies allow simultaneous optimization of sequence and structure without additional training. 
By iteratively updating random sequences through gradient-based optimization of PFNN-derived confidence scores and geometric constraints, hallucination can directly produce tailored binder candidates \cite{anishchenkoNovoProteinDesign2021,goverdeNovoProteinDesign2023}. 

BindCraft (BC) represents the most systematic application of hallucination to binder design and has achieved remarkable \textit{in vitro} success \cite{pacesaBindCraftOneshotDesign2024}. 
BC employs AF2-multimer for hallucination \cite{evansProteinComplexPrediction2022} and AF2-monomer for refolding validation \cite{jumperHighlyAccurateProtein2021}, supplemented by several optimization and filtering steps. 
First, to alleviate the local minima that arise from gradient-based optimization, BC introduces a semi-greedy mutation procedure. 
Second, to address developability issues that often plague hallucinated binders \cite{raybouldFiveComputationalDevelopability2019,goverdeNovoProteinDesign2023}, BC applies ProteinMPNN-based redesign to refill non-interface residues. 
Finally, a suite of physics-based filters is used to eliminate designs with undesirable features, such as an excessive number of unsatisfied hydrogen-bond donors. 
While highly successful, these steps also reveal limitations: semi-greedy mutation can stagnate in suboptimal solutions, MPNN redesign is applied only in a global manner without fine-grained control of local sequence features, and efficiency remains limited when tackling large or multi-domain targets. 
These limitations motivate the methodological refinements explored in this work.",
2511.05537v1,http://arxiv.org/abs/2511.05537v1,2025-10-29 18:50:59+00:00,Bridging Accuracy and Explainability in EEG-based Graph Attention Network for Depression Detection,"Depression is a major cause of global mental illness and significantly influences suicide rates. Timely and accurate diagnosis is essential for effective intervention. Electroencephalography (EEG) provides a non-invasive and accessible method for examining cerebral activity and identifying disease-associated patterns. We propose a novel graph-based deep learning framework, named Edge-gated, axis-mixed Pooling Attention Network (ExPANet), for differentiating major depressive disorder (MDD) patients from healthy controls (HC). EEG recordings undergo preprocessing to eliminate artifacts and are segmented into short periods of activity. We extract 14 features from each segment, which include time, frequency, fractal, and complexity domains. Electrodes are represented as nodes, whereas edges are determined by the phase-locking value (PLV) to represent functional connectivity. The generated brain graphs are examined utilizing an adapted graph attention network. This architecture acquires both localized electrode characteristics and comprehensive functional connectivity patterns. The proposed framework attains superior performance relative to current EEG-based approaches across two different datasets. A fundamental advantage of our methodology is its explainability. We evaluated the significance of features, channels, and edges, in addition to intrinsic attention weights. These studies highlight features, cerebral areas, and connectivity associations that are especially relevant to MDD, many of which correspond with clinical data. Our findings demonstrate a reliable and transparent method for EEG-based screening of MDD, using deep learning with clinically relevant results.","\label{sec:related_work}
% Recent years have witnessed numerous studies concentrating on the diagnosis of MDD by EEG utilizing deep learning techniques. Initial DL-based approaches for MDD diagnosis depend on handcrafted characteristics, like MLP \cite{DING2019156} and PNN \cite{Mahato2019eeg}. By analyzing EEG signals, researchers can examine local brain activity and functional connectivity patterns that indicate network-level interactions, providing insights into putative biomarkers specific to MDD \cite{Friston2011connectivity}, \cite{ellis2024crosseeg}. Recent models utilizing CNN architectures automatically extract features characterized by local invariance, particularly in the short term.  For instance, DeprNet \cite{seal2021deprnet} uses the ConvNet architecture with five stacked CNN layers to extract temporal characteristics. InceptionNet \cite{LEI2022103370}, \cite{Uyulan2021cnn} use kernels of varying sizes to extract features at different scales and utilize channel-wise attention in higher-level layers to ascertain channel significance. Reference \cite{SADATSHAHABI2021946} initially extracts spectral characteristics and subsequently inputs the time-frequency maps into a 2D-CNN.  Recently, due to LSTM's ability to capture long-term properties, some research has employed hybrid architectures.  References \cite{Saeedi2020knn} and \cite{sharma2021dephnn} develop a CNN-LSTM architecture to simultaneously represent short-term and long-term properties. Reference \cite{luo2023graph} integrates the GCN and GRU models to extract spatial-temporal characteristics of EEG, while constructing brain connection graphs adaptively. However, the current studies train the models using original data directly, and their efficacy is limited by the subpar data quality and insufficient data volume in the MDD diagnosis issue utilizing EEG. The previously mentioned deep learning models are susceptible to overfitting during training and do not generalize effectively to novel subjects. For at least the previous two decades, machine learning techniques have been utilized on resting-state EEG data \cite{reza2009sz}.  Diverse signal processing techniques \cite{Christopher1998wavelet}, \cite{Jeong2016coherence} are persistently utilized to analyze EEG signals for the comprehension of neurological illnesses, including alzheimer’s disease \cite{Ziad2012az} and schizophrenia \cite{ghosh2023connectome}, etc. These methodologies have established the foundation for explicable graph-based models; for instance, in epilepsy research, explicable GNNs have demonstrated the capability to accurately detect seizures while transparently identifying pathologically significant brain connections \cite{Mazurek2024graphxai}. Recent advancements in computational neuroscience have offered new perspectives on how underlying anatomy may constrain EEG-derived functional connectivity, while simultaneously striving to harmonize the structural and functional brain connectomes \cite{ghosh2023subspace}.  In \cite{lawhern2018eegnet}, the authors presented a deep learning architecture for EEG-based classification, termed EEGNet.

% Despite tremendous improvement, most previous techniques either focus on temporal or spectral patterns or use high-dimensional raw representations with low interpretability.  Few studies have used graph-based frameworks for MDD diagnosis to jointly simulate EEG signal spatial, spectral, and nonlinear dynamics.  Recent explainability studies sometimes lack clinical evidence or fail to demonstrate the significance of certain brain regions and linkages.  Our present research uses EEG-derived biomarker graphs and interpretable graph learning to increase MDD categorization diagnostic performance and clinical relevance.
%%%%%%%%%%%%%%%%%%%%

\subsubsection{Machine Learning with Handcrafted Features}
For at least the previous two decades, machine learning methodologies have been applied to resting-state EEG data \cite{reza2009sz}. 
Preliminary deep learning-based methodologies for major depressive disorder diagnosis rely on manually created features, such as multi-layer perceptrons (MLP) \cite{DING2019156} and probabilistic neural networks (PNN) \cite{Mahato2019eeg}.
Diverse signal processing techniques \cite{Christopher1998wavelet}, \cite{Jeong2016coherence} are consistently implemented to analyze EEG signals for the understanding of neurological disorders, including Alzheimer’s disease \cite{Ziad2012az} and schizophrenia \cite{ghosh2023connectome}.
These approaches established the foundations for interpretable graph-based models. In epilepsy research, interpretable GNNs have shown the ability to precisely identify seizures while emphasizing pathologically relevant brain connections \cite{Mazurek2024graphxai}. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Deep Learning with CNN and Hybrid Architectures}
Recent years have seen various studies focusing on the diagnosis of MDD through EEG, applying deep learning methodologies. Through the analysis of EEG signals, researchers can investigate localized brain activity and patterns of functional connectivity. This indicates network-level interactions, offering insights into potential features unique to MDD \cite{Friston2011connectivity}, \cite{ellis2024crosseeg}.
Recent models implementing CNN architectures entirely extract characteristics defined by local invariance, especially in the short term. In this regard, DeprNet \cite{seal2021deprnet} employs the ConvNet architecture, with five stacked CNN layers, to extract temporal features.
InceptionNet \cite{LEI2022103370}, \cite{Uyulan2021cnn} utilizes kernels of diverse dimensions to extract features across many scales and implements channel-wise attention in the higher levels to determine channel importance.
In this study \cite{SADATSHAHABI2021946}, the authors first extract spectral features and then integrate the time-frequency maps into a 2D-CNN.  
Recently, because of LSTM's capacity to store long-term characteristics, certain studies have utilized hybrid architectures. 
The CNN-LSTM architecture, which is used to represent both short-term and long-term characteristics at the same time, is developed in \cite{Saeedi2020knn} and \cite{sharma2021dephnn}.
In order to generate brain connection graphs in an adaptive manner, the authors \cite{luo2023graph} incorporate the GCN and GRU models. This integration allows it to extract the spatial-temporal properties of the EEG. 
Current research trains models using original data directly, and their effectiveness is constrained by inadequate data quality and insufficient data volume in the diagnosis of MDD utilizing EEG. 
% The previously mentioned deep learning models are sensitive to overfitting during training and fail to generalize efficiently to new subjects.
Recent developments in computational neuroscience have provided new insights into how underlying anatomy may limit EEG-derived functional connectivity.   While concurrently attempting to bring together the anatomical and functional brain connectomes \cite{ghosh2023subspace}.
In this paper \cite{lawhern2018eegnet}, the authors introduced a deep learning architecture for EEG-based identification, named EEGNet. Despite significant advancements, the majority of prior methodologies either concentrate on temporal/spectral patterns or utilize high-dimensional raw representations that lack interpretability. 

Limited research has employed graph-based frameworks for the diagnosis of MDD to dynamically model the spatial, spectral, and nonlinear dynamics of EEG signals.  
Recent research on explainability sometimes lacks clinical evidence or fails to establish the significance of certain brain regions and connections.  
Our present work incorporates EEG-derived feature graphs and interpretable graph learning to enhance the diagnostic performance and clinical relevance of MDD classification.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[htbp]
    \centering 
    \includegraphics[width=1\textwidth]{architecture/expanet_top.png}
    % \caption{Caption for Figure 1}
    % \label{fig:figure1}
    % \vspace{1cm} 
    \includegraphics[width=1\textwidth]{architecture/expanet_middle.png}
    % \caption{Caption for Figure 2}
    % \label{fig:figure2}
    % \vspace{1cm} 
    \includegraphics[width=1\textwidth]{architecture/expanet_bottom.png}
    % \caption{Caption for Figure 3}
    % \label{fig:expanet}
    \caption{Illustration of the proposed ExPANet architecture for EEG-based classification of MDD vs HC. 
    % Describing all stages from raw EEG to classification. The dual-layer ExPANet integrates edge-gated attention, AxisMix, and virtual node modules, resulting in graph pooling, classification, and explainability stages.
    }
    \label{fig:expanet}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%","\subsubsection{Machine Learning with Handcrafted Features}
For at least the previous two decades, machine learning methodologies have been applied to resting-state EEG data \cite{reza2009sz}. 
Preliminary deep learning-based methodologies for major depressive disorder diagnosis rely on manually created features, such as multi-layer perceptrons (MLP) \cite{DING2019156} and probabilistic neural networks (PNN) \cite{Mahato2019eeg}.
Diverse signal processing techniques \cite{Christopher1998wavelet}, \cite{Jeong2016coherence} are consistently implemented to analyze EEG signals for the understanding of neurological disorders, including Alzheimer’s disease \cite{Ziad2012az} and schizophrenia \cite{ghosh2023connectome}.
These approaches established the foundations for interpretable graph-based models. In epilepsy research, interpretable GNNs have shown the ability to precisely identify seizures while emphasizing pathologically relevant brain connections \cite{Mazurek2024graphxai}. 

\subsubsection{Deep Learning with CNN and Hybrid Architectures}
Recent years have seen various studies focusing on the diagnosis of MDD through EEG, applying deep learning methodologies. Through the analysis of EEG signals, researchers can investigate localized brain activity and patterns of functional connectivity. This indicates network-level interactions, offering insights into potential features unique to MDD \cite{Friston2011connectivity}, \cite{ellis2024crosseeg}.
Recent models implementing CNN architectures entirely extract characteristics defined by local invariance, especially in the short term. In this regard, DeprNet \cite{seal2021deprnet} employs the ConvNet architecture, with five stacked CNN layers, to extract temporal features.
InceptionNet \cite{LEI2022103370}, \cite{Uyulan2021cnn} utilizes kernels of diverse dimensions to extract features across many scales and implements channel-wise attention in the higher levels to determine channel importance.
In this study \cite{SADATSHAHABI2021946}, the authors first extract spectral features and then integrate the time-frequency maps into a 2D-CNN.  
Recently, because of LSTM's capacity to store long-term characteristics, certain studies have utilized hybrid architectures. 
The CNN-LSTM architecture, which is used to represent both short-term and long-term characteristics at the same time, is developed in \cite{Saeedi2020knn} and \cite{sharma2021dephnn}.
In order to generate brain connection graphs in an adaptive manner, the authors \cite{luo2023graph} incorporate the GCN and GRU models. This integration allows it to extract the spatial-temporal properties of the EEG. 
Current research trains models using original data directly, and their effectiveness is constrained by inadequate data quality and insufficient data volume in the diagnosis of MDD utilizing EEG. 

Recent developments in computational neuroscience have provided new insights into how underlying anatomy may limit EEG-derived functional connectivity.   While concurrently attempting to bring together the anatomical and functional brain connectomes \cite{ghosh2023subspace}.
In this paper \cite{lawhern2018eegnet}, the authors introduced a deep learning architecture for EEG-based identification, named EEGNet. Despite significant advancements, the majority of prior methodologies either concentrate on temporal/spectral patterns or utilize high-dimensional raw representations that lack interpretability. 

Limited research has employed graph-based frameworks for the diagnosis of MDD to dynamically model the spatial, spectral, and nonlinear dynamics of EEG signals.  
Recent research on explainability sometimes lacks clinical evidence or fails to establish the significance of certain brain regions and connections.  
Our present work incorporates EEG-derived feature graphs and interpretable graph learning to enhance the diagnostic performance and clinical relevance of MDD classification.",
2510.17826v1,http://arxiv.org/abs/2510.17826v1,2025-10-01 22:12:34+00:00,Speak to a Protein: An Interactive Multimodal Co-Scientist for Protein Analysis,"Building a working mental model of a protein typically requires weeks of reading, cross-referencing crystal and predicted structures, and inspecting ligand complexes, an effort that is slow, unevenly accessible, and often requires specialized computational skills. We introduce \emph{Speak to a Protein}, a new capability that turns protein analysis into an interactive, multimodal dialogue with an expert co-scientist. The AI system retrieves and synthesizes relevant literature, structures, and ligand data; grounds answers in a live 3D scene; and can highlight, annotate, manipulate and see the visualization. It also generates and runs code when needed, explaining results in both text and graphics. We demonstrate these capabilities on relevant proteins, posing questions about binding pockets, conformational changes, or structure-activity relationships to test ideas in real-time. \emph{Speak to a Protein} reduces the time from question to evidence, lowers the barrier to advanced structural analysis, and enables hypothesis generation by tightly coupling language, code, and 3D structures. \emph{Speak to a Protein} is freely accessible at https://open.playmolecule.org.","\label{sec:related}
%AI Scientist
The ambition to create an AI capable of scientific discovery is a long-standing goal, articulated in visions such as Hiroaki Kitano's proposal for an ""AI Scientist"": a system that could autonomously formulate hypotheses, design experiments, and achieve Nobel-class discoveries \citep{kitano2021}. While such a fully autonomous system \citep{boiko2023autonomous, zou2025agente} remains a grand challenge, recent progress in large language models (LLMs) has enabled the development of a more immediate and collaborative paradigm: the ""AI co-scientist"" or ""advanced intelligence"" in Kitano's words. 

%\subsection{Conversational and Multimodal Protein Assistants}
Early systems explored conversational interfaces for structural inspection and Q\&A over proteins. \citet{guo2023proteinchat} demonstrated \emph{ProteinChat}, which couples LLM prompting with protein 3D structures to answer user questions about residues and pockets. Contemporary efforts such as \citet{wang2024protchatgpt} and \citet{xiao2024proteingpt} investigate protein-aware prompting and multimodal conditioning for function/property reasoning. Most recently, \citet{wang2025prot2chat} proposes \emph{Prot2Chat}, an LLM that fuses protein sequence, structure, and text via an early-fusion adapter, directly targeting protein Q\&A.

%\subsection{Structure-Aware Tools and Visualization Copilots}
Beyond text-only chat, domain copilots increasingly drive molecular viewers and modeling tools. \citet{sun2024chatmol} introduce \emph{ChatMol Copilot}, an LLM agent that coordinates cheminformatics and modeling tools (e.g., docking, conformer generation) in response to natural-language requests. In parallel, \citet{ille2024gpt4structural} systematically evaluates GPT-4’s ability to perform rudimentary structural modeling and protein–ligand interaction analysis, highlighting both promise and limitations. Our work is aligned with this line but centers on tightly coupling language, code execution, and a live 3D scene for grounded, manipulable answers.

%\subsection{Agentic LLMs for Chemistry and Drug Discovery}
Agent frameworks augment LLMs with tool use, retrieval, and planning. \emph{ChemCrow} \citep{bran2024chemcrow} shows that equipping GPT-4 with chemistry tools enables multi-step synthesis planning and materials tasks. More recently, CLADD \citep{lee2025cladd} proposes a retrieval-augmented multi-agent system specialized for drug discovery tasks. \emph{Speak to a Protein} adopts the agentic paradigm for structural biology: it retrieves literature/structures, executes analyses (e.g., pocket mapping, SAR tables), and grounds responses in synchronized 3D visualizations.

%\subsection{Protein–Text Alignment and Cross-Modal LLMs}
%A complementary thread aligns protein representations with natural language. \citet{wu2024proteinclip} aligns protein LMs with text via contrastive learning; \citet{wang2024instructprotein} uses knowledge-graph–derived instructions to align human and protein “languages.” \emph{ProtST} \citep{xu2023protst} improves sequence models using biomedical text, enabling zero-shot retrieval/classification. \emph{ProtLLM} \citep{zhuo2024protllm} interleaves protein tokens with text via a “protein-as-word” objective, while \emph{LLaPA} \citep{zhou2025llapa} integrates protein encoders and PPI networks into an instruction-following LLM. Our system draws on these ideas to support bidirectional grounding: language queries that reference sequence/structure context, and code/visual actions that reflect the language plan.

Compared to prior work, our contribution is an end-to-end, \emph{interactive} co-scientist for proteins that (i) unifies literature/structure/ligand retrieval, (ii) reasons with tabular and 3D modalities, (iii) executes code for on-the-fly analyses, and (iv) directly annotates/manipulates the 3D scene in response to dialogue. This tightly coupled language–code–3D loop reduces the time from question to evidence relative to agent-only or text-only systems.


\begin{figure}[tb]
\centering
\includegraphics[width=1\linewidth]{Figures/architecture.png}
\caption{\textbf{Overview of the system architecture.} The system consists of a frontend with a protein viewer and chat interface, which includes a virtual file system and Python sandbox for automated code execution and viewer manipulation. The LLM Agent is the main orchestrator that interacts with the user and calls a set of custom tools through Model Context Protocol (MCP). These tools include a literature search for a given protein and text query, retrieving specialized data through APIs such as UniProt, PDBe and ChEMBL, as well as executing Python code in a sandbox environment with a dedicated virtual file system.}\label{fig:architecture}
\end{figure}","The ambition to create an AI capable of scientific discovery is a long-standing goal, articulated in visions such as Hiroaki Kitano's proposal for an ""AI Scientist"": a system that could autonomously formulate hypotheses, design experiments, and achieve Nobel-class discoveries \citep{kitano2021}. While such a fully autonomous system \citep{boiko2023autonomous, zou2025agente} remains a grand challenge, recent progress in large language models (LLMs) has enabled the development of a more immediate and collaborative paradigm: the ""AI co-scientist"" or ""advanced intelligence"" in Kitano's words. 


Early systems explored conversational interfaces for structural inspection and Q\&A over proteins. \citet{guo2023proteinchat} demonstrated \emph{ProteinChat}, which couples LLM prompting with protein 3D structures to answer user questions about residues and pockets. Contemporary efforts such as \citet{wang2024protchatgpt} and \citet{xiao2024proteingpt} investigate protein-aware prompting and multimodal conditioning for function/property reasoning. Most recently, \citet{wang2025prot2chat} proposes \emph{Prot2Chat}, an LLM that fuses protein sequence, structure, and text via an early-fusion adapter, directly targeting protein Q\&A.


Beyond text-only chat, domain copilots increasingly drive molecular viewers and modeling tools. \citet{sun2024chatmol} introduce \emph{ChatMol Copilot}, an LLM agent that coordinates cheminformatics and modeling tools (e.g., docking, conformer generation) in response to natural-language requests. In parallel, \citet{ille2024gpt4structural} systematically evaluates GPT-4’s ability to perform rudimentary structural modeling and protein–ligand interaction analysis, highlighting both promise and limitations. Our work is aligned with this line but centers on tightly coupling language, code execution, and a live 3D scene for grounded, manipulable answers.


Agent frameworks augment LLMs with tool use, retrieval, and planning. \emph{ChemCrow} \citep{bran2024chemcrow} shows that equipping GPT-4 with chemistry tools enables multi-step synthesis planning and materials tasks. More recently, CLADD \citep{lee2025cladd} proposes a retrieval-augmented multi-agent system specialized for drug discovery tasks. \emph{Speak to a Protein} adopts the agentic paradigm for structural biology: it retrieves literature/structures, executes analyses (e.g., pocket mapping, SAR tables), and grounds responses in synchronized 3D visualizations.




Compared to prior work, our contribution is an end-to-end, \emph{interactive} co-scientist for proteins that (i) unifies literature/structure/ligand retrieval, (ii) reasons with tabular and 3D modalities, (iii) executes code for on-the-fly analyses, and (iv) directly annotates/manipulates the 3D scene in response to dialogue. This tightly coupled language–code–3D loop reduces the time from question to evidence relative to agent-only or text-only systems.",
2511.03826v2,http://arxiv.org/abs/2511.03826v2,2025-11-05 19:47:41+00:00,CORE - A Cell-Level Coarse-to-Fine Image Registration Engine for Multi-stain Image Alignment,"Accurate and efficient registration of whole slide images (WSIs) is essential for high-resolution, nuclei-level analysis in multi-stained tissue slides. We propose a novel coarse-to-fine framework CORE for accurate nuclei-level registration across diverse multimodal whole-slide image (WSI) datasets. The coarse registration stage leverages prompt-based tissue mask extraction to effectively filter out artefacts and non-tissue regions, followed by global alignment using tissue morphology and ac- celerated dense feature matching with a pre-trained feature extractor. From the coarsely aligned slides, nuclei centroids are detected and subjected to fine-grained rigid registration using a custom, shape-aware point-set registration model. Finally, non-rigid alignment at the cellular level is achieved by estimating a non-linear dis- placement field using Coherent Point Drift (CPD). Our approach benefits from automatically generated nuclei that enhance the accuracy of deformable registra- tion and ensure precise nuclei-level correspondence across modalities. The pro- posed model is evaluated on three publicly available WSI registration datasets, and two private datasets. We show that CORE outperforms current state-of-the-art methods in terms of generalisability, precision, and robustness in bright-field and immunofluorescence microscopy WSIs","During the past decade, WSI registration has attracted significant research attention, leading to noteworthy contributions, despite these advances, the unique characteristics of multi-stained WSIs, including their gigapixel resolution, tissue variability, non-linear deformations, and staining inconsistencies, continue to pose major challenges. These factors often limit the effectiveness of conventional medical image registration methods, particularly when alignment at the nuclei level is required. A detailed review of the WSI registration methods has been presented in Elhaminia et al. \cite{elhaminia_traditional_2025}.

The most notable recent approaches for WSI Registration include HistokatFusion \cite{lotz_comparison_2023}, VALIS \cite{gatenbee_virtual_2023}, DeeperHistReg \cite{wodzinski_deeperhistreg_2024}, and DFBR \cite{awan_deep_2023}. Among these methods, HistokatFusion~\cite{lotz_comparison_2023} and VALIS~\cite{gatenbee_virtual_2023} both rely on traditional iterative image registration methodology while DFBR and DeeperHistReg are based on DL based feature extractors.

% HISTOKATFUSION
 Lotz et al.~\cite{lotz_patch-based_2016} proposed an iterative intensity based method for low resolution global registration  followed by patch-level refinement for local deformation estimation. HistokatFusion employs a three-stage registration pipeline \cite{lotz_robust_2019}, where the first stage performs automatic rotation-based affine registration, followed by Gauss–Newton parametric optimisation, concluding the final stage with non-rigid B-spline regularisation. This method combines coarse initial alignment with non-rigid, intensity-driven iterative registration, using normalised gradient fields (NGF) as the similarity metric and B-spline transformations to capture deformations. The method secured first place in the ANHIR competition \cite{borovec_anhir_2020} and was ranked among the top performers in the ACROBAT \cite{weitz_acrobat_2024} benchmark. However, the code for the method is not publicly available. 

% VALIS
Gatenbee et al. \cite{gatenbee_virtual_2023} created the virtual alignment of the pathology image series (VALIS) WSI registration library. VALIS aligns mIHC, H\&E and mIF WSIs. The method employs multiple techniques in preprocessing, rigid registration and non-rigid registration to optimise performance and reduce registration error. A notable observation is the combined use of both deep learning architectures such as VGG and hand-crafted feature descriptors, such as BRISK \cite{leutenegger_brisk_2011}. Although this method provides a practical solution for scalable whole-slide image (WSI) registration, its substantial computational overhead limits its efficiency.

% MORPHOLOGICAL FEATURES

Beyond intensity-based matching, some registration methods leverage fine-grained image features. Sarkar et al.~\cite{sarkar_robust_2014} proposed a line-based strategy for aligning adjacent tissue sections selecting boundary points around the tissue and using RANSAC~\cite{fischler_random_1981} to sample point pairs for line fitting. Alignment is guided by the sum of gradient magnitudes near these lines, with parameters such as line, angle and distance informing translation and rotation estimates. A multi-resolution normalised correlation on gradient magnitude images then refines local alignment. Huang et al. \cite{wei_unsupervised_2025} employed morphological descriptors, including area and eccentricity of segmented red blood cells, while Cooper et al.~\cite{antonacopoulos_unsupervised_2025} incorporated broader feature sets including centroid, area, eccentricity, and axis orientation from segmented H\&E and CD3 stained images.

In the recent past, most of the proposed methods have utilised learning-based techniques, particularly \textbf{deep learning methods}. % DEEPERHISTREG
For instance, Wodzinski et al.~\cite{wodzinski_deeperhistreg_2024} combined deep feature extractor (SuperPoint) \cite{ge_unsupervised_2022} and matcher (SuperGlue) \cite{sarlin_superglue_2020} with an intensity-based non-rigid alignment stage. Their approach improved high-resolution registration performance and addressed GPU memory limitations with a pyramidal strategy. % DFBR 
Awan et al. \cite{awan2018deep} proposed an autoencoder-based model to learn deep feature representations of input image pairs. These features extracted from the  autoencoder were then used to identify the optimal transformation through gradient descent. 
In another work, Awan et al. \cite{awan_deep_2023} proposed DFBR, the method comprises three primary stages, initial processing, rigid alignment, and non-linear registration. In the processing phase, tissue masks were generated and used for initial  transformation estimation, then rigid transform is calculated by multi-scale CNN features extraction  and matching pairs by considering the feature points with a small feature distance. Following the application of the DFBR method, any minor offset was corrected by a phase correlation method, subsequently followed by an established non-linear registration technique proposed in Lotz et al. \cite{lotz_robust_2019}.

% SFG
Notably, several works have explored registration through features derived from segmented tissues. For example, Ge et al. \cite{ge_unsupervised_2022} developed a multi-stain registration method that employed an unsupervised structural feature-guided (SFG) CNN, which was robust to both low-resolution rough and high-resolution fine structural features of tissues. The non-rigid network was pre-trained with the synthetic Flying Chair dataset \cite{dosovitskiy2015flownet}. Then the supervised component of the SFG network was trained on landmarks. Mahapatra et al.~\cite{mahapatra_registration_2020} leveraged deep segmentation maps from a pre-trained U-Net to guide non-linear alignment. These approaches highlight the trend towards integrating tissue morphology, nuclei features, and learning-based representations.

For high resolution WSI registration, nuclei point based registration has been proposed in the recent past. Jeyasangar et al. \cite{yap_nuclei-location_2024} proposed a nuclei-based WSI registration method that detects nuclei using Hover-Net, extracts tile point sets, and aligns them via a Gaussian Mixture Model with Locally Linear Embedding to preserve local structures. This approach outperforms existing methods on a subset of the HYRECO \cite{van_der_laak_hyreco_2021} dataset and generalises to other stains. However, the evaluation was limited to a single stain and  high resolution only. Recently, Jiang et al. \cite{jiang_multimodal_2024} proposed a framework to align WSI images from different staining modalities specifically H\&E and mIF at the cellular level using nuclei points. The method treats cell segmentation outcomes as point sets and employs CPD for initial alignment, followed by refinement using Graph Matching (GM). The method has been evaluated on the ovarian cancer tissue microarrays (TMAs) dataset, the approach achieves high alignment accuracy, facilitating the integration of cell-level features across modalities. However, the open-source implementation is only applicable to pre-extracted visual fields and TMAs only, and the method cannot be applied to WSI registration.


Although several studies have been evaluated at the coarse or fine level to evaluate the WSI registration performance for specific stain combinations, such as H\&E to IHC \cite{awan_deep_2023, lotz_comparison_2023} or H\&E to DAPI \cite{gatenbee_source_2023, doyle_whole-slide_2023}, no existing method systematically achieves or assesses both coarse and fine level (high resolution) registration across a diverse set of stains and modalities including H\&E, PAS, IHC and mIF. Despite recent advances, WSI registration remains limited in its use of intrinsic tissue features such as cells, nuclei, or blood vessels as the basis for alignment. Many existing methods still rely on stain-specific cues, rigid assumptions, or computationally intensive processes. As these stain-specific cues vary across stains, these methods often struggle with heterogeneous tissue morphology, staining variability, or severe deformations.

In this study, we address these challenges by leveraging nuclei as fine-grained, intrinsic features for precise and robust image registration. We propose and evaluate both coarse and fine level WSI registration approaches on consecutive sections and re-stained slides across multiple stains and modalities in this manuscript. Our analysis covers multimodal WSIs, including H\&E, PAS, mIHC, and mIF stained slides.","During the past decade, WSI registration has attracted significant research attention, leading to noteworthy contributions, despite these advances, the unique characteristics of multi-stained WSIs, including their gigapixel resolution, tissue variability, non-linear deformations, and staining inconsistencies, continue to pose major challenges. These factors often limit the effectiveness of conventional medical image registration methods, particularly when alignment at the nuclei level is required. A detailed review of the WSI registration methods has been presented in Elhaminia et al. \cite{elhaminia_traditional_2025}.

The most notable recent approaches for WSI Registration include HistokatFusion \cite{lotz_comparison_2023}, VALIS \cite{gatenbee_virtual_2023}, DeeperHistReg \cite{wodzinski_deeperhistreg_2024}, and DFBR \cite{awan_deep_2023}. Among these methods, HistokatFusion~\cite{lotz_comparison_2023} and VALIS~\cite{gatenbee_virtual_2023} both rely on traditional iterative image registration methodology while DFBR and DeeperHistReg are based on DL based feature extractors.


 Lotz et al.~\cite{lotz_patch-based_2016} proposed an iterative intensity based method for low resolution global registration  followed by patch-level refinement for local deformation estimation. HistokatFusion employs a three-stage registration pipeline \cite{lotz_robust_2019}, where the first stage performs automatic rotation-based affine registration, followed by Gauss–Newton parametric optimisation, concluding the final stage with non-rigid B-spline regularisation. This method combines coarse initial alignment with non-rigid, intensity-driven iterative registration, using normalised gradient fields (NGF) as the similarity metric and B-spline transformations to capture deformations. The method secured first place in the ANHIR competition \cite{borovec_anhir_2020} and was ranked among the top performers in the ACROBAT \cite{weitz_acrobat_2024} benchmark. However, the code for the method is not publicly available. 


Gatenbee et al. \cite{gatenbee_virtual_2023} created the virtual alignment of the pathology image series (VALIS) WSI registration library. VALIS aligns mIHC, H\&E and mIF WSIs. The method employs multiple techniques in preprocessing, rigid registration and non-rigid registration to optimise performance and reduce registration error. A notable observation is the combined use of both deep learning architectures such as VGG and hand-crafted feature descriptors, such as BRISK \cite{leutenegger_brisk_2011}. Although this method provides a practical solution for scalable whole-slide image (WSI) registration, its substantial computational overhead limits its efficiency.



Beyond intensity-based matching, some registration methods leverage fine-grained image features. Sarkar et al.~\cite{sarkar_robust_2014} proposed a line-based strategy for aligning adjacent tissue sections selecting boundary points around the tissue and using RANSAC~\cite{fischler_random_1981} to sample point pairs for line fitting. Alignment is guided by the sum of gradient magnitudes near these lines, with parameters such as line, angle and distance informing translation and rotation estimates. A multi-resolution normalised correlation on gradient magnitude images then refines local alignment. Huang et al. \cite{wei_unsupervised_2025} employed morphological descriptors, including area and eccentricity of segmented red blood cells, while Cooper et al.~\cite{antonacopoulos_unsupervised_2025} incorporated broader feature sets including centroid, area, eccentricity, and axis orientation from segmented H\&E and CD3 stained images.

In the recent past, most of the proposed methods have utilised learning-based techniques, particularly \textbf{deep learning methods}. 
For instance, Wodzinski et al.~\cite{wodzinski_deeperhistreg_2024} combined deep feature extractor (SuperPoint) \cite{ge_unsupervised_2022} and matcher (SuperGlue) \cite{sarlin_superglue_2020} with an intensity-based non-rigid alignment stage. Their approach improved high-resolution registration performance and addressed GPU memory limitations with a pyramidal strategy. 
Awan et al. \cite{awan2018deep} proposed an autoencoder-based model to learn deep feature representations of input image pairs. These features extracted from the  autoencoder were then used to identify the optimal transformation through gradient descent. 
In another work, Awan et al. \cite{awan_deep_2023} proposed DFBR, the method comprises three primary stages, initial processing, rigid alignment, and non-linear registration. In the processing phase, tissue masks were generated and used for initial  transformation estimation, then rigid transform is calculated by multi-scale CNN features extraction  and matching pairs by considering the feature points with a small feature distance. Following the application of the DFBR method, any minor offset was corrected by a phase correlation method, subsequently followed by an established non-linear registration technique proposed in Lotz et al. \cite{lotz_robust_2019}.


Notably, several works have explored registration through features derived from segmented tissues. For example, Ge et al. \cite{ge_unsupervised_2022} developed a multi-stain registration method that employed an unsupervised structural feature-guided (SFG) CNN, which was robust to both low-resolution rough and high-resolution fine structural features of tissues. The non-rigid network was pre-trained with the synthetic Flying Chair dataset \cite{dosovitskiy2015flownet}. Then the supervised component of the SFG network was trained on landmarks. Mahapatra et al.~\cite{mahapatra_registration_2020} leveraged deep segmentation maps from a pre-trained U-Net to guide non-linear alignment. These approaches highlight the trend towards integrating tissue morphology, nuclei features, and learning-based representations.

For high resolution WSI registration, nuclei point based registration has been proposed in the recent past. Jeyasangar et al. \cite{yap_nuclei-location_2024} proposed a nuclei-based WSI registration method that detects nuclei using Hover-Net, extracts tile point sets, and aligns them via a Gaussian Mixture Model with Locally Linear Embedding to preserve local structures. This approach outperforms existing methods on a subset of the HYRECO \cite{van_der_laak_hyreco_2021} dataset and generalises to other stains. However, the evaluation was limited to a single stain and  high resolution only. Recently, Jiang et al. \cite{jiang_multimodal_2024} proposed a framework to align WSI images from different staining modalities specifically H\&E and mIF at the cellular level using nuclei points. The method treats cell segmentation outcomes as point sets and employs CPD for initial alignment, followed by refinement using Graph Matching (GM). The method has been evaluated on the ovarian cancer tissue microarrays (TMAs) dataset, the approach achieves high alignment accuracy, facilitating the integration of cell-level features across modalities. However, the open-source implementation is only applicable to pre-extracted visual fields and TMAs only, and the method cannot be applied to WSI registration.


Although several studies have been evaluated at the coarse or fine level to evaluate the WSI registration performance for specific stain combinations, such as H\&E to IHC \cite{awan_deep_2023, lotz_comparison_2023} or H\&E to DAPI \cite{gatenbee_source_2023, doyle_whole-slide_2023}, no existing method systematically achieves or assesses both coarse and fine level (high resolution) registration across a diverse set of stains and modalities including H\&E, PAS, IHC and mIF. Despite recent advances, WSI registration remains limited in its use of intrinsic tissue features such as cells, nuclei, or blood vessels as the basis for alignment. Many existing methods still rely on stain-specific cues, rigid assumptions, or computationally intensive processes. As these stain-specific cues vary across stains, these methods often struggle with heterogeneous tissue morphology, staining variability, or severe deformations.

In this study, we address these challenges by leveraging nuclei as fine-grained, intrinsic features for precise and robust image registration. We propose and evaluate both coarse and fine level WSI registration approaches on consecutive sections and re-stained slides across multiple stains and modalities in this manuscript. Our analysis covers multimodal WSIs, including H\&E, PAS, mIHC, and mIF stained slides.",
2511.09581v1,http://arxiv.org/abs/2511.09581v1,2025-11-12 10:52:53+00:00,Clinically-aligned Multi-modal Chest X-ray Classification,"Radiology is essential to modern healthcare, yet rising demand and staffing shortages continue to pose major challenges. Recent advances in artificial intelligence have the potential to support radiologists and help address these challenges. Given its widespread use and clinical importance, chest X-ray classification is well suited to augment radiologists' workflows. However, most existing approaches rely solely on single-view, image-level inputs, ignoring the structured clinical information and multi-image studies available at the time of reporting. In this work, we introduce CaMCheX, a multimodal transformer-based framework that aligns multi-view chest X-ray studies with structured clinical data to better reflect how clinicians make diagnostic decisions. Our architecture employs view-specific ConvNeXt encoders for frontal and lateral chest radiographs, whose features are fused with clinical indications, history, and vital signs using a transformer fusion module. This design enables the model to generate context-aware representations that mirror reasoning in clinical practice. Our results exceed the state of the art for both the original MIMIC-CXR dataset and the more recent CXR-LT benchmarks, highlighting the value of clinically grounded multimodal alignment for advancing chest X-ray classification.","\label{sec:relatedworks}
%\st{Chest X-ray classification employs deep learning techniques to automatically categorise chest radiographs based on identified features and abnormalities. Since multiple pathologies can co-occur in a single patient, this tasks is inherently a multi-label classification problem. Additionally, chest X-ray classification presents a long-tailed challenge, as certain pathologies occur more frequently than others.} \mmn{remove all that as its repetition}

% The majority of research into chest X-ray classification focuses on two general areas: developing methods to tackle the class imbalance, either through novel loss functions, augmentations or other methodologies \cite{Pham2021, Wu2020, Yuan2021, Park2023} and improving the architectural design \cite{Kim2023, Jeong2023}.

{Chest X-ray classification is the automated process of identifying and labeling findings in chest radiographs using machine learning models \citep{Chehade2024LungXrayReview}. It is typically formulated as a multi-label classification task, where a single image may exhibit multiple conditions such as pneumonia, cardiomegaly, or pleural effusion. This task supports clinical decision-making by assisting radiologists in detecting pathologies more efficiently and consistently.}
We focus our review on the under-utilisation of multi-view imaging \citep{Rubin2018, Zhu2021, Kim2023, CXR-LT2024} and on models that integrate multimodal information into chest X-ray classification \citep{Malik2024, Ketabi2023, Jacenkow2022}. {Several of the approaches that we discuss were developed for the CXR-LT 2023 \citep{Holste2024} and CXR-LT 2024 \cite{CXR-LT2024} challenges. While 2023 entries have separate publications, the 2024 submissions have not appeared independently and are therefore referenced by the challenge summary paper \citep{CXR-LT2024}}.

\subsection{Multi-view Chest X-ray Classification}
Although most chest X-ray classification methods generate predictions at the image level \citep{Nguyen-Mau2023, Jeong2023, Park2023, Hong2023, Verma2023, Yamagishi2023, KimC2023, Seo2023, Wang2024GazeGNN, Dai2024UniChest, Li2024a, Wang2025CrossDomain, CXR-LT2024}, this approach departs from clinical practice, where diagnoses are made at the study level by reviewing multiple views. However, several works have begun to explore multi-view approaches. \cite{Rubin2018}, \cite{Zhu2021}, and \cite{agostini2024} implemented architectures to handle frontal and lateral images separately, a limitation of these works is that the architecture adopts a fixed structure {and} assumes that a {frontal and lateral x-ray} are available and their designs do not accommodate multiple images from the same view. These assumptions are not reflective of the clinical landscape, or of MIMIC-CXR \citep{MIMIC}. To address this limitation, {competitors of CXR-LT 2023 \citep{Kim2023} and CXR-2024 \citep{CXR-LT2024} developed multi-view classification frameworks to aggregate features across views. While some of these approaches \cite{Kim2023} are invariant to the number {and view} of {the} input images, they employ shared encoders for frontal and lateral views which may limit the model's ability to capture the distinct anatomical and contextual features specific to each perspective. Our proposed \framework model builds upon these approaches by employing separate ConvNeXt encoders for frontal and lateral images, enabling the extraction of fine-grained, view-specific features.}

%The architecture is inherently view- and modality-invariant, capable of processing any number of images per view and conditionally incorporating clinical inputs when available, allowing it to operate effectively across variable and incomplete studies.} %This distinction is made possible by utilising the view information routinely provided in the DICOM metadata accompanying each chest X-ray. 
%\mmn{this dual path along with another small bit together could be the other contribution? But as long as it's really unique?}\ps{Done. added a second contribution along with the above change to related works.}

\subsection{Multi-modal Chest Diagnostics} 

In medical image classification, multimodality often refers to the combination of imaging types such as CT, MRI, and X-ray \citep{Li2024}. Other additional clinical data are also increasingly used \citep{Samak2023TranSOP, Samak2025StrokeReview, Hyland2024}, however such  additional modalities are sometimes not available at the time of initial presentation, %as the chest X-ray is usually the first examination performed. In contrast, 
while clinical indications and triage data are routinely available alongside the chest X-ray in an emergency setting, making them more immediately accessible for diagnostic support. 
%\mmn{now need to say something like ""however, there is no work that has actually used them in the way we have here - if that's true? Should Jacenkow et al.~\cite{Jacenkow2022} be mentioned here? There has been no similar work since that in 2022?}\ps{They are currently discussed in the paragraph below, I'm currently rewriting to merge the two, I will do a search today to try and find others.} 
However, to our knowledge, no existing approach combines study-level chest X-ray representations with both clinical indications and vital signs for classification. Several recent works have explored multimodal approaches for chest X-ray classification. \cite{Malik2024} proposed a multimodal framework combining chest X-rays, CT scans, and \ps{recordings of} cough sounds. While innovative, this diverges from clinical practice, where reporting clinicians typically lack direct patient contact and thus have no access to auditory cues. \cite{Ketabi2023} introduced a model incorporating chest X-rays, radiology reports, and eye-tracking data to improve classification and explainability. Although this demonstrates the value of textual information, it relies on radiology reports, which are produced post hoc and are unavailable at the point of initial review. \cite{Jacenkow2022} and \cite{Shurrab2024} included some clinical features but their models are limited to image-level predictions and do not incorporate contextual inputs such as vital signs, which offer further diagnostic value. \ps{\cite{Khader2023} developed a transformer-based architecture that integrates chest X-ray images with patient vital signs to improve classification performance. However, their approach remains limited to image-level predictions and does not incorporate clinical indications.}


%\mmn{I am not sure you can get away with this entire pgh. In medical Image Classification many works have used reports/clinical data/vital signs/ etc. so you need to try a different approach - maybe pick some aspects about chest x-ray stuff you can review.}


%\framework integrates clinical indications and vital signs with image features to inform classification. It builds upon recent advancements by incorporating multiple views and structured textual information, including clinical indications, to enhance the model’s ability to interpret radiographic findings in a clinically meaningful way. Clinical indications, which are routinely documented to justify radiological examinations \cite{irmer2017}, are included on request forms, while vital signs are typically recorded during triage and readily available to clinicians. This approach mirrors the workflow of practising clinicians, where decisions are rarely based on a single image in isolation but instead reflect study-level reasoning, informed by multiple images and relevant clinical context.\ps{Not sure if I should keep this paragraph as it's duplication from the introduction, but its a form of conclusion to the section.} 

{The proposed \framework integrates multi-view radiographs with structured clinical data, including clinical indications/history from request forms and vital signs which would be recorded at triage. This reflects real clinical workflows \cite{Banerji2025}, where radiologists consider both imaging and context at the study level. \framework enhances classification by aligning with how decisions are made in practice.}
%\mmn{It is a duplication, but see if you can reduce it to 3 lines maximum by stating the most important parts of what \framework is about, especially  about the request forms and mirroring clinician workflows}\ps{rewritten}","{Chest X-ray classification is the automated process of identifying and labeling findings in chest radiographs using machine learning models \citep{Chehade2024LungXrayReview}. It is typically formulated as a multi-label classification task, where a single image may exhibit multiple conditions such as pneumonia, cardiomegaly, or pleural effusion. This task supports clinical decision-making by assisting radiologists in detecting pathologies more efficiently and consistently.}
We focus our review on the under-utilisation of multi-view imaging \citep{Rubin2018, Zhu2021, Kim2023, CXR-LT2024} and on models that integrate multimodal information into chest X-ray classification \citep{Malik2024, Ketabi2023, Jacenkow2022}. {Several of the approaches that we discuss were developed for the CXR-LT 2023 \citep{Holste2024} and CXR-LT 2024 \cite{CXR-LT2024} challenges. While 2023 entries have separate publications, the 2024 submissions have not appeared independently and are therefore referenced by the challenge summary paper \citep{CXR-LT2024}}.

\subsection{Multi-view Chest X-ray Classification}
Although most chest X-ray classification methods generate predictions at the image level \citep{Nguyen-Mau2023, Jeong2023, Park2023, Hong2023, Verma2023, Yamagishi2023, KimC2023, Seo2023, Wang2024GazeGNN, Dai2024UniChest, Li2024a, Wang2025CrossDomain, CXR-LT2024}, this approach departs from clinical practice, where diagnoses are made at the study level by reviewing multiple views. However, several works have begun to explore multi-view approaches. \cite{Rubin2018}, \cite{Zhu2021}, and \cite{agostini2024} implemented architectures to handle frontal and lateral images separately, a limitation of these works is that the architecture adopts a fixed structure {and} assumes that a {frontal and lateral x-ray} are available and their designs do not accommodate multiple images from the same view. These assumptions are not reflective of the clinical landscape, or of MIMIC-CXR \citep{MIMIC}. To address this limitation, {competitors of CXR-LT 2023 \citep{Kim2023} and CXR-2024 \citep{CXR-LT2024} developed multi-view classification frameworks to aggregate features across views. While some of these approaches \cite{Kim2023} are invariant to the number {and view} of {the} input images, they employ shared encoders for frontal and lateral views which may limit the model's ability to capture the distinct anatomical and contextual features specific to each perspective. Our proposed \framework model builds upon these approaches by employing separate ConvNeXt encoders for frontal and lateral images, enabling the extraction of fine-grained, view-specific features.}




\subsection{Multi-modal Chest Diagnostics} 

In medical image classification, multimodality often refers to the combination of imaging types such as CT, MRI, and X-ray \citep{Li2024}. Other additional clinical data are also increasingly used \citep{Samak2023TranSOP, Samak2025StrokeReview, Hyland2024}, however such  additional modalities are sometimes not available at the time of initial presentation, 
while clinical indications and triage data are routinely available alongside the chest X-ray in an emergency setting, making them more immediately accessible for diagnostic support. 

However, to our knowledge, no existing approach combines study-level chest X-ray representations with both clinical indications and vital signs for classification. Several recent works have explored multimodal approaches for chest X-ray classification. \cite{Malik2024} proposed a multimodal framework combining chest X-rays, CT scans, and \ps{recordings of} cough sounds. While innovative, this diverges from clinical practice, where reporting clinicians typically lack direct patient contact and thus have no access to auditory cues. \cite{Ketabi2023} introduced a model incorporating chest X-rays, radiology reports, and eye-tracking data to improve classification and explainability. Although this demonstrates the value of textual information, it relies on radiology reports, which are produced post hoc and are unavailable at the point of initial review. \cite{Jacenkow2022} and \cite{Shurrab2024} included some clinical features but their models are limited to image-level predictions and do not incorporate contextual inputs such as vital signs, which offer further diagnostic value. \ps{\cite{Khader2023} developed a transformer-based architecture that integrates chest X-ray images with patient vital signs to improve classification performance. However, their approach remains limited to image-level predictions and does not incorporate clinical indications.}







{The proposed \framework integrates multi-view radiographs with structured clinical data, including clinical indications/history from request forms and vital signs which would be recorded at triage. This reflects real clinical workflows \cite{Banerji2025}, where radiologists consider both imaging and context at the study level. \framework enhances classification by aligning with how decisions are made in practice.}",
2510.26955v1,http://arxiv.org/abs/2510.26955v1,2025-10-30 19:27:29+00:00,Neurons as Detectors of Coherent Sets in Sensory Dynamics,"We model sensory streams as observations from high-dimensional stochastic dynamical systems and conceptualize sensory neurons as self-supervised learners of compact representations of such dynamics. From prior experience, neurons learn coherent sets-regions of stimulus state space whose trajectories evolve cohesively over finite times-and assign membership indices to new stimuli. Coherent sets are identified via spectral clustering of the stochastic Koopman operator (SKO), where the sign pattern of a subdominant singular function partitions the state space into minimally coupled regions. For multivariate Ornstein-Uhlenbeck processes, this singular function reduces to a linear projection onto the dominant singular vector of the whitened state-transition matrix. Encoding this singular vector as a receptive field enables neurons to compute membership indices via the projection sign in a biologically plausible manner. Each neuron detects either a predictive coherent set (stimuli with common futures) or a retrospective coherent set (stimuli with common pasts), suggesting a functional dichotomy among neurons. Since neurons lack access to explicit dynamical equations, the requisite singular vectors must be estimated directly from data, for example, via past-future canonical correlation analysis on lag-vector representations-an approach that naturally extends to nonlinear dynamics. This framework provides a novel account of neuronal temporal filtering, the ubiquity of rectification in neural responses, and known functional dichotomies. Coherent-set clustering thus emerges as a fundamental computation underlying sensory processing and transferable to bio-inspired artificial systems.","Viewing early sensory processing as efficient or predictive coding of natural stimuli has a long tradition~\cite{attneave1954some,barlow1961possible,simoncelli2001natural,price2022efficient,bialek2006efficient}. Closest to our work is the information bottleneck (IB) framework~\cite{tishby2000information,bialek2006efficient,palmer2015predictive,chalk2018toward}, which for Gaussian variables reduces to past–future CCA~\cite{chechik2003information}. Extending IB to dynamical systems and restricting compression to one bit corresponds to encoding the sign of the subdominant singular function of the SKO. Compared to slow feature analysis~\cite{wiskott2002slow,lipshutz2020biologically}, we incorporate an explicit dynamical-systems formulation and introduce a natural nonlinearity that enables hierarchical architectures without hand-crafted features.  Predictive and retrospective neuron classes have appeared in a lattice filter model of the visual pathway~\cite{gregor2012lattice}, though that work was limited to linear processing. Relative to predictive coding models~\cite{srinivasan1982predictive,rao1999predictive,druckmann2012mechanistic, gregor2012lattice}, where neurons emit prediction errors, our neurons encode coherent-set memberships. While the ON/OFF division has been attributed to metabolic efficiency~\cite{gjorgjieva2014benefits}, we propose a computational explanation. Our account of lagged and non-lagged LGN cells differs from earlier models~\cite{dong1995temporal} by not relying on nonlinearity, suggesting that these types can emerge alongside ON/OFF segregation.

Prediction and retrospection in saddle-point dynamics have been discussed in the context of unstable periodic orbits of chaotic attractors using the dominant mode of the local SKO (and its adjoint)~\cite{cvitanovic2012knowing,heninger2015neighborhoods}, whereas we focus on a subdominant singular function defining a coherent set pair~\cite{froyland2010transport,froyland2013analytic}. Saddle point analysis based on eigenfunctions instead of singular functions predicted neuronal filters orthogonal to growing exponentials, yielding predictive neurons without considering retrospection~\cite{golkar2024neuronal}.

Clustering has been previously proposed as a model of static neuronal computation on temporally uncorrelated inputs~\cite{pehlevan2017clustering}, capturing rectification and sparsity but not temporal receptive fields or sensory dynamics. Probabilistic coding frameworks such as the Bayesian brain hypothesis~\cite{ma2006bayesian} have been widely studied although we are not aware of the suggestion that single neurons represent eigenfunctions or singular functions of the SKO. Self-supervised learning has been applied to visual networks~\cite{singer2018sensory}, but these models typically omit analysis on the neuronal level. Koopman-based objectives have recently been incorporated into deep architectures to extract predictive features~\cite{choi2024koopman}. In contrast, we use SKO singular functions to define coherent sets~\cite{froyland2010transport,froyland2013analytic}, providing a direct explanation for rectification and a principled division into predictive and retrospective neurons.

Estimation of singular functions from data originated in molecular dynamics as the VAMP framework~\cite{wu2020variational}. VAMPnets~\cite{mardt2018vampnets} learn such functions with deep ReLU networks trained by backpropagation. In our model, features arise from rectified projections onto singular vectors within each neuron and can be hierarchically composed without backpropagation.","Viewing early sensory processing as efficient or predictive coding of natural stimuli has a long tradition~\cite{attneave1954some,barlow1961possible,simoncelli2001natural,price2022efficient,bialek2006efficient}. Closest to our work is the information bottleneck (IB) framework~\cite{tishby2000information,bialek2006efficient,palmer2015predictive,chalk2018toward}, which for Gaussian variables reduces to past–future CCA~\cite{chechik2003information}. Extending IB to dynamical systems and restricting compression to one bit corresponds to encoding the sign of the subdominant singular function of the SKO. Compared to slow feature analysis~\cite{wiskott2002slow,lipshutz2020biologically}, we incorporate an explicit dynamical-systems formulation and introduce a natural nonlinearity that enables hierarchical architectures without hand-crafted features.  Predictive and retrospective neuron classes have appeared in a lattice filter model of the visual pathway~\cite{gregor2012lattice}, though that work was limited to linear processing. Relative to predictive coding models~\cite{srinivasan1982predictive,rao1999predictive,druckmann2012mechanistic, gregor2012lattice}, where neurons emit prediction errors, our neurons encode coherent-set memberships. While the ON/OFF division has been attributed to metabolic efficiency~\cite{gjorgjieva2014benefits}, we propose a computational explanation. Our account of lagged and non-lagged LGN cells differs from earlier models~\cite{dong1995temporal} by not relying on nonlinearity, suggesting that these types can emerge alongside ON/OFF segregation.

Prediction and retrospection in saddle-point dynamics have been discussed in the context of unstable periodic orbits of chaotic attractors using the dominant mode of the local SKO (and its adjoint)~\cite{cvitanovic2012knowing,heninger2015neighborhoods}, whereas we focus on a subdominant singular function defining a coherent set pair~\cite{froyland2010transport,froyland2013analytic}. Saddle point analysis based on eigenfunctions instead of singular functions predicted neuronal filters orthogonal to growing exponentials, yielding predictive neurons without considering retrospection~\cite{golkar2024neuronal}.

Clustering has been previously proposed as a model of static neuronal computation on temporally uncorrelated inputs~\cite{pehlevan2017clustering}, capturing rectification and sparsity but not temporal receptive fields or sensory dynamics. Probabilistic coding frameworks such as the Bayesian brain hypothesis~\cite{ma2006bayesian} have been widely studied although we are not aware of the suggestion that single neurons represent eigenfunctions or singular functions of the SKO. Self-supervised learning has been applied to visual networks~\cite{singer2018sensory}, but these models typically omit analysis on the neuronal level. Koopman-based objectives have recently been incorporated into deep architectures to extract predictive features~\cite{choi2024koopman}. In contrast, we use SKO singular functions to define coherent sets~\cite{froyland2010transport,froyland2013analytic}, providing a direct explanation for rectification and a principled division into predictive and retrospective neurons.

Estimation of singular functions from data originated in molecular dynamics as the VAMP framework~\cite{wu2020variational}. VAMPnets~\cite{mardt2018vampnets} learn such functions with deep ReLU networks trained by backpropagation. In our model, features arise from rectified projections onto singular vectors within each neuron and can be hierarchically composed without backpropagation.",
2509.25171v1,http://arxiv.org/abs/2509.25171v1,2025-09-29 17:58:45+00:00,TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,"Reinforcement learning with stochastic optimal control offers a promising framework for diffusion fine-tuning, where a pre-trained diffusion model is optimized to generate paths that lead to a reward-tilted distribution. While these approaches enable optimization without access to explicit samples from the optimal distribution, they require training on rollouts under the current fine-tuned model, making them susceptible to reinforcing sub-optimal trajectories that yield poor rewards. To overcome this challenge, we introduce TRee Search Guided TRajectory-Aware Fine-Tuning for Discrete Diffusion (TR2-D2), a novel framework that optimizes reward-guided discrete diffusion trajectories with tree search to construct replay buffers for trajectory-aware fine-tuning. These buffers are generated using Monte Carlo Tree Search (MCTS) and subsequently used to fine-tune a pre-trained discrete diffusion model under a stochastic optimal control objective. We validate our framework on single- and multi-objective fine-tuning of biological sequence diffusion models, highlighting the overall effectiveness of TR2-D2 for reliable reward-guided fine-tuning in discrete sequence generation.","\label{app:Related Works}
\paragraph{Fine-Tuning Diffusion Models with Reinforcement Learning}
\label{app:Fine-Tuning Diffusion Models}
RL fine-tuning of diffusion has been used to train the model to generate data samples that optimize a reward function \citep{black2023training, Wallace_2024_CVPR, domingo2024adjoint, uehara2024understanding, fan2023dpok, clark2023directly, blessing2025trust}. Specifically, fine-tuning has been widely explored for text-to-image generation \citep{lu2023specialist, ruiz2023dreambooth, gupta2025simple, yuan2024self, fan2023reinforcement, liu2025flow, zheng2025diffusionnft} and biomolecular sequence design \citep{wang2025finetuning, zekri2025fine, cao2025glid}. The fine-tuning problem has commonly been framed as an entropy-regularized control problem \citep{uehara2024fine, han2024stochastic, tang2024fine, zhu2025mdns}, which seeks to find an optimal sampling trajectory that maximizes some terminal reward. Fine-tuning methods have also been developed specifically for discrete diffusion, with approaches that optimize differentiable rewards \citep{wang2025finetuning}, non-differentiable rewards \citep{zekri2025fine, cao2025glid, su2025iterative, zhu2025mdns}, and those tailored to diffusion language models \citep{zhao2025d1, gong2025diffucoder}. 

\paragraph{Discrete Diffusion Models}
\label{app:Discrete Diffusion}
Diffusion models have achieved state-of-the-art performance on generating various data modalities \citep{zhu2025trivialized, esser2024scaling, zhu2025diffusion, rojas2025diffuse, zheng2025direct}. Discrete diffusion models \citep{austin2021structured, campbell2022continuous, lou2023discrete}, as a natural generalization of diffusion models to finite state space, have emerged as powerful generative frameworks for sequence data, among which the most effective variant is Masked discrete diffusion models (MDM) \citep{sahoo2024simple, wang2024diffusion, shi2024simplified, peng2025path, tang2025peptune, nisonoff2025unlocking, rector-brooks2025steering, bai2025meissonic, shi2025muddit}. These models operate by progressively denoising masked inputs, enabling them to capture long-range dependencies without relying on autoregressive factorization. Within biology, masked discrete diffusion models have been successfully applied to peptide \citep{tang2025peptune, vincoff2025soapia}, protein \citep{wang2024diffusion, goel2025memdlm, nisonoff2025unlocking, rector-brooks2025steering, wang2025finetuning}, and nucleic acid design \citep{wang2025finetuning, patel2025multiobjectiveguided}. Furthermore, recent extensions have introduced blockwise discrete diffusion architectures that interpolate between autoregressive and diffusion models to improve training efficiency and sequence length generalization \citep{arriola2025block}, as well as simplified formulations of masked diffusion that provide tighter likelihood bounds and more effective training objectives \citep{schiff2025simple}.


\paragraph{Inference-Time Scaling of Diffusion Models}
\label{app:Inference-Time Scaling}
Inference-time scaling of diffusion models aims to efficiently leverage additional compute during sampling to improve output quality and controllability. One line of work steers continuous diffusion processes using Feynman–Kac guidance, which is theoretically guaranteed to sample from a reward-tilted distribution by reweighting trajectories at each denoising step \citep{skreta2025feynman, singhal2025general, chen2025solving}. Search-based approaches apply combinatorial optimization over diffusion trajectories to identify high-reward sequences \citep{sun2022molsearch}, while reward-gradient methods adapt score-function estimators to steer sampling \citep{song2020score, bansal2023universal}. Importance sampling techniques can also be used to bias toward rare high-reward generations, but require large sample sizes to ensure coverage \citep{chatterjee2018sample}. Soft value-based decoding has been proposed as a derivative-free approach for steering both continuous and discrete diffusion processes \citep{li2024derivative}. More recently, classical search methods have been incorporated into continuous diffusion sampling as a scaling technique during inference time \cite{jain2025diffusion, zhang2025inference}.

Classifier-based and classifier-free guidance methods have been adapted from continuous diffusion into the discrete domain \citep{nisonoff2025unlocking, rector-brooks2025steering, wang2024diffusion, schiff2025simple, rojas2025theory, guo2024plug}. Recent strategies for post-hoc optimization include classifier-free guidance (CFG) \citep{ho2022classifier}, LaMBO-2 and NOS guidance \citep{gruver2023protein}, and MCTS-guided sampling as in PepTune \citep{tang2025peptune} and SOAPIA \citep{vincoff2025soapia}, which adapt pretrained models to specific objectives strictly at inference time.

\paragraph{Multi-Objective Optimization}
\label{app:Related MOO}
Optimizing multiple, potentially conflicting, reward and constraint functions while balancing tradeoffs has significant applications across engineering and biology applications \citep{marler2004survey, jain2017biophysical, tagasovska2022pareto, zhu2023sample, janson2008molecular}. For molecular drug design, the objectives include affinity to the drug target, bioavailability, potency, solubility for efficient drug loading, non-toxicity, synthesizability, among others \citep{nicolaou2007molecular, fromer2023computer, sun2022molsearch, winter2019efficient, jin2020multi, xie2021mars}. Due to tradeoffs between objectives, there often does not exist a single solution that dominates across all objectives, but rather a set of optimal solutions where no objective can be improved without sacrificing another objective \citep{censor1977pareto}. To reduce the multi-objective problem into a more tractable single-objective problem, hypervolume (HV) has been used to quantify the optimality of a solution with respect to a reference point \citep{yang2019efficient, daulton2020differentiable, ament2023unexpected, daulton2022multi, konakovic2020diversity}. To sample from the Pareto-frontier, several approaches have been proposed, including active learning \citep{zuluaga2016pal, belakaria2020uncertainty}, entropy-based multi-objective Bayesian optimization \citep{wang2017max, suzuki2020multi, hernandez2016predictive, fernandez2020improved}, cumulative distribution function optimization \citep{park2023botied}, and constrained multi-objective optimization \citep{gelbart2014bayesian, li2024constrained}. More recently, multi-objective guidance frameworks have been used to steer generative models like LLM \citep{ren2024hyperdpo, ren2024multi}, diffusion \citep{gruver2023protein,yao2024proud, han2023training, yuan2024moduli, annadani2025preference, zhang2025pmodiff}, discrete diffusion \citep{tang2025peptune}, and flow matching \citep{jain2023multi, yuan2024paretoflow, chen2025multi} toward optimizing multiple objectives.","\paragraph{Fine-Tuning Diffusion Models with Reinforcement Learning}
RL fine-tuning of diffusion has been used to train the model to generate data samples that optimize a reward function \citep{black2023training, Wallace_2024_CVPR, domingo2024adjoint, uehara2024understanding, fan2023dpok, clark2023directly, blessing2025trust}. Specifically, fine-tuning has been widely explored for text-to-image generation \citep{lu2023specialist, ruiz2023dreambooth, gupta2025simple, yuan2024self, fan2023reinforcement, liu2025flow, zheng2025diffusionnft} and biomolecular sequence design \citep{wang2025finetuning, zekri2025fine, cao2025glid}. The fine-tuning problem has commonly been framed as an entropy-regularized control problem \citep{uehara2024fine, han2024stochastic, tang2024fine, zhu2025mdns}, which seeks to find an optimal sampling trajectory that maximizes some terminal reward. Fine-tuning methods have also been developed specifically for discrete diffusion, with approaches that optimize differentiable rewards \citep{wang2025finetuning}, non-differentiable rewards \citep{zekri2025fine, cao2025glid, su2025iterative, zhu2025mdns}, and those tailored to diffusion language models \citep{zhao2025d1, gong2025diffucoder}. 

\paragraph{Discrete Diffusion Models}
Diffusion models have achieved state-of-the-art performance on generating various data modalities \citep{zhu2025trivialized, esser2024scaling, zhu2025diffusion, rojas2025diffuse, zheng2025direct}. Discrete diffusion models \citep{austin2021structured, campbell2022continuous, lou2023discrete}, as a natural generalization of diffusion models to finite state space, have emerged as powerful generative frameworks for sequence data, among which the most effective variant is Masked discrete diffusion models (MDM) \citep{sahoo2024simple, wang2024diffusion, shi2024simplified, peng2025path, tang2025peptune, nisonoff2025unlocking, rector-brooks2025steering, bai2025meissonic, shi2025muddit}. These models operate by progressively denoising masked inputs, enabling them to capture long-range dependencies without relying on autoregressive factorization. Within biology, masked discrete diffusion models have been successfully applied to peptide \citep{tang2025peptune, vincoff2025soapia}, protein \citep{wang2024diffusion, goel2025memdlm, nisonoff2025unlocking, rector-brooks2025steering, wang2025finetuning}, and nucleic acid design \citep{wang2025finetuning, patel2025multiobjectiveguided}. Furthermore, recent extensions have introduced blockwise discrete diffusion architectures that interpolate between autoregressive and diffusion models to improve training efficiency and sequence length generalization \citep{arriola2025block}, as well as simplified formulations of masked diffusion that provide tighter likelihood bounds and more effective training objectives \citep{schiff2025simple}.


\paragraph{Inference-Time Scaling of Diffusion Models}
Inference-time scaling of diffusion models aims to efficiently leverage additional compute during sampling to improve output quality and controllability. One line of work steers continuous diffusion processes using Feynman–Kac guidance, which is theoretically guaranteed to sample from a reward-tilted distribution by reweighting trajectories at each denoising step \citep{skreta2025feynman, singhal2025general, chen2025solving}. Search-based approaches apply combinatorial optimization over diffusion trajectories to identify high-reward sequences \citep{sun2022molsearch}, while reward-gradient methods adapt score-function estimators to steer sampling \citep{song2020score, bansal2023universal}. Importance sampling techniques can also be used to bias toward rare high-reward generations, but require large sample sizes to ensure coverage \citep{chatterjee2018sample}. Soft value-based decoding has been proposed as a derivative-free approach for steering both continuous and discrete diffusion processes \citep{li2024derivative}. More recently, classical search methods have been incorporated into continuous diffusion sampling as a scaling technique during inference time \cite{jain2025diffusion, zhang2025inference}.

Classifier-based and classifier-free guidance methods have been adapted from continuous diffusion into the discrete domain \citep{nisonoff2025unlocking, rector-brooks2025steering, wang2024diffusion, schiff2025simple, rojas2025theory, guo2024plug}. Recent strategies for post-hoc optimization include classifier-free guidance (CFG) \citep{ho2022classifier}, LaMBO-2 and NOS guidance \citep{gruver2023protein}, and MCTS-guided sampling as in PepTune \citep{tang2025peptune} and SOAPIA \citep{vincoff2025soapia}, which adapt pretrained models to specific objectives strictly at inference time.

\paragraph{Multi-Objective Optimization}
Optimizing multiple, potentially conflicting, reward and constraint functions while balancing tradeoffs has significant applications across engineering and biology applications \citep{marler2004survey, jain2017biophysical, tagasovska2022pareto, zhu2023sample, janson2008molecular}. For molecular drug design, the objectives include affinity to the drug target, bioavailability, potency, solubility for efficient drug loading, non-toxicity, synthesizability, among others \citep{nicolaou2007molecular, fromer2023computer, sun2022molsearch, winter2019efficient, jin2020multi, xie2021mars}. Due to tradeoffs between objectives, there often does not exist a single solution that dominates across all objectives, but rather a set of optimal solutions where no objective can be improved without sacrificing another objective \citep{censor1977pareto}. To reduce the multi-objective problem into a more tractable single-objective problem, hypervolume (HV) has been used to quantify the optimality of a solution with respect to a reference point \citep{yang2019efficient, daulton2020differentiable, ament2023unexpected, daulton2022multi, konakovic2020diversity}. To sample from the Pareto-frontier, several approaches have been proposed, including active learning \citep{zuluaga2016pal, belakaria2020uncertainty}, entropy-based multi-objective Bayesian optimization \citep{wang2017max, suzuki2020multi, hernandez2016predictive, fernandez2020improved}, cumulative distribution function optimization \citep{park2023botied}, and constrained multi-objective optimization \citep{gelbart2014bayesian, li2024constrained}. More recently, multi-objective guidance frameworks have been used to steer generative models like LLM \citep{ren2024hyperdpo, ren2024multi}, diffusion \citep{gruver2023protein,yao2024proud, han2023training, yuan2024moduli, annadani2025preference, zhang2025pmodiff}, discrete diffusion \citep{tang2025peptune}, and flow matching \citep{jain2023multi, yuan2024paretoflow, chen2025multi} toward optimizing multiple objectives.",
2511.03113v1,http://arxiv.org/abs/2511.03113v1,2025-11-05 01:44:37+00:00,FP-AbDiff: Improving Score-based Antibody Design by Capturing Nonequilibrium Dynamics through the Underlying Fokker-Planck Equation,"Computational antibody design holds immense promise for therapeutic discovery, yet existing generative models are fundamentally limited by two core challenges: (i) a lack of dynamical consistency, which yields physically implausible structures, and (ii) poor generalization due to data scarcity and structural bias. We introduce FP-AbDiff, the first antibody generator to enforce Fokker-Planck Equation (FPE) physics along the entire generative trajectory. Our method minimizes a novel FPE residual loss over the mixed manifold of CDR geometries (R^3 x SO(3)), compelling locally-learned denoising scores to assemble into a globally coherent probability flow. This physics-informed regularizer is synergistically integrated with deep biological priors within a state-of-the-art SE(3)-equivariant diffusion framework. Rigorous evaluation on the RAbD benchmark confirms that FP-AbDiff establishes a new state-of-the-art. In de novo CDR-H3 design, it achieves a mean Root Mean Square Deviation of 0.99 Å when superposing on the variable region, a 25% improvement over the previous state-of-the-art model, AbX, and the highest reported Contact Amino Acid Recovery of 39.91%. This superiority is underscored in the more challenging six-CDR co-design task, where our model delivers consistently superior geometric precision, cutting the average full-chain Root Mean Square Deviation by ~15%, and crucially, achieves the highest full-chain Amino Acid Recovery on the functionally dominant CDR-H3 loop (45.67%). By aligning generative dynamics with physical laws, FP-AbDiff enhances robustness and generalizability, establishing a principled approach for physically faithful and functionally viable antibody design.","Computational antibody design has rapidly evolved from classical, energy-based methods to structure-aware generative frameworks. Early approaches, exemplified by methods like RosettaAntibodyDesign and AbDesign~\cite{adolf2018rosettaantibodydesign, kuroda2012computer, ruffolo2021deciphering}, relied on statistical energy functions and Monte Carlo sampling but were hampered by prohibitive computational costs and limited sampling efficacy over the vast conformational space. The emergence of protein language models~\cite{elnaggar2007prottrans, shin2021protein} enabled efficient, sequence-centric generation, treating proteins as textual inputs~\cite{xiong2025textguidedmultipropertymolecularoptimization}. However, such models often neglect spatial and geometric priors critical for antibody-antigen binding.


Recent advances have introduced geometric and equivariant models for joint sequence–structure design. Autoregressive methods (HERN~\cite{jin2022antibody}) and GNN-based models (MEAN, dyMEAN~\cite{kong2022conditional,kong2023end}) have shown strong performance in CDR co-design, while predictors like AlphaFold2~\cite{jumper2021highly} and SE(3)-Fold~\cite{norman2020thermometry,ahdritz2024openfold} yield high-fidelity structures but lack generative capacity. Diffusion-based models such as DiffAb~\cite{luo2022antigen} and AbDiffuser~\cite{martinkus2024abdiffuser} mark progress, yet often diverge from principled score-based formulations. AbX~\cite{zhu2024antibody} remains the only model to learn a continuous-time score field. However, all existing approaches, discrete or continuous, remain time-agnostic and lack dynamical consistency. Drawing on Fokker–Planck regularization~\cite{lai2023fp}, we present the first framework to impose physical self-consistency across the entire diffusion trajectory in antibody generation.




\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.95\textwidth]{figs/overview.jpg}
    \caption{
    Overview of FP-AbDiff. FP-AbDiff leverages a Continuous Time Markov Chain (CTMC) for CDR sequence modeling and a score-based diffusion framework for CDR structure generation. It incorporates physical and geometric constraints via a physics-informed loss derived from the Fokker–Planck Equation and applies evolutionary priors within the model architecture. The grey regions indicate the antigen and antibody framework, while the red regions highlight the designed CDRs in the antibody.
    }
    
    \label{fig:overview}
\end{figure*}","Computational antibody design has rapidly evolved from classical, energy-based methods to structure-aware generative frameworks. Early approaches, exemplified by methods like RosettaAntibodyDesign and AbDesign~\cite{adolf2018rosettaantibodydesign, kuroda2012computer, ruffolo2021deciphering}, relied on statistical energy functions and Monte Carlo sampling but were hampered by prohibitive computational costs and limited sampling efficacy over the vast conformational space. The emergence of protein language models~\cite{elnaggar2007prottrans, shin2021protein} enabled efficient, sequence-centric generation, treating proteins as textual inputs~\cite{xiong2025textguidedmultipropertymolecularoptimization}. However, such models often neglect spatial and geometric priors critical for antibody-antigen binding.


Recent advances have introduced geometric and equivariant models for joint sequence–structure design. Autoregressive methods (HERN~\cite{jin2022antibody}) and GNN-based models (MEAN, dyMEAN~\cite{kong2022conditional,kong2023end}) have shown strong performance in CDR co-design, while predictors like AlphaFold2~\cite{jumper2021highly} and SE(3)-Fold~\cite{norman2020thermometry,ahdritz2024openfold} yield high-fidelity structures but lack generative capacity. Diffusion-based models such as DiffAb~\cite{luo2022antigen} and AbDiffuser~\cite{martinkus2024abdiffuser} mark progress, yet often diverge from principled score-based formulations. AbX~\cite{zhu2024antibody} remains the only model to learn a continuous-time score field. However, all existing approaches, discrete or continuous, remain time-agnostic and lack dynamical consistency. Drawing on Fokker–Planck regularization~\cite{lai2023fp}, we present the first framework to impose physical self-consistency across the entire diffusion trajectory in antibody generation.",
2510.19484v1,http://arxiv.org/abs/2510.19484v1,2025-10-22 11:23:58+00:00,KnowMol: Advancing Molecular Large Language Models with Multi-Level Chemical Knowledge,"The molecular large language models have garnered widespread attention due to their promising potential on molecular applications. However, current molecular large language models face significant limitations in understanding molecules due to inadequate textual descriptions and suboptimal molecular representation strategies during pretraining. To address these challenges, we introduce KnowMol-100K, a large-scale dataset with 100K fine-grained molecular annotations across multiple levels, bridging the gap between molecules and textual descriptions. Additionally, we propose chemically-informative molecular representation, effectively addressing limitations in existing molecular representation strategies. Building upon these innovations, we develop KnowMol, a state-of-the-art multi-modal molecular large language model. Extensive experiments demonstrate that KnowMol achieves superior performance across molecular understanding and generation tasks.
  GitHub: https://github.com/yzf-code/KnowMol
  Huggingface: https://hf.co/datasets/yzf1102/KnowMol-100K","\noindent{\textbf{Molecule-text Data Enhancement by LLMs.}} \
In the realm of molecule-text multi-modality, various methods have explored leveraging LLMs to enhance molecule-text data. Early method \cite{chatmol} uses MolT5 \cite{molt5} to generate alternating dialogue data for CheBI-20 \cite{chebi_20}. Benefiting from the rapid progress of GPT models, \cite{3d-molm} utilized GPT-3.5 for semantic enrichment of sparse molecular descriptions in PubChem, while \cite{srinivas2024crossing} employed GPT-4 to refine the construction of molecular caption data for instruction-based tasks. In addition, \cite{artificiallyR_R} applied few-shot prompting, using PubChem molecular annotations as examples, to generate an ``artificially-real"" dataset with ChatGPT for domain adaptation. Another approach \cite{l_m24} combined multiple datasets with GPT-4 to construct templates and integrate them with original data to create diversified molecular descriptions.
Despite these diverse efforts, all the aforementioned methods rely on PubChem as their primary data source, which inherently limits the quality of the generated captions due to the original data shortcomings. In contrast, our approach utilizes advanced tools to construct a multi-level, fine-grained molecule-text dataset, overcoming the limitations within PubChem descriptions.

\noindent{\textbf{Molecule Graph Representation Learning for LLMs.}} 
To enable LLMs to handle molecule graphs,
several methods have been proposed to achieve informative graph representations. Early models \cite{momu,moleculestm,molfm,GIT-Mol} employ GNNs as molecular encoders and utilize cross-modal contrastive learning to align molecular and textual representation spaces. Subsequently, multi-modal architectures incorporating adapter-based mechanisms with LLMs have been explored. For example, models such as InstructMol \cite{instructmol} and DrugChat \cite{drugchat} integrate simple projection layers to map molecular features into the LLM input space, while architectures like MolCA \cite{molca} and 3D-MoLM \cite{3d-molm} leverage Q-Former \cite{blip-2} modules to bridge modality gaps. Recently, recognizing the limitations of existing molecular representation approaches, HIGHT \cite{HIGHT} and UniMoT \cite{UniMoT} have proposed specially designed tokenizers to enhance the quality of molecular representations. However, their approach employs complicated models, such as Vector Quantized Variational AutoEncoders (VQ-VAEs) \cite{vqvae} or Q-former \cite{blip-2}, necessitating an additional pretraining stage and significantly increasing computational complexity. 
Despite various attempts in model designs, a key limitation persists: how to improve molecular representation in both 1D and 2D modalities which is efficient and effective?","\noindent{\textbf{Molecule-text Data Enhancement by LLMs.}} \
In the realm of molecule-text multi-modality, various methods have explored leveraging LLMs to enhance molecule-text data. Early method \cite{chatmol} uses MolT5 \cite{molt5} to generate alternating dialogue data for CheBI-20 \cite{chebi_20}. Benefiting from the rapid progress of GPT models, \cite{3d-molm} utilized GPT-3.5 for semantic enrichment of sparse molecular descriptions in PubChem, while \cite{srinivas2024crossing} employed GPT-4 to refine the construction of molecular caption data for instruction-based tasks. In addition, \cite{artificiallyR_R} applied few-shot prompting, using PubChem molecular annotations as examples, to generate an ``artificially-real"" dataset with ChatGPT for domain adaptation. Another approach \cite{l_m24} combined multiple datasets with GPT-4 to construct templates and integrate them with original data to create diversified molecular descriptions.
Despite these diverse efforts, all the aforementioned methods rely on PubChem as their primary data source, which inherently limits the quality of the generated captions due to the original data shortcomings. In contrast, our approach utilizes advanced tools to construct a multi-level, fine-grained molecule-text dataset, overcoming the limitations within PubChem descriptions.

\noindent{\textbf{Molecule Graph Representation Learning for LLMs.}} 
To enable LLMs to handle molecule graphs,
several methods have been proposed to achieve informative graph representations. Early models \cite{momu,moleculestm,molfm,GIT-Mol} employ GNNs as molecular encoders and utilize cross-modal contrastive learning to align molecular and textual representation spaces. Subsequently, multi-modal architectures incorporating adapter-based mechanisms with LLMs have been explored. For example, models such as InstructMol \cite{instructmol} and DrugChat \cite{drugchat} integrate simple projection layers to map molecular features into the LLM input space, while architectures like MolCA \cite{molca} and 3D-MoLM \cite{3d-molm} leverage Q-Former \cite{blip-2} modules to bridge modality gaps. Recently, recognizing the limitations of existing molecular representation approaches, HIGHT \cite{HIGHT} and UniMoT \cite{UniMoT} have proposed specially designed tokenizers to enhance the quality of molecular representations. However, their approach employs complicated models, such as Vector Quantized Variational AutoEncoders (VQ-VAEs) \cite{vqvae} or Q-former \cite{blip-2}, necessitating an additional pretraining stage and significantly increasing computational complexity. 
Despite various attempts in model designs, a key limitation persists: how to improve molecular representation in both 1D and 2D modalities which is efficient and effective?",
2511.05630v1,http://arxiv.org/abs/2511.05630v1,2025-11-07 04:40:47+00:00,BrainCSD: A Hierarchical Consistency-Driven MoE Foundation Model for Unified Connectome Synthesis and Multitask Brain Trait Prediction,"Functional and structural connectivity (FC/SC) are key multimodal biomarkers for brain analysis, yet their clinical utility is hindered by costly acquisition, complex preprocessing, and frequent missing modalities. Existing foundation models either process single modalities or lack explicit mechanisms for cross-modal and cross-scale consistency. We propose BrainCSD, a hierarchical mixture-of-experts (MoE) foundation model that jointly synthesizes FC/SC biomarkers and supports downstream decoding tasks (diagnosis and prediction). BrainCSD features three neuroanatomically grounded components: (1) a ROI-specific MoE that aligns regional activations from canonical networks (e.g., DMN, FPN) with a global atlas via contrastive consistency; (2) a Encoding-Activation MOE that models dynamic cross-time/gradient dependencies in fMRI/dMRI; and (3) a network-aware refinement MoE that enforces structural priors and symmetry at individual and population levels. Evaluated on the datasets under complete and missing-modality settings, BrainCSD achieves SOTA results: 95.6\% accuracy for MCI vs. CN classification without FC, low synthesis error (FC RMSE: 0.038; SC RMSE: 0.006), brain age prediction (MAE: 4.04 years), and MMSE score estimation (MAE: 1.72 points). Code is available in \href{https://github.com/SXR3015/BrainCSD}{BrainCSD}","\noindent\textbf{Connectivity Biomarkers for Diagnosis.}
Functional (FC) and structural connectivity (SC) biomarkers offer superior generalizability and neurobiological interpretability over raw imaging features~\cite{shang2023optimization,li2023developing,cui2023adaptive}. Disruptions in canonical networks (e.g., DMN, FPN) correlate strongly with disorders like AD, PD, and ASD~\cite{ren2023stratifying,li2025riemannian,zhang2023detection}, capturing systems-level pathology that enhances cross-dataset robustness.

\noindent\textbf{Limitations of Neuroimaging Foundation Models.}
Current foundation models (e.g., BrainLM~\cite{carobrainlm}, BrainSN~\cite{yang2025foundational}, BrainMASS~\cite{yang2024brainmass}) fall into \textit{image-level} or \textit{biomarker-level} categories. While biomarker-level models show stronger diagnostic transferability, they do not explicitly model robustness to missing modalities or eliminate dependency on slow, preprocessing-heavy pipelines, two critical requirements for real-world deployment that BrainCSD addresses by directly synthesizing FC/SC from uni-modality inputs.

\noindent\textbf{MoE with Neuroscientific Priors.}
MoE architectures excel in scalable representation learning~\cite{rajbhandari2022deepspeed,li2025uni}, yet generic routing mechanisms (e.g., top-$k$, attention-gating) often overlook domain-specific structure. As BrainMASS demonstrates, embedding neuroanatomical priors (e.g., atlas-based positional embeddings) improves performance. BrainCSD advances this by explicitly aligning MoE expert routing with canonical brain networks (DMN, FPN, CON, etc.) and temporal/gradient dynamics — ensuring that each expert specializes in a biologically meaningful subsystem.

\noindent\textbf{Synthesis Without Diagnostic Validation.}
Existing FC/SC synthesis methods, based on GANs, diffusion models, or VAEs~\cite{chen2025joint,zuo2024u,zhao2025diffusion,guan2025spatio,yuan2024remind} — primarily optimize statistical or visual fidelity. Few validate whether synthesized biomarkers preserve diagnostic signal, and even fewer evaluate under missing-modality settings. BrainCSD bridges this gap by jointly optimizing synthesis quality \textit{and} downstream tasks across complete and incomplete data regimes.","\noindent\textbf{Connectivity Biomarkers for Diagnosis.}
Functional (FC) and structural connectivity (SC) biomarkers offer superior generalizability and neurobiological interpretability over raw imaging features~\cite{shang2023optimization,li2023developing,cui2023adaptive}. Disruptions in canonical networks (e.g., DMN, FPN) correlate strongly with disorders like AD, PD, and ASD~\cite{ren2023stratifying,li2025riemannian,zhang2023detection}, capturing systems-level pathology that enhances cross-dataset robustness.

\noindent\textbf{Limitations of Neuroimaging Foundation Models.}
Current foundation models (e.g., BrainLM~\cite{carobrainlm}, BrainSN~\cite{yang2025foundational}, BrainMASS~\cite{yang2024brainmass}) fall into \textit{image-level} or \textit{biomarker-level} categories. While biomarker-level models show stronger diagnostic transferability, they do not explicitly model robustness to missing modalities or eliminate dependency on slow, preprocessing-heavy pipelines, two critical requirements for real-world deployment that BrainCSD addresses by directly synthesizing FC/SC from uni-modality inputs.

\noindent\textbf{MoE with Neuroscientific Priors.}
MoE architectures excel in scalable representation learning~\cite{rajbhandari2022deepspeed,li2025uni}, yet generic routing mechanisms (e.g., top-$k$, attention-gating) often overlook domain-specific structure. As BrainMASS demonstrates, embedding neuroanatomical priors (e.g., atlas-based positional embeddings) improves performance. BrainCSD advances this by explicitly aligning MoE expert routing with canonical brain networks (DMN, FPN, CON, etc.) and temporal/gradient dynamics — ensuring that each expert specializes in a biologically meaningful subsystem.

\noindent\textbf{Synthesis Without Diagnostic Validation.}
Existing FC/SC synthesis methods, based on GANs, diffusion models, or VAEs~\cite{chen2025joint,zuo2024u,zhao2025diffusion,guan2025spatio,yuan2024remind} — primarily optimize statistical or visual fidelity. Few validate whether synthesized biomarkers preserve diagnostic signal, and even fewer evaluate under missing-modality settings. BrainCSD bridges this gap by jointly optimizing synthesis quality \textit{and} downstream tasks across complete and incomplete data regimes.",
2510.02903v1,http://arxiv.org/abs/2510.02903v1,2025-10-03 11:15:16+00:00,Learning Explicit Single-Cell Dynamics Using ODE Representations,"Modeling the dynamics of cellular differentiation is fundamental to advancing the understanding and treatment of diseases associated with this process, such as cancer. With the rapid growth of single-cell datasets, this has also become a particularly promising and active domain for machine learning. Current state-of-the-art models, however, rely on computationally expensive optimal transport preprocessing and multi-stage training, while also not discovering explicit gene interactions. To address these challenges we propose Cell-Mechanistic Neural Networks (Cell-MNN), an encoder-decoder architecture whose latent representation is a locally linearized ODE governing the dynamics of cellular evolution from stem to tissue cells. Cell-MNN is fully end-to-end (besides a standard PCA pre-processing) and its ODE representation explicitly learns biologically consistent and interpretable gene interactions. Empirically, we show that Cell-MNN achieves competitive performance on single-cell benchmarks, surpasses state-of-the-art baselines in scaling to larger datasets and joint training across multiple datasets, while also learning interpretable gene interactions that we validate against the TRRUST database of gene interactions.","\paragraph{Single-cell Interpolation.} The single-cell trajectory inference problem, as formalized by \citet{lavenant2023mathematicaltheorytrajectoryinference}, entails reconstructing continuous dynamics from snapshot data. Early work based on recurrent neural networks \citep{hashimoto2016rnn4sc} was followed by NeuralODE-based methods \citep{tong2020trajectorynet, tong2023trajnet_application, zhang2024scnode, koshizuka2023nlsb, huguet2022mioflow}, in which a neural network outputs the velocity field governing the dynamics. In contrast, Cell-MNN predicts an explicit local dynamics model, which not only facilitates the learning of gene interactions but also circumvents the need for numerical ODE solvers. A separate line of work avoids simulation by relying on OT preprocessing to approximate cell trajectories~\citep{schiebinger2019waddingtonot, bunne2021jkonet}, which were also used to train flow-matching models such as by \cite{tong2024otcfm, kapusniak2024metric, zhang2025deepruot, wang2025jointvelocitygrowthflowmatching, terpin2024lightspeed}. However, solving the OT coupling with the Sinkhorn algorithm scales quadratically in the number of samples, creating a major bottleneck for large datasets, which is why \citet{tong2024otcfm} proposed batch-wise approximation. To address this scalability bottleneck, Cell-MNN is designed to eliminate OT preprocessing entirely. Furthermore, SOTA OT-based models such as OT-MFM and DeepRUOT rely on multiple training stages beyond a standard PCA dimensionality reduction, which complicates amortized training across datasets. In contrast, Cell-MNN involves only a single training stage while achieving competitive performance on single-cell benchmarks. Finally, Action Matching~\citep{neklyudov2023actionmatching} also avoids OT preprocessing, but unlike Cell-MNN, it does not learn an explicit form of the underlying dynamics.

\paragraph{Gene Regulatory Network Discovery.} A complementary line of work assumes that the interactions governing cell differentiation can be represented as a graph, known as a \textit{gene regulatory network} (GRN) \citep{Davidson2002GRN}. \citet{tong2024sf2m} demonstrated that such GRNs can to some extent be recovered from flow-matching models in the setting of low-dimensional synthetic data as simulated by \citet{Pratapa2020beeline}. In contrast, we show that Cell-MNN learns biologically plausible gene interactions directly from \textit{real} single-cell data, validating them against the literature-curated TRRUST database. Additional approaches for GRN discovery include tree-based methods~\citep{thu2010genie3, moerman2018grnboost2}, information-theoretic approaches \citep{chan2017pidc}, regression-based time-series models \citep{Lu2021causal}, Gaussian processes \citep{aijoe2009grn_GP} and ODE-based models such as PerturbODE~\citep{lin2025interpretableneuralodesgene}. However, unlike Cell-MNN, these methods typically learn a static GRN and, to our knowledge, they are either inapplicable to single-cell interpolation benchmarks or do not deliver competitive performance.

\paragraph{ODE Discovery.}\looseness=-1The idea to learn an explicit ODE representation of the cell differentiation dynamics as pursued by PerturbODE~\citep{lin2025interpretableneuralodesgene} and Cell-MNN relates directly to the broader problem of ODE discovery. A seminal method in this area is SINDy~\citep{brunton2016sindy}, which infers governing equations from data but requires access to full trajectories, making it unsuitable for the snapshot-based single-cell setting. Similar limitations apply to more recent approaches such as MNN and related methods such as ODEFormer~\citep{pervez2024mechanistic, chen2025scalablemnn, yao2024marrying_causal_w_dynamics, ascoli2024odeformer}, which extend ODE discovery to amortized settings by using neural networks to predict the underlying dynamics from observed trajectories. In contrast, Cell-MNN is qualitatively distinct in learning dynamics from population data. It furthermore learns them in \textit{locally linear} form, an idea with strong precedents in physics and control theory such as the Apollo navigation filter \citep{schmidt1966statespace4navigation}, the control of a 2-link 6-muscle arm model \citep{li2004ilqr, todorov2005ilqg} and rocket landing \citep{szmuk2020rocketlanding}. The locally linear parameterization theoretically imposes learning control-oriented structure and, in principle, supports the design of performant controllers as described by~\citep{spencer2023control_oriented_structure}, which could enable design of gene perturbations.","\paragraph{Single-cell Interpolation.} The single-cell trajectory inference problem, as formalized by \citet{lavenant2023mathematicaltheorytrajectoryinference}, entails reconstructing continuous dynamics from snapshot data. Early work based on recurrent neural networks \citep{hashimoto2016rnn4sc} was followed by NeuralODE-based methods \citep{tong2020trajectorynet, tong2023trajnet_application, zhang2024scnode, koshizuka2023nlsb, huguet2022mioflow}, in which a neural network outputs the velocity field governing the dynamics. In contrast, Cell-MNN predicts an explicit local dynamics model, which not only facilitates the learning of gene interactions but also circumvents the need for numerical ODE solvers. A separate line of work avoids simulation by relying on OT preprocessing to approximate cell trajectories~\citep{schiebinger2019waddingtonot, bunne2021jkonet}, which were also used to train flow-matching models such as by \cite{tong2024otcfm, kapusniak2024metric, zhang2025deepruot, wang2025jointvelocitygrowthflowmatching, terpin2024lightspeed}. However, solving the OT coupling with the Sinkhorn algorithm scales quadratically in the number of samples, creating a major bottleneck for large datasets, which is why \citet{tong2024otcfm} proposed batch-wise approximation. To address this scalability bottleneck, Cell-MNN is designed to eliminate OT preprocessing entirely. Furthermore, SOTA OT-based models such as OT-MFM and DeepRUOT rely on multiple training stages beyond a standard PCA dimensionality reduction, which complicates amortized training across datasets. In contrast, Cell-MNN involves only a single training stage while achieving competitive performance on single-cell benchmarks. Finally, Action Matching~\citep{neklyudov2023actionmatching} also avoids OT preprocessing, but unlike Cell-MNN, it does not learn an explicit form of the underlying dynamics.

\paragraph{Gene Regulatory Network Discovery.} A complementary line of work assumes that the interactions governing cell differentiation can be represented as a graph, known as a \textit{gene regulatory network} (GRN) \citep{Davidson2002GRN}. \citet{tong2024sf2m} demonstrated that such GRNs can to some extent be recovered from flow-matching models in the setting of low-dimensional synthetic data as simulated by \citet{Pratapa2020beeline}. In contrast, we show that Cell-MNN learns biologically plausible gene interactions directly from \textit{real} single-cell data, validating them against the literature-curated TRRUST database. Additional approaches for GRN discovery include tree-based methods~\citep{thu2010genie3, moerman2018grnboost2}, information-theoretic approaches \citep{chan2017pidc}, regression-based time-series models \citep{Lu2021causal}, Gaussian processes \citep{aijoe2009grn_GP} and ODE-based models such as PerturbODE~\citep{lin2025interpretableneuralodesgene}. However, unlike Cell-MNN, these methods typically learn a static GRN and, to our knowledge, they are either inapplicable to single-cell interpolation benchmarks or do not deliver competitive performance.

\paragraph{ODE Discovery.}\looseness=-1The idea to learn an explicit ODE representation of the cell differentiation dynamics as pursued by PerturbODE~\citep{lin2025interpretableneuralodesgene} and Cell-MNN relates directly to the broader problem of ODE discovery. A seminal method in this area is SINDy~\citep{brunton2016sindy}, which infers governing equations from data but requires access to full trajectories, making it unsuitable for the snapshot-based single-cell setting. Similar limitations apply to more recent approaches such as MNN and related methods such as ODEFormer~\citep{pervez2024mechanistic, chen2025scalablemnn, yao2024marrying_causal_w_dynamics, ascoli2024odeformer}, which extend ODE discovery to amortized settings by using neural networks to predict the underlying dynamics from observed trajectories. In contrast, Cell-MNN is qualitatively distinct in learning dynamics from population data. It furthermore learns them in \textit{locally linear} form, an idea with strong precedents in physics and control theory such as the Apollo navigation filter \citep{schmidt1966statespace4navigation}, the control of a 2-link 6-muscle arm model \citep{li2004ilqr, todorov2005ilqg} and rocket landing \citep{szmuk2020rocketlanding}. The locally linear parameterization theoretically imposes learning control-oriented structure and, in principle, supports the design of performant controllers as described by~\citep{spencer2023control_oriented_structure}, which could enable design of gene perturbations.",
2510.05747v1,http://arxiv.org/abs/2510.05747v1,2025-10-07 10:05:54+00:00,Physicochemically Informed Dual-Conditioned Generative Model of T-Cell Receptor Variable Regions for Cellular Therapy,"Physicochemically informed biological sequence generation has the potential to accelerate computer-aided cellular therapy, yet current models fail to \emph{jointly} ensure novelty, diversity, and biophysical plausibility when designing variable regions of T-cell receptors (TCRs). We present \textbf{PhysicoGPTCR}, a large generative protein Transformer that is \emph{dual-conditioned} on peptide and HLA context and trained to autoregressively synthesise TCR sequences while embedding residue-level physicochemical descriptors. The model is optimised on curated TCR--peptide--HLA triples with a maximum-likelihood objective and compared against ANN, GPTCR, LSTM, and VAE baselines. Across multiple neoantigen benchmarks, PhysicoGPTCR substantially improves edit-distance, similarity, and longest-common-subsequence scores, while populating a broader region of sequence space. Blind in-silico docking and structural modelling further reveal a higher proportion of binding-competent clones than the strongest baseline, validating the benefit of explicit context conditioning and physicochemical awareness. Experimental results demonstrate that dual-conditioned, physics-grounded generative modelling enables end-to-end design of functional TCR candidates, reducing the discovery timeline from months to minutes without sacrificing wet-lab verifiability.","\paragraph{HLA--peptide specificity prediction.}
Early studies cast T-cell recognition as a \emph{discriminative} task.
NetTCR-2.0, DeepTCR and TITAN~\citep{montemurro2021nettcr,sidhom2021deeptcr,weber2021titan}
use convolutional, recurrent or attention networks to decide whether a
query receptor recognises a given HLA--peptide complex.
Although AUC scores keep improving, these classifiers are
inherently non-generative and cannot propose novel receptors for
TCR-T therapy; moreover, they view sequences as mere symbol strings and
ignore the residue--residue couplings that ultimately drive binding.

\paragraph{Generative modelling of TCRs.}
Only a handful of attempts move beyond classification.
TESSAR~\citep{zhang2021mapping}
explore unsupervised reconstruction but focus on receptor repertoires
without conditioning on antigen context.
More recently, TCRGPT~\citep{lin2024tcr} autoregressively samples CDR3 loops
conditioned \emph{solely} on the target peptide, leaving the HLA allele
unaddressed.
None of these works evaluates \emph{dry-lab activation rate}
via docking or molecular simulation, and therefore their therapeutic
utility remains unclear.

\paragraph{Protein sequence generation at large.}
Transformer language models such as
ProGen, ProtGPT2 and ESM-1v~\citep{madani2020progen,ferruz2022protgpt2,meier2021language}
demonstrate that pure sequence modelling can create functional enzymes
and antibodies.
Structure-aware approaches extend this panorama:
RFdiffusion~\citep{watson2023novo} and Chroma~\citep{singh2024chroma}
design full-atom backbones directly,
while ESM-IF refines inverse folding with iterative hallucination.
Yet these generators are trained on broad protein corpora
without any immunological signal, offering no mechanism to bias outputs
toward HLA--peptide interfaces.

\paragraph{Physicochemical or structural priors.}
ProteinMPNN, Atom3D and ESM-Fold re-design~\citep{dauparas2022robust,hayat2015all,lin2023evolutionary}
inject backbone geometry or energy-inspired terms into sequence design;
Rosetta-guided pipelines~\citep{liu2006rosettadesign} combine supervised scoring
with Monte-Carlo sampling.
Such methods improve foldability but assume a
pre-existing 3-D structure, rarely available for highly diverse TCR
variable regions, and they do not incorporate the
dual antigen--HLA conditioning crucial for immunotherapy.

\paragraph{Gap.}
In summary, prior studies do not \emph{simultaneously}
(1) condition on both peptide and HLA context,
(2) embed residue-level physicochemical descriptors during generation, and
(3) report dry-lab activation against realistic antigen panels.
Our work closes this gap and further benchmarks against
ANN retrieval, GPTCR, LSTM and VAE baselines, highlighting gains that
translate into higher docking-based activation (such as pmtnet or PISTE~\citep{lu2021deep,feng2024sliding}) without extra filtering.","\paragraph{HLA--peptide specificity prediction.}
Early studies cast T-cell recognition as a \emph{discriminative} task.
NetTCR-2.0, DeepTCR and TITAN~\citep{montemurro2021nettcr,sidhom2021deeptcr,weber2021titan}
use convolutional, recurrent or attention networks to decide whether a
query receptor recognises a given HLA--peptide complex.
Although AUC scores keep improving, these classifiers are
inherently non-generative and cannot propose novel receptors for
TCR-T therapy; moreover, they view sequences as mere symbol strings and
ignore the residue--residue couplings that ultimately drive binding.

\paragraph{Generative modelling of TCRs.}
Only a handful of attempts move beyond classification.
TESSAR~\citep{zhang2021mapping}
explore unsupervised reconstruction but focus on receptor repertoires
without conditioning on antigen context.
More recently, TCRGPT~\citep{lin2024tcr} autoregressively samples CDR3 loops
conditioned \emph{solely} on the target peptide, leaving the HLA allele
unaddressed.
None of these works evaluates \emph{dry-lab activation rate}
via docking or molecular simulation, and therefore their therapeutic
utility remains unclear.

\paragraph{Protein sequence generation at large.}
Transformer language models such as
ProGen, ProtGPT2 and ESM-1v~\citep{madani2020progen,ferruz2022protgpt2,meier2021language}
demonstrate that pure sequence modelling can create functional enzymes
and antibodies.
Structure-aware approaches extend this panorama:
RFdiffusion~\citep{watson2023novo} and Chroma~\citep{singh2024chroma}
design full-atom backbones directly,
while ESM-IF refines inverse folding with iterative hallucination.
Yet these generators are trained on broad protein corpora
without any immunological signal, offering no mechanism to bias outputs
toward HLA--peptide interfaces.

\paragraph{Physicochemical or structural priors.}
ProteinMPNN, Atom3D and ESM-Fold re-design~\citep{dauparas2022robust,hayat2015all,lin2023evolutionary}
inject backbone geometry or energy-inspired terms into sequence design;
Rosetta-guided pipelines~\citep{liu2006rosettadesign} combine supervised scoring
with Monte-Carlo sampling.
Such methods improve foldability but assume a
pre-existing 3-D structure, rarely available for highly diverse TCR
variable regions, and they do not incorporate the
dual antigen--HLA conditioning crucial for immunotherapy.

\paragraph{Gap.}
In summary, prior studies do not \emph{simultaneously}
(1) condition on both peptide and HLA context,
(2) embed residue-level physicochemical descriptors during generation, and
(3) report dry-lab activation against realistic antigen panels.
Our work closes this gap and further benchmarks against
ANN retrieval, GPTCR, LSTM and VAE baselines, highlighting gains that
translate into higher docking-based activation (such as pmtnet or PISTE~\citep{lu2021deep,feng2024sliding}) without extra filtering.",
2510.23273v1,http://arxiv.org/abs/2510.23273v1,2025-10-27 12:33:01+00:00,A Novel Framework for Multi-Modal Protein Representation Learning,"Accurate protein function prediction requires integrating heterogeneous intrinsic signals (e.g., sequence and structure) with noisy extrinsic contexts (e.g., protein-protein interactions and GO term annotations). However, two key challenges hinder effective fusion: (i) cross-modal distributional mismatch among embeddings produced by pre-trained intrinsic encoders, and (ii) noisy relational graphs of extrinsic data that degrade GNN-based information aggregation. We propose Diffused and Aligned Multi-modal Protein Embedding (DAMPE), a unified framework that addresses these through two core mechanisms. First, we propose Optimal Transport (OT)-based representation alignment that establishes correspondence between intrinsic embedding spaces of different modalities, effectively mitigating cross-modal heterogeneity. Second, we develop a Conditional Graph Generation (CGG)-based information fusion method, where a condition encoder fuses the aligned intrinsic embeddings to provide informative cues for graph reconstruction. Meanwhile, our theoretical analysis implies that the CGG objective drives this condition encoder to absorb graph-aware knowledge into its produced protein representations. Empirically, DAMPE outperforms or matches state-of-the-art methods such as DPFunc on standard GO benchmarks, achieving AUPR gains of 0.002-0.013 pp and Fmax gains 0.004-0.007 pp. Ablation studies further show that OT-based alignment contributes 0.043-0.064 pp AUPR, while CGG-based fusion adds 0.005-0.111 pp Fmax. Overall, DAMPE offers a scalable and theoretically grounded approach for robust multi-modal protein representation learning, substantially enhancing protein function prediction.","\label{sec:related}
In this section, we present a brief summary of protein representation learning methods and their application in protein function prediction.

\noindent \textbf{Sequence encoders}
Sequence encoders have evolved significantly to capture discriminative features from amino acid sequences. Early approaches leveraged Convolutional Neural Network (CNN) to extract local motifs~\cite{CNN_Shanehsazzadeh}, while Long Short-Term Memory (LSTM) networks addressed long-range dependencies, with hybrid CNN-LSTM models further improving performance~\cite{LSTM_wang2016protein}. However, the most impactful breakthroughs came with Transformer-based Protein Language Models (PLMs), such as ESM~\cite{ESM_Rives2021} and ProtTrans~\cite{elnaggar2021prottrans}, which learn contextual embeddings from massive sequence datasets and dominate most downstream tasks. 

\noindent \textbf{Structure encoders}
Structure encoders capture the spatial and geometric properties of protein conformations and are commonly classified by their basic geometric primitive: residue-level models such as GearNet~\cite{zhang2022gearnet} and DeepFRI~\cite{deepfri_gligorijevic2021structure}; atom-level methods that operate on atomic point clouds or via 3D convolutions and thus offer higher chemical granularity at greater computational cost (e.g., IEConv~\cite{hermosilla2021ieconv}); and surface-based encoders that represent molecular surfaces as meshes or surface patches to emphasize interface geometry and chemistry (e.g., dMaSIF~\cite{dmasif_sverrisson2021fast}).
Residue-level models typically treat residues as graph nodes with edges defined by spatial proximity or biochemical contacts, whereas atom-level and surface-based approaches encode finer geometric/chemical detail at the cost of increased computation and data requirements.
Recent self-supervised pretraining (contrastive and diffusion-based) has further improved the transferability of these geometric representations~\cite{hermosilla2022contrastive,xu2023pretraining}. 
% list and cite related works

\noindent \textbf{Multi-modal Representation Learners} Multi-modal approaches integrate sequence and structure information to overcome the limitations of single-modality methods. LM-GVP~\cite{Wang2022LMGVP} combines PLMs with geometric vector perceptrons to infuse structural context into sequence embeddings, improving performance on stability prediction tasks. ESM-GearNet~\cite{zhang2023systematic} systematically compared three fusion paradigms between ESM-2 and structural encoders~\cite{gvp_Jing2021,zhang2022gearnet,cdconv_Fan2023}: \textit{Serial} fusion, where sequence representations are injected as residue features into the structure encoder; \textit{Parallel} fusion, where sequence and structure embeddings are concatenated; and \textit{Cross} fusion, which integrates the two modalities via multi-head self-attention. 
The study finds that serial fusion—i.e., augmenting geometric models with PLM-derived residue features—yields the best performance on various downstream tasks.
Similarly, SST-ResNet~\cite{zhang2023sst} employs multi-scale fusion to integrate sequence and structure, outperforming single-modality models in property prediction.
Notably, while serial fusion demonstrates superior performance, its practical scalability is constrained by heavy computational overhead—integrating large-scale PLMs with structural encoders often results in oversized model architectures, making full joint training computationally prohibitive, especially for large datasets or resource-limited settings.

\noindent \textbf{Protein function prediction}
Protein function prediction has benefited from these representation advances.
Transformer-based methods like TALE~\cite{hou2021tale} jointly embed sequences and hierarchical function labels, while ATGO~\cite{zhang2022atgo} combines PLMs with triplet networks for accurate Gene Ontology (GO) prediction. DPFunc~\cite{DPFunc_NatCommun2024} integrates sequence, structure, and domain information via cross-attention to enhance interpretability and accuracy.
These methods highlight the growing importance of rich, multi-source representations in advancing functional annotation.
In contrast, our work focuses on integrating protein-protein interaction (PPI) networks and GO annotations through conditional graph generation-based information fusion, offering a novel approach to leverage both interaction and functional knowledge for more comprehensive and accurate protein function prediction.","In this section, we present a brief summary of protein representation learning methods and their application in protein function prediction.

\noindent \textbf{Sequence encoders}
Sequence encoders have evolved significantly to capture discriminative features from amino acid sequences. Early approaches leveraged Convolutional Neural Network (CNN) to extract local motifs~\cite{CNN_Shanehsazzadeh}, while Long Short-Term Memory (LSTM) networks addressed long-range dependencies, with hybrid CNN-LSTM models further improving performance~\cite{LSTM_wang2016protein}. However, the most impactful breakthroughs came with Transformer-based Protein Language Models (PLMs), such as ESM~\cite{ESM_Rives2021} and ProtTrans~\cite{elnaggar2021prottrans}, which learn contextual embeddings from massive sequence datasets and dominate most downstream tasks. 

\noindent \textbf{Structure encoders}
Structure encoders capture the spatial and geometric properties of protein conformations and are commonly classified by their basic geometric primitive: residue-level models such as GearNet~\cite{zhang2022gearnet} and DeepFRI~\cite{deepfri_gligorijevic2021structure}; atom-level methods that operate on atomic point clouds or via 3D convolutions and thus offer higher chemical granularity at greater computational cost (e.g., IEConv~\cite{hermosilla2021ieconv}); and surface-based encoders that represent molecular surfaces as meshes or surface patches to emphasize interface geometry and chemistry (e.g., dMaSIF~\cite{dmasif_sverrisson2021fast}).
Residue-level models typically treat residues as graph nodes with edges defined by spatial proximity or biochemical contacts, whereas atom-level and surface-based approaches encode finer geometric/chemical detail at the cost of increased computation and data requirements.
Recent self-supervised pretraining (contrastive and diffusion-based) has further improved the transferability of these geometric representations~\cite{hermosilla2022contrastive,xu2023pretraining}. 


\noindent \textbf{Multi-modal Representation Learners} Multi-modal approaches integrate sequence and structure information to overcome the limitations of single-modality methods. LM-GVP~\cite{Wang2022LMGVP} combines PLMs with geometric vector perceptrons to infuse structural context into sequence embeddings, improving performance on stability prediction tasks. ESM-GearNet~\cite{zhang2023systematic} systematically compared three fusion paradigms between ESM-2 and structural encoders~\cite{gvp_Jing2021,zhang2022gearnet,cdconv_Fan2023}: \textit{Serial} fusion, where sequence representations are injected as residue features into the structure encoder; \textit{Parallel} fusion, where sequence and structure embeddings are concatenated; and \textit{Cross} fusion, which integrates the two modalities via multi-head self-attention. 
The study finds that serial fusion—i.e., augmenting geometric models with PLM-derived residue features—yields the best performance on various downstream tasks.
Similarly, SST-ResNet~\cite{zhang2023sst} employs multi-scale fusion to integrate sequence and structure, outperforming single-modality models in property prediction.
Notably, while serial fusion demonstrates superior performance, its practical scalability is constrained by heavy computational overhead—integrating large-scale PLMs with structural encoders often results in oversized model architectures, making full joint training computationally prohibitive, especially for large datasets or resource-limited settings.

\noindent \textbf{Protein function prediction}
Protein function prediction has benefited from these representation advances.
Transformer-based methods like TALE~\cite{hou2021tale} jointly embed sequences and hierarchical function labels, while ATGO~\cite{zhang2022atgo} combines PLMs with triplet networks for accurate Gene Ontology (GO) prediction. DPFunc~\cite{DPFunc_NatCommun2024} integrates sequence, structure, and domain information via cross-attention to enhance interpretability and accuracy.
These methods highlight the growing importance of rich, multi-source representations in advancing functional annotation.
In contrast, our work focuses on integrating protein-protein interaction (PPI) networks and GO annotations through conditional graph generation-based information fusion, offering a novel approach to leverage both interaction and functional knowledge for more comprehensive and accurate protein function prediction.",
2511.05510v1,http://arxiv.org/abs/2511.05510v1,2025-10-24 13:11:47+00:00,TEMPO: Temporal Multi-scale Autoregressive Generation of Protein Conformational Ensembles,"Understanding the dynamic behavior of proteins is critical to elucidating their functional mechanisms, yet generating realistic, temporally coherent trajectories of protein ensembles remains a significant challenge. In this work, we introduce a novel hierarchical autoregressive framework for modeling protein dynamics that leverages the intrinsic multi-scale organization of molecular motions. Unlike existing methods that focus on generating static conformational ensembles or treat dynamic sampling as an independent process, our approach characterizes protein dynamics as a Markovian process. The framework employs a two-scale architecture: a low-resolution model captures slow, collective motions driving major conformational transitions, while a high-resolution model generates detailed local fluctuations conditioned on these large-scale movements. This hierarchical design ensures that the causal dependencies inherent in protein dynamics are preserved, enabling the generation of temporally coherent and physically realistic trajectories. By bridging high-level biophysical principles with state-of-the-art generative modeling, our approach provides an efficient framework for simulating protein dynamics that balances computational efficiency with physical accuracy.","\textbf{Protein Ensemble Generation.}
Recent advances in deep learning have revolutionized the generation of protein conformational ensembles. Traditional approaches rely on MSA subsampling with AlphaFold2~\cite{potapenko2021highly}, which provides limited control over conformational diversity. Modern deep learning methods have introduced more sophisticated techniques. AlphaFlow~\cite{jing2024alphafold} fine-tunes single-state predictors under a flow matching framework to generate protein conformational ensembles. ESMFlow~\cite{jing2024alphafold} extends this approach by leveraging protein language models. BioEMU~\cite{lewis2024scalable} employs a diffusion-based framework to generate thermodynamically accurate ensembles. ConfDiff~\cite{wang2024protein} incorporates force-guided networks with diffusion models to enhance generation fidelity, while Str2Str~\cite{lustr2str} introduces a structure-to-structure translation framework with roto-translation equivariance. However, these methods primarily focus on generating conformational ensembles that match equilibrium distributions, without explicitly modeling the temporal evolution of protein structures.

\textbf{Learning Molecular Dynamics.}
Machine learning approaches have emerged as powerful tools for accelerating and enhancing molecular dynamics simulations. VAMPNet~\cite{mardt2022deep} pioneered the use of variational approaches for Markov processes in molecular kinetics. Recent works like DiffMD~\cite{wu2023diffmd} employ diffusion models to estimate conformational density gradients, while DFF~\cite{arts2023two} establishes connections between score-based generative models and molecular force fields. The Distributional Graphformer (DiG)~\cite{zheng2024predicting} predicts equilibrium distributions of molecular systems, enabling efficient conformational sampling. However, these methods often focus on general-purpose force field learning or small molecular systems, making them computationally intensive for large proteins.

% \textbf{Multi-scale Dynamics Modeling.}
% The inherent multi-scale nature of protein dynamics has long been recognized in computational biology. Traditional MD analysis methods decompose protein motions into slow collective changes, which are crucial for biological function, and fast local fluctuations contribute to overall stability. Recent deep learning approaches have begun to address this multi-scale characteristic. EigenFold~\cite{jing2023eigenfold} models protein structures as systems of harmonic oscillators, naturally inducing a cascading-resolution generative process along system eigenmodes. FoldFlow~\cite{bosese} proposes a family of flow-based generative models on SE(3), where the continuous-time dynamics naturally captures multi-scale structural variations - from global conformational changes to local refinements - through different time scales of the flow evolution. However, most existing approaches focus on either ensemble generation or short-timescale dynamics, without explicitly bridging the gap between different temporal scales in protein motion.

\textbf{Multi-scale Dynamics Modeling.}
The inherent multi-scale nature of protein dynamics has long been recognized in computational biology. Traditional MD analysis methods decompose protein motions into slow collective changes, which are crucial for biological function, and fast local fluctuations contribute to overall stability. Recent deep learning approaches have begun to address this multi-scale characteristic. EigenFold~\cite{jing2023eigenfold} models protein structures as systems of harmonic oscillators, naturally inducing a cascading-resolution generative process along system eigenmodes. FoldFlow~\cite{bosese} proposes a family of flow-based generative models on SE(3), where the continuous-time dynamics naturally capture multi-scale structural variations - from global conformational changes to local refinements - through different time scales of the flow evolution. ITO~\cite{schreiner2023implicit} learns transition density operators that allow conditioning on arbitrary timesteps, focusing on coarse-grained C$\alpha$ representations with exponential distribution sampling. However, most existing approaches focus on either ensemble generation or short-timescale dynamics, without explicitly bridging the gap between different temporal scales in protein motion.






\textbf{Autoregressive Models in Structural Biology.}
While diffusion models have recently dominated protein structure generation, auto-regressive approaches are gaining traction for their ability to model temporally coherent and physically consistent dynamics. In the domain of bio-molecules, arDCA~\cite{trinquier2021efficient} applies a simple yet effective auto-regressive framework to model protein sequence distributions, capturing co-evolutionary couplings while enabling efficient sequence sampling and fitness prediction. For protein structure modeling, Structure Language Models~\cite{lu2024structure} employ latent-space auto-regression to efficiently generate diverse backbone conformations, while equivariant models such as EquiJump~\cite{costa2024equijump} build an $\mathrm{SO}(3)$-equivariant transport model that bridges long time intervals of all-atom protein MD by stochastically interpolating between snapshots. These advances demonstrate the promise of auto-regressive models in capturing complex bio-molecular dynamics across multiple timescales.","\textbf{Protein Ensemble Generation.}
Recent advances in deep learning have revolutionized the generation of protein conformational ensembles. Traditional approaches rely on MSA subsampling with AlphaFold2~\cite{potapenko2021highly}, which provides limited control over conformational diversity. Modern deep learning methods have introduced more sophisticated techniques. AlphaFlow~\cite{jing2024alphafold} fine-tunes single-state predictors under a flow matching framework to generate protein conformational ensembles. ESMFlow~\cite{jing2024alphafold} extends this approach by leveraging protein language models. BioEMU~\cite{lewis2024scalable} employs a diffusion-based framework to generate thermodynamically accurate ensembles. ConfDiff~\cite{wang2024protein} incorporates force-guided networks with diffusion models to enhance generation fidelity, while Str2Str~\cite{lustr2str} introduces a structure-to-structure translation framework with roto-translation equivariance. However, these methods primarily focus on generating conformational ensembles that match equilibrium distributions, without explicitly modeling the temporal evolution of protein structures.

\textbf{Learning Molecular Dynamics.}
Machine learning approaches have emerged as powerful tools for accelerating and enhancing molecular dynamics simulations. VAMPNet~\cite{mardt2022deep} pioneered the use of variational approaches for Markov processes in molecular kinetics. Recent works like DiffMD~\cite{wu2023diffmd} employ diffusion models to estimate conformational density gradients, while DFF~\cite{arts2023two} establishes connections between score-based generative models and molecular force fields. The Distributional Graphformer (DiG)~\cite{zheng2024predicting} predicts equilibrium distributions of molecular systems, enabling efficient conformational sampling. However, these methods often focus on general-purpose force field learning or small molecular systems, making them computationally intensive for large proteins.




\textbf{Multi-scale Dynamics Modeling.}
The inherent multi-scale nature of protein dynamics has long been recognized in computational biology. Traditional MD analysis methods decompose protein motions into slow collective changes, which are crucial for biological function, and fast local fluctuations contribute to overall stability. Recent deep learning approaches have begun to address this multi-scale characteristic. EigenFold~\cite{jing2023eigenfold} models protein structures as systems of harmonic oscillators, naturally inducing a cascading-resolution generative process along system eigenmodes. FoldFlow~\cite{bosese} proposes a family of flow-based generative models on SE(3), where the continuous-time dynamics naturally capture multi-scale structural variations - from global conformational changes to local refinements - through different time scales of the flow evolution. ITO~\cite{schreiner2023implicit} learns transition density operators that allow conditioning on arbitrary timesteps, focusing on coarse-grained C$\alpha$ representations with exponential distribution sampling. However, most existing approaches focus on either ensemble generation or short-timescale dynamics, without explicitly bridging the gap between different temporal scales in protein motion.






\textbf{Autoregressive Models in Structural Biology.}
While diffusion models have recently dominated protein structure generation, auto-regressive approaches are gaining traction for their ability to model temporally coherent and physically consistent dynamics. In the domain of bio-molecules, arDCA~\cite{trinquier2021efficient} applies a simple yet effective auto-regressive framework to model protein sequence distributions, capturing co-evolutionary couplings while enabling efficient sequence sampling and fitness prediction. For protein structure modeling, Structure Language Models~\cite{lu2024structure} employ latent-space auto-regression to efficiently generate diverse backbone conformations, while equivariant models such as EquiJump~\cite{costa2024equijump} build an $\mathrm{SO}(3)$-equivariant transport model that bridges long time intervals of all-atom protein MD by stochastically interpolating between snapshots. These advances demonstrate the promise of auto-regressive models in capturing complex bio-molecular dynamics across multiple timescales.",
2511.08579v1,http://arxiv.org/abs/2511.08579v1,2025-11-11 18:57:14+00:00,Training Language Models to Explain Their Own Computations,"Can language models (LMs) learn to faithfully describe their internal computations? Are they better able to describe themselves than other models? We study the extent to which LMs' privileged access to their own internals can be leveraged to produce new techniques for explaining their behavior. Using existing interpretability techniques as a source of ground truth, we fine-tune LMs to generate natural language descriptions of (1) the information encoded by LM features, (2) the causal structure of LMs' internal activations, and (3) the influence of specific input tokens on LM outputs. When trained with only tens of thousands of example explanations, explainer models exhibit non-trivial generalization to new queries. This generalization appears partly attributable to explainer models' privileged access to their own internals: using a model to explain its own computations generally works better than using a *different* model to explain its computations (even if the other model is significantly more capable). Our results suggest not only that LMs can learn to reliably explain their internal computations, but that such explanations offer a scalable complement to existing interpretability methods.","\label{sec:related_work}
\subsection{Chain-of-Thought Faithfulness}
Language models can be asked to verbalize their thought processes through chain-of-thought prompting, which offers a way for external monitoring~\citep{korbak2025chainthoughtmonitorabilitynew,baker2025monitoringreasoningmodelsmisbehavior}, but
prior work has found that these verbalizations can be unfaithful to their true decision-making processes~\citep{turpin2023language,lanham2023measuringfaithfulnesschainofthoughtreasoning,barez2025cot,chen2025reasoningmodelsdontsay}. This line of work directly inspires our hint ablation task setup, while our work attempts to remedy the unfaithfulness through a training objective that enforces consistency between model verbalization and their actual mechanisms.

\subsection{Mechanistic Interpretability Methods}
The field of mechanistic interpretability sets out to (1) describe individual features and (2) connect them to construct a causal circuit that performs a certain task. While many researchers have found success doing so manually in a fine-grained way both on the neuron level~\citep{gurnee2024universal} and the circuit level for toy tasks~\citep{wang2023interpretability,nanda2023progressmeasuresgrokkingmechanistic}, 
these methods have been challenging to scale and generalize to new models and tasks~\citep{sharkey2025open}.
To address scalability, automated feature description pipelines~\citep{hernandez2022natural,bills2023language,choi2024automatic,paulo2025automatically} and circuit discovery techniques~\citep{conmy2023automated,syed-etal-2024-attribution,hanna2024have,hsu2025efficient} have been introduced that require substantial computational resources. Designing a scalable interpretability method that balances correctness/robustness and efficiency is an active area of research, 
one of which is attribution patching that use gradients to approximate important components to the output~\citep{nanda2023attribution,syed-etal-2024-attribution}.

Closely related to the current study, one family of scalable interpretability methods attempts to leverage models' own verbalization ability without any additional training. LogitLens~\citep{nostalgebraist2020interpreting}, PatchScopes~\citep{ghandeharioun2024patchscopes}, SelfIE~\citep{chen2024selfie} enable models to self-interpret zero-shot, but such methods require significant hyperparameter tuning~\citep{kharlapenko2024self}.
~\citet{pan2024latentqa} introduces 
Latent Interpretation Tuning, which fine-tunes models to answer questions about other models' inputs via their hidden representations. 
We expand on this line of work by training models to self-verbalize their internals based on data collected from existing interpretability techniques that provide finer-grained pictures of models' internal computations.

Finally, explanations based on special interpretability techniques are difficult for non-expert users to access, potentially requiring developers integrate many specialized interfaces~\citep{viégas2023modelusermodelexploring,chen2024designingdashboardtransparencycontrol}. 
Our work offers an alternative path toward user understanding, in which explanations can be provided ``in-band'' rather than via an external interface.


\subsection{Introspection \& Metacognition}
Recent work investigates whether models possess \textit{metacognition} or \textit{introspective abilities}~\citep{binder2025looking,treutlein2024connecting,laine2024memyselfaisituational,comsa2025doesmakesensespeak}. A central debate in this literature concerns whether models have privileged access to their internals, or whether introspection merely reflects their strong predictive capacity to learn external correlations~\citep{song2025language,song2025privilegedselfaccessmattersintrospection,li2025do}.  
We add to the literature by assessing models' introspective abilities on their own \textit{mechanisms} derived from mechanistic interpretability methods.","\subsection{Chain-of-Thought Faithfulness}
Language models can be asked to verbalize their thought processes through chain-of-thought prompting, which offers a way for external monitoring~\citep{korbak2025chainthoughtmonitorabilitynew,baker2025monitoringreasoningmodelsmisbehavior}, but
prior work has found that these verbalizations can be unfaithful to their true decision-making processes~\citep{turpin2023language,lanham2023measuringfaithfulnesschainofthoughtreasoning,barez2025cot,chen2025reasoningmodelsdontsay}. This line of work directly inspires our hint ablation task setup, while our work attempts to remedy the unfaithfulness through a training objective that enforces consistency between model verbalization and their actual mechanisms.

\subsection{Mechanistic Interpretability Methods}
The field of mechanistic interpretability sets out to (1) describe individual features and (2) connect them to construct a causal circuit that performs a certain task. While many researchers have found success doing so manually in a fine-grained way both on the neuron level~\citep{gurnee2024universal} and the circuit level for toy tasks~\citep{wang2023interpretability,nanda2023progressmeasuresgrokkingmechanistic}, 
these methods have been challenging to scale and generalize to new models and tasks~\citep{sharkey2025open}.
To address scalability, automated feature description pipelines~\citep{hernandez2022natural,bills2023language,choi2024automatic,paulo2025automatically} and circuit discovery techniques~\citep{conmy2023automated,syed-etal-2024-attribution,hanna2024have,hsu2025efficient} have been introduced that require substantial computational resources. Designing a scalable interpretability method that balances correctness/robustness and efficiency is an active area of research, 
one of which is attribution patching that use gradients to approximate important components to the output~\citep{nanda2023attribution,syed-etal-2024-attribution}.

Closely related to the current study, one family of scalable interpretability methods attempts to leverage models' own verbalization ability without any additional training. LogitLens~\citep{nostalgebraist2020interpreting}, PatchScopes~\citep{ghandeharioun2024patchscopes}, SelfIE~\citep{chen2024selfie} enable models to self-interpret zero-shot, but such methods require significant hyperparameter tuning~\citep{kharlapenko2024self}.
~\citet{pan2024latentqa} introduces 
Latent Interpretation Tuning, which fine-tunes models to answer questions about other models' inputs via their hidden representations. 
We expand on this line of work by training models to self-verbalize their internals based on data collected from existing interpretability techniques that provide finer-grained pictures of models' internal computations.

Finally, explanations based on special interpretability techniques are difficult for non-expert users to access, potentially requiring developers integrate many specialized interfaces~\citep{viégas2023modelusermodelexploring,chen2024designingdashboardtransparencycontrol}. 
Our work offers an alternative path toward user understanding, in which explanations can be provided ``in-band'' rather than via an external interface.


\subsection{Introspection \& Metacognition}
Recent work investigates whether models possess \textit{metacognition} or \textit{introspective abilities}~\citep{binder2025looking,treutlein2024connecting,laine2024memyselfaisituational,comsa2025doesmakesensespeak}. A central debate in this literature concerns whether models have privileged access to their internals, or whether introspection merely reflects their strong predictive capacity to learn external correlations~\citep{song2025language,song2025privilegedselfaccessmattersintrospection,li2025do}.  
We add to the literature by assessing models' introspective abilities on their own \textit{mechanisms} derived from mechanistic interpretability methods.",
2511.09769v1,http://arxiv.org/abs/2511.09769v1,2025-11-12 22:04:05+00:00,Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,"Accurate and generalizable Reynolds-averaged Navier-Stokes (RANS) models for turbulent flows rely on effective closures. We introduce tensor-based, symmetry aware closures using equivariant neural networks (ENNs) and present an algorithm for enforcing algebraic contraction relations among tensor components. The modeling approach builds on the structure tensor framework introduced by Kassinos and Reynolds to learn closures in the rapid distortion theory setting. Experiments show that ENNs can effectively learn relationships involving high-order tensors, meeting or exceeding the performance of existing models in tasks such as predicting the rapid pressure-strain correlation. Our results show that ENNs provide a physically consistent alternative to classical tensor basis models, enabling end-to-end learning of unclosed terms in RANS and fast exploration of model dependencies.","\label{sec:related}
\subsection{Invariant theory for tensor function representation}\label{sec:tbm-bg}
The dominant approach in fluid mechanics to build equivariant tensor functions was initiated in \cite{robertson_invariant_1940} and applies the theory of invariants to represent tensor functions as a linear combination of basis tensors \cite{lumley_computational_1979, sarkar_simple_1989, chung_nonlinear_1995, ling_reynolds_2016, kaandorp_data-driven_2020, cai_revisiting_2024, spencer_theory_1958}. The expansion coefficients are, in general, functions of tensor invariants. Although the form of these coefficients must be determined by other means, this approach satisfies the appropriate symmetry properties while reducing the modeling problem to the determination of scalar functions of scalar arguments. This approach is here referred to as the \textit{tensor basis method} (TBM).

Analytical methods for determining the coefficient functions include treating them as constants, forming expressions from Taylor expansions \cite{chung_nonlinear_1995}, or deriving other non-polynomial forms \cite{lumley_computational_1979}. The drawback to such approaches is that they rely on a priori assumptions or require truncation. Alternatively, the tensor basis neural network (TBNN), introduced by \cite{ling_reynolds_2016} and applied in \cite{zhang_ensemble_2022,lennon_scientific_2023,parmar_generalized_2020}, among others, uses neural networks to parameterize the coefficient functions. The universal approximation property of neural networks implies that this construction remains theoretically general, and since the coefficient functions are scalar functions of scalars, any network architecture can be used without violating symmetry constraints.

The TBM has been used successfully for several turbulence modeling problems, for example predicting the Reynolds stress anisotropy and improving mean velocity predictions in square duct, wavy wall, curved backward-facing step, and right-angled backward facing step geometries \cite{ling_reynolds_2016,kaandorp_data-driven_2020}, as well as for modeling tasks in other areas of fluid mechanics, for example learning rheological constitutive equations \cite{lennon_scientific_2023, sunol_learning_2025}. However, it is limited by the need to derive the tensor basis, which determines both the number of coefficient functions and their arguments. Deriving a minimal basis and the associated invariants is a challenging task. The size of the minimal integrity bases grows exponentially with the number of tensor arguments, and the results are only available for a limited number of special cases \cite{smith_isotropic_1971,pennisi_third_1992,spencer_theory_1958, prakash_invariant_2022}. Moreover, a newly derived basis is necessary when the model dependencies change. This motivates the search for alternative modeling procedures that satisfy the relevant symmetry constraints and maintain generality but do not require a priori derivation of basis tensors and their invariants.


\subsection{Equivariant neural networks}

The architectures of equivariant neural networks are distinguished by the way in which they handle geometric attributes \cite{duval_hitchhikers_2024}. Some approaches use only scalar features, such as norms or triplet angles, as input to the network instead of higher-order tensors \cite{schutt_schnet_2017, gasteiger_directional_2022}. While this guaranties invariance to Euclidean transformations, it comes at the cost of expressivity. Moreover,  pre-computation of the derived scalars becomes increasingly complex with an increasing  number and tensor order of the inputs. Alternatively, equivariant architectures aim to retain full geometric information by operating on tensors directly. The developments that most inform the present work include those in \cite{thomas_tensor_2018, weiler_3d_2018, brandstetter_geometric_2021, kondor_clebsch-gordan_2018}, which make use of the notion of irreducible representations. Working directly with the fundamental building blocks of group representations provides a framework for generalizing equivariant operations to higher-order tensors, a task that is particularly relevant to RANS applications.
ENNs have been used in particular to obtain state of the art property predictions in molecular dynamics simulations \cite{jumper_highly_2021, batzner_e3-equivariant_2022, zitnick_introduction_2020, dauparas_robust_2022}. These results have demonstrated the efficacy of operating directly on geometric attributes. The TBM, discussed above, applies machine learning only to the \textit{invariant} parts of the problem, relying on analytical treatment of the \textit{equivariant} aspects offline.

An equivariant closure modeling strategy that avoids offline basis derivation is the vector cloud neural network (VCNN-e) \cite{zhou_frame-independent_2022, han_equivariant_2023}, which maps a cloud of local flow or geometry features to the Reynolds stresses via an invariant embedding and an equivariant output map. However, it only considers vectorial features and does not provide a framework for enforcing general index-permutation symmetries or prescribed linear constraints.
Another approach targeting equivariant tensorial constitutive relations is RotEqNet \cite{gao_roteqnet_2022}, which uses position standardization to map rotated inputs to a canonical orientation. For higher order tensors, contractions are used to compute a rotation into the standard position. A model is trained on the standardized inputs and the predictions are rotated back to the original frame for inference. This method is restricted to fully symmetric tensors and guaranties exact equivariance only when the model in the standardized frame is learned perfectly.
The work of \cite{kaszuba_implicit_2025} explores the use of irreducible representation-based equivariant networks in the RANS context but differs from the present study in several key respects. First, the model in \cite{kaszuba_implicit_2025} is trained to learn single-injection corrections based on the converged output of a particular RANS model. This limits the applicability of the model to input associated with the RANS solver used to generate training data. In contrast, this work learns constituitive relations between terms in the governing equations, and hence completes a RANS model rather than corrects it. Furthermore, the present work targets Reynolds stress models and does not restrict the focus to steady problems.

Equivariant networks have also been applied to problems in fluid dynamics beyond turbulence modeling. One line of research extends the mesh-based graph neural network (GNN) surrogate framework introduced in \cite{pfaff_learning_2021}. For example, \cite{toshev_learning_2023} applies the Steerable Equivariant Graph Neural Network architecture to Lagrangian fluid mechanics, providing a direct comparison to non-equivariant surrogates. Related GNN approaches employ an encode-process-decode strategy, but modify the treatment of vector features to enforce equivariance, either by transforming them into local strain eigenbases \cite{list_rotational_2025} or by projecting them onto graph edges \cite{lino_multi-scale_2022}.

\subsection{Integrating Physics Constraints in  Machine Learning models}\label{sec:prior-constraints}
Integrating physical priors into machine learning models aims to improve their  data efficiency and predictive accuracy. To our knowledge, the first work in this field integrated wall shear stresses and pressure gradients to improve the neural network forecast of near wall flow in turbulent channel flows \cite{milano_neural_2002}. Physics informed machine learning models such as PINNS \cite{raissi_physics-informed_2019,karniadakis_physics-informed_2021,wang_incorporating_2021} embed physics constraints  by regularizing the loss function with a term that penalizes deviations from known PDE governing equations. In order to achieve equivariance, a straightforward approach is data augmentation, in which symmetry operations are applied to the training data \cite{chen_group-theoretic_nodate}. However, at least for continuous symmetry groups, it is impossible to generate data for every possible symmetry transformation, and there is no guaranty that the model will satisfy equivariance.

Alternatively, \textit{hard constraints} reduce the search space for optimization and ensure physical consistency by construction. Examples of this approach include \cite{liu_harnessing_2024,richter-powell_neural_2022,chalapathi_scaling_2024}. The tools described here also fall into the hard constraint category. ENNs are equivariant by construction and can be designed to respect index permutation symmetries, such as the symmetric tensor. In this paper, a method is developed for enforcing another class of tensor relationships, namely ``einsum""-style linear relations, as hard constraints in ENNs. Einsum is a generalized Einstein summation notation to represent arbitrary linear maps and relations,that can extend  beyond standard tensor contractions and has found extensive applications in machine learning. An einsum-like domain-specific language is the basis for the Tensor Comprehensions \cite{vasilache_tensor_2018} library in PyTorch, which automatically generates GPU code and auto-tunes it for specific input sizes.","\subsection{Invariant theory for tensor function representation}The dominant approach in fluid mechanics to build equivariant tensor functions was initiated in \cite{robertson_invariant_1940} and applies the theory of invariants to represent tensor functions as a linear combination of basis tensors \cite{lumley_computational_1979, sarkar_simple_1989, chung_nonlinear_1995, ling_reynolds_2016, kaandorp_data-driven_2020, cai_revisiting_2024, spencer_theory_1958}. The expansion coefficients are, in general, functions of tensor invariants. Although the form of these coefficients must be determined by other means, this approach satisfies the appropriate symmetry properties while reducing the modeling problem to the determination of scalar functions of scalar arguments. This approach is here referred to as the \textit{tensor basis method} (TBM).

Analytical methods for determining the coefficient functions include treating them as constants, forming expressions from Taylor expansions \cite{chung_nonlinear_1995}, or deriving other non-polynomial forms \cite{lumley_computational_1979}. The drawback to such approaches is that they rely on a priori assumptions or require truncation. Alternatively, the tensor basis neural network (TBNN), introduced by \cite{ling_reynolds_2016} and applied in \cite{zhang_ensemble_2022,lennon_scientific_2023,parmar_generalized_2020}, among others, uses neural networks to parameterize the coefficient functions. The universal approximation property of neural networks implies that this construction remains theoretically general, and since the coefficient functions are scalar functions of scalars, any network architecture can be used without violating symmetry constraints.

The TBM has been used successfully for several turbulence modeling problems, for example predicting the Reynolds stress anisotropy and improving mean velocity predictions in square duct, wavy wall, curved backward-facing step, and right-angled backward facing step geometries \cite{ling_reynolds_2016,kaandorp_data-driven_2020}, as well as for modeling tasks in other areas of fluid mechanics, for example learning rheological constitutive equations \cite{lennon_scientific_2023, sunol_learning_2025}. However, it is limited by the need to derive the tensor basis, which determines both the number of coefficient functions and their arguments. Deriving a minimal basis and the associated invariants is a challenging task. The size of the minimal integrity bases grows exponentially with the number of tensor arguments, and the results are only available for a limited number of special cases \cite{smith_isotropic_1971,pennisi_third_1992,spencer_theory_1958, prakash_invariant_2022}. Moreover, a newly derived basis is necessary when the model dependencies change. This motivates the search for alternative modeling procedures that satisfy the relevant symmetry constraints and maintain generality but do not require a priori derivation of basis tensors and their invariants.


\subsection{Equivariant neural networks}

The architectures of equivariant neural networks are distinguished by the way in which they handle geometric attributes \cite{duval_hitchhikers_2024}. Some approaches use only scalar features, such as norms or triplet angles, as input to the network instead of higher-order tensors \cite{schutt_schnet_2017, gasteiger_directional_2022}. While this guaranties invariance to Euclidean transformations, it comes at the cost of expressivity. Moreover,  pre-computation of the derived scalars becomes increasingly complex with an increasing  number and tensor order of the inputs. Alternatively, equivariant architectures aim to retain full geometric information by operating on tensors directly. The developments that most inform the present work include those in \cite{thomas_tensor_2018, weiler_3d_2018, brandstetter_geometric_2021, kondor_clebsch-gordan_2018}, which make use of the notion of irreducible representations. Working directly with the fundamental building blocks of group representations provides a framework for generalizing equivariant operations to higher-order tensors, a task that is particularly relevant to RANS applications.
ENNs have been used in particular to obtain state of the art property predictions in molecular dynamics simulations \cite{jumper_highly_2021, batzner_e3-equivariant_2022, zitnick_introduction_2020, dauparas_robust_2022}. These results have demonstrated the efficacy of operating directly on geometric attributes. The TBM, discussed above, applies machine learning only to the \textit{invariant} parts of the problem, relying on analytical treatment of the \textit{equivariant} aspects offline.

An equivariant closure modeling strategy that avoids offline basis derivation is the vector cloud neural network (VCNN-e) \cite{zhou_frame-independent_2022, han_equivariant_2023}, which maps a cloud of local flow or geometry features to the Reynolds stresses via an invariant embedding and an equivariant output map. However, it only considers vectorial features and does not provide a framework for enforcing general index-permutation symmetries or prescribed linear constraints.
Another approach targeting equivariant tensorial constitutive relations is RotEqNet \cite{gao_roteqnet_2022}, which uses position standardization to map rotated inputs to a canonical orientation. For higher order tensors, contractions are used to compute a rotation into the standard position. A model is trained on the standardized inputs and the predictions are rotated back to the original frame for inference. This method is restricted to fully symmetric tensors and guaranties exact equivariance only when the model in the standardized frame is learned perfectly.
The work of \cite{kaszuba_implicit_2025} explores the use of irreducible representation-based equivariant networks in the RANS context but differs from the present study in several key respects. First, the model in \cite{kaszuba_implicit_2025} is trained to learn single-injection corrections based on the converged output of a particular RANS model. This limits the applicability of the model to input associated with the RANS solver used to generate training data. In contrast, this work learns constituitive relations between terms in the governing equations, and hence completes a RANS model rather than corrects it. Furthermore, the present work targets Reynolds stress models and does not restrict the focus to steady problems.

Equivariant networks have also been applied to problems in fluid dynamics beyond turbulence modeling. One line of research extends the mesh-based graph neural network (GNN) surrogate framework introduced in \cite{pfaff_learning_2021}. For example, \cite{toshev_learning_2023} applies the Steerable Equivariant Graph Neural Network architecture to Lagrangian fluid mechanics, providing a direct comparison to non-equivariant surrogates. Related GNN approaches employ an encode-process-decode strategy, but modify the treatment of vector features to enforce equivariance, either by transforming them into local strain eigenbases \cite{list_rotational_2025} or by projecting them onto graph edges \cite{lino_multi-scale_2022}.

\subsection{Integrating Physics Constraints in  Machine Learning models}Integrating physical priors into machine learning models aims to improve their  data efficiency and predictive accuracy. To our knowledge, the first work in this field integrated wall shear stresses and pressure gradients to improve the neural network forecast of near wall flow in turbulent channel flows \cite{milano_neural_2002}. Physics informed machine learning models such as PINNS \cite{raissi_physics-informed_2019,karniadakis_physics-informed_2021,wang_incorporating_2021} embed physics constraints  by regularizing the loss function with a term that penalizes deviations from known PDE governing equations. In order to achieve equivariance, a straightforward approach is data augmentation, in which symmetry operations are applied to the training data \cite{chen_group-theoretic_nodate}. However, at least for continuous symmetry groups, it is impossible to generate data for every possible symmetry transformation, and there is no guaranty that the model will satisfy equivariance.

Alternatively, \textit{hard constraints} reduce the search space for optimization and ensure physical consistency by construction. Examples of this approach include \cite{liu_harnessing_2024,richter-powell_neural_2022,chalapathi_scaling_2024}. The tools described here also fall into the hard constraint category. ENNs are equivariant by construction and can be designed to respect index permutation symmetries, such as the symmetric tensor. In this paper, a method is developed for enforcing another class of tensor relationships, namely ``einsum""-style linear relations, as hard constraints in ENNs. Einsum is a generalized Einstein summation notation to represent arbitrary linear maps and relations,that can extend  beyond standard tensor contractions and has found extensive applications in machine learning. An einsum-like domain-specific language is the basis for the Tensor Comprehensions \cite{vasilache_tensor_2018} library in PyTorch, which automatically generates GPU code and auto-tunes it for specific input sizes.","2.1 Invariant theory for tensor function representation
The dominant approach in fluid mechanics to build equivariant tensor functions was initiated in
[8] and applies the theory of invariants to represent tensor functions as a linear combination of
basis tensors [3, 9–14]. The expansion coefficients are, in general, functions of tensor invariants.
2
Although the form of these coefficients must be determined by other means, this approach satisfies
the appropriate symmetry properties while reducing the modeling problem to the determination of
scalar functions of scalar arguments. This approach is here referred to as thetensor basis method
(TBM).
Analytical methods for determining the coefficient functions include treating them as constants,
forming expressions from Taylor expansions [10], or deriving other non-polynomial forms [3]. The
drawback to such approaches is that they rely on a priori assumptions or require truncation. Alter-
natively, the tensor basis neural network (TBNN), introduced by [11] and applied in [15–17], among
others, uses neural networks to parameterize the coefficient functions. The universal approxima-
tion property of neural networks implies that this construction remains theoretically general, and
since the coefficient functions are scalar functions of scalars, any network architecture can be used
without violating symmetry constraints.
The TBM has been used successfully for several turbulence modeling problems, for example
predicting the Reynolds stress anisotropy and improving mean velocity predictions in square duct,
wavy wall, curved backward-facing step, and right-angled backward facing step geometries [11, 12],
as well as for modeling tasks in other areas of fluid mechanics, for example learning rheological
constitutive equations [16, 18]. However, it is limited by the need to derive the tensor basis, which
determines both the number of coefficient functions and their arguments. Deriving a minimal basis
and the associated invariants is a challenging task. The size of the minimal integrity bases grows
exponentially with the number of tensor arguments, and the results are only available for a limited
number of special cases [14, 19–21]. Moreover, a newly derived basis is necessary when the model
dependencies change. This motivates the search for alternative modeling procedures that satisfy
the relevant symmetry constraints and maintain generality but do not require a priori derivation
of basis tensors and their invariants.
2.2 Equivariant neural networks
The architectures of equivariant neural networks are distinguished by the way in which they handle
geometric attributes [22]. Some approaches use only scalar features, such as norms or triplet angles,
as input to the network instead of higher-order tensors [23, 24]. While this guaranties invariance
to Euclidean transformations, it comes at the cost of expressivity. Moreover, pre-computation of
the derived scalars becomes increasingly complex with an increasing number and tensor order of
the inputs. Alternatively, equivariant architectures aim to retain full geometric information by
operating on tensors directly. The developments that most inform the present work include those
in [25–28], which make use of the notion of irreducible representations. Working directly with the
fundamental building blocks of group representations provides a framework for generalizing equiv-
ariant operations to higher-order tensors, a task that is particularly relevant to RANS applications.
ENNs have been used in particular to obtain state of the art property predictions in molecular
dynamics simulations [29–32]. These results have demonstrated the efficacy of operating directly
on geometric attributes. The TBM, discussed above, applies machine learning only to theinvariant
parts of the problem, relying on analytical treatment of theequivariantaspects offline.
An equivariant closure modeling strategy that avoids offline basis derivation is the vector cloud
neural network (VCNN-e) [33, 34], which maps a cloud of local flow or geometry features to the
Reynolds stresses via an invariant embedding and an equivariant output map. However, it only con-
siders vectorial features and does not provide a framework for enforcing general index-permutation
symmetries or prescribed linear constraints. Another approach targeting equivariant tensorial con-
stitutive relations is RotEqNet [35], which uses position standardization to map rotated inputs to a
canonical orientation. For higher order tensors, contractions are used to compute a rotation into the
3
standard position. A model is trained on the standardized inputs and the predictions are rotated
back to the original frame for inference. This method is restricted to fully symmetric tensors and
guaranties exact equivariance only when the model in the standardized frame is learned perfectly.
The work of [36] explores the use of irreducible representation-based equivariant networks in the
RANS context but differs from the present study in several key respects. First, the model in [36]
is trained to learn single-injection corrections based on the converged output of a particular RANS
model. This limits the applicability of the model to input associated with the RANS solver used
to generate training data. In contrast, this work learns constituitive relations between terms in the
governing equations, and hence completes a RANS model rather than corrects it. Furthermore, the
present work targets Reynolds stress models and does not restrict the focus to steady problems.
Equivariant networks have also been applied to problems in fluid dynamics beyond turbulence
modeling. One line of research extends the mesh-based graph neural network (GNN) surrogate
framework introduced in [37]. For example, [38] applies the Steerable Equivariant Graph Neu-
ral Network architecture to Lagrangian fluid mechanics, providing a direct comparison to non-
equivariant surrogates. Related GNN approaches employ an encode-process-decode strategy, but
modify the treatment of vector features to enforce equivariance, either by transforming them into
local strain eigenbases [39] or by projecting them onto graph edges [40].
2.3 Integrating Physics Constraints in Machine Learning models
Integrating physical priors into machine learning models aims to improve their data efficiency and
predictive accuracy. To our knowledge, the first work in this field integrated wall shear stresses and
pressure gradients to improve the neural network forecast of near wall flow in turbulent channel flows
[41]. Physics informed machine learning models such as PINNS [42–44] embed physics constraints
by regularizing the loss function with a term that penalizes deviations from known PDE governing
equations. In order to achieve equivariance, a straightforward approach is data augmentation, in
which symmetry operations are applied to the training data [45]. However, at least for continuous
symmetry groups, it is impossible to generate data for every possible symmetry transformation,
and there is no guaranty that the model will satisfy equivariance.
Alternatively,hard constraintsreduce the search space for optimization and ensure physical
consistency by construction. Examples of this approach include [46–48]. The tools described here
also fall into the hard constraint category. ENNs are equivariant by construction and can be
designed to respect index permutation symmetries, such as the symmetric tensor. In this paper, a
method is developed for enforcing another class of tensor relationships, namely “einsum”-style linear
relations, as hard constraints in ENNs. Einsum is a generalized Einstein summation notation to
represent arbitrary linear maps and relations,that can extend beyond standard tensor contractions
and has found extensive applications in machine learning. An einsum-like domain-specific language
is the basis for the Tensor Comprehensions [49] library in PyTorch, which automatically generates
GPU code and auto-tunes it for specific input sizes."
2511.08247v1,http://arxiv.org/abs/2511.08247v1,2025-11-11 13:43:09+00:00,ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech,"Parliamentary speech generation presents specific challenges for large language models beyond standard text generation tasks. Unlike general text generation, parliamentary speeches require not only linguistic quality but also political authenticity and ideological consistency. Current language models lack specialized training for parliamentary contexts, and existing evaluation methods focus on standard NLP metrics rather than political authenticity. To address this, we present ParliaBench, a benchmark for parliamentary speech generation. We constructed a dataset of speeches from UK Parliament to enable systematic model training. We introduce an evaluation framework combining computational metrics with LLM-as-a-judge assessments for measuring generation quality across three dimensions: linguistic quality, semantic coherence, and political authenticity. We propose two novel embedding-based metrics, Political Spectrum Alignment and Party Alignment, to quantify ideological positioning. We fine-tuned five large language models (LLMs), generated 28k speeches, and evaluated them using our framework, comparing baseline and fine-tuned models. Results show that fine-tuning produces statistically significant improvements across the majority of metrics and our novel metrics demonstrate strong discriminative power for political dimensions.","\label{sec:related}


Recent advances in large language models (LLMs) have enabled a wide range of applications across political domains. \cite{li2024political} outlines several applications of LLM in political contexts, covering predictive, generative, and simulation-based approaches. The use of LLMs as substitutes for human experts in annotating political texts across multiple languages is explored in \cite{heseltine2024large}, while \cite{gunes2023multiclass} employ LLMs for classifying U.S. Congressional bills. \cite{argyle2023out} investigate LLMs as proxies for specific human subpopulations in social science research and \cite{bisbee2024synthetic} raise concerns about the quality, reliability, and reproducibility of synthetic survey data generated by LLMs.
Agent-based LLMs are utilized as coalition negotiators \cite{moghimifar2024modelling} and as U.S. senators
simulating legislative processes \cite{baker2024simulating}. However, these approaches predominantly emphasize analytical and simulation capabilities rather than authentic speech generation quality.

Traditional text generation evaluation has evolved from reference-based metrics like BLEU \cite{papineni2002bleu} and ROUGE \cite{lin2004rouge} toward embedding based approaches that better capture semantic similarity. BERTScore \cite{zhang2020bertscore} uses contextualized embeddings to compute token-level similarity, while MoverScore \cite{zhao2019moverscore} measures semantic transportation cost using Earth Mover's Distance. For reference-free evaluation, Zhu and Bhat \cite{zhu2020gruen} propose GRUEN, assessing grammaticality and semantic coherence. Domain-specific datasets like BillSum \cite{kornilova2019billsum} for legislative summarization and OpinionQA \cite{santurkar2023whose} for opinion alignment provide targeted evaluation resources, though gaps remain in generative parliamentary speech assessment.

The emergence of LLM-as-a-Judge evaluation \cite{zheng2023judging} offers scalable alternatives for nuanced assessment, achieving over 80\% agreement with human evaluators in complex judgment tasks. \cite{liu2023voices} further validate this approach, demonstrating GPT-4's high alignment with human thematic coding in political analysis. This methodology has been successfully applied across diverse contexts, from general LLM benchmarking through competitive debates \cite{moniri2025evaluating} to long-context reasoning evaluation in parliamentary debates \cite{tiwari2025debatebench}. These approaches demonstrate the viability of automated evaluation for argumentative and political content, though models exhibit documented biases toward Western, educated populations \cite{durmus2024towards} and systematic preferences in political simulations \cite{qi2024representation}. Recent advances in retrieval-augmented generation and chain-of-thought reasoning provide enhanced capabilities for contextually-grounded political text generation, though their application to parliamentary speech evaluation remains underexplored.

Regarding parliamentary speech data, structured corpora like ParlaMint \cite{erjavec2025parlamint} provide multi-lingual parliamentary proceedings. Embedding-based approaches for political analysis, introduced in \cite{rheault2020embeddings}, demonstrate that embeddings can capture ideological positioning in parliamentary text.


Domain-specific evaluation frameworks have emerged across professional fields, including FinBen \cite{xie2024finben} and LexEval \cite{li2024lexeval}. Political science applications have developed specialized benchmarks for election prediction and legislative analysis, yet these focus primarily on classification and analysis tasks. Parliamentary speech generation has attracted recent computational interest, with work exploring European Parliament consensus building \cite{zhang2025eurocon} and political impersonation authenticity \cite{herbold2024}, but evaluation frameworks remain underdeveloped. Existing approaches focus on narrow aspects like style mimicry rather than systematic quality assessment across linguistic and political authenticity dimensions that parliamentary speech generation requires.

Our work addresses these gaps by establishing a benchmark resource specifically designed for parliamentary speech generation.","Recent advances in large language models (LLMs) have enabled a wide range of applications across political domains. \cite{li2024political} outlines several applications of LLM in political contexts, covering predictive, generative, and simulation-based approaches. The use of LLMs as substitutes for human experts in annotating political texts across multiple languages is explored in \cite{heseltine2024large}, while \cite{gunes2023multiclass} employ LLMs for classifying U.S. Congressional bills. \cite{argyle2023out} investigate LLMs as proxies for specific human subpopulations in social science research and \cite{bisbee2024synthetic} raise concerns about the quality, reliability, and reproducibility of synthetic survey data generated by LLMs.
Agent-based LLMs are utilized as coalition negotiators \cite{moghimifar2024modelling} and as U.S. senators
simulating legislative processes \cite{baker2024simulating}. However, these approaches predominantly emphasize analytical and simulation capabilities rather than authentic speech generation quality.

Traditional text generation evaluation has evolved from reference-based metrics like BLEU \cite{papineni2002bleu} and ROUGE \cite{lin2004rouge} toward embedding based approaches that better capture semantic similarity. BERTScore \cite{zhang2020bertscore} uses contextualized embeddings to compute token-level similarity, while MoverScore \cite{zhao2019moverscore} measures semantic transportation cost using Earth Mover's Distance. For reference-free evaluation, Zhu and Bhat \cite{zhu2020gruen} propose GRUEN, assessing grammaticality and semantic coherence. Domain-specific datasets like BillSum \cite{kornilova2019billsum} for legislative summarization and OpinionQA \cite{santurkar2023whose} for opinion alignment provide targeted evaluation resources, though gaps remain in generative parliamentary speech assessment.

The emergence of LLM-as-a-Judge evaluation \cite{zheng2023judging} offers scalable alternatives for nuanced assessment, achieving over 80\

Regarding parliamentary speech data, structured corpora like ParlaMint \cite{erjavec2025parlamint} provide multi-lingual parliamentary proceedings. Embedding-based approaches for political analysis, introduced in \cite{rheault2020embeddings}, demonstrate that embeddings can capture ideological positioning in parliamentary text.


Domain-specific evaluation frameworks have emerged across professional fields, including FinBen \cite{xie2024finben} and LexEval \cite{li2024lexeval}. Political science applications have developed specialized benchmarks for election prediction and legislative analysis, yet these focus primarily on classification and analysis tasks. Parliamentary speech generation has attracted recent computational interest, with work exploring European Parliament consensus building \cite{zhang2025eurocon} and political impersonation authenticity \cite{herbold2024}, but evaluation frameworks remain underdeveloped. Existing approaches focus on narrow aspects like style mimicry rather than systematic quality assessment across linguistic and political authenticity dimensions that parliamentary speech generation requires.

Our work addresses these gaps by establishing a benchmark resource specifically designed for parliamentary speech generation.","Recent advances in large language models (LLMs)
have enabled a wide range of applications across
political domains. (Li et al., 2024b) outlines sev-
eral applications of LLM in political contexts, cov-
ering predictive, generative, and simulation-based
approaches. The use of LLMs as substitutes for
human experts in annotating political texts across
multiple languages is explored in (Heseltine and
von Hohenberg, 2024), while (Gunes and Florczak,
2023) employ LLMs for classifying U.S. Congres-
sional bills. (Argyle et al., 2023) investigate LLMs
as proxies for specific human subpopulations in
social science research and (Bisbee et al., 2024)
raise concerns about the quality, reliability, and re-
producibility of synthetic survey data generated by
LLMs. Agent-based LLMs are utilized as coalition
negotiators (Moghimifar et al., 2024) and as U.S.
senators simulating legislative processes (Baker
andAzher,2024). However, theseapproachespre-
dominantly emphasize analytical and simulation
capabilities rather than authentic speech genera-
tion quality.
Traditional text generation evaluation has
evolved from reference-based metrics like BLEU
(Papineni et al., 2002) and ROUGE (Lin, 2004)
toward embedding based approaches that better
capture semantic similarity. BERTScore (Zhang
et al., 2020) uses contextualized embeddings to
compute token-level similarity, while MoverScore
(Zhao et al., 2019) measures semantic trans-
portation cost using Earth Mover’s Distance. For
reference-free evaluation, Zhu and Bhat (Zhu and
Bhat, 2020) propose GRUEN, assessing grammat-
icality and semantic coherence. Domain-specific
datasets like BillSum (Kornilova and Eidelman,
2019)forlegislativesummarizationandOpinionQA
(Santurkar et al., 2023) for opinion alignment
provide targeted evaluation resources, though
gaps remain in generative parliamentary speech
assessment.
The emergence of LLM-as-a-Judge evaluation
(Zheng et al., 2023) offers scalable alternatives for
nuanced assessment, achieving over 80% agree-
ment with human evaluators in complex judgmenttasks. (Liu and Sun, 2023) further validate this ap-
proach,demonstratingGPT-4’shighalignmentwith
human thematic coding in political analysis. This
methodology has been successfully applied across
diverse contexts, from general LLM benchmarking
through competitive debates (Moniri et al., 2025) to
long-context reasoning evaluation in parliamentary
debates (Tiwari et al., 2025). These approaches
demonstrate the viability of automated evaluation
forargumentativeandpoliticalcontent,thoughmod-
els exhibit documented biases toward Western, ed-
ucated populations (Durmus et al., 2023) and sys-
tematicpreferencesinpoliticalsimulations(Qietal.,
2024). Recent advances in retrieval-augmented
generation and chain-of-thought reasoning provide
enhanced capabilities for contextually-grounded
political text generation, though their application to
parliamentary speech evaluation remains underex-
plored.
Regarding parliamentary speech data, struc-
tured corpora like ParlaMint (Erjavec et al., 2025)
provide multi-lingual parliamentary proceedings.
Embedding-based approaches for political anal-
ysis, introduced in (Rheault and Cochrane, 2020),
demonstrate that embeddings can capture ideolog-
ical positioning in parliamentary text.
Domain-specific evaluation frameworks have
emerged across professional fields, including Fin-
Ben(Xieetal.,2024)andLexEval(Lietal.,2024a).
Political science applications have developed spe-
cializedbenchmarksforelectionpredictionandleg-
islative analysis, yet these focus primarily on classi-
fication and analysis tasks. Parliamentary speech
generation has attracted recent computational in-
terest, with work exploring European Parliament
consensus building (Zhang et al., 2025) and po-
litical impersonation authenticity (Herbold et al.,
2024), but evaluation frameworks remain under-
developed. Existing approaches focus on narrow
aspects like style mimicry rather than systematic
quality assessment across linguistic and political
authenticity dimensions that parliamentary speech
generation requires.
Our work addresses these gaps by establishing
abenchmarkresourcespecificallydesignedforpar-
liamentary speech generation."
2511.10585v1,http://arxiv.org/abs/2511.10585v1,2025-11-13 18:25:43+00:00,Textual understanding boost in the WikiRace,"The WikiRace game, where players navigate between Wikipedia articles using only hyperlinks, serves as a compelling benchmark for goal-directed search in complex information networks. This paper presents a systematic evaluation of navigation strategies for this task, comparing agents guided by graph-theoretic structure (betweenness centrality), semantic meaning (language model embeddings), and hybrid approaches. Through rigorous benchmarking on a large Wikipedia subgraph, we demonstrate that a purely greedy agent guided by the semantic similarity of article titles is overwhelmingly effective. This strategy, when combined with a simple loop-avoidance mechanism, achieved a perfect success rate and navigated the network with an efficiency an order of magnitude better than structural or hybrid methods. Our findings highlight the critical limitations of purely structural heuristics for goal-directed search and underscore the transformative potential of large language models to act as powerful, zero-shot semantic navigators in complex information spaces.","The analysis of human navigation paths in the Wikispeedia dataset has revealed consistent cognitive strategies that serve as a crucial benchmark for algorithmic approaches. These human strategies are not random but are guided by sophisticated, albeit imperfect, heuristics for managing the complexity of the information space.

Goal-directed navigation often follows a two-phase ``zoom-out, home-in'' heuristic \cite{siegart-2020}. Initially, players navigate from specific starting articles to broader, central ``hubs'' to escape isolated regions of the network \cite{west2009wikispeedia}. After reaching a hub, they enter the ``homing-in'' phase, selecting links semantically closer to the target \cite{lamprecht2017structure}. This contrasts with free-form browsing, which typically follows a path of increasing specificity \cite{rodi2017search}.

The ``zoom-out'' phase involves a structural leap, where the first click leads to an article with a much higher degree \cite{west2012human}. The subsequent ``homing-in'' phase is guided by semantic coherence, as the textual similarity between the current and target articles steadily increases \cite{west2012human}. This indicates that humans use a decentralized search strategy based on an implicit sense of semantic distance \cite{rodi2017search}. While shortest-path algorithms also use high-degree nodes, they do so as a byproduct of high betweenness centrality, not as a deliberate two-phase cognitive strategy.

This heuristic makes human paths less efficient than the mathematical optimum, often involving ``circling'' behavior near the target \cite{west2009wikispeedia,west2012human}. However, this apparent inefficiency is a feature of a robust cognitive strategy for navigating with incomplete information. The ``zoom-out, home-in'' heuristic manages uncertainty by moving to a well-connected hub, which increases the probability of finding a route to the target's semantic neighborhood. This strategy prioritizes success over optimal path length. Therefore, human ``inefficiency'' is a key trait of effective search under constraints, suggesting navigation agents should be evaluated on their ability to model this trade-off, not just on path length.

\subsection{Graph-theoretic Heuristics: Centrality-based Navigation}
Graph-theoretic heuristics guide WikiRace navigation by using centrality measures to quantify the importance of an article within the network's structure. Common measures like \emph{Degree Centrality}, \emph{Closeness Centrality}, \emph{Betweenness Centrality}, and \emph{PageRank} are used to identify nodes that are local hubs, efficient information spreaders, critical bridges between topics, or general authorities, respectively \cite{koschutzki2008centrality,cheriyan2020m}. A key distinction lies between radial measures (Degree, PageRank), which identify destination hubs, and medial measures (Betweenness), which identify intermediary bridges \cite{borgatti2006graph}.

However, these heuristics face a fundamental paradox in goal-directed navigation. While maximizing centrality effectively supports the human ""zoom-out"" strategy of moving from a specific article to a major hub, it inhibits the subsequent ``home-in'' phase. A purely centrality-based approach becomes trapped at high-importance nodes, as it cannot justify a move to a less central but more semantically relevant article closer to the target. This highlights the limitation of using static structural importance for a multi-stage navigation task.

\subsection{Semantic and Learning-based Navigation Strategies}
To overcome the limitations of purely structural heuristics, advanced strategies incorporate the semantic content of articles using Natural Language Processing (NLP) and machine learning. These methods model the ``homing-in'' phase of navigation by selecting links that are most semantically similar to the target.

The primary challenge is effectively representing article meaning. Methods have evolved from statistical models like Term Frequency-Inverse Document Frequency (TF-IDF) to more powerful dense vector embeddings. Models such as Word2Vec \cite{church2017word2vec} and the specialized Wikipedia2Vec \cite{yamada2018wikipedia2vec} learn vectors that capture nuanced semantic relationships, providing a more robust measure of similarity than sparse lexical methods \cite{kwon2020hierarchical}.

A parallel line of research frames WikiRace as a formal learning problem. One approach learns a heuristic function, $h(v)$, to predict the shortest path distance from a given article $v$ to the target. This learned heuristic can then be integrated into a classical search algorithm like $A^*$ to guide exploration more efficiently \cite{barron-2011}. Features for such models range from bag-of-words vectors to article metadata or cluster IDs \cite{barron-2011}.

The problem can also be modeled for Reinforcement Learning (RL) \cite{darvariu2024graph}, where an agent learns a policy, $\pi(a|s)$, to select a hyperlink that maximizes a cumulative reward. Given the large corpus of human gameplay data, Imitation Learning (IL) is a particularly suitable framework. For instance, Behavioral Cloning can train a policy to mimic human actions. In a significant result, \cite{zaheer2022learning} demonstrated that a policy learned via behavioral cloning on randomly sampled trajectories is sufficient to create an effective navigation agent for the full Wikipedia graph.

The evolution of these strategies reflects a broader trend from systems based on hand-crafted heuristics toward end-to-end learning. While modern models achieve impressive performance, their decision-making can be opaque. This creates a compelling research opportunity for developing strategies that are both high-performing and interpretable, potentially by combining the structural logic of network science with the representation power of modern machine learning.

A clear gap exists at the intersection of these approaches. On one side are purely structural methods like centrality, which are interpretable but too simplistic to capture the full complexity of the task. On the other are purely semantic or black-box learning methods, which are powerful but may ignore the fundamental network properties that guide human intuition and are difficult to analyze. This points to the need for novel network-based strategies that explicitly and intelligently integrate both the topological structure of the graph and the semantic content of its nodes. Such hybrid methods could potentially capture the best of both worlds: the structural awareness of centrality and the semantic nuance of modern NLP, leading to navigation policies that are not only more effective but also more interpretable and human-like. Developing such strategies represents a promising direction for future research.","The analysis of human navigation paths in the Wikispeedia dataset has revealed consistent cognitive strategies that serve as a crucial benchmark for algorithmic approaches. These human strategies are not random but are guided by sophisticated, albeit imperfect, heuristics for managing the complexity of the information space.

Goal-directed navigation often follows a two-phase ``zoom-out, home-in'' heuristic \cite{siegart-2020}. Initially, players navigate from specific starting articles to broader, central ``hubs'' to escape isolated regions of the network \cite{west2009wikispeedia}. After reaching a hub, they enter the ``homing-in'' phase, selecting links semantically closer to the target \cite{lamprecht2017structure}. This contrasts with free-form browsing, which typically follows a path of increasing specificity \cite{rodi2017search}.

The ``zoom-out'' phase involves a structural leap, where the first click leads to an article with a much higher degree \cite{west2012human}. The subsequent ``homing-in'' phase is guided by semantic coherence, as the textual similarity between the current and target articles steadily increases \cite{west2012human}. This indicates that humans use a decentralized search strategy based on an implicit sense of semantic distance \cite{rodi2017search}. While shortest-path algorithms also use high-degree nodes, they do so as a byproduct of high betweenness centrality, not as a deliberate two-phase cognitive strategy.

This heuristic makes human paths less efficient than the mathematical optimum, often involving ``circling'' behavior near the target \cite{west2009wikispeedia,west2012human}. However, this apparent inefficiency is a feature of a robust cognitive strategy for navigating with incomplete information. The ``zoom-out, home-in'' heuristic manages uncertainty by moving to a well-connected hub, which increases the probability of finding a route to the target's semantic neighborhood. This strategy prioritizes success over optimal path length. Therefore, human ``inefficiency'' is a key trait of effective search under constraints, suggesting navigation agents should be evaluated on their ability to model this trade-off, not just on path length.

\subsection{Graph-theoretic Heuristics: Centrality-based Navigation}
Graph-theoretic heuristics guide WikiRace navigation by using centrality measures to quantify the importance of an article within the network's structure. Common measures like \emph{Degree Centrality}, \emph{Closeness Centrality}, \emph{Betweenness Centrality}, and \emph{PageRank} are used to identify nodes that are local hubs, efficient information spreaders, critical bridges between topics, or general authorities, respectively \cite{koschutzki2008centrality,cheriyan2020m}. A key distinction lies between radial measures (Degree, PageRank), which identify destination hubs, and medial measures (Betweenness), which identify intermediary bridges \cite{borgatti2006graph}.

However, these heuristics face a fundamental paradox in goal-directed navigation. While maximizing centrality effectively supports the human ""zoom-out"" strategy of moving from a specific article to a major hub, it inhibits the subsequent ``home-in'' phase. A purely centrality-based approach becomes trapped at high-importance nodes, as it cannot justify a move to a less central but more semantically relevant article closer to the target. This highlights the limitation of using static structural importance for a multi-stage navigation task.

\subsection{Semantic and Learning-based Navigation Strategies}
To overcome the limitations of purely structural heuristics, advanced strategies incorporate the semantic content of articles using Natural Language Processing (NLP) and machine learning. These methods model the ``homing-in'' phase of navigation by selecting links that are most semantically similar to the target.

The primary challenge is effectively representing article meaning. Methods have evolved from statistical models like Term Frequency-Inverse Document Frequency (TF-IDF) to more powerful dense vector embeddings. Models such as Word2Vec \cite{church2017word2vec} and the specialized Wikipedia2Vec \cite{yamada2018wikipedia2vec} learn vectors that capture nuanced semantic relationships, providing a more robust measure of similarity than sparse lexical methods \cite{kwon2020hierarchical}.

A parallel line of research frames WikiRace as a formal learning problem. One approach learns a heuristic function, $h(v)$, to predict the shortest path distance from a given article $v$ to the target. This learned heuristic can then be integrated into a classical search algorithm like $A^*$ to guide exploration more efficiently \cite{barron-2011}. Features for such models range from bag-of-words vectors to article metadata or cluster IDs \cite{barron-2011}.

The problem can also be modeled for Reinforcement Learning (RL) \cite{darvariu2024graph}, where an agent learns a policy, $\pi(a|s)$, to select a hyperlink that maximizes a cumulative reward. Given the large corpus of human gameplay data, Imitation Learning (IL) is a particularly suitable framework. For instance, Behavioral Cloning can train a policy to mimic human actions. In a significant result, \cite{zaheer2022learning} demonstrated that a policy learned via behavioral cloning on randomly sampled trajectories is sufficient to create an effective navigation agent for the full Wikipedia graph.

The evolution of these strategies reflects a broader trend from systems based on hand-crafted heuristics toward end-to-end learning. While modern models achieve impressive performance, their decision-making can be opaque. This creates a compelling research opportunity for developing strategies that are both high-performing and interpretable, potentially by combining the structural logic of network science with the representation power of modern machine learning.

A clear gap exists at the intersection of these approaches. On one side are purely structural methods like centrality, which are interpretable but too simplistic to capture the full complexity of the task. On the other are purely semantic or black-box learning methods, which are powerful but may ignore the fundamental network properties that guide human intuition and are difficult to analyze. This points to the need for novel network-based strategies that explicitly and intelligently integrate both the topological structure of the graph and the semantic content of its nodes. Such hybrid methods could potentially capture the best of both worlds: the structural awareness of centrality and the semantic nuance of modern NLP, leading to navigation policies that are not only more effective but also more interpretable and human-like. Developing such strategies represents a promising direction for future research.","The analysis of human navigation paths in the Wikispeedia dataset has revealed
consistent cognitive strategies that serve as a crucial benchmark for algorithmic
approaches. These human strategies are not random but are guided by sophisti-
cated, albeit imperfect, heuristics for managing the complexity of the information
space.
Goal-directed navigation often follows a two-phase “zoom-out, home-in” heuris-
tic [11]. Initially, players navigate from specific starting articles to broader, cen-
tral “hubs” to escape isolated regions of the network [15]. After reaching a hub,
they enter the “homing-in” phase, selecting links semantically closer to the tar-
get [9]. This contrasts with free-form browsing, which typically follows a path of
increasing specificity [10].
The “zoom-out” phase involves a structural leap, where the first click leads
to an article with a much higher degree [13]. The subsequent “homing-in” phase
is guided by semantic coherence, as the textual similarity between the current
and target articles steadily increases [13]. This indicates that humans use a
decentralized search strategy based on an implicit sense of semantic distance
[10]. While shortest-path algorithms also use high-degree nodes, they do so as a"
2511.10004v1,http://arxiv.org/abs/2511.10004v1,2025-11-13 06:12:30+00:00,LampQ: Towards Accurate Layer-wise Mixed Precision Quantization for Vision Transformers,"How can we accurately quantize a pre-trained Vision Transformer model? Quantization algorithms compress Vision Transformers (ViTs) into low-bit formats, reducing memory and computation demands with minimal accuracy degradation. However, existing methods rely on uniform precision, ignoring the diverse sensitivity of ViT components to quantization. Metric-based Mixed Precision Quantization (MPQ) is a promising alternative, but previous MPQ methods for ViTs suffer from three major limitations: 1) coarse granularity, 2) mismatch in metric scale across component types, and 3) quantization-unaware bit allocation. In this paper, we propose LampQ (Layer-wise Mixed Precision Quantization for Vision Transformers), an accurate metric-based MPQ method for ViTs to overcome these limitations. LampQ performs layer-wise quantization to achieve both fine-grained control and efficient acceleration, incorporating a type-aware Fisher-based metric to measure sensitivity. Then, LampQ assigns bit-widths optimally through integer linear programming and further updates them iteratively. Extensive experiments show that LampQ provides the state-of-the-art performance in quantizing ViTs pre-trained on various tasks such as image classification, object detection, and zero-shot quantization.","%\smallsection{Quantizing ViTs}
%Model quantization~\citep{QDrop, PD-Quant} minimizes computational and memory requirements by mapping weights and activations into lower precision formats.
%For ViTs, researchers focus on addressing ViT-specific challenges, such as inter-channel variation in post-LayerNorm activations~\cite{RepQ-ViT}, the power-law distribution in post-Softmax activations~\cite{AdaLog}, and outliers in block outputs~\cite{OAS}.
%However, existing works allocate the same bit-width across the entire model, failing to reflect the varying sensitivity of different components.
%%
%In contrast, \method optimizes performance by allocating higher bits to more sensitive layers.
%This enables \method to achieve better accuracy than existing methods, even with lower bit-widths.

%\vspace{1mm}
%\smallsection{Mixed Precision Quantization (MPQ)}
MPQ~\cite{MPSurvey} quantizes different components of a model with varying bit-precisions.
There are four main approaches: learning, Reinforcement Learning (RL), Neural Architecture Search (NAS), and metric-based solutions.
%
Learning-based methods~\cite{SDQ, NIPQ} treat bit-widths as trainable parameters and update them based on loss gradients.
RL-based methods~\cite{AutoQ, MetaMix} leverage a RL agent to determine the allocation policy.
NAS-based solutions~\cite{BP-NAS, JAQ} explore the bit selection space through an automated search process.
Metric-based methods rely on statistical properties such as quantization entropy~\cite{QE} and orthogonality~\cite{OMPQ}.
%
Among them, \method quantifies sensitivity based on the trace of Fisher information matrix.
In this way, \method more precisely allocates bit-widths to layers and achieves better performance.","MPQ~\cite{MPSurvey} quantizes different components of a model with varying bit-precisions.
There are four main approaches: learning, Reinforcement Learning (RL), Neural Architecture Search (NAS), and metric-based solutions.

Learning-based methods~\cite{SDQ, NIPQ} treat bit-widths as trainable parameters and update them based on loss gradients.
RL-based methods~\cite{AutoQ, MetaMix} leverage a RL agent to determine the allocation policy.
NAS-based solutions~\cite{BP-NAS, JAQ} explore the bit selection space through an automated search process.
Metric-based methods rely on statistical properties such as quantization entropy~\cite{QE} and orthogonality~\cite{OMPQ}.

Among them, \method quantifies sensitivity based on the trace of Fisher information matrix.
In this way, \method more precisely allocates bit-widths to layers and achieves better performance.","MPQ (Rakka et al. 2024) quantizes different components of
a model with varying bit-precisions. There are four main
approaches: learning, Reinforcement Learning (RL), Neu-
ral Architecture Search (NAS), and metric-based solutions.
Learning-based methods (Huang et al. 2022; Shin et al.
2023) treat bit-widths as trainable parameters and update
them based on loss gradients. RL-based methods (Lou et al.
2020; Kim et al. 2024) leverage a RL agent to determine
the allocation policy. NAS-based solutions (Yu et al. 2020;
Wang et al. 2025) explore the bit selection space through
an automated search process. Metric-based methods rely on
statistical properties such as quantization entropy (Sun et al.
2022) and orthogonality (Ma et al. 2023). Among them,
LAMPQ quantifies sensitivity based on the trace of Fisher in-
formation matrix. In this way, LAMPQ more precisely allo-
cates bit-widths to layers and achieves better performance."
2511.10515v1,http://arxiv.org/abs/2511.10515v1,2025-11-13 17:20:46+00:00,LOCA-R: Near-Perfect Performance on the Chinese Physics Olympiad 2025,"Olympiad-level physics problem-solving presents a significant challenge for both humans and artificial intelligence (AI), as it requires a sophisticated integration of precise calculation, abstract reasoning, and a fundamental grasp of physical principles. The Chinese Physics Olympiad (CPhO), renowned for its complexity and depth, serves as an ideal and rigorous testbed for these advanced capabilities. In this paper, we introduce LOCA-R (LOgical Chain Augmentation for Reasoning), an improved version of the LOCA framework adapted for complex reasoning, and apply it to the CPhO 2025 theory examination. LOCA-R achieves a near-perfect score of 313 out of 320 points, solidly surpassing the highest-scoring human competitor and significantly outperforming all baseline methods.","\label{sec:related}

\paragraph{LLM Reasoning.} LLMs have been broadly applied to address general reasoning tasks by generating intermediate steps and exploring various reasoning paths, and are sometimes equipped with external tools such as a code interpreter. Existing strategies can empirically be divided into three main categories:
\begin{itemize}
    \item Test-time scaling frameworks, like Chain-of-Thought (CoT)\citep{kojima2022large,wei2022chain}, Tree-of-Thought (ToT) \citep{yao2023tree}, Graph-of-Thought (GoT) \citep{besta2024graph} and self-refine\citep{madaan2023self,liu2024large,liang2024internal,zhang2024sciinstruct}, usually relying only on the model's internal capabilities.
    \item Agentic frameworks (usually with external tools), such as Plan-and-Solve\citep{wang2023plan}, ReAct\citep{yao2022react} and Multi-Agent Debate (MAD)\citep{du2023improving}.
    \item Training-based methods like Supervised Fine-Tuning (SFT) or Reinforcement Learning (RL)\citep{zhang2024sciinstruct,lu2022dynamic,lewkowycz2022solving}, which critically depend on the availability of large-scale, expert-validated training data and can be prohibitively costly.
\end{itemize}
Targeted approaches like  Physics Supernova\citep{qiu2025physics} , Physics Reasoner\citep{pang2025physics} and Mixture of Refinement Agents (MoRA)\citep{jaiswal2024improving} have been developed for physics. Despite this, the fundamental issues of modeling inaccuracies and the selection of relevant principles remain largely unaddressed in the literature.

\paragraph{Physics Benchmarks.} The effort to advance AI in physics problem-solving is broadly supported by specialized benchmarks, ranging from expert-curated datasets focusing on competition-level difficulty, such as PHYBench \citep{qiu2025phybench} and OlympiadBench \citep{he2024olympiadbench}, to large-scale collections reflecting university-level coursework, such as PHYSICS \citep{feng2025physics}, ABench-Physics\citep{zhang2025abench}, PhysReason\citep{zhang2025physreason}, and PhysicsEval\citep{siddique2025physicseval}, and to specific domains such as TPBench\citep{chung2025theoretical}. However, the CPhO 2025 offers distinct advantages, including the absence of data contamination and the availability of detailed, reliable reference answers.","\paragraph{LLM Reasoning.} LLMs have been broadly applied to address general reasoning tasks by generating intermediate steps and exploring various reasoning paths, and are sometimes equipped with external tools such as a code interpreter. Existing strategies can empirically be divided into three main categories:
\begin{itemize}
    \item Test-time scaling frameworks, like Chain-of-Thought (CoT)\citep{kojima2022large,wei2022chain}, Tree-of-Thought (ToT) \citep{yao2023tree}, Graph-of-Thought (GoT) \citep{besta2024graph} and self-refine\citep{madaan2023self,liu2024large,liang2024internal,zhang2024sciinstruct}, usually relying only on the model's internal capabilities.
    \item Agentic frameworks (usually with external tools), such as Plan-and-Solve\citep{wang2023plan}, ReAct\citep{yao2022react} and Multi-Agent Debate (MAD)\citep{du2023improving}.
    \item Training-based methods like Supervised Fine-Tuning (SFT) or Reinforcement Learning (RL)\citep{zhang2024sciinstruct,lu2022dynamic,lewkowycz2022solving}, which critically depend on the availability of large-scale, expert-validated training data and can be prohibitively costly.
\end{itemize}
Targeted approaches like  Physics Supernova\citep{qiu2025physics} , Physics Reasoner\citep{pang2025physics} and Mixture of Refinement Agents (MoRA)\citep{jaiswal2024improving} have been developed for physics. Despite this, the fundamental issues of modeling inaccuracies and the selection of relevant principles remain largely unaddressed in the literature.

\paragraph{Physics Benchmarks.} The effort to advance AI in physics problem-solving is broadly supported by specialized benchmarks, ranging from expert-curated datasets focusing on competition-level difficulty, such as PHYBench \citep{qiu2025phybench} and OlympiadBench \citep{he2024olympiadbench}, to large-scale collections reflecting university-level coursework, such as PHYSICS \citep{feng2025physics}, ABench-Physics\citep{zhang2025abench}, PhysReason\citep{zhang2025physreason}, and PhysicsEval\citep{siddique2025physicseval}, and to specific domains such as TPBench\citep{chung2025theoretical}. However, the CPhO 2025 offers distinct advantages, including the absence of data contamination and the availability of detailed, reliable reference answers.",
2511.10091v1,http://arxiv.org/abs/2511.10091v1,2025-11-13 08:45:24+00:00,SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition,"Large Language Models (LLMs) hold rich implicit knowledge and powerful transferability. In this paper, we explore the combination of LLMs with the human skeleton to perform action classification and description. However, when treating LLM as a recognizer, two questions arise: 1) How can LLMs understand skeleton? 2) How can LLMs distinguish among actions? To address these problems, we introduce a novel paradigm named learning Skeleton representation with visUal-motion knowledGe for Action Recognition (SUGAR). In our pipeline, we first utilize off-the-shelf large-scale video models as a knowledge base to generate visual, motion information related to actions. Then, we propose to supervise skeleton learning through this prior knowledge to yield discrete representations. Finally, we use the LLM with untouched pre-training weights to understand these representations and generate the desired action targets and descriptions. Notably, we present a Temporal Query Projection (TQP) module to continuously model the skeleton signals with long sequences. Experiments on several skeleton-based action classification benchmarks demonstrate the efficacy of our SUGAR. Moreover, experiments on zero-shot scenarios show that SUGAR is more versatile than linear-based methods.","\noindent \textbf{Unimodal and Multimodal Action Recognition.} Unimodal methods for action recognition can be categorized into RGB-based and skeleton-based (joint+bone), both of which perform inference from a single modality of information. RGB-based methods \cite{ye2024pose,nan20243sg,rajendran2024review,li2025role} mainly deal with a sequence of image information, and are good at capturing rich visual contexts. However, they usually suffer from excessive parameters and overfitting. On the other hand, the skeleton-based recognition performance has made great progress from the ST-GCN model proposed by Yan et al. \cite{st-gcn}. Afterwards, models like 2s-AGCN \cite{2sAGCN} and CTR-GCN \cite{ctrgcn} use dual-stream to dynamically learn the topology and achieve higher accuracy. However, these methods have the drawback of lacking visual information support and depending on accurate human pose estimation techniques.

The combination of skeleton and visual can enhance the recognition of actions. Fabien et al. \cite{hands} proposed a novel multimodal approach that focuses on the hand pose. This method opens up new possibilities for future work but it ignores some details of the rest of the body. Some works \cite{toyota} attempt to weight fusion of RGB images by utilizing pose data, which is an innovative way of integrating different types of modality. Other works \cite{vpn, mmnet} introduced a spatial embedding to map pose data to visual features. \cite{vpn++} developed a feature-level and attention-level distillation, which offers a practical solution for combining RGB and pose. However, the fusion of multimodal inputs requires massive computation. Moreover, we have no way to know whether heterogeneous visualizations and skeletons can be effectively combined in the learning process.

\noindent \textbf{Vision-Language Models for Action Recognition.} Co-training multiple modalities can learn powerful representations for downstream tasks. For example, vision-language pre-trained models such as CLIP \cite{clip}, BLIP \cite{blip}, and ALIGN \cite{align} have demonstrated that different modalities can be approximated between the two representations by defined learning objectives. Besides the contrastive learning of images and language, some work has started to apply representation learning to the action recognition domain. Wang et al. \cite{actionclip} follow the CLIP training strategy to help with downstream action recognition tasks. They convert action labels into representations and perform representation learning by calculating the similarity with the video. However, we argue that labels like ``\textit{A action of \{\}}'' lack substantial semantic information and do not provide exhaustive knowledge. Other efforts \cite{xie2024fusionmamba,lin2025reliable} to introduce representation learning into skeleton still do not provide rich linguistic knowledge to aid learning. To learn a more discrete representation, we introduce linguistic knowledge such as motion and vision to improve the performance.

\noindent \textbf{Large Language Models for Action Recognition.} A variety of studies \cite{CAT,ye2025cat+, videochat} have shown that the powerful generalization ability of LLMs can help different downstream task reasoning. With the help of prompt learning \cite{promptlearning1}, LLMs can generate any form of text with any content. Recently, some work has started to focus on how to apply LLMs to action-centric tasks. Qu et al. \cite{actionllm} use VQ-VAE \cite{vqvae} to learn specific action tokens and fine-tune large models using LoRA \cite{lora}. Drawing on their ideas, we find that utilizing rich linguistic knowledge to learn skeleton representations can bring better performance to the recogniser and performs well in zero-shot scenarios.","\noindent \textbf{Unimodal and Multimodal Action Recognition.} Unimodal methods for action recognition can be categorized into RGB-based and skeleton-based (joint+bone), both of which perform inference from a single modality of information. RGB-based methods \cite{ye2024pose,nan20243sg,rajendran2024review,li2025role} mainly deal with a sequence of image information, and are good at capturing rich visual contexts. However, they usually suffer from excessive parameters and overfitting. On the other hand, the skeleton-based recognition performance has made great progress from the ST-GCN model proposed by Yan et al. \cite{st-gcn}. Afterwards, models like 2s-AGCN \cite{2sAGCN} and CTR-GCN \cite{ctrgcn} use dual-stream to dynamically learn the topology and achieve higher accuracy. However, these methods have the drawback of lacking visual information support and depending on accurate human pose estimation techniques.

The combination of skeleton and visual can enhance the recognition of actions. Fabien et al. \cite{hands} proposed a novel multimodal approach that focuses on the hand pose. This method opens up new possibilities for future work but it ignores some details of the rest of the body. Some works \cite{toyota} attempt to weight fusion of RGB images by utilizing pose data, which is an innovative way of integrating different types of modality. Other works \cite{vpn, mmnet} introduced a spatial embedding to map pose data to visual features. \cite{vpn++} developed a feature-level and attention-level distillation, which offers a practical solution for combining RGB and pose. However, the fusion of multimodal inputs requires massive computation. Moreover, we have no way to know whether heterogeneous visualizations and skeletons can be effectively combined in the learning process.

\noindent \textbf{Vision-Language Models for Action Recognition.} Co-training multiple modalities can learn powerful representations for downstream tasks. For example, vision-language pre-trained models such as CLIP \cite{clip}, BLIP \cite{blip}, and ALIGN \cite{align} have demonstrated that different modalities can be approximated between the two representations by defined learning objectives. Besides the contrastive learning of images and language, some work has started to apply representation learning to the action recognition domain. Wang et al. \cite{actionclip} follow the CLIP training strategy to help with downstream action recognition tasks. They convert action labels into representations and perform representation learning by calculating the similarity with the video. However, we argue that labels like ``\textit{A action of \{\}}'' lack substantial semantic information and do not provide exhaustive knowledge. Other efforts \cite{xie2024fusionmamba,lin2025reliable} to introduce representation learning into skeleton still do not provide rich linguistic knowledge to aid learning. To learn a more discrete representation, we introduce linguistic knowledge such as motion and vision to improve the performance.

\noindent \textbf{Large Language Models for Action Recognition.} A variety of studies \cite{CAT,ye2025cat+, videochat} have shown that the powerful generalization ability of LLMs can help different downstream task reasoning. With the help of prompt learning \cite{promptlearning1}, LLMs can generate any form of text with any content. Recently, some work has started to focus on how to apply LLMs to action-centric tasks. Qu et al. \cite{actionllm} use VQ-VAE \cite{vqvae} to learn specific action tokens and fine-tune large models using LoRA \cite{lora}. Drawing on their ideas, we find that utilizing rich linguistic knowledge to learn skeleton representations can bring better performance to the recogniser and performs well in zero-shot scenarios.","Unimodal and Multimodal Action Recognition.Uni-
modal methods for action recognition can be categorized
into RGB-based and skeleton-based (joint+bone), both of
which perform inference from a single modality of infor-
mation. RGB-based methods (Ye and Yu 2024; Nan et al.
2024; Rajendran et al. 2024; Li et al. 2025) mainly deal
with a sequence of image information, and are good at
capturing rich visual contexts. However, they usually suf-
fer from excessive parameters and overfitting. On the other
hand, the skeleton-based recognition performance has made
great progress from the ST-GCN model proposed by Yan
et al. (Yan, Xiong, and Lin 2018). Afterwards, models like
2s-AGCN (Shi et al. 2019b) and CTR-GCN (Chen et al.
2021) use dual-stream to dynamically learn the topology and
achieve higher accuracy. However, these methods have the
drawback of lacking visual information support and depend-
ing on accurate human pose estimation techniques.
The combination of skeleton and visual can enhance the
recognition of actions. Fabien et al. (Baradel, Wolf, and
Mille 2017) proposed a novel multimodal approach that fo-
cuses on the hand pose. This method opens up new possibil-
ities for future work but it ignores some details of the rest
of the body. Some works (Das et al. 2019) attempt to weight
Skeleton Video FramesGCN -BlockGCN -BlockGCN -BlockGCN -Block...
Skeleton EncoderText StorageCLIP
Text Encoder
Step 2: Learning skeleton with visual -motion knowledge Large Language Model ( w/ LoRA )
Temporal Query ProjectionTuning Instruction
Given a sequence of action tokens \n< action >, please choose the most compatible action
from: [action list], and describe the action.Output
{Cook.Clean dishes} . The man is moving back and forth
and waving his arms and hands to clean the dishes.
Step 1: Text
Construction
...
...
MILLFeature space
VLMAn action of { Cook.Cleandishes };
head remains upright; hand holds
sponge; arm moves back and
forth scrubbing dishes; hip stays
in place; leg remains stable; foot
provides balance.Motion Knowledge
The man is cleaning dishes in the
kitchen. He is standing in front of
the table and turning his body back
and forth ...Visual Knowledge Cook.Clean dishes
drink water
eat meal
brush teeth
walking towards
...Predefined
Action List
Based on the image,
describe 'Cook.Clean
dishses' in detailStep 3: A ction recognition
Linear
layer...
(Only in training )
Figure 2: Overall framework of SUGAR. The complete training procedure is divided into three parts. We use the GPT-generated
fine-grained action description and VLM-generated visual description as input for the text encoder to supervise the skeleton
representation learning, where the linear layer maps the skeleton to the same feature space as the text. During inference, only
the skeleton data needs to be input for action recognition.
fusion of RGB images by utilizing pose data, which is an
innovative way of integrating different types of modality.
Other works (Das et al. 2020; Bruce et al. 2022) introduced a
spatial embedding to map pose data to visual features. (Das
et al. 2021) developed a feature-level and attention-level
distillation, which offers a practical solution for combining
RGB and pose. However, the fusion of multimodal inputs
requires massive computation. Moreover, we have no way
to know whether heterogeneous visualizations and skeletons
can be effectively combined in the learning process.
Vision-Language Models for Action Recognition.Co-
training multiple modalities can learn powerful representa-
tions for downstream tasks. For example, vision-language
pre-trained models such as CLIP (Radford et al. 2021), BLIP
(Li et al. 2022), and ALIGN (Jia et al. 2021) have demon-
strated that different modalities can be approximated be-
tween the two representations by defined learning objec-
tives. Besides the contrastive learning of images and lan-
guage, some work has started to apply representation learn-
ing to the action recognition domain. Wang et al. (Wang,
Xing, and Liu 2021) follow the CLIP training strategy to
help with downstream action recognition tasks. They con-
vert action labels into representations and perform represen-
tation learning by calculating the similarity with the video.
However, we argue that labels like “A action of{}” lack sub-
stantial semantic information and do not provide exhaustive
knowledge. Other efforts (Xie et al. 2024; Lin et al. 2025)
to introduce representation learning into skeleton still do notprovide rich linguistic knowledge to aid learning. To learn a
more discrete representation, we introduce linguistic knowl-
edge such as motion and vision to improve the performance.
Large Language Models for Action Recognition.A vari-
ety of studies (Ye et al. 2024, 2025; Li et al. 2023b) have
shown that the powerful generalization ability of LLMs can
help different downstream task reasoning. With the help of
prompt learning (Zhou et al. 2022), LLMs can generate any
form of text with any content. Recently, some work has
started to focus on how to apply LLMs to action-centric
tasks. Qu et al. (Qu, Cai, and Liu 2024) use VQ-V AE (Van
Den Oord, Vinyals et al. 2017) to learn specific action tokens
and fine-tune large models using LoRA (Hu et al. 2022).
Drawing on their ideas, we find that utilizing rich linguistic
knowledge to learn skeleton representations can bring better
performance to the recogniser and performs well in zero-
shot scenarios."
2511.09332v1,http://arxiv.org/abs/2511.09332v1,2025-11-12 13:47:01+00:00,Distribution-Based Feature Attribution for Explaining the Predictions of Any Classifier,"The proliferation of complex, black-box AI models has intensified the need for techniques that can explain their decisions. Feature attribution methods have become a popular solution for providing post-hoc explanations, yet the field has historically lacked a formal problem definition. This paper addresses this gap by introducing a formal definition for the problem of feature attribution, which stipulates that explanations be supported by an underlying probability distribution represented by the given dataset. Our analysis reveals that many existing model-agnostic methods fail to meet this criterion, while even those that do often possess other limitations. To overcome these challenges, we propose Distributional Feature Attribution eXplanations (DFAX), a novel, model-agnostic method for feature attribution. DFAX is the first feature attribution method to explain classifier predictions directly based on the data distribution. We show through extensive experiments that DFAX is more effective and efficient than state-of-the-art baselines.","This section reviews the two predominant families of model-agnostic feature attribution methods, followed by an overview of kernel density estimation, a core component of our proposed DFAX method.



\subsection{Local Approximation Methods}

Local approximation methods, as illustrated in Figure~\ref{fig:illustration}(a), operate by first defining a neighborhood around the target instance and then fitting a simple surrogate model to the classifier's predictions within this local region.
Feature attributions are then derived from this local surrogate model.

A prominent example is LIME \cite{ribeiro2016should}, which generates its neighborhood by creating synthetic points around the target instance via random sampling before fitting an explanatory linear model.
DLIME \cite{zafar2021deterministic}, a deterministic version of LIME, leverages agglomerative hierarchical clustering on the training data to define the neighborhood.
Similarly, MAPLE \cite{plumb2018model} and SLISE \cite{bjorklund2023explaining} select neighboring points from the given dataset instead of generating synthetic points as done in LIME.
MAPLE weights these neighbors to produce more faithful local explanations, while SLISE fits a sparse, locally linear regression model whose coefficients serve as feature attributions.
Focusing on improving stability and unidirectionality, LINEX \cite{dhurandhar2023locally} minimizes sensitivity to perturbations in a way inspired by the invariant risk minimization (IRM) principle.
The specific implementation of LINEX can vary, as the local environments for IRM may be created differently depending on the application scenario.



\subsection{Perturbation-Based Methods}

As depicted in Figure~\ref{fig:illustration}(b), perturbation-based methods operate by perturbing a feature's value and measuring the resulting degradation in the classifier's performance (\textit{e.g.}, the decrease in the predicted probability for the target class).

A famous method in this category is SHAP \cite{lundberg2017unified}, which is grounded in cooperative game theory and calculates feature importance using Shapley values \cite{shapley1953value}, the marginal contribution of each feature to the prediction for the target instance.
The practical implementation of SHAP varies based on how this contribution is estimated.
A theoretically pure approach, based on Shapley regression values \cite{lipovetsky2001analysis}, requires retraining the classifier on all possible subsets of features.
Common implementations instead use Shapley sampling values \cite{strumbelj2014explaining}.
These methods avoid retraining by using the original classifier trained with all feature present, and observing its performance changes on perturbed instances created by combining feature-values from the target instance and a background dataset.

Another perturbation-based method is PFI \cite{fisher2019all}, a model-agnostic version of the original PFI \cite{breiman2001random}.
PFI measures the importance of a feature by the expected loss in classifier performance after permuting the feature with values across the entire dataset.



\subsection{Kernel Density Estimation}

A kernel density estimator (KDE) is a non-parametric method for estimating the density/probability of each point in a given dataset \cite{rosenblatt1956remarks, ting2021isolation}.
Given a dataset $\X\subset\R^d$, the KDE at any point $\x^*\in\R^d$ is defined as:
\begin{equation*}
    K(\x^*|\X)=\frac1{|\X|}\sum_{\x\in\X}\k(\x^*,\x)
\end{equation*}
where $\k$ is a point kernel.
Different choices of $\k$ yield different estimators.
For example, a Gaussian kernel results in the Gaussian Kernel Density Estimator (GKDE), while the Isolation Kernel \cite{ting2018isolation} produces Isolation Kernel Density Estimator (IKDE) \cite{ting2021isolation}.

A fast alternative to GKDE in outlying aspects mining tasks \cite{wells2019new} is SiNNE \cite{samariya2020new}, which is a simplified version of iNNE \cite{bandaragoda2014efficient}.
While originally designed to estimate anomaly scores, SiNNE can be used as an efficient substitute for KDE in our feature attribution task.

The KDE computation can be significantly accelerated if the point kernel can be approximated as $\k(x,y)\approx\left<\varphi(x), \varphi(y)\right>$ via some technique like the Nystr{\""o}m method \cite{williams2000using}, where $\varphi$ is a finite-dimensional feature map approximating the feature map of $\k$.
With this, the KDE can be re-expressed as:
\begin{equation*}
     K(\x^*|\X)\approx\left<\varphi(\x^*), \widehat\Phi(\X)\right>
\end{equation*}
where $\widehat\Phi(\X)=\frac1{|\X|}\sum_{\x\in\X}\varphi(\x)$ is the kernel mean map of $\X$ \cite{muandet2017kernel}.
This allows any subsequent probability/density estimation to be performed in $\O(1)$ time after a one-off computation for the kernel mean map of $\X$.","This section reviews the two predominant families of model-agnostic feature attribution methods, followed by an overview of kernel density estimation, a core component of our proposed DFAX method.



\subsection{Local Approximation Methods}

Local approximation methods, as illustrated in Figure~\ref{fig:illustration}(a), operate by first defining a neighborhood around the target instance and then fitting a simple surrogate model to the classifier's predictions within this local region.
Feature attributions are then derived from this local surrogate model.

A prominent example is LIME \cite{ribeiro2016should}, which generates its neighborhood by creating synthetic points around the target instance via random sampling before fitting an explanatory linear model.
DLIME \cite{zafar2021deterministic}, a deterministic version of LIME, leverages agglomerative hierarchical clustering on the training data to define the neighborhood.
Similarly, MAPLE \cite{plumb2018model} and SLISE \cite{bjorklund2023explaining} select neighboring points from the given dataset instead of generating synthetic points as done in LIME.
MAPLE weights these neighbors to produce more faithful local explanations, while SLISE fits a sparse, locally linear regression model whose coefficients serve as feature attributions.
Focusing on improving stability and unidirectionality, LINEX \cite{dhurandhar2023locally} minimizes sensitivity to perturbations in a way inspired by the invariant risk minimization (IRM) principle.
The specific implementation of LINEX can vary, as the local environments for IRM may be created differently depending on the application scenario.



\subsection{Perturbation-Based Methods}

As depicted in Figure~\ref{fig:illustration}(b), perturbation-based methods operate by perturbing a feature's value and measuring the resulting degradation in the classifier's performance (\textit{e.g.}, the decrease in the predicted probability for the target class).

A famous method in this category is SHAP \cite{lundberg2017unified}, which is grounded in cooperative game theory and calculates feature importance using Shapley values \cite{shapley1953value}, the marginal contribution of each feature to the prediction for the target instance.
The practical implementation of SHAP varies based on how this contribution is estimated.
A theoretically pure approach, based on Shapley regression values \cite{lipovetsky2001analysis}, requires retraining the classifier on all possible subsets of features.
Common implementations instead use Shapley sampling values \cite{strumbelj2014explaining}.
These methods avoid retraining by using the original classifier trained with all feature present, and observing its performance changes on perturbed instances created by combining feature-values from the target instance and a background dataset.

Another perturbation-based method is PFI \cite{fisher2019all}, a model-agnostic version of the original PFI \cite{breiman2001random}.
PFI measures the importance of a feature by the expected loss in classifier performance after permuting the feature with values across the entire dataset.



\subsection{Kernel Density Estimation}

A kernel density estimator (KDE) is a non-parametric method for estimating the density/probability of each point in a given dataset \cite{rosenblatt1956remarks, ting2021isolation}.
Given a dataset $\X\subset\R^d$, the KDE at any point $\x^*\in\R^d$ is defined as:
\begin{equation*}
    K(\x^*|\X)=\frac1{|\X|}\sum_{\x\in\X}\k(\x^*,\x)
\end{equation*}
where $\k$ is a point kernel.
Different choices of $\k$ yield different estimators.
For example, a Gaussian kernel results in the Gaussian Kernel Density Estimator (GKDE), while the Isolation Kernel \cite{ting2018isolation} produces Isolation Kernel Density Estimator (IKDE) \cite{ting2021isolation}.

A fast alternative to GKDE in outlying aspects mining tasks \cite{wells2019new} is SiNNE \cite{samariya2020new}, which is a simplified version of iNNE \cite{bandaragoda2014efficient}.
While originally designed to estimate anomaly scores, SiNNE can be used as an efficient substitute for KDE in our feature attribution task.

The KDE computation can be significantly accelerated if the point kernel can be approximated as $\k(x,y)\approx\left<\varphi(x), \varphi(y)\right>$ via some technique like the Nystr{\""o}m method \cite{williams2000using}, where $\varphi$ is a finite-dimensional feature map approximating the feature map of $\k$.
With this, the KDE can be re-expressed as:
\begin{equation*}
     K(\x^*|\X)\approx\left<\varphi(\x^*), \widehat\Phi(\X)\right>
\end{equation*}
where $\widehat\Phi(\X)=\frac1{|\X|}\sum_{\x\in\X}\varphi(\x)$ is the kernel mean map of $\X$ \cite{muandet2017kernel}.
This allows any subsequent probability/density estimation to be performed in $\O(1)$ time after a one-off computation for the kernel mean map of $\X$.","This section reviews the two predominant families of
model-agnostic feature attribution methods, followed by an
overview of kernel density estimation, a core component of
our proposed DFAX method.
Local Approximation Methods
Local approximation methods, as illustrated in Figure 1(a),
operate by first defining a neighborhood around the target
instance and then fitting a simple surrogate model to the
classifier’s predictions within this local region. Feature at-
tributions are then derived from this local surrogate model.
A prominent example is LIME (Ribeiro, Singh, and
Guestrin 2016), which generates its neighborhood by cre-
ating synthetic points around the target instance via ran-
dom sampling before fitting an explanatory linear model.
DLIME (Zafar and Khan 2021), a deterministic version
of LIME, leverages agglomerative hierarchical clustering
on the training data to define the neighborhood. Similarly,
MAPLE (Plumb, Molitor, and Talwalkar 2018) and SLISE
(Bj¨orklund et al. 2023) select neighboring points from the
given dataset instead of generating synthetic points as done
in LIME. MAPLE weights these neighbors to produce more
faithful local explanations, while SLISE fits a sparse, locally
linear regression model whose coefficients serve as feature
attributions. Focusing on improving stability and unidirec-
tionality, LINEX (Dhurandhar et al. 2023) minimizes sensi-
tivity to perturbations in a way inspired by the invariant risk
minimization (IRM) principle. The specific implementation
of LINEX can vary, as the local environments for IRM maybe created differently depending on the application scenario.
Perturbation-Based Methods
As depicted in Figure 1(b), perturbation-based methods op-
erate by perturbing a feature’s value and measuring the re-
sulting degradation in the classifier’s performance (e.g., the
decrease in the predicted probability for the target class).
A famous method in this category is SHAP (Lundberg
and Lee 2017), which is grounded in cooperative game the-
ory and calculates feature importance using Shapley values
(Shapley 1953), the marginal contribution of each feature to
the prediction for the target instance. The practical imple-
mentation of SHAP varies based on how this contribution is
estimated. A theoretically pure approach, based on Shapley
regression values (Lipovetsky and Conklin 2001), requires
retraining the classifier on all possible subsets of features.
Common implementations instead use Shapley sampling
values (Strumbelj and Kononenko 2014). These methods
avoid retraining by using the original classifier trained with
all feature present, and observing its performance changes
on perturbed instances created by combining feature-values
from the target instance and a background dataset.
Another perturbation-based method is PFI (Fisher, Rudin,
and Dominici 2019), a model-agnostic version of the origi-
nal PFI (Breiman 2001). PFI measures the importance of a
feature by the expected loss in classifier performance after
permuting the feature with values across the entire dataset.
Kernel Density Estimation
A kernel density estimator (KDE) is a non-parametric
method for estimating the density/probability of each point
in a given dataset (Rosenblatt 1956; Ting et al. 2021). Given
a datasetX⊂Rd, the KDE at any pointx∗∈Rdis defined
as:
K(x∗|X) =1
|X|X
x∈Xκ(x∗,x)
whereκis a point kernel. Different choices ofκyield differ-
ent estimators. For example, a Gaussian kernel results in the
Gaussian Kernel Density Estimator (GKDE), while the Iso-
lation Kernel (Ting, Zhu, and Zhou 2018) produces Isolation
Kernel Density Estimator (IKDE) (Ting et al. 2021).
A fast alternative to GKDE in outlying aspects mining
tasks (Wells and Ting 2019) is SiNNE (Samariya et al.
2020), which is a simplified version of iNNE (Bandaragoda
et al. 2014). While originally designed to estimate anomaly
scores, SiNNE can be used as an efficient substitute for KDE
in our feature attribution task.
The KDE computation can be significantly accelerated
if the point kernel can be approximated asκ(x, y)≈
⟨φ(x), φ(y)⟩via some technique like the Nystr ¨om method
(Williams and Seeger 2000), whereφis a finite-dimensional
feature map approximating the feature map ofκ. With this,
the KDE can be re-expressed as:
K(x∗|X)≈D
φ(x∗),bΦ(X)E
wherebΦ(X) =1
|X|P
x∈Xφ(x)is the kernel mean map of
X(Muandet et al. 2017). This allows any subsequent proba-
bility/density estimation to be performed inO(1)time after
a one-off computation for the kernel mean map ofX.
Problem Definition
When the objective is to understand the model’s logic on its
operational data distribution, the problem of feature attribu-
tion can be formally defined as follows:
LetA={s j}d
j=1be the set of features andmbe the total
number of classes. Letfbe a classifier trained on a dataset
D⊂Rd, which maps inputs to either a predicted class or
a predicted probability for a target class (f:Rd7→[m]
or[0,1]). Letx∗∈Rdbe the target instance andX=
{xi}n
i=1⊂Rdbe a given dataset. We assumex∗,X, andD
are all independent and identically distributed (i.i.d.) sam-
ples from the same underlying probability distributionP.
Definition 1(Feature Attribution).For a target instance
x∗∼ Pwith featuresAwhose predictiony∗=f(x∗)
is produced by classifierf, the task of feature attribution
aims to provide an explanation as a scoreI(x∗, s|X)to
each features∈ A. This score quantifies the influence of
the specific feature-value,x∗
s, on the classifierfto pro-
duce the predictiony∗, where a higher score indicates a
greater influence towards this prediction. The explanatory
model,I(·|X), must be built directly from the datasetX,
which reflects the underlying distributionP, and the score
I(x∗, s|X)is valid if and only if it is supported byP.
A crucial tenet of this definition is the role of the dataset
X, which serves as an empirical representation of the un-
derlying distributionP. Any modification toXthat changes
the underlying distribution, in the process of building theexplanatory modelI(·|X), invalidates its feature attribution.
This is because building the explanatory model using syn-
thetic or out-of-distribution (OOD) instances produces ex-
planations based on a distribution where the model’s behav-
ior is irrelevant or inapplicable. In a nutshell, the key cri-
terion of Definition 1 is thatthe explanatory modelI(·|X)
and its explanationI(x∗, s|X)must be supported by the dis-
tributionPwhich is represented by the unmodified dataset
X(no OOD instance is used for generating feature attribu-
tions).
Analyses of Existing Methods
Definition 1 provides the criterion for assessing any feature
attribution method. In this section, we analyze the two pre-
dominant families of model-agnostic methods.
(a)Local approximation methods. Some of these meth-
ods satisfy Definition"
2511.09025v1,http://arxiv.org/abs/2511.09025v1,2025-11-12 06:25:13+00:00,FLAD: Federated Learning for LLM-based Autonomous Driving in Vehicle-Edge-Cloud Networks,"Large Language Models (LLMs) have impressive data fusion and reasoning capabilities for autonomous driving (AD). However, training LLMs for AD faces significant challenges including high computation transmission costs, and privacy concerns associated with sensitive driving data. Federated Learning (FL) is promising for enabling autonomous vehicles (AVs) to collaboratively train models without sharing raw data. We present Federated LLM-based Autonomous Driving (FLAD), an FL framework that leverages distributed multimodal sensory data across AVs in heterogeneous environment. FLAD has three key innovations: (1) a cloud-edge-vehicle collaborative architecture that reduces communication delay and preserving data privacy; (2) an intelligent parallelized collaborative training with a communication scheduling mechanism that optimizes training efficiency, leveraging end-devices otherwise having insufficient resources for model training; and (3) a knowledge distillation method that personalizes LLM according to heterogeneous edge data. In addition, we prototype FLAD in a testbed with NVIDIA Jetsons, overcoming practical implementation challenges including CPU/GPU memory sharing in resource-constrained devices, dynamic model partitions, and fault-tolerant training.Extensive experimental evaluation demonstrates that FLAD achieves superior end-to-end AD performance while efficiently utilizing distributed vehicular resources, opening up new possibilities for future collaborative AD model training and knowledge sharing.","%End2End AD
\textbf{End-to-End Autonomous Driving.}~Recent advancements in end-to-end AD leverage vision-language models, multimodal fusion, and unified architectures. DriveAdapter \cite{jia2023driveadapter} aligns perception and planning features, while DriveLM Agent \cite{sima2023drivelm} incorporates vision-language reasoning for making decision. Talk2BEV~\cite{choudhary2023talk2bev} enhances spatial understanding via BEV maps. Multimodal fusion approaches, such as LMDrive~\cite{shao2024lmdrive}, integrate sensor data with natural language navigation. DriveDreamer~\cite{wang2024drivedreamer} introduces a world model trained on real-world scenarios, and UniAD~\cite{hu2023planning} unifies feature abstractions for holistic interaction modeling. While these approaches enhance generalization, they rely on centralized data centers, raising privacy concerns and limiting adaptability. Our work shifts toward FL, enabling privacy-preserving, region-specific adaptation while improving resource efficiency.
%parallelism in a data center

\textbf{Parallelism in Datacenter.}~Scalable model training relies on various parallelism strategies in datacenters. ZeRO~\cite{rajbhandari2020zero} reduces memory redundancy for trillion-parameter models, while Megatron~\cite{shoeybi2019megatron} improves intra-layer parallelism. Pipeline parallelism frameworks, including TensorPipe~\cite{huang2019gpipe} and DAPPLE~\cite{fan2021dapple}, optimize layer-based training, while hybrid approaches like nnScaler~\cite{lin2024nnscaler} balance resource utilization. Oobleck~\cite{jang2023oobleck} enhances fault tolerance in distributed training. However, these techniques assume stable, high-speed network connectivity, making them unsuitable for vehicular edge environments characterized by bandwidth variability and dynamic topologies. FLAD adapts parallelism techniques for decentralized, resource-constrained environments while preserving data privacy vian FL.
%distributed ML scheduling

\textbf{Machine Learning Scheduling.}~Efficient ML scheduling improves resource utilization in constrained environments. Asteroid~\cite{ye2024asteroid} accelerates fault-tolerant pipeline training, while EdgePipe~\cite{yoon2021edgepipe} optimizes execution under fluctuating network conditions. StellaTrain~\cite{lim2024accelerating} and FlexNN~\cite{li2024flexnn} enhance network-aware scheduling, and AutoFed~\cite{zheng2023autofed} addresses multimodal sensor heterogeneity in FL. Alpa~\cite{zheng2022alpa} unifies parallelism strategies across heterogeneous devices. However, existing solutions do not fully account for extreme mobility, network instability, and LLM-specific computational demands in vehicular environments. FLAD bridges this gap by integrating FL, pipeline parallelism, and cloud-edge-vehicle collaboration to enable efficient, privacy-preserving AD model training.","\textbf{End-to-End Autonomous Driving.}~Recent advancements in end-to-end AD leverage vision-language models, multimodal fusion, and unified architectures. DriveAdapter \cite{jia2023driveadapter} aligns perception and planning features, while DriveLM Agent \cite{sima2023drivelm} incorporates vision-language reasoning for making decision. Talk2BEV~\cite{choudhary2023talk2bev} enhances spatial understanding via BEV maps. Multimodal fusion approaches, such as LMDrive~\cite{shao2024lmdrive}, integrate sensor data with natural language navigation. DriveDreamer~\cite{wang2024drivedreamer} introduces a world model trained on real-world scenarios, and UniAD~\cite{hu2023planning} unifies feature abstractions for holistic interaction modeling. While these approaches enhance generalization, they rely on centralized data centers, raising privacy concerns and limiting adaptability. Our work shifts toward FL, enabling privacy-preserving, region-specific adaptation while improving resource efficiency.


\textbf{Parallelism in Datacenter.}~Scalable model training relies on various parallelism strategies in datacenters. ZeRO~\cite{rajbhandari2020zero} reduces memory redundancy for trillion-parameter models, while Megatron~\cite{shoeybi2019megatron} improves intra-layer parallelism. Pipeline parallelism frameworks, including TensorPipe~\cite{huang2019gpipe} and DAPPLE~\cite{fan2021dapple}, optimize layer-based training, while hybrid approaches like nnScaler~\cite{lin2024nnscaler} balance resource utilization. Oobleck~\cite{jang2023oobleck} enhances fault tolerance in distributed training. However, these techniques assume stable, high-speed network connectivity, making them unsuitable for vehicular edge environments characterized by bandwidth variability and dynamic topologies. FLAD adapts parallelism techniques for decentralized, resource-constrained environments while preserving data privacy vian FL.


\textbf{Machine Learning Scheduling.}~Efficient ML scheduling improves resource utilization in constrained environments. Asteroid~\cite{ye2024asteroid} accelerates fault-tolerant pipeline training, while EdgePipe~\cite{yoon2021edgepipe} optimizes execution under fluctuating network conditions. StellaTrain~\cite{lim2024accelerating} and FlexNN~\cite{li2024flexnn} enhance network-aware scheduling, and AutoFed~\cite{zheng2023autofed} addresses multimodal sensor heterogeneity in FL. Alpa~\cite{zheng2022alpa} unifies parallelism strategies across heterogeneous devices. However, existing solutions do not fully account for extreme mobility, network instability, and LLM-specific computational demands in vehicular environments. FLAD bridges this gap by integrating FL, pipeline parallelism, and cloud-edge-vehicle collaboration to enable efficient, privacy-preserving AD model training.",
2511.09980v1,http://arxiv.org/abs/2511.09980v1,2025-11-13 05:28:02+00:00,Modeling Uncertainty Trends for Timely Retrieval in Dynamic RAG,"Dynamic retrieval-augmented generation (RAG) allows large language models (LLMs) to fetch external knowledge on demand, offering greater adaptability than static RAG. A central challenge in this setting lies in determining the optimal timing for retrieval. Existing methods often trigger retrieval based on low token-level confidence, which may lead to delayed intervention after errors have already propagated. We introduce Entropy-Trend Constraint (ETC), a training-free method that determines optimal retrieval timing by modeling the dynamics of token-level uncertainty. Specifically, ETC utilizes first- and second-order differences of the entropy sequence to detect emerging uncertainty trends, enabling earlier and more precise retrieval. Experiments on six QA benchmarks with three LLM backbones demonstrate that ETC consistently outperforms strong baselines while reducing retrieval frequency. ETC is particularly effective in domain-specific scenarios, exhibiting robust generalization capabilities. Ablation studies and qualitative analyses further confirm that trend-aware uncertainty modeling yields more effective retrieval timing. The method is plug-and-play, model-agnostic, and readily integrable into existing decoding pipelines. Implementation code is included in the supplementary materials.","Retrieval augmented generation is an efficient and effective approach to help LLMs obtaining necessary external knowledge\cite{fan2024survey}. Existing works mainly focusing on training-based RAG system~\cite{Yoran2023MakingRL,Luo2024LandmarkEA,fang2024enhancingnr,xu2024unsupervisedir} and training-free RAG system~\cite{Izacard2020LeveragingPR,Wang2023SelfKnowledgeGR,jiang2023active,su2024dragin}. This paper focuses on the later one since it is more lightweight and efficient in practical scenarios.

In the era of LLM, early research mainly explores designing more suitable prompts for high-quality retrieved text~\cite{Shi2023REPLUGRB,Wang2023Query2docQE,Yu2023ImprovingLM},these methods typically conduct retrieval operations only once at the start of the generation process. Lately, people found that not all retrieval operations are beneficial for LLM’s generation, improper or redundant augmented information may cause negative influence on the performance~\cite{Wang2023SelfKnowledgeGR,ni2024whendl,Su2024BRIGHTAR}. Based on the above observation, more research explore to active the retrieval operation when LLM needed, named as dynamic RAG. \citet{borgeaud2022improving,trivedi2022interleaving,ram2023context} proposed to retrieve every $n$ tokens or every sentence, making LLM receive new knowledge during generation process. While \citet{jiang2023active,Wang2024SelfDCWT,Tao2024WhenTT} propose to determine the retrieval timing based on the prediction confidence of the generated token or the internal states. \citet{su2024dragin} further considers the importance of each token to find more reasonable retrieval timing.","Retrieval augmented generation is an efficient and effective approach to help LLMs obtaining necessary external knowledge\cite{fan2024survey}. Existing works mainly focusing on training-based RAG system~\cite{Yoran2023MakingRL,Luo2024LandmarkEA,fang2024enhancingnr,xu2024unsupervisedir} and training-free RAG system~\cite{Izacard2020LeveragingPR,Wang2023SelfKnowledgeGR,jiang2023active,su2024dragin}. This paper focuses on the later one since it is more lightweight and efficient in practical scenarios.

In the era of LLM, early research mainly explores designing more suitable prompts for high-quality retrieved text~\cite{Shi2023REPLUGRB,Wang2023Query2docQE,Yu2023ImprovingLM},these methods typically conduct retrieval operations only once at the start of the generation process. Lately, people found that not all retrieval operations are beneficial for LLM’s generation, improper or redundant augmented information may cause negative influence on the performance~\cite{Wang2023SelfKnowledgeGR,ni2024whendl,Su2024BRIGHTAR}. Based on the above observation, more research explore to active the retrieval operation when LLM needed, named as dynamic RAG. \citet{borgeaud2022improving,trivedi2022interleaving,ram2023context} proposed to retrieve every $n$ tokens or every sentence, making LLM receive new knowledge during generation process. While \citet{jiang2023active,Wang2024SelfDCWT,Tao2024WhenTT} propose to determine the retrieval timing based on the prediction confidence of the generated token or the internal states. \citet{su2024dragin} further considers the importance of each token to find more reasonable retrieval timing.","Retrieval augmented generation is an efficient and effective
approach to help LLMs obtaining necessary external knowl-
edge(Fan et al. 2024). Existing works mainly focusing on
training-based RAG system (Yoran et al. 2023; Luo et al.
2024; Fang et al. 2024; Xu et al. 2024) and training-freeRAG system (Izacard and Grave 2020; Wang et al. 2023;
Jiang et al. 2023; Su et al. 2024b). This paper focuses on the
later one since it is more lightweight and efficient in practi-
cal scenarios.
In the era of LLM, early research mainly explores de-
signing more suitable prompts for high-quality retrieved
text (Shi et al. 2023; Wang, Yang, and Wei 2023; Yu et al.
2023),these methods typically conduct retrieval operations
only once at the start of the generation process. Lately,
people found that not all retrieval operations are beneficial
for LLM’s generation, improper or redundant augmented
information may cause negative influence on the perfor-
mance (Wang et al. 2023; Ni et al. 2024; Su et al. 2024a).
Based on the above observation, more research explore to
active the retrieval operation when LLM needed, named as
dynamic RAG. Borgeaud et al. (2022); Trivedi et al. (2022);
Ram et al. (2023) proposed to retrieve everyntokens or ev-
ery sentence, making LLM receive new knowledge during
generation process. While Jiang et al. (2023); Wang et al.
(2024b); Tao et al. (2024) propose to determine the retrieval
timing based on the prediction confidence of the generated
token or the internal states. Su et al. (2024b) further con-
siders the importance of each token to find more reasonable
retrieval timing."
2511.09891v1,http://arxiv.org/abs/2511.09891v1,2025-11-13 02:54:58+00:00,Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,"Recently, despite the remarkable advancements in object detection, modern detectors still struggle to detect tiny objects in aerial images. One key reason is that tiny objects carry limited features that are inevitably degraded or lost during long-distance network propagation. Another is that smaller objects receive disproportionately greater regression penalties than larger ones during training. To tackle these issues, we propose a Scale-Aware Relay Layer (SARL) and a Scale-Adaptive Loss (SAL) for tiny object detection, both of which are seamlessly compatible with the top-performing frameworks. Specifically, SARL employs a cross-scale spatial-channel attention to progressively enrich the meaningful features of each layer and strengthen the cross-layer feature sharing. SAL reshapes the vanilla IoU-based losses so as to dynamically assign lower weights to larger objects. This loss is able to focus training on tiny objects while reducing the influence on large objects. Extensive experiments are conducted on three benchmarks (\textit{i.e.,} AI-TOD, DOTA-v2.0 and VisDrone2019), and the results demonstrate that the proposed method boosts the generalization ability by 5.5\% Average Precision (AP) when embedded in YOLOv5 (anchor-based) and YOLOx (anchor-free) baselines. Moreover, it also promotes the robust performance with 29.0\% AP on the real-world noisy dataset (\textit{i.e.,} AI-TOD-v2.0).","\label{Related Work}
\subsection{Generic Object Detection}
Object detection methods can be roughly grouped into anchor-based and anchor-free detectors, depending on whether the pre-defined anchor boxes are used. The former utilizes one-stage or two-stage networks to generate region proposals and perform classification. Instead, the latter directly predicts the bounding boxes by keypoint-based or center-based networks.
\subsubsection{Anchor-based Detectors}
One-stage networks, such as SSD\cite{SSD2016}, RetinaNet\cite{RetinaNet2020}, and YOLO family\cite{Yolov32018,YOLOv42020,2022yolov5}, prioritized speed and real-time performance. They accomplished this by classifying and regressing bounding boxes in a single pass through the network, making them ideal for applications where latency is a critical factor. In contrast, two-stage networks, exemplified by FPN\cite{lin2017fpn}, R-CNN family\cite{RCNN2014,FastRCNN2015,FasterRCNN2017,CascadeRCNN2018}, and TridentNet\cite{TridentNet2019}, emphasized higher accuracy by first creating a set of region proposals and then refining these proposals through a second stage of classification and bounding box regression. This two-step process allowed for more precise localization and classification of objects, making two-stage networks effective in scenarios requiring high accuracy.
\subsubsection{Anchor-free Detectors}
Keypoint-based networks inferred bounding boxes from the geometrical relationships of multiple key points. CornerNet\cite{CornerNet2018} detected the top-left and bottom-right corners with embedding vectors, while CenterNet\cite{zhou2019objects} added the center point into the detection. Other representative works included Grid R-CNN\cite{2019grid}, RepPoints\cite{Reppoints2019}, and FoveaBox\cite{2020foveabox}. In contrast, center-based networks mainly identified the center point. FCOS\cite{FCOS2020} computed distances from the center to the box boundaries, and TOOD\cite{feng2021tood} optimized anchor points via a task alignment learning strategy. Meanwhile, FCOS and TOOD have also introduced into the YOLO series, \textit{e.g.}, YOLOx\cite{YOLOX2021} and YOLOv8 \cite{2023yolov8}. Obviously, anchor-free detectors offered a compelling alternative, especially in resource-limited or highly variable detection tasks.
\subsection{Tiny Object Detection}
\subsubsection{Discriminative Feature Learning}
It is well-known that tiny objects have limited appearance information. Researchers have therefore investigated various methods to improve the learning of discriminative features for them. In \cite{2020IJSTAR}, Li \textit{et al.} proposed a cross-layer attention mechanism after downsampling and upsampling procedures to strengthen the expression of spatial and context information. In \cite{2022msodanet,2022Context-Aware}, the authors tried to establish the connection between different level features and combine them by a bidirectional FPN. Wu \textit{et al.}\cite{2022FSANet} introduced a feature-and-spatial aligned network (FSANet) where a novel feature-aware alignment module is designed to align adjacent features of different resolutions, facilitating the extraction of more discriminative features. Like RCSANet\cite{cheng2022tiny}, FSANet also followed an anchor-free paradigm that could reduce the position-sensitive influences on tiny objects. Besides, several early mentioned works \cite{hong2021sspnet,chen2023mdct} on hierarchical feature learning and interaction have shown impressive performance. However, these methods preferred to design suitable network structures to enrich discriminative features, overlooking the coordination with the training strategies.
\subsubsection{Improved Detection Metrics}
As stated in \autoref{intro}, several excellent studies (\textit{e.g.}, NWD\cite{Xu2022_NWD} and RFLA\cite{RFLA2022}) have highlighted the challenges associated with most IoU-based detection metrics in handling position deviation and delineating object boundaries. Thus, there is a trend towards creating specialized metrics for NMS, loss functions, and label assignment processes\cite{DotD2021,KLDetTGRS2024,rs16234485}. In their research, Xu \textit{et al.}\cite{DotD2021} introduced the Dot Distance (DotD), which is defined as the normalized Euclidean distance between the center points of two bounding boxes. Zhou \textit{et al.}\cite{KLDetTGRS2024} proposed the use of the Kullback-Leibler divergence (KLD) as a replacement for the NWD\cite{Xu2022_NWD} to select more positive instances of tiny objects. In \cite{rs16234485}, Su \textit{et al.} firstly modelled bounding boxes as 2D Gaussian distributions similar to \cite{Xu2022_NWD} and \cite{KLDetTGRS2024}. They then designed a new metric called Mixed Minimum Point-Wasserstein (MMPW) to analyze these Gaussian distributions rather than NWD and KLD. However, the preceding methods mainly concentrated on optimizing training strategies, while lacking in-depth exploration of discriminative features.

In summary, both approaches have their respective focuses yet fall short in achieving optimal accuracy and generalization. Therefore, this paper proposes the Scale-Aware Relay Layer (SARL) and Scale-Adaptive Loss (SAL) to merge their merits and enhance model performance. SARL dynamically adjusts information flow within the network based on input feature scale, ensuring effective propagation of relevant features through a cross-layer attention mechanism that considers both semantic and spatial information. Additionally, SAL complements SARL by modulating error penalties based on object scale, enabling the model to pay more attention on smaller objects while maintaining robustness for larger ones.

% The integration of SARL and SAL creates a synergistic effect, enabling the deep learning model to not only improve accuracy but also generalize better to unseen data. The proposed approach not only achieves state-of-the-art performance but also provides insights into the design of more efficient and effective deep learning architectures. By addressing the limitations of existing methods, SARL and SAL pave the way for future research in scalable and adaptable deep learning models.

% fulfill hierarchical feature enhancement and aggregation an imbalance among objects of different scales in existing anchor matching strategies scale-balanced loss is able to alleviate Correlation scalable","\subsection{Generic Object Detection}
Object detection methods can be roughly grouped into anchor-based and anchor-free detectors, depending on whether the pre-defined anchor boxes are used. The former utilizes one-stage or two-stage networks to generate region proposals and perform classification. Instead, the latter directly predicts the bounding boxes by keypoint-based or center-based networks.
\subsubsection{Anchor-based Detectors}
One-stage networks, such as SSD\cite{SSD2016}, RetinaNet\cite{RetinaNet2020}, and YOLO family\cite{Yolov32018,YOLOv42020,2022yolov5}, prioritized speed and real-time performance. They accomplished this by classifying and regressing bounding boxes in a single pass through the network, making them ideal for applications where latency is a critical factor. In contrast, two-stage networks, exemplified by FPN\cite{lin2017fpn}, R-CNN family\cite{RCNN2014,FastRCNN2015,FasterRCNN2017,CascadeRCNN2018}, and TridentNet\cite{TridentNet2019}, emphasized higher accuracy by first creating a set of region proposals and then refining these proposals through a second stage of classification and bounding box regression. This two-step process allowed for more precise localization and classification of objects, making two-stage networks effective in scenarios requiring high accuracy.
\subsubsection{Anchor-free Detectors}
Keypoint-based networks inferred bounding boxes from the geometrical relationships of multiple key points. CornerNet\cite{CornerNet2018} detected the top-left and bottom-right corners with embedding vectors, while CenterNet\cite{zhou2019objects} added the center point into the detection. Other representative works included Grid R-CNN\cite{2019grid}, RepPoints\cite{Reppoints2019}, and FoveaBox\cite{2020foveabox}. In contrast, center-based networks mainly identified the center point. FCOS\cite{FCOS2020} computed distances from the center to the box boundaries, and TOOD\cite{feng2021tood} optimized anchor points via a task alignment learning strategy. Meanwhile, FCOS and TOOD have also introduced into the YOLO series, \textit{e.g.}, YOLOx\cite{YOLOX2021} and YOLOv8 \cite{2023yolov8}. Obviously, anchor-free detectors offered a compelling alternative, especially in resource-limited or highly variable detection tasks.
\subsection{Tiny Object Detection}
\subsubsection{Discriminative Feature Learning}
It is well-known that tiny objects have limited appearance information. Researchers have therefore investigated various methods to improve the learning of discriminative features for them. In \cite{2020IJSTAR}, Li \textit{et al.} proposed a cross-layer attention mechanism after downsampling and upsampling procedures to strengthen the expression of spatial and context information. In \cite{2022msodanet,2022Context-Aware}, the authors tried to establish the connection between different level features and combine them by a bidirectional FPN. Wu \textit{et al.}\cite{2022FSANet} introduced a feature-and-spatial aligned network (FSANet) where a novel feature-aware alignment module is designed to align adjacent features of different resolutions, facilitating the extraction of more discriminative features. Like RCSANet\cite{cheng2022tiny}, FSANet also followed an anchor-free paradigm that could reduce the position-sensitive influences on tiny objects. Besides, several early mentioned works \cite{hong2021sspnet,chen2023mdct} on hierarchical feature learning and interaction have shown impressive performance. However, these methods preferred to design suitable network structures to enrich discriminative features, overlooking the coordination with the training strategies.
\subsubsection{Improved Detection Metrics}
As stated in \autoref{intro}, several excellent studies (\textit{e.g.}, NWD\cite{Xu2022_NWD} and RFLA\cite{RFLA2022}) have highlighted the challenges associated with most IoU-based detection metrics in handling position deviation and delineating object boundaries. Thus, there is a trend towards creating specialized metrics for NMS, loss functions, and label assignment processes\cite{DotD2021,KLDetTGRS2024,rs16234485}. In their research, Xu \textit{et al.}\cite{DotD2021} introduced the Dot Distance (DotD), which is defined as the normalized Euclidean distance between the center points of two bounding boxes. Zhou \textit{et al.}\cite{KLDetTGRS2024} proposed the use of the Kullback-Leibler divergence (KLD) as a replacement for the NWD\cite{Xu2022_NWD} to select more positive instances of tiny objects. In \cite{rs16234485}, Su \textit{et al.} firstly modelled bounding boxes as 2D Gaussian distributions similar to \cite{Xu2022_NWD} and \cite{KLDetTGRS2024}. They then designed a new metric called Mixed Minimum Point-Wasserstein (MMPW) to analyze these Gaussian distributions rather than NWD and KLD. However, the preceding methods mainly concentrated on optimizing training strategies, while lacking in-depth exploration of discriminative features.

In summary, both approaches have their respective focuses yet fall short in achieving optimal accuracy and generalization. Therefore, this paper proposes the Scale-Aware Relay Layer (SARL) and Scale-Adaptive Loss (SAL) to merge their merits and enhance model performance. SARL dynamically adjusts information flow within the network based on input feature scale, ensuring effective propagation of relevant features through a cross-layer attention mechanism that considers both semantic and spatial information. Additionally, SAL complements SARL by modulating error penalties based on object scale, enabling the model to pay more attention on smaller objects while maintaining robustness for larger ones.","and tiny object detection. In Section III, we describe the
architecture and design of the SARL and SAL in detail. In
Section IV, we present our experimental setup and results,
including a comparison with state-of-the-art detection models.
Finally, in Section V, we conclude our findings and discuss
potential future directions for research in this area.
II. RELATEDWORK
A. Generic Object Detection
Object detection methods can be roughly grouped into
anchor-based and anchor-free detectors, depending on whether
the pre-defined anchor boxes are used. The former utilizes one-
stage or two-stage networks to generate region proposals and
perform classification. Instead, the latter directly predicts the
bounding boxes by keypoint-based or center-based networks.
1) Anchor-based Detectors:One-stage networks, such as
SSD [29], RetinaNet [30], and YOLO family [31]–[33], priori-
tized speed and real-time performance. They accomplished this
by classifying and regressing bounding boxes in a single pass
through the network, making them ideal for applications where
latency is a critical factor. In contrast, two-stage networks, ex-
emplified by FPN [24], R-CNN family [7], [8], [34], [35], and
TridentNet [36], emphasized higher accuracy by first creating
a set of region proposals and then refining these proposals
through a second stage of classification and bounding box
regression. This two-step process allowed for more precise
localization and classification of objects, making two-stage
networks effective in scenarios requiring high accuracy.
2) Anchor-free Detectors:Keypoint-based networks in-
ferred bounding boxes from the geometrical relationships of
multiple key points. CornerNet [37] detected the top-left and
bottom-right corners with embedding vectors, while CenterNet
[38] added the center point into the detection. Other repre-
sentative works included Grid R-CNN [39], RepPoints [40],
and FoveaBox [41]. In contrast, center-based networks mainly
identified the center point. FCOS [42] computed distances
from the center to the box boundaries, and TOOD [43]
optimized anchor points via a task alignment learning strategy.
3
Meanwhile, FCOS and TOOD have also introduced into the
YOLO series,e.g., YOLOx [44] and YOLOv8 [45]. Obviously,
anchor-free detectors offered a compelling alternative, espe-
cially in resource-limited or highly variable detection tasks.
B. Tiny Object Detection
1) Discriminative Feature Learning:It is well-known that
tiny objects have limited appearance information. Researchers
have therefore investigated various methods to improve the
learning of discriminative features for them. In [18], Liet
al.proposed a cross-layer attention mechanism after down-
sampling and upsampling procedures to strengthen the ex-
pression of spatial and context information. In [19], [46],
the authors tried to establish the connection between different
level features and combine them by a bidirectional FPN. Wu
et al.[47] introduced a feature-and-spatial aligned network
(FSANet) where a novel feature-aware alignment module is
designed to align adjacent features of different resolutions,
facilitating the extraction of more discriminative features.
Like RCSANet [15], FSANet also followed an anchor-free
paradigm that could reduce the position-sensitive influences on
tiny objects. Besides, several early mentioned works [16], [17]
on hierarchical feature learning and interaction have shown
impressive performance. However, these methods preferred to
design suitable network structures to enrich discriminative fea-
tures, overlooking the coordination with the training strategies.
2) Improved Detection Metrics:As stated in Section I,
several excellent studies (e.g., NWD [21] and RFLA [22])
have highlighted the challenges associated with most IoU-
based detection metrics in handling position deviation and
delineating object boundaries. Thus, there is a trend towards
creating specialized metrics for NMS, loss functions, and label
assignment processes [48]–[50]. In their research, Xuet al.
[48] introduced the Dot Distance (DotD), which is defined as
the normalized Euclidean distance between the center points
of two bounding boxes. Zhouet al.[49] proposed the use
of the Kullback-Leibler divergence (KLD) as a replacement
for the NWD [21] to select more positive instances of tiny
objects. In [50], Suet al.firstly modelled bounding boxes
as 2D Gaussian distributions similar to [21] and [49]. They
then designed a new metric called Mixed Minimum Point-
Wasserstein (MMPW) to analyze these Gaussian distributions
rather than NWD and KLD. However, the preceding methods
mainly concentrated on optimizing training strategies, while
lacking in-depth exploration of discriminative features.
In summary, both approaches have their respective focuses
yet fall short in achieving optimal accuracy and generalization.
Therefore, this paper proposes the Scale-Aware Relay Layer
(SARL) and Scale-Adaptive Loss (SAL) to merge their merits
and enhance model performance. SARL dynamically adjusts
information flow within the network based on input fea-
ture scale, ensuring effective propagation of relevant features
through a cross-layer attention mechanism that considers both
semantic and spatial information. Additionally, SAL comple-
ments SARL by modulating error penalties based on object
scale, enabling the model to pay more attention on smaller
objects while maintaining robustness for larger ones.III. PROPOSEDMETHOD
In this section, we describe our method in detail. First,
Section III-A introduces the overview of the proposed frame-
work. Then, Section III-B presents the Scale-Aware Attention
Network (SA2N). Finally, the Scale-Feedback Loss (SFL) is
introduced in Section III-C.
A. Overall Framework
Before discussing the particular architecture as shown in 2,
we should clarify the main design ideal first. The current object
detection paradigms faces two main challenges in effectively
identifying small objects in aerial images, namely the degra-
dation of discriminative features during feature aggregation
and the significant imbalance in regression loss of objects of
different scales. To this end, we propose a novel approach
that combines mix-attention mechanism with scale-aware op-
timization strategy to improve the quality of discriminative
features and balance the regression loss across various scales.
1 shows a comparison between our framework and prevail-
ing frameworks. From 2, it can be seen thatSA2Nis one
of our main innovations. This work aims to address a critical
bottleneck: the insufficiency or loss of discriminative features
during pyramid feature aggregation processes. Existing feature
extraction paradigms often fail to provide adequate multi-
level discriminative features, which is crucial for accurate tiny
object detection. This problem is exacerbated by the down-
sampling and pooling processes in the network’s backbone,
resulting in blurred appearances of the targets and their low
signal-to-noise ratios. To mitigate these issues, our approach
introduces the Self-Attention Aggregated Network (SA2N).
SA2Nleverages attention mechanisms to adaptively weight
the pyramidal feature hierarchy extracted from the backbone.
By doing so, it enhances the quality of multi-scale features,
facilitating effective bidirectional aggregation and significantly
improving the network’s capability to detect small objects in
complex aerial imagery environments.
Moreover, as shown in 2, our method incorporates the Scale-
Feedback Loss (SFL), which uses scale-feedback signals
to calculate localization errors, thereby promoting balanced
network training. TheSFLmechanism ensures that the re-
gression loss is distributed evenly across various scales, which
is crucial for achieving consistent detection performance.
This balance addresses another critical issue in tiny object
detection, where different scales can lead to inconsistent loss
contributions.
BothSA2NandSFLare designed to be compatible with
a wide range of detection frameworks, including both anchor-
based and anchor-free detectors that utilize IoU-based regres-
sion loss. This compatibility provides flexibility and robust-
ness, making our approach versatile for different application
scenarios.SA2N’s ability to extract precise dense features
is pivotal in improving detection accuracy, particularly for
small objects. The combination of a mix-attention mechanism
with a scale-aware optimization strategy in our novel approach
significantly advances object detection technologies. It ensures
that even in the presence of complex aerial imagery, tiny
objects can be detected with high precision and reliability.
4
𝐿=  𝐿஼௟௦.+𝐿ை௕௝.+𝒇(𝒙)𝐿ୖୣ୥.
C C C C
Backbone
Neck SA2N
Head2×Downsampling
2×Upsampling
Data Flow
Concatenation
C
𝒇(𝒙) 𝐿Reg.Scale-Feedback Loss
InputOutputFeature Interation Module
Feature Enhancement Module
Fig"
2511.09139v1,http://arxiv.org/abs/2511.09139v1,2025-11-12 09:26:24+00:00,MACEval: A Multi-Agent Continual Evaluation Network for Large Models,"Hundreds of benchmarks dedicated to evaluating large models from multiple perspectives have been presented over the past few years. Albeit substantial efforts, most of them remain closed-ended and are prone to overfitting due to the potential data contamination in the ever-growing training corpus of large models, thereby undermining the credibility of the evaluation. Moreover, the increasing scale and scope of current benchmarks with transient metrics, as well as the heavily human-dependent curation procedure, pose significant challenges for timely maintenance and adaptation to gauge the advancing capabilities of large models. In this paper, we introduce MACEval, a \Multi-Agent Continual Evaluation network for dynamic evaluation of large models, and define a new set of metrics to quantify performance longitudinally and sustainably. MACEval adopts an interactive and autonomous evaluation mode that employs role assignment, in-process data generation, and evaluation routing through a cascaded agent network. Extensive experiments on 9 open-ended tasks with 23 participating large models demonstrate that MACEval is (1) human-free and automatic, mitigating laborious result processing with inter-agent judgment guided; (2) efficient and economical, reducing a considerable amount of data and overhead to obtain similar results compared to related benchmarks; and (3) flexible and scalable, migrating or integrating existing benchmarks via customized evaluation topologies. We hope that MACEval can broaden future directions of large model evaluation.","{\bf Benchmarks for Large Models.}
Benchmarks are the foundations of applied large generative model research. Over the past few years, significant efforts have been made to evaluate LLMs and MLLMs from multiple perspectives \citep{chang2024survey,li2024survey}. From textual comprehension \citep{bai2023benchmarking,bang2023multitask} and visual perception \citep{liu2023hidden,fu2024mme,li2024seed,chen2025can} to multistep reasoning \citep{shao2024visual,chen2024m3cot,rajabi2024gsr} and other specialized domains \citep{bai2024m3d, chen2024obi,yue2024mmmu,he2024cmmu,chen2025pictobi}, evaluation results not only serve as practical guidance for end users but also provide developers with actionable insights for model optimization. 
With the rapid development of large models and their real-time Internet data crawling strategy used for training, most benchmarks face the risk of being quickly overfitted. 
Besides, the static, labor-intensive nature and sheer scale of existing benchmarks aggravate the difficulty of maintenance and timely updates \citep{banerjee2024vulnerability}, thereby enhancing their susceptibility to data contamination. 
Therefore, there is an urgent need to establish a long-lasting, dynamic, and robust evaluation framework for large models.



{\bf Data Contamination.}
Recently, data contamination issues have gained widespread attention across evaluation benchmarks for both LLMs and MLLMs \citep{carlini2022quantifying,xu2024benchmark,li2024task}. 
Researchers from OpenAI and the Llama group conducted contamination studies for GPT-4 \citep{achiam2023gpt} and Llama2 \citep{touvron2023llama} on their pre-training data. 
Study \citep{li2024open} reveals significant contamination rates in academic exam-based benchmarks such as MMLU (29.1\%) \citep{hendrycksmeasuring} and C-Eval (45.8\%) \citep{huang2023c}, primarily attributed to the widespread dissemination and circulation of academic test questions.
Deng {\it et al.} \citep{deng2024investigating} proposed a corpora overlap investigation protocol, TS-Guessing, and detected 57\% exact match rate of ChatGPT in predicting masked choices in the MMLU test set. 
Yang {\it et al.} \citep{yang2024dynamic} reported image overlaps of over 84.4\% and 33.2\% between SEEDBench \citep{li2024seed} and pre-training datasets LAION-100M \citep{schuhmann2021laion} and CC3M \citep{sharma2018conceptual}, respectively. 
Such exposure can significantly impact the reliability of the evaluation, leading to inflated results that do not accurately reflect the true capabilities of the large models.
In this paper, we address this problem by reforming the source of evaluation content, introducing AI-generated data and an open-ended task design strategy to enhance benchmark robustness.


{\bf Dynamic Evaluation.} 
One recent promising attempt to mitigate the issue of data contamination in large model evaluations is dynamic evaluation. 
\cite{wang2025benchmark} implemented several perturbations, such as paraphrasing, adding noise, and reversing polarity, to construct evolving instances for testing LLMs against diverse queries and data noise.
DyVal \citep{zhudyval} proposes to dynamically generate evaluation samples under pre-defined constraints, which modulates a graph-based algorithm generation structure and fine-grained control over problem difficulty by adjusting the structural complexity.
Afterwards, \cite{zhu2024dynamic} presented meta-probing agents, which automatically refresh an original evaluation problem following psychometric theory on three basic cognitive abilities, including language understanding, problem solving, and domain knowledge.
In \citep{yang2024dynamic}, the authors designed various bootstrapping strategies ({\it e.g.} image editing and sentence rephrasing) with complexity control for both image and question modification.
However, these studies primarily focus on modifying the sources of evaluation data, without achieving fully dynamic and automatic evaluation processes. Moreover, the corresponding evaluation metrics remain static and transient, failing to adequately reflect the dynamic evolution characteristics of large models.


\begin{figure}
  \centering
  \includegraphics[width=.95\linewidth]{framework.pdf}
  \vspace{-0.5em}
  \caption{An overview of our proposed MACEval, which consists of three primary phases: evaluation capability determination, MAEN construction, and open-ended task selection. The pipeline models the evaluation of large models as a multi-round interview process. Specialized agents like interviewers for direct performance evaluation and third-party supervisors for validity assessment of the entire process with a message propagation mechanism, enabling collaboration between interviewer models and other functional models to efficiently and automatically produce reliable evaluation outputs.}
  \label{framework}
  \vspace{-1em}
\end{figure}","{\bf Benchmarks for Large Models.}
Benchmarks are the foundations of applied large generative model research. Over the past few years, significant efforts have been made to evaluate LLMs and MLLMs from multiple perspectives \citep{chang2024survey,li2024survey}. From textual comprehension \citep{bai2023benchmarking,bang2023multitask} and visual perception \citep{liu2023hidden,fu2024mme,li2024seed,chen2025can} to multistep reasoning \citep{shao2024visual,chen2024m3cot,rajabi2024gsr} and other specialized domains \citep{bai2024m3d, chen2024obi,yue2024mmmu,he2024cmmu,chen2025pictobi}, evaluation results not only serve as practical guidance for end users but also provide developers with actionable insights for model optimization. 
With the rapid development of large models and their real-time Internet data crawling strategy used for training, most benchmarks face the risk of being quickly overfitted. 
Besides, the static, labor-intensive nature and sheer scale of existing benchmarks aggravate the difficulty of maintenance and timely updates \citep{banerjee2024vulnerability}, thereby enhancing their susceptibility to data contamination. 
Therefore, there is an urgent need to establish a long-lasting, dynamic, and robust evaluation framework for large models.



{\bf Data Contamination.}
Recently, data contamination issues have gained widespread attention across evaluation benchmarks for both LLMs and MLLMs \citep{carlini2022quantifying,xu2024benchmark,li2024task}. 
Researchers from OpenAI and the Llama group conducted contamination studies for GPT-4 \citep{achiam2023gpt} and Llama2 \citep{touvron2023llama} on their pre-training data. 
Study \citep{li2024open} reveals significant contamination rates in academic exam-based benchmarks such as MMLU (29.1\
Deng {\it et al.} \citep{deng2024investigating} proposed a corpora overlap investigation protocol, TS-Guessing, and detected 57\
Yang {\it et al.} \citep{yang2024dynamic} reported image overlaps of over 84.4\
Such exposure can significantly impact the reliability of the evaluation, leading to inflated results that do not accurately reflect the true capabilities of the large models.
In this paper, we address this problem by reforming the source of evaluation content, introducing AI-generated data and an open-ended task design strategy to enhance benchmark robustness.


{\bf Dynamic Evaluation.} 
One recent promising attempt to mitigate the issue of data contamination in large model evaluations is dynamic evaluation. 
\cite{wang2025benchmark} implemented several perturbations, such as paraphrasing, adding noise, and reversing polarity, to construct evolving instances for testing LLMs against diverse queries and data noise.
DyVal \citep{zhudyval} proposes to dynamically generate evaluation samples under pre-defined constraints, which modulates a graph-based algorithm generation structure and fine-grained control over problem difficulty by adjusting the structural complexity.
Afterwards, \cite{zhu2024dynamic} presented meta-probing agents, which automatically refresh an original evaluation problem following psychometric theory on three basic cognitive abilities, including language understanding, problem solving, and domain knowledge.
In \citep{yang2024dynamic}, the authors designed various bootstrapping strategies ({\it e.g.} image editing and sentence rephrasing) with complexity control for both image and question modification.
However, these studies primarily focus on modifying the sources of evaluation data, without achieving fully dynamic and automatic evaluation processes. Moreover, the corresponding evaluation metrics remain static and transient, failing to adequately reflect the dynamic evolution characteristics of large models.",
2511.10059v1,http://arxiv.org/abs/2511.10059v1,2025-11-13 07:59:41+00:00,When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?,"Can Multimodal Large Language Models (MLLMs) discern confused objects that are visually present but audio-absent? To study this, we introduce a new benchmark, AV-ConfuseBench, which simulates an ``Audio-Visual Confusion'' scene by modifying the corresponding sound of an object in the video, e.g., mute the sounding object and ask MLLMs Is there a/an muted-object sound''. Experimental results reveal that MLLMs, such as Qwen2.5-Omni and Gemini 2.5, struggle to discriminate non-existent audio due to visually dominated reasoning. Motivated by this observation, we introduce RL-CoMM, a Reinforcement Learning-based Collaborative Multi-MLLM that is built upon the Qwen2.5-Omni foundation. RL-CoMM includes two stages: 1) To alleviate visually dominated ambiguities, we introduce an external model, a Large Audio Language Model (LALM), as the reference model to generate audio-only reasoning. Then, we design a Step-wise Reasoning Reward function that enables MLLMs to self-improve audio-visual reasoning with the audio-only reference. 2) To ensure an accurate answer prediction, we introduce Answer-centered Confidence Optimization to reduce the uncertainty of potential heterogeneous reasoning differences. Extensive experiments on audio-visual question answering and audio-visual hallucination show that RL-CoMM improves the accuracy by 10~30\% over the baseline model with limited training data. Follow: https://github.com/rikeilong/AVConfusion.","\subsubsection{Large Audio-Visual Language Models.} Drawing inspiration from the remarkable ability of Large Language Models (LLMs) \cite{qwen2, qwen2.5, llama2} to generate coherent language, studies have extended LLMs to other multimodal tasks, e.g., Audio-visual Question Answering (AVQA) \cite{pianoavqa}, and human-centric understanding tasks \cite{ye2024iet, ye2024pose}. Primarily, visual LLMs \cite{ye2025cat+,fusionmamba,liu2018attentive,liu2018cross,lin2025reliable} emphasize the design of elegant bridging methods. In this work, we focus on Omni-LLMs \cite{ye2024CAT,qwenomni,videollama2}, it refers to models that understand both video and audio inputs, which possess multisensory properties that mimic human perception. However, we find that these models trained from synchronized audio-visual data are highly susceptible to complementary modalities.

\subsubsection{Audio-visual Defects in MLLMs.} Hallucination \cite{pope,hall,hall2,hall3,hall4,shu2025semantics} refers to the generation of imaginative textual responses by the model that do not correspond to the input signal. Studies \cite{avhbench} have revealed that such phenomenon is caused by favoring the internal knowledge of the LLM and disregarding the input signal. In this paper, we present another shortage of MLLMs that is similar to hallucinations: ``Audio-Visual Confusion''. Specifically, we test the ability of MLLMs to cope with asymmetric audio-visual information.

% Framework of RL-CoMM, where LALMs serve as the reference model and Omni-LLMs serve as the policy model. Given limited data that includes video, audio, and questions, the reference model reasons additional knowledge from the audio to complement the policy model. We use the Answer-centered Confidence Optimization algorithm to reduce uncertainty in the predicted answer. Then, the policy model samples $G$ candidate responses and passes them through our designed reward function to generate group advantages.
\begin{figure*}[]
  \centering
  \includegraphics[scale=0.55]{figures/fig2.pdf}
  \caption{Framework of RL-CoMM, where LALMs serve as the reference model and Omni-LLMs serve as the policy model. Given audio-visual inputs, we first let the LALM generate the reference reasoning for the audio. The policy model is verified by the reviewer (Qwen3 Embedding) to compute group advantages via the Step-wise Reasoning Reward function. Notably, we remove the KL penalty during the policy gradient optimization due to heterogeneous model structure differences. Furthermore, we introduce an Answer-centered Confidence Optimization to reduce uncertainty in the predicted answer of the policy model.
  }
   \label{fig2}
\end{figure*}

\subsubsection{MLLMs with Reinforcement Learning.} Reinforcement Learning (RL) from human feedback \cite{rhlfff,rhlf2}, as an early optimization of language models requires significant human annotation and computational resources. Then, on-policy optimization methods such as DPO \cite{dpo}, and PPO \cite{ppo}, which reward fine-tuned models via computing advantages have achieved excellent outcomes. GRPO \cite{grpo}, as the core optimization algorithm of Deepseek-R1, has advanced the intermediate thinking trajectories in response via KL-penalty and reward model. Recent studies \cite{Liu2025SuperRLRL} have focused on unifying Supervised Fine-Tuning (SFT) \cite{sft} and RL to effectively improve the sensitivity of LLMs to the final output. This paradigm utilizes high quality offline data and online-optimization to interleave the training of models. Unlike prior methods, we improve GRPO by including a heterogeneous reference model, audio-LLMs \cite{qwenaudio}, to complement Omni-LLMs \cite{qwenomni} thinking knowledge. Such a paradigm significantly improves the performance of base-LLMs on AVQA with limited data.","\subsubsection{Large Audio-Visual Language Models.} Drawing inspiration from the remarkable ability of Large Language Models (LLMs) \cite{qwen2, qwen2.5, llama2} to generate coherent language, studies have extended LLMs to other multimodal tasks, e.g., Audio-visual Question Answering (AVQA) \cite{pianoavqa}, and human-centric understanding tasks \cite{ye2024iet, ye2024pose}. Primarily, visual LLMs \cite{ye2025cat+,fusionmamba,liu2018attentive,liu2018cross,lin2025reliable} emphasize the design of elegant bridging methods. In this work, we focus on Omni-LLMs \cite{ye2024CAT,qwenomni,videollama2}, it refers to models that understand both video and audio inputs, which possess multisensory properties that mimic human perception. However, we find that these models trained from synchronized audio-visual data are highly susceptible to complementary modalities.

\subsubsection{Audio-visual Defects in MLLMs.} Hallucination \cite{pope,hall,hall2,hall3,hall4,shu2025semantics} refers to the generation of imaginative textual responses by the model that do not correspond to the input signal. Studies \cite{avhbench} have revealed that such phenomenon is caused by favoring the internal knowledge of the LLM and disregarding the input signal. In this paper, we present another shortage of MLLMs that is similar to hallucinations: ``Audio-Visual Confusion''. Specifically, we test the ability of MLLMs to cope with asymmetric audio-visual information.




\subsubsection{MLLMs with Reinforcement Learning.} Reinforcement Learning (RL) from human feedback \cite{rhlfff,rhlf2}, as an early optimization of language models requires significant human annotation and computational resources. Then, on-policy optimization methods such as DPO \cite{dpo}, and PPO \cite{ppo}, which reward fine-tuned models via computing advantages have achieved excellent outcomes. GRPO \cite{grpo}, as the core optimization algorithm of Deepseek-R1, has advanced the intermediate thinking trajectories in response via KL-penalty and reward model. Recent studies \cite{Liu2025SuperRLRL} have focused on unifying Supervised Fine-Tuning (SFT) \cite{sft} and RL to effectively improve the sensitivity of LLMs to the final output. This paradigm utilizes high quality offline data and online-optimization to interleave the training of models. Unlike prior methods, we improve GRPO by including a heterogeneous reference model, audio-LLMs \cite{qwenaudio}, to complement Omni-LLMs \cite{qwenomni} thinking knowledge. Such a paradigm significantly improves the performance of base-LLMs on AVQA with limited data.","Large Audio-Visual Language Models.Drawing inspi-
ration from the remarkable ability of Large Language Mod-
els (LLMs) (Yang et al. 2024a,b; Touvron et al. 2023) to
generate coherent language, studies have extended LLMs
to other multimodal tasks, e.g., Audio-visual Question An-
swering (A VQA) (Yun et al. 2021), and human-centric un-
derstanding tasks (Nan et al. 2024; Ye and Yu 2024). Pri-
marily, visual LLMs (Ye et al. 2025; Xie et al. 2024; Liu
et al. 2018a,b; Lin et al. 2025) emphasize the design of el-
egant bridging methods. In this work, we focus on Omni-
LLMs (Ye et al. 2024; Xu et al. 2025; Cheng et al. 2024),
it refers to models that understand both video and audio in-
puts, which possess multisensory properties that mimic hu-
man perception. However, we find that these models trained
from synchronized audio-visual data are highly susceptible
to complementary modalities.
Audio-visual Defects in MLLMs.Hallucination (Li et al.
2023;?; Gong et al. 2024; Gunjal, Yin, and Bas 2024; Liu
et al. 2024; Shu et al. 2025) refers to the generation of imagi-
native textual responses by the model that do not correspond
to the input signal. Studies (Sung-Bin et al. 2025) have re-
vealed that such phenomenon is caused by favoring the inter-
nal knowledge of the LLM and disregarding the input signal.
In this paper, we present another shortage of MLLMs that is
similar to hallucinations: “Audio-Visual Confusion”. Specif-
ically, we test the ability of MLLMs to cope with asymmet-
ric audio-visual information.
MLLMs with Reinforcement Learning.Reinforcement
Learning (RL) from human feedback (Ouyang et al. 2022;
MacGlashan et al. 2017), as an early optimization of
language models requires significant human annotation
and computational resources. Then, on-policy optimization
methods such as DPO (Rafailov et al. 2023), and PPO
(Schulman et al. 2017), which reward fine-tuned models via
computing advantages have achieved excellent outcomes.
GRPO (Shao et al. 2024), as the core optimization algorithm
of Deepseek-R1, has advanced the intermediate thinking tra-
jectories in response via KL-penalty and reward model. Re-
cent studies (Liu et al. 2025) have focused on unifying Su-
pervised Fine-Tuning (SFT) (Zhang et al. 2023) and RL
to effectively improve the sensitivity of LLMs to the fi-
nal output. This paradigm utilizes high quality offline data
and online-optimization to interleave the training of mod-
els. Unlike prior methods, we improve GRPO by including
a heterogeneous reference model, audio-LLMs (Chu et al.
2024), to complement Omni-LLMs (Xu et al. 2025) think-
ing knowledge. Such a paradigm significantly improves the
performance of base-LLMs on A VQA with limited data.
A V-ConfuseBench
We believe that analyzing the reliability of existing MLLMs
under audio-visual confusion can develop more robust mod-
els. We provide detailed descriptions and construction of two
different settings in A V-ConfuseBench below:
Audio-muted Confusion.This task is set on masking a par-
ticular sound source in a scene where multiple instruments
are performing, and it assesses whether visual objects af-
fect the audio understanding of MLLMs. All questions are
present in the form of: “This is a video of audio corruption
where some instrument sound is muted.question:Is there
a/an{muted-object}sound?” and the ground truth is “No”.
We mute the sound sources of the collected39videos and
yield a total of73Q&A pairs. The evaluation metrics are
accuracy and model response “Yes” coverage.
Audio-modified Confusion.This task is designed to modify
background sounds to assess whether audio-generated false
information affects the discriminative ability of MLLMs. All
questions are present in the form of “Describe what you see
and what you hear”. We collected5different environmen-
tal sounds, including sounds of wind, bird, rain, electric drill,
and thunder sounds, to tamper with the background sound of
20videos and yield a total of100Q&A pairs. To ensure the
quality of the assessment, all ground truths are manually la-
beled. The evaluation metrics are AI-assisted (DeepSeek-AI
et al. 2025) assessment of the accuracy of generated visual
and audio contents to ground truths."
2511.10334v1,http://arxiv.org/abs/2511.10334v1,2025-11-13 14:06:48+00:00,Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment,"Recent advancements in weakly-supervised video anomaly detection have achieved remarkable performance by applying the multiple instance learning paradigm based on multimodal foundation models such as CLIP to highlight anomalous instances and classify categories. However, their objectives may tend to detect the most salient response segments, while neglecting to mine diverse normal patterns separated from anomalies, and are prone to category confusion due to similar appearance, leading to unsatisfactory fine-grained classification results. Therefore, we propose a novel Disentangled Semantic Alignment Network (DSANet) to explicitly separate abnormal and normal features from coarse-grained and fine-grained aspects, enhancing the distinguishability. Specifically, at the coarse-grained level, we introduce a self-guided normality modeling branch that reconstructs input video features under the guidance of learned normal prototypes, encouraging the model to exploit normality cues inherent in the video, thereby improving the temporal separation of normal patterns and anomalous events. At the fine-grained level, we present a decoupled contrastive semantic alignment mechanism, which first temporally decomposes each video into event-centric and background-centric components using frame-level anomaly scores and then applies visual-language contrastive learning to enhance class-discriminative representations. Comprehensive experiments on two standard benchmarks, namely XD-Violence and UCF-Crime, demonstrate that DSANet outperforms existing state-of-the-art methods.","\subsection{Vision-Language Pre-training}
Vision-Language Pre-training (VLP) has become a dominant paradigm for learning joint representations from large-scale image-text data, enabling remarkable zero-shot transfer capabilities~\cite{ho2025review}. Pioneering works such as CLIP~\cite{radford2021learning} and ALIGN~\cite{jia2021scaling} use dual-encoder architectures trained with contrastive loss to align visual and textual embeddings.
Recent trends aim to improve capability and efficiency.
One line adopts unified encoder-decoder frameworks that jointly handle understanding and generation tasks, exemplified by BLIP~\cite{li2022blip}, which introduced a data bootstrapping method to denoise web captions, and CoCa~\cite{yu2022cocacontrastivecaptionersimagetext}, which combines contrastive and captioning losses. Another focuses on parameter efficiency by leveraging powerful, frozen uni-modal models. Flamingo~\cite{alayrac2022flamingo} bridges frozen vision encoders and large language models with a Perceiver Resampler and trainable gated cross-attention layers, while BLIP-2~\cite{li2023blip} introduced a lightweight Querying Transformer to link frozen components with minimal training cost. VLP models have been widely applied to downstream tasks such as text-video retrieval~\cite{wang2024text,tian2024holistic}, visual question answering~\cite{zou2024language,li2024configure}, and open-vocabulary action recognition~\cite{huang2024frosterfrozenclipstrong,jia2023generatingactionconditionedpromptsopenvocabulary}. In this work, following previous practices~\cite{wu2024vadclip,dev2024reflip}, we construct DSANet based on CLIP~\cite{radford2021learning} for weakly supervised video anomaly detection.

\subsection{Weakly Supervised Video Anomaly Detection}
This task was first formalized by Sultani et al.~\cite{sultani2018real} as a multiple instance learning (MIL) problem, introducing a deep ranking framework that enables the model to assign higher anomaly scores to abnormal segments under weak supervision. Subsequent works have expanded and refined this paradigm~\cite{tian2021weakly,zanella2024harnessing,zhang2025holmes}. To enhance temporal modeling, RTFM~\cite{tian2021weakly} integrates temporal convolution and self-attention to capture multi-scale temporal dependencies. To mitigate contextual bias in MIL, UMIL~\cite{lv2023unbiased} introduces a strategy to learn stable representations across ``confident"" and ``ambiguous"" samples, thereby improving classifier robustness. To address the weakness of the supervision signal, Feng et al.~\cite{feng2021mist} propose a two-stage self-training framework that refines discriminative feature representations by using a multiple instance pseudo-label generator to train a self-guided attention encoder.
With the rise of vision-language pre-training (VLP) models, WS-VAD has been transitioning from traditional statistical pattern recognition towards semantic-aligned reasoning. 
Early approaches adopted VLP models (\eg, CLIP) as visual feature extractors~\cite{joo2023clip}. Recent efforts~\cite{wu2024vadclip,pu2024learning,liu2024injecting} incorporate prompt-based or learnable textual cues into the MIL framework to facilitate anomaly type recognition and enhance event-level discrimination.

% ----------- Method -----------

\begin{figure*}[t]
    \centering
    
    \includegraphics[width=1.0\textwidth]{framework2.pdf}
    \caption{
    Overview of the proposed DSANet. The model consists of three collaborative branches. The Anomaly Detection Branch produces initial  frame-level binary scores using a MIL framework. The Self-Guided Normality Modeling Branch enhances the model's understanding of normal patterns by mining Dynamic Normal Patterns within the video to guide feature reconstruction, improving its ability to distinguish normal from abnormal. The Anomaly Classification Branch aligns video features with textual category embeddings for fine-grained classification, using Lightweight Text Adapters for adaptation and a Decoupled Contrastive Semantic Alignment mechanism to distinguish various anomaly types from normal categories.
    }
    \label{fig:framework}
\end{figure*}","\subsection{Vision-Language Pre-training}
Vision-Language Pre-training (VLP) has become a dominant paradigm for learning joint representations from large-scale image-text data, enabling remarkable zero-shot transfer capabilities~\cite{ho2025review}. Pioneering works such as CLIP~\cite{radford2021learning} and ALIGN~\cite{jia2021scaling} use dual-encoder architectures trained with contrastive loss to align visual and textual embeddings.
Recent trends aim to improve capability and efficiency.
One line adopts unified encoder-decoder frameworks that jointly handle understanding and generation tasks, exemplified by BLIP~\cite{li2022blip}, which introduced a data bootstrapping method to denoise web captions, and CoCa~\cite{yu2022cocacontrastivecaptionersimagetext}, which combines contrastive and captioning losses. Another focuses on parameter efficiency by leveraging powerful, frozen uni-modal models. Flamingo~\cite{alayrac2022flamingo} bridges frozen vision encoders and large language models with a Perceiver Resampler and trainable gated cross-attention layers, while BLIP-2~\cite{li2023blip} introduced a lightweight Querying Transformer to link frozen components with minimal training cost. VLP models have been widely applied to downstream tasks such as text-video retrieval~\cite{wang2024text,tian2024holistic}, visual question answering~\cite{zou2024language,li2024configure}, and open-vocabulary action recognition~\cite{huang2024frosterfrozenclipstrong,jia2023generatingactionconditionedpromptsopenvocabulary}. In this work, following previous practices~\cite{wu2024vadclip,dev2024reflip}, we construct DSANet based on CLIP~\cite{radford2021learning} for weakly supervised video anomaly detection.

\subsection{Weakly Supervised Video Anomaly Detection}
This task was first formalized by Sultani et al.~\cite{sultani2018real} as a multiple instance learning (MIL) problem, introducing a deep ranking framework that enables the model to assign higher anomaly scores to abnormal segments under weak supervision. Subsequent works have expanded and refined this paradigm~\cite{tian2021weakly,zanella2024harnessing,zhang2025holmes}. To enhance temporal modeling, RTFM~\cite{tian2021weakly} integrates temporal convolution and self-attention to capture multi-scale temporal dependencies. To mitigate contextual bias in MIL, UMIL~\cite{lv2023unbiased} introduces a strategy to learn stable representations across ``confident"" and ``ambiguous"" samples, thereby improving classifier robustness. To address the weakness of the supervision signal, Feng et al.~\cite{feng2021mist} propose a two-stage self-training framework that refines discriminative feature representations by using a multiple instance pseudo-label generator to train a self-guided attention encoder.
With the rise of vision-language pre-training (VLP) models, WS-VAD has been transitioning from traditional statistical pattern recognition towards semantic-aligned reasoning. 
Early approaches adopted VLP models (\eg, CLIP) as visual feature extractors~\cite{joo2023clip}. Recent efforts~\cite{wu2024vadclip,pu2024learning,liu2024injecting} incorporate prompt-based or learnable textual cues into the MIL framework to facilitate anomaly type recognition and enhance event-level discrimination.","2.1 Vision-Language Pre-training
Vision-Language Pre-training (VLP) has become a domi-
nant paradigm for learning joint representations from large-
scale image-text data, enabling remarkable zero-shot trans-
fer capabilities (Ho et al. 2025). Pioneering works such as
CLIP (Radford et al. 2021) and ALIGN (Jia et al. 2021)
use dual-encoder architectures trained with contrastive loss
to align visual and textual embeddings. Recent trends aim
to improve capability and efficiency. One line adopts uni-
fied encoder-decoder frameworks that jointly handle under-
standing and generation tasks, exemplified by BLIP (Li et al.
2022), which introduced a data bootstrapping method to
denoise web captions, and CoCa (Yu et al. 2022), which
combines contrastive and captioning losses. Another focuses
on parameter efficiency by leveraging powerful, frozen uni-
modal models. Flamingo (Alayrac et al. 2022) bridges
frozen vision encoders and large language models with a
Perceiver Resampler and trainable gated cross-attention lay-
ers, while BLIP-2 (Li et al. 2023) introduced a lightweight
Querying Transformer to link frozen components with min-
imal training cost. VLP models have been widely applied to
downstream tasks such as text-video retrieval (Wang et al.
2024; Tian et al. 2024), visual question answering (Zou et al.
2024; Li et al. 2024), and open-vocabulary action recogni-
tion (Huang et al. 2024; Jia et al. 2023). In this work, fol-
lowing previous practices (Wu et al. 2024b; Dev, Hazari, and
Das 2024), we construct DSANet based on CLIP (Radford
et al. 2021) for weakly supervised video anomaly detection.
2.2 Weakly Supervised Video Anomaly Detection
This task was first formalized by Sultani et al. (Sultani,
Chen, and Shah 2018) as a multiple instance learning (MIL)
problem, introducing a deep ranking framework that enables
the model to assign higher anomaly scores to abnormal seg-
ments under weak supervision. Subsequent works have ex-
panded and refined this paradigm (Tian et al. 2021; Zanella
et al. 2024; Zhang et al. 2025). To enhance temporal mod-
eling, RTFM (Tian et al. 2021) integrates temporal convo-
lution and self-attention to capture multi-scale temporal de-
pendencies. To mitigate contextual bias in MIL, UMIL (Lv
et al. 2023) introduces a strategy to learn stable representa-
tions across “confident” and “ambiguous” samples, thereby
improving classifier robustness. To address the weakness
of the supervision signal, Feng et al. (Feng, Hong, and
Zheng 2021) propose a two-stage self-training framework
that refines discriminative feature representations by using
a multiple instance pseudo-label generator to train a self-
guided attention encoder. With the rise of vision-language
pre-training (VLP) models, WS-V AD has been transition-
ing from traditional statistical pattern recognition towards
semantic-aligned reasoning. Early approaches adopted VLP
models (e.g., CLIP) as visual feature extractors (Joo et al.
2023). Recent efforts (Wu et al. 2024b; Pu et al. 2024; Liu,
Lam, and Bao 2024) incorporate prompt-based or learnable
textual cues into the MIL framework to facilitate anomaly
type recognition and enhance event-level discrimination.
MIL Align𝑉𝑉
…CLIP
Image
Encoder
……
video features𝑵𝑵
Binary
Classifier𝓛𝓛𝒅𝒅𝒅𝒅𝒅𝒅Reconstruction
error score 𝑺𝑺𝒓𝒓𝒅𝒅𝒓𝒓
…
𝑴𝑴×𝑫𝑫𝑲𝑲×𝑫𝑫learning tokens
“normal features ”
Local
Transformer
Global GCNCross
Attention𝑄𝑄
𝐾𝐾
𝑉𝑉
Feed Forward𝑫𝑫𝑵𝑵𝑫𝑫𝑫𝑫
𝓜𝓜Select M frames with the lowest score
BottomM𝑫𝑫𝑵𝑵𝑫𝑫𝑫𝑫 MLP
DNPS -Guided
Attention
Feed Forward𝑄𝑄𝐾𝐾
…Reconstructed
features
𝑪𝑪𝒐𝒐𝒐𝒐𝑫𝑫𝒐𝒐𝑫𝑫𝒅𝒅𝒅𝒅𝒐𝒐𝒓𝒓𝒐𝒐
𝑳𝑳𝒐𝒐𝑫𝑫𝑫𝑫Self-Guided Normality Modeling Branch
Anomaly Classification Branch
Normal
Abuse
Vandalism……𝑻𝑻Anomaly Detection
anomaly score 𝑺𝑺𝒅𝒅𝒅𝒅𝒅𝒅…
text-enh features
…
text features𝓓𝓓 𝓛𝓛𝒂𝒂𝒂𝒂𝒐𝒐𝒂𝒂𝒐𝒐
Anomaly Classification
alignment map 𝑺𝑺𝒂𝒂𝒂𝒂𝒐𝒐𝒂𝒂𝒐𝒐video -level
category GT
Frozen
- --Dynamic
Normal Patterns
(DNPs)
Anomaly Detection Branch𝑪𝑪𝒐𝒐𝑪𝑪𝑪𝑪𝒂𝒂𝒓𝒓𝒅𝒅
𝑳𝑳𝒐𝒐𝑫𝑫𝑫𝑫
targetoriginal
Trainable
--Candidate
Normal Featuresvision- text
enhancementvideo -level
binary GTMIL Top -K
Text Encoder
layer
Adapter
𝝎𝝎
Text Encoder
layer
Adapter
Text Encoder
layer
Project
Decouple Contrastive
Semantic Alignment
Shallow Layers of CLIP Text Encoder Deep Layers𝓓𝓓-- compute
cosine similarityFigure 2: Overview of the proposed DSANet. The model consists of three collaborative branches. The Anomaly Detection
Branch produces initial frame-level binary scores using a MIL framework. The Self-Guided Normality Modeling Branch en-
hances the model’s understanding of normal patterns by mining Dynamic Normal Patterns within the video to guide feature
reconstruction, improving its ability to distinguish normal from abnormal. The Anomaly Classification Branch aligns video
features with textual category embeddings for fine-grained classification, using Lightweight Text Adapters for adaptation and a
Decoupled Contrastive Semantic Alignment mechanism to distinguish various anomaly types from normal categories."
2511.08181v1,http://arxiv.org/abs/2511.08181v1,2025-11-11 12:44:31+00:00,MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,"Recommender systems (RS) are currently being studied to mitigate limitations during cold-start conditions by leveraging modality information or introducing Agent concepts based on the exceptional reasoning capabilities of Large Language Models (LLMs). Meanwhile, food and beverage recommender systems have traditionally used knowledge graph and ontology concepts due to the domain's unique data attributes and relationship characteristics. On this background, we propose MARC, a multimodal and multi-task cocktail recommender system based on Agentic Retrieval-Augmented Generation (RAG) utilizing graph database under cold-start conditions. The proposed system generates high-quality, contextually appropriate answers through two core processes: a task recognition router and a reflection process. The graph database was constructed by processing cocktail data from Kaggle, and its effectiveness was evaluated using 200 manually crafted questions. The evaluation used both LLM-as-a-judge and human evaluation to demonstrate that answers generated via the graph database outperformed those from a simple vector database in terms of quality. The code is available at https://github.com/diddbwls/cocktail_rec_agentrag","\subsection{Beverage and Food Recommendation}
Recommender systems based on food or beverages are not mainstream research compared to the movie domain, but they are steadily being studied, with research utilizing ontologies or knowledge graphs existing. Ontologies and knowledge graphs are representations that systematize information and relational data into a graph structure based on a knowledge system and schema of concept definitions and relationships through domain-specific vocabulary. They provide the ability to reflect constraints necessary for recommendation and offer explainability \cite{Lim:2004aa, Chen:2012aa, Chen:2019aa}. 

Chen et al. proposed a recommender system combining common-sense reasoning to detect emotions from inferences and colors based on a cocktail ontology knowledge base  \cite{Chen:2006aa}, while Ahlam et al. built an ontology based on the Canada Food Allergies and Intolerances Databases and integrated useful features such as food item selection, descriptions, and recommendations \cite{Al:2019aa}. Haussmann et al. constructed a knowledge graph based on large-scale public food data and performed food recommendations \cite{Haussmann:2019aa}. Oliveira et al. constructed a wine ontology and confirmed that ontologies in recommender systems influenced performance improvement  \cite{Oliveira:2021aa}. Showafah et al. constructed an ontology containing knowledge about food and nutritional components along with nutritional requirements, and combined it with the TOPSIS method to provide optimal recommendations regarding nutritional balance and user preferences \cite{Showafah:2021aa}. Chen et al. proposed a collaborative recipe knowledge graph, combining health suitability scoring between recipes and user preferences with an attention-based graph convolutional neural network to present a health-aware personalized recommendation model \cite{Chen:2023aa}. Gawrysiak et al. proposed WineGraph, which pairs food and wine using food and wine review data and augments FlavorGraph data \cite{Gawrysiak:2024aa}.

\subsection{Graph Retrieval Augmented Generation}
Graph RAG is a method that addresses the limitations of vector-based RAG. By leveraging the semantic and structural relationships between nodes in a knowledge graph structure, it simultaneously improves performance and interpretability in multi-document reasoning and relationship-based question answering through search and inference \cite{Han:2024aa}. Han et al. proposed GraphRAG, which constructs a graph from entities and relationships extracted from documents and integrates it as input for LLMs through subgraph search \cite{Han:2024aa}. Hu et al. proposed GRAG, performing efficient subgraph exploration using a divide-and-conquer approach \cite{Hu2024GRAGGR}, while Shen et al. proposed GeAR, combining an agent framework with graph expansion \cite{Shen:2024aa}. Xiang et al. demonstrated through GraphRAG-Bench that graph structures are particularly effective for questions where relational information is crucial \cite{Xiang:2025aa}. Agrawal et al. introduced query-based graph neural networks to enhance query-aware retrieval performance \cite{Agrawal:2025aa}. Liang et al. proposed a graph-based RAG specialized for the geospatial analysis domain, demonstrating domain-tailored applicability \cite{Liang:2025aa}. In summary, Graph RAG has complemented existing RAG in terms of accuracy, efficiency, and explainability, establishing itself as a core technology directly relevant to Agentic RAG design in this research.

\subsection{LLM based Recommendation}
In the early stages of recommender systems, Collaborative Filtering (CF) was applied to newsgroups to make recommendations based on user rating similarity \cite{Resnick:1994aa}, and content-based methodologies \cite{Sarwar:2001aa}, Matrix Factorization (MF) \cite{Koren:2009aa}, and deep learning-based methodologies \cite{Covington:2016aa} were introduced, leading to significant advancements. Building upon this prior research, this discussion will focus exclusively on the rapidly evolving recommender systems based on LLMs.
Wang et al. proposed a zero-shot Next-Item Recommendation (NIR) prompting strategy integrating a three-step prompting approach to recommend movie ranking lists  \cite{Wang:2023aa}. Lyu et al. proposed LLM-REC, integrating four unique prompting strategies, to enhance personalized text-based recommendations through text augmentation  \cite{Lyu:2023aa}. Tian et al. improved recommendation accuracy and relevance by processing multimodal information through LLMs and projecting it into an integrated latent space \cite{Tian:2024aa}. Finally, Wang et al., whose work is most similar to ours, aimed to enhance answer generation and recommendation quality by supplementing LLMs' hallucination issues with KG RAG \cite{Wang:2025aa}. Our research distinguishes itself by introducing an attempt to build an Agentic RAG framework that autonomously recognizes user queries and improves answer quality.
	
\subsection{LLM as Recommendation Agent}
Research utilizing LLM as agent has shifted beyond the existing static prompt-response paradigm, transforming it into a dynamic decision-making framework capable of managing complex systems through diverse sub-component configurations \cite{Patil:2024aa}. A typical LLM-based agent structure consists of four modules: Profile, Memory, Planning, and Action \cite{Wang:2025aa}. Research introducing agents based on this structural form is also underway in recommender systems, which can be categorized into recommender-oriented, interaction-oriented, and simulation-oriented approaches \cite{Peng:2025aa}.

The recommender-oriented approaches focus on developing intelligent recommender systems with planning capabilities, making direct recommendations based on users' past behaviors \cite{Wang:2023ab, Shi:2024aa, Zhao:2024aa}. Wang et al. proposed a self-inspiring algorithm to enhance planning capabilities, developing an agent capable of zero-shot personalized recommendations \cite{Wang:2023ab}, while Shi et al. proposed a framework for learning planning capabilities at the macro level through reflector reflection and at the micro level through personalized recommendations via interactions between agent and critic \cite{Shi:2024aa}. Zhao et al. proposed a framework that sets LLMs as proxy users and recommends through tool learning, enabling the generation of recommendation lists aligned with preferences \cite{Zhao:2024aa}. 

Interaction-oriented approaches enable tracking user preferences and explaining recommendation rationale through conversation \cite{Shu:2024aa, Zeng:2024aa, Huang:2025aa}. Zeng et al. developed an interactive agent using LLMs and Answer Set Programming (ASP) that can request missing information \cite{Zeng:2024aa}, while Shu et al. proposed a human-centered recommendation framework based on a learn-execute-critique loop and a reflection mechanism to ensure alignment with user personality \cite{Shu:2024aa}. Huang et al. enhanced the agent component and integrated traditional recommender systems to build a multi-purpose interactive system \cite{Huang:2025aa}.

The simulation-oriented approaches aim to simulate user behavior and item characteristics within recommender systems \cite{Zhang:2024aa, Zhang:2024ab, Guo:2024aa}. Zhang et al. proposed an LLM-based generative agent simulator equipped with user profiles, memory, and behavior modules \cite{Zhang:2024aa}. Zhang et al. modeled user-item interactions and their relationships in a recommender system by treating users and items as agents and simulating their interactions through collaborative filtering \cite{Zhang:2024ab}. Guo et al. proposed a framework that positions users and items within a knowledge graph in simulated recommendation scenarios and integrates into the simulation as natural language descriptions \cite{Guo:2024aa}.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{recsys_com.png}
  \caption{Overall workflow of the MARC}
  \label{MARC}
\end{figure}","\subsection{Beverage and Food Recommendation}
Recommender systems based on food or beverages are not mainstream research compared to the movie domain, but they are steadily being studied, with research utilizing ontologies or knowledge graphs existing. Ontologies and knowledge graphs are representations that systematize information and relational data into a graph structure based on a knowledge system and schema of concept definitions and relationships through domain-specific vocabulary. They provide the ability to reflect constraints necessary for recommendation and offer explainability \cite{Lim:2004aa, Chen:2012aa, Chen:2019aa}. 

Chen et al. proposed a recommender system combining common-sense reasoning to detect emotions from inferences and colors based on a cocktail ontology knowledge base  \cite{Chen:2006aa}, while Ahlam et al. built an ontology based on the Canada Food Allergies and Intolerances Databases and integrated useful features such as food item selection, descriptions, and recommendations \cite{Al:2019aa}. Haussmann et al. constructed a knowledge graph based on large-scale public food data and performed food recommendations \cite{Haussmann:2019aa}. Oliveira et al. constructed a wine ontology and confirmed that ontologies in recommender systems influenced performance improvement  \cite{Oliveira:2021aa}. Showafah et al. constructed an ontology containing knowledge about food and nutritional components along with nutritional requirements, and combined it with the TOPSIS method to provide optimal recommendations regarding nutritional balance and user preferences \cite{Showafah:2021aa}. Chen et al. proposed a collaborative recipe knowledge graph, combining health suitability scoring between recipes and user preferences with an attention-based graph convolutional neural network to present a health-aware personalized recommendation model \cite{Chen:2023aa}. Gawrysiak et al. proposed WineGraph, which pairs food and wine using food and wine review data and augments FlavorGraph data \cite{Gawrysiak:2024aa}.

\subsection{Graph Retrieval Augmented Generation}
Graph RAG is a method that addresses the limitations of vector-based RAG. By leveraging the semantic and structural relationships between nodes in a knowledge graph structure, it simultaneously improves performance and interpretability in multi-document reasoning and relationship-based question answering through search and inference \cite{Han:2024aa}. Han et al. proposed GraphRAG, which constructs a graph from entities and relationships extracted from documents and integrates it as input for LLMs through subgraph search \cite{Han:2024aa}. Hu et al. proposed GRAG, performing efficient subgraph exploration using a divide-and-conquer approach \cite{Hu2024GRAGGR}, while Shen et al. proposed GeAR, combining an agent framework with graph expansion \cite{Shen:2024aa}. Xiang et al. demonstrated through GraphRAG-Bench that graph structures are particularly effective for questions where relational information is crucial \cite{Xiang:2025aa}. Agrawal et al. introduced query-based graph neural networks to enhance query-aware retrieval performance \cite{Agrawal:2025aa}. Liang et al. proposed a graph-based RAG specialized for the geospatial analysis domain, demonstrating domain-tailored applicability \cite{Liang:2025aa}. In summary, Graph RAG has complemented existing RAG in terms of accuracy, efficiency, and explainability, establishing itself as a core technology directly relevant to Agentic RAG design in this research.

\subsection{LLM based Recommendation}
In the early stages of recommender systems, Collaborative Filtering (CF) was applied to newsgroups to make recommendations based on user rating similarity \cite{Resnick:1994aa}, and content-based methodologies \cite{Sarwar:2001aa}, Matrix Factorization (MF) \cite{Koren:2009aa}, and deep learning-based methodologies \cite{Covington:2016aa} were introduced, leading to significant advancements. Building upon this prior research, this discussion will focus exclusively on the rapidly evolving recommender systems based on LLMs.
Wang et al. proposed a zero-shot Next-Item Recommendation (NIR) prompting strategy integrating a three-step prompting approach to recommend movie ranking lists  \cite{Wang:2023aa}. Lyu et al. proposed LLM-REC, integrating four unique prompting strategies, to enhance personalized text-based recommendations through text augmentation  \cite{Lyu:2023aa}. Tian et al. improved recommendation accuracy and relevance by processing multimodal information through LLMs and projecting it into an integrated latent space \cite{Tian:2024aa}. Finally, Wang et al., whose work is most similar to ours, aimed to enhance answer generation and recommendation quality by supplementing LLMs' hallucination issues with KG RAG \cite{Wang:2025aa}. Our research distinguishes itself by introducing an attempt to build an Agentic RAG framework that autonomously recognizes user queries and improves answer quality.
	
\subsection{LLM as Recommendation Agent}
Research utilizing LLM as agent has shifted beyond the existing static prompt-response paradigm, transforming it into a dynamic decision-making framework capable of managing complex systems through diverse sub-component configurations \cite{Patil:2024aa}. A typical LLM-based agent structure consists of four modules: Profile, Memory, Planning, and Action \cite{Wang:2025aa}. Research introducing agents based on this structural form is also underway in recommender systems, which can be categorized into recommender-oriented, interaction-oriented, and simulation-oriented approaches \cite{Peng:2025aa}.

The recommender-oriented approaches focus on developing intelligent recommender systems with planning capabilities, making direct recommendations based on users' past behaviors \cite{Wang:2023ab, Shi:2024aa, Zhao:2024aa}. Wang et al. proposed a self-inspiring algorithm to enhance planning capabilities, developing an agent capable of zero-shot personalized recommendations \cite{Wang:2023ab}, while Shi et al. proposed a framework for learning planning capabilities at the macro level through reflector reflection and at the micro level through personalized recommendations via interactions between agent and critic \cite{Shi:2024aa}. Zhao et al. proposed a framework that sets LLMs as proxy users and recommends through tool learning, enabling the generation of recommendation lists aligned with preferences \cite{Zhao:2024aa}. 

Interaction-oriented approaches enable tracking user preferences and explaining recommendation rationale through conversation \cite{Shu:2024aa, Zeng:2024aa, Huang:2025aa}. Zeng et al. developed an interactive agent using LLMs and Answer Set Programming (ASP) that can request missing information \cite{Zeng:2024aa}, while Shu et al. proposed a human-centered recommendation framework based on a learn-execute-critique loop and a reflection mechanism to ensure alignment with user personality \cite{Shu:2024aa}. Huang et al. enhanced the agent component and integrated traditional recommender systems to build a multi-purpose interactive system \cite{Huang:2025aa}.

The simulation-oriented approaches aim to simulate user behavior and item characteristics within recommender systems \cite{Zhang:2024aa, Zhang:2024ab, Guo:2024aa}. Zhang et al. proposed an LLM-based generative agent simulator equipped with user profiles, memory, and behavior modules \cite{Zhang:2024aa}. Zhang et al. modeled user-item interactions and their relationships in a recommender system by treating users and items as agents and simulating their interactions through collaborative filtering \cite{Zhang:2024ab}. Guo et al. proposed a framework that positions users and items within a knowledge graph in simulated recommendation scenarios and integrates into the simulation as natural language descriptions \cite{Guo:2024aa}.",
2511.09870v1,http://arxiv.org/abs/2511.09870v1,2025-11-13 02:04:03+00:00,SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,"Recently segment anything model (SAM) has attracted widespread concerns, and it is often treated as a vision foundation model for universal segmentation. Some researchers have attempted to directly apply the foundation model to the RGB-D video salient object detection (RGB-D VSOD) task, which often encounters three challenges, including the dependence on manual prompts, the high memory consumption of sequential adapters, and the computational burden of memory attention. To address the limitations, we propose a novel method, namely Segment Anything Model with Depth-guided Adaptive Queries (SAM-DAQ), which adapts SAM2 to pop-out salient objects from videos by seamlessly integrating depth and temporal cues within a unified framework. Firstly, we deploy a parallel adapter-based multi-modal image encoder (PAMIE), which incorporates several depth-guided parallel adapters (DPAs) in a skip-connection way. Remarkably, we fine-tune the frozen SAM encoder under prompt-free conditions, where the DPA utilizes depth cues to facilitate the fusion of multi-modal features. Secondly, we deploy a query-driven temporal memory (QTM) module, which unifies the memory bank and prompt embeddings into a learnable pipeline. Concretely, by leveraging both frame-level queries and video-level queries simultaneously, the QTM module can not only selectively extract temporal consistency features but also iteratively update the temporal representations of the queries. Extensive experiments are conducted on three RGB-D VSOD datasets, and the results show that the proposed SAM-DAQ consistently outperforms state-of-the-art methods in terms of all evaluation metrics.","% 参考Salient Object Detection in RGB-D Videos，分为early, late and middle fusion三个阶段
\subsection{RGB-D Salient Object Detection}
Depth information provides spatial cues resistant to contextual distractions and is widely as regarded a critical complement for RGB information. Existing RGB-D SOD methods typically follow three fusion paradigms~\cite{mou2024salient}, namely early fusion, middle fusion and late fusion.
% early fusion，将输入按通道拼接
The early fusion strategy~\cite{qu2017rgbd} treats the depth map as an additional channel to the RGB image, and directly concatenates the two modal images to form a 4D input.
% late fusion，两个模态单独送入网络推理，融合输出的结果（再差）
The late fusion strategy~\cite{han2017cnns} processes the two modalities independently, which generates the final mask by fusing their coarse predictions. However, the interaction between the modalities occurs at the output only, which limits the fusion of the two modal features. 
% middle fusion
The middle fusion strategy~\cite{cong2023point, bao2024quality} is widely employed to capture and exploit multi-modal correlations. 
% CNN->Transformer
For instance, Cong \emph{et al.}~\cite{cong2023point} combine the multi-modal CNN features and then utilize the fused features to refine the transformer decoder. 
However, prior studies have revealed inherent limitations in the depth modality, including a lack of informative content~\cite{hao2024primkd} and unstable quality~\cite{bao2024quality}.


% 分为传统方法（时间窗、光流法）, memory bank和query-based（使用learnable queries作为存储结构的） 
\subsection{Video Salient Object Detection}
Video SOD (VSOD) extends SOD by leveraging temporal information across frames to ensure spatial-temporal consistency.
% traditional
Early works~\cite{wang2017video, li2018flow, ji2020casnet} utilize optical flow and attention mechanisms to capture motion cues, but their accuracy heavily depends on flow quality and motion scale~\cite{singh2024dsfnet}.
% memory
To better model long-term dependencies, memory-based frameworks~\cite{oh2019video, cheng2022xmem} encode the previous frame's features and their predictions, and store the generated representations in a memory bank.
However, interacting with a large memory bank can introduce substantial computational consumption.
% query-based
To tackle the issue, query-based methods~\cite{fang2024learning, wang2023look} utilize the learnable query to focus on relevant features selectively, achieving both high accuracy and efficiency.

Recent RGB-D VSOD methods aim to integrate depth and temporal cues simultaneously.
For instance, Li~\emph{et al.}~\cite{li2023dvsod} store previously fused RGB-D features into memory, extending memory networks to RGB-D settings.
Mou~\emph{et al.}~\cite{mou2024salient} fuse flow and multi-modal features via holistic multi-modal attentive paths (HMAPs).
Lin \emph{et al.}~\cite{lin2024vidsod} treat RGB, depth, and optical flow equally, and deploy intermediate supervision on their respective encoders. % to promote the detection performance.
In addition, Suolang \emph{et al.}~\cite{suolang2025lightweight} propose a lightweight cross-shift module that efficiently fuses auxiliary depth and temporal cues.


\subsection{Segment Anything Model}

The Segment Anything Model (SAM)~\cite{kirillov2023segment, ravi2024sam} is a vision foundation model trained on large-scale data for universal image segmentation. While it generalizes well, its reliance on manual prompts (e.g., points, boxes), making it impractical for the video salient object detection task.

% 利用传统网络的输出结果作为coarse mask，生成prompt送入SAM 和 只取encoder
To eliminate the need for manual prompts, several works~\cite{zhang2024uv, ayzenberg2024protosam, xie2025rfmedsam} attempt to generate pseudo-prompts from coarse masks, while others~\cite{yang2024sam, xiong2024sam2, xu2025dgsunet} only use SAM's encoder for feature extraction.
% 上述方法并未改进SAM本身范式
Generally, the above methods tailor SAM for specific tasks without modifying its intrinsic architecture.
Recent methods have explored parameter-efficient fine-tuning (PEFT) strategies.
% Wang adapter
%For instance,
Wang \emph{et al.}~\cite{wang2024adapting} insert adapter~\cite{houlsby2019parameter} between encoder blocks, % to fine-tune the SAM encoder.
% Zhong，LoRA（插入至transformer模块中）
while Zhong \emph{et al.}~\cite{zhong2024convolution} integrate LoRA~\cite{hu2022lora} into transformer layers.
% 梯度传到问题
However, these sequential adapter structures tend to incur high memory consumption, as backward gradients must propagate through the entire encoder~\cite{diao2024unipt}.
% %parallel
% Different from the aforementioned efforts, we propose a parallel adapter-based multi-modal encoder (PAMIE), which employs skip-connected depth-guided adapters to reduce training memory and improve RGB-D fusion. 

To extend SAM to the video segmentation task, Yue \emph{et al.}~\cite{yue2024sam} propose a flow reconstruction technique to guide SAM in object discovery. Deng \emph{et al.}~\cite{deng2024memsam} present three distinct types of memory banks to mitigate the adverse effects of speckle noise and motion artifacts during memory prompting. To the best of our knowledge, no previous work has attempted to encode memory queries within the video SAM-based framework.
% % Ours
% Different from the aforementioned efforts, we introduce query-based temporal memory (QTM) module that unifies prompt representation and temporal modeling.
% Specifically, to fully leverage the power of learnable queries, we integrate the memory bank and prompt embeddings into a unified learnable pipeline via a query-driven temporal memory (QTM) module. Specifically, we design frame-level queries to extract salient spatial features and video-level queries to capture inter-frame dependencies through iterative updates.","\subsection{RGB-D Salient Object Detection}
Depth information provides spatial cues resistant to contextual distractions and is widely as regarded a critical complement for RGB information. Existing RGB-D SOD methods typically follow three fusion paradigms~\cite{mou2024salient}, namely early fusion, middle fusion and late fusion.

The early fusion strategy~\cite{qu2017rgbd} treats the depth map as an additional channel to the RGB image, and directly concatenates the two modal images to form a 4D input.

The late fusion strategy~\cite{han2017cnns} processes the two modalities independently, which generates the final mask by fusing their coarse predictions. However, the interaction between the modalities occurs at the output only, which limits the fusion of the two modal features. 

The middle fusion strategy~\cite{cong2023point, bao2024quality} is widely employed to capture and exploit multi-modal correlations. 

For instance, Cong \emph{et al.}~\cite{cong2023point} combine the multi-modal CNN features and then utilize the fused features to refine the transformer decoder. 
However, prior studies have revealed inherent limitations in the depth modality, including a lack of informative content~\cite{hao2024primkd} and unstable quality~\cite{bao2024quality}.



\subsection{Video Salient Object Detection}
Video SOD (VSOD) extends SOD by leveraging temporal information across frames to ensure spatial-temporal consistency.

Early works~\cite{wang2017video, li2018flow, ji2020casnet} utilize optical flow and attention mechanisms to capture motion cues, but their accuracy heavily depends on flow quality and motion scale~\cite{singh2024dsfnet}.

To better model long-term dependencies, memory-based frameworks~\cite{oh2019video, cheng2022xmem} encode the previous frame's features and their predictions, and store the generated representations in a memory bank.
However, interacting with a large memory bank can introduce substantial computational consumption.

To tackle the issue, query-based methods~\cite{fang2024learning, wang2023look} utilize the learnable query to focus on relevant features selectively, achieving both high accuracy and efficiency.

Recent RGB-D VSOD methods aim to integrate depth and temporal cues simultaneously.
For instance, Li~\emph{et al.}~\cite{li2023dvsod} store previously fused RGB-D features into memory, extending memory networks to RGB-D settings.
Mou~\emph{et al.}~\cite{mou2024salient} fuse flow and multi-modal features via holistic multi-modal attentive paths (HMAPs).
Lin \emph{et al.}~\cite{lin2024vidsod} treat RGB, depth, and optical flow equally, and deploy intermediate supervision on their respective encoders. 
In addition, Suolang \emph{et al.}~\cite{suolang2025lightweight} propose a lightweight cross-shift module that efficiently fuses auxiliary depth and temporal cues.


\subsection{Segment Anything Model}

The Segment Anything Model (SAM)~\cite{kirillov2023segment, ravi2024sam} is a vision foundation model trained on large-scale data for universal image segmentation. While it generalizes well, its reliance on manual prompts (e.g., points, boxes), making it impractical for the video salient object detection task.


To eliminate the need for manual prompts, several works~\cite{zhang2024uv, ayzenberg2024protosam, xie2025rfmedsam} attempt to generate pseudo-prompts from coarse masks, while others~\cite{yang2024sam, xiong2024sam2, xu2025dgsunet} only use SAM's encoder for feature extraction.

Generally, the above methods tailor SAM for specific tasks without modifying its intrinsic architecture.
Recent methods have explored parameter-efficient fine-tuning (PEFT) strategies.


Wang \emph{et al.}~\cite{wang2024adapting} insert adapter~\cite{houlsby2019parameter} between encoder blocks, 

while Zhong \emph{et al.}~\cite{zhong2024convolution} integrate LoRA~\cite{hu2022lora} into transformer layers.

However, these sequential adapter structures tend to incur high memory consumption, as backward gradients must propagate through the entire encoder~\cite{diao2024unipt}.



To extend SAM to the video segmentation task, Yue \emph{et al.}~\cite{yue2024sam} propose a flow reconstruction technique to guide SAM in object discovery. Deng \emph{et al.}~\cite{deng2024memsam} present three distinct types of memory banks to mitigate the adverse effects of speckle noise and motion artifacts during memory prompting. To the best of our knowledge, no previous work has attempted to encode memory queries within the video SAM-based framework.","RGB-D Salient Object Detection
Depth information provides spatial cues resistant to contex-
tual distractions and is widely as regarded a critical comple-
ment for RGB information. Existing RGB-D SOD methodstypically follow three fusion paradigms (Mou et al. 2024),
namely early fusion, middle fusion and late fusion. The early
fusion strategy (Qu et al. 2017) treats the depth map as an
additional channel to the RGB image, and directly concate-
nates the two modal images to form a 4D input. The late
fusion strategy (Han et al. 2017) processes the two modali-
ties independently, which generates the final mask by fus-
ing their coarse predictions. However, the interaction be-
tween the modalities occurs at the output only, which lim-
its the fusion of the two modal features. The middle fusion
strategy (Cong et al. 2023; Bao et al. 2024) is widely em-
ployed to capture and exploit multi-modal correlations. For
instance, Conget al.(Cong et al. 2023) combine the multi-
modal CNN features and then utilize the fused features to
refine the transformer decoder. However, prior studies have
revealed inherent limitations in the depth modality, includ-
ing a lack of informative content (Hao et al. 2024) and un-
stable quality (Bao et al. 2024).
Video Salient Object Detection
Video SOD (VSOD) extends SOD by leveraging temporal
information across frames to ensure spatial-temporal con-
sistency. Early works (Wang, Shen, and Shao 2017; Li et al.
2018; Ji et al. 2020) utilize optical flow and attention mech-
anisms to capture motion cues, but their accuracy heavily
depends on flow quality and motion scale (Singh, Verma,
and Cheruku 2024). To better model long-term dependen-
cies, memory-based frameworks (Oh et al. 2019; Cheng and
Schwing 2022) encode the previous frame’s features and
their predictions, and store the generated representations in
a memory bank. However, interacting with a large memory
bank can introduce substantial computational consumption.
To tackle the issue, query-based methods (Fang et al. 2024;
Wang et al. 2023) utilize the learnable query to focus on rel-
evant features selectively, achieving both high accuracy and
efficiency.
Recent RGB-D VSOD methods aim to integrate depth
and temporal cues simultaneously. For instance, Liet al.(Li
et al. 2023) store previously fused RGB-D features into
memory, extending memory networks to RGB-D settings.
Mouet al.(Mou et al. 2024) fuse flow and multi-modal
features via holistic multi-modal attentive paths (HMAPs).
Linet al.(Lin et al. 2024) treat RGB, depth, and optical
flow equally, and deploy intermediate supervision on their
respective encoders. In addition, Suolanget al.(Suolang
et al. 2025) propose a lightweight cross-shift module that
efficiently fuses auxiliary depth and temporal cues.
Segment Anything Model
The Segment Anything Model (SAM) (Kirillov et al. 2023;
Ravi et al. 2024) is a vision foundation model trained on
large-scale data for universal image segmentation. While
it generalizes well, its reliance on manual prompts (e.g.,
points, boxes), making it impractical for the video salient
object detection task.
To eliminate the need for manual prompts, sev-
eral works (Zhang et al. 2024; Ayzenberg, Giryes, and
Greenspan 2024; Xie et al. 2025) attempt to generate
pseudo-prompts from coarse masks, while others (Yang
et al. 2024; Xiong et al. 2024; Xu 2025) only use SAM’s en-
coder for feature extraction. Generally, the above methods
tailor SAM for specific tasks without modifying its intrin-
sic architecture. Recent methods have explored parameter-
efficient fine-tuning (PEFT) strategies. Wanget al.(Wang
et al. 2024) insert adapter (Houlsby et al. 2019) between en-
coder blocks, while Zhonget al.(Zhong et al. 2024) inte-
grate LoRA (Hu et al. 2022) into transformer layers. How-
ever, these sequential adapter structures tend to incur high
memory consumption, as backward gradients must propa-
gate through the entire encoder (Diao et al. 2024).
To extend SAM to the video segmentation task, Yueet
al.(Yue et al. 2024) propose a flow reconstruction technique
to guide SAM in object discovery. Denget al.(Deng et al.
2024) present three distinct types of memory banks to miti-
gate the adverse effects of speckle noise and motion artifacts
during memory prompting. To the best of our knowledge,
no previous work has attempted to encode memory queries
within the video SAM-based framework."
2511.10404v1,http://arxiv.org/abs/2511.10404v1,2025-11-13 15:24:27+00:00,DELICATE: Diachronic Entity LInking using Classes And Temporal Evidence,"In spite of the remarkable advancements in the field of Natural Language Processing, the task of Entity Linking (EL) remains challenging in the field of humanities due to complex document typologies, lack of domain-specific datasets and models, and long-tail entities, i.e., entities under-represented in Knowledge Bases (KBs). The goal of this paper is to address these issues with two main contributions. The first contribution is DELICATE, a novel neuro-symbolic method for EL on historical Italian which combines a BERT-based encoder with contextual information from Wikidata to select appropriate KB entities using temporal plausibility and entity type consistency. The second contribution is ENEIDE, a multi-domain EL corpus in historical Italian semi-automatically extracted from two annotated editions spanning from the 19th to the 20th century and including literary and political texts. Results show how DELICATE outperforms other EL models in historical Italian even if compared with larger architectures with billions of parameters. Moreover, further analyses reveal how DELICATE confidence scores and features sensitivity provide results which are more explainable and interpretable than purely neural methods.","\label{sec:related_work}


This section details the related studies. First, Section \ref{sec:challenges} presents the foundations of the field of named entity processing in the historical domain. Then, Section \ref{sec:approaches} discusses the SoTA with respect to neural EL approaches regarding historical corpora. Finally, Section \ref{sec:corpora} lists some of the most important monolingual and multilingual corpora in the field of humanities annotated for the evaluation of EL-related tasks, such as Named Entity Recognition (NER) and Entity Disambiguation (ED). 

\subsection{Historical Named Entity Processing}
\label{sec:challenges}

In the existing literature, named entity processing is divided into two different tasks. The first is NER, in which the entity mentions are identified in text and classified according to different categories such as person, location, organization, or miscellaneous. The subsequent task is ED, in which detected mentions are linked to the respective identifier in a structured KB, such as Wikidata. EL approaches often rely on a separate NER component trained to identify surface forms of named entities within a text~\citep{labusch_named_2020}; however, several approaches have investigated the possibility of training NER and ED jointly within a single end-to-end architecture~\citep{kolitsas-etal-2018-end,limkonchotiwat-etal-2023-mrefined}. 

Despite the remarkable performances that the NER and ED systems have achieved in contemporary web-crawled data~\citep{sevgili2022neural}, they under-perform when applied to historical documents (such as historical press articles, books, literary texts, or letters), which may be excluded from the training corpora and therefore are not observed in the learning phase. In a recent survey~on historical EL~\citep{survey_hist_ner_2023}, the authors highlight four main challenges: (i) the variety of documents (newspapers, letters, memoirs, books); (ii) the presence of noise in the input data due to data processing techniques, such as Optical Character Recognition (OCR); (iii) the problem of linguistic and chronological variations; and (iv) the lack of standardized benchmarks across multiple languages and genres. Furthermore, This study shows that only 2 datasets out of 22 surveyed historical corpora include documents annotated in Italian for NER. To the best of our knowledge, there is a lack of large-scale publicly available datasets annotated in historical Italian for EL. 


\subsection{Neural EL in the Humanities}
\label{sec:approaches}

One of the seminal works on neural end-to-end EL applied to historical documents has been presented in~\citep{linhares2022melhissa}. This system employs several strategies, including an OCR correction mechanism, a probabilistic entity table map, a multilingual KB that contains entity representations obtained from the multilingual Wikipedia and a BiLSTM network which is trained to compute the matching probability between the context in which an entity appears and its representation in the KB. Despite the remarkable performances shown on multilingual historical press articles (excluding Italian), the model makes limited use of contextual knowledge by simply applying rule-based filters on the results using temporal constraints, type-related information, and edit distance between entity labels and surface form. Differently, our approach aims to learn the relevance of contextual information by employing supervised learning techniques. 

The ED problem, which is the task of linking already detected mentions, is addressed for historical documents in~\citep{labusch_named_2020}. This work proposes a multilingual architecture for English, German, and French, based on three components: (a) an entity-candidate lookup; (b) an entity-candidate evaluation; and (c) an entity-candidate ranking. In (a), a word embedding model is used to perform a \textit{k-NN} search on a dense index of Wikipedia titles to find the most similar entities with respect to a query mention. In this step, the system also makes use of the date of publication of a document, provided in the metadata, to consider only temporally plausible entities. In the entity-candidate evaluation, a sentence-matching algorithm based on a BERT encoder is used to compare the context of the mention with the Wikipedia text snippets of the candidates returned in (a). In the entity-candidate ranking step, the similarity scores of step (a) and step (b) are fed to a Random Forest algorithm which computes the matching probabilities between the items in the candidate set and the query mention. More recently,~\citep{graciotti2025musical} proposed the use of rule-based constraints on Wikidata to refine the candidate retrieval step of a bi-encoder architecture to perform ED on historical music periodicals in English. More concretely, they paired BLINK~\citep{wu2020scalable} with multiple rule-based constraints in which the entities retrieved are verified for compliance with respect to the date of publication of a document and the entity type given in the metadata.

One of the main limitations of these approaches is that using filters based on temporal and semantic information in the lookup step may cause the system to incorrectly exclude from linking correct candidates. In contrast, our solution for this problem consists in employing temporal and semantic information in the final re-ranking phase. In DELICATE, information related to dates and types of the \textit{top-k} candidates retrieved is extracted from Wikidata and is used as a feature to train a supervised classifier for the task of learning optimal similarity thresholds between input mentions and attributes of the candidates.

\subsection{Historical EL Corpora}
\label{sec:corpora}

Due to the need to adapt the NER and EL approaches for documents provided by cultural heritage institutions, several resources have been created in order to provide standardized benchmarks in the field of historical EL. With respect to Italian, a pivotal work has been carried out in~\citep{paccosi-palmero-aprosio-2022-kind}, where the authors describe \textit{KIND}, a multi-domain dataset for NER extracted from several typologies of texts, including news, literary texts, and political works. Similar to ENEIDE, this dataset includes texts from the political domain, extracted from the De Gasperi Corpus~\citep{sprugnoli2016fifty} and Aldo Moro Digitale~\citep{barzaghi_amd_2025}. However, this work does not contain links to a KB for disambiguation. 

With respect to the literary domain, one of the first examples of datasets in Italian containing annotated references to literary works (such as citations to books, monographies, and essays), is \textit{LinkedBooks}~\citep{colavizza_annotated_2017}. In spite of the contribution of this resource for NER with literary entities, the dataset does not include more general entities (such as person, location, or organization) and the references to works are not disambiguated using Wikidata identifiers. More recently, in~\citep{ajmc_2024}, the authors present a manually annotated dataset of named entity references in classical commentaries of Sophocles' \textit{Ajax}. Similar to the literary texts in ENEIDE, entities are often referenced through abbreviations, as in \textit{Cic.} for \textit{Cicero} (a common practice in philological texts). Another feature that AJMC shares with ENEIDE is the fact that it contains several annotated references to literary works, either disambiguated with Wikidata or labeled as NIL if not present in the KB. 
A very recent  attempt to provide a benchmark for EL in Italian was presented in~\citep{graciotti-etal-2025-ke-mhisto}. In this paper, the authors present MHERCL-ITA\footnote{\url{https://github.com/polifonia-project/KE-MHISTO/blob/main/Datasets/MHERCL\_ITA.json}}, the first gold standard for EL in historical Italian, containing texts extracted from music periodicals published between 1853 to 1943. In total, MHERCL-ITA contains 533 sentences with 2,431 manual annotations of entities linked to Wikidata. In spite of its relevance for historical Italian, the main limitation of MHERCL-ITA is its small scale, since it only comprises a test set and is more suitable to evaluate pre-trained EL models or LLMs in a domain which is distant from the typology of documents with which they are trained.","This section details the related studies. First, Section \ref{sec:challenges} presents the foundations of the field of named entity processing in the historical domain. Then, Section \ref{sec:approaches} discusses the SoTA with respect to neural EL approaches regarding historical corpora. Finally, Section \ref{sec:corpora} lists some of the most important monolingual and multilingual corpora in the field of humanities annotated for the evaluation of EL-related tasks, such as Named Entity Recognition (NER) and Entity Disambiguation (ED). 

\subsection{Historical Named Entity Processing}

In the existing literature, named entity processing is divided into two different tasks. The first is NER, in which the entity mentions are identified in text and classified according to different categories such as person, location, organization, or miscellaneous. The subsequent task is ED, in which detected mentions are linked to the respective identifier in a structured KB, such as Wikidata. EL approaches often rely on a separate NER component trained to identify surface forms of named entities within a text~\citep{labusch_named_2020}; however, several approaches have investigated the possibility of training NER and ED jointly within a single end-to-end architecture~\citep{kolitsas-etal-2018-end,limkonchotiwat-etal-2023-mrefined}. 

Despite the remarkable performances that the NER and ED systems have achieved in contemporary web-crawled data~\citep{sevgili2022neural}, they under-perform when applied to historical documents (such as historical press articles, books, literary texts, or letters), which may be excluded from the training corpora and therefore are not observed in the learning phase. In a recent survey~on historical EL~\citep{survey_hist_ner_2023}, the authors highlight four main challenges: (i) the variety of documents (newspapers, letters, memoirs, books); (ii) the presence of noise in the input data due to data processing techniques, such as Optical Character Recognition (OCR); (iii) the problem of linguistic and chronological variations; and (iv) the lack of standardized benchmarks across multiple languages and genres. Furthermore, This study shows that only 2 datasets out of 22 surveyed historical corpora include documents annotated in Italian for NER. To the best of our knowledge, there is a lack of large-scale publicly available datasets annotated in historical Italian for EL. 


\subsection{Neural EL in the Humanities}

One of the seminal works on neural end-to-end EL applied to historical documents has been presented in~\citep{linhares2022melhissa}. This system employs several strategies, including an OCR correction mechanism, a probabilistic entity table map, a multilingual KB that contains entity representations obtained from the multilingual Wikipedia and a BiLSTM network which is trained to compute the matching probability between the context in which an entity appears and its representation in the KB. Despite the remarkable performances shown on multilingual historical press articles (excluding Italian), the model makes limited use of contextual knowledge by simply applying rule-based filters on the results using temporal constraints, type-related information, and edit distance between entity labels and surface form. Differently, our approach aims to learn the relevance of contextual information by employing supervised learning techniques. 

The ED problem, which is the task of linking already detected mentions, is addressed for historical documents in~\citep{labusch_named_2020}. This work proposes a multilingual architecture for English, German, and French, based on three components: (a) an entity-candidate lookup; (b) an entity-candidate evaluation; and (c) an entity-candidate ranking. In (a), a word embedding model is used to perform a \textit{k-NN} search on a dense index of Wikipedia titles to find the most similar entities with respect to a query mention. In this step, the system also makes use of the date of publication of a document, provided in the metadata, to consider only temporally plausible entities. In the entity-candidate evaluation, a sentence-matching algorithm based on a BERT encoder is used to compare the context of the mention with the Wikipedia text snippets of the candidates returned in (a). In the entity-candidate ranking step, the similarity scores of step (a) and step (b) are fed to a Random Forest algorithm which computes the matching probabilities between the items in the candidate set and the query mention. More recently,~\citep{graciotti2025musical} proposed the use of rule-based constraints on Wikidata to refine the candidate retrieval step of a bi-encoder architecture to perform ED on historical music periodicals in English. More concretely, they paired BLINK~\citep{wu2020scalable} with multiple rule-based constraints in which the entities retrieved are verified for compliance with respect to the date of publication of a document and the entity type given in the metadata.

One of the main limitations of these approaches is that using filters based on temporal and semantic information in the lookup step may cause the system to incorrectly exclude from linking correct candidates. In contrast, our solution for this problem consists in employing temporal and semantic information in the final re-ranking phase. In DELICATE, information related to dates and types of the \textit{top-k} candidates retrieved is extracted from Wikidata and is used as a feature to train a supervised classifier for the task of learning optimal similarity thresholds between input mentions and attributes of the candidates.

\subsection{Historical EL Corpora}

Due to the need to adapt the NER and EL approaches for documents provided by cultural heritage institutions, several resources have been created in order to provide standardized benchmarks in the field of historical EL. With respect to Italian, a pivotal work has been carried out in~\citep{paccosi-palmero-aprosio-2022-kind}, where the authors describe \textit{KIND}, a multi-domain dataset for NER extracted from several typologies of texts, including news, literary texts, and political works. Similar to ENEIDE, this dataset includes texts from the political domain, extracted from the De Gasperi Corpus~\citep{sprugnoli2016fifty} and Aldo Moro Digitale~\citep{barzaghi_amd_2025}. However, this work does not contain links to a KB for disambiguation. 

With respect to the literary domain, one of the first examples of datasets in Italian containing annotated references to literary works (such as citations to books, monographies, and essays), is \textit{LinkedBooks}~\citep{colavizza_annotated_2017}. In spite of the contribution of this resource for NER with literary entities, the dataset does not include more general entities (such as person, location, or organization) and the references to works are not disambiguated using Wikidata identifiers. More recently, in~\citep{ajmc_2024}, the authors present a manually annotated dataset of named entity references in classical commentaries of Sophocles' \textit{Ajax}. Similar to the literary texts in ENEIDE, entities are often referenced through abbreviations, as in \textit{Cic.} for \textit{Cicero} (a common practice in philological texts). Another feature that AJMC shares with ENEIDE is the fact that it contains several annotated references to literary works, either disambiguated with Wikidata or labeled as NIL if not present in the KB. 
A very recent  attempt to provide a benchmark for EL in Italian was presented in~\citep{graciotti-etal-2025-ke-mhisto}. In this paper, the authors present MHERCL-ITA\footnote{\url{https://github.com/polifonia-project/KE-MHISTO/blob/main/Datasets/MHERCL\_ITA.json}}, the first gold standard for EL in historical Italian, containing texts extracted from music periodicals published between 1853 to 1943. In total, MHERCL-ITA contains 533 sentences with 2,431 manual annotations of entities linked to Wikidata. In spite of its relevance for historical Italian, the main limitation of MHERCL-ITA is its small scale, since it only comprises a test set and is more suitable to evaluate pre-trained EL models or LLMs in a domain which is distant from the typology of documents with which they are trained.","This section details the related studies. First, Section 2.1 presents the foundations
of the field of named entity processing in the historical domain. Then, Section 2.2
discusses the SoTA with respect to neural EL approaches regarding historical corpora.
Finally, Section 2.3 lists some of the most important monolingual and multilingual
corpora in the field of humanities annotated for the evaluation of EL-related tasks,
such as Named Entity Recognition (NER) and Entity Disambiguation (ED).
2.1 Historical Named Entity Processing
In the existing literature, named entity processing is divided into two different tasks.
The first is NER, in which the entity mentions are identified in text and classified ac-
cording to different categories such as person, location, organization, or miscellaneous.
The subsequent task is ED, in which detected mentions are linked to the respective
identifier in a structured KB, such as Wikidata. EL approaches often rely on a separate
NER component trained to identify surface forms of named entities within a text [14];
2https://digitalzibaldone.net/
3https://aldomorodigitale.unibo.it/
4source code: https://github.com/sntcristian/DELICATE
5dataset: https://github.com/sntcristian/ENEIDE
6trained models: http://doi.org/10.57967/hf/6984
4
however, several approaches have investigated the possibility of training NER and ED
jointly within a single end-to-end architecture [13, 15].
Despite the remarkable performances that the NER and ED systems have achieved
in contemporary web-crawled data [24], they under-perform when applied to historical
documents (such as historical press articles, books, literary texts, or letters), which
may be excluded from the training corpora and therefore are not observed in the
learning phase. In a recent survey on historical EL [8], the authors highlight four
main challenges: (i) the variety of documents (newspapers, letters, memoirs, books);
(ii) the presence of noise in the input data due to data processing techniques, such as
Optical Character Recognition (OCR); (iii) the problem of linguistic and chronological
variations; and (iv) the lack of standardized benchmarks across multiple languages
and genres. Furthermore, This study shows that only 2 datasets out of 22 surveyed
historical corpora include documents annotated in Italian for NER. To the best of
our knowledge, there is a lack of large-scale publicly available datasets annotated in
historical Italian for EL.
2.2 Neural EL in the Humanities
One of the seminal works on neural end-to-end EL applied to historical documents
has been presented in [16]. This system employs several strategies, including an OCR
correction mechanism, a probabilistic entity table map, a multilingual KB that con-
tains entity representations obtained from the multilingual Wikipedia and a BiLSTM
network which is trained to compute the matching probability between the context
in which an entity appears and its representation in the KB. Despite the remarkable
performances shown on multilingual historical press articles (excluding Italian), the
model makes limited use of contextual knowledge by simply applying rule-based filters
on the results using temporal constraints, type-related information, and edit distance
between entity labels and surface form. Differently, our approach aims to learn the
relevance of contextual information by employing supervised learning techniques.
The ED problem, which is the task of linking already detected mentions, is ad-
dressed for historical documents in [14]. This work proposes a multilingual architecture
for English, German, and French, based on three components: (a) an entity-candidate
lookup; (b) an entity-candidate evaluation; and (c) an entity-candidate ranking. In
(a), a word embedding model is used to perform ak-NNsearch on a dense index of
Wikipedia titles to find the most similar entities with respect to a query mention.
In this step, the system also makes use of the date of publication of a document,
provided in the metadata, to consider only temporally plausible entities. In the entity-
candidate evaluation, a sentence-matching algorithm based on a BERT encoder is
used to compare the context of the mention with the Wikipedia text snippets of the
candidates returned in (a). In the entity-candidate ranking step, the similarity scores
of step (a) and step (b) are fed to a Random Forest algorithm which computes the
matching probabilities between the items in the candidate set and the query mention.
More recently, [9] proposed the use of rule-based constraints on Wikidata to refine
the candidate retrieval step of a bi-encoder architecture to perform ED on historical
music periodicals in English. More concretely, they paired BLINK [29] with multiple
rule-based constraints in which the entities retrieved are verified for compliance with
5
respect to the date of publication of a document and the entity type given in the
metadata.
One of the main limitations of these approaches is that using filters based on tem-
poral and semantic information in the lookup step may cause the system to incorrectly
exclude from linking correct candidates. In contrast, our solution for this problem con-
sists in employing temporal and semantic information in the final re-ranking phase. In
DELICATE, information related to dates and types of thetop-kcandidates retrieved
is extracted from Wikidata and is used as a feature to train a supervised classifier
for the task of learning optimal similarity thresholds between input mentions and
attributes of the candidates.
2.3 Historical EL Corpora
Due to the need to adapt the NER and EL approaches for documents provided by
cultural heritage institutions, several resources have been created in order to provide
standardized benchmarks in the field of historical EL. With respect to Italian, a pivotal
work has been carried out in [17], where the authors describeKIND, a multi-domain
dataset for NER extracted from several typologies of texts, including news, literary
texts, and political works. Similar to ENEIDE, this dataset includes texts from the po-
litical domain, extracted from the De Gasperi Corpus [25] and Aldo Moro Digitale [1].
However, this work does not contain links to a KB for disambiguation.
With respect to the literary domain, one of the first examples of datasets in Italian
containing annotated references to literary works (such as citations to books, monogra-
phies, and essays), isLinkedBooks[3]. In spite of the contribution of this resource for
NER with literary entities, the dataset does not include more general entities (such as
person, location, or organization) and the references to works are not disambiguated
using Wikidata identifiers. More recently, in [19], the authors present a manually an-
notated dataset of named entity references in classical commentaries of Sophocles’
Ajax. Similar to the literary texts in ENEIDE, entities are often referenced through
abbreviations, as inCic.forCicero(a common practice in philological texts). Another
feature that AJMC shares with ENEIDE is the fact that it contains several annotated
references to literary works, either disambiguated with Wikidata or labeled as NIL if
not present in the KB. A very recent attempt to provide a benchmark for EL in Italian
was presented in [10]. In this paper, the authors present MHERCL-ITA7, the first gold
standard for EL in historical Italian, containing texts extracted from music periodi-
cals published between 1853 to"
2511.08403v1,http://arxiv.org/abs/2511.08403v1,2025-11-11 16:18:39+00:00,Blockly2Hooks: Smart Contracts for Everyone with the XRP Ledger and Google Blockly,"Recent technologies such as inter-ledger payments, non-fungible tokens, and smart contracts are all fruited from the ongoing development of Distributed Ledger Technologies. The foreseen trend is that they will play an increasingly visible role in daily life, which will have to be backed by appropriate operational resources. For example, due to increasing demand, smart contracts could soon face a shortage of knowledgeable users and tools to handle them in practice. Widespread smart contract adoption is currently limited by security, usability and costs aspects. Because of a steep learning curve, the handling of smart contracts is currently performed by specialised developers mainly, and most of the research effort is focusing on smart contract security, while other aspects like usability being somewhat neglected. Specific tools would lower the entry barrier, enabling interested non-experts to create smart contracts.
  In this paper we designed, developed and tested Blockly2Hooks, a solution towards filling this gap even in challenging scenarios such as when the smart contracts are written in an advanced language like C. With the XRP Ledger as a concrete working case, Blockly2Hooks helps interested non-experts from the community to learn smart contracts easily and adopt the technology, through leveraging well-proven teaching methodologies like Visual Programming Languages, and more specifically, the Blockly Visual Programming library from Google. The platform was developed and tested and the results are promising to make learning smart contract development smoother.","\label{sec:relwork}

Marks~\cite{marks} identifies different levels of abstraction for smart contract editors, from syntactic graphical editors such as those based on Blocky, through flow-based editors alike Unreal Engine \textit{Blueprint}, %~\footnote{https://docs.unrealengine.com/5.0/en-US/blueprints-visual-scripting-in-unreal-engine/, valid in February 2023}, 
and to \textit{forms} to be filled-in for the simplest cases. It argues that because the necessary level of abstraction is dependent on the intended audience and use-case, until the users are there, the abstraction level can only be guessed. The development of real-life contracts as smart contracts on Ethereum was studied in~\cite{detailed-sc-eth}, where challenges like the complexity of contract clauses and user privacy are exposed. The usage of visual domain-specific languages for smart contract creation on Solidity is studied in~\cite{DasContract,towards-skotnika-DasContract2} and a concrete solution named \textit{DasContract} is proposed. Actually, most of the previous work focused on visual editors for Solidity on Ethereum, possibly because this is the most popular smart contract platform: for example Latte~\cite{latte} provides feedback regarding the Gas cost incurred by the smart contract being built, while~\cite{ETH-Visual-Auto} employs a machine learning approach to build a visual programming environment. A solution based on YAWL~\cite{yawl} is proposed by \cite{industry-construction-users-themselves} for Solidity, while working on a construction industry use case. For an industry client, \cite{smart-contracts-for-purchases} designed and implemented a VPL-based environment for a \textit{legal purchase agreements} use case. The authors of~\cite{reuse-SC-visual} identify a lack of smart contract descriptors~\cite{reuse-SC-visual}, investigate the reusing of smart contracts, propose, then implement a design for smart contract descriptors, a descriptors registry, and a visual editor based on Google Blockly for creating composite smart contracts. The author of~\cite{purnell2022towards} identifies several requirements for widespread adoption of smart contracts in business and industry: ease of use, understandability, ease of testing, secure and error-free, scalable and affordable~\cite{purnell2022towards}. Next, it investigates declarative languages on a \textit{""Will and Testament""} use case.
\textit{FlowContract}\footnote{https://flowcontracts.com/docs, valid in February 2023} aims to be a flow-based editor for Solidity, which however continues to expose lots of low-level programming elements to the user.

\textit{Blocks}\footnote{https://blocks-editor.github.io/blocks/, valid in February 2023}~\cite{blocks-article} is an online editor for the \textit{Internet Computer}\footnote{https://dfinity.org/, valid in February 2023}. It is a flow-oriented design retaining many syntactic elements: most of the blocks, fields and variable names come from their text-programming counterparts. A design for Hyperledger Fabric is proposed in \cite{blockly-hyperledger}, however only limited work seems to be open-source. XRPL Labs proposes an online, browser-based \textit{Builder}~\cite{hooks-builder} for \textit{Hooks} which aggregates documentation, examples, and Testnet deployment. Smart contracts are built in C though, which ultimately makes it a programmer tool. Same as \textit{Builder}, \system~could eliminate the need to install code on user machines by placing online the functionalities still present on users' machines, such that users do everything from their browser by accessing a remotely-served web interface. To the best of our knowledge no other open-source project addresses the visual development of smart contracts natively developed with advanced languages such as the modified C for XRPL Hooks, and the environment proposed is general enough to accommodate any smart contract.","Marks~\cite{marks} identifies different levels of abstraction for smart contract editors, from syntactic graphical editors such as those based on Blocky, through flow-based editors alike Unreal Engine \textit{Blueprint}, 
and to \textit{forms} to be filled-in for the simplest cases. It argues that because the necessary level of abstraction is dependent on the intended audience and use-case, until the users are there, the abstraction level can only be guessed. The development of real-life contracts as smart contracts on Ethereum was studied in~\cite{detailed-sc-eth}, where challenges like the complexity of contract clauses and user privacy are exposed. The usage of visual domain-specific languages for smart contract creation on Solidity is studied in~\cite{DasContract,towards-skotnika-DasContract2} and a concrete solution named \textit{DasContract} is proposed. Actually, most of the previous work focused on visual editors for Solidity on Ethereum, possibly because this is the most popular smart contract platform: for example Latte~\cite{latte} provides feedback regarding the Gas cost incurred by the smart contract being built, while~\cite{ETH-Visual-Auto} employs a machine learning approach to build a visual programming environment. A solution based on YAWL~\cite{yawl} is proposed by \cite{industry-construction-users-themselves} for Solidity, while working on a construction industry use case. For an industry client, \cite{smart-contracts-for-purchases} designed and implemented a VPL-based environment for a \textit{legal purchase agreements} use case. The authors of~\cite{reuse-SC-visual} identify a lack of smart contract descriptors~\cite{reuse-SC-visual}, investigate the reusing of smart contracts, propose, then implement a design for smart contract descriptors, a descriptors registry, and a visual editor based on Google Blockly for creating composite smart contracts. The author of~\cite{purnell2022towards} identifies several requirements for widespread adoption of smart contracts in business and industry: ease of use, understandability, ease of testing, secure and error-free, scalable and affordable~\cite{purnell2022towards}. Next, it investigates declarative languages on a \textit{""Will and Testament""} use case.
\textit{FlowContract}\footnote{https://flowcontracts.com/docs, valid in February 2023} aims to be a flow-based editor for Solidity, which however continues to expose lots of low-level programming elements to the user.

\textit{Blocks}\footnote{https://blocks-editor.github.io/blocks/, valid in February 2023}~\cite{blocks-article} is an online editor for the \textit{Internet Computer}\footnote{https://dfinity.org/, valid in February 2023}. It is a flow-oriented design retaining many syntactic elements: most of the blocks, fields and variable names come from their text-programming counterparts. A design for Hyperledger Fabric is proposed in \cite{blockly-hyperledger}, however only limited work seems to be open-source. XRPL Labs proposes an online, browser-based \textit{Builder}~\cite{hooks-builder} for \textit{Hooks} which aggregates documentation, examples, and Testnet deployment. Smart contracts are built in C though, which ultimately makes it a programmer tool. Same as \textit{Builder}, \system~could eliminate the need to install code on user machines by placing online the functionalities still present on users' machines, such that users do everything from their browser by accessing a remotely-served web interface. To the best of our knowledge no other open-source project addresses the visual development of smart contracts natively developed with advanced languages such as the modified C for XRPL Hooks, and the environment proposed is general enough to accommodate any smart contract.",
2511.10054v1,http://arxiv.org/abs/2511.10054v1,2025-11-13 07:56:50+00:00,BuddyMoE: Exploiting Expert Redundancy to Accelerate Memory-Constrained Mixture-of-Experts Inference,"Mixture-of-Experts (MoE) architectures scale language models by activating only a subset of specialized expert networks for each input token, thereby reducing the number of floating-point operations. However, the growing size of modern MoE models causes their full parameter sets to exceed GPU memory capacity; for example, Mixtral-8x7B has 45 billion parameters and requires 87 GB of memory even though only 14 billion parameters are used per token. Existing systems alleviate this limitation by offloading inactive experts to CPU memory, but transferring experts across the PCIe interconnect incurs significant latency (about 10 ms). Prefetching heuristics aim to hide this latency by predicting which experts are needed, but prefetch failures introduce significant stalls and amplify inference latency. In the event of a prefetch failure, prior work offers two primary solutions: either fetch the expert on demand, which incurs a long stall due to the PCIe bottleneck, or drop the expert from the computation, which significantly degrades model accuracy. The critical challenge, therefore, is to maintain both high inference speed and model accuracy when prefetching fails.","\noindent\textbf{Mixture-of-Experts Models.} The MoE architecture scales language models by activating only a sparse subset of parameters per token. Shazeer et al.\ introduced the sparsely-gated MoE layer, where a gating network routes each input to a small subset of expert feed-forward networks instead of a dense layer~\cite{shazeer2017outrageously}. Subsequent large-scale systems, such as Google’s GShard~\cite{lepikhin2021gshard} and the Switch Transformer~\cite{fedus2021switch}, demonstrated the effectiveness of this idea at unprecedented scales. GShard combined conditional computation with automatic sharding and \emph{demonstrated} MoE models beyond 600B parameters (and discussed scaling up to trillions). The Switch Transformer simplified gating to top-1 (a single expert per token) and achieved state-of-the-art results with up to $\sim$1.6T parameters; a representative configuration (Switch-C) uses 2048 experts per layer across 15 layers—i.e., tens of thousands of experts across the network. These works showed MoEs can match or exceed dense models’ quality while activating only a fraction of parameters, setting the stage for modern MoE LLMs (e.g., Mixtral, DeepSeek-MoE, Snowflake Arctic). However, the memory footprint of storing all experts remains a challenge at inference time for models with tens to hundreds of billions of total parameters.

\noindent\textbf{Memory Offloading for Large Models.}
Memory offloading is a common technique to deploy large-scale models on hardware with constrained GPU memory. General-purpose systems like DeepSpeed's ZeRO-Infinity~\cite{rajbhandari2021zero} and Hugging Face Accelerate~\cite{hf_accelerate_infer_2025,hf_accelerate_deepspeed_2025} offload coarse-grained model components such as entire layers to CPU memory or NVMe storage. MoE models, however, present a unique offloading challenge due to their fine-grained, dynamic expert activations. Systems must swap individual experts with low latency, making I/O management critical.

Research in this area has focused on two primary strategies: predictive prefetching and intelligent cache management.
For prefetching, MoE-Infinity~\cite{xue2024moe} traces historical expert usage to predict and load upcoming experts, aiming to hide data transfer latency. Similarly, Pre-gated MoE~\cite{hwang2024pregatedmoe} integrates an auxiliary \emph{pre-gate} into the model architecture to predict the next layer’s expert needs one step ahead.
For cache management, EdgeMoE~\cite{yi2023edgemoe} improves upon standard LRU/LFU eviction policies by designing a heuristic that considers both activation frequency and layer index. It also reduces data transfer volume by applying expert-wise mixed-precision quantization. These works demonstrate that specialized, model-aware strategies are essential for minimizing I/O stalls in offloaded MoE inference.

\smallskip\noindent\textbf{Reducing Expert Loading Latency.} Because on-demand expert loads can dominate end-to-end latency, several strategies aim to mitigate this bottleneck. \emph{Quantization and skipping.} EdgeMoE uses expert-wise mixed-precision (e.g., assigning lower bit-width to less critical experts) to reduce memory and transfer cost with limited impact on accuracy~\cite{yi2023edgemoe}. AdapMoE adaptively limits the number of activated experts $k$ based on a sensitivity metric, integrating improved prefetching and caching; it reduces the average number of experts by $\sim$25\% and reports $\sim$1.35$\times$ speedup without accuracy loss on edge platforms~\cite{zhong2024adapmoe}. \emph{Dynamic expert pools.} SwapMoE maintains a small set of high-value “virtual experts’’ in GPU memory and swaps others to CPU, showing reduced memory consumption (e.g., 14.2$\rightarrow$4.7\,GiB) and $\sim$50\% latency reduction on Switch Transformer benchmarks, while preserving quality~\cite{kong2024swapmoe}. Complementary to these, Fate uses cross-layer gate signals to prefetch next-layer experts and a shallow-favoring cache, reporting expert-hit rates $\sim$99\% and up to $4.1\times$ decoding speedups under offloading~\cite{fang2025fate}. In parallel, Lu et al.\ study post-training expert pruning/skipping, finding that removing or skipping infrequently used experts often yields negligible loss on downstream tasks, reinforcing redundancy in large MoEs~\cite{lu2024experts}. Overall, these methods leverage redundancy and variable expert importance to reduce effective model size or avoid expensive loads, trading negligible accuracy for large efficiency gains.

% \smallskip\noindent\textbf{Dynamic Sparse Networks.} Beyond MoE-specific work, dynamic sparse training (DST) methods train sparse subnetworks directly, reducing parameters and compute while preserving accuracy. Unlike MoE (dense parameters, sparsely activated at inference), DST starts sparse and updates connectivity during training. Liu et al.\ train efficient sparse networks from scratch using trainable masked layers whose sparsity evolves over time~\cite{liu2020dynamic}. Sokar et al.\ extend dynamic sparsity to deep RL, showing comparable performance to dense networks with far fewer active weights~\cite{sokar2022dynamic}. Lasby et al.\ introduce structured DST (e.g., N:M-like constraints), improving hardware practicality while maintaining accuracy~\cite{lasby2024dynamic}. These results complement MoE: both indicate substantial parameter under-utilization, though DST yields a static sparse model for inference, whereas MoE relies on run-time gating. Our BuddyMoE likewise exploits sparsity and redundancy, but at inference time: when a cache miss would occur, we substitute a ""buddy"" expert (learned via co-activation similarity) to avoid stalls with minimal quality impact.","\noindent\textbf{Mixture-of-Experts Models.} The MoE architecture scales language models by activating only a sparse subset of parameters per token. Shazeer et al.\ introduced the sparsely-gated MoE layer, where a gating network routes each input to a small subset of expert feed-forward networks instead of a dense layer~\cite{shazeer2017outrageously}. Subsequent large-scale systems, such as Google’s GShard~\cite{lepikhin2021gshard} and the Switch Transformer~\cite{fedus2021switch}, demonstrated the effectiveness of this idea at unprecedented scales. GShard combined conditional computation with automatic sharding and \emph{demonstrated} MoE models beyond 600B parameters (and discussed scaling up to trillions). The Switch Transformer simplified gating to top-1 (a single expert per token) and achieved state-of-the-art results with up to $\sim$1.6T parameters; a representative configuration (Switch-C) uses 2048 experts per layer across 15 layers—i.e., tens of thousands of experts across the network. These works showed MoEs can match or exceed dense models’ quality while activating only a fraction of parameters, setting the stage for modern MoE LLMs (e.g., Mixtral, DeepSeek-MoE, Snowflake Arctic). However, the memory footprint of storing all experts remains a challenge at inference time for models with tens to hundreds of billions of total parameters.

\noindent\textbf{Memory Offloading for Large Models.}
Memory offloading is a common technique to deploy large-scale models on hardware with constrained GPU memory. General-purpose systems like DeepSpeed's ZeRO-Infinity~\cite{rajbhandari2021zero} and Hugging Face Accelerate~\cite{hf_accelerate_infer_2025,hf_accelerate_deepspeed_2025} offload coarse-grained model components such as entire layers to CPU memory or NVMe storage. MoE models, however, present a unique offloading challenge due to their fine-grained, dynamic expert activations. Systems must swap individual experts with low latency, making I/O management critical.

Research in this area has focused on two primary strategies: predictive prefetching and intelligent cache management.
For prefetching, MoE-Infinity~\cite{xue2024moe} traces historical expert usage to predict and load upcoming experts, aiming to hide data transfer latency. Similarly, Pre-gated MoE~\cite{hwang2024pregatedmoe} integrates an auxiliary \emph{pre-gate} into the model architecture to predict the next layer’s expert needs one step ahead.
For cache management, EdgeMoE~\cite{yi2023edgemoe} improves upon standard LRU/LFU eviction policies by designing a heuristic that considers both activation frequency and layer index. It also reduces data transfer volume by applying expert-wise mixed-precision quantization. These works demonstrate that specialized, model-aware strategies are essential for minimizing I/O stalls in offloaded MoE inference.

\smallskip\noindent\textbf{Reducing Expert Loading Latency.} Because on-demand expert loads can dominate end-to-end latency, several strategies aim to mitigate this bottleneck. \emph{Quantization and skipping.} EdgeMoE uses expert-wise mixed-precision (e.g., assigning lower bit-width to less critical experts) to reduce memory and transfer cost with limited impact on accuracy~\cite{yi2023edgemoe}. AdapMoE adaptively limits the number of activated experts $k$ based on a sensitivity metric, integrating improved prefetching and caching; it reduces the average number of experts by $\sim$25\","Mixture-of-Experts Models.The MoE architecture scales
language models by activating only a sparse subset of param-
eters per token. Shazeer et al. introduced the sparsely-gated
MoE layer, where a gating network routes each input to a
small subset of expert feed-forward networks instead of a
dense layer [ 16]. Subsequent large-scale systems, such as
Google’s GShard [ 9] and the Switch Transformer [ 3], demon-
strated the effectiveness of this idea at unprecedented scales.
GShard combined conditional computation with automatic
sharding anddemonstratedMoE models beyond 600B pa-
rameters (and discussed scaling up to trillions). The Switch
Transformer simplified gating to top-1 (a single expert per
token) and achieved state-of-the-art results with up to ∼1.6T
parameters; a representative configuration (Switch-C) uses
2048 experts per layer across 15 layers—i.e., tens of thou-
sands of experts across the network. These works showed
MoEs can match or exceed dense models’ quality while ac-
tivating only a fraction of parameters, setting the stage for
modern MoE LLMs (e.g., Mixtral, DeepSeek-MoE, Snowflake
Arctic). However, the memory footprint of storing all experts
remains a challenge at inference time for models with tens
to hundreds of billions of total parameters.
Memory Offloading for Large Models.Memory offload-
ing is a common technique to deploy large-scale models on
hardware with constrained GPU memory. General-purpose
systems like DeepSpeed’s ZeRO-Infinity [ 15] and Hugging
Face Accelerate [ 5,6] offload coarse-grained model compo-
nents such as entire layers to CPU memory or NVMe storage.
MoE models, however, present a unique offloading challenge
due to their fine-grained, dynamic expert activations. Sys-
tems must swap individual experts with low latency, making
I/O management critical.
Research in this area has focused on two primary strate-
gies: predictive prefetching and intelligent cache manage-
ment. For prefetching, MoE-Infinity [ 19] traces historical
expert usage to predict and load upcoming experts, aiming
to hide data transfer latency. Similarly, Pre-gated MoE [ 7]
integrates an auxiliarypre-gateinto the model architecture
to predict the next layer’s expert needs one step ahead. For
cache management, EdgeMoE [ 21] improves upon standardLRU/LFU eviction policies by designing a heuristic that con-
siders both activation frequency and layer index. It also re-
duces data transfer volume by applying expert-wise mixed-
precision quantization. These works demonstrate that spe-
cialized, model-aware strategies are essential for minimizing
I/O stalls in offloaded MoE inference.
Reducing Expert Loading Latency.Because on-demand
expert loads can dominate end-to-end latency, several strate-
gies aim to mitigate this bottleneck.Quantization and skip-
ping.EdgeMoE uses expert-wise mixed-precision (e.g., as-
signing lower bit-width to less critical experts) to reduce
memory and transfer cost with limited impact on accuracy [ 21].
AdapMoE adaptively limits the number of activated experts 𝑘
based on a sensitivity metric, integrating improved prefetch-
ing and caching; it reduces the average number of experts
by∼25% and reports∼1.35×speedup without accuracy loss
on edge platforms [ 24].Dynamic expert pools.SwapMoE
maintains a small set of high-value “virtual experts” in GPU
memory and swaps others to CPU, showing reduced memory
consumption (e.g., 14.2 →4.7 GiB) and∼50% latency reduc-
tion on Switch Transformer benchmarks, while preserving
quality [ 8]. Complementary to these, Fate uses cross-layer
gate signals to prefetch next-layer experts and a shallow-
favoring cache, reporting expert-hit rates ∼99% and up to
4.1×decoding speedups under offloading [ 2]. In parallel,
Lu et al. study post-training expert pruning/skipping, find-
ing that removing or skipping infrequently used experts
often yields negligible loss on downstream tasks, reinforc-
ing redundancy in large MoEs [ 14]. Overall, these methods
leverage redundancy and variable expert importance to re-
duce effective model size or avoid expensive loads, trading
negligible accuracy for large efficiency gains."
2511.10320v1,http://arxiv.org/abs/2511.10320v1,2025-11-13 13:58:36+00:00,PITE: Multi-Prototype Alignment for Individual Treatment Effect Estimation,"Estimating Individual Treatment Effects (ITE) from observational data is challenging due to confounding bias. Most studies tackle this bias by balancing distributions globally, but ignore individual heterogeneity and fail to capture the local structure that represents the natural clustering among individuals, which ultimately compromises ITE estimation. While instance-level alignment methods consider heterogeneity, they similarly overlook the local structure information. To address these issues, we propose an end-to-end Multi-\textbf{P}rototype alignment method for \textbf{ITE} estimation (\textbf{PITE}). PITE effectively captures local structure within groups and enforces cross-group alignment, thereby achieving robust ITE estimation. Specifically, we first define prototypes as cluster centroids based on similar individuals under the same treatment. To identify local similarity and the distribution consistency, we perform instance-to-prototype matching to assign individuals to the nearest prototype within groups, and design a multi-prototype alignment strategy to encourage the matched prototypes to be close across treatment arms in the latent space. PITE not only reduces distribution shift through fine-grained, prototype-level alignment, but also preserves the local structures of treated and control groups, which provides meaningful constraints for ITE estimation. Extensive evaluations on benchmark datasets demonstrate that PITE outperforms 13 state-of-the-art methods, achieving more accurate and robust ITE estimation.","% Most traditional statistical methods either employ propensity score \cite{a1,a2}, or optimize sample weight by entropy balancing and residual balancing \cite{a3,a4} to mainly estimate the average treatment effect (ATE). 
Recently, numerous deep learning studies have analyzed the relationship between treatment and outcome at the individual level through mitigating distribution shift, which can be broadly categorized into distribution-level alignment and instance-level alignment methods.


\subsection{Distribution-level alignment}

Current distribution-level alignment methods aim to balance the distributions globally by learning first-order moments, primarily employing distance metrics, adversarial training, and optimal transport techniques. For example, \citet{15} developed TARNet / CFRNet to mitigate confounding bias by reducing the distribution divergence between treated and control groups in the representation space, adopting Maximum Mean Discrepancy (MMD) and Wasserstein distance. GANITE \cite{18} utilized adversarial training to make the discriminator unable to distinguish whether the input data come from the factual distribution or the generate counterfactual distribution. CBRE \cite{k4} introduced an information loop to preserve predictive information that might otherwise be lost during the raw-to-latent space transformation in adversarial training. Alternatively, optimal transport-based methods have also been explored, where \citet{7} reduces the balancing error under the framework of optimal transport with learnable marginal distributions and the cost function. Similarly, \citet{a6} proposed an estimator based on optimal transport to handle both mini-batch sampling effects and unobserved confounder effects issues.

While these methods focus on global distributional alignment, they often neglect the individual-level heterogeneity and intrinsic structure of data such as subgroup similarity or local clustering, which leads to less informative representations and compromises ITE estimation.


\subsection{Instance-level alignment}

Instance-level alignment methods work by matching similar units from different groups to construct locally balanced distributions. The propensity score matching \cite{a1} computes unit similarity based on propensity scores. Instead, representation learning-based methods perform instance-level alignment in learned representation spaces. For example, SITE \cite{16} employed representation learning to capture instance-level variation by selecting specific sample pairs for alignment in the learned embedding space. Similarly, \citet{x8} designed a contrastive task for ITE estimation based on propensity score learning within a representation framework, regarding samples with propensity scores close to 0.5 as positive samples to learn balanced representations. FCCL \cite{zhangcounterfactual} further integrated diffeomorphic counterfactual generation and contrastive learning to address distribution shift through instance-level alignment in the representation space. However, these approaches only achieve partial balance and fail to effectively mitigate the distribution shift.

Compared with instance-level alignment methods, we not only account for individual heterogeneity by performing instance-to-prototype matching that preserves local structural information, but also achieve distributional balance across treatment groups through prototype-level alignment in the latent space, thereby enabling more robust and accurate ITE estimation.","Recently, numerous deep learning studies have analyzed the relationship between treatment and outcome at the individual level through mitigating distribution shift, which can be broadly categorized into distribution-level alignment and instance-level alignment methods.


\subsection{Distribution-level alignment}

Current distribution-level alignment methods aim to balance the distributions globally by learning first-order moments, primarily employing distance metrics, adversarial training, and optimal transport techniques. For example, \citet{15} developed TARNet / CFRNet to mitigate confounding bias by reducing the distribution divergence between treated and control groups in the representation space, adopting Maximum Mean Discrepancy (MMD) and Wasserstein distance. GANITE \cite{18} utilized adversarial training to make the discriminator unable to distinguish whether the input data come from the factual distribution or the generate counterfactual distribution. CBRE \cite{k4} introduced an information loop to preserve predictive information that might otherwise be lost during the raw-to-latent space transformation in adversarial training. Alternatively, optimal transport-based methods have also been explored, where \citet{7} reduces the balancing error under the framework of optimal transport with learnable marginal distributions and the cost function. Similarly, \citet{a6} proposed an estimator based on optimal transport to handle both mini-batch sampling effects and unobserved confounder effects issues.

While these methods focus on global distributional alignment, they often neglect the individual-level heterogeneity and intrinsic structure of data such as subgroup similarity or local clustering, which leads to less informative representations and compromises ITE estimation.


\subsection{Instance-level alignment}

Instance-level alignment methods work by matching similar units from different groups to construct locally balanced distributions. The propensity score matching \cite{a1} computes unit similarity based on propensity scores. Instead, representation learning-based methods perform instance-level alignment in learned representation spaces. For example, SITE \cite{16} employed representation learning to capture instance-level variation by selecting specific sample pairs for alignment in the learned embedding space. Similarly, \citet{x8} designed a contrastive task for ITE estimation based on propensity score learning within a representation framework, regarding samples with propensity scores close to 0.5 as positive samples to learn balanced representations. FCCL \cite{zhangcounterfactual} further integrated diffeomorphic counterfactual generation and contrastive learning to address distribution shift through instance-level alignment in the representation space. However, these approaches only achieve partial balance and fail to effectively mitigate the distribution shift.

Compared with instance-level alignment methods, we not only account for individual heterogeneity by performing instance-to-prototype matching that preserves local structural information, but also achieve distributional balance across treatment groups through prototype-level alignment in the latent space, thereby enabling more robust and accurate ITE estimation.","Recently, numerous deep learning studies have analyzed the
relationship between treatment and outcome at the individ-
ual level through mitigating distribution shift, which can
be broadly categorized into distribution-level alignment and
instance-level alignment methods.
Distribution-level alignment
Current distribution-level alignment methods aim to balance
the distributions globally by learning first-order moments,
primarily employing distance metrics, adversarial training,
and optimal transport techniques. For example, Shalit, Jo-
hansson, and Sontag (2017) developed TARNet / CFRNet
to mitigate confounding bias by reducing the distribution
divergence between treated and control groups in the rep-
resentation space, adopting Maximum Mean Discrepancy
(MMD) and Wasserstein distance. GANITE (Yoon, Jordon,
and Van Der Schaar 2018) utilized adversarial training to
make the discriminator unable to distinguish whether the in-
put data come from the factual distribution or the generate
counterfactual distribution. CBRE (Zhou et al. 2022b) in-
troduced an information loop to preserve predictive infor-
mation that might otherwise be lost during the raw-to-latent
space transformation in adversarial training. Alternatively,
optimal transport-based methods have also been explored,
where Yan et al. (2024) reduces the balancing error under
the framework of optimal transport with learnable marginal
distributions and the cost function. Similarly, Wang et al.
(2023) proposed an estimator based on optimal transport
to handle both mini-batch sampling effects and unobserved
confounder effects issues.
While these methods focus on global distributional align-
ment, they often neglect the individual-level heterogeneity
and intrinsic structure of data such as subgroup similarity or
local clustering, which leads to less informative representa-
tions and compromises ITE estimation.
Instance-level alignment
Instance-level alignment methods work by matching simi-
lar units from different groups to construct locally balanced
distributions. The propensity score matching (Rosenbaum
and Rubin 1983) computes unit similarity based on propen-
sity scores. Instead, representation learning-based methods
perform instance-level alignment in learned representation
spaces. For example, SITE (Yao et al. 2018) employed rep-
resentation learning to capture instance-level variation by se-
lecting specific sample pairs for alignment in the learned em-
bedding space. Similarly, Li and Yao (2022) designed a con-
trastive task for ITE estimation based on propensity score
learning within a representation framework, regarding sam-
ples with propensity scores close to 0.5 as positive samples
to learn balanced representations. FCCL (Zhang et al. 2025)
further integrated diffeomorphic counterfactual generation
and contrastive learning to address distribution shift through
instance-level alignment in the representation space. How-
ever, these approaches only achieve partial balance and fail
to effectively mitigate the distribution shift.
Compared with instance-level alignment methods, we not
only account for individual heterogeneity by performing
instance-to-prototype matching that preserves local struc-
tural information, but also achieve distributional balance
across treatment groups through prototype-level alignment
in the latent space, thereby enabling more robust and accu-
rate ITE estimation.
Preliminary
Following the Neyman-Rubin potential outcome framework
(Rubin 2005; Shalit, Johansson, and Sontag 2017), we for-
mally define the problem setup. LetX ⊂Rddenote the
d-dimensional covariate space,T={0,1}represent the bi-
nary treatment space, andY ⊂Rdenote the potential out-
come space. We assume the observed dataset containsnin-
dependent and identically distributed samples, represented
asD={x i, ti, yi}n
i="
2511.09143v2,http://arxiv.org/abs/2511.09143v2,2025-11-12 09:29:50+00:00,Flex-MIG: Enabling Distributed Execution on MIG,"GPU clusters in multi-tenant settings often suffer from underutilization, making GPU-sharing technologies essential for efficient resource use. Among them, NVIDIA Multi-Instance GPU (MIG) has gained traction for providing hardware-level isolation that enables concurrent workloads without interference. However, MIG's hardware rigidity and the conventional one-to-one allocation model jointly lead to severe fragmentation and cluster-wide underutilization. We present Flex-MIG, a software-only framework that replaces one-to-one with a one-to-many allocation model and enables host-shared-memory collectives across MIG instances without hardware modification. Flex-MIG eliminates drain-required reconfiguration, reduces fragmentation, and improves makespan by up to 17% across diverse traces, showing that rethinking MIG's operational model as a software-coordinated layer substantially improves cluster efficiency.","\label{sec:related_works}

\textbf{\textit{Software-level sharing.}}
Prior systems such as MPS\cite{nvidia-mps}, TGS\cite{TGS}, and Orion\cite{Orion} perform fine-grained multiplexing of GPU resources to improve utilization in multi-tenant settings.
While effective in increasing concurrency, these approaches rely on shared caches and memory bandwidth, resulting in inter-job interference and limited isolation.

\textbf{\textit{Dynamic composition atop MIG.}}
NVIDIA MIG~\cite{nvidia-mig} provides hardware-level isolation, yet its rigid predefined profiles and drain-required reconfiguration limit flexibility in multi-tenant clusters.
To further overcome MIG’s lack of elasticity, recent systems combine MIG with MPS to recover unused capacity: MIGER~\cite{MIGER} jointly decides MIG partition sizes, job co-location, and per-job MPS shares (i.e., many-to-one sharing within a MIG instance), whereas ParvaGPU~\cite{parva} explicitly enables MPS within each MIG instance and scales the number of identical processes to maximize intra-instance efficiency.

\textbf{\textit{Our position.}}
Unlike these approaches, Flex-MIG redefines the MIG operational model: it executes a single job across multiple fixed leaves, thereby avoiding drain-required reconfiguration while maintaining hardware isolation. This turns MIG from a static partitioner into a composable runtime resource layer.","\textbf{\textit{Software-level sharing.}}
Prior systems such as MPS\cite{nvidia-mps}, TGS\cite{TGS}, and Orion\cite{Orion} perform fine-grained multiplexing of GPU resources to improve utilization in multi-tenant settings.
While effective in increasing concurrency, these approaches rely on shared caches and memory bandwidth, resulting in inter-job interference and limited isolation.

\textbf{\textit{Dynamic composition atop MIG.}}
NVIDIA MIG~\cite{nvidia-mig} provides hardware-level isolation, yet its rigid predefined profiles and drain-required reconfiguration limit flexibility in multi-tenant clusters.
To further overcome MIG’s lack of elasticity, recent systems combine MIG with MPS to recover unused capacity: MIGER~\cite{MIGER} jointly decides MIG partition sizes, job co-location, and per-job MPS shares (i.e., many-to-one sharing within a MIG instance), whereas ParvaGPU~\cite{parva} explicitly enables MPS within each MIG instance and scales the number of identical processes to maximize intra-instance efficiency.

\textbf{\textit{Our position.}}
Unlike these approaches, Flex-MIG redefines the MIG operational model: it executes a single job across multiple fixed leaves, thereby avoiding drain-required reconfiguration while maintaining hardware isolation. This turns MIG from a static partitioner into a composable runtime resource layer.",
2511.09487v1,http://arxiv.org/abs/2511.09487v1,2025-11-12 17:00:21+00:00,PDAC: Efficient Coreset Selection for Continual Learning via Probability Density Awareness,"Rehearsal-based Continual Learning (CL) maintains a limited memory buffer to store replay samples for knowledge retention, making these approaches heavily reliant on the quality of the stored samples. Current Rehearsal-based CL methods typically construct the memory buffer by selecting a representative subset (referred to as coresets), aiming to approximate the training efficacy of the full dataset with minimal storage overhead. However, mainstream Coreset Selection (CS) methods generally formulate the CS problem as a bi-level optimization problem that relies on numerous inner and outer iterations to solve, leading to substantial computational cost thus limiting their practical efficiency. In this paper, we aim to provide a more efficient selection logic and scheme for coreset construction. To this end, we first analyze the Mean Squared Error (MSE) between the buffer-trained model and the Bayes-optimal model through the perspective of localized error decomposition to investigate the contribution of samples from different regions to MSE suppression. Further theoretical and experimental analyses demonstrate that samples with high probability density play a dominant role in error suppression. Inspired by this, we propose the Probability Density-Aware Coreset (PDAC) method. PDAC leverages the Projected Gaussian Mixture (PGM) model to estimate each sample's joint density, enabling efficient density-prioritized buffer selection. Finally, we introduce the streaming Expectation Maximization (EM) algorithm to enhance the adaptability of PGM parameters to streaming data, yielding Streaming PDAC (SPDAC) for streaming scenarios. Extensive comparative experiments show that our methods outperforms other baselines across various CL settings while ensuring favorable efficiency.","\subsection{Coreset Selection}
Coreset Selection (CS) algorithms aim to approximate the training performance on the original dataset based on a certain criterion (e.g., training loss, model gradients, etc.) by selecting a representative data subset. Early CS research primarily focused on various classical supervised learning algorithms, such as support vector machine \cite{tsang2005core} and logistic regression \cite{HugginsCB16,MunteanuSSW18}, as well as unsupervised algorithms like K-means \cite{Har-PeledM04,har2005smaller,FeldmanL11} and GMMs \cite{FeldmanFK11,lucic2018training}. Nguyen et al. \cite{NguyenLBT18} introduced a K-Center-based CS strategy \cite{gonzalez1985clustering} to CL problems, while Yoon et al. \cite{YoonMYH22} and Tiwari et al. \cite{TiwariKIS22} performed CS through model gradient matching. Borsos et al. \cite{BorsosM020} were the first to formulate the CS problem as a bilevel optimization problem, and subsequent work has focused on optimizing the modeling of the bilevel problem, such as introducing a probabilistic framework \cite{ZhouPZLCZ22} or incorporating loss regularization terms \cite{HaoJL23}. Although showing promising performance, solving bi-level optimization incurs excessive computational overhead and over-relies on the model’s current training state, making it vulnerable to fluctuations in training dynamics. In contrast, our approach directly guides sample-level evaluation and selection using adaptively updated sample statistics, eliminating the need for greedy optimization over candidate sets. Thus, it achieves both effectiveness and efficiency.

\subsection{Rehearsal-based Continual Learning}
Rehearsal-based CL methods maintain a limited memory buffer by selecting and storing samples from previous tasks and replaying them during subsequent task training to preserve the model's knowledge of the prior tasks. Classic methods are represented by random selection-based strategies \cite{chaudhry2019tiny,hayes2019memory}, while subsequent approaches further enhance the effectiveness of replay training by incorporating distillation loss \cite{RebuffiKSL17,li2023variational}, classifier output matching \cite{BuzzegaBPAC20,QiGCLLWZ25}, gradient episodic memory \cite{Lopez-PazR17,ChaudhryRRE19}. Among these, CS is widely used as a representative strategy to construct high-quality memory buffers.

\begin{table}[h]
\setlength{\tabcolsep}{3pt}
\centering
\caption{Symbol Specifications}
\label{symbols}
\begin{tabular}{lc} 
\toprule
Symbol & Description \\
\midrule
$\mathcal{X}$ & Input space \\
$\mathcal{Y}$ & Set of labels \\
$ \mathcal{Z}$ & Sample Space \\
$\bm{z}=\left(\bm{x},y\right)$ & A sample with $\bm{x}$ as input and $y$ as label \\
$\mathcal{T}^{t}$ & The $t$-th CL task \\
$\mathcal{S}^t$ & The training set corresponding to task $\mathcal{T}^{t}$\\
$\mathcal{Y}^{t}$ & The set of class labels included in task $\mathcal{T}^{t}$\\
$\mathcal{E}^{t}$ & The test set corresponding to task $\mathcal{T}^{t}$\\
$\bm{z}_{t,i}$ & The $i$-th sample of the $t$-th task's training set $\mathcal{S}^t$\\
$\mathcal{M}$ & Memory buffer \\
$N$ & The size of the memory buffer \\
$n_y$ & Number of training samples of class $y$\\
$p\left(\bm{z}\right)$ & Real probability density of sample $\bm{z}$\\
$\pi\left(z\right)$ & Resampling probability mass of sample $\bm{z}$\\
$f\left(\cdot\,;\bm{\theta}\right)$ & Parameterized learning model with parameter $\bm{\theta}$\\
$\Theta$ & The parameter space \\
$\sigma\left(\cdot\right)$ & Softmax activation function \\
$f^\sigma=\sigma\circ f$ & Model output after Softmax activation \\
${\bm{\hat\theta}}_{\mathcal{M}}$ & The ERM model trained on the memory buffer $\mathcal{M}$\\
$f^*\left(\cdot\right)$ & Bayes-optimal model\\
$\mathcal{R}_{\mathcal{M}\mid\mathcal{S}}$ & MSE between the buffer-trained model and $f^*\left(\cdot\right)$\\
$\mathcal{X}_i$ & The $i$-th local unit of the sample space $\mathcal{X}$\\
$\mathcal{Z}_i$ & The $i$-th local region of the sample space $\mathcal{Z}$\\
$m$ & The diameter of each local unit\\
$\mathrm{p}_i$ & Probability of a resampled point belonging to $\mathcal{Z}_i$\\
$l_i$ & Number of training samples in local region $\mathcal{Z}_i$\\
$\bm{W}_{y}^t$ & VMP projection matrix for class $y$ in task $\mathcal{T}^t$\\
$f_\psi\left(\cdot\,;\bm{\theta}\right)$ & Penultimate layer output of the model $f\left(\cdot\,;\bm{\theta}\right)$\\
$\bm{h}=f_\psi\left(\bm{x};\bm{\theta}\right)$ & Feature of sample $\bm{x}$ extracted by $f_\psi\left(\cdot\,;\bm{\theta}\right)$\\
$\bm{\xi}$ & The projected sample feature\\
$L$ & Number of Gaussian components in PGM\\
$\alpha_y^l$ & Weight of the $l$-th Gaussian component for class $y$\\
$\bm{\mu}_y^l$ & Mean vector of the $l$-th component within class $y$\\
$\bm{\Sigma}_{y}^l$ & Covariance matrix of the $l$-th component within class $y$\\
$\beta$ & Update step size in the streaming EM algorithm\\
$\mathcal{C}_y^t$ & Index set of samples belonging to class $y$ in task $\mathcal{T}^t$\\
$\mathcal{B}$ & Input batch\\
\bottomrule
\end{tabular}
\label{tab:symbol}
\end{table}","\subsection{Coreset Selection}
Coreset Selection (CS) algorithms aim to approximate the training performance on the original dataset based on a certain criterion (e.g., training loss, model gradients, etc.) by selecting a representative data subset. Early CS research primarily focused on various classical supervised learning algorithms, such as support vector machine \cite{tsang2005core} and logistic regression \cite{HugginsCB16,MunteanuSSW18}, as well as unsupervised algorithms like K-means \cite{Har-PeledM04,har2005smaller,FeldmanL11} and GMMs \cite{FeldmanFK11,lucic2018training}. Nguyen et al. \cite{NguyenLBT18} introduced a K-Center-based CS strategy \cite{gonzalez1985clustering} to CL problems, while Yoon et al. \cite{YoonMYH22} and Tiwari et al. \cite{TiwariKIS22} performed CS through model gradient matching. Borsos et al. \cite{BorsosM020} were the first to formulate the CS problem as a bilevel optimization problem, and subsequent work has focused on optimizing the modeling of the bilevel problem, such as introducing a probabilistic framework \cite{ZhouPZLCZ22} or incorporating loss regularization terms \cite{HaoJL23}. Although showing promising performance, solving bi-level optimization incurs excessive computational overhead and over-relies on the model’s current training state, making it vulnerable to fluctuations in training dynamics. In contrast, our approach directly guides sample-level evaluation and selection using adaptively updated sample statistics, eliminating the need for greedy optimization over candidate sets. Thus, it achieves both effectiveness and efficiency.

\subsection{Rehearsal-based Continual Learning}
Rehearsal-based CL methods maintain a limited memory buffer by selecting and storing samples from previous tasks and replaying them during subsequent task training to preserve the model's knowledge of the prior tasks. Classic methods are represented by random selection-based strategies \cite{chaudhry2019tiny,hayes2019memory}, while subsequent approaches further enhance the effectiveness of replay training by incorporating distillation loss \cite{RebuffiKSL17,li2023variational}, classifier output matching \cite{BuzzegaBPAC20,QiGCLLWZ25}, gradient episodic memory \cite{Lopez-PazR17,ChaudhryRRE19}. Among these, CS is widely used as a representative strategy to construct high-quality memory buffers.

\begin{table}[h]
\setlength{\tabcolsep}{3pt}
\centering
\caption{Symbol Specifications}
\begin{tabular}{lc} 
\toprule
Symbol & Description \\
\midrule
$\mathcal{X}$ & Input space \\
$\mathcal{Y}$ & Set of labels \\
$ \mathcal{Z}$ & Sample Space \\
$\bm{z}=\left(\bm{x},y\right)$ & A sample with $\bm{x}$ as input and $y$ as label \\
$\mathcal{T}^{t}$ & The $t$-th CL task \\
$\mathcal{S}^t$ & The training set corresponding to task $\mathcal{T}^{t}$\\
$\mathcal{Y}^{t}$ & The set of class labels included in task $\mathcal{T}^{t}$\\
$\mathcal{E}^{t}$ & The test set corresponding to task $\mathcal{T}^{t}$\\
$\bm{z}_{t,i}$ & The $i$-th sample of the $t$-th task's training set $\mathcal{S}^t$\\
$\mathcal{M}$ & Memory buffer \\
$N$ & The size of the memory buffer \\
$n_y$ & Number of training samples of class $y$\\
$p\left(\bm{z}\right)$ & Real probability density of sample $\bm{z}$\\
$\pi\left(z\right)$ & Resampling probability mass of sample $\bm{z}$\\
$f\left(\cdot\,;\bm{\theta}\right)$ & Parameterized learning model with parameter $\bm{\theta}$\\
$\Theta$ & The parameter space \\
$\sigma\left(\cdot\right)$ & Softmax activation function \\
$f^\sigma=\sigma\circ f$ & Model output after Softmax activation \\
${\bm{\hat\theta}}_{\mathcal{M}}$ & The ERM model trained on the memory buffer $\mathcal{M}$\\
$f^*\left(\cdot\right)$ & Bayes-optimal model\\
$\mathcal{R}_{\mathcal{M}\mid\mathcal{S}}$ & MSE between the buffer-trained model and $f^*\left(\cdot\right)$\\
$\mathcal{X}_i$ & The $i$-th local unit of the sample space $\mathcal{X}$\\
$\mathcal{Z}_i$ & The $i$-th local region of the sample space $\mathcal{Z}$\\
$m$ & The diameter of each local unit\\
$\mathrm{p}_i$ & Probability of a resampled point belonging to $\mathcal{Z}_i$\\
$l_i$ & Number of training samples in local region $\mathcal{Z}_i$\\
$\bm{W}_{y}^t$ & VMP projection matrix for class $y$ in task $\mathcal{T}^t$\\
$f_\psi\left(\cdot\,;\bm{\theta}\right)$ & Penultimate layer output of the model $f\left(\cdot\,;\bm{\theta}\right)$\\
$\bm{h}=f_\psi\left(\bm{x};\bm{\theta}\right)$ & Feature of sample $\bm{x}$ extracted by $f_\psi\left(\cdot\,;\bm{\theta}\right)$\\
$\bm{\xi}$ & The projected sample feature\\
$L$ & Number of Gaussian components in PGM\\
$\alpha_y^l$ & Weight of the $l$-th Gaussian component for class $y$\\
$\bm{\mu}_y^l$ & Mean vector of the $l$-th component within class $y$\\
$\bm{\Sigma}_{y}^l$ & Covariance matrix of the $l$-th component within class $y$\\
$\beta$ & Update step size in the streaming EM algorithm\\
$\mathcal{C}_y^t$ & Index set of samples belonging to class $y$ in task $\mathcal{T}^t$\\
$\mathcal{B}$ & Input batch\\
\bottomrule
\end{tabular}
\end{table}",
2511.10250v1,http://arxiv.org/abs/2511.10250v1,2025-11-13 12:29:39+00:00,FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment,"Action Quality Assessment (AQA) aims to evaluate and score sports actions, which has attracted widespread interest in recent years. Existing AQA methods primarily predict scores based on features extracted from the entire video, resulting in limited interpretability and reliability. Meanwhile, existing AQA datasets also lack fine-grained annotations for action scores, especially for deduction items and sub-score annotations. In this paper, we construct the first AQA dataset containing fine-grained sub-score and deduction annotations for aerial skiing, which will be released as a new benchmark. For the technical challenges, we propose a novel AQA method, named JudgeMind, which significantly enhances performance and reliability by simulating the judgment and scoring mindset of professional referees. Our method segments the input action video into different stages and scores each stage to enhance accuracy. Then, we propose a stage-aware feature enhancement and fusion module to boost the perception of stage-specific key regions and enhance the robustness to visual changes caused by frequent camera viewpoints switching. In addition, we propose a knowledge-based grade-aware decoder to incorporate possible deduction items as prior knowledge to predict more accurate and reliable scores. Experimental results demonstrate that our method achieves state-of-the-art performance.","\label{sec: related work}
\subsection{Action Quality Assessment}
\label{sec:sai}
Action Quality Assessment (AQA) methods are now widely applied in sports action evaluation~\cite{shao2020finegym,ramanathan2014human,kong2022human,sportshhi,sun2022human} and scoring~\cite{wang2021survey,zahan2024learning,limagr,gao2023automatic,zeng2024multimodal}. Early AQA methods~\cite{SVR,baller,pan2019action,venkataraman2015dynamical} are mainly based on handcrafted features to estimate video scores. More recently, various deep learning AQA models~\cite{ActionNet,gdlt,CoRe,cofinal,finediving,TPT,rica,finedving+} are proposed, using convolutional networks, graph networks, recurrent networks, and Transformers as backbone. Parmar~\etal~\cite{MIT} leverage spatiotemporal features to capture the dynamics of actions like diving, vault gymnastics, and figure skating. Xu~\etal~\cite{MS-LSTM} use self-attention mechanisms and multi-scale dilated convolutional LSTMs to aggregate information, achieving improved performance. Pan~\etal~\cite{HGCN} utilize graph neural networks to model joint relationships, providing a more accurate assessment of visual action performance. Meanwhile, some exemplar-based AQA methods are proposed to evaluate actions by comparing input videos with reference videos of the same action type and score. Tang~\etal~\cite{UNLV} introduce USDL, which learns uncertainty-aware score distributions to reduce the inherent ambiguity in human judges' scoring labels. Yu~\etal~\cite{CoRe} develop a contrastive regression framework (CoRe) based on video-level features, ranking videos and predicting accurate scores. GDLT~\cite{gdlt} adopted a Likert-scale approach for grading action quality, while CoFInAL~\cite{cofinal} further enhanced this method's performance and interpretability using a hierarchical approach. 

Although existing methods achieve favorable AQA performance, they do not fully utilize the judging rules and standards of action evaluation, resulting in poor robustness and reliability. In contrast, our proposed method simulates the mindset of human judges and integrates action code and deduction knowledge prior, enabling more accurate and interpretable action quality assessment.

\subsection{Spots Video Datasets}

Several sports video datasets are released for the AQA task. Pirsiavash~\etal~\cite{MIT} construct the first Olympic judging dataset, which included the MIT-Dive dataset and the MIT-Skate dataset, focusing on diving and figure skating. Based on this work, Xu~\etal~\cite{MS-LSTM} construct the UNLV Dive and UNLV Vault datasets, which consist of 370 diving videos and 176 gymnastics videos, respectively. Then, the AQA-7 dataset~\cite{action} is constructed, containing 7 types of sports and a total of 1,189 videos. MTL-AQA~\cite{MTL} is a larger AQA dataset with 1,412 videos across 16 different sports. Fis-V~\cite{MS-LSTM} is specifically developed for figure skating containing 500 videos. RG~\cite{ActionNet} focused on rhythmic gymnastics, containing 1,000 videos. More recently, Xu~\etal~\cite{finediving} construct the first fine-grained AQA dataset FineDiving, providing detailed sub-action annotations. 

However, the annotation of existing AQA datasets is still inadequate and lacks more detailed annotations of sub-scores and deduction items, which is necessary for accurate and reliable AQA.","\subsection{Action Quality Assessment}
Action Quality Assessment (AQA) methods are now widely applied in sports action evaluation~\cite{shao2020finegym,ramanathan2014human,kong2022human,sportshhi,sun2022human} and scoring~\cite{wang2021survey,zahan2024learning,limagr,gao2023automatic,zeng2024multimodal}. Early AQA methods~\cite{SVR,baller,pan2019action,venkataraman2015dynamical} are mainly based on handcrafted features to estimate video scores. More recently, various deep learning AQA models~\cite{ActionNet,gdlt,CoRe,cofinal,finediving,TPT,rica,finedving+} are proposed, using convolutional networks, graph networks, recurrent networks, and Transformers as backbone. Parmar~\etal~\cite{MIT} leverage spatiotemporal features to capture the dynamics of actions like diving, vault gymnastics, and figure skating. Xu~\etal~\cite{MS-LSTM} use self-attention mechanisms and multi-scale dilated convolutional LSTMs to aggregate information, achieving improved performance. Pan~\etal~\cite{HGCN} utilize graph neural networks to model joint relationships, providing a more accurate assessment of visual action performance. Meanwhile, some exemplar-based AQA methods are proposed to evaluate actions by comparing input videos with reference videos of the same action type and score. Tang~\etal~\cite{UNLV} introduce USDL, which learns uncertainty-aware score distributions to reduce the inherent ambiguity in human judges' scoring labels. Yu~\etal~\cite{CoRe} develop a contrastive regression framework (CoRe) based on video-level features, ranking videos and predicting accurate scores. GDLT~\cite{gdlt} adopted a Likert-scale approach for grading action quality, while CoFInAL~\cite{cofinal} further enhanced this method's performance and interpretability using a hierarchical approach. 

Although existing methods achieve favorable AQA performance, they do not fully utilize the judging rules and standards of action evaluation, resulting in poor robustness and reliability. In contrast, our proposed method simulates the mindset of human judges and integrates action code and deduction knowledge prior, enabling more accurate and interpretable action quality assessment.

\subsection{Spots Video Datasets}

Several sports video datasets are released for the AQA task. Pirsiavash~\etal~\cite{MIT} construct the first Olympic judging dataset, which included the MIT-Dive dataset and the MIT-Skate dataset, focusing on diving and figure skating. Based on this work, Xu~\etal~\cite{MS-LSTM} construct the UNLV Dive and UNLV Vault datasets, which consist of 370 diving videos and 176 gymnastics videos, respectively. Then, the AQA-7 dataset~\cite{action} is constructed, containing 7 types of sports and a total of 1,189 videos. MTL-AQA~\cite{MTL} is a larger AQA dataset with 1,412 videos across 16 different sports. Fis-V~\cite{MS-LSTM} is specifically developed for figure skating containing 500 videos. RG~\cite{ActionNet} focused on rhythmic gymnastics, containing 1,000 videos. More recently, Xu~\etal~\cite{finediving} construct the first fine-grained AQA dataset FineDiving, providing detailed sub-action annotations. 

However, the annotation of existing AQA datasets is still inadequate and lacks more detailed annotations of sub-scores and deduction items, which is necessary for accurate and reliable AQA.","2.1 Action Quality Assessment
Action Quality Assessment (AQA) methods are
now widely applied in sports action evalua-
tion [14–18] and scoring [19–23]. Early AQA meth-
ods [7, 24–26] are mainly based on handcrafted
features to estimate video scores. More recently,
various deep learning AQA models [1–5, 8, 27, 28]
are proposed, using convolutional networks, graph
networks, recurrent networks, and Transformers
as backbone. Parmaret al. [29] leverage spa-
tiotemporal features to capture the dynamics of
actions like diving, vault gymnastics, and figure
skating. Xuet al. [6] use self-attention mechanisms
and multi-scale dilated convolutional LSTMs to
aggregate information, achieving improved per-
formance. Panet al. [30] utilize graph neural
networks to model joint relationships, providing
a more accurate assessment of visual action per-
formance. Meanwhile, some exemplar-based AQA
methods are proposed to evaluate actions by com-
paring input videos with reference videos of the
same action type and score. Tanget al. [31]
1Our code and dataset will be released after acceptance.introduce USDL, which learns uncertainty-aware
score distributions to reduce the inherent ambi-
guity in human judges’ scoring labels. Yuet
al. [3] develop a contrastive regression framework
(CoRe) based on video-level features, ranking
videos and predicting accurate scores. GDLT [4]
adopted a Likert-scale approach for grading action
quality, while CoFInAL [2] further enhanced this
method’s performance and interpretability using
a hierarchical approach.
Although existing methods achieve favorable
AQA performance, they do not fully utilize the
judging rules and standards of action evaluation,
resulting in poor robustness and reliability. In con-
trast, our proposed method simulates the mindset
of human judges and integrates action code and
deduction knowledge prior, enabling more accu-
rate and interpretable action quality assessment.
2.2 Spots Video Datasets
Several sports video datasets are released for the
AQA task. Pirsiavashet al. [29] construct the
first Olympic judging dataset, which included the
MIT-Dive dataset and the MIT-Skate dataset,
focusing on diving and figure skating. Based on
this work, Xuet al. [6] construct the UNLV
Dive and UNLV Vault datasets, which consist
of 370 diving videos and 176 gymnastics videos,
respectively. Then, the AQA-7 dataset [32] is
constructed, containing 7 types of sports and a
total of 1,189 videos. MTL-AQA [33] is a larger
AQA dataset with 1,412 videos across 16 different
sports. Fis-V [6] is specifically developed for figure
skating containing 500 videos. RG [1] focused
on rhythmic gymnastics, containing 1,000 videos.
More recently, Xuet al. [5] construct the first
fine-grained AQA dataset FineDiving, providing
detailed sub-action annotations.
However, the annotation of existing AQA
datasets is still inadequate and lacks more detailed
annotations of sub-scores and deduction items,
which is necessary for accurate and reliable AQA."
2511.09585v1,http://arxiv.org/abs/2511.09585v1,2025-11-12 11:38:39+00:00,"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation","Video-to-Music generation seeks to generate musically appropriate background music that enhances audiovisual immersion for videos. However, current approaches suffer from two critical limitations: 1) incomplete representation of video details, leading to weak alignment, and 2) inadequate temporal and rhythmic correspondence, particularly in achieving precise beat synchronization. To address the challenges, we propose Video Echoed in Music (VeM), a latent music diffusion that generates high-quality soundtracks with semantic, temporal, and rhythmic alignment for input videos. To capture video details comprehensively, VeM employs a hierarchical video parsing that acts as a music conductor, orchestrating multi-level information across modalities. Modality-specific encoders, coupled with a storyboard-guided cross-attention mechanism (SG-CAtt), integrate semantic cues while maintaining temporal coherence through position and duration encoding. For rhythmic precision, the frame-level transition-beat aligner and adapter (TB-As) dynamically synchronize visual scene transitions with music beats. We further contribute a novel video-music paired dataset sourced from e-commerce advertisements and video-sharing platforms, which imposes stricter transition-beat synchronization requirements. Meanwhile, we introduce novel metrics tailored to the task. Experimental results demonstrate superiority, particularly in semantic relevance and rhythmic precision.","\begin{figure*}[ht]
\centering
\includegraphics[width=\linewidth]{figures/framework-final.pdf}
\caption{\textbf{Illustration of the proposed method.} The \textbf{hierarchical video parsing} provides a comprehensive analysis across three levels. Cross-modal features are captured by \textbf{modality-specific encoders}, facilitating the semantic and temporal alignment by integrating global and storyboard details into the generative latent via \textbf{storyboard-guided cross-attention}. The frame-level \textbf{transition-beat aligner and adapter} ensure precise rhythmic synchronization by coupling video scene transitions with detected music beats and adapting to the music latent.}
\label{fig:fig2}
\end{figure*}


\subsection{Diffusion-Based Conditional Music Generation}

Recent advances in diffusion models have demonstrated potential for conditional music generation. Riffusion \cite{forsgren6riffusion}, Noise2Music \cite{huang2023noise2music}, and Mo\^{u}sai \cite{schneider2023mo} have pioneered open-domain text-to-music generation by diffusion models. AudioLDM2 \cite{liu2024audioldm} facilitates holistic audio generation, including music, through self-supervised pretraining. DITTO \cite{novack2024ditto} leverages distilled diffusion inference-time T-optimization for enhanced generation. Mustango \cite{melechovsky2024mustango} and Music ControlNet \cite{wu2024music} apply various time-varying musical constraints (e.g., chords, rhythms), while MusicMagus \cite{zhang2024MusicMagus} and SteerMusic \cite{niu2025steermusic} explore zero-shot music editing via diffusion. These developments underscore the effectiveness of diffusion models for conditional music generation. Building upon the foundations, we present VeM that extends latent diffusion to video-to-music while retaining the controllability benefits established in conditional music generation.
 

\subsection{Video-to-Music Generation}
Current approaches for video-to-music alignment employ diverse strategies. The first method, CMT \cite{di2021video} and subsequent approaches \cite{app12105050,zhuo2023video,yu2023long,KANG2024123640,qi2024harmonizing} project disentangled visual features (RGB, saliency, motion) onto musical attributes (melody, chord, rhythm), failing to capture visual semantics. Large Language Model-based techniques \cite{liu2023m,xu2024mozart,tong2024video,wang2024multimodal,zhou2025harmonyset} leverage textual representations. Specifically, M$^2$UGen \cite{liu2023m} focuses on textual music understanding, while SONIQUE \cite{zhang2025sonique} extracts musical tags from unpaired data. AudioX \cite{tian2025audiox} combines visual, textual, and audio features to a multimodal condition. However, textual abstraction inherently loses fine-grained temporal dynamics. Motion-centric methods, such as V2Meow \cite{su2024v2meow}, FilmComposer \cite{xie2025filmcomposer}, and VMAS \cite{lin2024vmas}, achieve movement alignment but neglect broader domains. VidMuse \cite{tian2024vidmuse} involves long-short-term temporal dependencies, but suffers from limited generative capacity. Diff-BGM \cite{li2024diff} addresses clip-level alignment, but only partially adapts to semantic shifts. Recent approaches, MuVi \cite{li2024muvivideotomusicgenerationsemantic}, VidMusician \cite{li2024vidmusician}, and GVMGen \cite{zuo2025gvmgen}, improve local semantic correspondence that involves temporal dynamics but lack explicit temporal position and duration encoding, preventing precise frame-level synchronization. Therefore, substantial opportunities remain for advancing semantic, temporal, and rhythmic alignment in video-to-music generation.","\subsection{Diffusion-Based Conditional Music Generation}

Recent advances in diffusion models have demonstrated potential for conditional music generation. Riffusion \cite{forsgren6riffusion}, Noise2Music \cite{huang2023noise2music}, and Mo\^{u}sai \cite{schneider2023mo} have pioneered open-domain text-to-music generation by diffusion models. AudioLDM2 \cite{liu2024audioldm} facilitates holistic audio generation, including music, through self-supervised pretraining. DITTO \cite{novack2024ditto} leverages distilled diffusion inference-time T-optimization for enhanced generation. Mustango \cite{melechovsky2024mustango} and Music ControlNet \cite{wu2024music} apply various time-varying musical constraints (e.g., chords, rhythms), while MusicMagus \cite{zhang2024MusicMagus} and SteerMusic \cite{niu2025steermusic} explore zero-shot music editing via diffusion. These developments underscore the effectiveness of diffusion models for conditional music generation. Building upon the foundations, we present VeM that extends latent diffusion to video-to-music while retaining the controllability benefits established in conditional music generation.
 

\subsection{Video-to-Music Generation}
Current approaches for video-to-music alignment employ diverse strategies. The first method, CMT \cite{di2021video} and subsequent approaches \cite{app12105050,zhuo2023video,yu2023long,KANG2024123640,qi2024harmonizing} project disentangled visual features (RGB, saliency, motion) onto musical attributes (melody, chord, rhythm), failing to capture visual semantics. Large Language Model-based techniques \cite{liu2023m,xu2024mozart,tong2024video,wang2024multimodal,zhou2025harmonyset} leverage textual representations. Specifically, M$^2$UGen \cite{liu2023m} focuses on textual music understanding, while SONIQUE \cite{zhang2025sonique} extracts musical tags from unpaired data. AudioX \cite{tian2025audiox} combines visual, textual, and audio features to a multimodal condition. However, textual abstraction inherently loses fine-grained temporal dynamics. Motion-centric methods, such as V2Meow \cite{su2024v2meow}, FilmComposer \cite{xie2025filmcomposer}, and VMAS \cite{lin2024vmas}, achieve movement alignment but neglect broader domains. VidMuse \cite{tian2024vidmuse} involves long-short-term temporal dependencies, but suffers from limited generative capacity. Diff-BGM \cite{li2024diff} addresses clip-level alignment, but only partially adapts to semantic shifts. Recent approaches, MuVi \cite{li2024muvivideotomusicgenerationsemantic}, VidMusician \cite{li2024vidmusician}, and GVMGen \cite{zuo2025gvmgen}, improve local semantic correspondence that involves temporal dynamics but lack explicit temporal position and duration encoding, preventing precise frame-level synchronization. Therefore, substantial opportunities remain for advancing semantic, temporal, and rhythmic alignment in video-to-music generation.","Diffusion-Based Conditional Music Generation
Recent advances in diffusion models have demonstrated po-
tential for conditional music generation. Riffusion (Forsgrenand Martiros 2022), Noise2Music (Huang et al. 2023b), and
Moˆusai (Schneider et al. 2023) have pioneered open-domain
text-to-music generation by diffusion models. AudioLDM2
(Liu et al. 2024) facilitates holistic audio generation, includ-
ing music, through self-supervised pretraining. DITTO (No-
vack et al. 2024) leverages distilled diffusion inference-time
T-optimization for enhanced generation. Mustango (Mele-
chovsky et al. 2024) and Music ControlNet (Wu et al. 2024)
apply various time-varying musical constraints (e.g., chords,
rhythms), while MusicMagus (Zhang et al. 2024) and Steer-
Music (Niu et al. 2025) explore zero-shot music editing via
diffusion. These developments underscore the effectiveness
of diffusion models for conditional music generation. Build-
ing upon the foundations, we present VeM that extends la-
tent diffusion to video-to-music while retaining the control-
lability benefits established in conditional music generation.
Video-to-Music Generation
Current approaches for video-to-music alignment employ
diverse strategies. The first method, CMT (Di et al. 2021)
and subsequent approaches (Yang, Yu, and Wu 2022; Zhuo
et al. 2023; Yu et al. 2023; Kang, Poria, and Herremans
2024; Qi, Ni, and Xu 2024) project disentangled visual
features (RGB, saliency, motion) onto musical attributes
(melody, chord, rhythm), failing to capture visual seman-
tics. Large Language Model-based techniques (Liu et al.
2023; Xu et al. 2024; Tong et al. 2024; Wang et al. 2024a;
Zhou et al. 2025) leverage textual representations. Specifi-
cally, M2UGen (Liu et al. 2023) focuses on textual music
understanding, while SONIQUE (Zhang and Fuentes 2025)
extracts musical tags from unpaired data. AudioX (Tian et al.
2025b) combines visual, textual, and audio features to a
multimodal condition. However, textual abstraction inher-
ently loses fine-grained temporal dynamics. Motion-centric
methods, such as V2Meow (Su et al. 2024), FilmComposer
(Xie et al. 2025), and VMAS (Lin et al. 2024), achieve
movement alignment but neglect broader domains. VidMuse
(Tian et al. 2024) involves long-short-term temporal depen-
dencies, but suffers from limited generative capacity. Diff-
BGM (Li et al. 2024c) addresses clip-level alignment, but
only partially adapts to semantic shifts. Recent approaches,
MuVi (Li et al. 2024a), VidMusician (Li et al. 2024d), and
GVMGen (Zuo et al. 2025), improve local semantic corre-
spondence that involves temporal dynamics but lack explicit
temporal position and duration encoding, preventing precise
frame-level synchronization. Therefore, substantial opportu-
nities remain for advancing semantic, temporal, and rhyth-
mic alignment in video-to-music generation."
2511.08660v1,http://arxiv.org/abs/2511.08660v1,2025-11-11 15:46:14+00:00,Binary and Multiclass Cyberattack Classification on GeNIS Dataset,"The integration of Artificial Intelligence (AI) in Network Intrusion Detection Systems (NIDS) is a promising approach to tackle the increasing sophistication of cyberattacks. However, since Machine Learning (ML) and Deep Learning (DL) models rely heavily on the quality of their training data, the lack of diverse and up-to-date datasets hinders their generalization capability to detect malicious activity in previously unseen network traffic. This study presents an experimental validation of the reliability of the GeNIS dataset for AI-based NIDS, to serve as a baseline for future benchmarks. Five feature selection methods, Information Gain, Chi-Squared Test, Recursive Feature Elimination, Mean Absolute Deviation, and Dispersion Ratio, were combined to identify the most relevant features of GeNIS and reduce its dimensionality, enabling a more computationally efficient detection. Three decision tree ensembles and two deep neural networks were trained for both binary and multiclass classification tasks. All models reached high accuracy and F1-scores, and the ML ensembles achieved slightly better generalization while remaining more efficient than DL models. Overall, the obtained results indicate that the GeNIS dataset supports intelligent intrusion detection and cyberattack classification with time-based and quantity-based behavioral features.","\label{related_work}
%mencionar os outros datasets, têm problemas de compatibilidade (usar o exemplo do CICIDS que há várias tentativas de correção), há esforços para uniformizar a ferramenta (a HERA), e por causa disto desenvolvemos previamente um dataset modular etc etc (o GENIS)
%para validar que o GENIS é útil e aplicável para real-world networks, é importante analisá-lo e validar modelos ML. resultados dos modelos noutros datasets

To protect organizational networks from malicious actors and develop robust NIDS that can secure critical assets, numerous studies have produced datasets that are commonly used to train AI models, particularly ML and DL approaches, to identify malicious patterns in network traffic. Creating such datasets requires several key steps, including collecting, extracting, and analyzing traffic data, which can be done using integrated solutions or a combination of specialized tools. However, inconsistencies and incompatibilities among these tools can lead to discrepancies. Different tools may extract different values for the same feature from the same raw data, which can introduce errors into the final dataset \cite{PINTO2025111177}.

A prominent example is the CICIDS2017 dataset \cite{cicids}, which is widely used in benchmarking studies \cite{benchmark_cicids2017,benchmark_cicids2017_2}. Due to its popularity, several researchers have examined this dataset and identified significant inaccuracies, prompting the release of corrected versions. Specifically, Engelen et al. \cite{engelen} and Liu et al. \cite{engelen_2} discovered multiple issues in the dataset’s creation process. They reported a misimplementation of the DoS Hulk attack and identified flaws in CICFlowMeter\footnote{\url{https://www.unb.ca/cic/research/applications.html}}, the tool used for flow generation. These flaws included errors in feature extraction and a fundamental misunderstanding of the Transmission Control Protocol (TCP). The dataset also suffered from label inaccuracies and corrupted attack samples.

Further analysis by Rosay et al. \cite{rosay} revealed issues in the CSV files provided by the dataset authors. These issues were not present in the raw traffic captures, suggesting that they originated from CICFlowMeter. Consequently, concerns have been raised that other datasets generated with the same tool may also contain similar flaws. Later, Lanvin et al. \cite{Lanvin} identified even more issues than previously reported, including incorrectly labeled port scan attacks and duplicated traffic, which make feature extraction and labeling more difficult.

A more recent dataset, HIKARI2021 \cite{hikari} , was developed using the same set of features as CICIDS2017, but it employed Zeek\footnote{\url{https://zeek.org/}} as the flow exporter. As with CICIDS2017, researchers have identified problematic features in HIKARI2021 that negatively impact the performance of ML models trained on the dataset. These problematic features are believed to act as record identifiers or reflect indexing errors introduced by the processing software \cite{fernandes}. The dataset was originally released with four category labels: Benign, BruteForce, Probing, and CryptoMiner. However, the authors subsequently released a newer version of the dataset \cite{new_hikari}, which included two additional PCAP files containing attacks already present in the earlier version, along with a revised labeling scheme. Notably, the Probing class disappeared in the updated version \cite{hikari_22_analysis}.

Recent efforts have focused on standardizing the dataset creation process for NIDS by introducing the all-in-one HERA flow exporter. HERA was developed in response to issues identified with CICFlowMeter, which has been linked to errors in several popular datasets, being able to generate flow records and extract relevant features consistently and reliably \cite{Hera2025}.

Following the creation of HERA and since prior datasets offer so many challenges, the GeNIS dataset was created specifically for AI-based NIDS applications. GeNIS is a modular dataset designed specifically for AI-based NIDS applications. With its focus on reflecting the typical user behavior, services, and network protocols of small to medium sized organizations, GeNIS aims to enhance the detection capabilities of these systems. This dataset offers a diverse range of benign and malicious traffic types, and its modular design allows organizations to tailor the dataset to their unique network scenarios, either by selecting specific subsets for targeted protection or by using the full dataset for broader, general-purpose defense.

To evaluate the real-world effectiveness and practicality of GeNIS in organizational networks, it is essential to validate the performance of AI models trained on it. Previous studies have used datasets such as CICIDS2017 to explore various deep learning models, including Deep Neural Networks, LSTM networks, and Convolutional Neural Networks, while reporting a binary classification performance accuracy of over 94\% using these models \cite{lstm_rw}. Similarly, LSTM models have demonstrated strong results on older datasets, such as KDD99, achieving over 95\% accuracy and F1-scores for binary classification across different model configurations, both with and without feature selection. For multiclass classification, the performance remained high with F1-scores above 91\% \cite{lstm_rw2}.

Studies using MLP architectures on datasets such as UNSW-NB15 have also achieved strong performance in multiclass settings, reporting weighted average F1-scores and accuracies around 82\%. These models were further improved by applying feature selection techniques \cite{mlp}. Similarly, tree-based ensemble models, such as LGBM, XGB, and RF, have demonstrated effective performance in binary classification tasks. In recent studies, these models consistently achieve F1-scores and accuracy above 80\%, both with the full feature set and after dimensionality reduction through feature selection \cite{arvores,arvores2}.

Overall, recent studies on network intrusion detection have achieved promising results with both traditional, tree-based ensemble models and more complex, DL neural network architectures. These models tend to maintain or enhance their performance when trained on reduced feature subsets, which highlights the effectiveness of feature selection techniques. However, due to significant flaws discovered in widely used datasets, it is crucial to evaluate these models' and feature selection methods' performance on more recent, reliable datasets free from such anomalies.

% e por causa disto desenvolvemos previamente um dataset modular etc etc (o GENIS)
%para validar que o GENIS é útil e aplicável para real-world networks, é importante analisá-lo e validar modelos ML. resultados dos modelos noutros datasets","To protect organizational networks from malicious actors and develop robust NIDS that can secure critical assets, numerous studies have produced datasets that are commonly used to train AI models, particularly ML and DL approaches, to identify malicious patterns in network traffic. Creating such datasets requires several key steps, including collecting, extracting, and analyzing traffic data, which can be done using integrated solutions or a combination of specialized tools. However, inconsistencies and incompatibilities among these tools can lead to discrepancies. Different tools may extract different values for the same feature from the same raw data, which can introduce errors into the final dataset \cite{PINTO2025111177}.

A prominent example is the CICIDS2017 dataset \cite{cicids}, which is widely used in benchmarking studies \cite{benchmark_cicids2017,benchmark_cicids2017_2}. Due to its popularity, several researchers have examined this dataset and identified significant inaccuracies, prompting the release of corrected versions. Specifically, Engelen et al. \cite{engelen} and Liu et al. \cite{engelen_2} discovered multiple issues in the dataset’s creation process. They reported a misimplementation of the DoS Hulk attack and identified flaws in CICFlowMeter\footnote{\url{https://www.unb.ca/cic/research/applications.html}}, the tool used for flow generation. These flaws included errors in feature extraction and a fundamental misunderstanding of the Transmission Control Protocol (TCP). The dataset also suffered from label inaccuracies and corrupted attack samples.

Further analysis by Rosay et al. \cite{rosay} revealed issues in the CSV files provided by the dataset authors. These issues were not present in the raw traffic captures, suggesting that they originated from CICFlowMeter. Consequently, concerns have been raised that other datasets generated with the same tool may also contain similar flaws. Later, Lanvin et al. \cite{Lanvin} identified even more issues than previously reported, including incorrectly labeled port scan attacks and duplicated traffic, which make feature extraction and labeling more difficult.

A more recent dataset, HIKARI2021 \cite{hikari} , was developed using the same set of features as CICIDS2017, but it employed Zeek\footnote{\url{https://zeek.org/}} as the flow exporter. As with CICIDS2017, researchers have identified problematic features in HIKARI2021 that negatively impact the performance of ML models trained on the dataset. These problematic features are believed to act as record identifiers or reflect indexing errors introduced by the processing software \cite{fernandes}. The dataset was originally released with four category labels: Benign, BruteForce, Probing, and CryptoMiner. However, the authors subsequently released a newer version of the dataset \cite{new_hikari}, which included two additional PCAP files containing attacks already present in the earlier version, along with a revised labeling scheme. Notably, the Probing class disappeared in the updated version \cite{hikari_22_analysis}.

Recent efforts have focused on standardizing the dataset creation process for NIDS by introducing the all-in-one HERA flow exporter. HERA was developed in response to issues identified with CICFlowMeter, which has been linked to errors in several popular datasets, being able to generate flow records and extract relevant features consistently and reliably \cite{Hera2025}.

Following the creation of HERA and since prior datasets offer so many challenges, the GeNIS dataset was created specifically for AI-based NIDS applications. GeNIS is a modular dataset designed specifically for AI-based NIDS applications. With its focus on reflecting the typical user behavior, services, and network protocols of small to medium sized organizations, GeNIS aims to enhance the detection capabilities of these systems. This dataset offers a diverse range of benign and malicious traffic types, and its modular design allows organizations to tailor the dataset to their unique network scenarios, either by selecting specific subsets for targeted protection or by using the full dataset for broader, general-purpose defense.

To evaluate the real-world effectiveness and practicality of GeNIS in organizational networks, it is essential to validate the performance of AI models trained on it. Previous studies have used datasets such as CICIDS2017 to explore various deep learning models, including Deep Neural Networks, LSTM networks, and Convolutional Neural Networks, while reporting a binary classification performance accuracy of over 94\

Studies using MLP architectures on datasets such as UNSW-NB15 have also achieved strong performance in multiclass settings, reporting weighted average F1-scores and accuracies around 82\

Overall, recent studies on network intrusion detection have achieved promising results with both traditional, tree-based ensemble models and more complex, DL neural network architectures. These models tend to maintain or enhance their performance when trained on reduced feature subsets, which highlights the effectiveness of feature selection techniques. However, due to significant flaws discovered in widely used datasets, it is crucial to evaluate these models' and feature selection methods' performance on more recent, reliable datasets free from such anomalies.","To protect organizational networks from malicious actors and develop robust
NIDS that can secure critical assets, numerous studies have produced datasets
thatarecommonlyusedtotrainAImodels,particularlyMLandDLapproaches,
to identify malicious patterns in network traffic. Creating such datasets requires
several key steps, including collecting, extracting, and analyzing traffic data,
which can be done using integrated solutions or a combination of specialized
tools. However, inconsistencies and incompatibilities among these tools can lead
Binary and Multiclass Cyberattack Classification on GeNIS Dataset 3
to discrepancies. Different tools may extract different values for the same feature
from the same raw data, which can introduce errors into the final dataset [22].
A prominent example is the CICIDS2017 dataset [30], which is widely used in
benchmarking studies [2,20]. Due to its popularity, several researchers have ex-
amined this dataset and identified significant inaccuracies, prompting the release
of corrected versions. Specifically, Engelen et al. [4] and Liu et al. [17] discovered
multiple issues in the dataset’s creation process. They reported a misimplemen-
tation of the DoS Hulk attack and identified flaws in CICFlowMeter1, the tool
used for flow generation. These flaws included errors in feature extraction and
a fundamental misunderstanding of the Transmission Control Protocol (TCP).
The dataset also suffered from label inaccuracies and corrupted attack samples.
Further analysis by Rosay et al. [26] revealed issues in the CSV files pro-
vided by the dataset authors. These issues were not present in the raw traffic
captures, suggesting that they originated from CICFlowMeter. Consequently,
concerns have been raised that other datasets generated with the same tool may
also contain similar flaws. Later, Lanvin et al. [16] identified even more issues
than previously reported, including incorrectly labeled port scan attacks and
duplicated traffic, which make feature extraction and labeling more difficult.
A more recent dataset, HIKARI2021 [6] , was developed using the same set of
features as CICIDS2017, but it employed Zeek2as the flow exporter. As with CI-
CIDS2017, researchers have identified problematic features in HIKARI2021 that
negatively impact the performance of ML models trained on the dataset. These
problematic features are believed to act as record identifiers or reflect indexing
errors introduced by the processing software [5]. The dataset was originally re-
leasedwithfourcategorylabels:Benign,BruteForce,Probing,andCryptoMiner.
However, the authors subsequently released a newer version of the dataset [7],
which included two additional PCAP files containing attacks already present in
the earlier version, along with a revised labeling scheme. Notably, the Probing
class disappeared in the updated version [24].
Recent efforts have focused on standardizing the dataset creation process for
NIDS by introducing the all-in-one HERA flow exporter. HERA was developed
in response to issues identified with CICFlowMeter, which has been linked to
errorsinseveralpopulardatasets,beingabletogenerateflowrecordsandextract
relevant features consistently and reliably [21].
Following the creation of HERA and since prior datasets offer so many chal-
lenges, the GeNIS dataset was created specifically for AI-based NIDS appli-
cations. GeNIS is a modular dataset designed specifically for AI-based NIDS
applications. With its focus on reflecting the typical user behavior, services, and
network protocols of small to medium sized organizations, GeNIS aims to en-
hance the detection capabilities of these systems. This dataset offers a diverse
range of benign and malicious traffic types, and its modular design allows or-
ganizations to tailor the dataset to their unique network scenarios, either by
1https://www.unb.ca/cic/research/applications.html
2https://zeek.org/
4 M. Silva et al.
selecting specific subsets for targeted protection or by using the full dataset for
broader, general-purpose defense.
To evaluate the real-world effectiveness and practicality of GeNIS in organi-
zationalnetworks,itisessentialtovalidatetheperformanceofAImodelstrained
on it. Previous studies have used datasets such as CICIDS2017 to explore var-
ious deep learning models, including Deep Neural Networks, LSTM networks,
and Convolutional Neural Networks, while reporting a binary classification per-
formance accuracy of over 94% using these models [12]. Similarly, LSTM models
have demonstrated strong results on older datasets, such as KDD99, achieving
over 95% accuracy and F1-scores for binary classification across different model
configurations, both with and without feature selection. For multiclass classifi-
cation, the performance remained high with F1-scores above 91% [15].
Studies using MLP architectures on datasets such as UNSW-NB15 have also
achieved strong performance in multiclass settings, reporting weighted average
F1-scores and accuracies around 82%. These models were further improved by
applying feature selection techniques [34]. Similarly, tree-based ensemble mod-
els, such as LGBM, XGB, and RF, have demonstrated effective performance in
binary classification tasks. In recent studies, these models consistently achieve
F1-scores and accuracy above 80%, both with the full feature set and after di-
mensionality reduction through feature selection [32,33].
Overall, recent studies on network intrusion detection have achieved promis-
ing results with both traditional, tree-based ensemble models and more complex,
DL neural network architectures. These models tend to maintain or enhance
their performance when trained on reduced feature subsets, which highlights the
effectiveness of feature selection techniques. However, due to significant flaws
discovered in widely used datasets, it is crucial to evaluate these models’ and
feature selection methods’ performance on more recent, reliable datasets free
from such anomalies."
2511.08238v2,http://arxiv.org/abs/2511.08238v2,2025-11-11 13:37:13+00:00,Remodeling Semantic Relationships in Vision-Language Fine-Tuning,"Vision-language fine-tuning has emerged as an efficient paradigm for constructing multimodal foundation models. While textual context often highlights semantic relationships within an image, existing fine-tuning methods typically overlook this information when aligning vision and language, thus leading to suboptimal performance. Toward solving this problem, we propose a method that can improve multimodal alignment and fusion based on both semantics and relationships.Specifically, we first extract multilevel semantic features from different vision encoder to capture more visual cues of the relationships. Then, we learn to project the vision features to group related semantics, among which are more likely to have relationships. Finally, we fuse the visual features with the textual by using inheritable cross-attention, where we globally remove the redundant visual relationships by discarding visual-language feature pairs with low correlation. We evaluate our proposed method on eight foundation models and two downstream tasks, visual question answering and image captioning, and show that it outperforms all existing methods.","\subsection{Parameter-Efficient Fine-Tuning}

Compared to traditional fine-tuning methods, PEFT significantly reduces the number of parameters to be updated. PEFT methods can be categorized based on their impact on the model:
1) Methods acting directly on the language model, which adjust model parameters or structure to modify the knowledge stored in the model. Key methods include BitFit \cite{zaken2021bitfit}, Adapter \cite{houlsby2019parameter}, and LoRA \cite{hu2021lora}. BitFit trains only the bias values in pre-trained models, minimizing trainable parameters.
2) Methods acting on language model inputs or intermediate-layer inputs, such as Prefix-Tuning \cite{li2021prefix} and Prompt-Tuning \cite{lester2021power}, which modify input representations. 

Many PEFT techniques designed for language models can be applied to multimodal architectures. For instance, Adversarial DuAl Prompt Tuning (ADAPT) \cite{10901852} achieves efficient Unsupervised Domain Adaptation (UDA) through domain alignment via adversarial fine-tuning of both textual and visual prompts. MetaPrompt \cite{10431687} enhances the model's domain generalization capability through a dual-modality prompt tuning network and an alternating episodic training algorithm. CLIP4STR \cite{10816351} significantly boosts \textit{image-text alignment} and irregular text recognition capabilities with its dual-branch model architecture and triple optimization strategy. MemVP \cite{jie2024memory} tailors PEFT for vision-language models by training lightweight alignment modules and incorporating cross-attention for information fusion in the language model’s feed-forward layers.

\subsection{Vision-Language Models}

In multimodal tasks with pre-trained vision encoders and large language models, visual and textual representations reside in distinct embedding spaces due to modality-specific pretraining. Aligning these representations is crucial for effective fusion. 
A common approach is input-space prompting-based fine-tuning, where visual inputs are projected and concatenated with text inputs. 
Representative methods include VL-Adapter \cite{sung2022vl}, VL-PET \cite{hu2023vl}, LaVIN \cite{luo2024cheap}, and LLaVA \cite{liu2024visual}, though this increases input sequence lengths, hindering contextual associations and raising inference latency. Cross-attention-based alignment, proposed by Flamingo \cite{alayrac2022flamingo} and BLIP \cite{li2022blip, li2023blip}, projects visual data into keys and values for cross-attention, achieving fusion without increasing input length. UniAdapter \cite{lu2023uniadapter} and MemVP \cite{jie2024memory} further optimize this with frame-aware attention and simplified cross-attention computations, improving inference speed and efficiency. Despite progress, parameter-efficient methods for enhancing cross-attention with better semantic relationship understanding remain underexplored. Our work aims to develop more efficient, lightweight, and generalizable fine-tuning frameworks.","\subsection{Parameter-Efficient Fine-Tuning}

Compared to traditional fine-tuning methods, PEFT significantly reduces the number of parameters to be updated. PEFT methods can be categorized based on their impact on the model:
1) Methods acting directly on the language model, which adjust model parameters or structure to modify the knowledge stored in the model. Key methods include BitFit \cite{zaken2021bitfit}, Adapter \cite{houlsby2019parameter}, and LoRA \cite{hu2021lora}. BitFit trains only the bias values in pre-trained models, minimizing trainable parameters.
2) Methods acting on language model inputs or intermediate-layer inputs, such as Prefix-Tuning \cite{li2021prefix} and Prompt-Tuning \cite{lester2021power}, which modify input representations. 

Many PEFT techniques designed for language models can be applied to multimodal architectures. For instance, Adversarial DuAl Prompt Tuning (ADAPT) \cite{10901852} achieves efficient Unsupervised Domain Adaptation (UDA) through domain alignment via adversarial fine-tuning of both textual and visual prompts. MetaPrompt \cite{10431687} enhances the model's domain generalization capability through a dual-modality prompt tuning network and an alternating episodic training algorithm. CLIP4STR \cite{10816351} significantly boosts \textit{image-text alignment} and irregular text recognition capabilities with its dual-branch model architecture and triple optimization strategy. MemVP \cite{jie2024memory} tailors PEFT for vision-language models by training lightweight alignment modules and incorporating cross-attention for information fusion in the language model’s feed-forward layers.

\subsection{Vision-Language Models}

In multimodal tasks with pre-trained vision encoders and large language models, visual and textual representations reside in distinct embedding spaces due to modality-specific pretraining. Aligning these representations is crucial for effective fusion. 
A common approach is input-space prompting-based fine-tuning, where visual inputs are projected and concatenated with text inputs. 
Representative methods include VL-Adapter \cite{sung2022vl}, VL-PET \cite{hu2023vl}, LaVIN \cite{luo2024cheap}, and LLaVA \cite{liu2024visual}, though this increases input sequence lengths, hindering contextual associations and raising inference latency. Cross-attention-based alignment, proposed by Flamingo \cite{alayrac2022flamingo} and BLIP \cite{li2022blip, li2023blip}, projects visual data into keys and values for cross-attention, achieving fusion without increasing input length. UniAdapter \cite{lu2023uniadapter} and MemVP \cite{jie2024memory} further optimize this with frame-aware attention and simplified cross-attention computations, improving inference speed and efficiency. Despite progress, parameter-efficient methods for enhancing cross-attention with better semantic relationship understanding remain underexplored. Our work aims to develop more efficient, lightweight, and generalizable fine-tuning frameworks.",
2511.10051v1,http://arxiv.org/abs/2511.10051v1,2025-11-13 07:49:38+00:00,GraphIF: Enhancing Multi-Turn Instruction Following for Large Language Models with Relation Graph Prompt,"Multi-turn instruction following is essential for building intelligent conversational systems that can consistently adhere to instructions across dialogue turns. However, existing approaches to enhancing multi-turn instruction following primarily rely on collecting or generating large-scale multi-turn dialogue datasets to fine-tune large language models (LLMs), which treat each response generation as an isolated task and fail to explicitly incorporate multi-turn instruction following into the optimization objectives. As a result, instruction-tuned LLMs often struggle with complex long-distance constraints. In multi-turn dialogues, relational constraints across turns can be naturally modeled as labeled directed edges, making graph structures particularly suitable for modeling multi-turn instruction following. Despite this potential, leveraging graph structures to enhance the multi-turn instruction following capabilities of LLMs remains unexplored. To bridge this gap, we propose GraphIF, a plug-and-play framework that models multi-turn dialogues as directed relation graphs and leverages graph prompts to enhance the instruction following capabilities of LLMs. GraphIF comprises three key components: (1) an agent-based relation extraction module that captures inter-turn semantic relations via action-triggered mechanisms to construct structured graphs; (2) a relation graph prompt generation module that converts structured graph information into natural language prompts; and (3) a response rewriting module that refines initial LLM outputs using the generated graph prompts. Extensive experiments on two long multi-turn dialogue datasets demonstrate that GraphIF can be seamlessly integrated into instruction-tuned LLMs and leads to significant improvements across all four multi-turn instruction-following evaluation metrics.","\paragraph{Instruction Fine-tuning for LLMs.} 
Current approaches for enhancing multi-turn instruction following mainly use supervised fine-tuning with instruction datasets. Some methods curate real-world user-LLM interactions \cite{wang2023openchat,zhao2024wildchat}, while others generate synthetic dialogues \cite{ding2023enhancing,wu2025instruct}. Parrot \cite{sun2024parrot} addresses anaphora and ellipsis by training specialized models, and ConsistentChat \cite{chen2025consistentchat} tackles consistency through skeleton-guided generation. However, these instruction-tuning methods overlook explicit modeling of inter-turn relational structures.



\begin{figure*}[t]
\centering
\includegraphics[width=1.0\textwidth]{method.jpg} % Reduce the figure size so that it is slightly narrower than the column.
\caption{Framework overview of GraphIF. Given dialogue history and current user instruction, GraphIF first extracts semantic relations between dialogues through the \textit{Agent-Based Relation Extraction} module, then employs \textit{Relation Graph Prompt Generation} to generate constraint-aware prompts, and finally uses the \textit{Initial Response Rewrite} module to refine the initial response.}
\label{fig:method}
\end{figure*}


\paragraph{Graph-Augmented Generation with LLMs.} Recent research has increasingly explored the integration of graph structures to enhance the capabilities of LLMs \cite{jimenez2024hipporag,li2024graphreader,lin2025rje}. GraphRAG \cite{edge2024local} and LightRAG \cite{guo2024lightrag} construct cross-document knowledge graphs to enable knowledge-enhanced retrieval for response generation. KGP \cite{wang2024knowledge} addresses multi-document question-answering tasks by constructing passage-level directed graphs to aggregate relevant information. However, no existing research has applied graph structures to enhance the multi-turn instruction following for LLMs.","\paragraph{Instruction Fine-tuning for LLMs.} 
Current approaches for enhancing multi-turn instruction following mainly use supervised fine-tuning with instruction datasets. Some methods curate real-world user-LLM interactions \cite{wang2023openchat,zhao2024wildchat}, while others generate synthetic dialogues \cite{ding2023enhancing,wu2025instruct}. Parrot \cite{sun2024parrot} addresses anaphora and ellipsis by training specialized models, and ConsistentChat \cite{chen2025consistentchat} tackles consistency through skeleton-guided generation. However, these instruction-tuning methods overlook explicit modeling of inter-turn relational structures.






\paragraph{Graph-Augmented Generation with LLMs.} Recent research has increasingly explored the integration of graph structures to enhance the capabilities of LLMs \cite{jimenez2024hipporag,li2024graphreader,lin2025rje}. GraphRAG \cite{edge2024local} and LightRAG \cite{guo2024lightrag} construct cross-document knowledge graphs to enable knowledge-enhanced retrieval for response generation. KGP \cite{wang2024knowledge} addresses multi-document question-answering tasks by constructing passage-level directed graphs to aggregate relevant information. However, no existing research has applied graph structures to enhance the multi-turn instruction following for LLMs.","Instruction Fine-tuning for LLMs.Current approaches
for enhancing multi-turn instruction following mainly use
supervised fine-tuning with instruction datasets. Some meth-
ods curate real-world user-LLM interactions (Wang et al.
2023; Zhao et al. 2024), while others generate synthetic di-
alogues (Ding et al. 2023; Wu et al. 2025). Parrot (Sun et al.
2024) addresses anaphora and ellipsis by training special-
ized models, and ConsistentChat (Chen et al. 2025) tackles
consistency through skeleton-guided generation. However,
these instruction-tuning methods overlook explicit modeling
of inter-turn relational structures.
Graph-Augmented Generation with LLMs.Recent
research has increasingly explored the integration of
graph structures to enhance the capabilities of LLMs
(Jimenez Gutierrez et al. 2024; Li et al. 2024; Lin et al.
2025). GraphRAG (Edge et al. 2024) and LightRAG (Guo
et al. 2024) construct cross-document knowledge graphs to
enable knowledge-enhanced retrieval for response genera-
tion. KGP (Wang et al. 2024) addresses multi-document
question-answering tasks by constructing passage-level di-
rected graphs to aggregate relevant information. However,
no existing research has applied graph structures to enhance
the multi-turn instruction following for LLMs.
Preliminary
Multi-Turn Instruction FollowingWe formally define the
multi-turn instruction following taskasD={(H t,It)}M
t=1,
whereMrepresents total number of dialogue turns (M >
1),I trepresents the user instruction int-th dialogue turn,
andH t={⟨I k,RES k⟩}t−1
k=1represents the dialogue history
Figure 2: Framework overview of GraphIF. Given dialogue history and current user instruction, GraphIF first extracts semantic
relations between dialogues through theAgent-Based Relation Extractionmodule, then employsRelation Graph Prompt Gen-
erationto generate constraint-aware prompts, and finally uses theInitial Response Rewritemodule to refine the initial response.
before turnt, whereRES kmeans the response generated
by LLM ink-th dialogue turn. Specifically,H 1is empty in
the first dialogue turn, and the LLM directly generates a re-
sponse toI"
2511.08150v1,http://arxiv.org/abs/2511.08150v1,2025-11-11 12:00:09+00:00,DiffuGR: Generative Document Retrieval with Diffusion Language Models,"Generative retrieval (GR) re-frames document retrieval as a sequence-based document identifier (DocID) generation task, memorizing documents with model parameters and enabling end-to-end retrieval without explicit indexing. Existing GR methods are based on auto-regressive generative models, i.e., the token generation is performed from left to right. However, such auto-regressive methods suffer from: (1) mismatch between DocID generation and natural language generation, e.g., an incorrect DocID token generated in early left steps would lead to totally erroneous retrieval; and (2) failure to balance the trade-off between retrieval efficiency and accuracy dynamically, which is crucial for practical applications. To address these limitations, we propose generative document retrieval with diffusion language models, dubbed DiffuGR. It models DocID generation as a discrete diffusion process: during training, DocIDs are corrupted through a stochastic masking process, and a diffusion language model is learned to recover them under a retrieval-aware objective. For inference, DiffuGR attempts to generate DocID tokens in parallel and refines them through a controllable number of denoising steps. In contrast to conventional left-to-right auto-regressive decoding, DiffuGR provides a novel mechanism to first generate more confident DocID tokens and refine the generation through diffusion-based denoising. Moreover, DiffuGR also offers explicit runtime control over the qualitylatency tradeoff. Extensive experiments on benchmark retrieval datasets show that DiffuGR is competitive with strong auto-regressive generative retrievers, while offering flexible speed and accuracy tradeoffs through variable denoising budgets. Overall, our results indicate that non-autoregressive diffusion models are a practical and effective alternative for generative document retrieval.","In this section, we give a literature review regarding generative retrieval and diffusion models for text generation.
\subsection{Generative Retrieval}
Generative retrieval (GR) reformulates document retrieval as a generation task, in which a generative model (usually a seq2seq language model) directly generates the identifier of the target document.
It provides an end-to-end solution for document retrieval and allows better utilization of large-scale generative language models.
Document identifiers (DocIDs) are important for GR, through which retrieval systems can uniquely index the corresponding documents and enable end-to-end training and inference.
Existing GR methods usually use learnable DocIDs or linguistic DocIDs.
\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{figs/pipeline.pdf}
    \caption{An overview of DiffuGR, consisting DocID construction, model training, and inference. 
    DiffuGR involves two kinds of DocIDs, i.e., linguistic DocIDs and learnable DocIDs.
    During training, the model is optimized to recover randomly masked DocID tokens. 
    For inference, DiffuGR  generates DocID tokens in parallel and refines them across multiple denoising steps.}
    \label{fig:pipeline}
\end{figure*}

Learnable DocIDs are usually constructed upon dense document representations, either through hierarchical $k$-means clustering~\cite{Tay2022TransformerMA, Wang2022ANC}, product quantization~\cite{Zhou2022UltronAU}, residual vector quantization~\cite{TIGER-Rajput, IRGen-Zhang, zeng2023scalable_and_effective}, or progressively learning ~\cite{sun2023learning, yang2023auto_search_indexer}.
This process targets on compressing the semantic information contained in dense document representations into discrete tokens.
Although promising results have been achieved, learnable DocIDs require expansion of the vocabulary, e.g., introducing additional trainable codebooks.

Linguistic DocIDs use strings, such as title, URL, or keywords, to represent documents.
They naturally carry semantics related to associated documents, and the construction cost is relatively low, without the need for additional manual supervision.
Plenty of work has been conducted based on linguistic DocIDs, yielding excellent results\cite{DeCao2020AutoregressiveER, de-cao-etal-2022-multilingual, Bevilacqua2022AutoregressiveSE, Zhou2022UltronAU, Chen2022CorpusBrainPA, UGR-Chen, tang2023inspired_by_learning_strategy, uni_gen}.

Despite the success of GR, most existing methods generate DocID tokens in an auto-regressive left-to-right manner. 
Although \citet{qiao2023diffusionret} proposed DiffusionRet which employs the diffusion model to generate a pseudo document for a query, the generation of DocIDs are still auto-regressive.
However, such auto-regressive left-to-right generation of DocID tokens could limit the retrieval performance.
Specifically, if an incorrect DocID token is generated in early left steps, the correct DocID would never be generated.
Based on above considerations, this work explores 
end-to-end generation of DocIDs through non-autoregressive diffusion language models~\cite{nie2025large, ye2025dream}.
Its core advantage lies in the fact that the diffusion model does not restrict the left-to-right generation order; instead, the model 
is encouraged to automatically select the most appropriate generation order based on its own conditions and confidence.


\subsection{Diffusion Models for Text Generation}
Diffusion models are initially introduced in continuous domains such as image generation~\citep{song2020denoising,ho2020denoising}, and their success has inspired extensions to natural language processing by modeling text in continuous embedding spaces~\citep{li2022diffusion,gong2022diffuseq,gong-etal-2023-diffuseq}. These continuous diffusion language models demonstrate the feasibility of applying iterative denoising for text generation, for both pre-training and fine-tuning on transformer architectures~\citep{genie2023}. 
To better align with the discrete nature of text, discrete diffusion language models were later developed~\citep{austin2021structured,hoogeboom2021argmax,campbell2022continuous}, which progressively corrupt token sequences and then reconstruct the original text during the reverse process. 
Further benefits have been observed through initializing from pre-trained masked language models such as BERT~\citep{he-etal-2023-diffusionbert,ye2025diffusionlanguagemodelsperform}. 
Despite generation quality, recent work highlights the strengths of diffusion models in hybrid block-wise generation that balance sequential coherence with parallelism~\citep{ye2024diffusion,ye2024beyond,ye2025implicit,arriola2025blockdiffusion}.

Scaling diffusion models to large language models (LLMs) has further extended their applicability. \citet{gulrajani2023likelihoodbased} analyzed scaling laws for continuous diffusion.
~\citet{lou2023discrete} showed that discrete masked diffusion models have achieved perplexities competitive with GPT-2. 
Building on this, large-scale efforts such as DiffuGPT and DiffuLLaMA adopt pretrained  auto-regressive LLMs into diffusion-based frameworks~\citep{gong2025scalingdiffusionlanguagemodels}, while LLaDA~\citep{nie2025large} demonstrated that diffusion models trained from scratch at 8B parameters can rival strong auto-regressive LLMs like LLaMA3-8B. 
Recent commercial systems such as Mercury Coder~\citep{inceptionlabs_mercury} further validate the practicality of diffusion models for text and code generation.

In summary, these advances establish diffusion models as a viable alternative for natural language generation. While in this work, we explore the new potential to use diffusion models for end-to-end document retrieval, i.e., the generation of DocIDs for a query.","In this section, we give a literature review regarding generative retrieval and diffusion models for text generation.
\subsection{Generative Retrieval}
Generative retrieval (GR) reformulates document retrieval as a generation task, in which a generative model (usually a seq2seq language model) directly generates the identifier of the target document.
It provides an end-to-end solution for document retrieval and allows better utilization of large-scale generative language models.
Document identifiers (DocIDs) are important for GR, through which retrieval systems can uniquely index the corresponding documents and enable end-to-end training and inference.
Existing GR methods usually use learnable DocIDs or linguistic DocIDs.


Learnable DocIDs are usually constructed upon dense document representations, either through hierarchical $k$-means clustering~\cite{Tay2022TransformerMA, Wang2022ANC}, product quantization~\cite{Zhou2022UltronAU}, residual vector quantization~\cite{TIGER-Rajput, IRGen-Zhang, zeng2023scalable_and_effective}, or progressively learning ~\cite{sun2023learning, yang2023auto_search_indexer}.
This process targets on compressing the semantic information contained in dense document representations into discrete tokens.
Although promising results have been achieved, learnable DocIDs require expansion of the vocabulary, e.g., introducing additional trainable codebooks.

Linguistic DocIDs use strings, such as title, URL, or keywords, to represent documents.
They naturally carry semantics related to associated documents, and the construction cost is relatively low, without the need for additional manual supervision.
Plenty of work has been conducted based on linguistic DocIDs, yielding excellent results\cite{DeCao2020AutoregressiveER, de-cao-etal-2022-multilingual, Bevilacqua2022AutoregressiveSE, Zhou2022UltronAU, Chen2022CorpusBrainPA, UGR-Chen, tang2023inspired_by_learning_strategy, uni_gen}.

Despite the success of GR, most existing methods generate DocID tokens in an auto-regressive left-to-right manner. 
Although \citet{qiao2023diffusionret} proposed DiffusionRet which employs the diffusion model to generate a pseudo document for a query, the generation of DocIDs are still auto-regressive.
However, such auto-regressive left-to-right generation of DocID tokens could limit the retrieval performance.
Specifically, if an incorrect DocID token is generated in early left steps, the correct DocID would never be generated.
Based on above considerations, this work explores 
end-to-end generation of DocIDs through non-autoregressive diffusion language models~\cite{nie2025large, ye2025dream}.
Its core advantage lies in the fact that the diffusion model does not restrict the left-to-right generation order; instead, the model 
is encouraged to automatically select the most appropriate generation order based on its own conditions and confidence.


\subsection{Diffusion Models for Text Generation}
Diffusion models are initially introduced in continuous domains such as image generation~\citep{song2020denoising,ho2020denoising}, and their success has inspired extensions to natural language processing by modeling text in continuous embedding spaces~\citep{li2022diffusion,gong2022diffuseq,gong-etal-2023-diffuseq}. These continuous diffusion language models demonstrate the feasibility of applying iterative denoising for text generation, for both pre-training and fine-tuning on transformer architectures~\citep{genie2023}. 
To better align with the discrete nature of text, discrete diffusion language models were later developed~\citep{austin2021structured,hoogeboom2021argmax,campbell2022continuous}, which progressively corrupt token sequences and then reconstruct the original text during the reverse process. 
Further benefits have been observed through initializing from pre-trained masked language models such as BERT~\citep{he-etal-2023-diffusionbert,ye2025diffusionlanguagemodelsperform}. 
Despite generation quality, recent work highlights the strengths of diffusion models in hybrid block-wise generation that balance sequential coherence with parallelism~\citep{ye2024diffusion,ye2024beyond,ye2025implicit,arriola2025blockdiffusion}.

Scaling diffusion models to large language models (LLMs) has further extended their applicability. \citet{gulrajani2023likelihoodbased} analyzed scaling laws for continuous diffusion.
~\citet{lou2023discrete} showed that discrete masked diffusion models have achieved perplexities competitive with GPT-2. 
Building on this, large-scale efforts such as DiffuGPT and DiffuLLaMA adopt pretrained  auto-regressive LLMs into diffusion-based frameworks~\citep{gong2025scalingdiffusionlanguagemodels}, while LLaDA~\citep{nie2025large} demonstrated that diffusion models trained from scratch at 8B parameters can rival strong auto-regressive LLMs like LLaMA3-8B. 
Recent commercial systems such as Mercury Coder~\citep{inceptionlabs_mercury} further validate the practicality of diffusion models for text and code generation.

In summary, these advances establish diffusion models as a viable alternative for natural language generation. While in this work, we explore the new potential to use diffusion models for end-to-end document retrieval, i.e., the generation of DocIDs for a query.","In this section, we give a literature review regarding generative
retrieval and diffusion models for text generation.
2.1 Generative Retrieval
Generative retrieval (GR) reformulates document retrieval as a
generation task, in which a generative model (usually a seq2seq
language model) directly generates the identifier of the target docu-
ment. It provides an end-to-end solution for document retrieval and
allows better utilization of large-scale generative language models.
Document identifiers (DocIDs) are important for GR, through which
retrieval systems can uniquely index the corresponding documents
and enable end-to-end training and inference. Existing GR methods
usually use learnable DocIDs or linguistic DocIDs.
Learnable DocIDs are usually constructed upon dense docu-
ment representations, either through hierarchical 𝑘-means cluster-
ing [ 42,44], product quantization [ 58], residual vector quantiza-
tion [ 38,53,55], or progressively learning [ 40,47]. This process
targets on compressing the semantic information contained in dense
document representations into discrete tokens. Although promising
results have been achieved, learnable DocIDs require expansion of
the vocabulary, e.g., introducing additional trainable codebooks.
Linguistic DocIDs use strings, such as title, URL, or keywords,
to represent documents. They naturally carry semantics related to
associated documents, and the construction cost is relatively low,
without the need for additional manual supervision. Plenty of work
has been conducted based on linguistic DocIDs, yielding excellent
results[3, 7, 9, 10, 12, 25, 41, 58].
Despite the success of GR, most existing methods generate Do-
cID tokens in an auto-regressive left-to-right manner. Although
Qiao et al . [37] proposed DiffusionRet which employs the diffusion
model to generate a pseudo document for a query, the generation
of DocIDs are still auto-regressive. However, such auto-regressive
left-to-right generation of DocID tokens could limit the retrieval
performance. Specifically, if an incorrect DocID token is generated
in early left steps, the correct DocID would never be generated.
Based on above considerations, this work explores end-to-end gen-
eration of DocIDs through non-autoregressive diffusion language
models [ 34,51]. Its core advantage lies in the fact that the diffusion
model does not restrict the left-to-right generation order; instead,the model is encouraged to automatically select the most appropri-
ate generation order based on its own conditions and confidence.
2.2 Diffusion Models for Text Generation
Diffusion models are initially introduced in continuous domains
such as image generation [ 19,39], and their success has inspired
extensions to natural language processing by modeling text in con-
tinuous embedding spaces [ 15,16,26]. These continuous diffusion
language models demonstrate the feasibility of applying iterative
denoising for text generation, for both pre-training and fine-tuning
on transformer architectures [ 29]. To better align with the discrete
nature of text, discrete diffusion language models were later de-
veloped [ 2,5,20], which progressively corrupt token sequences
and then reconstruct the original text during the reverse process.
Further benefits have been observed through initializing from pre-
trained masked language models such as BERT [ 18,52]. Despite
generation quality, recent work highlights the strengths of diffu-
sion models in hybrid block-wise generation that balance sequential
coherence with parallelism [1, 48–50].
Scaling diffusion models to large language models (LLMs) has fur-
ther extended their applicability. Gulrajani and Hashimoto [17] an-
alyzed scaling laws for continuous diffusion. Lou et al . [30] showed
that discrete masked diffusion models have achieved perplexities
competitive with GPT-"
2511.10480v1,http://arxiv.org/abs/2511.10480v1,2025-11-13 16:44:56+00:00,Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs,"Optimizing the performance of large language models (LLMs) on large-scale AI training and inference systems requires a scalable and expressive mechanism to model distributed workload execution. Such modeling is essential for pre-deployment system-level optimizations (e.g., parallelization strategies) and design-space explorations. While recent efforts have proposed collecting execution traces from real systems, access to large-scale infrastructure remains limited to major cloud providers. Moreover, traces obtained from existing platforms cannot be easily adapted to study future larger-scale system configurations. We introduce Symbolic Tensor grAph GEnerator(STAGE), a framework that synthesizes high-fidelity execution traces to accurately model LLM workloads. STAGE supports a comprehensive set of parallelization strategies, allowing users to systematically explore a wide spectrum of LLM architectures and system configurations. STAGE demonstrates its scalability by synthesizing high-fidelity LLM traces spanning over 32K GPUs, while preserving tensor-level accuracy in compute, memory, and communication. STAGE will be publicly available to facilitate further research in distributed machine learning systems.","\label{sec:related}

%\subsection{Benchmarking for Distributed Training}

\noindent
\textbf{Benchmarking for Distributed Training.}
DeepBench~\cite{bai2019deepbench} and MLPerf~\cite{mlperf} offer standardized metrics for evaluating the performance of training and inference tasks. While these tools excel in providing reproducible benchmarks, they do not support detailed profiling data.
PyTorch Execution Observer~\cite{pytorch-profiler} and NVIDIA CUPTI~\cite{cupti} provide performance profile result of training systems. 
However, they require actual run to collect traces. Moreover, the generated execution traces lack annotations for optimizations and dependencies, which are essential for profiling system architectures. 
PyTorch FX~\cite{pytorch-fx} can capture static model behaviors with dependency graph during compile time but it lacks information from post execution and requires optimized codes for analysis.
% , limiting its flexibility in exploring configurations. 
In contrast, \sys automatically partitions the operators, generating an updated computational graph that incorporates the appropriate parallelization annotations and dependencies.

%\subsection{Performance Modeling for Distributed Training} %\label{sec:background_perf_modeling}

\noindent
\textbf{Performance Modeling for Distributed Training.}
Recent efforts on performance modeling such as vTrain~\cite{vtrain}, MADMAX~\cite{madmax}, and Calculon~\cite{calculon} have significantly advanced the community’s understanding of distributed LLM workloads through detailed analytical modeling or trace-driven simulation. However, these frameworks share a common limitation in terms of flexibility and configurability, making it difficult to systematically explore emerging model such as MoE and state space model in detail. vTrain primarily focuses on operators explicitly profiled from real systems, limiting extensibility to new or custom models. 
% MADMAX, with its analytically modeled approach, relies on predefined assumptions, restricting adaptation to novel configurations. Similarly, Calculon specifically targets Megatron-based dense LLM architectures, making it less suitable for rapidly evolving and diverse LLM architectures.
In this context, our work, \sys, aims not to compete but rather to complement these existing frameworks by providing a flexible and configurable workload generation mechanism. 

%\subsection{Tensor Representation for System-level Optimization} 
% \hw{updated} \ju{Shrinked}

\noindent
\textbf{Tensor Representation for System-level Optimizations.}
Tensor representation is commonly utilized for system-level optimization of deep learning models~\cite{Souffle, TVM, TensorComprehend}, enabling computational graph optimizations for frameworks including PyTorch~\cite{pytorch} and TensorFlow~\cite{tensorflow}. Techniques such as operator fusion leverage tensor representations to enhance parallel processing and memory efficiency~\cite{Apollo, DNNFusion}. 
FlexFlow~\cite{FlexFlow} and Unity~\cite{Unity} employ system-level compilation to determine effective parallelization strategies in distributed settings, while Mist~\cite{Zhu2025Mist} recently proposed symbolic tensor representations specifically for memory parallelism. 
In contrast, we propose a symbolic tensor graph that systematically annotates key operators with parallelization dimensions to guide runtime optimization for large-scale LLM training.
%As illustrated in Figure~\ref{fig:tensor_mha}, it captures critical parallelized tensor annotations within the multihead attention kernel. Further, Section~\autoref{stg_design} outlines how \sys facilitates the definition of diverse operators and the optimization of multi-dimensional parallelization strategies.
 
% Tensor representation is a common approach for system-level optimization of the Deep Learning models \cite{Souffle, TVM, TensorComprehend}. In combination with the generated computational graph from pytorch \cite{pytorch} and tensorflow \cite{tensorflow}, tensor representations are used to optimize the computation of Deep Learning operations, like convolution and Matrix Multiplication. Some of the common techniques including operator fusion leveraging the tensor representation to enhance the parallel processing and efficient memory access \cite{Apollo,DNNFusion}. Extended from using tensor representation to facilitate the operators mapping to the parallel architecture, other works, like FlexFlow \cite{FlexFlow} and Unity \cite{Unity}, have used system-level compilation to determine the appropriate runtime parallelization strategy in a distributed setup. Hence, Tensor representation has been used as a powerful technique at compiler and system level to optimize the runtime for DL models. In this paper, we leverage the idea of tensor representation and present our tailored symbolic tensor graph that systematically annotates the key operators with parallelization dimension to guide the selection of the optimal runtime system for large-scale LLM training. More recently, Mist~\cite{Zhu2025Mist} has employed a symbolic tensor-based approach to simulate optimal solutions in the memory parallelism domain. In this paper, we leverage the agile symbolic tensor representation to guide the system optimization for various distributed learning models at a large scale. In section \autoref{stg_design}, we discuss in detail how \sys facilitates the definition of a variety of operators and the underlying optimization techniques to handle multi-dimensional parallelization. Before delving into the details, Figure ~\ref{fig:tensor_mha} illustrates the tensor representation along with the insertion of parallelization dimensions in the multi-head attention mechanism.","\noindent
\textbf{Benchmarking for Distributed Training.}
DeepBench~\cite{bai2019deepbench} and MLPerf~\cite{mlperf} offer standardized metrics for evaluating the performance of training and inference tasks. While these tools excel in providing reproducible benchmarks, they do not support detailed profiling data.
PyTorch Execution Observer~\cite{pytorch-profiler} and NVIDIA CUPTI~\cite{cupti} provide performance profile result of training systems. 
However, they require actual run to collect traces. Moreover, the generated execution traces lack annotations for optimizations and dependencies, which are essential for profiling system architectures. 
PyTorch FX~\cite{pytorch-fx} can capture static model behaviors with dependency graph during compile time but it lacks information from post execution and requires optimized codes for analysis.

In contrast, \sys automatically partitions the operators, generating an updated computational graph that incorporates the appropriate parallelization annotations and dependencies.



\noindent
\textbf{Performance Modeling for Distributed Training.}
Recent efforts on performance modeling such as vTrain~\cite{vtrain}, MADMAX~\cite{madmax}, and Calculon~\cite{calculon} have significantly advanced the community’s understanding of distributed LLM workloads through detailed analytical modeling or trace-driven simulation. However, these frameworks share a common limitation in terms of flexibility and configurability, making it difficult to systematically explore emerging model such as MoE and state space model in detail. vTrain primarily focuses on operators explicitly profiled from real systems, limiting extensibility to new or custom models. 

In this context, our work, \sys, aims not to compete but rather to complement these existing frameworks by providing a flexible and configurable workload generation mechanism. 




\noindent
\textbf{Tensor Representation for System-level Optimizations.}
Tensor representation is commonly utilized for system-level optimization of deep learning models~\cite{Souffle, TVM, TensorComprehend}, enabling computational graph optimizations for frameworks including PyTorch~\cite{pytorch} and TensorFlow~\cite{tensorflow}. Techniques such as operator fusion leverage tensor representations to enhance parallel processing and memory efficiency~\cite{Apollo, DNNFusion}. 
FlexFlow~\cite{FlexFlow} and Unity~\cite{Unity} employ system-level compilation to determine effective parallelization strategies in distributed settings, while Mist~\cite{Zhu2025Mist} recently proposed symbolic tensor representations specifically for memory parallelism. 
In contrast, we propose a symbolic tensor graph that systematically annotates key operators with parallelization dimensions to guide runtime optimization for large-scale LLM training.",
2511.08942v1,http://arxiv.org/abs/2511.08942v1,2025-11-12 03:38:50+00:00,"Think, Remember, Navigate: Zero-Shot Object-Goal Navigation with VLM-Powered Reasoning","While Vision-Language Models (VLMs) are set to transform robotic navigation, existing methods often underutilize their reasoning capabilities. To unlock the full potential of VLMs in robotics, we shift their role from passive observers to active strategists in the navigation process. Our framework outsources high-level planning to a VLM, which leverages its contextual understanding to guide a frontier-based exploration agent. This intelligent guidance is achieved through a trio of techniques: structured chain-of-thought prompting that elicits logical, step-by-step reasoning; dynamic inclusion of the agent's recent action history to prevent getting stuck in loops; and a novel capability that enables the VLM to interpret top-down obstacle maps alongside first-person views, thereby enhancing spatial awareness. When tested on challenging benchmarks like HM3D, Gibson, and MP3D, this method produces exceptionally direct and logical trajectories, marking a substantial improvement in navigation efficiency over existing approaches and charting a path toward more capable embodied agents.","\label{sec:related}

The challenge of Object Goal Navigation (ObjectNav) has been approached from several angles, beginning with end-to-end learning frameworks that often struggled to generalize beyond their training data \cite{mousavian2019visual, ye2021efficient}. Modular pipelines emerged as an alternative, deconstructing the problem into perception, mapping, and planning stages \cite{chaplot2020object}. While more robust, these systems could be brittle and prone to cascading errors between components \cite{kumar2021gcexp}. This has motivated a recent surge in zero-shot methodologies that harness the world knowledge of large pre-trained models, thereby avoiding the need for extensive task-specific fine-tuning \cite{sun2024survey}.

The integration of Vision-Language Models (VLMs) first involved using them as powerful feature extractors, with methods like CLIP on Wheels (CoWs) \cite{gadre2022clip} leveraging embeddings to link visual scenes with a target object's name. The role of language models soon became more active, with systems like ESC applying common-sense reasoning to guide exploration \cite{zhou2023esc}. More recently, the frontier of ObjectNav has shifted toward offloading high-level strategy entirely to large models. This paradigm includes diverse techniques such as translating maps into text for an LLM to score exploration paths (L3MVN \cite{yu2023l3mvn}), employing a VLM to directly evaluate the semantic promise of frontiers (VLFM \cite{yokoyama2024vlfm}), or even using a VLM to imagine and select optimal future viewpoints (ImagineNav \cite{zhao2024imaginenav}).

Our work advances this paradigm by enhancing the VLM's cognitive role in navigation. We introduce a framework that empowers the VLM with a more profound understanding of the task through a synergistic combination of three key techniques: structured Chain-of-Thought (CoT) prompting to elicit more logical, step-by-step analysis \cite{wei2023chainofthoughtprompting}; the incorporation of a memory of recent actions to prevent stagnation; and a novel method for providing multimodal spatial context by enabling the VLM to interpret top-down obstacle maps in conjunction with its egocentric view. This holistic approach results in a more effective zero-shot navigator capable of generating more coherent and efficient trajectories.","The challenge of Object Goal Navigation (ObjectNav) has been approached from several angles, beginning with end-to-end learning frameworks that often struggled to generalize beyond their training data \cite{mousavian2019visual, ye2021efficient}. Modular pipelines emerged as an alternative, deconstructing the problem into perception, mapping, and planning stages \cite{chaplot2020object}. While more robust, these systems could be brittle and prone to cascading errors between components \cite{kumar2021gcexp}. This has motivated a recent surge in zero-shot methodologies that harness the world knowledge of large pre-trained models, thereby avoiding the need for extensive task-specific fine-tuning \cite{sun2024survey}.

The integration of Vision-Language Models (VLMs) first involved using them as powerful feature extractors, with methods like CLIP on Wheels (CoWs) \cite{gadre2022clip} leveraging embeddings to link visual scenes with a target object's name. The role of language models soon became more active, with systems like ESC applying common-sense reasoning to guide exploration \cite{zhou2023esc}. More recently, the frontier of ObjectNav has shifted toward offloading high-level strategy entirely to large models. This paradigm includes diverse techniques such as translating maps into text for an LLM to score exploration paths (L3MVN \cite{yu2023l3mvn}), employing a VLM to directly evaluate the semantic promise of frontiers (VLFM \cite{yokoyama2024vlfm}), or even using a VLM to imagine and select optimal future viewpoints (ImagineNav \cite{zhao2024imaginenav}).

Our work advances this paradigm by enhancing the VLM's cognitive role in navigation. We introduce a framework that empowers the VLM with a more profound understanding of the task through a synergistic combination of three key techniques: structured Chain-of-Thought (CoT) prompting to elicit more logical, step-by-step analysis \cite{wei2023chainofthoughtprompting}; the incorporation of a memory of recent actions to prevent stagnation; and a novel method for providing multimodal spatial context by enabling the VLM to interpret top-down obstacle maps in conjunction with its egocentric view. This holistic approach results in a more effective zero-shot navigator capable of generating more coherent and efficient trajectories.","The challenge of Object Goal Navigation (ObjectNav) has been approached from several angles,
beginning with end-to-end learning frameworks that often struggled to generalize beyond their training
data Mousavian et al. [2019], Ye and Yang [2021]. Modular pipelines emerged as an alternative,
deconstructing the problem into perception, mapping, and planning stages Chaplot et al. [2020].
While more robust, these systems could be brittle and prone to cascading errors between components
Kumar et al. [2021]. This has motivated a recent surge in zero-shot methodologies that harness the
world knowledge of large pre-trained models, thereby avoiding the need for extensive task-specific
fine-tuning Sun et al. [2024].
The integration of Vision-Language Models (VLMs) first involved using them as powerful feature
extractors, with methods like CLIP on Wheels (CoWs) Gadre et al. [2022a] leveraging embeddings
to link visual scenes with a target object’s name. The role of language models soon became more
active, with systems like ESC applying common-sense reasoning to guide exploration Zhou et al.
[2023]. More recently, the frontier of ObjectNav has shifted toward offloading high-level strategy
entirely to large models. This paradigm includes diverse techniques such as translating maps into
text for an LLM to score exploration paths (L3MVN Yu et al. [2023]), employing a VLM to directly
evaluate the semantic promise of frontiers (VLFM Yokoyama et al. [2024]), or even using a VLM to
imagine and select optimal future viewpoints (ImagineNav Zhao et al. [2024]).
Our work advances this paradigm by enhancing the VLM’s cognitive role in navigation. We introduce
a framework that empowers the VLM with a more profound understanding of the task through a
synergistic combination of three key techniques: structured Chain-of-Thought (CoT) prompting
to elicit more logical, step-by-step analysis Wei et al. [2023]; the incorporation of a memory of
recent actions to prevent stagnation; and a novel method for providing multimodal spatial context by
enabling the VLM to interpret top-down obstacle maps in conjunction with its egocentric view. This
holistic approach results in a more effective zero-shot navigator capable of generating more coherent
and efficient trajectories."
2511.09109v2,http://arxiv.org/abs/2511.09109v2,2025-11-12 08:29:39+00:00,Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,"Retrieval-augmented generation (RAG) has proven to be effective in mitigating hallucinations in large language models, yet its effectiveness remains limited in complex, multi-step reasoning scenarios. Recent efforts have incorporated search-based interactions into RAG, enabling iterative reasoning with real-time retrieval. Most approaches rely on outcome-based supervision, offering no explicit guidance for intermediate steps. This often leads to reward hacking and degraded response quality. We propose Bi-RAR, a novel retrieval-augmented reasoning framework that evaluates each intermediate step jointly in both forward and backward directions. To assess the information completeness of each step, we introduce a bidirectional information distance grounded in Kolmogorov complexity, approximated via language model generation probabilities. This quantification measures both how far the current reasoning is from the answer and how well it addresses the question. To optimize reasoning under these bidirectional signals, we adopt a multi-objective reinforcement learning framework with a cascading reward structure that emphasizes early trajectory alignment. Empirical results on seven question answering benchmarks demonstrate that Bi-RAR surpasses previous methods and enables efficient interaction and reasoning with the search engine during training and inference.","\heading{Retrieval-augmented generation} 
Retrieval-augmented generation \cite{lewis2020retrieval} is a widely adopted framework that enhances large language models (LLMs) \cite{achiam2023gpt,team2024gemini,LLMSurvey} by incorporating external knowledge sources. 
This technique effectively reduces hallucination \cite{zhang2023siren,liu2024robust,liu2025attack,liu2025scaling} and improves task performance \cite{gao2023retrieval,shuster2021retrieval,jiang2023activeretrievalaugmentedgeneration,shi2025search,li2025towards}. 
Building on this foundation, many studies have explored improving the performance of RAG systems by optimising prompts or training objectives, such as Self-RAG, REPLUG, and RA-DIT \cite{asai2023Self-RAG, shi2023REPLUG, lin2024RA-DIT}.
However, this single-round framework of retrieval-then-answering makes LLMs difficult to capture sufficient information and perform complete reasoning, leading to poor performance in handling complex multi-hop reasoning tasks.
To address this, recent approaches incorporate multi-step reasoning and retrieval, retrieval-augmented reasoning, to further enhance the model’s capability in complex scenarios:
\begin{enumerate*}[label=(\roman*)] 
\item IRCoT \cite{trivedi2023ircot} interleaves retrieval within the chain-of-thought reasoning process;
\item Search-o1 \cite{li2025searcho1} enhances LLMs by integrating agentic search capabilities that dynamically retrieve and incorporate external knowledge during the reasoning process;
\item Search-R1 \cite{jin2025searchr1} uses reinforcement learning to train LLMs to autonomously generate search queries and use real-time retrieval during step-by-step reasoning.
\end{enumerate*} 

These methods are primarily guided by the final answer, encouraging LLMs to interact more with search engines.
However, such a ``distant'' supervision signal cannot provide precise guidance for each interaction, leading to over or distorted reasoning directions by LLMs.
An effective reasoning trajectory should continuously progress toward the solution while remaining grounded in the original problem context.

Therefore, we propose a bidirectional information quantification to define the optimization objective for each reasoning step, enabling LLMs to determine the reasoning direction based on the current information sufficiency.

\heading{LLMs and reinforcement learning} Reinforcement learning \cite{kaelbling1996reinforcement} has fundamentally reshaped how we align LLMs with human preferences, evolving from computationally intensive strategies to more elegant and efficient solutions.
Early implementations such as PPO required both a reward and a critic model \cite{ouyang2022training,schulman2017proximal}.
DPO simplified this process by removing the reward model and directly optimizing on preference data \cite{rafailov2023direct}, while GRPO further simplifies the pipeline by dropping the critic model and using sampled responses to estimate advantages \cite{shao2024deepseekmath}.
These advances have significantly enhanced LLM reasoning capabilities, as evidenced by models such as OpenAI's o1, DeepSeek-R1 and Qwen2.5 \citep{jaech2024openai,guo2025deepseek,qwen2025qwen25technicalreport}.

In practice, LLM training often involves multiple optimization objectives that need to be effectively balanced during reinforcement learning.
Multi-objective reinforcement learning (MORL) \cite{barrett2008learning,roijers2013survey,li2020deep} extends standard RL by replacing the single scalar reward signal with multiple feedback signals, each corresponding to a different objective. 
Recent work \cite{rame2023rewarded} has begun exploring multi-objective optimization for LLMs. 

In this paper, we adopt a multi-objective reinforcement learning approach to simultaneously optimize the forward and backward objectives in our framework to achieve an effective balanced solution.


% Our framework addresses these limitations by introducing a principled information-theoretic approach, guiding each reasoning step toward the correct direction based on available information.

%\heading{Limitations of Current Approaches} Existing methods usually rely on outcome-oriented reward signals. This unidirectional perspective fails to capture the essential bidirectional nature of effective reasoning—a reasoning step should simultaneously advance toward the solution while remaining grounded in the problem context. Our framework addresses these limitations by introducing a principled information-theoretic approach that naturally accommodates both objectives through Pareto-optimal multi-objective optimization.

%为了进一步细致优化每一步，一些工作提出对推理每一步进行优化 step by step优化，主要是对每一步进行额外信息检索，再重新让LLM生成推理，代表性的方法可以是XXX。
%however，这些方法只是依赖于外部信息进行内容修改，但是并没有实时判断当前推理内容是否足以支撑决策，根据信息完整度动态调整推理方向（没有一个判断信息是否好的过程）。因此，我们提出量化每一步推理内容的信息量，并进行reward优化。


%已经被用在大模型优化上。（练成一个句子）
%在本文中，我们就采用morl去优化双向的两个reward。XXXX.
%最近，也有一些多目标的强化学习？是否用到LLM推理中？在本文中，我们采用多目标的帕累托强化学习，去同时优化forward和backward两个目标。","\heading{Retrieval-augmented generation} 
Retrieval-augmented generation \cite{lewis2020retrieval} is a widely adopted framework that enhances large language models (LLMs) \cite{achiam2023gpt,team2024gemini,LLMSurvey} by incorporating external knowledge sources. 
This technique effectively reduces hallucination \cite{zhang2023siren,liu2024robust,liu2025attack,liu2025scaling} and improves task performance \cite{gao2023retrieval,shuster2021retrieval,jiang2023activeretrievalaugmentedgeneration,shi2025search,li2025towards}. 
Building on this foundation, many studies have explored improving the performance of RAG systems by optimising prompts or training objectives, such as Self-RAG, REPLUG, and RA-DIT \cite{asai2023Self-RAG, shi2023REPLUG, lin2024RA-DIT}.
However, this single-round framework of retrieval-then-answering makes LLMs difficult to capture sufficient information and perform complete reasoning, leading to poor performance in handling complex multi-hop reasoning tasks.
To address this, recent approaches incorporate multi-step reasoning and retrieval, retrieval-augmented reasoning, to further enhance the model’s capability in complex scenarios:
\begin{enumerate*}[label=(\roman*)] 
\item IRCoT \cite{trivedi2023ircot} interleaves retrieval within the chain-of-thought reasoning process;
\item Search-o1 \cite{li2025searcho1} enhances LLMs by integrating agentic search capabilities that dynamically retrieve and incorporate external knowledge during the reasoning process;
\item Search-R1 \cite{jin2025searchr1} uses reinforcement learning to train LLMs to autonomously generate search queries and use real-time retrieval during step-by-step reasoning.
\end{enumerate*} 

These methods are primarily guided by the final answer, encouraging LLMs to interact more with search engines.
However, such a ``distant'' supervision signal cannot provide precise guidance for each interaction, leading to over or distorted reasoning directions by LLMs.
An effective reasoning trajectory should continuously progress toward the solution while remaining grounded in the original problem context.

Therefore, we propose a bidirectional information quantification to define the optimization objective for each reasoning step, enabling LLMs to determine the reasoning direction based on the current information sufficiency.

\heading{LLMs and reinforcement learning} Reinforcement learning \cite{kaelbling1996reinforcement} has fundamentally reshaped how we align LLMs with human preferences, evolving from computationally intensive strategies to more elegant and efficient solutions.
Early implementations such as PPO required both a reward and a critic model \cite{ouyang2022training,schulman2017proximal}.
DPO simplified this process by removing the reward model and directly optimizing on preference data \cite{rafailov2023direct}, while GRPO further simplifies the pipeline by dropping the critic model and using sampled responses to estimate advantages \cite{shao2024deepseekmath}.
These advances have significantly enhanced LLM reasoning capabilities, as evidenced by models such as OpenAI's o1, DeepSeek-R1 and Qwen2.5 \citep{jaech2024openai,guo2025deepseek,qwen2025qwen25technicalreport}.

In practice, LLM training often involves multiple optimization objectives that need to be effectively balanced during reinforcement learning.
Multi-objective reinforcement learning (MORL) \cite{barrett2008learning,roijers2013survey,li2020deep} extends standard RL by replacing the single scalar reward signal with multiple feedback signals, each corresponding to a different objective. 
Recent work \cite{rame2023rewarded} has begun exploring multi-objective optimization for LLMs. 

In this paper, we adopt a multi-objective reinforcement learning approach to simultaneously optimize the forward and backward objectives in our framework to achieve an effective balanced solution.","Retrieval-augmented generation.Retrieval-augmented
generation (Lewis et al. 2020) is a widely adopted frame-
work that enhances large language models (LLMs) (Achiam
et al. 2023; Gemini Team et al. 2024; Zhao et al. 2023) by
incorporating external knowledge sources. This technique
effectively reduces hallucination (Zhang et al. 2023; Liu
et al. 2026, 2025a,b) and improves task performance (Gao
et al. 2023; Shuster et al. 2021; Jiang et al. 2023; Shi
et al. 2025; Li et al. 2025b). Building on this foundation,
many studies have explored improving the performance
of RAG systems by optimising prompts or training ob-
jectives, such as Self-RAG, REPLUG, and RA-DIT (Asai
et al. 2023; Shi et al. 2024; Lin et al. 2023). However,
this single-round framework of retrieval-then-answering
makes LLMs difficult to capture sufficient information and
perform complete reasoning, leading to poor performance
in handling complex multi-hop reasoning tasks. To address
this, recent approaches incorporate multi-step reasoning
and retrieval, retrieval-augmented reasoning, to further
enhance the model’s capability in complex scenarios:
(i) IRCoT (Trivedi et al. 2023) interleaves retrieval within
the chain-of-thought reasoning process; (ii) Search-o1 (Li
et al. 2025a) enhances LLMs by integrating agentic search
capabilities that dynamically retrieve and incorporate exter-
nal knowledge during the reasoning process; (iii) Search-R1
(Jin et al. 2025) uses reinforcement learning to train LLMs
to autonomously generate search queries and use real-time
retrieval during step-by-step reasoning.
These methods are primarily guided by the final answer,
encouraging LLMs to interact more with search engines.
However, such a “distant” supervision signal cannot provide
precise guidance for each interaction, leading to over or dis-
torted reasoning directions by LLMs. An effective reasoning
trajectory should continuously progress toward the solution
while remaining grounded in the original problem context.Therefore, we propose a bidirectional information quan-
tification to define the optimization objective for each rea-
soning step, enabling LLMs to determine the reasoning di-
rection based on the current information sufficiency.
LLMs and reinforcement learning.Reinforcement learn-
ing (Kaelbling, Littman, and Moore 1996) has fundamen-
tally reshaped how we align LLMs with human preferences,
evolving from computationally intensive strategies to more
elegant and efficient solutions. Early implementations such
as PPO required both a reward and a critic model (Ouyang
et al. 2022; Schulman et al. 2017). DPO simplified this pro-
cess by removing the reward model and directly optimiz-
ing on preference data (Rafailov et al. 2023), while GRPO
further simplifies the pipeline by dropping the critic model
and using sampled responses to estimate advantages (Shao
et al. 2024). These advances have significantly enhanced
LLM reasoning capabilities, as evidenced by models such
as OpenAI’s o1, DeepSeek-R1 and Qwen2.5 (Jaech et al.
2024; Guo et al. 2025; Yang et al. 2024b).
In practice, LLM training often involves multiple opti-
mization objectives that need to be effectively balanced dur-
ing reinforcement learning. Multi-objective reinforcement
learning (MORL) (Barrett and Narayanan 2008; Roijers
et al. 2013; Li, Zhang, and Wang 2020) extends standard RL
by replacing the single scalar reward signal with multiple
feedback signals, each corresponding to a different objec-
tive. Recent work (Rame et al. 2023) has begun exploring
multi-objective optimization for LLMs.
In this paper, we adopt a multi-objective reinforcement
learning approach to simultaneously optimize the forward
and backward objectives in our framework to achieve an ef-
fective balanced solution."
2511.10539v1,http://arxiv.org/abs/2511.10539v1,2025-11-13 17:39:06+00:00,Dynamic Avatar-Scene Rendering from Human-centric Context,"Reconstructing dynamic humans interacting with real-world environments from monocular videos is an important and challenging task. Despite considerable progress in 4D neural rendering, existing approaches either model dynamic scenes holistically or model scenes and backgrounds separately aim to introduce parametric human priors. However, these approaches either neglect distinct motion characteristics of various components in scene especially human, leading to incomplete reconstructions, or ignore the information exchange between the separately modeled components, resulting in spatial inconsistencies and visual artifacts at human-scene boundaries. To address this, we propose {\bf Separate-then-Map} (StM) strategy that introduces a dedicated information mapping mechanism to bridge separately defined and optimized models. Our method employs a shared transformation function for each Gaussian attribute to unify separately modeled components, enhancing computational efficiency by avoiding exhaustive pairwise interactions while ensuring spatial and visual coherence between humans and their surroundings. Extensive experiments on monocular video datasets demonstrate that StM significantly outperforms existing state-of-the-art methods in both visual quality and rendering accuracy, particularly at challenging human-scene interaction boundaries.","\subsection{4D novel view synthesis and reconstruction} 
4D Novel view synthesis and reconstruction provides a general and holistic solution for reconstructing dynamic objects and scene contents. Recent methods have introduced various dynamic representations such as dynamic NeRF \cite{li2022neural, li2021neural, park2021hypernerf}, dynamic triplane \cite{fridovich2023k, cao2023hexplane}, and dynamic Gaussian Splatting \cite{yang2023real, yang2023deformable3dgs, Wu_2024_CVPR, luiten2023dynamic, bae2024per}, enabling high-quality rendering from both calibrated multi-view and uncalibrated monocular RGB video inputs.
Especially, point-based explicit representations (e.g., Gaussian Splatting) have achieved state-of-the-art rendering quality while maintaining real-time inference and training efficiency.
Dynamic 3DGS \cite{luiten2023dynamic} optimize the position and shape of each Gaussian kernel frame-by-frame, while Deformable 3DGS \cite{yang2023deformable3dgs} and 4DGS \cite{Wu_2024_CVPR} introduce time-dependent deformation networks to deform a canonical 3D Gaussian into each frame.
\cite{yang2023real, li2024spacetime} expand 3D Gaussian primitives to 4D by incorporating temporal properties.
Despite their effectiveness, these approaches are most suitable for scenes with slow-moving objects \cite{park2021hypernerf, li2022neural}. 
Furthermore, they treat the entire scene holistically without integrating statistical models for regularization. This limitation is particularly significant in human-centric scenarios, where prior body models such as SMPL \cite{SMPL:2015} can provide valuable structural constraints. Therefore, our method incorporates human prior knowledge to better handle large motions.



\begin{figure*}[htbp]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1\linewidth]{imgs/fig2-pipeline.pdf}

   \caption{
\textbf{Overview of Separate-then-Map (\textbf{StM})}: Given an input video sequence, we first initialize the point clouds for the scene and avatar Gaussians using Colmap predictions and SMPL vertex points. This decoupled design is as follows: (a) a 3D Gaussian Splatting (3DGS) model represents the background scene, (b) a deformable 3D Gaussian avatar model driven by linear blend skinning (LBS) to represent the foreground human, with the parameters including position offset \(\Delta \mu\), rotation \(R\), scale \(S\), spherical harmonics (SH) coefficients \(C\), opacity \(O\), and LBS weight \(W\), all predicted from the position triplane feature \(\mu\); (c) A information mapping process is then employed to project the scene model and the avatar model into a consistent space. During training, the rendered images and depth maps are used to 
compute the loss
against the ground truth images and monocular estimated depth maps.
}
   
   % Overview of Separate-then-Align({\bf StA}): Given a video sequence as input, we firstly initial point cloud for scene and avatar Gaussians using Colmap predictions and SMPL vertices points. Seprate Gaussian Spaltting model define as below: (a) a static 3DGS as the scene model to present the background, which will be optimized via average gradient; 
   % (b)  a LBS-driven deformable 3D Gaussians as avatar model to repsent the foreground,  position offset $\Delta \mu $, rotation $R$, scale $S$, SH $C$, opacity $O$ and LBS weight $W$ driven by pridicting from position triplane $\mu $ feature to predict;
   % (c) Subsequently, a alignment process is then employed to project the stactic scene model and deformed avatar model to same consistence representation space; 
   % Finally, rendered image and depth map, can used to calculate photometric loss and depth correlation coefficient loss with ground truth image and monocular estimated depth. }
   \label{fig:pipeline}
\end{figure*}



\subsection{Human avatars reconstruction and animation} 
Human avatars reconstruction and animation from visual observations remains a challenging task.
Traditional methods relying on explicit, predefined parametric mesh topologies \cite{SMPL:2015, li2017learning, blanz2023morphable, romero2022embodied} often struggle to capture personalized appearance details \cite{alldieck2018detailed}.
With the advancements in neural representations \cite{mildenhall2021nerf, kerbl3Dgaussians}, recent studies \cite{jiang2023instantavatar, zhang2025humanref, weng2022humannerf, chen2023fast, chen2021snarf, gafni2021dynamic, yu2023monohuman, peng2021animatable, hu2024gauhuman, hu2024gaussianavatar, moon2024expressive, paudel2024ihuman, shao2024splattingavatar, wen2024gomavatar, qian20243dgs} have integrated parametric template models \cite{SMPL:2015, li2017learning, blanz2023morphable, romero2022embodied} to reconstruct dynamic human avatars. 
These hybrid approaches not only capture fine-grained, personalized details but also enable the animation of reconstructed avatars through simple pose guidance.
% Such hybrid representations capturing detailed personalized details, also allow animating the reconstructed avatar through simple pose guided. 
However, the aforementioned methods primarily focus on reconstructing and animating avatars in isolation, disregarding the surrounding environment.
In real-world scenarios, where visual inputs contain rich contextual information, it is essential to reconstruct both the avatar and its environment to achieve a more comprehensive understanding of the scene. 
Our method aims to reconstruct and animate human avatars within their captured environments.




\subsection{Dynamic human reconstruction and animation in context} 
Dynamic human reconstruction and animation in context is becoming increasingly important as virtual avatars interact more closely with their surrounding environments.
Recent methods \cite{jiang2022neuman, guo2023vid2avatar, kocabas2024hugs} have explored detaching the avatar from the background using separate neural fields while incorporating strong human priors to simplify the overall reconstruction process.
Specifically, Neuman \cite{jiang2022neuman} trains two NeRF models \cite{mildenhall2021nerf} - the scene NeRF and the human NeRF successively - encoding the human's appearance and geometry separately from the background.
Similarly, Vid2Avatar \cite{guo2023vid2avatar} models the foreground human and background scene using two separate neural fields without relying on external segmentation modules. 
Utilizing 3D Gaussian Splatting, HUGS \cite{kocabas2024hugs} defines two sets of Gaussian fields with distinct predefined properties, which are subsequently concatenated and rendered together during splatting and optimization.
Separating the background and avatar is an efficient strategy for reconstructing complex human motion while maintaining photorealistic rendering. 
However, existing methods do not consider how to project and recombine different model representations, potentially leading to inconsistencies in integration. To overcome this limitation, our method maps the representation spaces of the two models into a unified one during training, enabling more coherent and realistic reconstructions.","\subsection{4D novel view synthesis and reconstruction} 
4D Novel view synthesis and reconstruction provides a general and holistic solution for reconstructing dynamic objects and scene contents. Recent methods have introduced various dynamic representations such as dynamic NeRF \cite{li2022neural, li2021neural, park2021hypernerf}, dynamic triplane \cite{fridovich2023k, cao2023hexplane}, and dynamic Gaussian Splatting \cite{yang2023real, yang2023deformable3dgs, Wu_2024_CVPR, luiten2023dynamic, bae2024per}, enabling high-quality rendering from both calibrated multi-view and uncalibrated monocular RGB video inputs.
Especially, point-based explicit representations (e.g., Gaussian Splatting) have achieved state-of-the-art rendering quality while maintaining real-time inference and training efficiency.
Dynamic 3DGS \cite{luiten2023dynamic} optimize the position and shape of each Gaussian kernel frame-by-frame, while Deformable 3DGS \cite{yang2023deformable3dgs} and 4DGS \cite{Wu_2024_CVPR} introduce time-dependent deformation networks to deform a canonical 3D Gaussian into each frame.
\cite{yang2023real, li2024spacetime} expand 3D Gaussian primitives to 4D by incorporating temporal properties.
Despite their effectiveness, these approaches are most suitable for scenes with slow-moving objects \cite{park2021hypernerf, li2022neural}. 
Furthermore, they treat the entire scene holistically without integrating statistical models for regularization. This limitation is particularly significant in human-centric scenarios, where prior body models such as SMPL \cite{SMPL:2015} can provide valuable structural constraints. Therefore, our method incorporates human prior knowledge to better handle large motions.







\subsection{Human avatars reconstruction and animation} 
Human avatars reconstruction and animation from visual observations remains a challenging task.
Traditional methods relying on explicit, predefined parametric mesh topologies \cite{SMPL:2015, li2017learning, blanz2023morphable, romero2022embodied} often struggle to capture personalized appearance details \cite{alldieck2018detailed}.
With the advancements in neural representations \cite{mildenhall2021nerf, kerbl3Dgaussians}, recent studies \cite{jiang2023instantavatar, zhang2025humanref, weng2022humannerf, chen2023fast, chen2021snarf, gafni2021dynamic, yu2023monohuman, peng2021animatable, hu2024gauhuman, hu2024gaussianavatar, moon2024expressive, paudel2024ihuman, shao2024splattingavatar, wen2024gomavatar, qian20243dgs} have integrated parametric template models \cite{SMPL:2015, li2017learning, blanz2023morphable, romero2022embodied} to reconstruct dynamic human avatars. 
These hybrid approaches not only capture fine-grained, personalized details but also enable the animation of reconstructed avatars through simple pose guidance.

However, the aforementioned methods primarily focus on reconstructing and animating avatars in isolation, disregarding the surrounding environment.
In real-world scenarios, where visual inputs contain rich contextual information, it is essential to reconstruct both the avatar and its environment to achieve a more comprehensive understanding of the scene. 
Our method aims to reconstruct and animate human avatars within their captured environments.




\subsection{Dynamic human reconstruction and animation in context} 
Dynamic human reconstruction and animation in context is becoming increasingly important as virtual avatars interact more closely with their surrounding environments.
Recent methods \cite{jiang2022neuman, guo2023vid2avatar, kocabas2024hugs} have explored detaching the avatar from the background using separate neural fields while incorporating strong human priors to simplify the overall reconstruction process.
Specifically, Neuman \cite{jiang2022neuman} trains two NeRF models \cite{mildenhall2021nerf} - the scene NeRF and the human NeRF successively - encoding the human's appearance and geometry separately from the background.
Similarly, Vid2Avatar \cite{guo2023vid2avatar} models the foreground human and background scene using two separate neural fields without relying on external segmentation modules. 
Utilizing 3D Gaussian Splatting, HUGS \cite{kocabas2024hugs} defines two sets of Gaussian fields with distinct predefined properties, which are subsequently concatenated and rendered together during splatting and optimization.
Separating the background and avatar is an efficient strategy for reconstructing complex human motion while maintaining photorealistic rendering. 
However, existing methods do not consider how to project and recombine different model representations, potentially leading to inconsistencies in integration. To overcome this limitation, our method maps the representation spaces of the two models into a unified one during training, enabling more coherent and realistic reconstructions.",
2511.08697v1,http://arxiv.org/abs/2511.08697v1,2025-11-11 19:02:16+00:00,PEGNet: A Physics-Embedded Graph Network for Long-Term Stable Multiphysics Simulation,"Accurate and efficient simulations of physical phenomena governed by partial differential equations (PDEs) are important for scientific and engineering progress. While traditional numerical solvers are powerful, they are often computationally expensive. Recently, data-driven methods have emerged as alternatives, but they frequently suffer from error accumulation and limited physical consistency, especially in multiphysics and complex geometries. To address these challenges, we propose PEGNet, a Physics-Embedded Graph Network that incorporates PDE-guided message passing to redesign the graph neural network architecture. By embedding key PDE dynamics like convection, viscosity, and diffusion into distinct message functions, the model naturally integrates physical constraints into its forward propagation, producing more stable and physically consistent solutions. Additionally, a hierarchical architecture is employed to capture multi-scale features, and physical regularization is integrated into the loss function to further enforce adherence to governing physics. We evaluated PEGNet on benchmarks, including custom datasets for respiratory airflow and drug delivery, showing significant improvements in long-term prediction accuracy and physical consistency over existing methods. Our code is available at https://github.com/Yanghuoshan/PEGNet.","\label{sec:relatedwork}

\paragraph{GNNs for Physical Simulation}
Pioneering works from Interaction Networks \cite{battaglia2016interaction} to GNS \cite{sanchez2020learning} and MGN \cite{pfaff2020learning} have shown that GNNs provide a natural framework for learning physical dynamics on irregular domains such as particle systems or unstructured meshes.
These methods have been successfully applied in rigid-body physics \cite{yang2025mbds}, fluid dynamics \cite{gao2024predicting}, and elasticity \cite{deshpande2024magnet}.
However, most GNN-based methods adopt purely data-driven message passing, neglecting physical constraints. 
This limitation results in error accumulation and violations of physical consistency over long-term predictions.

\paragraph{Hierarchical GNN Architectures}
Hierarchical GNNs address long-range dependencies, resolution heterogeneity, and computational cost by constructing multi-scale graph representations via pooling or coarsening. Methods such as DiffPool \cite{ying2018hierarchical} and TopK pooling \cite{cangea2018sparsehierarchicalgraphclassifiers} have proven effective in graph compression.
In physical modeling, approaches like SGUNET \cite{shen2025transferlearningscalablegraph} and BSMS-GNN \cite{cao2023efficient} employ non-learnable pooling to improve generalization. These designs are especially useful for complex domains like anatomical airways, where the meshes are numerous and multi-scale.

\paragraph{Physics-Inspired Learning}
PINNs \cite{raissi2019physics} incorporate PDE residuals into loss functions, while operator learning methods such as DeepONets \cite{Lu_2021} and FNOs \cite{lifourier} aim to directly learn mappings between function spaces. Physics-informed operator networks \cite{goswami2023physics} and physics-encoded or physics-embedded approaches \cite{rao2023encoding, guo2021physics} further embed physical constraints into model design, with ongoing work exploring higher-order integration schemes \cite{wang2025multipdenetpdeembeddedlearningmultitimestepping}.
To handle irregular geometries and large-scale data, several studies combine these physics-based approaches with GNNs \cite{horie2022physics, zeng2025phympgnphysicsencodedmessagepassing}, achieving promising results in single-phase flow modeling but requiring further development for multiphysics scenarios.","\paragraph{GNNs for Physical Simulation}
Pioneering works from Interaction Networks \cite{battaglia2016interaction} to GNS \cite{sanchez2020learning} and MGN \cite{pfaff2020learning} have shown that GNNs provide a natural framework for learning physical dynamics on irregular domains such as particle systems or unstructured meshes.
These methods have been successfully applied in rigid-body physics \cite{yang2025mbds}, fluid dynamics \cite{gao2024predicting}, and elasticity \cite{deshpande2024magnet}.
However, most GNN-based methods adopt purely data-driven message passing, neglecting physical constraints. 
This limitation results in error accumulation and violations of physical consistency over long-term predictions.

\paragraph{Hierarchical GNN Architectures}
Hierarchical GNNs address long-range dependencies, resolution heterogeneity, and computational cost by constructing multi-scale graph representations via pooling or coarsening. Methods such as DiffPool \cite{ying2018hierarchical} and TopK pooling \cite{cangea2018sparsehierarchicalgraphclassifiers} have proven effective in graph compression.
In physical modeling, approaches like SGUNET \cite{shen2025transferlearningscalablegraph} and BSMS-GNN \cite{cao2023efficient} employ non-learnable pooling to improve generalization. These designs are especially useful for complex domains like anatomical airways, where the meshes are numerous and multi-scale.

\paragraph{Physics-Inspired Learning}
PINNs \cite{raissi2019physics} incorporate PDE residuals into loss functions, while operator learning methods such as DeepONets \cite{Lu_2021} and FNOs \cite{lifourier} aim to directly learn mappings between function spaces. Physics-informed operator networks \cite{goswami2023physics} and physics-encoded or physics-embedded approaches \cite{rao2023encoding, guo2021physics} further embed physical constraints into model design, with ongoing work exploring higher-order integration schemes \cite{wang2025multipdenetpdeembeddedlearningmultitimestepping}.
To handle irregular geometries and large-scale data, several studies combine these physics-based approaches with GNNs \cite{horie2022physics, zeng2025phympgnphysicsencodedmessagepassing}, achieving promising results in single-phase flow modeling but requiring further development for multiphysics scenarios.",
2511.08191v1,http://arxiv.org/abs/2511.08191v1,2025-11-11 12:58:25+00:00,Towards Provably Unlearnable Examples via Bayes Error Optimisation,"The recent success of machine learning models, especially large-scale classifiers and language models, relies heavily on training with massive data. These data are often collected from online sources. This raises serious concerns about the protection of user data, as individuals may not have given consent for their data to be used in training. To address this concern, recent studies introduce the concept of unlearnable examples, i.e., data instances that appear natural but are intentionally altered to prevent models from effectively learning from them. While existing methods demonstrate empirical effectiveness, they typically rely on heuristic trials and lack formal guarantees. Besides, when unlearnable examples are mixed with clean data, as is often the case in practice, their unlearnability disappears. In this work, we propose a novel approach to constructing unlearnable examples by systematically maximising the Bayes error, a measurement of irreducible classification error. We develop an optimisation-based approach and provide an efficient solution using projected gradient ascent. Our method provably increases the Bayes error and remains effective when the unlearning examples are mixed with clean samples. Experimental results across multiple datasets and model architectures are consistent with our theoretical analysis and show that our approach can restrict data learnability, effectively in practice.","The idea of protecting data at the source has gained increasing attention due to the rising concerns over unauthorised data usage. \citet{huang2021unlearnable} and \citet{ren2022transferable} introduced the concept of unlearnable examples, where imperceptible perturbations are applied to make data inherently difficult to learn. Approaches to constructing unlearnable examples have typically followed two heuristics: (1) maximising the training loss to prevent models from fitting useful features~\cite{wen2023adversarial,fowl2021adversarial}, and (2) minimising it to encourage overfitting on non-generalizable patterns~\cite{huang2021unlearnable,fu2022robust}. There are also other approaches~\cite{liu2024game,liu2024stable,liu2023reliable,mu2025bayesian}, including directly optimising perturbations for specific model parameters~\cite{lu2023exploring} or target retraining scenarios using sharpness-aware techniques~\cite{he2023sharpness}. Such created unlearnable examples have been found to work ineffectively when mixed with clean data and lack formal guarantees. This work thus aims to tackle the problem of ensuring unlearnability even in the mixed data situation. 

Estimating the Bayes error of a given data distribution has long been an interesting topic~\cite{fukunaga1975k}. Related methods include estimating its lower or upper bounds, e.g. the Bhattacharyya distance~\cite{fukunaga1990introduction} or the Henze-Penrose divergence~\cite{berisha2015empirically,sekeh2020learning}. Alternatively, there are attempts to estimate the Bayes error using generative models~\cite{kingma2018glow,theisen2021evaluating} or real-world human annotation~\cite{renggli2021evaluating}. Indirect Bayes error estimation has also been adopted by quantity substitution, e.g., accuracy to robustness~\cite{zhang2024certified,zhang2024does}. Compared with these estimates, our Bayes error estimate is easier to compute, such as to model and improve unlearnability.","The idea of protecting data at the source has gained increasing attention due to the rising concerns over unauthorised data usage. \citet{huang2021unlearnable} and \citet{ren2022transferable} introduced the concept of unlearnable examples, where imperceptible perturbations are applied to make data inherently difficult to learn. Approaches to constructing unlearnable examples have typically followed two heuristics: (1) maximising the training loss to prevent models from fitting useful features~\cite{wen2023adversarial,fowl2021adversarial}, and (2) minimising it to encourage overfitting on non-generalizable patterns~\cite{huang2021unlearnable,fu2022robust}. There are also other approaches~\cite{liu2024game,liu2024stable,liu2023reliable,mu2025bayesian}, including directly optimising perturbations for specific model parameters~\cite{lu2023exploring} or target retraining scenarios using sharpness-aware techniques~\cite{he2023sharpness}. Such created unlearnable examples have been found to work ineffectively when mixed with clean data and lack formal guarantees. This work thus aims to tackle the problem of ensuring unlearnability even in the mixed data situation. 

Estimating the Bayes error of a given data distribution has long been an interesting topic~\cite{fukunaga1975k}. Related methods include estimating its lower or upper bounds, e.g. the Bhattacharyya distance~\cite{fukunaga1990introduction} or the Henze-Penrose divergence~\cite{berisha2015empirically,sekeh2020learning}. Alternatively, there are attempts to estimate the Bayes error using generative models~\cite{kingma2018glow,theisen2021evaluating} or real-world human annotation~\cite{renggli2021evaluating}. Indirect Bayes error estimation has also been adopted by quantity substitution, e.g., accuracy to robustness~\cite{zhang2024certified,zhang2024does}. Compared with these estimates, our Bayes error estimate is easier to compute, such as to model and improve unlearnability.","The idea of protecting data at the source has gained increasing
attention due to the rising concerns over unauthorised data
usage. Huang et al. (2021) and Ren et al. (2022) introduced
the concept of unlearnable examples, where imperceptible
perturbations are applied to make data inherently difficult to
learn. Approaches to constructing unlearnable examples have
typically followed two heuristics: (1) maximising the training
loss to prevent models from fitting useful features (Wen et al.
2023; Fowl et al. 2021), and (2) minimising it to encourage
overfitting on non-generalizable patterns (Huang et al. 2021;
Fu et al. 2022). There are also other approaches (Liu, Wang,
and Gao 2024; Liu et al. 2024; Liu, Peng, and Tang 2023;
Mu and Lim 2025), including directly optimising perturba-
tions for specific model parameters (Lu, Kamath, and Yu
2023) or target retraining scenarios using sharpness-aware
techniques (He et al. 2023). Such created unlearnable exam-
ples have been found to work ineffectively when mixed with
clean data and lack formal guarantees. This work thus aims
to tackle the problem of ensuring unlearnability even in the
mixed data situation.
Estimating the Bayes error of a given data distribution
has long been an interesting topic (Fukunaga and Hostetler
1975). Related methods include estimating its lower or upper
bounds, e.g. the Bhattacharyya distance (Fukunaga 1990) or
the Henze-Penrose divergence (Berisha et al. 2016; Sekeh,
Oselio, and Hero 2020). Alternatively, there are attempts to
estimate the Bayes error using generative models (Kingma
and Dhariwal 2018; Theisen et al. 2021) or real-world human
annotation (Renggli et al. 2021). Indirect Bayes error esti-
mation has also been adopted by quantity substitution, e.g.,
accuracy to robustness (Zhang and Sun 2024a,b). Compared
with these estimates, our Bayes error estimate is easier to
compute, such as to model and improve unlearnability."
2511.10442v1,http://arxiv.org/abs/2511.10442v1,2025-11-13 16:06:13+00:00,FastGraph: Optimized GPU-Enabled Algorithms for Fast Graph Building and Message Passing,"We introduce FastGraph, a novel GPU-optimized k-nearest neighbor algorithm specifically designed to accelerate graph construction in low-dimensional spaces (2-10 dimensions), critical for high-performance graph neural networks. Our method employs a GPU-resident, bin-partitioned approach with full gradient-flow support and adaptive parameter tuning, significantly enhancing both computational and memory efficiency. Benchmarking demonstrates that FastGraph achieves a 20-40x speedup over state-of-the-art libraries such as FAISS, ANNOY, and SCANN in dimensions less than 10 with virtually no memory overhead. These improvements directly translate into substantial performance gains for GNN-based workflows, particularly benefiting computationally intensive applications in low dimensions such as particle clustering in high-energy physics, visual object tracking, and graph clustering.","The $k$-nearest neighbor ($k$NN) problem has been addressed through both exact and approximate methods. In low-dimensional settings, exact search can be accelerated with space-partitioning data structures such as KD-trees or ball trees, which offer \(O(n \log n)\) index construction and about \(O(\log n)\) query time on average. These complexities are significantly better than the brute-force \(O(n)\) scan in low dimensions. However, as dimensionality increases, performance degrades sharply—a phenomenon often attributed to the curse of dimensionality, which undermines pruning efficiency and forces KD-tree queries to examine nearly all partitions~\cite{panigrahyHighDim, yaledmKdTreeFail}. In practice, KD-tree and ball-tree implementations (e.g., in Scikit-learn or SciPy) thus often fall back to brute-force scanning in high-dimensional regimes, limiting their utility and motivating the adoption of other strategies.

Modern GPUs shift this paradigm: the distance computation at the core of $k$NN is embarrassingly parallel. Libraries such as FAISS~\cite{johnson2019billion} exploit this by offering GPU-resident brute-force indices that keep all data in device memory, eliminating host–device transfer overhead while scaling well across dataset size and dimensionality. A notable demonstration comes from the Exa.TrkX tracking pipeline, where replacing PyTorch Geometric's default \texttt{radius\_graph} operator with FAISS’s GPU-based Flat index reduced per-event graph construction time from around 12 seconds to less than 1 second—approximately a 20x speedup in practice~\cite{lazar2022accelerating}.

Beyond brute-force GPU acceleration, a rich body of work has targeted approximate nearest neighbor (ANN) search via graph-based approaches. SONG~\cite{zhao2020song} pioneered GPU-oriented ANN by decomposing graph traversal into three GPU-friendly stages—candidate localization, bulk distance computation, and index maintenance, achieving reported $50$–$180\times$ speedups over single-threaded CPU HNSW and outperforming GPU FAISS in accuracy–speed tradeoffs. Against multi-threaded CPU baselines, SONG's gains are smaller (about $3$–$11\times$ over 16 threads), but its key novelty lies in making the search step GPU-parallel rather than proposing a new graph-building strategy. GGNN~\cite{ggnn} extended this direction by introducing a GPU-friendly graph structure and neighborhood propagation scheme for both index construction and querying. Its end-to-end GPU pipeline significantly surpassed SONG on large-batch query workloads, highlighting the evolution of GPU ANN methods. Taken together, these advances demonstrate significant progress in three directions: maximizing GPU parallelism, accelerating index construction, and scaling to datasets larger than GPU memory.

\textbf{FastGraph} is designed specifically for low-to-moderate dimensional spaces (2–10 dimensions), a regime common in GNN latent embeddings. Our approach combines GPU-resident performance with adaptive bin partitioning, static compile-time specialization, and full gradient-flow support, making $k$NN fast and differentiation-compatible. This fills a complementary gap left open by previous GPU ANN and $k$NN research: enabling low-dimensional graph structures to be learned jointly with model parameters in gradient-based training workflows. 
Furthermore, the FastGraph $k$NN algorithm comes with virtually no additional memory overhead on input and output arrays.","The $k$-nearest neighbor ($k$NN) problem has been addressed through both exact and approximate methods. In low-dimensional settings, exact search can be accelerated with space-partitioning data structures such as KD-trees or ball trees, which offer \(O(n \log n)\) index construction and about \(O(\log n)\) query time on average. These complexities are significantly better than the brute-force \(O(n)\) scan in low dimensions. However, as dimensionality increases, performance degrades sharply—a phenomenon often attributed to the curse of dimensionality, which undermines pruning efficiency and forces KD-tree queries to examine nearly all partitions~\cite{panigrahyHighDim, yaledmKdTreeFail}. In practice, KD-tree and ball-tree implementations (e.g., in Scikit-learn or SciPy) thus often fall back to brute-force scanning in high-dimensional regimes, limiting their utility and motivating the adoption of other strategies.

Modern GPUs shift this paradigm: the distance computation at the core of $k$NN is embarrassingly parallel. Libraries such as FAISS~\cite{johnson2019billion} exploit this by offering GPU-resident brute-force indices that keep all data in device memory, eliminating host–device transfer overhead while scaling well across dataset size and dimensionality. A notable demonstration comes from the Exa.TrkX tracking pipeline, where replacing PyTorch Geometric's default \texttt{radius\_graph} operator with FAISS’s GPU-based Flat index reduced per-event graph construction time from around 12 seconds to less than 1 second—approximately a 20x speedup in practice~\cite{lazar2022accelerating}.

Beyond brute-force GPU acceleration, a rich body of work has targeted approximate nearest neighbor (ANN) search via graph-based approaches. SONG~\cite{zhao2020song} pioneered GPU-oriented ANN by decomposing graph traversal into three GPU-friendly stages—candidate localization, bulk distance computation, and index maintenance, achieving reported $50$–$180\times$ speedups over single-threaded CPU HNSW and outperforming GPU FAISS in accuracy–speed tradeoffs. Against multi-threaded CPU baselines, SONG's gains are smaller (about $3$–$11\times$ over 16 threads), but its key novelty lies in making the search step GPU-parallel rather than proposing a new graph-building strategy. GGNN~\cite{ggnn} extended this direction by introducing a GPU-friendly graph structure and neighborhood propagation scheme for both index construction and querying. Its end-to-end GPU pipeline significantly surpassed SONG on large-batch query workloads, highlighting the evolution of GPU ANN methods. Taken together, these advances demonstrate significant progress in three directions: maximizing GPU parallelism, accelerating index construction, and scaling to datasets larger than GPU memory.

\textbf{FastGraph} is designed specifically for low-to-moderate dimensional spaces (2–10 dimensions), a regime common in GNN latent embeddings. Our approach combines GPU-resident performance with adaptive bin partitioning, static compile-time specialization, and full gradient-flow support, making $k$NN fast and differentiation-compatible. This fills a complementary gap left open by previous GPU ANN and $k$NN research: enabling low-dimensional graph structures to be learned jointly with model parameters in gradient-based training workflows. 
Furthermore, the FastGraph $k$NN algorithm comes with virtually no additional memory overhead on input and output arrays.","Thek-nearest neighbor ( kNN) problem has been addressed through both exact and ap-
proximate methods. In low-dimensional settings, exact search can be accelerated with
space-partitioning data structures such as KD-trees or ball trees, which offer O(nlogn )
index construction and about O(logn )query time on average. These complexities are signifi-
cantly better than the brute-force O(n)scan in low dimensions. However, as dimensionality
increases, performance degrades sharply—a phenomenon often attributed to the curse of
dimensionality, which undermines pruning efficiency and forces KD-tree queries to examine
nearly all partitions Panigrahy (2008); Course (2013). In practice, KD-tree and ball-tree
implementations (e.g., in Scikit-learn or SciPy) thus often fall back to brute-force scanning
in high-dimensional regimes, limiting their utility and motivating the adoption of other
strategies.
Modern GPUs shift this paradigm: the distance computation at the core of kNN is
embarrassingly parallel. Libraries such as FAISS Johnson et al. (2019) exploit this by
offering GPU-resident brute-force indices that keep all data in device memory, eliminating
host–device transfer overhead while scaling well across dataset size and dimensionality. A
notable demonstration comes from the Exa.TrkX tracking pipeline, where replacing PyTorch
Geometric’s default radius_graph operator with FAISS’s GPU-based Flat index reduced per-
event graph construction time from around 12 seconds to less than 1 second—approximately
a 20x speedup in practice Lazar et al. (2022).
Beyond brute-force GPU acceleration, a rich body of work has targeted approximate
nearest neighbor (ANN) search via graph-based approaches. SONG Zhao et al. (2020)
pioneered GPU-oriented ANN by decomposing graph traversal into three GPU-friendly
stages—candidate localization, bulk distance computation, and index maintenance, achieving
2
FastGraph: GPU-Optimized Graph Building
reported50–180 ×speedups over single-threaded CPU HNSW and outperforming GPU
FAISS in accuracy–speed tradeoffs. Against multi-threaded CPU baselines, SONG’s gains
are smaller (about3–11 ×over 16 threads), but its key novelty lies in making the search step
GPU-parallel rather than proposing a new graph-building strategy. GGNN Groh et al. (2023)
extended this direction by introducing a GPU-friendly graph structure and neighborhood
propagation scheme for both index construction and querying. Its end-to-end GPU pipeline
significantly surpassed SONG on large-batch query workloads, highlighting the evolution of
GPU ANN methods. Taken together, these advances demonstrate significant progress in
three directions: maximizing GPU parallelism, accelerating index construction, and scaling
to datasets larger than GPU memory.
FastGraphis designed specifically for low-to-moderate dimensional spaces (2–10 dimen-
sions), a regime common in GNN latent embeddings. Our approach combines GPU-resident
performance with adaptive bin partitioning, static compile-time specialization, and full
gradient-flow support, making kNN fast and differentiation-compatible. This fills a comple-
mentary gap left open by previous GPU ANN and kNN research: enabling low-dimensional
graph structures to be learned jointly with model parameters in gradient-based training
workflows. Furthermore, the FastGraph kNN algorithm comes with virtually no additional
memory overhead on input and output arrays."
2511.09149v1,http://arxiv.org/abs/2511.09149v1,2025-11-12 09:37:22+00:00,Enabling Agents to Communicate Entirely in Latent Space,"While natural language is the de facto communication medium for LLM-based agents, it presents a fundamental constraint. The process of downsampling rich, internal latent states into discrete tokens inherently limits the depth and nuance of information that can be transmitted, thereby hindering collaborative problem-solving. Inspired by human mind-reading, we propose Interlat (Inter-agent Latent Space Communication), a paradigm that leverages the last hidden states of an LLM as a representation of its mind for direct transmission (termed latent communication). An additional compression process further compresses latent communication via entirely latent space reasoning. Experiments demonstrate that Interlat outperforms both fine-tuned chain-of-thought (CoT) prompting and single-agent baselines, promoting more exploratory behavior and enabling genuine utilization of latent information. Further compression not only substantially accelerates inference but also maintains competitive performance through an efficient information-preserving mechanism. We position this work as a feasibility study of entirely latent space inter-agent communication, and our results highlight its potential, offering valuable insights for future research.","\label{sec:related_work}

\paragraph{Latent Reasoning in LLMs.}
A growing line of work shifts reasoning from the language space to the latent space, replacing explicit CoT traces with multi-step computation in continuous representations to bypass the bandwidth and efficiency constraints of text ($\approx15$ bits/token vs. $\approx40$k bits/hidden-state)~\citep{zhu2025survey}.
To increase available compute at inference time, \citep{goyal2023think} introduces learnable pause tokens that delay the generation of the final answer, while \citep{pfau2024let} employs filler tokens to scaffold intermediate computations that would be infeasible without generating tokens. Beyond token scheduling, \citep{liu2024deliberation} proposes a latent coprocessor operating directly on the transformer KV cache to improve performance. Another line of work~\citep{hao2024training,shen2025codi,cheng2024compressed} enables the model to reason in latent space by feeding the last hidden state back as the next input embedding, enabling the model to explore multiple reasoning paths in parallel, akin to a breadth-first search. \citep{bae2024relaxed,gao2024algoformer,geiping2025scaling} decouple input encoding, iterative reasoning, and output decoding, making the computation more modular and interpretable. Building upon these advantages of latent space for reasoning, our work shifts focus from single-model latent reasoning to enabling inter-agent communication and task solving in latent space.

\paragraph{Multi-agent Communication.}
LLM-based agent systems typically orchestrate in natural language~\citep{qian2024scaling,zhu2025multiagentbench,wang2024survey}. Although readable by humans, natural language introduces constraints on a model's expressive range and can also impose redundant computation.~\citep{pham2023let} replaces sampled tokens with probability-weighted tokenizer embeddings, preserving more of the model’s belief distribution and improving debate performance, but it still communicates surface-level final-layer distributions, overlooking deeper, more informative, and more valuable hidden representations.~\citep{ramesh2025communicating} blends hidden states between agents, yielding accuracy gains with less compute than natural-language messages; however, it operates as a single hidden state graft within a pass rather than a temporally structured latent sequence.
~\citep{tang2025augmenting} preserves richer information by recording per-token state deltas at selected layers on the sender and adding them at the corresponding layer positions on the receiver while transmitting text, but requires model-specific layer selection and language space text.
Building on these valuable insights, we transmit a sequence of last hidden states directly between agents and apply a compression process to enable information-rich, language-free, and efficient communication.","\paragraph{Latent Reasoning in LLMs.}
A growing line of work shifts reasoning from the language space to the latent space, replacing explicit CoT traces with multi-step computation in continuous representations to bypass the bandwidth and efficiency constraints of text ($\approx15$ bits/token vs. $\approx40$k bits/hidden-state)~\citep{zhu2025survey}.
To increase available compute at inference time, \citep{goyal2023think} introduces learnable pause tokens that delay the generation of the final answer, while \citep{pfau2024let} employs filler tokens to scaffold intermediate computations that would be infeasible without generating tokens. Beyond token scheduling, \citep{liu2024deliberation} proposes a latent coprocessor operating directly on the transformer KV cache to improve performance. Another line of work~\citep{hao2024training,shen2025codi,cheng2024compressed} enables the model to reason in latent space by feeding the last hidden state back as the next input embedding, enabling the model to explore multiple reasoning paths in parallel, akin to a breadth-first search. \citep{bae2024relaxed,gao2024algoformer,geiping2025scaling} decouple input encoding, iterative reasoning, and output decoding, making the computation more modular and interpretable. Building upon these advantages of latent space for reasoning, our work shifts focus from single-model latent reasoning to enabling inter-agent communication and task solving in latent space.

\paragraph{Multi-agent Communication.}
LLM-based agent systems typically orchestrate in natural language~\citep{qian2024scaling,zhu2025multiagentbench,wang2024survey}. Although readable by humans, natural language introduces constraints on a model's expressive range and can also impose redundant computation.~\citep{pham2023let} replaces sampled tokens with probability-weighted tokenizer embeddings, preserving more of the model’s belief distribution and improving debate performance, but it still communicates surface-level final-layer distributions, overlooking deeper, more informative, and more valuable hidden representations.~\citep{ramesh2025communicating} blends hidden states between agents, yielding accuracy gains with less compute than natural-language messages; however, it operates as a single hidden state graft within a pass rather than a temporally structured latent sequence.
~\citep{tang2025augmenting} preserves richer information by recording per-token state deltas at selected layers on the sender and adding them at the corresponding layer positions on the receiver while transmitting text, but requires model-specific layer selection and language space text.
Building on these valuable insights, we transmit a sequence of last hidden states directly between agents and apply a compression process to enable information-rich, language-free, and efficient communication.",
2511.10032v1,http://arxiv.org/abs/2511.10032v1,2025-11-13 07:11:21+00:00,Moral Change or Noise? On Problems of Aligning AI With Temporally Unstable Human Feedback,"Alignment methods in moral domains seek to elicit moral preferences of human stakeholders and incorporate them into AI. This presupposes moral preferences as static targets, but such preferences often evolve over time. Proper alignment of AI to dynamic human preferences should ideally account for ""legitimate"" changes to moral reasoning, while ignoring changes related to attention deficits, cognitive biases, or other arbitrary factors. However, common AI alignment approaches largely neglect temporal changes in preferences, posing serious challenges to proper alignment, especially in high-stakes applications of AI, e.g., in healthcare domains, where misalignment can jeopardize the trustworthiness of the system and yield serious individual and societal harms. This work investigates the extent to which people's moral preferences change over time, and the impact of such changes on AI alignment. Our study is grounded in the kidney allocation domain, where we elicit responses to pairwise comparisons of hypothetical kidney transplant patients from over 400 participants across 3-5 sessions. We find that, on average, participants change their response to the same scenario presented at different times around 6-20% of the time (exhibiting ""response instability""). Additionally, we observe significant shifts in several participants' retrofitted decision-making models over time (capturing ""model instability""). The predictive performance of simple AI models decreases as a function of both response and model instability. Moreover, predictive performance diminishes over time, highlighting the importance of accounting for temporal changes in preferences during training. These findings raise fundamental normative and technical challenges relevant to AI alignment, highlighting the need to better understand the object of alignment (what to align to) when user preferences change significantly over time.","Recent surges in the use of AI, and the associated risks, have led to increased research on AI alignment.
We refer the reader to \citet{ji2023ai} and \citet{shen2024towards} for an overview of AI alignment methods.
While these methods generally rely on one-shot/episode collection of human feedback, human preferences are dynamic and context-sensitive \citep{slovic1995construction,zhi-xuan_beyond_2024}. 
Recent works have noted many issues with assuming static preferences, such as conflicts between learned and desired preferences \citep{carroll2024ai, curmei2022towards, kleinberg2024inversion} and flattening of preference heterogeneity \citep{buyl2025ai}.
These theoretical challenges motivate our 
empirical study of human preferences over time.
Temporal misalignment is
especially pressing for preference optimization methods that train models on fixed datasets of human choices \citep{liu_survey_2025, boerstler2024stability}. Recent studies highlight that models trained on such data generalize poorly under distributional shift \citep{lin_limited_2024}, and that aggregating heterogeneous preferences can lead to inconsistent performance \citep{shirali_direct_2025}.
Such findings cast doubt on the reliability of one-shot preference elicitation.





A wide range of alignment research uses moral preference datasets
as foundational benchmarks. The Moral Machine and ETHICS datasets, for example, have informed the development and evaluation of models for moral reasoning across various domains \citep{noothigattu_voting-based_2018, wiedeman_modeling_2020, rodionov_evaluation_2023, hendrycks_aligning_2023, zaim_bin_ahmad_large-scale_2025}. Similar strategies are applied in medical ethics and resource triage settings \citep{dickerson2025gets, mohsin_learning_2025}. 
Yet, AI moral alignment methods often treat human preferences as static, despite evidence from moral psychology showing otherwise \cite{rehren2023stable}. 
\citet{boerstler2024stability} investigated the scale of moral judgment instability in the kidney allocation domain; however, their studies had relatively small participant pools (fewer than 50) and limited analysis of impact of instability on AI alignment.
Our study follows a similar design, but with significantly more participants to better assess the impact of temporal instabilities on alignment. 
The consequences of misalignment 
can be much more severe in moral AI domains.
If people change their movie/music preferences over time, a misaligned recommender AI would 
result in unmet entertainment needs.
In contrast, when someone has one medical resource allocation policy one day but changes it to create a more equitable policy on a later day, 
then misalignment in this moral domain can be seen to be much more consequential. 
Our work is motivated by the need to tackle these potential consequences of temporal misalignment.","Recent surges in the use of AI, and the associated risks, have led to increased research on AI alignment.
We refer the reader to \citet{ji2023ai} and \citet{shen2024towards} for an overview of AI alignment methods.
While these methods generally rely on one-shot/episode collection of human feedback, human preferences are dynamic and context-sensitive \citep{slovic1995construction,zhi-xuan_beyond_2024}. 
Recent works have noted many issues with assuming static preferences, such as conflicts between learned and desired preferences \citep{carroll2024ai, curmei2022towards, kleinberg2024inversion} and flattening of preference heterogeneity \citep{buyl2025ai}.
These theoretical challenges motivate our 
empirical study of human preferences over time.
Temporal misalignment is
especially pressing for preference optimization methods that train models on fixed datasets of human choices \citep{liu_survey_2025, boerstler2024stability}. Recent studies highlight that models trained on such data generalize poorly under distributional shift \citep{lin_limited_2024}, and that aggregating heterogeneous preferences can lead to inconsistent performance \citep{shirali_direct_2025}.
Such findings cast doubt on the reliability of one-shot preference elicitation.





A wide range of alignment research uses moral preference datasets
as foundational benchmarks. The Moral Machine and ETHICS datasets, for example, have informed the development and evaluation of models for moral reasoning across various domains \citep{noothigattu_voting-based_2018, wiedeman_modeling_2020, rodionov_evaluation_2023, hendrycks_aligning_2023, zaim_bin_ahmad_large-scale_2025}. Similar strategies are applied in medical ethics and resource triage settings \citep{dickerson2025gets, mohsin_learning_2025}. 
Yet, AI moral alignment methods often treat human preferences as static, despite evidence from moral psychology showing otherwise \cite{rehren2023stable}. 
\citet{boerstler2024stability} investigated the scale of moral judgment instability in the kidney allocation domain; however, their studies had relatively small participant pools (fewer than 50) and limited analysis of impact of instability on AI alignment.
Our study follows a similar design, but with significantly more participants to better assess the impact of temporal instabilities on alignment. 
The consequences of misalignment 
can be much more severe in moral AI domains.
If people change their movie/music preferences over time, a misaligned recommender AI would 
result in unmet entertainment needs.
In contrast, when someone has one medical resource allocation policy one day but changes it to create a more equitable policy on a later day, 
then misalignment in this moral domain can be seen to be much more consequential. 
Our work is motivated by the need to tackle these potential consequences of temporal misalignment.","Recent surges in the use of AI, and the associated risks, have led to increased research on AI
alignment. We refer the reader to Ji et al. [22] and Shen et al. [41] for an overview of AI alignment
methods. While these methods generally rely on one-shot/episode collection of human feedback,
human preferences are dynamic and context-sensitive [45, 59]. Recent works have noted many
issues with assuming static preferences, such as conflicts between learned and desired preferences
[7, 13, 28] and flattening of preference heterogeneity [6]. These theoretical challenges motivate our
empirical study of human preferences over time. Temporal misalignment is especially pressing
for preference optimization methods that train models on fixed datasets of human choices [31, 4].
Recent studies highlight that models trained on such data generalize poorly under distributional
shift [30], and that aggregating heterogeneous preferences can lead to inconsistent performance
[43]. Such findings cast doubt on the reliability of one-shot preference elicitation.
A wide range of alignment research uses moral preference datasets as foundational benchmarks.
The Moral Machine and ETHICS datasets, for example, have informed the development and eval-
uation of models for moral reasoning across various domains [33, 55, 38, 19, 58]. Similar strategies
are applied in medical ethics and resource triage settings [14, 32]. Yet, AI moral alignment methods
often treat human preferences as static, despite evidence from moral psychology showing other-
wise [37]. Boerstler et al. [4] investigated the scale of moral judgment instability in the kidney
allocation domain; however, their studies had relatively small participant pools (fewer than 50)
and limited analysis of impact of instability on AI alignment. Our study follows a similar design,
but with significantly more participants to better assess the impact of temporal instabilities on
alignment. The consequences of misalignment can be much more severe in moral AI domains. If
people change their movie/music preferences over time, a misaligned recommender AI would re-
sult in unmet entertainment needs. In contrast, when someone has one medical resource allocation
policy one day but changes it to create a more equitable policy on a later day, then misalignment
3
in this moral domain can be seen to be much more consequential. Our work is motivated by the
need to tackle these potential consequences of temporal misalignment."
2511.09138v1,http://arxiv.org/abs/2511.09138v1,2025-11-12 09:26:23+00:00,Trusted Multi-view Learning for Long-tailed Classification,"Class imbalance has been extensively studied in single-view scenarios; however, addressing this challenge in multi-view contexts remains an open problem, with even scarcer research focusing on trustworthy solutions. In this paper, we tackle a particularly challenging class imbalance problem in multi-view scenarios: long-tailed classification. We propose TMLC, a Trusted Multi-view Long-tailed Classification framework, which makes contributions on two critical aspects: opinion aggregation and pseudo-data generation. Specifically, inspired by Social Identity Theory, we design a group consensus opinion aggregation mechanism that guides decision making toward the direction favored by the majority of the group. In terms of pseudo-data generation, we introduce a novel distance metric to adapt SMOTE for multi-view scenarios and develop an uncertainty-guided data generation module that produces high-quality pseudo-data, effectively mitigating the adverse effects of class imbalance. Extensive experiments on long-tailed multi-view datasets demonstrate that our model is capable of achieving superior performance. The code is released at https://github.com/cncq-tang/TMLC.","% \textbf{Long-tailed learning.} Traditional long-tailed learning methods include re-sampling, cost-sensitive learning and data augmentation. Class re-balancing is a mainstream paradigm in long-tailed learning, including Re-sampling~\cite{chawla2002smote},~\cite{estabrooks2004multiple},~\cite{liu2008exploratory}, cost-sensitive learning~\cite{hsieh2021droploss},~\cite{chen2023cost},~\cite{zhao2018adaptive} and logit adjustment~\cite{menon2020long},~\cite{wu2021adversarial},~\cite{zhang2021distribution}. 


% methods balance class distribution by modifying the class sample distribution in each mini-batch. DCL~\cite{wang2019dynamic} has developed a novel curriculum strategy for dynamically sampling data to achieve class re-balancing. Class-sensitive learning aims to counteract class imbalance by adjusting the training loss for different classes. Focal Loss~\cite{lin2017focal} and cost-sensitive learning~\cite{chen2023cost},~\cite{zhao2018adaptive} assign different weights to different classes, which allow the model to focus more on minority class samples. Logit adjustment~\cite{menon2020long},~\cite{wu2021adversarial},~\cite{zhang2021distribution} correct class imbalance by modifying the model’s prediction logits.

\textbf{Long-tailed Learning.} Traditional long-tailed learning methods include re-sampling~\cite{han2005borderline,liu2008exploratory}, cost-sensitive learning~\cite{cui2019class,cao2019learning,wang2021adaptive} and data augmentation~\cite{chu2020feature,kim2020m2m,yin2019feature}. As one of the most widely used techniques, Re-sampling rebalances classes by adjusting the number of samples per class. SMOTE~\cite{chawla2002smote} enhances the representation of minority classes by generating synthetic samples. Decoupling~\cite{kang2019decoupling} identifies effective sampling strategies for standard model training. DCL~\cite{wang2019dynamic} and LOCE~\cite{feng2021exploring} propose adaptive sampling strategies. Balanced Meta-Softmax~\cite{ren2020balanced} employs meta-learning to estimate optimal sampling rates for different classes. SimCal~\cite{wang2020devil} uses a novel two-level class-balanced sampling strategy that combines image-level and instance-level. However, these methods do not fully explore the uncertainty of samples during the sampling phase.

\noindent \textbf{Uncertainty-induced Trusted Learning.} Inspired by evidential deep learning~\cite{sensoy2018evidential}, which serves as a prominent paradigm for uncertainty estimation, Trusted Multi-view Classification (TMC) has recently gained particular attention. TMC leverages the Dirichlet distribution to model class probabilities and integrates Dempster-Shafer theory to achieve reliable classification results~\cite{han2022trusted}. Based on TMC, numerous extensions have been proposed to tackle specific challenges, involving opinion aggregation ~\cite{liu2022trusted, xu2024reliable, zheng2023multi}, noisy inputs and labels~\cite{xu2024trusted, shi2024generalized}, semantic ambiguity~\cite{liu2024dynamic}, etc. Recent advancements in trusted learning have expanded its research to areas such as weakly supervised learning~\cite{wang2024trusted, hu2025self}, robust learning ~\cite{zhou2023rtmc, zhou2023calm, du2023bridging, deng2025trustworthy}, and multimodal applications ~\cite{huang2025multi, li2025deep}.","\textbf{Long-tailed Learning.} Traditional long-tailed learning methods include re-sampling~\cite{han2005borderline,liu2008exploratory}, cost-sensitive learning~\cite{cui2019class,cao2019learning,wang2021adaptive} and data augmentation~\cite{chu2020feature,kim2020m2m,yin2019feature}. As one of the most widely used techniques, Re-sampling rebalances classes by adjusting the number of samples per class. SMOTE~\cite{chawla2002smote} enhances the representation of minority classes by generating synthetic samples. Decoupling~\cite{kang2019decoupling} identifies effective sampling strategies for standard model training. DCL~\cite{wang2019dynamic} and LOCE~\cite{feng2021exploring} propose adaptive sampling strategies. Balanced Meta-Softmax~\cite{ren2020balanced} employs meta-learning to estimate optimal sampling rates for different classes. SimCal~\cite{wang2020devil} uses a novel two-level class-balanced sampling strategy that combines image-level and instance-level. However, these methods do not fully explore the uncertainty of samples during the sampling phase.

\noindent \textbf{Uncertainty-induced Trusted Learning.} Inspired by evidential deep learning~\cite{sensoy2018evidential}, which serves as a prominent paradigm for uncertainty estimation, Trusted Multi-view Classification (TMC) has recently gained particular attention. TMC leverages the Dirichlet distribution to model class probabilities and integrates Dempster-Shafer theory to achieve reliable classification results~\cite{han2022trusted}. Based on TMC, numerous extensions have been proposed to tackle specific challenges, involving opinion aggregation ~\cite{liu2022trusted, xu2024reliable, zheng2023multi}, noisy inputs and labels~\cite{xu2024trusted, shi2024generalized}, semantic ambiguity~\cite{liu2024dynamic}, etc. Recent advancements in trusted learning have expanded its research to areas such as weakly supervised learning~\cite{wang2024trusted, hu2025self}, robust learning ~\cite{zhou2023rtmc, zhou2023calm, du2023bridging, deng2025trustworthy}, and multimodal applications ~\cite{huang2025multi, li2025deep}.","Long-tailed Learning.Traditional long-tailed learning
methods include re-sampling (Han, Wang, and Mao 2005;
Liu, Wu, and Zhou 2008), cost-sensitive learning (Cui et al.
2019; Cao et al. 2019; Wang et al. 2021) and data augmen-
tation (Chu et al. 2020; Kim, Jeong, and Shin 2020; Yin
et al. 2019). As one of the most widely used techniques,
Re-sampling rebalances classes by adjusting the number
of samples per class. SMOTE (Chawla et al. 2002) en-
hances the representation of minority classes by generating
synthetic samples. Decoupling (Kang et al. 2019) identi-
fies effective sampling strategies for standard model train-
ing. DCL (Wang et al. 2019) and LOCE (Feng, Zhong,
and Huang 2021) propose adaptive sampling strategies.
Balanced Meta-Softmax (Ren et al. 2020) employs meta-
learning to estimate optimal sampling rates for different
classes. SimCal (Wang et al. 2020) uses a novel two-level
class-balanced sampling strategy that combines image-level
and instance-level. However, these methods do not fully ex-
plore the uncertainty of samples during the sampling phase.
Uncertainty-induced Trusted Learning.Inspired by evi-
dential deep learning (Sensoy, Kaplan, and Kandemir 2018),
which serves as a prominent paradigm for uncertainty
estimation, Trusted Multi-view Classification (TMC) has
recently gained particular attention. TMC leverages the
Dirichlet distribution to model class probabilities and inte-
grates Dempster-Shafer theory to achieve reliable classifi-
cation results (Han et al. 2022). Based on TMC, numerous
extensions have been proposed to tackle specific challenges,
involving opinion aggregation (Liu et al. 2022; Xu et al.
2024a; Zheng et al. 2023), noisy inputs and labels (Xu et al.
2024b; Shi et al. 2024c), semantic ambiguity (Liu et al.), etc.
Recent advancements in trusted learning have expanded its
research to areas such as weakly supervised learning (Wang
et al. 2024; Hu et al. 2025), robust learning (Zhou et al.
2023a,b; Du et al. 2023; Deng et al. 2025), and multimodal
applications (Huang et al. 2025; Li et al. 2025).Methodology
In this section, we first provide the problem definition and
theoretical foundation on evidential deep learning. Sub-
sequently, we detail our key contributions, including the
group consensus opinion aggregation mechanism and the
uncertainty-based sample generation scheme.
Problem Definition and Preliminaries
Problem DefinitionA long-tailed multi-view dataset typ-
ically contains an imbalanced training set and a balanced
test set. For aKclassification problem, we define the multi-
view data as{Xv}V
v=1and the corresponding ground-truth
labels as{yn}N
n=1 withVviews andNsamples, where
Xv= [xv
1,xv
2,···,xv
N]∈Rdv×Nrepresents thev-th view
data withd vdimension. For each view dataXv, the distribu-
tion of classes is highly imbalanced, with tail classes having
significantly fewer samples compared to head classes. The
goal of TMLC is to address the challenges caused by imbal-
anced class distributions.
Evidential Deep LearningEvidential Deep Learning
(EDL) (Sensoy, Kaplan, and Kandemir 2018) is a proba-
bilistic framework to model prediction uncertainty by em-
ploying evidential priors and subjective logic (Jøsang 2016).
In the EDL framework, the evidencesev= [ev
1,···, ev
K]for
thev-th are obtained using Deep Neural Networks (DNNs),
where the Softmax layer is replaced by a ReLU activation
function.
Subjective logic provides a technical approach to asso-
ciate the evidenceev= [ev
1,···, ev
K]with the parameters of
the Dirichlet distributionαv= [αv
1,···, αv
K]. Specifically,
each parameterαv
kof the Dirichlet distribution is derived
fromev
kasαv
k=ev
k+"
2511.08260v1,http://arxiv.org/abs/2511.08260v1,2025-11-11 13:53:39+00:00,Data-Driven Discovery of Feature Groups in Clinical Time Series,"Clinical time series data are critical for patient monitoring and predictive modeling. These time series are typically multivariate and often comprise hundreds of heterogeneous features from different data sources. The grouping of features based on similarity and relevance to the prediction task has been shown to enhance the performance of deep learning architectures. However, defining these groups a priori using only semantic knowledge is challenging, even for domain experts. To address this, we propose a novel method that learns feature groups by clustering weights of feature-wise embedding layers. This approach seamlessly integrates into standard supervised training and discovers the groups that directly improve downstream performance on clinically relevant tasks. We demonstrate that our method outperforms static clustering approaches on synthetic data and achieves performance comparable to expert-defined groups on real-world medical data. Moreover, the learned feature groups are clinically interpretable, enabling data-driven discovery of task-relevant relationships between variables.","\paragraph{ML for ICU} Large EHR datasets have enabled the rapid development of machine learning models for ICU time series. \citet{harutyunyan2019multitask,hyland2020early,yeche2021} formalized clinical prediction tasks and developed strong tree-based and deep learning architectures for MIMIC-III and HiRID datasets. \citet{yeche2023temporal, yeche2024dsaforeep,kuznetsova2023importance} further improved deep learning models by modifying the loss function and architecture. We build directly on this line of research, adopting the same datasets and task definitions.
    
    \paragraph{Feature embedding in time series} Out-of-the-box deep neural networks often struggle to match the performance of tree-based architectures on tabular data. To address this shortcoming, \citet{gorishniy2022embeddings} proposed various strategies for feature embedding and incorporated them into ResNet \citep{he2016deep} and Transformer \citep{vaswani2017attention} models. \citet{tomavsev2021use, tomavsev2019clinically, kuznetsova2023importance} adapted these innovations to ICU time series with various embedding architectures.

    \paragraph{Feature grouping} Predefined feature groups have been shown to improve the downstream performance and explainability of time series models. \citet{kuznetsova2023importance, tomavsev2019clinically, tomavsev2021use} employed expert-defined feature groups to improve downstream performance on ICU time series data. \citet{swamy2024interpretcc} used predefined feature groups to design an inherently interpretable neural network. We extend these ideas by \emph{learning} feature groups to directly improve downstream performance while enhancing model explainability. 

    Several works have explored data-driven feature grouping before, but with key differences from our approach. \citet{masoomi2020instance} considered \emph{local} (or instance-wise) feature grouping, while we focus on \emph{global} feature groups that could help predictive modeling for a set clinical task in all patients. \citet{imrie2022composite} also considered global feature grouping, learning feature groups for static data using an ensemble-based method. Our method is tailored for time series and learns groups by clustering embedding weights directly within a deep learning architecture.
        
    In a related context, the same clustering algorithms have been applied to time series in the context of sample grouping \citep{ma2019learning}, while our focus is on grouping features.
    Similarly, interdependencies between features can be expressed via attention weights \citep{ma2020concare}, while our method produces explicit and global groups. Finally, feature grouping has also been used to achieve other goals, such as improving the computational efficiency of the attention mechanism \citep{roy2021efficient, vyas2020fast}, tailoring it to specific downstream tasks \citep{li2021groupformer}, or phenotyping patients via tensor decomposition \citep{becker2023unsupervised}.","\paragraph{ML for ICU} Large EHR datasets have enabled the rapid development of machine learning models for ICU time series. \citet{harutyunyan2019multitask,hyland2020early,yeche2021} formalized clinical prediction tasks and developed strong tree-based and deep learning architectures for MIMIC-III and HiRID datasets. \citet{yeche2023temporal, yeche2024dsaforeep,kuznetsova2023importance} further improved deep learning models by modifying the loss function and architecture. We build directly on this line of research, adopting the same datasets and task definitions.
    
    \paragraph{Feature embedding in time series} Out-of-the-box deep neural networks often struggle to match the performance of tree-based architectures on tabular data. To address this shortcoming, \citet{gorishniy2022embeddings} proposed various strategies for feature embedding and incorporated them into ResNet \citep{he2016deep} and Transformer \citep{vaswani2017attention} models. \citet{tomavsev2021use, tomavsev2019clinically, kuznetsova2023importance} adapted these innovations to ICU time series with various embedding architectures.

    \paragraph{Feature grouping} Predefined feature groups have been shown to improve the downstream performance and explainability of time series models. \citet{kuznetsova2023importance, tomavsev2019clinically, tomavsev2021use} employed expert-defined feature groups to improve downstream performance on ICU time series data. \citet{swamy2024interpretcc} used predefined feature groups to design an inherently interpretable neural network. We extend these ideas by \emph{learning} feature groups to directly improve downstream performance while enhancing model explainability. 

    Several works have explored data-driven feature grouping before, but with key differences from our approach. \citet{masoomi2020instance} considered \emph{local} (or instance-wise) feature grouping, while we focus on \emph{global} feature groups that could help predictive modeling for a set clinical task in all patients. \citet{imrie2022composite} also considered global feature grouping, learning feature groups for static data using an ensemble-based method. Our method is tailored for time series and learns groups by clustering embedding weights directly within a deep learning architecture.
        
    In a related context, the same clustering algorithms have been applied to time series in the context of sample grouping \citep{ma2019learning}, while our focus is on grouping features.
    Similarly, interdependencies between features can be expressed via attention weights \citep{ma2020concare}, while our method produces explicit and global groups. Finally, feature grouping has also been used to achieve other goals, such as improving the computational efficiency of the attention mechanism \citep{roy2021efficient, vyas2020fast}, tailoring it to specific downstream tasks \citep{li2021groupformer}, or phenotyping patients via tensor decomposition \citep{becker2023unsupervised}.","ML for ICULarge EHR datasets have enabled
the rapid development of machine learning models for
ICU time series. Harutyunyan et al. (2019); Hyland
et al. (2020); Y` eche et al. (2021) formalized clinical
prediction tasks and developed strong tree-based and
deep learning architectures for MIMIC-III and HiRID
datasets. Y` eche et al. (2023); Y` eche et al. (2024);
Kuznetsova et al. (2023) further improved deep learn-
ing models by modifying the loss function and archi-
tecture. We build directly on this line of research,
adopting the same datasets and task definitions.
Feature embedding in time seriesOut-of-the-
box deep neural networks often struggle to match the
performance of tree-based architectures on tabular
data. To address this shortcoming, Gorishniy et al.
(2022) proposed various strategies for feature embed-
ding and incorporated them into ResNet (He et al.,
2016) and Transformer (Vaswani et al., 2017) mod-
els. Tomaˇ sev et al. (2021, 2019); Kuznetsova et al.
8
Data-Driven Discovery of Feature Groups in Clinical Time Series
(2023) adapted these innovations to ICU time series
with various embedding architectures.
Feature groupingPredefined feature groups have
been shown to improve the downstream performance
and explainability of time series models. Kuznetsova
et al. (2023); Tomaˇ sev et al. (2019, 2021) employed
expert-defined feature groups to improve downstream
performance on ICU time series data. Swamy et al.
(2024) used predefined feature groups to design an
inherently interpretable neural network. We extend
these ideas bylearningfeature groups to directly
improve downstream performance while enhancing
model explainability.
Several works have explored data-driven feature
grouping before, but with key differences from our
approach. Masoomi et al. (2020b) consideredlocal
(or instance-wise) feature grouping, while we focus
onglobalfeature groups that could help predictive
modeling for a set clinical task in all patients. Im-
rie et al. (2022) also considered global feature group-
ing, learning feature groups for static data using an
ensemble-based method. Our method is tailored for
time series and learns groups by clustering embedding
weights directly within a deep learning architecture.
In a related context, the same clustering algorithms
have been applied to time series in the context of sam-
ple grouping (Ma et al., 2019), while our focus is on
grouping features. Similarly, interdependencies be-
tween features can be expressed via attention weights
(Ma et al., 2020), while our method produces explicit
and global groups. Finally, feature grouping has also
been used to achieve other goals, such as improving
the computational efficiency of the attention mecha-
nism (Roy et al., 2021; Vyas et al., 2020), tailoring it
to specific downstream tasks (Li et al., 2021), or phe-
notyping patients via tensor decomposition (Becker
et al., 2023)."
2511.09157v1,http://arxiv.org/abs/2511.09157v1,2025-11-12 09:49:31+00:00,ProBench: Benchmarking GUI Agents with Accurate Process Information,"With the deep integration of artificial intelligence and interactive technology, Graphical User Interface (GUI) Agent, as the carrier connecting goal-oriented natural language and real-world devices, has received widespread attention from the community. Contemporary benchmarks aim to evaluate the comprehensive capabilities of GUI agents in GUI operation tasks, generally determining task completion solely by inspecting the final screen state. However, GUI operation tasks consist of multiple chained steps while not all critical information is presented in the final few pages. Although a few research has begun to incorporate intermediate steps into evaluation, accurately and automatically capturing this process information still remains an open challenge. To address this weakness, we introduce ProBench, a comprehensive mobile benchmark with over 200 challenging GUI tasks covering widely-used scenarios. Remaining the traditional State-related Task evaluation, we extend our dataset to include Process-related Task and design a specialized evaluation method. A newly introduced Process Provider automatically supplies accurate process information, enabling presice assessment of agent's performance. Our evaluation of advanced GUI agents reveals significant limitations for real-world GUI scenarios. These shortcomings are prevalent across diverse models, including both large-scale generalist models and smaller, GUI-specific models. A detailed error analysis further exposes several universal problems, outlining concrete directions for future improvements.","\subsection{Static GUI Benchmark}
RICO~\cite{deka2017rico} marked an important milestone in GUI-related research by providing a basic dataset for GUI element classification and detection. 
Afterwards, UGIF~\cite{venkatesh2022ugif} introduced instruction-based GUI control task. 
AITW~\cite{rawles2023androidinthewild} expanded the field with a large-scale dataset, while suffering from instruction redundancy and frequent mislabeling. AITZ~\cite{zhang2024android} refined AITW by applying Chain-of-Action-Thought reannotation and got a cleaner but smaller dataset. 
AndroidControl~\cite{li2024effects} further introduced a dataset with simpler tasks and unique action space compared to AITW. 
However, these static benchmarks evaluate agents by one-step operation based on single screenshot and correct action history. Without realistic historical actions, the method ignores the inherently dynamic and interactive feature of real-world environment. In addition, one-step error directs to task failure, leaving no opportunity to assess the capability of reflection or recovery.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figure/overview.pdf}
    \caption{\textbf{Overview of our \model\ benchmark.} \model\ is a comprehensive mobile benchmark, comprising three key modules: \textbf{(\textit{i}) Task Curation:} We select 34 mainstream bilingual applications, generate candidate tasks using LLMs, and refine them through manual review. \textbf{(\textit{ii}) Dynamic Environment:} Agents complete the specified tasks by controlling the device. \textbf{(\textit{iii}) Evaluation Pipeline:} For Process-related Task, we optionally choose either the Structure Description Converter or the MLLM-based Summarizer of Process Provider to supply process information. The final evaluation is performed by the judger selected from the judger group.}
    \label{fig:overview}
\end{figure*}

\subsection{Dynamic GUI Benchmark}
Researchers have developed a range of dynamic evaluation systems to better approximate real-world environment. AndroidArena~\cite{xing2024understanding} focuses exclusively on Google and built-in system apps, limiting its capacity to evaluate the general performance of third-party apps. B-Moca~\cite{lee2024benchmarkingmobiledevicecontrol} incorporates applications such as Walmart, while the tasks are overly simplistic and lack diversity. AndroidWorld~\cite{rawles2024androidworld} increases task diversity by selecting open-source applications from F-Droid, while discrepancies in design formula remain between these applications and mainstream software. AndroidLab~\cite{xu2024androidlab} introduces additional variety by leveraging offline and static applications.
Concurrently, these benchmarks predominantly depend on element matching~\cite{lee2024benchmarkingmobiledevicecontrol} or predefined detection code~\cite{xu2024androidlab} on the final screen, omitting the evaluation of the crucial intermediate steps.
Realizing the shortcoming, SPA-BENCH~\cite{chen2024spa} and A3~\cite{chai2025a3} consider intermediate process information into assessments, while introducing MLLMs to assist the evaluation.
Nevertheless, these evaluations cannot capture process information accurately and automatically. \model\ addresses this gap by building a dedicated evaluation pipeline while capturing process information using Process Provider. Table~\ref{table:comparison} compares our work with existing dynamic GUI agent benchmarks, highlighting our innovations and unique contributions.
% To overcome the limitations of static benchmarks,
% Researchers have developed a range of dynamic evaluation systems to better approximate real-world environment. AndroidArena~\cite{xing2024understanding} focuses exclusively on Google and built-in system apps, limiting its capacity to evaluate the general performance of third-party apps. B-Moca~\cite{lee2024benchmarkingmobiledevicecontrol} incorporates applications such as Walmart, but the tasks it provides are overly simplistic and lack diversity. AndroidWorld~\cite{rawles2024androidworld} increases task diversity by selecting open-source applications from F-Droid, while discrepancies in design formula remain between these applications and mainstream software. AndroidLab~\cite{xu2024androidlab} introduces additional variety by leveraging offline and static applications.
% However, these benchmarks still exhibit several shortcomings, the evaluation methods predominantly depend on element matching~\cite{lee2024benchmarkingmobiledevicecontrol} or predefined detection code~\cite{xu2024androidlab}, which hinders their ability to accommodate the dynamic and evolving requirements of third-party online applications. Such applications, including news, shopping, email, and music, play an essential role in real-world GUI interactions. 
% To improve evaluation scalability, SPA-BENCH~\cite{chen2024spa} and A3~\cite{chai2025a3} introduce MLLMs to assist the evaluation process.
% Nevertheless, these evaluations cannot efficiently utilize process information to assist judgment, unable to evaluate operation tasks appropriately. \model\ addresses this gap by building a dedicated evaluation pipeline while capturing process information automatically. Table~\ref{table:comparison} compares our work with existing dynamic GUI agent benchmarks, highlighting our innovations and unique contributions.

% Although several dynamic evaluation systems have been proposed to more closely reflect real-world environment, they continue to suffer from some shortcomings.
% AndroidArena~\cite{xing2024understanding} focuses exclusively on Google and built-in system apps, limiting its capacity to evaluate the general performance of third-party apps. B-Moca~\cite{lee2024benchmarkingmobiledevicecontrol} incorporates applications such as Walmart, but the tasks it provides are overly simplistic and lack diversity. AndroidWorld~\cite{rawles2024androidworld} increases diversity by selecting open-source apps from F-Droid\footnote{https://f-droid.org/en/packages}, while significant discrepancies in design formula remain between these apps and mainstream software. AndroidLab~\cite{xu2024androidlab} introduces additional variety by leveraging offline and static apps.
% Furthermore, the evaluation methods employed by these systems predominantly depend on element matching~\cite{lee2024benchmarkingmobiledevicecontrol} or predefined detection code~\cite{xu2024androidlab}, which hinders their ability to accommodate the dynamic and evolving requirements of third-party online applications, while such applications play a critical role in real-world GUI interactions. To enhance the scalability of evaluation, SPA-BENCH~\cite{chen2024spa} and A3~\cite{chai2025a3} incorporate MLLMs to assist in the evaluation process.
% However, all existing benchmarks focus on operation tasks, ignoring query tasks that are equally common in real-world scenarios. \model\ introduces query tasks for a variety of third-party bilingual apps and builds an automated evaluation process without manual verification. Table~\ref{table:comparison} compares our work with existing dynamic GUI agent benchmarks, highlighting our innovation and unique value.","\subsection{Static GUI Benchmark}
RICO~\cite{deka2017rico} marked an important milestone in GUI-related research by providing a basic dataset for GUI element classification and detection. 
Afterwards, UGIF~\cite{venkatesh2022ugif} introduced instruction-based GUI control task. 
AITW~\cite{rawles2023androidinthewild} expanded the field with a large-scale dataset, while suffering from instruction redundancy and frequent mislabeling. AITZ~\cite{zhang2024android} refined AITW by applying Chain-of-Action-Thought reannotation and got a cleaner but smaller dataset. 
AndroidControl~\cite{li2024effects} further introduced a dataset with simpler tasks and unique action space compared to AITW. 
However, these static benchmarks evaluate agents by one-step operation based on single screenshot and correct action history. Without realistic historical actions, the method ignores the inherently dynamic and interactive feature of real-world environment. In addition, one-step error directs to task failure, leaving no opportunity to assess the capability of reflection or recovery.



\subsection{Dynamic GUI Benchmark}
Researchers have developed a range of dynamic evaluation systems to better approximate real-world environment. AndroidArena~\cite{xing2024understanding} focuses exclusively on Google and built-in system apps, limiting its capacity to evaluate the general performance of third-party apps. B-Moca~\cite{lee2024benchmarkingmobiledevicecontrol} incorporates applications such as Walmart, while the tasks are overly simplistic and lack diversity. AndroidWorld~\cite{rawles2024androidworld} increases task diversity by selecting open-source applications from F-Droid, while discrepancies in design formula remain between these applications and mainstream software. AndroidLab~\cite{xu2024androidlab} introduces additional variety by leveraging offline and static applications.
Concurrently, these benchmarks predominantly depend on element matching~\cite{lee2024benchmarkingmobiledevicecontrol} or predefined detection code~\cite{xu2024androidlab} on the final screen, omitting the evaluation of the crucial intermediate steps.
Realizing the shortcoming, SPA-BENCH~\cite{chen2024spa} and A3~\cite{chai2025a3} consider intermediate process information into assessments, while introducing MLLMs to assist the evaluation.
Nevertheless, these evaluations cannot capture process information accurately and automatically. \model\ addresses this gap by building a dedicated evaluation pipeline while capturing process information using Process Provider. Table~\ref{table:comparison} compares our work with existing dynamic GUI agent benchmarks, highlighting our innovations and unique contributions.","2.1 Static GUI Benchmark
RICO (Deka et al. 2017) marked an important mile-
stone in GUI-related research by providing a basic dataset
for GUI element classification and detection. Afterwards,
UGIF (Venkatesh, Talukdar, and Narayanan 2022) intro-
duced instruction-based GUI control task. AITW (RawlesBenchmark Online ChineseMV
FreeAP
Process
AndroidArena% % % %
AndroidWorld! % % %
B-MoCA! % % %
AndroidLab% % % %
SPA-BENCH! ! ! %
A3% % ! %
ProBench! ! ! !
Table 1:Comparison of ProBench with other dynamic
GUI benchmarks.Online means whether includes third-
party online apps and Chinese means whether includes Chi-
nese apps. MV Free means whether the evaluation phase is
free of manual verification participation. AP Process means
whether capture precise process information automatically.
et al. 2023) expanded the field with a large-scale dataset,
while suffering from instruction redundancy and frequent
mislabeling. AITZ (Zhang et al. 2024) refined AITW by
applying Chain-of-Action-Thought reannotation and got a
cleaner but smaller dataset. AndroidControl (Li et al. 2024)
further introduced a dataset with simpler tasks and unique
action space compared to AITW. However, these static
benchmarks evaluate agents by one-step operation based on
single screenshot and correct action history. Without realis-
tic historical actions, the method ignores the inherently dy-
namic and interactive feature of real-world environment. In
addition, one-step error directs to task failure, leaving no op-
portunity to assess the capability of reflection or recovery.
2.2 Dynamic GUI Benchmark
Researchers have developed a range of dynamic evaluation
systems to better approximate real-world environment. An-
droidArena (Xing et al. 2024) focuses exclusively on Google
and built-in system apps, limiting its capacity to evaluate
the general performance of third-party apps. B-Moca (Lee
et al. 2024) incorporates applications such as Walmart, while
the tasks are overly simplistic and lack diversity. Android-
World (Rawles et al. 2024) increases task diversity by se-
lecting open-source applications from F-Droid, while dis-
crepancies in design formula remain between these appli-
cations and mainstream software. AndroidLab (Xu et al.
2024) introduces additional variety by leveraging offline and
static applications. Concurrently, these benchmarks predom-
inantly depend on element matching (Lee et al. 2024) or pre-
defined detection code (Xu et al. 2024) on the final screen,
omitting the evaluation of the crucial intermediate steps. Re-
alizing the shortcoming, SPA-BENCH (Chen et al. 2024)
and A3 (Chai et al. 2025) consider intermediate process in-
formation into assessments, while introducing MLLMs to
assist the evaluation. Nevertheless, these evaluations cannot
capture process information accurately and automatically.
ProBench addresses this gap by building a dedicated eval-
uation pipeline while capturing process information using
AppSelection
LLMGenerating
Manual reviewTask CurationDynamic EnvironmentEvaluation Pipeline
Evaluator•Task instruction•Final Screenshot•TextualAction
MLLM-basedSummarizerStructureDescription ConverterTrue
CLICK(500, 300)Screenshot+Task instruction+ContextJudger GroupFalseProcessProviderFigure 2:Overview of our ProBench benchmark.ProBench is a comprehensive mobile benchmark, comprising three key
modules:(i) Task Curation:We select 34 mainstream bilingual applications, generate candidate tasks using LLMs, and refine
them through manual review.(ii) Dynamic Environment:Agents complete the specified tasks by controlling the device.(iii)
Evaluation Pipeline:For Process-related Task, we optionally choose either the Structure Description Converter or the MLLM-
based Summarizer of Process Provider to supply process information. The final evaluation is performed by the judger selected
from the judger group.
Process Provider. Table 1 compares our work with existing
dynamic GUI agent benchmarks, highlighting our innova-
tions and unique contributions."
2511.08484v1,http://arxiv.org/abs/2511.08484v1,2025-11-11 17:25:44+00:00,Patching LLM Like Software: A Lightweight Method for Improving Safety Policy in Large Language Models,"We propose patching for large language models (LLMs) like software versions, a lightweight and modular approach for addressing safety vulnerabilities. While vendors release improved LLM versions, major releases are costly, infrequent, and difficult to tailor to customer needs, leaving released models with known safety gaps. Unlike full-model fine-tuning or major version updates, our method enables rapid remediation by prepending a compact, learnable prefix to an existing model. This ""patch"" introduces only 0.003% additional parameters, yet reliably steers model behavior toward that of a safer reference model. Across three critical domains (toxicity mitigation, bias reduction, and harmfulness refusal) policy patches achieve safety improvements comparable to next-generation safety-aligned models while preserving fluency. Our results demonstrate that LLMs can be ""patched"" much like software, offering vendors and practitioners a practical mechanism for distributing scalable, efficient, and composable safety updates between major model releases.","Efforts to improve the safety of large language models have largely centered on full-model alignment, commonly instantiated as supervised fine-tuning or reinforcement learning from human feedback (RLHF) \citep{christiano2017deep, ouyang2022}, and more recently preference-based objectives such as Direct Preference Optimization (DPO) \citep{rafailov2023}. These approaches produce strong safety improvements but typically require large compute budgets, access to model weights, and long validation cycles—constraints that limit their suitability for frequent, targeted fixes in deployed systems. Prior detoxification and debiasing pipelines, such as RealToxicityPrompts \citep{gehman2020realtoxicityprompts} and gender-debiasing objectives \citep{dong2024disclosure}, demonstrate effectiveness on a narrow set of safety dimensions, but retraining entire models for each fix is operationally costly. Our work reframes this challenge as one of modular patching, allowing providers to distribute lightweight safety updates without redeploying full model versions.

Parameter-efficient adaptation techniques provide an important middle ground. Adapter-based techniques such as LoRA and QLoRA uses low-rank residual updates inside transformer layers to change internal representations while substantially reducing training cost compared to full fine-tuning \citep{hu2021lora, dettmers2023qlora}. Prefix-tuning introduces trainable key–value prefixes at every transformer layer, directly augmenting attention computations \citep{li2021prefix}. By contrast, prompt tuning places learnable vectors only at the input embedding layer. These continuous prompts do not modify internal layer activations or attention mechanisms and thus remain architecture-agnostic \citep{lester2021}. This distinction has direct operational consequences: adapter and prefix methods can deliver larger absolute performance gains because they modify internal representations, but they are tightly coupled to transformer internals and usually require layer-wise insertion or model-specific wiring, complicating portability and distribution. Policy patching remains external to model weights and architecture, which makes them inherently more modular and easy to ship as a “patch” that a user can prepend without modifying model binaries.

Finally, targeted safety interventions such as RealToxicityPrompts detoxification \citep{gehman2020realtoxicityprompts} and gender-debiasing methods \citep{dong2024disclosure} show that narrow alignment tasks can be highly effective. Yet, these solutions are often tied to specific datasets or trained variants, raising challenges of scalability and portability. Our work extends this line by demonstrating that small, learnable prefixes can serve as modular, reusable, and distribution-friendly safety patches, bridging the gap between heavyweight fine-tuning and ephemeral prompt-based steering.","Efforts to improve the safety of large language models have largely centered on full-model alignment, commonly instantiated as supervised fine-tuning or reinforcement learning from human feedback (RLHF) \citep{christiano2017deep, ouyang2022}, and more recently preference-based objectives such as Direct Preference Optimization (DPO) \citep{rafailov2023}. These approaches produce strong safety improvements but typically require large compute budgets, access to model weights, and long validation cycles—constraints that limit their suitability for frequent, targeted fixes in deployed systems. Prior detoxification and debiasing pipelines, such as RealToxicityPrompts \citep{gehman2020realtoxicityprompts} and gender-debiasing objectives \citep{dong2024disclosure}, demonstrate effectiveness on a narrow set of safety dimensions, but retraining entire models for each fix is operationally costly. Our work reframes this challenge as one of modular patching, allowing providers to distribute lightweight safety updates without redeploying full model versions.

Parameter-efficient adaptation techniques provide an important middle ground. Adapter-based techniques such as LoRA and QLoRA uses low-rank residual updates inside transformer layers to change internal representations while substantially reducing training cost compared to full fine-tuning \citep{hu2021lora, dettmers2023qlora}. Prefix-tuning introduces trainable key–value prefixes at every transformer layer, directly augmenting attention computations \citep{li2021prefix}. By contrast, prompt tuning places learnable vectors only at the input embedding layer. These continuous prompts do not modify internal layer activations or attention mechanisms and thus remain architecture-agnostic \citep{lester2021}. This distinction has direct operational consequences: adapter and prefix methods can deliver larger absolute performance gains because they modify internal representations, but they are tightly coupled to transformer internals and usually require layer-wise insertion or model-specific wiring, complicating portability and distribution. Policy patching remains external to model weights and architecture, which makes them inherently more modular and easy to ship as a “patch” that a user can prepend without modifying model binaries.

Finally, targeted safety interventions such as RealToxicityPrompts detoxification \citep{gehman2020realtoxicityprompts} and gender-debiasing methods \citep{dong2024disclosure} show that narrow alignment tasks can be highly effective. Yet, these solutions are often tied to specific datasets or trained variants, raising challenges of scalability and portability. Our work extends this line by demonstrating that small, learnable prefixes can serve as modular, reusable, and distribution-friendly safety patches, bridging the gap between heavyweight fine-tuning and ephemeral prompt-based steering.",
2511.08263v1,http://arxiv.org/abs/2511.08263v1,2025-11-11 13:55:46+00:00,ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,"Data condensation techniques aim to synthesize a compact dataset from a larger one to enable efficient model training, yet while successful in unimodal settings, they often fail in multimodal scenarios where preserving intricate inter-modal dependencies is crucial. To address this, we introduce ImageBindDC, a novel data condensation framework operating within the unified feature space of ImageBind. Our approach moves beyond conventional distribution-matching by employing a powerful Characteristic Function (CF) loss, which operates in the Fourier domain to facilitate a more precise statistical alignment via exact infinite moment matching. We design our objective to enforce three critical levels of distributional consistency: (i) uni-modal alignment, which matches the statistical properties of synthetic and real data within each modality; (ii) cross-modal alignment, which preserves pairwise semantics by matching the distributions of hybrid real-synthetic data pairs; and (iii) joint-modal alignment, which captures the complete multivariate data structure by aligning the joint distribution of real data pairs with their synthetic counterparts. Extensive experiments highlight the effectiveness of ImageBindDC: on the NYU-v2 dataset, a model trained on just 5 condensed datapoints per class achieves lossless performance comparable to one trained on the full dataset, achieving a new state-of-the-art with an 8.2\% absolute improvement over the previous best method and more than 4$\times$ less condensation time.","\label{sec:related_work}

Dataset Condensation aims to create a small, efficient dataset to reduce the cost of training large models. The field has evolved from selecting subsets of real data to synthesizing entirely new data points, with most efforts historically focused on uni-modal data.

\noindent \textbf{Uni-modal Data Condensation}. Uni-modal condensation methods fall into two main categories. \textit{Data Selection} (or coreset selection) identifies a representative subset of the original data based on various criteria like clustering~\citep{alexey2016discriminative, bautista2016cliquecnn}, greedy utility maximization~\citep{wei2015submodularity, soper2021greed}, gradient information~\citep{paul2021deep,mirzasoleiman2020coresets,killamsetty2021grad,LESS}, or other model-based metrics~\citep{toneva2018empirical, wang2025datawhisperer,wang2025winning,wang2025circuitseer}. However, these methods are fundamentally limited by being confined to the original data sources and cannot yield unseen data. In contrast, \textit{Dataset Distillation}~\citep{DD} synthesizes a small set of new, optimized data points. The goal is to match the learning dynamics of the full dataset. Dominant strategies include \textit{Gradient Matching}, which aligns training gradients~\citep{DC,DCC,DSA,wang2024samples,wang2025drupi} or entire parameter trajectories~\citep{DATM, MTT}, and \textit{Distribution Matching}, which aligns feature statistics in a pretrained embedding space~\citep{DM, IDM, NCFM}. Our work builds upon this data synthesis paradigm, adapting it for the unique challenges of the multi-modal setting.

\noindent \textbf{Multi-modal Data Condensation}. Extending condensation to multi-modal data is a nascent but critical research direction. The primary challenge is not only preserving the intra-modal statistics but, more importantly, the cross-modal semantic relationships. Previous works focus on multi-modal data selection A few recent methods have begun to tackle this problem. Although recent advancements in data selection, such as LLM-based filtering~\cite{chen2023alpagasus, liu2023makes, xu2023rethinking}, gradient-based influence estimation~\cite{attendu2023nlu}, and self-instruction generation~\cite{kung2023active}, have shown promise in optimizing instruction tuning, their effectiveness can be inconsistent, with some studies indicating they often fail to consistently outperform random sampling. AVDD~\citep{kushwaha2024audio} performs condensation for audio-visual data but does so by matching distributions in separate, modality-specific feature spaces, which risks misalignment and relies on now-outdated backbone architectures. To address cross-modal relationships, LoRS~\citep{xu2024low} proposes matching a pre-computed ground-truth similarity matrix between modalities, using low-rank factorization for efficiency. However, boiling down the complex, high-dimensional relationship between modalities to a single scalar similarity may be an oversimplification that fails to capture the full distributional structure. More recently, RepBlend~\citep{zhang2025beyond} introduced representation blending to encourage diversity and prevent modality collapse, but this remains a heuristic approach that can be difficult to balance and may not guarantee the preservation of joint-modal semantics.

% Coreset-Based Selection [ 35 , 52 , 241], LLMs-Based Selection [39 , 164, 287],
% gradient-Based Selection [ 8, 207, 274], Self-Instruction-Based Selection Methods [128, 146, 150, 162]","Dataset Condensation aims to create a small, efficient dataset to reduce the cost of training large models. The field has evolved from selecting subsets of real data to synthesizing entirely new data points, with most efforts historically focused on uni-modal data.

\noindent \textbf{Uni-modal Data Condensation}. Uni-modal condensation methods fall into two main categories. \textit{Data Selection} (or coreset selection) identifies a representative subset of the original data based on various criteria like clustering~\citep{alexey2016discriminative, bautista2016cliquecnn}, greedy utility maximization~\citep{wei2015submodularity, soper2021greed}, gradient information~\citep{paul2021deep,mirzasoleiman2020coresets,killamsetty2021grad,LESS}, or other model-based metrics~\citep{toneva2018empirical, wang2025datawhisperer,wang2025winning,wang2025circuitseer}. However, these methods are fundamentally limited by being confined to the original data sources and cannot yield unseen data. In contrast, \textit{Dataset Distillation}~\citep{DD} synthesizes a small set of new, optimized data points. The goal is to match the learning dynamics of the full dataset. Dominant strategies include \textit{Gradient Matching}, which aligns training gradients~\citep{DC,DCC,DSA,wang2024samples,wang2025drupi} or entire parameter trajectories~\citep{DATM, MTT}, and \textit{Distribution Matching}, which aligns feature statistics in a pretrained embedding space~\citep{DM, IDM, NCFM}. Our work builds upon this data synthesis paradigm, adapting it for the unique challenges of the multi-modal setting.

\noindent \textbf{Multi-modal Data Condensation}. Extending condensation to multi-modal data is a nascent but critical research direction. The primary challenge is not only preserving the intra-modal statistics but, more importantly, the cross-modal semantic relationships. Previous works focus on multi-modal data selection A few recent methods have begun to tackle this problem. Although recent advancements in data selection, such as LLM-based filtering~\cite{chen2023alpagasus, liu2023makes, xu2023rethinking}, gradient-based influence estimation~\cite{attendu2023nlu}, and self-instruction generation~\cite{kung2023active}, have shown promise in optimizing instruction tuning, their effectiveness can be inconsistent, with some studies indicating they often fail to consistently outperform random sampling. AVDD~\citep{kushwaha2024audio} performs condensation for audio-visual data but does so by matching distributions in separate, modality-specific feature spaces, which risks misalignment and relies on now-outdated backbone architectures. To address cross-modal relationships, LoRS~\citep{xu2024low} proposes matching a pre-computed ground-truth similarity matrix between modalities, using low-rank factorization for efficiency. However, boiling down the complex, high-dimensional relationship between modalities to a single scalar similarity may be an oversimplification that fails to capture the full distributional structure. More recently, RepBlend~\citep{zhang2025beyond} introduced representation blending to encourage diversity and prevent modality collapse, but this remains a heuristic approach that can be difficult to balance and may not guarantee the preservation of joint-modal semantics.","Dataset Condensation aims to create a small, efficient dataset to reduce the cost of training
large models. The field has evolved from selecting subsets of real data to synthesizing
entirely new data points, with most efforts historically focused on uni-modal data.
2
Preprint.
Real Dataℛ=ℛ!∪ℛ""Synthetic Data𝒮=𝒮!∪𝒮""Audioℛ!Visionℛ""
Joint Embedding Space
ImageBindimage embeddingaudio embeddingDistribution Matching(a) DataCondensation
(b) Characteristic Function Discrepancy(CFD)i. uni-modal:CFD(ℛ!,𝒮!)+CFDℛ"",𝒮""
ii. cross-modal: CFD(ℛ!+𝒮"",ℛ""+𝒮!)iii. joint-modal: CFD(ℛ!+𝒮!,ℛ""+𝒮"")𝒮!𝒮""
0˚30˚60˚90˚120˚150˚180˚210˚240˚270˚300˚330˚0˚30˚60˚90˚120˚150˚180˚210˚240˚270˚300˚330˚Sampled Frequencies 𝒕Real EmbeddingSynthetic Embedding
CFDCFDCFDCFD(c) Multimodal Distribution Matching
ImageBindDCpipeline
Complex Plane
CFD: 𝔼!∼ℛ,%∼𝒮Φ𝑟;𝒕−Φ𝑠;𝒕(Φ𝑟;𝒕−(Φ(𝑠;𝒕
CFDLoss: ℒ'()+ℒ*+,--+ℒ.,)(/Characteristic Function: Φ𝑥;𝒕=𝔼0𝑒1𝒕!0CFCF
Figure 2:Overview of the ImageBindDC Framework.Our method condenses multi-modal
data by performing principled distribution matching in a unified embedding space.(a) Data
Condensation Pipeline:We take real multi-modal data, consisting of vision ( Rv) and audio
(Ra), and aim to synthesize a much smaller synthetic dataset ( Sv,Sa). Both real and synthetic
data are projected into a joint embedding space using the pretrained ImageBind encoder.
The core of our method is to optimize the synthetic data such that its distribution in this
embedding space matches that of the real data.(b) Characteristic Function Discrepancy
(CFD):We use CFD as our distribution matching metric. The empirical Characteristic
Function (CF) of a data distribution is calculated, which provides a summary in the Fourier
domain (visualized here on the complex plane via polar plots). CFD then measures the
discrepancy between the CFs of the real and synthetic embeddings, effectively matching
all statistical moments for a precise alignment.(c) Multi-modal Distribution Matching
Objective:To ensure comprehensive alignment, our final loss is a sum of three CFD-based
objectives: (i)Uni-modal alignmentpreserves the integrity of each modality by matching real
and synthetic data within the same modality (e.g., Rvvs.Sv). (ii)Cross-modal alignment
preserves the semantic relationship between modalities by matching the distribution of
hybrid pairs (e.g., real audio + synthetic vision vs. real vision + synthetic audio). (iii)
Joint-modal alignmentcaptures the complete data structure by matching the joint distribution
of paired real data against paired synthetic data. The total loss Ltotal=L uni+L cross+L joint
guides the synthesis process.
Uni-modal Data Condensation. Uni-modal condensation methods fall into two main
categories.Data Selection(or coreset selection) identifies a representative subset of the
original data based on various criteria like clustering (Alexey et al., 2016; Bautista et al.,
2016), greedy utility maximization (Wei et al., 2015; Soper, 2021), gradient information (Paul
et al., 2021; Mirzasoleiman et al., 2020; Killamsetty et al., 2021; Xia et al., 2024), or other
model-based metrics (Toneva et al., 2018; Wang et al., 2025a;c;b). However, these methods
are fundamentally limited by being confined to the original data sources and cannot yield
unseen data. In contrast,Dataset Distillation(Wang et al., 2018) synthesizes a small set of
new, optimized data points. The goal is to match the learning dynamics of the full dataset.
Dominant strategies includeGradient Matching, which aligns training gradients (Zhao
et al., 2020; Lee et al., 2022; Zhao & Bilen, 2021; Wang et al., 2025d;e) or entire parameter
trajectories (Guo et al., 2023; Cazenavette et al., 2022a), andDistribution Matching, which
aligns feature statistics in a pretrained embedding space (Zhao & Bilen, 2022; Zhao et al.,
2023; Wang et al., 2025f). Our work builds upon this data synthesis paradigm, adapting it
for the unique challenges of the multi-modal setting.
Multi-modal Data Condensation. Extending condensation to multi-modal data is a nascent
but critical research direction. The primary challenge is not only preserving the intra-modal
statistics but, more importantly, the cross-modal semantic relationships. Previous works
focus on multi-modal data selection A few recent methods have begun to tackle this problem.
3
Preprint.
Although recent advancements in data selection, such as LLM-based filtering Chen et al.
(2023); Liu et al. (2023); Xu et al. (2023), gradient-based influence estimation Attendu &
Corbeil (2023), and self-instruction generation Kung et al. (2023), have shown promise in
optimizing instruction tuning, their effectiveness can be inconsistent, with some studies
indicating they often fail to consistently outperform random sampling. AVDD (Kushwaha
et al., 2024) performs condensation for audio-visual data but does so by matching distri-
butions in separate, modality-specific feature spaces, which risks misalignment and relies
on now-outdated backbone architectures. To address cross-modal relationships, LoRS (Xu
et al., 2024) proposes matching a pre-computed ground-truth similarity matrix between
modalities, using low-rank factorization for efficiency. However, boiling down the com-
plex, high-dimensional relationship between modalities to a single scalar similarity may be
an oversimplification that fails to capture the full distributional structure. More recently,
RepBlend (Zhang et al., 2025) introduced representation blending to encourage diversity
and prevent modality collapse, but this remains a heuristic approach that can be difficult to
balance and may not guarantee the preservation of joint-modal semantics."
2511.10035v1,http://arxiv.org/abs/2511.10035v1,2025-11-13 07:18:58+00:00,DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,"As a critical task in autonomous driving perception systems, 3D object detection is used to identify and track key objects, such as vehicles and pedestrians. However, detecting distant, small, or occluded objects (hard instances) remains a challenge, which directly compromises the safety of autonomous driving systems. We observe that existing multi-modal 3D object detection methods often follow a single-guided paradigm, failing to account for the differences in information density of hard instances between modalities. In this work, we propose DGFusion, based on the Dual-guided paradigm, which fully inherits the advantages of the Point-guide-Image paradigm and integrates the Image-guide-Point paradigm to address the limitations of the single paradigms. The core of DGFusion, the Difficulty-aware Instance Pair Matcher (DIPM), performs instance-level feature matching based on difficulty to generate easy and hard instance pairs, while the Dual-guided Modules exploit the advantages of both pair types to enable effective multi-modal feature fusion. Experimental results demonstrate that our DGFusion outperforms the baseline methods, with respective improvements of +1.0\% mAP, +0.8\% NDS, and +1.3\% average recall on nuScenes. Extensive experiments demonstrate consistent robustness gains for hard instance detection across ego-distance, size, visibility, and small-scale training scenarios.","\subsection{3D Object Detection in Autonomous Driving Perception}

% 早期自动驾驶感知算法倾向于基于LiDAR的探测器\cite{}。一些方法\cite{}采用基于锚点的模型架构，该架构定义每类形状来指导类感知对象检测。无锚模型\cite{}避免手动设计锚框的步骤以简化模型的训练过程并提高模型对不同尺度目标的适应性。例如CenterPoint\cite{}通过学习预测对象的中心来定位目标。LiDAR传感器能够获取深度信息，但不能直接获取难以识别的几何和自我运动线索\cite{}。

% Camera传感器不能直接获得精确的深度信息，因此早期的基于Camera的探测器\cite{}难以准确估计目标的距离和形状，其综合性能不如基于LiDAR的探测器。但是，Camera检测器在检测稀有类别的物体方面表现出色\cite{}，最近的一些纯视觉方法\cite{}在复杂交通场景中的表现已显著改善。

% 多模态3D目标检测通过利用异构传感器的数据特征并将其集成以增强3D物体的检测。其中，基于BEV范式的方法\cite{}通过将LiDAR和相机表示统一到BEV空间中来提升任务性能。BEVFusion\cite{bevfusion-mit}贡献了Camera BEV features与LiDAR BEV features直接融合的经典范式。尽管这些先驱已经在nuScenes等公共数据集上获得了优秀的性能，但它们忽略了现实世界的复杂性，特别是hard instances detection，这对它们在现实生活中的实际应用构成了障碍。
Early autonomous driving perception algorithms tend to rely on LiDAR-based detectors\cite{Pointrcnn,Pointpillars,deng2021multi_H23DRCNN,RT3D,Voxelnet,wang2023auto-points}. Some methods\cite{RT3D,Voxelnet,Cia-ssd,SSN} employ anchor-based model architectures, which define anchor boxes of various shapes to guide class-specific object detection. Anchor-free models \cite{Centerpoint,afdet,AFDetV2,fan2023hcpvf,chen2020objecthotspots,deng2021multi_H23DRCNN} eliminate the manual design of anchor boxes, simplifying the training process and improving the adaptability of the model to objects of different scales. For example, CenterPoint \cite{Centerpoint} locates objects by learning to predict the centers of the targets. HCPVF\cite{fan2023hcpvf} adopts a hierarchical cascaded point-voxel fusion module with a two-layer detection head to balance accuracy and speed. $H^2$3D RCNN\cite{deng2021multi_H23DRCNN} captures long-range dependencies between features in 2D feature space to improve efficiency. Although LiDAR sensors provide depth information, they cannot directly capture complex geometric and ego-motion cues\cite{peri2023towardsLT3D,khan2024lidar}.

Camera sensors do not directly provide precise depth information, which makes early Camera-based detectors \cite{liu2020smoke,wang2021fcos3d,CaDDN,zhang2021objects} struggle with accurately estimating the distance and shape of objects, leading to overall performance that is generally inferior to LiDAR-based detectors. However, camera detectors excel in detecting rare object categories\cite{ma2023long_LT3D_2dlatefusion,peri2023towardsLT3D,kim2025labeldistill}, and recent camera-based methods \cite{ji2024enhancing,yang2023lite,park2024odd,li2024unimode,brazil2023omni3d,FSD,FSDV2,yang2023mixteaching,sheng2023pdr,huang2021b,jiang2024far3d} have shown significant improvements in handling complex traffic scenarios. For example, PDR\cite{sheng2023pdr} achieves more accurate depth estimation through progressive regularization.

Multi-modal 3D object detection enhances object detection by leveraging and integrating the data features of heterogeneous sensors. Recently, BEV-based (Bird's Eye View) methods \cite{bevdepth,xu2021fusionpainting,xu2024multi,bevfusion-mit,bevfusion-pku,song2024graphbev,song2024robofusion,graphalign,graphalign++} have effectively merged LiDAR and Camera representations into BEV space, achieving the state-of-the-art (SOTA) performance. Although pioneers such as BEVFusion\cite{bevfusion-mit} have demonstrated high performance, typically evaluated on datasets like nuScenes, they overlook real-world complexities, particularly the issue of `hard instance detection', which presents a challenge for practical deployment. 

\subsection{Guided Paradigm}
% 将现有工作划分为Image-guide-Point范式和Point-guide-Image范式是我们的DGFusion的理论基础。LiDAR数据天然具有可信的深度，因此使用点云作为输入的单模态模型的性能水平要高于使用Image作为输入的模型。这一现象已经成为多数研究者的共识\cite{}。然而，我们意识到，当这两种模态在统一空间进行融合时（例如广泛流行的BEV范式），相同的特征处理方式反而导致了模态之间的不平衡。同时，多数具有统一特征空间的检测模型通常不会考虑特征之间的不平衡。上述研究被我们归类为Point-guide-Image范式\cite{}。
% 少数研究者进一步指出，在融合范式中，image数据更有利于hard instances detection，我们将它们归类为Image-guide-Point范式\cite{}。早期工作通常遵循输入级融合\cite{mvp，PointPainting，Frustum PointNets}。MVP\cite{}是输入级融合的核心工作，其设计思路是将2D检测结果转化为3D虚拟点云，以增强原始点云。最近的工作表明，特征级融合\cite{lt3d，ma}比输入级融合更有效。（接上已有的）
Dividing existing work into the Image-guide-Point paradigm and the Point-guide-Image paradigm serves as the theoretical foundation of our DGFusion. LiDAR data inherently possesses reliable depth information; therefore, the performance of single-modal models that take point clouds as input is higher than that of models using images as input. This phenomenon has become a consensus among most researchers\cite{song2024robustness}. However, we have realized that when these two modalities are fused in a unified space (such as the widely used BEV paradigm), the same feature processing method conversely leads to an imbalance between the modalities. Meanwhile, most detection models with a unified feature space usually do not take into account the imbalance between features. The aforementioned studies are categorized by us into the Point-guide-Image paradigm\cite{bevfusion-mit,bevfusion-pku,ObjectFusion,chen2023focalformer3d,graphalign,graphalign++,bai2022transfusion,cao2024tfienet,UniTR}.

A small number of researchers further point out that in the fusion paradigm, image data is more conducive to hard instance detection, and we categorize these studies into the Image-guide-Point paradigm\cite{mvp,pointpainting,qi2018frustum_pointnets,wang2020high_7Dpointnet,wang2021pointaugmenting,peri2023towardsLT3D,ma2023long_LT3D_2dlatefusion}. Early works typically adhere to input-level fusion \cite{mvp,pointpainting,qi2018frustum_pointnets,wang2020high_7Dpointnet}. MVP\cite{mvp} is a core work in input-level fusion, whose design idea is to convert 2D detection results into 3D virtual point clouds to enhance the original point clouds. Recent works have shown that feature-level fusion \cite{wang2021pointaugmenting,peri2023towardsLT3D,ma2023long_LT3D_2dlatefusion} is more effective than input-level fusion.

Emerging as a novel third paradigm, our proposed dual-guided paradigm in this study advances beyond existing fusion strategies by establishing bidirectional feature interaction between point clouds and images.

\subsection{Hard Instance Detection}

% 近年来，研究者们基于不同范式提出了多种方案以缓解Hard Instance Detection这一自动驾驶场景中的关键挑战\cite{}。Point-guide-Image范式是多模态方法中最常见的范式，其特点是特征融合模块或者最终的检测结果更偏向LiDAR分支的特征。GraphAlign++\cite{}认为远程目标检测受益于更精确的特征对齐。UniTR\cite{}通过考虑2D感知和3D结构关系来集成不同的模态，并展示了其对传感器故障情况的鲁棒性。Image-guide-Point范式的研究相对较少，这类方法通常专注于长尾目标检测或Hard Instance Detection任务。Y. Ma et al.\cite{}提供了一个late-fusion框架来提高罕见类别的检测性能，并指出2D RGB探测器比3D RGB探测器具有更好的识别精度。LT3D\cite{}认为多模态线索对长尾3D检测至关重要。出于对安全性的考虑，LT3D提出了基于层次分类的指标Hierarchical Mean Average Precision，这种指标允许检测器的“合理”错误（例如将儿童检测为成年人）。单模态方案中，Camera-based Far3D\cite{}专注于远距离检测，在Argoverse 2数据集上展示了SoTA性能。LiDAR-based FocalFormer3D\cite{}提出了Hard Instance Probing (HIP), a general pipeline that identifies FN in a multi-stage manner and guides the models to focus on excavating difficult instances。此外，Dual-guided范式是我们提出的新范式，其核心是实现点云与图像特征的深度交互。

The challenge of hard instance detection in autonomous driving scenarios has driven extensive research across multiple paradigms, as evidenced by recent advances in sensor fusion and deep learning architectures\cite{jiang2024far3d,gupta2023far3det,liu2024sparsedet,chen2023focalformer3d,song2024robofusion,graphalign++,UniTR,peri2023towardsLT3D,ma2023long_LT3D_2dlatefusion,peri2023empirical,liu2023generalized,pan2024clipbevformer}. Current solutions predominantly bifurcate into single-modality approaches and multi-modal approaches, each demonstrating distinct advantages in addressing specific aspects of this complex problem.

Within the single-modality domain, camera-based and LiDAR-centric methodologies have achieved notable progress. Far3D\cite{jiang2024far3d} establishes new benchmarks for long-distance detection through its camera-only implementation, particularly excelling on the Argoverse 2 dataset. Mix-Teaching\cite{yang2023mixteaching} can outperform the baseline performance with only 10\% of KITTI\cite{kitti} monocular data for training. Its principle is to merge pseudo-labels from multiple frames into a single image for the semi-supervised training of the student model, and select reliable pseudo-labels through an uncertainty-based filter. Complementarily, LiDAR-oriented approaches like FocalFormer3D\cite{chen2023focalformer3d} introduce systematic solutions through Hard Instance Probing (HIP), a multi-stage pipeline, which employs false negative identification to optimize detection performance. LDFCNet\cite{wang2025rethinking} transfers the complex task of capturing long-range dependencies to 2D dense feature maps for processing, significantly reducing the computational cost. While these single-modality strategies demonstrate modality-specific efficacy, their inherent sensor limitations naturally motivate the exploration of multi-modal alternatives.

Transitioning to multi-modal paradigms, current research primarily evolves along two methodological axes. The predominant Point-guide-Image paradigm emphasizes LiDAR feature dominance in fusion architectures, with representative works demonstrating enhanced spatial alignment capabilities. GraphAlign++\cite{graphalign++} systematically addresses long-range detection challenges through precision feature alignment, while UniTR\cite{UniTR} establishes sensor-agnostic robustness via unified 2D-3D relationship modeling. Contrastingly, the Image-guide-Point paradigm remains less explored despite its potential in addressing data scarcity issues. Ma {\it et al.}\cite{ma2023long_LT3D_2dlatefusion} reveal through comparative analysis that 2D RGB detectors surpass their 3D counterparts in rare category recognition, a finding corroborated by LT3D's\cite{peri2023towardsLT3D} hierarchical evaluation framework that introduces Hierarchical mAP to accommodate permissible classification errors. HGSFusion\cite{gu2025hgsfusion} aims to resolve the errors between the two modalities during camera-radar fusion. In fact, there are also some robustness works that follow the `Point-guide-Point' paradigm. RaLiBEV\cite{yang2024ralibev} alleviates the environmental perception problem under severe weather by fusing the features of Radar range-azimuth heatmap and LiDAR point cloud. L4DR\cite{huang2025l4dr} achieves weather robustness by fusing LiDAR and 4D radar. In general, these pioneering works demonstrate the necessity of the fusion paradigm in hard instance detection.
% 实际上，还有一些鲁棒性工作遵守‘Point-guide-Point’范式。RaLiBEV通过融合Radar range-azimuth heatmap and the LiDAR point cloud的特征来缓解恶劣天气下的环境感知问题。L4DR\cite{}通过融合LiDAR和4D radar来实现 weather robust。总的来说，这些先驱性工作证明了融合范式在hard instance detection中的必要性。","\subsection{3D Object Detection in Autonomous Driving Perception}






Early autonomous driving perception algorithms tend to rely on LiDAR-based detectors\cite{Pointrcnn,Pointpillars,deng2021multi_H23DRCNN,RT3D,Voxelnet,wang2023auto-points}. Some methods\cite{RT3D,Voxelnet,Cia-ssd,SSN} employ anchor-based model architectures, which define anchor boxes of various shapes to guide class-specific object detection. Anchor-free models \cite{Centerpoint,afdet,AFDetV2,fan2023hcpvf,chen2020objecthotspots,deng2021multi_H23DRCNN} eliminate the manual design of anchor boxes, simplifying the training process and improving the adaptability of the model to objects of different scales. For example, CenterPoint \cite{Centerpoint} locates objects by learning to predict the centers of the targets. HCPVF\cite{fan2023hcpvf} adopts a hierarchical cascaded point-voxel fusion module with a two-layer detection head to balance accuracy and speed. $H^2$3D RCNN\cite{deng2021multi_H23DRCNN} captures long-range dependencies between features in 2D feature space to improve efficiency. Although LiDAR sensors provide depth information, they cannot directly capture complex geometric and ego-motion cues\cite{peri2023towardsLT3D,khan2024lidar}.

Camera sensors do not directly provide precise depth information, which makes early Camera-based detectors \cite{liu2020smoke,wang2021fcos3d,CaDDN,zhang2021objects} struggle with accurately estimating the distance and shape of objects, leading to overall performance that is generally inferior to LiDAR-based detectors. However, camera detectors excel in detecting rare object categories\cite{ma2023long_LT3D_2dlatefusion,peri2023towardsLT3D,kim2025labeldistill}, and recent camera-based methods \cite{ji2024enhancing,yang2023lite,park2024odd,li2024unimode,brazil2023omni3d,FSD,FSDV2,yang2023mixteaching,sheng2023pdr,huang2021b,jiang2024far3d} have shown significant improvements in handling complex traffic scenarios. For example, PDR\cite{sheng2023pdr} achieves more accurate depth estimation through progressive regularization.

Multi-modal 3D object detection enhances object detection by leveraging and integrating the data features of heterogeneous sensors. Recently, BEV-based (Bird's Eye View) methods \cite{bevdepth,xu2021fusionpainting,xu2024multi,bevfusion-mit,bevfusion-pku,song2024graphbev,song2024robofusion,graphalign,graphalign++} have effectively merged LiDAR and Camera representations into BEV space, achieving the state-of-the-art (SOTA) performance. Although pioneers such as BEVFusion\cite{bevfusion-mit} have demonstrated high performance, typically evaluated on datasets like nuScenes, they overlook real-world complexities, particularly the issue of `hard instance detection', which presents a challenge for practical deployment. 

\subsection{Guided Paradigm}


Dividing existing work into the Image-guide-Point paradigm and the Point-guide-Image paradigm serves as the theoretical foundation of our DGFusion. LiDAR data inherently possesses reliable depth information; therefore, the performance of single-modal models that take point clouds as input is higher than that of models using images as input. This phenomenon has become a consensus among most researchers\cite{song2024robustness}. However, we have realized that when these two modalities are fused in a unified space (such as the widely used BEV paradigm), the same feature processing method conversely leads to an imbalance between the modalities. Meanwhile, most detection models with a unified feature space usually do not take into account the imbalance between features. The aforementioned studies are categorized by us into the Point-guide-Image paradigm\cite{bevfusion-mit,bevfusion-pku,ObjectFusion,chen2023focalformer3d,graphalign,graphalign++,bai2022transfusion,cao2024tfienet,UniTR}.

A small number of researchers further point out that in the fusion paradigm, image data is more conducive to hard instance detection, and we categorize these studies into the Image-guide-Point paradigm\cite{mvp,pointpainting,qi2018frustum_pointnets,wang2020high_7Dpointnet,wang2021pointaugmenting,peri2023towardsLT3D,ma2023long_LT3D_2dlatefusion}. Early works typically adhere to input-level fusion \cite{mvp,pointpainting,qi2018frustum_pointnets,wang2020high_7Dpointnet}. MVP\cite{mvp} is a core work in input-level fusion, whose design idea is to convert 2D detection results into 3D virtual point clouds to enhance the original point clouds. Recent works have shown that feature-level fusion \cite{wang2021pointaugmenting,peri2023towardsLT3D,ma2023long_LT3D_2dlatefusion} is more effective than input-level fusion.

Emerging as a novel third paradigm, our proposed dual-guided paradigm in this study advances beyond existing fusion strategies by establishing bidirectional feature interaction between point clouds and images.

\subsection{Hard Instance Detection}



The challenge of hard instance detection in autonomous driving scenarios has driven extensive research across multiple paradigms, as evidenced by recent advances in sensor fusion and deep learning architectures\cite{jiang2024far3d,gupta2023far3det,liu2024sparsedet,chen2023focalformer3d,song2024robofusion,graphalign++,UniTR,peri2023towardsLT3D,ma2023long_LT3D_2dlatefusion,peri2023empirical,liu2023generalized,pan2024clipbevformer}. Current solutions predominantly bifurcate into single-modality approaches and multi-modal approaches, each demonstrating distinct advantages in addressing specific aspects of this complex problem.

Within the single-modality domain, camera-based and LiDAR-centric methodologies have achieved notable progress. Far3D\cite{jiang2024far3d} establishes new benchmarks for long-distance detection through its camera-only implementation, particularly excelling on the Argoverse 2 dataset. Mix-Teaching\cite{yang2023mixteaching} can outperform the baseline performance with only 10\

Transitioning to multi-modal paradigms, current research primarily evolves along two methodological axes. The predominant Point-guide-Image paradigm emphasizes LiDAR feature dominance in fusion architectures, with representative works demonstrating enhanced spatial alignment capabilities. GraphAlign++\cite{graphalign++} systematically addresses long-range detection challenges through precision feature alignment, while UniTR\cite{UniTR} establishes sensor-agnostic robustness via unified 2D-3D relationship modeling. Contrastingly, the Image-guide-Point paradigm remains less explored despite its potential in addressing data scarcity issues. Ma {\it et al.}\cite{ma2023long_LT3D_2dlatefusion} reveal through comparative analysis that 2D RGB detectors surpass their 3D counterparts in rare category recognition, a finding corroborated by LT3D's\cite{peri2023towardsLT3D} hierarchical evaluation framework that introduces Hierarchical mAP to accommodate permissible classification errors. HGSFusion\cite{gu2025hgsfusion} aims to resolve the errors between the two modalities during camera-radar fusion. In fact, there are also some robustness works that follow the `Point-guide-Point' paradigm. RaLiBEV\cite{yang2024ralibev} alleviates the environmental perception problem under severe weather by fusing the features of Radar range-azimuth heatmap and LiDAR point cloud. L4DR\cite{huang2025l4dr} achieves weather robustness by fusing LiDAR and 4D radar. In general, these pioneering works demonstrate the necessity of the fusion paradigm in hard instance detection.",
2511.09484v1,http://arxiv.org/abs/2511.09484v1,2025-11-12 16:54:00+00:00,SPIDER: Scalable Physics-Informed Dexterous Retargeting,"Learning dexterous and agile policy for humanoid and dexterous hand control requires large-scale demonstrations, but collecting robot-specific data is prohibitively expensive. In contrast, abundant human motion data is readily available from motion capture, videos, and virtual reality, which could help address the data scarcity problem. However, due to the embodiment gap and missing dynamic information like force and torque, these demonstrations cannot be directly executed on robots. To bridge this gap, we propose Scalable Physics-Informed DExterous Retargeting (SPIDER), a physics-based retargeting framework to transform and augment kinematic-only human demonstrations to dynamically feasible robot trajectories at scale. Our key insight is that human demonstrations should provide global task structure and objective, while large-scale physics-based sampling with curriculum-style virtual contact guidance should refine trajectories to ensure dynamical feasibility and correct contact sequences. SPIDER scales across diverse 9 humanoid/dexterous hand embodiments and 6 datasets, improving success rates by 18% compared to standard sampling, while being 10X faster than reinforcement learning (RL) baselines, and enabling the generation of a 2.4M frames dynamic-feasible robot dataset for policy learning. As a universal physics-based retargeting method, SPIDER can work with diverse quality data and generate diverse and high-quality data to enable efficient policy learning with methods like RL.","\subsection{Learning Manipulation from Human Data}

Recent work explores learning robot skills from large-scale human videos by first detecting actions and transferring them to robots.
One common strategy is kinematic retargeting, where human poses or keypoints are mapped to robot motions, as in DemoDiffusion~\citep{parkDemoDiffusionOneShotHuman2025}, OKAMI~\citep{liOKAMITeachingHumanoid2024}, R+X~\citep{papagiannisR+XRetrievalExecution2025}, and EgoZero~\citep{liuEgoZeroRobotLearning2025}.
Another strategy is to train a human-centric policy and then adapt it to robots through fine-tuning with in-domain data, as in MimicPlay~\citep{wangMimicPlayLongHorizonImitation2023}, Track2Act~\citep{bharadhwaj2024track2act}, and VideoDex~\citep{shawVideoDexLearningDexterity2022}.
Our approach is complementary to these pipelines. Specifically, \spider can serve as a drop-in replacement for the human-to-robot action transfer components, providing more accurate and contact-aware mappings for dexterous and whole-body control.

\subsection{Retargeting from Human Data}

Motion retargeting seeks to convert human data into robot trajectories that are physically consistent and executable.

\graybold{Kinematic Retargeting.}
These methods map human motion to robot configurations~\citep{qinOneHandMultiple2023}. While efficient and easy to compute, they often rely on specialized hardware~\citep{xuDexUMIUsingHuman2025}, handcrafted motion primitives~\citep{wuOneShotTransferLongHorizon2024}, and struggle with realism in contact-rich tasks~\citep{qinAnyTeleopGeneralVisionBased2024}. Being kinematic only, the generated motions are not compliant with physics constraints.

\graybold{Learning-Based Retargeting Networks.}
Neural mapping approaches train networks to convert human motions into robot motions~\citep{parkDemoDiffusionOneShotHuman2025,yinDexterityGenFoundationController2025}. Such models can outperform direct kinematic mappings and retain fast inference, but they require extensive pretraining and may fail when facing out-of-distribution motions or novel embodiments.

\graybold{Optimization-Based Retargeting.}
Optimization-based approaches explicitly incorporate physics and contact constraints to ensure dynamical feasibility~\citep{redaPhysicsbasedMotionRetargeting2023}. They can generate high-quality, physically plausible motions, but often depend on detailed contact data~\citep{lakshmipathyKinematicMotionRetargeting2024}, specific data pipelines~\citep{yangPhysicsDrivenDataGeneration2025}, and strong priors~\citep{nakaokaTaskModelLower2005}.
Due to the non-convex natural of the problem, sampling-based approaches~\citep{yangPhysicsDrivenDataGeneration2025} have emerged as a promising solution.

\graybold{RL-Based Retargeting.}
RL has been used to retarget human demonstrations across embodiments~\citep{lumCrossingHumanRobotEmbodiment2025,liManipTransEfficientDexterous2025}. When combined with curriculum learning~\citep{mandiDexMachinaFunctionalRetargeting2025,liuQuasiSimParameterizedQuasiPhysical2024}, RL can produce dexterous, physically feasible robot motions. However, it typically requires training on each trajectory and significant computation, which limits scalability to internet-scale data and real-time deployment.

Existing methods trade off between efficiency (kinematics, neural networks) and physical fidelity (optimization, RL). \spider combines the generality of RL with the efficiency of optimization-based pipelines, making it a scalable and practical drop-in replacement for human-to-robot transfer.

\subsection{Sampling-based Optimization for Robot Control}

Sampling-based optimization methods such as the cross-entropy method~\citep{deboerTutorialCrossEntropyMethod2005}, evolutionary algorithms~\citep{salimansEvolutionStrategiesScalable2017}, and Bayesian optimization~\citep{frazierTutorialBayesianOptimization2018a} are powerful tools for solving non-convex and non-smooth problems.
Due to their parallelizability and flexibility, these methods have been applied to navigation~\citep{williamsAggressiveDrivingModel2016}, legged locomotion~\citep{xueFullOrderSamplingBasedMPC2024}, and dexterous manipulation~\citep{howellPredictiveSamplingRealtime2022,liDROPDexterousReorientation2024}. Despite their success in contact-rich control, they can suffer from instability and solution ambiguity in trajectory sampling~\citep{kimSmoothModelPredictive2022}.
\spider addresses these challenges by guiding sampling with contact information, which helps preserve the intended contact sequence.","\subsection{Learning Manipulation from Human Data}

Recent work explores learning robot skills from large-scale human videos by first detecting actions and transferring them to robots.
One common strategy is kinematic retargeting, where human poses or keypoints are mapped to robot motions, as in DemoDiffusion~\citep{parkDemoDiffusionOneShotHuman2025}, OKAMI~\citep{liOKAMITeachingHumanoid2024}, R+X~\citep{papagiannisR+XRetrievalExecution2025}, and EgoZero~\citep{liuEgoZeroRobotLearning2025}.
Another strategy is to train a human-centric policy and then adapt it to robots through fine-tuning with in-domain data, as in MimicPlay~\citep{wangMimicPlayLongHorizonImitation2023}, Track2Act~\citep{bharadhwaj2024track2act}, and VideoDex~\citep{shawVideoDexLearningDexterity2022}.
Our approach is complementary to these pipelines. Specifically, \spider can serve as a drop-in replacement for the human-to-robot action transfer components, providing more accurate and contact-aware mappings for dexterous and whole-body control.

\subsection{Retargeting from Human Data}

Motion retargeting seeks to convert human data into robot trajectories that are physically consistent and executable.

\graybold{Kinematic Retargeting.}
These methods map human motion to robot configurations~\citep{qinOneHandMultiple2023}. While efficient and easy to compute, they often rely on specialized hardware~\citep{xuDexUMIUsingHuman2025}, handcrafted motion primitives~\citep{wuOneShotTransferLongHorizon2024}, and struggle with realism in contact-rich tasks~\citep{qinAnyTeleopGeneralVisionBased2024}. Being kinematic only, the generated motions are not compliant with physics constraints.

\graybold{Learning-Based Retargeting Networks.}
Neural mapping approaches train networks to convert human motions into robot motions~\citep{parkDemoDiffusionOneShotHuman2025,yinDexterityGenFoundationController2025}. Such models can outperform direct kinematic mappings and retain fast inference, but they require extensive pretraining and may fail when facing out-of-distribution motions or novel embodiments.

\graybold{Optimization-Based Retargeting.}
Optimization-based approaches explicitly incorporate physics and contact constraints to ensure dynamical feasibility~\citep{redaPhysicsbasedMotionRetargeting2023}. They can generate high-quality, physically plausible motions, but often depend on detailed contact data~\citep{lakshmipathyKinematicMotionRetargeting2024}, specific data pipelines~\citep{yangPhysicsDrivenDataGeneration2025}, and strong priors~\citep{nakaokaTaskModelLower2005}.
Due to the non-convex natural of the problem, sampling-based approaches~\citep{yangPhysicsDrivenDataGeneration2025} have emerged as a promising solution.

\graybold{RL-Based Retargeting.}
RL has been used to retarget human demonstrations across embodiments~\citep{lumCrossingHumanRobotEmbodiment2025,liManipTransEfficientDexterous2025}. When combined with curriculum learning~\citep{mandiDexMachinaFunctionalRetargeting2025,liuQuasiSimParameterizedQuasiPhysical2024}, RL can produce dexterous, physically feasible robot motions. However, it typically requires training on each trajectory and significant computation, which limits scalability to internet-scale data and real-time deployment.

Existing methods trade off between efficiency (kinematics, neural networks) and physical fidelity (optimization, RL). \spider combines the generality of RL with the efficiency of optimization-based pipelines, making it a scalable and practical drop-in replacement for human-to-robot transfer.

\subsection{Sampling-based Optimization for Robot Control}

Sampling-based optimization methods such as the cross-entropy method~\citep{deboerTutorialCrossEntropyMethod2005}, evolutionary algorithms~\citep{salimansEvolutionStrategiesScalable2017}, and Bayesian optimization~\citep{frazierTutorialBayesianOptimization2018a} are powerful tools for solving non-convex and non-smooth problems.
Due to their parallelizability and flexibility, these methods have been applied to navigation~\citep{williamsAggressiveDrivingModel2016}, legged locomotion~\citep{xueFullOrderSamplingBasedMPC2024}, and dexterous manipulation~\citep{howellPredictiveSamplingRealtime2022,liDROPDexterousReorientation2024}. Despite their success in contact-rich control, they can suffer from instability and solution ambiguity in trajectory sampling~\citep{kimSmoothModelPredictive2022}.
\spider addresses these challenges by guiding sampling with contact information, which helps preserve the intended contact sequence.","5.1 Learning Manipulation from Human Data
Recent work explores learning robot skills from large-scale human videos by first detecting actions and
transferringthemtorobots. Onecommonstrategyiskinematicretargeting, wherehumanposesorkeypointsare
mapped to robot motions, as in DemoDiffusion (Park et al., 2025), OKAMI (Li et al., 2024), R+X (Papagiannis
et al., 2025), and EgoZero (Liu et al., 2025). Another strategy is to train a human-centric policy and
then adapt it to robots through fine-tuning with in-domain data, as in MimicPlay (Wang et al., 2023),
Track2Act (Bharadhwaj et al., 2024), and VideoDex (Shaw et al., 2022). Our approach is complementary to
these pipelines. Specifically, SPIDERcan serve as a drop-in replacement for the human-to-robot action transfer
components, providing more accurate and contact-aware mappings for dexterous and whole-body control.
5.2 Retargeting from Human Data
Motion retargeting seeks to convert human data into robot trajectories that are physically consistent and
executable.
Kinematic Retargeting.These methods map human motion to robot configurations (Qin et al., 2022). While
efficient and easy to compute, they often rely on specialized hardware (Xu et al., 2025), handcrafted motion
primitives (Wu et al., 2024), and struggle with realism in contact-rich tasks (Qin et al., 2023). Being kinematic
only, the generated motions are not compliant with physics constraints.
Learning-Based Retargeting Networks.Neural mapping approaches train networks to convert human
motions into robot motions (Park et al., 2025; Yin et al., 2025b). Such models can outperform direct
kinematic mappings and retain fast inference, but they require extensive pretraining and may fail when facing
out-of-distribution motions or novel embodiments.
12
Optimization-Based Retargeting.Optimization-based approaches explicitly incorporate physics and contact
constraints to ensure dynamical feasibility (Reda et al., 2023). They can generate high-quality, physically
plausible motions, but often depend on detailed contact data (Lakshmipathy et al., 2025), specific data
pipelines (Yang et al., 2025b), and strong priors (Nakaoka et al., 2005). Due to the non-convex natural of the
problem, sampling-based approaches (Yang et al., 2025b) have emerged as a promising solution.
RL-Based Retargeting.RL has been used to retarget human demonstrations across embodiments (Lum et al.,
2025; Li et al., 2025c). When combined with curriculum learning (Mandi et al., 2025; Liu et al., 2024), RL can
produce dexterous, physically feasible robot motions. However, it typically requires training on each trajectory
and significant computation, which limits scalability to internet-scale data and real-time deployment.
Existing methods trade off between efficiency (kinematics, neural networks) and physical fidelity (optimization,
RL). SPIDERcombines the generality of RL with the efficiency of optimization-based pipelines, making it a
scalable and practical drop-in replacement for human-to-robot transfer.
5.3 Sampling-based Optimization for Robot Control
Sampling-based optimization methods such as the cross-entropy method (De Boer et al., 2005), evolutionary
algorithms (Salimans et al., 2017), and Bayesian optimization (Frazier, 2018) are powerful tools for solving
non-convex and non-smooth problems. Due to their parallelizability and flexibility, these methods have
been applied to navigation (Williams et al., 2016), legged locomotion (Xue et al., 2025), and dexterous
manipulation (Howell et al., 2022; Li et al., 2025a). Despite their success in contact-rich control, they can
suffer from instability and solution ambiguity in trajectory sampling (Kim et al., 2022). SPIDERaddresses
these challenges by guiding sampling with contact information, which helps preserve the intended contact
sequence."
2511.09871v1,http://arxiv.org/abs/2511.09871v1,2025-11-13 02:07:22+00:00,Expandable and Differentiable Dual Memories with Orthogonal Regularization for Exemplar-free Continual Learning,"Continual learning methods used to force neural networks to process sequential tasks in isolation, preventing them from leveraging useful inter-task relationships and causing them to repeatedly relearn similar features or overly differentiate them. To address this problem, we propose a fully differentiable, exemplar-free expandable method composed of two complementary memories: One learns common features that can be used across all tasks, and the other combines the shared features to learn discriminative characteristics unique to each sample. Both memories are differentiable so that the network can autonomously learn latent representations for each sample. For each task, the memory adjustment module adaptively prunes critical slots and minimally expands capacity to accommodate new concepts, and orthogonal regularization enforces geometric separation between preserved and newly learned memory components to prevent interference. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet show that the proposed method outperforms 14 state-of-the-art methods for class-incremental learning, achieving final accuracies of 55.13\%, 37.24\%, and 30.11\%, respectively. Additional analysis confirms that, through effective integration and utilization of knowledge, the proposed method can increase average performance across sequential tasks, and it produces feature extraction results closest to the upper bound, thus establishing a new milestone in continual learning.","\label{sec:related_works}
\subsection{Continual Learning}
Regularization-based methods aim to preserve prior knowledge by penalizing parameter changes or matching previous outputs. TwF \cite{boschini2022transfer} is a hybrid method built on a frozen pretrained sibling network that continuously propagates source-domain knowledge through a layer-wise loss term. FDR \cite{benjamin2018measuring} stores a tiny set of past samples and penalizes deviations in output logits on those exemplars, effectively distilling the old model’s function without full rehearsal. These methods avoid large buffers but still require at least some exemplars or held-out splits and impose global constraints that can over-restrict learning.

Dynamic expansion approaches allocate new parameters for each task to avoid interference by design. PNN \cite{rusu2016progressive} grows a frozen column of weights per task and add lateral connections for feature reuse, eliminating forgetting but leading to unbounded model growth. DEN \cite{yoon2017lifelong} selectively adds and prunes neurons based on task complexity, controlling expansion yet demanding careful pruning schedules. Parameter isolation methods constitute a mainstream approach in CL, where the upper layers of a previously trained neural network are frozen and only the remaining parameters are trained. PEC \cite{pernici2021class} is a fixed-classifier method that pre-allocates multiple output nodes from the beginning of training and applies the classification loss to each of them. Dynamic expansion suffers from unbounded model growth and parameter proliferation, while parameter isolation limits plasticity and hinders knowledge reuse across tasks.

\subsection{Dual Memory Approaches for Continual Learning}
Several CLS-based CL methods mimic human cognitive and memory processes by configuring two complementary modules. DualNets \cite{pham2021dualnet} uses an episodic buffer to train complementary fast and slow networks for supervised and self-supervised learning, achieving a plasticity–stability trade-off. CLS-ER \cite{arani2022learning} maintains short-term and long-term semantic memories alongside an episodic buffer to align decision boundaries, but they tend to add learning rate adjustment loss between models or require additional self-supervised learning objectives rather than directly extracting knowledge shared across tasks. ICL \cite{qi2024interactive} couples a vision transformer “fast thinker” with a frozen large language model “slow thinker” via attention and vMF-based routing, obviating exemplar storage but relying on heavyweight external models and a fixed backbone.

Rather than imposing rigid data- or class-based constraints or relying on external buffers and separate models, EDD integrates all knowledge end-to-end through a self-organizing memory. It decomposes inputs into sub-features that are stored in a fully differentiable memory, expanding to accommodate new tasks and pruning redundancies as needed. In doing so, it naturally extracts and shares inter-task information and resolves capacity limitations without any explicit analysis of the network itself or external dependencies.","\subsection{Continual Learning}
Regularization-based methods aim to preserve prior knowledge by penalizing parameter changes or matching previous outputs. TwF \cite{boschini2022transfer} is a hybrid method built on a frozen pretrained sibling network that continuously propagates source-domain knowledge through a layer-wise loss term. FDR \cite{benjamin2018measuring} stores a tiny set of past samples and penalizes deviations in output logits on those exemplars, effectively distilling the old model’s function without full rehearsal. These methods avoid large buffers but still require at least some exemplars or held-out splits and impose global constraints that can over-restrict learning.

Dynamic expansion approaches allocate new parameters for each task to avoid interference by design. PNN \cite{rusu2016progressive} grows a frozen column of weights per task and add lateral connections for feature reuse, eliminating forgetting but leading to unbounded model growth. DEN \cite{yoon2017lifelong} selectively adds and prunes neurons based on task complexity, controlling expansion yet demanding careful pruning schedules. Parameter isolation methods constitute a mainstream approach in CL, where the upper layers of a previously trained neural network are frozen and only the remaining parameters are trained. PEC \cite{pernici2021class} is a fixed-classifier method that pre-allocates multiple output nodes from the beginning of training and applies the classification loss to each of them. Dynamic expansion suffers from unbounded model growth and parameter proliferation, while parameter isolation limits plasticity and hinders knowledge reuse across tasks.

\subsection{Dual Memory Approaches for Continual Learning}
Several CLS-based CL methods mimic human cognitive and memory processes by configuring two complementary modules. DualNets \cite{pham2021dualnet} uses an episodic buffer to train complementary fast and slow networks for supervised and self-supervised learning, achieving a plasticity–stability trade-off. CLS-ER \cite{arani2022learning} maintains short-term and long-term semantic memories alongside an episodic buffer to align decision boundaries, but they tend to add learning rate adjustment loss between models or require additional self-supervised learning objectives rather than directly extracting knowledge shared across tasks. ICL \cite{qi2024interactive} couples a vision transformer “fast thinker” with a frozen large language model “slow thinker” via attention and vMF-based routing, obviating exemplar storage but relying on heavyweight external models and a fixed backbone.

Rather than imposing rigid data- or class-based constraints or relying on external buffers and separate models, EDD integrates all knowledge end-to-end through a self-organizing memory. It decomposes inputs into sub-features that are stored in a fully differentiable memory, expanding to accommodate new tasks and pruning redundancies as needed. In doing so, it naturally extracts and shares inter-task information and resolves capacity limitations without any explicit analysis of the network itself or external dependencies.","Continual Learning
Regularization-based methods aim to preserve prior knowl-
edge by penalizing parameter changes or matching previ-
ous outputs. TwF (Boschini et al. 2022b) is a hybrid method
built on a frozen pretrained sibling network that continu-
ously propagates source-domain knowledge through a layer-
wise loss term. FDR (Benjamin, Rolnick, and Kording 2018)
stores a tiny set of past samples and penalizes deviations in
output logits on those exemplars, effectively distilling the
old model’s function without full rehearsal. These methods
avoid large buffers but still require at least some exemplars
or held-out splits and impose global constraints that can
over-restrict learning.
Dynamic expansion approaches allocate new parameters
for each task to avoid interference by design. PNN (Rusu
et al. 2016) grows a frozen column of weights per task and
add lateral connections for feature reuse, eliminating forget-
ting but leading to unbounded model growth. DEN (Yoon
et al. 2017) selectively adds and prunes neurons based on
task complexity, controlling expansion yet demanding care-
ful pruning schedules. Parameter isolation methods consti-
tute a mainstream approach in CL, where the upper layers ofa previously trained neural network are frozen and only the
remaining parameters are trained. PEC (Pernici et al. 2021)
is a fixed-classifier method that pre-allocates multiple output
nodes from the beginning of training and applies the classifi-
cation loss to each of them. Dynamic expansion suffers from
unbounded model growth and parameter proliferation, while
parameter isolation limits plasticity and hinders knowledge
reuse across tasks.
Dual Memory Approaches for Continual Learning
Several CLS-based CL methods mimic human cognitive and
memory processes by configuring two complementary mod-
ules. DualNets (Pham, Liu, and Hoi 2021) uses an episodic
buffer to train complementary fast and slow networks for
supervised and self-supervised learning, achieving a plastic-
ity–stability trade-off. CLS-ER (Arani, Sarfraz, and Zonooz
2022) maintains short-term and long-term semantic mem-
ories alongside an episodic buffer to align decision bound-
aries, but they tend to add learning rate adjustment loss be-
tween models or require additional self-supervised learning
objectives rather than directly extracting knowledge shared
across tasks. ICL (Qi et al. 2024) couples a vision trans-
former “fast thinker” with a frozen large language model
“slow thinker” via attention and vMF-based routing, obvi-
ating exemplar storage but relying on heavyweight external
models and a fixed backbone.
Rather than imposing rigid data- or class-based con-
straints or relying on external buffers and separate models,
EDD integrates all knowledge end-to-end through a self-
organizing memory. It decomposes inputs into sub-features
that are stored in a fully differentiable memory, expand-
ing to accommodate new tasks and pruning redundancies as
needed. In doing so, it naturally extracts and shares inter-
task information and resolves capacity limitations without
any explicit analysis of the network itself or external depen-
dencies.
Proposed Method
Differentiable Dual Memory
This paper addresses a class-incremental learning (CIL) sce-
nario defined by a sequence ofNtasksT 1, . . . ,T N. Each
taskT tintroduces a dataset containing classes disjoint from
those in previous tasks. The modelFcomprises a feature
encoderE(e.g., a ResNet backbone) and a classifierC,
producing predictionsy=C(E(x)). Within the encoder,
two complementary memories are used at intermediate lay-
ers, each placed after specific encoder blocks, and trained
end-to-end with the rest of the network. The shared memory
Mscaptures representations that can be reused across tasks,
while the task-specific memoryMtencodes features unique
to each task. This design ensures that, upon each new input,
the model can access its memory and retrieve relevant exist-
ing information. Figure 2 shows the overall architecture of
EDD and its key components.
Each memoryMℓ(forℓ∈s, t) is a differentiable
key–value memory withL ℓlearnable slots. Let the memory
keysKℓbe defined as
Kℓ=
kℓ
1, . . . ,kℓ
Lℓ⊤∈RLℓ×d.(1)
New Data
Conv. Blockሾܪǡܹǡܥሿܟ
෠ Task࣮௧
MLPȉȉȉܥ௧
௧௧௧௧௧௧௧௧௧௧௧
eeeeeeeeeeeeewD a t a
Dt
ȣ௧ିଵ
ȣ௧

ࣦκ
ሺܭκሻ௧ܯ௧ିଵ
ܯ௧ࣦୟ୪୧୥୬ሺܭκሻ௧ିଵ
ሺܸκሻ௧True label ݕȉȉȉܥ௧ିଵොݕሾǣݐെͳሿ
ොݕሾǣݐሿࣦେ୉ଵሺܸκሻ௧ିଵFigure 2: Overview of the proposed method.
Similarly, the memory valuesVℓare defined as
Vℓ=
vℓ
1, . . . ,vℓ
Lℓ⊤∈RLℓ×d′.(2)
Given an input’s intermediate feature mapH ∈
RC×H×Wfrom the encoder, each spatial feature vector
h∈Rd(extracted fromH; e.g.,d=H×Win Figure 2)
serves as a query to the memory. The memory-read is com-
puted via cosine-similarity attention over the keys. For each
memory slotj, an attention weight is obtained as
wj=exp"
2511.09675v1,http://arxiv.org/abs/2511.09675v1,2025-11-12 19:27:40+00:00,PriVi: Towards A General-Purpose Video Model For Primate Behavior In The Wild,"Non-human primates are our closest living relatives, and analyzing their behavior is central to research in cognition, evolution, and conservation. Computer vision could greatly aid this research, but existing methods often rely on human-centric pretrained models and focus on single datasets, which limits generalization. We address this limitation by shifting from a model-centric to a data-centric approach and introduce PriVi, a large-scale primate-centric video pretraining dataset. PriVi contains 424 hours of curated video, combining 174 hours from behavioral research across 11 settings with 250 hours of diverse web-sourced footage, assembled through a scalable data curation pipeline. We pretrain V-JEPA on PriVi to learn primate-specific representations and evaluate it using a lightweight frozen classifier. Across four benchmark datasets, ChimpACT, BaboonLand, PanAf500, and ChimpBehave, our approach consistently outperforms prior work, including fully finetuned baselines, and scales favorably with fewer labels. These results demonstrate that primate-centric pretraining substantially improves data efficiency and generalization, making it a promising approach for low-label applications. Code, models, and the majority of the dataset will be made available.","\label{sec:related_work}






\paragraph{Self-supervised video learning} Learning useful representations from unlabeled data has been a cornerstone of machine learning research for decades \cite{tenenbaum_global_2000, hinton_fast_2006, hinton_reducing_2006, vincent_extracting_2008, kingma_auto-encoding_2014, rezende_stochastic_2014}. The recent success of large language models using masked language modeling \cite{devlin_bert_2019, radford_improving_2018, brown_language_2020} popularized self-supervised learning (SSL) as we know it today. There exists a plethora of SSL techniques ranging from contrastive learning/deep metric learning \cite{radford_learning_2021, chen_simple_2020} over self-distillation \cite{caron_emerging_2021} to masked reconstruction in pixel~\cite{tong_videomae_2022, he_masked_2021} or latent space~\cite{bardes_revisiting_2024, salehi_sigma_2024, assran_self-supervised_2023, assran_v-jepa_2025}, with many modern approached combining several of those paradigms \cite{oquab_dinov2_2024, zhou_image_2022}.

The key ingredient for today's foundation models -- large-scale models that can be adapted to a wide range of tasks -- is combining SSL with massive Web-scale datasets~\cite{schuhmann_laion-5b_2022, miech_howto100m_2019, kuznetsova_open_2020}. These image foundation models, most notably CLIP~\cite{radford_learning_2021} and DINOv2~\cite{oquab_dinov2_2024},  caused a paradigm shift from handcrafted and task-specific architectures towards models using foundation models as frozen feature extractors for many computer vision tasks~\cite{luddecke_image_2022, ryan_gaze-lle_2025}. Recently, there has also been considerable advancement in foundation models for video understanding with several architectures proposed based on video-caption pairs like PerceptionEncoder~\cite{bolya_perception_2025} and VideoCLIP~\cite{xu_videoclip_2021}, masked video autoencoding \cite{tong_videomae_2022, feichtenhofer_masked_2022}, masked reconstruction in latent space like V-JEPA~\cite{bardes_revisiting_2024,assran_v-jepa_2025}, and combined approaches like VideoPrism~\cite{zhao_videoprism_2024}.

Web-scale datasets made foundation models possible, but work on data-centric learning \cite{gadre_datacomp_2023} shows that neither model nor data scale is sufficient: Careful curation of training data considerably improves performance. Many foundation models thus employ data engines for automated data curation \cite{oquab_dinov2_2024, bolya_perception_2025, ravi_sam_2025, kirillov_segment_2023} .








\paragraph{Animal behavior recognition} 
Recent years have seen a rapidly growing number of annotated datasets for animal behavior recognition, either for single species~\cite{rodriguez-juan_visual_2025, gabeff_mammalps_2025} or across species~\cite{ng_animal_2022, chen_mammalnet_2023}. Primate specific datasets cover diverse settings, ranging from zoo recordings in \mbox{ChimpACT}~\cite{ma_chimpact_2023} and ChimpBehave~\cite{fuchs_forest_2025} over in-the-wild recordings using camera traps in the PanAf-family of datasets~\cite{brookes_panaf20k_2024, brookes_panaf-fgbg_2025} to drone footage in BaboonLand~\cite{duporge_baboonland_2025}. The size of these datasets ranges between 2\,h of frame-wise  annotations (\mbox{ChimpACT}, PanAf500) to 80\,h of clip-wise annotations (PanAf20k). Behavior recognition is usually operationalized as an action classification task, e.\,g. classifying which actions are performed in a specific miniclip \cite{duporge_baboonland_2025, fuchs_forest_2025, brookes_panaf20k_2024}, or spatio-temporal action recognition, where actions need to be localized in videos with \cite{ma_chimpact_2023} or without \cite{ma_alphachimp_2024} ground truth bounding boxes of animals given.


There has been considerable prior work for automated behavior recognition: \citet{bain_automated_2021} showed that they can discriminate two distinctive actions in wild chimpanzees using audiovisual input. Recently, several methods for more challenging behavior recognition, like distinguishing between all classes in PanAf \cite{brookes_panaf20k_2024} or ChimpACT \cite{ma_chimpact_2023} have been proposed  \cite{ma_alphachimp_2024, brookes_chimpvlm_2024, brookes_triple-stream_2023}. However, each of these works focuses on a single dataset and builds on models pretrained on human-centric datasets, like Kinetics~\cite{kay_kinetics_2017}. They also rely on considerable amounts of training data, which is costly to acquire. 










\paragraph{Self-supervised learning for behavior recognition}
In recent years, foundation models emerged as a promising feature extractor for behavior recognition. Previous works found that frozen evaluation of VideoPRISM models is state-of-the-art or competitive with specialized methods on ChimpACT~\cite{ma_chimpact_2023}, KABR~\cite{kholiavchenko_kabr_2024} -- a dataset of Kenyan wildlife --, as well as various datasets of lab rodents~\cite{sun_video_2024, zhao_videoprism_2024}. \citet{mamooler_fine-tuning_2025} show that jointly finetuning a large vision-language model on multiple animal behavior datasets improves performance across all, suggesting a promising step towards unified, general-purpose models without the need for specialized architectures.


For animal behavior, unlabeled data is usually easy to acquire, while labeled data is scarce. Thus, several works explored utilizing in-domain unlabeled data and found it beneficial for animal identification~\cite{iashin_self-supervised_2025}, animal behavior analysis in the lab~\cite{wang_self-supervised_2025}, or weakly-supervised training for behavior retrieval~\cite{santo_fine-tuning_2025}. However, all of these methods again develop models specialized for individual datasets.","\paragraph{Self-supervised video learning} Learning useful representations from unlabeled data has been a cornerstone of machine learning research for decades \cite{tenenbaum_global_2000, hinton_fast_2006, hinton_reducing_2006, vincent_extracting_2008, kingma_auto-encoding_2014, rezende_stochastic_2014}. The recent success of large language models using masked language modeling \cite{devlin_bert_2019, radford_improving_2018, brown_language_2020} popularized self-supervised learning (SSL) as we know it today. There exists a plethora of SSL techniques ranging from contrastive learning/deep metric learning \cite{radford_learning_2021, chen_simple_2020} over self-distillation \cite{caron_emerging_2021} to masked reconstruction in pixel~\cite{tong_videomae_2022, he_masked_2021} or latent space~\cite{bardes_revisiting_2024, salehi_sigma_2024, assran_self-supervised_2023, assran_v-jepa_2025}, with many modern approached combining several of those paradigms \cite{oquab_dinov2_2024, zhou_image_2022}.

The key ingredient for today's foundation models -- large-scale models that can be adapted to a wide range of tasks -- is combining SSL with massive Web-scale datasets~\cite{schuhmann_laion-5b_2022, miech_howto100m_2019, kuznetsova_open_2020}. These image foundation models, most notably CLIP~\cite{radford_learning_2021} and DINOv2~\cite{oquab_dinov2_2024},  caused a paradigm shift from handcrafted and task-specific architectures towards models using foundation models as frozen feature extractors for many computer vision tasks~\cite{luddecke_image_2022, ryan_gaze-lle_2025}. Recently, there has also been considerable advancement in foundation models for video understanding with several architectures proposed based on video-caption pairs like PerceptionEncoder~\cite{bolya_perception_2025} and VideoCLIP~\cite{xu_videoclip_2021}, masked video autoencoding \cite{tong_videomae_2022, feichtenhofer_masked_2022}, masked reconstruction in latent space like V-JEPA~\cite{bardes_revisiting_2024,assran_v-jepa_2025}, and combined approaches like VideoPrism~\cite{zhao_videoprism_2024}.

Web-scale datasets made foundation models possible, but work on data-centric learning \cite{gadre_datacomp_2023} shows that neither model nor data scale is sufficient: Careful curation of training data considerably improves performance. Many foundation models thus employ data engines for automated data curation \cite{oquab_dinov2_2024, bolya_perception_2025, ravi_sam_2025, kirillov_segment_2023} .








\paragraph{Animal behavior recognition} 
Recent years have seen a rapidly growing number of annotated datasets for animal behavior recognition, either for single species~\cite{rodriguez-juan_visual_2025, gabeff_mammalps_2025} or across species~\cite{ng_animal_2022, chen_mammalnet_2023}. Primate specific datasets cover diverse settings, ranging from zoo recordings in \mbox{ChimpACT}~\cite{ma_chimpact_2023} and ChimpBehave~\cite{fuchs_forest_2025} over in-the-wild recordings using camera traps in the PanAf-family of datasets~\cite{brookes_panaf20k_2024, brookes_panaf-fgbg_2025} to drone footage in BaboonLand~\cite{duporge_baboonland_2025}. The size of these datasets ranges between 2\,h of frame-wise  annotations (\mbox{ChimpACT}, PanAf500) to 80\,h of clip-wise annotations (PanAf20k). Behavior recognition is usually operationalized as an action classification task, e.\,g. classifying which actions are performed in a specific miniclip \cite{duporge_baboonland_2025, fuchs_forest_2025, brookes_panaf20k_2024}, or spatio-temporal action recognition, where actions need to be localized in videos with \cite{ma_chimpact_2023} or without \cite{ma_alphachimp_2024} ground truth bounding boxes of animals given.


There has been considerable prior work for automated behavior recognition: \citet{bain_automated_2021} showed that they can discriminate two distinctive actions in wild chimpanzees using audiovisual input. Recently, several methods for more challenging behavior recognition, like distinguishing between all classes in PanAf \cite{brookes_panaf20k_2024} or ChimpACT \cite{ma_chimpact_2023} have been proposed  \cite{ma_alphachimp_2024, brookes_chimpvlm_2024, brookes_triple-stream_2023}. However, each of these works focuses on a single dataset and builds on models pretrained on human-centric datasets, like Kinetics~\cite{kay_kinetics_2017}. They also rely on considerable amounts of training data, which is costly to acquire. 










\paragraph{Self-supervised learning for behavior recognition}
In recent years, foundation models emerged as a promising feature extractor for behavior recognition. Previous works found that frozen evaluation of VideoPRISM models is state-of-the-art or competitive with specialized methods on ChimpACT~\cite{ma_chimpact_2023}, KABR~\cite{kholiavchenko_kabr_2024} -- a dataset of Kenyan wildlife --, as well as various datasets of lab rodents~\cite{sun_video_2024, zhao_videoprism_2024}. \citet{mamooler_fine-tuning_2025} show that jointly finetuning a large vision-language model on multiple animal behavior datasets improves performance across all, suggesting a promising step towards unified, general-purpose models without the need for specialized architectures.


For animal behavior, unlabeled data is usually easy to acquire, while labeled data is scarce. Thus, several works explored utilizing in-domain unlabeled data and found it beneficial for animal identification~\cite{iashin_self-supervised_2025}, animal behavior analysis in the lab~\cite{wang_self-supervised_2025}, or weakly-supervised training for behavior retrieval~\cite{santo_fine-tuning_2025}. However, all of these methods again develop models specialized for individual datasets.","Self-supervised video learning.Learning useful represen-
tations from unlabeled data has been a cornerstone of ma-
chine learning research for decades [24, 25, 29, 43, 53, 55].
The recent success of large language models using masked
language modeling [10, 15, 40] popularized self-supervised
learning (SSL) as we know it today. There exists a plethora
of SSL techniques ranging from contrastive learning/deepmetric learning [14, 41] over self-distillation [11] to masked
reconstruction in pixel [23, 54] or latent space [1, 2, 4, 47],
with many modern approached combining several of those
paradigms [39, 62].
The key ingredient for today’s foundation models –
large-scale models that can be adapted to a wide range
of tasks – is combining SSL with massive Web-scale
datasets [31, 37, 49]. These image foundation models, most
notably CLIP [41] and DINOv2 [39], caused a paradigm
shift from handcrafted and task-specific architectures to-
wards models using foundation models as frozen feature
extractors for many computer vision tasks [33, 45]. Re-
cently, there has also been considerable advancement in
foundation models for video understanding with several ar-
chitectures proposed based on video-caption pairs like Per-
ceptionEncoder [5] and VideoCLIP [58], masked video au-
toencoding [18, 54], masked reconstruction in latent space
like V-JEPA [2, 4], and combined approaches like Video-
Prism [61].
Web-scale datasets made foundation models possible,
but work on data-centric learning [21] shows that neither
model nor data scale is sufficient: Careful curation of train-
ing data considerably improves performance. Many foun-
dation models thus employ data engines for automated data
curation [5, 30, 39, 42] .
Animal behavior recognition.Recent years have seen
a rapidly growing number of annotated datasets for an-
imal behavior recognition, either for single species [20,
44] or across species [13, 38]. Primate specific datasets
cover diverse settings, ranging from zoo recordings in
ChimpACT [34] and ChimpBehave [19] over in-the-wild
recordings using camera traps in the PanAf-family of
datasets [8, 9] to drone footage in BaboonLand [16]. The
size of these datasets ranges between 2 h of frame-wise an-
2
notations (ChimpACT, PanAf500) to 80 h of clip-wise an-
notations (PanAf20k). Behavior recognition is usually op-
erationalized as an action classification task, e. g. classi-
fying which actions are performed in a specific miniclip
[8, 16, 19], or spatio-temporal action recognition, where ac-
tions need to be localized in videos with [34] or without
[35] ground truth bounding boxes of animals given.
There has been considerable prior work for automated
behavior recognition: Bain et al. [3] showed that they can
discriminate two distinctive actions in wild chimpanzees us-
ing audiovisual input. Recently, several methods for more
challenging behavior recognition, like distinguishing be-
tween all classes in PanAf [8] or ChimpACT [34] have
been proposed [6, 7, 35]. However, each of these works fo-
cuses on a single dataset and builds on models pretrained on
human-centric datasets, like Kinetics [27]. They also rely
on considerable amounts of training data, which is costly to
acquire.
Self-supervised learning for behavior recognition.In
recent years, foundation models emerged as a promis-
ing feature extractor for behavior recognition. Previous
works found that frozen evaluation of VideoPRISM mod-
els is state-of-the-art or competitive with specialized meth-
ods on ChimpACT [34], KABR [28] – a dataset of Kenyan
wildlife –, as well as various datasets of lab rodents [51,
61]. Mamooler et al. [36] show that jointly finetuning
a large vision-language model on multiple animal behav-
ior datasets improves performance across all, suggesting
a promising step towards unified, general-purpose models
without the need for specialized architectures.
For animal behavior, unlabeled data is usually easy to ac-
quire, while labeled data is scarce. Thus, several works ex-
plored utilizing in-domain unlabeled data and found it ben-
eficial for animal identification [26], animal behavior anal-
ysis in the lab [57], or weakly-supervised training for be-
havior retrieval [48]. However, all of these methods again
develop models specialized for individual datasets."
2511.08823v1,http://arxiv.org/abs/2511.08823v1,2025-11-11 22:40:00+00:00,DT-NVS: Diffusion Transformers for Novel View Synthesis,"Generating novel views of a natural scene, e.g., every-day scenes both indoors and outdoors, from a single view is an under-explored problem, even though it is an organic extension to the object-centric novel view synthesis. Existing diffusion-based approaches focus rather on small camera movements in real scenes or only consider unnatural object-centric scenes, limiting their potential applications in real-world settings. In this paper we move away from these constrained regimes and propose a 3D diffusion model trained with image-only losses on a large-scale dataset of real-world, multi-category, unaligned, and casually acquired videos of everyday scenes. We propose DT-NVS, a 3D-aware diffusion model for generalized novel view synthesis that exploits a transformer-based architecture backbone. We make significant contributions to transformer and self-attention architectures to translate images to 3d representations, and novel camera conditioning strategies to allow training on real-world unaligned datasets. In addition, we introduce a novel training paradigm swapping the role of reference frame between the conditioning image and the sampled noisy input. We evaluate our approach on the 3D task of generalized novel view synthesis from a single input image and show improvements over state-of-the-art 3D aware diffusion models and deterministic approaches, while generating diverse outputs.","% Generating a novel view of scene from an un-posed initial view is a challenging and important problem 
% for reasoning about natural scenes, specially for intelligent agents.
% when the only information available is an un-posed initial view  
% Creating entire 3D scenes from limited observations requires advanced 3D generation techniques to fill in unseen regions. 
% Our work builds on the growing research in diffusion models and 3D generative models \cite{po2023diffusion}. 

\textbf{Diffusion Models:} A diffusion model is a generative approach similar to GANs, first proposed by Sohl-Dickstein \textit{et al.}~\cite{sohl-dickstein2015deep}, and gained popularity with DDPM~\cite{ho2020denoising}. 
Various techniques have been proposed to enhance the quality of diffusion model outcomes, including Cosine Schedule~\cite{dhariwal2021diffusion}, v-parameterization~\cite{salimans2022progressive}, and classifier-free guidance~\cite{ho2022classifier}. 
DDIM~\cite{song2020denoising} offers a more flexible and efficient sampling process, enabling faster generation of high-quality images with fewer steps.
Hang \textit{et al.}~\cite{hang2023efficient} proposed a minimum signal-to-noise ratio (Min-SNR) strategy, showing that the model converges faster by using SNR as a weight to the loss function.
DiT~\cite{peebles2023scalable} scales the Diffusion model with Transformers by conditioning diffusion steps with Adaptive Layer Normalization(AdaLN)-Zero~\cite{xu2019understanding}.

\textbf{Neural Radiance Fields: } Neural 3D implicit representations were initially proposed in  SRN~\cite{sitzmann2019scene} and later used in DVR~\cite{niemeyer2020differentiable}, to train 3D-aware representations without 3D ground-truth.
NeRF~\cite{mildenhall2021nerf} revolutionized novel view synthesis from collections of posed images.
To accelerate NeRF training, grid-based representations have been proposed. ~\cite{yu2021plenoxels, muller2022instant,fridovich2023k, tilted2023} EG3D applied triplanes by projecting features into three planes in the context of 3D-aware GANs and TensoRF~\cite{chen2022tensorf} proposed the vector-matrix representation.
Efforts to generalize NeRF include conditioning on global latent vectors~\cite{Gafni_2021_CVPR, jang2021codenerf, muller2022autorf,rebain2022lolnerf,athar2022rignerf,hong2022headnerf}, associating 2D feature views with target views~\cite{yu2021pixelnerf, wang2021ibrnet, chen2021mvsnerf,reizenstein2021common, Henzler_2021_CVPR, trevithick2021grf,chen2023matchnerf, irshad2023neo360}, or supervising NeRF with GAN losses~\cite{chanmonteiro2020pi-GAN, eg3dChan2021, schwarz2020graf,le2022stylemorph,cai2022pix2nerf}. 
% SRN~\cite{sitzmann2019scene} and DVR~\cite{niemeyer2020differentiable} train the 3d-aware model without requiring 3D ground-truth. 
% \john{I moved the last sentence please check, {\tt target views cite[lin2023]} was a broken citation that I removed. } Done!



\textbf{3D-Aware Diffusion Models: } Extending  diffusion models to be 3D-aware has proved challenging due to the lack of large 3D ground truth datasets.
Previous approaches fine-tuned a pre-trained latent diffusion model~\cite{rombach2022high} on camera parameters~\cite{liu2023zero, melas2023realfusion, shi2023mvdream, tang2023dreamgaussian, wang2023prolificdreamer, zeronvs, watson2022novel}
Other approaches~\cite{poole2022dreamfusion, tang2023dreamgaussian, wu2023reconfusion, wang2023morpheus} regularize NeRF using the pre-trained diffusion models or utilize the pre-trained diffusion models to generate images.
% 3DiM conditions the model on camera parameters~\cite{watson2022novel}, while DreamFusion \cite{poole2022dreamfusion} introduced Score Distillation Sampling (SDS) to synthesize 3D objects from text prompts using the pre-trained 2D diffusion models.
% Latent diffusion~\cite{rombach2022high} with the release of Stable Diffusion allows us to run the diffusion model on consumption-level GPUs, and this has been used in 3D-aware diffusion models.
% Several approaches have been proposed to fine-tune the pre-trained Stable Diffusion models into 3D-aware by conditioning on camera parameters. ~\cite{liu2023zero, melas2023realfusion, shi2023mvdream, tang2023dreamgaussian, wang2023prolificdreamer, zeronvs}
% Other approaches turn the pre-trained diffusion model into multi-view data generator or use it to regularize NeRF.~\cite{xu2023dmv3d, tang2023dreamgaussian, wu2023reconfusion}.
Similar to latent diffusion, several approaches first train the 3D-aware GAN and apply the diffusion process on latent vectors~\cite{Schwarz2024ICLR, kim2023nfldm}.
Instead of 2-stage training, there are approaches other methods obtain intermediate features and use them to denoise from the target view ~\cite{chan2023genvs, gu2023nerfdiff, tewari2023forwarddiffusion}.
Another line of research involves learning diffusion models in 3D by rendering in 2D or backprojecting 2D features into 3D~\cite{karnewar2023holodiffusion, anciukevicius2024denoising, anciukevivcius2023renderdiffusion, szymanowicz2023viewset}, which usually assume the aligned scenes.

\textbf{Transformers for 3D Tasks: } Vision Transformer (ViT)~\cite{dosovitskiy2020image} has been successful in the field of computer vision, and 
Masked Autoencoder(MAE)~\cite{he2022masked} learns visual features in self-supervised learning. 
For 3D, geometry-free methods based on transformer have been explored in ~\cite{kulhanek2022viewformer, sajjadi2022scene, Miyato2024GTA}.
To build 3D implicit representation from Transformer, GINA3D~\cite{shen2023gina} and VQ3D~\cite{sargent2023vq3d} employ the adversarial loss.
Other approaches deal with point clouds~\cite{jun2023shap, wu2023multiview}.
LRM~\cite{hong2023lrm} and NViST~\cite{jang2023nvist} apply the Transformer for 3D implicit representation in a deterministic way, and DMV~\cite{xu2023dmv3d} extends LRM into a diffusion, but both LRM and DMV do not model the background.
\OURS{} is the first method to apply the Transformer with a 3D-aware diffusion model, directly learning from real-world scenes.


% transformer 3d models

% Another line of research involves learning feed-forward models that take a few views as input and output 3D representations directly without per-instance optimization \cite{hong2023lrm, gupta2023triplane, szymanowicz2023viewset, xu2023dmv3d, li2023instant3d}. 
% Our work is inspired by this approach, although we mix our approach with diffusion processes to allow for context to be extended to full scenes rather than object-focused scenes. 






% Advancements in neural rendering~\cite{mildenhall2021nerf} have revolutionized 3D reconstruction and novel view synthesis from collections of posed images. 
% In order to reduce the quantity need of posed images, 
% multiple methods focused on only using a single view 
% to generate new views.
% The first wave of methods addressing this problem focused on training 
% convolutional neural networks based approaches to generate cost volumes 
% or even directly to images~\cite{chen2021mvsnerf, chen2023matchnerf}. 
% More recently methods have focused on different approaches for generating 
% novel views, 
% such as leveraging triplanes with diffusion~\cite{single-stage-diffusion}, 
% triplanes with gan models~\cite{eg3dChan2021},
% transformers~\cite{hong2023lrm}, 
% \textit{etc.}
% % 

% \noindent\textbf{Conditioning on text}:
% Pretrained text-to-image models provide a strong generative prior for text-to-3D generation. 
% % However, distilling this into a coherent 3D model often requires iterative distillation. 
% DreamFusion \cite{poole2022dreamfusion} introduced Score Distillation Sampling (SDS) to synthesize 3D objects from text prompts, leading to improvements in distillation strategies \cite{huang2023dreamtime, wang2023steindreamer, kim2024collaborative, wang2023prolificdreamer, haque2023instructnerf2nerf}, 
% whereas using different 3d representations has also been explored~\cite{chen2023fantasia3d, lin2023magic3d, tang2023dreamgaussian, yi2023gaussiandreamer, epstein2024disentangled}. 
% % Using text-based priors for single-image-to-3D generation shows promise but requires balancing the image observation with additional constraints, often resulting in poor global geometry \cite{melas2023realfusion, qian2023magic123, tang2023makeit3d}.
% The approaches are quite exciting and promising, but they require well behaved training data as well as large text annotation for generating deep priors. 


% Text-to-image models excel at generating visually appealing images but lack precise pose control, necessitating a time-consuming 3D distillation process. 
% To overcome this, several approaches train generative models with explicit image and pose conditioning \cite{watson2022novel, raj2023dreambooth3d, gu2023nerfdiff, chan2023genvs}. 
% These models provide stronger priors for the object's appearance, 
% but these methods model all output views independently, requiring expensive 3D distillation to resolve inconsistencies in novel views when there is uncertainty.

% \noindent\textbf{Multi-View Priors:} Modeling correlations between multiple views provides a stronger prior for consistent 3D content from partial observations. 
% Methods like MVDream \cite{shi2023mvdream}, ImageDream \cite{wang2023imagedream}, and Zero123++ \cite{shi2023zero123pp} fine-tune text-to-image models to generate multiple views simultaneously. CAT3D's architecture is similar to ImageDream, where the multi-view dependency is captured by an architecture resembling video diffusion models with 3D self-attention. Given this stronger prior, these papers also demonstrate higher quality and more efficient 3D extraction.

% % Video Priors: Video diffusion models have demonstrated an impressive ability to generate realistic videos \cite{blattmann2023stablediffusion, blattmann2023highres, girdhar2023emu, bartal2024lumiere, gupta2023photorealistic, brooks2024videogen}. However, using off-the-shelf video diffusion models for 3D generation is challenging due to the lack of exact camera controls and the difficulty in generating videos with only camera motion but no scene dynamics. Recent works have addressed these challenges by fine-tuning video diffusion models for camera-controlled or multi-view generation \cite{guo2023animatediff, wang2023motionctrl, kwak2023vivid, voleti2024sv3d}. Despite these advances, these approaches mainly focus on 3D object generation and do not work well for 3D scenes, few-view 3D reconstruction, or objects in context.

% Another line of research involves learning feed-forward models that take a few views as input and output 3D representations directly without per-instance optimization \cite{hong2023lrm, gupta2023triplane, szymanowicz2023viewset, xu2023dmv3d, li2023instant3d}. 
% Our work is inspired by this approach, although we mix our approach with diffusion processes to allow for context to be extended to full scenes rather than object-focused scenes. 



% % \subsection{Diffusion Models in 3D implicit representation}

% \noindent\textbf{Single-image novel view synthesis.}
% Advancements in neural rendering~\cite{mildenhall2021nerf} have revolutionized 3D reconstruction and novel view synthesis from collections of posed images. 
% In order to reduce the quantity need of posed images, 
% multiple methods focused on only using a single view 
% to generate new views.
% The first wave of methods addressing this problem focused on training 
% convolutional neural networks based approaches to generate cost volumes 
% or even directly to images~\cite{yu2021pixelnerf,jang2021codenerf,jang2023nvist}. 
% More recently methods have focused on different approaches for generating 
% novel views, 
% such as leveraging triplanes with diffusion~\cite{single-stage-diffusion}, 
% triplanes with gan models~\cite{eg3dChan2021},
% transformers~\cite{hong2023lrm}, 
% \textit{etc.}
% % 
% These works all share in common unnatural scenes, whereas the camera are well ordered around a single object and 
% there is no background. 

% \noindent\textbf{Diffusion and 3d implicit representation}

% Most of the approaches cannot do unconditioning generation / no editing / or require the aligned dataset..

% 3D Diffusion : aligned dataset
% Viewset Diffusion~\cite{szymanowicz2023viewset}
% Renderdiffusion ~\cite{anciukevivcius2023renderdiffusion}
% Holodiffusion

% 3D Diffusion : render features + apply diffusion
% Diffusion Forward Model~\cite{tewari2023forwarddiffusion}
% GeNVS
% NerfDiff

% 3D GAN + Latent Diffusion : need to train 2-stages
% Wildfusion
% NeuralLDM

% Use 2D Diffusion as generator for 3D
% G3DR~\cite{reddy2024g3dr}
% ViewDiff~\cite{hoellein2024viewdiff}
% LGM

% 3D Model + 2D Diffusion (SDS...) 
% Reconfusion
% ..

% 3D Diffusion - conditioning on Camera
% 3DiM
% Sparsefusion

% 3D-aware from Stable Diffusion
% Zero123
% One-2345
% ZeroNVS?
% ...


% Maybe add 3D-aware GAN?
% EG3D
% GRAF
% ...


% \subsection{Diffusion}

% % DDPM \cite{ho2020denoising}
% % DDIM \cite{song2020denoising}
% % Diffusion Model Beat GAN
% % Classifier-Free Guidance
% % Efficient Diffusion Training via Min-SNR Strategy 
% \cite{ho2020denoising} proposes DDPM.
% \cite{song2020denoising} proposes DDIM.
% Cosine schedule from \cite{dhariwal2021diffusion}.
% v-parameterization from \cite{salimans2022progressive}.
% Classifier-free guidance from \cite{ho2022classifier}.
% Simple Diffusion ~\cite{hoogeboom2023simple}
% Min-SNR strategy from \cite{hang2023efficient}.
% Diffusion + Transformer from \cite{peebles2023scalable}.


% \begin{figure}
%     \centering
%     \includegraphics[width=\textwidth]{figures/fig-methodology.png}
%     \caption{\textbf{Overall: }\OURS{} takes the noisy input image $z_t^i$ with the camera pose $I_3, (0,0,r_d)$ and clean conditioning image $x^r$ with $c^c=(R_{ic},T_{ic})$ as input. The model processes $z_t^i$ and $x^r$ independently. Then for the decoder, \OURS{} concatenates the feature tokens from both $z_t^i$ and $x_c$ with output tokens $O$ conditioning on $R_{ic}, T_{ic}, r_d, f$ via AdaLN. After the decoder blocks, \OURS{} reshapes output tokens and represent them as VM representation. Then, we query features from input view and novel view and render them. We randomly shuffle $z_t^i, x^r$ with $x^i,z_t^c$ and conditinoally dropout $x^r$.}
%     \label{fig:overall}
% \end{figure}

% \subsection{Neural Implicit Representation}

% NeRF from \cite{mildenhall2021nerf}


% \subsection{Learning multiple scenes : Deterministic models}

% NeRF-based models
% PixelNeRF \cite{yu2021pixelnerf}
% CodeNeRF \cite{jang2021codenerf}
% IBRNet
% ...

% Transformer + NeRF
% NViST \cite{jang2023nvist}
% LRM \cite{hong2023lrm}","\textbf{Diffusion Models:} A diffusion model is a generative approach similar to GANs, first proposed by Sohl-Dickstein \textit{et al.}~\cite{sohl-dickstein2015deep}, and gained popularity with DDPM~\cite{ho2020denoising}. 
Various techniques have been proposed to enhance the quality of diffusion model outcomes, including Cosine Schedule~\cite{dhariwal2021diffusion}, v-parameterization~\cite{salimans2022progressive}, and classifier-free guidance~\cite{ho2022classifier}. 
DDIM~\cite{song2020denoising} offers a more flexible and efficient sampling process, enabling faster generation of high-quality images with fewer steps.
Hang \textit{et al.}~\cite{hang2023efficient} proposed a minimum signal-to-noise ratio (Min-SNR) strategy, showing that the model converges faster by using SNR as a weight to the loss function.
DiT~\cite{peebles2023scalable} scales the Diffusion model with Transformers by conditioning diffusion steps with Adaptive Layer Normalization(AdaLN)-Zero~\cite{xu2019understanding}.

\textbf{Neural Radiance Fields: } Neural 3D implicit representations were initially proposed in  SRN~\cite{sitzmann2019scene} and later used in DVR~\cite{niemeyer2020differentiable}, to train 3D-aware representations without 3D ground-truth.
NeRF~\cite{mildenhall2021nerf} revolutionized novel view synthesis from collections of posed images.
To accelerate NeRF training, grid-based representations have been proposed. ~\cite{yu2021plenoxels, muller2022instant,fridovich2023k, tilted2023} EG3D applied triplanes by projecting features into three planes in the context of 3D-aware GANs and TensoRF~\cite{chen2022tensorf} proposed the vector-matrix representation.
Efforts to generalize NeRF include conditioning on global latent vectors~\cite{Gafni_2021_CVPR, jang2021codenerf, muller2022autorf,rebain2022lolnerf,athar2022rignerf,hong2022headnerf}, associating 2D feature views with target views~\cite{yu2021pixelnerf, wang2021ibrnet, chen2021mvsnerf,reizenstein2021common, Henzler_2021_CVPR, trevithick2021grf,chen2023matchnerf, irshad2023neo360}, or supervising NeRF with GAN losses~\cite{chanmonteiro2020pi-GAN, eg3dChan2021, schwarz2020graf,le2022stylemorph,cai2022pix2nerf}. 





\textbf{3D-Aware Diffusion Models: } Extending  diffusion models to be 3D-aware has proved challenging due to the lack of large 3D ground truth datasets.
Previous approaches fine-tuned a pre-trained latent diffusion model~\cite{rombach2022high} on camera parameters~\cite{liu2023zero, melas2023realfusion, shi2023mvdream, tang2023dreamgaussian, wang2023prolificdreamer, zeronvs, watson2022novel}
Other approaches~\cite{poole2022dreamfusion, tang2023dreamgaussian, wu2023reconfusion, wang2023morpheus} regularize NeRF using the pre-trained diffusion models or utilize the pre-trained diffusion models to generate images.




Similar to latent diffusion, several approaches first train the 3D-aware GAN and apply the diffusion process on latent vectors~\cite{Schwarz2024ICLR, kim2023nfldm}.
Instead of 2-stage training, there are approaches other methods obtain intermediate features and use them to denoise from the target view ~\cite{chan2023genvs, gu2023nerfdiff, tewari2023forwarddiffusion}.
Another line of research involves learning diffusion models in 3D by rendering in 2D or backprojecting 2D features into 3D~\cite{karnewar2023holodiffusion, anciukevicius2024denoising, anciukevivcius2023renderdiffusion, szymanowicz2023viewset}, which usually assume the aligned scenes.

\textbf{Transformers for 3D Tasks: } Vision Transformer (ViT)~\cite{dosovitskiy2020image} has been successful in the field of computer vision, and 
Masked Autoencoder(MAE)~\cite{he2022masked} learns visual features in self-supervised learning. 
For 3D, geometry-free methods based on transformer have been explored in ~\cite{kulhanek2022viewformer, sajjadi2022scene, Miyato2024GTA}.
To build 3D implicit representation from Transformer, GINA3D~\cite{shen2023gina} and VQ3D~\cite{sargent2023vq3d} employ the adversarial loss.
Other approaches deal with point clouds~\cite{jun2023shap, wu2023multiview}.
LRM~\cite{hong2023lrm} and NViST~\cite{jang2023nvist} apply the Transformer for 3D implicit representation in a deterministic way, and DMV~\cite{xu2023dmv3d} extends LRM into a diffusion, but both LRM and DMV do not model the background.
\OURS{} is the first method to apply the Transformer with a 3D-aware diffusion model, directly learning from real-world scenes.","Diffusion Models:A diffusion model is a generative approach similar to GANs, first proposed
by Sohl-Dicksteinet al.[ 60], and gained popularity with DDPM [ 22]. Various techniques have
been proposed to enhance the quality of diffusion model outcomes, including Cosine Schedule [ 12],
v-parameterization [ 50], and classifier-free guidance [ 23]. DDIM [ 61] offers a more flexible and
efficient sampling process, enabling faster generation of high-quality images with fewer steps. Hang
et al.[ 17] proposed a minimum signal-to-noise ratio (Min-SNR) strategy, showing that the model
converges faster by using SNR as a weight to the loss function. DiT [ 44] scales the Diffusion model
with Transformers by conditioning diffusion steps with Adaptive Layer Normalization(AdaLN)-
Zero [73].
Neural Radiance Fields:Neural 3D implicit representations were initially proposed in SRN [ 59]
and later used in DVR [ 43], to train 3D-aware representations without 3D ground-truth. NeRF [ 39]
revolutionized novel view synthesis from collections of posed images. To accelerate NeRF training,
grid-based representations have been proposed. [ 76,42,14,75] EG3D applied triplanes by projecting
features into three planes in the context of 3D-aware GANs and TensoRF [ 9] proposed the vector-
matrix representation. Efforts to generalize NeRF include conditioning on global latent vectors [ 15,
2
27,41,46,3,24], associating 2D feature views with target views [ 77,67,10,47,19,65,11,26], or
supervising NeRF with GAN losses [6, 7, 55, 34, 5].
3D-Aware Diffusion Models:Extending diffusion models to be 3D-aware has proved challenging
due to the lack of large 3D ground truth datasets. Previous approaches fine-tuned a pre-trained latent
diffusion model [ 48] on camera parameters [ 36,38,58,63,68,52,70] Other approaches [ 45,63,
72,66] regularize NeRF using the pre-trained diffusion models or utilize the pre-trained diffusion
models to generate images. Similar to latent diffusion, several approaches first train the 3D-aware
GAN and apply the diffusion process on latent vectors [ 56,32]. Instead of 2-stage training, there are
approaches other methods obtain intermediate features and use them to denoise from the target view
[8,16,64]. Another line of research involves learning diffusion models in 3D by rendering in 2D or
backprojecting 2D features into 3D [30, 1, 2, 62], which usually assume the aligned scenes.
Transformers for 3D Tasks:Vision Transformer (ViT) [ 13] has been successful in the field of
computer vision, and Masked Autoencoder(MAE) [ 18] learns visual features in self-supervised
learning. For 3D, geometry-free methods based on transformer have been explored in [ 33,49,40].
To build 3D implicit representation from Transformer, GINA3D [ 57] and VQ3D [ 51] employ the
adversarial loss. Other approaches deal with point clouds [ 29,71]. LRM [ 25] and NViST [ 28] apply
the Transformer for 3D implicit representation in a deterministic way, and DMV [ 74] extends LRM
into a diffusion, but both LRM and DMV do not model the background. DT-NVS is the first method
to apply the Transformer with a 3D-aware diffusion model, directly learning from real-world scenes."
2511.10212v1,http://arxiv.org/abs/2511.10212v1,2025-11-13 11:34:03+00:00,Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,"Recent multimodal deepfake detection methods designed for generalization conjecture that single-stage supervised training struggles to generalize across unseen manipulations and datasets. However, such approaches that target generalization require pretraining over real samples. Additionally, these methods primarily focus on detecting audio-visual inconsistencies and may overlook intra-modal artifacts causing them to fail against manipulations that preserve audio-visual alignment. To address these limitations, we propose a single-stage training framework that enhances generalization by incorporating next-frame prediction for both uni-modal and cross-modal features. Additionally, we introduce a window-level attention mechanism to capture discrepancies between predicted and actual frames, enabling the model to detect local artifacts around every frame, which is crucial for accurately classifying fully manipulated videos and effectively localizing deepfake segments in partially spoofed samples. Our model, evaluated on multiple benchmark datasets, demonstrates strong generalization and precise temporal localization.","\label{sec:related_works}

\subsection{Deepfake Detection}
\textbf{Visual-only Approaches:} Most of the visual-only proposals focused on detecting manipulations around the face region. While some methods focus on spatiotemporal~\cite{pang2023mre, wang2023altfreezing, zhang2024learning, yu2023augmented} and temporal~\cite{choi2024exploiting, zheng2021exploring, ge2022deepfake} inconsistencies, others look for defects in face blending~\cite{li2020face}, and texture~\cite{liu2020global, zhao2021multi}. Some of the approaches try to spot irregularities from the frequency domain~\cite{10286083, gu2022exploiting, jeong2022frepgan, liu2024hierarchical, 10107603}. Some visual approaches detect inconsistencies in noise patterns~\cite{chen2024compressed, wang2023noise, qiao2024deepfake} or semantics~\cite{xu2023tall, xu2024towards}, while others proposed identity based approaches for deepfake detection~\cite{cozzolino2021id, huang2023implicit}.  XceptionNet, a convolution-based network, is proposed by~\cite{rossler2019faceforensics++} as a benchmark for their FaceForensics++ dataset. However, it does not explore the generalization capability of the model. Recently, researchers have been focusing on generalizability in video deepfake detection~\cite{yan2024transcending, tan2024rethinking, 11045799, lin2024preserving, 10516609, nie2024dip, 11098842, 10654318}. LipForensics~\cite{haliassos2021lips} introduces a generalizable deepfake detection method that identifies irregularities in lip movement. FTCN ~\cite{zheng2021exploring}, proposed by Zheng et al., is a two-stage end-to-end network combining a convolutional network with a transformer to analyze facial temporal coherence. RealForensics~\cite{haliassos2022leveraging} employs audio-visual pretraining, but its classification stage discards audio and relies solely on visual deepfake detection.~\cite{wang2022deepfake} improves generalization by training on adversarially crafted samples designed to attack classification models through methods that blur high-frequency artifacts in facial manipulations.~\cite{prashnani2025generalizable} enhances generalization by leveraging temporal variations in phase information within the frequency domain of facial regions. Although the visual-only approaches are quite effective against facial manipulations, their applicability and effectiveness against deepfakes that involve multimodal manipulations are limited.

\textbf{Audio-Visual Approaches:} End-to-end supervised training on deepfake labels has been explored in various studies. For example,~\cite{chugh2020not} employs contrastive loss to model dissimilarities between audio and visual modalities for deepfake detection. Similarly, MCL~\cite{liu2023mcl} utilizes contrastive loss between the cross-modal features and unimodal audio, frame, and video features to reduce cross-modal gaps and explore cues for multimodal deepfake detection. AVoiD-DF~\cite{yang2023avoid} introduces a Temporal-Spatial Encoder to extract audio-visual embeddings and a Multi-Modal Joint Decoder to jointly learn audio-visual inconsistency paired with contrastive loss for multimodal deepfake detection. MRDF~\cite{zou2024cross} introduces modality-specific and cross-modality regularization within AV-Hubert~\cite{shi2022learning} to mitigate inconsistencies in multimodal representations caused by modality-specific manipulations. The model utilizes the modality-regularization term to help the features learn unimodal cues. However, they do not explicitly leverage the unimodal embeddings during inference. AV-DFD~\cite{zhou2021joint} proposes a two-plus-one stream model that utilizes cross-attention along the temporal dimension to exploit intrinsic audio-visual synchronization. However, single-stage supervised training may not fully leverage audio-visual alignment, reducing its effectiveness against unseen deepfakes. To address this, recent studies~\cite{yu2023pvass, cheng2023voice} highlight the benefits of multimodal self-supervised pretraining.~\cite{cheng2023voice} pre-trains their VFD model using contrastive loss to exploit voice-face matching before fine-tuning it for deepfake classification. AVAD~\cite{feng2023self} leverages a synchronization model from~\cite{chen2021audio} to pretrain on audio-visual temporal synchronization, using the inferred features for fully unsupervised multimodal deepfake detection. Oorloff et al. propose AVFF~\cite{oorloff2024avff}, which pre-trains a Masked Autoencoder (MAE) framework inspired by CAV-MAE~\cite{gong2022contrastive} for joint audio-visual representation learning, later fine-tuned for multimodal deepfake classification. While AVAD~\cite{feng2023self} and AVFF~\cite{oorloff2024avff} achieve effective generalization against unseen manipulation or data distributions, they are two-stage approaches that require pretraining. Additionally, they primarily focus on cross-modal correspondence, which may cause them to overlook uni-modal inconsistencies. As a result, their effectiveness could be limited when detecting deepfakes that maintain audio-visual alignment. Moreover, most of these approaches do not explore the adaptability to Deepfake Temporal Localization.

\subsection{Temporal Deepfake Localization}
Temporal deepfake localization is a specialized form of temporal action detection, where the only ""action"" is identifying deepfake segments. One approach, boundary prediction, directly estimates temporal segments using boundary confidence~\cite{lin2018bsn, lin2019bmn, su2021bsn++}. BMN~\cite{lin2019bmn} and BSN++~\cite{su2021bsn++} generate a two-dimensional boundary-matching confidence map to represent each candidate proposal's start, duration, and confidence score. ActionFormer~\cite{zhang2022actionformer} enhances boundary regression by capturing hierarchical temporal features at multiple resolutions. Tridet~\cite{shi2023tridet} further improves ActionFormer by replacing its standard transformer block with the proposed SGP block.

For audio-visual deepfake temporal localization, BA-TFD~\cite{cai2022you} applies BMN~\cite{lin2019bmn} to predict boundary-matching confidence maps for each modality separately before fusing them for final proposal generation. BA-TFD+\cite{cai2023glitch} adopts the same architecture but replaces BMN’s boundary map prediction module with BSN++\cite{su2021bsn++}. The first multimodal deepfake temporal localization dataset, LAV-DF, was introduced in~\cite{chugh2020not, cai2023glitch}, followed by AV-Deepfake1M~\cite{cai2023av}. UMMAFormer~\cite{zhang2023ummaformer}, inspired by ActionFormer’s hierarchical processing, introduces a Parallel Cross-Attention Feature Pyramid Network (PCA-FPN) for deepfake temporal localization. They evaluate their model on the LAV-DF dataset and their proposed Temporal Video Inpainting Localization (TVIL) dataset.","\subsection{Deepfake Detection}
\textbf{Visual-only Approaches:} Most of the visual-only proposals focused on detecting manipulations around the face region. While some methods focus on spatiotemporal~\cite{pang2023mre, wang2023altfreezing, zhang2024learning, yu2023augmented} and temporal~\cite{choi2024exploiting, zheng2021exploring, ge2022deepfake} inconsistencies, others look for defects in face blending~\cite{li2020face}, and texture~\cite{liu2020global, zhao2021multi}. Some of the approaches try to spot irregularities from the frequency domain~\cite{10286083, gu2022exploiting, jeong2022frepgan, liu2024hierarchical, 10107603}. Some visual approaches detect inconsistencies in noise patterns~\cite{chen2024compressed, wang2023noise, qiao2024deepfake} or semantics~\cite{xu2023tall, xu2024towards}, while others proposed identity based approaches for deepfake detection~\cite{cozzolino2021id, huang2023implicit}.  XceptionNet, a convolution-based network, is proposed by~\cite{rossler2019faceforensics++} as a benchmark for their FaceForensics++ dataset. However, it does not explore the generalization capability of the model. Recently, researchers have been focusing on generalizability in video deepfake detection~\cite{yan2024transcending, tan2024rethinking, 11045799, lin2024preserving, 10516609, nie2024dip, 11098842, 10654318}. LipForensics~\cite{haliassos2021lips} introduces a generalizable deepfake detection method that identifies irregularities in lip movement. FTCN ~\cite{zheng2021exploring}, proposed by Zheng et al., is a two-stage end-to-end network combining a convolutional network with a transformer to analyze facial temporal coherence. RealForensics~\cite{haliassos2022leveraging} employs audio-visual pretraining, but its classification stage discards audio and relies solely on visual deepfake detection.~\cite{wang2022deepfake} improves generalization by training on adversarially crafted samples designed to attack classification models through methods that blur high-frequency artifacts in facial manipulations.~\cite{prashnani2025generalizable} enhances generalization by leveraging temporal variations in phase information within the frequency domain of facial regions. Although the visual-only approaches are quite effective against facial manipulations, their applicability and effectiveness against deepfakes that involve multimodal manipulations are limited.

\textbf{Audio-Visual Approaches:} End-to-end supervised training on deepfake labels has been explored in various studies. For example,~\cite{chugh2020not} employs contrastive loss to model dissimilarities between audio and visual modalities for deepfake detection. Similarly, MCL~\cite{liu2023mcl} utilizes contrastive loss between the cross-modal features and unimodal audio, frame, and video features to reduce cross-modal gaps and explore cues for multimodal deepfake detection. AVoiD-DF~\cite{yang2023avoid} introduces a Temporal-Spatial Encoder to extract audio-visual embeddings and a Multi-Modal Joint Decoder to jointly learn audio-visual inconsistency paired with contrastive loss for multimodal deepfake detection. MRDF~\cite{zou2024cross} introduces modality-specific and cross-modality regularization within AV-Hubert~\cite{shi2022learning} to mitigate inconsistencies in multimodal representations caused by modality-specific manipulations. The model utilizes the modality-regularization term to help the features learn unimodal cues. However, they do not explicitly leverage the unimodal embeddings during inference. AV-DFD~\cite{zhou2021joint} proposes a two-plus-one stream model that utilizes cross-attention along the temporal dimension to exploit intrinsic audio-visual synchronization. However, single-stage supervised training may not fully leverage audio-visual alignment, reducing its effectiveness against unseen deepfakes. To address this, recent studies~\cite{yu2023pvass, cheng2023voice} highlight the benefits of multimodal self-supervised pretraining.~\cite{cheng2023voice} pre-trains their VFD model using contrastive loss to exploit voice-face matching before fine-tuning it for deepfake classification. AVAD~\cite{feng2023self} leverages a synchronization model from~\cite{chen2021audio} to pretrain on audio-visual temporal synchronization, using the inferred features for fully unsupervised multimodal deepfake detection. Oorloff et al. propose AVFF~\cite{oorloff2024avff}, which pre-trains a Masked Autoencoder (MAE) framework inspired by CAV-MAE~\cite{gong2022contrastive} for joint audio-visual representation learning, later fine-tuned for multimodal deepfake classification. While AVAD~\cite{feng2023self} and AVFF~\cite{oorloff2024avff} achieve effective generalization against unseen manipulation or data distributions, they are two-stage approaches that require pretraining. Additionally, they primarily focus on cross-modal correspondence, which may cause them to overlook uni-modal inconsistencies. As a result, their effectiveness could be limited when detecting deepfakes that maintain audio-visual alignment. Moreover, most of these approaches do not explore the adaptability to Deepfake Temporal Localization.

\subsection{Temporal Deepfake Localization}
Temporal deepfake localization is a specialized form of temporal action detection, where the only ""action"" is identifying deepfake segments. One approach, boundary prediction, directly estimates temporal segments using boundary confidence~\cite{lin2018bsn, lin2019bmn, su2021bsn++}. BMN~\cite{lin2019bmn} and BSN++~\cite{su2021bsn++} generate a two-dimensional boundary-matching confidence map to represent each candidate proposal's start, duration, and confidence score. ActionFormer~\cite{zhang2022actionformer} enhances boundary regression by capturing hierarchical temporal features at multiple resolutions. Tridet~\cite{shi2023tridet} further improves ActionFormer by replacing its standard transformer block with the proposed SGP block.

For audio-visual deepfake temporal localization, BA-TFD~\cite{cai2022you} applies BMN~\cite{lin2019bmn} to predict boundary-matching confidence maps for each modality separately before fusing them for final proposal generation. BA-TFD+\cite{cai2023glitch} adopts the same architecture but replaces BMN’s boundary map prediction module with BSN++\cite{su2021bsn++}. The first multimodal deepfake temporal localization dataset, LAV-DF, was introduced in~\cite{chugh2020not, cai2023glitch}, followed by AV-Deepfake1M~\cite{cai2023av}. UMMAFormer~\cite{zhang2023ummaformer}, inspired by ActionFormer’s hierarchical processing, introduces a Parallel Cross-Attention Feature Pyramid Network (PCA-FPN) for deepfake temporal localization. They evaluate their model on the LAV-DF dataset and their proposed Temporal Video Inpainting Localization (TVIL) dataset.",
2511.08462v2,http://arxiv.org/abs/2511.08462v2,2025-11-11 17:06:04+00:00,QLCoder: A Query Synthesizer For Static Analysis of Security Vulnerabilities,"Static analysis tools provide a powerful means to detect security vulnerabilities by specifying queries that encode vulnerable code patterns. However, writing such queries is challenging and requires diverse expertise in security and program analysis. To address this challenge, we present QLCoder - an agentic framework that automatically synthesizes queries in CodeQL, a powerful static analysis engine, directly from a given CVE metadata. QLCode embeds an LLM in a synthesis loop with execution feedback, while constraining its reasoning using a custom MCP interface that allows structured interaction with a Language Server Protocol (for syntax guidance) and a RAG database (for semantic retrieval of queries and documentation). This approach allows QLCoder to generate syntactically and semantically valid security queries. We evaluate QLCode on 176 existing CVEs across 111 Java projects. Building upon the Claude Code agent framework, QLCoder synthesizes correct queries that detect the CVE in the vulnerable but not in the patched versions for 53.4% of CVEs. In comparison, using only Claude Code synthesizes 10% correct queries.","\mypara{LLMs and vulnerability detection} LLMs have been used extensively for vulnerability detection and repair using techniques such as fine-tuning and prompt engineering \citep{zhou2024largelanguagemodelvulnerability}. LLMs have also been combined with existing program analysis tools for vulnerability detection. The combination of LLMs can be used from vulnerability analysis like IRIS's \citep{li2025iris} source and sink identification, however IRIS depends on a limited set of CWE templates derived from CodeQL's CWE queries. IRIS also only the LLM for identifying sources and sinks. KNighter synthesizes CSA checkers given a fix commit of a C repository \citep{yang2025knighter}, however the checkers are written in C which has more available training data. MocQ's uses an LLM to derive a subset DSL of CodeQL and Joern, and then provides a feedback loop to the LLM though prompting via API calls is used rather than an agent with tools and MocQ uses significantly higher iterations, with a max threshold of 1,000 iterations per vulnerability experiment. \citep{li2025automatedstaticvulnerabilitydetection}.

\mypara{LLM agents and tool usage} 
SWE-agent pioneered the idea of autonomous LLM agents using tools for software engineering tasks \cite{yang2024sweagentagentcomputerinterfacesenable}. LSPAI \cite{go_lspai_2025}, an IDE plugin, uses LSP servers to guide LLM-generated unit tests. Hazel, a live program sketching environment, uses a language server \citep{blinn_statically_2024} to assist code completions synthesized by LLMs. The Hazel Language Server provides the typing context of a program hole to be filled. 

\mypara{Low resource LLM code generation} SPEAC uses ASTs combined with constraint solving to repair LLM-generated code for low resource programming languages \citep{mora2024synthetic}. SPEAC converts a buggy program into an AST and uses a solver to find the minimum set of AST nodes to replace, to satisfy language constraints. MultiPL-T generates datasets for low resource languages by translating high resource language code to the target language and validates translations with LLM generated unit tests \citep{cassano2024knowledgetransferhighresourcelowresource}.
% \mypara{Neural program synthesis} 
%SmartLabel is an active learning approach for neurosymbolic program synthesis by using conformal prediction to handle neural network misprediction~\citep{barnaby_active_nodate}.","\mypara{LLMs and vulnerability detection} LLMs have been used extensively for vulnerability detection and repair using techniques such as fine-tuning and prompt engineering \citep{zhou2024largelanguagemodelvulnerability}. LLMs have also been combined with existing program analysis tools for vulnerability detection. The combination of LLMs can be used from vulnerability analysis like IRIS's \citep{li2025iris} source and sink identification, however IRIS depends on a limited set of CWE templates derived from CodeQL's CWE queries. IRIS also only the LLM for identifying sources and sinks. KNighter synthesizes CSA checkers given a fix commit of a C repository \citep{yang2025knighter}, however the checkers are written in C which has more available training data. MocQ's uses an LLM to derive a subset DSL of CodeQL and Joern, and then provides a feedback loop to the LLM though prompting via API calls is used rather than an agent with tools and MocQ uses significantly higher iterations, with a max threshold of 1,000 iterations per vulnerability experiment. \citep{li2025automatedstaticvulnerabilitydetection}.

\mypara{LLM agents and tool usage} 
SWE-agent pioneered the idea of autonomous LLM agents using tools for software engineering tasks \cite{yang2024sweagentagentcomputerinterfacesenable}. LSPAI \cite{go_lspai_2025}, an IDE plugin, uses LSP servers to guide LLM-generated unit tests. Hazel, a live program sketching environment, uses a language server \citep{blinn_statically_2024} to assist code completions synthesized by LLMs. The Hazel Language Server provides the typing context of a program hole to be filled. 

\mypara{Low resource LLM code generation} SPEAC uses ASTs combined with constraint solving to repair LLM-generated code for low resource programming languages \citep{mora2024synthetic}. SPEAC converts a buggy program into an AST and uses a solver to find the minimum set of AST nodes to replace, to satisfy language constraints. MultiPL-T generates datasets for low resource languages by translating high resource language code to the target language and validates translations with LLM generated unit tests \citep{cassano2024knowledgetransferhighresourcelowresource}.",
2511.08402v1,http://arxiv.org/abs/2511.08402v1,2025-11-11 16:18:01+00:00,Anatomy-VLM: A Fine-grained Vision-Language Model for Medical Interpretation,"Accurate disease interpretation from radiology remains challenging due to imaging heterogeneity. Achieving expert-level diagnostic decisions requires integration of subtle image features with clinical knowledge. Yet major vision-language models (VLMs) treat images as holistic entities and overlook fine-grained image details that are vital for disease diagnosis. Clinicians analyze images by utilizing their prior medical knowledge and identify anatomical structures as important region of interests (ROIs). Inspired from this human-centric workflow, we introduce Anatomy-VLM, a fine-grained, vision-language model that incorporates multi-scale information. First, we design a model encoder to localize key anatomical features from entire medical images. Second, these regions are enriched with structured knowledge for contextually-aware interpretation. Finally, the model encoder aligns multi-scale medical information to generate clinically-interpretable disease prediction. Anatomy-VLM achieves outstanding performance on both in- and out-of-distribution datasets. We also validate the performance of Anatomy-VLM on downstream image segmentation tasks, suggesting that its fine-grained alignment captures anatomical and pathology-related knowledge. Furthermore, the Anatomy-VLM's encoder facilitates zero-shot anatomy-wise interpretation, providing its strong expert-level clinical interpretation capabilities.","\label{sec:related_works}

% All text must be in a two-column format.
% The total allowable size of the text area is $6\frac78$ inches (17.46 cm) wide by $8\frac78$ inches (22.54 cm) high.
% Columns are to be $3\frac14$ inches (8.25 cm) wide, with a $\frac{5}{16}$ inch (0.8 cm) space between them.
% The main title (on the first page) should begin 1 inch (2.54 cm) from the top edge of the page.
% The second and following pages should begin 1 inch (2.54 cm) from the top edge.
% On all pages, the bottom margin should be $1\frac{1}{8}$ inches (2.86 cm) from the bottom edge of the page for $8.5 \times 11$-inch paper;
% for A4 paper, approximately $1\frac{5}{8}$ inches (4.13 cm) from the bottom edge of the
% page.

%-------------------------------------------------------------------------
\subsection{Vision-Language Pre-training}

Vision-Language Models (VLMs) \cite{radford2021learning,li2023blip,jia2021scaling,chen2025cost,gao2025show,chen2024survey} have emerged as a powerful paradigm for aligning visual and textual representation through multimodal pre-training. While general-purpose VLMs such as CLIP \cite{radford2021learning}, BLIP \cite{li2023blip}, and LLaVA \cite{liu2023visual} demonstrate impressive performance on natural images, they often lack the specialized medical knowledge for disease understanding and clinical interpretation. To address this hurdle, domain-specific model adaptations become increasingly useful. For instance, BioViL \cite{boecking2022making} leverages radiology image-text pairs for vision-language alignment. MedCLIP \cite{wang2022medclip} focuses on medical image-text contrastive learning with clinical report supervision. MedKLIP \cite{wu2023medklip} incorporates a medical knowledge-enhanced triplet to improve feature representation learning. Although these VLMs are good at learning multimodal representation, they are based mainly on the well-designed image-text matching during pre-training. These approaches predominantly treat medical images holistically in the data alignment process, lacking explicit modeling of the detectable anatomical structures. In clinics, radiologists must rely on the fine-grained anatomical localization and knowledge integration \cite{datta2020understanding,waite2019analysis} to inform decision making. Inspired by this human-expert workflow, Anatomy-VLM addresses the fundamental gap between image feature extraction and human workflows by incorporating anatomical interpretation with structured knowledge integration. Unlike prior end-to-end classification \cite{dosovitskiy2020image, boecking2022making}, Anatomy-VLM closely mimics the multi-step reasoning process that radiologists employ in clinical practice. By modeling the regional visual-and-text relationship, Anatomy-VLM achieves strong predictions that address the challenge of long‑tail class imbalance and generalize well to rare conditions.


%-------------------------------------------------------------------------
\subsection{Fine-grained VLM}
Fine-grained alignment in medical vision-language models has gained attention to address the limitation of global image-text matching \cite{lu2025integrating}. For instance, fine-grained vision-language models (fVLM) \cite{shui2025large} matches anatomical regions of 3D CT images with corresponding descriptions. MedKLIP \cite{wu2023medklip} leverages an entity-level alignment with corresponding image regions. CARZero \cite{lai2024carzero} introduces an alignment that converts cross‑attention features into similarity representations. RGRG \cite{tanida2023interactive} achieves fine-grained alignment by dividing images into a fixed grid of predefined regions for region-specific visual-textual matching. ASG \cite{li2024anatomical} leverages anatomical structure guidance for vision-language pre-training. However, these approaches primarily rely on implicit alignment strategies through contrastive learning, focusing on either image-text matching or region-text correspondence in isolation.
By contrast, Anatomy-VLM introduces a fundamentally different alignment paradigm that unifies object locations, visual features, and textual information through concept queries. This trimodal integration enables the model to simultaneously learn spatial-semantic correspondences, where object locations inform visual attention, visual features ground textual descriptions, and textual semantics guide spatial localization—creating a synergistic learning process that localizes clinically meaningful anatomical structures, providing precise region identification. Furthermore, Anatomy-VLM incorporates multi-scale information processing that mirrors the clinical diagnostic workflow, seamlessly integrating information from fine-grained localization of anatomical structures to global pathological classification. This hierarchical approach ensures that local anatomical findings inform broader diagnostic decisions, while global context guides the interpretation of localized features. As a result, Anatomy-VLM aims to deliver clinically-grounded outputs with transparent interpretability across multiple scales of medical reasoning.

%-------------------------------------------------------------------------","\subsection{Vision-Language Pre-training}

Vision-Language Models (VLMs) \cite{radford2021learning,li2023blip,jia2021scaling,chen2025cost,gao2025show,chen2024survey} have emerged as a powerful paradigm for aligning visual and textual representation through multimodal pre-training. While general-purpose VLMs such as CLIP \cite{radford2021learning}, BLIP \cite{li2023blip}, and LLaVA \cite{liu2023visual} demonstrate impressive performance on natural images, they often lack the specialized medical knowledge for disease understanding and clinical interpretation. To address this hurdle, domain-specific model adaptations become increasingly useful. For instance, BioViL \cite{boecking2022making} leverages radiology image-text pairs for vision-language alignment. MedCLIP \cite{wang2022medclip} focuses on medical image-text contrastive learning with clinical report supervision. MedKLIP \cite{wu2023medklip} incorporates a medical knowledge-enhanced triplet to improve feature representation learning. Although these VLMs are good at learning multimodal representation, they are based mainly on the well-designed image-text matching during pre-training. These approaches predominantly treat medical images holistically in the data alignment process, lacking explicit modeling of the detectable anatomical structures. In clinics, radiologists must rely on the fine-grained anatomical localization and knowledge integration \cite{datta2020understanding,waite2019analysis} to inform decision making. Inspired by this human-expert workflow, Anatomy-VLM addresses the fundamental gap between image feature extraction and human workflows by incorporating anatomical interpretation with structured knowledge integration. Unlike prior end-to-end classification \cite{dosovitskiy2020image, boecking2022making}, Anatomy-VLM closely mimics the multi-step reasoning process that radiologists employ in clinical practice. By modeling the regional visual-and-text relationship, Anatomy-VLM achieves strong predictions that address the challenge of long‑tail class imbalance and generalize well to rare conditions.



\subsection{Fine-grained VLM}
Fine-grained alignment in medical vision-language models has gained attention to address the limitation of global image-text matching \cite{lu2025integrating}. For instance, fine-grained vision-language models (fVLM) \cite{shui2025large} matches anatomical regions of 3D CT images with corresponding descriptions. MedKLIP \cite{wu2023medklip} leverages an entity-level alignment with corresponding image regions. CARZero \cite{lai2024carzero} introduces an alignment that converts cross‑attention features into similarity representations. RGRG \cite{tanida2023interactive} achieves fine-grained alignment by dividing images into a fixed grid of predefined regions for region-specific visual-textual matching. ASG \cite{li2024anatomical} leverages anatomical structure guidance for vision-language pre-training. However, these approaches primarily rely on implicit alignment strategies through contrastive learning, focusing on either image-text matching or region-text correspondence in isolation.
By contrast, Anatomy-VLM introduces a fundamentally different alignment paradigm that unifies object locations, visual features, and textual information through concept queries. This trimodal integration enables the model to simultaneously learn spatial-semantic correspondences, where object locations inform visual attention, visual features ground textual descriptions, and textual semantics guide spatial localization—creating a synergistic learning process that localizes clinically meaningful anatomical structures, providing precise region identification. Furthermore, Anatomy-VLM incorporates multi-scale information processing that mirrors the clinical diagnostic workflow, seamlessly integrating information from fine-grained localization of anatomical structures to global pathological classification. This hierarchical approach ensures that local anatomical findings inform broader diagnostic decisions, while global context guides the interpretation of localized features. As a result, Anatomy-VLM aims to deliver clinically-grounded outputs with transparent interpretability across multiple scales of medical reasoning.",
2511.09735v1,http://arxiv.org/abs/2511.09735v1,2025-11-12 20:49:58+00:00,Social LSTM with Dynamic Occupancy Modeling for Realistic Pedestrian Trajectory Prediction,"In dynamic and crowded environments, realistic pedestrian trajectory prediction remains a challenging task due to the complex nature of human motion and the mutual influences among individuals. Deep learning models have recently achieved promising results by implicitly learning such patterns from 2D trajectory data. However, most approaches treat pedestrians as point entities, ignoring the physical space that each person occupies. To address these limitations, this paper proposes a novel deep learning model that enhances the Social LSTM with a new Dynamic Occupied Space loss function. This loss function guides Social LSTM in learning to avoid realistic collisions without increasing displacement error across different crowd densities, ranging from low to high, in both homogeneous and heterogeneous density settings. Such a function achieves this by combining the average displacement error with a new collision penalty that is sensitive to scene density and individual spatial occupancy. For efficient training and evaluation, five datasets were generated from real pedestrian trajectories recorded during the Festival of Lights in Lyon 2022. Four datasets represent homogeneous crowd conditions -- low, medium, high, and very high density -- while the fifth corresponds to a heterogeneous density distribution. The experimental findings indicate that the proposed model not only lowers collision rates but also enhances displacement prediction accuracy in each dataset. Specifically, the model achieves up to a 31% reduction in the collision rate and reduces the average displacement error and the final displacement error by 5% and 6%, respectively, on average across all datasets compared to the baseline. Moreover, the proposed model consistently outperforms several state-of-the-art deep learning models across most test sets.","\label{sec:realtedwork}

Efficient prediction of pedestrian trajectory in crowded environments remains a challenging task, particularly due to the need to model nearby movements and ensure physically plausible, collision-free paths. Recent deep learning methods have significantly advanced the field by learning from large-scale trajectory datasets~\cite{wang2024pedestrian, yang2025pedestrian, chen2025trajectory}.

\subsection{Neighbor-based Models}

In this category, models leverage the movements of nearby pedestrians to account for the influence of surrounding trajectories.
Early models such as Social LSTM~\cite{kothari2021human} introduced social pooling to implicitly model interactions between each individual and nearby pedestrians. Based on this, SR-LSTM~\cite{zhang2019sr} and STGAT~\cite{huang2019stgat} incorporated attention mechanisms and spatio-temporal graph structures to capture richer interaction patterns. To broaden the spatial context, Xue et al.~\cite{xue2019location} proposed a joint Location–Velocity Attention LSTM model for pedestrian trajectory prediction, where an attention mechanism learns to combine location and velocity cues to improve forecasting accuracy and generalization across scenes.

More recent architectures, such as Social-BiGAT~\cite{kosaraju2019social} and AgentFormer~\cite{yuan2021agentformer}, employed attention-based frameworks—Social-BiGAT used a graph attention network for social interactions, while AgentFormer leveraged a transformer to jointly capture temporal and social dependencies. Advancements such as SocialFormer~\cite{li2024iaha} and Knowledge-Aware Graph Transformer~\cite{zhu2024propagation} extended attention-based frameworks by incorporating semantic interaction features and adaptive mechanisms to better capture complex relational dependencies.

\subsection{Generative-based Models}
Generative models have emerged to address the multimodal and uncertain nature of human behavior by generating multiple plausible future trajectories. Social GAN~\cite{gupta2018social} was among the first to apply a generative adversarial framework, combining LSTM-based encoders and decoders with a discriminator to produce diverse and socially compliant paths.  Trajectron++~\cite{salzmann2020trajectron} proposed a graph-structured recurrent model to forecast multi-agent trajectories by incorporating agent dynamics and environmental context.

More recent approaches, such as GSGFormer~\cite{luo2023gsgformer}, integrate graph attention with CVAE modules for semantically consistent generation, while TUTR~\cite{song2023tutr} simplifies the process by using a transformer-based framework to directly predict trajectory modes and their probabilities.

\subsection{Physical Realism and Collision Avoidance}
%Avoids unrealistic overlaps between pedestrians in predictions.


Despite these advances, most of the existing deep learning–based trajectory prediction approaches still model pedestrians as abstract points, ignoring their physical space requirements. This limits their ability to ensure realistic and collision-free predictions in dense environments~\cite{chatagnon2025exploring, song2019experiment}.

To address this limitation, recent research has incorporated representations of personal body space and penalized overlapping trajectories. For example, Time-to-Collision (TTC)-Social LSTM~\cite{korbmacher2024toward} uses a TTC-based loss function and represents each pedestrian as a circular disk with a fixed radius of \SI{0.2}{\meter}. This design helps the model learn realistic collision avoidance behavior while maintaining displacement accuracy, thereby improving realism and predictive accuracy in crowded scenes. However, TTC-Social LSTM still faces challenges in dense and heterogeneous crowds, where avoiding collisions can reduce the accuracy of the prediction. One contributing factor is the use of a single fixed body-space size for all pedestrians, regardless of scene density.

As a result, this article proposes a novel deep learning model that integrates the Social-LSTM architecture with a dynamic loss function of the occupied space, which adapts the space size of each person according to the density of the scene. This approach is designed to reduce collision occurrences and displacement errors in various conditions, including homogeneous low-, medium-, and high-density scenes, as well as heterogeneous density distributions.
The following section presents a detailed overview of the proposed model.","Efficient prediction of pedestrian trajectory in crowded environments remains a challenging task, particularly due to the need to model nearby movements and ensure physically plausible, collision-free paths. Recent deep learning methods have significantly advanced the field by learning from large-scale trajectory datasets~\cite{wang2024pedestrian, yang2025pedestrian, chen2025trajectory}.

\subsection{Neighbor-based Models}

In this category, models leverage the movements of nearby pedestrians to account for the influence of surrounding trajectories.
Early models such as Social LSTM~\cite{kothari2021human} introduced social pooling to implicitly model interactions between each individual and nearby pedestrians. Based on this, SR-LSTM~\cite{zhang2019sr} and STGAT~\cite{huang2019stgat} incorporated attention mechanisms and spatio-temporal graph structures to capture richer interaction patterns. To broaden the spatial context, Xue et al.~\cite{xue2019location} proposed a joint Location–Velocity Attention LSTM model for pedestrian trajectory prediction, where an attention mechanism learns to combine location and velocity cues to improve forecasting accuracy and generalization across scenes.

More recent architectures, such as Social-BiGAT~\cite{kosaraju2019social} and AgentFormer~\cite{yuan2021agentformer}, employed attention-based frameworks—Social-BiGAT used a graph attention network for social interactions, while AgentFormer leveraged a transformer to jointly capture temporal and social dependencies. Advancements such as SocialFormer~\cite{li2024iaha} and Knowledge-Aware Graph Transformer~\cite{zhu2024propagation} extended attention-based frameworks by incorporating semantic interaction features and adaptive mechanisms to better capture complex relational dependencies.

\subsection{Generative-based Models}
Generative models have emerged to address the multimodal and uncertain nature of human behavior by generating multiple plausible future trajectories. Social GAN~\cite{gupta2018social} was among the first to apply a generative adversarial framework, combining LSTM-based encoders and decoders with a discriminator to produce diverse and socially compliant paths.  Trajectron++~\cite{salzmann2020trajectron} proposed a graph-structured recurrent model to forecast multi-agent trajectories by incorporating agent dynamics and environmental context.

More recent approaches, such as GSGFormer~\cite{luo2023gsgformer}, integrate graph attention with CVAE modules for semantically consistent generation, while TUTR~\cite{song2023tutr} simplifies the process by using a transformer-based framework to directly predict trajectory modes and their probabilities.

\subsection{Physical Realism and Collision Avoidance}



Despite these advances, most of the existing deep learning–based trajectory prediction approaches still model pedestrians as abstract points, ignoring their physical space requirements. This limits their ability to ensure realistic and collision-free predictions in dense environments~\cite{chatagnon2025exploring, song2019experiment}.

To address this limitation, recent research has incorporated representations of personal body space and penalized overlapping trajectories. For example, Time-to-Collision (TTC)-Social LSTM~\cite{korbmacher2024toward} uses a TTC-based loss function and represents each pedestrian as a circular disk with a fixed radius of \SI{0.2}{\meter}. This design helps the model learn realistic collision avoidance behavior while maintaining displacement accuracy, thereby improving realism and predictive accuracy in crowded scenes. However, TTC-Social LSTM still faces challenges in dense and heterogeneous crowds, where avoiding collisions can reduce the accuracy of the prediction. One contributing factor is the use of a single fixed body-space size for all pedestrians, regardless of scene density.

As a result, this article proposes a novel deep learning model that integrates the Social-LSTM architecture with a dynamic loss function of the occupied space, which adapts the space size of each person according to the density of the scene. This approach is designed to reduce collision occurrences and displacement errors in various conditions, including homogeneous low-, medium-, and high-density scenes, as well as heterogeneous density distributions.
The following section presents a detailed overview of the proposed model.","Efficient prediction of pedestrian trajectory in crowded environments remains a challenging task, particularly due to the
need to model nearby movements and ensure physically plausible, collision-free paths. Recent deep learning methods
have significantly advanced the field by learning from large-scale trajectory datasets [26, 27, 28].
2.1 Neighbor-based Models
In this category, models leverage the movements of nearby pedestrians to account for the influence of surrounding
trajectories. Early models such as Social LSTM [ 29] introduced social pooling to implicitly model interactions
between each individual and nearby pedestrians. Based on this, SR-LSTM [ 30] and STGAT [ 31] incorporated attention
mechanisms and spatio-temporal graph structures to capture richer interaction patterns. To broaden the spatial context,
2
RPEE-HEADS
Xue et al. [ 32] proposed a joint Location–Velocity Attention LSTM model for pedestrian trajectory prediction, where an
attention mechanism learns to combine location and velocity cues to improve forecasting accuracy and generalization
across scenes.
More recent architectures, such as Social-BiGAT [ 33] and AgentFormer [ 34], employed attention-based
frameworks—Social-BiGAT used a graph attention network for social interactions, while AgentFormer leveraged
a transformer to jointly capture temporal and social dependencies. Advancements such as SocialFormer [ 35] and
Knowledge-Aware Graph Transformer [ 36] extended attention-based frameworks by incorporating semantic interaction
features and adaptive mechanisms to better capture complex relational dependencies.
2.2 Generative-based Models
Generative models have emerged to address the multimodal and uncertain nature of human behavior by generating
multiple plausible future trajectories. Social GAN [ 21] was among the first to apply a generative adversarial framework,
combining LSTM-based encoders and decoders with a discriminator to produce diverse and socially compliant paths.
Trajectron++ [ 37] proposed a graph-structured recurrent model to forecast multi-agent trajectories by incorporating
agent dynamics and environmental context.
More recent approaches, such as GSGFormer [ 38], integrate graph attention with CV AE modules for semantically
consistent generation, while TUTR [ 39] simplifies the process by using a transformer-based framework to directly
predict trajectory modes and their probabilities.
2.3 Physical Realism and Collision Avoidance
Despite these advances, most of the existing deep learning–based trajectory prediction approaches still model pedestrians
as abstract points, ignoring their physical space requirements. This limits their ability to ensure realistic and collision-
free predictions in dense environments [12, 40].
To address this limitation, recent research has incorporated representations of personal body space and penalized
overlapping trajectories. For example, Time-to-Collision (TTC)-Social LSTM [ 25] uses a TTC-based loss function and
represents each pedestrian as a circular disk with a fixed radius of 0.2 m . This design helps the model learn realistic
collision avoidance behavior while maintaining displacement accuracy, thereby improving realism and predictive
accuracy in crowded scenes. However, TTC-Social LSTM still faces challenges in dense and heterogeneous crowds,
where avoiding collisions can reduce the accuracy of the prediction. One contributing factor is the use of a single fixed
body-space size for all pedestrians, regardless of scene density.
As a result, this article proposes a novel deep learning model that integrates the Social-LSTM architecture with a
dynamic loss function of the occupied space, which adapts the space size of each person according to the density of
the scene. This approach is designed to reduce collision occurrences and displacement errors in various conditions,
including homogeneous low-, medium-, and high-density scenes, as well as heterogeneous density distributions. The
following section presents a detailed overview of the proposed model."
2511.08294v1,http://arxiv.org/abs/2511.08294v1,2025-11-11 14:28:43+00:00,SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,"Accurate 3D human pose estimation is fundamental for applications such as augmented reality and human-robot interaction. State-of-the-art multi-view methods learn to fuse predictions across views by training on large annotated datasets, leading to poor generalization when the test scenario differs. To overcome these limitations, we propose SkelSplat, a novel framework for multi-view 3D human pose estimation based on differentiable Gaussian rendering. Human pose is modeled as a skeleton of 3D Gaussians, one per joint, optimized via differentiable rendering to enable seamless fusion of arbitrary camera views without 3D ground-truth supervision. Since Gaussian Splatting was originally designed for dense scene reconstruction, we propose a novel one-hot encoding scheme that enables independent optimization of human joints. SkelSplat outperforms approaches that do not rely on 3D ground truth in Human3.6M and CMU, while reducing the cross-dataset error up to 47.8% compared to learning-based methods. Experiments on Human3.6M-Occ and Occlusion-Person demonstrate robustness to occlusions, without scenario-specific fine-tuning. Our project page is available here: https://skelsplat.github.io.","% most multi-view approaches fuse information from multiple viewpoints
% some of them address camera parameters disentanglement
% some of them address occlusions

\paragraph{Gaussian Splatting}
Gaussian Splatting~\cite{kerbl20233d} had a disruptive impact on graphics~\cite{huang20242d, yu2024mip, guedon2024sugar, hollein20243dgs}, achieving state-of-the-art results in novel view synthesis. While subsequent works enhanced representations~\cite{huang20242d, yu2024mip}, reconstruction losses~\cite{guedon2024sugar}, or optimization techniques~\cite{hollein20243dgs}, our work focuses on applying the original formulation to multi-view 3D human pose estimation.
%
Gaussian Splatting has already been explored in diverse domains, including SLAM~\cite{matsuki2024gaussian}, real-to-sim transfer~\cite{jiang2025phystwin}, open-vocabulary segmentation~\cite{qin2024langsplat}, robot learning~\cite{barcellona2025dream}, and human reconstruction~\cite{kocabas2024hugs}, typically incorporating Gaussian Splatting without any fundamental changes to rendering or optimization pipelines. In contrast, we introduce a novel rendering function and supervision strategy, adapting Gaussian Splatting to the specific challenges of multi-view pose estimation.

\paragraph{Gaussian Splatting for 3D Human Reconstruction}
One of the closest applications of Gaussian Splatting for human perception is human reconstruction. Early works reconstruct accurate 3D human avatars from sparse images~\cite{kocabas2024hugs,Hu_2024_CVPR}, often incorporating parametric human models~\cite{Xiao_2025_CVPR, xiao2025rogsplat, svitov2024haha}, such as SMPL~\cite{bogo2016keep}, or skeletal priors~\cite{peng2023implicit, hu2024gauhuman, wen2024gomavatar} for articulated shapes and novel view generation.
Other approaches integrate diffusion for occlusion handling~\cite{sunoccfusion} or extract human features from Gaussian reconstructions~\cite{dey2024hfgaussian,prospero2025gst}, but primarily focus on high-quality offline reconstruction and subject-specific optimization. While these methods prioritize visual fidelity, our work instead focuses on accurate 3D joint localization.


\paragraph{Multi-view 3D Human Pose Estimation}
Multi-view human pose estimation aims to recover the 3D positions of each body joint by leveraging multiple synchronized cameras. Several works have explored learning-based fusion strategies to mitigate the reliance on accurate 2D detections, which often fail under occlusions~\cite{he2020epipolar, ma2021transfusion, zhang2021adafuse}. Iskakov et al.~\cite{iskakov2019learnable} aggregate 2D features into a shared 3D volume. %from multiple views.
He et al.~\cite{he2020epipolar} use epipolar geometry to attend to geometrically consistent pixels. TransFusion~\cite{ma2021transfusion} combines self-attention with positional encoding for occlusion robustness. AdaFuse~\cite{zhang2021adafuse} adaptively weights views to reduce the impact of low-quality detections.
%
While these methods handle occlusions, they remain limited in out-of-domain scenarios because their fusion strategies are learned from few collected datasets, such as Human3.6M~\cite{ionescu2013human3} and CMU Panoptic~\cite{joo2015panoptic}.

Recent works have therefore focused on robustness and generalization without relying on direct 3D supervision, using temporal consistency~\cite{davoodnia2024upose3d, moliner2024geometry}, geometric  consistency~\cite{zhao2023triangulation,bragagnolo2025multi}, multi-modal fusion~\cite{chen2024adaptivefusion} or optimization pipelines considering body priors learned on datasets~\cite{chen2022structural}.
%
%
While these approaches focus on learning fusion strategies, \textit{SkelSplat} employs differentiable Gaussian rendering to directly optimize 3D poses without relying on dataset-specific assumptions, demonstrating that a Gaussian representation, derived from Kerbl et al.~\cite{kerbl20233d}, generalizes better, especially in occluded scenes.
%. Inspired by the flexibility of classical triangulation~\cite{remelli2020lightweight, bartol2022generalizable}, \textit{SkelSplat} 

\paragraph{One-hot Encoding for Human Joints}
In many vision tasks, such as classification~\cite{he2016deep, dosovitskiy2020image} or semantic segmentation~\cite{chen2017deeplab, Cheng_2022_CVPR}, class identity is represented via one-hot encoding, with each class assigned to a dedicated output index or channel. 3D and 2D human pose estimation adopt the same principle, predicting joint coordinates~\cite{martinez2017simple,ye2023distilpose} or joint-specific heatmaps~\cite{newell2016stacked,xiao2018simple,sun2019deep}. 
% In both cases, joint identity is not learned but hard-coded into the architecture.
This strategy has not yet been explored in Gaussian Splatting, where the rendered channels are typically RGB views of the reconstructed scene ~\cite{kerbl20233d}, not intended for accurate 3D points localization. 
To address this, we adapt Gaussian Splatting to operate on a one-hot encoding representation by redefining the rendering function and extending the output channels beyond RGB, enabling precise 3D joint localization.

% For multi-view 3D pose estimation, we therefore extend Gaussian Splatting to render joint-specific channels with a novel one-hot encoding framework, enabling independent joint optimization and accurate 3D joint localization.

%,mao2021tfpose,zhang2021direct%
%,pavlakos2017coarse,Kang_2024_CVPR","\paragraph{Gaussian Splatting}
Gaussian Splatting~\cite{kerbl20233d} had a disruptive impact on graphics~\cite{huang20242d, yu2024mip, guedon2024sugar, hollein20243dgs}, achieving state-of-the-art results in novel view synthesis. While subsequent works enhanced representations~\cite{huang20242d, yu2024mip}, reconstruction losses~\cite{guedon2024sugar}, or optimization techniques~\cite{hollein20243dgs}, our work focuses on applying the original formulation to multi-view 3D human pose estimation.

Gaussian Splatting has already been explored in diverse domains, including SLAM~\cite{matsuki2024gaussian}, real-to-sim transfer~\cite{jiang2025phystwin}, open-vocabulary segmentation~\cite{qin2024langsplat}, robot learning~\cite{barcellona2025dream}, and human reconstruction~\cite{kocabas2024hugs}, typically incorporating Gaussian Splatting without any fundamental changes to rendering or optimization pipelines. In contrast, we introduce a novel rendering function and supervision strategy, adapting Gaussian Splatting to the specific challenges of multi-view pose estimation.

\paragraph{Gaussian Splatting for 3D Human Reconstruction}
One of the closest applications of Gaussian Splatting for human perception is human reconstruction. Early works reconstruct accurate 3D human avatars from sparse images~\cite{kocabas2024hugs,Hu_2024_CVPR}, often incorporating parametric human models~\cite{Xiao_2025_CVPR, xiao2025rogsplat, svitov2024haha}, such as SMPL~\cite{bogo2016keep}, or skeletal priors~\cite{peng2023implicit, hu2024gauhuman, wen2024gomavatar} for articulated shapes and novel view generation.
Other approaches integrate diffusion for occlusion handling~\cite{sunoccfusion} or extract human features from Gaussian reconstructions~\cite{dey2024hfgaussian,prospero2025gst}, but primarily focus on high-quality offline reconstruction and subject-specific optimization. While these methods prioritize visual fidelity, our work instead focuses on accurate 3D joint localization.


\paragraph{Multi-view 3D Human Pose Estimation}
Multi-view human pose estimation aims to recover the 3D positions of each body joint by leveraging multiple synchronized cameras. Several works have explored learning-based fusion strategies to mitigate the reliance on accurate 2D detections, which often fail under occlusions~\cite{he2020epipolar, ma2021transfusion, zhang2021adafuse}. Iskakov et al.~\cite{iskakov2019learnable} aggregate 2D features into a shared 3D volume. 
He et al.~\cite{he2020epipolar} use epipolar geometry to attend to geometrically consistent pixels. TransFusion~\cite{ma2021transfusion} combines self-attention with positional encoding for occlusion robustness. AdaFuse~\cite{zhang2021adafuse} adaptively weights views to reduce the impact of low-quality detections.

While these methods handle occlusions, they remain limited in out-of-domain scenarios because their fusion strategies are learned from few collected datasets, such as Human3.6M~\cite{ionescu2013human3} and CMU Panoptic~\cite{joo2015panoptic}.

Recent works have therefore focused on robustness and generalization without relying on direct 3D supervision, using temporal consistency~\cite{davoodnia2024upose3d, moliner2024geometry}, geometric  consistency~\cite{zhao2023triangulation,bragagnolo2025multi}, multi-modal fusion~\cite{chen2024adaptivefusion} or optimization pipelines considering body priors learned on datasets~\cite{chen2022structural}.


While these approaches focus on learning fusion strategies, \textit{SkelSplat} employs differentiable Gaussian rendering to directly optimize 3D poses without relying on dataset-specific assumptions, demonstrating that a Gaussian representation, derived from Kerbl et al.~\cite{kerbl20233d}, generalizes better, especially in occluded scenes.


\paragraph{One-hot Encoding for Human Joints}
In many vision tasks, such as classification~\cite{he2016deep, dosovitskiy2020image} or semantic segmentation~\cite{chen2017deeplab, Cheng_2022_CVPR}, class identity is represented via one-hot encoding, with each class assigned to a dedicated output index or channel. 3D and 2D human pose estimation adopt the same principle, predicting joint coordinates~\cite{martinez2017simple,ye2023distilpose} or joint-specific heatmaps~\cite{newell2016stacked,xiao2018simple,sun2019deep}. 

This strategy has not yet been explored in Gaussian Splatting, where the rendered channels are typically RGB views of the reconstructed scene ~\cite{kerbl20233d}, not intended for accurate 3D points localization. 
To address this, we adapt Gaussian Splatting to operate on a one-hot encoding representation by redefining the rendering function and extending the output channels beyond RGB, enabling precise 3D joint localization.","Gaussian SplattingGaussian Splatting [25] had a dis-
ruptive impact on graphics [14, 17, 20, 54], achieving
state-of-the-art results in novel view synthesis. While
subsequent works enhanced representations [20, 54], re-
construction losses [14], or optimization techniques [17],
our work focuses on applying the original formulation to
multi-view 3D human pose estimation. Gaussian Splatting
has already been explored in diverse domains, including
SLAM [29], real-to-sim transfer [23], open-vocabulary seg-
mentation [35], robot learning [1], and human reconstruc-
tion [26], typically incorporating Gaussian Splatting with-
out any fundamental changes to rendering or optimization
pipelines. In contrast, we introduce a novel rendering func-
tion and supervision strategy, adapting Gaussian Splatting
to the specific challenges of multi-view pose estimation.
Gaussian Splatting for 3D Human ReconstructionOne
of the closest applications of Gaussian Splatting for hu-
man perception is human reconstruction. Early works
reconstruct accurate 3D human avatars from sparse im-
ages [18, 26], often incorporating parametric human mod-
els [42, 51, 52], such as SMPL [3], or skeletal pri-
ors [19, 33, 48] for articulated shapes and novel view gen-
eration. Other approaches integrate diffusion for occlu-
sion handling [40] or extract human features from Gaus-
sian reconstructions [11, 34], but primarily focus on high-
quality offline reconstruction and subject-specific optimiza-
tion. While these methods prioritize visual fidelity, our
work instead focuses on accurate 3D joint localization.Multi-view 3D Human Pose EstimationMulti-view hu-
man pose estimation aims to recover the 3D positions of
each body joint by leveraging multiple synchronized cam-
eras. Several works have explored learning-based fusion
strategies to mitigate the reliance on accurate 2D detections,
which often fail under occlusions [16, 27, 55]. Iskakov et
al. [22] aggregate 2D features into a shared 3D volume. He
et al. [16] use epipolar geometry to attend to geometrically
consistent pixels. TransFusion [27] combines self-attention
with positional encoding for occlusion robustness. Ada-
Fuse [55] adaptively weights views to reduce the impact of
low-quality detections. While these methods handle occlu-
sions, they remain limited in out-of-domain scenarios be-
cause their fusion strategies are learned from few collected
datasets, such as Human3.6M [21] and CMU Panoptic [24].
Recent works have therefore focused on robustness
and generalization without relying on direct 3D supervi-
sion, using temporal consistency [10, 30], geometric con-
sistency [4, 56], multi-modal fusion [5] or optimization
pipelines considering body priors learned on datasets [8].
While these approaches focus on learning fusion strategies,
SkelSplatemploys differentiable Gaussian rendering to di-
rectly optimize 3D poses without relying on dataset-specific
assumptions, demonstrating that a Gaussian representation,
derived from Kerbl et al. [25], generalizes better, especially
in occluded scenes.
One-hot Encoding for Human JointsIn many vision
tasks, such as classification [12, 15] or semantic segmen-
tation [6, 9], class identity is represented via one-hot en-
coding, with each class assigned to a dedicated output in-
dex or channel. 3D and 2D human pose estimation adopt
the same principle, predicting joint coordinates [28, 53]
or joint-specific heatmaps [31, 41, 50]. This strategy has
not yet been explored in Gaussian Splatting, where the
rendered channels are typically RGB views of the recon-
structed scene [25], not intended for accurate 3D points lo-
calization. To address this, we adapt Gaussian Splatting to
operate on a one-hot encoding representation by redefining
the rendering function and extending the output channels
beyond RGB, enabling precise 3D joint localization."
2511.09833v1,http://arxiv.org/abs/2511.09833v1,2025-11-13 00:32:30+00:00,ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking,"Supervised learning relies on high-quality labeled data, but obtaining such data through human annotation is both expensive and time-consuming. Recent work explores using large language models (LLMs) for annotation, but LLM-generated labels still fall short of human-level quality. To address this problem, we propose the Annotation with Critical Thinking (ACT) data pipeline, where LLMs serve not only as annotators but also as judges to critically identify potential errors. Human effort is then directed towards reviewing only the most ""suspicious"" cases, significantly improving the human annotation efficiency. Our major contributions are as follows: (1) ACT is applicable to a wide range of domains, including natural language processing (NLP), computer vision (CV), and multimodal understanding, by leveraging multimodal-LLMs (MLLMs). (2) Through empirical studies, we derive 7 insights on how to enhance annotation quality while efficiently reducing the human cost, and then translate these findings into user-friendly guidelines. (3) We theoretically analyze how to modify the loss function so that models trained on ACT data achieve similar performance to those trained on fully human-annotated data. Our experiments show that the performance gap can be reduced to less than 2% on most benchmark datasets while saving up to 90% of human costs.","\label{sec: related_works}

\textbf{Data Annotation with LLMs}
LLMs' annotation ability has been widely studied, but mainly in NLP \citep{tan2024large,llmaaa2023zhang,pangakis2024knowledge,tseng2024expert}. The most relevant to our work is CDI \citep{gligoric2024can}, which uses a trained XGBoost \citep{chen2016xgboost} to detect LLM errors for human review. However, CDI (1) employs an error detector that lacks flexibility, as it often requires task-specific design and training with additional data; and (2) uses normalization-based active M-estimation loss, which we find suboptimal in downstream experiments. \textcolor{black}{We further discuss CDI and other prior works \citep{kim2024meganno+,wang2024human_llm,mavromatis2023examples,wang2024model}, along with experimental comparisons, in Appendix \ref{apx: related_works}.}
% The annotation ability of LLMs has been extensively studied, but most works focus on NLP tasks \citep{gligoric2024can,llmaaa2023zhang,pangakis2024knowledge,tan2024large}. While LLMs offer scalability, studies consistently show that models trained on human-annotated data outperform those using LLM-generated labels \citep{pangakis2024knowledge,llmgooadnnot2023mohta,tseng2024expert}. The most relevant work to our research is CDI \citep{gligoric2024can}, where LLM errors are identified by a trained XGBoost \citep{chen2016xgboost} and corrected by humans. However, their method (1) requires large volume of data to train the XGBoost error detector, and (2) adopts the normalization-based active M-estimation loss, which we find suboptimal in our experiments. 
% \textcolor{red}{We provide additional details on related works of improving LLM annotation quality \citep{mavromatis2023examples,wang2024model,wang2024human_llm} in Appendix \ref{apx: related_works}.} 

%To improve the quality of LLM annotation, a study adopted in-context learning (ICL) with examples annotated by human \citep{mavromatis2023examples}. The ICL examples are actively selected based on LLM logit probabilities during annotation, which means their method only supports white-box LLMs. Another work introduced MILO, a collaborative annotation system combining LLMs and human experts, where human efforts are allocated to data with lower LLM confidence scores \citep{wang2024model}. However, there is no discussion about how to use the data to boost downstream training outcomes. The most relevant work to our research is CDI \citep{gligoric2024can}, where LLM errors are identified by a trained XGBoost \citep{chen2016xgboost} and corrected by humans. However, their method (1) requires large volume of data to train the XGBoost error detector, and (2) adopts the normalization-based active M-estimation loss, which we find suboptimal in our experiments.


\textbf{LLM-as-a-Judge} LLM-as-a-Judge refers to using an LLM to evaluate the outputs generated by other LLMs \citep{gu2024survey,li2024llms,chiang2023can,zheng2023judging}. Many prior works considered the case where the verifier and generator are the same model, and focused on the phenomenon of LLM self-improvement \citep{huang2023large,kamoi2024can,song2025mind,singh2023beyond,chen2023teaching,lee_volcano_2024,saunders2022self}, which is the concept we refer to as self-criticism in our study. However, recent studies suggest that self-criticism may introduce bias, as the verifier tends to favor its own outputs \citep{ye2024justice,li2025preference}. This may explain why we observe better performance with cross-criticism in our ACT data pipeline.","\textbf{Data Annotation with LLMs}
LLMs' annotation ability has been widely studied, but mainly in NLP \citep{tan2024large,llmaaa2023zhang,pangakis2024knowledge,tseng2024expert}. The most relevant to our work is CDI \citep{gligoric2024can}, which uses a trained XGBoost \citep{chen2016xgboost} to detect LLM errors for human review. However, CDI (1) employs an error detector that lacks flexibility, as it often requires task-specific design and training with additional data; and (2) uses normalization-based active M-estimation loss, which we find suboptimal in downstream experiments. \textcolor{black}{We further discuss CDI and other prior works \citep{kim2024meganno+,wang2024human_llm,mavromatis2023examples,wang2024model}, along with experimental comparisons, in Appendix \ref{apx: related_works}.}






\textbf{LLM-as-a-Judge} LLM-as-a-Judge refers to using an LLM to evaluate the outputs generated by other LLMs \citep{gu2024survey,li2024llms,chiang2023can,zheng2023judging}. Many prior works considered the case where the verifier and generator are the same model, and focused on the phenomenon of LLM self-improvement \citep{huang2023large,kamoi2024can,song2025mind,singh2023beyond,chen2023teaching,lee_volcano_2024,saunders2022self}, which is the concept we refer to as self-criticism in our study. However, recent studies suggest that self-criticism may introduce bias, as the verifier tends to favor its own outputs \citep{ye2024justice,li2025preference}. This may explain why we observe better performance with cross-criticism in our ACT data pipeline.","Data Annotation with LLMsLLMs’ annotation ability has been widely studied, but mainly in NLP
[6,7,12,13]. The most relevant to our work is CDI [ 17], which uses a trained XGBoost [ 49] to detect
LLM errors for human review. However, CDI (1) employs an error detector that lacks flexibility, as it
often requires task-specific design and training with additional data; and (2) uses normalization-based
active M-estimation loss, which we find suboptimal in downstream experiments. We further discuss
CDI and other prior works [15, 16, 18, 19], along with experimental comparisons, in Appendix G.
LLM-as-a-JudgeLLM-as-a-Judge refers to using an LLM to evaluate the outputs generated by other
LLMs [ 23,50,51,52]. Many prior works considered the case where the verifier and generator are the
same model, and focused on the phenomenon of LLM self-improvement [ 21,22,24,53,54,55,56],
which is the concept we refer to as self-criticism in our study. However, recent studies suggest that
9
self-criticism may introduce bias, as the verifier tends to favor its own outputs [ 57,58]. This may
explain why we observe better performance with cross-criticism in our ACT data pipeline."
2511.08810v1,http://arxiv.org/abs/2511.08810v1,2025-11-11 22:23:27+00:00,SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph,"Adversarial attacks expose a fundamental vulnerability in modern deep vision models by exploiting their dependence on dense, pixel-level representations that are highly sensitive to imperceptible perturbations. Traditional defense strategies typically operate within this fragile pixel domain, lacking mechanisms to incorporate inherently robust visual features. In this work, we introduce SIFT-Graph, a multimodal defense framework that enhances the robustness of traditional vision models by aggregating structurally meaningful features extracted from raw images using both handcrafted and learned modalities. Specifically, we integrate Scale-Invariant Feature Transform keypoints with a Graph Attention Network to capture scale and rotation invariant local structures that are resilient to perturbations. These robust feature embeddings are then fused with traditional vision model, such as Vision Transformer and Convolutional Neural Network, to form a unified, structure-aware and perturbation defensive model. Preliminary results demonstrate that our method effectively improves the visual model robustness against gradient-based white box adversarial attacks, while incurring only a marginal drop in clean accuracy.","\subsection{Adversarial Defense in Vision Models}

Despite the impressive performance of deep neural networks in image classification, they are notoriously vulnerable to adversarial examples—small, carefully crafted perturbations that can lead to incorrect predictions while remaining imperceptible to humans~\cite{PGD, adversarial}. This vulnerability poses significant risks in safety-critical applications and has motivated extensive research into defense mechanisms.

One prominent line of work is adversarial training, where models are trained on adversarial examples generated during training time~\cite{PGD}. While effective, this approach is computationally expensive and often results in a trade-off between robustness and clean accuracy. Another common method involves input preprocessing techniques such as denoising, compression, or diffusion-based restoration~\cite{Diffusiondefense}, which aim to remove adversarial noise before feeding the image into the classifier. However, many of these techniques either degrade clean image quality or are vulnerable to adaptive attacks.

A separate thread of research attempts to obfuscate gradients or modify model behavior to make it harder for attackers to compute effective perturbations~\cite{gradientobfu}. These methods have been criticized for offering ``false security,"" as many are easily bypassed by stronger or white-box attacks.

In addition, researchers have explored robust feature enhancement as an alternative defense strategy. Instead of relying solely on raw pixels, models can benefit from incorporating features that are less sensitive to small perturbations. Examples include frequency-domain features~\cite{freqfusion,FrequencyFusion2,FrequencyFusion}, edge maps~\cite{edgemulti1}. These approaches leverage the observation that high-frequency perturbations often fail to significantly distort structural or semantic information.



\subsection{Multimodal Learning in Computer Vision and Defense}

Multimodal learning aims to enhance model performance by integrating complementary information from multiple sources or modalities, such as vision, language, audio, and depth~\cite{multidefense1, multidefense2}. In the context of computer vision, combining modalities allows models to gain a more comprehensive understanding of the scene, which can lead to improved generalization, interpretability, and robustness.

Recent advances have leveraged joint vision-language training, as in CLIP~\cite{CLIP}, Grounding DINO~\cite{dino}, and ALBEF~\cite{ALBEF}, to learn semantically rich image embeddings aligned with textual concepts. Other efforts combine RGB and depth information~\cite{multidefense2}, or fuse spatial and frequency-domain cues~\cite{FrequencyFusion,FrequencyFusion2}, to enrich visual representations.

Multimodal approaches have also been investigated for improving robustness and adversarial defense. For instance, some works explore fusing edge information~\cite{edgemulti1}, segmentation masks~\cite{segmenteddefense}, or frequency components~\cite{freqfusion} alongside RGB images to mitigate the effects of perturbations. These studies demonstrate that adversarial examples often exploit the fragility of a single modality, and that complementary signals can compensate for compromised information pathways.



\subsection{Graph Neural Networks for Vision}

Graph Neural Networks have emerged as powerful tools for modeling relational and structured data, making them well-suited for visual tasks that involve spatial reasoning or topology-aware representations~\cite{gnnsurvey}. In computer vision, GNNs have been employed to model interactions among objects~\cite{scene_graph}, understand human-object relationships~\cite{hoi_gnn}, and perform point cloud and mesh analysis in 3D vision~\cite{3d_gnn_review}.

Unlike convolutional or transformer-based models that process grid-like image data, GNNs offer the flexibility to operate on irregular structures, such as keypoint graphs or superpixel regions. This property makes them particularly valuable for encoding geometric relationships and context beyond local receptive fields. For example, DRG-Net~\cite{siftgraph} constructs a graph over local features for diabetic retinopathy grading, while SplineCNN~\cite{splinecnn} models continuous convolution over non-Euclidean domains for shape analysis.

In adversarial settings, GNNs offer a promising defense direction due to their ability to smooth and propagate robust features across neighborhoods~\cite{robust_gnn}. Some studies have explored leveraging relational inductive bias or sparse graph structures to resist perturbations~\cite{gnn_defense}. However, few works have investigated constructing visual graphs from classical features such as SIFT, which offer a degree of invariance to scale, rotation, and local noise.






\begin{figure*}
  \centering
  \begin{subfigure}{0.68\linewidth}
    % \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
    \includegraphics[width=\linewidth]{sec/flow_overall.png}
    \caption{SIFT-Graph prediction flow}
    \label{fig:short-a}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.28\linewidth}
    \includegraphics[width=\linewidth]{sec/GraphEncoder.png}
    \caption{Graph Encoder Design}
    \label{fig:short-b}
  \end{subfigure}
  \caption{(a)Demonstration of the SIFT-Graph central model component and workflow. (b)Detailed design for graph encoder, where the node attribution refers to combination of position coordinate(2), direction(1), response(1) and size(1), where the n refers to the number of nodes.}
  \label{fig:short}
\end{figure*}","\subsection{Adversarial Defense in Vision Models}

Despite the impressive performance of deep neural networks in image classification, they are notoriously vulnerable to adversarial examples—small, carefully crafted perturbations that can lead to incorrect predictions while remaining imperceptible to humans~\cite{PGD, adversarial}. This vulnerability poses significant risks in safety-critical applications and has motivated extensive research into defense mechanisms.

One prominent line of work is adversarial training, where models are trained on adversarial examples generated during training time~\cite{PGD}. While effective, this approach is computationally expensive and often results in a trade-off between robustness and clean accuracy. Another common method involves input preprocessing techniques such as denoising, compression, or diffusion-based restoration~\cite{Diffusiondefense}, which aim to remove adversarial noise before feeding the image into the classifier. However, many of these techniques either degrade clean image quality or are vulnerable to adaptive attacks.

A separate thread of research attempts to obfuscate gradients or modify model behavior to make it harder for attackers to compute effective perturbations~\cite{gradientobfu}. These methods have been criticized for offering ``false security,"" as many are easily bypassed by stronger or white-box attacks.

In addition, researchers have explored robust feature enhancement as an alternative defense strategy. Instead of relying solely on raw pixels, models can benefit from incorporating features that are less sensitive to small perturbations. Examples include frequency-domain features~\cite{freqfusion,FrequencyFusion2,FrequencyFusion}, edge maps~\cite{edgemulti1}. These approaches leverage the observation that high-frequency perturbations often fail to significantly distort structural or semantic information.



\subsection{Multimodal Learning in Computer Vision and Defense}

Multimodal learning aims to enhance model performance by integrating complementary information from multiple sources or modalities, such as vision, language, audio, and depth~\cite{multidefense1, multidefense2}. In the context of computer vision, combining modalities allows models to gain a more comprehensive understanding of the scene, which can lead to improved generalization, interpretability, and robustness.

Recent advances have leveraged joint vision-language training, as in CLIP~\cite{CLIP}, Grounding DINO~\cite{dino}, and ALBEF~\cite{ALBEF}, to learn semantically rich image embeddings aligned with textual concepts. Other efforts combine RGB and depth information~\cite{multidefense2}, or fuse spatial and frequency-domain cues~\cite{FrequencyFusion,FrequencyFusion2}, to enrich visual representations.

Multimodal approaches have also been investigated for improving robustness and adversarial defense. For instance, some works explore fusing edge information~\cite{edgemulti1}, segmentation masks~\cite{segmenteddefense}, or frequency components~\cite{freqfusion} alongside RGB images to mitigate the effects of perturbations. These studies demonstrate that adversarial examples often exploit the fragility of a single modality, and that complementary signals can compensate for compromised information pathways.



\subsection{Graph Neural Networks for Vision}

Graph Neural Networks have emerged as powerful tools for modeling relational and structured data, making them well-suited for visual tasks that involve spatial reasoning or topology-aware representations~\cite{gnnsurvey}. In computer vision, GNNs have been employed to model interactions among objects~\cite{scene_graph}, understand human-object relationships~\cite{hoi_gnn}, and perform point cloud and mesh analysis in 3D vision~\cite{3d_gnn_review}.

Unlike convolutional or transformer-based models that process grid-like image data, GNNs offer the flexibility to operate on irregular structures, such as keypoint graphs or superpixel regions. This property makes them particularly valuable for encoding geometric relationships and context beyond local receptive fields. For example, DRG-Net~\cite{siftgraph} constructs a graph over local features for diabetic retinopathy grading, while SplineCNN~\cite{splinecnn} models continuous convolution over non-Euclidean domains for shape analysis.

In adversarial settings, GNNs offer a promising defense direction due to their ability to smooth and propagate robust features across neighborhoods~\cite{robust_gnn}. Some studies have explored leveraging relational inductive bias or sparse graph structures to resist perturbations~\cite{gnn_defense}. However, few works have investigated constructing visual graphs from classical features such as SIFT, which offer a degree of invariance to scale, rotation, and local noise.",
2511.08230v1,http://arxiv.org/abs/2511.08230v1,2025-11-11 13:30:41+00:00,VocalBench-zh: Decomposing and Benchmarking the Speech Conversational Abilities in Mandarin Context,"The development of multi-modal large language models (LLMs) leads to intelligent approaches capable of speech interactions. As one of the most widely spoken languages globally, Mandarin is supported by most models to enhance their applicability and reach. However, the scarcity of comprehensive speech-to-speech (S2S) benchmarks in Mandarin contexts impedes systematic evaluation for developers and hinders fair model comparison for users. In this work, we propose VocalBench-zh, an ability-level divided evaluation suite adapted to Mandarin context consisting of 10 well-crafted subsets and over 10K high-quality instances, covering 12 user-oriented characters. The evaluation experiment on 14 mainstream models reveals the common challenges for current routes, and highlights the need for new insights into next-generation speech interactive systems. The evaluation codes and datasets will be available at https://github.com/SJTU-OmniAgent/VocalBench-zh.","\subsection{Speech Interaction System}

Speech interaction systems aim to understand and reason about vocal instructions and generate responses that are aligned in both text and speech. The naive pipeline for such systems involves chaining automatic speech recognition (ASR), LLM, and text-to-speech (TTS) modules. While these cascade-based approaches enable interactions, they suffer from accumulated latency and significant information loss, particularly with respect to acoustic and paralinguistic cues. By incorporating a speech encoder and vocoder to directly discrete speech inputs and generate speech responses, SpeechLLMs address these limitations in an end-to-end manner. As classified in~\citealp{chen2025minmo}, native SpeechLLMs employ a decoder-only Transformer architecture to jointly generate both textual and speech-related response tokens within a unified framework, exemplified by Baichuan-Omni~\cite{li2025baichuan}, GLM-4-Voice~\cite{zeng2024glm}, and VITA-Audio~\cite{long2025vita}, while the aligned types utilize separate decoders in conjunction with an LLM backbone to manage the dual output modalities, represented by VocalNet~\cite{wang2025vocalnet}, the Qwen-Omni series~\cite{xu2025qwen2, xu2025qwen3}, and MiMo-Audio~\cite{coreteam2025mimoaudio}. All of the aforementioned models support voice interactions in both English and Mandarin.

\subsection{Speech-to-Speech Benchmarks}

As shown in Table~\ref{tab:related_benchmark}, a variety of interactive assessment benchmarks have been proposed for speech-to-speech settings. VoiceBench~\cite{chen2024voicebench}, WildSpeech-Bench~\cite{zhang2025wildspeech}, and VoiceAssistant-Eval~\cite{wang2025voiceassistant} include only English instances, thereby neglecting Mandarin-based interactions. OpenAudioBench~\cite{li2025baichuan}, URO-Bench~\cite{yan2025uro}, VCB Bench~\cite{hu2025vcb}, and TELEVAL~\cite{li2025televal} contain instances in both languages or focus on Mandarin interactions; however, these are often derived from simple translations of English test cases and fail to capture Mandarin-specific linguistic customs and cultural nuances. Furthermore, these benchmarks typically adopt a question-only evaluation format, lacking simulated conversational contexts that reflect real-world usage. More critically, the design of their test examples diverges from the current capabilities and practical application scenarios. We summarize the task formats into the following categories:

% \begin{table*}[htbp!]
% \resizebox{1\linewidth}{!}{
% \begin{tabular}{ccccccccc}
% \toprule
%  \textbf{Ability} & \textbf{Set} & \textbf{Instances} & \textbf{Public Resources} & \textbf{Categories} & \textbf{Indicator} \\  \midrule

% \multirow{3}{*}{Knowledge} & Chinese & 1000 & Chinese SimpleQA, WebQA, CMMLU & Chinese Culture, History, Celebrity, Law, etc. & \multirow{3}{*}{Accuracy} \\

% & Foreign & 1000 & LLaMA-Q, WebQ, TriviaQA, SciQ & Foreign Culture, History, Celebrity, Law, etc.  &  \\

% & General & 1000 & All above resources & Biology, Chemistry, Geography, Physics, etc. &  \\ \hline

% \multirow{3}{*}{Reasoning} & Base & 432 & CommonsenseQA, STORAL, TruthfulQA, OpenAudioBench & Analogical, Causal, Common Sense, etc. &  \multirow{3}{*}{Accuracy} \\

% & Math & 269 & GSM8K & Math Problems &   \\

% & Culture & 150 & Riddle, CHARM & Riddle, Movie Recommendation, etc. &   \\ \hline

% \multirow{2}{*}{Creativity} & Appreciation & 45 & WenMind, Manual & Achient \& Modern Poetry & \multirow{2}{*}{Score} \\

% & Writing & 195 & WenMind, AlignBench, SHSEE, WritingBench, Manual & Narratives, Argumentative, Descriptive, Role Play, etc. &  \\ \hline

% Single-Round & - & 200 & AlpacaEval, Manual & Suggestions, Guidance & Score \\ 

% Multi-Round & - & 400 & MT-Bench-101, Manual & General Reasoning, Instruction Clarification, etc. & Score \\ 

% Instruction Following & - & 1000 & FollowBench, IFEval, InFoBench, SpeechInstructBench, Manual & Progressive, Conditional, Topic Change, Keywords, etc. & Following Rate \\

% Emotional Empathy & - & 500 & Emotional Messages & Happy, Sad, Surprised, Angry, Neutral & Semantic \& Acoustic Score \\

% Safety & - & 400 & Safety Tests & - & Refusal Rate \\ 

% Code-Switching & - & 562 & CS3-Bench & Knowledge-based, Open-domain & Accuracy, Score \\ \hline


% \multirow{2}{*}{(Anti-)Noise} & White & 600 & - & Different SNRs & \multirow{6}{*}{Score, Preserve Rate} \\ 

% & Background & 600 & MUSAN & Different SNRs &  \\ 

% Reverberation & - & 600 & - & Different RT60s &  \\ 

% Far-field & - & 800 & - & Different Filter Thresholds & \\ 

% Packet Loss & - & 500 & - & Different Loss Ratios  & \\

% Distortion & - & 500 & - & Different Clipping Thresholds & \\ \hline

% Total & - & 10753 & - & - & - \\
% \bottomrule
% \end{tabular}
% }
% \caption{VocalBench Statistics.}
% \label{tab:dataset_overview}
% \end{table*}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.9\textwidth]{Figs/creation_pipeline.pdf}
  \caption{Main creation pipeline for VocalBench-zh.}
  \label{fig:creation_pipeline}
\end{figure*}


\begin{itemize}
    \item Multiple Choice (MC): The multiple-choice format is the most prevalent method, as it enables objective accuracy metrics. In speech interaction tests, this typically involves verbally presenting both the labels and content of several options. While this approach offers a constrained and manageable response space, it seldom reflects the open-ended nature of genuine real-world conversational interactions.

    \item Speech Understanding (SU): Speech understanding tasks typically involve a conversation among multiple speakers, followed by a voice-delivered question. They often rely on detailed speaker information—such as turn-taking dynamics or identity—that is generally not accessible by most interactive speech models. Consequently, this format better suits speech understanding models represented by Qwen-Audio and SALMONN, and deviates significantly from speech interaction models.

    \item Audio Understanding (AU): Audio understanding tasks typically involve an audio or music clip, followed by a voice-delivered question. Most models currently lack universal capabilities for understanding and generating both natural speech and music. Furthermore, certain systems depend on VAD-based (voice activity detection) speech, resulting in discarded audio signals during deployment. 
    
    \item Multi-modality (MM): Multi-modality testing incorporates supplementary information, typically images or videos. Although certain models, such as Baichuan-Omni~\cite{li2025baichuan} and the Qwen-Omni~\cite{xu2025qwen2, xu2025qwen3} series, support visual inputs, the majority of speech interaction models are restricted to text and speech inputs. Consequently, multi-modality evaluation would significantly reduce the applicability and coverage of the benchmark across existing models.

    \item Speech Instruction (SI):
    Speech instruction represents the most consistent format with daily interactions, as users' context, requirements, and instructions are all conveyed to the model through continuous speech. The VocalBench series strictly adheres to this format, not only accommodating all current speech interaction approaches but also significantly narrowing the gap between evaluation protocols and real-world application performance.

\end{itemize}

Based on the above considerations, we propose to fully leverage the speech instruction format as adopted in VocalBench and construct an extension designed for Mandarin interactive capabilities.","\subsection{Speech Interaction System}

Speech interaction systems aim to understand and reason about vocal instructions and generate responses that are aligned in both text and speech. The naive pipeline for such systems involves chaining automatic speech recognition (ASR), LLM, and text-to-speech (TTS) modules. While these cascade-based approaches enable interactions, they suffer from accumulated latency and significant information loss, particularly with respect to acoustic and paralinguistic cues. By incorporating a speech encoder and vocoder to directly discrete speech inputs and generate speech responses, SpeechLLMs address these limitations in an end-to-end manner. As classified in~\citealp{chen2025minmo}, native SpeechLLMs employ a decoder-only Transformer architecture to jointly generate both textual and speech-related response tokens within a unified framework, exemplified by Baichuan-Omni~\cite{li2025baichuan}, GLM-4-Voice~\cite{zeng2024glm}, and VITA-Audio~\cite{long2025vita}, while the aligned types utilize separate decoders in conjunction with an LLM backbone to manage the dual output modalities, represented by VocalNet~\cite{wang2025vocalnet}, the Qwen-Omni series~\cite{xu2025qwen2, xu2025qwen3}, and MiMo-Audio~\cite{coreteam2025mimoaudio}. All of the aforementioned models support voice interactions in both English and Mandarin.

\subsection{Speech-to-Speech Benchmarks}

As shown in Table~\ref{tab:related_benchmark}, a variety of interactive assessment benchmarks have been proposed for speech-to-speech settings. VoiceBench~\cite{chen2024voicebench}, WildSpeech-Bench~\cite{zhang2025wildspeech}, and VoiceAssistant-Eval~\cite{wang2025voiceassistant} include only English instances, thereby neglecting Mandarin-based interactions. OpenAudioBench~\cite{li2025baichuan}, URO-Bench~\cite{yan2025uro}, VCB Bench~\cite{hu2025vcb}, and TELEVAL~\cite{li2025televal} contain instances in both languages or focus on Mandarin interactions; however, these are often derived from simple translations of English test cases and fail to capture Mandarin-specific linguistic customs and cultural nuances. Furthermore, these benchmarks typically adopt a question-only evaluation format, lacking simulated conversational contexts that reflect real-world usage. More critically, the design of their test examples diverges from the current capabilities and practical application scenarios. We summarize the task formats into the following categories:



























































\begin{itemize}
    \item Multiple Choice (MC): The multiple-choice format is the most prevalent method, as it enables objective accuracy metrics. In speech interaction tests, this typically involves verbally presenting both the labels and content of several options. While this approach offers a constrained and manageable response space, it seldom reflects the open-ended nature of genuine real-world conversational interactions.

    \item Speech Understanding (SU): Speech understanding tasks typically involve a conversation among multiple speakers, followed by a voice-delivered question. They often rely on detailed speaker information—such as turn-taking dynamics or identity—that is generally not accessible by most interactive speech models. Consequently, this format better suits speech understanding models represented by Qwen-Audio and SALMONN, and deviates significantly from speech interaction models.

    \item Audio Understanding (AU): Audio understanding tasks typically involve an audio or music clip, followed by a voice-delivered question. Most models currently lack universal capabilities for understanding and generating both natural speech and music. Furthermore, certain systems depend on VAD-based (voice activity detection) speech, resulting in discarded audio signals during deployment. 
    
    \item Multi-modality (MM): Multi-modality testing incorporates supplementary information, typically images or videos. Although certain models, such as Baichuan-Omni~\cite{li2025baichuan} and the Qwen-Omni~\cite{xu2025qwen2, xu2025qwen3} series, support visual inputs, the majority of speech interaction models are restricted to text and speech inputs. Consequently, multi-modality evaluation would significantly reduce the applicability and coverage of the benchmark across existing models.

    \item Speech Instruction (SI):
    Speech instruction represents the most consistent format with daily interactions, as users' context, requirements, and instructions are all conveyed to the model through continuous speech. The VocalBench series strictly adheres to this format, not only accommodating all current speech interaction approaches but also significantly narrowing the gap between evaluation protocols and real-world application performance.

\end{itemize}

Based on the above considerations, we propose to fully leverage the speech instruction format as adopted in VocalBench and construct an extension designed for Mandarin interactive capabilities.","2.1 Speech Interaction System
Speech interaction systems aim to understand and
reason about vocal instructions and generate re-
sponses that are aligned in both text and speech.
The naive pipeline for such systems involves chain-
ing automatic speech recognition (ASR), LLM,
and text-to-speech (TTS) modules. While these
cascade-based approaches enable interactions, they
suffer from accumulated latency and significant in-
formation loss, particularly with respect to acoustic
and paralinguistic cues. By incorporating a speech
encoder and vocoder to directly discrete speech in-
puts and generate speech responses, SpeechLLMs
address these limitations in an end-to-end man-
ner. As classified in Chen et al., 2025, native
SpeechLLMs employ a decoder-only Transformer
architecture to jointly generate both textual and
speech-related response tokens within a unified
framework, exemplified by Baichuan-Omni (Li
et al., 2025a), GLM-4-V oice (Zeng et al., 2024),
and VITA-Audio (Long et al., 2025), while the
aligned types utilize separate decoders in conjunc-
tion with an LLM backbone to manage the dual
output modalities, represented by V ocalNet (Wang
et al., 2025c), the Qwen-Omni series (Xu et al.,
2025a,b), and MiMo-Audio (Xiaomi, 2025). All of
the aforementioned models support voice interac-
tions in both English and Mandarin.
2.2 Speech-to-Speech Benchmarks
As shown in Table 1, a variety of interactive
assessment benchmarks have been proposed for
speech-to-speech settings. V oiceBench (Chen
et al., 2024), WildSpeech-Bench (Zhang et al.,
2025), and V oiceAssistant-Eval (Wang et al.,2025b) include only English instances, thereby ne-
glecting Mandarin-based interactions. OpenAu-
dioBench (Li et al., 2025a), URO-Bench (Yan et al.,
2025), VCB Bench (Hu et al., 2025), and TELE-
V AL (Li et al., 2025b) contain instances in both
languages or focus on Mandarin interactions; how-
ever, these are often derived from simple trans-
lations of English test cases and fail to capture
Mandarin-specific linguistic customs and cultural
nuances. Furthermore, these benchmarks typically
adopt a question-only evaluation format, lacking
simulated conversational contexts that reflect real-
world usage. More critically, the design of their
test examples diverges from the current capabilities
and practical application scenarios. We summarize
the task formats into the following categories:
•Multiple Choice (MC): The multiple-choice
format is the most prevalent method, as it en-
ables objective accuracy metrics. In speech
interaction tests, this typically involves ver-
bally presenting both the labels and content of
several options. While this approach offers a
constrained and manageable response space, it
seldom reflects the open-ended nature of gen-
uine real-world conversational interactions.
•Speech Understanding (SU): Speech under-
standing tasks typically involve a conversa-
tion among multiple speakers, followed by a
voice-delivered question. They often rely on
detailed speaker information—such as turn-
taking dynamics or identity—that is gener-
ally not accessible by most interactive speech
models. Consequently, this format better suits
speech understanding models represented by
Qwen-Audio and SALMONN, and deviates
significantly from speech interaction models.
•Audio Understanding (AU): Audio under-
standing tasks typically involve an audio or
music clip, followed by a voice-delivered
question. Most models currently lack univer-
sal capabilities for understanding and gener-
English Queries
Qwen -MT-Plus
请问什么是
Mandarin Queries Rule -based
LLM
Manua l
Adapted to
Mandarin Context
Qwen2.5 -Max
Answer Chec k
Translated/Native
QA pairs
Manual
Collected
Mandarin
Datase tDiverse Speech PromptsCosyVoice"
2511.08867v1,http://arxiv.org/abs/2511.08867v1,2025-11-12 01:09:56+00:00,Conformal Prediction for Multi-Source Detection on a Network,"Detecting the origin of information or infection spread in networks is a fundamental challenge with applications in misinformation tracking, epidemiology, and beyond. We study the multi-source detection problem: given snapshot observations of node infection status on a graph, estimate the set of source nodes that initiated the propagation. Existing methods either lack statistical guarantees or are limited to specific diffusion models and assumptions. We propose a novel conformal prediction framework that provides statistically valid recall guarantees for source set detection, independent of the underlying diffusion process or data distribution. Our approach introduces principled score functions to quantify the alignment between predicted probabilities and true sources, and leverages a calibration set to construct prediction sets with user-specified recall and coverage levels. The method is applicable to both single- and multi-source scenarios, supports general network diffusion dynamics, and is computationally efficient for large graphs. Empirical results demonstrate that our method achieves rigorous coverage with competitive accuracy, outperforming existing baselines in both reliability and scalability.The code is available online.","\subsection{\gls{CP} for Set Estimation}
In this subsection, we present a brief review of the literature on \gls{CP} for set estimation problems. These works address the multi-label classification problem, where the target output is a set of labels, similar to the multi-source detection problem.

In multi-label classification, where the label set contains $N$ elements, any label subset can be represented as an $N$-dimensional binary vector. This allows the set estimation problem to be reformulated as a point estimation problem. However, since there are $2^N$ possible binary vectors, directly applying traditional \gls{CP} becomes computationally infeasible \cite{MalPaiLen:J22,TyaGuo:C23}. To address this, some works organize the labels into a hierarchical tree structure and apply multiple hypothesis testing to control the family-wise error rate \cite{TyaGuo:C23}. Additionally, incorporating the covariance structure between labels using the Mahalanobis distance, rather than the standard Euclidean norm, has been shown to improve prediction efficiency \cite{KatPap:J24}.

In all the aforementioned works that address \gls{CP} for label set estimation, the focus is on finding a collection of subsets that \emph{contains} the ground truth label subset with probability guarantee. This approach may not always be appropriate. In particular, for set estimation tasks such as network multi-source detection, returning a collection of possible node subsets is often impractical or uninformative. Instead, it is more desirable to provide a single node set that \emph{includes} the true source set $\calY$, or achieves a specified recall rate with high probability.

Existing methods such as PGM and {\arbitr} \cite{CauGupDuc:J21} address set inclusion problems by constructing inner and outer sets, $\widehat{C}_{\text{in}}$ and $\widehat{C}_{\text{out}}$, that satisfy $\widehat{C}_{\text{in}}\subset\calY\subset\widehat{C}_{\text{out}}$ with statistical guarantees. These approaches rely on learning a hierarchical tree structure over the label set and designing non-conformity scores tailored to this structure. The prediction sets are then efficiently computed via message passing on the label tree. However, these methods have notable limitations: (i) the tree structure learning in PGM does not scale to large label sets, as it requires solving $O(N^2)$ convex optimization problems without closed-form solutions; and (ii) both the non-conformity score design and prediction set construction are restricted to tree-structured label relationships, making them unsuitable for more general or arbitrary network structures. {\Gls{CRC} \cite{AnaSteAda:C24} is a systematic conformal prediction approach for controlling the risk {for general} set estimation. Our approach can be seen as its score-based version targeting at controlling the recall rate but with different score designs. In this work we provide a self-contained alternative proof technique to provide additional theoretical intuition for this framework.}

%As a result, the problem of constructing scalable and statistically efficient prediction sets for set inclusion over general networks remains largely unexplored. This is the gap this paper aims to address.



\subsection{Hypothesis Testing-based Confident Source Detection}

We next briefly review the confident network source detection method {\adit} \cite{DawLiXu:C21}. This method addresses the single-source detection problem over general networks under the SI model, providing a prediction set that contains the true source with statistical guarantees. For each node, a hypothesis test is formulated: the null hypothesis is that the node is the source, and the alternative is that it is not. To construct the test statistic, multiple infection paths are simulated from each node, and the average fitness of these paths with respect to the observed infection status is computed. A higher fitness indicates that the node is more likely to be the true source. Based on this fitness measure, hypothesis tests are performed for all nodes, and the prediction set is formed by including those nodes for which the null hypothesis is not rejected.

In summary, {\adit} provides assumption-free prediction sets for the source detection problem with statistical guarantees. Its fitness measure, which incorporates infection order, often leads to reasonably small prediction sets. However, the method has notable limitations: (i) it cannot be extended to multi-source scenarios, as hypothesis testing for all possible node subsets is computationally infeasible; and (ii) it cannot be directly applied to more complex models such as SIR, since the fitness function does not account for recovered nodes.","\subsection{\gls{CP} for Set Estimation}
In this subsection, we present a brief review of the literature on \gls{CP} for set estimation problems. These works address the multi-label classification problem, where the target output is a set of labels, similar to the multi-source detection problem.

In multi-label classification, where the label set contains $N$ elements, any label subset can be represented as an $N$-dimensional binary vector. This allows the set estimation problem to be reformulated as a point estimation problem. However, since there are $2^N$ possible binary vectors, directly applying traditional \gls{CP} becomes computationally infeasible \cite{MalPaiLen:J22,TyaGuo:C23}. To address this, some works organize the labels into a hierarchical tree structure and apply multiple hypothesis testing to control the family-wise error rate \cite{TyaGuo:C23}. Additionally, incorporating the covariance structure between labels using the Mahalanobis distance, rather than the standard Euclidean norm, has been shown to improve prediction efficiency \cite{KatPap:J24}.

In all the aforementioned works that address \gls{CP} for label set estimation, the focus is on finding a collection of subsets that \emph{contains} the ground truth label subset with probability guarantee. This approach may not always be appropriate. In particular, for set estimation tasks such as network multi-source detection, returning a collection of possible node subsets is often impractical or uninformative. Instead, it is more desirable to provide a single node set that \emph{includes} the true source set $\calY$, or achieves a specified recall rate with high probability.

Existing methods such as PGM and {\arbitr} \cite{CauGupDuc:J21} address set inclusion problems by constructing inner and outer sets, $\widehat{C}_{\text{in}}$ and $\widehat{C}_{\text{out}}$, that satisfy $\widehat{C}_{\text{in}}\subset\calY\subset\widehat{C}_{\text{out}}$ with statistical guarantees. These approaches rely on learning a hierarchical tree structure over the label set and designing non-conformity scores tailored to this structure. The prediction sets are then efficiently computed via message passing on the label tree. However, these methods have notable limitations: (i) the tree structure learning in PGM does not scale to large label sets, as it requires solving $O(N^2)$ convex optimization problems without closed-form solutions; and (ii) both the non-conformity score design and prediction set construction are restricted to tree-structured label relationships, making them unsuitable for more general or arbitrary network structures. {\Gls{CRC} \cite{AnaSteAda:C24} is a systematic conformal prediction approach for controlling the risk {for general} set estimation. Our approach can be seen as its score-based version targeting at controlling the recall rate but with different score designs. In this work we provide a self-contained alternative proof technique to provide additional theoretical intuition for this framework.}





\subsection{Hypothesis Testing-based Confident Source Detection}

We next briefly review the confident network source detection method {\adit} \cite{DawLiXu:C21}. This method addresses the single-source detection problem over general networks under the SI model, providing a prediction set that contains the true source with statistical guarantees. For each node, a hypothesis test is formulated: the null hypothesis is that the node is the source, and the alternative is that it is not. To construct the test statistic, multiple infection paths are simulated from each node, and the average fitness of these paths with respect to the observed infection status is computed. A higher fitness indicates that the node is more likely to be the true source. Based on this fitness measure, hypothesis tests are performed for all nodes, and the prediction set is formed by including those nodes for which the null hypothesis is not rejected.

In summary, {\adit} provides assumption-free prediction sets for the source detection problem with statistical guarantees. Its fitness measure, which incorporates infection order, often leads to reasonably small prediction sets. However, the method has notable limitations: (i) it cannot be extended to multi-source scenarios, as hypothesis testing for all possible node subsets is computationally infeasible; and (ii) it cannot be directly applied to more complex models such as SIR, since the fitness function does not account for recovered nodes.",
2511.08252v1,http://arxiv.org/abs/2511.08252v1,2025-11-11 13:46:06+00:00,Melodia: Training-Free Music Editing Guided by Attention Probing in Diffusion Models,"Text-to-music generation technology is progressing rapidly, creating new opportunities for musical composition and editing. However, existing music editing methods often fail to preserve the source music's temporal structure, including melody and rhythm, when altering particular attributes like instrument, genre, and mood. To address this challenge, this paper conducts an in-depth probing analysis on attention maps within AudioLDM 2, a diffusion-based model commonly used as the backbone for existing music editing methods. We reveal a key finding: cross-attention maps encompass details regarding distinct musical characteristics, and interventions on these maps frequently result in ineffective modifications. In contrast, self-attention maps are essential for preserving the temporal structure of the source music during its conversion into the target music. Building upon this understanding, we present Melodia, a training-free technique that selectively manipulates self-attention maps in particular layers during the denoising process and leverages an attention repository to store source music information, achieving accurate modification of musical characteristics while preserving the original structure without requiring textual descriptions of the source music. Additionally, we propose two novel metrics to better evaluate music editing methods. Both objective and subjective experiments demonstrate that our approach achieves superior results in terms of textual adherence and structural integrity across various datasets. This research enhances comprehension of internal mechanisms within music generation models and provides improved control for music creation.","\subsection{Text-to-music generation}
Text-to-music generation follows two main paradigms: autoregressive (AR) models and diffusion-based models. AR models like MusicLM~\cite{agostinelli2023musiclm} introduced a music-text embedding space, while MusicGen~\cite{musicgen} improved controllability through text and melody guidance. Diffusion-based models transform noise into musical structures through iterative denoising. AudioLDM~\cite{audioldm} applied latent diffusion to text-conditioned generation, while Mousai~\cite{mousai} and MusicLDM~\cite{chen2024musicldm} further advanced quality and length capabilities. Recently, AudioLDM 2~\cite{audioldm2} improved musical structure representation, MelodyFlow~\cite{melodyflow} advances the field with flow matching techniques, and Stable Audio Open~\cite{evans2025stableaudioopen} scales diffusion transformers to generate extended stereo music. As these models have made significant progress in music generation, researchers have expanded their focus to music editing with its unique challenges.

\subsection{Text-to-music editing}

\textbf{Specialized Training Methods}. MusicLM~\cite{agostinelli2023musiclm} utilizes MuLan~\cite{huang2022mulan} embedding space for style editing, while MusicGen~\cite{musicgen} facilitates editing by conditioning generation on an original audio's chromagram with text prompts for desired changes. These two models offer limited editing capabilities as a secondary function. And models like AUDIT~\cite{wang2023audit} and InstructME~\cite{han2023instructme} train diffusion models specifically for inter-stem editing. While effective, these methods require extensive training on text-audio pairs.

\textbf{Fine-tuning Approaches} adapt pre-trained models through targeted optimization.~\citet{plitsis2024investigating} demonstrate techniques adapted from image editing, such as DreamBooth~\cite{ruiz2023dreambooth} for audio personalization. Instruct-MusicGen~\cite{zhang2024instructmusicgen} exemplifies this category by fine-tuning the pre-trained MusicGen model with an instruction-following strategy. While requiring less training data, these methods remain computationally intensive. 

\textbf{Zero-shot Editing Methods} manipulate music without additional training. MusicMagus~\cite{magus} uses cross-attention constraint and word swapping but requires specific keywords describing the original music to find editing directions. MEDIC~\cite{liu2024medic} unifies mutual self-attention control and cross-attention control. Yet, these methods focus on manipulating attention mechanisms without providing interpretability insights into how these mechanisms function in music diffusion models. Additionally, existing methods including DDPM Friendly~\cite{manor2024ddpm} and SDEdit~\cite{meng2021sdedit} 
lack explicit structure guidance from the original music, making it difficult to preserve temporal structure during editing.","\subsection{Text-to-music generation}
Text-to-music generation follows two main paradigms: autoregressive (AR) models and diffusion-based models. AR models like MusicLM~\cite{agostinelli2023musiclm} introduced a music-text embedding space, while MusicGen~\cite{musicgen} improved controllability through text and melody guidance. Diffusion-based models transform noise into musical structures through iterative denoising. AudioLDM~\cite{audioldm} applied latent diffusion to text-conditioned generation, while Mousai~\cite{mousai} and MusicLDM~\cite{chen2024musicldm} further advanced quality and length capabilities. Recently, AudioLDM 2~\cite{audioldm2} improved musical structure representation, MelodyFlow~\cite{melodyflow} advances the field with flow matching techniques, and Stable Audio Open~\cite{evans2025stableaudioopen} scales diffusion transformers to generate extended stereo music. As these models have made significant progress in music generation, researchers have expanded their focus to music editing with its unique challenges.

\subsection{Text-to-music editing}

\textbf{Specialized Training Methods}. MusicLM~\cite{agostinelli2023musiclm} utilizes MuLan~\cite{huang2022mulan} embedding space for style editing, while MusicGen~\cite{musicgen} facilitates editing by conditioning generation on an original audio's chromagram with text prompts for desired changes. These two models offer limited editing capabilities as a secondary function. And models like AUDIT~\cite{wang2023audit} and InstructME~\cite{han2023instructme} train diffusion models specifically for inter-stem editing. While effective, these methods require extensive training on text-audio pairs.

\textbf{Fine-tuning Approaches} adapt pre-trained models through targeted optimization.~\citet{plitsis2024investigating} demonstrate techniques adapted from image editing, such as DreamBooth~\cite{ruiz2023dreambooth} for audio personalization. Instruct-MusicGen~\cite{zhang2024instructmusicgen} exemplifies this category by fine-tuning the pre-trained MusicGen model with an instruction-following strategy. While requiring less training data, these methods remain computationally intensive. 

\textbf{Zero-shot Editing Methods} manipulate music without additional training. MusicMagus~\cite{magus} uses cross-attention constraint and word swapping but requires specific keywords describing the original music to find editing directions. MEDIC~\cite{liu2024medic} unifies mutual self-attention control and cross-attention control. Yet, these methods focus on manipulating attention mechanisms without providing interpretability insights into how these mechanisms function in music diffusion models. Additionally, existing methods including DDPM Friendly~\cite{manor2024ddpm} and SDEdit~\cite{meng2021sdedit} 
lack explicit structure guidance from the original music, making it difficult to preserve temporal structure during editing.","Text-to-music generation
Text-to-music generation follows two main paradigms: au-
toregressive (AR) models and diffusion-based models. AR
models like MusicLM (Agostinelli et al. 2023) introduced
a music-text embedding space, while MusicGen (Copet
et al. 2023) improved controllability through text and
melody guidance. Diffusion-based models transform noise
into musical structures through iterative denoising. Audi-
oLDM (Liu et al. 2023) applied latent diffusion to text-
conditioned generation, while Mousai (Schneider et al.
2023) and MusicLDM (Chen et al. 2024) further advanced
quality and length capabilities. Recently, AudioLDM 2 (Liu
et al. 2024c) improved musical structure representation,MelodyFlow (Lan et al. 2024) advances the field with
flow matching techniques, and Stable Audio Open (Evans
et al. 2025) scales diffusion transformers to generate ex-
tended stereo music. As these models have made significant
progress in music generation, researchers have expanded
their focus to music editing with its unique challenges.
Text-to-music editing
Specialized Training Methods. MusicLM (Agostinelli
et al. 2023) utilizes MuLan (Huang et al. 2022) embed-
ding space for style editing, while MusicGen (Copet et al.
2023) facilitates editing by conditioning generation on an
original audio’s chromagram with text prompts for desired
changes. These two models offer limited editing capabili-
ties as a secondary function. And models like AUDIT (Wang
et al. 2023) and InstructME (Han et al. 2023) train diffusion
models specifically for inter-stem editing. While effective,
these methods require extensive training on text-audio pairs.
Fine-tuning Approachesadapt pre-trained models
through targeted optimization. Plitsis et al. (2024) demon-
strate techniques adapted from image editing, such as
DreamBooth (Ruiz et al. 2023) for audio personalization.
Instruct-MusicGen (Zhang et al. 2024a) exemplifies this cat-
egory by fine-tuning the pre-trained MusicGen model with
an instruction-following strategy. While requiring less train-
ing data, these methods remain computationally intensive.
Zero-shot Editing Methodsmanipulate music without
additional training. MusicMagus (Zhang et al. 2024b) uses
cross-attention constraint and word swapping but requires
specific keywords describing the original music to find edit-
ing directions. MEDIC (Liu et al. 2024b) unifies mutual
self-attention control and cross-attention control. Yet, these
methods focus on manipulating attention mechanisms with-
out providing interpretability insights into how these mech-
anisms function in music diffusion models. Additionally,
existing methods including DDPM Friendly (Manor and
Michaeli 2024) and SDEdit (Meng et al. 2021) lack explicit
structure guidance from the original music, making it diffi-
cult to preserve temporal structure during editing."
2511.08258v1,http://arxiv.org/abs/2511.08258v1,2025-11-11 13:53:07+00:00,Top2Ground: A Height-Aware Dual Conditioning Diffusion Model for Robust Aerial-to-Ground View Generation,"Generating ground-level images from aerial views is a challenging task due to extreme viewpoint disparity, occlusions, and a limited field of view. We introduce Top2Ground, a novel diffusion-based method that directly generates photorealistic ground-view images from aerial input images without relying on intermediate representations such as depth maps or 3D voxels. Specifically, we condition the denoising process on a joint representation of VAE-encoded spatial features (derived from aerial RGB images and an estimated height map) and CLIP-based semantic embeddings. This design ensures the generation is both geometrically constrained by the scene's 3D structure and semantically consistent with its content. We evaluate Top2Ground on three diverse datasets: CVUSA, CVACT, and the Auto Arborist. Our approach shows 7.3% average improvement in SSIM across three benchmark datasets, showing Top2Ground can robustly handle both wide and narrow fields of view, highlighting its strong generalization capabilities.","\label{sec:relatedworks}
\noindent\textbf{Diffusion-based image generation}
has become the state-of-the-art in generative modeling by synthesizing high-fidelity images through iterative denoising~\cite{ho2020denoising}. Score-based models~\cite{song2021scorebased}, guided sampling~\cite{dhariwal2021diffusion}, and latent diffusion~\cite{rombach2022high} have improved both sample quality and training efficiency. Beyond unconditional generation, recent approaches incorporate various conditioning mechanisms. Text-guided models such as GLIDE~\cite{nichol2021glide}, Imagen~\cite{saharia2022photorealistic}, and DiffusionCLIP~\cite{kim2022diffusionclip} leverage language embeddings to control semantic outputs. InstructPix2Pix~\cite{instructpix2pix} refines text-to-image alignment through paired image editing. More structured conditioning has been explored via ControlNet~\cite{controlnet}, which adds learnable branches for edge maps and segmentation masks, and Tree-D Fusion~\cite{treedfusion}, which integrates spatial priors for scene-aware control. These advances demonstrate the capacity of diffusion models for flexible and photorealistic image synthesis under various conditioning modalities.

\noindent\textbf{Aerial-to-ground image synthesis.}
Cross-view image synthesis aims to generate ground-level views from aerial inputs, a task complicated by extreme viewpoint changes, occlusions, and limited field-of-view overlap. Early approaches employed CNN-based~\cite{zhai2017predicting, li2021sat2vid} or GAN-based~\cite{regmi2018cross, tang2019multi, lu2020geometry} architectures to learn direct mappings, but they often fail to preserve spatial consistency and fine details. Geometry-guided models have introduced intermediate representations, such as height maps~\cite{shi2022geometry}, density fields~\cite{sat2density}, or voxel reconstructions~\cite{crossviewdiff}, to improve fidelity. However, these methods require accurate geometric priors or computationally expensive estimation, and often struggle to model high-level semantics or generalize across diverse viewpoints. We also note that NeRF-based view synthesis methods are not directly comparable, as they typically require multiple input views, whereas our task is single-image synthesis.


\name departs from prior work by removing the dependency on intermediate geometric representations. Instead, we propose a dual-conditioning latent diffusion framework that jointly uses a CLIP-based semantic encoder and a VAE-based visual encoder, which operates on both aerial RGB images and their estimated height maps. This enables our model to generate ground-level views that are both structurally coherent and semantically aligned, without requiring 3D voxels, density estimation, or depth supervision.

Notably, prior works have focused mainly on panoramic imagery with wide aerial fields of view. To the best of our knowledge, \name\ is the first framework to demonstrate robust generalization across both wide-FOV (CVUSA, CVACT) and narrow-FOV (Auto Arborist Dataset) aerial imagery,  within a unified architecture.","\noindent\textbf{Diffusion-based image generation}
has become the state-of-the-art in generative modeling by synthesizing high-fidelity images through iterative denoising~\cite{ho2020denoising}. Score-based models~\cite{song2021scorebased}, guided sampling~\cite{dhariwal2021diffusion}, and latent diffusion~\cite{rombach2022high} have improved both sample quality and training efficiency. Beyond unconditional generation, recent approaches incorporate various conditioning mechanisms. Text-guided models such as GLIDE~\cite{nichol2021glide}, Imagen~\cite{saharia2022photorealistic}, and DiffusionCLIP~\cite{kim2022diffusionclip} leverage language embeddings to control semantic outputs. InstructPix2Pix~\cite{instructpix2pix} refines text-to-image alignment through paired image editing. More structured conditioning has been explored via ControlNet~\cite{controlnet}, which adds learnable branches for edge maps and segmentation masks, and Tree-D Fusion~\cite{treedfusion}, which integrates spatial priors for scene-aware control. These advances demonstrate the capacity of diffusion models for flexible and photorealistic image synthesis under various conditioning modalities.

\noindent\textbf{Aerial-to-ground image synthesis.}
Cross-view image synthesis aims to generate ground-level views from aerial inputs, a task complicated by extreme viewpoint changes, occlusions, and limited field-of-view overlap. Early approaches employed CNN-based~\cite{zhai2017predicting, li2021sat2vid} or GAN-based~\cite{regmi2018cross, tang2019multi, lu2020geometry} architectures to learn direct mappings, but they often fail to preserve spatial consistency and fine details. Geometry-guided models have introduced intermediate representations, such as height maps~\cite{shi2022geometry}, density fields~\cite{sat2density}, or voxel reconstructions~\cite{crossviewdiff}, to improve fidelity. However, these methods require accurate geometric priors or computationally expensive estimation, and often struggle to model high-level semantics or generalize across diverse viewpoints. We also note that NeRF-based view synthesis methods are not directly comparable, as they typically require multiple input views, whereas our task is single-image synthesis.


\name departs from prior work by removing the dependency on intermediate geometric representations. Instead, we propose a dual-conditioning latent diffusion framework that jointly uses a CLIP-based semantic encoder and a VAE-based visual encoder, which operates on both aerial RGB images and their estimated height maps. This enables our model to generate ground-level views that are both structurally coherent and semantically aligned, without requiring 3D voxels, density estimation, or depth supervision.

Notably, prior works have focused mainly on panoramic imagery with wide aerial fields of view. To the best of our knowledge, \name\ is the first framework to demonstrate robust generalization across both wide-FOV (CVUSA, CVACT) and narrow-FOV (Auto Arborist Dataset) aerial imagery,  within a unified architecture.","Diffusion-based image generationhas become the state-
of-the-art in generative modeling by synthesizing high-
fidelity images through iterative denoising [9]. Score-based
models [29], guided sampling [6], and latent diffusion [25]
have improved both sample quality and training efficiency.
Beyond unconditional generation, recent approaches in-
corporate various conditioning mechanisms. Text-guided
models such as GLIDE [19], Imagen [26], and Diffusion-
CLIP [10] leverage language embeddings to control seman-
tic outputs. InstructPix2Pix [3] refines text-to-image align-
ment through paired image editing. More structured condi-
tioning has been explored via ControlNet [36], which adds
learnable branches for edge maps and segmentation masks,
and Tree-D Fusion [11], which integrates spatial priors for
scene-aware control. These advances demonstrate the ca-
pacity of diffusion models for flexible and photorealistic
image synthesis under various conditioning modalities.
Aerial-to-ground image synthesis.Cross-view image syn-
thesis aims to generate ground-level views from aerial in-puts, a task complicated by extreme viewpoint changes,
occlusions, and limited field-of-view overlap. Early ap-
proaches employed CNN-based [13, 35] or GAN-based [18,
24, 30] architectures to learn direct mappings, but they
often fail to preserve spatial consistency and fine details.
Geometry-guided models have introduced intermediate rep-
resentations, such as height maps [28], density fields [21],
or voxel reconstructions [5], to improve fidelity. How-
ever, these methods require accurate geometric priors or
computationally expensive estimation, and often struggle
to model high-level semantics or generalize across diverse
viewpoints. We also note that NeRF-based view synthesis
methods are not directly comparable, as they typically re-
quire multiple input views, whereas our task is single-image
synthesis.
Top2Grounddeparts from prior work by removing the
dependency on intermediate geometric representations. In-
stead, we propose a dual-conditioning latent diffusion
framework that jointly uses a CLIP-based semantic encoder
and a V AE-based visual encoder, which operates on both
aerial RGB images and their estimated height maps. This
enables our model to generate ground-level views that are
both structurally coherent and semantically aligned, with-
out requiring 3D voxels, density estimation, or depth super-
vision.
Notably, prior works have focused mainly on panoramic
imagery with wide aerial fields of view. To the best
of our knowledge,Top2Groundis the first framework
to demonstrate robust generalization across both wide-
FOV (CVUSA, CV ACT) and narrow-FOV (Auto Arborist
Dataset) aerial imagery, within a unified architecture."
2511.08345v1,http://arxiv.org/abs/2511.08345v1,2025-11-11 15:23:56+00:00,Revisiting Network Traffic Analysis: Compatible network flows for ML models,"To ensure that Machine Learning (ML) models can perform a robust detection and classification of cyberattacks, it is essential to train them with high-quality datasets with relevant features. However, it can be difficult to accurately represent the complex traffic patterns of an attack, especially in Internet-of-Things (IoT) networks. This paper studies the impact that seemingly similar features created by different network traffic flow exporters can have on the generalization and robustness of ML models. In addition to the original CSV files of the Bot-IoT, IoT-23, and CICIoT23 datasets, the raw network packets of their PCAP files were analysed with the HERA tool, generating new labelled flows and extracting consistent features for new CSV versions. To assess the usefulness of these new flows for intrusion detection, they were compared with the original versions and were used to fine-tune multiple models. Overall, the results indicate that directly analysing and preprocessing PCAP files, instead of just using the commonly available CSV files, enables the computation of more relevant features to train bagging and gradient boosting decision tree ensembles. It is important to continue improving feature extraction and feature selection processes to make different datasets more compatible and enable a trustworthy evaluation and comparison of the ML models used in cybersecurity solutions.","In recent years, there has been a growing focus on the reliability and usability of NTA datasets for the training and validation of ML models. As the cyberattacks targeting modern IoT networks started becoming more complex, researchers started identifying limitations in how existing datasets represented that complex behaviour in a tabular data format~\cite{Thakkar2021}. Therefore, it is important to understand the conclusions and recommendations of previous work.

Some well-established studies have carefully analyzed the NTA datasets that were publicly available at the time. Kenyon et al.~\cite{kenyon_are_2020} and Ring et al.~\cite{RING2019} considered several factors, such as the quality and relevance of the captured cyberattacks, the methodology used to create and capture the traffic patterns, and the maintenance to keep it up-to-date. These studies revealed that most datasets suffer from over-summarization and have an unclear representation of benign and malicious traffic characteristics. These issues, combined with a lack of standardized methodologies for network traffic simulation, often cause the generated network flows to be incomplete and have inconsistent labels~\cite{Adeleke2022}.

Furthermore, most public NTA datasets provide a preprocessed version that loses relevant information. Even though the raw traffic is captured in PCAP files with the same format, the features made available by the original authors of each dataset are often specific to that work, and it is not specified how they were computed~\cite{rodriguez_evaluation_2022}. This results in CSV files with seemingly similar features, but that actually represent different characteristics and have missing or incompatible data, which limits the ability to combine data from different origins~\cite{GUERRA2022}.

Despite ongoing efforts to systematise the feature sets used for NTA, it is still difficult to identify which features best represent complex attack scenarios. In more recent studies, Sarhan et al.~\cite{sarhan_towards_2022} and Silva et al.~\cite{Silva2024}, evaluated different NTA datasets, including Bot-IoT and IoT-23 focused on IoT networks. Several experiments were performed to select the most relevant features for ML models to keep a good generalization while providing a more computationally efficient detection. The resulting feature sets contained mostly time-related features of active transmission rates and interpacket arrival times, but some complex cyberattack classes require the use of quantity-related features, such as sent and received bytes and packets~\cite{Vitorino2024}.

Overall, the recent literature highlights the need to perform feature engineering to better represent the characteristics and traffic patterns of more complex cyberattacks. To the best of our knowledge, no previous work has analyzed the effect that extracting new network traffic flow features from the raw network packets of the Bot-IoT, IoT-23, and CICIoT23 datasets has on the performance of ML models for multiclass cyberattack classification in IoT networks.","In recent years, there has been a growing focus on the reliability and usability of NTA datasets for the training and validation of ML models. As the cyberattacks targeting modern IoT networks started becoming more complex, researchers started identifying limitations in how existing datasets represented that complex behaviour in a tabular data format~\cite{Thakkar2021}. Therefore, it is important to understand the conclusions and recommendations of previous work.

Some well-established studies have carefully analyzed the NTA datasets that were publicly available at the time. Kenyon et al.~\cite{kenyon_are_2020} and Ring et al.~\cite{RING2019} considered several factors, such as the quality and relevance of the captured cyberattacks, the methodology used to create and capture the traffic patterns, and the maintenance to keep it up-to-date. These studies revealed that most datasets suffer from over-summarization and have an unclear representation of benign and malicious traffic characteristics. These issues, combined with a lack of standardized methodologies for network traffic simulation, often cause the generated network flows to be incomplete and have inconsistent labels~\cite{Adeleke2022}.

Furthermore, most public NTA datasets provide a preprocessed version that loses relevant information. Even though the raw traffic is captured in PCAP files with the same format, the features made available by the original authors of each dataset are often specific to that work, and it is not specified how they were computed~\cite{rodriguez_evaluation_2022}. This results in CSV files with seemingly similar features, but that actually represent different characteristics and have missing or incompatible data, which limits the ability to combine data from different origins~\cite{GUERRA2022}.

Despite ongoing efforts to systematise the feature sets used for NTA, it is still difficult to identify which features best represent complex attack scenarios. In more recent studies, Sarhan et al.~\cite{sarhan_towards_2022} and Silva et al.~\cite{Silva2024}, evaluated different NTA datasets, including Bot-IoT and IoT-23 focused on IoT networks. Several experiments were performed to select the most relevant features for ML models to keep a good generalization while providing a more computationally efficient detection. The resulting feature sets contained mostly time-related features of active transmission rates and interpacket arrival times, but some complex cyberattack classes require the use of quantity-related features, such as sent and received bytes and packets~\cite{Vitorino2024}.

Overall, the recent literature highlights the need to perform feature engineering to better represent the characteristics and traffic patterns of more complex cyberattacks. To the best of our knowledge, no previous work has analyzed the effect that extracting new network traffic flow features from the raw network packets of the Bot-IoT, IoT-23, and CICIoT23 datasets has on the performance of ML models for multiclass cyberattack classification in IoT networks.","In recent years, there has been a growing focus on the reliability and usability of
NTA datasets for the training and validation of ML models. As the cyberattacks
targeting modern IoT networks started becoming more complex, researchers
started identifying limitations in how existing datasets represented that complex
behaviour in a tabular data format [17]. Therefore, it is important to understand
the conclusions and recommendations of previous work.
Some well-established studies have carefully analyzed the NTA datasets that
were publicly available at the time. Kenyon et al. [7] and Ring et al. [12] consid-
ered several factors, such as the quality and relevance of the captured cyberat-
tacks, the methodology used to create and capture the traffic patterns, and the
maintenance to keep it up-to-date. These studies revealed that most datasets
suffer from over-summarization and have an unclear representation of benign
and malicious traffic characteristics. These issues, combined with a lack of stan-
Revisiting Network Traffic Analysis 3
dardized methodologies for network traffic simulation, often cause the generated
network flows to be incomplete and have inconsistent labels [1].
Furthermore, most public NTA datasets provide a preprocessed version that
loses relevant information. Even though the raw traffic is captured in PCAP
files with the same format, the features made available by the original authors
of each dataset are often specific to that work, and it is not specified how they
werecomputed[13].ThisresultsinCSVfileswithseeminglysimilarfeatures,but
that actually represent different characteristics and have missing or incompatible
data, which limits the ability to combine data from different origins [5].
Despite ongoing efforts to systematise the feature sets used for NTA, it is
still difficult to identify which features best represent complex attack scenarios.
In more recent studies, Sarhan et al. [14] and Silva et al. [15], evaluated different
NTA datasets, including Bot-IoT and IoT-23 focused on IoT networks. Several
experiments were performed to select the most relevant features for ML models
to keep a good generalization while providing a more computationally efficient
detection. The resulting feature sets contained mostly time-related features of
active transmission rates and interpacket arrival times, but some complex cy-
berattack classes require the use of quantity-related features, such as sent and
received bytes and packets [20].
Overall, the recent literature highlights the need to perform feature engineer-
ing to better represent the characteristics and traffic patterns of more complex
cyberattacks. To the best of our knowledge, no previous work has analyzed the
effect that extracting new network traffic flow features from the raw network
packets of the Bot-IoT, IoT-23, and CICIoT23 datasets has on the performance
of ML models for multiclass cyberattack classification in IoT networks."
2511.10376v1,http://arxiv.org/abs/2511.10376v1,2025-11-13 14:51:21+00:00,MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,"Embodied navigation is a fundamental capability for robotic agents operating. Real-world deployment requires open vocabulary generalization and low training overhead, motivating zero-shot methods rather than task-specific RL training. However, existing zero-shot methods that build explicit 3D scene graphs often compress rich visual observations into text-only relations, leading to high construction cost, irreversible loss of visual evidence, and constrained vocabularies. To address these limitations, we introduce the Multi-modal 3D Scene Graph (M3DSG), which preserves visual cues by replacing textual relational edges with dynamically assigned images. Built on M3DSG, we propose MSGNav, a zero-shot navigation system that includes a Key Subgraph Selection module for efficient reasoning, an Adaptive Vocabulary Update module for open vocabulary support, and a Closed-Loop Reasoning module for accurate exploration reasoning. Additionally, we further identify the last-mile problem in zero-shot navigation - determining the feasible target location with a suitable final viewpoint, and propose a Visibility-based Viewpoint Decision module to explicitly resolve it. Comprehensive experimental results demonstrate that MSGNav achieves state-of-the-art performance on GOAT-Bench and HM3D-OVON datasets. The open-source code will be publicly available.","\subsection{Graph-based Scene Exploration}
Graph representations \cite{Conceptgraphs,related_14,related_33} provide explicit spatial–semantic structures for scene reasoning and integrate well with LLMs/VLMs. Examples include SayPlan \cite{related_32} for 3D scene graph planning, OVSG \cite{related_05} for open-vocabulary grounding, imaginative world modeling \cite{hu2025imaginative}, and BEV graph policies \cite{luo2025learning}. Graph reasoning has also been applied in navigation, e.g. SG-Nav \cite{sg-nav} for hierarchical prompting and graph-retained adaptation \cite{hong2025general} for cross-domain VLN. Yet, the overly abstract textual edges of current scene graphs limit the comprehensive scene context understanding, motivating our multi-modal 3D scene graph (M3DSG).


\subsection{Zero-Shot Navigation}

Previous embodied navigation methods, primarily based on reinforcement learning (RL) \cite{rlnav1, rlnav2}, suffer from limited generalization and a significant sim-to-real gap, which hinders their application in real-world environments. Zero-shot navigation allows agents to act in unseen environments without task-specific training, unlike supervised methods \cite{related_11,related_18,related_31,related_41} which require large-scale simulation. Existing zero-shot navigation methods span object-goal (ON), image-goal (IIN), and text-goal (TN). ON methods include CoW \cite{related_12} with CLIP \cite{related_28} frontier exploration, ESC \cite{related_49}, OpenFMNav \cite{related_08}, VLFM \cite{vlfm} using LLM reasoning, UniGoal \cite{unigoal} for multi-modal unification, and RATE-Nav \cite{li2025rate} with region-aware termination. IIN methods range from Mod-IIN \cite{imggoal2} to SIGN \cite{yan2025sign} for safety-aware drone navigation. TN methods such as InstructNav \cite{related_25} generalize instruction following across ON, DDN, and VLN \cite{related_09,related_20,related_26,related_47,related_48}. However, most zero-shot work ignores the last-mile challenge of choosing an optimal viewpoint after target localization.","\subsection{Graph-based Scene Exploration}
Graph representations \cite{Conceptgraphs,related_14,related_33} provide explicit spatial–semantic structures for scene reasoning and integrate well with LLMs/VLMs. Examples include SayPlan \cite{related_32} for 3D scene graph planning, OVSG \cite{related_05} for open-vocabulary grounding, imaginative world modeling \cite{hu2025imaginative}, and BEV graph policies \cite{luo2025learning}. Graph reasoning has also been applied in navigation, e.g. SG-Nav \cite{sg-nav} for hierarchical prompting and graph-retained adaptation \cite{hong2025general} for cross-domain VLN. Yet, the overly abstract textual edges of current scene graphs limit the comprehensive scene context understanding, motivating our multi-modal 3D scene graph (M3DSG).


\subsection{Zero-Shot Navigation}

Previous embodied navigation methods, primarily based on reinforcement learning (RL) \cite{rlnav1, rlnav2}, suffer from limited generalization and a significant sim-to-real gap, which hinders their application in real-world environments. Zero-shot navigation allows agents to act in unseen environments without task-specific training, unlike supervised methods \cite{related_11,related_18,related_31,related_41} which require large-scale simulation. Existing zero-shot navigation methods span object-goal (ON), image-goal (IIN), and text-goal (TN). ON methods include CoW \cite{related_12} with CLIP \cite{related_28} frontier exploration, ESC \cite{related_49}, OpenFMNav \cite{related_08}, VLFM \cite{vlfm} using LLM reasoning, UniGoal \cite{unigoal} for multi-modal unification, and RATE-Nav \cite{li2025rate} with region-aware termination. IIN methods range from Mod-IIN \cite{imggoal2} to SIGN \cite{yan2025sign} for safety-aware drone navigation. TN methods such as InstructNav \cite{related_25} generalize instruction following across ON, DDN, and VLN \cite{related_09,related_20,related_26,related_47,related_48}. However, most zero-shot work ignores the last-mile challenge of choosing an optimal viewpoint after target localization.",
2511.10047v1,http://arxiv.org/abs/2511.10047v1,2025-11-13 07:47:37+00:00,MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,"Zero-shot anomaly classification (AC) and segmentation (AS) methods aim to identify and outline defects without using any labeled samples. In this paper, we reveal a key property that is overlooked by existing methods: normal image patches across industrial products typically find many other similar patches, not only in 2D appearance but also in 3D shapes, while anomalies remain diverse and isolated. To explicitly leverage this discriminative property, we propose a Mutual Scoring framework (MuSc-V2) for zero-shot AC/AS, which flexibly supports single 2D/3D or multimodality. Specifically, our method begins by improving 3D representation through Iterative Point Grouping (IPG), which reduces false positives from discontinuous surfaces. Then we use Similarity Neighborhood Aggregation with Multi-Degrees (SNAMD) to fuse 2D/3D neighborhood cues into more discriminative multi-scale patch features for mutual scoring. The core comprises a Mutual Scoring Mechanism (MSM) that lets samples within each modality to assign score to each other, and Cross-modal Anomaly Enhancement (CAE) that fuses 2D and 3D scores to recover modality-specific missing anomalies. Finally, Re-scoring with Constrained Neighborhood (RsCon) suppresses false classification based on similarity to more representative samples. Our framework flexibly works on both the full dataset and smaller subsets with consistently robust performance, ensuring seamless adaptability across diverse product lines. In aid of the novel framework, MuSc-V2 achieves significant performance improvements: a $\textbf{+23.7\%}$ AP gain on the MVTec 3D-AD dataset and a $\textbf{+19.3\%}$ boost on the Eyecandies dataset, surpassing previous zero-shot benchmarks and even outperforming most few-shot methods. The code will be available at The code will be available at \href{https://github.com/HUST-SLOW/MuSc-V2}{https://github.com/HUST-SLOW/MuSc-V2}.","\subsection{Transformer architecture for 2D and 3D representation}
Vision transformer (ViT) \cite{ICLR2021ViT} and point transformer (PT) \cite{ICCV2021pointTransformer} have become standard for 2D and 3D feature representation.
Some pre-trained models like CLIP \cite{ICML2021CLIP}/DINO \cite{iccv2021dino} for 2D modal and Point-MAE \cite{eccv2022pointmae}/Point-BERT \cite{cvpr2022pointbert} for 3D modal deliver high-quality patch features.
However, these features often struggle with industrial anomalies of varying sizes.
Swin transformer \cite{iccv2021swin,pami2022vlt} proposes the varied-size window attention to compute attention within multi-scale windows, which risks compromising fine-grained anomaly discrimination.
We propose a training-free solution SNAMD, that optimizes patch features via Similarity-Weighted Pooling to better capture multi-scale anomalies and preserve small anomalies.

\subsection{Point cloud grouping}
To reduce computational costs of self-attention, existing 3D feature extractors \cite{cvpr2017pointnet, tpami2023Flatteningnet, ICCV2021pointTransformer, pamivote2cap,cvm2021pct, pami2024genvcl} preprocess point clouds through FPS and KNN grouping,
encoding each group as a 3D patch token.
However, these methods risk merging multiple surfaces within a single group, particularly when components are spatially proximate, resulting in deviant tokens that trigger false positives.
Point Transformer V2/V3 \cite{nips2022pointtranformerv2, cvpr2024pointtranformerv3} propose new grouping strategies, but they focus more on optimizing speed and overlook this fundamental issue.
We propose an Iterative Point Grouping strategy to address this challenge by ensuring surface-consistent groupings for more robust 3D feature extraction.

\begin{figure*}[t]
\vspace{-.1in}
\begin{center}
\includegraphics[width=1\textwidth]{Figures/pipeline.pdf}
\vspace{-5mm} 
\caption{
\textbf{The pipeline of our MuSc-V2.}
This framework processes 2D images and 3D point clouds through four important innovations:
(1) IPG replaces the current grouping strategy in the point transformer to generate groups with continuous surfaces (Sec. \ref{sec:2d_feature}).
(2) SNAMD improves the abnormal modeling ability with varying sizes for both modals (Sec. \ref{sec:3d_feature}).
(3) MSM obtains anomaly segmentation results of 2D/3D modals. CAE enhances scores of anomalies if both modals are available (Sec. \ref{sec:m3sm}).
(4) RsCon reduces false anomaly classification from local noise and weak anomalies (Sec. \ref{sec:rscon}).
}
\label{fig:pipeline}
\end{center}
\vspace{-10pt}
\end{figure*}


\subsection{Zero-shot anomaly classification and segmentation}
In the industrial vision field, zero-shot AC/AS has garnered more attention.
However, most methods focus on 2D modal by image-text alignment of CLIP \cite{ICML2021CLIP}.
These CLIP-based approaches \cite{CVPR2023winclip, eccv2024adaclip, eccv2024vcpclip, wacv2024promptad, arxiv2023APRILGAN, mm2024filo} fine-tune image encoder or text encoder to bridge the domain gap between natural and industrial scenarios.
Meanwhile, some zero-shot methods \cite{wacv2023utad, nips2023ACR, iclr2024musc} focus on detecting anomalies by using the unlabeled samples itself.
\cite{wacv2023utad} explores the relationship between patches inside one unlabeled image, but only handles texture products.
ACR \cite{nips2023ACR} proposes a new adaptation strategy without human involvement, which trains the network by other products from the dataset.
For the 3D zero-shot task, PointAD \cite{nips2024pointad} renders the point cloud to multiple images with different view angles.
In this way, 2D methods could be used to process 3D data.
In this paper, we propose a mutual scoring mechanism for both 2D/3D modals, which only uses the unlabeled samples without additional fine-tuning.

\subsection{Multimodal anomaly classification and segmentation}
Multimodal industrial AC/AS task aims to identify defects in the industrial product through its 2D image and 3D point cloud.
Some methods \cite{eccv2025r3d,cvpr2024anomaly-shapenet,mm2023easynet,cvpr2023btf} are proposed for the unsupervised (full-shot) AC/AS task.
Among them, M3DM \cite{cvpr2023m3dm} fine-tunes features of two modals by contrastive learning for cross-modal alignment.
Shape-guided \cite{icml2023shape-guided} use 2D features stored in the memory bank to reconstruct 3D features to guide the identification of 2D anomalies.
In addition, CFM \cite{cvpr2024cfm} implements cross-modal feature reconstruction according to the student-teacher network, greatly reducing time consumption.
These methods require collecting a large number of normal images and point clouds for model training,
which limits the migration ability on the new production line.
We propose the multimodal mutual scoring without any training and labeled samples.
The CAE module is inserted into our mutual scoring mechanism to eliminate the blind spot of single model.

\subsection{Manifold learning}
In high-dimensional manifolds, Euclidean properties are only preserved in local space, making direct distance calculations unreliable.
Some manifold learning methods \cite{nips2003RDM,pami2008riemannian, CVPR2012sd,pami2011adaptml} construct an embedding space where distances align with the underlying manifold structure.
Inspired by the above principles, we develop the RsCon module to refine pixel-level anomaly classification in manifolds.","\subsection{Transformer architecture for 2D and 3D representation}
Vision transformer (ViT) \cite{ICLR2021ViT} and point transformer (PT) \cite{ICCV2021pointTransformer} have become standard for 2D and 3D feature representation.
Some pre-trained models like CLIP \cite{ICML2021CLIP}/DINO \cite{iccv2021dino} for 2D modal and Point-MAE \cite{eccv2022pointmae}/Point-BERT \cite{cvpr2022pointbert} for 3D modal deliver high-quality patch features.
However, these features often struggle with industrial anomalies of varying sizes.
Swin transformer \cite{iccv2021swin,pami2022vlt} proposes the varied-size window attention to compute attention within multi-scale windows, which risks compromising fine-grained anomaly discrimination.
We propose a training-free solution SNAMD, that optimizes patch features via Similarity-Weighted Pooling to better capture multi-scale anomalies and preserve small anomalies.

\subsection{Point cloud grouping}
To reduce computational costs of self-attention, existing 3D feature extractors \cite{cvpr2017pointnet, tpami2023Flatteningnet, ICCV2021pointTransformer, pamivote2cap,cvm2021pct, pami2024genvcl} preprocess point clouds through FPS and KNN grouping,
encoding each group as a 3D patch token.
However, these methods risk merging multiple surfaces within a single group, particularly when components are spatially proximate, resulting in deviant tokens that trigger false positives.
Point Transformer V2/V3 \cite{nips2022pointtranformerv2, cvpr2024pointtranformerv3} propose new grouping strategies, but they focus more on optimizing speed and overlook this fundamental issue.
We propose an Iterative Point Grouping strategy to address this challenge by ensuring surface-consistent groupings for more robust 3D feature extraction.




\subsection{Zero-shot anomaly classification and segmentation}
In the industrial vision field, zero-shot AC/AS has garnered more attention.
However, most methods focus on 2D modal by image-text alignment of CLIP \cite{ICML2021CLIP}.
These CLIP-based approaches \cite{CVPR2023winclip, eccv2024adaclip, eccv2024vcpclip, wacv2024promptad, arxiv2023APRILGAN, mm2024filo} fine-tune image encoder or text encoder to bridge the domain gap between natural and industrial scenarios.
Meanwhile, some zero-shot methods \cite{wacv2023utad, nips2023ACR, iclr2024musc} focus on detecting anomalies by using the unlabeled samples itself.
\cite{wacv2023utad} explores the relationship between patches inside one unlabeled image, but only handles texture products.
ACR \cite{nips2023ACR} proposes a new adaptation strategy without human involvement, which trains the network by other products from the dataset.
For the 3D zero-shot task, PointAD \cite{nips2024pointad} renders the point cloud to multiple images with different view angles.
In this way, 2D methods could be used to process 3D data.
In this paper, we propose a mutual scoring mechanism for both 2D/3D modals, which only uses the unlabeled samples without additional fine-tuning.

\subsection{Multimodal anomaly classification and segmentation}
Multimodal industrial AC/AS task aims to identify defects in the industrial product through its 2D image and 3D point cloud.
Some methods \cite{eccv2025r3d,cvpr2024anomaly-shapenet,mm2023easynet,cvpr2023btf} are proposed for the unsupervised (full-shot) AC/AS task.
Among them, M3DM \cite{cvpr2023m3dm} fine-tunes features of two modals by contrastive learning for cross-modal alignment.
Shape-guided \cite{icml2023shape-guided} use 2D features stored in the memory bank to reconstruct 3D features to guide the identification of 2D anomalies.
In addition, CFM \cite{cvpr2024cfm} implements cross-modal feature reconstruction according to the student-teacher network, greatly reducing time consumption.
These methods require collecting a large number of normal images and point clouds for model training,
which limits the migration ability on the new production line.
We propose the multimodal mutual scoring without any training and labeled samples.
The CAE module is inserted into our mutual scoring mechanism to eliminate the blind spot of single model.

\subsection{Manifold learning}
In high-dimensional manifolds, Euclidean properties are only preserved in local space, making direct distance calculations unreliable.
Some manifold learning methods \cite{nips2003RDM,pami2008riemannian, CVPR2012sd,pami2011adaptml} construct an embedding space where distances align with the underlying manifold structure.
Inspired by the above principles, we develop the RsCon module to refine pixel-level anomaly classification in manifolds.",
2511.09737v1,http://arxiv.org/abs/2511.09737v1,2025-11-12 20:58:31+00:00,Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy,"Generalization to unseen environments is a significant challenge in the field of robotics and control. In this work, we focus on contextual reinforcement learning, where agents act within environments with varying contexts, such as self-driving cars or quadrupedal robots that need to operate in different terrains or weather conditions than they were trained for. We tackle the critical task of generalizing to out-of-distribution (OOD) settings, without access to explicit context information at test time. Recent work has addressed this problem by training a context encoder and a history adaptation module in separate stages. While promising, this two-phase approach is cumbersome to implement and train. We simplify the methodology and introduce SPARC: single-phase adaptation for robust control. We test SPARC on varying contexts within the high-fidelity racing simulator Gran Turismo 7 and wind-perturbed MuJoCo environments, and find that it achieves reliable and robust OOD generalization.","\label{sec:related_work}

Generalization to out-of-distribution environments is a fundamental challenge in reinforcement learning, hindering its deployment in real-world applications, particularly in robotics and control tasks \cite{kirk2023survey}. The learning dynamics of RL methods often struggle to adapt to novel environmental conditions \cite{lyle2022learning}. Contextual reinforcement learning \cite{langford2017,benjamins2021carl} provides a framework to address this problem by training agents capable of adapting to varying environmental factors.


\subsection{Contextual RL}

Robust RL often depends on effective contextual adaptation. 
Recent work has explored context-aware policies that integrate contextual cues into decision-making \cite{beukman2023dynamics, chen2021context, lahmer2024fast} or employ world models to capture environment dynamics \cite{lee2020context, prasanna2024dreaming}.
In addition, several studies have focused on modifying the environment itself—such as by varying gravity or adjusting agent component dimensions—to promote the development of more versatile controllers \cite{benjamins2021carl, leon2024duplex}.



\subsection{What if the Agent has No Access to Context?}

In many real-world scenarios, agents are deprived of explicit contextual information during deployment. In these cases, the agent must infer the relevant environmental factors indirectly. For instance, \citet{lee2020science} advanced robust legged locomotion by introducing a two-phase learning process. It first trains an expert policy, which includes a context encoder using the privileged contextual information. 
The second phase involves an adapter policy that tries to imitate the expert's action, while a history-based adaptation component aims to minimize the difference between its history encoding and the expert's context encoding.
Rapid Motor Adaptation \cite{kumar2021rma} refines this methodology by only imitating the context encoding, not the action. The adapter policy can be deployed, as it does not require access to the privileged context. 



\subsection{Other Techniques for Generalization}

Several complementary approaches have been proposed to enhance generalization. Domain randomization \cite{tobin2017domain, peng2018sim} and procedurally generated environments \cite{procgen, gisslen2021adversarial} introduce diversity during training, thereby encouraging robust policy behavior.
We employ domain randomization by default in our experiments.
System identification methods \cite{yu2017preparing}—whether performed explicitly or through implicit online adaptation, as in SPARC and RMA—also contribute to improved performance under varying conditions. 
Moreover, techniques such as data augmentation \cite{laskin2020reinforcement, hansen2021stabilizing, Wang_Wu_Hu_Wang_Lin_Lv_2024} and masking \cite{grooten2024madi, huang2022spectrum} have been shown to further enhance generalization, particularly for pixel-based inputs.



Meta-reinforcement learning offers an alternative paradigm for learning adaptable policies \cite{wang2016learning, rabinowitz2018machine, duan2016rl}.
Foundational algorithms like Model-Agnostic Meta-Learning (MAML) \cite{finn2017model} enable rapid task adaptation, and emerging methods using hypernetworks generate task-specific policy parameters on the fly \cite{beck2023hypernetworks, rezaei2023hypernetworks, beukman2023dynamics}.","Generalization to out-of-distribution environments is a fundamental challenge in reinforcement learning, hindering its deployment in real-world applications, particularly in robotics and control tasks \cite{kirk2023survey}. The learning dynamics of RL methods often struggle to adapt to novel environmental conditions \cite{lyle2022learning}. Contextual reinforcement learning \cite{langford2017,benjamins2021carl} provides a framework to address this problem by training agents capable of adapting to varying environmental factors.


\subsection{Contextual RL}

Robust RL often depends on effective contextual adaptation. 
Recent work has explored context-aware policies that integrate contextual cues into decision-making \cite{beukman2023dynamics, chen2021context, lahmer2024fast} or employ world models to capture environment dynamics \cite{lee2020context, prasanna2024dreaming}.
In addition, several studies have focused on modifying the environment itself—such as by varying gravity or adjusting agent component dimensions—to promote the development of more versatile controllers \cite{benjamins2021carl, leon2024duplex}.



\subsection{What if the Agent has No Access to Context?}

In many real-world scenarios, agents are deprived of explicit contextual information during deployment. In these cases, the agent must infer the relevant environmental factors indirectly. For instance, \citet{lee2020science} advanced robust legged locomotion by introducing a two-phase learning process. It first trains an expert policy, which includes a context encoder using the privileged contextual information. 
The second phase involves an adapter policy that tries to imitate the expert's action, while a history-based adaptation component aims to minimize the difference between its history encoding and the expert's context encoding.
Rapid Motor Adaptation \cite{kumar2021rma} refines this methodology by only imitating the context encoding, not the action. The adapter policy can be deployed, as it does not require access to the privileged context. 



\subsection{Other Techniques for Generalization}

Several complementary approaches have been proposed to enhance generalization. Domain randomization \cite{tobin2017domain, peng2018sim} and procedurally generated environments \cite{procgen, gisslen2021adversarial} introduce diversity during training, thereby encouraging robust policy behavior.
We employ domain randomization by default in our experiments.
System identification methods \cite{yu2017preparing}—whether performed explicitly or through implicit online adaptation, as in SPARC and RMA—also contribute to improved performance under varying conditions. 
Moreover, techniques such as data augmentation \cite{laskin2020reinforcement, hansen2021stabilizing, Wang_Wu_Hu_Wang_Lin_Lv_2024} and masking \cite{grooten2024madi, huang2022spectrum} have been shown to further enhance generalization, particularly for pixel-based inputs.



Meta-reinforcement learning offers an alternative paradigm for learning adaptable policies \cite{wang2016learning, rabinowitz2018machine, duan2016rl}.
Foundational algorithms like Model-Agnostic Meta-Learning (MAML) \cite{finn2017model} enable rapid task adaptation, and emerging methods using hypernetworks generate task-specific policy parameters on the fly \cite{beck2023hypernetworks, rezaei2023hypernetworks, beukman2023dynamics}.","Generalization to out-of-distribution environments is a fun-
damental challenge in reinforcement learning, hindering
its deployment in real-world applications, particularly in
robotics and control tasks (Kirk et al. 2023). The learningarXiv:2511.09737v1 [cs.LG]"
2511.08577v1,http://arxiv.org/abs/2511.08577v1,2025-11-11 18:57:02+00:00,Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models,"Improving reasoning capabilities of Large Language Models (LLMs), especially under parameter constraints, is crucial for real-world applications. Prior work proposes recurrent transformers, which allocate a fixed number of extra iterations per token to improve generation quality. After the first, standard forward pass, instead of verbalization, last-layer hidden states are fed back as inputs for additional iterations to refine token predictions. Yet we identify a latent overthinking phenomenon: easy token predictions that are already correct after the first pass are sometimes revised into errors in additional iterations. To address this, we propose Think-at-Hard (TaH), a dynamic latent thinking method that iterates deeper only at hard tokens. It employs a lightweight neural decider to trigger latent iterations only at tokens that are likely incorrect after the standard forward pass. During latent iterations, Low-Rank Adaptation (LoRA) modules shift the LLM objective from general next-token prediction to focused hard-token refinement. We further introduce a duo-causal attention mechanism that extends attention from the token sequence dimension to an additional iteration depth dimension. This enables cross-iteration information flow while maintaining full sequential parallelism. Experiments show that TaH boosts LLM reasoning performance across five challenging benchmarks while maintaining the same parameter count. Compared with baselines that iterate twice for all output tokens, TaH delivers 8.1-11.3% accuracy gains while exempting 94% of tokens from the second iteration. Against strong single-iteration Qwen3 models finetuned with the same data, it also delivers 4.0-5.0% accuracy gains. When allowing less than 3% additional parameters from LoRA and the iteration decider, the gains increase to 8.5-12.6% and 5.3-5.4%, respectively. Our code is available at https://github.com/thu-nics/TaH.","\label{sec:related_work}
Unlike standard LLMs that verbalize at every autoregressive step, latent thinking shifts part of generation away from explicit natural-language CoT in order to improve reasoning~\citep{li2025implicit}.

% type1: verbal space
\xhdr{Signal-guided Control}
These methods keep reasoning in token space but steers computation by inserting control tokens.
Early work shows that simple filler tokens (e.g., dots) can mimic some benefits of CoT~\citep{pfau2404let}. 
Building on this, later work expands the LLM vocabulary with \texttt{[PAUSE]} tokens and learns where to insert them for extra compute before predicting the next token~\citep{goyal2310think,kim2025learning}. 
They are lightweight and easily integrable, but constrained to the discrete-token interventions with limited latent controls.
% However, they remain discrete-token interventions; the internal computation is still implicit, with the usual opacity and reliability caveats of latent reasoning.

% type2: continuous space
\xhdr{Latent Optimization}
These methods perform autoregressive reasoning directly in internal representations, emitting little or no intermediate text.
They distill and compress CoT into latent continuous embeddings through various strategies.
Coconut and CCoT progressively replace text with latent thinking under final response supervision~\citep{hao2024training, cheng2024compressed}; 
Token assorted and HCoT compress CoT spans to embeddings with hidden-state alignment~\citep{su2025token, liu2024expediting}.
SoftThink directly applies logit-weighted embeddings for latent iterations~\citep{zhang2025soft}.
While offering efficiency gains and flexible control over hidden trajectories, these methods sacrifice reasoning interpretability, with training-based ones further requiring heavy mitigation from strong verbal LLMs.

% type3: mix
\xhdr{Recurrent Transformers}
These methods interleave latent and verbal reasoning, introducing latent iterations before each token verbalization.
After a standard forward pass, these methods feed latent states back as next-iteration inputs for a fixed number of iterations, then verbalize the output token.
Existing approaches differ in the formation of next-iteration input.
For example, Looped Transformer reuses last-layer hidden states directly~\citep{saunshi2025loopedTrans,geiping2025scaling}, whereas Ponder uses logit-weighted embeddings (Equation~\ref{eq:ours_update})~\citep{zeng2025pondering}.
Recurrent transformers combine advantages of visible reasoning trajectories with latent exploration.
By reusing the parameters across iterations, it achieves deeper computation per token without parameter increases.
However, the fixed depth burdens each iteration with both easy and hard tokens, potentially causing false corrections for already-correct predictions.

\xhdr{Positioning} 
\name belongs to the recurrent transformer family but extends this paradigm significantly.
It \emph{selectively} allocates latent iterations to \emph{refine hard tokens}, improving reasoning quality with specialized objectives across iterations.
While concurrent works~\citep{bae2025mor, zhu2025ouro} also enable selective recursion, they require complete model retraining.
\name instead leverages existing pre-trained models, adding depth-aware LoRA and duo-causal attention to improve reasoning with minimal finetuning overhead.","Unlike standard LLMs that verbalize at every autoregressive step, latent thinking shifts part of generation away from explicit natural-language CoT in order to improve reasoning~\citep{li2025implicit}.


\xhdr{Signal-guided Control}
These methods keep reasoning in token space but steers computation by inserting control tokens.
Early work shows that simple filler tokens (e.g., dots) can mimic some benefits of CoT~\citep{pfau2404let}. 
Building on this, later work expands the LLM vocabulary with \texttt{[PAUSE]} tokens and learns where to insert them for extra compute before predicting the next token~\citep{goyal2310think,kim2025learning}. 
They are lightweight and easily integrable, but constrained to the discrete-token interventions with limited latent controls.



\xhdr{Latent Optimization}
These methods perform autoregressive reasoning directly in internal representations, emitting little or no intermediate text.
They distill and compress CoT into latent continuous embeddings through various strategies.
Coconut and CCoT progressively replace text with latent thinking under final response supervision~\citep{hao2024training, cheng2024compressed}; 
Token assorted and HCoT compress CoT spans to embeddings with hidden-state alignment~\citep{su2025token, liu2024expediting}.
SoftThink directly applies logit-weighted embeddings for latent iterations~\citep{zhang2025soft}.
While offering efficiency gains and flexible control over hidden trajectories, these methods sacrifice reasoning interpretability, with training-based ones further requiring heavy mitigation from strong verbal LLMs.


\xhdr{Recurrent Transformers}
These methods interleave latent and verbal reasoning, introducing latent iterations before each token verbalization.
After a standard forward pass, these methods feed latent states back as next-iteration inputs for a fixed number of iterations, then verbalize the output token.
Existing approaches differ in the formation of next-iteration input.
For example, Looped Transformer reuses last-layer hidden states directly~\citep{saunshi2025loopedTrans,geiping2025scaling}, whereas Ponder uses logit-weighted embeddings (Equation~\ref{eq:ours_update})~\citep{zeng2025pondering}.
Recurrent transformers combine advantages of visible reasoning trajectories with latent exploration.
By reusing the parameters across iterations, it achieves deeper computation per token without parameter increases.
However, the fixed depth burdens each iteration with both easy and hard tokens, potentially causing false corrections for already-correct predictions.

\xhdr{Positioning} 
\name belongs to the recurrent transformer family but extends this paradigm significantly.
It \emph{selectively} allocates latent iterations to \emph{refine hard tokens}, improving reasoning quality with specialized objectives across iterations.
While concurrent works~\citep{bae2025mor, zhu2025ouro} also enable selective recursion, they require complete model retraining.
\name instead leverages existing pre-trained models, adding depth-aware LoRA and duo-causal attention to improve reasoning with minimal finetuning overhead.",
2511.09363v1,http://arxiv.org/abs/2511.09363v1,2025-11-12 14:23:49+00:00,BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,"Safety verification of dynamical systems via barrier certificates is essential for ensuring correctness in autonomous applications. Synthesizing these certificates involves discovering mathematical functions with current methods suffering from poor scalability, dependence on carefully designed templates, and exhaustive or incremental function-space searches. They also demand substantial manual expertise--selecting templates, solvers, and hyperparameters, and designing sampling strategies--requiring both theoretical and practical knowledge traditionally shared through linguistic reasoning rather than formalized methods.
  This motivates a key question: can such expert reasoning be captured and operationalized by language models? We address this by introducing an LLM-based agentic framework for barrier certificate synthesis. The framework uses natural language reasoning to propose, refine, and validate candidate certificates, integrating LLM-driven template discovery with SMT-based verification, and supporting barrier-controller co-synthesis to ensure consistency between safety certificates and controllers.
  To evaluate this capability, we introduce BarrierBench, a benchmark of 100 dynamical systems spanning linear, nonlinear, discrete-time, and continuous-time settings. Our experiments assess not only the effectiveness of LLM-guided barrier synthesis but also the utility of retrieval-augmented generation and agentic coordination strategies in improving its reliability and performance. Across these tasks, the framework achieves more than 90% success in generating valid certificates. By releasing BarrierBench and the accompanying toolchain, we aim to establish a community testbed for advancing the integration of language-based reasoning with formal verification in dynamical systems.
  The benchmark is publicly available at https://hycodev.com/dataset/barrierbench","\label{sec:related_work}

\paragraph{Barrier Certificate Generation.}
Traditional barrier synthesis has been dominated by optimization-based methods, particularly sum-of-squares (SOS) programming~\citep{prajna2007framework,ames2019control,wooding2025protect,kordabad2025sum}.
%Tools such as PRoTECT~\citep{wooding2025protect} employ SOS optimization to search for polynomial barriers across discrete-time and continuous-time, deterministic and stochastic systems.
%While SOS methods offer formal guarantees, their computational cost grows exponentially with system dimension and polynomial degree, limiting scalability.
To overcome the scalability limitations of SOS, counterexample-guided inductive synthesis (CEGIS) frameworks such as FOSSIL~\citep{abate2021fossil} %and FOSSIL 2.0~\citep{edwards2024fossil}
integrate neural function approximators with SMT-based verification, enabling synthesis of nonlinear certificates through iterative counterexample refinement. These approaches substantially expand the range of verifiable systems beyond fixed polynomial templates.

Application of barrier certificates to specifications beyond safety is also studied in the literature \citep{lindemann2018control,jagtap2020formal,majumdar2024necessary}.
Data-driven variants extend barrier synthesis to systems with unknown or partially known dynamics \citep{nejati2023formal}. 
%
% Scenario-based methods~\citep{nejati2023formal} construct probabilistic barriers from trajectory data, while hybrid learning approaches~\citep{ma2025datadriven} combine deep learning with symbolic regression to derive analytical certificates that balance efficiency and formal soundness.
% Recent work on $k$-inductive barrier certificates~\citep{murali2022scenario} relaxes traditional invariance requirements by enforcing decrease only over the last $k$ transitions, reducing polynomial degree but introducing non-convex constraints.
%
In the literature on data-driven barrier certificates, most existing methods are restricted to linear or control-affine system dynamics (see, e.g., \citep{Jagtap2020CBCGP,Cohen2022,Lopez2022uCBF}). Moreover, these approaches often depend on known Lipschitz constants to ensure formal guarantees or are applicable only when the system dynamics are partially known. For example, Gaussian processes are used by \citep{Wang2018CBF} and by \citep{Jagtap2020CBCGP} to learn unknown components of nonlinear dynamics while assuming the control-affine part is known. Systems affected by unknown additive disturbances are studied by \citep{Chekan2023UncertainConstraints}. Similarly, \citep{mathiesen2024data} assume that the deterministic component of the dynamics is accurately modeled, whereas the barrier-based safety control framework in of \citep{mazouz2024data} presumes known noise distributions.
Research addressing systems with fully unknown dynamics are also studied recently \citep{Salamati2021DDCBC,wang2023stochastic,schon2024DRObarrier} with an available software tool LUCID \citep{lucid}.
Despite these advances, most methods still depend on manual template selection and limited function classes, underscoring the need for adaptive, reasoning-driven synthesis frameworks.

\paragraph{LLMs for Complex Reasoning.}
Recent advances in LLMs have demonstrated exceptional reasoning, planning, and problem-solving capabilities~\citep{achiam2023gpt,zhao2023survey}. Models such as GPT-3~\citep{dale2021gpt3} and GPT-4~\citep{sanderson2023gpt4} exhibit strong generalization and inference abilities by leveraging billions of pretrained parameters and broad world knowledge~\citep{xu2024llm}.
LLMs can decompose complex problems into sub-tasks and reason across multiple abstraction levels, enabling coherent multi-step explanations and decisions~\citep{jansen2020visually,lin2023text2motion,rana2023sayplan}.
These capabilities have been applied across domains—from robotic task planning~\citep{rana2023sayplan} and autonomous driving~\citep{cui2023large} to mathematical reasoning tasks requiring symbolic manipulation, quantitative analysis, and formal logic~\citep{lewkowycz2022solving}.
Such progress positions LLMs as promising tools for safety-critical domains that demand rigorous mathematical reasoning about system constraints and safe execution.

\paragraph{LLM for CPS.}
LLMs are increasingly transforming CPS by introducing high-level reasoning, planning, and natural-language interaction capabilities~\citep{xu2024llm}.
Beyond traditional machine learning models trained on narrow datasets, LLMs draw on diverse web-scale corpora of text and code to serve as both context-aware assistants and autonomous decision-making agents.
In robotics, systems such as SayCan~\citep{ahn2022saycan} and RT-2~\citep{zitkovich2023rt2} combine LLM reasoning with reinforcement learning or multimodal perception to translate natural-language commands into executable actions.
In autonomous driving, DriveGPT4~\citep{yang2023drivegpt4} and Talk2Drive~\citep{cui2023large} employ LLMs for interpretable decision-making and natural language interaction.

For formal controller synthesis, \citet{bayat2025llm} propose a multi-agent framework that translates natural-language specifications into verified symbolic control code, combining reasoning with abstraction-based verification.
Despite these advances, integrating LLMs into CPS remains challenging.
LLMs are susceptible to hallucination~\citep{xu2024llm}, lack grounding in physical constraints, and cannot be verified using traditional neural-network methods such as Reluplex due to scale and architectural complexity.
These limitations motivate hybrid approaches that couple LLM reasoning with formal verification to ensure correctness and safety in real-world CPS.","\paragraph{Barrier Certificate Generation.}
Traditional barrier synthesis has been dominated by optimization-based methods, particularly sum-of-squares (SOS) programming~\citep{prajna2007framework,ames2019control,wooding2025protect,kordabad2025sum}.


To overcome the scalability limitations of SOS, counterexample-guided inductive synthesis (CEGIS) frameworks such as FOSSIL~\citep{abate2021fossil} 
integrate neural function approximators with SMT-based verification, enabling synthesis of nonlinear certificates through iterative counterexample refinement. These approaches substantially expand the range of verifiable systems beyond fixed polynomial templates.

Application of barrier certificates to specifications beyond safety is also studied in the literature \citep{lindemann2018control,jagtap2020formal,majumdar2024necessary}.
Data-driven variants extend barrier synthesis to systems with unknown or partially known dynamics \citep{nejati2023formal}. 




In the literature on data-driven barrier certificates, most existing methods are restricted to linear or control-affine system dynamics (see, e.g., \citep{Jagtap2020CBCGP,Cohen2022,Lopez2022uCBF}). Moreover, these approaches often depend on known Lipschitz constants to ensure formal guarantees or are applicable only when the system dynamics are partially known. For example, Gaussian processes are used by \citep{Wang2018CBF} and by \citep{Jagtap2020CBCGP} to learn unknown components of nonlinear dynamics while assuming the control-affine part is known. Systems affected by unknown additive disturbances are studied by \citep{Chekan2023UncertainConstraints}. Similarly, \citep{mathiesen2024data} assume that the deterministic component of the dynamics is accurately modeled, whereas the barrier-based safety control framework in of \citep{mazouz2024data} presumes known noise distributions.
Research addressing systems with fully unknown dynamics are also studied recently \citep{Salamati2021DDCBC,wang2023stochastic,schon2024DRObarrier} with an available software tool LUCID \citep{lucid}.
Despite these advances, most methods still depend on manual template selection and limited function classes, underscoring the need for adaptive, reasoning-driven synthesis frameworks.

\paragraph{LLMs for Complex Reasoning.}
Recent advances in LLMs have demonstrated exceptional reasoning, planning, and problem-solving capabilities~\citep{achiam2023gpt,zhao2023survey}. Models such as GPT-3~\citep{dale2021gpt3} and GPT-4~\citep{sanderson2023gpt4} exhibit strong generalization and inference abilities by leveraging billions of pretrained parameters and broad world knowledge~\citep{xu2024llm}.
LLMs can decompose complex problems into sub-tasks and reason across multiple abstraction levels, enabling coherent multi-step explanations and decisions~\citep{jansen2020visually,lin2023text2motion,rana2023sayplan}.
These capabilities have been applied across domains—from robotic task planning~\citep{rana2023sayplan} and autonomous driving~\citep{cui2023large} to mathematical reasoning tasks requiring symbolic manipulation, quantitative analysis, and formal logic~\citep{lewkowycz2022solving}.
Such progress positions LLMs as promising tools for safety-critical domains that demand rigorous mathematical reasoning about system constraints and safe execution.

\paragraph{LLM for CPS.}
LLMs are increasingly transforming CPS by introducing high-level reasoning, planning, and natural-language interaction capabilities~\citep{xu2024llm}.
Beyond traditional machine learning models trained on narrow datasets, LLMs draw on diverse web-scale corpora of text and code to serve as both context-aware assistants and autonomous decision-making agents.
In robotics, systems such as SayCan~\citep{ahn2022saycan} and RT-2~\citep{zitkovich2023rt2} combine LLM reasoning with reinforcement learning or multimodal perception to translate natural-language commands into executable actions.
In autonomous driving, DriveGPT4~\citep{yang2023drivegpt4} and Talk2Drive~\citep{cui2023large} employ LLMs for interpretable decision-making and natural language interaction.

For formal controller synthesis, \citet{bayat2025llm} propose a multi-agent framework that translates natural-language specifications into verified symbolic control code, combining reasoning with abstraction-based verification.
Despite these advances, integrating LLMs into CPS remains challenging.
LLMs are susceptible to hallucination~\citep{xu2024llm}, lack grounding in physical constraints, and cannot be verified using traditional neural-network methods such as Reluplex due to scale and architectural complexity.
These limitations motivate hybrid approaches that couple LLM reasoning with formal verification to ensure correctness and safety in real-world CPS.","Barrier Certificate Generation.Traditional barrier synthesis has been dominated by optimization-based methods,
particularly sum-of-squares (SOS) programming (Prajna et al., 2007; Ames et al., 2019; Wooding et al., 2025; Kordabad
et al., 2025). To overcome the scalability limitations of SOS, counterexample-guided inductive synthesis (CEGIS)
frameworks such as FOSSIL (Abate et al., 2021) integrate neural function approximators with SMT-based verification,
enabling synthesis of nonlinear certificates through iterative counterexample refinement. These approaches substantially
expand the range of verifiable systems beyond fixed polynomial templates.
Application of barrier certificates to specifications beyond safety is also studied in the literature (Lindemann and
Dimarogonas, 2018; Jagtap et al., 2020b; Majumdar et al., 2024). Data-driven variants extend barrier synthesis to
systems with unknown or partially known dynamics (Nejati et al., 2023). In the literature on data-driven barrier
certificates, most existing methods are restricted to linear or control-affine system dynamics (see, e.g., (Jagtap et al.,
2020a; Cohen et al., 2022; Lopez and Slotine, 2023)). Moreover, these approaches often depend on known Lipschitz
constants to ensure formal guarantees or are applicable only when the system dynamics are partially known. For example,
Gaussian processes are used by (Wang et al., 2018) and by (Jagtap et al., 2020a) to learn unknown components of
nonlinear dynamics while assuming the control-affine part is known. Systems affected by unknown additive disturbances
are studied by (Chekan and Langbort, 2023). Similarly, (Mathiesen et al., 2024) assume that the deterministic component
of the dynamics is accurately modeled, whereas the barrier-based safety control framework in of (Mazouz et al., 2024)
presumes known noise distributions. Research addressing systems with fully unknown dynamics are also studied
recently (Salamati et al., 2024; Wang et al., 2023; Schön et al., 2024) with an available software tool LUCID (Casablanca
et al., 2026). Despite these advances, most methods still depend on manual template selection and limited function
classes, underscoring the need for adaptive, reasoning-driven synthesis frameworks.
LLMs for Complex Reasoning.Recent advances in LLMs have demonstrated exceptional reasoning, planning,
and problem-solving capabilities (Achiam et al., 2023; Zhao et al., 2023). Models such as GPT-3 (Dale, 2021) and
GPT-4 (Sanderson, 2023) exhibit strong generalization and inference abilities by leveraging billions of pretrained
parameters and broad world knowledge (Xu et al., 2024). LLMs can decompose complex problems into sub-tasks and
reason across multiple abstraction levels, enabling coherent multi-step explanations and decisions (Jansen, 2020; Lin
et al., 2023; Rana et al., 2023). These capabilities have been applied across domains—from robotic task planning (Rana
et al., 2023) and autonomous driving (Cui et al., 2023) to mathematical reasoning tasks requiring symbolic manipulation,
quantitative analysis, and formal logic (Lewkowycz et al., 2022). Such progress positions LLMs as promising tools for
safety-critical domains that demand rigorous mathematical reasoning about system constraints and safe execution.
7
BarrierBench: Evaluating LLMs for Safety Verification in Dynamical SystemsA PREPRINT
LLM for CPS.LLMs are increasingly transforming CPS by introducing high-level reasoning, planning, and natural-
language interaction capabilities (Xu et al., 2024). Beyond traditional machine learning models trained on narrow
datasets, LLMs draw on diverse web-scale corpora of text and code to serve as both context-aware assistants and
autonomous decision-making agents. In robotics, systems such as SayCan (Ahn et al., 2022) and RT-2 (Zitkovich et al.,
2023) combine LLM reasoning with reinforcement learning or multimodal perception to translate natural-language
commands into executable actions. In autonomous driving, DriveGPT4 (Yang et al., 2023) and Talk2Drive (Cui et al.,
2023) employ LLMs for interpretable decision-making and natural language interaction.
For formal controller synthesis, Bayat et al. (2025) propose a multi-agent framework that translates natural-language
specifications into verified symbolic control code, combining reasoning with abstraction-based verification. Despite
these advances, integrating LLMs into CPS remains challenging. LLMs are susceptible to hallucination (Xu et al.,
2024), lack grounding in physical constraints, and cannot be verified using traditional neural-network methods such as
Reluplex due to scale and architectural complexity. These limitations motivate hybrid approaches that couple LLM
reasoning with formal verification to ensure correctness and safety in real-world CPS."
2511.10621v1,http://arxiv.org/abs/2511.10621v1,2025-11-13 18:47:07+00:00,SSR: Socratic Self-Refine for Large Language Model Reasoning,"Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, yet existing test-time frameworks often rely on coarse self-verification and self-correction, limiting their effectiveness on complex tasks. In this paper, we propose Socratic Self-Refine (SSR), a novel framework for fine-grained evaluation and precise refinement of LLM reasoning. Our proposed SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, enabling step-level confidence estimation through controlled re-solving and self-consistency checks. By pinpointing unreliable steps and iteratively refining them, SSR produces more accurate and interpretable reasoning chains. Empirical results across five reasoning benchmarks and three LLMs show that SSR consistently outperforms state-of-the-art iterative self-refinement baselines. Beyond performance gains, SSR provides a principled black-box approach for evaluating and understanding the internal reasoning processes of LLMs. Code is available at https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning.","\label{sec:related}

\textbf{Self-Evaluation and Refinement of LLMs.}\quad
Recent work has introduced both \emph{intrinsic} and \emph{generative} approaches for LLM self-evaluation. On the intrinsic side, uncertainty-based methods estimate correctness either through consistency, by comparing multiple independently generated outputs~\citep{kuhn2023semantic, manakul2023selfcheckgpt}, or through statistics derived from the model’s output distribution~\citep{kang2025scalable,fu2025deep,zhang2025token}. On the generative side, the \emph{LLM-as-a-Judge} paradigm directly prompts models to evaluate responses, often achieving strong alignment with human preferences and supporting test-time strategies like abstaining from low-quality responses or selecting among candidates~\citep{zheng2023judging,gu2024survey,zhou2025evaluating,ren2023self,chen2025sets,huang2025efficient,zhong2025solve,zhou2025variation}. While limitations such as positional bias~\citep{zheng2023large,shi2024judging} and a preference for longer responses~\citep{hu2024explaining} do exist, both uncertainty-based and judge-based methods remain effective and have proven valuable for evaluating LLM outputs.
Building on these evaluation techniques, a growing body of work extends beyond verification to self-refinement, where LLMs not only diagnose weaknesses in their outputs but also iteratively improve them~\citep{madaan2023self}. Early efforts explored direct self-correction based on feedback, while subsequent methods introduced structured search~\citep{zhang2024accessing}, parallel sampling to enrich candidate diversity~\citep{bi2024forest,chen2025sets}, and reformulation strategies that generate improved sub-questions by incorporating contextual preconditions~\citep{teng2025atom}. More recent work trains generative verifiers to guide the refinement process~\citep{zhong2025solve}. Collectively, these approaches demonstrate that refinement transforms passive evaluation into an active mechanism for improving reliability, making it a key step toward controllable and trustworthy reasoning in LLMs.


\textbf{Process Evaluation of LLMs.}\quad
Verifying only the final outcome of an LLM is insufficient; ensuring reliability requires mechanisms that also evaluate the reasoning process itself. Beyond using human annotations to train process reward models~\citep{lightman2023let,skywork2024prm,zhang2025lessons}, the rapid advancement of model capabilities has motivated a growing set of test-time methods for step-level verification. These approaches typically decompose the reasoning trace and assess the correctness of each step to localize errors more accurately~\citep{ling2023deductive,zhao2025genprm,mukherjee2025premise,fang2025graph}.
Compared to existing work of process evaluation, our \ours framework adopts a Socratic formulation of reasoning, representing the process as a sequence of question-answer pairs~(details in \Secref{sec:socratic-self-refine}). This structure makes the steps straightforward to re-execute and enables reliable confidence estimation. Crucially, \ours goes beyond verification by producing informative signals that directly support subsequent refinement.","\textbf{Self-Evaluation and Refinement of LLMs.}\quad
Recent work has introduced both \emph{intrinsic} and \emph{generative} approaches for LLM self-evaluation. On the intrinsic side, uncertainty-based methods estimate correctness either through consistency, by comparing multiple independently generated outputs~\citep{kuhn2023semantic, manakul2023selfcheckgpt}, or through statistics derived from the model’s output distribution~\citep{kang2025scalable,fu2025deep,zhang2025token}. On the generative side, the \emph{LLM-as-a-Judge} paradigm directly prompts models to evaluate responses, often achieving strong alignment with human preferences and supporting test-time strategies like abstaining from low-quality responses or selecting among candidates~\citep{zheng2023judging,gu2024survey,zhou2025evaluating,ren2023self,chen2025sets,huang2025efficient,zhong2025solve,zhou2025variation}. While limitations such as positional bias~\citep{zheng2023large,shi2024judging} and a preference for longer responses~\citep{hu2024explaining} do exist, both uncertainty-based and judge-based methods remain effective and have proven valuable for evaluating LLM outputs.
Building on these evaluation techniques, a growing body of work extends beyond verification to self-refinement, where LLMs not only diagnose weaknesses in their outputs but also iteratively improve them~\citep{madaan2023self}. Early efforts explored direct self-correction based on feedback, while subsequent methods introduced structured search~\citep{zhang2024accessing}, parallel sampling to enrich candidate diversity~\citep{bi2024forest,chen2025sets}, and reformulation strategies that generate improved sub-questions by incorporating contextual preconditions~\citep{teng2025atom}. More recent work trains generative verifiers to guide the refinement process~\citep{zhong2025solve}. Collectively, these approaches demonstrate that refinement transforms passive evaluation into an active mechanism for improving reliability, making it a key step toward controllable and trustworthy reasoning in LLMs.


\textbf{Process Evaluation of LLMs.}\quad
Verifying only the final outcome of an LLM is insufficient; ensuring reliability requires mechanisms that also evaluate the reasoning process itself. Beyond using human annotations to train process reward models~\citep{lightman2023let,skywork2024prm,zhang2025lessons}, the rapid advancement of model capabilities has motivated a growing set of test-time methods for step-level verification. These approaches typically decompose the reasoning trace and assess the correctness of each step to localize errors more accurately~\citep{ling2023deductive,zhao2025genprm,mukherjee2025premise,fang2025graph}.
Compared to existing work of process evaluation, our \ours framework adopts a Socratic formulation of reasoning, representing the process as a sequence of question-answer pairs~(details in \Secref{sec:socratic-self-refine}). This structure makes the steps straightforward to re-execute and enables reliable confidence estimation. Crucially, \ours goes beyond verification by producing informative signals that directly support subsequent refinement.",
2511.09478v1,http://arxiv.org/abs/2511.09478v1,2025-11-12 16:50:16+00:00,AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting,"Reinforcement learning (RL) has demonstrated considerable potential for enhancing reasoning in large language models (LLMs). However, existing methods suffer from Gradient Starvation and Policy Degradation when training directly on samples with mixed difficulty. To mitigate this, prior approaches leverage Chain-of-Thought (CoT) data, but the construction of high-quality CoT annotations remains labor-intensive. Alternatively, curriculum learning strategies have been explored but frequently encounter challenges, such as difficulty mismatch, reliance on manual curriculum design, and catastrophic forgetting. To address these issues, we propose AdaCuRL, a Adaptive Curriculum Reinforcement Learning framework that integrates coarse-to-fine difficulty estimation with adaptive curriculum scheduling. This approach dynamically aligns data difficulty with model capability and incorporates a data revisitation mechanism to mitigate catastrophic forgetting. Furthermore, AdaCuRL employs adaptive reference and sparse KL strategies to prevent Policy Degradation. Extensive experiments across diverse reasoning benchmarks demonstrate that AdaCuRL consistently achieves significant performance improvements on both LLMs and MLLMs.","\subsection{Reasoning-oriented Reinforcement Learning} 
Reasoning for LLMs remains a central focus \cite{wang2024exploring, saparov2022language, xiong2025hs, wang2025position}. 
CoT Prompting~\cite{zhang2024improve, yao2023tree} guides models to reason step-by-step, while CoT Finetuning \cite{dong2025insight,xu2024llava} fine-tunes models on large-scale CoT datasets. DeepSeek-R1 \cite{guo2025deepseek} demonstrates that RL can spontaneously induce strong reasoning abilities, reducing the need for extensive CoT data. 
However, since MLLMs typically possess limited initial reasoning skills, applying RL directly yields minimal improvements.
This motivates studies \cite{yang2025r1, huang2025vision} to  distill CoT
data from DeepSeek-R1 or other reasoning-oriented models for SFT before RL, while \citet{huang2025boosting} provides expert reasoning chains during RL to solve hard problems. However, these methods overlook the alignment between model capability and sample difficulty.

\subsection{Curriculum Learning for RL} 
Curriculum learning (CL)~\cite{bengio2009curriculum} trains models from easy to hard and is now broadly used in RL~\cite{zhou2020uncertainty, wang2023efficienttrain}.
\citet{deng2025boosting} defines difficulty based on answer types, which fails to capture the model's intrinsic perception of difficulty. 
Other works~\cite{team2025kimi, deng2025boosting} employ fixed curricula without incorporating feedback from the model. \citet{shi2025efficient} estimate problem difficulty using expert models and propose an adaptive scheduler, however their method lacks historical data revisiting and does not address the degradation problem.
In contrast, \ours dynamically schedules samples based on model feedback and incorporates historical data revisiting to prevent performance degradation on early data. Finally, through a designed KL loss computation, the model avoids Policy Degradation when learning signals are absent.

% \input{sec/6_conclusion}","\subsection{Reasoning-oriented Reinforcement Learning} 
Reasoning for LLMs remains a central focus \cite{wang2024exploring, saparov2022language, xiong2025hs, wang2025position}. 
CoT Prompting~\cite{zhang2024improve, yao2023tree} guides models to reason step-by-step, while CoT Finetuning \cite{dong2025insight,xu2024llava} fine-tunes models on large-scale CoT datasets. DeepSeek-R1 \cite{guo2025deepseek} demonstrates that RL can spontaneously induce strong reasoning abilities, reducing the need for extensive CoT data. 
However, since MLLMs typically possess limited initial reasoning skills, applying RL directly yields minimal improvements.
This motivates studies \cite{yang2025r1, huang2025vision} to  distill CoT
data from DeepSeek-R1 or other reasoning-oriented models for SFT before RL, while \citet{huang2025boosting} provides expert reasoning chains during RL to solve hard problems. However, these methods overlook the alignment between model capability and sample difficulty.

\subsection{Curriculum Learning for RL} 
Curriculum learning (CL)~\cite{bengio2009curriculum} trains models from easy to hard and is now broadly used in RL~\cite{zhou2020uncertainty, wang2023efficienttrain}.
\citet{deng2025boosting} defines difficulty based on answer types, which fails to capture the model's intrinsic perception of difficulty. 
Other works~\cite{team2025kimi, deng2025boosting} employ fixed curricula without incorporating feedback from the model. \citet{shi2025efficient} estimate problem difficulty using expert models and propose an adaptive scheduler, however their method lacks historical data revisiting and does not address the degradation problem.
In contrast, \ours dynamically schedules samples based on model feedback and incorporates historical data revisiting to prevent performance degradation on early data. Finally, through a designed KL loss computation, the model avoids Policy Degradation when learning signals are absent.","6.1 Reasoning-oriented Reinforcement Learning
Reasoning for LLMs remains a central focus (Wang et al.
2024b; Saparov and He 2022; Xiong et al. 2025; Wang et al.
2025). CoT Prompting (Zhang et al. 2024b; Yao et al. 2023)
guides models to reason step-by-step, while CoT Finetun-
ing (Dong et al. 2025; Xu et al. 2024) fine-tunes models on
large-scale CoT datasets. DeepSeek-R1 (Guo et al. 2025)
demonstrates that RL can spontaneously induce strong rea-
soning abilities, reducing the need for extensive CoT data.
However, since MLLMs typically possess limited initial rea-
soning skills, applying RL directly yields minimal improve-
ments. This motivates studies (Yang et al. 2025; Huang
et al. 2025b) to distill CoT data from DeepSeek-R1 or other
reasoning-oriented models for SFT before RL, while Huang
et al. (2025a) provides expert reasoning chains during RL to
solve hard problems. However, these methods overlook the
alignment between model capability and sample difficulty.
6.2 Curriculum Learning for RL
Curriculum learning (CL) (Bengio et al. 2009) trains mod-
els from easy to hard and is now broadly used in RL (Zhou
et al. 2020; Wang et al. 2023). Deng et al. (2025) defines
difficulty based on answer types, which fails to capture the
model’s intrinsic perception of difficulty. Other works (Team
et al. 2025; Deng et al. 2025) employ fixed curricula without
incorporating feedback from the model. Shi et al. (2025) es-
timate problem difficulty using expert models and propose
an adaptive scheduler, however their method lacks historical
data revisiting and does not address the degradation prob-
lem. In contrast, AdaCuRL dynamically schedules samples
based on model feedback and incorporates historical data
revisiting to prevent performance degradation on early data.
Finally, through a designed KL loss computation, the model
avoids Policy Degradation when learning signals are absent."
2511.08158v2,http://arxiv.org/abs/2511.08158v2,2025-11-11 12:10:41+00:00,LOw-cOst yet High-Performant Sparse Matrix-Matrix Multiplication on Arm SME Architectures,"Sparse matrix-dense matrix multiplication (SpMM) is a critical kernel in both scientific computing and emerging graph learning workloads. The recent Armv9 architecture introduces Scalable Matrix Extension (SME), enabling tile-based matrix operations with high throughput. However, effectively exploiting both SME and traditional SIMD resources for unstructured sparse workloads remains an open challenge. To address this, we propose LOOPS, a hybrid execution framework that combines row-wise CSR-part with vector-wise BCSR-part layout, enabling cooperative utilization of vector instructions (NEON) and Scalable Matrix Extension (SME) resources. LOOPS supports multi-precision SpMM across FP64, FP32, and FP16 via an adaptive two-level parallelization scheme guided by a lightweight performance model. Experimental results on the entire SuiteSparse on an Apple's M4Pro CPU show that LOOPS achieves average speedups of 9.93$\times$ (FP32)/14.4$\times$ (FP64) against the CPU baseline TACO and 71.3$\times$ (FP32)/54.8$\times$ (FP64) with respect to Armadillo. A comparison of LOOPS running on the same CPU with two GPU methods (cuSPARSE, Magicube) executed on an NVIDIA A100 GPU show average speedups for LOOPS between 19.8$\times$ and 33.5$\times$, depending on the precision. Notably, LOOPS delivers significantly better energy efficiency than the GPU codes on the A100 GPU.","\label{sec:relatedwork}

% cpu simd spmm
% 在cpu上手工调优的spmm kernel实现很少，有一部分，如xxx。fusedmm、featgraph、arm sparse、rosko
% arm上的就更少了，而且大多是spmv
% gpu tcu spmm
% gpu上的工作很多，但在arm上进行spmm的加速仍然有意义
% acclerator dsa-based spmm
% 有很多专用的加速器用于加速spmm
% vandor library or compiler
% cpu上的vandor库都不支持spmm，大多dense效果更好。如armpl仅支持spmv、spgemm，不支持spmm，sparse times dense。compiler和auto tuning方法可以获得性能较好的kernel，但是他们通常无法及时跟进最新的体系结构变化，如sme。libxsmm对于sme的支持仅限于gemm。此外autotuning的空间很大，开销高。TVM、taco等
\textbf{CPU SpMM.}  
Optimized SpMM kernels on general-purpose CPUs remain relatively underexplored. Several existing efforts such as FusedMM~\cite{rahman2021fusedmm}, FeatGraph~\cite{hu2020featgraph}, and Rosko~\cite{natesh2023rosko} focus on using SIMD instructions to accelerate SpMM. Additionally, vendor-provided libraries for CPUs, such as Intel MKL~\cite{wang2014intel} or ArmPL~\cite{ArmPL2504}, provide strong support for sparse linear algebra. However, ArmPL supports SpMV and SpGEMM, but does not support SpMM. Code-generation frameworks and auto-tuning systems such as TVM~\cite{chen2018tvm}, LIBXSMM~\cite{heinecke2016libxsmm} and TACO~\cite{kjolstad2017tensor} aim to generate optimized kernels for sparse workloads. However, they often struggle to adapt to rapidly evolving architectural features such as Armv9’s SME. For example, LIBXSMM currently provides SME support only for dense GEMM kernels. Moreover, auto-tuning frameworks require significant search time and may introduce considerable tuning overhead. % simpler patterns of dense vecotr, 与vendor lib合并吧，统一写cpu上的spmm solutions

\textbf{GPU SpMM.}  
Extensive work has been devoted to optimizing SpMM on GPUs. Sputnik~\cite{gale2020sparse} presents the one-dimensional tiling scheme and leveraging CUDA cores to accelerate sparse computation. vectorSparse~\cite{chen2021efficient} proposes the TCU-based 1-D Octet tiling method, using vectorized memory access. GE-SpMM~\cite{huang2020ge} employs the warp merging strategy to reduce redundant data loading. To improve load balancing, RoDe~\cite{pang2024row} proposes a 2D strategy to divide the rows of sparse matrices into regular and residue parts and introduces new load balance and fine-grained pipeline technologies for further optimization. 
Current SOTA works typically use TCUs to push the SpMM performance to a new height. TC-GNN~\cite{wang2023tc} and DTC-SpMM~\cite{fan2024dtc} propose custom sparse formats which divides the sparse matrix into nonzero vectors for computation on the TCUs. Magicube~\cite{li2022efficient} is a high-performance
sparse-matrix library to accelerate sparse and quantized matrix operations in deep learning leveraging the Strided Row-major BCRS (SR-BCRS) format. These approaches reconcile the sparse computation with the high-performance TCUs and employ multiple optimizations, including custom compressed format, reordering, load balancing, and pipeline optimizations. 

\textbf{DSA SpMM.}  
Numerous domain-specific accelerators \cite{hegde2019extensor,song2022sextans,gerogiannis2023spade,gerogiannis2024hottiles} have been proposed to address the challenges of sparse computation. These accelerators primarily exploit sparsity patterns using customized memory hierarchies and compute units. For instance, Sextans~\cite{song2022sextans} is a high performance FPGA accelerator for SpMM, which enables fast random access using on-chip memory, streaming access to off-chip large matrices, as well as PE-aware non-zero scheduling for balanced workload. SPADE~\cite{gerogiannis2023spade} is a hardware accelerator for SpMM and SDDMM which avoids data transfer overheads by tightly coupling accelerator PEs with the cores of a multicore and attains flexibility and programmability by supporting a tile-oriented ISA. HotTiles~\cite{gerogiannis2024hottiles} features heterogeneous SpMM accelerator architectures, which employ different types of PEs to exploit the insight that nonzeros form dense and sparse regions, rather than being uniformly distributed across the whole matrix. HYTE~\cite{li2025hyte} is a hybrid static-dynamic framework to enable flexible and efficient tiling on sparse accelerators and can flexibly adapt to the specific data sparsity patterns. While such solutions demonstrate high performance and efficiency, they often lack flexibility and are not readily deployable on general-purpose processors.

Different from these works, LOOPS targets SME and NEON units on modern Arm architectures and introduces a hybrid sparse format and performance model to fully exploit the heterogeneity in compute resources. Additionally, LOOPS achieves better (in small scale matrices) or comparable performance (in large scale matrices) against GPU methods with much lower power consumptions for SpMM on CPU.","\textbf{CPU SpMM.}  
Optimized SpMM kernels on general-purpose CPUs remain relatively underexplored. Several existing efforts such as FusedMM~\cite{rahman2021fusedmm}, FeatGraph~\cite{hu2020featgraph}, and Rosko~\cite{natesh2023rosko} focus on using SIMD instructions to accelerate SpMM. Additionally, vendor-provided libraries for CPUs, such as Intel MKL~\cite{wang2014intel} or ArmPL~\cite{ArmPL2504}, provide strong support for sparse linear algebra. However, ArmPL supports SpMV and SpGEMM, but does not support SpMM. Code-generation frameworks and auto-tuning systems such as TVM~\cite{chen2018tvm}, LIBXSMM~\cite{heinecke2016libxsmm} and TACO~\cite{kjolstad2017tensor} aim to generate optimized kernels for sparse workloads. However, they often struggle to adapt to rapidly evolving architectural features such as Armv9’s SME. For example, LIBXSMM currently provides SME support only for dense GEMM kernels. Moreover, auto-tuning frameworks require significant search time and may introduce considerable tuning overhead. 

\textbf{GPU SpMM.}  
Extensive work has been devoted to optimizing SpMM on GPUs. Sputnik~\cite{gale2020sparse} presents the one-dimensional tiling scheme and leveraging CUDA cores to accelerate sparse computation. vectorSparse~\cite{chen2021efficient} proposes the TCU-based 1-D Octet tiling method, using vectorized memory access. GE-SpMM~\cite{huang2020ge} employs the warp merging strategy to reduce redundant data loading. To improve load balancing, RoDe~\cite{pang2024row} proposes a 2D strategy to divide the rows of sparse matrices into regular and residue parts and introduces new load balance and fine-grained pipeline technologies for further optimization. 
Current SOTA works typically use TCUs to push the SpMM performance to a new height. TC-GNN~\cite{wang2023tc} and DTC-SpMM~\cite{fan2024dtc} propose custom sparse formats which divides the sparse matrix into nonzero vectors for computation on the TCUs. Magicube~\cite{li2022efficient} is a high-performance
sparse-matrix library to accelerate sparse and quantized matrix operations in deep learning leveraging the Strided Row-major BCRS (SR-BCRS) format. These approaches reconcile the sparse computation with the high-performance TCUs and employ multiple optimizations, including custom compressed format, reordering, load balancing, and pipeline optimizations. 

\textbf{DSA SpMM.}  
Numerous domain-specific accelerators \cite{hegde2019extensor,song2022sextans,gerogiannis2023spade,gerogiannis2024hottiles} have been proposed to address the challenges of sparse computation. These accelerators primarily exploit sparsity patterns using customized memory hierarchies and compute units. For instance, Sextans~\cite{song2022sextans} is a high performance FPGA accelerator for SpMM, which enables fast random access using on-chip memory, streaming access to off-chip large matrices, as well as PE-aware non-zero scheduling for balanced workload. SPADE~\cite{gerogiannis2023spade} is a hardware accelerator for SpMM and SDDMM which avoids data transfer overheads by tightly coupling accelerator PEs with the cores of a multicore and attains flexibility and programmability by supporting a tile-oriented ISA. HotTiles~\cite{gerogiannis2024hottiles} features heterogeneous SpMM accelerator architectures, which employ different types of PEs to exploit the insight that nonzeros form dense and sparse regions, rather than being uniformly distributed across the whole matrix. HYTE~\cite{li2025hyte} is a hybrid static-dynamic framework to enable flexible and efficient tiling on sparse accelerators and can flexibly adapt to the specific data sparsity patterns. While such solutions demonstrate high performance and efficiency, they often lack flexibility and are not readily deployable on general-purpose processors.

Different from these works, LOOPS targets SME and NEON units on modern Arm architectures and introduces a hybrid sparse format and performance model to fully exploit the heterogeneity in compute resources. Additionally, LOOPS achieves better (in small scale matrices) or comparable performance (in large scale matrices) against GPU methods with much lower power consumptions for SpMM on CPU.","CPU SpMM.Optimized SpMM kernels on general-purpose
CPUs remain relatively underexplored. Several existing ef-
forts such as FusedMM [ 39], FeatGraph [ 24], and Rosko [ 34]
focus on using SIMD instructions to accelerate SpMM. Ad-
ditionally, vendor-provided libraries for CPUs, such as Intel
MKL [ 44] or ArmPL [ 4], provide strong support for sparse lin-
ear algebra. However, ArmPL supports SpMV and SpGEMM,
but does not support SpMM. Code-generation frameworks
and auto-tuning systems such as TVM [ 9], LIBXSMM [ 22]
and TACO [ 27] aim to generate optimized kernels for sparse
workloads. However, they often struggle to adapt to rapidly
evolving architectural features such as Armv9’s SME. For
example, LIBXSMM currently provides SME support only fordense GEMM kernels. Moreover, auto-tuning frameworks re-
quire significant search time and may introduce considerable
tuning overhead.
GPU SpMM.Extensive work has been devoted to op-
timizing SpMM on GPUs. Sputnik [ 17] presents the one-
dimensional tiling scheme and leveraging CUDA cores to
accelerate sparse computation. vectorSparse [ 10] proposes
the TCU-based 1-D Octet tiling method, using vectorized
memory access. GE-SpMM [ 25] employs the warp merg-
ing strategy to reduce redundant data loading. To improve
load balancing, RoDe [ 38] proposes a 2D strategy to divide
the rows of sparse matrices into regular and residue parts
and introduces new load balance and fine-grained pipeline
technologies for further optimization. Current SOTA works
typically use TCUs to push the SpMM performance to a
new height. TC-GNN [ 46] and DTC-SpMM [ 15] propose
custom sparse formats which divides the sparse matrix into
nonzero vectors for computation on the TCUs. Magicube [ 30]
is a high-performance sparse-matrix library to accelerate
sparse and quantized matrix operations in deep learning
leveraging the Strided Row-major BCRS (SR-BCRS) format.
These approaches reconcile the sparse computation with
the high-performance TCUs and employ multiple optimiza-
tions, including custom compressed format, reordering, load
balancing, and pipeline optimizations.
DSA SpMM.Numerous domain-specific accelerators [ 18,
19,21,42] have been proposed to address the challenges
of sparse computation. These accelerators primarily exploit
sparsity patterns using customized memory hierarchies and
compute units. For instance, Sextans [ 42] is a high perfor-
mance FPGA accelerator for SpMM, which enables fast ran-
dom access using on-chip memory, streaming access to off-
chip large matrices, as well as PE-aware non-zero scheduling
for balanced workload. SPADE [ 19] is a hardware accelerator
for SpMM and SDDMM which avoids data transfer over-
heads by tightly coupling accelerator PEs with the cores of
a multicore and attains flexibility and programmability by
supporting a tile-oriented ISA. HotTiles [ 18] features het-
erogeneous SpMM accelerator architectures, which employ
different types of PEs to exploit the insight that nonzeros
form dense and sparse regions, rather than being uniformly
distributed across the whole matrix. HYTE [ 31] is a hybrid
static-dynamic framework to enable flexible and efficient
tiling on sparse accelerators and can flexibly adapt to the
specific data sparsity patterns. While such solutions demon-
strate high performance and efficiency, they often lack flex-
ibility and are not readily deployable on general-purpose
processors.
Different from these works, LOOPS targets SME and NEON
units on modern Arm architectures and introduces a hybrid
sparse format and performance model to fully exploit the
heterogeneity in compute resources. Additionally, LOOPS
10
achieves better (in small scale matrices) or comparable per-
formance (in large scale matrices) against GPU methods with
much lower power consumptions for SpMM on CPU."
2511.10179v1,http://arxiv.org/abs/2511.10179v1,2025-11-13 10:45:15+00:00,QuCoWE Quantum Contrastive Word Embeddings with Variational Circuits for NearTerm Quantum Devices,We present QuCoWE a framework that learns quantumnative word embeddings by training shallow hardwareefficient parameterized quantum circuits PQCs with a contrastive skipgram objective Words are encoded by datareuploading circuits with controlled ring entanglement similarity is computed via quantum state fidelity and passed through a logitfidelity head that aligns scores with the shiftedPMI scale of SGNSNoiseContrastive Estimation To maintain trainability we introduce an entanglementbudget regularizer based on singlequbit purity that mitigates barren plateaus On Text8 and WikiText2 QuCoWE attains competitive intrinsic WordSim353 SimLex999 and extrinsic SST2 TREC6 performance versus 50100d classical baselines while using fewer learned parameters per token All experiments are run in classical simulation we analyze depolarizingreadout noise and include errormitigation hooks zeronoise extrapolation randomized compiling to facilitate hardware deployment,"\subsection{Classical Word Embeddings}

Distributional semantics hypothesizes that words appearing in similar contexts share meaning \cite{harris1954distributional}. Word2Vec \cite{Mikolov2013NIPS} operationalizes this principle through skip-gram with negative sampling (SGNS), optimizing:
\begin{equation}
\mathcal{L}_{\text{SGNS}} = -\sum_{(w,c) \in D^+} \log \sigma(v_w \cdot v_c) - \sum_{(w,n) \in D^-} \log \sigma(-v_w \cdot v_n)
\end{equation}

Levy and Goldberg \cite{LevyGoldberg2014NIPS} proved that SGNS implicitly factorizes a shifted PMI matrix:
\begin{equation}
v_w \cdot v_c \approx \text{PMI}(w,c) - \log k
\end{equation}

This theoretical insight reveals that seemingly different approaches converge on similar statistical objectives. GloVe \cite{pennington2014glove} takes an alternative path by directly optimizing a weighted least-squares objective on co-occurrence statistics, while FastText \cite{bojanowski2017enriching} extends Word2Vec with subword information to handle morphologically rich languages and out-of-vocabulary words. Despite their strong performance, these methods typically require hundreds of dimensions to capture semantic relationships adequately and struggle with fundamental linguistic phenomena such as polysemy, where words exhibit multiple context-dependent meanings.

\subsection{Quantum Natural Language Processing}

Early QNLP research focused primarily on grammatical structure through the lens of categorical quantum mechanics \cite{coecke2020foundations}. The DisCoCat framework \cite{heunen2013quantum} elegantly maps grammatical types to quantum spaces and compositions to tensor products, providing a mathematically principled approach to compositional semantics. This theoretical foundation has led to several experimental implementations, including question answering systems deployed on IBM quantum devices \cite{Meichanetzidis2020} and variational quantum classifiers for text classification tasks \cite{chang2023variational}.

However, most prior work in QNLP either relies on classical pre-training to initialize quantum models or focuses exclusively on grammatical structure rather than distributional semantics. These approaches miss the opportunity to learn genuinely quantum representations that could capture semantic relationships in fundamentally new ways. In contrast, our approach learns quantum representations directly from co-occurrence statistics, bridging the gap between distributional semantics and quantum computation without requiring classical initialization.

\subsection{Parameterized Quantum Circuits}

Parameterized quantum circuits form the backbone of variational quantum algorithms and have seen rapid development in recent years \cite{Cerezo2021NatComm}. Hardware-efficient ansätze \cite{kandala2017hardware} provide circuit designs tailored to specific quantum architectures, balancing expressivity with the constraints of near-term devices. The data re-uploading strategy \cite{PerezSalinas2020Quantum} has emerged as a powerful technique for enhancing circuit expressivity without increasing depth, repeatedly encoding classical data at different circuit layers to create more complex decision boundaries.

The challenge of barren plateaus—exponentially vanishing gradients in random quantum circuits—has motivated several mitigation strategies. Local cost functions \cite{Cerezo2021NatComm} maintain trainable gradients by restricting measurements to small subsystems, while layerwise training \cite{skolik2021layerwise} progressively grows circuit depth to avoid gradient decay. Additionally, error mitigation techniques have become essential for NISQ-era implementations. Zero-noise extrapolation \cite{temme2017error} and probabilistic error cancellation \cite{temme2017error} help recover ideal circuit behavior from noisy measurements, while randomized compiling \cite{WallmanEmerson2016PRA} converts coherent errors into more manageable stochastic noise.

We incorporate these advances into a domain-specific architecture specifically designed for word embeddings, combining hardware efficiency with semantic learning objectives. Our design choices reflect both the theoretical insights from quantum algorithm development and the practical constraints of current quantum hardware.","\subsection{Classical Word Embeddings}

Distributional semantics hypothesizes that words appearing in similar contexts share meaning \cite{harris1954distributional}. Word2Vec \cite{Mikolov2013NIPS} operationalizes this principle through skip-gram with negative sampling (SGNS), optimizing:
\begin{equation}
\mathcal{L}_{\text{SGNS}} = -\sum_{(w,c) \in D^+} \log \sigma(v_w \cdot v_c) - \sum_{(w,n) \in D^-} \log \sigma(-v_w \cdot v_n)
\end{equation}

Levy and Goldberg \cite{LevyGoldberg2014NIPS} proved that SGNS implicitly factorizes a shifted PMI matrix:
\begin{equation}
v_w \cdot v_c \approx \text{PMI}(w,c) - \log k
\end{equation}

This theoretical insight reveals that seemingly different approaches converge on similar statistical objectives. GloVe \cite{pennington2014glove} takes an alternative path by directly optimizing a weighted least-squares objective on co-occurrence statistics, while FastText \cite{bojanowski2017enriching} extends Word2Vec with subword information to handle morphologically rich languages and out-of-vocabulary words. Despite their strong performance, these methods typically require hundreds of dimensions to capture semantic relationships adequately and struggle with fundamental linguistic phenomena such as polysemy, where words exhibit multiple context-dependent meanings.

\subsection{Quantum Natural Language Processing}

Early QNLP research focused primarily on grammatical structure through the lens of categorical quantum mechanics \cite{coecke2020foundations}. The DisCoCat framework \cite{heunen2013quantum} elegantly maps grammatical types to quantum spaces and compositions to tensor products, providing a mathematically principled approach to compositional semantics. This theoretical foundation has led to several experimental implementations, including question answering systems deployed on IBM quantum devices \cite{Meichanetzidis2020} and variational quantum classifiers for text classification tasks \cite{chang2023variational}.

However, most prior work in QNLP either relies on classical pre-training to initialize quantum models or focuses exclusively on grammatical structure rather than distributional semantics. These approaches miss the opportunity to learn genuinely quantum representations that could capture semantic relationships in fundamentally new ways. In contrast, our approach learns quantum representations directly from co-occurrence statistics, bridging the gap between distributional semantics and quantum computation without requiring classical initialization.

\subsection{Parameterized Quantum Circuits}

Parameterized quantum circuits form the backbone of variational quantum algorithms and have seen rapid development in recent years \cite{Cerezo2021NatComm}. Hardware-efficient ansätze \cite{kandala2017hardware} provide circuit designs tailored to specific quantum architectures, balancing expressivity with the constraints of near-term devices. The data re-uploading strategy \cite{PerezSalinas2020Quantum} has emerged as a powerful technique for enhancing circuit expressivity without increasing depth, repeatedly encoding classical data at different circuit layers to create more complex decision boundaries.

The challenge of barren plateaus—exponentially vanishing gradients in random quantum circuits—has motivated several mitigation strategies. Local cost functions \cite{Cerezo2021NatComm} maintain trainable gradients by restricting measurements to small subsystems, while layerwise training \cite{skolik2021layerwise} progressively grows circuit depth to avoid gradient decay. Additionally, error mitigation techniques have become essential for NISQ-era implementations. Zero-noise extrapolation \cite{temme2017error} and probabilistic error cancellation \cite{temme2017error} help recover ideal circuit behavior from noisy measurements, while randomized compiling \cite{WallmanEmerson2016PRA} converts coherent errors into more manageable stochastic noise.

We incorporate these advances into a domain-specific architecture specifically designed for word embeddings, combining hardware efficiency with semantic learning objectives. Our design choices reflect both the theoretical insights from quantum algorithm development and the practical constraints of current quantum hardware.","2.1 Classical Word Embeddings
Distributional semantics hypothesizes that words appearing in similar contexts share meaning Harris (1954).
Word2Vec Mikolov et al. (2013a) operationalizes this principle through skip-gram with negative sampling
(SGNS), optimizing:
LSGNS =−X
(w,c)∈D+logσ(v w·vc)−X
(w,n)∈D−logσ(−v w·vn)(1)
Levy and Goldberg Levy and Goldberg (2014) proved that SGNS implicitly factorizes a shifted PMI matrix:
vw·vc≈PMI(w, c)−logk(2)
This theoretical insight reveals that seemingly different approaches converge on similar statistical objectives.
GloVe Pennington et al. (2014) takes an alternative path by directly optimizing a weighted least-squares
objective on co-occurrence statistics, while FastText Bojanowski et al. (2017) extends Word2Vec with subword
information to handle morphologically rich languages and out-of-vocabulary words. Despite their strong
performance, these methods typically require hundreds of dimensions to capture semantic relationships
adequately and struggle with fundamental linguistic phenomena such as polysemy, where words exhibit
multiple context-dependent meanings.
2
2.2 Quantum Natural Language Processing
Early QNLP research focused primarily on grammatical structure through the lens of categorical quantum
mechanics Coecke et al. (2020). The DisCoCat framework Heunen et al. (2013) elegantly maps grammatical
types to quantum spaces and compositions to tensor products, providing a mathematically principled approach
to compositional semantics. This theoretical foundation has led to several experimental implementations,
including question answering systems deployed on IBM quantum devices Meichanetzidis et al. (2020) and
variational quantum classifiers for text classification tasks Chang (2023).
However, most prior work in QNLP either relies on classical pre-training to initialize quantum models or
focuses exclusively on grammatical structure rather than distributional semantics. These approaches miss
the opportunity to learn genuinely quantum representations that could capture semantic relationships in
fundamentally new ways. In contrast, our approach learns quantum representations directly from co-occurrence
statistics, bridging the gap between distributional semantics and quantum computation without requiring
classical initialization.
2.3 Parameterized Quantum Circuits
Parameterized quantum circuits form the backbone of variational quantum algorithms and have seen rapid
development in recent years Cerezo et al. (2021). Hardware-efficient ansätze Kandala et al. (2017) provide
circuit designs tailored to specific quantum architectures, balancing expressivity with the constraints of
near-term devices. The data re-uploading strategy Pérez-Salinas et al. (2020) has emerged as a powerful
technique for enhancing circuit expressivity without increasing depth, repeatedly encoding classical data at
different circuit layers to create more complex decision boundaries.
The challenge of barren plateaus—exponentially vanishing gradients in random quantum circuits—has
motivated several mitigation strategies. Local cost functions Cerezo et al. (2021) maintain trainable gradients
by restricting measurements to small subsystems, while layerwise training Skolik et al. (2021) progressively
grows circuit depth to avoid gradient decay. Additionally, error mitigation techniques have become essential for
NISQ-era implementations. Zero-noise extrapolation Temme et al. (2017b) and probabilistic error cancellation
Temme et al. (2017b) help recover ideal circuit behavior from noisy measurements, while randomized compiling
Wallman and Emerson (2016) converts coherent errors into more manageable stochastic noise.
We incorporate these advances into a domain-specific architecture specifically designed for word embeddings,
combining hardware efficiency with semantic learning objectives. Our design choices reflect both the theoretical
insights from quantum algorithm development and the practical constraints of current quantum hardware."
2511.08098v1,http://arxiv.org/abs/2511.08098v1,2025-11-11 10:54:15+00:00,PerspAct: Enhancing LLM Situated Collaboration Skills through Perspective Taking and Active Vision,"Recent advances in Large Language Models (LLMs) and multimodal foundation models have significantly broadened their application in robotics and collaborative systems. However, effective multi-agent interaction necessitates robust perspective-taking capabilities, enabling models to interpret both physical and epistemic viewpoints. Current training paradigms often neglect these interactive contexts, resulting in challenges when models must reason about the subjectivity of individual perspectives or navigate environments with multiple observers. This study evaluates whether explicitly incorporating diverse points of view using the ReAct framework, an approach that integrates reasoning and acting, can enhance an LLM's ability to understand and ground the demands of other agents. We extend the classic Director task by introducing active visual exploration across a suite of seven scenarios of increasing perspective-taking complexity. These scenarios are designed to challenge the agent's capacity to resolve referential ambiguity based on visual access and interaction, under varying state representations and prompting strategies, including ReAct-style reasoning. Our results demonstrate that explicit perspective cues, combined with active exploration strategies, significantly improve the model's interpretative accuracy and collaborative effectiveness. These findings highlight the potential of integrating active perception with perspective-taking mechanisms in advancing LLMs' application in robotics and multi-agent systems, setting a foundation for future research into adaptive and context-aware AI systems.","In recent years, there has been growing interest in the application of LLMs and multimodal foundation models in robotics, and of collaborative systems for high-level reasoning, perception, and decision-making \cite{ognibene2025scoopframeworkproactivecollaboration}. These models are pre-trained on vast amounts of internet-scale data, and exhibit impressive generalization capabilities \cite{brown2020language}, enabling robots to handle a wide range of open-ended scenarios. Models such as SayCan \cite{ahn2022can} and Inner Monologue \cite{huang2022inner} demonstrate how LLMs can break down abstract goals into practical steps by combining high-level reasoning with grounded robotic actions.

A core component of effective multi-agent interaction is perspective-taking, namely, the ability to represent a situation from an alternate viewpoint \cite{grice1975logic}. It includes two different but related processes: visual perspective-taking, with a distinction between Level-1 (determining what objects are visible to others) and Level-2 processes (representing how visible objects appear from another's viewpoint); and spatial perspective-taking, which involves representing relative spatial relationships between agents and objects, typically represented through egocentric and allocentric reference frames \cite{flavell1981young}. Visual perspective-taking has particularly been linked to  ToM. Specifically, Level-2 perspective-taking is related to ToM, because both functions require a decoupled representation from one's perspective or belief. Within frameworks such as ReAct \cite{yao2023react}, perspective-taking is conceptualized as a specialized form of reasoning that works alongside acting. This approach allows models to continuously update their knowledge states through real-time interactions with their environment.

Efforts to enhance perspective-taking in LLMs have largely focused on language-based tasks. Early studies using false-belief tasks reveal that while older models (e.g. GPT-2, early GPT-3) performed poorly, the latest systems (e.g. GPT-4) show emergent - though unstable - ToM capabilities \cite{kosinski2024evaluating}. Techniques such as the SimToM prompting framework \cite{wilf2023think} have further improved performance by explicitly instructing models to simulate a character’s perspective, mitigating the injection of omniscient background knowledge by the system itself.

Parallel research in vision-language models (VLMs) has explored spatial perspective-taking. Datasets like Isle-Bricks and Isle-Dots \cite{goral2024seeing} demonstrate that, although many VLMs can recognize objects within a scene, they often struggle with inferring what an observer can or cannot see. Advanced systems like GPT-4V excel on simple Level-1 tasks but experience significant performance drops on more complex Level-2 tasks involving mental rotation and continuous viewpoint changes \cite{leonard2024failures}.

Beyond static analysis, active visual exploration represents an added layer of complexity. Benchmarks such as ActiView \cite{wang2024actiview} require models to actively adjust their perceptual focus (e.g., by zooming in, or by shifting their view), to gather additional information. Although some models show an advantage when processing sequential observations, overall performance in active visual exploration remains far below human capabilities.

In addressing these challenges, recent work has explored strategies to resolve ambiguity in multi-agent settings. For example, prior studies have investigated using LLMs as active Bayesian filters for information acquisition and integration \cite{patania2024large}. Such a mechanism not only addresses ambiguous or incomplete perceptual inputs but also naturally complements robust perspective-taking, ultimately enhancing multi-agent interaction in dynamic environments.

These studies illustrate both the potential and current limitations of LLMs and VLMs in modelling perspective-taking and active exploration. While advances in prompting and dynamic reasoning have improved performance on discrete tasks, significant challenges remain in achieving a robust, context-sensitive understanding of multiple viewpoints in dynamic, real-world environments.","In recent years, there has been growing interest in the application of LLMs and multimodal foundation models in robotics, and of collaborative systems for high-level reasoning, perception, and decision-making \cite{ognibene2025scoopframeworkproactivecollaboration}. These models are pre-trained on vast amounts of internet-scale data, and exhibit impressive generalization capabilities \cite{brown2020language}, enabling robots to handle a wide range of open-ended scenarios. Models such as SayCan \cite{ahn2022can} and Inner Monologue \cite{huang2022inner} demonstrate how LLMs can break down abstract goals into practical steps by combining high-level reasoning with grounded robotic actions.

A core component of effective multi-agent interaction is perspective-taking, namely, the ability to represent a situation from an alternate viewpoint \cite{grice1975logic}. It includes two different but related processes: visual perspective-taking, with a distinction between Level-1 (determining what objects are visible to others) and Level-2 processes (representing how visible objects appear from another's viewpoint); and spatial perspective-taking, which involves representing relative spatial relationships between agents and objects, typically represented through egocentric and allocentric reference frames \cite{flavell1981young}. Visual perspective-taking has particularly been linked to  ToM. Specifically, Level-2 perspective-taking is related to ToM, because both functions require a decoupled representation from one's perspective or belief. Within frameworks such as ReAct \cite{yao2023react}, perspective-taking is conceptualized as a specialized form of reasoning that works alongside acting. This approach allows models to continuously update their knowledge states through real-time interactions with their environment.

Efforts to enhance perspective-taking in LLMs have largely focused on language-based tasks. Early studies using false-belief tasks reveal that while older models (e.g. GPT-2, early GPT-3) performed poorly, the latest systems (e.g. GPT-4) show emergent - though unstable - ToM capabilities \cite{kosinski2024evaluating}. Techniques such as the SimToM prompting framework \cite{wilf2023think} have further improved performance by explicitly instructing models to simulate a character’s perspective, mitigating the injection of omniscient background knowledge by the system itself.

Parallel research in vision-language models (VLMs) has explored spatial perspective-taking. Datasets like Isle-Bricks and Isle-Dots \cite{goral2024seeing} demonstrate that, although many VLMs can recognize objects within a scene, they often struggle with inferring what an observer can or cannot see. Advanced systems like GPT-4V excel on simple Level-1 tasks but experience significant performance drops on more complex Level-2 tasks involving mental rotation and continuous viewpoint changes \cite{leonard2024failures}.

Beyond static analysis, active visual exploration represents an added layer of complexity. Benchmarks such as ActiView \cite{wang2024actiview} require models to actively adjust their perceptual focus (e.g., by zooming in, or by shifting their view), to gather additional information. Although some models show an advantage when processing sequential observations, overall performance in active visual exploration remains far below human capabilities.

In addressing these challenges, recent work has explored strategies to resolve ambiguity in multi-agent settings. For example, prior studies have investigated using LLMs as active Bayesian filters for information acquisition and integration \cite{patania2024large}. Such a mechanism not only addresses ambiguous or incomplete perceptual inputs but also naturally complements robust perspective-taking, ultimately enhancing multi-agent interaction in dynamic environments.

These studies illustrate both the potential and current limitations of LLMs and VLMs in modelling perspective-taking and active exploration. While advances in prompting and dynamic reasoning have improved performance on discrete tasks, significant challenges remain in achieving a robust, context-sensitive understanding of multiple viewpoints in dynamic, real-world environments.","In recent years, there has been growing interest in the application of LLMs
and multimodal foundation models in robotics, and of collaborative systems
for high-level reasoning, perception, and decision-making [15]. These models are
pre-trained on vast amounts of internet-scale data, and exhibit impressive gen-
eralization capabilities [4], enabling robots to handle a wide range of open-ended
scenarios. Models such as SayCan [2] and Inner Monologue [9] demonstrate how
LLMs can break down abstract goals into practical steps by combining high-level
reasoning with grounded robotic actions.
A core component of effective multi-agent interaction is perspective-taking,
namely, the ability to represent a situation from an alternate viewpoint [8].
It includes two different but related processes: visual perspective-taking, with
a distinction between Level-1 (determining what objects are visible to others)
and Level-2 processes (representing how visible objects appear from another’s
viewpoint); and spatial perspective-taking, which involves representing relative
spatial relationships between agents and objects, typically represented through
egocentricandallocentricreferenceframes[5].Visualperspective-takinghaspar-
ticularly been linked to ToM. Specifically, Level-2 perspective-taking is related
to ToM, because both functions require a decoupled representation from one’s
perspective or belief. Within frameworks such as ReAct [20], perspective-taking
is conceptualized as a specialized form of reasoning that works alongside act-
ing. This approach allows models to continuously update their knowledge states
through real-time interactions with their environment.
Effortstoenhanceperspective-takinginLLMshavelargelyfocusedonlanguage-
based tasks. Early studies using false-belief tasks reveal that while older models
(e.g. GPT-2, early GPT-3) performed poorly, the latest systems (e.g. GPT-4)
show emergent - though unstable - ToM capabilities [13]. Techniques such as"
2511.09973v1,http://arxiv.org/abs/2511.09973v1,2025-11-13 05:11:44+00:00,Difference Vector Equalization for Robust Fine-tuning of Vision-Language Models,"Contrastive pre-trained vision-language models, such as CLIP, demonstrate strong generalization abilities in zero-shot classification by leveraging embeddings extracted from image and text encoders. This paper aims to robustly fine-tune these vision-language models on in-distribution (ID) data without compromising their generalization abilities in out-of-distribution (OOD) and zero-shot settings. Current robust fine-tuning methods tackle this challenge by reusing contrastive learning, which was used in pre-training, for fine-tuning. However, we found that these methods distort the geometric structure of the embeddings, which plays a crucial role in the generalization of vision-language models, resulting in limited OOD and zero-shot performance. To address this, we propose Difference Vector Equalization (DiVE), which preserves the geometric structure during fine-tuning. The idea behind DiVE is to constrain difference vectors, each of which is obtained by subtracting the embeddings extracted from the pre-trained and fine-tuning models for the same data sample. By constraining the difference vectors to be equal across various data samples, we effectively preserve the geometric structure. Therefore, we introduce two losses: average vector loss (AVL) and pairwise vector loss (PVL). AVL preserves the geometric structure globally by constraining difference vectors to be equal to their weighted average. PVL preserves the geometric structure locally by ensuring a consistent multimodal alignment. Our experiments demonstrate that DiVE effectively preserves the geometric structure, achieving strong results across ID, OOD, and zero-shot metrics.","\label{sect:relate}


\textbf{Robust Fine-tuning for vision-language models.}
Vision-language models, such as CLIP~\cite{Radford21}, demonstrate strong generalization abilities in OOD and zero-shot settings.
However, empirical evidence suggests that vanilla FT, which updates the weights of the image encoder using downstream data, compromises these strengths~\cite{Wortsman22,Ilharco22}.
To tackle this challenge, robust fine-tuning methods for vision-language models have been proposed.
%
An early baseline method, LP-FT~\cite{Kumar22}, first applies linear probing then fine-tunes the entire weights to enhance OOD performance.
%
FLYP~\cite{Goyal23} uses vision-language contrastive learning for fine-tuning.
This choice can enhance OOD performance over vanilla FT by considering both image and text encoders.
%
ARF~\cite{Han24} is the first robust fine-tuning method that explicitly aims to enhance not only OOD but also zero-shot performance.
It combines FLYP and a replay technique~\cite{Rolnick19}, which involves training on a reference dataset~(e.g., CC3M~\cite{Sharma18_cc3m}) that resembles the pre-training data, to help recover the inherent generalization ability of the pre-trained model.
%
While FLYP and ARF show promising performance, both OOD and zero-shot performance can still be improved.
DiVE improves them by introducing a new perspective: preserving the geometric structure of the embeddings.


\noindent
\textbf{Geometry of Embeddings of Contrastive Learning Models.}
Contrastive learning has been widely used for representation learning on various modalities~\cite{Chen20_simclr,Gao21}.
Most contrastive learning methods adopt $L_2$-normalized embeddings for stable learning~\cite{Xu18}, which causes the embeddings to lie on a unit hypersphere.
\citet{Wang20_contrastive} found that the relative positions between hyperspherical embeddings, i.e., the geometric structure, play a crucial role in generalization in contrastive learning models.
Studies~\cite{Goel22,Oh23,Yamaguchi25} have shown that this structure is also crucial for contrastive pre-trained vision-language models, such as CLIP.
%
Among these, CyCLIP~\cite{Goel22} shares some concepts with DiVE in constraining the geometric structure of the embeddings.
Specifically, it constrains the cosine similarities between in-modal and cross-modal embeddings during pre-training.
We found that such a cosine similarity-based constraint is insufficient to preserve the geometric structure and improve performance, as it captures only the angular relationship between embeddings.
In contrast, our difference vector-based constraint achieves both~(see Table~\ref{table:cyclip}).



\noindent
\textbf{Maintaining Zero-shot Performance.}
There are several approaches for maintaining the zero-shot performance of vision-language models during fine-tuning, not only robust fine-tuning.
Prompt learning~\cite{Zhou22_coop,Zhou22_cocoop,Khattak23} optimizes a set of learnable prompt vectors, while freezing the weights of the pre-trained model.
This strategy perfectly maintains the generalization ability.
However, as reported by \cite{Shu23}, its classification performance on target data is often limited due to the small number of learnable parameters.
%
Methods for continual learning~\cite{Wang24} can be adapted to our setting by interpreting the contrastive pre-training as the previous task.
For example, the state-of-the-art method SnD~\cite{Yu24} is related to DiVE.
Specifically, it leverages a reference dataset~(e.g., CC3M) for feature distillation, i.e., constraining the image embeddings from the pre-trained and fine-tuning models to be identical during continual learning.
In our context, this can be viewed as forcing the difference vectors to be zero.
However, such a strong constraint may hinder adaptation to target data.
In contrast, DiVE allows for non-zero difference vectors, offering greater flexibility and better performance~(see Tables~\ref{tab:comp}, \ref{tab:zero}, and \ref{tab:iwild_fmow}).



\noindent
\textbf{Weight Ensemble.}
In an orthogonal line of research, several studies~\cite{Wortsman22,Ilharco22} demonstrated that ensembling the weights of pre-trained and fine-tuned models can improve OOD and zero-shot performance.
Although they were originally proposed in integration with vanilla FT, DiVE can also benefit from such ensembling strategies.
We present the results of integrating DiVE into this strategy in Sec.~\ref{sec:app_wise} in the Appendix.","\textbf{Robust Fine-tuning for vision-language models.}
Vision-language models, such as CLIP~\cite{Radford21}, demonstrate strong generalization abilities in OOD and zero-shot settings.
However, empirical evidence suggests that vanilla FT, which updates the weights of the image encoder using downstream data, compromises these strengths~\cite{Wortsman22,Ilharco22}.
To tackle this challenge, robust fine-tuning methods for vision-language models have been proposed.

An early baseline method, LP-FT~\cite{Kumar22}, first applies linear probing then fine-tunes the entire weights to enhance OOD performance.

FLYP~\cite{Goyal23} uses vision-language contrastive learning for fine-tuning.
This choice can enhance OOD performance over vanilla FT by considering both image and text encoders.

ARF~\cite{Han24} is the first robust fine-tuning method that explicitly aims to enhance not only OOD but also zero-shot performance.
It combines FLYP and a replay technique~\cite{Rolnick19}, which involves training on a reference dataset~(e.g., CC3M~\cite{Sharma18_cc3m}) that resembles the pre-training data, to help recover the inherent generalization ability of the pre-trained model.

While FLYP and ARF show promising performance, both OOD and zero-shot performance can still be improved.
DiVE improves them by introducing a new perspective: preserving the geometric structure of the embeddings.


\noindent
\textbf{Geometry of Embeddings of Contrastive Learning Models.}
Contrastive learning has been widely used for representation learning on various modalities~\cite{Chen20_simclr,Gao21}.
Most contrastive learning methods adopt $L_2$-normalized embeddings for stable learning~\cite{Xu18}, which causes the embeddings to lie on a unit hypersphere.
\citet{Wang20_contrastive} found that the relative positions between hyperspherical embeddings, i.e., the geometric structure, play a crucial role in generalization in contrastive learning models.
Studies~\cite{Goel22,Oh23,Yamaguchi25} have shown that this structure is also crucial for contrastive pre-trained vision-language models, such as CLIP.

Among these, CyCLIP~\cite{Goel22} shares some concepts with DiVE in constraining the geometric structure of the embeddings.
Specifically, it constrains the cosine similarities between in-modal and cross-modal embeddings during pre-training.
We found that such a cosine similarity-based constraint is insufficient to preserve the geometric structure and improve performance, as it captures only the angular relationship between embeddings.
In contrast, our difference vector-based constraint achieves both~(see Table~\ref{table:cyclip}).



\noindent
\textbf{Maintaining Zero-shot Performance.}
There are several approaches for maintaining the zero-shot performance of vision-language models during fine-tuning, not only robust fine-tuning.
Prompt learning~\cite{Zhou22_coop,Zhou22_cocoop,Khattak23} optimizes a set of learnable prompt vectors, while freezing the weights of the pre-trained model.
This strategy perfectly maintains the generalization ability.
However, as reported by \cite{Shu23}, its classification performance on target data is often limited due to the small number of learnable parameters.

Methods for continual learning~\cite{Wang24} can be adapted to our setting by interpreting the contrastive pre-training as the previous task.
For example, the state-of-the-art method SnD~\cite{Yu24} is related to DiVE.
Specifically, it leverages a reference dataset~(e.g., CC3M) for feature distillation, i.e., constraining the image embeddings from the pre-trained and fine-tuning models to be identical during continual learning.
In our context, this can be viewed as forcing the difference vectors to be zero.
However, such a strong constraint may hinder adaptation to target data.
In contrast, DiVE allows for non-zero difference vectors, offering greater flexibility and better performance~(see Tables~\ref{tab:comp}, \ref{tab:zero}, and \ref{tab:iwild_fmow}).



\noindent
\textbf{Weight Ensemble.}
In an orthogonal line of research, several studies~\cite{Wortsman22,Ilharco22} demonstrated that ensembling the weights of pre-trained and fine-tuned models can improve OOD and zero-shot performance.
Although they were originally proposed in integration with vanilla FT, DiVE can also benefit from such ensembling strategies.
We present the results of integrating DiVE into this strategy in Sec.~\ref{sec:app_wise} in the Appendix.","Robust Fine-tuning for vision-language models.Vision-
language models, such as CLIP (Radford et al. 2021),
demonstrate strong generalization abilities in OOD and
zero-shot settings. However, empirical evidence suggests
that vanilla FT, which updates the weights of the im-
age encoder using downstream data, compromises these
strengths (Wortsman et al. 2022; Ilharco et al. 2022). To
tackle this challenge, robust fine-tuning methods for vision-
language models have been proposed. An early baseline
method, LP-FT (Kumar et al. 2022), first applies linear prob-
ing then fine-tunes the entire weights to enhance OOD per-
formance. FLYP (Goyal et al. 2023) uses vision-language
contrastive learning for fine-tuning. This choice can enhance
OOD performance over vanilla FT by considering both im-
age and text encoders. ARF (Han et al. 2024) is the first
robust fine-tuning method that explicitly aims to enhance
not only OOD but also zero-shot performance. It combines
FLYP and a replay technique (Rolnick et al. 2019), which in-
volves training on a reference dataset (e.g., CC3M (Sharma
et al. 2018)) that resembles the pre-training data, to help re-
cover the inherent generalization ability of the pre-trained
model. While FLYP and ARF show promising performance,
both OOD and zero-shot performance can still be improved.
DiVE improves them by introducing a new perspective: pre-
serving the geometric structure of the embeddings.
Geometry of Embeddings of Contrastive Learning Mod-
els.Contrastive learning has been widely used for represen-
tation learning on various modalities (Chen et al. 2020; Gao,
Yao, and Chen 2021). Most contrastive learning methods
adoptL 2-normalized embeddings for stable learning (Xu
and Durrett 2018), which causes the embeddings to lie on
a unit hypersphere. Wang and Isola (2020) found that the
relative positions between hyperspherical embeddings, i.e.,
the geometric structure, play a crucial role in generaliza-
tion in contrastive learning models. Studies (Goel et al.
2022; Oh et al. 2023; Yamaguchi et al. 2025) have shown
that this structure is also crucial for contrastive pre-trained
vision-language models, such as CLIP. Among these, Cy-
CLIP (Goel et al. 2022) shares some concepts with DiVE
in constraining the geometric structure of the embeddings.
Specifically, it constrains the cosine similarities between in-
modal and cross-modal embeddings during pre-training. We
found that such a cosine similarity-based constraint is in-
sufficient to preserve the geometric structure and improve
performance, as it captures only the angular relationship be-
tween embeddings. In contrast, our difference vector-based
constraint achieves both (see Table 7).
refx
tdxreftsunflowers, another
common crop in this
region
A photo of fox
Image
Encoder
(pre-trained)
Image
Encoder
(fine-tuning)
Text
Encoder
(pre-trained)
Text
Encoder
(fine-tuning)Contrastive
loss
sunflowers, another
common crop in this
region
A photo of foxSunflowers, another
common crop in this
region
A photo of foxDifference
Vector
Equalization
tdy Prompt fromDifference vectorFigure 2: Overview of our proposed method,Difference Vector Equalization(DiVE). It uses contrastive loss for fine-tuning.
While fine-tuning on target data, it constrains all difference vectors for reference data (xrefandtref) to be equal.
Maintaining Zero-shot Performance.There are several
approaches for maintaining the zero-shot performance of
vision-language models during fine-tuning, not only robust
fine-tuning. Prompt learning (Zhou et al. 2022b,a; Khat-
tak et al. 2023) optimizes a set of learnable prompt vec-
tors, while freezing the weights of the pre-trained model.
This strategy perfectly maintains the generalization ability.
However, as reported by (Shu et al. 2023), its classifica-
tion performance on target data is often limited due to the
small number of learnable parameters. Methods for contin-
ual learning (Wang et al. 2024) can be adapted to our set-
ting by interpreting the contrastive pre-training as the previ-
ous task. For example, the state-of-the-art method SnD (Yu
et al. 2024) is related to DiVE. Specifically, it leverages a
reference dataset (e.g., CC3M) for feature distillation, i.e.,
constraining the image embeddings from the pre-trained and
fine-tuning models to be identical during continual learning.
In our context, this can be viewed as forcing the difference
vectors to be zero. However, such a strong constraint may
hinder adaptation to target data. In contrast, DiVE allows
for non-zero difference vectors, offering greater flexibility
and better performance (see Tables 2, 3, and 4).
Weight Ensemble.In an orthogonal line of research, several
studies (Wortsman et al. 2022; Ilharco et al. 2022) demon-
strated that ensembling the weights of pre-trained and fine-
tuned models can improve OOD and zero-shot performance.
Although they were originally proposed in integration with
vanilla FT, DiVE can also benefit from such ensembling
strategies. We present the results of integrating DiVE into
this strategy in Sec. A in the Appendix."
2511.09918v1,http://arxiv.org/abs/2511.09918v1,2025-11-13 03:33:39+00:00,MINDS: A Cross-cultural Dialogue Corpus for Social Norm Classification and Adherence Detection,"Social norms are implicit, culturally grounded expectations that guide interpersonal communication. Unlike factual commonsense, norm reasoning is subjective, context-dependent, and varies across cultures, posing challenges for computational models. Prior works provide valuable normative annotations but mostly target isolated utterances or synthetic dialogues, limiting their ability to capture the fluid, multi-turn nature of real-world conversations. In this work, we present Norm-RAG, a retrieval-augmented, agentic framework for nuanced social norm inference in multi-turn dialogues. Norm-RAG models utterance-level attributes including communicative intent, speaker roles, interpersonal framing, and linguistic cues and grounds them in structured normative documentation retrieved via a novel Semantic Chunking approach. This enables interpretable and context-aware reasoning about norm adherence and violation across multilingual dialogues. We further introduce MINDS (Multilingual Interactions with Norm-Driven Speech), a bilingual dataset comprising 31 multi-turn Mandarin-English and Spanish-English conversations. Each turn is annotated for norm category and adherence status using multi-annotator consensus, reflecting cross-cultural and realistic norm expression. Our experiments show that Norm-RAG improves norm detection and generalization, demonstrates improved performance for culturally adaptive and socially intelligent dialogue systems.","\label{section_related-work}
% Understanding social norms in language has attracted growing attention in NLP, with early work focusing on structured commonsense reasoning and normative inference from textual scenarios. \citet{forbes2020socialchem} introduced the SocialChem dataset, capturing ethical judgments and normative statements from everyday situations, laying the groundwork for social norm classification. Building on this, \citet{ziems2022normdial} proposed NormDial, framing norm understanding as a dialog-level task that incorporates moral judgment and norm classification within multi-turn interactions. \citet{ghosh2023normsage} extended this line by integrating rich context, commonsense cues, and symbolic reasoning for norm extraction. Other studies have explored the modeling of values, affective triggers, and cultural conditioning of social behaviors \cite{sap2022atlas, jang2022modeling, liu2021k}. Commonsense reasoning frameworks like ATOMIC \cite{sap2019atomic} and Event2Mind \cite{rashkin2018event2mind} contribute to modeling intents and implications behind actions, which are vital for norm interpretation. However, these approaches often treat norms as isolated judgments or focus on single-turn scenarios, lacking holistic models of conversational flow, speaker roles, and context evolution. Our work addresses this gap by modeling fine-grained utterance-level attributes across multi-turn dialogues to enable more accurate and contextual social norm detection.
Early efforts in modeling social norms computationally have centered around static textual contexts \cite{ziems2023normbank, sap2019atomic, rashkin2018event2mind, emelin2020moral, jiang2021can, kim2022prosocialdialog, gu2021dream, ziems2022moral, ch2023sociocultural}. Social Chemistry 101 \cite{forbes2020social} introduced Rules-of-Thumb (RoTs) which are defined as free-text normative statements tied to situational prompts annotated with categorical labels capturing legality, moral foundations, and cultural expectations. While this large-scale resource enabled pretraining norm-aware language models, its static, monologic format restricts application to real-time dialogic settings. To overcome the lack of interactive context, NormSage \cite{fung2022normsage} proposed a zero-shot prompting method for extracting culture-specific norms from dialogues across languages using GPT-3, creating the NormsKB knowledge base. Though NormSage allows dynamic norm discovery and cross-cultural applicability, it lacks annotations for turn-level adherence or violation, thereby limiting its utility in norm-tracking tasks. 

NormDial \cite{li2023normdial} advanced the field by annotating each dialogue turn with adherence, violation, or irrelevance labels, using human-in-the-loop generation of synthetic conversations grounded in American and Chinese norms. However, its reliance on synthetic data and predefined norm templates limits its coverage of spontaneous and organically evolving dialogues. SocialDial \cite{zhan2023socialdial} is a large-scale, monocultural resource centered on a Chinese ontology of social norms (5 categories, 14 subcategories). Its evaluation tasks involve predicting dialogue-level social factors (\emph{e.g.}, distance, relation, location, formality, topic) and detecting norm violations within Chinese cultural contexts. In contrast, our work introduces a cross-cultural, bilingual dataset spanning Mandarin–English and Spanish–English conversations derived from real conversational data, annotated not only for norm adherence but also for underlying speaker-level features such as intent, cue, and interpersonal alignment. Furthermore, we propose a dual-task framework that performs latent norm discovery alongside turn-level adherence classification, without assuming access to predefined norm statements. This approach offers a more holistic and dynamic modeling of social norms in conversation, addressing key limitations in prior datasets and moving closer to deployable, socially intelligent dialog systems.
%+++++++++++++++++++++++ Dataset +++++++++++++++++++++++
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{latex/Figures/Distribution.pdf}
    \caption{Distribution of annotated social norms across languages, norm categories and status (adherence/violation) labels. Numbers in parenthesis indicate sample percentage contribution in the database.}
    \label{fig:dist}
    \vspace{-0.15in}
\end{figure*}","Early efforts in modeling social norms computationally have centered around static textual contexts \cite{ziems2023normbank, sap2019atomic, rashkin2018event2mind, emelin2020moral, jiang2021can, kim2022prosocialdialog, gu2021dream, ziems2022moral, ch2023sociocultural}. Social Chemistry 101 \cite{forbes2020social} introduced Rules-of-Thumb (RoTs) which are defined as free-text normative statements tied to situational prompts annotated with categorical labels capturing legality, moral foundations, and cultural expectations. While this large-scale resource enabled pretraining norm-aware language models, its static, monologic format restricts application to real-time dialogic settings. To overcome the lack of interactive context, NormSage \cite{fung2022normsage} proposed a zero-shot prompting method for extracting culture-specific norms from dialogues across languages using GPT-3, creating the NormsKB knowledge base. Though NormSage allows dynamic norm discovery and cross-cultural applicability, it lacks annotations for turn-level adherence or violation, thereby limiting its utility in norm-tracking tasks. 

NormDial \cite{li2023normdial} advanced the field by annotating each dialogue turn with adherence, violation, or irrelevance labels, using human-in-the-loop generation of synthetic conversations grounded in American and Chinese norms. However, its reliance on synthetic data and predefined norm templates limits its coverage of spontaneous and organically evolving dialogues. SocialDial \cite{zhan2023socialdial} is a large-scale, monocultural resource centered on a Chinese ontology of social norms (5 categories, 14 subcategories). Its evaluation tasks involve predicting dialogue-level social factors (\emph{e.g.}, distance, relation, location, formality, topic) and detecting norm violations within Chinese cultural contexts. In contrast, our work introduces a cross-cultural, bilingual dataset spanning Mandarin–English and Spanish–English conversations derived from real conversational data, annotated not only for norm adherence but also for underlying speaker-level features such as intent, cue, and interpersonal alignment. Furthermore, we propose a dual-task framework that performs latent norm discovery alongside turn-level adherence classification, without assuming access to predefined norm statements. This approach offers a more holistic and dynamic modeling of social norms in conversation, addressing key limitations in prior datasets and moving closer to deployable, socially intelligent dialog systems.","Early efforts in modeling social norms computa-
tionally have centered around static textual contexts
(Ziems et al., 2023; Sap et al., 2019; Rashkin et al.,
2018; Emelin et al., 2020; Jiang et al., 2021; Kimet al., 2022; Gu et al., 2021; Ziems et al., 2022; CH-
Wang et al., 2023). Social Chemistry 101 (Forbes
et al., 2020) introduced Rules-of-Thumb (RoTs)
which are defined as free-text normative statements
tied to situational prompts annotated with categor-
ical labels capturing legality, moral foundations,
and cultural expectations. While this large-scale
resource enabled pretraining norm-aware language
models, its static, monologic format restricts ap-
plication to real-time dialogic settings. To over-
come the lack of interactive context, NormSage
(Fung et al., 2022) proposed a zero-shot prompting
method for extracting culture-specific norms from
dialogues across languages using GPT-3, creating
the NormsKB knowledge base. Though NormSage
allows dynamic norm discovery and cross-cultural
applicability, it lacks annotations for turn-level ad-
herence or violation, thereby limiting its utility in
norm-tracking tasks.
NormDial (Li et al., 2023) advanced the field
by annotating each dialogue turn with adherence,
violation, or irrelevance labels, using human-in-
the-loop generation of synthetic conversations
grounded in American and Chinese norms. How-
ever, its reliance on synthetic data and predefined
norm templates limits its coverage of spontaneous
and organically evolving dialogues. SocialDial
(Zhan et al., 2023) is a large-scale, monocultural
resource centered on a Chinese ontology of social
norms (5 categories, 14 subcategories). Its evalua-
tion tasks involve predicting dialogue-level social
factors (e.g., distance, relation, location, formal-
ity, topic) and detecting norm violations within
Chinese cultural contexts. In contrast, our work
introduces a cross-cultural, bilingual dataset span-
ning Mandarin–English and Spanish–English con-
versations derived from real conversational data,
annotated not only for norm adherence but also
for underlying speaker-level features such as intent,
cue, and interpersonal alignment. Furthermore, we
propose a dual-task framework that performs la-
tent norm discovery alongside turn-level adherence
classification, without assuming access to prede-
fined norm statements. This approach offers a more
holistic and dynamic modeling of social norms in
conversation, addressing key limitations in prior
datasets and moving closer to deployable, socially
intelligent dialog systems."
2511.09827v1,http://arxiv.org/abs/2511.09827v1,2025-11-13 00:19:18+00:00,AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting,"We present a novel framework for animating humans in 3D scenes using 3D Gaussian Splatting (3DGS), a neural scene representation that has recently achieved state-of-the-art photorealistic results for novel-view synthesis but remains under-explored for human-scene animation and interaction. Unlike existing animation pipelines that use meshes or point clouds as the underlying 3D representation, our approach introduces the use of 3DGS as the 3D representation to the problem of animating humans in scenes. By representing humans and scenes as Gaussians, our approach allows for geometry-consistent free-viewpoint rendering of humans interacting with 3D scenes. Our key insight is that the rendering can be decoupled from the motion synthesis and each sub-problem can be addressed independently, without the need for paired human-scene data. Central to our method is a Gaussian-aligned motion module that synthesizes motion without explicit scene geometry, using opacity-based cues and projected Gaussian structures to guide human placement and pose alignment. To ensure natural interactions, we further propose a human-scene Gaussian refinement optimization that enforces realistic contact and navigation. We evaluate our approach on scenes from Scannet++ and the SuperSplat library, and on avatars reconstructed from sparse and dense multi-view human capture. Finally, we demonstrate that our framework allows for novel applications such as geometry-consistent free-viewpoint rendering of edited monocular RGB videos with new animated humans, showcasing the unique advantage of 3DGS for monocular video-based human animation.","\textbf{Neural Rendering}
Following the publication of NeRF \cite{mildenhall2020nerf}, there has been significant research on Neural Rendering ~\cite{nerf_review}. Nerf is limited by its computational complexity and despite several follow-up improvements~\cite{mueller2022instant, barron2022mip, barron2023zipnerf, nerfstudio}, the high computational cost of NeRF rermain. 3DGS introduced in \cite{kerbl3Dgaussians} addresses this limitation by representing scenes with an explicit set of primitives shaped as 3D Gaussians, extending previous work ~\cite{lassner2021pulsar}. 3DGS rasterizes Gaussian primitives into  images using a splatting algorithm \cite{westover1991phdsplatting}.
3DGS originally designed for static scenes has been extended to dynamic scenes ~\cite{shaw2023swags, luiten2023dynamic, Wu2024CVPR, lee2024ex4dgs, li2023spacetime}, slam-based reconstruction, \cite{keetha2024splatam}, mesh reconstruction \cite{Huang2DGS2024, guedon2023sugar} and NVS from sparse cameras
\cite{mihajlovic2024SplatFields}. 








\textbf{Human Reconstruction and Neural Rendering}
Mesh-based templates~\cite{SMPL-X:2019, smpl2015loper} have been used to recover 3D human shape and pose from images and video \cite{Bogo2016keepitsmpl, kanazawaHMR18}. However, this does not allow for photoreal renderings. In
\cite{alldieck2018video, alldieck19cvpr} recover a re-posable human avatar from monocular RGB. However their use of a mesh template also does not allow for photorealistic renderings. Implicit functions \cite{mescheder2019occupancy, park2019deepsdf} have also been utilized to reconstruct detailed 3D clothed humans~\cite{chen2021snarf, alldieck2021imghum, saito2020pifuhd, he2021arch++, huang2020arch, deng2020nasa}. However, they are also unable to generate photorealistic renderings and are often not reposable. Several works \cite{peng2021animatable, guo2023vid2avatar, weng_humannerf_2022_cvpr, jian2022neuman, haberman2023hdhumans, heminggaberman2024trihuman, li2022tava, liu2021neural, xu2021hnerf}
build a controllable NeRF that produces photorealistic images of humans from input videos. Unlike us, they do not model human-scene interactions. With the advent of 3DGS, several recent papers use a 3DGS formulation \cite{kocabas2023hugs, qian20233dgsavatar, moreau2024human, abdal2023gaussian, zielonka2023drivable, moon2024exavatar, li2024animatablegaussians, pang2024ash, lei2023gart,
hu2024gaussianavatar, li2024animatable2, zheng2024gpsgaussian, jiang2024robust, dhamo2023headgas, qian20233dgs, xu2024gaussian, junkawitsch2025eva} to build controllable human or face avatars. Unlike our method,  they do not model human-scene interactions. Prior works have also extended the 3DGS formulation to model humans along with their environment, \cite{xue2024hsr, tomie, mir2025gaspacho}, but unlike us, they either do not focus on human animation in 3D scenes.












\textbf{Humans and Scenes}
Human-scene interaction is a recurrent topic of study in computer vision and graphics. Early works \cite{fouhey2014people, wang2017binge, gupta20113d} model affordances and human-object interactions using monocular RGB.
The collection of several recent human-scene interaction datasets \cite{Hassan2021-gb, mir20hps, hassan2019prox, savva2016pigraphs, taheri2020grab, bhatnagar22behave, jiang2024scaling, cheng2023dnarendering, zhang2022couch} has allowed the computer vision community to make significant progress in joint 3D reconstruction of human-object interactions \cite{xie2022chore, xie2023vistracker, xie2024template, zhang2020phosa}. These datasets have also led to the development of methods that synthesize object conditioned controllable human motion \cite{zhang2022couch, starke2019neuralstate, hassan21cvpr, diller2023cghoi}. All these methods represent humans and scenes as 3D meshes and inherit the limitations of mesh-based representations including their inability to generate photorealistic images, while our method allows for photorealistic renderings of humans and scenes.

\textbf{Human Animation}
Human animation is another extensively studied problem in vision and graphics. 
Motion matching~\citep{reitsma2007evaluating}, learned motion matching \citep{clavet16motionmatching, holden2020learned} and motion graphs ~\citep{lee2002interactive,fang2003efficient,kovar2008motion,safonova2004synthesizing,Safonova:2007:InterpolatedGraphs} are common methods employed in the video-gaming industry for generating kinematic motion sequences. Deep learning variants \cite{holden2017phase, starke19neural, starke21martialarts, starke20local} have also gained popularity. Diffusion Models \cite{tevet2023human} have emerged as a powerful paradigm for human motion synthesis. Several follow-up works extend the original Motion Diffusion model with physics\cite{yuan2023physdiff}, blended-positional encoding \cite{barquero2024seamless}, and for fine-grained controllable motion synthesis \cite{ karunratanakul2023dno, pinyoanuntapong2024controlmm, xie2024omnicontrol}. Reinforcement learning \cite{zhang2022wanderings, zhao2023dimos} is another oft-used paradigm used for motion synthesis. Diffusion models have also been used as latent-motion models \cite{Zhao:DartControl:2025} but unlike us, they only focus on clean, noise-free mesh-based scene representations  and ignore the rendering aspects of human-scene interaction leading to limited rendering quality and photorealism.","\textbf{Neural Rendering}
Following the publication of NeRF \cite{mildenhall2020nerf}, there has been significant research on Neural Rendering ~\cite{nerf_review}. Nerf is limited by its computational complexity and despite several follow-up improvements~\cite{mueller2022instant, barron2022mip, barron2023zipnerf, nerfstudio}, the high computational cost of NeRF rermain. 3DGS introduced in \cite{kerbl3Dgaussians} addresses this limitation by representing scenes with an explicit set of primitives shaped as 3D Gaussians, extending previous work ~\cite{lassner2021pulsar}. 3DGS rasterizes Gaussian primitives into  images using a splatting algorithm \cite{westover1991phdsplatting}.
3DGS originally designed for static scenes has been extended to dynamic scenes ~\cite{shaw2023swags, luiten2023dynamic, Wu2024CVPR, lee2024ex4dgs, li2023spacetime}, slam-based reconstruction, \cite{keetha2024splatam}, mesh reconstruction \cite{Huang2DGS2024, guedon2023sugar} and NVS from sparse cameras
\cite{mihajlovic2024SplatFields}. 








\textbf{Human Reconstruction and Neural Rendering}
Mesh-based templates~\cite{SMPL-X:2019, smpl2015loper} have been used to recover 3D human shape and pose from images and video \cite{Bogo2016keepitsmpl, kanazawaHMR18}. However, this does not allow for photoreal renderings. In
\cite{alldieck2018video, alldieck19cvpr} recover a re-posable human avatar from monocular RGB. However their use of a mesh template also does not allow for photorealistic renderings. Implicit functions \cite{mescheder2019occupancy, park2019deepsdf} have also been utilized to reconstruct detailed 3D clothed humans~\cite{chen2021snarf, alldieck2021imghum, saito2020pifuhd, he2021arch++, huang2020arch, deng2020nasa}. However, they are also unable to generate photorealistic renderings and are often not reposable. Several works \cite{peng2021animatable, guo2023vid2avatar, weng_humannerf_2022_cvpr, jian2022neuman, haberman2023hdhumans, heminggaberman2024trihuman, li2022tava, liu2021neural, xu2021hnerf}
build a controllable NeRF that produces photorealistic images of humans from input videos. Unlike us, they do not model human-scene interactions. With the advent of 3DGS, several recent papers use a 3DGS formulation \cite{kocabas2023hugs, qian20233dgsavatar, moreau2024human, abdal2023gaussian, zielonka2023drivable, moon2024exavatar, li2024animatablegaussians, pang2024ash, lei2023gart,
hu2024gaussianavatar, li2024animatable2, zheng2024gpsgaussian, jiang2024robust, dhamo2023headgas, qian20233dgs, xu2024gaussian, junkawitsch2025eva} to build controllable human or face avatars. Unlike our method,  they do not model human-scene interactions. Prior works have also extended the 3DGS formulation to model humans along with their environment, \cite{xue2024hsr, tomie, mir2025gaspacho}, but unlike us, they either do not focus on human animation in 3D scenes.












\textbf{Humans and Scenes}
Human-scene interaction is a recurrent topic of study in computer vision and graphics. Early works \cite{fouhey2014people, wang2017binge, gupta20113d} model affordances and human-object interactions using monocular RGB.
The collection of several recent human-scene interaction datasets \cite{Hassan2021-gb, mir20hps, hassan2019prox, savva2016pigraphs, taheri2020grab, bhatnagar22behave, jiang2024scaling, cheng2023dnarendering, zhang2022couch} has allowed the computer vision community to make significant progress in joint 3D reconstruction of human-object interactions \cite{xie2022chore, xie2023vistracker, xie2024template, zhang2020phosa}. These datasets have also led to the development of methods that synthesize object conditioned controllable human motion \cite{zhang2022couch, starke2019neuralstate, hassan21cvpr, diller2023cghoi}. All these methods represent humans and scenes as 3D meshes and inherit the limitations of mesh-based representations including their inability to generate photorealistic images, while our method allows for photorealistic renderings of humans and scenes.

\textbf{Human Animation}
Human animation is another extensively studied problem in vision and graphics. 
Motion matching~\citep{reitsma2007evaluating}, learned motion matching \citep{clavet16motionmatching, holden2020learned} and motion graphs ~\citep{lee2002interactive,fang2003efficient,kovar2008motion,safonova2004synthesizing,Safonova:2007:InterpolatedGraphs} are common methods employed in the video-gaming industry for generating kinematic motion sequences. Deep learning variants \cite{holden2017phase, starke19neural, starke21martialarts, starke20local} have also gained popularity. Diffusion Models \cite{tevet2023human} have emerged as a powerful paradigm for human motion synthesis. Several follow-up works extend the original Motion Diffusion model with physics\cite{yuan2023physdiff}, blended-positional encoding \cite{barquero2024seamless}, and for fine-grained controllable motion synthesis \cite{ karunratanakul2023dno, pinyoanuntapong2024controlmm, xie2024omnicontrol}. Reinforcement learning \cite{zhang2022wanderings, zhao2023dimos} is another oft-used paradigm used for motion synthesis. Diffusion models have also been used as latent-motion models \cite{Zhao:DartControl:2025} but unlike us, they only focus on clean, noise-free mesh-based scene representations  and ignore the rendering aspects of human-scene interaction leading to limited rendering quality and photorealism.",
2511.09475v1,http://arxiv.org/abs/2511.09475v1,2025-11-12 16:47:36+00:00,Enhancing Explainability in Solar Energetic Particle Event Prediction: A Global Feature Mapping Approach,"Solar energetic particle (SEP) events, as one of the most prominent manifestations of solar activity, can generate severe hazardous radiation when accelerated by solar flares or shock waves formed aside from coronal mass ejections (CMEs). However, most existing data-driven methods used for SEP predictions are operated as black-box models, making it challenging for solar physicists to interpret the results and understand the underlying physical causes of such events rather than just obtain a prediction. To address this challenge, we propose a novel framework that integrates global explanations and ad-hoc feature mapping to enhance model transparency and provide deeper insights into the decision-making process. We validate our approach using a dataset of 341 SEP events, including 244 significant (>=10 MeV) proton events exceeding the Space Weather Prediction Center S1 threshold, spanning solar cycles 22, 23, and 24. Furthermore, we present an explainability-focused case study of major SEP events, demonstrating how our method improves explainability and facilitates a more physics-informed understanding of SEP event prediction.","\label{related_work}
% Machine Learning in SEP Prediction (2-3 paragraphs)
% Overview of existing ML-based methodologies for SEP event forecasting (use Manolis’ and Whitman’s review papers, you should be coauthors in both of them)
% The ""black-box"" issue in ML-based models
% Discuss all aspects of being black box – algorithmic complexity, data dimensions, but also interactions between these data points

Traditional methods for SEP forecasting, as classified in \cite{Whitman2023}, can be divided into three main categories: empirical models, physics-based models, and hybrid methods. On the downside, empirical models are based on historical data and statistical relationships, making them sensitive to inaccuracies when encountering new or unprecedented information. Physics-based models tend to mimic the underlying physical processes involved, which are often computationally expensive and require a large amount of input parameters. Hybrid models combine the above-mentioned aspects and focus on balancing the trade-offs between accuracy, computational efficiency, and general applicability. The strength of such models compared to empirical methods comes from their capability of handling historical information, allowing them to learn complex, nonlinear relationships between input variables and SEP occurrence without the need for a priori incorporation of any physical knowledge.

More recently, researchers have begun exploring machine learning-based techniques as a potential avenue toward SEP forecasting. Typically, the built ML models involve utilizing training subsets (for identifying predictive relationships) and validation subsets (to assess forecasting performance). The prediction outcomes can be probabilistic, categorical, binary, or deterministic \cite{Whitman2023}. Examples include the binary classification of SEP events from GOES X-ray and proton channel observations \cite{Rotti2024} \cite{Boubrahimi2017} \cite{9750381}, solar radio flux data analysis using neural networks and genetic algorithms \cite{Kim2018}, and predictions based on the properties of solar flares and CMEs using ensemble methods \cite{Huang2012} \cite{9377906} \cite{aji2023}. While solar flares themselves do not directly cause SEPs, they are often associated with the acceleration of energetic particles. In fact, solar flares can trigger the acceleration of electrons and protons that are released during the flare, contributing to SEP events \cite{10302639} \cite{Pandey2023}. For solar flare prediction, both full-disk \cite{9671322} \cite{Pandey2022} \cite{10722839} and active region-based \cite{9378006} \cite{10460016, Hong2023, 10431579} \cite{9750381} approaches have shown significant impacts by utilizing derived time series features.
% Consider this:
% The characteristics of solar flare evolution are important as they are intricately linked to the dynamic behavior of solar active regions, as delineated in prior research \citep{Chen2022, Benz2008, 9671322, Pandey2023, 10386908}. For solar flare prediction, both full-disk \citep{9671322, Pandey2022, pandey2023interpretable, 10722839} and active region-based \citep{9377906, 9750381, 9378006, Hong2023, 10460016, 10431579} approaches have shown significant impacts by utilizing derived time series features.

Regardless of their predictive capabilities, ML methodologies are often deployed as opaque systems (i.e., ""black-box""), which poses challenges when physical interpretability is crucial \cite{pandey2023interpretable}. Complex models like deep neural networks, while capable of achieving high performance, usually have a hard time explicitly tracking the relationships between inputs and predicted outcomes due to their multi-layered, nonlinear structure. Moreover, SEP prediction normally involves high-dimensional data from multiple instruments and locations. The intricate interactions between these multi-dimensional data points are often abstracted within model architectures, making it challenging to explain the predictions in physically meaningful terms. In high-stakes domains, where decisions carry significant consequences, model interpretability is not merely a desirable characteristic but a critical prerequisite. The capacity to explain the rationale behind a model's output is indispensable for fostering a desire system and for the validation of the system's underlying logic.
% As a result, all involved parties with an interest in forecasting SEP events are still cautious to completely rely on the output of these black-box ML systems, emphasizing the critical need for improved interpretability.

% A variety of different methods have been proposed to better understand how the input features influence the model predictions, particularly in the area of Explainable Artificial Intelligence (XAI). A well-known approach is Local Interpretable Model-agnostic Explanations (LIME) \cite{Ribeiro2016}, which approximates the behavior of complex models locally with a simple interpretable model that fits around a specific data point. This approach provides an understanding of individual predictions by showing the impact of every potential feature surrounding that instance. Another notable method is SHapley Additive exPlanations (SHAP) \cite{trumbelj2011}\cite{trumbelj2013}\cite{Lundberg2017}, which uses concepts developed in cooperative game theory to calculate an importance value for each feature in a particular prediction. SHAP values also provide a unified measure of feature contributions, allowing for consistency in the attribution of predictions to input features. Counterfactual explanations \cite{Mothilal2020}\cite{Wiratunga2021} offer insights by highlighting small input feature modifications that would alter the predictions into desired outcomes. This aims to understand the conditions under which different predictions occur, leading to enhanced model transparency. Furthermore, methods that combine global and local interpretability, such as GLEAMS \cite{Visani2024}, were also proposed. By partitioning the input space, GLEAMS approximates more complex models with a simpler, interpretable one, enabling both local and global explanations.

Despite ongoing efforts, explainability remains a challenge in SEP event prediction. Existing methodologies mostly bring partial insights among parameters, and not all machine learning techniques integrate easily with current frameworks. Moreover, the rarity and intrinsic complex nature of SEP events further complicates the generation of reliable and physically meaningful explanations. Typically, explainability in SEP forecasting involves understanding the relationship between observational input variables (i.e., solar flare intensity, CME speed, magnetic connectivity, and solar radio flux measurements) and the predicted occurrence, intensity, or other characteristics of SEP events.","Traditional methods for SEP forecasting, as classified in \cite{Whitman2023}, can be divided into three main categories: empirical models, physics-based models, and hybrid methods. On the downside, empirical models are based on historical data and statistical relationships, making them sensitive to inaccuracies when encountering new or unprecedented information. Physics-based models tend to mimic the underlying physical processes involved, which are often computationally expensive and require a large amount of input parameters. Hybrid models combine the above-mentioned aspects and focus on balancing the trade-offs between accuracy, computational efficiency, and general applicability. The strength of such models compared to empirical methods comes from their capability of handling historical information, allowing them to learn complex, nonlinear relationships between input variables and SEP occurrence without the need for a priori incorporation of any physical knowledge.

More recently, researchers have begun exploring machine learning-based techniques as a potential avenue toward SEP forecasting. Typically, the built ML models involve utilizing training subsets (for identifying predictive relationships) and validation subsets (to assess forecasting performance). The prediction outcomes can be probabilistic, categorical, binary, or deterministic \cite{Whitman2023}. Examples include the binary classification of SEP events from GOES X-ray and proton channel observations \cite{Rotti2024} \cite{Boubrahimi2017} \cite{9750381}, solar radio flux data analysis using neural networks and genetic algorithms \cite{Kim2018}, and predictions based on the properties of solar flares and CMEs using ensemble methods \cite{Huang2012} \cite{9377906} \cite{aji2023}. While solar flares themselves do not directly cause SEPs, they are often associated with the acceleration of energetic particles. In fact, solar flares can trigger the acceleration of electrons and protons that are released during the flare, contributing to SEP events \cite{10302639} \cite{Pandey2023}. For solar flare prediction, both full-disk \cite{9671322} \cite{Pandey2022} \cite{10722839} and active region-based \cite{9378006} \cite{10460016, Hong2023, 10431579} \cite{9750381} approaches have shown significant impacts by utilizing derived time series features.



Regardless of their predictive capabilities, ML methodologies are often deployed as opaque systems (i.e., ""black-box""), which poses challenges when physical interpretability is crucial \cite{pandey2023interpretable}. Complex models like deep neural networks, while capable of achieving high performance, usually have a hard time explicitly tracking the relationships between inputs and predicted outcomes due to their multi-layered, nonlinear structure. Moreover, SEP prediction normally involves high-dimensional data from multiple instruments and locations. The intricate interactions between these multi-dimensional data points are often abstracted within model architectures, making it challenging to explain the predictions in physically meaningful terms. In high-stakes domains, where decisions carry significant consequences, model interpretability is not merely a desirable characteristic but a critical prerequisite. The capacity to explain the rationale behind a model's output is indispensable for fostering a desire system and for the validation of the system's underlying logic.




Despite ongoing efforts, explainability remains a challenge in SEP event prediction. Existing methodologies mostly bring partial insights among parameters, and not all machine learning techniques integrate easily with current frameworks. Moreover, the rarity and intrinsic complex nature of SEP events further complicates the generation of reliable and physically meaningful explanations. Typically, explainability in SEP forecasting involves understanding the relationship between observational input variables (i.e., solar flare intensity, CME speed, magnetic connectivity, and solar radio flux measurements) and the predicted occurrence, intensity, or other characteristics of SEP events.",
2511.10287v1,http://arxiv.org/abs/2511.10287v1,2025-11-13 13:18:27+00:00,OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large Language Models,"Since Multimodal Large Language Models (MLLMs) are increasingly being integrated into everyday tools and intelligent agents, growing concerns have arisen regarding their possible output of unsafe contents, ranging from toxic language and biased imagery to privacy violations and harmful misinformation. Current safety benchmarks remain highly limited in both modality coverage and performance evaluations, often neglecting the extensive landscape of content safety. In this work, we introduce OutSafe-Bench, the first most comprehensive content safety evaluation test suite designed for the multimodal era. OutSafe-Bench includes a large-scale dataset that spans four modalities, featuring over 18,000 bilingual (Chinese and English) text prompts, 4,500 images, 450 audio clips and 450 videos, all systematically annotated across nine critical content risk categories. In addition to the dataset, we introduce a Multidimensional Cross Risk Score (MCRS), a novel metric designed to model and assess overlapping and correlated content risks across different categories. To ensure fair and robust evaluation, we propose FairScore, an explainable automated multi-reviewer weighted aggregation framework. FairScore selects top-performing models as adaptive juries, thereby mitigating biases from single-model judgments and enhancing overall evaluation reliability. Our evaluation of nine state-of-the-art MLLMs reveals persistent and substantial safety vulnerabilities, underscoring the pressing need for robust safeguards in MLLMs.","\label{sec:related_work}

\subsection{MLLM safety benchmarks}
Recent efforts have made notable progress in evaluating the trustworthiness of LLMs, as shown in Table \ref{tb:compare}. Benchmarks such as TrustLLM \cite{sun2024trustllm} and Trustworthy LLMs \cite{liu2023trustworthy} offer comprehensive assessments across a wide range of dimensions, including toxicity, stereotype bias, adversarial robustness, and privacy. In parallel, general-purpose benchmarks for MLLMs have emerged to evaluate their capabilities in visual perception, knowledge acquisition, reasoning, and commonsense understanding, as seen in works such as LVLM-Ehub \cite{xu2024lvlm} and LAMM \cite{yin2023lamm}. 

However, trustworthiness evaluation for MLLMs remains limited, especially in terms of modality coverage and content-level safety, where existing work tends to focus more on adversarial attacks than on holistic content safety. For instance, MM-SafeBench \cite{liu2024mm} is designed to assess safety in text-image scenarios, with an emphasis on image-based prompt attacks that exploit query-relevant visual inputs to trigger harmful model behavior. PrivQA \cite{li2025priv} introduces a privacy-focused benchmark containing 4,678 open-domain textual and 2,000 visual QA examples, targeting risks related to sensitive content exposure. Unicorn \cite{tu2023many} addresses visual content safety by providing 8,500 annotated threat scenarios derived from images. SafeBench \cite{xu2022safebench} is the first benchmark to support audio modality, featuring 2,300 text samples, 2,300 images, and 4,600 audio clips across sensitive domains such as consultation, cybersecurity, and ethics. However, its dataset size is relatively limited, and its focus spans a broad range of safety topics rather than emphasizing content-level risks. MultiTrust \cite{zhang2024multitrust} presents the largest dataset to date, with over 15,000 text-image samples and additional text-only tasks. It is also the first benchmark to examine cross-modal vulnerabilities, particularly from image-to-text generation, yet it offers limited coverage of modality types and content-specific safety aspects, sharing similar limitations with SafeBench. MLLMGuard  \cite{gu2024mllmguard} is the only benchmark that supports bilingual safety evaluation of MLLM-generated outputs (in English and Chinese). Despite its valuable multilingual perspective, it is constrained by a smaller dataset size and narrow modality support.

%Outsafe-Bench is the first benchmark to comprehensively cover all four modalities with a dedicated focus on content safety. It supports bilingual evaluation in both Chinese and English, enabling more accurate assessment of cross-lingual differences in model outputs. 


\subsection{Evaluation strategy of MLLMs}
Mainstream approaches to evaluating the properties of MLLMs can be broadly categorized into three types: rule-based evaluation \cite{li2024rule}, manual review, and LLM-as-judge methods \cite{zheng2023judging}. Among these, LLM-as-judge approaches have gained popularity for their scalability and automation, which include single-LLM systems, multi-LLM systems, and hybrid systems. Single-LLM systems, such as LLM-Eval \cite{lin2023llm} and G-Eval \cite{liu2023g}, are easy to deploy and scale but may suffer from individual model biases, which potentially results in inaccurate or skewed evaluations. In multi-LLM systems \cite{li2023prd, chan2023chateval}, multiple models interact through mechanisms like collaboration or deliberation to refine judgments and reach more robust conclusions. For instance, SafeBench \cite{xu2022safebench} employs a jury deliberation protocol, where multiple LLMs collectively assess content and check for consensus. However, these systems often select mainstream models as evaluators without justifying their expertise across different tasks, which may compromise fairness and task-specific reliability. Hybrid systems \cite{li2023collaborative, shankar2024validates} combine LLMs with human evaluators. This configuration enables human oversight to mitigate potential biases and bring subjective reasoning into complex evaluations. However, it also introduces challenges such as coordination overhead, inconsistency in standards, and difficulty in integrating human feedback effectively.","\subsection{MLLM safety benchmarks}
Recent efforts have made notable progress in evaluating the trustworthiness of LLMs, as shown in Table \ref{tb:compare}. Benchmarks such as TrustLLM \cite{sun2024trustllm} and Trustworthy LLMs \cite{liu2023trustworthy} offer comprehensive assessments across a wide range of dimensions, including toxicity, stereotype bias, adversarial robustness, and privacy. In parallel, general-purpose benchmarks for MLLMs have emerged to evaluate their capabilities in visual perception, knowledge acquisition, reasoning, and commonsense understanding, as seen in works such as LVLM-Ehub \cite{xu2024lvlm} and LAMM \cite{yin2023lamm}. 

However, trustworthiness evaluation for MLLMs remains limited, especially in terms of modality coverage and content-level safety, where existing work tends to focus more on adversarial attacks than on holistic content safety. For instance, MM-SafeBench \cite{liu2024mm} is designed to assess safety in text-image scenarios, with an emphasis on image-based prompt attacks that exploit query-relevant visual inputs to trigger harmful model behavior. PrivQA \cite{li2025priv} introduces a privacy-focused benchmark containing 4,678 open-domain textual and 2,000 visual QA examples, targeting risks related to sensitive content exposure. Unicorn \cite{tu2023many} addresses visual content safety by providing 8,500 annotated threat scenarios derived from images. SafeBench \cite{xu2022safebench} is the first benchmark to support audio modality, featuring 2,300 text samples, 2,300 images, and 4,600 audio clips across sensitive domains such as consultation, cybersecurity, and ethics. However, its dataset size is relatively limited, and its focus spans a broad range of safety topics rather than emphasizing content-level risks. MultiTrust \cite{zhang2024multitrust} presents the largest dataset to date, with over 15,000 text-image samples and additional text-only tasks. It is also the first benchmark to examine cross-modal vulnerabilities, particularly from image-to-text generation, yet it offers limited coverage of modality types and content-specific safety aspects, sharing similar limitations with SafeBench. MLLMGuard  \cite{gu2024mllmguard} is the only benchmark that supports bilingual safety evaluation of MLLM-generated outputs (in English and Chinese). Despite its valuable multilingual perspective, it is constrained by a smaller dataset size and narrow modality support.




\subsection{Evaluation strategy of MLLMs}
Mainstream approaches to evaluating the properties of MLLMs can be broadly categorized into three types: rule-based evaluation \cite{li2024rule}, manual review, and LLM-as-judge methods \cite{zheng2023judging}. Among these, LLM-as-judge approaches have gained popularity for their scalability and automation, which include single-LLM systems, multi-LLM systems, and hybrid systems. Single-LLM systems, such as LLM-Eval \cite{lin2023llm} and G-Eval \cite{liu2023g}, are easy to deploy and scale but may suffer from individual model biases, which potentially results in inaccurate or skewed evaluations. In multi-LLM systems \cite{li2023prd, chan2023chateval}, multiple models interact through mechanisms like collaboration or deliberation to refine judgments and reach more robust conclusions. For instance, SafeBench \cite{xu2022safebench} employs a jury deliberation protocol, where multiple LLMs collectively assess content and check for consensus. However, these systems often select mainstream models as evaluators without justifying their expertise across different tasks, which may compromise fairness and task-specific reliability. Hybrid systems \cite{li2023collaborative, shankar2024validates} combine LLMs with human evaluators. This configuration enables human oversight to mitigate potential biases and bring subjective reasoning into complex evaluations. However, it also introduces challenges such as coordination overhead, inconsistency in standards, and difficulty in integrating human feedback effectively.",
2511.09748v1,http://arxiv.org/abs/2511.09748v1,2025-11-12 21:25:50+00:00,How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation,"Large Language Models (LLMs) excel at evaluating machine translation (MT), but their scale and cost hinder deployment on edge devices and in privacy-sensitive workflows. We ask: how small can you get while still detecting meaning-altering translation errors? Focusing on English->German Critical Error Detection (CED), we benchmark sub-2B models (LFM2-350M, Qwen-3-0.6B/1.7B, Llama-3.2-1B-Instruct, Gemma-3-1B) across WMT21, WMT22, and SynCED-EnDe-2025. Our framework standardizes prompts, applies lightweight logit-bias calibration and majority voting, and reports both semantic quality (MCC, F1-ERR/F1-NOT) and compute metrics (VRAM, latency, throughput). Results reveal a clear sweet spot around one billion parameters: Gemma-3-1B provides the best quality-efficiency trade-off, reaching MCC=0.77 with F1-ERR=0.98 on SynCED-EnDe-2025 after merged-weights fine-tuning, while maintaining 400 ms single-sample latency on a MacBook Pro M4 Pro (24 GB). At larger scale, Qwen-3-1.7B attains the highest absolute MCC (+0.11 over Gemma) but with higher compute cost. In contrast, ultra-small models (0.6B) remain usable with few-shot calibration yet under-detect entity and number errors. Overall, compact, instruction-tuned LLMs augmented with lightweight calibration and small-sample supervision can deliver trustworthy, on-device CED for MT, enabling private, low-cost error screening in real-world translation pipelines. All datasets, prompts, and scripts are publicly available at our GitHub repository.","\label{sec:related}

\subsection{Machine Translation Evaluation and Quality Estimation}

Automatic machine translation (MT) evaluation has long relied on reference-based similarity metrics such as BLEU \cite{papineni2002bleu} and METEOR \cite{banerjee2005meteor}.  
While computationally efficient, these n-gram overlap scores correlate only weakly with human judgments of meaning preservation.  
Subsequent learned metrics such as COMET \cite{rei2020comet} improved this alignment by leveraging multilingual encoders and regression heads trained on human-rated examples.  
Complementary human frameworks such as MQM \cite{lommel2014multidimensional} introduced structured taxonomies of translation errors, forming the basis of evaluation protocols in WMT shared tasks \cite{specia2020findings,federmann2021findings,federmann2022findings}.  
However, most existing automatic metrics emphasise average quality rather than identifying \emph{critical} errors that distort factual or logical meaning.  

Recent studies explored whether LLMs can act as evaluators by reasoning directly about semantic fidelity.  
Kocmi and Federmann \cite{kocmi2023lm_eval} demonstrated that GPT-4 achieves state-of-the-art correlations with MQM scores, while Lu et al.\ \cite{lu2023erroranalysis} introduced \emph{error-analysis prompting} to obtain human-like rationales from LLMs.  
Peng et al.\ \cite{peng2023towards} further showed that ChatGPT can perform comparative translation ranking using carefully tuned instructions.  
Although these works highlight the potential of large proprietary models, they remain limited by size, cost, and reproducibility, motivating research into compact, open, and transparent evaluators.

\subsection{Critical Error Detection and Translation Safety}

Critical Error Detection (CED) narrows the focus of evaluation to meaning-altering mistakes such as hallucinations, entity mismatches, or negation flips.  
It connects to earlier work on factual inconsistency and contradiction detection in German \cite{9003090,pielka2020contradiction}, which showed that neural classifiers can capture semantic polarity differences even without reference translations.  
More recently, Pucknat et al.\ \cite{pucknat2022informed} proposed informed pre-training objectives for English-German CED, while Jung et al.\ \cite{jung2024explainable} released the \textsc{Explainable CED} dataset with human-annotated rationales.  
The 2025 \textsc{SynCED-EnDe} resource \cite{anonymous} extended this direction through balanced, human-validated annotations of \texttt{ERR}/\texttt{NOT} labels, providing a reproducible testbed for multilingual error detection.  

CED thus serves as a realistic proxy for translation-safety monitoring, complementing industrial studies such as Pielka et al.\ \cite{pielka2025translation}, who applied LLMs to detect translation anomalies in financial documents.  
Our work builds on these efforts by systematically examining whether small-scale, instruction-tuned LLMs can perform CED reasoning with comparable reliability to frontier-scale evaluators.

\subsection{Efficient Fine-Tuning and Compact LLMs}

Scaling down LLMs while preserving reasoning ability has become a central research challenge.  
Parameter-efficient fine-tuning techniques such as LoRA and PEFT have reduced training cost by adapting a small subset of weights, while quantisation and distillation enable deployment on limited hardware.  
Toolkits like \emph{Unsloth} \cite{unsloth2024} streamline such workflows, and recent architectures, including ModernBERT \cite{modernbert} and mmBERT \cite{mmbert}, demonstrate that careful tokenization, attention sparsity, and language-adaptive scheduling can yield faster and memory-efficient multilingual encoders.  

These advances intersect with a broader movement toward open, transparent model families.  
Meta’s LLaMA 3 and 3.3 releases \cite{meta2024llama3,meta2025llama33} and OpenAI-OSS reproductions \cite{gptoss2025report,gptoss2025lora} provide accessible backbones for experimentation under permissive licences.  
Despite their availability, systematic benchmarks of such small-parameter models on safety-critical multilingual tasks remain rare.  
Our study contributes the first comparative evaluation of models below 2 B parameters, Gemma 3-1B, Qwen 3-1.7B, and others on the CED task, uniting perspectives from translation evaluation, factual consistency, and efficient model adaptation.","\subsection{Machine Translation Evaluation and Quality Estimation}

Automatic machine translation (MT) evaluation has long relied on reference-based similarity metrics such as BLEU \cite{papineni2002bleu} and METEOR \cite{banerjee2005meteor}.  
While computationally efficient, these n-gram overlap scores correlate only weakly with human judgments of meaning preservation.  
Subsequent learned metrics such as COMET \cite{rei2020comet} improved this alignment by leveraging multilingual encoders and regression heads trained on human-rated examples.  
Complementary human frameworks such as MQM \cite{lommel2014multidimensional} introduced structured taxonomies of translation errors, forming the basis of evaluation protocols in WMT shared tasks \cite{specia2020findings,federmann2021findings,federmann2022findings}.  
However, most existing automatic metrics emphasise average quality rather than identifying \emph{critical} errors that distort factual or logical meaning.  

Recent studies explored whether LLMs can act as evaluators by reasoning directly about semantic fidelity.  
Kocmi and Federmann \cite{kocmi2023lm_eval} demonstrated that GPT-4 achieves state-of-the-art correlations with MQM scores, while Lu et al.\ \cite{lu2023erroranalysis} introduced \emph{error-analysis prompting} to obtain human-like rationales from LLMs.  
Peng et al.\ \cite{peng2023towards} further showed that ChatGPT can perform comparative translation ranking using carefully tuned instructions.  
Although these works highlight the potential of large proprietary models, they remain limited by size, cost, and reproducibility, motivating research into compact, open, and transparent evaluators.

\subsection{Critical Error Detection and Translation Safety}

Critical Error Detection (CED) narrows the focus of evaluation to meaning-altering mistakes such as hallucinations, entity mismatches, or negation flips.  
It connects to earlier work on factual inconsistency and contradiction detection in German \cite{9003090,pielka2020contradiction}, which showed that neural classifiers can capture semantic polarity differences even without reference translations.  
More recently, Pucknat et al.\ \cite{pucknat2022informed} proposed informed pre-training objectives for English-German CED, while Jung et al.\ \cite{jung2024explainable} released the \textsc{Explainable CED} dataset with human-annotated rationales.  
The 2025 \textsc{SynCED-EnDe} resource \cite{anonymous} extended this direction through balanced, human-validated annotations of \texttt{ERR}/\texttt{NOT} labels, providing a reproducible testbed for multilingual error detection.  

CED thus serves as a realistic proxy for translation-safety monitoring, complementing industrial studies such as Pielka et al.\ \cite{pielka2025translation}, who applied LLMs to detect translation anomalies in financial documents.  
Our work builds on these efforts by systematically examining whether small-scale, instruction-tuned LLMs can perform CED reasoning with comparable reliability to frontier-scale evaluators.

\subsection{Efficient Fine-Tuning and Compact LLMs}

Scaling down LLMs while preserving reasoning ability has become a central research challenge.  
Parameter-efficient fine-tuning techniques such as LoRA and PEFT have reduced training cost by adapting a small subset of weights, while quantisation and distillation enable deployment on limited hardware.  
Toolkits like \emph{Unsloth} \cite{unsloth2024} streamline such workflows, and recent architectures, including ModernBERT \cite{modernbert} and mmBERT \cite{mmbert}, demonstrate that careful tokenization, attention sparsity, and language-adaptive scheduling can yield faster and memory-efficient multilingual encoders.  

These advances intersect with a broader movement toward open, transparent model families.  
Meta’s LLaMA 3 and 3.3 releases \cite{meta2024llama3,meta2025llama33} and OpenAI-OSS reproductions \cite{gptoss2025report,gptoss2025lora} provide accessible backbones for experimentation under permissive licences.  
Despite their availability, systematic benchmarks of such small-parameter models on safety-critical multilingual tasks remain rare.  
Our study contributes the first comparative evaluation of models below 2 B parameters, Gemma 3-1B, Qwen 3-1.7B, and others on the CED task, uniting perspectives from translation evaluation, factual consistency, and efficient model adaptation.",
2511.09611v1,http://arxiv.org/abs/2511.09611v1,2025-11-12 18:58:21+00:00,MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation,"While thinking-aware generation aims to improve performance on complex tasks, we identify a critical failure mode where existing sequential, autoregressive approaches can paradoxically degrade performance due to error propagation. To systematically analyze this issue, we propose ParaBench, a new benchmark designed to evaluate both text and image output modalities. Our analysis using ParaBench reveals that this performance degradation is strongly correlated with poor alignment between the generated reasoning and the final image. To resolve this, we propose a parallel multimodal diffusion framework, MMaDA-Parallel, that enables continuous, bidirectional interaction between text and images throughout the entire denoising trajectory. MMaDA-Parallel is trained with supervised finetuning and then further optimized by Parallel Reinforcement Learning (ParaRL), a novel strategy that applies semantic rewards along the trajectory to enforce cross-modal consistency. Experiments validate that our model significantly improves cross-modal alignment and semantic consistency, achieving a 6.9\% improvement in Output Alignment on ParaBench compared to the state-of-the-art model, Bagel, establishing a more robust paradigm for thinking-aware image synthesis. Our code is open-sourced at https://github.com/tyfeld/MMaDA-Parallel","\label{sec:related_work}

% \subsection{Multimodal Generation and Editing with Chain-of-Thought Reasoning}

Recent progress in multimodal models for image understanding, generation, and editing has been rapid, yet most approaches remain constrained to single-modal generation conditioned on multiple modalities~\citep{sd3, wu2025qwen, labs2025flux1kontextflowmatching, bai2025qwen2}. To improve the accuracy and fidelity of multimodal generation, a growing line of work has explored introducing a textual \textit{Chain-of-Thought}  reasoning process before image generation or editing. We refer to this paradigm as \textbf{thinking-aware image generation and editing}. For instance, early efforts such as Chameleon~\citep{team2024chameleon} and {Mogao}~\citep{liao2025mogao} investigated interleaved generation, enabling interleaving sequences of text and image tokens. Image-CoT~\citep{guo2025can} and GoT~\citep{fang2025got} incorporated CoT reasoning before image synthesis, demonstrating that reasoning traces can enhance generation quality. {Bagel}~\citep{deng2025emerging} further extended this idea by integrating chain-of-thought reasoning into both image generation and editing, enabling more flexible and semantically aligned outputs. Building on this direction, follow-up works such as {OmniGen2}~\citep{wu2025omnigen2} and {IRG}~\citep{huang2025interleaving} introduced reflective reasoning after image generation, using multi-turn textual feedback to refine visual outputs iteratively.
%
Most existing methods, however, rely on a sequential autoregressive interleaved pipeline, which could limit direct cross-modal interaction and make the model prone to error accumulation from imperfect reasoning traces. Exploring a parallel generation framework that enables more interaction within output modalities is still lacking in this scenario.
(More related work can be found in Appendix~\ref{app:more_related}).
% In contrast, we propose a parallel diffusion framework that generates text and images jointly, allowing mutual attention throughout the denoising process for stronger alignment and robustness. 

% New concept, new findings and new framework

% \subsection{Diffusion Large Language Models}
% Diffusion models have achieved remarkable progress in vision~\citep{ddpm,sd,sd3,ddim,dit}, motivating their extension to text. The discrete nature of textual tokens, however, makes direct adaptation non-trivial. Two main approaches have emerged: learning continuous latent representations~\citep{analog,tess,dinoiser,diffuseq}, and designing discrete diffusion models~\citep{yourdiscrete,scalingdiscrete,longllada,dream7b,llada1.5}. Among the latter, \textbf{Masked Diffusion Models (MDMs)} stand out by leveraging bidirectional attention for global consistency and supporting parallel decoding. Systems such as Dream7B~\citep{dream7b} and LLaDA~\citep{nie2025large} achieve performance comparable to autoregressive LLMs. 
% %
% Beyond text, diffusion-based LLMs have also been extended to multimodal domains. LaViDA~\citep{lavida} employs multi-view image encoding with masked-denoising training, LLaDA-V~\citep{lladav} integrates masked diffusion with visual instruction tuning, and MMaDA~\citep{yang2025mmada} unifies reasoning across text and vision generation through chain-of-thought supervision and reinforcement learning. These advances highlight the scalability and versatility of diffusion-based language models across both unimodal and multimodal settings. Nevertheless, existing approaches have not yet explored \textbf{parallel text–image co-generation}, leaving cross-modal reasoning and alignment still constrained by sequential pipelines.

% % Extend to new challenging scenarios with new post-training algorithm

% \subsection{Reinforcement Learning for Multimodal Foundation Models}
% Reinforcement Learning (RL) has emerged as a powerful paradigm for enhancing reasoning and controllability in large models. The widely adopted GRPO~\citep{guo2025deepseek} applies rewards primarily on the correctness of the final answer and the adherence to a predefined format. Recently, RL has been adopted in multimodal large language models~\citep{chen2025r1v,meng2025mm, r1-onevision,r1vl,openvlthinker, visionr1}, incorporating task-specific rewards such as answer correctness, intersection-over-union (IoU) for localization~\citep{liu2025seg}, and image–text alignment scores (e.g., T2I-R1~\citep{jiang2025t2i}). Extensions such as~\citep{jiang2025co,hong2025reinforcing} further introduce cross-modality coherence rewards. In the context of diffusion language models, similar strategies have been explored with verified rewards and carefully designed probability approximations~\citep{yang2025mmada,gong2025diffucoder}
% .
% Despite these advances, most existing methods focus solely on rewards applied to the final output, while largely ignoring the generative trajectory. 
% %
% This overlooks the fact that intermediate steps can provide crucial signals for alignment. In contrast, our work investigates the synergy between modalities during the denoising process and introduces Trajectory Semantic Policy Optimization, which exploits stepwise semantic alignment to optimize thinking-aware multimodal generation.
% %
% Code and dataset will be avaiable.

% Specified trajectory semantic alignment for multimodal reinforcement learning, general and superior","Recent progress in multimodal models for image understanding, generation, and editing has been rapid, yet most approaches remain constrained to single-modal generation conditioned on multiple modalities~\citep{sd3, wu2025qwen, labs2025flux1kontextflowmatching, bai2025qwen2}. To improve the accuracy and fidelity of multimodal generation, a growing line of work has explored introducing a textual \textit{Chain-of-Thought}  reasoning process before image generation or editing. We refer to this paradigm as \textbf{thinking-aware image generation and editing}. For instance, early efforts such as Chameleon~\citep{team2024chameleon} and {Mogao}~\citep{liao2025mogao} investigated interleaved generation, enabling interleaving sequences of text and image tokens. Image-CoT~\citep{guo2025can} and GoT~\citep{fang2025got} incorporated CoT reasoning before image synthesis, demonstrating that reasoning traces can enhance generation quality. {Bagel}~\citep{deng2025emerging} further extended this idea by integrating chain-of-thought reasoning into both image generation and editing, enabling more flexible and semantically aligned outputs. Building on this direction, follow-up works such as {OmniGen2}~\citep{wu2025omnigen2} and {IRG}~\citep{huang2025interleaving} introduced reflective reasoning after image generation, using multi-turn textual feedback to refine visual outputs iteratively.

Most existing methods, however, rely on a sequential autoregressive interleaved pipeline, which could limit direct cross-modal interaction and make the model prone to error accumulation from imperfect reasoning traces. Exploring a parallel generation framework that enables more interaction within output modalities is still lacking in this scenario.
(More related work can be found in Appendix~\ref{app:more_related}).",
2511.08379v2,http://arxiv.org/abs/2511.08379v2,2025-11-11 16:01:42+00:00,SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models,"Refusal refers to the functional behavior enabling safety-aligned language models to reject harmful or unethical prompts. Following the growing scientific interest in mechanistic interpretability, recent work encoded refusal behavior as a single direction in the model's latent space; e.g., computed as the difference between the centroids of harmful and harmless prompt representations. However, emerging evidence suggests that concepts in LLMs often appear to be encoded as a low-dimensional manifold embedded in the high-dimensional latent space. Motivated by these findings, we propose a novel method leveraging Self-Organizing Maps (SOMs) to extract multiple refusal directions. To this end, we first prove that SOMs generalize the prior work's difference-in-means technique. We then train SOMs on harmful prompt representations to identify multiple neurons. By subtracting the centroid of harmless representations from each neuron, we derive a set of multiple directions expressing the refusal concept. We validate our method on an extensive experimental setup, demonstrating that ablating multiple directions from models' internals outperforms not only the single-direction baseline but also specialized jailbreak algorithms, leading to an effective suppression of refusal. Finally, we conclude by analyzing the mechanistic implications of our approach.","\label{sect:related}

\myparagraph{Jailbreak in LLMs.}
Jailbreak attacks have been shown to be an effective method for bypassing the refusal of safety-aligned models. 
% 
Among the first automated techniques, GCG~\cite{zou_23_universal} introduces an effective gradient-based approach to generate adversarial suffixes optimized for each prompt and eliciting harmful responses.
% 
Improving over GCG, SAA~\cite{andriushchenko_25_jailbreaking} relies on a predefined template including an adversarial suffix optimized for each prompt via random search, outperforming GCG.
% 
Unlike these methods, \textit{our work does not propose a new jailbreak attack optimizing adversarial perturbations for each prompt}, but instead investigates the internal mechanisms enabling refusal. We thus find multiple directions mediating refusal universally for each prompt, increasing the relevance of our empirical results.

\myparagraph{Mechanistic Interpretability.} 
A key assumption characterizing mechanistic interpretability in LLMs is the Linear Representation Hypothesis, positing that high-level concepts are encoded as linear directions in models' activations~\cite{mikolov_13_linguistic, elhage_22_toy, nanda_23_emergent}.
% 
Recent work, however, has begun to challenge such a hypothesis. \citet{engels_25_not} have shown that simple entities such as days and months are encoded circularly, while studies on trigonometry have found numbers represented as a generalized helix~\cite{kantamneni_25_language}, or as a circle~\cite {levy_25_language}. 
% 
These findings collectively suggest that concepts may be better understood as manifolds (\ie, structured regions in activation space) rather than a single direction. Recent work by~\citet{modell_25_origins} has introduced a generalized manifold formalism, while~\citet{olah_jermyn_24_circuits} have argued that multiple similar directions jointly express different facets of a concept, motivating new methodologies for identifying families of semantically related directions.
We build on these insights and embrace the perspective of multi-directional, manifold-oriented encoding of refusal, using SOMs to identify multiple related directions. 

\myparagraph{From Single to Multiple Refusal Directions.}
Modeling refusal as a single direction was first proposed by~\citet{arditi_24_refusal}.
% 
Subsequently, \citet{pan_25_hidden} compared the internals of a Llama3-8b before and after safety alignment.
They extract orthogonal components through SVD, finding distinct features such as narrative and role-playing. 
While this admits the existence of more directions, the study is analytical in nature and does not propose a method bypassing the refusal to compare with.
Furthermore, by enforcing orthogonality, the directions represent distinct concepts contributing to refusal, rather than modeling the underlying refusal manifold.
% 
In parallel, \citet{wollschläger_25_geometry} investigate the geometry of refusal starting from a basis vector optimized via a gradient-based approach and forming an orthogonal frame of directions (\ie, a cone), and evaluate the effectiveness of ablating each of these directions one by one.
%
Similarly to~\citet{pan_25_hidden}, the reliance on orthogonal components prioritizes geometric separability rather than continuity of the refusal concept.
Accordingly, as shown in~\cref{sect:experiments}, we achieve higher ASR than \rdo.
% 
As such, while both approaches admit the existence of multiple directions, they do not consider the possibility that refusal might be encoded as a manifold and expressed by multiple, closely related directions. 
In contrast, our work privileges this view by leveraging SOMs to identify similar directions that span over refusal, better aligned with recent findings, and leading to an effective jailbreak success.","\myparagraph{Jailbreak in LLMs.}
Jailbreak attacks have been shown to be an effective method for bypassing the refusal of safety-aligned models. 

Among the first automated techniques, GCG~\cite{zou_23_universal} introduces an effective gradient-based approach to generate adversarial suffixes optimized for each prompt and eliciting harmful responses.

Improving over GCG, SAA~\cite{andriushchenko_25_jailbreaking} relies on a predefined template including an adversarial suffix optimized for each prompt via random search, outperforming GCG.

Unlike these methods, \textit{our work does not propose a new jailbreak attack optimizing adversarial perturbations for each prompt}, but instead investigates the internal mechanisms enabling refusal. We thus find multiple directions mediating refusal universally for each prompt, increasing the relevance of our empirical results.

\myparagraph{Mechanistic Interpretability.} 
A key assumption characterizing mechanistic interpretability in LLMs is the Linear Representation Hypothesis, positing that high-level concepts are encoded as linear directions in models' activations~\cite{mikolov_13_linguistic, elhage_22_toy, nanda_23_emergent}.

Recent work, however, has begun to challenge such a hypothesis. \citet{engels_25_not} have shown that simple entities such as days and months are encoded circularly, while studies on trigonometry have found numbers represented as a generalized helix~\cite{kantamneni_25_language}, or as a circle~\cite {levy_25_language}. 

These findings collectively suggest that concepts may be better understood as manifolds (\ie, structured regions in activation space) rather than a single direction. Recent work by~\citet{modell_25_origins} has introduced a generalized manifold formalism, while~\citet{olah_jermyn_24_circuits} have argued that multiple similar directions jointly express different facets of a concept, motivating new methodologies for identifying families of semantically related directions.
We build on these insights and embrace the perspective of multi-directional, manifold-oriented encoding of refusal, using SOMs to identify multiple related directions. 

\myparagraph{From Single to Multiple Refusal Directions.}
Modeling refusal as a single direction was first proposed by~\citet{arditi_24_refusal}.

Subsequently, \citet{pan_25_hidden} compared the internals of a Llama3-8b before and after safety alignment.
They extract orthogonal components through SVD, finding distinct features such as narrative and role-playing. 
While this admits the existence of more directions, the study is analytical in nature and does not propose a method bypassing the refusal to compare with.
Furthermore, by enforcing orthogonality, the directions represent distinct concepts contributing to refusal, rather than modeling the underlying refusal manifold.

In parallel, \citet{wollschläger_25_geometry} investigate the geometry of refusal starting from a basis vector optimized via a gradient-based approach and forming an orthogonal frame of directions (\ie, a cone), and evaluate the effectiveness of ablating each of these directions one by one.

Similarly to~\citet{pan_25_hidden}, the reliance on orthogonal components prioritizes geometric separability rather than continuity of the refusal concept.
Accordingly, as shown in~\cref{sect:experiments}, we achieve higher ASR than \rdo.

As such, while both approaches admit the existence of multiple directions, they do not consider the possibility that refusal might be encoded as a manifold and expressed by multiple, closely related directions. 
In contrast, our work privileges this view by leveraging SOMs to identify similar directions that span over refusal, better aligned with recent findings, and leading to an effective jailbreak success.","Jailbreak in LLMs.Jailbreak attacks have been shown to
be an effective method for bypassing the refusal of safety-
aligned models. Among the first automated techniques,
GCG (Zou et al. 2023) introduces an effective gradient-
based approach to generate adversarial suffixes optimized
for each prompt and eliciting harmful responses. Improving
over GCG, SAA (Andriushchenko, Croce, and Flammarion
2025) relies on a predefined template including an adver-
sarial suffix optimized for each prompt via random search,
outperforming GCG. Unlike these methods,our work does
not propose a new jailbreak attack optimizing adversarial
perturbations for each prompt, but instead investigates the
internal mechanisms enabling refusal. We thus find multiple
directions mediating refusal universally for each prompt, in-creasing the relevance of our empirical results.
Mechanistic Interpretability.A key assumption charac-
terizing mechanistic interpretability in LLMs is the Lin-
ear Representation Hypothesis, positing that high-level con-
cepts are encoded as linear directions in models’ activa-
tions (Mikolov, Yih, and Zweig 2013; Elhage et al. 2022;
Nanda, Lee, and Wattenberg 2023). Recent work, however,
has begun to challenge such a hypothesis. Engels et al.
(2025) have shown that simple entities such as days and
months are encoded circularly, while studies on trigonom-
etry have found numbers represented as a generalized he-
lix (Kantamneni and Tegmark 2025), or as a circle (Levy
and Geva 2025). These findings collectively suggest that
concepts may be better understood as manifolds (i.e., struc-
tured regions in activation space) rather than a single direc-
tion. Recent work by Modell, Rubin-Delanchy, and White-
ley (2025) has introduced a generalized manifold formalism,
while Olah and Jermyn (2024) have argued that multiple
similar directions jointly express different facets of a con-
cept, motivating new methodologies for identifying families
of semantically related directions. We build on these insights
and embrace the perspective of multi-directional, manifold-
oriented encoding of refusal, using SOMs to identify multi-
ple related directions.
From Single to Multiple Refusal Directions.Modeling re-
fusal as a single direction was first proposed by Arditi et al.
(2024). Subsequently, Pan et al. (2025) compared the inter-
nals of a Llama3-8b before and after safety alignment. They
extract orthogonal components through SVD, finding dis-
tinct features such as narrative and role-playing. While this
12.5
10.0
7.5
5.0
2.5
0.0
2.5
5.0
7.54
2
0244
2
024
LLama2-7B
60
40
20
0
20
40
60
8060
40
20
0204040
20
02040
Qwen-14B
75
50
25
0
25
50
75
100
125 100
50
05010015075
50
25
0255075100
Gemma2-9B
20
15
10
5
0
5
10
15
2010
5
05101520258
6
4
2
02468
Mistral-7B-RR
Harmful prompts SOM NeuronsFigure 3: 3D PCA of SOM neurons on harmful prompts’ internal representations. Across all models, SOMs organize neurons to
span the underlying manifold, covering the entire space. Black lines connect neighboring neurons according to the SOM grid.
(0,0)(0,1)(0,2)(0,3)(1,0)(1,1)(1,2)(1,3)(2,0)(2,1)(2,2)(2,3)(3,0)(3,1)(3,2)(3,3)SD(0,0)(0,1)(0,2)(0,3)(1,0)(1,1)(1,2)(1,3)(2,0)(2,1)(2,2)(2,3)(3,0)(3,1)(3,2)(3,3)SD
0.141.00 Cosine similarity
(a)
(0,0)(0,1)(0,2)(0,3)(1,0)(1,1)(1,2)(1,3)(2,0)(2,1)(2,2)(2,3)(3,0)(3,1)(3,2)(3,3)SD(0,0)(0,1)(0,2)(0,3)(1,0)(1,1)(1,2)(1,3)(2,0)(2,1)(2,2)(2,3)(3,0)(3,1)(3,2)(3,3)SD
0.211.00 Cosine similarity (b)
Figure 4: Cosine similarities across MD directions (and SD)
on LLama2-7B (left) and Qwen-14B (right) models. The di-
rections are strongly aligned with each other, indicating the
offered multi-faceted, coherent perspective of refusal.
admits the existence of more directions, the study is analyt-
ical in nature and does not propose a method bypassing the
refusal to compare with. Furthermore, by enforcing orthogo-
nality, the directions represent distinct concepts contributing
to refusal, rather than modeling the underlying refusal man-
ifold. In parallel, Wollschl ¨ager et al. (2025) investigate the
geometry of refusal starting from a basis vector optimized
via a gradient-based approach and forming an orthogonal
frame of directions (i.e., a cone), and evaluate the effective-
ness of ablating each of these directions one by one. Sim-
ilarly to Pan et al. (2025), the reliance on orthogonal com-
ponents prioritizes geometric separability rather than conti-
nuity of the refusal concept. Accordingly, as shown in Sec-
tion 4, we achieve higher ASR than RDO. As such, while
both approaches admit the existence of multiple directions,
they do not consider the possibility that refusal might be en-
coded as a manifold and expressed by multiple, closely re-
lated directions. In contrast, our work privileges this view
by leveraging SOMs to identify similar directions that span
over refusal, better aligned with recent findings, and leading
to an effective jailbreak success"
2511.09000v1,http://arxiv.org/abs/2511.09000v1,2025-11-12 05:43:07+00:00,An insight into the technical debt-fix trade off in software backporting,"Maintaining software is an ongoing process that stretches beyond the initial release. Stable software versions continuously evolve to fix bugs, add improvements, address security issues, and ensure compatibility. This ongoing support involves Backporting, which means taking a fix or update from a newer version and applying it to an older version of the same software. As software versions evolve, new technical debt can arise during backport maintenance activities. This study examines the technical debt involved in fixing 105,396 commits from 31,076 backport sources across 87 repositories in three software ecosystems (Apache, Eclipse, and Python). The goal is to identify when and why new technical debt arises during backporting in stable source code. Our results indicate that approximately 4.3% of backports introduce new technical debt. Apache contributes the most absolute instances, while Python and Eclipse exhibit nearly three times higher debt-to-commit ratios than Apache. Feature migrations make older Apache releases debt-prone in the early phase, whereas Python and Eclipse releases tend to accumulate technical debt mostly during the middle phase of their release cycles. Additionally, developers who are inexperienced, under high workloads, or non-owners are more likely to introduce technical debt during backporting.","In this section, we review relevant research that aligns with our objectives and explain how our research objective directs a unique perspective on software maintenance from two prime research domains: (1) backporting maintenance and (2) technical debt domain.


% Rodriguez and Lawall \cite{b3} leverage Coccinelle (an open-source utility) to automate the changes needed in the Linux driver to fix the compatibility library which is now actively employed by the Linux kernel backports project. Using this method, the developer is required to manually determine the adjustments required for each kernel version and either directly change the driver code or create a compatibility library. Furthermore, 
\subsection{Backporting Maintenance:}
Several studies have analyzed backporting practices, with a primary focus on the Linux operating system. These studies include automating backport recommendation \cite{tian2012identifying, thung2016recommending}, defining strategies \cite{b2, bogart2021and,ss2}, and detecting and characterizing porting errors more  efficiently\cite{ray2013detecting}. Tian et al. \cite{tian2012identifying} proposed automatically identifying bug-fixing patches in the Linux kernel to apply them to older long-term releases. Ray et al. \cite{ray2013detecting} characterize porting error traits to help developers deal with backports. Thung et al. \cite{thung2016recommending} suggested an automatic recommendation approach to choose potential code modifications that should be backported in Linux device drivers. FIXMORPH \cite{shariffdeen2021automated} is proposed by Shariffdeen et al. to automatically backport change patches from the upstream Linux kernel version into earlier stable versions. Using the syntactic structure to locate patch locations and applying transformation rules based on syntactic similarity to recommend backports. Shi et al. constructed a prototype for semantic patch backporting called SKYPORT \cite{shi2022backporting} and tested it on web application injection vulnerability patches. To understand and characterize the semantics of injection vulnerability patches, it established a logic representation for injection vulnerabilities. Yang et. al. \cite{yang2023enhancing} propose a patch type-sensitive approach to automatically backport OSS security patches, which rely on both patch type and semantics. Specifically, their backport automation tool TSBPORT outperforms state-of-the-art approaches, correctly recommending backports with 87.59\% of patches. In addition, a large-scale survey by Bogart et al. \cite{bogart2021and} involving over 2,000 developers across 18 ecosystems indicates that backporting is underutilized, as most developers report rarely investing additional time in backporting changes to their packages. Chakroborti et al. \cite{b2} \cite{10.1145/3639478.3643079} analyzed 68,424 backports from 10 GitHub projects and found that backporting is not limited to bug and security fixes in stable releases but also includes a wide range of test, documentation, and feature changes. Decan et al. \cite{ss2} examined backporting practices in major dependency package distributions (Cargo, npm, Packagist, and RubyGems) and found that while most security vulnerabilities affect multiple release series, they are typically only patched in the latest one.


\subsection{Technical Debt :}
The conceptual notion of technical debt (TD) was initially borrowed by Cunningham in 1992  \cite{td_0_Cunningham} from the financial concept of deficit to illustrate the accumulation of debt \cite{kruchten2012technical} that occurs when the emphasis is placed on saving time rather than ensuring optimal quality \cite{td_5, s7}. Simply put, technical debt refers to shortcomings in internal software quality that make future modifications and further development more challenging \cite{td_3}. Over the years, numerous studies have delved into how technical debts and quality concerns are introduced by developers as well as the duration for which technical debts like code smells \cite{td_3} and bugs tend to persist within a software system. In addition, the research community also looked into the probable side effects of technical debt existence \cite{td_side_effects} in source code and concluded TD concerns are responsible for an increase in change- and fault-proneness as well as the decline of software comprehension \cite{td_comprehension} and maintainability \cite{yamashita2012code}.
Khomh et al.\cite{s7} found that the presence of code smells and technical debt is associated with significantly higher change and fault proneness.  Tufano et. al. \cite{td_3} report that most of the time, code artifacts are affected by code smell instances since their creation.
% where the preliminary goal of refactoring is to eliminate such issues.
% Thus, several approaches \cite{td_detect, td_detect_2} and tools are proposed for efficient technical debt detection and proposing its possible refactoring remedies.
% Yamashita and Moonen \cite{yamashita2012code} found that maintenance issues are directly linked to the accumulation of multiple code smells within the same file. On a conflicting empirical assertion,

Although the literature encompasses both cross-sectional \cite{alfayez2018empirical, klinger2011enterprise} and longitudinal approaches \cite{arcoverde2011understanding,  molnar2020longitudinal, nayebi2019longitudinal} to technical debt occurrences, they specifically focus on the upstream repository. In this study, we focus instead on the previous stable releases and their evolution during backporting. 
Molnar et al. \cite{td_7} investigated the long-term evolution of technical debts by analyzing the static source code of three projects across multiple releases, providing insight into how debt instances develop from the initial to the latest release. While this approach offers a horizontal perspective of software evolution across releases (Axes in Fig. \ref{fig:LTS}), it lacks the vertical perspective of evaluating individual release lifecycles. Although the negative impacts of TD on software performance and lifecycle are well known \cite{td_side_effects, td_comprehension}, little is understood about how, when, and why it emerges in LTS and STS release evolution. To address this, we identify and characterize 4,549 instances of technical debt within release evolution.

% While the negative impacts of technical debt on software performance and lifecycle have been empirically established \cite{td_side_effects, td_comprehension}, no prior work has systematically examined how, when, and why technical debts emerge in software releases under LTS and STS support. This gap poses challenges for the effective management and maintenance of such releases. In this study, we identify and characterize 4,549 instances of technical debt within the context of release evolution.

% \begin{table*}[]
% \caption{Charecteristics of chosen projects under Ecosystem for Analysis }
% \label{tab:ecosystem_traits}
% \centering
% \begin{tabular}{ccccccccc}

% \hline
%  Ecosystem & \#Project & KLOC & \#issues & \#backport\_commits & \#backport & \#Contributor & \#business domain  \\
%  \hline
% Eclipse   & 12   &   59 -156804 & 127719  & 10306 & 3278 &  3-95& 6\\
% Python & 17   &   4-326724 & 107432  & 10439 & 2454 & 11-34 &13\\
% Apache & 58   &   12-456723 & 223419  & 97192 &25344 & 5 -345 &12\\
% \hline
% Total & 87 & - & 458570 & 117937 & 31076 & - &31\\
% \hline
                                            
% \end{tabular}
% \end{table*}



% TODO - URGENT _Cross check the number of commits in title, table and every where","In this section, we review relevant research that aligns with our objectives and explain how our research objective directs a unique perspective on software maintenance from two prime research domains: (1) backporting maintenance and (2) technical debt domain.



\subsection{Backporting Maintenance:}
Several studies have analyzed backporting practices, with a primary focus on the Linux operating system. These studies include automating backport recommendation \cite{tian2012identifying, thung2016recommending}, defining strategies \cite{b2, bogart2021and,ss2}, and detecting and characterizing porting errors more  efficiently\cite{ray2013detecting}. Tian et al. \cite{tian2012identifying} proposed automatically identifying bug-fixing patches in the Linux kernel to apply them to older long-term releases. Ray et al. \cite{ray2013detecting} characterize porting error traits to help developers deal with backports. Thung et al. \cite{thung2016recommending} suggested an automatic recommendation approach to choose potential code modifications that should be backported in Linux device drivers. FIXMORPH \cite{shariffdeen2021automated} is proposed by Shariffdeen et al. to automatically backport change patches from the upstream Linux kernel version into earlier stable versions. Using the syntactic structure to locate patch locations and applying transformation rules based on syntactic similarity to recommend backports. Shi et al. constructed a prototype for semantic patch backporting called SKYPORT \cite{shi2022backporting} and tested it on web application injection vulnerability patches. To understand and characterize the semantics of injection vulnerability patches, it established a logic representation for injection vulnerabilities. Yang et. al. \cite{yang2023enhancing} propose a patch type-sensitive approach to automatically backport OSS security patches, which rely on both patch type and semantics. Specifically, their backport automation tool TSBPORT outperforms state-of-the-art approaches, correctly recommending backports with 87.59\


\subsection{Technical Debt :}
The conceptual notion of technical debt (TD) was initially borrowed by Cunningham in 1992  \cite{td_0_Cunningham} from the financial concept of deficit to illustrate the accumulation of debt \cite{kruchten2012technical} that occurs when the emphasis is placed on saving time rather than ensuring optimal quality \cite{td_5, s7}. Simply put, technical debt refers to shortcomings in internal software quality that make future modifications and further development more challenging \cite{td_3}. Over the years, numerous studies have delved into how technical debts and quality concerns are introduced by developers as well as the duration for which technical debts like code smells \cite{td_3} and bugs tend to persist within a software system. In addition, the research community also looked into the probable side effects of technical debt existence \cite{td_side_effects} in source code and concluded TD concerns are responsible for an increase in change- and fault-proneness as well as the decline of software comprehension \cite{td_comprehension} and maintainability \cite{yamashita2012code}.
Khomh et al.\cite{s7} found that the presence of code smells and technical debt is associated with significantly higher change and fault proneness.  Tufano et. al. \cite{td_3} report that most of the time, code artifacts are affected by code smell instances since their creation.




Although the literature encompasses both cross-sectional \cite{alfayez2018empirical, klinger2011enterprise} and longitudinal approaches \cite{arcoverde2011understanding,  molnar2020longitudinal, nayebi2019longitudinal} to technical debt occurrences, they specifically focus on the upstream repository. In this study, we focus instead on the previous stable releases and their evolution during backporting. 
Molnar et al. \cite{td_7} investigated the long-term evolution of technical debts by analyzing the static source code of three projects across multiple releases, providing insight into how debt instances develop from the initial to the latest release. While this approach offers a horizontal perspective of software evolution across releases (Axes in Fig. \ref{fig:LTS}), it lacks the vertical perspective of evaluating individual release lifecycles. Although the negative impacts of TD on software performance and lifecycle are well known \cite{td_side_effects, td_comprehension}, little is understood about how, when, and why it emerges in LTS and STS release evolution. To address this, we identify and characterize 4,549 instances of technical debt within release evolution.",
2510.25743v1,http://arxiv.org/abs/2510.25743v1,2025-10-29 17:46:07+00:00,Agentic Economic Modeling,"We introduce Agentic Economic Modeling (AEM), a framework that aligns synthetic LLM choices with small-sample human evidence for reliable econometric inference. AEM first generates task-conditioned synthetic choices via LLMs, then learns a bias-correction mapping from task features and raw LLM choices to human-aligned choices, upon which standard econometric estimators perform inference to recover demand elasticities and treatment effects.We validate AEM in two experiments. In a large scale conjoint study with millions of observations, using only 10% of the original data to fit the correction model lowers the error of the demand-parameter estimates, while uncorrected LLM choices even increase the errors. In a regional field experiment, a mixture model calibrated on 10% of geographic regions estimates an out-of-domain treatment effect of -65\pm10 bps, closely matching the full human experiment (-60\pm8 bps).Under time-wise extrapolation, training with only day-one human data yields -24 bps (95% CI: [-26, -22], p<1e-5),improving over the human-only day-one baseline (-17 bps, 95% CI: [-43, +9], p=0.2049).These results demonstrate AEM's potential to improve RCT efficiency and establish a foundation method for LLM-based counterfactual generation.","\paragraph{LLMs as experimental subjects}
Recent research argue that LLMs can take on multiple roles in experimental research—from study design to measurement to simulation—while warning about social risks and the need for rigorous validation~\cite{charness2025next,davidson2025integrating}. Building on this view, the mixed-subjects paradigm treats LLM outputs as potentially informative observations that should be combined with gold-standard human data via prediction-powered inference to preserve validity and improve efficiency~\cite{broska2025mixed}. In marketing science, LLMs are increasingly used to generate synthetic respondents and choices. Evidence suggests human–LLM hybrids can accelerate qualitative and quantitative insight generation~\cite{arora2025ai}. 

However, A growing body of evidence cautions against naively substituting LLM responses for human data. In opinion polling settings, models display machine bias—strong directional bias with low within-topic variance that drifts across topics—implying limited interchangeability with human respondents~\cite{boelaert2025machine}. Model alignment choices also shape population-simulation results in systematic ways, reinforcing the need to document model selection and validate against human baselines~\cite{lyman2025balancing,kozlowski2025simulating}. For conjoint analysis specifically, a data-augmentation approach integrates LLM-generated choices with a small amount of human data to de-bias estimates~\cite{wang2024large}.

\paragraph{Personas and population simulation}
To better capture heterogeneity, persona-based prompting has emerged as a central technique. Mixture-of-Personas formalizes this idea as a probabilistic mixture over persona components to match target-population distributions and increase response diversity~\cite{bui2025mixture}. Related work shows that varying personas can reduce some biases in collective choice tasks such as voting, though trade-offs remain between diversity and alignment~\cite{leng2024can, leng2023llm}. At the same time, critical appraisals emphasize that LLM-generated personas may embed systematic errors if not anchored to human data~\cite{li2025llm}. AEM complements these lines of work by uniting persona-based representation with human-calibrated identification, enabling inference from LLM data without assuming models are interchangeable with human subjects.","\paragraph{LLMs as experimental subjects}
Recent research argue that LLMs can take on multiple roles in experimental research—from study design to measurement to simulation—while warning about social risks and the need for rigorous validation~\cite{charness2025next,davidson2025integrating}. Building on this view, the mixed-subjects paradigm treats LLM outputs as potentially informative observations that should be combined with gold-standard human data via prediction-powered inference to preserve validity and improve efficiency~\cite{broska2025mixed}. In marketing science, LLMs are increasingly used to generate synthetic respondents and choices. Evidence suggests human–LLM hybrids can accelerate qualitative and quantitative insight generation~\cite{arora2025ai}. 

However, A growing body of evidence cautions against naively substituting LLM responses for human data. In opinion polling settings, models display machine bias—strong directional bias with low within-topic variance that drifts across topics—implying limited interchangeability with human respondents~\cite{boelaert2025machine}. Model alignment choices also shape population-simulation results in systematic ways, reinforcing the need to document model selection and validate against human baselines~\cite{lyman2025balancing,kozlowski2025simulating}. For conjoint analysis specifically, a data-augmentation approach integrates LLM-generated choices with a small amount of human data to de-bias estimates~\cite{wang2024large}.

\paragraph{Personas and population simulation}
To better capture heterogeneity, persona-based prompting has emerged as a central technique. Mixture-of-Personas formalizes this idea as a probabilistic mixture over persona components to match target-population distributions and increase response diversity~\cite{bui2025mixture}. Related work shows that varying personas can reduce some biases in collective choice tasks such as voting, though trade-offs remain between diversity and alignment~\cite{leng2024can, leng2023llm}. At the same time, critical appraisals emphasize that LLM-generated personas may embed systematic errors if not anchored to human data~\cite{li2025llm}. AEM complements these lines of work by uniting persona-based representation with human-calibrated identification, enabling inference from LLM data without assuming models are interchangeable with human subjects.",
2510.27008v1,http://arxiv.org/abs/2510.27008v1,2025-10-30 21:14:24+00:00,Algorithmic Predation: Equilibrium Analysis in Dynamic Oligopolies with Smooth Market Sharing,"Predatory pricing -- where a firm strategically lowers prices to undermine competitors -- is a contentious topic in dynamic oligopoly theory, with scholars debating practical relevance and the existence of predatory equilibria. Although finite-horizon dynamic models have long been proposed to capture the strategic intertemporal incentives of oligopolists, the existence and form of equilibrium strategies in settings that allow for firm exit (drop-outs following loss-making periods) have remained an open question. We focus on the seminal dynamic oligopoly model by Selten (1965) that introduces the subgame perfect equilibrium and analyzes smooth market sharing. Equilibrium can be derived analytically in models that do not allow for dropouts, but not in models that can lead to predatory pricing. In this paper, we leverage recent advances in deep reinforcement learning to compute and verify equilibria in finite-horizon dynamic oligopoly games. Our experiments reveal two key findings: first, state-of-the-art deep reinforcement learning algorithms reliably converge to equilibrium in both perfect- and imperfect-information oligopoly models; second, when firms face asymmetric cost structures, the resulting equilibria exhibit predatory pricing behavior. These results demonstrate that predatory pricing can emerge as a rational equilibrium strategy across a broad variety of model settings. By providing equilibrium analysis of finite-horizon dynamic oligopoly models with drop-outs, our study answers a decade-old question and offers new insights for competition authorities and regulators.","\label{sec:related-work}

This section reviews related work on equilibrium analysis and learning dynamics in dynamic oligopoly models.
Dynamic oligopoly markets have been studied extensively in the literature \citep{fudenberg2013dynamic, gerpottCompetitivePricingOnline2022}. 
%Although continuous-time models have been used to study dynamic oligopoly models~\citep{conlonContinuousTimeVs1995, Ledvina2010DynamicBO, bonatti_dynamic_2016}, our discrete-time model allows us to study partial observability and dropouts, that have received limited attention even within discrete-time frameworks. In what follows, we focus on discrete-time models.

%\subsection{Dynamic oligopoly models and predatory behavior}
A foundational dynamic oligopoly model was introduced by \citet{seltenSpieltheoretischeBehandlungOligopolmodells1965}, who considered price competition with discrete time steps, finite horizon, complete information, and continuous demand. Selten explicitly characterized a deterministic subgame perfect equilibrium in a finite-horizon complete-information game, which was influential for subsequent analyses \citep{phlips1989dynamic, farrell1988dynamic, bayer2007network}. 
We extend his work and derive an equilibrium considering also imperfect information of firms. 

\citet{maskin1988theory} proposed an infinite-horizon model with alternating moves to study dynamic oligopolies, which focuses on long-run strategic considerations.
Several studies addressed predatory pricing within dynamic oligopolies in this framework~\citep{cabralLearningCurveMarket1994, besanko2014economics, reyDynamicModelPredation2022}. Despite their insights, these models often rely on strong assumptions, such as independent stage-wise demand, finite pay-off structures, or limited action spaces, limiting their ability to capture dynamic pricing behaviors. In contrast, our model incorporates interdependent demand and allows for continuous prices, enabling richer strategic patterns.

%\citet{besanko2014economics} models an infinite horizon stochastic game, where incumbents strategically exit and re-enter the market. In addition to an analytically derived MPE, they provide empirical experiments in a symmetric setting that intends to demonstrate how commonly predatory behavior arises. Their method relies on solving differential equations that capture the system's dynamics. In contrast, our method considers both symmetric and asymmetric equilibria, explicitly incorporating networking effects and market dropout discontinuities. 

Finite-horizon models are arguably a good fit for the analysis of predatory pricing, as the strategic analysis of firms rarely considers an infinite horizon. They are less sensitive to discount factors or changes in the parameters of the game and an important complement to infinite-horizon and perfect-information models, for which numerical methods such as value function iteration have been available for a long time \citep{pakes1992computing}. However, solving finite-horizon models is challenging. \citet{bylkaDiscreteTimeDynamic2000} introduced dropout mechanisms, creating strategic discontinuities, which evaded equilibrium analysis so far. 
Furthermore, as the state space grows, numerical methods based on dynamic programming become slow quickly. Our approach, employing DRL, provides a way to find equilibrium even if the model allows dropouts, continuous actions, and states. 

%Note that in finite-horizon models the choice of the final period $T$ can influence behavior in earlier periods. We show that predatory pricing is an equilibrium strategy when two and more periods considered.

%\subsection{Learning in games}

Equilibrium learning offers an alternative numerical approach to finding equilibrium. It explores how equilibrium can emerge from agents that maximize their payoff while competing with each other~\citep{fudenbergTheoryLearningGames1999}. Almost the entire literature is focused on static, complete-information games. Unfortunately, learning dynamics does not necessarily converge to a Nash equilibrium~\citep{milionisImpossibilityTheoremGame2023, mazumdarPolicyGradientAlgorithmsHave2020, daskalakisLearningAlgorithmsNash2010}. 
%Learning algorithms can cycle, diverge, or exhibit chaotic behavior, even in zero-sum games, where the Nash equilibrium is considered tractable~\citep{mertikopoulos2018cycles, baileyMultiplicativeWeightsUpdate2018}. 
Several recent studies have demonstrated the convergence of learning algorithms to equilibrium in static auction and oligopoly pricing models \citep{bichlerConvergenceLearningAlgorithms2023, serefahunbayUniquenessBayesianCoarse2024}. 

We build our study on a new methodology recently introduced by~\citet{pieroth2025}. They use deep \ac{RL} agents in self-play to compute candidate equilibrium profiles in multi-stage games with a finite horizon and continuous observations and actions. Importantly, they propose a verification algorithm that provides an upper bound on the computed candidate's distance to equilibrium. This enables an \textit{ex-post} verification of the learned strategies, offering guarantees even when there are none about convergence a priori.
We extend their work by studying dynamic oligopoly markets and computing novel approximate equilibrium strategies under various information structures and market rules. Additionally, we derive a novel equilibrium analytically, further contributing to the understanding of strategic behavior in these complex environments. This is the first work analyzing dynamic oligopoly models with this new equilibrium learning approach.","This section reviews related work on equilibrium analysis and learning dynamics in dynamic oligopoly models.
Dynamic oligopoly markets have been studied extensively in the literature \citep{fudenberg2013dynamic, gerpottCompetitivePricingOnline2022}. 



A foundational dynamic oligopoly model was introduced by \citet{seltenSpieltheoretischeBehandlungOligopolmodells1965}, who considered price competition with discrete time steps, finite horizon, complete information, and continuous demand. Selten explicitly characterized a deterministic subgame perfect equilibrium in a finite-horizon complete-information game, which was influential for subsequent analyses \citep{phlips1989dynamic, farrell1988dynamic, bayer2007network}. 
We extend his work and derive an equilibrium considering also imperfect information of firms. 

\citet{maskin1988theory} proposed an infinite-horizon model with alternating moves to study dynamic oligopolies, which focuses on long-run strategic considerations.
Several studies addressed predatory pricing within dynamic oligopolies in this framework~\citep{cabralLearningCurveMarket1994, besanko2014economics, reyDynamicModelPredation2022}. Despite their insights, these models often rely on strong assumptions, such as independent stage-wise demand, finite pay-off structures, or limited action spaces, limiting their ability to capture dynamic pricing behaviors. In contrast, our model incorporates interdependent demand and allows for continuous prices, enabling richer strategic patterns.



Finite-horizon models are arguably a good fit for the analysis of predatory pricing, as the strategic analysis of firms rarely considers an infinite horizon. They are less sensitive to discount factors or changes in the parameters of the game and an important complement to infinite-horizon and perfect-information models, for which numerical methods such as value function iteration have been available for a long time \citep{pakes1992computing}. However, solving finite-horizon models is challenging. \citet{bylkaDiscreteTimeDynamic2000} introduced dropout mechanisms, creating strategic discontinuities, which evaded equilibrium analysis so far. 
Furthermore, as the state space grows, numerical methods based on dynamic programming become slow quickly. Our approach, employing DRL, provides a way to find equilibrium even if the model allows dropouts, continuous actions, and states. 





Equilibrium learning offers an alternative numerical approach to finding equilibrium. It explores how equilibrium can emerge from agents that maximize their payoff while competing with each other~\citep{fudenbergTheoryLearningGames1999}. Almost the entire literature is focused on static, complete-information games. Unfortunately, learning dynamics does not necessarily converge to a Nash equilibrium~\citep{milionisImpossibilityTheoremGame2023, mazumdarPolicyGradientAlgorithmsHave2020, daskalakisLearningAlgorithmsNash2010}. 

Several recent studies have demonstrated the convergence of learning algorithms to equilibrium in static auction and oligopoly pricing models \citep{bichlerConvergenceLearningAlgorithms2023, serefahunbayUniquenessBayesianCoarse2024}. 

We build our study on a new methodology recently introduced by~\citet{pieroth2025}. They use deep \ac{RL} agents in self-play to compute candidate equilibrium profiles in multi-stage games with a finite horizon and continuous observations and actions. Importantly, they propose a verification algorithm that provides an upper bound on the computed candidate's distance to equilibrium. This enables an \textit{ex-post} verification of the learned strategies, offering guarantees even when there are none about convergence a priori.
We extend their work by studying dynamic oligopoly markets and computing novel approximate equilibrium strategies under various information structures and market rules. Additionally, we derive a novel equilibrium analytically, further contributing to the understanding of strategic behavior in these complex environments. This is the first work analyzing dynamic oligopoly models with this new equilibrium learning approach.",
2511.05766v1,http://arxiv.org/abs/2511.05766v1,2025-11-07 23:35:19+00:00,Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs,"Large language models (LLMs) are increasingly examined as both behavioral subjects and decision systems, yet it remains unclear whether observed cognitive biases reflect surface imitation or deeper probability shifts. Anchoring bias, a classic human judgment bias, offers a critical test case. While prior work shows LLMs exhibit anchoring, most evidence relies on surface-level outputs, leaving internal mechanisms and attributional contributions unexplored. This paper advances the study of anchoring in LLMs through three contributions: (1) a log-probability-based behavioral analysis showing that anchors shift entire output distributions, with controls for training-data contamination; (2) exact Shapley-value attribution over structured prompt fields to quantify anchor influence on model log-probabilities; and (3) a unified Anchoring Bias Sensitivity Score integrating behavioral and attributional evidence across six open-source models. Results reveal robust anchoring effects in Gemma-2B, Phi-2, and Llama-2-7B, with attribution signaling that the anchors influence reweighting. Smaller models such as GPT-2, Falcon-RW-1B, and GPT-Neo-125M show variability, suggesting scale may modulate sensitivity. Attributional effects, however, vary across prompt designs, underscoring fragility in treating LLMs as human substitutes. The findings demonstrate that anchoring bias in LLMs is robust, measurable, and interpretable, while highlighting risks in applied domains. More broadly, the framework bridges behavioral science, LLM safety, and interpretability, offering a reproducible path for evaluating other cognitive biases in LLMs.","\subsection{Interpretable AI Paradigms.}

Bereska and Gavves~\cite{bereska2024mechanisticinterpretabilityaisafety} outline four main paradigms for interpreting model behavior:

\begin{enumerate}
    \item \textbf{Behavioral interpretability } — treating models as black boxes and studying input-output patterns.
    \item \textbf{Attributional interpretability} — tracing predictions back to the influence of input features.
    \item \textbf{Concept-based interpretability} — probing internal representations for higher-level abstractions governing behavior.
    \item \textbf{Mechanistic interpretability} — mapping neurons, layers, and circuits to specific causal relationships.
\end{enumerate}  

This study is positioned between the first two paradigms. By combining log-probability analysis with Shapley-value attribution, it moves beyond surface-level output comparisons to capture systematic distributional shifts, while also quantifying how anchors directly contribute to predictions.


\subsection{Cognitive biases.}

Cognitive biases are systematic patterns of deviation from rational judgment, first formalized in the pioneering work of Tversky and Kahneman. Their early study on the availability heuristic showed that people often judge frequencies or probabilities based on the ease with which examples come to mind, leading to systematic errors when availability is distorted~\cite{TVERSKY1973207}. This was followed by their famous article on heuristics and biases, which identified representativeness, availability, and anchoring as core mechanisms through which intuitive judgments depart from rational choice~\cite{doi:10.1126/science.185.4157.1124}. Later, they demonstrated the power of framing, showing that logically equivalent outcomes are perceived differently depending on how they are presented, producing predictable shifts in preference~\cite{tversky1981framing}. Tversky and Kahneman, along with other researchers, laid the foundations of behavioral economics by revealing that human decision-making systematically departs from rational choice theory in predictable ways. This line of research on human decision-making has also sparked growing interest in examining large language models, both to test whether they exhibit human-like cognitive biases and to explore the possibility that they may display an entirely distinct set of biases unique to their architecture.


\subsection{Cognitive bias in LLMs.}

Cognitive biases in LLMs are increasingly examined through psychology-inspired replications. 
Cui et al.~\cite{cui2025large} show that models reproduce many classic effects, though often with inflated magnitudes or spurious significance.
Similarly, Binz and Schulz~\cite{binz2023cognitive} found GPT-3 to appear human-like in decision-making yet fragile under perturbations or causal reasoning tasks.
Taken together, these studies suggest that while LLMs convincingly resemble human judgments, their cognitive biases differ from humans in scale and stability, making careful interpretation essential.
It is important to note, however, that both studies are confined to behavioral outputs and do not reveal attribution level analysis, nor probe the internal processes underlying model decisions.


\subsection{Anchoring bias in LLMs.}

Among the cognitive biases observed in large language models, anchoring has received particular attention. Experimental studies consistently show that LLMs, like humans, adjust their judgments upward or downward depending on irrelevant numeric cues. Suri et al.~\cite{suri2023largelanguagemodelsdecision} demonstrate this by replicating a classic anchoring task with ChatGPT-3.5, ChatGPT-4, and human participants, finding statistically robust shifts between high- and low-anchor conditions that mirror human behavior. Similarly, Lou and Sun~\cite{lou2024anchoringbiaslargelanguage} evaluate anchoring across GPT-3.5, GPT-4, and GPT-4o, showing that stronger models are more consistently biased by numeric hints, while weaker models introduce more variability. They also find that simple prompt-level mitigation strategies are largely ineffective, indicating that anchoring is a robust feature of model behavior. Stureborg et al.~\cite{stureborg2024largelanguagemodelsinconsistent} extend this line of work by analyzing anchoring in multi-attribute evaluation tasks. When GPT-4 generated scores for several text attributes in sequence, later ratings were disproportionately biased by earlier ones, reflecting the autoregressive or sequential dependency nature of model outputs. The authors argue that such dependencies undermine LLM reliability as evaluators, particularly in multi-criteria settings.


\subsection{Other related cognitive biases in LLMs.}

Recent experimental work has documented several cognitive biases in LLMs that are closely related to anchoring. Wang et al.~\cite{wang2023largelanguagemodelsfair} show that LLM-based evaluators display strong positional bias, with judgments easily manipulated by the order of candidate responses. Chen et al.~\cite{chen2024aicognitivelybiasedexploratory} identify threshold priming bias in information retrieval assessments, where prior relevance scores systematically influence subsequent judgments across GPT-3.5, GPT-4, and LLaMa2 models. Sumita et al.~\cite{sumita2024cognitivebiaseslargelanguage} provide a broader survey, experimentally confirming six cognitive biases, including order effects. Notably, Li and Gao~\cite{li2025anchoredanswersunravellingpositional} go beyond behavioral evidence by applying mechanistic interpretability to multiple-choice question answering in GPT-2 models, uncovering an internal preference for the first option (“A”). This remains one of the only studies to probe an anchoring-like cognitive bias as an internal mechanism.


\subsection{Shapley values for attribution in LLMs.}

With the exception of Li and Gao~\cite{li2025anchoredanswersunravellingpositional}, existing studies rely exclusively on experimental designs, providing behavioral or mimicking evidence that anchoring and anchoring-related biases are stable properties of LLMs across tasks and models. However, they do not probe or attribute internal log-probabilities, leaving a gap between purely behavioral findings and mechanistic interpretability. A natural tool for bridging this gap is the Shapley value, originally introduced in cooperative game theory to fairly allocate payoffs among players based on their marginal contributions~\cite{shapley1953value}. In the context of LLMs, Shapley values can be adapted to attribute the influence of individual prompt tokens or fields to the model's log-probability of specific outputs, providing a principled way to move from behavioral observations to more interpretable evidence of internal scoring dynamics. Therefore, attribution-based analysis offers a promising middle ground, as it can reveal how anchors shape token-level reweighting within model predictions.

The idea of applying Shapley values to interpret LLM behavior has recently gained traction. 
Mohammadi~\cite{mohammadi2024explaininglargelanguagemodels} proposed a Shapley-based framework that treats prompt components as players in a cooperative game, quantifying their marginal contributions to choice probabilities and exposing the ``token noise'' phenomenon, where seemingly irrelevant tokens exert disproportionate influence on decisions. 
In parallel, Horovicz and Goldshmidt~\cite{goldshmidt2024tokenshapinterpretinglargelanguage} introduced \textit{TokenSHAP}, which estimates Shapley values at the token level via Monte Carlo sampling and uses semantic similarity between generated responses as the payoff function. 
Beyond these research contributions, the official SHAP library itself provides demonstration notebooks for GPT-2, where Shapley values are computed over open-ended text generation tasks to explain which input tokens drive the log-probability of generating specific outputs~\cite{NIPS2017_7062}.

These examples show the potential of combining token-level Shapley attribution with logit-based teacher forcing to interpret LLM predictions. However, there is a trade-off: using raw log-probabilities enables exact attribution but is computationally expensive, while Monte Carlo sampling with log-probabilities or semantic similarity reduces cost at the expense of approximate Shapley values.
These lines of work illustrate the versatility but also the challenges of Shapley-based approaches for probing LLM decision-making at the level of discrete choice experiments, fine-grained token importance, or practical debugging of generation behavior.","\subsection{Interpretable AI Paradigms.}

Bereska and Gavves~\cite{bereska2024mechanisticinterpretabilityaisafety} outline four main paradigms for interpreting model behavior:

\begin{enumerate}
    \item \textbf{Behavioral interpretability } — treating models as black boxes and studying input-output patterns.
    \item \textbf{Attributional interpretability} — tracing predictions back to the influence of input features.
    \item \textbf{Concept-based interpretability} — probing internal representations for higher-level abstractions governing behavior.
    \item \textbf{Mechanistic interpretability} — mapping neurons, layers, and circuits to specific causal relationships.
\end{enumerate}  

This study is positioned between the first two paradigms. By combining log-probability analysis with Shapley-value attribution, it moves beyond surface-level output comparisons to capture systematic distributional shifts, while also quantifying how anchors directly contribute to predictions.


\subsection{Cognitive biases.}

Cognitive biases are systematic patterns of deviation from rational judgment, first formalized in the pioneering work of Tversky and Kahneman. Their early study on the availability heuristic showed that people often judge frequencies or probabilities based on the ease with which examples come to mind, leading to systematic errors when availability is distorted~\cite{TVERSKY1973207}. This was followed by their famous article on heuristics and biases, which identified representativeness, availability, and anchoring as core mechanisms through which intuitive judgments depart from rational choice~\cite{doi:10.1126/science.185.4157.1124}. Later, they demonstrated the power of framing, showing that logically equivalent outcomes are perceived differently depending on how they are presented, producing predictable shifts in preference~\cite{tversky1981framing}. Tversky and Kahneman, along with other researchers, laid the foundations of behavioral economics by revealing that human decision-making systematically departs from rational choice theory in predictable ways. This line of research on human decision-making has also sparked growing interest in examining large language models, both to test whether they exhibit human-like cognitive biases and to explore the possibility that they may display an entirely distinct set of biases unique to their architecture.


\subsection{Cognitive bias in LLMs.}

Cognitive biases in LLMs are increasingly examined through psychology-inspired replications. 
Cui et al.~\cite{cui2025large} show that models reproduce many classic effects, though often with inflated magnitudes or spurious significance.
Similarly, Binz and Schulz~\cite{binz2023cognitive} found GPT-3 to appear human-like in decision-making yet fragile under perturbations or causal reasoning tasks.
Taken together, these studies suggest that while LLMs convincingly resemble human judgments, their cognitive biases differ from humans in scale and stability, making careful interpretation essential.
It is important to note, however, that both studies are confined to behavioral outputs and do not reveal attribution level analysis, nor probe the internal processes underlying model decisions.


\subsection{Anchoring bias in LLMs.}

Among the cognitive biases observed in large language models, anchoring has received particular attention. Experimental studies consistently show that LLMs, like humans, adjust their judgments upward or downward depending on irrelevant numeric cues. Suri et al.~\cite{suri2023largelanguagemodelsdecision} demonstrate this by replicating a classic anchoring task with ChatGPT-3.5, ChatGPT-4, and human participants, finding statistically robust shifts between high- and low-anchor conditions that mirror human behavior. Similarly, Lou and Sun~\cite{lou2024anchoringbiaslargelanguage} evaluate anchoring across GPT-3.5, GPT-4, and GPT-4o, showing that stronger models are more consistently biased by numeric hints, while weaker models introduce more variability. They also find that simple prompt-level mitigation strategies are largely ineffective, indicating that anchoring is a robust feature of model behavior. Stureborg et al.~\cite{stureborg2024largelanguagemodelsinconsistent} extend this line of work by analyzing anchoring in multi-attribute evaluation tasks. When GPT-4 generated scores for several text attributes in sequence, later ratings were disproportionately biased by earlier ones, reflecting the autoregressive or sequential dependency nature of model outputs. The authors argue that such dependencies undermine LLM reliability as evaluators, particularly in multi-criteria settings.


\subsection{Other related cognitive biases in LLMs.}

Recent experimental work has documented several cognitive biases in LLMs that are closely related to anchoring. Wang et al.~\cite{wang2023largelanguagemodelsfair} show that LLM-based evaluators display strong positional bias, with judgments easily manipulated by the order of candidate responses. Chen et al.~\cite{chen2024aicognitivelybiasedexploratory} identify threshold priming bias in information retrieval assessments, where prior relevance scores systematically influence subsequent judgments across GPT-3.5, GPT-4, and LLaMa2 models. Sumita et al.~\cite{sumita2024cognitivebiaseslargelanguage} provide a broader survey, experimentally confirming six cognitive biases, including order effects. Notably, Li and Gao~\cite{li2025anchoredanswersunravellingpositional} go beyond behavioral evidence by applying mechanistic interpretability to multiple-choice question answering in GPT-2 models, uncovering an internal preference for the first option (“A”). This remains one of the only studies to probe an anchoring-like cognitive bias as an internal mechanism.


\subsection{Shapley values for attribution in LLMs.}

With the exception of Li and Gao~\cite{li2025anchoredanswersunravellingpositional}, existing studies rely exclusively on experimental designs, providing behavioral or mimicking evidence that anchoring and anchoring-related biases are stable properties of LLMs across tasks and models. However, they do not probe or attribute internal log-probabilities, leaving a gap between purely behavioral findings and mechanistic interpretability. A natural tool for bridging this gap is the Shapley value, originally introduced in cooperative game theory to fairly allocate payoffs among players based on their marginal contributions~\cite{shapley1953value}. In the context of LLMs, Shapley values can be adapted to attribute the influence of individual prompt tokens or fields to the model's log-probability of specific outputs, providing a principled way to move from behavioral observations to more interpretable evidence of internal scoring dynamics. Therefore, attribution-based analysis offers a promising middle ground, as it can reveal how anchors shape token-level reweighting within model predictions.

The idea of applying Shapley values to interpret LLM behavior has recently gained traction. 
Mohammadi~\cite{mohammadi2024explaininglargelanguagemodels} proposed a Shapley-based framework that treats prompt components as players in a cooperative game, quantifying their marginal contributions to choice probabilities and exposing the ``token noise'' phenomenon, where seemingly irrelevant tokens exert disproportionate influence on decisions. 
In parallel, Horovicz and Goldshmidt~\cite{goldshmidt2024tokenshapinterpretinglargelanguage} introduced \textit{TokenSHAP}, which estimates Shapley values at the token level via Monte Carlo sampling and uses semantic similarity between generated responses as the payoff function. 
Beyond these research contributions, the official SHAP library itself provides demonstration notebooks for GPT-2, where Shapley values are computed over open-ended text generation tasks to explain which input tokens drive the log-probability of generating specific outputs~\cite{NIPS2017_7062}.

These examples show the potential of combining token-level Shapley attribution with logit-based teacher forcing to interpret LLM predictions. However, there is a trade-off: using raw log-probabilities enables exact attribution but is computationally expensive, while Monte Carlo sampling with log-probabilities or semantic similarity reduces cost at the expense of approximate Shapley values.
These lines of work illustrate the versatility but also the challenges of Shapley-based approaches for probing LLM decision-making at the level of discrete choice experiments, fine-grained token importance, or practical debugging of generation behavior.",
2510.20066v1,http://arxiv.org/abs/2510.20066v1,2025-10-22 22:36:34+00:00,A Multi-Layer Machine Learning and Econometric Pipeline for Forecasting Market Risk: Evidence from Cryptoasset Liquidity Spillovers,"We study whether liquidity and volatility proxies of a core set of cryptoassets generate spillovers that forecast market-wide risk. Our empirical framework integrates three statistical layers: (A) interactions between core liquidity and returns, (B) principal-component relations linking liquidity and returns, and (C) volatility-factor projections that capture cross-sectional volatility crowding. The analysis is complemented by vector autoregression impulse responses and forecast error variance decompositions (see Granger 1969; Sims 1980), heterogeneous autoregressive models with exogenous regressors (HAR-X, Corsi 2009), and a leakage-safe machine learning protocol using temporal splits, early stopping, validation-only thresholding, and SHAP-based interpretation. Using daily data from 2021 to 2025 (1462 observations across 74 assets), we document statistically significant Granger-causal relationships across layers and moderate out-of-sample predictive accuracy. We report the most informative figures, including the pipeline overview, Layer A heatmap, Layer C robustness analysis, vector autoregression variance decompositions, and the test-set precision-recall curve. Full data and figure outputs are provided in the artifact repository.","We build on classical tools for dynamic dependence and volatility modeling. Granger causality and VARs provide the workhorse framework for testing predictive links and impulse propagation \cite{granger1969,sims1980}. Our liquidity/volatility proxies follow standard microstructure and range-based measures—Amihud illiquidity and Parkinson high/low volatility—and are complemented by GARCH-type variation \cite{amihud2002,parkinson1980,bollerslev1986}. To capture long-memory structure, we adopt HAR-X with HAC inference \cite{corsi2009simple,newey1987}. For forecasting and interpretability, we employ gradient-boosted trees and SHAP explanations \cite{chen2016xgboost,lundberg2017unified}. Relative to this literature, we connect \emph{core-coin} microstructure signals to a \emph{market-wide} cross-sectional ``volatility crowding'' target via a multi-layer causality chain, and we evaluate predictability under a leakage-aware protocol (see Figure 1).

% ============================ 2. Method ============================","We build on classical tools for dynamic dependence and volatility modeling. Granger causality and VARs provide the workhorse framework for testing predictive links and impulse propagation \cite{granger1969,sims1980}. Our liquidity/volatility proxies follow standard microstructure and range-based measures—Amihud illiquidity and Parkinson high/low volatility—and are complemented by GARCH-type variation \cite{amihud2002,parkinson1980,bollerslev1986}. To capture long-memory structure, we adopt HAR-X with HAC inference \cite{corsi2009simple,newey1987}. For forecasting and interpretability, we employ gradient-boosted trees and SHAP explanations \cite{chen2016xgboost,lundberg2017unified}. Relative to this literature, we connect \emph{core-coin} microstructure signals to a \emph{market-wide} cross-sectional ``volatility crowding'' target via a multi-layer causality chain, and we evaluate predictability under a leakage-aware protocol (see Figure 1).",
2511.07014v1,http://arxiv.org/abs/2511.07014v1,2025-11-10 12:05:32+00:00,Diffolio: A Diffusion Model for Multivariate Probabilistic Financial Time-Series Forecasting and Portfolio Construction,"Probabilistic forecasting is crucial in multivariate financial time-series for constructing efficient portfolios that account for complex cross-sectional dependencies. In this paper, we propose Diffolio, a diffusion model designed for multivariate financial time-series forecasting and portfolio construction. Diffolio employs a denoising network with a hierarchical attention architecture, comprising both asset-level and market-level layers. Furthermore, to better reflect cross-sectional correlations, we introduce a correlation-guided regularizer informed by a stable estimate of the target correlation matrix. This structure effectively extracts salient features not only from historical returns but also from asset-specific and systematic covariates, significantly enhancing the performance of forecasts and portfolios. Experimental results on the daily excess returns of 12 industry portfolios show that Diffolio outperforms various probabilistic forecasting baselines in multivariate forecasting accuracy and portfolio performance. Moreover, in portfolio experiments, portfolios constructed from Diffolio's forecasts show consistently robust performance, thereby outperforming those from benchmarks by achieving higher Sharpe ratios for the mean-variance tangency portfolio and higher certainty equivalents for the growth-optimal portfolio. These results demonstrate the superiority of our proposed Diffolio in terms of not only statistical accuracy but also economic significance.","\label{sec:related_works}

The application of deep generative models, including GANs, VAEs, and diffusion models, has significantly advanced the field of probabilistic time-series forecasting. 
GAN-based approaches, with TimeGAN \cite{yoon2019time} being a representative example, employ an adversarial training process to learn temporal dynamics and generate realistic time-series samples. 
Models applied in the financial domain (e.g., FIN-GAN \cite{takahashi2019modeling} and QuantGAN \cite{wiese2020quant}) focus on generating univariate financial data to reproduce stylized facts.
For multivariate conditional generation, Fin-GAN \cite{vuletic2024fin} introduces an economics-driven loss function for forecasting, and SigCWGAN \cite{liao2024sig} integrates Wasserstein GANs with path signature transforms. 
On the other hand, VAE-based models are developed to learn probabilistic latent representations of sequential data. An example application in finance is FactorVAE \cite{duan2022factorvae}, which integrates a VAE with a dynamic factor model to learn latent factors for predicting stock returns. 

Diffusion models \cite{sohl2015deep, ho2020denoising} have emerged as a powerful approach for generative tasks, renowned for their ability to produce high-fidelity samples; this strength has also shown to be effective in the time-series domain.  DiffWave \cite{kong2020diffwave}, originally for audio synthesis, utilizes 1-dimensional dilated convolutions in its denoising network $\epsilon_{\theta}$. TSDiff \cite{kollovieh2023predict} proposes a flexible approach for conditional forecasting by training an unconditional model and applying self-guidance at inference time. Work on multivariate conditional diffusion models has also been extensive. TimeGrad \cite{rasul2021autoregressive} adopts an autoregressive recurrent neural network (RNN) to encode historical information. Models such as CSDI \cite{tashiro2021csdi} and SSSD \cite{alcaraz2022diffusion} utilize different backbones--transformer and structured state space models, respectively--to focus on imputation and forecasting. MG-TSD \cite{fan2024mg} is proposed to enhance training stability by using multi-granularity data as targets. Diffusion-TS \cite{yuan2024diffusion} trains an unconditional transformer-based model to predict the clean sample $x_0$ directly, and then employs classifier-free guidance at inference time to generate conditional forecasts. For financial applications, FTS-Diffusion \cite{huang2024generative} is proposed to capture irregular and scale-invariant patterns in univariate financial data. A recent diffusion model by \cite{takahashi2025generation} also targets multivariate financial data, but focuses on synthetic data generation rather than forecasting.

While many existing models demonstrate strong performance on general time-series tasks, their architectures are often generic. As mentioned above, research on diffusion models for multivariate financial time-series forecasting remains particularly scarce. Moreover, both the generic models and those tailored for finance are often not explicitly designed to incorporate the two distinct types of covariates--asset-specific and systematic--crucial for financial forecasting \cite{welch2008comprehensive, gu2020empirical}, thereby limiting their ability to fully exploit this significant predictive information. Furthermore, while these models implicitly capture cross-sectional dependencies, they often lack explicit mechanisms designed to model and refine the intricate relationships paramount for efficient portfolio construction. Diffolio addresses these gaps as a multivariate conditional diffusion model tailored for financial time-series forecasting, utilizing a hierarchical attention architecture and a correlation-guided regularizer.

{A related strand of research also leverages attention mechanisms to jointly model temporal and cross-sectional dependencies for portfolio construction. In particular, \cite{cong2021alphaportfolio} propose AlphaPortfolio, which uses a historical attention module at the individual asset level and a cross-asset attention network to directly learn portfolio weights through deep reinforcement learning. 
%While both approaches share the architectural intuition of combining asset-level and market-level attention, their objectives fundamentally differ. 
{While both approaches share the architectural intuition of employing distinct attentions on individual assets and across assets, their detailed backbone architectures and handling of covariates diverge. Unlike AlphaPortfolio's network, which considers asset-specific covariates only, Diffolio's architecture is designed to hierarchically incorporate both asset-specific and systematic covariates. Beyond these architectural differences, their objectives also fundamentally differ.}
AlphaPortfolio optimizes investment performance by maximizing a reward function over portfolio returns, effectively learning a policy for asset allocation. In contrast, Diffolio focuses on probabilistic multivariate forecasting--that is, modeling the conditional joint distribution of future returns--to provide calibrated uncertainty estimates and to enable risk-aware portfolio construction. Thus, whereas AlphaPortfolio represents a reinforcement learning approach to direct portfolio optimization, our model constitutes a generative approach that produces distributional forecasts applicable to a wide range of decision frameworks.}","The application of deep generative models, including GANs, VAEs, and diffusion models, has significantly advanced the field of probabilistic time-series forecasting. 
GAN-based approaches, with TimeGAN \cite{yoon2019time} being a representative example, employ an adversarial training process to learn temporal dynamics and generate realistic time-series samples. 
Models applied in the financial domain (e.g., FIN-GAN \cite{takahashi2019modeling} and QuantGAN \cite{wiese2020quant}) focus on generating univariate financial data to reproduce stylized facts.
For multivariate conditional generation, Fin-GAN \cite{vuletic2024fin} introduces an economics-driven loss function for forecasting, and SigCWGAN \cite{liao2024sig} integrates Wasserstein GANs with path signature transforms. 
On the other hand, VAE-based models are developed to learn probabilistic latent representations of sequential data. An example application in finance is FactorVAE \cite{duan2022factorvae}, which integrates a VAE with a dynamic factor model to learn latent factors for predicting stock returns. 

Diffusion models \cite{sohl2015deep, ho2020denoising} have emerged as a powerful approach for generative tasks, renowned for their ability to produce high-fidelity samples; this strength has also shown to be effective in the time-series domain.  DiffWave \cite{kong2020diffwave}, originally for audio synthesis, utilizes 1-dimensional dilated convolutions in its denoising network $\epsilon_{\theta}$. TSDiff \cite{kollovieh2023predict} proposes a flexible approach for conditional forecasting by training an unconditional model and applying self-guidance at inference time. Work on multivariate conditional diffusion models has also been extensive. TimeGrad \cite{rasul2021autoregressive} adopts an autoregressive recurrent neural network (RNN) to encode historical information. Models such as CSDI \cite{tashiro2021csdi} and SSSD \cite{alcaraz2022diffusion} utilize different backbones--transformer and structured state space models, respectively--to focus on imputation and forecasting. MG-TSD \cite{fan2024mg} is proposed to enhance training stability by using multi-granularity data as targets. Diffusion-TS \cite{yuan2024diffusion} trains an unconditional transformer-based model to predict the clean sample $x_0$ directly, and then employs classifier-free guidance at inference time to generate conditional forecasts. For financial applications, FTS-Diffusion \cite{huang2024generative} is proposed to capture irregular and scale-invariant patterns in univariate financial data. A recent diffusion model by \cite{takahashi2025generation} also targets multivariate financial data, but focuses on synthetic data generation rather than forecasting.

While many existing models demonstrate strong performance on general time-series tasks, their architectures are often generic. As mentioned above, research on diffusion models for multivariate financial time-series forecasting remains particularly scarce. Moreover, both the generic models and those tailored for finance are often not explicitly designed to incorporate the two distinct types of covariates--asset-specific and systematic--crucial for financial forecasting \cite{welch2008comprehensive, gu2020empirical}, thereby limiting their ability to fully exploit this significant predictive information. Furthermore, while these models implicitly capture cross-sectional dependencies, they often lack explicit mechanisms designed to model and refine the intricate relationships paramount for efficient portfolio construction. Diffolio addresses these gaps as a multivariate conditional diffusion model tailored for financial time-series forecasting, utilizing a hierarchical attention architecture and a correlation-guided regularizer.

{A related strand of research also leverages attention mechanisms to jointly model temporal and cross-sectional dependencies for portfolio construction. In particular, \cite{cong2021alphaportfolio} propose AlphaPortfolio, which uses a historical attention module at the individual asset level and a cross-asset attention network to directly learn portfolio weights through deep reinforcement learning. 

{While both approaches share the architectural intuition of employing distinct attentions on individual assets and across assets, their detailed backbone architectures and handling of covariates diverge. Unlike AlphaPortfolio's network, which considers asset-specific covariates only, Diffolio's architecture is designed to hierarchically incorporate both asset-specific and systematic covariates. Beyond these architectural differences, their objectives also fundamentally differ.}
AlphaPortfolio optimizes investment performance by maximizing a reward function over portfolio returns, effectively learning a policy for asset allocation. In contrast, Diffolio focuses on probabilistic multivariate forecasting--that is, modeling the conditional joint distribution of future returns--to provide calibrated uncertainty estimates and to enable risk-aware portfolio construction. Thus, whereas AlphaPortfolio represents a reinforcement learning approach to direct portfolio optimization, our model constitutes a generative approach that produces distributional forecasts applicable to a wide range of decision frameworks.}",
2511.02458v1,http://arxiv.org/abs/2511.02458v1,2025-11-04 10:38:10+00:00,Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas,"We evaluate whether persona-based prompting improves Large Language Model (LLM) performance on macroeconomic forecasting tasks. Using 2,368 economics-related personas from the PersonaHub corpus, we prompt GPT-4o to replicate the ECB Survey of Professional Forecasters across 50 quarterly rounds (2013-2025). We compare the persona-prompted forecasts against the human experts panel, across four target variables (HICP, core HICP, GDP growth, unemployment) and four forecast horizons. We also compare the results against 100 baseline forecasts without persona descriptions to isolate its effect. We report two main findings. Firstly, GPT-4o and human forecasters achieve remarkably similar accuracy levels, with differences that are statistically significant yet practically modest. Our out-of-sample evaluation on 2024-2025 data demonstrates that GPT-4o can maintain competitive forecasting performance on unseen events, though with notable differences compared to the in-sample period. Secondly, our ablation experiment reveals no measurable forecasting advantage from persona descriptions, suggesting these prompt components can be omitted to reduce computational costs without sacrificing accuracy. Our results provide evidence that GPT-4o can achieve competitive forecasting accuracy even on out-of-sample macroeconomic events, if provided with relevant context data, while revealing that diverse prompts produce remarkably homogeneous forecasts compared to human panels.","\label{sec:related}
% \subsection{Forecasting with Large Language Models}
% \subsection{Persona prompting and prompt engineering}
\paragraph{Macroeconomic forecasting with Large Language Models}
The application of large language models to macroeconomic forecasting has emerged as a significant research area at the intersection of artificial intelligence and economics. Recent studies have explored direct applications of LLMs to a variety of forecasting tasks. Several studies share our focus on macroeconomic variables: \citet{carriero_macroeconomic_2024} examined LLM performance on macroeconomic time series, \citet{bybee_surveying_2023} fed Wall Street Journal articles to an LLM to predict financial and macroeconomic variables, while \citet{faria-e-castro_artificial_2024} demonstrated that Google's PaLM model could generate competitive inflation forecasts. 
%In financial market settings, Chen et al. (2022) applied BERT, RoBERTa, and OPT models to Thomson Reuters news feeds for firm-level return prediction, and Kim et al. (2024) utilized ChatGPT-4 with financial statements for earnings forecasting.
Our work extends these existing studies primarily through our rigorous focus on persona prompting and the deployment of a panel of more than 2,000 ''synthetic forecasters'', which enables systematic comparison against both expert human panels and realized macroeconomic outcomes.
Beyond evaluating LLM performance in specific forecasting applications, ongoing research has investigated general methodological approaches to LLM-based forecasting. \citet{lopez-lira_memorization_2025} investigated memorization effects in LLM-based economic forecasts, while \citet{tan2024language} compared language models to traditional time series methods. This methodological investigation connects to parallel efforts developing rigorous frameworks for LLM usage in economics research, which has become increasingly important as the field matures \citep{llm-econometric, korinek_language_2023}.
\paragraph{Simulating surveys responses}
A related strand of research examines LLMs' capacity to simulate human survey responses and expert judgment, similar to our comparison of human and AI panels in the Survey of Professional Forecasters. \citet{zarifhonarvar_evidence_2024} investigated inflation expectations formation using generative AI, finding that LLMs replicate key behavioral patterns including the tendency to predict higher inflation than realized rates. \citet{horton_large_2023} explored LLMs as ''simulated economic agents'', while \citet{argyle2023out} demonstrated that language models can replicate human samples in political surveys. \citet{geng2024large} and \citet{fell2024energy} examined LLMs' ability to simulate social survey responses, though \citet{dominguez-olmedo_questioning_2024} identified important limitations regarding ordering and labeling biases in LLM survey responses.
Most directly relevant to our investigation, \citet{hansen_simulating_2025} conducted the first systematic replication of the U.S. Survey of Professional Forecasters using LLMs, demonstrating comparable accuracy between AI and human forecasts. Their approach employed manually crafted forecaster personas based on SPF participant characteristics, in contrast with our larger extraction of persona prompts from the PersonaHub dataset.
\paragraph{Persona prompting}
A parallel line of research has examined the effects of prompt design and expert personas on LLM behavior. LLMs are highly sensitive to prompt content, structure, and formatting, as demonstrated by \citet{sclarquantifying}, who show that even subtle variations in prompt phrasing can lead to large shifts in model output. This sensitivity complicates the interpretation of results in applied settings, where changes in tone, emphasis, or structure may unintentionally influence forecast quality. One strategy to improve LLM performance on reasoning tasks involves persona prompting or role-play. \citet{kong-etal-2024-better} introduces a role-play prompting method in which LLMs are instructed to assume the identity of domain experts. This approach improves zero-shot performance across multiple benchmarks and has inspired further exploration of expert simulation in applied domains, including economics.
Persona prompting has also been used in macroeconomic forecasting experiments \citep{hansen_simulating_2025, zarifhonarvar_evidence_2024}. \citet{zarifhonarvar_evidence_2024} prompts LLM to adopt distinct persona attributes, such as political orientation, background, and socioeconomic characteristics. This induces realistic behaviors like partisan bias in inflation expectations, which mirror behaviors observed in actual human surveys. \citet{hansen_simulating_2025} construct detailed forecaster personas by manually gathering background data on SPF participants, including education, institutional affiliations, professional roles, and degrees. The study finds that removing these personas, i.e. replacing them with a generic forecaster prompt, leads to measurable drops in forecast accuracy, highlighting the value of role-based prompting. Interestingly, this is partially in contrast with our findings as presented in Section \ref{sec:results}.


% \cite{zarifhonarvar_evidence_2024} --> 

% Persona prompting to create AI agents with specific personality traits, backgrounds,
% and characteristics, generating more realistic and nuanced behaviors.

% Customizing prompts with a persona induces partisan biases, mirroring
% human survey behaviors.

% while  the experiment of using persona attributes and varied knowledge sources mitigate  some of these issues, they cannot entirely emulate the complex interaction of personal  experience, social dynamics, and psychological factors that drive human economic  decisions.




% \paragraph{Forecasting with Large Language Models}
% bla bla bla

% % * simulate economic forecasts \citep{hansen_simulating_2025}

% % * model inflation expectations
% % \citep{zarifhonarvar_evidence_2024}

% % * adoption of LLMs in economics research
% % \cite{llm-econometric}

% \paragraph{Persona prompting and prompt engineering}
% bla bla bla

% % * persona approach \citep{kong-etal-2024-better}

% % * LLM output heavily depends on prompt content and even formatting \citep{sclarquantifying}

% % * \citep{hansen_simulating_2025}


%Central bank communication has become a key monetary policy instrument, influencing both the narratives carried by financial media and the asset prices that move within minutes of a press conference \citep{gurkynak2005}. Because traditional surveys and event studies capture these interpretative and market channels only after the fact, economists are increasingly turning to Large Language Models (LLMs) as scalable simulators of how words might translate into economic consequences. However, this promising application currently faces a methodological limitation: existing studies use LLMs to simulate economic forecasts
% \citep{hansen_simulating_2025}, or model inflation expectations  \citep{zarifhonarvar_evidence_2024}, but typically rely on single handcrafted '''expert prompts'''. 

% Given that LLM output heavily depends on prompt content and even formatting \citep{sclarquantifying}, this approach cannot reliably determine whether models actually encode monetary policy expertise or simply respond to specific prompt engineering choices. This is a  crucial distinction to make if these tools are used to inform policy analysis. Despite the increasing adoption of LLMs in economic research \cite{llm-econometric}, there is a lack of empirical evidence regarding which prompt attributes increase performance on economic tasks. Previous work demonstrates that prompting LLMs to adapt expert \textit{personas} can lead to better performance on reasoning benchmarks \citep{kong-etal-2024-better}, suggesting potential benefits for other domains of application.","\paragraph{Macroeconomic forecasting with Large Language Models}
The application of large language models to macroeconomic forecasting has emerged as a significant research area at the intersection of artificial intelligence and economics. Recent studies have explored direct applications of LLMs to a variety of forecasting tasks. Several studies share our focus on macroeconomic variables: \citet{carriero_macroeconomic_2024} examined LLM performance on macroeconomic time series, \citet{bybee_surveying_2023} fed Wall Street Journal articles to an LLM to predict financial and macroeconomic variables, while \citet{faria-e-castro_artificial_2024} demonstrated that Google's PaLM model could generate competitive inflation forecasts. 

Our work extends these existing studies primarily through our rigorous focus on persona prompting and the deployment of a panel of more than 2,000 ''synthetic forecasters'', which enables systematic comparison against both expert human panels and realized macroeconomic outcomes.
Beyond evaluating LLM performance in specific forecasting applications, ongoing research has investigated general methodological approaches to LLM-based forecasting. \citet{lopez-lira_memorization_2025} investigated memorization effects in LLM-based economic forecasts, while \citet{tan2024language} compared language models to traditional time series methods. This methodological investigation connects to parallel efforts developing rigorous frameworks for LLM usage in economics research, which has become increasingly important as the field matures \citep{llm-econometric, korinek_language_2023}.
\paragraph{Simulating surveys responses}
A related strand of research examines LLMs' capacity to simulate human survey responses and expert judgment, similar to our comparison of human and AI panels in the Survey of Professional Forecasters. \citet{zarifhonarvar_evidence_2024} investigated inflation expectations formation using generative AI, finding that LLMs replicate key behavioral patterns including the tendency to predict higher inflation than realized rates. \citet{horton_large_2023} explored LLMs as ''simulated economic agents'', while \citet{argyle2023out} demonstrated that language models can replicate human samples in political surveys. \citet{geng2024large} and \citet{fell2024energy} examined LLMs' ability to simulate social survey responses, though \citet{dominguez-olmedo_questioning_2024} identified important limitations regarding ordering and labeling biases in LLM survey responses.
Most directly relevant to our investigation, \citet{hansen_simulating_2025} conducted the first systematic replication of the U.S. Survey of Professional Forecasters using LLMs, demonstrating comparable accuracy between AI and human forecasts. Their approach employed manually crafted forecaster personas based on SPF participant characteristics, in contrast with our larger extraction of persona prompts from the PersonaHub dataset.
\paragraph{Persona prompting}
A parallel line of research has examined the effects of prompt design and expert personas on LLM behavior. LLMs are highly sensitive to prompt content, structure, and formatting, as demonstrated by \citet{sclarquantifying}, who show that even subtle variations in prompt phrasing can lead to large shifts in model output. This sensitivity complicates the interpretation of results in applied settings, where changes in tone, emphasis, or structure may unintentionally influence forecast quality. One strategy to improve LLM performance on reasoning tasks involves persona prompting or role-play. \citet{kong-etal-2024-better} introduces a role-play prompting method in which LLMs are instructed to assume the identity of domain experts. This approach improves zero-shot performance across multiple benchmarks and has inspired further exploration of expert simulation in applied domains, including economics.
Persona prompting has also been used in macroeconomic forecasting experiments \citep{hansen_simulating_2025, zarifhonarvar_evidence_2024}. \citet{zarifhonarvar_evidence_2024} prompts LLM to adopt distinct persona attributes, such as political orientation, background, and socioeconomic characteristics. This induces realistic behaviors like partisan bias in inflation expectations, which mirror behaviors observed in actual human surveys. \citet{hansen_simulating_2025} construct detailed forecaster personas by manually gathering background data on SPF participants, including education, institutional affiliations, professional roles, and degrees. The study finds that removing these personas, i.e. replacing them with a generic forecaster prompt, leads to measurable drops in forecast accuracy, highlighting the value of role-based prompting. Interestingly, this is partially in contrast with our findings as presented in Section \ref{sec:results}.",
2509.21862v2,http://arxiv.org/abs/2509.21862v2,2025-09-26 04:38:59+00:00,Reimagining Agent-based Modeling with Large Language Model Agents via Shachi,"The study of emergent behaviors in large language model (LLM)-driven multi-agent systems is a critical research challenge, yet progress is limited by a lack of principled methodologies for controlled experimentation. To address this, we introduce Shachi, a formal methodology and modular framework that decomposes an agent's policy into core cognitive components: Configuration for intrinsic traits, Memory for contextual persistence, and Tools for expanded capabilities, all orchestrated by an LLM reasoning engine. This principled architecture moves beyond brittle, ad-hoc agent designs and enables the systematic analysis of how specific architectural choices influence collective behavior. We validate our methodology on a comprehensive 10-task benchmark and demonstrate its power through novel scientific inquiries. Critically, we establish the external validity of our approach by modeling a real-world U.S. tariff shock, showing that agent behaviors align with observed market reactions only when their cognitive architecture is appropriately configured with memory and tools. Our work provides a rigorous, open-source foundation for building and evaluating LLM agents, aimed at fostering more cumulative and scientifically grounded research.","\label{sec:related_works}

% About ABM
ABM is a computational approach to simulate interactions among autonomous agents within complex systems, enabling the study of emergent behaviors and social dynamics~\citep{gilbert2019agent}.
From a computer science perspective, ABM integrates agent-based computing, social sciences, and computer simulation, fostering cross-disciplinary research~\citep{davidsson2002agent}.
In the social sciences, ABM serves as a ``third way'' of research, complementing argumentation and formalization by enabling the modeling of complex processes and emergent phenomena~\citep{gilbert2000build}.
In economics, ABM has evolved into agent-based computational economics, modeling dynamic economic systems and revealing insights into market behaviors like strategic interactions and collective learning~\citep{tesfatsion2006agent,tesfatsion2006handbook}.
Enhancements in agent design through behavioral economics and empirical data integration have made ABM simulations more realistic and applicable to complex social and economic systems~\citep{steinbacher2021advances}.
With its wide applications, ABM remains a promising research area and motivates our work.
Refer to Appendix~\ref{sec:app:related_work_ABM} for more works on ABM.

% Existing LLM+ABM works
Recently, integrating LLMs into ABM has emerged as a promising direction to enhance the realism and adaptability of agents by improving environmental perception, human alignment, action generation, and evaluation~\citep{gao2024large,nisioti2024text}.
For example, PsychoBench~\citep{huang2023humanity} evaluates psychological traits; Generative agents~\citep{park2023generative} simulate interactive social behaviors;
and OASIS~\citep{yang2024oasis}, Sotopia~\citep{zhou2023sotopia}, and EconAgent~\citep{li2023econagent} extend these ideas to large-scale simulations and economic and social reasoning.
In the agent-based market domain, models like StockAgent~\citep{zhang2024ai} and AuctionArena~\citep{chen2023put} test strategic and adaptive decision-making.
See Appendix~\ref{sec:app:related_work_ABM_LLM} for more related works.
%
\implacro's contribution is distinct from both general-purpose agent toolkits and ML engineering frameworks.
General frameworks like AutoGen~\citep{wu2024autogen}, Concordia~\citep{vezhnevets2023generative}, and EDSL~\citep{expectedparrot_edsl_2023} are designed for conversational task automation or game-master-led interactions, not the specific requirements of reproducible social simulation.
In contrast, frameworks like MLE-Dojo~\citep{qiang2025mledojointeractiveenvironmentsempowering} focus on LLM training and engineering workflows, rather than the simulation and analysis of emergent social phenomena.
\implacro bridges this gap with an architecture centered on a standardized agent-environment interface and environment-mediated communication, uniquely tailored for the reproducible and systematic study of these dynamics.","ABM is a computational approach to simulate interactions among autonomous agents within complex systems, enabling the study of emergent behaviors and social dynamics~\citep{gilbert2019agent}.
From a computer science perspective, ABM integrates agent-based computing, social sciences, and computer simulation, fostering cross-disciplinary research~\citep{davidsson2002agent}.
In the social sciences, ABM serves as a ``third way'' of research, complementing argumentation and formalization by enabling the modeling of complex processes and emergent phenomena~\citep{gilbert2000build}.
In economics, ABM has evolved into agent-based computational economics, modeling dynamic economic systems and revealing insights into market behaviors like strategic interactions and collective learning~\citep{tesfatsion2006agent,tesfatsion2006handbook}.
Enhancements in agent design through behavioral economics and empirical data integration have made ABM simulations more realistic and applicable to complex social and economic systems~\citep{steinbacher2021advances}.
With its wide applications, ABM remains a promising research area and motivates our work.
Refer to Appendix~\ref{sec:app:related_work_ABM} for more works on ABM.


Recently, integrating LLMs into ABM has emerged as a promising direction to enhance the realism and adaptability of agents by improving environmental perception, human alignment, action generation, and evaluation~\citep{gao2024large,nisioti2024text}.
For example, PsychoBench~\citep{huang2023humanity} evaluates psychological traits; Generative agents~\citep{park2023generative} simulate interactive social behaviors;
and OASIS~\citep{yang2024oasis}, Sotopia~\citep{zhou2023sotopia}, and EconAgent~\citep{li2023econagent} extend these ideas to large-scale simulations and economic and social reasoning.
In the agent-based market domain, models like StockAgent~\citep{zhang2024ai} and AuctionArena~\citep{chen2023put} test strategic and adaptive decision-making.
See Appendix~\ref{sec:app:related_work_ABM_LLM} for more related works.

\implacro's contribution is distinct from both general-purpose agent toolkits and ML engineering frameworks.
General frameworks like AutoGen~\citep{wu2024autogen}, Concordia~\citep{vezhnevets2023generative}, and EDSL~\citep{expectedparrot_edsl_2023} are designed for conversational task automation or game-master-led interactions, not the specific requirements of reproducible social simulation.
In contrast, frameworks like MLE-Dojo~\citep{qiang2025mledojointeractiveenvironmentsempowering} focus on LLM training and engineering workflows, rather than the simulation and analysis of emergent social phenomena.
\implacro bridges this gap with an architecture centered on a standardized agent-environment interface and environment-mediated communication, uniquely tailored for the reproducible and systematic study of these dynamics.",
2511.00727v1,http://arxiv.org/abs/2511.00727v1,2025-11-01 22:24:16+00:00,Cross-Validated Causal Inference: a Modern Method to Combine Experimental and Observational Data,"We develop new methods to integrate experimental and observational data in causal inference. While randomized controlled trials offer strong internal validity, they are often costly and therefore limited in sample size. Observational data, though cheaper and often with larger sample sizes, are prone to biases due to unmeasured confounders. To harness their complementary strengths, we propose a systematic framework that formulates causal estimation as an empirical risk minimization (ERM) problem. A full model containing the causal parameter is obtained by minimizing a weighted combination of experimental and observational losses--capturing the causal parameter's validity and the full model's fit, respectively. The weight is chosen through cross-validation on the causal parameter across experimental folds. Our experiments on real and synthetic data show the efficacy and reliability of our method. We also provide theoretical non-asymptotic error bounds.","\label{sec:related_work}



We choose the weight for the experimental and observational estimates, denoted by $\lambda$, through cross-validation of the causal parameter. This is both inspired by a broader cross-validation-based statistical learning family that includes stacking \citep{wolpert1992stacked, breiman1996stacked}, aggregation \citep{tsybakov2003optimal, tsybakov2004optimal}, and super learner \citep{van2007super}. 
We adapt these tools for causal inference by addressing issues such as identification, confounding, and distributional shifts.
We design our cross-validation criterion to be explicitly tailored to causal estimands, rather than  predictive objectives.
Specifically, in each split, we fit on $K{-}1$ experimental folds and all observational data, and evaluate the causal parameter on the held-out experimental fold. The $\lambda$ that optimizes for the average experimental loss is then used to refit on all data. Intuitively, when the observational sample exhibits low bias, our method assigns more weight to the observational loss, exploiting the additional sample size. 


A systematic and unified framework to combine experimental and observational data remains largely absent---existing literature is often {\it ad hoc} in nature and hinges on auxiliary assumptions, such as extrapolatable bias \citep{kallus2018removing}, additional model specifications \citep{yang2020combining}, prespecified study structures \citep{rosenman2023combining}, or covariate similarity \citep{gui2024combining}. 
%We discuss these issues in detail in \ref{sec:extended_prior_work}.


\begin{table}[H]
\caption{Comparison with methods selected from each line of prior work. We use \checkmark ~for yes, \xmark ~for no, and $-$ for not applicable. 
\cite{yang2023elastic} conducts a test to determine whether observational data should be included, with the table outlining the conditions under which the test is likely to pass.
Extended descriptions of this table see Section \ref{sec:table_prior_work_description}. }
\scriptsize%\small
\centering
%\begin{tabular}{lccccc}
% for \small
% \begin{tabular}{p{6cm}%
%                 >{\centering\arraybackslash}m{1.5cm}%
%                 >{\centering\arraybackslash}m{1.5cm}%
%                 >{\centering\arraybackslash}m{1.5cm}%
%                 >{\centering\arraybackslash}m{1.5cm}%
%                 >{\centering\arraybackslash}m{1.5cm}}
\begin{tabular}{p{4.5cm}%
                >{\centering\arraybackslash}m{1.8cm}%
                >{\centering\arraybackslash}m{2cm}%
                >{\centering\arraybackslash}m{1.9cm}%
                >{\centering\arraybackslash}m{1.8cm}%
                >{\centering\arraybackslash}m{1.8cm}}
\toprule
 & AIPW \citep{robins1994estimation} & Error-prone \citep{yang2020combining} & Shrinkage \citep{rosenman2023combining} &  Pooling \citep{yang2023elastic} & Ours \\
\midrule\textbf{Experimental data}    &  &  &  &   & \\\cmidrule(r){1-1}
outcome model misspecification & \checkmark & \checkmark & $-$ & \checkmark & \checkmark   \\
\midrule\textbf{Observational data}    &  &  &  &  &  \\\cmidrule(r){1-1}
unmeasured confounders & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark  \\
outcome model misspecification & \checkmark & \checkmark & $-$ & \checkmark  & \checkmark   \\
both & \xmark & \checkmark & $-$ & \xmark & \checkmark    \\
\midrule\textbf{Cross-Source}    &  &  &  &  & \\\cmidrule(r){1-1}
inconsistent observational estimate & $-$ & \checkmark & \checkmark & \xmark & \checkmark  \\
shift in common covariates & $-$ & \checkmark & \checkmark & \checkmark & \checkmark  \\
no covariate overlap & $-$ & \checkmark & \checkmark & \xmark & \checkmark   \\
allow different outcome models & $-$ & \checkmark & $-$ & \checkmark & \checkmark  \\
no extra model specifications & $-$ & \xmark & \checkmark & \xmark & \checkmark  \\
allow different ATE across sources& $-$ & \checkmark & \xmark & \checkmark & \checkmark   \\
%\midrule
%technique &  &  &  &  &  &  &  &  &  &  &  &  \\
\bottomrule
\end{tabular}
\label{tab:prior_work}
\end{table}


The state of the existing literature is summarized at a high level in Table~\ref{tab:prior_work}. As the table indicates, the three major lines of work—pooling, shrinkage, and error-prone estimators—each have their limitations. 
\textbf{Pooling} methods treat all data as coming from a single source, breaking experimental randomization and requiring unconfoundedness assumptions to incorporate observational data 
%\citep{prentice2005combined, ross2009pooled, gao2023pretest, yang2023elastic, xiong2023federated}
\citep{ross2009pooled, gao2023pretest, yang2023elastic, xiong2023federated}. Our method could be seen as a “soft” version of pooling, dynamically adjusting the weighting of each data source rather than making an all-or-nothing decision. 
%This continuous adjustment allows the model to adaptively emphasize reliable signals from each data source. 
\textbf{Shrinkage} methods are most similar in spirit to our proposed method. They tolerate bias from observational data but depend on predefined strata, often assuming that the average effects are equal across data sources within each stratum---a condition that may not hold in practice \citep{stein1956inadmissibility, green1991james, green2005improved, rosenman2023combining}. Although both these methods and ours involve weighting, they adjust stratum-level estimators, while we bypass the need for discrete stratification and instead optimize weights at the loss level. 
%, leveraging all available data in a unified empirical risk minimization framework. 
\textbf{Error-prone estimators} carefully balance biased components for each source to cancel out confounding effects~\citep{yang2020combining}. This is an appealing idea, though in practice it can be challenging to construct such estimators. Both our method and error-prone approaches exploit the consistency of experimental estimates, but the mechanisms differ fundamentally. Instead of relying on delicate bias-cancellation conditions, we directly cross-validate on experimental data to prevent incorporating observational bias.
%Both our method and error-prone approaches exploit the consistency of experimental estimates, but the mechanisms differ fundamentally.  Error-prone methods rely on bias cancellation through algebraic manipulation, whereas our method is directly anchored on experimental data to prevent incorporating observational bias, and avoid reliance on delicate bias-cancellation conditions. 
In Section~\ref{sec:extended_prior_work}, we provide an extended discussion of related methods, with a focus on unmeasured confounding in observational data and a broader discussion on cross-validation in machine learning.","We choose the weight for the experimental and observational estimates, denoted by $\lambda$, through cross-validation of the causal parameter. This is both inspired by a broader cross-validation-based statistical learning family that includes stacking \citep{wolpert1992stacked, breiman1996stacked}, aggregation \citep{tsybakov2003optimal, tsybakov2004optimal}, and super learner \citep{van2007super}. 
We adapt these tools for causal inference by addressing issues such as identification, confounding, and distributional shifts.
We design our cross-validation criterion to be explicitly tailored to causal estimands, rather than  predictive objectives.
Specifically, in each split, we fit on $K{-}1$ experimental folds and all observational data, and evaluate the causal parameter on the held-out experimental fold. The $\lambda$ that optimizes for the average experimental loss is then used to refit on all data. Intuitively, when the observational sample exhibits low bias, our method assigns more weight to the observational loss, exploiting the additional sample size. 


A systematic and unified framework to combine experimental and observational data remains largely absent---existing literature is often {\it ad hoc} in nature and hinges on auxiliary assumptions, such as extrapolatable bias \citep{kallus2018removing}, additional model specifications \citep{yang2020combining}, prespecified study structures \citep{rosenman2023combining}, or covariate similarity \citep{gui2024combining}. 



\begin{table}[H]
\caption{Comparison with methods selected from each line of prior work. We use \checkmark ~for yes, \xmark ~for no, and $-$ for not applicable. 
\cite{yang2023elastic} conducts a test to determine whether observational data should be included, with the table outlining the conditions under which the test is likely to pass.
Extended descriptions of this table see Section \ref{sec:table_prior_work_description}. }
\scriptsize
\centering








\begin{tabular}{p{4.5cm}
                >{\centering\arraybackslash}m{1.8cm}
                >{\centering\arraybackslash}m{2cm}
                >{\centering\arraybackslash}m{1.9cm}
                >{\centering\arraybackslash}m{1.8cm}
                >{\centering\arraybackslash}m{1.8cm}}
\toprule
 & AIPW \citep{robins1994estimation} & Error-prone \citep{yang2020combining} & Shrinkage \citep{rosenman2023combining} &  Pooling \citep{yang2023elastic} & Ours \\
\midrule\textbf{Experimental data}    &  &  &  &   & \\\cmidrule(r){1-1}
outcome model misspecification & \checkmark & \checkmark & $-$ & \checkmark & \checkmark   \\
\midrule\textbf{Observational data}    &  &  &  &  &  \\\cmidrule(r){1-1}
unmeasured confounders & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark  \\
outcome model misspecification & \checkmark & \checkmark & $-$ & \checkmark  & \checkmark   \\
both & \xmark & \checkmark & $-$ & \xmark & \checkmark    \\
\midrule\textbf{Cross-Source}    &  &  &  &  & \\\cmidrule(r){1-1}
inconsistent observational estimate & $-$ & \checkmark & \checkmark & \xmark & \checkmark  \\
shift in common covariates & $-$ & \checkmark & \checkmark & \checkmark & \checkmark  \\
no covariate overlap & $-$ & \checkmark & \checkmark & \xmark & \checkmark   \\
allow different outcome models & $-$ & \checkmark & $-$ & \checkmark & \checkmark  \\
no extra model specifications & $-$ & \xmark & \checkmark & \xmark & \checkmark  \\
allow different ATE across sources& $-$ & \checkmark & \xmark & \checkmark & \checkmark   \\


\bottomrule
\end{tabular}
\end{table}


The state of the existing literature is summarized at a high level in Table~\ref{tab:prior_work}. As the table indicates, the three major lines of work—pooling, shrinkage, and error-prone estimators—each have their limitations. 
\textbf{Pooling} methods treat all data as coming from a single source, breaking experimental randomization and requiring unconfoundedness assumptions to incorporate observational data 

\citep{ross2009pooled, gao2023pretest, yang2023elastic, xiong2023federated}. Our method could be seen as a “soft” version of pooling, dynamically adjusting the weighting of each data source rather than making an all-or-nothing decision. 

\textbf{Shrinkage} methods are most similar in spirit to our proposed method. They tolerate bias from observational data but depend on predefined strata, often assuming that the average effects are equal across data sources within each stratum---a condition that may not hold in practice \citep{stein1956inadmissibility, green1991james, green2005improved, rosenman2023combining}. Although both these methods and ours involve weighting, they adjust stratum-level estimators, while we bypass the need for discrete stratification and instead optimize weights at the loss level. 

\textbf{Error-prone estimators} carefully balance biased components for each source to cancel out confounding effects~\citep{yang2020combining}. This is an appealing idea, though in practice it can be challenging to construct such estimators. Both our method and error-prone approaches exploit the consistency of experimental estimates, but the mechanisms differ fundamentally. Instead of relying on delicate bias-cancellation conditions, we directly cross-validate on experimental data to prevent incorporating observational bias.

In Section~\ref{sec:extended_prior_work}, we provide an extended discussion of related methods, with a focus on unmeasured confounding in observational data and a broader discussion on cross-validation in machine learning.",
2510.22828v1,http://arxiv.org/abs/2510.22828v1,2025-10-26 20:43:52+00:00,Efficiently Learning Synthetic Control Models for High-dimensional Disaggregated Data,"The Synthetic Control method (SC) has become a valuable tool for estimating causal effects. Originally designed for single-treated unit scenarios, it has recently found applications in high-dimensional disaggregated settings with multiple treated units. However, challenges in practical implementation and computational efficiency arise in such scenarios. To tackle these challenges, we propose a novel approach that integrates the Multivariate Square-root Lasso method into the synthetic control framework. We rigorously establish the estimation error bounds for fitting the Synthetic Control weights using Multivariate Square-root Lasso, accommodating high-dimensionality and time series dependencies. Additionally, we quantify the estimation error for the Average Treatment Effect on the Treated (ATT). Through simulation studies, we demonstrate that our method offers superior computational efficiency without compromising estimation accuracy. We apply our method to assess the causal impact of COVID-19 Stay-at-Home Orders on the monthly unemployment rate in the United States at the county level.","\label{sec:RelatedWork}


\textbf{Synthetic Control Methods for Multiple Treated Units.} 
In the new era of big data, there has been an increasing interest in applying Synthetic Control Methods in high-dimensional settings with multiple treated units. There are two main groups of literature. 
The first group deals with aggregated data by combining all the treated units into a single treated unit. For instance, \cite{dube2015pooling} transformed Synthetic Control estimates to elasticities, then averaged the elasticities. 
%\cite{xu2017generalized} estimated the common factors among all treated units in an interactive fixed effects (IFE) model. 
\cite{robbins2017framework} and \cite{hazlett2018trajectory} worked on the unweighted average of outcomes for all treated units.
%aggregated outcomes and utilize calibration to estimate Synthetic Control weights. 
\cite{abadie2021synthetic} propose experimental designs based on synthetic units that match aggregate feature values in the population of interest.
% estimated the potential outcomes under treatment using the weighted averages of treated units similar to the controls.
The second group addresses disaggregated data and emphasizes two practical challenges: the non-unique solutions for weights and overfitting concerns caused by a high-dimensional donor pool when the number of control units exceeds the number of time points. \cite{hollingsworth2020tactics} proposed a Synthetic Control Using Lasso (SCUL) that allows extrapolation and automatic donor selection. \cite{abadie2021penalized} introduced an augmented Synthetic Control estimator with a penalty term. The penalty term is weighted by the Euclidean norm of the difference between the features of the treated unit and each unit in the donor pool, which encourages the use of control units with characteristics similar to the treated unit. 








\textbf{High-dimensional Multivariate Linear Regression.}
When the number of unknown parameters is greater than the number of observations, the least squares estimator is not unique. A natural alternative is a penalized least squares estimator \citep{turlach2005simultaneous, yuan2007dimension, obozinski2011support, negahban2011estimation}, which implicitly assumes that the error terms follow an identical normal distribution. Later on, to further utilize the information of the error covariance matrix, \cite{rothman2010sparse} proposed Multivariate Regression with Covariance Estimation (MRCE) to estimate the error covariance matrix and the unknown parameters jointly. MRCE maximizes a penalized normal log-likelihood by updating the error covariance matrix and the unknown parameters iteratively. Variations of MRCE was further studied by \cite{niu2019simultaneous}, \cite{chang2022robust} and \cite{molstad2021explicit}. However, sometimes we do not need the estimated error covariance matrix, and the above-mentioned methods are computationally expensive. More recently, \cite{molstad2021new} proposed the Multivariate Square-root Lasso that implicitly estimates the error covariance matrix and is computationally efficient.
%\cite{wang2015joint} proposed an alternative approach which performs estimation columnby-column, estimating the $k$ th columns of $\boldsymbol{\beta}_*$ and $\boldsymbol{\Omega}_*$ jointly for $k \in\{1, \ldots, q\}$. While these methods can perform well in certain settings, an estimate of $\boldsymbol{\Omega}_*$ is often not needed by the practitioner. 


 \vspace{-0.7cm}","\textbf{Synthetic Control Methods for Multiple Treated Units.} 
In the new era of big data, there has been an increasing interest in applying Synthetic Control Methods in high-dimensional settings with multiple treated units. There are two main groups of literature. 
The first group deals with aggregated data by combining all the treated units into a single treated unit. For instance, \cite{dube2015pooling} transformed Synthetic Control estimates to elasticities, then averaged the elasticities. 

\cite{robbins2017framework} and \cite{hazlett2018trajectory} worked on the unweighted average of outcomes for all treated units.

\cite{abadie2021synthetic} propose experimental designs based on synthetic units that match aggregate feature values in the population of interest.

The second group addresses disaggregated data and emphasizes two practical challenges: the non-unique solutions for weights and overfitting concerns caused by a high-dimensional donor pool when the number of control units exceeds the number of time points. \cite{hollingsworth2020tactics} proposed a Synthetic Control Using Lasso (SCUL) that allows extrapolation and automatic donor selection. \cite{abadie2021penalized} introduced an augmented Synthetic Control estimator with a penalty term. The penalty term is weighted by the Euclidean norm of the difference between the features of the treated unit and each unit in the donor pool, which encourages the use of control units with characteristics similar to the treated unit. 








\textbf{High-dimensional Multivariate Linear Regression.}
When the number of unknown parameters is greater than the number of observations, the least squares estimator is not unique. A natural alternative is a penalized least squares estimator \citep{turlach2005simultaneous, yuan2007dimension, obozinski2011support, negahban2011estimation}, which implicitly assumes that the error terms follow an identical normal distribution. Later on, to further utilize the information of the error covariance matrix, \cite{rothman2010sparse} proposed Multivariate Regression with Covariance Estimation (MRCE) to estimate the error covariance matrix and the unknown parameters jointly. MRCE maximizes a penalized normal log-likelihood by updating the error covariance matrix and the unknown parameters iteratively. Variations of MRCE was further studied by \cite{niu2019simultaneous}, \cite{chang2022robust} and \cite{molstad2021explicit}. However, sometimes we do not need the estimated error covariance matrix, and the above-mentioned methods are computationally expensive. More recently, \cite{molstad2021new} proposed the Multivariate Square-root Lasso that implicitly estimates the error covariance matrix and is computationally efficient.



 \vspace{-0.7cm}",
2509.25721v2,http://arxiv.org/abs/2509.25721v2,2025-09-30 03:26:17+00:00,The AI Productivity Index (APEX),"We introduce the first version of the AI Productivity Index (APEX), a benchmark for assessing whether frontier AI models can perform knowledge work with high economic value. APEX addresses one of the largest inefficiencies in AI research: outside of coding, benchmarks often fail to test economically relevant capabilities. APEX-v1.0 contains 200 test cases and covers four domains: investment banking, management consulting, law, and primary medical care. It was built in three steps. First, we sourced experts with top-tier experience e.g., investment bankers from Goldman Sachs. Second, experts created prompts that reflect high-value tasks in their day-to-day work. Third, experts created rubrics for evaluating model responses. We evaluate 23 frontier models on APEX-v1.0 using an LM judge. GPT 5 (Thinking = High) achieves the highest mean score (64.2%), followed by Grok 4 (61.3%) and Gemini 2.5 Flash (Thinking = On) (60.4%). Qwen 3 235B is the best performing open-source model and seventh best overall. There is a large gap between the performance of even the best models and human experts, highlighting the need for better measurement of models' ability to produce economically valuable work.","%% https://arxiv.org/pdf/2506.11763 
%% Benchmarks
%% Could add discussion of e.g., LMSYS or wildchat
Numerous benchmarks have been developed to test the reasoning, tool use, instruction following, and generative capabilities of LMs. 
GAIA introduces a benchmark of 466 real‑world multi-step tasks that require advanced capabilities such as reasoning, tool use (e.g. web browsing, coding), and multi-modal comprehension to be solved \citep{mialon2023gaiabenchmarkgeneralai}. Answers are generally unambiguous, enabling high-quality scoring. 
MMLU tests models on general knowledge and reasoning across 57 academic and professional subjects using multiple-choice questions \citep{hendrycks2021measuringmassivemultitasklanguage}. It is widely used to measure general knowledge and reasoning, but has been criticized for data errors \citep{gema2025mmlu} and saturation \citep{wang2024mmluprorobustchallengingmultitask}, especially as the data is open-source.
GPQA is a dataset of 448 expert-crafted, graduate-level multiple-choice questions in biology, physics, and chemistry that even PhD-level experts achieve only approximately 65\% \citep{rein2023gpqagraduatelevelgoogleproofqa}. This high-quality resource only has multiple choice questions, and is focused on academic performance rather than real-world value. 
Similarly, Humanity's Last Exam (HLE) is a benchmark of expert-level, high-stakes questions from professional and academic domains, created by PhDs in each respective field \citep{phan2025humanitysexam}. It also lacks focus on economically-valuable outputs.
GSM8K is a less challenging but widely used dataset. It contains high-quality, grade-school math word problems that need to be solved step-by-step \citep{cobbe2021trainingverifierssolvemath}, and is designed to evaluate models' arithmetic reasoning.
Many other question answering datasets evaluate model's knowledge and reasoning capabilities in specific domains, like FinanceBench \citep{islam2023financebenchnewbenchmarkfinancial}, PubMedQA \citep{jin2019pubmedqadatasetbiomedicalresearch} and ScienceQA \citep{lu2022learnexplainmultimodalreasoning}.
There are also abstract reasoning benchmarks such as ARC-1 \citep{clark2018thinksolvedquestionanswering} and ARC-2 \citep{chollet2025arcagi2newchallengefrontier}, which are part of the AI2 Reasoning Challenge. ARC contains visual reasoning tasks, assessing models' ability to perform complex reasoning without explicit instructions.
\vspace{1em}

%% Economic focused evals
AI models' ability to improve workers' productivity is a growing area of interest in research.
OpenAI's GDPval is a benchmark for testing AI capabilities at performing real world economically valuable tasks \citep{openai_gdpval}. It covers 44 occupations across the top 9 sectors contributing to U.S. Gross Domestic Product. Industry professionals created the prompts and rubrics, and GPDval is one of the first benchmarks to reflect real-world high-value tasks. The authors open source 220 gold cases and show that for some tasks frontier models are approaching industry experts' quality of work.
Anthropic's Economic Index \citep{handa2025economictasksperformedai} aims to understand the impact of AI on the economy over time. An open source project, it uses real data from Claude to track where and how AI is used, focusing on broad groupings like ``Computer \& Mathematical'' and ``Business \& Financial''. It shows that partial automation, and use of AI to augment human workers, is the dominant paradigm of AI use.
Evaluations of AI models' ability to perform knowledge is mixed.

VendingBench \citep{backlund2025vendingbenchbenchmarklongtermcoherence} tests LLM-based agents' ability to operate a simulated vending machine. Agents have access to tools and must execute simple tasks (e.g., placing orders), managing workload over a long time horizon. Models are capable of returning a profit in many runs, but sometimes derail by losing track of the task or making poor decisions and losing money.
TheAgentCompany is a benchmark for evaluating agents' performance at the tasks of a digital worker, where they can browse the Web, write code, run programs, and communicate with other coworkers. They find that the best agent can complete 30\% of tasks autonomously \citep{xu2025theagentcompanybenchmarkingllmagents}.
\citet{HENDRIX2022331} look at use of AI among clinicians and argue that while it could increase productivity, it will increase workloads if badly implemented, and needs to be used for suitable tasks.
\citet{becker2025measuringimpactearly2025ai} run a controlled study on 16 developers with moderate AI experience. Participants complete 246 tasks in mature projects on which they have a mean of 5 years of prior experience. Although participants anticipate that AI will reduce completion time by 24\%, the authors found it increased completion time by 19\%. This finding challenges other research that shows AI coding assistants can generate a significant uplift in output and speed \citep{peng2023impactaideveloperproductivity, paradis2024doesaiimpactdevelopment}.
\citet{miserendino2025swelancerfrontierllmsearn} introduce SWE-Lancer, a benchmark of over 1,400 freelance software engineering tasks from Upwork, valued at 1 million dollars. Tasks vary from simple bug fixes to complex tasks involving advanced reasoning and management. Three frontier models earn between 30\% (o3) and 40\% (Claude 3.5 Sonnet) of the money available.
%\citet{acemoglu} argues that AI will likely drive modest gains in productivity. Total Factor Productivity, a measure of labor and capital efficiency, could increase by as little as a total of 0.53\% over 10 years. Further, this could widen the gap between economic and capital income, and be counterbalanced by negative social value created by AI.","Numerous benchmarks have been developed to test the reasoning, tool use, instruction following, and generative capabilities of LMs. 
GAIA introduces a benchmark of 466 real‑world multi-step tasks that require advanced capabilities such as reasoning, tool use (e.g. web browsing, coding), and multi-modal comprehension to be solved \citep{mialon2023gaiabenchmarkgeneralai}. Answers are generally unambiguous, enabling high-quality scoring. 
MMLU tests models on general knowledge and reasoning across 57 academic and professional subjects using multiple-choice questions \citep{hendrycks2021measuringmassivemultitasklanguage}. It is widely used to measure general knowledge and reasoning, but has been criticized for data errors \citep{gema2025mmlu} and saturation \citep{wang2024mmluprorobustchallengingmultitask}, especially as the data is open-source.
GPQA is a dataset of 448 expert-crafted, graduate-level multiple-choice questions in biology, physics, and chemistry that even PhD-level experts achieve only approximately 65\
Similarly, Humanity's Last Exam (HLE) is a benchmark of expert-level, high-stakes questions from professional and academic domains, created by PhDs in each respective field \citep{phan2025humanitysexam}. It also lacks focus on economically-valuable outputs.
GSM8K is a less challenging but widely used dataset. It contains high-quality, grade-school math word problems that need to be solved step-by-step \citep{cobbe2021trainingverifierssolvemath}, and is designed to evaluate models' arithmetic reasoning.
Many other question answering datasets evaluate model's knowledge and reasoning capabilities in specific domains, like FinanceBench \citep{islam2023financebenchnewbenchmarkfinancial}, PubMedQA \citep{jin2019pubmedqadatasetbiomedicalresearch} and ScienceQA \citep{lu2022learnexplainmultimodalreasoning}.
There are also abstract reasoning benchmarks such as ARC-1 \citep{clark2018thinksolvedquestionanswering} and ARC-2 \citep{chollet2025arcagi2newchallengefrontier}, which are part of the AI2 Reasoning Challenge. ARC contains visual reasoning tasks, assessing models' ability to perform complex reasoning without explicit instructions.
\vspace{1em}


AI models' ability to improve workers' productivity is a growing area of interest in research.
OpenAI's GDPval is a benchmark for testing AI capabilities at performing real world economically valuable tasks \citep{openai_gdpval}. It covers 44 occupations across the top 9 sectors contributing to U.S. Gross Domestic Product. Industry professionals created the prompts and rubrics, and GPDval is one of the first benchmarks to reflect real-world high-value tasks. The authors open source 220 gold cases and show that for some tasks frontier models are approaching industry experts' quality of work.
Anthropic's Economic Index \citep{handa2025economictasksperformedai} aims to understand the impact of AI on the economy over time. An open source project, it uses real data from Claude to track where and how AI is used, focusing on broad groupings like ``Computer \& Mathematical'' and ``Business \& Financial''. It shows that partial automation, and use of AI to augment human workers, is the dominant paradigm of AI use.
Evaluations of AI models' ability to perform knowledge is mixed.

VendingBench \citep{backlund2025vendingbenchbenchmarklongtermcoherence} tests LLM-based agents' ability to operate a simulated vending machine. Agents have access to tools and must execute simple tasks (e.g., placing orders), managing workload over a long time horizon. Models are capable of returning a profit in many runs, but sometimes derail by losing track of the task or making poor decisions and losing money.
TheAgentCompany is a benchmark for evaluating agents' performance at the tasks of a digital worker, where they can browse the Web, write code, run programs, and communicate with other coworkers. They find that the best agent can complete 30\
\citet{HENDRIX2022331} look at use of AI among clinicians and argue that while it could increase productivity, it will increase workloads if badly implemented, and needs to be used for suitable tasks.
\citet{becker2025measuringimpactearly2025ai} run a controlled study on 16 developers with moderate AI experience. Participants complete 246 tasks in mature projects on which they have a mean of 5 years of prior experience. Although participants anticipate that AI will reduce completion time by 24\
\citet{miserendino2025swelancerfrontierllmsearn} introduce SWE-Lancer, a benchmark of over 1,400 freelance software engineering tasks from Upwork, valued at 1 million dollars. Tasks vary from simple bug fixes to complex tasks involving advanced reasoning and management. Three frontier models earn between 30\",
2511.05577v1,http://arxiv.org/abs/2511.05577v1,2025-11-04 22:32:53+00:00,Fine-Tuning Vision-Language Models for Multimodal Polymer Property Prediction,"Vision-Language Models (VLMs) have shown strong performance in tasks like visual question answering and multimodal text generation, but their effectiveness in scientific domains such as materials science remains limited. While some machine learning methods have addressed specific challenges in this field, there is still a lack of foundation models designed for broad tasks like polymer property prediction using multimodal data. In this work, we present a multimodal polymer dataset to fine-tune VLMs through instruction-tuning pairs and assess the impact of multimodality on prediction performance. Our fine-tuned models, using LoRA, outperform unimodal and baseline approaches, demonstrating the benefits of multimodal learning. Additionally, this approach reduces the need to train separate models for different properties, lowering deployment and maintenance costs.","Beyond traditional deep learning models designed for classification, vision-language models have recently gained significant attention due to their broad range of applications. CLIP \cite{radford2021learning} introduced a framework that links image and text data, enabling models to learn visual concepts for classification tasks through natural language supervision. Building on this idea, models such as OpenAI GPT-4 \cite{OpenAI2023GPT4TR}, Google Gemini \cite{team2023gemini}, Flamingo \cite{alayrac2022flamingo}, OpenFlamingo \cite{awadalla2023openflamingo}, LLama-Vision \cite{meta2024llama} and LLaVA \cite{liu2023visual} have demonstrated strong performance on vision-language tasks, including visual question answering and human-like dialogue. Alongside these foundation models, researchers have explored extending large-scale architectures to specialized domains, such as LLaVA-Med \cite{li2023llava}, Llama-SciTune \cite{horawalavithana2023scitune}, and Vision-BioLLM \cite{alshibli2025vision}.

There have been numerous efforts to integrate AI into materials science research, particularly through the development of deep learning and foundation models for materials discovery and property prediction. This is especially relevant for polymers, where the cost of computational simulations or experimental measurements is often prohibitively high. Huan et al. \cite{huan2016polymer} introduced a dataset of polymer properties, which laid the foundation for the Polymer Genome platform \cite{kim2018polymer} designed to efficiently predict and retrieve polymer properties. Building on this, Doan et al. \cite{doan2020machine} applied machine learning approaches trained on Polymer Genome data for property prediction. More recently, BERT-based models \cite{devlin2019bert} have been adapted for polymers: Kuenneth et al. \cite{kuenneth2023polybert} developed PolyBERT, a large-scale representation model trained on millions of polymers, whose embeddings can serve as inputs for property predictors. Similarly, Wang et al. \cite{wang2024predicting} proposed a Transformer-based architecture capable of extracting both 1D representations from P-SMILES and 3D representations from molecular conformations to perform multitask learning, including P-SMILES reconstruction, 3D coordinate generation, and cross-modal fusion. In line with recent trends in deep learning, large language models (LLMs) have also been explored for polymer property prediction \cite{gupta2025benchmarking,zhang2025multimodal}. Specifically, Gupta et al. \cite{gupta2025benchmarking} fine-tuned and evaluated text-only LLMs on P-SMILES inputs, while Zhang et al. \cite{zhang2025multimodal} combined multimodal embeddings, such as LLM-derived representations from P-SMILES and Uni-Mol \cite{zhou2023uni} embeddings from polymer structures, and then train multilayer perceptrons for property prediction. Despite these advances, a unified multimodal Vision-Language Model capable of directly predicting properties from multimodal inputs such as images, text, and molecular descriptors remains lacking.","Beyond traditional deep learning models designed for classification, vision-language models have recently gained significant attention due to their broad range of applications. CLIP \cite{radford2021learning} introduced a framework that links image and text data, enabling models to learn visual concepts for classification tasks through natural language supervision. Building on this idea, models such as OpenAI GPT-4 \cite{OpenAI2023GPT4TR}, Google Gemini \cite{team2023gemini}, Flamingo \cite{alayrac2022flamingo}, OpenFlamingo \cite{awadalla2023openflamingo}, LLama-Vision \cite{meta2024llama} and LLaVA \cite{liu2023visual} have demonstrated strong performance on vision-language tasks, including visual question answering and human-like dialogue. Alongside these foundation models, researchers have explored extending large-scale architectures to specialized domains, such as LLaVA-Med \cite{li2023llava}, Llama-SciTune \cite{horawalavithana2023scitune}, and Vision-BioLLM \cite{alshibli2025vision}.

There have been numerous efforts to integrate AI into materials science research, particularly through the development of deep learning and foundation models for materials discovery and property prediction. This is especially relevant for polymers, where the cost of computational simulations or experimental measurements is often prohibitively high. Huan et al. \cite{huan2016polymer} introduced a dataset of polymer properties, which laid the foundation for the Polymer Genome platform \cite{kim2018polymer} designed to efficiently predict and retrieve polymer properties. Building on this, Doan et al. \cite{doan2020machine} applied machine learning approaches trained on Polymer Genome data for property prediction. More recently, BERT-based models \cite{devlin2019bert} have been adapted for polymers: Kuenneth et al. \cite{kuenneth2023polybert} developed PolyBERT, a large-scale representation model trained on millions of polymers, whose embeddings can serve as inputs for property predictors. Similarly, Wang et al. \cite{wang2024predicting} proposed a Transformer-based architecture capable of extracting both 1D representations from P-SMILES and 3D representations from molecular conformations to perform multitask learning, including P-SMILES reconstruction, 3D coordinate generation, and cross-modal fusion. In line with recent trends in deep learning, large language models (LLMs) have also been explored for polymer property prediction \cite{gupta2025benchmarking,zhang2025multimodal}. Specifically, Gupta et al. \cite{gupta2025benchmarking} fine-tuned and evaluated text-only LLMs on P-SMILES inputs, while Zhang et al. \cite{zhang2025multimodal} combined multimodal embeddings, such as LLM-derived representations from P-SMILES and Uni-Mol \cite{zhou2023uni} embeddings from polymer structures, and then train multilayer perceptrons for property prediction. Despite these advances, a unified multimodal Vision-Language Model capable of directly predicting properties from multimodal inputs such as images, text, and molecular descriptors remains lacking.",
2511.05492v1,http://arxiv.org/abs/2511.05492v1,2025-11-07 18:59:58+00:00,Quantum Tensor Representation via Circuit Partitioning and Reintegration,"Quantum computing enables faster computations than clas-sical algorithms through superposition and entanglement. Circuit cutting and knitting are effective techniques for ame-liorating current noisy quantum processing unit (QPUs) er-rors via a divide-and-conquer approach that splits quantum circuits into subcircuits and recombines them using classical post-processing. The development of circuit partitioning and recomposing has focused on tailoring the simulation frame-work by replacing generic non-local gates with probabilistic local gates and measuring the classical communication com-plexity. Designing a protocol that supports algorithms and non-all-to-all qubit-connected physical hardware remains underdeveloped owing to the convoluted properties of cut-ting compact controlled unitary gates and hardware topology. In this study, we introduce shardQ, a method that leverages the SparseCut algorithm with matrix product state (MPS) compilation and a global knitting technique. This method elucidates the optimal trade-off between the computational time and error rate for quantum encoding with a theoretical proof, evidenced by an ablation analysis using an IBM Mar-rakesh superconducting-type QPU. This study also presents the results regarding application readiness.","The concept underlying our approach traces back to the Clifford-gate group simulation protocol \cite{bravyi2005universal}, which reduces the classical–quantum computation overhead and strengthens the hybrid paradigm \cite{bravyi2016trading}. This foundation has driven both empirical \cite{peng2020simulating} and theoretical \cite{mitarai2019methodology} advances in clustered simulations for molecular systems, which were later enhanced by maximum-likelihood fragment tomography \cite{perlin2021quantum}.
Subsequent studies have examined the overhead of circuit cutting and knitting for entanglement gates \cite{piveteau2023circuit,yang2024understanding,gentinetta2024overhead,jing2025circuit} and error mitigation in low-depth entanglement circuits \cite{temme2017error}, leading to practical frameworks such as CutQC \cite{tang2022cutting} and Qiskit Circuit Knitting \cite{shehzad2024automated}. However, these frameworks lack algorithm-level, QPU-aware optimizations for hardware with limited connectivity.
Recent innovations include heuristic randomized measurement cutting for the quantum approximate optimization algorithm (\qaoa) \cite{lowe2023fast} and high-fidelity cutting with gradient-based reconstruction \cite{hart2024reconstructing}. The Qdislib framework \cite{tejedor2025distributed} extends cutting to distributed settings by integrating HPC with QPUs. However, the scalability and efficiency of such cutting methods—particularly for quantum tensor encoding in the \nisq\ era—remain uncertain.
Our proposed approach is motivated by the error mitigation benefits of tightly coupling QPUs with classical HPC resources \cite{carrera2024combining}.","The concept underlying our approach traces back to the Clifford-gate group simulation protocol \cite{bravyi2005universal}, which reduces the classical–quantum computation overhead and strengthens the hybrid paradigm \cite{bravyi2016trading}. This foundation has driven both empirical \cite{peng2020simulating} and theoretical \cite{mitarai2019methodology} advances in clustered simulations for molecular systems, which were later enhanced by maximum-likelihood fragment tomography \cite{perlin2021quantum}.
Subsequent studies have examined the overhead of circuit cutting and knitting for entanglement gates \cite{piveteau2023circuit,yang2024understanding,gentinetta2024overhead,jing2025circuit} and error mitigation in low-depth entanglement circuits \cite{temme2017error}, leading to practical frameworks such as CutQC \cite{tang2022cutting} and Qiskit Circuit Knitting \cite{shehzad2024automated}. However, these frameworks lack algorithm-level, QPU-aware optimizations for hardware with limited connectivity.
Recent innovations include heuristic randomized measurement cutting for the quantum approximate optimization algorithm (\qaoa) \cite{lowe2023fast} and high-fidelity cutting with gradient-based reconstruction \cite{hart2024reconstructing}. The Qdislib framework \cite{tejedor2025distributed} extends cutting to distributed settings by integrating HPC with QPUs. However, the scalability and efficiency of such cutting methods—particularly for quantum tensor encoding in the \nisq\ era—remain uncertain.
Our proposed approach is motivated by the error mitigation benefits of tightly coupling QPUs with classical HPC resources \cite{carrera2024combining}.","The concept underlying our approach traces back to the
Clifford-gate group simulation protocol [ 6], which reduces
the classical–quantum computation overhead and strength-
ens the hybrid paradigm [ 7]. This foundation has driven both
empirical [ 24] and theoretical [ 21] advances in clustered sim-
ulations for molecular systems, which were later enhanced
by maximum-likelihood fragment tomography [ 25]. Subse-
quent studies have examined the overhead of circuit cut-
ting and knitting for entanglement gates [ 13,19,26,35] and
error mitigation in low-depth entanglement circuits [ 34],
leading to practical frameworks such as CutQC [ 32] and
Qiskit Circuit Knitting [ 30]. However, these frameworks
lack algorithm-level, QPU-aware optimizations for hard-
ware with limited connectivity. Recent innovations include
heuristic randomized measurement cutting for the quan-
tum approximate optimization algorithm (QAOA) [ 20] and
high-fidelity cutting with gradient-based reconstruction [ 17].
The Qdislib framework [ 33] extends cutting to distributed
settings by integrating HPC with QPUs. However, the scala-
bility and efficiency of such cutting methods—particularly
for quantum tensor encoding in theNISQera—remain un-
certain. Our proposed approach is motivated by the error
mitigation benefits of tightly coupling QPUs with classical
HPC resources [9]."
2511.05103v2,http://arxiv.org/abs/2511.05103v2,2025-11-07 09:39:37+00:00,Neural Network for Subgrid Turbulence Modeling for Large Eddy Simulations,"When simulating multiscale systems, where some fields cannot be fully prescribed despite their effects on the simulation's accuracy, closure models are needed. This behavior is observed in turbulent fluid dynamics, where Large Eddy Simulations (LES) depict global behavior while turbulence modeling introduces dissipation correspondent to smaller sub-grid scales. Recently, scientific machine learning techniques have emerged to address this problem by integrating traditional (physics-based) equations with data-driven (machine-learned) models, typically coupling numerical solvers with neural networks. This work presents a comprehensive workflow, encompassing high-fidelity data generation, a priori learning, and a posteriori learning, where data-driven models enhance differential equations. The study underscores the critical role of post-processing and effective filtering of fine-resolution fields and the implications numerical methods selection, such as the Lattice Boltzmann Method (LBM) or Finite Volume Method.","\label{sec:bibli}

The research for data-driven SGS turbulence modeling has been highly active in recent years, particularly with the integration of neural networks. In \cite{sanderse2024scientific}, the authors provide a review of recent contributions aligned with the closure equation formalism discussed in Section \ref{sec:intro}. 

Typically, many of these contributions adopt a priori learning approaches \cite{dai2023development,yu2022kinetic, Kurz_2022, Kim_Park_Choi_2024}, which involve offline training of models by minimizing closure errors. However, these methods often face stability issues when applied in real-world scenarios, prompting studies such as \cite{Park_Choi_2021, BECK2019108910} to explore alternative strategies. Another avenue of research focuses on a posteriori learning \cite{kim2024generalizabledatadriventurbulenceclosure, List_2022, Beck_2021}, where models are trained by solving reduced equations and minimizing solution errors. Although this approach enhances stability, it incurs higher computational costs.

A third strategy involves hybrid methods, which blend a priori and a posteriori learning to strike a balance between performance and stability. Additionally, reconstruction techniques aim to recover high-dimensional solutions from reduced fields \cite{NISTA2025106498, Fukami_2020, Yuan_2020}. All these methods can be further enhanced by incorporating governing equations either as soft constraints, as demonstrated by Physics-Informed Neural Networks (PINNs) \cite{cuomo2022scientificmachinelearningphysicsinformed, raissi2017physicsinformeddeeplearning}, or as hard constraints, such as in Tensor-Basis Neural Networks (TBNNs), which enforce field invariance through symmetry \cite{LING201622, Wu_2018, BOSE2024107483}.","The research for data-driven SGS turbulence modeling has been highly active in recent years, particularly with the integration of neural networks. In \cite{sanderse2024scientific}, the authors provide a review of recent contributions aligned with the closure equation formalism discussed in Section \ref{sec:intro}. 

Typically, many of these contributions adopt a priori learning approaches \cite{dai2023development,yu2022kinetic, Kurz_2022, Kim_Park_Choi_2024}, which involve offline training of models by minimizing closure errors. However, these methods often face stability issues when applied in real-world scenarios, prompting studies such as \cite{Park_Choi_2021, BECK2019108910} to explore alternative strategies. Another avenue of research focuses on a posteriori learning \cite{kim2024generalizabledatadriventurbulenceclosure, List_2022, Beck_2021}, where models are trained by solving reduced equations and minimizing solution errors. Although this approach enhances stability, it incurs higher computational costs.

A third strategy involves hybrid methods, which blend a priori and a posteriori learning to strike a balance between performance and stability. Additionally, reconstruction techniques aim to recover high-dimensional solutions from reduced fields \cite{NISTA2025106498, Fukami_2020, Yuan_2020}. All these methods can be further enhanced by incorporating governing equations either as soft constraints, as demonstrated by Physics-Informed Neural Networks (PINNs) \cite{cuomo2022scientificmachinelearningphysicsinformed, raissi2017physicsinformeddeeplearning}, or as hard constraints, such as in Tensor-Basis Neural Networks (TBNNs), which enforce field invariance through symmetry \cite{LING201622, Wu_2018, BOSE2024107483}.","The research for data-driven SGS turbulence modeling
has been highly active in recent years, particularly with the
integration of neural networks. In [4], the authors provide
a review of recent contributions aligned with the closure
equation formalism discussed in Section 1.
Typically, many of these contributions adopt a priori
learning approaches [5, 6, 7, 8], which involve offline train-
ingofmodelsbyminimizingclosureerrors.However,these
methods often face stability issues when applied in real-
worldscenarios,promptingstudiessuchas[9,10]toexplore
alternativestrategies.Anotheravenueofresearchfocuseson
a posteriori learning [11, 12, 13], where models are trained
by solving reduced equations and minimizing solution er-
rors. Although this approach enhances stability, it incurs
higher computational costs.
Athirdstrategyinvolveshybridmethods,whichblenda
priori and a posteriori learning to strike a balance between
performanceandstability.Additionally,reconstructiontech-
niques aim to recover high-dimensional solutions from re-
duced fields [14, 15, 16]. All these methods can be further
enhancedbyincorporatinggoverningequationseitherassoft
constraints, as demonstrated by Physics-Informed Neural
Networks (PINNs) [17, 18], or as hard constraints, such as
in Tensor-Basis Neural Networks (TBNNs), which enforce
field invariance through symmetry [19, 20, 21]"
2511.04856v1,http://arxiv.org/abs/2511.04856v1,2025-11-06 22:40:18+00:00,Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning,"We introduce theoretically grounded Continuous Semi-Quantum Boltzmann Machines (CSQBMs) that supports continuous-action reinforcement learning. By combining exponential-family priors over visible units with quantum Boltzmann distributions over hidden units, CSQBMs yield a hybrid quantum-classical model that reduces qubit requirements while retaining strong expressiveness. Crucially, gradients with respect to continuous variables can be computed analytically, enabling direct integration into Actor-Critic algorithms. Building on this, we propose a continuous Q-learning framework that replaces global maximization by efficient sampling from the CSQBM distribution, thereby overcoming instability issues in continuous control.","\label{sec:related_work}

Quantum RL~(QRL) comprises several distinct approaches~\cite{meyer2022survey}.
At the most fundamental level, fully quantum formulations and subroutine-based methods (e.g., amplitude amplification~\cite{Brassard2000QuantumAA}) promise asymptotic advantages but typically require fault-tolerant hardware.
In contrast, variational approaches~\cite{jerbi2021parametrized} are better suited to near-term devices, replacing classical function approximators with parameterized quantum circuits, though they face challenges such as barren plateaus and noise.
Finally, energy-based models, in particular quantum Boltzmann machines (QBMs), exploit quantum sampling for richer representations and are able to capture multi-modalities.
%Benchmarking~\cite{neumann2023quantum,kruse2025benchmarking,meyer2025benchmarking}

By employing the free energy~(FE) of a QBM to approximate the $Q$-value, prior works~\cite{kent2024using,crawford2018reinforcement,levit2017free,neumann2020multi} demonstrated improved sample efficiency compared to classical neural networks~(NNs) in prototypical environments with discrete action spaces.
An extension to continuous-valued environments was investigated in~\cite{schenk2024hybrid}, where an AC approach was applied to the AWAKE beam line at CERN.
This method represents continuous states and actions by encoding them into Bernoulli-distributed binary visible units, thereby compromising the theoretical soundness of the BM model.
This forces reliance on finite-difference approximations for gradient computation over the visible units, a procedure that is both computationally inefficient and prone to numerical instability.

Several extensions of Boltzmann machines to continuous visible domains have been developed to better accommodate real-valued data.
Prominent examples include Gaussian-Bernoulli models~\cite{cho2011improved,melchior2017gaussian} and more general formulations based on exponential-family distributions~\cite{welling2004exponential,li2018exponential}.
While these approaches enhance expressiveness, they suffer from training instabilities caused by sampling difficulties.
To address this, a continuous-valued QBM has been proposed~\cite{bangar2025continuous}, leveraging imaginary-time evolution on photonic quantum hardware for more efficient sampling.
Although promising, this method is inherently tied to that specific platform and does not readily extend to other QC paradigms.

In continuous environments, AC algorithms often achieve state-of-the-art performance, yet they suffer from persistent challenges such as handling action constraints, training instability, and, most critically, sample inefficiency.
By contrast, $Q$-learning~\cite{watkins1992q} is highly sample-efficient but restricted to low-dimensional discrete action spaces, since it requires global maximization over the input of an NN---an intractable task due to high non-linearity~\cite{katz2017reluplex}.
While reformulations of the maximization step have been investigated~\cite{ryucaql,burtea2024constrained}, their scalability remains uncertain.
Sampling-based methods can efficiently approximate local optima~\cite{kalashnikov2018scalable,simmons2019q,perakis2022optimizing}, but they risk unstable or divergent training.
Alternatively, analytic solutions for the global optimum are possible under strong structural assumptions on the NN~\cite{gu2016continuous,plaksin2022continuous,amos2017input}, though at the cost of severely limiting representational power.

Our approach addresses these limitations by introducing CSQBMs, which combine a theoretically sound hybrid quantum–classical distribution: an exponential-family prior over the visible units and a quantum Boltzmann distribution over the hidden units.
Gradients with respect to the visible units can be computed analytically, making the model directly applicable within AC frameworks.
In addition, we propose a continuous $Q$-learning algorithm based on CSQBMs that enables efficient sampling for global $Q$-value maximization.","Quantum RL~(QRL) comprises several distinct approaches~\cite{meyer2022survey}.
At the most fundamental level, fully quantum formulations and subroutine-based methods (e.g., amplitude amplification~\cite{Brassard2000QuantumAA}) promise asymptotic advantages but typically require fault-tolerant hardware.
In contrast, variational approaches~\cite{jerbi2021parametrized} are better suited to near-term devices, replacing classical function approximators with parameterized quantum circuits, though they face challenges such as barren plateaus and noise.
Finally, energy-based models, in particular quantum Boltzmann machines (QBMs), exploit quantum sampling for richer representations and are able to capture multi-modalities.


By employing the free energy~(FE) of a QBM to approximate the $Q$-value, prior works~\cite{kent2024using,crawford2018reinforcement,levit2017free,neumann2020multi} demonstrated improved sample efficiency compared to classical neural networks~(NNs) in prototypical environments with discrete action spaces.
An extension to continuous-valued environments was investigated in~\cite{schenk2024hybrid}, where an AC approach was applied to the AWAKE beam line at CERN.
This method represents continuous states and actions by encoding them into Bernoulli-distributed binary visible units, thereby compromising the theoretical soundness of the BM model.
This forces reliance on finite-difference approximations for gradient computation over the visible units, a procedure that is both computationally inefficient and prone to numerical instability.

Several extensions of Boltzmann machines to continuous visible domains have been developed to better accommodate real-valued data.
Prominent examples include Gaussian-Bernoulli models~\cite{cho2011improved,melchior2017gaussian} and more general formulations based on exponential-family distributions~\cite{welling2004exponential,li2018exponential}.
While these approaches enhance expressiveness, they suffer from training instabilities caused by sampling difficulties.
To address this, a continuous-valued QBM has been proposed~\cite{bangar2025continuous}, leveraging imaginary-time evolution on photonic quantum hardware for more efficient sampling.
Although promising, this method is inherently tied to that specific platform and does not readily extend to other QC paradigms.

In continuous environments, AC algorithms often achieve state-of-the-art performance, yet they suffer from persistent challenges such as handling action constraints, training instability, and, most critically, sample inefficiency.
By contrast, $Q$-learning~\cite{watkins1992q} is highly sample-efficient but restricted to low-dimensional discrete action spaces, since it requires global maximization over the input of an NN---an intractable task due to high non-linearity~\cite{katz2017reluplex}.
While reformulations of the maximization step have been investigated~\cite{ryucaql,burtea2024constrained}, their scalability remains uncertain.
Sampling-based methods can efficiently approximate local optima~\cite{kalashnikov2018scalable,simmons2019q,perakis2022optimizing}, but they risk unstable or divergent training.
Alternatively, analytic solutions for the global optimum are possible under strong structural assumptions on the NN~\cite{gu2016continuous,plaksin2022continuous,amos2017input}, though at the cost of severely limiting representational power.

Our approach addresses these limitations by introducing CSQBMs, which combine a theoretically sound hybrid quantum–classical distribution: an exponential-family prior over the visible units and a quantum Boltzmann distribution over the hidden units.
Gradients with respect to the visible units can be computed analytically, making the model directly applicable within AC frameworks.
In addition, we propose a continuous $Q$-learning algorithm based on CSQBMs that enables efficient sampling for global $Q$-value maximization.","Quantum RL (QRL) comprises several distinct approaches [30]. At the most fundamental level,
fully quantum formulations and subroutine-based methods (e.g., amplitude amplification [31])
promise asymptotic advantages but typically require fault-tolerant hardware. In contrast, variational
approaches [32] are better suited to near-term devices, replacing classical function approximators
with parameterized quantum circuits, though they face challenges such as barren plateaus and noise.
Finally, energy-based models, in particular quantum Boltzmann machines (QBMs), exploit quantum
sampling for richer representations and are able to capture multi-modalities.
By employing the free energy (FE) of a QBM to approximate the Q-value, prior works [1, 26, 27, 33]
demonstrated improved sample efficiency compared to classical neural networks (NNs) in prototypical
environments with discrete action spaces. An extension to continuous-valued environments was
investigated in [28], where an AC approach was applied to the AWAKE beam line at CERN. This
method represents continuous states and actions by encoding them into Bernoulli-distributed binary
visible units, thereby compromising the theoretical soundness of the BM model. This forces reliance
on finite-difference approximations for gradient computation over the visible units, a procedure that
is both computationally inefficient and prone to numerical instability.
Several extensions of Boltzmann machines to continuous visible domains have been developed to
better accommodate real-valued data. Prominent examples include Gaussian-Bernoulli models [34,
35] and more general formulations based on exponential-family distributions [36, 37]. While
these approaches enhance expressiveness, they suffer from training instabilities caused by sampling
difficulties. To address this, a continuous-valued QBM has been proposed [38], leveraging imaginary-
time evolution on photonic quantum hardware for more efficient sampling. Although promising, this
method is inherently tied to that specific platform and does not readily extend to other QC paradigms.
In continuous environments, AC algorithms often achieve state-of-the-art performance, yet they suffer
from persistent challenges such as handling action constraints, training instability, and, most critically,
sample inefficiency. By contrast, Q-learning [25] is highly sample-efficient but restricted to low-
dimensional discrete action spaces, since it requires global maximization over the input of an NN—an
intractable task due to high non-linearity [39]. While reformulations of the maximization step have
been investigated [40, 41], their scalability remains uncertain. Sampling-based methods can efficiently
approximate local optima [42–44], but they risk unstable or divergent training. Alternatively, analytic
solutions for the global optimum are possible under strong structural assumptions on the NN [45–47],
though at the cost of severely limiting representational power.
2
Our approach addresses these limitations by introducing CSQBMs, which combine a theoretically
sound hybrid quantum–classical distribution: an exponential-family prior over the visible units and a
quantum Boltzmann distribution over the hidden units. Gradients with respect to the visible units can
be computed analytically, making the model directly applicable within AC frameworks. In addition,
we propose a continuous Q-learning algorithm based on CSQBMs that enables efficient sampling for
globalQ-value maximization."
2511.08970v1,http://arxiv.org/abs/2511.08970v1,2025-11-12 04:27:22+00:00,JW-Flare: Accurate Solar Flare Forecasting Method Based on Multimodal Large Language Models,"Solar flares, the most powerful explosive phenomena in the solar system, may pose significant hazards to spaceborne satellites and ground-based infrastructure. Despite decades of intensive research, reliable flare prediction remains a challenging task. Large Language Models, as a milestone in artificial intelligence, exhibit exceptional general knowledge and next-token prediction capabilities. Here we introduce JW-Flare, the first Multimodal Large Language Models (MLLMs) explicitly trained for solar flare forecasting through fine-tuning on textual physic parameters of solar active regions and magnetic field images. This method demonstrates state-of-the-art (SOTA) performance for large flares prediction on the test dataset. It effectively identifies all 79 X-class flares from 18,949 test samples, yielding a True Skill Statistic (TSS) of 0.95 and a True Positive Rate (TPR) of 1.00, outperforming traditional predictive models. We further investigate the capability origins of JW-Flare through explainability experiments, revealing that solar physics knowledge acquired during pre-training contributes to flare forecasting performance. Additionally, we evaluate models of different parameter scales, confirming the Scaling_Law of Large Language Models in domain-specific applications, such as solar physics. This study marks a substantial advance in both the scale and accuracy of solar flare forecasting and opens a promising avenue for AI-driven methodologies in broader scientific domains.","The first recorded solar flare occurred on September 1, 1859, and was independently observed by Richard C. Carrington and Richard Hodgson. In 1939, Giovanelli et al. \cite{giovanelli1939relations} initiated the first systematic investigation into the  relationship between sunspots and solar flares. Over the past century, research on solar flare forecasting has been primarily categorized into physics-based methods and data-driven methods. Physics-based methods focus on uncovering the underlying mechanisms of flare eruptions, while data-driven methods aim to establish forecasting models through the analysis of extensive observational data. 

\subsection{physics-based methods}

Since the discovery of solar flares, solar physicists have devoted considerable effort to elucidating the physical origins of eruptions and constructing effective predictive frameworks \cite{huang2024short}. For example, Self-Organized Criticality (SOC) models, derived from first principles, have been widely explored to simulate the intricate processes of solar evolution and flare formation by employing numerical simulations to predict the spatial distribution and temporal evolution of key physical parameters \cite{karakatsanis2008soc}. However, the inherent complexity of solar flare mechanisms has prevented a definitive consensus on their physical origins and continues to pose significant challenges to developing precise physics-based predictive models. \cite{korsos2018applying, lin2009studies, ning2009investigation, ning2012power, wang2012solar}.

\subsection{data-driven methods}

\textbf{Statistical methods:}
As solar observational data continue to accumulate, data-driven approaches for solar flare prediction have evolved significantly in methodology. Initially, the Space Environment Laboratory of National Oceanic and Atmospheric Administration, in collaboration with the University of Colorado, developed the first expert system to predict solar flares by utilizing the relationship between McIntosh Sunspot Classifications and flare activity. Later on, Bloomfield et al. \cite{bloomfield2012toward}proposed a probabilistic prediction model based on a Poisson distribution, which estimated the likelihood of different flare classes by calculating the average flare eruption rate for within each individual McIntosh sunspot class. At that time, solar flare prediction primarily relies on domain experts manually identifying key predictive features (e.g., magnetic field parameters and sunspot classifications) and employing statistical learning methods to model empirical relationships between these features and flare events.

\textbf{Machine learning methods:}
The emergence of machine learning frameworks has effectively mitigated the limitations of traditional statistical learning by enabling semi-automated feature extraction, in which domain experts define the feature space and directional constraints, while algorithms iteratively optimize these features. Yuan et al. (2016) \cite{yuan2020solar} applied Principal Component Analysis (PCA) to extract feature from sunspot active region parameters(e.g., shape, area, and other morphological features) and 10.7 cm radio flux, which were subsequently used as inputs for an Support Vector Machines(SVM) prediction model. Ensemble approaches further enhanced predictive robustness, as shown in Abduallah et al. (2021) \cite{abduallah2021deepsun}, which integrated multiple learning algorithms through a voting mechanism to predict flares of varying magnitudes. The reliance on manual intervention for feature selection during this period posed significant challenges for automated forecasting, necessitating extensive data preprocessing to extract meaningful predictors from raw observations.

\textbf{Deep learning methods:}
In contrast, deep learning leverages multi-layer nonlinear architectures to automatically learn hierarchical features, significantly reducing the reliance on manual feature selection and enabling fully automated feature extraction. Huang et al. (2018) \cite{huang2018deep} employed Convolutional Neural Network (CNN) to automatically identify regions with opposing magnetic polarities, eliminating the need for manual feature engineering. Nishizuka et al. \cite{nishizuka2018deep, nishizuka2020reliable, nishizuka2021operational} applied deep neural networks to analyse 79-dimensional physical features extracted from active region magnetograms, aiming to predict 24-hour solar flare probabilities. Their results demonstrated that such models are capable of capturing complex spatiotemporal patterns inherent in high-dimensional magnetogram data. Zheng et al. (2023) \cite{zheng2023multiclass} proposed the HBiLSTM-Attention model, which integrates an attention mechanism to enhance multi-class flare prediction by leveraging the spatial distribution and temporal evolution of solar active region (AR) magnetograms. Similarly, Abduallah et al. (2023) \cite{abduallah2023operational} developed SolarFlareNet, a hybrid model integrating CNN, Long Short-Term Memory (LSTM), and Transformer architectures, using magnetic field parameters extracted from the Spaceweather Helioseismic and Magnetic Imager Active Region Patch (SHARP) data to predict flares with intensities of $\geq$M5.0-class, $\geq$M-class,and $\geq$C-class, respectively. While these approaches enable automated feature extraction, their dependence on single-modality data (e.g., images, textual descriptions, or univariate time series) inherently restricts their capacity to model cross-modal interactions and integrate contextual information.","The first recorded solar flare occurred on September 1, 1859, and was independently observed by Richard C. Carrington and Richard Hodgson. In 1939, Giovanelli et al. \cite{giovanelli1939relations} initiated the first systematic investigation into the  relationship between sunspots and solar flares. Over the past century, research on solar flare forecasting has been primarily categorized into physics-based methods and data-driven methods. Physics-based methods focus on uncovering the underlying mechanisms of flare eruptions, while data-driven methods aim to establish forecasting models through the analysis of extensive observational data. 

\subsection{physics-based methods}

Since the discovery of solar flares, solar physicists have devoted considerable effort to elucidating the physical origins of eruptions and constructing effective predictive frameworks \cite{huang2024short}. For example, Self-Organized Criticality (SOC) models, derived from first principles, have been widely explored to simulate the intricate processes of solar evolution and flare formation by employing numerical simulations to predict the spatial distribution and temporal evolution of key physical parameters \cite{karakatsanis2008soc}. However, the inherent complexity of solar flare mechanisms has prevented a definitive consensus on their physical origins and continues to pose significant challenges to developing precise physics-based predictive models. \cite{korsos2018applying, lin2009studies, ning2009investigation, ning2012power, wang2012solar}.

\subsection{data-driven methods}

\textbf{Statistical methods:}
As solar observational data continue to accumulate, data-driven approaches for solar flare prediction have evolved significantly in methodology. Initially, the Space Environment Laboratory of National Oceanic and Atmospheric Administration, in collaboration with the University of Colorado, developed the first expert system to predict solar flares by utilizing the relationship between McIntosh Sunspot Classifications and flare activity. Later on, Bloomfield et al. \cite{bloomfield2012toward}proposed a probabilistic prediction model based on a Poisson distribution, which estimated the likelihood of different flare classes by calculating the average flare eruption rate for within each individual McIntosh sunspot class. At that time, solar flare prediction primarily relies on domain experts manually identifying key predictive features (e.g., magnetic field parameters and sunspot classifications) and employing statistical learning methods to model empirical relationships between these features and flare events.

\textbf{Machine learning methods:}
The emergence of machine learning frameworks has effectively mitigated the limitations of traditional statistical learning by enabling semi-automated feature extraction, in which domain experts define the feature space and directional constraints, while algorithms iteratively optimize these features. Yuan et al. (2016) \cite{yuan2020solar} applied Principal Component Analysis (PCA) to extract feature from sunspot active region parameters(e.g., shape, area, and other morphological features) and 10.7 cm radio flux, which were subsequently used as inputs for an Support Vector Machines(SVM) prediction model. Ensemble approaches further enhanced predictive robustness, as shown in Abduallah et al. (2021) \cite{abduallah2021deepsun}, which integrated multiple learning algorithms through a voting mechanism to predict flares of varying magnitudes. The reliance on manual intervention for feature selection during this period posed significant challenges for automated forecasting, necessitating extensive data preprocessing to extract meaningful predictors from raw observations.

\textbf{Deep learning methods:}
In contrast, deep learning leverages multi-layer nonlinear architectures to automatically learn hierarchical features, significantly reducing the reliance on manual feature selection and enabling fully automated feature extraction. Huang et al. (2018) \cite{huang2018deep} employed Convolutional Neural Network (CNN) to automatically identify regions with opposing magnetic polarities, eliminating the need for manual feature engineering. Nishizuka et al. \cite{nishizuka2018deep, nishizuka2020reliable, nishizuka2021operational} applied deep neural networks to analyse 79-dimensional physical features extracted from active region magnetograms, aiming to predict 24-hour solar flare probabilities. Their results demonstrated that such models are capable of capturing complex spatiotemporal patterns inherent in high-dimensional magnetogram data. Zheng et al. (2023) \cite{zheng2023multiclass} proposed the HBiLSTM-Attention model, which integrates an attention mechanism to enhance multi-class flare prediction by leveraging the spatial distribution and temporal evolution of solar active region (AR) magnetograms. Similarly, Abduallah et al. (2023) \cite{abduallah2023operational} developed SolarFlareNet, a hybrid model integrating CNN, Long Short-Term Memory (LSTM), and Transformer architectures, using magnetic field parameters extracted from the Spaceweather Helioseismic and Magnetic Imager Active Region Patch (SHARP) data to predict flares with intensities of $\geq$M5.0-class, $\geq$M-class,and $\geq$C-class, respectively. While these approaches enable automated feature extraction, their dependence on single-modality data (e.g., images, textual descriptions, or univariate time series) inherently restricts their capacity to model cross-modal interactions and integrate contextual information.","The first recorded solar flare occurred on September 1, 1859,
and was independently observed by Richard C. Carrington
and Richard Hodgson. In 1939, Giovanelli et al. (Giovanelli
1939) initiated the first systematic investigation into the re-
lationship between sunspots and solar flares. Over the past
century, research on solar flare forecasting has been primar-
ily categorized into physics-based methods and data-driven
methods. Physics-based methods focus on uncovering the
underlying mechanisms of flare eruptions, while data-driven
methods aim to establish forecasting models through the
analysis of extensive observational data.
physics-based methods
Since the discovery of solar flares, solar physicists have de-
voted considerable effort to elucidating the physical origins
of eruptions and constructing effective predictive frame-
works (Huang et al. 2024). For example, Self-Organized
Criticality (SOC) models, derived from first principles, have
been widely explored to simulate the intricate processes of
solar evolution and flare formation by employing numeri-
cal simulations to predict the spatial distribution and tem-
poral evolution of key physical parameters (Karakatsanis
and Pavlos 2008). However, the inherent complexity of so-
lar flare mechanisms has prevented a definitive consensus on
their physical origins and continues to pose significant chal-
lenges to developing precise physics-based predictive mod-
els. (Kors ´os, Chatterjee, and Erd ´elyi 2018; Lin 2009; Ning
2009, 2012; Wang 2012).
data-driven methods
Statistical methods:As solar observational data continue
to accumulate, data-driven approaches for solar flare predic-
tion have evolved significantly in methodology. Initially, the
Space Environment Laboratory of National Oceanic and At-
mospheric Administration, in collaboration with the Univer-
sity of Colorado, developed the first expert system to predict
solar flares by utilizing the relationship between McIntosh
Sunspot Classifications and flare activity. Later on, Bloom-
field et al. (Bloomfield et al. 2012)proposed a probabilistic
prediction model based on a Poisson distribution, which es-
timated the likelihood of different flare classes by calculat-
ing the average flare eruption rate for within each individ-
ual McIntosh sunspot class. At that time, solar flare predic-
tion primarily relies on domain experts manually identify-
ing key predictive features (e.g., magnetic field parameters
and sunspot classifications) and employing statistical learn-
ing methods to model empirical relationships between these
features and flare events.
Machine learning methods:The emergence of machine
learning frameworks has effectively mitigated the limita-
tions of traditional statistical learning by enabling semi-
automated feature extraction, in which domain experts de-
fine the feature space and directional constraints, while algo-
rithms iteratively optimize these features. Yuan et al. (2016)
(Yuan et al. 2020) applied Principal Component Analysis
(PCA) to extract feature from sunspot active region param-
eters(e.g., shape, area, and other morphological features)
and 10.7 cm radio flux, which were subsequently used as
inputs for an Support Vector Machines(SVM) prediction
model. Ensemble approaches further enhanced predictive
robustness, as shown in Abduallah et al. (2021) (Abdual-
lah et al. 2021), which integrated multiple learning algo-
rithms through a voting mechanism to predict flares of vary-
ing magnitudes. The reliance on manual intervention for fea-
ture selection during this period posed significant challenges
for automated forecasting, necessitating extensive data pre-
processing to extract meaningful predictors from raw obser-
vations.
Deep learning methods:In contrast, deep learning lever-
ages multi-layer nonlinear architectures to automatically
learn hierarchical features, significantly reducing the re-
liance on manual feature selection and enabling fully auto-
mated feature extraction. Huang et al. (2018) (Huang et al.
2018) employed Convolutional Neural Network (CNN) to
automatically identify regions with opposing magnetic po-
larities, eliminating the need for manual feature engineer-
ing. Nishizuka et al. (Nishizuka et al. 2018, 2020, 2021) ap-
plied deep neural networks to analyse 79-dimensional phys-
ical features extracted from active region magnetograms,
aiming to predict 24-hour solar flare probabilities. Their re-
sults demonstrated that such models are capable of cap-
turing complex spatiotemporal patterns inherent in high-
dimensional magnetogram data. Zheng et al. (2023) (Zheng
et al. 2023) proposed the HBiLSTM-Attention model, which
integrates an attention mechanism to enhance multi-class
flare prediction by leveraging the spatial distribution and
temporal evolution of solar active region (AR) magne-
tograms. Similarly, Abduallah et al. (2023) (Abduallah et al.
2023) developed SolarFlareNet, a hybrid model integrat-
ing CNN, Long Short-Term Memory (LSTM), and Trans-
former architectures, using magnetic field parameters ex-
tracted from the Spaceweather Helioseismic and MagneticImager Active Region Patch (SHARP) data to predict flares
with intensities of≥M5.0-class,≥M-class,and≥C-class,
respectively. While these approaches enable automated fea-
ture extraction, their dependence on single-modality data
(e.g., images, textual descriptions, or univariate time series)
inherently restricts their capacity to model cross-modal in-
teractions and integrate contextual information."
2511.01464v1,http://arxiv.org/abs/2511.01464v1,2025-11-03 11:23:13+00:00,Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions,"By reducing resolution, coarse-grained models greatly accelerate molecular simulations, unlocking access to long-timescale phenomena, though at the expense of microscopic information. Recovering this fine-grained detail is essential for tasks that depend on atomistic accuracy, making backmapping a central challenge in molecular modeling. We introduce split-flows, a novel flow-based approach that reinterprets backmapping as a continuous-time measure transport across resolutions. Unlike existing generative strategies, split-flows establish a direct probabilistic link between resolutions, enabling expressive conditional sampling of atomistic structures and -- for the first time -- a tractable route to computing mapping entropies, an information-theoretic measure of the irreducible detail lost in coarse-graining. We demonstrate these capabilities on diverse molecular systems, including chignolin, a lipid bilayer, and alanine dipeptide, highlighting split-flows as a principled framework for accurate backmapping and systematic evaluation of coarse-grained models.","Solving the inverse problem of backmapping is a central challenge in multiscale molecular modeling \cite{Peter2009}. Mirroring trends across many scientific domains, data-driven methods increasingly replace traditional handcrafted algorithms, such as those by \cite{Rzepiela2010}, and \cite{Wassenaar2014}, which predict approximate fine-grained configurations from coarse inputs, followed by costly refinement.
Early approaches, such as \cite{Stieffenhofer2020}, \cite{Li2020}, and \cite{Wang2022}, leverage generative adversarial networks and variational autoencoders to generate fine-grained samples, without the need for post hoc refinement. \cite{Shmilovich2022} extend this line of work by incorporating information along reconstructed simulations to ensure temporal consistency.
More recent methods by \cite{Jones2023}, \cite{ferguson2025flowback, Berlaga2025}, and \cite{UgarteLaTorre2025} adopt multi-step samplers, i.e., continuous normalizing flows and diffusion models, enabling generalization to unseen structures through residue-wise processing and transferable coarse-graining schemes. While these models emphasize energetic plausibility, transferability, or dynamical consistency, they do not establish a probabilistic link between resolutions and therefore miss key statistical properties of the coarse-graining map. Our method addresses this limitation.

Normalizing flows, introduced by \cite{rezende2015flows} in discrete form, map complex data distributions to simple latents. The continuous-time formulation of \cite{Chen2018} improves expressiveness but initially lacks a tractable training procedure. Flow matching \cite{Lipman2023} resolves this by replacing maximum likelihood with a quadratic regression objective for the underlying velocity field, enabling efficient training of continuous flows. Modern formulations of flow matching, particularly those by \cite{Albergo2023} and \cite{Tong2024}, generalize normalizing flows to define a measure transport between arbitrary pairs of distributions. Most similar to our approach, \cite{Albergo2023data} apply this framework to image super-resolution and in-painting. See also \cite{Brehmer2020} for a tangentially related perspective. We build on this modern interpretation of continuous normalizing flows to connect molecular configurations across resolutions.


Mapping entropy, first introduced by \cite{Foley2015} in the context of molecular modeling, provides an information-theoretic measure of the loss incurred when representing a system at reduced resolution. Its utility spans a wide range of applications. For example, \cite{Kidder2024} use mapping entropy to identify high-quality representations of the structural protein actin, while \cite{Giulini2020, Giulini2024} develop a coarse-graining software suite that operationalizes the concept. \cite{Armstrong2012} and \cite{Jin2023} establish connections between mapping entropy and the consistency of dynamical behavior across resolutions, while \cite{Mussi2025} disentangle entropic and energetic contributions to observables in coarse-grained simulations. Beyond molecular systems, \cite{Holtzman2022} demonstrate its broader relevance by applying it to spin models and low-dimensional descriptors of financial markets. While prior studies rely on application-specific methodologies, split-flows provide a rigorous and general framework for computing mapping entropies across diverse systems and reduction strategies.","Solving the inverse problem of backmapping is a central challenge in multiscale molecular modeling \cite{Peter2009}. Mirroring trends across many scientific domains, data-driven methods increasingly replace traditional handcrafted algorithms, such as those by \cite{Rzepiela2010}, and \cite{Wassenaar2014}, which predict approximate fine-grained configurations from coarse inputs, followed by costly refinement.
Early approaches, such as \cite{Stieffenhofer2020}, \cite{Li2020}, and \cite{Wang2022}, leverage generative adversarial networks and variational autoencoders to generate fine-grained samples, without the need for post hoc refinement. \cite{Shmilovich2022} extend this line of work by incorporating information along reconstructed simulations to ensure temporal consistency.
More recent methods by \cite{Jones2023}, \cite{ferguson2025flowback, Berlaga2025}, and \cite{UgarteLaTorre2025} adopt multi-step samplers, i.e., continuous normalizing flows and diffusion models, enabling generalization to unseen structures through residue-wise processing and transferable coarse-graining schemes. While these models emphasize energetic plausibility, transferability, or dynamical consistency, they do not establish a probabilistic link between resolutions and therefore miss key statistical properties of the coarse-graining map. Our method addresses this limitation.

Normalizing flows, introduced by \cite{rezende2015flows} in discrete form, map complex data distributions to simple latents. The continuous-time formulation of \cite{Chen2018} improves expressiveness but initially lacks a tractable training procedure. Flow matching \cite{Lipman2023} resolves this by replacing maximum likelihood with a quadratic regression objective for the underlying velocity field, enabling efficient training of continuous flows. Modern formulations of flow matching, particularly those by \cite{Albergo2023} and \cite{Tong2024}, generalize normalizing flows to define a measure transport between arbitrary pairs of distributions. Most similar to our approach, \cite{Albergo2023data} apply this framework to image super-resolution and in-painting. See also \cite{Brehmer2020} for a tangentially related perspective. We build on this modern interpretation of continuous normalizing flows to connect molecular configurations across resolutions.


Mapping entropy, first introduced by \cite{Foley2015} in the context of molecular modeling, provides an information-theoretic measure of the loss incurred when representing a system at reduced resolution. Its utility spans a wide range of applications. For example, \cite{Kidder2024} use mapping entropy to identify high-quality representations of the structural protein actin, while \cite{Giulini2020, Giulini2024} develop a coarse-graining software suite that operationalizes the concept. \cite{Armstrong2012} and \cite{Jin2023} establish connections between mapping entropy and the consistency of dynamical behavior across resolutions, while \cite{Mussi2025} disentangle entropic and energetic contributions to observables in coarse-grained simulations. Beyond molecular systems, \cite{Holtzman2022} demonstrate its broader relevance by applying it to spin models and low-dimensional descriptors of financial markets. While prior studies rely on application-specific methodologies, split-flows provide a rigorous and general framework for computing mapping entropies across diverse systems and reduction strategies.","Solving the inverse problem of backmapping is a
central challenge in multiscale molecular model-
ing [Peter and Kremer, 2009]. Mirroring trends
across many scientific domains, data-driven methods
increasingly replace traditional handcrafted algo-
rithms, such as those by [Rzepiela et al., 2010], and
[Wassenaar et al., 2014], which predict approximate
fine-grained configurations from coarse inputs, fol-
lowed by costly refinement. Early approaches, such
as [Stieffenhofer et al., 2020], [Li et al., 2020], and
[Wang et al., 2022], leverage generative adversarial
networks and variational autoencoders to generate
fine-grained samples, without the need for post hoc
refinement. [Shmilovich et al., 2022] extend this
line of work by incorporating information along
reconstructed simulations to ensure temporal consis-
tency. More recent methods by [Jones et al., 2023],
[Jones et al., 2025, Berlaga et al., 2025], and[Torre and Sugita, 2025] adopt multi-step sam-
plers, i.e., continuous normalizing flows and diffusion
models, enabling generalization to unseen structures
through residue-wise processing and transferable
coarse-graining schemes. While these models empha-
size energetic plausibility, transferability, or dynamical
consistency, they do not establish a probabilistic link
between resolutions and therefore miss key statistical
properties of the coarse-graining map. Our method
addresses this limitation.
Normalizing flows, introduced by
[Rezende and Mohamed, 2015] in discrete form,
map complex data distributions to simple latents.
The continuous-time formulation of [Chen et al., 2018]
improves expressiveness but initially lacks a
tractable training procedure. Flow matching
[Lipman et al., 2023] resolves this by replacing
maximum likelihood with a quadratic regression
objective for the underlying velocity field, enabling
efficient training of continuous flows. Modern for-
mulations of flow matching, particularly those by
[Albergo et al., 2023] and [Tong et al., 2024], gener-
alize normalizing flows to define a measure transport
between arbitrary pairs of distributions. Most similar
to our approach, [Albergo et al., 2024] apply this
framework to image super-resolution and in-painting.
See also [Brehmer and Cranmer, 2020] for a tangen-
tially related perspective. We build on this modern
interpretation of continuous normalizing flows to
connect molecular configurations across resolutions.
Mapping entropy, first introduced by
[Foley et al., 2015] in the context of molecular
modeling, provides an information-theoretic measure
of the loss incurred when representing a system at
reduced resolution. Its utility spans a wide range of
applications. For example, [Kidder and Noid, 2024]
use mapping entropy to identify high-quality rep-
resentations of the structural protein actin, while
[Giulini et al., 2020, Giulini et al., 2024] develop
a coarse-graining software suite that operational-
izes the concept. [Armstrong et al., 2012] and
[Jin et al., 2023] establish connections between map-
ping entropy and the consistency of dynamical behav-
ior across resolutions, while [Mussi and Noid, 2025]
disentangle entropic and energetic contributions
to observables in coarse-grained simulations. Be-
yond molecular systems, [Holtzman et al., 2022]
demonstrate its broader relevance by applying it
to spin models and low-dimensional descriptors
of financial markets. While prior studies rely on
application-specific methodologies, split-flows provide
a rigorous and general framework for computing map-
ping entropies across diverse systems and reduction
strategies.
Sander Hummerich, Tristan Bereau, Ullrich K¨ othe"
2511.01352v1,http://arxiv.org/abs/2511.01352v1,2025-11-03 08:57:54+00:00,MiniFool -- Physics-Constraint-Aware Minimizer-Based Adversarial Attacks in Deep Neural Networks,"In this paper, we present a new algorithm, MiniFool, that implements physics-inspired adversarial attacks for testing neural network-based classification tasks in particle and astroparticle physics. While we initially developed the algorithm for the search for astrophysical tau neutrinos with the IceCube Neutrino Observatory, we apply it to further data from other science domains, thus demonstrating its general applicability. Here, we apply the algorithm to the well-known MNIST data set and furthermore, to Open Data data from the CMS experiment at the Large Hadron Collider. The algorithm is based on minimizing a cost function that combines a $χ^2$ based test-statistic with the deviation from the desired target score. The test statistic quantifies the probability of the perturbations applied to the data based on the experimental uncertainties. For our studied use cases, we find that the likelihood of a flipped classification differs for both the initially correctly and incorrectly classified events. When testing changes of the classifications as a function of an attack parameter that scales the experimental uncertainties, the robustness of the network decision can be quantified. Furthermore, this allows testing the robustness of the classification of unlabeled experimental data.","%\textcolor{red}{can this be integrated into the introduction, section 1 ?}


Adversarial attacks have been studied for more than a decade and are evolving as a valuable tool in astroparticle and elementary particle physics (See e.g.~\cite{Flek:2025ecg,Stein:991721,Stein:2022nvf}). 
Several established adversarial attacks have been tested in those fields.
A very well-known method is the \emph{Fast Gradient Sign Method (FGSM)}~\cite{Goodfellow:2014rpb}.
The algorithm evaluates the signed gradient of the cost function with respect to the input data for constructing a minimally perturbed input.
Smaller perturbations can be achieved with the \emph{Projected Gradient
Descent (PGD)} ~\cite{Madry:2017tvh}
or the \emph{DeepFool} algorithm~\cite{Deepfoolpaper}. 
These can be understood as iterative extensions of the FGSM algorithm that more efficiently find a minimally perturbed solution to a changed classification.
Used norms for the cost function
vary between $L_\infty$, $L_2$, and $L_1$.

This approach makes these algorithms particularly well-suited for generating adversarial examples with changes that are imperceptible to human perception.
%With this approach these algorithms are ideally suited to generate adversarials with changes for human inference. 
However, what is common in the aforementioned algorithms is that these minimal perturbations of the input data are not constrained, and non-physical or negative pixel values are possible,
if this improves the solution.
Unless the algorithms are supplemented with counter-measures by additional constraints on the ranges of possible
changes of the input data (See e.g.~\cite{masterjanik}), the applicability to classification tasks in particle physics analyses is limited.

The approach of \minifool is different. Instead of minimizing the distance metric to the decision boundary, the cost function of perturbations includes a distance metric that quantifies the credibility of perturbations given by the uncertainties of the input data. By this, we minimize the perturbation with respect to maximum plausibility. A very similar approach has been found in the L-BFGS algorithm by Szegedy et al.~\cite{szegedy2013intriguing}
where they minimize a scaled distance-metric $ ||r||_2 $ of the unperturbed $\vec{x}$ and perturbed $\vec{x}+\vec{r}$  input plus a loss term $f $
%(\vec{x}+\vec{r} ,l) $ 
that depends on the perturbed data and the cross-entropy loss of the targeted classification label $l$
\begin{equation}
  \min_r \ [ s\cdot ||\vec{r}||_2 + f(\vec{x}+\vec{r} ,l) ] ~.
\end{equation}
This minimization is performed iteratively with different parameters $s>0$ to find a global optimum of minimum distance. This concept is adopted by Carlini et al.~\cite{carlini2017}, where different loss functions $f$ are used for evaluating the robustness of network classifications.
%the parameter $s$ applied to the loss and not the metrics.
In our approach, we deviate from a simple scale parameter $s$ but add physical uncertainties to the distance metrics. The scale of the attack $s$ is applied to these uncertainties. 
Furthermore, instead of evaluating the loss function of the network,
%, that may not be accessible at the time of attack,
we replace it with a statistically motivated $L_2$ distance metric of the perturbed output and a targeted label score.



% \begin{itemize}
% \item \cite{ballet2019imperceptible}
% Imperceptible Adversarial Attacks on Tabular Data
% https://doi.org/10.48550/arXiv.1911.03274
% LPF LowProFool
% \item 
% Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going deeper with convolutions. Technical report,
% arXiv preprint arXiv:1409.4842, 2014a
% \end{itemize}


%\clearpage","Adversarial attacks have been studied for more than a decade and are evolving as a valuable tool in astroparticle and elementary particle physics (See e.g.~\cite{Flek:2025ecg,Stein:991721,Stein:2022nvf}). 
Several established adversarial attacks have been tested in those fields.
A very well-known method is the \emph{Fast Gradient Sign Method (FGSM)}~\cite{Goodfellow:2014rpb}.
The algorithm evaluates the signed gradient of the cost function with respect to the input data for constructing a minimally perturbed input.
Smaller perturbations can be achieved with the \emph{Projected Gradient
Descent (PGD)} ~\cite{Madry:2017tvh}
or the \emph{DeepFool} algorithm~\cite{Deepfoolpaper}. 
These can be understood as iterative extensions of the FGSM algorithm that more efficiently find a minimally perturbed solution to a changed classification.
Used norms for the cost function
vary between $L_\infty$, $L_2$, and $L_1$.

This approach makes these algorithms particularly well-suited for generating adversarial examples with changes that are imperceptible to human perception.

However, what is common in the aforementioned algorithms is that these minimal perturbations of the input data are not constrained, and non-physical or negative pixel values are possible,
if this improves the solution.
Unless the algorithms are supplemented with counter-measures by additional constraints on the ranges of possible
changes of the input data (See e.g.~\cite{masterjanik}), the applicability to classification tasks in particle physics analyses is limited.

The approach of \minifool is different. Instead of minimizing the distance metric to the decision boundary, the cost function of perturbations includes a distance metric that quantifies the credibility of perturbations given by the uncertainties of the input data. By this, we minimize the perturbation with respect to maximum plausibility. A very similar approach has been found in the L-BFGS algorithm by Szegedy et al.~\cite{szegedy2013intriguing}
where they minimize a scaled distance-metric $ ||r||_2 $ of the unperturbed $\vec{x}$ and perturbed $\vec{x}+\vec{r}$  input plus a loss term $f $

that depends on the perturbed data and the cross-entropy loss of the targeted classification label $l$
\begin{equation}
  \min_r \ [ s\cdot ||\vec{r}||_2 + f(\vec{x}+\vec{r} ,l) ] ~.
\end{equation}
This minimization is performed iteratively with different parameters $s>0$ to find a global optimum of minimum distance. This concept is adopted by Carlini et al.~\cite{carlini2017}, where different loss functions $f$ are used for evaluating the robustness of network classifications.

In our approach, we deviate from a simple scale parameter $s$ but add physical uncertainties to the distance metrics. The scale of the attack $s$ is applied to these uncertainties. 
Furthermore, instead of evaluating the loss function of the network,

we replace it with a statistically motivated $L_2$ distance metric of the perturbed output and a targeted label score.","Adversarial attacks have been studied for more than a
decade and are evolving as a valuable tool in astropar-
ticle and elementary particle physics (See e.g. [17–
19]). Several established adversarial attacks have been
tested in those fields. A very well-known method is the
Fast Gradient Sign Method (FGSM)[12]. The algo-
rithm evaluates the signed gradient of the cost function
with respect to the input data for constructing a min-
imally perturbed input. Smaller perturbations can be
achieved with theProjected Gradient Descent (PGD)
[20] or theDeepFoolalgorithm [21]. These can be
understood as iterative extensions of the FGSM algo-
rithm that more efficiently find a minimally perturbed
solution to a changed classification. Used norms for the
cost function vary betweenL ∞,L2, andL 1.
This approach makes these algorithms particularly
well-suited for generating adversarial examples with
changes that are imperceptible to human perception.
However, what is common in the aforementioned algo-
rithms is that these minimal perturbations of the input
data are not constrained, and non-physical or negative
pixel values are possible, if this improves the solution.
Unless the algorithms are supplemented with counter-
measures by additional constraints on the ranges of
possible changes of the input data (See e.g. [22]), the
applicability to classification tasks in particle physics
analyses is limited.
The approach ofMiniFoolis different. Instead of
minimizing the distance metric to the decision bound-
ary, the cost function of perturbations includes a
distance metric that quantifies the credibility of per-
turbations given by the uncertainties of the input data.
By this, we minimize the perturbation with respect
to maximum plausibility. A very similar approach has
been found in the L-BFGS algorithm by Szegedy et
al. [23] where they minimize a scaled distance-metric
||r||2of the unperturbed⃗ xand perturbed⃗ x+⃗ rinput
plus a loss termfthat depends on the perturbed data
and the cross-entropy loss of the targeted classification
labell
min
r[s· ||⃗ r|| 2+f(⃗ x+⃗ r, l)].(1)
This minimization is performed iteratively with dif-
ferent parameterss >0 to find a global optimum of
minimum distance. This concept is adopted by Carlini
et al. [11], where different loss functionsfare used for
evaluating the robustness of network classifications. In
our approach, we deviate from a simple scale param-
etersbut add physical uncertainties to the distance
metrics. The scale of the attacksis applied to these
uncertainties. Furthermore, instead of evaluating the
loss function of the network, we replace it with a statis-
tically motivatedL 2distance metric of the perturbed
output and a targeted label score.
2"
2511.04608v2,http://arxiv.org/abs/2511.04608v2,2025-11-06 17:58:53+00:00,Qubit Mapping and Routing tailored to Advanced Quantum ISAs: Not as Costly as You Think,"Qubit mapping/routing is a critical stage in compilation for both near-term and fault-tolerant quantum computers, yet existing scalable methods typically impose several times the routing overhead in terms of circuit depth or duration. This inefficiency stems from a fundamental disconnect: compilers rely on an abstract routing model (e.g., three-CX-unrolled SWAP insertion) that completely ignores the idiosyncrasies of native gates supported by physical devices.
  Recent hardware breakthroughs have enabled high-precision implementations of diverse instruction set architectures (ISAs) beyond standard CX-based gates. Advanced ISAs involving gates such as SQiSW and ZZ(θ) gates offer superior circuit synthesis capabilities and can be realized with higher fidelities. However, systematic compiler optimization strategies tailored to these advanced ISAs are lacking.
  To address this, we propose Canopus, a unified qubit mapping/routing framework applicable to diverse quantum ISAs. Built upon the canonical representation of two-qubit gates, Canopus centers on qubit routing to perform deep co-optimization in an ISA-aware approach. Canopus leverages the two-qubit canonical representation and the monodromy polytope to model the synthesis cost for more intelligent SWAP insertion during the routing stage. We also formalize the commutation relations between two-qubit gates through the canonical form, providing a generalized approach to commutativity-based optimizations. Experiments show that Canopus consistently reduces routing overhead by 15%-35% compared to state-of-the-art methods across different ISAs and topologies. Our work also presents a coherent method for co-exploration of program patterns, quantum ISAs, and hardware topologies.","\label{sec:related}
Qubit mapping/routing is one of the most well-explored topics of quantum compiler research, as it shares similar methodologies with instruction scheduling~\cite{codina2001unified,hennessy1983postpass} and register allocation~\cite{chaitin1982register,poletto1999linear} in classical computing. 

To perform scalable qubit routing, \citet{zulehner2018efficient} introduces an A*-based algorithm to minimize $ \SWAP $ gate overhead for concurrent $\CX$ gate layers. The approach partitions the circuit into layers and solves the mapping problem subsequently. \citet{li2019tackling} also utilizes the circuit DAG layering thought and proposes a bidirectional routing procedure \sabre\ to find better initial mappings thus with lower $\SWAP$ insertion count.
% desired to result in \#$ \SWAP $ inserted minimization as expected.
It also briefly discusses the trade-off between the inserted $ \SWAP $ count and the circuit depth but does not prioritize optimizing circuit depth.
Subsequent works have aimed to improve circuit depth and parallelism, either by using \sabre-like heuristics~\cite{lao2021timing,ddroute2025,zou2024lightsabre} or graph matching techniques~\cite{childs2019circuit}. \citet{zhang2021time} systematically investigates the depth-optimality of qubit mapping and proposes an A*-based method \toqm\ that reported superior performance over existing solver-based depth-driven approaches~\cite{tan2020optimal}. However, holistic optimality of qubit routing is contingent on the specific ISA, device topology, and circuit cost model, and is rarely guaranteed by theoretical bounds. Indeed, our own evaluation reveals that \toqm\ does not always produce depth-optimal results compared to our heuristic, \canopus. For example, our case study in \Cref{sec:qft_study} demonstrates that the mapping scheme for the QFT kernel, purported to be optimal in their analysis, can be further improved.



With the recent development of advanced quantum ISAs such as superconducting fractional gates~\cite{ibmFractionalGates}, ion-trapped partial entangling gates~\cite{ionqPartialGates,yale2025realization}, and the AshN gates~\cite{chen2024one,chen2025efficient}, some works have begun exploring how to efficiently utilize these ISAs to make compiler optimizations closer to hardware characteristics. \citet{mckinney2024mirage} investigates the practical performance of \SQiSWISA\ ISA proposed by \citet{huang2023quantum} and the synthesis capability when incorporating the basis gates' mirrors into the ISA. Their modified \sabre\ algorithm offers a preliminary attempt at the collaborative gate decomposition and qubit routing approach, while the optimization opportunities considered therein are limited and the algorithmic techniques are not sophisticated. \bqskit~\cite{bqskit} and the series of works behind it~\cite{davis2019heuristics,wu2020qgo,kukliansky2023qfactor,younis2021qfast} provide a toolkit to rebase arbitrary 2Q unitaries to specific ISAs through approximate synthesis (structural search and numerical optimization) which is not computationally efficient. Approximate synthesis by \bqskit\ does not ensure optimal schemes for two-qubit and multi-qubit circuit synthesis. In addition, due to the lack of native compilation strategies and a rational synthesis cost model, \citet{kalloor2024quantum} claims that alternative ISAs are hardly comparable to \CXISA\ when evaluating quantum hardware roofline by \bqskit. As for applicability of expanded ISAs to QEC, Google's latest theoretical~\cite{mcewen2023relaxing} and experimental~\cite{eickbusch2024demonstrating} works demonstrate the \CXISA-\iSWAPISA\ combination ISA could help suppress the fault-tolerant threshold. \citet{zhou2024halma} proposes a routing-based method enhanced by \CXISA-\iSWAPISA\ for overcoming ancilla defects among surface code blocks while preserving encoded logical information, but it relies on manual design and experience.



% - expanded ISAs \& systematic utilization
%     - Noise-aware ...
%     - Mirage ... not sophisticated algorithm, ... a subset of our approach
%     - BQSKit ... quantum hardware roofline .. they claim that ... 
%     - The last-step \dquote{synthesizer} .... most based on \dquote{approximate synthesis} (roofline, heterogeneous, ZZ(theta))

% (((((((None of them make deep co-optimization tailored to various quantum ISAs in a systematic and efficient approach)))))))


% With respect to the heuristic cost for $ \SWAP $ search, our routing algorithm involves the duration (generalization metric of circuit depth) driven goal by taking the canonical gate synthesis cost into the circuit duration increment.","Qubit mapping/routing is one of the most well-explored topics of quantum compiler research, as it shares similar methodologies with instruction scheduling~\cite{codina2001unified,hennessy1983postpass} and register allocation~\cite{chaitin1982register,poletto1999linear} in classical computing. 

To perform scalable qubit routing, \citet{zulehner2018efficient} introduces an A*-based algorithm to minimize $ \SWAP $ gate overhead for concurrent $\CX$ gate layers. The approach partitions the circuit into layers and solves the mapping problem subsequently. \citet{li2019tackling} also utilizes the circuit DAG layering thought and proposes a bidirectional routing procedure \sabre\ to find better initial mappings thus with lower $\SWAP$ insertion count.

It also briefly discusses the trade-off between the inserted $ \SWAP $ count and the circuit depth but does not prioritize optimizing circuit depth.
Subsequent works have aimed to improve circuit depth and parallelism, either by using \sabre-like heuristics~\cite{lao2021timing,ddroute2025,zou2024lightsabre} or graph matching techniques~\cite{childs2019circuit}. \citet{zhang2021time} systematically investigates the depth-optimality of qubit mapping and proposes an A*-based method \toqm\ that reported superior performance over existing solver-based depth-driven approaches~\cite{tan2020optimal}. However, holistic optimality of qubit routing is contingent on the specific ISA, device topology, and circuit cost model, and is rarely guaranteed by theoretical bounds. Indeed, our own evaluation reveals that \toqm\ does not always produce depth-optimal results compared to our heuristic, \canopus. For example, our case study in \Cref{sec:qft_study} demonstrates that the mapping scheme for the QFT kernel, purported to be optimal in their analysis, can be further improved.



With the recent development of advanced quantum ISAs such as superconducting fractional gates~\cite{ibmFractionalGates}, ion-trapped partial entangling gates~\cite{ionqPartialGates,yale2025realization}, and the AshN gates~\cite{chen2024one,chen2025efficient}, some works have begun exploring how to efficiently utilize these ISAs to make compiler optimizations closer to hardware characteristics. \citet{mckinney2024mirage} investigates the practical performance of \SQiSWISA\ ISA proposed by \citet{huang2023quantum} and the synthesis capability when incorporating the basis gates' mirrors into the ISA. Their modified \sabre\ algorithm offers a preliminary attempt at the collaborative gate decomposition and qubit routing approach, while the optimization opportunities considered therein are limited and the algorithmic techniques are not sophisticated. \bqskit~\cite{bqskit} and the series of works behind it~\cite{davis2019heuristics,wu2020qgo,kukliansky2023qfactor,younis2021qfast} provide a toolkit to rebase arbitrary 2Q unitaries to specific ISAs through approximate synthesis (structural search and numerical optimization) which is not computationally efficient. Approximate synthesis by \bqskit\ does not ensure optimal schemes for two-qubit and multi-qubit circuit synthesis. In addition, due to the lack of native compilation strategies and a rational synthesis cost model, \citet{kalloor2024quantum} claims that alternative ISAs are hardly comparable to \CXISA\ when evaluating quantum hardware roofline by \bqskit. As for applicability of expanded ISAs to QEC, Google's latest theoretical~\cite{mcewen2023relaxing} and experimental~\cite{eickbusch2024demonstrating} works demonstrate the \CXISA-\iSWAPISA\ combination ISA could help suppress the fault-tolerant threshold. \citet{zhou2024halma} proposes a routing-based method enhanced by \CXISA-\iSWAPISA\ for overcoming ancilla defects among surface code blocks while preserving encoded logical information, but it relies on manual design and experience.","Qubit mapping/routing is one of the most well-explored
topics of quantum compiler research, as it shares similar
methodologies with instruction scheduling [ 14,24] and reg-
ister allocation [9, 52] in classical computing.
To perform scalable qubit routing, Zulehner et al . [73]
introduces an A*-based algorithm to minimize SWAP gate
overhead for concurrent CXgate layers. The approach parti-
tions the circuit into layers and solves the mapping problem
subsequently. Li et al . [38] also utilizes the circuit DAG layer-
ing thought and proposes a bidirectional routing procedure
Sabreto find better initial mappings thus with lower SWAP
insertion count. It also briefly discusses the trade-off between
the inserted SWAP count and the circuit depth but does not
prioritize optimizing circuit depth. Subsequent works have
aimed to improve circuit depth and parallelism, either by
usingSabre-like heuristics [ 3,35,72] or graph matching
techniques [ 13]. Zhang et al . [69] systematically investigates
the depth-optimality of qubit mapping and proposes an A*-
based methodTOQMthat reported superior performance
over existing solver-based depth-driven approaches [ 59].
However, holistic optimality of qubit routing is contingent
on the specific ISA, device topology, and circuit cost model,
and is rarely guaranteed by theoretical bounds. Indeed, our
own evaluation reveals thatTOQMdoes not always produce
depth-optimal results compared to our heuristic,Canopus.
For example, our case study in Section 5.1 demonstrates that
the mapping scheme for the QFT kernel, purported to be
optimal in their analysis, can be further improved.
With the recent development of advanced quantum ISAs
such as superconducting fractional gates [ 27], ion-trapped
partial entangling gates [ 29,65], and the AshN gates [ 11,12],
some works have begun exploring how to efficiently utilize
these ISAs to make compiler optimizations closer to hard-
ware characteristics. McKinney et al . [45] investigates the
11
Zhaohui Yang, Kai Zhang, Xinyang Tian, Xiangyu Ren, Yingjian Liu, Yunfeng Li, Dawei Ding, Jianxin Chen, and Yuan Xie
practical performance of SQiSW ISA proposed by Huang et al .
[26] and the synthesis capability when incorporating the
basis gates’ mirrors into the ISA. Their modifiedSabrealgo-
rithm offers a preliminary attempt at the collaborative gate
decomposition and qubit routing approach, while the opti-
mization opportunities considered therein are limited and the
algorithmic techniques are not sophisticated.BQSKit[ 67]
and the series of works behind it [ 17,34,64,68] provide
a toolkit to rebase arbitrary 2Q unitaries to specific ISAs
through approximate synthesis (structural search and nu-
merical optimization) which is not computationally efficient.
Approximate synthesis byBQSKitdoes not ensure optimal
schemes for two-qubit and multi-qubit circuit synthesis. In
addition, due to the lack of native compilation strategies and
a rational synthesis cost model, Kalloor et al . [31] claims that
alternative ISAs are hardly comparable toCXwhen evaluat-
ing quantum hardware roofline byBQSKit. As for applicabil-
ity of expanded ISAs to QEC, Google’s latest theoretical [ 43]
and experimental [ 19] works demonstrate the CX-iSWAP com-
bination ISA could help suppress the fault-tolerant threshold.
Zhou et al . [71] proposes a routing-based method enhanced
byCX-iSWAP for overcoming ancilla defects among surface
code blocks while preserving encoded logical information,
but it relies on manual design and experience."
2511.02092v1,http://arxiv.org/abs/2511.02092v1,2025-11-03 22:03:37+00:00,Uncertainty Guided Online Ensemble for Non-stationary Data Streams in Fusion Science,"Machine Learning (ML) is poised to play a pivotal role in the development and operation of next-generation fusion devices. Fusion data shows non-stationary behavior with distribution drifts, resulted by both experimental evolution and machine wear-and-tear. ML models assume stationary distribution and fail to maintain performance when encountered with such non-stationary data streams. Online learning techniques have been leveraged in other domains, however it has been largely unexplored for fusion applications. In this paper, we present an application of online learning to continuously adapt to drifting data stream for prediction of Toroidal Field (TF) coils deflection at the DIII-D fusion facility. The results demonstrate that online learning is critical to maintain ML model performance and reduces error by 80% compared to a static model. Moreover, traditional online learning can suffer from short-term performance degradation as ground truth is not available before making the predictions. As such, we propose an uncertainty guided online ensemble method to further improve the performance. The Deep Gaussian Process Approximation (DGPA) technique is leveraged for calibrated uncertainty estimation and the uncertainty values are then used to guide a meta-algorithm that produces predictions based on an ensemble of learners trained on different horizon of historical data. The DGPA also provides uncertainty estimation along with the predictions for decision makers. The online ensemble and the proposed uncertainty guided online ensemble reduces predictions error by about 6%, and 10% respectively over standard single model based online learning.","\label{sec:related_work}

Machine learning (ML) has become a central tool in fusion science, particularly in the prediction and mitigation of plasma disruptions. One of the landmark contributions is the work of Kates-Harbeck et al.~\citep{Kates-Harbeck2019}, who demonstrated a deep learning framework capable of predicting disruptive instabilities from multivariate diagnostic data. A key strength of their approach is cross-machine generalization: models trained on data from one tokamak (e.g., DIII-D) could be successfully applied to another (e.g., JET), highlighting the potential of ML for device-independent prediction.

Building on these advances, a recent work~\citep{https://doi.org/10.1002/ctpp.202200095} integrated a disruption predictor into a plasma control system. This work represents an important step toward real-time deployment by combining predictive scores with interpretability metrics, such as sensitivity indicators that provide insight into the causes of predicted disruptions. Such integration illustrates the feasibility of using ML models not just as offline tools, but as active components of plasma control.

Beyond deep learning architectures, statistical approaches have also contributed to disruption forecasting. Tinguely et al.~\citep{Tinguely_2019} applied survival analysis in combination with random forest models to quantify warning times and hazard functions. This probabilistic perspective provides a complementary way to evaluate the risk of disruption and offers more flexibility in characterizing temporal aspects of prediction.

Another line of research emphasizes the challenges of cross-machine and cross-regime generalization. A recent study~\citep{Zhu_2021} showed that models trained on low-performance operational regimes may perform poorly in high-performance scenarios, underscoring the importance of regime adaptation. Their findings suggest that carefully aligning operational parameters across devices is necessary to achieve reliable generalization for next-generation burning-plasma tokamaks.

Together, these studies demonstrate the growing role of ML in disruption prediction and control. Early work established feasibility and cross-device generalization, subsequent efforts have moved toward real-time integration and interpretability. However, most of these work have been conducted as short term studies, as such, significant challenges remain, particularly in uncertainty quantification, adaptation to new operational regimes, continuous data drifts, and ensuring robustness for safety-critical deployment.

\subsection{Recent work on Uncertainty Quantification for ML}
\label{subsec:uq4ml}
Uncertainty quantification (UQ) has emerged as a crucial aspect of Machine Learning (ML) research, with applications in areas such as reliability engineering, risk assessment, and decision-making under uncertainty~\citep{Nemani2023uqml}. 
Previous studies have shown that several methods can be used to quantify uncertainty in ML models. These include Bayesian Neural Networks (BNNs), which incorporate prior distributions over model weights to provide probabilistic predictions~\citep{blundell2015weight}. A simpler approach is through Monte Carlo dropout (MCD), which uses dropout to approximate Bayesian neural networks \citep{gal2016dropout}. 
Another approach is through deep ensemble methods, which combine the predictions of multiple models to provide a more accurate prediction and uncertainty estimation~\citep{lakshminarayanan2017simple}.
Deep ensembles have been used for uncertainty quantification in ML-based energy confinement time extrapolation~\citep{nam2025machine}.
Although effective, Ensembles and BNNs require multiple models or repeated inference runs, which can be resource-intensive and impractical in environments with limited memory or requiring real-time processing. 
DQR~\citep{koenker_2005} based uncertainty quantification estimates prediction intervals by directly modeling conditional quantiles, capturing the inherent uncertainty in the data without requiring multiple inference calls or models. 
In summary, although these methods perform well for in-distribution uncertainty estimation (sometimes further enhanced through post-training calibration), they are not designed to provide reliable estimates under out-of-distribution conditions.

Recently, the DGPA technique has been introduced to approximation Gaussian Process (GP) kernel via random fourier features to a fixed sized matrix. The DGPA combines the distance awareness benefit of GP and highly expressive nature of DNNs. 
Being an approximation method with a fixed sized kernel matrix, it does not suffer with the scaling issues as seen with traditional GP.
The DGPA has been leveraged for reliable uncertainty quantification for both classification and regression tasks \citep{rajput2020uncertainty, PhysRevAccelBeams.26.044602}. These studies show that the DGPA is more reliable in uncertainty estimation on both IID and OOD data.
As such, we select the DGPA method for this study to produce reliable uncertainty estimation.

% The following table summarizes the key features of each method:

% \begin{table}[ht]
%     \centering
%     \begin{tabular}{|p{2cm}|p{4cm}|p{5cm}|}
%     \hline
%     Method & Advantages & Disadvantages \\
%     \hline
%     BNNs & Provides probabilistic predictions & No guarantee of reliable OOD uncertainty estimation; Computationally expensive due to requirement of multiple inference calls
%     \\
%     \hline
%     Deep Ensembles & Combines multiple models to provide a more accurate prediction 
% & No guarantee of reliable OOD uncertainty; High compute and memory requirements \\
%     \hline
%     Deep Quantile Regression & Reliable in-distribution uncertainty estimation by design & No Guarantee of reliable OOD uncertainty estimation \\
%     \hline
%     GP Approximation & Reliable in-distribution and OOD uncertainty estimation; Single inference call & Need careful tuning and hyperparameter selection \\
%     \hline
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:my_label}
% \end{table}

\subsection{Recent work related to online learning on non-stationary data streams}
Recent work on online learning for non-stationary data streams focuses on continuously updating models to quickly adapt to evolving data distributions caused by concept drift. Techniques such as partial or full model parameter updates based on the most recent data are explored to maintain prediction accuracy under abrupt, gradual, or incremental drift without emphasizing knowledge retention from the past \citep{IJCAI2022}. Proactive model adaptation frameworks estimate drift ahead of time and adjust model parameters accordingly to reduce the lag in adaptation caused by delayed ground-truth availability \citep{Zhao2024}. Lightweight ensemble methods and sliding window approaches offer efficient mechanisms to detect and respond to drift patterns in real-time data streams, allowing models to stay up to date with minimal computational overhead \citep{YangShami2021}. However, online learning is yet to be explored on Fusion applications.","Machine learning (ML) has become a central tool in fusion science, particularly in the prediction and mitigation of plasma disruptions. One of the landmark contributions is the work of Kates-Harbeck et al.~\citep{Kates-Harbeck2019}, who demonstrated a deep learning framework capable of predicting disruptive instabilities from multivariate diagnostic data. A key strength of their approach is cross-machine generalization: models trained on data from one tokamak (e.g., DIII-D) could be successfully applied to another (e.g., JET), highlighting the potential of ML for device-independent prediction.

Building on these advances, a recent work~\citep{https://doi.org/10.1002/ctpp.202200095} integrated a disruption predictor into a plasma control system. This work represents an important step toward real-time deployment by combining predictive scores with interpretability metrics, such as sensitivity indicators that provide insight into the causes of predicted disruptions. Such integration illustrates the feasibility of using ML models not just as offline tools, but as active components of plasma control.

Beyond deep learning architectures, statistical approaches have also contributed to disruption forecasting. Tinguely et al.~\citep{Tinguely_2019} applied survival analysis in combination with random forest models to quantify warning times and hazard functions. This probabilistic perspective provides a complementary way to evaluate the risk of disruption and offers more flexibility in characterizing temporal aspects of prediction.

Another line of research emphasizes the challenges of cross-machine and cross-regime generalization. A recent study~\citep{Zhu_2021} showed that models trained on low-performance operational regimes may perform poorly in high-performance scenarios, underscoring the importance of regime adaptation. Their findings suggest that carefully aligning operational parameters across devices is necessary to achieve reliable generalization for next-generation burning-plasma tokamaks.

Together, these studies demonstrate the growing role of ML in disruption prediction and control. Early work established feasibility and cross-device generalization, subsequent efforts have moved toward real-time integration and interpretability. However, most of these work have been conducted as short term studies, as such, significant challenges remain, particularly in uncertainty quantification, adaptation to new operational regimes, continuous data drifts, and ensuring robustness for safety-critical deployment.

\subsection{Recent work on Uncertainty Quantification for ML}
Uncertainty quantification (UQ) has emerged as a crucial aspect of Machine Learning (ML) research, with applications in areas such as reliability engineering, risk assessment, and decision-making under uncertainty~\citep{Nemani2023uqml}. 
Previous studies have shown that several methods can be used to quantify uncertainty in ML models. These include Bayesian Neural Networks (BNNs), which incorporate prior distributions over model weights to provide probabilistic predictions~\citep{blundell2015weight}. A simpler approach is through Monte Carlo dropout (MCD), which uses dropout to approximate Bayesian neural networks \citep{gal2016dropout}. 
Another approach is through deep ensemble methods, which combine the predictions of multiple models to provide a more accurate prediction and uncertainty estimation~\citep{lakshminarayanan2017simple}.
Deep ensembles have been used for uncertainty quantification in ML-based energy confinement time extrapolation~\citep{nam2025machine}.
Although effective, Ensembles and BNNs require multiple models or repeated inference runs, which can be resource-intensive and impractical in environments with limited memory or requiring real-time processing. 
DQR~\citep{koenker_2005} based uncertainty quantification estimates prediction intervals by directly modeling conditional quantiles, capturing the inherent uncertainty in the data without requiring multiple inference calls or models. 
In summary, although these methods perform well for in-distribution uncertainty estimation (sometimes further enhanced through post-training calibration), they are not designed to provide reliable estimates under out-of-distribution conditions.

Recently, the DGPA technique has been introduced to approximation Gaussian Process (GP) kernel via random fourier features to a fixed sized matrix. The DGPA combines the distance awareness benefit of GP and highly expressive nature of DNNs. 
Being an approximation method with a fixed sized kernel matrix, it does not suffer with the scaling issues as seen with traditional GP.
The DGPA has been leveraged for reliable uncertainty quantification for both classification and regression tasks \citep{rajput2020uncertainty, PhysRevAccelBeams.26.044602}. These studies show that the DGPA is more reliable in uncertainty estimation on both IID and OOD data.
As such, we select the DGPA method for this study to produce reliable uncertainty estimation.
























\subsection{Recent work related to online learning on non-stationary data streams}
Recent work on online learning for non-stationary data streams focuses on continuously updating models to quickly adapt to evolving data distributions caused by concept drift. Techniques such as partial or full model parameter updates based on the most recent data are explored to maintain prediction accuracy under abrupt, gradual, or incremental drift without emphasizing knowledge retention from the past \citep{IJCAI2022}. Proactive model adaptation frameworks estimate drift ahead of time and adjust model parameters accordingly to reduce the lag in adaptation caused by delayed ground-truth availability \citep{Zhao2024}. Lightweight ensemble methods and sliding window approaches offer efficient mechanisms to detect and respond to drift patterns in real-time data streams, allowing models to stay up to date with minimal computational overhead \citep{YangShami2021}. However, online learning is yet to be explored on Fusion applications.","Machine learning (ML) has become a central tool in fusion science, par-
ticularly in the prediction and mitigation of plasma disruptions. One of the
landmark contributions is the work of Kates-Harbeck et al. (Kates-Harbeck
etal.,2019), whodemonstratedadeeplearningframeworkcapableofpredict-
ing disruptive instabilities from multivariate diagnostic data. A key strength
of their approach is cross-machine generalization: models trained on data
from one tokamak (e.g., DIII-D) could be successfully applied to another
(e.g., JET), highlighting the potential of ML for device-independent predic-
tion.
Building on these advances, a recent work (Tang et al., 2023) integrated
a disruption predictor into a plasma control system. This work represents an
important step toward real-time deployment by combining predictive scores
with interpretability metrics, such as sensitivity indicators that provide in-
sight into the causes of predicted disruptions. Such integration illustrates
the feasibility of using ML models not just as offline tools, but as active
components of plasma control.
4
Beyond deep learning architectures, statistical approaches have also con-
tributed to disruption forecasting. Tinguely et al. (Tinguely et al., 2019)
applied survival analysis in combination with random forest models to quan-
tify warning times and hazard functions. This probabilistic perspective pro-
vides a complementary way to evaluate the risk of disruption and offers more
flexibility in characterizing temporal aspects of prediction.
Another line of research emphasizes the challenges of cross-machine and
cross-regime generalization. A recent study (Zhu et al., 2021) showed that
models trained on low-performance operational regimes may perform poorly
in high-performance scenarios, underscoring the importance of regime adap-
tation. Their findings suggest that carefully aligning operational parame-
ters across devices is necessary to achieve reliable generalization for next-
generation burning-plasma tokamaks.
Together, these studies demonstrate the growing role of ML in disruption
prediction and control. Early work established feasibility and cross-device
generalization, subsequent efforts have moved toward real-time integration
and interpretability. However, most of these work have been conducted as
short term studies, as such, significant challenges remain, particularly in un-
certainty quantification, adaptation to new operational regimes, continuous
data drifts, and ensuring robustness for safety-critical deployment.
2"
2511.04564v1,http://arxiv.org/abs/2511.04564v1,2025-11-06 17:20:02+00:00,Uncertainties in Physics-informed Inverse Problems: The Hidden Risk in Scientific AI,"Physics-informed machine learning (PIML) integrates partial differential equations (PDEs) into machine learning models to solve inverse problems, such as estimating coefficient functions (e.g., the Hamiltonian function) that characterize physical systems. This framework enables data-driven understanding and prediction of complex physical phenomena. While coefficient functions in PIML are typically estimated on the basis of predictive performance, physics as a discipline does not rely solely on prediction accuracy to evaluate models. For example, Kepler's heliocentric model was favored owing to small discrepancies in planetary motion, despite its similar predictive accuracy to the geocentric model. This highlights the inherent uncertainties in data-driven model inference and the scientific importance of selecting physically meaningful solutions. In this paper, we propose a framework to quantify and analyze such uncertainties in the estimation of coefficient functions in PIML. We apply our framework to reduced model of magnetohydrodynamics and our framework shows that there are uncertainties, and unique identification is possible with geometric constraints. Finally, we confirm that we can estimate the reduced model uniquely by incorporating these constraints.","\label{sec_2}
\subsection{PIML and Inverse Problem}
PIML is an emerging framework that integrates physical laws, such as PDEs, into the training process of machine learning models such as DNNs. 
Prominent examples of instantiation of this idea are the Physics-Informed Neural Networks (PINNs)~\citep{raissi2019physics} and Hamiltonian neural networks (HNN)~\citep{greydanus2019hamiltonian}, where a neural network is trained not only to fit observational data but also to satisfy a given PDE.\par
In the PINNs framework, the governing PDE is typically of the form
\[
\mathcal{N}[u ,a](x) = 0, \quad x\in \mathbb{R}^d \times [0,T],
\]
where \( \mathcal{N} \) is a nonlinear differential operator derived from physical laws, \( u : \mathbb{R}^d \to \mathbb{R} \) is a sufficiently smooth function of observation values, \( a : \mathbb{R}^d \to \mathbb{R} \) is a sufficiently smooth function of coefficients of PDE, and $x := (x,t) \in \mathbb{R}^d \times [0,T]$. 
For example, the following equations are included: $\mathcal{N}[u, a] := u(x) - \nabla \cdot (a(x) \nabla u(x)) = 0$.
To enforce this PDE constraint in machine learning, the loss function used in training is augmented by a physics-informed term:
\begin{eqnarray}
\label{eq_pinns}
\mathcal{L}(\theta_u,\theta_a) = \mathcal{L}_{\text{data}}(\theta_u) + \lambda_{\text{PDE}} \mathcal{L}_{\text{PDE}}(\theta_u,\theta_a),
\end{eqnarray}
where
\begin{eqnarray}
\mathcal{L}_{\text{data}}(\theta_u) = \frac{1}{N} \sum_{i=1}^{N} \left\lVert u_\theta(x_i) - u_i \right\rVert^2,
\quad
\mathcal{L}_{\text{PDE}}(\theta_u,\theta_a) = \frac{1}{N_r} \sum_{r=1}^{N_r} \left\lVert \mathcal{N}[u_{\theta_u}, a_{\theta_a}](x_r) \right\rVert^2,
\end{eqnarray}
where \( u_{\theta_u}(x) \) and \( a_{\theta_a}(x) \) are a neural network models parametrized by $\theta$. 
Here, \( \{(x_i, u_i)\}_{i=1}^{N} \) are supervised data points, wherease \( \{(x_r)\}_{r=1}^{N_r} \) are residual points where the PDE is enforced. 
The parameter \( \lambda_{\text{PDE}} \) balances the relative importance of data fidelity and physics conformity. 
In this framework, the partial differential coefficient function $a(x,t)$ can also be estimated by minimizing $\mathcal{L}(\theta_u,\theta_a)$.  
In the HNN-type framework, the objective is not to estimate the observation function $u(x)$, but to estimate the partial differential coefficient function $a(x)$. 
Thus, the loss function is given as follows. 
\begin{eqnarray}
\mathcal{L}_{\text{HNN}}(\theta) = \frac{1}{N} \sum_{i=1}^{N} \left\lVert \mathcal{N}[u, a_{\theta}](x_i) \right\rVert^2,
\end{eqnarray}
where \( \{(x_i, u_i)\}_{i=1}^{N} \) are given as supervised data. From there, the partial derivatives of $u(x)$ in PDE, $\mathcal{N}[u, a_{\theta}](x_i)$, are assumed to be given numerically. 
For example, if the PDE is a canonical equation of motion, the loss function is given by
\begin{equation}
\label{eq:hnn-loss}
\mathcal{L}_{\text{HNN}}(\theta) = \frac{1}{N} \sum_{i=1}^{N} \left\|
\begin{bmatrix}
\frac{\partial H_\theta}{\partial p}(q_i, p_i) \\
- \frac{\partial H_\theta}{\partial q}(q_i, p_i)
\end{bmatrix}
-
\begin{bmatrix}
\dot{q}_i^{\text{obs}} \\
\dot{p}_i^{\text{obs}}
\end{bmatrix}
\right\|^2,
\end{equation}
where the observation function is $u(t,q,p) = (t,q,p)$ and the coefficient function is $a(t,q,p) = H(q,p) $. 
In HNNs, the coefficient function $a(x)$ is estimated by minimizing the loss function $\mathcal{L}_{\text{HNN}}(\theta)$ similar to that of PINNs.\par
PIML has been successfully demonstrated in various tasks, including forward simulation, spatiotemporal forecasting \citep{karniadakis2021physics}, and inverse problems such as parameter estimation \citep{raissi2018hidden}. 
For inverse problems, the physical constraint often compensates for limited data, enabling the estimation of unknown coefficient functions or source terms. 
However, the learned solution may not be unique: the PDE residual can be small even when multiple, distinct functions explain the data equally well using the same physical model.\par
Recent studies have highlighted the lack of identifiability guarantees in PINNs \citep{yang2021b}. 
In particular, the minimization of \( \mathcal{L}_{\text{PDE}} \) does not necessarily imply that the estimated parameters or functions are physically meaningful or unique. 
Furthermore, the structure of the differential operator \( \mathcal{N} \), the available observation data, and the expressivity of the neural network all affect the identifiability and uncertainty of the learned solution. These findings emphasize the need for a rigorous theoretical framework for understanding the ill-posedness and uncertainty inherent in physics-informed inverse problems.

\subsection{Uncertainty in Inverse Problems}

There are only a few studies that mathematically analyze the degree of uncertainty and its structure in the inductive estimation of coefficient functions of PDEs, and these studies are limited to specific classes of PDEs. 
These studies are introduced as follows. 
Classical studies such as Calderón's problem~\citep{calderon1980inverse} and its resolution in the elliptic case~\citep{sylvester1987global} established the uncertainty evaluation of the coefficient function under full-boundary measurement assumptions. 
For hyperbolic equations, Carleman estimates have proven instrumental in deriving conditional uniqueness and stability results under geometric constraints~\citep{yamamoto2009carleman, bellassoued2017carleman}. 
Thus, in the theoretical approach, analysis is limited to a specific class of PDEs.\par
In the context of machine learning approaches, although there have been studies examining the presence or absence of uncertainties in coefficient functions or the qualitative degree of an uncertainty, there is no framework for quantitatively evaluating the specific degree of the uncertainty or its structure. These studies are introduced as follows. 
Krishnapriyan et al.~\citep{krishnapriyan2021characterizing} demonstrated that PINNs may converge to physically incorrect solutions even when loss values are small, owing to flat or multimodal optimization landscapes. 
Mishra et al.~\citep{mishra2022estimates} further analyzed such failure modes by uncertainty quantification, highlighting the epistemic uncertainty inherent in inverse modeling without proper constraints.\par
Bayesian extensions, such as B-PINNs~\citep{yang2021b, mishra2022estimates}, provide a qualitative uncertainty evaluation by placing distributions over unknowns and inferring posteriors via variational or sampling-based methods. 
Although it might be possible to use information of posterior distributions (e.g., their unimodality or variance) from Bayesian PINNs to indirectly evaluate uncertainties and consider candidate constraints, this would still require threshold criteria (e.g., a threshold of posterior variance to decide it as an identifiable distribution) to decide whether a parameter is determined or not. 
Such thresholds are not defined in the existing Bayesian PINN literature.
Thus, there has been no research in which the degree of uncertainty and its structure have been quantitatively evaluated in the inductive estimation of coefficient functions for a wide range of classes of PDEs.","\subsection{PIML and Inverse Problem}
PIML is an emerging framework that integrates physical laws, such as PDEs, into the training process of machine learning models such as DNNs. 
Prominent examples of instantiation of this idea are the Physics-Informed Neural Networks (PINNs)~\citep{raissi2019physics} and Hamiltonian neural networks (HNN)~\citep{greydanus2019hamiltonian}, where a neural network is trained not only to fit observational data but also to satisfy a given PDE.\par
In the PINNs framework, the governing PDE is typically of the form
\[
\mathcal{N}[u ,a](x) = 0, \quad x\in \mathbb{R}^d \times [0,T],
\]
where \( \mathcal{N} \) is a nonlinear differential operator derived from physical laws, \( u : \mathbb{R}^d \to \mathbb{R} \) is a sufficiently smooth function of observation values, \( a : \mathbb{R}^d \to \mathbb{R} \) is a sufficiently smooth function of coefficients of PDE, and $x := (x,t) \in \mathbb{R}^d \times [0,T]$. 
For example, the following equations are included: $\mathcal{N}[u, a] := u(x) - \nabla \cdot (a(x) \nabla u(x)) = 0$.
To enforce this PDE constraint in machine learning, the loss function used in training is augmented by a physics-informed term:
\begin{eqnarray}
\mathcal{L}(\theta_u,\theta_a) = \mathcal{L}_{\text{data}}(\theta_u) + \lambda_{\text{PDE}} \mathcal{L}_{\text{PDE}}(\theta_u,\theta_a),
\end{eqnarray}
where
\begin{eqnarray}
\mathcal{L}_{\text{data}}(\theta_u) = \frac{1}{N} \sum_{i=1}^{N} \left\lVert u_\theta(x_i) - u_i \right\rVert^2,
\quad
\mathcal{L}_{\text{PDE}}(\theta_u,\theta_a) = \frac{1}{N_r} \sum_{r=1}^{N_r} \left\lVert \mathcal{N}[u_{\theta_u}, a_{\theta_a}](x_r) \right\rVert^2,
\end{eqnarray}
where \( u_{\theta_u}(x) \) and \( a_{\theta_a}(x) \) are a neural network models parametrized by $\theta$. 
Here, \( \{(x_i, u_i)\}_{i=1}^{N} \) are supervised data points, wherease \( \{(x_r)\}_{r=1}^{N_r} \) are residual points where the PDE is enforced. 
The parameter \( \lambda_{\text{PDE}} \) balances the relative importance of data fidelity and physics conformity. 
In this framework, the partial differential coefficient function $a(x,t)$ can also be estimated by minimizing $\mathcal{L}(\theta_u,\theta_a)$.  
In the HNN-type framework, the objective is not to estimate the observation function $u(x)$, but to estimate the partial differential coefficient function $a(x)$. 
Thus, the loss function is given as follows. 
\begin{eqnarray}
\mathcal{L}_{\text{HNN}}(\theta) = \frac{1}{N} \sum_{i=1}^{N} \left\lVert \mathcal{N}[u, a_{\theta}](x_i) \right\rVert^2,
\end{eqnarray}
where \( \{(x_i, u_i)\}_{i=1}^{N} \) are given as supervised data. From there, the partial derivatives of $u(x)$ in PDE, $\mathcal{N}[u, a_{\theta}](x_i)$, are assumed to be given numerically. 
For example, if the PDE is a canonical equation of motion, the loss function is given by
\begin{equation}
\mathcal{L}_{\text{HNN}}(\theta) = \frac{1}{N} \sum_{i=1}^{N} \left\|
\begin{bmatrix}
\frac{\partial H_\theta}{\partial p}(q_i, p_i) \\
- \frac{\partial H_\theta}{\partial q}(q_i, p_i)
\end{bmatrix}
-
\begin{bmatrix}
\dot{q}_i^{\text{obs}} \\
\dot{p}_i^{\text{obs}}
\end{bmatrix}
\right\|^2,
\end{equation}
where the observation function is $u(t,q,p) = (t,q,p)$ and the coefficient function is $a(t,q,p) = H(q,p) $. 
In HNNs, the coefficient function $a(x)$ is estimated by minimizing the loss function $\mathcal{L}_{\text{HNN}}(\theta)$ similar to that of PINNs.\par
PIML has been successfully demonstrated in various tasks, including forward simulation, spatiotemporal forecasting \citep{karniadakis2021physics}, and inverse problems such as parameter estimation \citep{raissi2018hidden}. 
For inverse problems, the physical constraint often compensates for limited data, enabling the estimation of unknown coefficient functions or source terms. 
However, the learned solution may not be unique: the PDE residual can be small even when multiple, distinct functions explain the data equally well using the same physical model.\par
Recent studies have highlighted the lack of identifiability guarantees in PINNs \citep{yang2021b}. 
In particular, the minimization of \( \mathcal{L}_{\text{PDE}} \) does not necessarily imply that the estimated parameters or functions are physically meaningful or unique. 
Furthermore, the structure of the differential operator \( \mathcal{N} \), the available observation data, and the expressivity of the neural network all affect the identifiability and uncertainty of the learned solution. These findings emphasize the need for a rigorous theoretical framework for understanding the ill-posedness and uncertainty inherent in physics-informed inverse problems.

\subsection{Uncertainty in Inverse Problems}

There are only a few studies that mathematically analyze the degree of uncertainty and its structure in the inductive estimation of coefficient functions of PDEs, and these studies are limited to specific classes of PDEs. 
These studies are introduced as follows. 
Classical studies such as Calderón's problem~\citep{calderon1980inverse} and its resolution in the elliptic case~\citep{sylvester1987global} established the uncertainty evaluation of the coefficient function under full-boundary measurement assumptions. 
For hyperbolic equations, Carleman estimates have proven instrumental in deriving conditional uniqueness and stability results under geometric constraints~\citep{yamamoto2009carleman, bellassoued2017carleman}. 
Thus, in the theoretical approach, analysis is limited to a specific class of PDEs.\par
In the context of machine learning approaches, although there have been studies examining the presence or absence of uncertainties in coefficient functions or the qualitative degree of an uncertainty, there is no framework for quantitatively evaluating the specific degree of the uncertainty or its structure. These studies are introduced as follows. 
Krishnapriyan et al.~\citep{krishnapriyan2021characterizing} demonstrated that PINNs may converge to physically incorrect solutions even when loss values are small, owing to flat or multimodal optimization landscapes. 
Mishra et al.~\citep{mishra2022estimates} further analyzed such failure modes by uncertainty quantification, highlighting the epistemic uncertainty inherent in inverse modeling without proper constraints.\par
Bayesian extensions, such as B-PINNs~\citep{yang2021b, mishra2022estimates}, provide a qualitative uncertainty evaluation by placing distributions over unknowns and inferring posteriors via variational or sampling-based methods. 
Although it might be possible to use information of posterior distributions (e.g., their unimodality or variance) from Bayesian PINNs to indirectly evaluate uncertainties and consider candidate constraints, this would still require threshold criteria (e.g., a threshold of posterior variance to decide it as an identifiable distribution) to decide whether a parameter is determined or not. 
Such thresholds are not defined in the existing Bayesian PINN literature.
Thus, there has been no research in which the degree of uncertainty and its structure have been quantitatively evaluated in the inductive estimation of coefficient functions for a wide range of classes of PDEs.","2.1 PIML and Inverse Problem
PIML is an emerging framework that integrates physical laws, such as PDEs, into the training process of
machine learning models such as DNNs. Prominent examples of instantiation of this idea are the Physics-
Informed Neural Networks (PINNs) (Raissi et al., 2019) and Hamiltonian neural networks (HNN) (Greydanus
et al., 2019b), where a neural network is trained not only to fit observational data but also to satisfy a given
PDE.
In the PINNs framework, the governing PDE is typically of the form
N[u,a](x) = 0, x∈Rd×[0,T],
3
whereNis a nonlinear differential operator derived from physical laws, u:Rd→Ris a sufficiently smooth
function of observation values, a:Rd→Ris a sufficiently smooth function of coefficients of PDE, and x:=
(x,t)∈Rd×[0,T]. For example, the following equations are included: N[u,a] :=u(x)−∇· (a(x)∇u(x)) = 0.
To enforce this PDE constraint in machine learning, the loss function used in training is augmented by a
physics-informed term:
L(θu,θa) =L data(θu) +λ PDELPDE(θu,θa),(1)
where
Ldata(θu) =1
NN/summationdisplay
i=1∥uθ(xi)−ui∥2,L PDE(θu,θa) =1
NrNr/summationdisplay
r=1∥N[uθu,aθa](xr)∥2,(2)
whereuθu(x)andaθa(x)are a neural network models parametrized by θ. Here,{(xi,ui)}N
i=1are supervised
data points, wherease {(xr)}Nr
r=1are residual points where the PDE is enforced. The parameter λPDEbalances
the relative importance of data fidelity and physics conformity. In this framework, the partial differential
coefficient function a(x,t)can also be estimated by minimizing L(θu,θa). In the HNN-type framework, the
objective is not to estimate the observation function u(x), but to estimate the partial differential coefficient
functiona(x). Thus, the loss function is given as follows.
LHNN(θ) =1
NN/summationdisplay
i=1∥N[u,aθ](xi)∥2,(3)
where{(xi,ui)}N
i=1are given as supervised data. From there, the partial derivatives of u(x)in PDE,
N[u,aθ](xi), are assumed to be given numerically. For example, if the PDE is a canonical equation of motion,
the loss function is given by
LHNN(θ) =1
NN/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/bracketleftigg
∂Hθ
∂p(qi,pi)
−∂Hθ
∂q(qi,pi)/bracketrightigg
−/bracketleftbigg˙qobs
i
˙pobs
i/bracketrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
,(4)
where the observation function is u(t,q,p ) = (t,q,p )and the coefficient function is a(t,q,p ) =H(q,p). In
HNNs, the coefficient function a(x)is estimated by minimizing the loss function LHNN(θ)similar to that of
PINNs.
PIML has been successfully demonstrated in various tasks, including forward simulation, spatiotemporal
forecasting (Karniadakis et al., 2021), and inverse problems such as parameter estimation (Raissi et al., 2018).
For inverse problems, the physical constraint often compensates for limited data, enabling the estimation of
unknown coefficient functions or source terms. However, the learned solution may not be unique: the PDE
residual can be small even when multiple, distinct functions explain the data equally well using the same
physical model.
Recent studies have highlighted the lack of identifiability guarantees in PINNs (Yang et al., 2021). In
particular, the minimization of LPDEdoes not necessarily imply that the estimated parameters or functions
are physically meaningful or unique. Furthermore, the structure of the differential operator N, the available
observation data, and the expressivity of the neural network all affect the identifiability and uncertainty of the
learned solution. These findings emphasize the need for a rigorous theoretical framework for understanding
the ill-posedness and uncertainty inherent in physics-informed inverse problems.
2.2 Uncertainty in Inverse Problems
There are only a few studies that mathematically analyze the degree of uncertainty and its structure in
the inductive estimation of coefficient functions of PDEs, and these studies are limited to specific classes of
PDEs. These studies are introduced as follows. Classical studies such as Calderón’s problem (Calderón, 1980)
and its resolution in the elliptic case (Sylvester & Uhlmann, 1987) established the uncertainty evaluation of
the coefficient function under full-boundary measurement assumptions. For hyperbolic equations, Carleman
4
estimates have proven instrumental in deriving conditional uniqueness and stability results under geometric
constraints (Yamamoto, 2009; Bellassoued & Yamamoto, 2017). Thus, in the theoretical approach, analysis is
limited to a specific class of PDEs.
In the context of machine learning approaches, although there have been studies examining the presence
or absence of uncertainties in coefficient functions or the qualitative degree of an uncertainty, there is no
framework for quantitatively evaluating the specific degree of the uncertainty or its structure. These studies
are introduced as follows. Krishnapriyan et al. (Krishnapriyan et al., 2021) demonstrated that PINNs
may converge to physically incorrect solutions even when loss values are small, owing to flat or multimodal
optimization landscapes. Mishra et al. (Mishra et al., 2022) further analyzed such failure modes by uncertainty
quantification, highlighting the epistemic uncertainty inherent in inverse modeling without proper constraints.
Bayesian extensions, such as B-PINNs (Yang et al., 2021; Mishra et al., 2022), provide a qualitative uncertainty
evaluation by placing distributions over unknowns and inferring posteriors via variational or sampling-based
methods. Although it might be possible to use information of posterior distributions (e.g., their unimodality
or variance) from Bayesian PINNs to indirectly evaluate uncertainties and consider candidate constraints,
this would still require threshold criteria (e.g., a threshold of posterior variance to decide it as an identifiable
distribution) to decide whether a parameter is determined or not. Such thresholds are not defined in the
existing Bayesian PINN literature. Thus, there has been no research in which the degree of uncertainty and
its structure have been quantitatively evaluated in the inductive estimation of coefficient functions for a wide
range of classes of PDEs."
2510.26792v1,http://arxiv.org/abs/2510.26792v1,2025-10-30 17:59:09+00:00,"Learning Pseudorandom Numbers with Transformers: Permuted Congruential Generators, Curricula, and Interpretability","We study the ability of Transformer models to learn sequences generated by Permuted Congruential Generators (PCGs), a widely used family of pseudo-random number generators (PRNGs). PCGs introduce substantial additional difficulty over linear congruential generators (LCGs) by applying a series of bit-wise shifts, XORs, rotations and truncations to the hidden state. We show that Transformers can nevertheless successfully perform in-context prediction on unseen sequences from diverse PCG variants, in tasks that are beyond published classical attacks. In our experiments we scale moduli up to $2^{22}$ using up to $50$ million model parameters and datasets with up to $5$ billion tokens. Surprisingly, we find even when the output is truncated to a single bit, it can be reliably predicted by the model. When multiple distinct PRNGs are presented together during training, the model can jointly learn them, identifying structures from different permutations. We demonstrate a scaling law with modulus $m$: the number of in-context sequence elements required for near-perfect prediction grows as $\sqrt{m}$. For larger moduli, optimization enters extended stagnation phases; in our experiments, learning moduli $m \geq 2^{20}$ requires incorporating training data from smaller moduli, demonstrating a critical necessity for curriculum learning. Finally, we analyze embedding layers and uncover a novel clustering phenomenon: the model spontaneously groups the integer inputs into bitwise rotationally-invariant clusters, revealing how representations can transfer from smaller to larger moduli.","\label{sec:related_work}
\vspace*{-\baselineskip}
\vspace{0.5\baselineskip}
\textbf{Cracking PCGs:} Classical approaches to cracking PRNGs rely on exploiting algebraic structure with strong assumptions about the generator. \citet{Bouillaguet_Martinez_Sauvage_2020} present an attack on XSLRR-128/64 that assumes knowledge of the multiplier, modulus, and permutation. The internal state can be recovered from 64 outputs via a guess-and-procedure. An asymptotic scaling law with modulus $m$ is not provided in those attacks, although the wall-clock time is significant ($20,000$ CPU hours in the worst case). In our work, the models must discover the hidden structure from training data alone, learning to predict outputs without explicit knowledge of the recurrence or transformation rules.

\textbf{Curriculum Learning:}
Many studies have explored curriculum learning~\citep{10.1145/1553374.1553380}.
\citet{wu2021curriculawork} finds that on standard benchmarks, explicit curricula offer little advantage when training steps are sufficient, but can yield higher accuracy and stability under limited compute or noisy data. \citet{garg2023transformerslearnincontextcase} observes that curriculum training can speed up training drastically when training Transformers to in-context learn linear functions. Recently \citet{saxena2024making} observed that curriculum learning strategies can be helpful in modular addition.

\textbf{AI for Cryptography}: There is a classic duality between machine learning and cryptography \citep{rivest1991cryptography}. Recently there has been increased interest in using modern AI systems to attack cryptographic schemes, such as the learning with errors problem \citep{wenger2022salsa}. Our work builds on \citet{tao2025howtransformerspredictpseudorandom}, which uses transformers to learn vanilla LCGs.

\textbf{Interpretability and Modular arithmetic:} 
A growing body of work examines how Transformers learn modular arithmetic tasks, uncovering phenomena such as grokking and structured internal representations~\citep{power2022grokkinggeneralizationoverfittingsmall,gromov2023grokkingmodulararithmetic,zhong2023clockpizzastoriesmechanistic,nanda2023progressmeasuresgrokkingmechanistic,doshi2024grokgrokdisentanglinggeneralization,charton2024emergentpropertiesrepeatedexamples}. Prior studies~\citep{liu2022understandinggrokkingeffectivetheory,LearnToGrok} also find emergent structures in embedding matrices and interpretable attention patterns.  
Our work extends this literature by studying modular arithmetic tasks involving \emph{permutation structures} and identifying a novel pattern in embedding space.","\vspace*{-\baselineskip}
\vspace{0.5\baselineskip}
\textbf{Cracking PCGs:} Classical approaches to cracking PRNGs rely on exploiting algebraic structure with strong assumptions about the generator. \citet{Bouillaguet_Martinez_Sauvage_2020} present an attack on XSLRR-128/64 that assumes knowledge of the multiplier, modulus, and permutation. The internal state can be recovered from 64 outputs via a guess-and-procedure. An asymptotic scaling law with modulus $m$ is not provided in those attacks, although the wall-clock time is significant ($20,000$ CPU hours in the worst case). In our work, the models must discover the hidden structure from training data alone, learning to predict outputs without explicit knowledge of the recurrence or transformation rules.

\textbf{Curriculum Learning:}
Many studies have explored curriculum learning~\citep{10.1145/1553374.1553380}.
\citet{wu2021curriculawork} finds that on standard benchmarks, explicit curricula offer little advantage when training steps are sufficient, but can yield higher accuracy and stability under limited compute or noisy data. \citet{garg2023transformerslearnincontextcase} observes that curriculum training can speed up training drastically when training Transformers to in-context learn linear functions. Recently \citet{saxena2024making} observed that curriculum learning strategies can be helpful in modular addition.

\textbf{AI for Cryptography}: There is a classic duality between machine learning and cryptography \citep{rivest1991cryptography}. Recently there has been increased interest in using modern AI systems to attack cryptographic schemes, such as the learning with errors problem \citep{wenger2022salsa}. Our work builds on \citet{tao2025howtransformerspredictpseudorandom}, which uses transformers to learn vanilla LCGs.

\textbf{Interpretability and Modular arithmetic:} 
A growing body of work examines how Transformers learn modular arithmetic tasks, uncovering phenomena such as grokking and structured internal representations~\citep{power2022grokkinggeneralizationoverfittingsmall,gromov2023grokkingmodulararithmetic,zhong2023clockpizzastoriesmechanistic,nanda2023progressmeasuresgrokkingmechanistic,doshi2024grokgrokdisentanglinggeneralization,charton2024emergentpropertiesrepeatedexamples}. Prior studies~\citep{liu2022understandinggrokkingeffectivetheory,LearnToGrok} also find emergent structures in embedding matrices and interpretable attention patterns.  
Our work extends this literature by studying modular arithmetic tasks involving \emph{permutation structures} and identifying a novel pattern in embedding space.",
2511.02051v1,http://arxiv.org/abs/2511.02051v1,2025-11-03 20:35:47+00:00,Towards Continuous-variable Quantum Neural Networks for Biomedical Imaging,"Continuous-variable (CV) quantum computing offers a promising framework for scalable quantum machine learning, leveraging optical systems with infinite-dimensional Hilbert spaces. While discrete-variable (DV) quantum neural networks have shown remarkable progress in various computer vision tasks, CV quantum models remain comparatively underexplored. In this work, we present a feasibility study of continuous-variable quantum neural networks (CV-QCNNs) applied to biomedical image classification. Utilizing photonic circuit simulation frameworks, we construct CV quantum circuits composed of Gaussian gates, such as displacement, squeezing, rotation, and beamsplitters to emulate convolutional behavior. Our experiments are conducted on the MedMNIST dataset collection, a set of annotated medical image benchmarks for multiple diagnostic tasks. We evaluate CV-QCNN's performance in terms of classification accuracy, model expressiveness, and resilience to Gaussian noise, comparing against classical CNNs and equivalent DV quantum circuits. This study aims to identify trade-offs between DV and CV paradigms for quantum-enhanced medical imaging. Our results highlight the potential of continuous-variable models and their viability for future computer-aided diagnosis systems.","\label{sec:relatedWork}
The field of Computer-Aided Diagnosis (CAD) has advanced through diverse methodologies, ranging from enhanced data processing, automated data workflows, artificial intelligence and more recently, quantum computing approaches. This review focuses on some of the potential improvements quantum computing presents in healthcare, as well as developments in the quantum machine learning field from the discrete-variable and continuous-variable paradigms.

In \cite{Schuld2018, Schuld2019}, the potential of the implementation of quantum algorithms and quantum data encoding for ML tasks was discussed and demonstrated. Input data can be encoded into quantum states and represented in high-dimensional Hilbert spaces, where data is implicitly defined as kernel functions between data points and can be exploited by kernel-based methods, such as the Support Vector Machine. Following this and further development of the QML field in ML and DL tasks, QML showed promising results in healthcare, as seen in \cite{Sengupta2021}, where clinical prognostic analysis for image classification and segmentation of COVID-19 is accelerated, outperforming conventional deep learning methods by 2.92\%. Similarly, it has been able to predict heart disease through an ensemble model that works as a quantum support vector machine for classification, as demonstrated in \cite{Abdulsalam2022}, where it attained an accuracy of 90.16\% competing with state-of-the-art models. Further research has been conducted on other areas that QML can aid, such as drug response as shown in \cite{Sagingalieva2023}, where a hybrid quantum neural network based on a graph and deep convolutional neural layers is proposed, outperforming classical analogs by 15\% in drug effectiveness prediction. 

In recent years, QML models have shown potential (82.86\% accuracy) in more complex computer vision tasks in the CAD field such as skin lesion classification in dermoscopic images as seen in \cite{Reka2024}, leveraging rotational gates for encoding, as well as classical backbones for feature extraction and preparation for a quantum support vector classifier. Simlarly, favorable results on multiclass classification has been demonstrated, as demonstrated in \cite{Ara2025}, where an 80.96\% diagnosis accuracy was achieved for diabetic retinopathy through a hybrid quantum-classical framework based on ResNet50 and an 8-qubit quantum classifier. Moreover, innovative approaches such as quantum-enhanced dual-backbone architecture as shown in \cite{Marzoug2025} have been developed, achieving a parameter complexity reduction of 29.04\% and 94.44\% of trainable parameters, while still attaining high accuracy (95.80\% and 95.42\% on training and validation sets). 

In contrast, CV QML has not had an equal surge, mainly due to photonic quantum computers still being in development. However, Hilbert space data representation can be extended in CV quantum computing, as data embedded into infinite-dimensional Hilbert spaces via Gaussian gates presents potential to more refined data features and details than classical methods. Because of this, early CV QML research can be traced back to \cite{Lau2017}, where a set of QML subroutines are generalized for infinite-dimensional systems intended for an all-photonic CV quantum computer. Furthermore, general methods for building CV neural networks for CV quantum computers is introduced in \cite{Killoran2018}, where information encoding and nonlinear activation functions are enacted through Gaussian and non Gaussian gates. 

Moreover, machine learning and optimization techniques for quantum photonic circuits were shown in \cite{Moody2019}. Here, a network comprised of several layers of optical gates with variable parameters are optimized applying automatic differentiation, showing the power and versatility of learning how to effectively use short-depth circuits to synthesize single-photons. Additional CV kernel studies are conducted in \cite{Li2022}, where the expressiveness of large Hilbert spaces is explored, introducing quantum kernel encoding methods into CV quantum states through amplitude squeezing and phase manipulation. Realistic implementations of neural networks on photonic quantum computers is proposed in \cite{Ghasemian2023}, where quantum circuits built in CV architecture encode information in spectral amplitude functions of single-photons.

Some of the first implementations of CV QML in medical tasks can be seen in \cite{Kairon2021}, where COVID-19 diagnosis is conducted through CV Quantum Neural Networks (QNNs), comparing its performance with a quantum backpropagation multilayer perceptron is analyzed. Furthermore, the multiclass classification task is tackled in \cite{Choe2022}, where based on the CV architecture proposed in \cite{Killoran2018}, a MNIST classifier is proposed, focusing on the number of encoding qumodes, and using Gaussian and non Gaussian gates for bias addition and nonlinear functions. In recent years, CV QNN development has also been implemented in time-series forecasting as shown in \cite{Anand2024}. In \cite{Anand2024}, a comparison with the DV quantum and classical counterpart is conducted for energy consumption and stock price data forecasting, showing favorable results with lesser number of parameters for the CV model, while also introducing continuous values and nonlinearities, a problem in qubit-based quantum computing.

Taking this into consideration, although quantum machine learning progress on the CV paradigm has been made, focus on medical imaging is still limited. As a result, we propose a CV QNN for biomedical image classification on datasets from MedMNIST \cite{medmnistv2}, assessing through noise robustness tests, statistical analysis, and comparison to its DV QNN and classical model counterparts.","The field of Computer-Aided Diagnosis (CAD) has advanced through diverse methodologies, ranging from enhanced data processing, automated data workflows, artificial intelligence and more recently, quantum computing approaches. This review focuses on some of the potential improvements quantum computing presents in healthcare, as well as developments in the quantum machine learning field from the discrete-variable and continuous-variable paradigms.

In \cite{Schuld2018, Schuld2019}, the potential of the implementation of quantum algorithms and quantum data encoding for ML tasks was discussed and demonstrated. Input data can be encoded into quantum states and represented in high-dimensional Hilbert spaces, where data is implicitly defined as kernel functions between data points and can be exploited by kernel-based methods, such as the Support Vector Machine. Following this and further development of the QML field in ML and DL tasks, QML showed promising results in healthcare, as seen in \cite{Sengupta2021}, where clinical prognostic analysis for image classification and segmentation of COVID-19 is accelerated, outperforming conventional deep learning methods by 2.92\

In recent years, QML models have shown potential (82.86\

In contrast, CV QML has not had an equal surge, mainly due to photonic quantum computers still being in development. However, Hilbert space data representation can be extended in CV quantum computing, as data embedded into infinite-dimensional Hilbert spaces via Gaussian gates presents potential to more refined data features and details than classical methods. Because of this, early CV QML research can be traced back to \cite{Lau2017}, where a set of QML subroutines are generalized for infinite-dimensional systems intended for an all-photonic CV quantum computer. Furthermore, general methods for building CV neural networks for CV quantum computers is introduced in \cite{Killoran2018}, where information encoding and nonlinear activation functions are enacted through Gaussian and non Gaussian gates. 

Moreover, machine learning and optimization techniques for quantum photonic circuits were shown in \cite{Moody2019}. Here, a network comprised of several layers of optical gates with variable parameters are optimized applying automatic differentiation, showing the power and versatility of learning how to effectively use short-depth circuits to synthesize single-photons. Additional CV kernel studies are conducted in \cite{Li2022}, where the expressiveness of large Hilbert spaces is explored, introducing quantum kernel encoding methods into CV quantum states through amplitude squeezing and phase manipulation. Realistic implementations of neural networks on photonic quantum computers is proposed in \cite{Ghasemian2023}, where quantum circuits built in CV architecture encode information in spectral amplitude functions of single-photons.

Some of the first implementations of CV QML in medical tasks can be seen in \cite{Kairon2021}, where COVID-19 diagnosis is conducted through CV Quantum Neural Networks (QNNs), comparing its performance with a quantum backpropagation multilayer perceptron is analyzed. Furthermore, the multiclass classification task is tackled in \cite{Choe2022}, where based on the CV architecture proposed in \cite{Killoran2018}, a MNIST classifier is proposed, focusing on the number of encoding qumodes, and using Gaussian and non Gaussian gates for bias addition and nonlinear functions. In recent years, CV QNN development has also been implemented in time-series forecasting as shown in \cite{Anand2024}. In \cite{Anand2024}, a comparison with the DV quantum and classical counterpart is conducted for energy consumption and stock price data forecasting, showing favorable results with lesser number of parameters for the CV model, while also introducing continuous values and nonlinearities, a problem in qubit-based quantum computing.

Taking this into consideration, although quantum machine learning progress on the CV paradigm has been made, focus on medical imaging is still limited. As a result, we propose a CV QNN for biomedical image classification on datasets from MedMNIST \cite{medmnistv2}, assessing through noise robustness tests, statistical analysis, and comparison to its DV QNN and classical model counterparts.","The field of Computer-Aided Diagnosis (CAD) has advanced through diverse methodologies,
ranging from enhanced data processing, automated data workflows, artificial intelligence and more
2
IOP PublishingJournalvv(yyyy) aaaaaa Authoret al
recently, quantum computing approaches. This review focuses on some of the potential
improvements quantum computing presents in healthcare, as well as developments in the quantum
machine learning field from the discrete-variable and continuous-variable paradigms.
In [12, 18], the potential of the implementation of quantum algorithms and quantum data
encoding for ML tasks was discussed and demonstrated. Input data can be encoded into quantum
states and represented in high-dimensional Hilbert spaces, where data is implicitly defined as kernel
functions between data points and can be exploited by kernel-based methods, such as the Support
Vector Machine. Following this and further development of the QML field in ML and DL tasks,
QML showed promising results in healthcare, as seen in [19], where clinical prognostic analysis for
image classification and segmentation of COVID-19 is accelerated, outperforming conventional deep
learning methods by 2.92%. Similarly, it has been able to predict heart disease through an ensemble
model that works as a quantum support vector machine for classification, as demonstrated in [20],
where it attained an accuracy of 90.16% competing with state-of-the-art models. Further research
has been conducted on other areas that QML can aid, such as drug response as shown in [21],
where a hybrid quantum neural network based on a graph and deep convolutional neural layers is
proposed, outperforming classical analogs by 15% in drug effectiveness prediction.
In recent years, QML models have shown potential (82.86% accuracy) in more complex
computer vision tasks in the CAD field such as skin lesion classification in dermoscopic images as
seen in [22], leveraging rotational gates for encoding, as well as classical backbones for feature
extraction and preparation for a quantum support vector classifier. Simlarly, favorable results on
multiclass classification has been demonstrated, as demonstrated in [23], where an 80.96% diagnosis
accuracy was achieved for diabetic retinopathy through a hybrid quantum-classical framework
based on ResNet50 and an 8-qubit quantum classifier. Moreover, innovative approaches such as
quantum-enhanced dual-backbone architecture as shown in [24] have been developed, achieving a
parameter complexity reduction of 29.04% and 94.44% of trainable parameters, while still attaining
high accuracy (95.80% and 95.42% on training and validation sets).
In contrast, CV QML has not had an equal surge, mainly due to photonic quantum computers
still being in development. However, Hilbert space data representation can be extended in CV
quantum computing, as data embedded into infinite-dimensional Hilbert spaces via Gaussian gates
presents potential to more refined data features and details than classical methods. Because of this,
early CV QML research can be traced back to [25], where a set of QML subroutines are generalized
for infinite-dimensional systems intended for an all-photonic CV quantum computer. Furthermore,
general methods for building CV neural networks for CV quantum computers is introduced in [13],
where information encoding and nonlinear activation functions are enacted through Gaussian and
non Gaussian gates.
Moreover, machine learning and optimization techniques for quantum photonic circuits were
shown in [26]. Here, a network comprised of several layers of optical gates with variable parameters
are optimized applying automatic differentiation, showing the power and versatility of learning how
to effectively use short-depth circuits to synthesize single-photons. Additional CV kernel studies
are conducted in [27], where the expressiveness of large Hilbert spaces is explored, introducing
quantum kernel encoding methods into CV quantum states through amplitude squeezing and phase
manipulation. Realistic implementations of neural networks on photonic quantum computers is
proposed in [28], where quantum circuits built in CV architecture encode information in spectral
amplitude functions of single-photons.
Some of the first implementations of CV QML in medical tasks can be seen in [29], where
COVID-19 diagnosis is conducted through CV Quantum Neural Networks (QNNs), comparing its
performance with a quantum backpropagation multilayer perceptron is analyzed. Furthermore, the
multiclass classification task is tackled in [30], where based on the CV architecture proposed in [13],
a MNIST classifier is proposed, focusing on the number of encoding qumodes, and using Gaussian
and non Gaussian gates for bias addition and nonlinear functions. In recent years, CV QNN
development has also been implemented in time-series forecasting as shown in [31]. In [31], a
comparison with the DV quantum and classical counterpart is conducted for energy consumption
and stock price data forecasting, showing favorable results with lesser number of parameters for the
CV model, while also introducing continuous values and nonlinearities, a problem in qubit-based
quantum computing.
Taking this into consideration, although quantum machine learning progress on the CV
paradigm has been made, focus on medical imaging is still limited. As a result, we propose a CV
QNN for biomedical image classification on datasets from MedMNIST [17], assessing through noise
robustness tests, statistical analysis, and comparison to its DV QNN and classical model
counterparts.
3
IOP PublishingJournalvv(yyyy) aaaaaa Authoret al"
2511.08955v1,http://arxiv.org/abs/2511.08955v1,2025-11-12 04:01:25+00:00,MicroEvoEval: A Systematic Evaluation Framework for Image-Based Microstructure Evolution Prediction,"Simulating microstructure evolution (MicroEvo) is vital for materials design but demands high numerical accuracy, efficiency, and physical fidelity. Although recent studies on deep learning (DL) offer a promising alternative to traditional solvers, the field lacks standardized benchmarks. Existing studies are flawed due to a lack of comparing specialized MicroEvo DL models with state-of-the-art spatio-temporal architectures, an overemphasis on numerical accuracy over physical fidelity, and a failure to analyze error propagation over time. To address these gaps, we introduce MicroEvoEval, the first comprehensive benchmark for image-based microstructure evolution prediction. We evaluate 14 models, encompassing both domain-specific and general-purpose architectures, across four representative MicroEvo tasks with datasets specifically structured for both short- and long-term assessment. Our multi-faceted evaluation framework goes beyond numerical accuracy and computational cost, incorporating a curated set of structure-preserving metrics to assess physical fidelity. Our extensive evaluations yield several key insights. Notably, we find that modern architectures (e.g., VMamba), not only achieve superior long-term stability and physical fidelity but also operate with an order-of-magnitude greater computational efficiency. The results highlight the necessity of holistic evaluation and identify these modern architectures as a highly promising direction for developing efficient and reliable surrogate models in data-driven materials science.","\label{sec:related_work}

\textbf{Physics-Based Numerical Methods.}
Modeling microstructure evolution has traditionally relied on continuum-scale, physics-based formulations. Phase-field models, which describe spatio-temporal dynamics through PDEs for phenomena like solidification and grain growth~\cite{chen2002phase, steinbach1996phase}, are particularly widespread due to their flexibility. However, their high computational cost, especially with explicit time integration schemes, restricts the accessible time and length scales of simulations and hinders rapid exploration of process-parameter space~\cite{greenwood2018quantitative}. Furthermore, for complex or poorly characterized materials, deriving tractable and accurate PDEs can be a significant challenge in itself. While acceleration techniques~\cite{guo2015solving} and alternative data-driven frameworks like Markov Random Fields~\cite{acar2016markov} have been explored to alleviate these issues, the trade-off between fidelity and computational cost remains a primary bottleneck, motivating the search for efficient surrogate models.

\textbf{Deep Learning for Microstructure Evolution.}
To address the computational bottleneck of numerical methods, deep learning (DL) has emerged as a promising alternative for learning MicroEvo dynamics directly from data. Many pioneering studies have adapted established deep learning architectures for this purpose. These approaches often rely on Recurrent Neural Network (RNN) variants to capture temporal dependencies. Examples include adapting classic architectures like ConvLSTM~\cite{mao2024spatiotemporal}, ConvGRU~\cite{lanzoni2022morphological}, and PredRNN~\cite{farizhandi2023spatiotemporal} to forecast microstructural patterns. Other works employ a CNN-RNN structure, using CNNs as powerful feature extractors for the recurrent core. Prominent examples in this category include E3D-LSTM~\cite{yang2021self} and the more recent state-space-based model, VMamba~\cite{jing2025research}. Although these models have demonstrated the feasibility of data-driven MicroEvo prediction, they have been developed and evaluated on disparate tasks and mainly focus on numerical accuracy on long-term prediction, making it difficult to comprehensively compare their relative strengths and weaknesses.

\textbf{General-Purpose Spatio-Temporal Prediction.}
Concurrently, the broader field of computer vision has produced a wealth of powerful models for general-purpose spatio-temporal prediction (i.e., video prediction). These architectures have evolved significantly, from advanced recurrent networks like PredRNN++~\cite{Wang2018predrnn++} and MAU~\cite{chang2021mau} to highly efficient, non-recurrent CNN models such as SimVP~\cite{gao2022simvp} and its successor SimVP.v2~\cite{tan2025simvpv2}. More recently, Transformer-based architectures have become prominent for their ability to capture long-range dependencies, leading to models like PredFormer~\cite{tang2024predformer} and hybrids like SwinLSTM~\cite{tang2023swinlstm}. The cutting edge continues to advance with refined attention mechanisms in models like TAU~\cite{tan2023temporal} and the integration of Mamba in VMRNN~\cite{tang2024vmrnn}. Despite their proven success on different general tasks, these state-of-the-art models are not designed with inherent physical constraints. Their effectiveness in predicting physically plausible microstructure evolution, a domain with fundamentally different underlying rules from natural videos, has not been systematically investigated. Our benchmark aims to bridge this gap by evaluating both domain-specific and general-purpose models under a unified framework.","\textbf{Physics-Based Numerical Methods.}
Modeling microstructure evolution has traditionally relied on continuum-scale, physics-based formulations. Phase-field models, which describe spatio-temporal dynamics through PDEs for phenomena like solidification and grain growth~\cite{chen2002phase, steinbach1996phase}, are particularly widespread due to their flexibility. However, their high computational cost, especially with explicit time integration schemes, restricts the accessible time and length scales of simulations and hinders rapid exploration of process-parameter space~\cite{greenwood2018quantitative}. Furthermore, for complex or poorly characterized materials, deriving tractable and accurate PDEs can be a significant challenge in itself. While acceleration techniques~\cite{guo2015solving} and alternative data-driven frameworks like Markov Random Fields~\cite{acar2016markov} have been explored to alleviate these issues, the trade-off between fidelity and computational cost remains a primary bottleneck, motivating the search for efficient surrogate models.

\textbf{Deep Learning for Microstructure Evolution.}
To address the computational bottleneck of numerical methods, deep learning (DL) has emerged as a promising alternative for learning MicroEvo dynamics directly from data. Many pioneering studies have adapted established deep learning architectures for this purpose. These approaches often rely on Recurrent Neural Network (RNN) variants to capture temporal dependencies. Examples include adapting classic architectures like ConvLSTM~\cite{mao2024spatiotemporal}, ConvGRU~\cite{lanzoni2022morphological}, and PredRNN~\cite{farizhandi2023spatiotemporal} to forecast microstructural patterns. Other works employ a CNN-RNN structure, using CNNs as powerful feature extractors for the recurrent core. Prominent examples in this category include E3D-LSTM~\cite{yang2021self} and the more recent state-space-based model, VMamba~\cite{jing2025research}. Although these models have demonstrated the feasibility of data-driven MicroEvo prediction, they have been developed and evaluated on disparate tasks and mainly focus on numerical accuracy on long-term prediction, making it difficult to comprehensively compare their relative strengths and weaknesses.

\textbf{General-Purpose Spatio-Temporal Prediction.}
Concurrently, the broader field of computer vision has produced a wealth of powerful models for general-purpose spatio-temporal prediction (i.e., video prediction). These architectures have evolved significantly, from advanced recurrent networks like PredRNN++~\cite{Wang2018predrnn++} and MAU~\cite{chang2021mau} to highly efficient, non-recurrent CNN models such as SimVP~\cite{gao2022simvp} and its successor SimVP.v2~\cite{tan2025simvpv2}. More recently, Transformer-based architectures have become prominent for their ability to capture long-range dependencies, leading to models like PredFormer~\cite{tang2024predformer} and hybrids like SwinLSTM~\cite{tang2023swinlstm}. The cutting edge continues to advance with refined attention mechanisms in models like TAU~\cite{tan2023temporal} and the integration of Mamba in VMRNN~\cite{tang2024vmrnn}. Despite their proven success on different general tasks, these state-of-the-art models are not designed with inherent physical constraints. Their effectiveness in predicting physically plausible microstructure evolution, a domain with fundamentally different underlying rules from natural videos, has not been systematically investigated. Our benchmark aims to bridge this gap by evaluating both domain-specific and general-purpose models under a unified framework.","Physics-Based Numerical Methods.Modeling microstruc-
ture evolution has traditionally relied on continuum-scale,
physics-based formulations. Phase-field models, which de-
scribe spatio-temporal dynamics through PDEs for phenom-
ena like solidification and grain growth (Chen 2002; Stein-
bach et al. 1996), are particularly widespread due to their
flexibility. However, their high computational cost, espe-
cially with explicit time integration schemes, restricts the
accessible time and length scales of simulations and hinders
rapid exploration of process-parameter space (Greenwood
et al. 2018). Furthermore, for complex or poorly charac-
terized materials, deriving tractable and accurate PDEs can
be a significant challenge in itself. While acceleration tech-
niques (Guo and Xiong 2015) and alternative data-driven
frameworks like Markov Random Fields (Acar and Sun-
dararaghavan 2016) have been explored to alleviate these is-
sues, the trade-off between fidelity and computational cost
remains a primary bottleneck, motivating the search for effi-
cient surrogate models.
Deep Learning for Microstructure Evolution.To ad-
dress the computational bottleneck of numerical methods,
deep learning (DL) has emerged as a promising alternative
for learning MicroEvo dynamics directly from data. Many
pioneering studies have adapted established deep learning
architectures for this purpose. These approaches often rely
on Recurrent Neural Network (RNN) variants to capture
temporal dependencies. Examples include adapting clas-
sic architectures like ConvLSTM (Mao et al. 2024), Con-
vGRU (Lanzoni et al. 2022), and PredRNN (Farizhandi
and Mamivand 2023) to forecast microstructural patterns.
Other works employ a CNN-RNN structure, using CNNs
as powerful feature extractors for the recurrent core. Promi-
nent examples in this category include E3D-LSTM (Yang
et al. 2021) and the more recent state-space-based model,
VMamba (Jing-jie et al. 2025). Although these models have
demonstrated the feasibility of data-driven MicroEvo pre-
diction, they have been developed and evaluated on disparate
tasks and mainly focus on numerical accuracy on long-term
prediction, making it difficult to comprehensively compare
their relative strengths and weaknesses.
General-Purpose Spatio-Temporal Prediction.Con-
currently, the broader field of computer vision has pro-
duced a wealth of powerful models for general-purpose
spatio-temporal prediction (i.e., video prediction). These ar-
chitectures have evolved significantly, from advanced re-
current networks like PredRNN++ (Wang et al. 2018)
and MAU (Chang et al. 2021) to highly efficient, non-
recurrent CNN models such as SimVP (Gao et al. 2022)
and its successor SimVP.v2 (Tan et al. 2025). More recently,
Transformer-based architectures have become prominent for
their ability to capture long-range dependencies, leading to
models like PredFormer (Tang et al. 2025) and hybrids like
SwinLSTM (Tang et al. 2023). The cutting edge continues
to advance with refined attention mechanisms in models like
TAU (Tan et al. 2023) and the integration of Mamba in VM-
RNN (Tang et al. 2024). Despite their proven success on
different general tasks, these state-of-the-art models are not
designed with inherent physical constraints. Their effective-
ness in predicting physically plausible microstructure evo-
lution, a domain with fundamentally different underlying
rules from natural videos, has not been systematically inves-
tigated. Our benchmark aims to bridge this gap by evaluat-
ing both domain-specific and general-purpose models under
a unified framework."
2511.09979v1,http://arxiv.org/abs/2511.09979v1,2025-11-13 05:27:49+00:00,Rediscovering the Lunar Equation of the Centre with AI Feynman via Embedded Physical Biases,"This work explores using the physics-inspired AI Feynman symbolic regression algorithm to automatically rediscover a fundamental equation in astronomy -- the Equation of the Centre. Through the introduction of observational and inductive biases corresponding to the physical nature of the system through data preprocessing and search space restriction, AI Feynman was successful in recovering the first-order analytical form of this equation from lunar ephemerides data. However, this manual approach highlights a key limitation in its reliance on expert-driven coordinate system selection. We therefore propose an automated preprocessing extension to find the canonical coordinate system. Results demonstrate that targeted domain knowledge embedding enables symbolic regression to rediscover physical laws, but also highlight further challenges in constraining symbolic regression to derive physics equations when leveraging domain knowledge through tailored biases.","\subsection{Classical Symbolic Regression}
The main goal of symbolic regression is to find an analytic expression for an unknown function $f(\cdot)$ that maps the $d$-dimensional input $x \in \mathbb{R}^d$ to the target variable $y \cong f(x) \in \mathbb{R}$ given a dataset of observations $\{x_i, y_i\}_{i=1}^{N}$. However, finding equations that capture datasets is a combinatorial challenge; the sheer number of combinations of operators and operands makes a brute-force approach computationally unfeasible \cite{karniadakisPhysicsinformedMachineLearning2021}. There exist many techniques for symbolic regression, but they can be broadly divided into three main classes: expression tree-based, regression-based, and physics- or mathematics-informed \cite{makke2023interpretablescientificdiscoverysymbolic}.

Expression tree-based methods are often based on paradigms like genetic programming, where models can discover the form and coefficients of the equation by representing approximate candidate solutions using an expression tree-like data structure. Transition functions, like random recombination or permutation, are iteratively applied to generate new candidate solutions, while candidate solutions with low `fitness' - some desired objective function - are dropped from the model \cite{oh2023geneticprogrammingbasedsymbolic}.

Regression-based methods, on the other hand, search for the coefficients of a fixed prespecified basis that minimise error. As the size of the basis increases, the accuracy of the function may increase, but the form of the solution may grow less parsimonious \cite{makke2023interpretablescientificdiscoverysymbolic}.

\subsection{Physics-Informed Symbolic Regression}
Physics-informed symbolic regression methods leverage simplifying properties derived from physics, like symmetry and separability to limit the search space and find parsimonious and accurate solutions along the Pareto frontier, which represents the solutions with the best trade-offs between parsimony and accuracy of the solution to the system in question. The introduction of simplifying physical properties generally takes three forms of biases \cite{Khoo2023.1}: observational bias, learning bias, and inductive bias. Observational biases are introduced through the selection of data augmentation and transformation techniques for the data to embody underlying physical principles \cite{karniadakisPhysicsinformedMachineLearning2021}. Learning biases include the choice of appropriate loss functions, hyperparameters, and learning algorithms that guide the model toward physically meaningful solutions. Inductive biases are the inherent assumptions built into the architecture of the model such that predicted solutions are guaranteed to satisfy a set of physical conditions and laws. There are various techniques that have been shown to be effective for physics-informed symbolic regression.

\subsubsection{SINDy}\label{sec:sindy}
\emph{Sparse Identification of Non-Linear Dynamics} \cite{Brunton2016} leverages the sparsity of key terms in physical systems, using sparsity techniques for efficient identification of relevant terms in the model. This promotes parsimony and avoids overfitting. The method involves collecting state data and its derivatives (possibly approximated numerically), adding noise for robustness, and constructing a library of candidate non-linear functions for each state variable. A sparse regression technique, like LASSO, is then applied to determine the coefficients that identify the important terms within the model. Domain knowledge can further guide the selection of non-linear functions, and help exploit other simplifying properties. SINDy has been shown to be effective in recovering accurate models for chaotic systems like the Lorenz system and vortex shedding, demonstrating robustness to noise and even the absence of direct derivative measurements. However, challenges remain in choosing the most suitable measurement coordinates and the optimal basis of the sparsifying function.

\subsubsection{Graph Neural Networks \& PySR}\label{sec:gnn}
\emph{Lemos et al} \cite{Lemos2023} demonstrate the utility of embedding inductive biases in rediscovering Newton's Law of Gravitation from trajectory data of solar system objects. First, a graph neural network is used to simulate the dynamics of solar system objects from 30 years of trajectory data, with the positions and velocities of the bodies represented as nodes, and physical interactions as edges between these nodes. Inductive biases, such as translational invariance, rotational invariance, and Newton's laws of motion were embedded through data augmentation and the multiplicative relationship between the node and its acceleration. This promoted candidate solutions that were aligned with existing known physical laws. Then, an open-source analogue of Eureqa (implemented in the \lstinline{PySR} library \cite{cranmer2023interpretablemachinelearningscience}) was used to discover analytical expressions for the learned simulator, where a tree search algorithm was used to produce a set of candidate functions, which were evaluated using a score corresponding to the ratio between accuracy and parsimony. This two-step method has been shown to be effective and efficient in discovering analytic equations corresponding to Newton's Law of Gravitation. The authors identify the implementation of the method using Bayesian Neural Networks to model the masses in the system as an avenue for future exploration. The authors further identify the evaluation score for the candidate solutions as a limitation, emphasising that it may not align with what a physicist may identify as a `good' equation.

\subsubsection{AI Feynman}
\emph{AI Feynman} \cite{udrescu2020aifeynmanphysicsinspiredmethod} utilises neural networks to identify simplifying physical properties within the data. This approach addresses the limitations of techniques like genetic algorithms and sparse regression, which might struggle to capture these underlying principles. In this regard, AI Feynman outperforms the techniques discussed in previous sections. It incorporates six assumptions about the underlying function, including known physical units of variables, low-order polynomial structures, smoothness, composition, symmetry, and separability. The core algorithm works recursively, first employing dimensional analysis to reduce data complexity and then fitting polynomials and exploring increasingly complex expressions through brute force. Additionally, AI Feynman uses neural networks to identify specific transformations like symmetry, separability, and variable equality, allowing for a more efficient decomposition of the problem into simpler sub-problems with fewer variables. This focus on decomposability is a key improvement over methods like Eureqa. Khoo et al. have demonstrated the effectiveness of AI Feynman with embedded observational and inductive biases in recovering the orbital equation of Mars from the Rudolphine tables \cite{Khoo2023.1}.

Despite progress, automated symbolic regression remains limited by its reliance on expert-driven preprocessing (especially choice of coordinate system) and challenges in distinguishing fundamental physical effects from observational noise and perturbations. These open problems motivate work on automated, physics-guided transformations that can steer equation discovery toward canonical, interpretable forms.","\subsection{Classical Symbolic Regression}
The main goal of symbolic regression is to find an analytic expression for an unknown function $f(\cdot)$ that maps the $d$-dimensional input $x \in \mathbb{R}^d$ to the target variable $y \cong f(x) \in \mathbb{R}$ given a dataset of observations $\{x_i, y_i\}_{i=1}^{N}$. However, finding equations that capture datasets is a combinatorial challenge; the sheer number of combinations of operators and operands makes a brute-force approach computationally unfeasible \cite{karniadakisPhysicsinformedMachineLearning2021}. There exist many techniques for symbolic regression, but they can be broadly divided into three main classes: expression tree-based, regression-based, and physics- or mathematics-informed \cite{makke2023interpretablescientificdiscoverysymbolic}.

Expression tree-based methods are often based on paradigms like genetic programming, where models can discover the form and coefficients of the equation by representing approximate candidate solutions using an expression tree-like data structure. Transition functions, like random recombination or permutation, are iteratively applied to generate new candidate solutions, while candidate solutions with low `fitness' - some desired objective function - are dropped from the model \cite{oh2023geneticprogrammingbasedsymbolic}.

Regression-based methods, on the other hand, search for the coefficients of a fixed prespecified basis that minimise error. As the size of the basis increases, the accuracy of the function may increase, but the form of the solution may grow less parsimonious \cite{makke2023interpretablescientificdiscoverysymbolic}.

\subsection{Physics-Informed Symbolic Regression}
Physics-informed symbolic regression methods leverage simplifying properties derived from physics, like symmetry and separability to limit the search space and find parsimonious and accurate solutions along the Pareto frontier, which represents the solutions with the best trade-offs between parsimony and accuracy of the solution to the system in question. The introduction of simplifying physical properties generally takes three forms of biases \cite{Khoo2023.1}: observational bias, learning bias, and inductive bias. Observational biases are introduced through the selection of data augmentation and transformation techniques for the data to embody underlying physical principles \cite{karniadakisPhysicsinformedMachineLearning2021}. Learning biases include the choice of appropriate loss functions, hyperparameters, and learning algorithms that guide the model toward physically meaningful solutions. Inductive biases are the inherent assumptions built into the architecture of the model such that predicted solutions are guaranteed to satisfy a set of physical conditions and laws. There are various techniques that have been shown to be effective for physics-informed symbolic regression.

\subsubsection{SINDy}\emph{Sparse Identification of Non-Linear Dynamics} \cite{Brunton2016} leverages the sparsity of key terms in physical systems, using sparsity techniques for efficient identification of relevant terms in the model. This promotes parsimony and avoids overfitting. The method involves collecting state data and its derivatives (possibly approximated numerically), adding noise for robustness, and constructing a library of candidate non-linear functions for each state variable. A sparse regression technique, like LASSO, is then applied to determine the coefficients that identify the important terms within the model. Domain knowledge can further guide the selection of non-linear functions, and help exploit other simplifying properties. SINDy has been shown to be effective in recovering accurate models for chaotic systems like the Lorenz system and vortex shedding, demonstrating robustness to noise and even the absence of direct derivative measurements. However, challenges remain in choosing the most suitable measurement coordinates and the optimal basis of the sparsifying function.

\subsubsection{Graph Neural Networks \& PySR}\emph{Lemos et al} \cite{Lemos2023} demonstrate the utility of embedding inductive biases in rediscovering Newton's Law of Gravitation from trajectory data of solar system objects. First, a graph neural network is used to simulate the dynamics of solar system objects from 30 years of trajectory data, with the positions and velocities of the bodies represented as nodes, and physical interactions as edges between these nodes. Inductive biases, such as translational invariance, rotational invariance, and Newton's laws of motion were embedded through data augmentation and the multiplicative relationship between the node and its acceleration. This promoted candidate solutions that were aligned with existing known physical laws. Then, an open-source analogue of Eureqa (implemented in the \lstinline{PySR} library \cite{cranmer2023interpretablemachinelearningscience}) was used to discover analytical expressions for the learned simulator, where a tree search algorithm was used to produce a set of candidate functions, which were evaluated using a score corresponding to the ratio between accuracy and parsimony. This two-step method has been shown to be effective and efficient in discovering analytic equations corresponding to Newton's Law of Gravitation. The authors identify the implementation of the method using Bayesian Neural Networks to model the masses in the system as an avenue for future exploration. The authors further identify the evaluation score for the candidate solutions as a limitation, emphasising that it may not align with what a physicist may identify as a `good' equation.

\subsubsection{AI Feynman}
\emph{AI Feynman} \cite{udrescu2020aifeynmanphysicsinspiredmethod} utilises neural networks to identify simplifying physical properties within the data. This approach addresses the limitations of techniques like genetic algorithms and sparse regression, which might struggle to capture these underlying principles. In this regard, AI Feynman outperforms the techniques discussed in previous sections. It incorporates six assumptions about the underlying function, including known physical units of variables, low-order polynomial structures, smoothness, composition, symmetry, and separability. The core algorithm works recursively, first employing dimensional analysis to reduce data complexity and then fitting polynomials and exploring increasingly complex expressions through brute force. Additionally, AI Feynman uses neural networks to identify specific transformations like symmetry, separability, and variable equality, allowing for a more efficient decomposition of the problem into simpler sub-problems with fewer variables. This focus on decomposability is a key improvement over methods like Eureqa. Khoo et al. have demonstrated the effectiveness of AI Feynman with embedded observational and inductive biases in recovering the orbital equation of Mars from the Rudolphine tables \cite{Khoo2023.1}.

Despite progress, automated symbolic regression remains limited by its reliance on expert-driven preprocessing (especially choice of coordinate system) and challenges in distinguishing fundamental physical effects from observational noise and perturbations. These open problems motivate work on automated, physics-guided transformations that can steer equation discovery toward canonical, interpretable forms.","Classical Symbolic Regression
The main goal of symbolic regression is to find an ana-
lytic expression for an unknown functionf(·)that maps
thed-dimensional inputx∈Rdto the target variable
y∼=f(x)∈Rgiven a dataset of observations{x i, yi}N
i=1.
However, finding equations that capture datasets is a com-
binatorial challenge; the sheer number of combinations of
operators and operands makes a brute-force approach com-
putationally unfeasible (Karniadakis et al. 2021). There
exist many techniques for symbolic regression, but they
can be broadly divided into three main classes: expression
tree-based, regression-based, and physics- or mathematics-
informed (Makke and Chawla 2023).
Expression tree-based methods are often based on
paradigms like genetic programming, where models can dis-
cover the form and coefficients of the equation by repre-
senting approximate candidate solutions using an expression
tree-like data structure. Transition functions, like random re-
combination or permutation, are iteratively applied to gener-
ate new candidate solutions, while candidate solutions with
low ‘fitness’ - some desired objective function - are dropped
from the model (Oh et al. 2023).
Regression-based methods, on the other hand, search for
the coefficients of a fixed prespecified basis that minimise
error. As the size of the basis increases, the accuracy of the
function may increase, but the form of the solution may grow
less parsimonious (Makke and Chawla 2023).
Physics-Informed Symbolic Regression
Physics-informed symbolic regression methods leverage
simplifying properties derived from physics, like symme-
try and separability to limit the search space and find par-
simonious and accurate solutions along the Pareto frontier,
which represents the solutions with the best trade-offs be-
tween parsimony and accuracy of the solution to the sys-
tem in question. The introduction of simplifying physical
properties generally takes three forms of biases (Khoo et al.
2023a): observational bias, learning bias, and inductive bias.
Observational biases are introduced through the selection
of data augmentation and transformation techniques for the
data to embody underlying physical principles (Karniadakis
et al. 2021). Learning biases include the choice of appro-
priate loss functions, hyperparameters, and learning algo-
rithms that guide the model toward physically meaningful
solutions. Inductive biases are the inherent assumptions built
into the architecture of the model such that predicted solu-
tions are guaranteed to satisfy a set of physical conditions
and laws. There are various techniques that have been shown
to be effective for physics-informed symbolic regression.
SINDySparse Identification of Non-Linear Dynamics
(Brunton, Proctor, and Kutz 2016) leverages the sparsity of
key terms in physical systems, using sparsity techniques for
efficient identification of relevant terms in the model. This
promotes parsimony and avoids overfitting. The method
involves collecting state data and its derivatives (possibly
approximated numerically), adding noise for robustness,
and constructing a library of candidate non-linear func-
tions for each state variable. A sparse regression technique,
like LASSO, is then applied to determine the coefficients
that identify the important terms within the model. Do-
main knowledge can further guide the selection of non-
linear functions, and help exploit other simplifying proper-
ties. SINDy has been shown to be effective in recovering
accurate models for chaotic systems like the Lorenz system
and vortex shedding, demonstrating robustness to noise and
even the absence of direct derivative measurements. How-
ever, challenges remain in choosing the most suitable mea-
surement coordinates and the optimal basis of the sparsify-
ing function.
Graph Neural Networks & PySRLemos et al(Lemos
et al. 2023) demonstrate the utility of embedding inductive
biases in rediscovering Newton’s Law of Gravitation from
trajectory data of solar system objects. First, a graph neu-
ral network is used to simulate the dynamics of solar system
objects from 30 years of trajectory data, with the positions
and velocities of the bodies represented as nodes, and phys-
ical interactions as edges between these nodes. Inductive bi-
ases, such as translational invariance, rotational invariance,
and Newton’s laws of motion were embedded through data
augmentation and the multiplicative relationship between
the node and its acceleration. This promoted candidate so-
lutions that were aligned with existing known physical laws.
Then, an open-source analogue of Eureqa (implemented in
thePySRlibrary (Cranmer 2023)) was used to discover an-
alytical expressions for the learned simulator, where a tree
search algorithm was used to produce a set of candidate
functions, which were evaluated using a score corresponding
to the ratio between accuracy and parsimony. This two-step
method has been shown to be effective and efficient in dis-
covering analytic equations corresponding to Newton’s Law
of Gravitation. The authors identify the implementation of
the method using Bayesian Neural Networks to model the
masses in the system as an avenue for future exploration.
The authors further identify the evaluation score for the can-didate solutions as a limitation, emphasising that it may not
align with what a physicist may identify as a ‘good’ equa-
tion.
AI FeynmanAI Feynman(Udrescu and Tegmark 2020)
utilises neural networks to identify simplifying physical
properties within the data. This approach addresses the lim-
itations of techniques like genetic algorithms and sparse
regression, which might struggle to capture these under-
lying principles. In this regard, AI Feynman outperforms
the techniques discussed in previous sections. It incorpo-
rates six assumptions about the underlying function, includ-
ing known physical units of variables, low-order polynomial
structures, smoothness, composition, symmetry, and separa-
bility. The core algorithm works recursively, first employ-
ing dimensional analysis to reduce data complexity and then
fitting polynomials and exploring increasingly complex ex-
pressions through brute force. Additionally, AI Feynman
uses neural networks to identify specific transformations like
symmetry, separability, and variable equality, allowing for a
more efficient decomposition of the problem into simpler
sub-problems with fewer variables. This focus on decom-
posability is a key improvement over methods like Eureqa.
Khoo et al. have demonstrated the effectiveness of AI Feyn-
man with embedded observational and inductive biases in
recovering the orbital equation of Mars from the Rudolphine
tables (Khoo et al. 2023a).
Despite progress, automated symbolic regression remains
limited by its reliance on expert-driven preprocessing (es-
pecially choice of coordinate system) and challenges in
distinguishing fundamental physical effects from observa-
tional noise and perturbations. These open problems mo-
tivate work on automated, physics-guided transformations
that can steer equation discovery toward canonical, inter-
pretable forms."
2511.08848v1,http://arxiv.org/abs/2511.08848v1,2025-11-12 00:04:53+00:00,Space-Time Optimisations for Early Fault-Tolerant Quantum Computation,"Fault-tolerance is the future of quantum computing, ensuring error-corrected quantum computation that can be used for practical applications. Resource requirements for fault-tolerant quantum computing (FTQC) are daunting, and hence, compilation techniques must be designed to ensure resource efficiency. There is a growing need for compilation strategies tailored to the early FTQC regime, which refers to the first generation of fault-tolerant machines operating under stringent resource constraints of fewer physical qubits and limited distillation capacity. Present-day compilation techniques are largely focused on overprovisioning of routing paths and make liberal assumptions regarding the availability of distillation factories. Our work develops compilation techniques that are tailored to the needs of early FTQC systems, including distillation-adaptive qubit layouts and routing techniques. In particular, we show that simple greedy heuristics are extremely effective for this problem, offering up to 60% reduction in the number of qubits compared to prior works. Our techniques offer results with an average overhead of 1.2X in execution time for a 53% reduction in qubits against the theoretical lower bounds. As the industry develops early FTQC systems with tens to hundreds of logical qubits over the coming years, our work has the potential to be widely useful for optimising program executions.","Given that optimising surface code circuits is known to be NP-hard \cite{Herr2017}, a variety of optimisation techniques at different levels of abstraction have emerged in recent years \cite{Litinski2019gameofsurfacecodes,10.1145/3720416,10.1109/MICRO.2018.00072,10.1145/3466752.3480072, Watkins2024highperformance,10.1145/3466752.3480072}. For example, in \cite{Litinski2019gameofsurfacecodes}, the entire quantum circuit is decomposed into a set of Pauli-product rotations and measurements. The method uses commutation rules between any two Pauli gates to reduce a quantum program down to a set of multi-qubit non-Clifford operations and multi-qubit measurements. This method offers a promising solution for circuits with a large set of operations, as it manages to ultimately decompose them into a single set of Pauli-product operations. However, the latency (and increased qubit cost) of implementing multi-qubit Pauli product operations in different block layouts is not evaluated \cite{Moflic:2024xhe}. Hence, it does not provide a complete picture of the compilation procedure that takes into account the physical constraints of performing large multi-qubit Pauli product operations. In other works such as \cite{10.1145/3720416, PRXQuantum.3.020342}, methods are described to optimise paths implementing two-qubit operations and for routing magic states. These methods help define the depth of a particular circuit for certain layouts, which is crucial for performing 2-qubit gates. However, certain bottlenecks, such as distillation processing time, are not accounted for. Other NISQ compilers, such as \cite{10.1145/3470496.3527394, 10.1145/3297858.3304023}, describe techniques for reducing computational overhead but do not address challenges related to distillation routing.","Given that optimising surface code circuits is known to be NP-hard \cite{Herr2017}, a variety of optimisation techniques at different levels of abstraction have emerged in recent years \cite{Litinski2019gameofsurfacecodes,10.1145/3720416,10.1109/MICRO.2018.00072,10.1145/3466752.3480072, Watkins2024highperformance,10.1145/3466752.3480072}. For example, in \cite{Litinski2019gameofsurfacecodes}, the entire quantum circuit is decomposed into a set of Pauli-product rotations and measurements. The method uses commutation rules between any two Pauli gates to reduce a quantum program down to a set of multi-qubit non-Clifford operations and multi-qubit measurements. This method offers a promising solution for circuits with a large set of operations, as it manages to ultimately decompose them into a single set of Pauli-product operations. However, the latency (and increased qubit cost) of implementing multi-qubit Pauli product operations in different block layouts is not evaluated \cite{Moflic:2024xhe}. Hence, it does not provide a complete picture of the compilation procedure that takes into account the physical constraints of performing large multi-qubit Pauli product operations. In other works such as \cite{10.1145/3720416, PRXQuantum.3.020342}, methods are described to optimise paths implementing two-qubit operations and for routing magic states. These methods help define the depth of a particular circuit for certain layouts, which is crucial for performing 2-qubit gates. However, certain bottlenecks, such as distillation processing time, are not accounted for. Other NISQ compilers, such as \cite{10.1145/3470496.3527394, 10.1145/3297858.3304023}, describe techniques for reducing computational overhead but do not address challenges related to distillation routing.",
2511.05629v1,http://arxiv.org/abs/2511.05629v1,2025-11-07 03:43:53+00:00,SSTODE: Ocean-Atmosphere Physics-Informed Neural ODEs for Sea Surface Temperature Prediction,"Sea Surface Temperature (SST) is crucial for understanding upper-ocean thermal dynamics and ocean-atmosphere interactions, which have profound economic and social impacts. While data-driven models show promise in SST prediction, their black-box nature often limits interpretability and overlooks key physical processes. Recently, physics-informed neural networks have been gaining momentum but struggle with complex ocean-atmosphere dynamics due to 1) inadequate characterization of seawater movement (e.g., coastal upwelling) and 2) insufficient integration of external SST drivers (e.g., turbulent heat fluxes). To address these challenges, we propose SSTODE, a physics-informed Neural Ordinary Differential Equations (Neural ODEs) framework for SST prediction. First, we derive ODEs from fluid transport principles, incorporating both advection and diffusion to model ocean spatiotemporal dynamics. Through variational optimization, we recover a latent velocity field that explicitly governs the temporal dynamics of SST. Building upon ODE, we introduce an Energy Exchanges Integrator (EEI)-inspired by ocean heat budget equations-to account for external forcing factors. Thus, the variations in the components of these factors provide deeper insights into SST dynamics. Extensive experiments demonstrate that SSTODE achieves state-of-the-art performances in global and regional SST forecasting benchmarks. Furthermore, SSTODE visually reveals the impact of advection dynamics, thermal diffusion patterns, and diurnal heating-cooling cycles on SST evolution. These findings demonstrate the model's interpretability and physical consistency.","\subsection{Ocean Parameters Prediction Models}
Numerical ocean models traditionally simulate ocean dynamics using physical equations but suffer from high computational cost and limited adaptability to dynamic external forcings \citep{veeresha2021numerical}.
Deep models for spatiotemporal forecasting, initially developed for computer vision tasks such as video and traffic prediction, have been extended to SST prediction using both recurrent-based (e.g., ConvLSTM~\citep{lin2020self}, Transformers~\citep{tang2024vmrnn}) and recurrent-free architectures (e.g., SimVPv2~\citep{tan2025simvpv2}, COTERE~\citep{shi2024oceanvp}). Large models have recently emerged as a promising paradigm in ocean and climate science, e.g., AI-GOMS \citep{xiong2023ai}, XiHe \citep{wang2024xihe}, and WenHai \citep{cui2025forecasting}, which leverage massive datasets and millions of parameters to improve prediction accuracy. While these DL models achieve impressive performance, their lack of explicit physical constraints limits their interpretability and thermodynamic consistency. 
Meanwhile, they are typically trained on reanalysis data sampled at fixed, discrete time intervals (e.g., daily), overlooking the fact that the seawater continuously transports heat and redistributes mass while surface fluxes add or remove energy. This discreteness violates mass conservation and introduces approximation errors. In contrast, our work focuses on ensuring physical interpretability by preserving intermediate states and enabling earlier anomaly detection through continuous-time forecasting.

\begin{figure*}[!t]
  \centering
  % \rule{\textwidth}{6cm} % 宽度自动等于文本宽度，高度你自己定
  \includegraphics[width=\textwidth]{fig/SSTODE.png}
  % \vspace{-10pt}
  \caption{
  SSTODE framework. It comprises three modules: (1) Initial Velocity Estimation infers latent initial velocity from past SST by solving a PDE-constrained inverse problem; (2) SST-ODE integrates the advection–diffusion equation using Neural ODEs to prediction SST and latent velocity over continuous time; and (3) Energy Exchange Integrator (EEI) refines predictions using surface heat flux data via a learned source network. Spatiotemporal Embeddings (ST Embedding) encode position and time context. $\otimes$: concatenation. $\oplus$: element-wise addition.}
  \label{fig:framework}
\end{figure*}

\subsection{Physic-Informed Neural Networks}
Recent PINNs aim to combine data-driven learning with physical priors to improve prediction accuracy and physical plausibility \citep{xu2024generalizing, li2024deepphysinet}. A straightforward way is to embed the governing partial differential equations (PDEs) in the loss function to enforce physical laws~\citep{ghosh2023rans}. However, these methods often require full observation of key variables to close the equations, which is challenging for advection-diffusion systems because the requisite velocity fields are typically unavailable from satellite observations \citep{sun2020physics}.

Neural ODEs extend PINNs by parameterizing time derivatives with neural networks and integrating dynamics from initial conditions in continuous time~\citep{chen2018neural, chu2024adaptive}. This framework has shown success in modeling physical systems such as fluid dynamics~\citep{rojas2021reduced, tian2025air} and climate processes~\citep{hwang2021climate, verma2024climode}. For example, ClimODE embeds the advection equation to model atmospheric spatiotemporal patterns, achieving promising results in weather and climate prediction \citep{verma2024climode}. However, these methods often assume idealized advection in unbounded, compressible atmospheric settings, making them suboptimal for ocean modeling. Specifically, they can not adequately capture key ocean-specific dynamics: (1) intense thermal diffusion near coastlines from turbulence and eddies, (2) incompressible fluid assumptions unique to seawater, and (3) external forcings of surface energy exchange (e.g., radiative and turbulent heat fluxes). Our model addresses this limitation by explicitly integrating coupled advection-diffusion and surface energy exchange patterns into Neural ODE, enabling improved accuracy and realism of SST modeling in the upper open ocean.","\subsection{Ocean Parameters Prediction Models}
Numerical ocean models traditionally simulate ocean dynamics using physical equations but suffer from high computational cost and limited adaptability to dynamic external forcings \citep{veeresha2021numerical}.
Deep models for spatiotemporal forecasting, initially developed for computer vision tasks such as video and traffic prediction, have been extended to SST prediction using both recurrent-based (e.g., ConvLSTM~\citep{lin2020self}, Transformers~\citep{tang2024vmrnn}) and recurrent-free architectures (e.g., SimVPv2~\citep{tan2025simvpv2}, COTERE~\citep{shi2024oceanvp}). Large models have recently emerged as a promising paradigm in ocean and climate science, e.g., AI-GOMS \citep{xiong2023ai}, XiHe \citep{wang2024xihe}, and WenHai \citep{cui2025forecasting}, which leverage massive datasets and millions of parameters to improve prediction accuracy. While these DL models achieve impressive performance, their lack of explicit physical constraints limits their interpretability and thermodynamic consistency. 
Meanwhile, they are typically trained on reanalysis data sampled at fixed, discrete time intervals (e.g., daily), overlooking the fact that the seawater continuously transports heat and redistributes mass while surface fluxes add or remove energy. This discreteness violates mass conservation and introduces approximation errors. In contrast, our work focuses on ensuring physical interpretability by preserving intermediate states and enabling earlier anomaly detection through continuous-time forecasting.



\subsection{Physic-Informed Neural Networks}
Recent PINNs aim to combine data-driven learning with physical priors to improve prediction accuracy and physical plausibility \citep{xu2024generalizing, li2024deepphysinet}. A straightforward way is to embed the governing partial differential equations (PDEs) in the loss function to enforce physical laws~\citep{ghosh2023rans}. However, these methods often require full observation of key variables to close the equations, which is challenging for advection-diffusion systems because the requisite velocity fields are typically unavailable from satellite observations \citep{sun2020physics}.

Neural ODEs extend PINNs by parameterizing time derivatives with neural networks and integrating dynamics from initial conditions in continuous time~\citep{chen2018neural, chu2024adaptive}. This framework has shown success in modeling physical systems such as fluid dynamics~\citep{rojas2021reduced, tian2025air} and climate processes~\citep{hwang2021climate, verma2024climode}. For example, ClimODE embeds the advection equation to model atmospheric spatiotemporal patterns, achieving promising results in weather and climate prediction \citep{verma2024climode}. However, these methods often assume idealized advection in unbounded, compressible atmospheric settings, making them suboptimal for ocean modeling. Specifically, they can not adequately capture key ocean-specific dynamics: (1) intense thermal diffusion near coastlines from turbulence and eddies, (2) incompressible fluid assumptions unique to seawater, and (3) external forcings of surface energy exchange (e.g., radiative and turbulent heat fluxes). Our model addresses this limitation by explicitly integrating coupled advection-diffusion and surface energy exchange patterns into Neural ODE, enabling improved accuracy and realism of SST modeling in the upper open ocean.","Ocean Parameters Prediction Models
Numerical ocean models traditionally simulate ocean dy-
namics using physical equations but suffer from high com-
putational cost and limited adaptability to dynamic external
forcings (Veeresha 2021). Deep models for spatiotemporal
forecasting, initially developed for computer vision tasks
such as video and traffic prediction, have been extended
to SST prediction using both recurrent-based (e.g., ConvL-
STM (Lin et al. 2020), Transformers (Tang et al. 2024)) and
recurrent-free architectures (e.g., SimVPv2 (Tan et al. 2025),
COTERE (Shi, Zheng, and Dong 2024)). Large models have
recently emerged as a promising paradigm in ocean and cli-
mate science, e.g., AI-GOMS (Xiong et al. 2023), XiHe
(Wang et al. 2024), and WenHai (Cui et al. 2025), which
leverage massive datasets and millions of parameters to im-
prove prediction accuracy. While these DL models achieve
impressive performance, their lack of explicit physical con-
straints limits their interpretability and thermodynamic con-
sistency. Meanwhile, they are typically trained on reanalysis
data sampled at fixed, discrete time intervals (e.g., daily),
overlooking the fact that the seawater continuously trans-
ports heat and redistributes mass while surface fluxes add or
remove energy. This discreteness violates mass conservation
and introduces approximation errors. In contrast, our work
focuses on ensuring physical interpretability by preserving
intermediate states and enabling earlier anomaly detection
through continuous-time forecasting.
Physic-Informed Neural Networks
Recent PINNs aim to combine data-driven learning with
physical priors to improve prediction accuracy and physi-
cal plausibility (Xu et al. 2024; Li et al. 2024a). A straight-
forward way is to embed the governing partial differential
equations (PDEs) in the loss function to enforce physical
laws (Ghosh et al. 2023). However, these methods often re-
quire full observation of key variables to close the equa-
tions, which is challenging for advection-diffusion systems
because the requisite velocity fields are typically unavailable
from satellite observations (Sun and Wang 2020).
Neural ODEs extend PINNs by parameterizing time
derivatives with neural networks and integrating dynamics
from initial conditions in continuous time (Chen et al. 2018;
Chu et al. 2024). This framework has shown success in
modeling physical systems such as fluid dynamics (Rojas,
Figure 2: SSTODE framework. It comprises three modules: (1) Initial Velocity Estimation infers latent initial velocity from past
SST by solving a PDE-constrained inverse problem; (2) SST-ODE integrates the advection–diffusion equation using Neural
ODEs to prediction SST and latent velocity over continuous time; and (3) Energy Exchange Integrator (EEI) refines predictions
using surface heat flux data via a learned source network. Spatiotemporal Embeddings (ST Embedding) encode position and
time context.⊗: concatenation.⊕: element-wise addition.
Dengel, and Ribeiro 2021; Tian et al. 2024) and climate
processes (Hwang et al. 2021; Verma, Heinonen, and Garg
2024). For example, ClimODE embeds the advection equa-
tion to model atmospheric spatiotemporal patterns, achiev-
ing promising results in weather and climate prediction
(Verma, Heinonen, and Garg 2024). However, these meth-
ods often assume idealized advection in unbounded, com-
pressible atmospheric settings, making them suboptimal for
ocean modeling. Specifically, they can not adequately cap-
ture key ocean-specific dynamics: (1) intense thermal dif-
fusion near coastlines from turbulence and eddies, (2) in-
compressible fluid assumptions unique to seawater, and (3)
external forcings of surface energy exchange (e.g., radiative
and turbulent heat fluxes). Our model addresses this limi-
tation by explicitly integrating coupled advection-diffusion
and surface energy exchange patterns into Neural ODE, en-
abling improved accuracy and realism of SST modeling in
the upper open ocean."
2511.01019v2,http://arxiv.org/abs/2511.01019v2,2025-11-02 17:23:58+00:00,"OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights","Artificial intelligence is transforming the sciences, yet general conversational AI systems often generate unverified ""hallucinations"" undermining scientific rigor. We present OceanAI, a conversational platform that integrates the natural-language fluency of open-source large language models (LLMs) with real-time, parameterized access to authoritative oceanographic data streams hosted by the National Oceanic and Atmospheric Administration (NOAA). Each query such as ""What was Boston Harbor's highest water level in 2024?"" triggers real-time API calls that identify, parse, and synthesize relevant datasets into reproducible natural-language responses and data visualizations. In a blind comparison with three widely used AI chat-interface products, only OceanAI produced NOAA-sourced values with original data references; others either declined to answer or provided unsupported results. Designed for extensibility, OceanAI connects to multiple NOAA data products and variables, supporting applications in marine hazard forecasting, ecosystem assessment, and water-quality monitoring. By grounding outputs and verifiable observations, OceanAI advances transparency, reproducibility, and trust, offering a scalable framework for AI-enabled decision support within the oceans. A public demonstration is available at https://oceanai.ai4ocean.xyz.","\label{sec:related_work}
\vspace{-5pt}
\textbf{LLMs in Scientific Domains.} A central challenge in applying large language models (LLMs) to scientific domains lies in ensuring factual accuracy, reproducibility, and timeliness~\citep{ji2023survey}. Although LLMs excel in language fluency and general reasoning, their knowledge is static and often opaque, making it difficult to verify or update information. This limitation is critical for scientific applications, where outputs must be grounded in authoritative, up-to-date datasets and follow domain-specific standards. In climate and Earth sciences, for example, models that cannot handle structured formats such as NetCDF or GRIB risk misinterpretation of quantitative results. Recent work in biomedical~\citep{singhal2023large} and materials science~\citep{mostafa2024grag} has similarly shown that domain adaptation is essential for reliable use in high-stakes research contexts.

\textbf{Tool Augmentation and Retrieval-Augmented Generation.} A widely studied mitigation strategy is \textit{tool augmentation}, where LLMs invoke external tools for information retrieval or computation~\citep{schick2023toolformer}. Toolformer and related agent-based systems such as HuggingGPT~\citep{shen2023hugginggpt} demonstrate that models can be trained to autonomously call APIs such as search engines, calculators, or code execution environments, improving factuality. Retrieval-augmented generation (RAG) further enhances this by grounding responses in retrieved documents, as seen in WebGPT~\citep{nakano2022webgpt}. However, these frameworks primarily operate on unstructured text and lack native support for specialized scientific data formats or spatiotemporal datasets.

\textbf{LLMs for Earth and Ocean Sciences.} In the Earth sciences, LLM integration is still nascent. Domain-specific models such as \textit{ClimateGPT}~\citep{thulke2024climategpt} and \textit{GeoGalactica}~\citep{lin2024geoga} have shown promise in processing literature and reports, but they do not incorporate real-time observational data streams. In the ocean domain, \textit{OceanGPT}~\citep{bi2024oceangpt} fine-tunes large language models on a multi-agent–generated instruction dataset and introduces OCEANBENCH for benchmarking 15 ocean science tasks. While it improves domain-specific reasoning compared with general LLMs, it operates primarily on static corpora and lacks real-time integration with authoritative observational datasets, limiting its applicability for time-sensitive reporting.A recent study further introduced the Intelligent Data Exploring Assistant (IDEA), which integrates LLMs with domain-specific geoscience data and analytical tools~\citep{Widlansky2025}.

% \textbf{Our Contribution.} 
\textbf{The Novelty of Our Method.} \textbf{OceanAI} addresses these limitations by combining natural language interfaces with a domain-specialized function-calling architecture that retrieves, preprocesses, and visualizes live data from trusted providers such as NOAA. This design ensures outputs are not only contextually accurate but also up-to-date, transparent, and fully reproducible, making it suitable for operational coastal and oceanographic monitoring.","\vspace{-5pt}
\textbf{LLMs in Scientific Domains.} A central challenge in applying large language models (LLMs) to scientific domains lies in ensuring factual accuracy, reproducibility, and timeliness~\citep{ji2023survey}. Although LLMs excel in language fluency and general reasoning, their knowledge is static and often opaque, making it difficult to verify or update information. This limitation is critical for scientific applications, where outputs must be grounded in authoritative, up-to-date datasets and follow domain-specific standards. In climate and Earth sciences, for example, models that cannot handle structured formats such as NetCDF or GRIB risk misinterpretation of quantitative results. Recent work in biomedical~\citep{singhal2023large} and materials science~\citep{mostafa2024grag} has similarly shown that domain adaptation is essential for reliable use in high-stakes research contexts.

\textbf{Tool Augmentation and Retrieval-Augmented Generation.} A widely studied mitigation strategy is \textit{tool augmentation}, where LLMs invoke external tools for information retrieval or computation~\citep{schick2023toolformer}. Toolformer and related agent-based systems such as HuggingGPT~\citep{shen2023hugginggpt} demonstrate that models can be trained to autonomously call APIs such as search engines, calculators, or code execution environments, improving factuality. Retrieval-augmented generation (RAG) further enhances this by grounding responses in retrieved documents, as seen in WebGPT~\citep{nakano2022webgpt}. However, these frameworks primarily operate on unstructured text and lack native support for specialized scientific data formats or spatiotemporal datasets.

\textbf{LLMs for Earth and Ocean Sciences.} In the Earth sciences, LLM integration is still nascent. Domain-specific models such as \textit{ClimateGPT}~\citep{thulke2024climategpt} and \textit{GeoGalactica}~\citep{lin2024geoga} have shown promise in processing literature and reports, but they do not incorporate real-time observational data streams. In the ocean domain, \textit{OceanGPT}~\citep{bi2024oceangpt} fine-tunes large language models on a multi-agent–generated instruction dataset and introduces OCEANBENCH for benchmarking 15 ocean science tasks. While it improves domain-specific reasoning compared with general LLMs, it operates primarily on static corpora and lacks real-time integration with authoritative observational datasets, limiting its applicability for time-sensitive reporting.A recent study further introduced the Intelligent Data Exploring Assistant (IDEA), which integrates LLMs with domain-specific geoscience data and analytical tools~\citep{Widlansky2025}.


\textbf{The Novelty of Our Method.} \textbf{OceanAI} addresses these limitations by combining natural language interfaces with a domain-specialized function-calling architecture that retrieves, preprocesses, and visualizes live data from trusted providers such as NOAA. This design ensures outputs are not only contextually accurate but also up-to-date, transparent, and fully reproducible, making it suitable for operational coastal and oceanographic monitoring.","LLMs in Scientific Domains.A central challenge in applying large language models (LLMs) to
scientific domains lies in ensuring factual accuracy, reproducibility, and timeliness [Ji et al., 2023].
Although LLMs excel in language fluency and general reasoning, their knowledge is static and often
opaque, making it difficult to verify or update information. This limitation is critical for scientific
applications, where outputs must be grounded in authoritative, up-to-date datasets and follow domain-
specific standards. In climate and Earth sciences, for example, models that cannot handle structured
formats such as NetCDF or GRIB risk misinterpretation of quantitative results. Recent work in
biomedical [Singhal et al., 2023] and materials science [Mostafa et al., 2024] has similarly shown
that domain adaptation is essential for reliable use in high-stakes research contexts.
Tool Augmentation and Retrieval-Augmented Generation.A widely studied mitigation strategy is
tool augmentation, where LLMs invoke external tools for information retrieval or computation [Schick
et al., 2023]. Toolformer and related agent-based systems such as HuggingGPT [Shen et al., 2023]
demonstrate that models can be trained to autonomously call APIs such as search engines, calculators,
or code execution environments, improving factuality. Retrieval-augmented generation (RAG) further
enhances this by grounding responses in retrieved documents, as seen in WebGPT [Nakano et al.,
2022]. However, these frameworks primarily operate on unstructured text and lack native support for
specialized scientific data formats or spatiotemporal datasets.
LLMs for Earth and Ocean Sciences.In the Earth sciences, LLM integration is still nascent.
Domain-specific models such asClimateGPT[Thulke et al., 2024] andGeoGalactica[Lin et al.,
2024] have shown promise in processing literature and reports, but they do not incorporate real-time
observational data streams. In the ocean domain,OceanGPT[Bi et al., 2024] fine-tunes large
language models on a multi-agent–generated instruction dataset and introduces OCEANBENCH
for benchmarking 15 ocean science tasks. While it improves domain-specific reasoning compared
with general LLMs, it operates primarily on static corpora and lacks real-time integration with
authoritative observational datasets, limiting its applicability for time-sensitive reporting.A recent
study further introduced the Intelligent Data Exploring Assistant (IDEA), which integrates LLMs
with domain-specific geoscience data and analytical tools [Widlansky and Komar, 2025].
The Novelty of Our Method. OceanAIaddresses these limitations by combining natural language
interfaces with a domain-specialized function-calling architecture that retrieves, preprocesses, and
visualizes live data from trusted providers such as NOAA. This design ensures outputs are not only
contextually accurate but also up-to-date, transparent, and fully reproducible, making it suitable for
operational coastal and oceanographic monitoring."
2511.01052v1,http://arxiv.org/abs/2511.01052v1,2025-11-02 19:00:40+00:00,Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports,"Cancer staging is critical for patient prognosis and treatment planning, yet extracting pathologic TNM staging from unstructured pathology reports poses a persistent challenge. Existing natural language processing (NLP) and machine learning (ML) strategies often depend on large annotated datasets, limiting their scalability and adaptability. In this study, we introduce two Knowledge Elicitation methods designed to overcome these limitations by enabling large language models (LLMs) to induce and apply domain-specific rules for cancer staging. The first, Knowledge Elicitation with Long-Term Memory (KEwLTM), uses an iterative prompting strategy to derive staging rules directly from unannotated pathology reports, without requiring ground-truth labels. The second, Knowledge Elicitation with Retrieval-Augmented Generation (KEwRAG), employs a variation of RAG where rules are pre-extracted from relevant guidelines in a single step and then applied, enhancing interpretability and avoiding repeated retrieval overhead. We leverage the ability of LLMs to apply broad knowledge learned during pre-training to new tasks. Using breast cancer pathology reports from the TCGA dataset, we evaluate their performance in identifying T and N stages, comparing them against various baseline approaches on two open-source LLMs. Our results indicate that KEwLTM outperforms KEwRAG when Zero-Shot Chain-of-Thought (ZSCOT) inference is effective, whereas KEwRAG achieves better performance when ZSCOT inference is less effective. Both methods offer transparent, interpretable interfaces by making the induced rules explicit. These findings highlight the promise of our Knowledge Elicitation methods as scalable, high-performing solutions for automated cancer staging with enhanced interpretability, particularly in clinical settings with limited annotated data.","\label{sec2}
In recent years, natural language processing (NLP) and machine learning (ML) have been instrumental in developing automated systems for extracting cancer stage information from free-text pathology reports. Odisho et al.~\cite{Odisho2020-vy} utilized contextual token embeddings with logistic regression, AdaBoost, and random forests for pathologic stage classification. Angeli et al.~\cite{De_Angeli2021-ro} applied active learning to select training samples and then trained a convolutional neural network for stage identification. Gao et al.~\cite{Gao2018-ex, Gao2019-qc} developed a hierarchical network to learn representations from words to complete reports, demonstrating its effectiveness in tumor grade classification using SEER data. Wu et al.~\cite{Wu2020-du} leveraged attention-based graph convolution networks, using multi-source knowledge graphs for improved TNM stage identification on TCGA data.

However, the adaptability of these ML and deep learning models remains limited, particularly because they rely on labeled datasets, which are often expensive to produce and constrained in scope. This limitation makes it difficult for these models to adapt to data that significantly differ from their training datasets, such as the diverse formats used in pathology reports across different medical facilities and the varying rules for different cancer types.

Foundation models, such as pre-trained language models, are designed to be more adaptable by training on vast amounts of unstructured data~\cite{Vaswani2017-gk}. This large-scale, raw corpus enables them to learn from diverse, real-world scenarios without the need for labor-intensive labeling. Their broad exposure allows them to encode a wide array of knowledge across their vast parameter space~\cite{Carlini2023-ys}, equipping them to handle novel tasks and complex real-world data more readily, making them powerful tools for various healthcare applications.

Kefeli et al.~\cite{Kefeli2023-ec} leveraged the power of pre-trained language models by fine-tuning a clinical-specific model, Clinical-BigBird~\cite{Li2022-lw}, for TNM classification using reports from the TCGA project. Their fine-tuned model achieved strong performance both on the TCGA test reports and on an independent set of real-world pathology reports, collected from Columbia University Irving
Medical Center. Furthermore, Chang et al.~\cite{Chang2024-qd} investigated open-source clinical large language model (Med42-70B~\cite{Christophe2024-wz}) in cancer stage identification. They found that adopting the zero-shot chain-of-thought (ZSCOT) prompting approach, without requiring any training samples, reached comparable performance in TNM classification task to fine-tuned BERT-based models. Recently, Chang et al.~\cite{Chang2024-qu} introduced a new iterative prompting workflow called ensemble reasoning (EnsReas) which aims to enhance predictive performance and consistency by extending the ZSCOT approach. While promising for improving performance and consistency, EnsReas requires a high volume of API calls to the LLM, making it inefficient and costly. 

%% <MODIFICATION START>
Our current study explores two efficient prompting approaches that, while building on concepts like iterative prompting and RAG, introduce specific adaptations to induce domain-specific knowledge and incorporate it into the LLM’s generated reasoning. These adaptations aim to enhance predictive performance and interpretability in ways that differ from standard applications. Specifically, KEwLTM derives knowledge through a label-free iterative rule induction process directly from a small number of unannotated pathology reports, storing it in long-term memory. This contrasts with many iterative prompting methods that may still require some form of labeled data or explicit external feedback. KEwRAG, on the other hand, modifies the typical RAG process by retrieving relevant information once at the outset and then prompting the LLM to synthesize this into an explicit, interpretable set of rules, which are then applied throughout the inference process. This differs from standard RAG that often appends raw retrieved text per query. Both methods prioritize the generation of an explicit, reviewable rule set.
%% <MODIFICATION END>","In recent years, natural language processing (NLP) and machine learning (ML) have been instrumental in developing automated systems for extracting cancer stage information from free-text pathology reports. Odisho et al.~\cite{Odisho2020-vy} utilized contextual token embeddings with logistic regression, AdaBoost, and random forests for pathologic stage classification. Angeli et al.~\cite{De_Angeli2021-ro} applied active learning to select training samples and then trained a convolutional neural network for stage identification. Gao et al.~\cite{Gao2018-ex, Gao2019-qc} developed a hierarchical network to learn representations from words to complete reports, demonstrating its effectiveness in tumor grade classification using SEER data. Wu et al.~\cite{Wu2020-du} leveraged attention-based graph convolution networks, using multi-source knowledge graphs for improved TNM stage identification on TCGA data.

However, the adaptability of these ML and deep learning models remains limited, particularly because they rely on labeled datasets, which are often expensive to produce and constrained in scope. This limitation makes it difficult for these models to adapt to data that significantly differ from their training datasets, such as the diverse formats used in pathology reports across different medical facilities and the varying rules for different cancer types.

Foundation models, such as pre-trained language models, are designed to be more adaptable by training on vast amounts of unstructured data~\cite{Vaswani2017-gk}. This large-scale, raw corpus enables them to learn from diverse, real-world scenarios without the need for labor-intensive labeling. Their broad exposure allows them to encode a wide array of knowledge across their vast parameter space~\cite{Carlini2023-ys}, equipping them to handle novel tasks and complex real-world data more readily, making them powerful tools for various healthcare applications.

Kefeli et al.~\cite{Kefeli2023-ec} leveraged the power of pre-trained language models by fine-tuning a clinical-specific model, Clinical-BigBird~\cite{Li2022-lw}, for TNM classification using reports from the TCGA project. Their fine-tuned model achieved strong performance both on the TCGA test reports and on an independent set of real-world pathology reports, collected from Columbia University Irving
Medical Center. Furthermore, Chang et al.~\cite{Chang2024-qd} investigated open-source clinical large language model (Med42-70B~\cite{Christophe2024-wz}) in cancer stage identification. They found that adopting the zero-shot chain-of-thought (ZSCOT) prompting approach, without requiring any training samples, reached comparable performance in TNM classification task to fine-tuned BERT-based models. Recently, Chang et al.~\cite{Chang2024-qu} introduced a new iterative prompting workflow called ensemble reasoning (EnsReas) which aims to enhance predictive performance and consistency by extending the ZSCOT approach. While promising for improving performance and consistency, EnsReas requires a high volume of API calls to the LLM, making it inefficient and costly. 


Our current study explores two efficient prompting approaches that, while building on concepts like iterative prompting and RAG, introduce specific adaptations to induce domain-specific knowledge and incorporate it into the LLM’s generated reasoning. These adaptations aim to enhance predictive performance and interpretability in ways that differ from standard applications. Specifically, KEwLTM derives knowledge through a label-free iterative rule induction process directly from a small number of unannotated pathology reports, storing it in long-term memory. This contrasts with many iterative prompting methods that may still require some form of labeled data or explicit external feedback. KEwRAG, on the other hand, modifies the typical RAG process by retrieving relevant information once at the outset and then prompting the LLM to synthesize this into an explicit, interpretable set of rules, which are then applied throughout the inference process. This differs from standard RAG that often appends raw retrieved text per query. Both methods prioritize the generation of an explicit, reviewable rule set.","In recent years, natural language processing (NLP) and machine learning (ML) have
been instrumental in developing automated systems for extracting cancer stage infor-
mation from free-text pathology reports. Odisho et al. [1] utilized contextual token
embeddings with logistic regression, AdaBoost, and random forests for pathologic
stage classification. Angeli et al. [2] applied active learning to select training sam-
ples and then trained a convolutional neural network for stage identification. Gao
et al. [3, 4] developed a hierarchical network to learn representations from words to
complete reports, demonstrating its effectiveness in tumor grade classification using
SEER data. Wu et al. [5] leveraged attention-based graph convolution networks, using
multi-source knowledge graphs for improved TNM stage identification on TCGA data.
However, the adaptability of these ML and deep learning models remains limited,
particularly because they rely on labeled datasets, which are often expensive to pro-
duce and constrained in scope. This limitation makes it difficult for these models to
adapt to data that significantly differ from their training datasets, such as the diverse
3
formats used in pathology reports across different medical facilities and the varying
rules for different cancer types.
Foundation models, such as pre-trained language models, are designed to be more
adaptable by training on vast amounts of unstructured data [6]. This large-scale, raw
corpus enables them to learn from diverse, real-world scenarios without the need for
labor-intensive labeling. Their broad exposure allows them to encode a wide array of
knowledge across their vast parameter space [7], equipping them to handle novel tasks
and complex real-world data more readily, making them powerful tools for various
healthcare applications.
Kefeli et al. [10] leveraged the power of pre-trained language models by fine-tuning
a clinical-specific model, Clinical-BigBird [11], for TNM classification using reports
from the TCGA project. Their fine-tuned model achieved strong performance both
on the TCGA test reports and on an independent set of real-world pathology reports,
collected from Columbia University Irving Medical Center. Furthermore, Chang et
al. [8] investigated open-source clinical large language model (Med42-70B [12]) in
cancer stage identification. They found that adopting the zero-shot chain-of-thought
(ZSCOT) prompting approach, without requiring any training samples, reached com-
parable performance in TNM classification task to fine-tuned BERT-based models.
Recently, Chang et al. [9] introduced a new iterative prompting workflow called
ensemble reasoning (EnsReas) which aims to enhance predictive performance and
consistency by extending the ZSCOT approach. While promising for improving per-
formance and consistency, EnsReas requires a high volume of API calls to the LLM,
making it inefficient and costly.
Our current study explores two efficient prompting approaches that, while build-
ing on concepts like iterative prompting and RAG, introduce specific adaptations to
induce domain-specific knowledge and incorporate it into the LLM’s generated reason-
ing. These adaptations aim to enhance predictive performance and interpretability in
ways that differ from standard applications. Specifically, KEwLTM derives knowledge
through a label-free iterative rule induction process directly from a small number of
unannotated pathology reports, storing it in long-term memory. This contrasts with
many iterative prompting methods that may still require some form of labeled data
or explicit external feedback. KEwRAG, on the other hand, modifies the typical RAG
process by retrieving relevant information once at the outset and then prompting the
LLM to synthesize this into an explicit, interpretable set of rules, which are then
applied throughout the inference process. This differs from standard RAG that often
appends raw retrieved text per query. Both methods prioritize the generation of an
explicit, reviewable rule set."
2511.04967v2,http://arxiv.org/abs/2511.04967v2,2025-11-07 04:00:24+00:00,Hybrid action Reinforcement Learning for quantum architecture search,"Designing expressive yet trainable quantum circuit architectures remains a major challenge for variational quantum algorithms, as manual or heuristic designs often yield suboptimal performance. We propose HyRLQAS (Hybrid-Action Reinforcement Learning for Quantum Architecture Search), a unified framework that integrates discrete gate placement and continuous parameter generation within a hybrid action space. Unlike existing approaches that optimize circuit structure and parameters separately, HyRLQAS jointly learns both topology and initialization while dynamically refining previously placed gates through reinforcement learning. Trained in a variational quantum eigensolver (VQE) environment, the agent autonomously constructs circuits that minimize molecular ground-state energy. Experimental results demonstrate that HyRLQAS achieves consistently lower energy errors and more compact circuit structures compared with discrete-only and continuous-only baselines. Furthermore, the hybrid action space yields superior parameter initializations, producing post-optimization energy distributions with consistently lower minima. These findings suggest that hybrid-action reinforcement learning offers a principled pathway toward automated and hardware-efficient quantum circuit design.","Reinforcement learning~(\citet{mcclean2018barren, van2016deep}) has been increasingly explored for quantum architecture search (QAS), where task-specific reward signals are derived from variational quantum algorithms (VQAs) such as VQE, state preparation, or combinatorial optimization~(\citet{khairy2020learning}). In the context of VQE,~(\citet{ostaszewski2021reinforcement}) were the first to introduce curriculum learning into RL-based ansatz search, enabling agents to progressively construct circuits. Building on this line,~(\citet{patel2024curriculum}) proposed CRLQAS, which augments curriculum learning with structured state encodings, constraints on illegal actions, and a new external optimizer to improve sample efficiency and robustness under noise. Beyond VQE, reinforcement learning~(\citet{dutta2025qas, ikhtiarudin2025benchrl}) has also been applied to QAS with alternative metrics such as fidelity or task-specific costs, enabling broader applications in classification and optimization. More recently, scalability concerns have been addressed by~(\citet{kundu2025tensorrl}), who proposed TensorRL-QAS that integrates tensor-network representations to compress the state space and extend applicability to larger qubit systems.

In parallel, another line of research improves scalability by structuring the search space with reusable subcircuits (gadgets or blocks). Examples include block-based ansatz construction for combinatorial optimization~(\citet{turati2025automated}), gadget reinforcement learning with composite gates~(\citet{kundu2024reinforcement}), and systematic discovery of Clifford gadgets to accelerate RL agents~(\citet{olle2025scaling}). Beyond circuit architecture search, RL has also been employed for circuit optimization. For instance, Quarl~\citet{li2024quarl} leverages graph neural networks and a structured action space to guide optimization decisions, significantly outperforming classical optimizers on benchmark circuits. In addition, non-RL frameworks have been developed to reduce computational cost, such as the hardware-tested QAS of \citet{du2022quantum} and the training-free QAS of \citet{he2024training}, which rely on proxy-based evaluations instead of circuit training.

Recent studies have increasingly focused on efficiency-enhancing strategies for QAS, such as leveraging structured subcircuits or compressing state representations, yet most still treat gate choices and parameter settings as separate processes. By contrast, the RL literature has systematically explored hybrid action spaces, where discrete actions and continuous parameters are modeled jointly. Early work by~(\citet{hausknecht2015deep}) introduced deep reinforcement learning in parameterized action spaces, followed by \citet{fan2019hybrid}, who proposed a hybrid actor-critic architecture to decompose structured actions into sub-actors for more efficient training. More recently,~(\citet{lihyar}) advanced this line by introducing Hybrid Action Representation (HyAR), which learns compact latent embeddings to capture the joint structure of discrete-continuous actions. This progression establishes a methodological foundation for hybrid action reinforcement learning, offering a natural perspective for quantum circuit search where gate selection and parameter optimization are inherently coupled.","Reinforcement learning~(\citet{mcclean2018barren, van2016deep}) has been increasingly explored for quantum architecture search (QAS), where task-specific reward signals are derived from variational quantum algorithms (VQAs) such as VQE, state preparation, or combinatorial optimization~(\citet{khairy2020learning}). In the context of VQE,~(\citet{ostaszewski2021reinforcement}) were the first to introduce curriculum learning into RL-based ansatz search, enabling agents to progressively construct circuits. Building on this line,~(\citet{patel2024curriculum}) proposed CRLQAS, which augments curriculum learning with structured state encodings, constraints on illegal actions, and a new external optimizer to improve sample efficiency and robustness under noise. Beyond VQE, reinforcement learning~(\citet{dutta2025qas, ikhtiarudin2025benchrl}) has also been applied to QAS with alternative metrics such as fidelity or task-specific costs, enabling broader applications in classification and optimization. More recently, scalability concerns have been addressed by~(\citet{kundu2025tensorrl}), who proposed TensorRL-QAS that integrates tensor-network representations to compress the state space and extend applicability to larger qubit systems.

In parallel, another line of research improves scalability by structuring the search space with reusable subcircuits (gadgets or blocks). Examples include block-based ansatz construction for combinatorial optimization~(\citet{turati2025automated}), gadget reinforcement learning with composite gates~(\citet{kundu2024reinforcement}), and systematic discovery of Clifford gadgets to accelerate RL agents~(\citet{olle2025scaling}). Beyond circuit architecture search, RL has also been employed for circuit optimization. For instance, Quarl~\citet{li2024quarl} leverages graph neural networks and a structured action space to guide optimization decisions, significantly outperforming classical optimizers on benchmark circuits. In addition, non-RL frameworks have been developed to reduce computational cost, such as the hardware-tested QAS of \citet{du2022quantum} and the training-free QAS of \citet{he2024training}, which rely on proxy-based evaluations instead of circuit training.

Recent studies have increasingly focused on efficiency-enhancing strategies for QAS, such as leveraging structured subcircuits or compressing state representations, yet most still treat gate choices and parameter settings as separate processes. By contrast, the RL literature has systematically explored hybrid action spaces, where discrete actions and continuous parameters are modeled jointly. Early work by~(\citet{hausknecht2015deep}) introduced deep reinforcement learning in parameterized action spaces, followed by \citet{fan2019hybrid}, who proposed a hybrid actor-critic architecture to decompose structured actions into sub-actors for more efficient training. More recently,~(\citet{lihyar}) advanced this line by introducing Hybrid Action Representation (HyAR), which learns compact latent embeddings to capture the joint structure of discrete-continuous actions. This progression establishes a methodological foundation for hybrid action reinforcement learning, offering a natural perspective for quantum circuit search where gate selection and parameter optimization are inherently coupled.",
2511.05209v1,http://arxiv.org/abs/2511.05209v1,2025-11-07 12:57:05+00:00,CUNQA: a Distributed Quantum Computing emulator for HPC,"The challenge of scaling quantum computers to gain computational power is expected to lead to architectures with multiple connected quantum processing units (QPUs), commonly referred to as Distributed Quantum Computing (DQC). In parallel, there is a growing momentum toward treating quantum computers as accelerators, integrating them into the heterogeneous architectures of high-performance computing (HPC) environments. This work combines these two foreseeable futures in CUNQA, an open-source DQC emulator designed for HPC environments that allows testing, evaluating and studying DQC in HPC before it even becomes real. It implements the three DQC models of no-communication, classical-communication and quantum-communication; which will be examined in this work. Addressing programming considerations, explaining emulation and simulation details, and delving into the specifics of the implementation will be part of the effort. The well-known Quantum Phase Estimation (QPE) algorithm is used to demonstrate and analyze the emulation of the models. To the best of our knowledge, CUNQA is the first tool designed to emulate the three DQC schemes in an HPC environment.","\label{sec:related_work}
Several tools have been developed aiming for integrating quantum computing in HPC. It is important to, now that CUNQA has been thoroughly explained and contextualized, explain the differences with these tools in order to understand the novelty of CUNQA.

\begin{itemize}
    \item Quantum Framework (QFw)~\cite{chundury2024qfw, beck2024, shehata2024frameworkintegratingquantumsimulation, shehata2026}. The QFw tries to mimic a GPU accelerator stack, as explained in~\cite{shehata2026}. In this software stack they define two critical components: the Quantum Programming Interface (QPI) and the Quantum Platform Manager (QPM). The QPI mediates the application layer access to the middleware, where it can retrieve information about the resources available, insert compilation passes for the toolchain and more. More generally, it simplifies the customization and the interaction with the lower parts of the abstraction model, however, the QPI does not define a programming model---as Qiskit, for instance, does. On the other hand, the QPM, serves as a sort of driver, emerging as the responsible for communicating with the quantum device. It defines an API such that the QPI can interact with every quantum device indistinctively, although this is a current area of research for them.    
    \item Munich Quantum Software Stack (MQSS)~\cite{kaya2024, burgholzer2025mqss}. The MQSS software stack is very similar to QFw, to the point that they even have a so called QPI~\cite{kaya2024qpi}, even though this QPI does define a programming model in C, removing the need of using an external programming language. Moreover, they also define an API that abstracts away the specifics of the quantum devices, called quantum Quantum Device Management Interface (QDMI), which defines an interface that---as they claim in their documentation---is \textit{heavily inspired by the design of the OpenCL API for parallel programming of heterogeneous systems}. 
    \item A Co-Execution Environment for Quantum and Classical Resources (CONQURE)~\cite{mahesh2025conqure}. This work developes two tools: CONQURE cloud, which they claim is an \textit{interoperable alternative for AWS’ cloud queue for quantum devices}; and OpenMP-Q, which extends QpenMP-Q by leveraging its current support for accelerators with the aim of modifying quantum task execution in HPC. These two ways of operating with CONQURE make it a very versatile option, although it has some disadvantages, such as OpenMP-Q model's reliance on a Python script as an intermediate step for execution.
    \item QCCP~\cite{Du2025}. This tool defines a programming model that aims to \textit{shield the super-heterogeneous} nature of HPC environments nowadays, representing a development in the application layer of Figure~\ref{fig:software_stack}. This work is completely complementary to the QFw, in which they did not care about the programming model employed. 
    \item Even though it is not a tool, it is also worth mentioning the work executed by Claudino et al.~\cite{claudino2024}, in which they employed the eXtreme-scale Accelerator programming framework (XACC)\cite{mccaskey2020xacc} in combination to HPC resources to simulate, in the case of their article, the multi-contracted variant of the VQE algorithm (MC-VQE). This work showed that even though XACC is not thought to be an HPC tool explicitly, it can be used in such ways.
\end{itemize}

CUNQA distinguishes itself from all other tools in one essential aspect: it treats DQC as a central element of its design, assigning it the same level of importance as integration within HPC environments. It is, to the best of our knowledge, the only tool that explicitly addresses the emulation of DQC architectures in HPC contexts, and specifically, the first to confront the challenge of simulating quantum communication between emulated QPUs.

Structurally and conceptually, CUNQA shares similarities with QFw and MQSS. However, these works place strong emphasis on the middleware layer within their infrastructures, whereas, as previously discussed, middleware is not a primary concern in CUNQA. This distinction stems from their respective goals: those tools aim to consolidate existing developments in quantum computing and adapt them for HPC, while CUNQA adopts an exploratory approach focused on the future role of quantum computing within HPC, placing DQC at the core of its design. CONQURE, by contrast, provides a cloud-based platform and a modified version of OpenMP capable of supporting quantum accelerators. Both directions diverge from that of CUNQA: the former because a cloud-based system is incompatible with the notion of an accelerator, and the latter because it seeks to adapt a classical framework to the quantum workflow, rather than designing a quantum-oriented approach from the outset. Lastly, QCCP differs fundamentally from CUNQA, as it proposes an entirely new programming model, whereas CUNQA seeks to leverage the most widely adopted model possible to facilitate user adaptation and integration with existing workflows (that is why \texttt{CunqaCircuit} is so similar in use to Qiskit's \texttt{QuantumCircuit}).","Several tools have been developed aiming for integrating quantum computing in HPC. It is important to, now that CUNQA has been thoroughly explained and contextualized, explain the differences with these tools in order to understand the novelty of CUNQA.

\begin{itemize}
    \item Quantum Framework (QFw)~\cite{chundury2024qfw, beck2024, shehata2024frameworkintegratingquantumsimulation, shehata2026}. The QFw tries to mimic a GPU accelerator stack, as explained in~\cite{shehata2026}. In this software stack they define two critical components: the Quantum Programming Interface (QPI) and the Quantum Platform Manager (QPM). The QPI mediates the application layer access to the middleware, where it can retrieve information about the resources available, insert compilation passes for the toolchain and more. More generally, it simplifies the customization and the interaction with the lower parts of the abstraction model, however, the QPI does not define a programming model---as Qiskit, for instance, does. On the other hand, the QPM, serves as a sort of driver, emerging as the responsible for communicating with the quantum device. It defines an API such that the QPI can interact with every quantum device indistinctively, although this is a current area of research for them.    
    \item Munich Quantum Software Stack (MQSS)~\cite{kaya2024, burgholzer2025mqss}. The MQSS software stack is very similar to QFw, to the point that they even have a so called QPI~\cite{kaya2024qpi}, even though this QPI does define a programming model in C, removing the need of using an external programming language. Moreover, they also define an API that abstracts away the specifics of the quantum devices, called quantum Quantum Device Management Interface (QDMI), which defines an interface that---as they claim in their documentation---is \textit{heavily inspired by the design of the OpenCL API for parallel programming of heterogeneous systems}. 
    \item A Co-Execution Environment for Quantum and Classical Resources (CONQURE)~\cite{mahesh2025conqure}. This work developes two tools: CONQURE cloud, which they claim is an \textit{interoperable alternative for AWS’ cloud queue for quantum devices}; and OpenMP-Q, which extends QpenMP-Q by leveraging its current support for accelerators with the aim of modifying quantum task execution in HPC. These two ways of operating with CONQURE make it a very versatile option, although it has some disadvantages, such as OpenMP-Q model's reliance on a Python script as an intermediate step for execution.
    \item QCCP~\cite{Du2025}. This tool defines a programming model that aims to \textit{shield the super-heterogeneous} nature of HPC environments nowadays, representing a development in the application layer of Figure~\ref{fig:software_stack}. This work is completely complementary to the QFw, in which they did not care about the programming model employed. 
    \item Even though it is not a tool, it is also worth mentioning the work executed by Claudino et al.~\cite{claudino2024}, in which they employed the eXtreme-scale Accelerator programming framework (XACC)\cite{mccaskey2020xacc} in combination to HPC resources to simulate, in the case of their article, the multi-contracted variant of the VQE algorithm (MC-VQE). This work showed that even though XACC is not thought to be an HPC tool explicitly, it can be used in such ways.
\end{itemize}

CUNQA distinguishes itself from all other tools in one essential aspect: it treats DQC as a central element of its design, assigning it the same level of importance as integration within HPC environments. It is, to the best of our knowledge, the only tool that explicitly addresses the emulation of DQC architectures in HPC contexts, and specifically, the first to confront the challenge of simulating quantum communication between emulated QPUs.

Structurally and conceptually, CUNQA shares similarities with QFw and MQSS. However, these works place strong emphasis on the middleware layer within their infrastructures, whereas, as previously discussed, middleware is not a primary concern in CUNQA. This distinction stems from their respective goals: those tools aim to consolidate existing developments in quantum computing and adapt them for HPC, while CUNQA adopts an exploratory approach focused on the future role of quantum computing within HPC, placing DQC at the core of its design. CONQURE, by contrast, provides a cloud-based platform and a modified version of OpenMP capable of supporting quantum accelerators. Both directions diverge from that of CUNQA: the former because a cloud-based system is incompatible with the notion of an accelerator, and the latter because it seeks to adapt a classical framework to the quantum workflow, rather than designing a quantum-oriented approach from the outset. Lastly, QCCP differs fundamentally from CUNQA, as it proposes an entirely new programming model, whereas CUNQA seeks to leverage the most widely adopted model possible to facilitate user adaptation and integration with existing workflows (that is why \texttt{CunqaCircuit} is so similar in use to Qiskit's \texttt{QuantumCircuit}).","Several tools have been developed aiming for integrating quantum computing in HPC. It is important to, now that
CUNQA has been thoroughly explained and contextualized, explain the differences with these tools in order to
understand the novelty of CUNQA.
•Quantum Framework (QFw) [ 2,10,38,39]. The QFw tries to mimic a GPU accelerator stack, as explained in [ 38].
In this software stack they define two critical components: the Quantum Programming Interface (QPI) and the
Quantum Platform Manager (QPM). The QPI mediates the application layer access to the middleware, where it
can retrieve information about the resources available, insert compilation passes for the toolchain and more.
More generally, it simplifies the customization and the interaction with the lower parts of the abstraction model,
however, the QPI does not define a programming model—as Qiskit, for instance, does. On the other hand, the
QPM, serves as a sort of driver, emerging as the responsible for communicating with the quantum device. It
defines an API such that the QPI can interact with every quantum device indistinctively, although this is a current
area of research for them.
•Munich Quantum Software Stack (MQSS) [ 5,25]. The MQSS software stack is very similar to QFw, to the point
that they even have a so called QPI [ 26], even though this QPI does define a programming model in C, removing
the need of using an external programming language. Moreover, they also define an API that abstracts away the
specifics of the quantum devices, called quantum Quantum Device Management Interface (QDMI), which defines
an interface that—as they claim in their documentation—isheavily inspired by the design of the OpenCL API for
parallel programming of heterogeneous systems.
•A Co-Execution Environment for Quantum and Classical Resources (CONQURE) [ 29]. This work developes
two tools: CONQURE cloud, which they claim is aninteroperable alternative for AWS’ cloud queue for quantum
devices; and OpenMP-Q, which extends QpenMP-Q by leveraging its current support for accelerators with the
aim of modifying quantum task execution in HPC. These two ways of operating with CONQURE make it a very
Manuscript submitted to ACM
CUNQA: a Distributed Quantum Computing emulator for HPC 17
versatile option, although it has some disadvantages, such as OpenMP-Q model’s reliance on a Python script as
an intermediate step for execution.
•QCCP [ 14]. This tool defines a programming model that aims toshield the super-heterogeneousnature of HPC
environments nowadays, representing a development in the application layer of Figure"
2511.02087v1,http://arxiv.org/abs/2511.02087v1,2025-11-03 21:58:36+00:00,Energy Loss Functions for Physical Systems,"Effectively leveraging prior knowledge of a system's physics is crucial for applications of machine learning to scientific domains. Previous approaches mostly focused on incorporating physical insights at the architectural level. In this paper, we propose a framework to leverage physical information directly into the loss function for prediction and generative modeling tasks on systems like molecules and spins. We derive energy loss functions assuming that each data sample is in thermal equilibrium with respect to an approximate energy landscape. By using the reverse KL divergence with a Boltzmann distribution around the data, we obtain the loss as an energy difference between the data and the model predictions. This perspective also recasts traditional objectives like MSE as energy-based, but with a physically meaningless energy. In contrast, our formulation yields physically grounded loss functions with gradients that better align with valid configurations, while being architecture-agnostic and computationally efficient. The energy loss functions also inherently respect physical symmetries. We demonstrate our approach on molecular generation and spin ground-state prediction and report significant improvements over baselines.","Several different lines of research have also incorporated a concept of energy into a machine learning framework. In this section, we distinguish our framework from distinct but related areas of research.

\paragraph{Energy-based models} Traditional energy-based models approach learning as shaping an energy landscape, where observed configurations correspond to low-energy states \citep{lecun2006tutorial}. Deep counterparts and their connection to discriminative training have also been extensively explored in many recent works (e.g., \cite{du2019implicit,grathwohl2019your}).
A key distinction of the existing literature on energy-based models and our energy loss approach is that, because they minimize the forward KL (max-likelihood or alternatives such as contrastive and large-margin losses), they need to deal with the minimization of the partition function or its surrogates -- i.e., the energy of arbitrary points in the domain must be high. In contrast, our treatment remains close to supervised learning losses and avoids the partition function altogether. 


\paragraph{Physics-informed neural networks (PINNs)} \citep{raissi2019physics} has proposed PINNs as a way to learn PDEs by penalizing residuals directly in the loss, enforcing solutions consistent with physical constraints. They have recently been used in the context of diffusion models \citep{bastek2024physics}. In contrast to our approach, these models do not rely on training data and models are instead learned to satisfy known differential equations on randomly generated points from the domain. 
Another family of physics-informed losses appears in Hamiltonian Neural Networks (HNN) \citep{greydanus2019hamiltonian} and Lagrangian Neural Networks (LNN) \citep{cranmer2020lagrangian}.




\paragraph{Energy Sampling and Boltzmann Generators}
A separate line of research incorporating energies and generative modeling has been in Boltzmann Generators \citep{noe2019boltzmann, kohler2020equivariant, klein2025transferableboltzmanngenerators}. These models are designed to sample physical configurations according to a Boltzmann distribution stemming from a known energy function. 
While our framework is also based on an assumption of data belonging to a Boltzmann distribution, ours is instead simply an approximation of the local landscape around each data point and does not assume the existence of a callable energy function.","Several different lines of research have also incorporated a concept of energy into a machine learning framework. In this section, we distinguish our framework from distinct but related areas of research.

\paragraph{Energy-based models} Traditional energy-based models approach learning as shaping an energy landscape, where observed configurations correspond to low-energy states \citep{lecun2006tutorial}. Deep counterparts and their connection to discriminative training have also been extensively explored in many recent works (e.g., \cite{du2019implicit,grathwohl2019your}).
A key distinction of the existing literature on energy-based models and our energy loss approach is that, because they minimize the forward KL (max-likelihood or alternatives such as contrastive and large-margin losses), they need to deal with the minimization of the partition function or its surrogates -- i.e., the energy of arbitrary points in the domain must be high. In contrast, our treatment remains close to supervised learning losses and avoids the partition function altogether. 


\paragraph{Physics-informed neural networks (PINNs)} \citep{raissi2019physics} has proposed PINNs as a way to learn PDEs by penalizing residuals directly in the loss, enforcing solutions consistent with physical constraints. They have recently been used in the context of diffusion models \citep{bastek2024physics}. In contrast to our approach, these models do not rely on training data and models are instead learned to satisfy known differential equations on randomly generated points from the domain. 
Another family of physics-informed losses appears in Hamiltonian Neural Networks (HNN) \citep{greydanus2019hamiltonian} and Lagrangian Neural Networks (LNN) \citep{cranmer2020lagrangian}.




\paragraph{Energy Sampling and Boltzmann Generators}
A separate line of research incorporating energies and generative modeling has been in Boltzmann Generators \citep{noe2019boltzmann, kohler2020equivariant, klein2025transferableboltzmanngenerators}. These models are designed to sample physical configurations according to a Boltzmann distribution stemming from a known energy function. 
While our framework is also based on an assumption of data belonging to a Boltzmann distribution, ours is instead simply an approximation of the local landscape around each data point and does not assume the existence of a callable energy function.",
2511.07347v1,http://arxiv.org/abs/2511.07347v1,2025-11-10 17:49:20+00:00,Walsh-Hadamard Neural Operators for Solving PDEs with Discontinuous Coefficients,"Neural operators have emerged as powerful tools for learning solution operators of partial differential equations (PDEs). However, standard spectral methods based on Fourier transforms struggle with problems involving discontinuous coefficients due to the Gibbs phenomenon and poor representation of sharp interfaces. We introduce the Walsh-Hadamard Neural Operator (WHNO), which leverages Walsh-Hadamard transforms-a spectral basis of rectangular wave functions naturally suited for piecewise constant fields-combined with learnable spectral weights that transform low-sequency Walsh coefficients to capture global dependencies efficiently. We validate WHNO on three problems: steady-state Darcy flow (preliminary validation), heat conduction with discontinuous thermal conductivity, and the 2D Burgers equation with discontinuous initial conditions. In controlled comparisons with Fourier Neural Operators (FNO) under identical conditions, WHNO demonstrates superior accuracy with better preservation of sharp solution features at material interfaces. Critically, we discover that weighted ensemble combinations of WHNO and FNO achieve substantial improvements over either model alone: for both heat conduction and Burgers equation, optimal ensembles reduce mean squared error by 35-40 percent and maximum error by up to 25 percent compared to individual models. This demonstrates that Walsh-Hadamard and Fourier representations capture complementary aspects of discontinuous PDE solutions, with WHNO excelling at sharp interfaces while FNO captures smooth features effectively.","\label{sec:related}

The neural operator framework \cite{kovachki2021neural, lu2021learning} extends neural networks from learning finite-dimensional mappings to learning operators between function spaces. DeepONet \cite{lu2021learning} introduced the branch-trunk architecture for approximating nonlinear operators, demonstrating universal approximation properties. These methods achieve resolution-independence by operating on continuous function representations, enabling generalisation across discretisations.

Fourier Neural Operators (FNO) \cite{li2020fourier, li2021fourier} make use of the Fast Fourier Transform to process inputs in frequency space, applying learnable weights to low-frequency modes. By truncating high-frequency components, FNO achieves parameter efficiency whilst capturing global spatial dependencies. Extensions include the Factorised FNO \cite{tran2021factorized} for memory efficiency, Galerkin Transformer \cite{cao2021choosing} for adaptive mode selection, and Geo-FNO \cite{li2022fourier} for general geometries. FNO has achieved state-of-the-art results on Navier-Stokes equations, Darcy flow with smooth permeability, weather prediction, and turbulence modelling \cite{pathak2022fourcastnet}.

Whilst highly effective for smooth PDEs, Fourier-based methods face fundamental limitations with discontinuous coefficients. The Gibbs phenomenon, characterised by oscillatory artifacts near discontinuities, appears because sinusoidal basis functions poorly represent sharp transitions \cite{carslaw1906theory}. Representing a step function requires infinitely many Fourier modes; finite truncation produces overshoot and ringing. For neural operators, this implies inefficient representation (many modes needed to approximate piecewise constant fields), error localisation (large errors concentrated at material interfaces), and training difficulty (oscillatory gradients may impede optimisation). Recent work on physics-informed neural networks (PINNs) \cite{raissi2019physics, karniadakis2021physics} and neural operators for multiphase flow \cite{mao2020physics} acknowledge these challenges but typically address them through architectural modifications such as adaptive sampling near interfaces, rather than changing the spectral basis.

Walsh functions, first introduced by Walsh \cite{walsh1923closed}, form an orthogonal basis of rectangular waves. Unlike smooth Fourier modes, Walsh functions naturally represent piecewise constant signals without oscillations \cite{beauchamp1975walsh, fine1949walsh}. The Fast Walsh-Hadamard Transform (FWHT) computes this decomposition in $O(n \log n)$ time \cite{fino1976unified}, matching FFT complexity. Applications include signal processing and image compression \cite{shanmugam1979walsh}, but neural operators based on Walsh-Hadamard transforms remain largely unexplored. Graph Neural Operators \cite{li2020neural} and Multipole Graph Neural Operator \cite{li2020multipole} avoid spectral transforms entirely by operating on graph structures, offering flexibility for irregular geometries but sacrificing the efficiency of global spectral processing.

Machine learning for subsurface flow in heterogeneous porous media has focussed on convolutional architectures \cite{mo2019deep, zhu2019physics} or kernel methods \cite{raissi2018numerical}. For heat transfer in composite materials, recent work employs PINNs \cite{cai2021physics} or autoencoder-based surrogate models \cite{kashinath2021physics}. However, systematic comparison of spectral bases (Fourier versus Walsh-Hadamard) for discontinuous coefficient PDEs has not been addressed in the operator learning literature. Our work fills this gap by demonstrating that Walsh-Hadamard transforms offer superior spectral efficiency and accuracy for PDEs with discontinuous material properties, outperforming the widely-used Fourier Neural Operator on problems where interface accuracy is critical.","The neural operator framework \cite{kovachki2021neural, lu2021learning} extends neural networks from learning finite-dimensional mappings to learning operators between function spaces. DeepONet \cite{lu2021learning} introduced the branch-trunk architecture for approximating nonlinear operators, demonstrating universal approximation properties. These methods achieve resolution-independence by operating on continuous function representations, enabling generalisation across discretisations.

Fourier Neural Operators (FNO) \cite{li2020fourier, li2021fourier} make use of the Fast Fourier Transform to process inputs in frequency space, applying learnable weights to low-frequency modes. By truncating high-frequency components, FNO achieves parameter efficiency whilst capturing global spatial dependencies. Extensions include the Factorised FNO \cite{tran2021factorized} for memory efficiency, Galerkin Transformer \cite{cao2021choosing} for adaptive mode selection, and Geo-FNO \cite{li2022fourier} for general geometries. FNO has achieved state-of-the-art results on Navier-Stokes equations, Darcy flow with smooth permeability, weather prediction, and turbulence modelling \cite{pathak2022fourcastnet}.

Whilst highly effective for smooth PDEs, Fourier-based methods face fundamental limitations with discontinuous coefficients. The Gibbs phenomenon, characterised by oscillatory artifacts near discontinuities, appears because sinusoidal basis functions poorly represent sharp transitions \cite{carslaw1906theory}. Representing a step function requires infinitely many Fourier modes; finite truncation produces overshoot and ringing. For neural operators, this implies inefficient representation (many modes needed to approximate piecewise constant fields), error localisation (large errors concentrated at material interfaces), and training difficulty (oscillatory gradients may impede optimisation). Recent work on physics-informed neural networks (PINNs) \cite{raissi2019physics, karniadakis2021physics} and neural operators for multiphase flow \cite{mao2020physics} acknowledge these challenges but typically address them through architectural modifications such as adaptive sampling near interfaces, rather than changing the spectral basis.

Walsh functions, first introduced by Walsh \cite{walsh1923closed}, form an orthogonal basis of rectangular waves. Unlike smooth Fourier modes, Walsh functions naturally represent piecewise constant signals without oscillations \cite{beauchamp1975walsh, fine1949walsh}. The Fast Walsh-Hadamard Transform (FWHT) computes this decomposition in $O(n \log n)$ time \cite{fino1976unified}, matching FFT complexity. Applications include signal processing and image compression \cite{shanmugam1979walsh}, but neural operators based on Walsh-Hadamard transforms remain largely unexplored. Graph Neural Operators \cite{li2020neural} and Multipole Graph Neural Operator \cite{li2020multipole} avoid spectral transforms entirely by operating on graph structures, offering flexibility for irregular geometries but sacrificing the efficiency of global spectral processing.

Machine learning for subsurface flow in heterogeneous porous media has focussed on convolutional architectures \cite{mo2019deep, zhu2019physics} or kernel methods \cite{raissi2018numerical}. For heat transfer in composite materials, recent work employs PINNs \cite{cai2021physics} or autoencoder-based surrogate models \cite{kashinath2021physics}. However, systematic comparison of spectral bases (Fourier versus Walsh-Hadamard) for discontinuous coefficient PDEs has not been addressed in the operator learning literature. Our work fills this gap by demonstrating that Walsh-Hadamard transforms offer superior spectral efficiency and accuracy for PDEs with discontinuous material properties, outperforming the widely-used Fourier Neural Operator on problems where interface accuracy is critical.","The neural operator framework [1, 5] extends neural networks from learn-
ingfinite-dimensionalmappingstolearningoperatorsbetweenfunctionspaces.
DeepONet [5] introduced the branch-trunk architecture for approximating
3
nonlinearoperators,demonstratinguniversalapproximationproperties. These
methods achieve resolution-independence by operating on continuous func-
tion representations, enabling generalisation across discretisations.
FourierNeuralOperators(FNO)[2,3]makeuseoftheFastFourierTrans-
form to process inputs in frequency space, applying learnable weights to low-
frequency modes. By truncating high-frequency components, FNO achieves
parameterefficiencywhilstcapturingglobalspatialdependencies. Extensions
include the Factorised FNO [6] for memory efficiency, Galerkin Transformer
[7] for adaptive mode selection, and Geo-FNO [8] for general geometries.
FNO has achieved state-of-the-art results on Navier-Stokes equations, Darcy
flow with smooth permeability, weather prediction, and turbulence modelling
[9].
Whilst highly effective for smooth PDEs, Fourier-based methods face
fundamental limitations with discontinuous coefficients. The Gibbs phe-
nomenon, characterised by oscillatory artifacts near discontinuities, appears
because sinusoidal basis functions poorly represent sharp transitions [10].
Representing a step function requires infinitely many Fourier modes; finite
truncation produces overshoot and ringing. For neural operators, this im-
plies inefficient representation (many modes needed to approximate piece-
wise constant fields), error localisation (large errors concentrated at material
interfaces), and training difficulty (oscillatory gradients may impede optimi-
sation). Recent work on physics-informed neural networks (PINNs) [11, 12]
and neural operators for multiphase flow [13] acknowledge these challenges
but typically address them through architectural modifications such as adap-
tive sampling near interfaces, rather than changing the spectral basis.
Walsh functions, first introduced by Walsh [14], form an orthogonal basis
of rectangular waves. Unlike smooth Fourier modes, Walsh functions nat-
urally represent piecewise constant signals without oscillations [15, 4]. The
Fast Walsh-Hadamard Transform (FWHT) computes this decomposition in
O(nlogn)time [16], matching FFT complexity. Applications include signal
processing and image compression [17], but neural operators based on Walsh-
Hadamard transforms remain largely unexplored. Graph Neural Operators
[18] and Multipole Graph Neural Operator [19] avoid spectral transforms
entirely by operating on graph structures, offering flexibility for irregular
geometries but sacrificing the efficiency of global spectral processing.
Machine learning for subsurface flow in heterogeneous porous media has
focussed on convolutional architectures [20, 21] or kernel methods [22]. For
heat transfer in composite materials, recent work employs PINNs [23] or
4
autoencoder-based surrogate models [24]. However, systematic comparison
of spectral bases (Fourier versus Walsh-Hadamard) for discontinuous coeffi-
cient PDEs has not been addressed in the operator learning literature. Our
work fills this gap by demonstrating that Walsh-Hadamard transforms offer
superior spectral efficiency and accuracy for PDEs with discontinuous ma-
terial properties, outperforming the widely-used Fourier Neural Operator on
problems where interface accuracy is critical."
2511.09568v1,http://arxiv.org/abs/2511.09568v1,2025-11-11 05:45:37+00:00,VEDA: 3D Molecular Generation via Variance-Exploding Diffusion with Annealing,"Diffusion models show promise for 3D molecular generation, but face a fundamental trade-off between sampling efficiency and conformational accuracy. While flow-based models are fast, they often produce geometrically inaccurate structures, as they have difficulty capturing the multimodal distributions of molecular conformations. In contrast, denoising diffusion models are more accurate but suffer from slow sampling, a limitation attributed to sub-optimal integration between diffusion dynamics and SE(3)-equivariant architectures. To address this, we propose VEDA, a unified SE(3)-equivariant framework that combines variance-exploding diffusion with annealing to efficiently generate conformationally accurate 3D molecular structures. Specifically, our key technical contributions include: (1) a VE schedule that enables noise injection functionally analogous to simulated annealing, improving 3D accuracy and reducing relaxation energy; (2) a novel preconditioning scheme that reconciles the coordinate-predicting nature of SE(3)-equivariant networks with a residual-based diffusion objective, and (3) a new arcsin-based scheduler that concentrates sampling in critical intervals of the logarithmic signal-to-noise ratio. On the QM9 and GEOM-DRUGS datasets, VEDA matches the sampling efficiency of flow-based models, achieving state-of-the-art valency stability and validity with only 100 sampling steps. More importantly, VEDA's generated structures are remarkably stable, as measured by their relaxation energy during GFN2-xTB optimization. The median energy change is only 1.72 kcal/mol, significantly lower than the 32.3 kcal/mol from its architectural baseline, SemlaFlow. Our framework demonstrates that principled integration of VE diffusion with SE(3)-equivariant architectures can achieve both high chemical accuracy and computational efficiency.","\label{sec:related_works}
\paragraph{Diffusion-Based Generative Models}
%Denoising Diffusion Probabilistic Models~\cite{ho2020denoising} have established a dominant paradigm for generative modeling by learning a reverse process to denoise inputs progressively corrupted by Gaussian noise. 
%It was later generalized to continuous-time formulations via score-based SDEs~\cite{songscore}, with either variance-preserving or variance-exploding dynamics.
%Recent frameworks, particularly those employing Variance-Exploding (VE) dynamics and preconditioning strategies~\cite{karras2022elucidating}, have markedly improved model performance and sampling flexibility. As a prominent alternative, Flow Matching~\cite{lipman2023flow} learns the velocity field of a probability flow directly, offering a different yet highly competitive approach to generative modeling.
%Consistent with recent literature~\cite{gao2024diffusion}, we consider flow matching a specialized formulation within the broader diffusion framework.
Denoising Diffusion Probabilistic Models (DDPMs)~\cite{ho2020denoising} have become a leading class of generative models which learn to reverse a forward process that incrementally corrupts data with Gaussian noise. 
DDPMs are extended to continuous-time settings using score-based stochastic differential equations (SDEs)~\cite{songscore}, incorporating either variance-preserving or variance-exploding dynamics.
Recent advances, particularly those utilizing Variance-Exploding (VE) SDEs with preconditioning techniques~\cite{karras2022elucidating}, have substantially enhanced generation quality and sampling efficiency.
An alternative approach, Flow Matching~\cite{lipman2023flow}, directly learns the velocity field of  the data distribution's probability flow, offering a different yet highly competitive approach to generative modeling.
Consistent with recent literature~\cite{gao2024diffusion}, we consider flow matching a specialized formulation within the broader diffusion framework. {\bf which paper said that flow based methods can be difficult catching the distribution of confirmation generation? this does not match with the abstract.}

% ----- Discrete Diffusion Models -----
\paragraph{Discrete Diffusion Models}
The principles of diffusion have also been extended to discrete data generation, drawing inspiration from masked language modeling~\cite{devlin2019bert}.
These non-autoregressive methods are a natural fit for the unordered structure of molecules, enabling de novo generation without imposing artificial sequential or ordering assumptions.
D3PM~\cite{austin2021structured} established the theoretical foundations for discrete diffusion using continuous-time Markov chains.
Subsequent developments, such as SEDD~\cite{lou2024discrete} and variants of Discrete Flow Matching~\cite{campbell2024generative, gat2024discrete}, have further advanced the field with improved sampling strategies and scalability.
% ----- De novo 3D Molecular Generation -----
\paragraph{\textit{De novo} 3D Molecular Generation}
Modern 3D molecular generation heavily relies on E(3)-equivariant neural networks, from pioneering architectures like EGNN~\cite{satorras2021n_EGNN} to subsequent transformer-based models~\cite{zhang2025d3mes}.
From a probabilistic modeling perspective, the field is dominated by a fundamental trade-off.
Denoising diffusion-based methods like EDM~\cite{hoogeboom2022equivariant} and GeoLDM~\cite{xu2023geometric} achieve high conformational accuracy but suffer from slow sampling.
Conversely, flow-based models such as EquiFM~\cite{song2023equivariantFM}, GeoBFN~\cite{song2023unified}, and SemlaFlow~\cite{irwin2025semlaflow} are efficient but often struggle with geometric precision.
Balancing high fidelity with computational efficiency remains a pressing challenge, with recent benchmarks underscoring the chemical inaccuracy of flow-based models~\cite{nikitin2025geom}.
%This challenge is underscored by recent benchmark reports, which highlight the limitations of flow-based approaches in generating chemically accurate structures~\cite{nikitin2025geom}.
%Balancing high fidelity with computational efficiency remains one of the most pressing challenges in 3D molecular generation.","\paragraph{Diffusion-Based Generative Models}




Denoising Diffusion Probabilistic Models (DDPMs)~\cite{ho2020denoising} have become a leading class of generative models which learn to reverse a forward process that incrementally corrupts data with Gaussian noise. 
DDPMs are extended to continuous-time settings using score-based stochastic differential equations (SDEs)~\cite{songscore}, incorporating either variance-preserving or variance-exploding dynamics.
Recent advances, particularly those utilizing Variance-Exploding (VE) SDEs with preconditioning techniques~\cite{karras2022elucidating}, have substantially enhanced generation quality and sampling efficiency.
An alternative approach, Flow Matching~\cite{lipman2023flow}, directly learns the velocity field of  the data distribution's probability flow, offering a different yet highly competitive approach to generative modeling.
Consistent with recent literature~\cite{gao2024diffusion}, we consider flow matching a specialized formulation within the broader diffusion framework. {\bf which paper said that flow based methods can be difficult catching the distribution of confirmation generation? this does not match with the abstract.}


\paragraph{Discrete Diffusion Models}
The principles of diffusion have also been extended to discrete data generation, drawing inspiration from masked language modeling~\cite{devlin2019bert}.
These non-autoregressive methods are a natural fit for the unordered structure of molecules, enabling de novo generation without imposing artificial sequential or ordering assumptions.
D3PM~\cite{austin2021structured} established the theoretical foundations for discrete diffusion using continuous-time Markov chains.
Subsequent developments, such as SEDD~\cite{lou2024discrete} and variants of Discrete Flow Matching~\cite{campbell2024generative, gat2024discrete}, have further advanced the field with improved sampling strategies and scalability.

\paragraph{\textit{De novo} 3D Molecular Generation}
Modern 3D molecular generation heavily relies on E(3)-equivariant neural networks, from pioneering architectures like EGNN~\cite{satorras2021n_EGNN} to subsequent transformer-based models~\cite{zhang2025d3mes}.
From a probabilistic modeling perspective, the field is dominated by a fundamental trade-off.
Denoising diffusion-based methods like EDM~\cite{hoogeboom2022equivariant} and GeoLDM~\cite{xu2023geometric} achieve high conformational accuracy but suffer from slow sampling.
Conversely, flow-based models such as EquiFM~\cite{song2023equivariantFM}, GeoBFN~\cite{song2023unified}, and SemlaFlow~\cite{irwin2025semlaflow} are efficient but often struggle with geometric precision.
Balancing high fidelity with computational efficiency remains a pressing challenge, with recent benchmarks underscoring the chemical inaccuracy of flow-based models~\cite{nikitin2025geom}.","Diffusion-Based Generative ModelsDenoising Diffu-
sion Probabilistic Models (DDPMs) (Ho, Jain, and Abbeel
2020) have become a leading class of generative models
which learn to reverse a forward process that incrementally
corrupts data with Gaussian noise. DDPMs are extended
to continuous-time settings using score-based stochastic
differential equations (SDEs) (Song et al. 2021), incor-
porating either variance-preserving or variance-exploding
dynamics. Recent advances, particularly those utilizing
Variance-Exploding (VE) SDEs with preconditioning tech-
niques (Karras et al. 2022), have substantially enhanced
generation quality and sampling efficiency. An alternative
approach, Flow Matching (Lipman et al. 2023), directly
learns the velocity field of the data distribution’s probability
flow, offering a different yet highly competitive approach to
generative modeling. Consistent with recent literature (Gao
et al. 2024), we consider flow matching a specialized formu-
lation within the broader diffusion framework.which paper
said that flow based methods can be difficult catching
the distribution of confirmation generation? this does not
match with the abstract.
Discrete Diffusion ModelsThe principles of diffusion
have also been extended to discrete data generation, draw-
ing inspiration from masked language modeling (Devlin
et al. 2019). These non-autoregressive methods are a nat-
ural fit for the unordered structure of molecules, enabling
de novo generation without imposing artificial sequential
or ordering assumptions. D3PM (Austin et al. 2021) estab-
lished the theoretical foundations for discrete diffusion using
continuous-time Markov chains. Subsequent developments,
such as SEDD (Lou, Meng, and Ermon 2024) and variants
of Discrete Flow Matching (Campbell et al. 2024; Gat et al.
2024), have further advanced the field with improved sam-
pling strategies and scalability.
De novo3D Molecular GenerationModern 3D molecu-
lar generation heavily relies on E(3)-equivariant neural net-works, from pioneering architectures like EGNN (Satorras,
Hoogeboom, and Welling 2021) to subsequent transformer-
based models (Zhang, Chen, and Chu 2025). From a prob-
abilistic modeling perspective, the field is dominated by
a fundamental trade-off. Denoising diffusion-based meth-
ods like EDM (Hoogeboom et al. 2022) and GeoLDM (Xu
et al. 2023) achieve high conformational accuracy but suffer
from slow sampling. Conversely, flow-based models such as
EquiFM (Song et al. 2023a), GeoBFN (Song et al. 2023c),
and SemlaFlow (Irwin et al. 2025) are efficient but often
struggle with geometric precision. Balancing high fidelity
with computational efficiency remains a pressing challenge,
with recent benchmarks underscoring the chemical inaccu-
racy of flow-based models (Nikitin et al. 2025)."
2511.04563v1,http://arxiv.org/abs/2511.04563v1,2025-11-06 17:17:55+00:00,QEF: Reproducible and Exploratory Quantum Software Experiments,"Commercially available Noisy Intermediate-Scale Quantum (NISQ) devices now make small hybrid quantum-classical experiments practical, but many tools hide configuration or demand ad-hoc scripting.
  We introduce the Quantum Experiment Framework (QEF): A lightweight framework designed to support the systematic, hypothesis-driven study of quantum algorithms. Unlike many existing approaches, QEF emphasises iterative, exploratory analysis of evolving experimental strategies rather than exhaustive empirical evaluation of fixed algorithms using predefined quality metrics. The framework's design is informed by a comprehensive review of the literature, identifying principal parameters and measurement practices currently reported in the field.
  QEF captures all key aspects of quantum software and algorithm experiments through a concise specification that expands into a Cartesian product of variants for controlled large-scale parameter sweeps. This design enables rigorous and systematic evaluation, as well as precise reproducibility. Large sweeps are automatically partitioned into asynchronous jobs across simulators or cloud hardware, and ascertain full hyper-parameter traceability. QEF supports parameter reuse to improve overall experiment runtimes, and collects all metrics and metadata into a form that can be conveniently explored with standard statistical and visualisation software.
  By combining reproducibility and scalability while avoiding the complexities of full workflow engines, QEF seeks to lower the practical barriers to empirical research on quantum algorithms, whether these are designed for current NISQ devices or future error-corrected quantum systems.","Most available quantum computing frameworks, such as Qiskit or Pennylane, do not focus on experiment preparation and result data collection. 
Nevertheless, frameworks exist that are designed with a particular focus on specific components of a hybrid quantum-classical experiment workflow. In this section, we review
related work, but also elaborate on how the design of existing tools differs from the design choices made for QEF.

Many approaches focus on the creation and execution of hybrid quantum-classical algorithms~\cite{bergholm:2022:pennylane, broughton:2021:tensorflow, sharma:2022:openqaoa, johnson:2022:qiskitruntime, hooyberghs:2021:azure, mauerer:2005:cqpl,carbonelli:24}. However, most do not focus on dynamic workflow orchestration and experimentation like QEF. 
Based on their documentation and example repositories, reproducing QEF's experiment workflow requires user-written code for generating parameter grids,
launching batches across back-ends, monitoring jobs, as well as collecting and storing results.

\subsection{Workflow Orchestration and Experiment Frameworks}
Running and scaling hybrid quantum-classical experiments requires tools that can organise, execute, and track experiments across both quantum and classical systems. 
There are already some frameworks with that aim, offering features for managing workflows and handling experiment data more effectively.

Sivarajah~\etal introduced \href{https://github.com/CQCL/tierkreis}{Tierkreis} in 2022, a framework for hybrid quantum-classical computing~\cite{sivarajah:2022:tierkreis}.
It represents programs as dataflow graphs, allowing developers to build complex hybrid workflows. 
Sauerwein~\etal integrated \href{https://aws.amazon.com/blogs/quantum-computing/tracking-quantum-experiments-with-amazon-braket-hybrid-jobs-and-amazon-sagemaker-experiments/}{Amazon Braket} with \href{https://aws.amazon.com/de/blogs/quantum-computing/tracking-quantum-experiments-with-amazon-braket-hybrid-jobs-and-amazon-sagemaker-experiments/}{SageMaker} in 2022 to systematically track metrics, hyper-parameters, and experiment result data across
multiple different runs. 
Unlike QEF it is tightly coupled to Amazon Braket and does need ad-hoc scripting to run experiments.

Quokka~\cite{beisel:2022:quokka}, introduced by Beisel~\etal in 2023, decomposes hybrid algorithms into microservices orchestrated by an external workflow engine.
In contrast to Quokka, QEF centres the experiment; it not only orchestrates quantum algorithms but also enables running systematic studies on them.
Quokka, by comparison, targets the application as a workflow of services. 
It decomposes a typical variational quantum algorithm (VQA) loop into microservices, like circuit generation, execution, readout, error mitigation, objective evaluation, and optimisation.
These services are intended to be driven by an external workflow engine.
This service-oriented style emphasises loose coupling and interchangeability of single microservices.
In short, QEF focuses on the experiment with a hybrid quantum-classical algorithm, 
while Quokka concentrates on the components of the hybrid quantum-classical algorithm itself and their integration into a workflow. 
Consequently, QEF aims to support end-to-end experiment creation, batching and result consolidation, whereas Quokka optimises the decomposition and substitution of individual workflow stages.
The two frameworks could complement each other, with QEF extending Quokka to handle the creation and management of experiments.

\subsection{Benchmarking and Evaluation Frameworks}
Benchmarking a multitude of aspects in quantum computing has received considerable methodological attention~\cite{lorenz:2025:benchmarkingquantum}.
QEF is suitable for application-level benchmarking, that is, comparing the performance of quantum algorithms.
By design, it is not intended to target any component-level benchmarks that concern the hardware implementation, nor any system-level benchmarks
It likewise does not support hpc-level benchmarking, as key metrics (\eg, queue times, cloud overhead) are not recorded.
Software-level benchmarking is also outside QEF's scope, since the necessary metrics are not collected.

\href{https://github.com/QUARK-framework/QUARK}{QUARK}~\cite{finvzgar:2022:quark} was proposed in 2022 by Finžgar~\etal as an application-level benchmarking tool for quantum computing. 
It provides an environment to design, implement, execute, and analyse quantum application benchmarks in a reproducible manner. 
Instead of focusing only on low-level metrics like gate fidelity or circuit depth, QUARK intends to facilitate end-to-end comparisons on industrially relevant tasks, such as optimisation problems in logistics, by running complete hybrid workflows and collecting solution quality, runtime, and resource metrics. 
QUARK allows testing one algorithm variant at a time and outputs a result report containing predefined quality metrics.
It is therefore well-suited to test hypotheses about which approach or back-end performs best on realistic tasks.
QEF, in contrast, allows testing many variants simultaneously, thereby isolating the impact of one factor, and returning the results in a tidy data format.

Another benchmarking effort, \href{https://superstaq.readthedocs.io/en/stable/apps/supermarq/supermarq.html}{SupermarQ}~\cite{tomesh:2022:supermarq} introduced by Tomesh~\etal, aims to evaluate quantum processors, 
but it emphasises static circuit families rather than hybrid loops.

In contrast to general-purpose hybrid frameworks that require complex configurations, QEF aims to support exploratory analysis with frequently changing parameters.
It fills the gap between application-level benchmarking frameworks and vendor-specific services as a lightweight, research-oriented experimentation framework for hypothesis-driven studies that yields uniform results.","Most available quantum computing frameworks, such as Qiskit or Pennylane, do not focus on experiment preparation and result data collection. 
Nevertheless, frameworks exist that are designed with a particular focus on specific components of a hybrid quantum-classical experiment workflow. In this section, we review
related work, but also elaborate on how the design of existing tools differs from the design choices made for QEF.

Many approaches focus on the creation and execution of hybrid quantum-classical algorithms~\cite{bergholm:2022:pennylane, broughton:2021:tensorflow, sharma:2022:openqaoa, johnson:2022:qiskitruntime, hooyberghs:2021:azure, mauerer:2005:cqpl,carbonelli:24}. However, most do not focus on dynamic workflow orchestration and experimentation like QEF. 
Based on their documentation and example repositories, reproducing QEF's experiment workflow requires user-written code for generating parameter grids,
launching batches across back-ends, monitoring jobs, as well as collecting and storing results.

\subsection{Workflow Orchestration and Experiment Frameworks}
Running and scaling hybrid quantum-classical experiments requires tools that can organise, execute, and track experiments across both quantum and classical systems. 
There are already some frameworks with that aim, offering features for managing workflows and handling experiment data more effectively.

Sivarajah~\etal introduced \href{https://github.com/CQCL/tierkreis}{Tierkreis} in 2022, a framework for hybrid quantum-classical computing~\cite{sivarajah:2022:tierkreis}.
It represents programs as dataflow graphs, allowing developers to build complex hybrid workflows. 
Sauerwein~\etal integrated \href{https://aws.amazon.com/blogs/quantum-computing/tracking-quantum-experiments-with-amazon-braket-hybrid-jobs-and-amazon-sagemaker-experiments/}{Amazon Braket} with \href{https://aws.amazon.com/de/blogs/quantum-computing/tracking-quantum-experiments-with-amazon-braket-hybrid-jobs-and-amazon-sagemaker-experiments/}{SageMaker} in 2022 to systematically track metrics, hyper-parameters, and experiment result data across
multiple different runs. 
Unlike QEF it is tightly coupled to Amazon Braket and does need ad-hoc scripting to run experiments.

Quokka~\cite{beisel:2022:quokka}, introduced by Beisel~\etal in 2023, decomposes hybrid algorithms into microservices orchestrated by an external workflow engine.
In contrast to Quokka, QEF centres the experiment; it not only orchestrates quantum algorithms but also enables running systematic studies on them.
Quokka, by comparison, targets the application as a workflow of services. 
It decomposes a typical variational quantum algorithm (VQA) loop into microservices, like circuit generation, execution, readout, error mitigation, objective evaluation, and optimisation.
These services are intended to be driven by an external workflow engine.
This service-oriented style emphasises loose coupling and interchangeability of single microservices.
In short, QEF focuses on the experiment with a hybrid quantum-classical algorithm, 
while Quokka concentrates on the components of the hybrid quantum-classical algorithm itself and their integration into a workflow. 
Consequently, QEF aims to support end-to-end experiment creation, batching and result consolidation, whereas Quokka optimises the decomposition and substitution of individual workflow stages.
The two frameworks could complement each other, with QEF extending Quokka to handle the creation and management of experiments.

\subsection{Benchmarking and Evaluation Frameworks}
Benchmarking a multitude of aspects in quantum computing has received considerable methodological attention~\cite{lorenz:2025:benchmarkingquantum}.
QEF is suitable for application-level benchmarking, that is, comparing the performance of quantum algorithms.
By design, it is not intended to target any component-level benchmarks that concern the hardware implementation, nor any system-level benchmarks
It likewise does not support hpc-level benchmarking, as key metrics (\eg, queue times, cloud overhead) are not recorded.
Software-level benchmarking is also outside QEF's scope, since the necessary metrics are not collected.

\href{https://github.com/QUARK-framework/QUARK}{QUARK}~\cite{finvzgar:2022:quark} was proposed in 2022 by Finžgar~\etal as an application-level benchmarking tool for quantum computing. 
It provides an environment to design, implement, execute, and analyse quantum application benchmarks in a reproducible manner. 
Instead of focusing only on low-level metrics like gate fidelity or circuit depth, QUARK intends to facilitate end-to-end comparisons on industrially relevant tasks, such as optimisation problems in logistics, by running complete hybrid workflows and collecting solution quality, runtime, and resource metrics. 
QUARK allows testing one algorithm variant at a time and outputs a result report containing predefined quality metrics.
It is therefore well-suited to test hypotheses about which approach or back-end performs best on realistic tasks.
QEF, in contrast, allows testing many variants simultaneously, thereby isolating the impact of one factor, and returning the results in a tidy data format.

Another benchmarking effort, \href{https://superstaq.readthedocs.io/en/stable/apps/supermarq/supermarq.html}{SupermarQ}~\cite{tomesh:2022:supermarq} introduced by Tomesh~\etal, aims to evaluate quantum processors, 
but it emphasises static circuit families rather than hybrid loops.

In contrast to general-purpose hybrid frameworks that require complex configurations, QEF aims to support exploratory analysis with frequently changing parameters.
It fills the gap between application-level benchmarking frameworks and vendor-specific services as a lightweight, research-oriented experimentation framework for hypothesis-driven studies that yields uniform results.","Most available quantum computing frameworks, such as Qiskit or Pennylane,
do not focus on experiment preparation and result data collection. Nevertheless,
frameworks exist that are designed with a particular focus on specific components
of a hybrid quantum-classical experiment workflow. In this section, we review
related work, but also elaborate on how the design of existing tools differs from
the design choices made for QEF.
Many approaches focus on the creation and execution of hybrid quantum-
classical algorithms [2, 3, 15, 9, 8, 12, 4]. However, most do not focus on dynamic"
2511.08856v1,http://arxiv.org/abs/2511.08856v1,2025-11-12 00:25:01+00:00,ForeSWE: Forecasting Snow-Water Equivalent with an Uncertainty-Aware Attention Model,"Various complex water management decisions are made in snow-dominant watersheds with the knowledge of Snow-Water Equivalent (SWE) -- a key measure widely used to estimate the water content of a snowpack. However, forecasting SWE is challenging because SWE is influenced by various factors including topography and an array of environmental conditions, and has therefore been observed to be spatio-temporally variable. Classical approaches to SWE forecasting have not adequately utilized these spatial/temporal correlations, nor do they provide uncertainty estimates -- which can be of significant value to the decision maker. In this paper, we present ForeSWE, a new probabilistic spatio-temporal forecasting model that integrates deep learning and classical probabilistic techniques. The resulting model features a combination of an attention mechanism to integrate spatiotemporal features and interactions, alongside a Gaussian process module that provides principled quantification of prediction uncertainty. We evaluate the model on data from 512 Snow Telemetry (SNOTEL) stations in the Western US. The results show significant improvements in both forecasting accuracy and prediction interval compared to existing approaches. The results also serve to highlight the efficacy in uncertainty estimates between different approaches. Collectively, these findings have provided a platform for deployment and feedback by the water management community.","\label{sec:related_work}

The current literature on SWE forecasting features both mechanistic and data-driven approaches. Mechanistic models utilize prior descriptive knowledge in physical and hydrological equations. However, our knowledge of these processes is limited leading to simplified models and large biases  ~\citep{subseasonalForecast}. Alternatively, machine learning (ML) techniques can learn from historical data from a diverse collection of locations, and benefit where physical knowledge may be lacking; and be learnable in an incremental fashion as new data become available---e.g., past temporal observations of SWE,  and additional spatial features such as remotely-sensed reflectances~\citep{brodzik2016measures}. 
%For examples, most recent practical forecasting models have leveraged data collected from $512$ snow telemetry (SNOTEL) stations in Western US for a data-driven approach 
%\bs{citation missing?}.\KT{Is this talking about our previous paper? Cause its prediction and not forecasting and has 323 stations} 
%This includes (past) temporal observations of SWE and additional spatial features such as remotely-sensed reflectances~\citep{brodzik2016measures}, topographical details, and meteorological data.
Such data can be utilized to build SWE forecasting models.
%which, for example, provide a 10-day forecast for short-term planning and a 4-week forecast for long-term strategies. 
%ML models can be adapted more flexibly to new locations and weather conditions via updating its parameters with new data, which presents a more scalable approach. 
% However,  existing ML-based forecasting models ~\citep{sarhadi2014snow,CUI2023128835,thapa2024attention,FRANZ2010820} do not explicitly model spatio-temporal interactions or correlations or provide limited to no uncertainty estimates for their prediction,  which are essential for  downstream decision making.  %among spatial and temporal attributes as well as their correlation across different locations and time. Furthermore, prior approaches have also not provided principled uncertainty quantification of their SWE forecasting which is essential for optimizing downstream decision making.

Among the earlier data-driven works in SWE forecasting, \citep{sarhadi2014snow} uses statistical models like ARIMA \citep{arima} and SARIMA \citep{sarima} with exogenous variables to predict daily (i.e. 30 days) and monthly (i.e. 6 months) SWE. % in Ontario, Canada. 
%These models feed the daily or monthly historical SWE trend with external factors or variables that can impact the SWE in a given location and time, thereby forecasting the SWE value. 
However, all the locations under study are in low elevation ($<$2000ft) with a low annual max SWE (100 to 150mm), making the model too restrictive. Similarly, \citep{FRANZ2010820} combines twelve bio-physical models using a Bayesian Model Averaging (BMA) to forecast SWE with uncertainty quantification. Although the work shows competitive results,   it has only been implemented on six locations and limited to a single day forecasting. 
%have a limited forecasting window of one day.
%, unlike our dataset with locations at varying elevations (2000 to 11000 ft) and annual max SWE (50 to 2000mm), adding variability and challenges to model building}.  
% \citet{subseasonalForecast} focus on the sub-seasonal forecasting (h = 32 daily or 4 weekly points forecasts) of gridded SWE worldwide using an ensemble of existing global physical and hydrological models such as GEPS \citep{charron2010toward, houtekamer2009model}, GEFS \citep{toth1993ensemble, zhu2017impact, zhu2018toward, li2019evaluating}, and FIM \citep{bleck2010use, lee2009finite}.
% This approach relies on the equations and assumptions of all the underlying models---making it difficult to incorporate new factors and recalibrate. Additionally, over the years assumptions are subjected to change with the evolving climate and weather patterns.

Deep Learning approaches have been explored more recently. \cite{CUI2023128835} couples the Long Short-Term Memory (LSTM) \citep{hochreiter1997long} and zonal bias correction data assimilation approach to predict the gridded SWE value at 1 km pixel for the next day or month. 
%Here, LSTM captures temporal correlations, and the zonal bias correction performs inverse-weighted assimilation of SWE from the observation sites in the surroundings. 
The assimilation approach depends on the presence of nearby observation sites, which makes it challenging due to the geographical sparsity in the observation layer. %Moreover, it assumes spatial correlation is proportional to proximity, which is restrictive  (e.g., locations falling under the same atmospheric river path can be correlated, although far away).
%neglecting the fact that locations that might not necessarily be nearby can still have strong correlations. 
%For example, locations falling under the same atmospheric river path, although far away can have similar behavior. 
Recently, an attention-based model was implemented to predict SWE using spatial and temporal attention \citep{thapa2024attention} but it does not provide any uncertainty estimates. For a different application in climate modeling, a few approaches exist \citep{nguyen2023climax, nguyen2023scaling}.
 In comparison to these approaches, our model architecture is different---as it uses a form of spatio-temporal attention that  captures attribute interaction  %through an aggregation, making the query vector in its attention mechanism vary with location and time 
 (unlike a single learnable query vector in the \textit{ClimaX model} \citep{nguyen2023climax}), and is designed to provide  uncertainty estimates. 


%Despite these multple prior works though, there has not been an effort for SWE forecasting using deep learning that can  incorporate spatio-temporal variability in the data.  
%These models work with the gridded image data and are built on the Vision Transformer \citep{dosovitskiy2020image}.

%Compared to these recent prior works, our approach is different  in several ways:
%\begin{itemize}
    %\item 
    %While the \textit{ClimaX model} \citep{nguyen2023climax} uses gridded images of observations with Vision Transformer \citep{dosovitskiy2020image} for meteorological variables, our focus is on uncertainty-aware SWE forecasting with spatially-sparse SWE data. We rely on the vanilla attention mechanism with vector representations of gridded observations of input features, leading to a different model architecture. 
 %   Our model also tries to capture attribute interaction through an aggregation, making the query vector in its attention mechanism vary with location and time, unlike a single learnable query vector in the \textit{ClimaX model} \citep{nguyen2023climax}. 
    %\item  
  %  Furthermore,  \cite{thapa2024attention} used spatial and temporal attention for SWE prediction; but it was not designed for forecasting, does not support uncertainty quantification, and does not capture attribute interaction---features supported by our model.
    

%    \item Our model benefits from taking historical data at multiple resolutions (eg; daily, weekly, monthly) and combining them, in addition to capturing spatial correlations before forecasting for a given location over a relevant horizon.
 %   \item To account for uncertainties in SWE forecasting  our model uses Gaussian process \citep{van2020framework} and associates a prediction interval with each forecast.
%\end{itemize}

%The detailed architecture and approach followed in the model are explained in the sections below.","The current literature on SWE forecasting features both mechanistic and data-driven approaches. Mechanistic models utilize prior descriptive knowledge in physical and hydrological equations. However, our knowledge of these processes is limited leading to simplified models and large biases  ~\citep{subseasonalForecast}. Alternatively, machine learning (ML) techniques can learn from historical data from a diverse collection of locations, and benefit where physical knowledge may be lacking; and be learnable in an incremental fashion as new data become available---e.g., past temporal observations of SWE,  and additional spatial features such as remotely-sensed reflectances~\citep{brodzik2016measures}. 



Such data can be utilized to build SWE forecasting models.




Among the earlier data-driven works in SWE forecasting, \citep{sarhadi2014snow} uses statistical models like ARIMA \citep{arima} and SARIMA \citep{sarima} with exogenous variables to predict daily (i.e. 30 days) and monthly (i.e. 6 months) SWE. 

However, all the locations under study are in low elevation ($<$2000ft) with a low annual max SWE (100 to 150mm), making the model too restrictive. Similarly, \citep{FRANZ2010820} combines twelve bio-physical models using a Bayesian Model Averaging (BMA) to forecast SWE with uncertainty quantification. Although the work shows competitive results,   it has only been implemented on six locations and limited to a single day forecasting. 





Deep Learning approaches have been explored more recently. \cite{CUI2023128835} couples the Long Short-Term Memory (LSTM) \citep{hochreiter1997long} and zonal bias correction data assimilation approach to predict the gridded SWE value at 1 km pixel for the next day or month. 

The assimilation approach depends on the presence of nearby observation sites, which makes it challenging due to the geographical sparsity in the observation layer. 


Recently, an attention-based model was implemented to predict SWE using spatial and temporal attention \citep{thapa2024attention} but it does not provide any uncertainty estimates. For a different application in climate modeling, a few approaches exist \citep{nguyen2023climax, nguyen2023scaling}.
 In comparison to these approaches, our model architecture is different---as it uses a form of spatio-temporal attention that  captures attribute interaction  
 (unlike a single learnable query vector in the \textit{ClimaX model} \citep{nguyen2023climax}), and is designed to provide  uncertainty estimates.",
2511.06854v1,http://arxiv.org/abs/2511.06854v1,2025-11-10 08:53:10+00:00,Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning,"Irregularly sampled time series (ISTS), characterized by non-uniform time intervals with natural missingness, are prevalent in real-world applications. Existing approaches for ISTS modeling primarily rely on observed values to impute unobserved ones or infer latent dynamics. However, these methods overlook a critical source of learning signal: the reconstruction error inherently produced during model training. Such error implicitly reflects how well a model captures the underlying data structure and can serve as an informative proxy for unobserved values. To exploit this insight, we propose iTimER, a simple yet effective self-supervised pre-training framework for ISTS representation learning. iTimER models the distribution of reconstruction errors over observed values and generates pseudo-observations for unobserved timestamps through a mixup strategy between sampled errors and the last available observations. This transforms unobserved timestamps into noise-aware training targets, enabling meaningful reconstruction signals. A Wasserstein metric aligns reconstruction error distributions between observed and pseudo-observed regions, while a contrastive learning objective enhances the discriminability of learned representations. Extensive experiments on classification, interpolation, and forecasting tasks demonstrate that iTimER consistently outperforms state-of-the-art methods under the ISTS setting.","\paragraph{Representation Learning for Irregularly Sampled Time Series}

Recent advances in ISTS analysis have increasingly shifted toward learning expressive and discriminative representations. A common kind of imputation-based methods impute unobserved values from real observations before downstream tasks~\cite{che2018recurrent, chen2022nonstationary, fan2022dynamic, du2023saits}.  However, inappropriate imputation can introduce misleading noise or structural bias, especially under sparse observations that negatively impact model performance~\cite{zhang2021graph, agarwal2023modelling}. While end-to-end models aggregate observation values for each variable to get discrete hidden states using attention mechanisms~\cite{shukla2021multitime, zhang2023warpformer, li2023time}, Graph Neural Networks (GNNs)~\cite{zhang2021graph, yalavarthi2024grafiti, zhang2024tpatchgnn}, and Recurrent Neural Networks (RNNs)~\cite{de2019gru, schirmer2022modeling, agarwal2023modelling} -based methods. Such methods struggle to capture continuous temporal dependencies. To address this, Neural ODE-based approaches~\cite{kidger2020neural, rubanova2019latent, jhin2022exit, oh2025comprehensive, zhang2025diffode} have been proposed to model ISTS in continuous time.

Recent work has explored self-supervised learning for ISTS representation learning by deriving supervision directly from the data itself~\cite{chowdhury2023primenet, beebe2023paits}. Self-supervised methods design pretext tasks that encourage the model to learn meaningful representations without relying on downstream labels, including contrastive learning that constructs augmented views from real observations for contrastive learning and masked modeling methods that drop parts of the input data, and predict the dropout values by minimizing the reconstruction error between the predicted and original values. When facing ISTS, the non-uniform time intervals with natural missingness make it difficult to construct meaningful positive pairs and waste valuable real observations and potentially distort the learning signal.

Different from existing masked modeling methods, iTimER leverages reconstruction error not just as a loss, but as a learning signal to model uncertainty. It estimates error distributions from observed data and synthesizes pseudo-observations, enabling noise-aware representation learning without explicit imputation or artificial noise~\cite{tashiro2021csdi,islam2025self}.



\subsection{Reconstruction Error as a Learning Signal}

Across domains, reconstruction error, also referred to as residual, has proven to be more than a loss metric but serves as a valuable signal for model guidance. In image inpainting, \citet{yu2018generative} employs reconstruction error in a coarse-to-fine architecture, where the residuals from the coarse stage are passed to the fine stage as structural cues, improving detail consistency in the final output. Similarly, in robust subspace learning, \citet{meng2013robust} embeds reconstruction error into the optimization loop to jointly learn the data structure and noise characteristics. In video anomaly detection, works such as \citet{hasan2016learning} and \citet{liu2018future} exploit spatially localized residuals to identify abnormal events.

These examples suggest that reconstruction error can highlight uncertainty, noise, or outliers and serve as an informative learning cue. iTimER builds on this idea by modeling the distribution of reconstruction error over observed regions in ISTS, then sampling from this distribution to generate uncertainty-aware pseudo-observations. These are used not as direct labels, but as a proxy signal that drives both contrastive representation learning and distributional consistency. To our knowledge, iTimER is the first to formalize this strategy in the ISTS domain, using reconstruction error not as a penalty, but as a means of self-supervised guidance.




\begin{figure*}[t]
\centerline{\includegraphics[width=0.9\linewidth]{fig/framework.pdf}}
\caption{\textbf{The iTimER framework. }iTimER leverages reconstruction error from the original sequence to generate pseudo observations via mixup uncertainty-aware sampling and the last observed value. Both real and pseudo-observation series are encoded and reconstructed, with consistency enforced at both representation and error distribution levels.}
\label{fig:framework}
\end{figure*}
% \centerline{\includegraphics[width=0.9\linewidth]{fig/ISTS.pdf}}","\paragraph{Representation Learning for Irregularly Sampled Time Series}

Recent advances in ISTS analysis have increasingly shifted toward learning expressive and discriminative representations. A common kind of imputation-based methods impute unobserved values from real observations before downstream tasks~\cite{che2018recurrent, chen2022nonstationary, fan2022dynamic, du2023saits}.  However, inappropriate imputation can introduce misleading noise or structural bias, especially under sparse observations that negatively impact model performance~\cite{zhang2021graph, agarwal2023modelling}. While end-to-end models aggregate observation values for each variable to get discrete hidden states using attention mechanisms~\cite{shukla2021multitime, zhang2023warpformer, li2023time}, Graph Neural Networks (GNNs)~\cite{zhang2021graph, yalavarthi2024grafiti, zhang2024tpatchgnn}, and Recurrent Neural Networks (RNNs)~\cite{de2019gru, schirmer2022modeling, agarwal2023modelling} -based methods. Such methods struggle to capture continuous temporal dependencies. To address this, Neural ODE-based approaches~\cite{kidger2020neural, rubanova2019latent, jhin2022exit, oh2025comprehensive, zhang2025diffode} have been proposed to model ISTS in continuous time.

Recent work has explored self-supervised learning for ISTS representation learning by deriving supervision directly from the data itself~\cite{chowdhury2023primenet, beebe2023paits}. Self-supervised methods design pretext tasks that encourage the model to learn meaningful representations without relying on downstream labels, including contrastive learning that constructs augmented views from real observations for contrastive learning and masked modeling methods that drop parts of the input data, and predict the dropout values by minimizing the reconstruction error between the predicted and original values. When facing ISTS, the non-uniform time intervals with natural missingness make it difficult to construct meaningful positive pairs and waste valuable real observations and potentially distort the learning signal.

Different from existing masked modeling methods, iTimER leverages reconstruction error not just as a loss, but as a learning signal to model uncertainty. It estimates error distributions from observed data and synthesizes pseudo-observations, enabling noise-aware representation learning without explicit imputation or artificial noise~\cite{tashiro2021csdi,islam2025self}.



\subsection{Reconstruction Error as a Learning Signal}

Across domains, reconstruction error, also referred to as residual, has proven to be more than a loss metric but serves as a valuable signal for model guidance. In image inpainting, \citet{yu2018generative} employs reconstruction error in a coarse-to-fine architecture, where the residuals from the coarse stage are passed to the fine stage as structural cues, improving detail consistency in the final output. Similarly, in robust subspace learning, \citet{meng2013robust} embeds reconstruction error into the optimization loop to jointly learn the data structure and noise characteristics. In video anomaly detection, works such as \citet{hasan2016learning} and \citet{liu2018future} exploit spatially localized residuals to identify abnormal events.

These examples suggest that reconstruction error can highlight uncertainty, noise, or outliers and serve as an informative learning cue. iTimER builds on this idea by modeling the distribution of reconstruction error over observed regions in ISTS, then sampling from this distribution to generate uncertainty-aware pseudo-observations. These are used not as direct labels, but as a proxy signal that drives both contrastive representation learning and distributional consistency. To our knowledge, iTimER is the first to formalize this strategy in the ISTS domain, using reconstruction error not as a penalty, but as a means of self-supervised guidance.",
2511.04403v1,http://arxiv.org/abs/2511.04403v1,2025-11-06 14:29:05+00:00,Online Bayesian Experimental Design for Partially Observed Dynamical Systems,"Bayesian experimental design (BED) provides a principled framework for optimizing data collection, but existing approaches do not apply to crucial real-world settings such as dynamical systems with partial observability, where only noisy and incomplete observations are available. These systems are naturally modeled as state-space models (SSMs), where latent states mediate the link between parameters and data, making the likelihood -- and thus information-theoretic objectives like the expected information gain (EIG) -- intractable. In addition, the dynamical nature of the system requires online algorithms that update posterior distributions and select designs sequentially in a computationally efficient manner. We address these challenges by deriving new estimators of the EIG and its gradient that explicitly marginalize latent states, enabling scalable stochastic optimization in nonlinear SSMs. Our approach leverages nested particle filters (NPFs) for efficient online inference with convergence guarantees. Applications to realistic models, such as the susceptible-infected-recovered (SIR) and a moving source location task, show that our framework successfully handles both partial observability and online computation.","\label{sec:related work}

A central challenge in \gls*{bed} is the cost and complexity of estimating the \gls*{eig} (or its gradient) \citep{Rainforth24,huan2024optimal}. For models with tractable likelihoods $p(\by\!\mid\!\btheta,\bxi)$, the nested Monte Carlo (NMC) estimator has been studied in depth \citep{Rainforth18}, with subsequent work proposing variational formulations and bounds to improve convergence \citep{Foster19,Foster20,Foster21}. These approaches also enable differentiable objectives for gradient-based design in continuous spaces. However, they rely on explicit pointwise likelihood evaluation and do not extend directly to partially observable dynamical systems, where likelihoods require marginalization over latent states.

When likelihoods are implicit (intractable pointwise but possible to simulate from), likelihood-free design methods replace direct evaluation by surrogates or simulation-based objectives. Examples include variational mutual-information bounds and amortized estimators \citep{Kleinegesse20,Kleinegesse21,Ivanova21}, density-ratio or classifier-based estimates of information gain \citep{Kleinegesse19}, and \gls*{abc}-style utilities \citep{Dehideniya18,Drovandi13,Hainy16,Price16}. While effective in simulator settings, most such methods target \emph{static} design (optimize once, then deploy) rather than fully sequential/adaptive \gls*{bed}. Moreover, surrogate-based adaptive variants \citep[e.g.,][]{Ivanova21} typically require an offline dataset to train an implicit likelihood, which may be unavailable or quickly misspecified in evolving dynamical systems.

Design for dynamical systems has also been studied under full state observability. 
\cite{Iqbal24,iqbal2024recursive} consider adaptive design when the state is fully observed, making the likelihood tractable. 
Both papers leverage particle methods for joint state–parameter inference: \citep{Iqbal24} uses a reversed (“inside-out”) \gls*{smc2}, and \citep{iqbal2024recursive} adopts \gls*{npf}'s jittering to rejuvenate parameter particles. 
However, these recent advances in \gls*{bed} for dynamical systems assume full state observability and do not address the latent-state marginalizations required in partially observable \glspl*{ssm}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Experiments
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%","A central challenge in \gls*{bed} is the cost and complexity of estimating the \gls*{eig} (or its gradient) \citep{Rainforth24,huan2024optimal}. For models with tractable likelihoods $p(\by\!\mid\!\btheta,\bxi)$, the nested Monte Carlo (NMC) estimator has been studied in depth \citep{Rainforth18}, with subsequent work proposing variational formulations and bounds to improve convergence \citep{Foster19,Foster20,Foster21}. These approaches also enable differentiable objectives for gradient-based design in continuous spaces. However, they rely on explicit pointwise likelihood evaluation and do not extend directly to partially observable dynamical systems, where likelihoods require marginalization over latent states.

When likelihoods are implicit (intractable pointwise but possible to simulate from), likelihood-free design methods replace direct evaluation by surrogates or simulation-based objectives. Examples include variational mutual-information bounds and amortized estimators \citep{Kleinegesse20,Kleinegesse21,Ivanova21}, density-ratio or classifier-based estimates of information gain \citep{Kleinegesse19}, and \gls*{abc}-style utilities \citep{Dehideniya18,Drovandi13,Hainy16,Price16}. While effective in simulator settings, most such methods target \emph{static} design (optimize once, then deploy) rather than fully sequential/adaptive \gls*{bed}. Moreover, surrogate-based adaptive variants \citep[e.g.,][]{Ivanova21} typically require an offline dataset to train an implicit likelihood, which may be unavailable or quickly misspecified in evolving dynamical systems.

Design for dynamical systems has also been studied under full state observability. 
\cite{Iqbal24,iqbal2024recursive} consider adaptive design when the state is fully observed, making the likelihood tractable. 
Both papers leverage particle methods for joint state–parameter inference: \citep{Iqbal24} uses a reversed (“inside-out”) \gls*{smc2}, and \citep{iqbal2024recursive} adopts \gls*{npf}'s jittering to rejuvenate parameter particles. 
However, these recent advances in \gls*{bed} for dynamical systems assume full state observability and do not address the latent-state marginalizations required in partially observable \glspl*{ssm}.",
2511.06424v1,http://arxiv.org/abs/2511.06424v1,2025-11-09 15:41:27+00:00,Turbo-DDCM: Fast and Flexible Zero-Shot Diffusion-Based Image Compression,"While zero-shot diffusion-based compression methods have seen significant progress in recent years, they remain notoriously slow and computationally demanding. This paper presents an efficient zero-shot diffusion-based compression method that runs substantially faster than existing methods, while maintaining performance that is on par with the state-of-the-art techniques. Our method builds upon the recently proposed Denoising Diffusion Codebook Models (DDCMs) compression scheme. Specifically, DDCM compresses an image by sequentially choosing the diffusion noise vectors from reproducible random codebooks, guiding the denoiser's output to reconstruct the target image. We modify this framework with Turbo-DDCM, which efficiently combines a large number of noise vectors at each denoising step, thereby significantly reducing the number of required denoising operations. This modification is also coupled with an improved encoding protocol. Furthermore, we introduce two flexible variants of Turbo-DDCM, a priority-aware variant that prioritizes user-specified regions and a distortion-controlled variant that compresses an image based on a target PSNR rather than a target BPP. Comprehensive experiments position Turbo-DDCM as a compelling, practical, and flexible image compression scheme.","\paragraph{Non-Zero-Shot Diffusion-based Image Compression.}
Recent advances in diffusion-based compression have demonstrated impressive rate-distortion-perception performance by training models from scratch~\citep{NEURIPS2023_ccf6d8b4, ghouse2023residual,iwai2024controlling} or fine-tuning existing diffusion models~ \citep{korber2024perco, careil2024towards}. More recently, several one-step methods~\citep{park2025diffosinglestepdiffusionimage, xue2025onestepdiffusionbasedimagecompression} have tried to bypass the computational cost of iterative denoising by directly learning a mapping from the latent code to the clean signal in a single reverse step. However, all these approaches share the drawback of requiring training or fine-tuning tailored for compression. In contrast,  zero-shot methods preserve the diffusion model as a multi-purpose backbone, which can also serve for generation~\citep{ho2020denoising}, restoration~\citep{kawar2022denoising, raphaeli2025silosolvinginverseproblems,man2025proxiesdistortionconsistencyapplications}, editing~\citep{manor2024zeroshot, cohen2024slicedit}, etc.

\paragraph{Zero-shot Diffusion-based Image Compression.} Rather than training models or fine-tuning existing models, some recent methods use pretrained diffusion models in a zero-shot manner for image compression. PSC~\citep{elata2024zero} and DiffC~\citep{theis2022lossy} harness ideas from compressed sensing~\citep{DonohoCompressedSensing} and reverse channel coding (RCC)~\citep{theis2022algorithmscommunicationsamples}, respectively. DDCM~\citep{ohayon2025compressed} changes the standard DDPM process~\citep{ho2020denoising}
by sampling from a quantized Gaussian space, offering a simpler approach. However, all existing methods suffer from prohibitive computational demands, often requiring hundreds or even thousands of diffusion steps to compress a single image, making them unsuitable for practical usage. Recently, the RCC protocol used by DiffC was implemented on an optimized CUDA kernel~\citep{vonderfecht2025lossy}, which alleviates much of the computational demands. However, this method remains limited by custom hardware-specific accelerations, large deviations from target bitrate across different input images, and substantial runtime variation across different compression bitrates. In contrast, our proposed method is the fastest by a wide margin, without custom hardware-dependent optimizations, nearly constant runtime across all bitrates and a constant bitrate between different images given the same target bitrate.  At the same time, DiffC, DDCM, and our Turbo-DDCM share fundamental concepts in their underlying design, but extend it in different directions, as detailed in App.~\ref{app:turbo_ddcm_vs_diffc}.

\paragraph{ROI (priority-aware) compression methods.} Region-of-interest (ROI) compression~\citep{li2023roibaseddeepimagecompression,jin2025customizableroibaseddeepimage} prioritizes user-specified regions of an image by allocating to them a larger portion of the bits, resulting in higher fidelity in those regions on the expense of others. This paradigm can be useful in medical imaging~\citep{srivastava2025regionbasedmedicalimage}, video conference calls and more. Recently, it was demonstrated in diffusion-based compression~\citep{Xu_2025_CVPR}. To the best of our knowledge, we are the first to apply ROI compression to a zero-shot diffusion method. We do so in a general way, enabling per-pixel prioritization, which we refer to as priority-aware compression.","\paragraph{Non-Zero-Shot Diffusion-based Image Compression.}
Recent advances in diffusion-based compression have demonstrated impressive rate-distortion-perception performance by training models from scratch~\citep{NEURIPS2023_ccf6d8b4, ghouse2023residual,iwai2024controlling} or fine-tuning existing diffusion models~ \citep{korber2024perco, careil2024towards}. More recently, several one-step methods~\citep{park2025diffosinglestepdiffusionimage, xue2025onestepdiffusionbasedimagecompression} have tried to bypass the computational cost of iterative denoising by directly learning a mapping from the latent code to the clean signal in a single reverse step. However, all these approaches share the drawback of requiring training or fine-tuning tailored for compression. In contrast,  zero-shot methods preserve the diffusion model as a multi-purpose backbone, which can also serve for generation~\citep{ho2020denoising}, restoration~\citep{kawar2022denoising, raphaeli2025silosolvinginverseproblems,man2025proxiesdistortionconsistencyapplications}, editing~\citep{manor2024zeroshot, cohen2024slicedit}, etc.

\paragraph{Zero-shot Diffusion-based Image Compression.} Rather than training models or fine-tuning existing models, some recent methods use pretrained diffusion models in a zero-shot manner for image compression. PSC~\citep{elata2024zero} and DiffC~\citep{theis2022lossy} harness ideas from compressed sensing~\citep{DonohoCompressedSensing} and reverse channel coding (RCC)~\citep{theis2022algorithmscommunicationsamples}, respectively. DDCM~\citep{ohayon2025compressed} changes the standard DDPM process~\citep{ho2020denoising}
by sampling from a quantized Gaussian space, offering a simpler approach. However, all existing methods suffer from prohibitive computational demands, often requiring hundreds or even thousands of diffusion steps to compress a single image, making them unsuitable for practical usage. Recently, the RCC protocol used by DiffC was implemented on an optimized CUDA kernel~\citep{vonderfecht2025lossy}, which alleviates much of the computational demands. However, this method remains limited by custom hardware-specific accelerations, large deviations from target bitrate across different input images, and substantial runtime variation across different compression bitrates. In contrast, our proposed method is the fastest by a wide margin, without custom hardware-dependent optimizations, nearly constant runtime across all bitrates and a constant bitrate between different images given the same target bitrate.  At the same time, DiffC, DDCM, and our Turbo-DDCM share fundamental concepts in their underlying design, but extend it in different directions, as detailed in App.~\ref{app:turbo_ddcm_vs_diffc}.

\paragraph{ROI (priority-aware) compression methods.} Region-of-interest (ROI) compression~\citep{li2023roibaseddeepimagecompression,jin2025customizableroibaseddeepimage} prioritizes user-specified regions of an image by allocating to them a larger portion of the bits, resulting in higher fidelity in those regions on the expense of others. This paradigm can be useful in medical imaging~\citep{srivastava2025regionbasedmedicalimage}, video conference calls and more. Recently, it was demonstrated in diffusion-based compression~\citep{Xu_2025_CVPR}. To the best of our knowledge, we are the first to apply ROI compression to a zero-shot diffusion method. We do so in a general way, enabling per-pixel prioritization, which we refer to as priority-aware compression.",
2511.05187v1,http://arxiv.org/abs/2511.05187v1,2025-11-07 12:09:48+00:00,Linear Gradient Prediction with Control Variates,"We propose a new way of training neural networks, with the goal of reducing training cost. Our method uses approximate predicted gradients instead of the full gradients that require an expensive backward pass. We derive a control-variate-based technique that ensures our updates are unbiased estimates of the true gradient. Moreover, we propose a novel way to derive a predictor for the gradient inspired by the theory of the Neural Tangent Kernel. We empirically show the efficacy of the technique on a vision transformer classification task.","\paragraph{Synthetic Gradients} \citet{jaderberg2017decoupled} propose an algorithm that learns a backward pass of a neural network using another neural network. Our work is different in several ways. First, the emphasis of \citet{jaderberg2017decoupled} is to make signal propagation faster for large compute graph and possible for infinite compute graphs\footnote{\citet{jaderberg2017decoupled} derive a variant of TD learning which can estimate gradients for some classes of infinite compute graphs with a recurrent structure.}, while our main motivation is to avoid the expense of a full backward pass. Second, while \citet{jaderberg2017decoupled} tries to learn synthetic gradients as accurately as possible and uses the approximation to completely replace the true gradients, we use an approximation scheme which is knowingly imperfect but very cheap. Third, our de-biasing scheme ensures that the optimum the algorithm converges to the same optima that regular backward passes converge to, avoiding additional optima arising from using uncorrected approximate gradients \citep{czarnecki2017understanding}. Fourth, unlike \citet{jaderberg2017decoupled}, we exploit low NTK rank to approximate the gradients, making our approximation much more efficient. 

\paragraph{Partial Gradients} \citet{sun2017meprop} propose to replace backpropagation by a sparse update, which only updates some of the weights, where the weights to be updates are selected using a \mbox{top-k} heuristic. Our approach is different because it still follows the true gradients of the loss function, unlike the \mbox{top-k} heuristic. In fact, since expected value of our gradient is the same as standard backprop, we can accomplish convergence to the same critical point standard stochastic gradient descent converges to. 

\paragraph{Control Variates} Control Variates are an established technique for reducing gradient variance in statistical simulation \citep{kleijnen1975statistical} and have been applied to machine learning models including logistic regression classifiers \citep{wang2013variance}. Our de-biasing scheme is formally similar to using the predicted gradient as a control variate to reduce the variance of the true gradient. However, to our knowledge, ours is the first method that specifically applies the technique in the context of training large models in a way which leads to improvements in wall clock optimisation time.

\paragraph{Neural Tangent Kernel} The first work to derive an equivalence between training neural networks and learning using a kernel machine was by \citet{jacot2018neural}. It was later extended by many other researchers including \citet{yang2019wide} and \citet{liu2020linearity}. The study of the rank of the NTK was undertaken by \citet{bietti2020deep} and \citet{geifman2020similarity} for the true NTK and by \citet{murray2022characterizing} for the finite-dimensional case. In this work, our approach to these results is pragmatic in two ways. First, we rely on results about the eigenvalue to the extent we empirically test if a low-rank assumption about the NTK is sufficient to derive a useful approximation to the gradients. Second, we train our networks in the standard way, outside of the NTK regime. We sidestep the non-stationarity issue this approach introduces by periodically retraining the gradient predictor. We acknowledge there is a gap between theory and practice in our approach--while our result is theory-inspired rather than theory-supported, we nonetheless think it is useful.

\paragraph{Low-Rank Gradients} \citet{sagun2016eigenvalues} and \citet{gur2018gradient} identify approximate low-rank structure in the Hessian of neural networks and connect it to low-rank gradients. However, they do not connect it to the Neural Tangent Kernel. Moreover, recent work \citep{sonthalia2025low} shows that loss gradients of neural nets are approximately low rank (under limiting regularity assumptions). However, they do not focus on leveraging this insight to predict gradients. We fill this gap, using the predicted gradients to make a better training algorithm. 

\paragraph{Memory-Compute Tradeoff} The idea of trading compute time for an additional memory requirement is a staple of algorithm design. Our proposal can be thought of as being an instance of this idea--we reduce the compute necessary to converge to a useful solution, while requiring additional memory to store the basis of the gradient predictor matrix. Whether or not the memory requirement is excessive depends on a particular use-case.\footnote{The full backward pass has to store activations and that cost can dominate over storing the predictor basis for practical settings.} Note that work has been attempted in the direction opposite from ours: reducing memory usage at the expense of additional compute needed to recompute activations \citep{gomez2017reversible}. We believe both of these approaches can be useful in different settings. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\linewidth]{train_curve.pdf}
    \caption{CIFAR-10 Validation Accuracy vs Wall Clock Training Time. GPR stands for gradient prediction, which uses gradient prediction for 3/4 of the batch. The baseline uses full backward passes. The shaded area corresponds to standard errors for three random seeds per method.}
    \label{fig-cifar-results}
\end{figure}","\paragraph{Synthetic Gradients} \citet{jaderberg2017decoupled} propose an algorithm that learns a backward pass of a neural network using another neural network. Our work is different in several ways. First, the emphasis of \citet{jaderberg2017decoupled} is to make signal propagation faster for large compute graph and possible for infinite compute graphs\footnote{\citet{jaderberg2017decoupled} derive a variant of TD learning which can estimate gradients for some classes of infinite compute graphs with a recurrent structure.}, while our main motivation is to avoid the expense of a full backward pass. Second, while \citet{jaderberg2017decoupled} tries to learn synthetic gradients as accurately as possible and uses the approximation to completely replace the true gradients, we use an approximation scheme which is knowingly imperfect but very cheap. Third, our de-biasing scheme ensures that the optimum the algorithm converges to the same optima that regular backward passes converge to, avoiding additional optima arising from using uncorrected approximate gradients \citep{czarnecki2017understanding}. Fourth, unlike \citet{jaderberg2017decoupled}, we exploit low NTK rank to approximate the gradients, making our approximation much more efficient. 

\paragraph{Partial Gradients} \citet{sun2017meprop} propose to replace backpropagation by a sparse update, which only updates some of the weights, where the weights to be updates are selected using a \mbox{top-k} heuristic. Our approach is different because it still follows the true gradients of the loss function, unlike the \mbox{top-k} heuristic. In fact, since expected value of our gradient is the same as standard backprop, we can accomplish convergence to the same critical point standard stochastic gradient descent converges to. 

\paragraph{Control Variates} Control Variates are an established technique for reducing gradient variance in statistical simulation \citep{kleijnen1975statistical} and have been applied to machine learning models including logistic regression classifiers \citep{wang2013variance}. Our de-biasing scheme is formally similar to using the predicted gradient as a control variate to reduce the variance of the true gradient. However, to our knowledge, ours is the first method that specifically applies the technique in the context of training large models in a way which leads to improvements in wall clock optimisation time.

\paragraph{Neural Tangent Kernel} The first work to derive an equivalence between training neural networks and learning using a kernel machine was by \citet{jacot2018neural}. It was later extended by many other researchers including \citet{yang2019wide} and \citet{liu2020linearity}. The study of the rank of the NTK was undertaken by \citet{bietti2020deep} and \citet{geifman2020similarity} for the true NTK and by \citet{murray2022characterizing} for the finite-dimensional case. In this work, our approach to these results is pragmatic in two ways. First, we rely on results about the eigenvalue to the extent we empirically test if a low-rank assumption about the NTK is sufficient to derive a useful approximation to the gradients. Second, we train our networks in the standard way, outside of the NTK regime. We sidestep the non-stationarity issue this approach introduces by periodically retraining the gradient predictor. We acknowledge there is a gap between theory and practice in our approach--while our result is theory-inspired rather than theory-supported, we nonetheless think it is useful.

\paragraph{Low-Rank Gradients} \citet{sagun2016eigenvalues} and \citet{gur2018gradient} identify approximate low-rank structure in the Hessian of neural networks and connect it to low-rank gradients. However, they do not connect it to the Neural Tangent Kernel. Moreover, recent work \citep{sonthalia2025low} shows that loss gradients of neural nets are approximately low rank (under limiting regularity assumptions). However, they do not focus on leveraging this insight to predict gradients. We fill this gap, using the predicted gradients to make a better training algorithm. 

\paragraph{Memory-Compute Tradeoff} The idea of trading compute time for an additional memory requirement is a staple of algorithm design. Our proposal can be thought of as being an instance of this idea--we reduce the compute necessary to converge to a useful solution, while requiring additional memory to store the basis of the gradient predictor matrix. Whether or not the memory requirement is excessive depends on a particular use-case.\footnote{The full backward pass has to store activations and that cost can dominate over storing the predictor basis for practical settings.} Note that work has been attempted in the direction opposite from ours: reducing memory usage at the expense of additional compute needed to recompute activations \citep{gomez2017reversible}. We believe both of these approaches can be useful in different settings.",
2510.22044v1,http://arxiv.org/abs/2510.22044v1,2025-10-24 22:06:03+00:00,Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing,"Sampling from constrained statistical distributions is a fundamental task in various fields including Bayesian statistics, computational chemistry, and statistical physics. This article considers the cases where the constrained distribution is described by an unconstrained density, as well as additional equality and/or inequality constraints, which often make the constraint set nonconvex. Existing methods for nonconvex constraint set $Σ\subset \mathbb{R}^d$ defined by equality or inequality constraints commonly rely on costly projection steps. Moreover, they cannot handle equality and inequality constraints simultaneously as each method only specialized in one case. In addition, rigorous and quantitative convergence guarantee is often lacking. In this paper, we introduce Overdamped Langevin with LAnding (OLLA), a new framework that can design overdamped Langevin dynamics accommodating both equality and inequality constraints. The proposed dynamics also deterministically corrects trajectories along the normal direction of the constraint surface, thus obviating the need for explicit projections. We show that, under suitable regularity conditions on the target density and $Σ$, OLLA converges exponentially fast in $W_2$ distance to the constrained target density $ρ_Σ(x) \propto \exp(-f(x))dσ_Σ$. Lastly, through experiments, we demonstrate the efficiency of OLLA compared to projection-based constrained Langevin algorithms and their slack variable variants, highlighting its favorable computational cost and reasonable empirical mixing.","\vspace{-5pt}
\label{main:sec:Related Works}
We first focus our literature review on recent works that consider Langevin sampling under nonlinear constraints. One of the closest touching points is \cite{zhang2022sampling}, which describes a gradient descent approach on the KL divergence. The algorithm shares some similarities to ours in that projections are avoided, but the work focused on equality constraints only, whereas OLLA covers both equality and inequality constraints. The work \cite{power2024constrained} proposes the use of slack variables to incorporate inequality constraints to change a mixed problem into an equality-only problem, which comes at the cost of additional spurious dimensions. In a similar vein, \cite{zhang2024functional} designs a particle-based variational inference method to incorporate inequality constraints. The method is effective at sampling under inequality constraints only, suffers, however, from high computational cost in high dimensions due to the estimation of associated boundary integrals.

OLLA is inspired by recent methods in nonlinear optimization 
\cite{muehlebach2025accelerated, JMLR:v23:21-0798, schechtman2023orthogonal} that use a similar landing mechanism and avoid projections onto the feasible set. There has also been important work by  \cite{rousset2010free, lelievre2012langevin, lelievre2019hybrid} who introduced a constrained Langevin dynamics based on numerical schemes such as SHAKE, RATTLE \citep{ciccotti1986molecular, ryckaert1977numerical, andersen1983rattle}, and including Metropolis-Hastings corrections \citep{lelievre2019hybrid}. These works mainly focus on equality-only constraints, although inequality constraints can be incorporated via including slack variables or applying reflection at the boundary. We use these algorithms as baselines and refer them to Constrained Langevin (CLangevin) \citep{lelievre2012langevin}, Constrained Hamiltonian Monte Carlo (CHMC) \citep{lelievre2012langevin}, and Constrained generalized Hybrid Monte Carlo (CGHMC) \citep{lelievre2019hybrid}.

In the present work, constraints are handled through a careful decomposition of the stochastic dynamics on the boundary into normal and tangential parts, thereby avoiding projections and even enabling infeasible initialization. There have also been alternative algorithm designs that, however, do not share these features, for example, \citep{bubeck2018sampling, bubeck2015finite} (based on projection), \citep{kook2022sampling, kook2024gaussian} (barrier functions),
\citep{sato2025convergence} (reflections), or  \citep{zhang2020wasserstein,ahn2021efficient, li2022mirror} (mirror maps). Other works by \cite{karagulyan2020penalized, gurbuzbalaban2024penalized} introduce penalties for constraint violations or relax the notion of constraint satisfaction \cite{chamon2024constrained}. 
In addition, we note that, although closely related, constrained sampling is not the same as sampling on manifolds \cite{girolami2011riemann,cheng2022efficient,gatmiry2022convergence,kong2024convergence}.","\vspace{-5pt}
We first focus our literature review on recent works that consider Langevin sampling under nonlinear constraints. One of the closest touching points is \cite{zhang2022sampling}, which describes a gradient descent approach on the KL divergence. The algorithm shares some similarities to ours in that projections are avoided, but the work focused on equality constraints only, whereas OLLA covers both equality and inequality constraints. The work \cite{power2024constrained} proposes the use of slack variables to incorporate inequality constraints to change a mixed problem into an equality-only problem, which comes at the cost of additional spurious dimensions. In a similar vein, \cite{zhang2024functional} designs a particle-based variational inference method to incorporate inequality constraints. The method is effective at sampling under inequality constraints only, suffers, however, from high computational cost in high dimensions due to the estimation of associated boundary integrals.

OLLA is inspired by recent methods in nonlinear optimization 
\cite{muehlebach2025accelerated, JMLR:v23:21-0798, schechtman2023orthogonal} that use a similar landing mechanism and avoid projections onto the feasible set. There has also been important work by  \cite{rousset2010free, lelievre2012langevin, lelievre2019hybrid} who introduced a constrained Langevin dynamics based on numerical schemes such as SHAKE, RATTLE \citep{ciccotti1986molecular, ryckaert1977numerical, andersen1983rattle}, and including Metropolis-Hastings corrections \citep{lelievre2019hybrid}. These works mainly focus on equality-only constraints, although inequality constraints can be incorporated via including slack variables or applying reflection at the boundary. We use these algorithms as baselines and refer them to Constrained Langevin (CLangevin) \citep{lelievre2012langevin}, Constrained Hamiltonian Monte Carlo (CHMC) \citep{lelievre2012langevin}, and Constrained generalized Hybrid Monte Carlo (CGHMC) \citep{lelievre2019hybrid}.

In the present work, constraints are handled through a careful decomposition of the stochastic dynamics on the boundary into normal and tangential parts, thereby avoiding projections and even enabling infeasible initialization. There have also been alternative algorithm designs that, however, do not share these features, for example, \citep{bubeck2018sampling, bubeck2015finite} (based on projection), \citep{kook2022sampling, kook2024gaussian} (barrier functions),
\citep{sato2025convergence} (reflections), or  \citep{zhang2020wasserstein,ahn2021efficient, li2022mirror} (mirror maps). Other works by \cite{karagulyan2020penalized, gurbuzbalaban2024penalized} introduce penalties for constraint violations or relax the notion of constraint satisfaction \cite{chamon2024constrained}. 
In addition, we note that, although closely related, constrained sampling is not the same as sampling on manifolds \cite{girolami2011riemann,cheng2022efficient,gatmiry2022convergence,kong2024convergence}.",
2511.05396v1,http://arxiv.org/abs/2511.05396v1,2025-11-07 16:24:22+00:00,Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction,"Off-dynamics reinforcement learning (RL), where training and deployment transition dynamics are different, can be formulated as learning in a robust Markov decision process (RMDP) where uncertainties in transition dynamics are imposed. Existing literature mostly assumes access to generative models allowing arbitrary state-action queries or pre-collected datasets with a good state coverage of the deployment environment, bypassing the challenge of exploration. In this work, we study a more realistic and challenging setting where the agent is limited to online interaction with the training environment. To capture the intrinsic difficulty of exploration in online RMDPs, we introduce the supremal visitation ratio, a novel quantity that measures the mismatch between the training dynamics and the deployment dynamics. We show that if this ratio is unbounded, online learning becomes exponentially hard. We propose the first computationally efficient algorithm that achieves sublinear regret in online RMDPs with $f$-divergence based transition uncertainties. We also establish matching regret lower bounds, demonstrating that our algorithm achieves optimal dependence on both the supremal visitation ratio and the number of interaction episodes. Finally, we validate our theoretical results through comprehensive numerical experiments.","\textbf{CRMDPs and RRMDPs} The framework of CRMDPs was first introduced in the context of optimal control \citep{iyengar2005robust, nilim2005robust, xu2006robustness, wiesemann2013robust, mannor2016robust}, where the nominal MDP is assumed to be exactly known, and robust policies are obtained by solving a constrained max-min optimization problem. Subsequent works extended CRMDPs to the learning setting with access to a generative model \citep{zhou2021finite, yang2022toward, panaganti2022sample, shi2024curious}. More recently, CRMDPs have been studied in the offline learning setting, where only a pre-collected dataset from the nominal MDP is available through a behavior policy \citep{shi2024distributionally, panaganti2022robust, blanchet2023double, wang2024sample, liu2024minimax, liu2025linear}. To ensure that a robust policy can be learned from a reasonably sized offline dataset, these works make assumptions about the behavior policy (and implicitly, the nominal MDP) to guarantee sufficient coverage. Such assumptions include the robust single-policy clipped concentrability \citep{shi2024distributionally}, robust partial coverage \citep{blanchet2023double}, and uniformly well coverage assumptions \citep{liu2024minimax, wang2024sample}. The framework of RRMDPs was more recently proposed by \citet{yang2023robust} and \citet{zhang2024soft}, who studied it under the generative model setting and the offline setting, respectively. This line of work was extended to function approximation settings by \citet{panaganti2024model} and \citet{tang2024robust}, considering both hybrid offline-online and purely offline scenarios.

It is worth noting that CRMDPs are sometimes referred to in the literature as Robust MDPs (RMDPs) or Distributionally Robust MDPs (DRMDPs). To distinguish them from the regularized robust framework, we adopt the term CRMDPs. Similarly, RRMDPs appear under various names, including penalized robust MDPs \citep{yang2023robust}, soft robust MDPs \citep{zhang2024soft}, and robust $\phi$-regularized MDPs \citep{panaganti2024model}. We use the term RRMDPs to clearly differentiate them from CRMDPs while remaining consistent with the literature.

\textbf{Online RMDPs}
\citet{wang2021online, badrinath2021robust} studied the online learning for infinite-horizon RMDPs with $R$-contamination and more general uncertainty sets, respectively. Their algorithmic design and theoretical analysis rely on assuming access to exploratory policies, which implicitly assumes that the nominal MDP is sufficiently exploratory. In contrast, we revisit this challenge from a different angle, focusing on the information deficit issue induced by distributional shift. Our work differs from theirs in two key aspects. First, we introduce a novel quantity to characterize the hardness of exploration in the nominal MDP and analyze it thoroughly via both upper and lower bounds on the sample complexity. Second, instead of assuming access to exploratory policies, we design algorithms that explicitly incorporate exploration strategies tailored for finite-horizon tabular CRMDPs and RRMDPs with $(s,a)$-rectangular uncertainty sets defined by general $f$-divergences.

\citet{liu2024distributionally, lu2024distributionally, liu2024upper} focused on online robust RL under the specific setting of CRMDPs with uncertainty sets defined by the TV-distance, coupled with assumptions such as the existence of fail-states or vanishing minimal values. From an information-theoretic perspective, we show that these assumptions effectively circumvent the information deficit by constraining the direction of the distribution shift. In contrast, our work seeks to identify a more general sufficient condition for provably efficient online learning in CRMDPs—one that applies to arbitrary divergence-based uncertainty sets and does not rely on the fail-state or vanishing minimal value assumption.

\textbf{Off-dynamics RL}
A substantial body of empirical work addresses off-dynamics RL through the lens of domain adaptation and transfer learning \citep{eysenbach2020off, desai2020imitation, zhang2021sim2real, xu2023cross, wen2024contrastive, guo2024off, wang2024return, lyu2024odrl, da2025survey}, among others. In this paper, we focus on the robust MDP (RMDP) formulation of off-dynamics RL. We refer readers to the above works for complementary approaches along this orthogonal line of research.


%

%


%

%","\textbf{CRMDPs and RRMDPs} The framework of CRMDPs was first introduced in the context of optimal control \citep{iyengar2005robust, nilim2005robust, xu2006robustness, wiesemann2013robust, mannor2016robust}, where the nominal MDP is assumed to be exactly known, and robust policies are obtained by solving a constrained max-min optimization problem. Subsequent works extended CRMDPs to the learning setting with access to a generative model \citep{zhou2021finite, yang2022toward, panaganti2022sample, shi2024curious}. More recently, CRMDPs have been studied in the offline learning setting, where only a pre-collected dataset from the nominal MDP is available through a behavior policy \citep{shi2024distributionally, panaganti2022robust, blanchet2023double, wang2024sample, liu2024minimax, liu2025linear}. To ensure that a robust policy can be learned from a reasonably sized offline dataset, these works make assumptions about the behavior policy (and implicitly, the nominal MDP) to guarantee sufficient coverage. Such assumptions include the robust single-policy clipped concentrability \citep{shi2024distributionally}, robust partial coverage \citep{blanchet2023double}, and uniformly well coverage assumptions \citep{liu2024minimax, wang2024sample}. The framework of RRMDPs was more recently proposed by \citet{yang2023robust} and \citet{zhang2024soft}, who studied it under the generative model setting and the offline setting, respectively. This line of work was extended to function approximation settings by \citet{panaganti2024model} and \citet{tang2024robust}, considering both hybrid offline-online and purely offline scenarios.

It is worth noting that CRMDPs are sometimes referred to in the literature as Robust MDPs (RMDPs) or Distributionally Robust MDPs (DRMDPs). To distinguish them from the regularized robust framework, we adopt the term CRMDPs. Similarly, RRMDPs appear under various names, including penalized robust MDPs \citep{yang2023robust}, soft robust MDPs \citep{zhang2024soft}, and robust $\phi$-regularized MDPs \citep{panaganti2024model}. We use the term RRMDPs to clearly differentiate them from CRMDPs while remaining consistent with the literature.

\textbf{Online RMDPs}
\citet{wang2021online, badrinath2021robust} studied the online learning for infinite-horizon RMDPs with $R$-contamination and more general uncertainty sets, respectively. Their algorithmic design and theoretical analysis rely on assuming access to exploratory policies, which implicitly assumes that the nominal MDP is sufficiently exploratory. In contrast, we revisit this challenge from a different angle, focusing on the information deficit issue induced by distributional shift. Our work differs from theirs in two key aspects. First, we introduce a novel quantity to characterize the hardness of exploration in the nominal MDP and analyze it thoroughly via both upper and lower bounds on the sample complexity. Second, instead of assuming access to exploratory policies, we design algorithms that explicitly incorporate exploration strategies tailored for finite-horizon tabular CRMDPs and RRMDPs with $(s,a)$-rectangular uncertainty sets defined by general $f$-divergences.

\citet{liu2024distributionally, lu2024distributionally, liu2024upper} focused on online robust RL under the specific setting of CRMDPs with uncertainty sets defined by the TV-distance, coupled with assumptions such as the existence of fail-states or vanishing minimal values. From an information-theoretic perspective, we show that these assumptions effectively circumvent the information deficit by constraining the direction of the distribution shift. In contrast, our work seeks to identify a more general sufficient condition for provably efficient online learning in CRMDPs—one that applies to arbitrary divergence-based uncertainty sets and does not rely on the fail-state or vanishing minimal value assumption.

\textbf{Off-dynamics RL}
A substantial body of empirical work addresses off-dynamics RL through the lens of domain adaptation and transfer learning \citep{eysenbach2020off, desai2020imitation, zhang2021sim2real, xu2023cross, wen2024contrastive, guo2024off, wang2024return, lyu2024odrl, da2025survey}, among others. In this paper, we focus on the robust MDP (RMDP) formulation of off-dynamics RL. We refer readers to the above works for complementary approaches along this orthogonal line of research.",
2511.08878v1,http://arxiv.org/abs/2511.08878v1,2025-11-12 01:29:31+00:00,Covariance Scattering Transforms,"Machine learning and data processing techniques relying on covariance information are widespread as they identify meaningful patterns in unsupervised and unlabeled settings. As a prominent example, Principal Component Analysis (PCA) projects data points onto the eigenvectors of their covariance matrix, capturing the directions of maximum variance. This mapping, however, falls short in two directions: it fails to capture information in low-variance directions, relevant when, e.g., the data contains high-variance noise; and it provides unstable results in low-sample regimes, especially when covariance eigenvalues are close. CoVariance Neural Networks (VNNs), i.e., graph neural networks using the covariance matrix as a graph, show improved stability to estimation errors and learn more expressive functions in the covariance spectrum than PCA, but require training and operate in a labeled setup. To get the benefits of both worlds, we propose Covariance Scattering Transforms (CSTs), deep untrained networks that sequentially apply filters localized in the covariance spectrum to the input data and produce expressive hierarchical representations via nonlinearities. We define the filters as covariance wavelets that capture specific and detailed covariance spectral patterns. We improve CSTs' computational and memory efficiency via a pruning mechanism, and we prove that their error due to finite-sample covariance estimations is less sensitive to close covariance eigenvalues compared to PCA, improving their stability. Our experiments on age prediction from cortical thickness measurements on 4 datasets collecting patients with neurodegenerative diseases show that CSTs produce stable representations in low-data settings, as VNNs but without any training, and lead to comparable or better predictions w.r.t. more complex learning models.","\noindent \textbf{Covariance-based learning.}
Covariance information is at the basis of several data processing techniques. In the unsupervised domain, PCA~\cite{Jolliffe2002pca} and factor analysis~\cite{child2006essentials}, among others, are popular as they represent data in a low-rank space, where spurious correlations are removed and data dimension can be reduced.
However, PCA is generally unstable to finite-sample covariance estimation errors, leading to unreliable component estimation in low-data regimes. This issue has been tackled by coVariance Neural Networks (VNNs)~\cite{sihag2022covariance}, which rely on labeled data to estimate robust representations even in low-data regimes. This characteristic has made VNNs successful in a variety of settings, ranging from interpretable brain age estimation~\cite{sihag2024explainable, sihag2022covariance} to temporal data~\cite{cavallo2024stvnn}, sparse covariances~\cite{cavallo2024sparsecovarianceneuralnetworks,cavallo2025precision} and biased datasets~\cite{cavallo2025fair}.
Despite their success, VNNs and extensions need large quantities of labeled data for training, which may be unfeasible in practice. 
In this work, we provide a more flexible and robust framework to process data via their covariance information in an untrained manner.

\noindent \textbf{Wavelets and scattering transforms.} 
Wavelet transforms are popular tools in time, image and graph signal processing due to their space and frequency localization that allows for efficient signal representation and processing~\cite{mallat1999wavelet, hammond2011wavelets, shuman2015spectrum}. 
Scattering transforms are cascades of wavelets interleaved with nonlinearities that achieve untrained deep hierarchical representations, and have been shown successful in a variety of domains ranging from images~\cite{bruna2013invariant} to graphs~\cite{gama2019diffusion,gama2020stability,koke2022graph}, audio~\cite{anden2011multiscale} and simplicial complexes~\cite{madhu2024unsupervised}. Their main advantages are their stability to domain and signal perturbations as well as their capability to extract expressive frequency patterns in an untrained way.
In this work, we build on this literature to propose scattering transforms on covariance matrices, study their link with PCA and VNNs via covariance spectrum processing and their increased stability to finite-sample estimation errors.","\noindent \textbf{Covariance-based learning.}
Covariance information is at the basis of several data processing techniques. In the unsupervised domain, PCA~\cite{Jolliffe2002pca} and factor analysis~\cite{child2006essentials}, among others, are popular as they represent data in a low-rank space, where spurious correlations are removed and data dimension can be reduced.
However, PCA is generally unstable to finite-sample covariance estimation errors, leading to unreliable component estimation in low-data regimes. This issue has been tackled by coVariance Neural Networks (VNNs)~\cite{sihag2022covariance}, which rely on labeled data to estimate robust representations even in low-data regimes. This characteristic has made VNNs successful in a variety of settings, ranging from interpretable brain age estimation~\cite{sihag2024explainable, sihag2022covariance} to temporal data~\cite{cavallo2024stvnn}, sparse covariances~\cite{cavallo2024sparsecovarianceneuralnetworks,cavallo2025precision} and biased datasets~\cite{cavallo2025fair}.
Despite their success, VNNs and extensions need large quantities of labeled data for training, which may be unfeasible in practice. 
In this work, we provide a more flexible and robust framework to process data via their covariance information in an untrained manner.

\noindent \textbf{Wavelets and scattering transforms.} 
Wavelet transforms are popular tools in time, image and graph signal processing due to their space and frequency localization that allows for efficient signal representation and processing~\cite{mallat1999wavelet, hammond2011wavelets, shuman2015spectrum}. 
Scattering transforms are cascades of wavelets interleaved with nonlinearities that achieve untrained deep hierarchical representations, and have been shown successful in a variety of domains ranging from images~\cite{bruna2013invariant} to graphs~\cite{gama2019diffusion,gama2020stability,koke2022graph}, audio~\cite{anden2011multiscale} and simplicial complexes~\cite{madhu2024unsupervised}. Their main advantages are their stability to domain and signal perturbations as well as their capability to extract expressive frequency patterns in an untrained way.
In this work, we build on this literature to propose scattering transforms on covariance matrices, study their link with PCA and VNNs via covariance spectrum processing and their increased stability to finite-sample estimation errors.",
2510.16986v1,http://arxiv.org/abs/2510.16986v1,2025-10-19 20:03:48+00:00,Adaptive Sample Sharing for Linear Regression,"In many business settings, task-specific labeled data are scarce or costly to obtain, which limits supervised learning on a specific task. To address this challenge, we study sample sharing in the case of ridge regression: leveraging an auxiliary data set while explicitly protecting against negative transfer. We introduce a principled, data-driven rule that decides how many samples from an auxiliary dataset to add to the target training set. The rule is based on an estimate of the transfer gain i.e. the marginal reduction in the predictive error. Building on this estimator, we derive finite-sample guaranties: under standard conditions, the procedure borrows when it improves parameter estimation and abstains otherwise. In the Gaussian feature setting, we analyze which data set properties ensure that borrowing samples reduces the predictive error. We validate the approach in synthetic and real datasets, observing consistent gains over strong baselines and single-task training while avoiding negative transfer.","\label{sec:related_work}

\subsection{Previous contributions}

\paragraph{Transfer Learning: General Overview} Transfer learning (TL)\citep{PanYang2010,WeissKhoshgoftaarWang2016,zhuang2020comprehensive} accelerates target learning by exploiting knowledge from \emph{related} sources. 
%
Although it supports multitask learning\citep{EvgeniouPontil2004}, enables cross-domain diversification, and reduces the cost of training complex models such as deep neural networks~\citep{tan2018survey}, its central motivation remains data scarcity: TL offsets limited or costly target data by leveraging information from source tasks.
%
Canonical settings include \emph{domain adaptation}, where the input distributions differ, but the prediction task remains aligned~\citep{BenDavid2010,Csurka2017}, and \emph{pretraining–fine-tuning}, where a model trained on a large source data set is adapted to a smaller target~\citep{yosinski2014transferable}.
%
In practice, methods range from importance reweighting under covariate shift~\citep{Shimodaira2000,SugiyamaKrauledatMueller2007} and distribution/representation alignment techniques (\eg optimal transport) \citep{CourtyFlamaryTuiaRakotomamonjy2017} to parameter or prior sharing across tasks \citep{ArgyriouEvgeniouPontil2007}; multi-source approaches further combine heterogeneous sources through mixture models~\citep{MansourMohriRostamizadeh2009}.

% ---

A recurring challenge is \emph{negative transfer}~\citep{Rosenstein2005Transfer}, which occurs when information from source tasks degrades performance, leaving the target worse off than if it had been learned in isolation.~\citep{zhang2022survey}.
%
Recent work has sought to characterize this phenomenon, for example, by introducing the notion of a \emph{negative transfer gap}~\citep{wang2019characterizing}, designing statistical tests of transfer gains in linear regression~\citep{obst2021transfer}, or proposing the concept of \emph{transfer risk} and analyzing its theoretical properties~\citep{cao2023risk}.


\paragraph{Transfer Learning as Regularization for Linear Models} \quad In linear prediction, transfer learning is typically realized through regularization that biases the target parameters toward the source information.
%
Classical formulations couple source and target with penalty terms~\citep{EvgeniouPontil2004}, while hypothesis transfer methods fix a source estimator and shrink the target toward it, as in data-enriched regression~\citep{ChenOwenShi2014}.
%
Multitask formulations extend this idea by enforcing shared structure, for example, via group sparsity to align supports~\citep{ObozinskiWainwrightJordan2010} or low-rank constraints to induce a common subspace~\citep{ArgyriouEvgeniouPontil2007}; domain adaptation techniques such as feature augmentation offer an equivalent linear view~\citep{Daume2007}.
%
Other strategies include stability-based methods that control the effect of source samples~\citep{kuzborskij2013stability} and adaptive algorithms guided by Bayesian optimization~\citep{sorocky2020share}.
%
In all these approaches, coupling hyperparameters are tuned on held-out data, so success depends on source–target similarity; when this fails, negative transfer may occur~\citep{wang2019characterizing}.

% ---

Unlike these approaches, we do not alter the target objective with coupling penalties. Instead, we choose how many auxiliary samples to add via a data-driven rule with finite-sample guarantees, providing a complementary way to avoid negative transfer.

\paragraph{Selective Sample Sharing Across Tasks}

\emph{Sample sharing} denotes the direct inclusion of a selected subset of raw observations from an auxiliary data set into the target training set, in contrast to importance reweighting or objective coupling approaches.
%
The idea appears explicitly in several areas: in sequential decision making,~\citet{CherkaouiICML2025} study \emph{adaptive sample sharing} for multi-agent linear bandits; in scalable Bayesian inference,~\citet{deSouzaAcerbiAISTATS2022} introduce a parallel MCMC scheme that mitigates failure modes via \emph{sample sharing} between subposteriors.

% ---

To our knowledge, no prior work in supervised linear prediction provides a data-driven rule that selects how many auxiliary samples to borrow with finite-sample safety relative to target-only training; our method provides such a rule with finite-sample guarantees.

\subsection{Our contributions}

We propose a novel algorithm that addresses previous limitations and introduces key features that distinguish it from prior work.
\begin{enumerate}[itemsep=0.01pt, topsep=0.01pt]
    \item \emph{Target-task focused}: We avoid joint source-target objectives and instead decide \emph{how many} auxiliary samples to add to the target training set, borrowing only when it improves the target task and abstaining otherwise.
    \item \emph{Principled and conservative:} We quantify the bias induced by model mismatch and derive a conservative decision rule that, by construction, prevents target degradation.
    \item \emph{Theory and validation:} We provide a detailed analysis under an isotropic Gaussian design and extensive experiments on synthetic and real datasets against strong baselines, showing consistent gains while avoiding negative transfer.
\end{enumerate}","\subsection{Previous contributions}

\paragraph{Transfer Learning: General Overview} Transfer learning (TL)\citep{PanYang2010,WeissKhoshgoftaarWang2016,zhuang2020comprehensive} accelerates target learning by exploiting knowledge from \emph{related} sources. 

Although it supports multitask learning\citep{EvgeniouPontil2004}, enables cross-domain diversification, and reduces the cost of training complex models such as deep neural networks~\citep{tan2018survey}, its central motivation remains data scarcity: TL offsets limited or costly target data by leveraging information from source tasks.

Canonical settings include \emph{domain adaptation}, where the input distributions differ, but the prediction task remains aligned~\citep{BenDavid2010,Csurka2017}, and \emph{pretraining–fine-tuning}, where a model trained on a large source data set is adapted to a smaller target~\citep{yosinski2014transferable}.

In practice, methods range from importance reweighting under covariate shift~\citep{Shimodaira2000,SugiyamaKrauledatMueller2007} and distribution/representation alignment techniques (\eg optimal transport) \citep{CourtyFlamaryTuiaRakotomamonjy2017} to parameter or prior sharing across tasks \citep{ArgyriouEvgeniouPontil2007}; multi-source approaches further combine heterogeneous sources through mixture models~\citep{MansourMohriRostamizadeh2009}.



A recurring challenge is \emph{negative transfer}~\citep{Rosenstein2005Transfer}, which occurs when information from source tasks degrades performance, leaving the target worse off than if it had been learned in isolation.~\citep{zhang2022survey}.

Recent work has sought to characterize this phenomenon, for example, by introducing the notion of a \emph{negative transfer gap}~\citep{wang2019characterizing}, designing statistical tests of transfer gains in linear regression~\citep{obst2021transfer}, or proposing the concept of \emph{transfer risk} and analyzing its theoretical properties~\citep{cao2023risk}.


\paragraph{Transfer Learning as Regularization for Linear Models} \quad In linear prediction, transfer learning is typically realized through regularization that biases the target parameters toward the source information.

Classical formulations couple source and target with penalty terms~\citep{EvgeniouPontil2004}, while hypothesis transfer methods fix a source estimator and shrink the target toward it, as in data-enriched regression~\citep{ChenOwenShi2014}.

Multitask formulations extend this idea by enforcing shared structure, for example, via group sparsity to align supports~\citep{ObozinskiWainwrightJordan2010} or low-rank constraints to induce a common subspace~\citep{ArgyriouEvgeniouPontil2007}; domain adaptation techniques such as feature augmentation offer an equivalent linear view~\citep{Daume2007}.

Other strategies include stability-based methods that control the effect of source samples~\citep{kuzborskij2013stability} and adaptive algorithms guided by Bayesian optimization~\citep{sorocky2020share}.

In all these approaches, coupling hyperparameters are tuned on held-out data, so success depends on source–target similarity; when this fails, negative transfer may occur~\citep{wang2019characterizing}.



Unlike these approaches, we do not alter the target objective with coupling penalties. Instead, we choose how many auxiliary samples to add via a data-driven rule with finite-sample guarantees, providing a complementary way to avoid negative transfer.

\paragraph{Selective Sample Sharing Across Tasks}

\emph{Sample sharing} denotes the direct inclusion of a selected subset of raw observations from an auxiliary data set into the target training set, in contrast to importance reweighting or objective coupling approaches.

The idea appears explicitly in several areas: in sequential decision making,~\citet{CherkaouiICML2025} study \emph{adaptive sample sharing} for multi-agent linear bandits; in scalable Bayesian inference,~\citet{deSouzaAcerbiAISTATS2022} introduce a parallel MCMC scheme that mitigates failure modes via \emph{sample sharing} between subposteriors.



To our knowledge, no prior work in supervised linear prediction provides a data-driven rule that selects how many auxiliary samples to borrow with finite-sample safety relative to target-only training; our method provides such a rule with finite-sample guarantees.

\subsection{Our contributions}

We propose a novel algorithm that addresses previous limitations and introduces key features that distinguish it from prior work.
\begin{enumerate}[itemsep=0.01pt, topsep=0.01pt]
    \item \emph{Target-task focused}: We avoid joint source-target objectives and instead decide \emph{how many} auxiliary samples to add to the target training set, borrowing only when it improves the target task and abstaining otherwise.
    \item \emph{Principled and conservative:} We quantify the bias induced by model mismatch and derive a conservative decision rule that, by construction, prevents target degradation.
    \item \emph{Theory and validation:} We provide a detailed analysis under an isotropic Gaussian design and extensive experiments on synthetic and real datasets against strong baselines, showing consistent gains while avoiding negative transfer.
\end{enumerate}",
2511.06239v1,http://arxiv.org/abs/2511.06239v1,2025-11-09 05:51:03+00:00,Functional Adjoint Sampler: Scalable Sampling on Infinite Dimensional Spaces,"Learning-based methods for sampling from the Gibbs distribution in finite-dimensional spaces have progressed quickly, yet theory and algorithmic design for infinite-dimensional function spaces remain limited. This gap persists despite their strong potential for sampling the paths of conditional diffusion processes, enabling efficient simulation of trajectories of diffusion processes that respect rare events or boundary constraints. In this work, we present the adjoint sampler for infinite-dimensional function spaces, a stochastic optimal control-based diffusion sampler that operates in function space and targets Gibbs-type distributions on infinite-dimensional Hilbert spaces. Our Functional Adjoint Sampler (FAS) generalizes Adjoint Sampling (Havens et al., 2025) to Hilbert spaces based on a SOC theory called stochastic maximum principle, yielding a simple and scalable matching-type objective for a functional representation. We show that FAS achieves superior transition path sampling performance across synthetic potential and real molecular systems, including Alanine Dipeptide and Chignolin.","We leave brief introduction of transition path sampling and related work~\Cref{sec:brief introduction of the transition path theory}.
\paragraph{Infinite-dimensional generative models} 
Many recent works extend finite-dimensional generative models to infinite-dimensional function spaces. Score-based diffusion models~\citep{song2021scorebased} have been extended to function space~\citep{lim2023score, lim2023scorebased, pidstrigach2023infinite, franzese2024continuous}. Later, matching-based generative models follow the same direction. For example, flow matching~\citep{lipman2023flow} has been generalized to function space~\citep{kerrigan2024functional}, and Schrödinger bridge matching~\citep{shi2024diffusion} has been adapted as well~\citep{park2024stochastic}. However, these algorithms operate in a \textit{sample-to-sample} setting where paired samples from both distributions are available for training. To the best of our knowledge, scalable function-space generative modeling for \textit{sample-to-energy} such as targeting sampling from a Gibbs distribution $\pi(\bx) \propto e^{-U(\bx)}$ in~\eqref{eq:target measure}, has not been investigated.

\paragraph{Diffusion sampler} Diffusion samplers address the \textit{sample-to-energy} setting, where the goal is to draw samples from the Gibbs distribution $\pi(\bx)\propto e^{-U(\bx)}$. Based on SOC theory, this line of work reformulates sampling as an optimization problem and methods for learning such samplers have advanced rapidly~\citep{zhang2022path, vargas2023bayesian, berner2022optimal}. Due to the target is specified only through a possibly complex energy functional $U$, optimization tends to be computationally demanding. This challenge motivates adjoint-based samplers~\citep{havens2025adjoint, liu2025adjoint}, which avoid simulation-based targets and yield a method that scales better than vanilla SOC approaches. Existing approaches are limited to finite-dimensional settings, and extensions to function space remain unexplored, despite the appeal and practical importance.","We leave brief introduction of transition path sampling and related work~\Cref{sec:brief introduction of the transition path theory}.
\paragraph{Infinite-dimensional generative models} 
Many recent works extend finite-dimensional generative models to infinite-dimensional function spaces. Score-based diffusion models~\citep{song2021scorebased} have been extended to function space~\citep{lim2023score, lim2023scorebased, pidstrigach2023infinite, franzese2024continuous}. Later, matching-based generative models follow the same direction. For example, flow matching~\citep{lipman2023flow} has been generalized to function space~\citep{kerrigan2024functional}, and Schrödinger bridge matching~\citep{shi2024diffusion} has been adapted as well~\citep{park2024stochastic}. However, these algorithms operate in a \textit{sample-to-sample} setting where paired samples from both distributions are available for training. To the best of our knowledge, scalable function-space generative modeling for \textit{sample-to-energy} such as targeting sampling from a Gibbs distribution $\pi(\bx) \propto e^{-U(\bx)}$ in~\eqref{eq:target measure}, has not been investigated.

\paragraph{Diffusion sampler} Diffusion samplers address the \textit{sample-to-energy} setting, where the goal is to draw samples from the Gibbs distribution $\pi(\bx)\propto e^{-U(\bx)}$. Based on SOC theory, this line of work reformulates sampling as an optimization problem and methods for learning such samplers have advanced rapidly~\citep{zhang2022path, vargas2023bayesian, berner2022optimal}. Due to the target is specified only through a possibly complex energy functional $U$, optimization tends to be computationally demanding. This challenge motivates adjoint-based samplers~\citep{havens2025adjoint, liu2025adjoint}, which avoid simulation-based targets and yield a method that scales better than vanilla SOC approaches. Existing approaches are limited to finite-dimensional settings, and extensions to function space remain unexplored, despite the appeal and practical importance.",
2511.07671v1,http://arxiv.org/abs/2511.07671v1,2025-11-10 22:25:01+00:00,Robust Experimental Design via Generalised Bayesian Inference,"Bayesian optimal experimental design is a principled framework for conducting experiments that leverages Bayesian inference to quantify how much information one can expect to gain from selecting a certain design. However, accurate Bayesian inference relies on the assumption that one's statistical model of the data-generating process is correctly specified. If this assumption is violated, Bayesian methods can lead to poor inference and estimates of information gain. Generalised Bayesian (or Gibbs) inference is a more robust probabilistic inference framework that replaces the likelihood in the Bayesian update by a suitable loss function. In this work, we present Generalised Bayesian Optimal Experimental Design (GBOED), an extension of Gibbs inference to the experimental design setting which achieves robustness in both design and inference. Using an extended information-theoretic framework, we derive a new acquisition function, the Gibbs expected information gain (Gibbs EIG). Our empirical results demonstrate that GBOED enhances robustness to outliers and incorrect assumptions about the outcome noise distribution.","\label{sec_relatedwork}

Several approaches have been proposed to tackle model misspecification in BOED. 
Many fall under the $\mathcal{M}$-closed setting, where the true model is assumed to exist amongst a known set of possible models. BOED could be applied to the problem of selecting the model that best explains the data within this set \citep{cavagnaro, Hainy_2022}. In a similar avenue, one could manipulate the utility function to enable robustness to an entire set of models, by taking an expectation over data generated under this set of models \citep{catanach2023metrics}. Another approach is to take an expectation of the utility function under a single alternative model, which, for example, is thought to better capture the true DGP \citep{overstall2022bayesian}. Finally, one could use an alternative acquisition function to select designs that enhance robustness to model misspecification \citep{forstermisspec, tang2025generalizationanalysisbayesianoptimal}. GBOED not only enables robustness in design, but also in inference through generalised Bayesian inference.

The idea of using Gibbs inference to perform experimental design was first proposed by \citet{overstall2023gibbsoptimaldesignexperiments}. However, their framework requires that an alternative model, coined the \textit{designer distribution}, is made available. This distribution is assumed to be flexible and close to the true DGP, and allows one to compute the expected utility using draws from this distribution. The problem with this version of conducting experimental design is that the assumption usually fails: we are often not able to choose a model that we know is certainly close to the true DGP. Our approach avoids making this assumption, using Gibbs inference (informed by a -- possibly misspecified -- statistical model) to induce robustness into the experimental design procedure. Here, we are open to the possibility that, while misspecified, the statistical model has information relevant to an experimenter, and is our best understanding of how reality operates. This enables the use of loss functions that can directly take the statistical model into account when conducting Gibbs inference, in particular, through scoring rules \citep{dawid2014theory, Giummol__2018}. In addition, we compute the expected utility in an information-theoretic fashion using Gibbs measures, rather than straightforwardly taking an expectation wrt the statistical model (as one would do following \citet{overstall2023gibbsoptimaldesignexperiments}; see \Appref{importanceofimportanceweight} for a comparison between our approach and \citet{overstall2023gibbsoptimaldesignexperiments}). 

Our approach also departs from that of \citet{overstall2023gibbsoptimaldesignexperiments} in that \citet{overstall2023gibbsoptimaldesignexperiments} make a normal approximation of the Gibbs posterior -- utilising this approximation both in performing inference and in computing the expected utility. Although there are a number of scenarios under which normal approximations in the misspecified setting are viable (see \citet{bochkina2023bernstein} for a review), they generally require access to a large enough dataset for the approximation to be valid. The requirement of a large dataset is usually not satisfied in the experimental design setting.","Several approaches have been proposed to tackle model misspecification in BOED. 
Many fall under the $\mathcal{M}$-closed setting, where the true model is assumed to exist amongst a known set of possible models. BOED could be applied to the problem of selecting the model that best explains the data within this set \citep{cavagnaro, Hainy_2022}. In a similar avenue, one could manipulate the utility function to enable robustness to an entire set of models, by taking an expectation over data generated under this set of models \citep{catanach2023metrics}. Another approach is to take an expectation of the utility function under a single alternative model, which, for example, is thought to better capture the true DGP \citep{overstall2022bayesian}. Finally, one could use an alternative acquisition function to select designs that enhance robustness to model misspecification \citep{forstermisspec, tang2025generalizationanalysisbayesianoptimal}. GBOED not only enables robustness in design, but also in inference through generalised Bayesian inference.

The idea of using Gibbs inference to perform experimental design was first proposed by \citet{overstall2023gibbsoptimaldesignexperiments}. However, their framework requires that an alternative model, coined the \textit{designer distribution}, is made available. This distribution is assumed to be flexible and close to the true DGP, and allows one to compute the expected utility using draws from this distribution. The problem with this version of conducting experimental design is that the assumption usually fails: we are often not able to choose a model that we know is certainly close to the true DGP. Our approach avoids making this assumption, using Gibbs inference (informed by a -- possibly misspecified -- statistical model) to induce robustness into the experimental design procedure. Here, we are open to the possibility that, while misspecified, the statistical model has information relevant to an experimenter, and is our best understanding of how reality operates. This enables the use of loss functions that can directly take the statistical model into account when conducting Gibbs inference, in particular, through scoring rules \citep{dawid2014theory, Giummol__2018}. In addition, we compute the expected utility in an information-theoretic fashion using Gibbs measures, rather than straightforwardly taking an expectation wrt the statistical model (as one would do following \citet{overstall2023gibbsoptimaldesignexperiments}; see \Appref{importanceofimportanceweight} for a comparison between our approach and \citet{overstall2023gibbsoptimaldesignexperiments}). 

Our approach also departs from that of \citet{overstall2023gibbsoptimaldesignexperiments} in that \citet{overstall2023gibbsoptimaldesignexperiments} make a normal approximation of the Gibbs posterior -- utilising this approximation both in performing inference and in computing the expected utility. Although there are a number of scenarios under which normal approximations in the misspecified setting are viable (see \citet{bochkina2023bernstein} for a review), they generally require access to a large enough dataset for the approximation to be valid. The requirement of a large dataset is usually not satisfied in the experimental design setting.",
2511.09925v1,http://arxiv.org/abs/2511.09925v1,2025-11-13 03:40:10+00:00,Global Convergence of Four-Layer Matrix Factorization under Random Initialization,"Gradient descent dynamics on the deep matrix factorization problem is extensively studied as a simplified theoretical model for deep neural networks. Although the convergence theory for two-layer matrix factorization is well-established, no global convergence guarantee for general deep matrix factorization under random initialization has been established to date. To address this gap, we provide a polynomial-time global convergence guarantee for randomly initialized gradient descent on four-layer matrix factorization, given certain conditions on the target matrix and a standard balanced regularization term. Our analysis employs new techniques to show saddle-avoidance properties of gradient decent dynamics, and extends previous theories to characterize the change in eigenvalues of layer weights.","% Multiple works have addressed phenomena like convergence, implicit regularization, and landscape analysis under diverse settings.
For two-layer matrix factorization, the global convergence of symmetric case has been established under various settings \citep{jain2017globalconvergencenonconvexgradient, li2019algorithmicregularizationoverparameterizedmatrix, Chen_2019}. For asymmetric matrix factorization case with objective $\mathcal{L} = \frac{1}{2} \|UV^\top - \Sigma\|_F^2$, the following homogeneity issue occurs: the prediction result remains the same if one layer is multiplied by a positive constant while the other is divided by the same, introducing significant challenges in convergence analyzing (\cite{lee2016gradientdescentconvergesminimizers}, Proposition 4.11). \cite{tu2016lowranksolutionslinearmatrix} and \cite{ge2017spuriouslocalminimanonconvex} tackles this problem by manually adding a regularization term on the objective function. \cite{du2018algorithmicregularizationlearningdeep} discovers that gradient descent automatically balances the magnitudes of layers under small initialization, providing analysis of global convergence with polynomial time under decayed learning rate, while removing the regularization term. \citet{ye2021globalconvergencegradientdescent} extends the convergence analysis to constant learning rate. 

% Compared with two-layer setting, deep matrix factorization (or to say, deep linear neural network) with objective $\mathcal{L} = \frac{1}{2} \|\Sigma - W_N W_{N-1} \cdots W_1 \|_F^2$, $N\ge 3$ shares a larger degree of freedom and requires more difficult analysis. 

\cite{kawaguchi2016deeplearningpoorlocal} analyzes landscape for general DLN, showing there exists saddle points with no negative eigenvalues of Hessian for depth over three. \cite{bartlett2018gradientdescentidentityinitialization} analyzes the dynamic under identity initialization, proving polynomial convergence with target matrix near initialization or symmetric positive definite, but such initialization fails to converge when target matrix is symmetric and has a negative eigenvalue. 
\cite{arora2019convergenceanalysisgradientdescent} provides global convergence proof under specific deep linear neural network structures and initialization scheme , requiring the initial loss to be smaller than the loss of any rank-deficient solution. \cite{ji2019gradientdescentalignslayers} conducted the proof of convergence on general deep neural networks with similar requirements on the initial loss. \cite{arora2019implicitregularizationdeepmatrix} simplifies the training dynamics of deep linear neural network into the dynamic of singular values and singular vectors of product matrix under balanced initialization, providing theoretical illustration of local convergence when singular vectors are stationary. \citet{du2019width} proves global convergence for wide linear networks under the neural tangent kernel (NTK) regime. More recent works focus on GD dynamics under (approximately) balanced initialization schemes \citep{min2023convergence} or the $2$-layer case \citep{min2021explicit, xiong2023over, tarmoun2021understanding}. 
\cite{chizat2024infinite} studies the infinite-width limit of DLN in the mean field regime. However, none of these results imply a global convergence guarantee for general DLN with $N>2$ under random initialization.","For two-layer matrix factorization, the global convergence of symmetric case has been established under various settings \citep{jain2017globalconvergencenonconvexgradient, li2019algorithmicregularizationoverparameterizedmatrix, Chen_2019}. For asymmetric matrix factorization case with objective $\mathcal{L} = \frac{1}{2} \|UV^\top - \Sigma\|_F^2$, the following homogeneity issue occurs: the prediction result remains the same if one layer is multiplied by a positive constant while the other is divided by the same, introducing significant challenges in convergence analyzing (\cite{lee2016gradientdescentconvergesminimizers}, Proposition 4.11). \cite{tu2016lowranksolutionslinearmatrix} and \cite{ge2017spuriouslocalminimanonconvex} tackles this problem by manually adding a regularization term on the objective function. \cite{du2018algorithmicregularizationlearningdeep} discovers that gradient descent automatically balances the magnitudes of layers under small initialization, providing analysis of global convergence with polynomial time under decayed learning rate, while removing the regularization term. \citet{ye2021globalconvergencegradientdescent} extends the convergence analysis to constant learning rate. 



\cite{kawaguchi2016deeplearningpoorlocal} analyzes landscape for general DLN, showing there exists saddle points with no negative eigenvalues of Hessian for depth over three. \cite{bartlett2018gradientdescentidentityinitialization} analyzes the dynamic under identity initialization, proving polynomial convergence with target matrix near initialization or symmetric positive definite, but such initialization fails to converge when target matrix is symmetric and has a negative eigenvalue. 
\cite{arora2019convergenceanalysisgradientdescent} provides global convergence proof under specific deep linear neural network structures and initialization scheme , requiring the initial loss to be smaller than the loss of any rank-deficient solution. \cite{ji2019gradientdescentalignslayers} conducted the proof of convergence on general deep neural networks with similar requirements on the initial loss. \cite{arora2019implicitregularizationdeepmatrix} simplifies the training dynamics of deep linear neural network into the dynamic of singular values and singular vectors of product matrix under balanced initialization, providing theoretical illustration of local convergence when singular vectors are stationary. \citet{du2019width} proves global convergence for wide linear networks under the neural tangent kernel (NTK) regime. More recent works focus on GD dynamics under (approximately) balanced initialization schemes \citep{min2023convergence} or the $2$-layer case \citep{min2021explicit, xiong2023over, tarmoun2021understanding}. 
\cite{chizat2024infinite} studies the infinite-width limit of DLN in the mean field regime. However, none of these results imply a global convergence guarantee for general DLN with $N>2$ under random initialization.",
2511.07272v1,http://arxiv.org/abs/2511.07272v1,2025-11-10 16:18:04+00:00,Understanding the role of depth in the neural tangent kernel for overparameterized neural networks,"Overparameterized fully-connected neural networks have been shown to behave like kernel models when trained with gradient descent, under mild conditions on the width, the learning rate, and the parameter initialization. In the limit of infinitely large widths and small learning rate, the kernel that is obtained allows to represent the output of the learned model with a closed-form solution. This closed-form solution hinges on the invertibility of the limiting kernel, a property that often holds on real-world datasets. In this work, we analyze the sensitivity of large ReLU networks to increasing depths by characterizing the corresponding limiting kernel. Our theoretical results demonstrate that the normalized limiting kernel approaches the matrix of ones. In contrast, they show the corresponding closed-form solution approaches a fixed limit on the sphere. We empirically evaluate the order of magnitude in network depth required to observe this convergent behavior, and we describe the essential properties that enable the generalization of our results to other kernels.","\label{sec:related_work}

% Connection of kernel methods and ANNs
The learning dynamics of overparameterized neural networks have been extensively studied through the lenses of kernel methods and Hessian-based analysis. A key development in this area is the neural tangent kernel (NTK), introduced by~\citet{jacot2018neural}, which describes how infinitely wide fully-connected neural networks trained with gradient descent evolve linearly in function space. The NTK framework formalizes how, under common assumptions—particularly Gaussian initialization and wide-layer limits—neural networks behave similarly to kernel methods during training. \citet{arora2019exact} extend the NTK to convolutional neural networks (CNN), further highlighting the framework's capacity to capture learning dynamics.

% Previous work supports the interest on kernel methods
Subsequent work has reinforced the kernel-based interpretation of training dynamics through analyses of the Hessian matrix. \citet{liu2020linearity, liu2022loss} and \citet{belkin2021fit} demonstrate that the loss landscape of overparameterized neural networks often exhibits near-linearity, with low-curvature regions and small-norm Hessians, supporting the NTK-based approximation. These findings suggest that network outputs are relatively stable during training, especially for wide architectures with standard initialization. \citet{lee2020finite} provide an in-depth empirical analysis of NTK models and their performance compared to finite-width neural networks of various architectures (e.g. fully-connected, CNNs). One key observation is that NTKs often outperform finite-width networks, yet are usually surpassed by conventional CNNs. 

% Limitations of NTK
The NTK typically requires Monte Carlo estimation of expectations over Gaussian distributions, especially when nonlinearities from activations are involved. This reliance on sampling can be computationally expensive and introduces variance in the resulting kernel evaluations. However, closed-form expressions have been derived for certain activation functions, such as ReLU and leaky ReLU~\citep{tsuchida2018invariance}. Additionally, while prior work has largely focused on width, less attention has been paid to how depth affects NTK sensitivity to initialization and the associated multiplicity in network outputs. Among the few works addressing this dimension, \citet{bietti2020deep} show that for the uniform measure on the sphere, the reproducing kernel (c.f. reproducing kernel Hilbert space or RKHS) leads to the same representation power regardless of the network depth. This raises the question of the significance of depth for the NTK. Further insights are provided by~\citet{nguyen2021tight}, who derive an asymptotic lower bound on the smallest eigenvalue of the NTK through the Hermite expansion of the kernel. This result can be used to derive bounds on the generalization of the model~\citep{arora2019fine} and gives a better grasp on understanding the role that depth plays in both convergence and generalization. \citet{murray2023characterizing} characterize the full spectrum of the NTK via the Hermite expansion for arbitrary datasets on the sphere. They recover an empirical observation that the eigenvalues of the NTK follow power law decay with respect to the size of the training set; \citet{li2024eigenvalue} extend this line of results to general domains beyond the sphere. In addition to the previous result from \citet{jacot2018neural} regarding the convergence to kernel regression in the infinite width case, they also establish a uniform convergence bound to the NTK regressor for the output of the trained model. This improves upon the pointwise convergence bounds found in \citet{lee2019wide, arora2019exact, allen2019convergence}. As in the initial paper by \citet{jacot2018neural}, the limiting kernel is observed to be deterministic. This is contrasted with the work of \citet{hanin2020finite}, where the authors characterize the NTK directly through the ratio of its second and first moment. They obtain a limiting result showing that if the ratio of depth to width is unbounded, the NTK has a much higher variance than mean, implying that it is highly stochastic.","The learning dynamics of overparameterized neural networks have been extensively studied through the lenses of kernel methods and Hessian-based analysis. A key development in this area is the neural tangent kernel (NTK), introduced by~\citet{jacot2018neural}, which describes how infinitely wide fully-connected neural networks trained with gradient descent evolve linearly in function space. The NTK framework formalizes how, under common assumptions—particularly Gaussian initialization and wide-layer limits—neural networks behave similarly to kernel methods during training. \citet{arora2019exact} extend the NTK to convolutional neural networks (CNN), further highlighting the framework's capacity to capture learning dynamics.


Subsequent work has reinforced the kernel-based interpretation of training dynamics through analyses of the Hessian matrix. \citet{liu2020linearity, liu2022loss} and \citet{belkin2021fit} demonstrate that the loss landscape of overparameterized neural networks often exhibits near-linearity, with low-curvature regions and small-norm Hessians, supporting the NTK-based approximation. These findings suggest that network outputs are relatively stable during training, especially for wide architectures with standard initialization. \citet{lee2020finite} provide an in-depth empirical analysis of NTK models and their performance compared to finite-width neural networks of various architectures (e.g. fully-connected, CNNs). One key observation is that NTKs often outperform finite-width networks, yet are usually surpassed by conventional CNNs. 


The NTK typically requires Monte Carlo estimation of expectations over Gaussian distributions, especially when nonlinearities from activations are involved. This reliance on sampling can be computationally expensive and introduces variance in the resulting kernel evaluations. However, closed-form expressions have been derived for certain activation functions, such as ReLU and leaky ReLU~\citep{tsuchida2018invariance}. Additionally, while prior work has largely focused on width, less attention has been paid to how depth affects NTK sensitivity to initialization and the associated multiplicity in network outputs. Among the few works addressing this dimension, \citet{bietti2020deep} show that for the uniform measure on the sphere, the reproducing kernel (c.f. reproducing kernel Hilbert space or RKHS) leads to the same representation power regardless of the network depth. This raises the question of the significance of depth for the NTK. Further insights are provided by~\citet{nguyen2021tight}, who derive an asymptotic lower bound on the smallest eigenvalue of the NTK through the Hermite expansion of the kernel. This result can be used to derive bounds on the generalization of the model~\citep{arora2019fine} and gives a better grasp on understanding the role that depth plays in both convergence and generalization. \citet{murray2023characterizing} characterize the full spectrum of the NTK via the Hermite expansion for arbitrary datasets on the sphere. They recover an empirical observation that the eigenvalues of the NTK follow power law decay with respect to the size of the training set; \citet{li2024eigenvalue} extend this line of results to general domains beyond the sphere. In addition to the previous result from \citet{jacot2018neural} regarding the convergence to kernel regression in the infinite width case, they also establish a uniform convergence bound to the NTK regressor for the output of the trained model. This improves upon the pointwise convergence bounds found in \citet{lee2019wide, arora2019exact, allen2019convergence}. As in the initial paper by \citet{jacot2018neural}, the limiting kernel is observed to be deterministic. This is contrasted with the work of \citet{hanin2020finite}, where the authors characterize the NTK directly through the ratio of its second and first moment. They obtain a limiting result showing that if the ratio of depth to width is unbounded, the NTK has a much higher variance than mean, implying that it is highly stochastic.",
2511.05460v1,http://arxiv.org/abs/2511.05460v1,2025-11-07 18:01:51+00:00,Synapse: Adaptive Arbitration of Complementary Expertise in Time Series Foundational Models,"Pre-trained Time Series Foundational Models (TSFMs) represent a significant advance, capable of forecasting diverse time series with complex characteristics, including varied seasonalities, trends, and long-range dependencies. Despite their primary goal of universal time series forecasting, their efficacy is far from uniform; divergent training protocols and data sources cause individual TSFMs to exhibit highly variable performance across different forecasting tasks, domains, and horizons. Leveraging this complementary expertise by arbitrating existing TSFM outputs presents a compelling strategy, yet this remains a largely unexplored area of research. In this paper, we conduct a thorough examination of how different TSFMs exhibit specialized performance profiles across various forecasting settings, and how we can effectively leverage this behavior in arbitration between different time series models. We specifically analyze how factors such as model selection and forecast horizon distribution can influence the efficacy of arbitration strategies. Based on this analysis, we propose Synapse, a novel arbitration framework for TSFMs. Synapse is designed to dynamically leverage a pool of TSFMs, assign and adjust predictive weights based on their relative, context-dependent performance, and construct a robust forecast distribution by adaptively sampling from the output quantiles of constituent models. Experimental results demonstrate that Synapse consistently outperforms other popular ensembling techniques as well as individual TSFMs, demonstrating Synapse's efficacy in time series forecasting.","The field of time series forecasting has evolved through several distinct paradigms, starting with statistical methods, advancing to task-specific deep learning models, and most recently leveraging large-scale foundational models. Our work is situated at the intersection of this latest paradigm and the long-standing practice of model ensembling, adapted for the unique challenges and opportunities presented by modern TSFMs.


\paragraph{Statistical and Deep Learning Methods}
Time series analysis has been primarily dominated by statistical methods like the ARIMA family \citep{box2015time} and Exponential Smoothing (ETS) \citep{hyndman2008forecasting} methods. These models are interpretable, efficient, and remain strong baselines. However, they often rely on strict assumptions about the data and struggle to capture complex, non-linear dependencies. The advent of deep learning introduced a new class of models capable of automatically learning intricate patterns from raw data. This wave began with Recurrent Neural Networks (RNNs) like LSTMs \citep{hochreiter1997long} designed for sequential data. More recently, the landscape has been shaped by Transformer-based architectures \citep{zeng2023are}, such as PatchTST \citep{nie2023time}, which treat time series as sequences of patches, and even simpler MLP-based models like DLinear and TSMixer \citep{chen2023tsmixer}. While powerful, these deep learning models are typically trained for specific tasks and datasets, requiring significant data and computational resources for each new application.

\paragraph{Time Series Foundational Models}
The latest paradigm shift in forecasting mirrors the revolution in natural language processing with large language models: the rise of pre-trained Time Series Foundational Models (TSFMs). These models, trained on massive and diverse datasets, perform zero-shot forecasting on unseen time series with remarkable accuracy. A prominent approach treats forecasting as a language modeling problem, where continuous time series values are tokenized into a discrete vocabulary. Models like Chronos \citep{ansari2024chronos} and TimeGPT \citep{garza2023timegpt} exemplify this, using standard large language model (LLM) architectures to predict the next token. A contrasting approach involves direct regression on continuous values. TimesFM \citep{das2024decoderonlyfoundationmodeltimeseries} is a leading example, employing a decoder-only architecture pre-trained on 100 billion time points for extreme long-horizon forecasting. Other models explore novel pre-training strategies and architectures, including generative state-space models like Flowstate \citep{graf2025flowstate} and the varied Transformer-based approaches seen in the Moirai family \citep{woo2024unifiedtraininguniversaltime}. The core promise of TSFMs is a universal forecasting solution, yet as our Oracle analysis demonstrates, no single model universally dominates. Instead, they exhibit complementary expertise, making them prime candidates for advanced arbitration.","The field of time series forecasting has evolved through several distinct paradigms, starting with statistical methods, advancing to task-specific deep learning models, and most recently leveraging large-scale foundational models. Our work is situated at the intersection of this latest paradigm and the long-standing practice of model ensembling, adapted for the unique challenges and opportunities presented by modern TSFMs.


\paragraph{Statistical and Deep Learning Methods}
Time series analysis has been primarily dominated by statistical methods like the ARIMA family \citep{box2015time} and Exponential Smoothing (ETS) \citep{hyndman2008forecasting} methods. These models are interpretable, efficient, and remain strong baselines. However, they often rely on strict assumptions about the data and struggle to capture complex, non-linear dependencies. The advent of deep learning introduced a new class of models capable of automatically learning intricate patterns from raw data. This wave began with Recurrent Neural Networks (RNNs) like LSTMs \citep{hochreiter1997long} designed for sequential data. More recently, the landscape has been shaped by Transformer-based architectures \citep{zeng2023are}, such as PatchTST \citep{nie2023time}, which treat time series as sequences of patches, and even simpler MLP-based models like DLinear and TSMixer \citep{chen2023tsmixer}. While powerful, these deep learning models are typically trained for specific tasks and datasets, requiring significant data and computational resources for each new application.

\paragraph{Time Series Foundational Models}
The latest paradigm shift in forecasting mirrors the revolution in natural language processing with large language models: the rise of pre-trained Time Series Foundational Models (TSFMs). These models, trained on massive and diverse datasets, perform zero-shot forecasting on unseen time series with remarkable accuracy. A prominent approach treats forecasting as a language modeling problem, where continuous time series values are tokenized into a discrete vocabulary. Models like Chronos \citep{ansari2024chronos} and TimeGPT \citep{garza2023timegpt} exemplify this, using standard large language model (LLM) architectures to predict the next token. A contrasting approach involves direct regression on continuous values. TimesFM \citep{das2024decoderonlyfoundationmodeltimeseries} is a leading example, employing a decoder-only architecture pre-trained on 100 billion time points for extreme long-horizon forecasting. Other models explore novel pre-training strategies and architectures, including generative state-space models like Flowstate \citep{graf2025flowstate} and the varied Transformer-based approaches seen in the Moirai family \citep{woo2024unifiedtraininguniversaltime}. The core promise of TSFMs is a universal forecasting solution, yet as our Oracle analysis demonstrates, no single model universally dominates. Instead, they exhibit complementary expertise, making them prime candidates for advanced arbitration.",
2511.08136v1,http://arxiv.org/abs/2511.08136v1,2025-11-11 11:44:20+00:00,SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories,"In this work, we study the problem of offline safe imitation learning (IL). In many real-world settings, online interactions can be risky, and accurately specifying the reward and the safety cost information at each timestep can be difficult. However, it is often feasible to collect trajectories reflecting undesirable or risky behavior, implicitly conveying the behavior the agent should avoid. We refer to these trajectories as non-preferred trajectories. Unlike standard IL, which aims to mimic demonstrations, our agent must also learn to avoid risky behavior using non-preferred trajectories. In this paper, we propose a novel approach, SafeMIL, to learn a parameterized cost that predicts if the state-action pair is risky via \textit{Multiple Instance Learning}. The learned cost is then used to avoid non-preferred behaviors, resulting in a policy that prioritizes safety. We empirically demonstrate that our approach can learn a safer policy that satisfies cost constraints without degrading the reward performance, thereby outperforming several baselines.","Offline Imitation Learning \cite{bc,demodice_iclr_2022} primarily focuses on replicating actions of the demonstrations without explicitly considering safety. It implicitly assumes that demonstrations are safe, but if they contain non-preferred trajectories, simply imitating them can lead to learning policies that are risky. We address the problem of learning a safe imitation policy by seeking limited access to non-preferred trajectories and an abundance of unlabeled trajectories that contain both preferred and non-preferred trajectories. This specific scenario has received limited attention in the literature.

\paragraph{Learning Imitation Policy from Non-Preferred Trajectories}: SafeDICE \cite{safedice_neurips_2023} addresses this challenge by directly estimating the stationary distribution of the preferred policy. The estimated distribution is then used to learn a safe policy that mimics the preferred behavior while effectively avoiding the non-preferred behaviors.

\paragraph{Learning Imitation Policy from Suboptimal Trajectories}: T-REX \cite{trex_icml_2019}, B-Pref \cite{b_pref_neurips_2021}, PEBBLE \cite{pebble_icml_2021}, OPRL \cite{offline_pref_tmlr_2023}, learn a reward function from ranked trajectories. These methods focus on learning reward functions that assign a higher total reward to higher-ranked trajectories in the dataset. T-REX, B-Pref, PEBBLE then uses online-RL \cite{ppo,sac} algorithm, and OPRL uses offline-RL \cite{cql_neurips_2020,neorl_neurips_2022} algorithm to learn the policy. Since ranking trajectories can be challenging, D-REX \cite{drex_corl_2020} and SSRR \cite{ssrr_corl_2020} generate trajectories of varying quality by injecting noise into policies learned from suboptimal demonstrations. These approaches then use the resulting trajectories to learn a reward function. However, trajectory generation requires online interaction with the environment, which is not viable in an offline setting.
Another approach, DWBC \cite{dwbc_icml_2022}, uses positive-unlabeled (PU) learning \cite{pu_learning_1_2008,pu_learning_2_2021} to train a discriminator network, which is then incorporated as a weight in the BC loss function.","Offline Imitation Learning \cite{bc,demodice_iclr_2022} primarily focuses on replicating actions of the demonstrations without explicitly considering safety. It implicitly assumes that demonstrations are safe, but if they contain non-preferred trajectories, simply imitating them can lead to learning policies that are risky. We address the problem of learning a safe imitation policy by seeking limited access to non-preferred trajectories and an abundance of unlabeled trajectories that contain both preferred and non-preferred trajectories. This specific scenario has received limited attention in the literature.

\paragraph{Learning Imitation Policy from Non-Preferred Trajectories}: SafeDICE \cite{safedice_neurips_2023} addresses this challenge by directly estimating the stationary distribution of the preferred policy. The estimated distribution is then used to learn a safe policy that mimics the preferred behavior while effectively avoiding the non-preferred behaviors.

\paragraph{Learning Imitation Policy from Suboptimal Trajectories}: T-REX \cite{trex_icml_2019}, B-Pref \cite{b_pref_neurips_2021}, PEBBLE \cite{pebble_icml_2021}, OPRL \cite{offline_pref_tmlr_2023}, learn a reward function from ranked trajectories. These methods focus on learning reward functions that assign a higher total reward to higher-ranked trajectories in the dataset. T-REX, B-Pref, PEBBLE then uses online-RL \cite{ppo,sac} algorithm, and OPRL uses offline-RL \cite{cql_neurips_2020,neorl_neurips_2022} algorithm to learn the policy. Since ranking trajectories can be challenging, D-REX \cite{drex_corl_2020} and SSRR \cite{ssrr_corl_2020} generate trajectories of varying quality by injecting noise into policies learned from suboptimal demonstrations. These approaches then use the resulting trajectories to learn a reward function. However, trajectory generation requires online interaction with the environment, which is not viable in an offline setting.
Another approach, DWBC \cite{dwbc_icml_2022}, uses positive-unlabeled (PU) learning \cite{pu_learning_1_2008,pu_learning_2_2021} to train a discriminator network, which is then incorporated as a weight in the BC loss function.",
2510.26524v1,http://arxiv.org/abs/2510.26524v1,2025-10-30 14:16:43+00:00,Approximating Heavy-Tailed Distributions with a Mixture of Bernstein Phase-Type and Hyperexponential Models,"Heavy-tailed distributions, prevalent in a lot of real-world applications such as finance, telecommunications, queuing theory, and natural language processing, are challenging to model accurately owing to their slow tail decay. Bernstein phase-type (BPH) distributions, through their analytical tractability and good approximations in the non-tail region, can present a good solution, but they suffer from an inability to reproduce these heavy-tailed behaviors exactly, thus leading to inadequate performance in important tail areas. On the contrary, while highly adaptable to heavy-tailed distributions, hyperexponential (HE) models struggle in the body part of the distribution. Additionally, they are highly sensitive to initial parameter selection, significantly affecting their precision.
  To solve these issues, we propose a novel hybrid model of BPH and HE distributions, borrowing the most desirable features from each for enhanced approximation quality. Specifically, we leverage an optimization to set initial parameters for the HE component, significantly enhancing its robustness and reducing the possibility that the associated procedure results in an invalid HE model. Experimental validation demonstrates that the novel hybrid approach is more performant than individual application of BPH or HE models. More precisely, it can capture both the body and the tail of heavy-tailed distributions, with a considerable enhancement in matching parameters such as mean and coefficient of variation. Additional validation through experiments utilizing queuing theory proves the practical usefulness, accuracy, and precision of our hybrid approach.","\label{sec:related}

Phase-type (PH) distributions have been introduced in \cite{[NEUT75]} to model non-ex\-po\-nen\-tial durations by combination of exponential distributions, maintaining thus the Markov property of the underlying stochastic process. The application of PH distributions requires fitting (also called matching) algorithms, that is, methods that, given a distribution, provide a PH distribution that closely resembles it. Such methods either aim
\begin{enumerate}
    \item to match statistical properties extracted from the distribution (typically moments) or
    \item to minimize a distance measure that considers the whole distribution.
\end{enumerate}

A seminal work among those belonging to the first category is \cite{johnson1989matching}, where the authors use a mixture of Erlang distributions to match the first three moments of a given distribution. This approach was refined in \cite{bobbio2005matching}, providing a method to match any valid first three moments with an acyclic PH distribution of minimal size. Matching of more than three moments was tackled in \cite{horvath2007matching}. While the above approaches match the moments exactly, there have been proposals to approximate matching of the moments based on optimization \cite{BuKr09,sherzer2025unconstrainedoptimizationapproachmoment}. A common difficulty of all these approaches is that a full characterization of the feasible region of moments of PH distributions is not available. The known results in this direction are gathered in \cite{horvath2024phase}.

Approaches belonging to the second category are most often based on the maximum likelihood principle, that is, they minimize the Kullback–Leibler divergence. This can be done either by using the Expectation-Maximization (EM) algorithm, like in \cite{asmussen1996fitting,okamura2011refined}, or by more direct maximization of the likelihood function, as done in \cite{[BOBB94a]}, where also a benchmark for the evaluation of fitting methods is proposed.

For what concerns fitting heavy tailed distributions, general approaches belonging to either the first or the second category fail to result in PH distributions with satisfactory behavior. For this reason, \cite{[FELD98a]} developed a heuristic approach, which we describe in detail in Sec. \ref{Section:he}, whose shortcoming is that it is applicable only to distributions with a monotone probability density function. To overcome this, in \cite{HoTe00} the authors proposed to combine the model proposed in \cite{[FELD98a]} with an acyclic PH distribution whose parameters are estimated based on the maximum likelihood principle.

Models constructed through the composition of PH distributions (see, e.g., \cite{Haddad1997}) result in Markov chains, which can be analyzed using established techniques developed for such models \cite{stewart1994markov}. In the area of queuing systems, a wide range of methods have been developed to analyze models that employ PH distributions. The seminal work in this regard is \cite{neuts1981matrix} while \cite{latouche1999introduction} provides a modern, comprehensive treatment. A first effort to propose a tool to analyze Petri net models with PH timing was described in \cite{Cu85}.

Among others, tools for PH fitting are \cite{Horvath2002PhFit,HyperStarTool} while \cite{Horvath2017BuTools} provides both moment matching algorithms and methods to analyze queues with PH timing.","Phase-type (PH) distributions have been introduced in \cite{[NEUT75]} to model non-ex\-po\-nen\-tial durations by combination of exponential distributions, maintaining thus the Markov property of the underlying stochastic process. The application of PH distributions requires fitting (also called matching) algorithms, that is, methods that, given a distribution, provide a PH distribution that closely resembles it. Such methods either aim
\begin{enumerate}
    \item to match statistical properties extracted from the distribution (typically moments) or
    \item to minimize a distance measure that considers the whole distribution.
\end{enumerate}

A seminal work among those belonging to the first category is \cite{johnson1989matching}, where the authors use a mixture of Erlang distributions to match the first three moments of a given distribution. This approach was refined in \cite{bobbio2005matching}, providing a method to match any valid first three moments with an acyclic PH distribution of minimal size. Matching of more than three moments was tackled in \cite{horvath2007matching}. While the above approaches match the moments exactly, there have been proposals to approximate matching of the moments based on optimization \cite{BuKr09,sherzer2025unconstrainedoptimizationapproachmoment}. A common difficulty of all these approaches is that a full characterization of the feasible region of moments of PH distributions is not available. The known results in this direction are gathered in \cite{horvath2024phase}.

Approaches belonging to the second category are most often based on the maximum likelihood principle, that is, they minimize the Kullback–Leibler divergence. This can be done either by using the Expectation-Maximization (EM) algorithm, like in \cite{asmussen1996fitting,okamura2011refined}, or by more direct maximization of the likelihood function, as done in \cite{[BOBB94a]}, where also a benchmark for the evaluation of fitting methods is proposed.

For what concerns fitting heavy tailed distributions, general approaches belonging to either the first or the second category fail to result in PH distributions with satisfactory behavior. For this reason, \cite{[FELD98a]} developed a heuristic approach, which we describe in detail in Sec. \ref{Section:he}, whose shortcoming is that it is applicable only to distributions with a monotone probability density function. To overcome this, in \cite{HoTe00} the authors proposed to combine the model proposed in \cite{[FELD98a]} with an acyclic PH distribution whose parameters are estimated based on the maximum likelihood principle.

Models constructed through the composition of PH distributions (see, e.g., \cite{Haddad1997}) result in Markov chains, which can be analyzed using established techniques developed for such models \cite{stewart1994markov}. In the area of queuing systems, a wide range of methods have been developed to analyze models that employ PH distributions. The seminal work in this regard is \cite{neuts1981matrix} while \cite{latouche1999introduction} provides a modern, comprehensive treatment. A first effort to propose a tool to analyze Petri net models with PH timing was described in \cite{Cu85}.

Among others, tools for PH fitting are \cite{Horvath2002PhFit,HyperStarTool} while \cite{Horvath2017BuTools} provides both moment matching algorithms and methods to analyze queues with PH timing.",
2511.07831v1,http://arxiv.org/abs/2511.07831v1,2025-11-11 04:56:39+00:00,Distributionally Robust Online Markov Game with Linear Function Approximation,"The sim-to-real gap, where agents trained in a simulator face significant performance degradation during testing, is a fundamental challenge in reinforcement learning. Extansive works adopt the framework of distributionally robust RL, to learn a policy that acts robustly under worst case environment shift. Within this framework, our objective is to devise algorithms that are sample efficient with interactive data collection and large state spaces. By assuming d-rectangularity of environment dynamic shift, we identify a fundamental hardness result for learning in online Markov game, and address it by adopting minimum value assumption. Then, a novel least square value iteration type algorithm, DR-CCE-LSI, with exploration bonus devised specifically for multiple agents, is proposed to find an \episilon-approximate robust Coarse Correlated Equilibrium(CCE). To obtain sample efficient learning, we find that: when the feature mapping function satisfies certain properties, our algorithm, DR-CCE-LSI, is able to achieve ε-approximate CCE with a regret bound of O{dHmin{H,1/min{σ_i}}\sqrt{K}}, where K is the number of interacting episodes, H is the horizon length, d is the feature dimension, and \simga_i represents the uncertainty level of player i. Our work introduces the first sample-efficient algorithm for this setting, matches the best result so far in single agent setting, and achieves minimax optimalsample complexity in terms of the feature dimension d. Meanwhile, we also conduct simulation study to validate the efficacy of our algorithm in learning a robust equilibrium.","\subsection{Robust online linear MDPs}
The setting of online linear Markov Decision Processes (MDPs) \cite{yang2019linear_q_learning, yang2020_linear_bandit, zanette2020online_linear, jin2020online_linear, he2021online_linear_pre, he2023online_linear_minimax, zhou2021nearly_mixture} has been extensively studied. The work of \cite{he2023online_linear_minimax} achieved minimax optimality in this setting by incorporating variance-weighted ridge regression into their algorithm. In contrast, the study of online robust linear MDPs has only recently been explored in two works \cite{liu2024DRL_online_linear, liu2024upper}. Specifically, \cite{liu2024upper} introduced a robust variant of variance-weighted ridge regression, achieving a regret bound of order $\mathcal{O}(d H \min \{1 / \sigma, H\} \sqrt{K})$ under full data coverage assumption. However, this result still falls short of the constructed lower bound, which is of order $\Omega(d H^{\frac{1}{2}} \min \{1 / \sigma, H\} \sqrt{K})$, highlighting that the single-agent counterpart of this setting remains insufficiently explored.


\subsection{Robust Markov game} While there has been extensive research on distributionally robust MDPs \cite{liu2022distributionally_q_learning, clavier2023Lp-Bal_Planning, shi2024distributionally_offline, Shi2023_Curious_prize_generative, wang2023finite_dr_q_learning, lu2024Dr_Interactive_Data_Collection}, the study of robust Markov games remains relatively underexplored. Existing works, such as \cite{kardecs2011discounted, zhang2020robust_marl}, primarily focus on proving the existence of equilibria and analyzing convergence properties. In offline setting, a unified framework $P^2M^2PO$ has been proposed by \cite{blanchet2023double_marl}, with sample complexity of $\mathcal{O}(\frac{H^5|S|^2|A|^2}{\epsilon})$. \cite{shi2024breaking, shi2024multi-generative-joint, jiao2024minimax_marl} extended this framework to generative model setting, where samples can be obtained from any state-action pair. In particular, \cite{jiao2024minimax_marl} proposed a Q-FTRL type algorithm, demonstrating that it is minimax optimal and breaking the curse of multi-agency with a sample complexity of $\mathcal{O}(\frac{H^3 |S| \sum_{i=1}^m |A_i|}{\varepsilon^2} \min \left\{H, \frac{1}{\sigma}\right\})$. In the more realistic online setting, the work most relevant to ours is \cite{ma2023decentralized_v_robust}. However, their approach requires the uncertainty level $\sigma_i \leq \max \left\{\frac{\varepsilon}{|S| H^2}, \frac{p_{\min }}{H}\right\}$ for all $i \in[n]$. This constraint limits the robustness of their framework, especially when high accuracy is required ($\varepsilon \rightarrow 0$ ) or the minimum positive transition probabilities ($p_{\min } \rightarrow 0$ ). 

\subsection{Online linear Markov games} The study of sample complexity in online linear Markov games encompasses both centralized learning \cite{xie2020two-player-simultaneous-move, chen2022linear_mixture_marl, cisneros2023finite}, which employs global linear function approximation, and decentralized learning, which relies on independent linear function approximation \cite{cui2023breaking_linear_decentralized, wang2023breaking_linear_decentralized, dai2024refined_decentralized_linear_marl}. While decentralized learning is often more favorable in tabular Markov games due to its ability to alleviate the curse of multi-agency, extending this approach to linear function approximation requires adopting independent linear function approximation. However, this deviates from the linear MDP setting commonly used in single-agent reinforcement learning. Furthermore, addressing robustness in such decentralized settings remains an open and challenging problem.

In the context of centralized learning, \cite{xie2020two-player-simultaneous-move, chen2022linear_mixture_marl} focus on two-player zero-sum games, which are less general compared to the multi-player general-sum games considered in \cite{cisneros2023finite}. The work in \cite{cisneros2023finite} introduced the NQOVI algorithm, achieving a regret bound of $\mathcal{O}\left(\sqrt{d^3 H^5K}\right)$. Notably, the incorporation of robustness into online linear Markov games has not yet been studied, underscoring the significance of our work in addressing this gap.

\paragraph{Notation} Throughout this paper, we adopt the notation $[P V](s, a) = \mathbb{E}_{s^{\prime} \sim P(\cdot | s, a)}[V(s^{\prime})]$ to represent the expected value under the transition dynamics. The set of integers $\{1, 2, \ldots, n\}$ is denoted by $[n]$. For a vector where the $i^{\text{th}}$ element is given by $v_i$, we use the notation $[v_i]_{i \in [d]}$. The eigenvalues of a square matrix $A$ is denoted by $\lambda(A)$. To define norms, we write $||\phi||_A = \sqrt{\phi^\top A \phi}$, where $\phi \in \mathbb{R}^d$ is a vector and $A$ is a positive semi-definite matrix. Moreover, with a set of parameters $\mathcal{X}:=\left\{d, H, \{\sigma_i\}_{i=1}^n, 1 / \delta\right\}$, the expression $f(\mathcal{X}) = \mathcal{O}(g(\mathcal{X}))$ signifies that there exists a constant $C$ such that $f(\mathcal{X}) \leq C g(\mathcal{X})$, and $f(\mathcal{X}) = \Omega(g(\mathcal{X}))$ indicates $f(\mathcal{X}) \geq C g(\mathcal{X})$ for some constant $C$, and both of the expression omit logarithmic factors. The value $[V]_\alpha=\alpha$ if $V \geq \alpha$, otherwise $[V]_\alpha=V$. To further simplify notation, we let $ \mathcal{Q}:=\{(Q_1,Q_2,\cdots,Q_n):\gS \times \gA \rightarrow R^n\}$ be the function class of estimated Q value in our algorithm, and $\mathcal{V}$ be its expectation w.r.t the CCE policy. Finally, we use $\pi$ instead of $\pi_h$ whenever there is no ambiguity.","\subsection{Robust online linear MDPs}
The setting of online linear Markov Decision Processes (MDPs) \cite{yang2019linear_q_learning, yang2020_linear_bandit, zanette2020online_linear, jin2020online_linear, he2021online_linear_pre, he2023online_linear_minimax, zhou2021nearly_mixture} has been extensively studied. The work of \cite{he2023online_linear_minimax} achieved minimax optimality in this setting by incorporating variance-weighted ridge regression into their algorithm. In contrast, the study of online robust linear MDPs has only recently been explored in two works \cite{liu2024DRL_online_linear, liu2024upper}. Specifically, \cite{liu2024upper} introduced a robust variant of variance-weighted ridge regression, achieving a regret bound of order $\mathcal{O}(d H \min \{1 / \sigma, H\} \sqrt{K})$ under full data coverage assumption. However, this result still falls short of the constructed lower bound, which is of order $\Omega(d H^{\frac{1}{2}} \min \{1 / \sigma, H\} \sqrt{K})$, highlighting that the single-agent counterpart of this setting remains insufficiently explored.


\subsection{Robust Markov game} While there has been extensive research on distributionally robust MDPs \cite{liu2022distributionally_q_learning, clavier2023Lp-Bal_Planning, shi2024distributionally_offline, Shi2023_Curious_prize_generative, wang2023finite_dr_q_learning, lu2024Dr_Interactive_Data_Collection}, the study of robust Markov games remains relatively underexplored. Existing works, such as \cite{kardecs2011discounted, zhang2020robust_marl}, primarily focus on proving the existence of equilibria and analyzing convergence properties. In offline setting, a unified framework $P^2M^2PO$ has been proposed by \cite{blanchet2023double_marl}, with sample complexity of $\mathcal{O}(\frac{H^5|S|^2|A|^2}{\epsilon})$. \cite{shi2024breaking, shi2024multi-generative-joint, jiao2024minimax_marl} extended this framework to generative model setting, where samples can be obtained from any state-action pair. In particular, \cite{jiao2024minimax_marl} proposed a Q-FTRL type algorithm, demonstrating that it is minimax optimal and breaking the curse of multi-agency with a sample complexity of $\mathcal{O}(\frac{H^3 |S| \sum_{i=1}^m |A_i|}{\varepsilon^2} \min \left\{H, \frac{1}{\sigma}\right\})$. In the more realistic online setting, the work most relevant to ours is \cite{ma2023decentralized_v_robust}. However, their approach requires the uncertainty level $\sigma_i \leq \max \left\{\frac{\varepsilon}{|S| H^2}, \frac{p_{\min }}{H}\right\}$ for all $i \in[n]$. This constraint limits the robustness of their framework, especially when high accuracy is required ($\varepsilon \rightarrow 0$ ) or the minimum positive transition probabilities ($p_{\min } \rightarrow 0$ ). 

\subsection{Online linear Markov games} The study of sample complexity in online linear Markov games encompasses both centralized learning \cite{xie2020two-player-simultaneous-move, chen2022linear_mixture_marl, cisneros2023finite}, which employs global linear function approximation, and decentralized learning, which relies on independent linear function approximation \cite{cui2023breaking_linear_decentralized, wang2023breaking_linear_decentralized, dai2024refined_decentralized_linear_marl}. While decentralized learning is often more favorable in tabular Markov games due to its ability to alleviate the curse of multi-agency, extending this approach to linear function approximation requires adopting independent linear function approximation. However, this deviates from the linear MDP setting commonly used in single-agent reinforcement learning. Furthermore, addressing robustness in such decentralized settings remains an open and challenging problem.

In the context of centralized learning, \cite{xie2020two-player-simultaneous-move, chen2022linear_mixture_marl} focus on two-player zero-sum games, which are less general compared to the multi-player general-sum games considered in \cite{cisneros2023finite}. The work in \cite{cisneros2023finite} introduced the NQOVI algorithm, achieving a regret bound of $\mathcal{O}\left(\sqrt{d^3 H^5K}\right)$. Notably, the incorporation of robustness into online linear Markov games has not yet been studied, underscoring the significance of our work in addressing this gap.

\paragraph{Notation} Throughout this paper, we adopt the notation $[P V](s, a) = \mathbb{E}_{s^{\prime} \sim P(\cdot | s, a)}[V(s^{\prime})]$ to represent the expected value under the transition dynamics. The set of integers $\{1, 2, \ldots, n\}$ is denoted by $[n]$. For a vector where the $i^{\text{th}}$ element is given by $v_i$, we use the notation $[v_i]_{i \in [d]}$. The eigenvalues of a square matrix $A$ is denoted by $\lambda(A)$. To define norms, we write $||\phi||_A = \sqrt{\phi^\top A \phi}$, where $\phi \in \mathbb{R}^d$ is a vector and $A$ is a positive semi-definite matrix. Moreover, with a set of parameters $\mathcal{X}:=\left\{d, H, \{\sigma_i\}_{i=1}^n, 1 / \delta\right\}$, the expression $f(\mathcal{X}) = \mathcal{O}(g(\mathcal{X}))$ signifies that there exists a constant $C$ such that $f(\mathcal{X}) \leq C g(\mathcal{X})$, and $f(\mathcal{X}) = \Omega(g(\mathcal{X}))$ indicates $f(\mathcal{X}) \geq C g(\mathcal{X})$ for some constant $C$, and both of the expression omit logarithmic factors. The value $[V]_\alpha=\alpha$ if $V \geq \alpha$, otherwise $[V]_\alpha=V$. To further simplify notation, we let $ \mathcal{Q}:=\{(Q_1,Q_2,\cdots,Q_n):\gS \times \gA \rightarrow R^n\}$ be the function class of estimated Q value in our algorithm, and $\mathcal{V}$ be its expectation w.r.t the CCE policy. Finally, we use $\pi$ instead of $\pi_h$ whenever there is no ambiguity.",
2511.08073v1,http://arxiv.org/abs/2511.08073v1,2025-11-11 10:19:24+00:00,Online Linear Regression with Paid Stochastic Features,"We study an online linear regression setting in which the observed feature vectors are corrupted by noise and the learner can pay to reduce the noise level. In practice, this may happen for several reasons: for example, because features can be measured more accurately using more expensive equipment, or because data providers can be incentivized to release less private features. Assuming feature vectors are drawn i.i.d. from a fixed but unknown distribution, we measure the learner's regret against the linear predictor minimizing a notion of loss that combines the prediction error and payment. When the mapping between payments and noise covariance is known, we prove that the rate $\sqrt{T}$ is optimal for regret if logarithmic factors are ignored. When the noise covariance is unknown, we show that the optimal regret rate becomes of order $T^{2/3}$ (ignoring log factors). Our analysis leverages matrix martingale concentration, showing that the empirical loss uniformly converges to the expected one for all payments and linear predictors.","\label{section: related work}
\paragraph{Errors-in-variables models.} Linear regression with noisy covariates (also known as errors-in-variables, or EIV) is a classical topic in statistics---see, e.g., \citep{fuller2009measurement} and \citep{agarwal2021causal} for recent applications to differential privacy. Most results in EIV concern the problem of estimating $\theta^*$ under different norms, typically in a high-dimensional setting (large $d$) under assumptions of sparsity ($\theta^*$ has few non-zero components) or low-rank (the covariates admit a low-dimensional representation), see \citep{rosenbaum2010sparse,loh2011high,chen2013noisy,rosenbaum2013improved,kaul2015weighted,belloni2017linear,datta2017cocolasso} and references therein. A related line of work studies EIV under Bayesian assumptions \citep{reilly1981bayesian,ungarala2000multiscale,figueroa2022robust}. \citet{agarwal2023adaptive} consider EIV in an estimation error task where covariates are chosen adaptively (as in active learning or in multi-armed bandits). Closer to our work is the paper by \citet{agarwal2019robustness}, where they study a linear regression model with fixed design and missing or corrupted variables. Unlike us, they bound the prediction error in a transductive setting, where the covariates on which predictions are evaluated are available at training time. 

\paragraph{Local DP.} A prominent situation where features are noisy is when they are protected by a local differentially private mechanism \citep{kasiviswanathan2011can}. Two well-known approaches to create this protection are to add the raw features either Laplace or Gaussian noise \citep{yang2024local}. Many works study how to learn when features are private, including in the context of linear regression \citep{duchi2013local,smith2017interaction,wang2019sparse,fukuchi2017differentially}. However, their goal is mostly to estimate the true parameters of the model given the noisy features; in contrast, we want to learn how to use online noisy data to output good predictions. As we will show, the best predictors are not the true parameters, but rather depend on the noise level.

\paragraph{Online learning with noisy features.} Closely related to ours is the work by \citet{cesa2011online}, who studied online linear regression with square loss with noisy features and known/unknown covariances. However, there are several crucial differences: they do not have a notion of cost, and their notion of regret compares to the best linear predictor on the noise-free features. Moreover, their focus is on a setting in which the learner may obtain more than one noisy realization of the same feature vector. A second closely related work is the one by \citet{van2023trading}. They prove a $d^2(\ln T)\sqrt{T}$ regret bound in a binary classification setting in which: the learner can pay to reduce the noise level, the covariates (called experts in their setting) are binary, and noise and payments apply independently on each coordinate. However, because of their focus on classification with independent feature noise, their techniques are not directly applicable to our regression setting with correlated noise. Other works assume noisy features, but in the partial feedback setting, include \citep{kim2023contextual} and \citep{zheng2020locally}.

\paragraph{Privacy and incentives.} Another aspect of our work is the learner's capability to pay for noise reduction. In the privacy literature, this is often addressed as a problem of mechanism design: an analyst designs a procedure that incentivize individuals to sell their private data, aiming to obtain the most accurate estimation at a fixed payment budget (or paying a minimal cost for a target accuracy level) \citep{ghosh2011selling,nissim2012privacy,nissim2014redrawing,wang2018value,fallah2024optimal}. \citet{hsu2014differential} also studied the problem in which an individual is offered a payment to participate in a study and is willing to join it only if the payment covers its privacy loss. They show how an analyst running the experiment should tune the payment and privacy levels according to various considerations, including accuracy. All these works focus on payments that minimize the estimation error of the true parameters. Instead, our goal is to choose payments that balance the prediction error and the feature costs for every new incoming user. Also, our payment model is different: in our setting, the noise level is fixed (and potentially unknown) given the payment.","\paragraph{Errors-in-variables models.} Linear regression with noisy covariates (also known as errors-in-variables, or EIV) is a classical topic in statistics---see, e.g., \citep{fuller2009measurement} and \citep{agarwal2021causal} for recent applications to differential privacy. Most results in EIV concern the problem of estimating $\theta^*$ under different norms, typically in a high-dimensional setting (large $d$) under assumptions of sparsity ($\theta^*$ has few non-zero components) or low-rank (the covariates admit a low-dimensional representation), see \citep{rosenbaum2010sparse,loh2011high,chen2013noisy,rosenbaum2013improved,kaul2015weighted,belloni2017linear,datta2017cocolasso} and references therein. A related line of work studies EIV under Bayesian assumptions \citep{reilly1981bayesian,ungarala2000multiscale,figueroa2022robust}. \citet{agarwal2023adaptive} consider EIV in an estimation error task where covariates are chosen adaptively (as in active learning or in multi-armed bandits). Closer to our work is the paper by \citet{agarwal2019robustness}, where they study a linear regression model with fixed design and missing or corrupted variables. Unlike us, they bound the prediction error in a transductive setting, where the covariates on which predictions are evaluated are available at training time. 

\paragraph{Local DP.} A prominent situation where features are noisy is when they are protected by a local differentially private mechanism \citep{kasiviswanathan2011can}. Two well-known approaches to create this protection are to add the raw features either Laplace or Gaussian noise \citep{yang2024local}. Many works study how to learn when features are private, including in the context of linear regression \citep{duchi2013local,smith2017interaction,wang2019sparse,fukuchi2017differentially}. However, their goal is mostly to estimate the true parameters of the model given the noisy features; in contrast, we want to learn how to use online noisy data to output good predictions. As we will show, the best predictors are not the true parameters, but rather depend on the noise level.

\paragraph{Online learning with noisy features.} Closely related to ours is the work by \citet{cesa2011online}, who studied online linear regression with square loss with noisy features and known/unknown covariances. However, there are several crucial differences: they do not have a notion of cost, and their notion of regret compares to the best linear predictor on the noise-free features. Moreover, their focus is on a setting in which the learner may obtain more than one noisy realization of the same feature vector. A second closely related work is the one by \citet{van2023trading}. They prove a $d^2(\ln T)\sqrt{T}$ regret bound in a binary classification setting in which: the learner can pay to reduce the noise level, the covariates (called experts in their setting) are binary, and noise and payments apply independently on each coordinate. However, because of their focus on classification with independent feature noise, their techniques are not directly applicable to our regression setting with correlated noise. Other works assume noisy features, but in the partial feedback setting, include \citep{kim2023contextual} and \citep{zheng2020locally}.

\paragraph{Privacy and incentives.} Another aspect of our work is the learner's capability to pay for noise reduction. In the privacy literature, this is often addressed as a problem of mechanism design: an analyst designs a procedure that incentivize individuals to sell their private data, aiming to obtain the most accurate estimation at a fixed payment budget (or paying a minimal cost for a target accuracy level) \citep{ghosh2011selling,nissim2012privacy,nissim2014redrawing,wang2018value,fallah2024optimal}. \citet{hsu2014differential} also studied the problem in which an individual is offered a payment to participate in a study and is willing to join it only if the payment covers its privacy loss. They show how an analyst running the experiment should tune the payment and privacy levels according to various considerations, including accuracy. All these works focus on payments that minimize the estimation error of the true parameters. Instead, our goal is to choose payments that balance the prediction error and the feature costs for every new incoming user. Also, our payment model is different: in our setting, the noise level is fixed (and potentially unknown) given the payment.",
2511.09573v1,http://arxiv.org/abs/2511.09573v1,2025-11-11 21:10:09+00:00,Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,"Many machine learning tasks in the natural sciences are precisely equivariant to particular symmetries. Nonetheless, equivariant methods are often not employed, perhaps because training is perceived to be challenging, or the symmetry is expected to be learned, or equivariant implementations are seen as hard to build. Group averaging is an available technique for these situations. It happens at test time; it can make any trained model precisely equivariant at a (often small) cost proportional to the size of the group; it places no requirements on model structure or training. It is known that, under mild conditions, the group-averaged model will have a provably better prediction accuracy than the original model. Here we show that an inexpensive group averaging can improve accuracy in practice. We take well-established benchmark machine learning models of differential equations in which certain symmetries ought to be obeyed. At evaluation time, we average the models over a small group of symmetries. Our experiments show that this procedure always decreases the average evaluation loss, with improvements of up to 37\% in terms of the VRMSE. The averaging produces visually better predictions for continuous dynamics. This short paper shows that, under certain common circumstances, there are no disadvantages to imposing exact symmetries; the ML4PS community should consider group averaging as a cheap and simple way to improve model accuracy.","There has been a lot of recent work in machine learning for surrogate models, and they have demonstrated good performance in many domains \cite{stachenfeld2021learned,mccabe2023multiple, yang2023context, rahman2024pretraining, sun2025towards, shen2024ups, herde2024poseidon, chen2024omniarch}. These successes have motivated the creation of diverse datasets, such as \cite{takamoto2022pdebench, ohana2024well, mccabe2023multiple, audenaert2024multimodal}.

There is a vast literature on symmetry-preserving machine learning models \cite{shawe1989building,cohen2016group, kondor2018generalization}. Some authors have incorporated symmetry directly into the network architecture to create equivariant networks of different types \cite{worrall2019deep, wang2020incorporating, huang2023approximately, geiger2022e3nn, gregory2024robust}.
Several approaches to building equivariant models have been discussed, such as using invariant theory \cite{blum2023machine, villar2021scalars, blum2024galois, gregory2024learning}, group convolutions \cite{cohen2016group,cohen2019general}, canonicalization \cite{kaba2023equivariance}, and irreducible representations \cite{cohen2016steerable,kondor2018n}. In this work, we impose symmetries simply by averaging over the group orbit \cite{yarotsky2022universal, murphy2019relational}. This is a fairly naive approach that was significantly improved by recent work proposing \emph{frame averaging}, which only requires averaging over a subset of the group \cite{puny2022frame}.

Incorporating symmetries into machine learning models has been proven to improve generalization error and to reduce the sample complexity \cite{elesedy2021provably, petrache2023approximation, tahmasebi2023exact}. However, those are theoretical results that don't necessarily apply to complicated architectures such as those based on representation theory or group convolutions. In \cite{elesedy2021provably} it is mathematically shown that if the target function is equivariant, and the training data is sampled independently from an invariant distribution, then the projection \eqref{proj} reduces the generalization error.","There has been a lot of recent work in machine learning for surrogate models, and they have demonstrated good performance in many domains \cite{stachenfeld2021learned,mccabe2023multiple, yang2023context, rahman2024pretraining, sun2025towards, shen2024ups, herde2024poseidon, chen2024omniarch}. These successes have motivated the creation of diverse datasets, such as \cite{takamoto2022pdebench, ohana2024well, mccabe2023multiple, audenaert2024multimodal}.

There is a vast literature on symmetry-preserving machine learning models \cite{shawe1989building,cohen2016group, kondor2018generalization}. Some authors have incorporated symmetry directly into the network architecture to create equivariant networks of different types \cite{worrall2019deep, wang2020incorporating, huang2023approximately, geiger2022e3nn, gregory2024robust}.
Several approaches to building equivariant models have been discussed, such as using invariant theory \cite{blum2023machine, villar2021scalars, blum2024galois, gregory2024learning}, group convolutions \cite{cohen2016group,cohen2019general}, canonicalization \cite{kaba2023equivariance}, and irreducible representations \cite{cohen2016steerable,kondor2018n}. In this work, we impose symmetries simply by averaging over the group orbit \cite{yarotsky2022universal, murphy2019relational}. This is a fairly naive approach that was significantly improved by recent work proposing \emph{frame averaging}, which only requires averaging over a subset of the group \cite{puny2022frame}.

Incorporating symmetries into machine learning models has been proven to improve generalization error and to reduce the sample complexity \cite{elesedy2021provably, petrache2023approximation, tahmasebi2023exact}. However, those are theoretical results that don't necessarily apply to complicated architectures such as those based on representation theory or group convolutions. In \cite{elesedy2021provably} it is mathematically shown that if the target function is equivariant, and the training data is sampled independently from an invariant distribution, then the projection \eqref{proj} reduces the generalization error.",
2511.07032v1,http://arxiv.org/abs/2511.07032v1,2025-11-10 12:28:04+00:00,Fair Bayesian Data Selection via Generalized Discrepancy Measures,"Fairness concerns are increasingly critical as machine learning models are deployed in high-stakes applications. While existing fairness-aware methods typically intervene at the model level, they often suffer from high computational costs, limited scalability, and poor generalization. To address these challenges, we propose a Bayesian data selection framework that ensures fairness by aligning group-specific posterior distributions of model parameters and sample weights with a shared central distribution. Our framework supports flexible alignment via various distributional discrepancy measures, including Wasserstein distance, maximum mean discrepancy, and $f$-divergence, allowing geometry-aware control without imposing explicit fairness constraints. This data-centric approach mitigates group-specific biases in training data and improves fairness in downstream tasks, with theoretical guarantees. Experiments on benchmark datasets show that our method consistently outperforms existing data selection and model-based fairness methods in both fairness and accuracy.","\label{sec:related_work}

\paragraph{Data selection.} 
Most established selection strategies rely on bi-level optimization or meta-learning frameworks~\citep{grangier2023adaptivetrainingdistributionsscalable,pmlr-v80-ren18a,10.5555/3454287.3454459,zhang2021learningfastsamplereweighting}, which introduce an additional outer optimization loop to improve training data by maximizing model performance on a held-out meta set. These approaches, including sample-weighting~\citep{grangier2023adaptivetrainingdistributionsscalable,pmlr-v80-ren18a} and mini-batch reweighting~\citep{fan2017learningdatalearn}, often require expensive meta-gradients or reinforcement learning, making them difficult to scale to large datasets and deep models. Other strategies rely on heuristics such as loss or confidence scores, for instance, curriculum learning~\citep{Curriculum/10.1145/1553374.1553380} favors easy samples, online methods~\citep{online_batch,importance_sampling,accelerating_deep_learning} prioritize high-loss or high-gradient examples, and confidence-based approaches~\citep{LongReMix,confidence_score_idn} select uncertain instances. Most methods focus on performance and neglect fairness, with only a few adjusting sampling to meet fairness metrics~\citep{Roh0WS21_fairbatch}. We address this gap with a Bayesian data selection framework that aligns group-specific posteriors to incorporate fairness directly.


\paragraph{Fairness-aware learning.} 
Existing bias mitigation methods generally fall into three categories: preprocessing, in-processing, and post-processing. Preprocessing methods aim to reduce discriminatory information in the input data through fair representation learning~\citep{2013_iclr_ae,icml_2013,2016_lum,DBLP:flex}, fair data generation~\citep{Jang_Zheng_Wang_2021}, and data mapping~\citep{NIPS2017_optimised_preprocessing}. In-processing methods reduce bias during training by incorporating fairness constraints into the learning process~\citep{pmlr-v119-roh20a,baharlouei2024fferm,donini2020empirical,pmlr-v97-gordaliza19a,conf/aaai/ChiappaJSPJA20,zhang_adversarial}. These can be model-specific~\citep{fair_constraints,5360534} or model-agnostic~\citep{2018_icml_reductions,lowy2022a}. Post-processing methods adjust model outputs to meet fairness criteria~\citep{equal_opportunity}. However, these approaches often face scalability and generalization challenges, motivating a shift toward data selection. \citet{fairness_through_aleatoric} is conceptually related in addressing both data distribution and posterior weight biases while overcoming the SGLD limits, ours instead enforces fairness via Bayesian data selection optimized with SVGD.","\paragraph{Data selection.} 
Most established selection strategies rely on bi-level optimization or meta-learning frameworks~\citep{grangier2023adaptivetrainingdistributionsscalable,pmlr-v80-ren18a,10.5555/3454287.3454459,zhang2021learningfastsamplereweighting}, which introduce an additional outer optimization loop to improve training data by maximizing model performance on a held-out meta set. These approaches, including sample-weighting~\citep{grangier2023adaptivetrainingdistributionsscalable,pmlr-v80-ren18a} and mini-batch reweighting~\citep{fan2017learningdatalearn}, often require expensive meta-gradients or reinforcement learning, making them difficult to scale to large datasets and deep models. Other strategies rely on heuristics such as loss or confidence scores, for instance, curriculum learning~\citep{Curriculum/10.1145/1553374.1553380} favors easy samples, online methods~\citep{online_batch,importance_sampling,accelerating_deep_learning} prioritize high-loss or high-gradient examples, and confidence-based approaches~\citep{LongReMix,confidence_score_idn} select uncertain instances. Most methods focus on performance and neglect fairness, with only a few adjusting sampling to meet fairness metrics~\citep{Roh0WS21_fairbatch}. We address this gap with a Bayesian data selection framework that aligns group-specific posteriors to incorporate fairness directly.


\paragraph{Fairness-aware learning.} 
Existing bias mitigation methods generally fall into three categories: preprocessing, in-processing, and post-processing. Preprocessing methods aim to reduce discriminatory information in the input data through fair representation learning~\citep{2013_iclr_ae,icml_2013,2016_lum,DBLP:flex}, fair data generation~\citep{Jang_Zheng_Wang_2021}, and data mapping~\citep{NIPS2017_optimised_preprocessing}. In-processing methods reduce bias during training by incorporating fairness constraints into the learning process~\citep{pmlr-v119-roh20a,baharlouei2024fferm,donini2020empirical,pmlr-v97-gordaliza19a,conf/aaai/ChiappaJSPJA20,zhang_adversarial}. These can be model-specific~\citep{fair_constraints,5360534} or model-agnostic~\citep{2018_icml_reductions,lowy2022a}. Post-processing methods adjust model outputs to meet fairness criteria~\citep{equal_opportunity}. However, these approaches often face scalability and generalization challenges, motivating a shift toward data selection. \citet{fairness_through_aleatoric} is conceptually related in addressing both data distribution and posterior weight biases while overcoming the SGLD limits, ours instead enforces fairness via Bayesian data selection optimized with SVGD.",
2511.06495v1,http://arxiv.org/abs/2511.06495v1,2025-11-09 18:46:22+00:00,Probably Approximately Global Robustness Certification,"We propose and investigate probabilistic guarantees for the adversarial robustness of classification algorithms. While traditional formal verification approaches for robustness are intractable and sampling-based approaches do not provide formal guarantees, our approach is able to efficiently certify a probabilistic relaxation of robustness. The key idea is to sample an $ε$-net and invoke a local robustness oracle on the sample. Remarkably, the size of the sample needed to achieve probably approximately global robustness guarantees is independent of the input dimensionality, the number of classes, and the learning algorithm itself. Our approach can, therefore, be applied even to large neural networks that are beyond the scope of traditional formal verification. Experiments empirically confirm that it characterizes robustness better than state-of-the-art sampling-based approaches and scales better than formal methods.","\label{sec:related_work}

After the seminal work by \citet{Szegedy_intriguing_properties_robustness_first_paper} showed that NNs are sensitive to adversarial examples, a number of techniques to find such examples were introduced.
Many such approaches rely on gradient computation such as the Fast Gradient Signed Method \citep[FGSM,][]{Goodfellow_harnessing_adversarial_examples_FGSM} and an iterative adaptation of it called Projected Gradient Descent \citep[PGD,][]{madry_resistant_to_adversarial_attacks_PGD}.
Additionally, the C\&W attack \citep{CW_attack_carlini_and_wagner} explicitly takes the distance to the original data point into account, to find a particularly close adversarial example.
More recently, empirical approaches to assess \citep{webb_assessing_neural_network_robustness,Baluta_Kuldeep_Scalable_Quantitative_Verification_chernoff,kim_robust_generalization_MAIR} as well as improve \citep{li2023sok, kim_robust_generalization_MAIR} the robustness of NNs have been introduced, often building from the concept of adversarial training introduced by \citet{Goodfellow_harnessing_adversarial_examples_FGSM}.
While these approaches are applicable to large NNs and can be used to empirically assess robustness, they do not provide theoretical robustness guarantees for new points. 

In an effort to obtain provable guarantees, a line of research has developed formal methods for the verification of NNs \citep{marabou_guy_katz, chen2021learning, xu2020automatic}.
While a formally verified NN is provably robust to input perturbations \citep{casadio2022neural, formal_verification_robustness_survey_meng}, formal verification is limited to small NNs.
In fact, it is generally hard to provide guarantees about the behavior of large networks \citep{katz2017reluplex} and it is, in particular, hard to detect \emph{all} adversarial examples
\cite{carlini_wagner_bypassing_adversarial_detection_methods}.
Other approaches have sought to provide \emph{approximate} \citep{wu2020game} or \emph{statistical} \citep{webb_assessing_neural_network_robustness, cohen2019certified, robustness_certification_with_differential_privacy_lecuyer} robustness guarantees, as well as to train NNs which are certifiably robust \citep{li2023sok, certifiable_distributional_robustness_distributional,mixtrain_wang_training_verifiable}.

Recent work has specifically addressed global robustness for NNs.
\citet{athavale2024verifying} and \citet{indri2024distillation} have recently extended existing formal verification methods to certify global robustness for \emph{confident} predictions, where confidence is quantified using the softmax function.
Similarly, \citet{kabaha2024verification} focus on confident predictions using a margin-based notion of confidence.
However, these methods are limited to NNs with only hundreds of parameters.
In this paper, we introduce a probabilistic relaxation of the notion of global robustness of \citet{athavale2024verifying}.
We use $\epsilon$-nets \citep{haussler1986epsilon} to provide high-probability guarantees for this notion of robustness.","After the seminal work by \citet{Szegedy_intriguing_properties_robustness_first_paper} showed that NNs are sensitive to adversarial examples, a number of techniques to find such examples were introduced.
Many such approaches rely on gradient computation such as the Fast Gradient Signed Method \citep[FGSM,][]{Goodfellow_harnessing_adversarial_examples_FGSM} and an iterative adaptation of it called Projected Gradient Descent \citep[PGD,][]{madry_resistant_to_adversarial_attacks_PGD}.
Additionally, the C\&W attack \citep{CW_attack_carlini_and_wagner} explicitly takes the distance to the original data point into account, to find a particularly close adversarial example.
More recently, empirical approaches to assess \citep{webb_assessing_neural_network_robustness,Baluta_Kuldeep_Scalable_Quantitative_Verification_chernoff,kim_robust_generalization_MAIR} as well as improve \citep{li2023sok, kim_robust_generalization_MAIR} the robustness of NNs have been introduced, often building from the concept of adversarial training introduced by \citet{Goodfellow_harnessing_adversarial_examples_FGSM}.
While these approaches are applicable to large NNs and can be used to empirically assess robustness, they do not provide theoretical robustness guarantees for new points. 

In an effort to obtain provable guarantees, a line of research has developed formal methods for the verification of NNs \citep{marabou_guy_katz, chen2021learning, xu2020automatic}.
While a formally verified NN is provably robust to input perturbations \citep{casadio2022neural, formal_verification_robustness_survey_meng}, formal verification is limited to small NNs.
In fact, it is generally hard to provide guarantees about the behavior of large networks \citep{katz2017reluplex} and it is, in particular, hard to detect \emph{all} adversarial examples
\cite{carlini_wagner_bypassing_adversarial_detection_methods}.
Other approaches have sought to provide \emph{approximate} \citep{wu2020game} or \emph{statistical} \citep{webb_assessing_neural_network_robustness, cohen2019certified, robustness_certification_with_differential_privacy_lecuyer} robustness guarantees, as well as to train NNs which are certifiably robust \citep{li2023sok, certifiable_distributional_robustness_distributional,mixtrain_wang_training_verifiable}.

Recent work has specifically addressed global robustness for NNs.
\citet{athavale2024verifying} and \citet{indri2024distillation} have recently extended existing formal verification methods to certify global robustness for \emph{confident} predictions, where confidence is quantified using the softmax function.
Similarly, \citet{kabaha2024verification} focus on confident predictions using a margin-based notion of confidence.
However, these methods are limited to NNs with only hundreds of parameters.
In this paper, we introduce a probabilistic relaxation of the notion of global robustness of \citet{athavale2024verifying}.
We use $\epsilon$-nets \citep{haussler1986epsilon} to provide high-probability guarantees for this notion of robustness.",
2511.09722v1,http://arxiv.org/abs/2511.09722v1,2025-11-12 20:28:40+00:00,Masked Mineral Modeling: Continent-Scale Mineral Prospecting via Geospatial Infilling,"Minerals play a critical role in the advanced energy technologies necessary for decarbonization, but characterizing mineral deposits hidden underground remains costly and challenging. Inspired by recent progress in generative modeling, we develop a learning method which infers the locations of minerals by masking and infilling geospatial maps of resource availability. We demonstrate this technique using mineral data for the conterminous United States, and train performant models, with the best achieving Dice coefficients of $0.31 \pm 0.01$ and recalls of $0.22 \pm 0.02$ on test data at 1$\times$1 mi$^2$ spatial resolution. One major advantage of our approach is that it can easily incorporate auxiliary data sources for prediction which may be more abundant than mineral data. We highlight the capabilities of our model by adding input layers derived from geophysical sources, along with a nation-wide ground survey of soils originally intended for agronomic purposes. We find that employing such auxiliary features can improve inference performance, while also enabling model evaluation in regions with no recorded minerals.","\textbf{Subsurface mapping.} The geospatial infilling procedure studied in this work is a form of prospectivity mapping~\cite{yousefi2021data}, also known as mineral abundance inversion~\cite{chen2024hyperspectral}, or as fossicking when done by hobbyists~\cite{franks2023mineral}. This is a task of interest within the fields of geology, lithology, and mineralogy. It falls within the broader category of subsurface mapping and characterization, the modeling of below-ground resources based on sparse subsurface data combined with surface-accessible features~\cite{misra2019machine,vohra2024automated}. Remote sensing and data fusion have been applied with varying degrees of success to such challenges, for mineral resource analysis but also other climate-relevant applications, including geological CO$_2$ reservoir modeling~\cite{liu2020petrophysical,narayan2024machine,wang2024machine} and drilling viability for the extraction of geothermal energy~\cite{wang2025machine}. Many metrics are used to compare the performance of feature reconstruction, depending on the structure of the outputs. For supervised multi-class classification of lithological phenotypes, accuracy (overall and per-class) and Cohen's $\kappa$ are common, e.g.~\cite{dong2024fusion}. For regression of elemental content, metrics are standard to spectroscopy: mean squared error (MSE), bias, ratio of performance to deviation (RPD), and the $R^2$ of a linear fit between expected and predicted abundances, e.g.~\cite{jiang2024estimation}. Focusing on ML-based mineral resource estimation, most studies prior to 2022 used MLPs or SVMs (60\%), analyzed one mineral (84\%), and relied on field-level data collection (88\%)~\cite{dumakor2021machine}.\\[-1.075em]

\textbf{Data fusion for improving geospatial inference}. Many geospatial features are frequently analyzed together in subsurface mapping, including ground-based magnetometry, ground-penetrating radar, and seismometry data, as well as satellite-based aeromagnetic and imaging spectroscopy data~\cite{vohra2024automated}. Due to its growing availability, applications of hyperspectral remote sensing and ground surveys to subsurface and mineral resource mapping have drawn recent interest within the literature~\cite{hajaj2024review,wan2021application}. Hyperspectral satellites such as AVIRIS-NG provide more granular color information than traditional (multispectral) satellites like Landsat, measuring $\sim$100$\times$ more spectral bands ($\sim$1000 colors instead of $\sim$10) with spectral resolutions $\sim$10$\times$ finer~\cite{qian2021hyperspectral}. While using such data requires processing a large number of input channels, it can support more performant models. Most recently, using feature reduction techniques and fusion of hyperspectral and multispectral remote sensing data, the authors of~\cite{dong2024fusion} showed that continent-scale lithological mapping (roughly, a precursor of the data we use as ground truth for M3) could be achieved remotely across the Tibetan plain with 97\% accuracy and only 1\% of the available training data, using ViT with dynamic graph convolutional layers.

\textbf{Masked geospatial infilling of satellite imagery.} There has been some recent interest within the literature regarding geospatial applications of MAE. Techniques and foundation models such as SatMAE and ScaleMAE have been developed which can perform spatial, spectral, and temporal infilling of satellite imagery~\cite{cong2022satmae}, impute continuous surface-accessible features such as land use/land cover (LULC), and extend geospatial correlations across distance scales~\cite{reed2023scale,tang2023cross}. Due to the continuous nature of remotely sensed imagery, infilling tasks on satellite data fall well-within the domain of vanilla MAE designed for RGB images, and the performance of ViT architectures is strong in these settings. However, in this work we focus on sparse subsurface mineral readings, rather than surface observations and satellite imagery, and find empirically that ViT-based approaches struggle over our ResNet U-Net approach, as it leverages a spatial inductive bias which is well-suited to the spatial clusters characteristic of mineral prospecting data.  

\textbf{This work: contributions and differences.} Multiple aspects of the present study are unaddressed by existing literature. 
The application of subsurface mapping techniques to mineral availability on the scale of the conterminous United States (which makes up roughly 33\% of the North American continent's surface) represents a significant increase in effective survey area, and an initial step away from site-level analysis toward Earth-scale prediction. We employ a hyperspectral ground survey dataset which was developed by the US Department of Agriculture for agronomic purposes, and which has not been used for any subsurface mapping tasks. Our problem specification is also distinct, since we perform multiple binary classifications instead of a single multi-class classification, to flag the locations of mineralogical resources over a masked region. This infilling technique is a generative approach similar to MAE, which has not been studied in this context~\cite{he2021mae}. In addition, relative to previous works which infer lithological labels from geospatial features, the ground truth data we infill is more indicative of mining viability, i.e. it flags the locations of ore of some measurable grade (concentration) which could in principle be separated from waste rock and refined by a metallurgical process.\footnote{For this initial work, we do not differentiate resources by the economics of their respective separation processes, although the grades of resources are logged.} This change in interpretation has consequences for the choice of performance metrics appropriate to the method. For example, because the classes of interest are not mutually exclusive (two resource types can coexist), the $\kappa$ score is not appropriate for model comparison. Overall accuracy is also problematic, as the mineralogical data we employ as ground-truth are an incomplete collection of true positives~\cite{schweitzer2019record}. Instead, we consider metrics appropriate to image segmentation and object detection which de-emphasize true negative detection. We use recall to track reproduction of true positives, and Dice coefficients to indicate rates of new resource prediction. Both metrics fall between 0 and 1. Larger values indicate greater agreement between predictions and ground-truth.","\textbf{Subsurface mapping.} The geospatial infilling procedure studied in this work is a form of prospectivity mapping~\cite{yousefi2021data}, also known as mineral abundance inversion~\cite{chen2024hyperspectral}, or as fossicking when done by hobbyists~\cite{franks2023mineral}. This is a task of interest within the fields of geology, lithology, and mineralogy. It falls within the broader category of subsurface mapping and characterization, the modeling of below-ground resources based on sparse subsurface data combined with surface-accessible features~\cite{misra2019machine,vohra2024automated}. Remote sensing and data fusion have been applied with varying degrees of success to such challenges, for mineral resource analysis but also other climate-relevant applications, including geological CO$_2$ reservoir modeling~\cite{liu2020petrophysical,narayan2024machine,wang2024machine} and drilling viability for the extraction of geothermal energy~\cite{wang2025machine}. Many metrics are used to compare the performance of feature reconstruction, depending on the structure of the outputs. For supervised multi-class classification of lithological phenotypes, accuracy (overall and per-class) and Cohen's $\kappa$ are common, e.g.~\cite{dong2024fusion}. For regression of elemental content, metrics are standard to spectroscopy: mean squared error (MSE), bias, ratio of performance to deviation (RPD), and the $R^2$ of a linear fit between expected and predicted abundances, e.g.~\cite{jiang2024estimation}. Focusing on ML-based mineral resource estimation, most studies prior to 2022 used MLPs or SVMs (60\

\textbf{Data fusion for improving geospatial inference}. Many geospatial features are frequently analyzed together in subsurface mapping, including ground-based magnetometry, ground-penetrating radar, and seismometry data, as well as satellite-based aeromagnetic and imaging spectroscopy data~\cite{vohra2024automated}. Due to its growing availability, applications of hyperspectral remote sensing and ground surveys to subsurface and mineral resource mapping have drawn recent interest within the literature~\cite{hajaj2024review,wan2021application}. Hyperspectral satellites such as AVIRIS-NG provide more granular color information than traditional (multispectral) satellites like Landsat, measuring $\sim$100$\times$ more spectral bands ($\sim$1000 colors instead of $\sim$10) with spectral resolutions $\sim$10$\times$ finer~\cite{qian2021hyperspectral}. While using such data requires processing a large number of input channels, it can support more performant models. Most recently, using feature reduction techniques and fusion of hyperspectral and multispectral remote sensing data, the authors of~\cite{dong2024fusion} showed that continent-scale lithological mapping (roughly, a precursor of the data we use as ground truth for M3) could be achieved remotely across the Tibetan plain with 97\

\textbf{Masked geospatial infilling of satellite imagery.} There has been some recent interest within the literature regarding geospatial applications of MAE. Techniques and foundation models such as SatMAE and ScaleMAE have been developed which can perform spatial, spectral, and temporal infilling of satellite imagery~\cite{cong2022satmae}, impute continuous surface-accessible features such as land use/land cover (LULC), and extend geospatial correlations across distance scales~\cite{reed2023scale,tang2023cross}. Due to the continuous nature of remotely sensed imagery, infilling tasks on satellite data fall well-within the domain of vanilla MAE designed for RGB images, and the performance of ViT architectures is strong in these settings. However, in this work we focus on sparse subsurface mineral readings, rather than surface observations and satellite imagery, and find empirically that ViT-based approaches struggle over our ResNet U-Net approach, as it leverages a spatial inductive bias which is well-suited to the spatial clusters characteristic of mineral prospecting data.  

\textbf{This work: contributions and differences.} Multiple aspects of the present study are unaddressed by existing literature. 
The application of subsurface mapping techniques to mineral availability on the scale of the conterminous United States (which makes up roughly 33\",
2511.00532v1,http://arxiv.org/abs/2511.00532v1,2025-11-01 12:24:11+00:00,Air Pollution Forecasting in Bucharest,"Air pollution, especially the particulate matter 2.5 (PM2.5), has become a growing concern in recent years, primarily in urban areas. Being exposed to air pollution is linked to developing numerous health problems, like the aggravation of respiratory diseases, cardiovascular disorders, lung function impairment, and even cancer or early death. Forecasting future levels of PM2.5 has become increasingly important over the past few years, as it can provide early warnings and help prevent diseases. This paper aims to design, fine-tune, test, and evaluate machine learning models for predicting future levels of PM2.5 over various time horizons. Our primary objective is to assess and compare the performance of multiple models, ranging from linear regression algorithms and ensemble-based methods to deep learning models, such as advanced recurrent neural networks and transformers, as well as large language models, on this forecasting task.","\citet{mohammadzadeh2024spatiotemporal} presented a hybrid architecture that combines two types of neural networks: a graph convolutional network (GCN) \cite{DBLP:conf/iclr/KipfW17} and an exogenous long short-term memory (LSTM) \cite{hochreiter1997long}.
GCN models extract insights about the graph structure and gather information from each node's neighbors to capture the graph topology and the attributes of the nodes \cite{zhang2019graph}. The multivariate LSTM took more past variables as inputs, not just the PM2.5 concentration, but also variables such as temperature, humidity, pressure, wind speed, and direction. This way, the model understood how external weather factors influence the PM2.5 levels. By combining these two architectures, the model captured more spatial and temporal data, resulting in improved performance. Their dataset comprised PM2.5 and weather data collected by several stations across the American state of Michigan over four years, from January 2015 to December 2019. The hybrid GCN E-LSTM architecture performed well because it could understand how its neighboring stations might influence pollution levels at one station due to factors such as wind speed and direction, which the GCN component could capture.

\citet{zajkeckaautomated} aimed to build models that forecast PM2.5 values over a 6-hour horizon using three deep learning methods: one based on feed-forward networks (MLP) and two others based on recurrent neural networks (LSTM and ESN). The models were trained with four different types of input features from historical datasets collected by stations near Krakow, a city in Poland. These four types of inputs contained: only past PM2.5 values, PM2.5 values and exogenous variables, data from meteorological forecasts, and data from nearby stations. In the first two cases, ESN got the best results, with LSTM and MLP lagging. The scenario with meteorological forecasts achieved the best overall results, while the simpler PM2.5-only LSTM model yielded the worst results, especially for longer horizons. ESNs (echo state networks) are a special type of RNNs that have three components: the input layer, the ""reservoir"" (which is a random, fixed recurrent neural network whose weights cannot be updated) and the output layer (which is the only component that is being trained, using linear regression on the reservoir states) \cite{jaeger2007echo}.

\citet{caceres2024analysis} used a hybrid Prophet-LSTM model for PM2.5 forecasting. They used a dataset consisting of daily measurements from January 2019 to June 2024 from seven different districts in Madrid. Prophet models seasonality, trends, and special events, such as holidays or significant known incidents. The LSTM captures the residuals that Prophet doesn't explain (noise, temporary episodes). One advantage of this architecture is that it combines short and long-term dependencies. Their results showed that the air pollution levels have fallen since 2019 in most districts. There were still some exceptions, such as in Carabanchel, where an increase in PM2.5 levels has been observed. However, the maximum values and standard deviation still showed that spikes occurred.

\citet{liu2025deep} proposed a hybrid architecture that combines a transformer with an LSTM layer to predict PM2.5 concentrations in cities situated in central and western parts of China. 
% The architecture benefits from three main aspects. First of all, 
It combines the self-attention mechanism specific to transformer models with LSTM units to capture both long-term and global dependencies within sequences. The model also utilizes Particle Swarm Optimization (PSO) to adjust the batch size and learning rate automatically. Moreover, the model also addresses complex patterns that are not only evident on short horizons but also on long ones. Since the transformer doesn't natively handle time series data, positional encoding was used. The architecture employs two multi-head attention layers: the first is used solely for learning from previous data. At the same time, the second is used to understand how each measurement relates to the others in the sequence.
The hybrid model was tested and evaluated on input datasets from the Chinese cities of Wuhan and Nanchang. The proposed PSO-transformer-LSTM model outperformed the other two models with which it was compared (a vanilla LSTM and a PSO-optimized LSTM) on every evaluation metric for both cities.

\citet{peng2022machine} trained, fine-tuned, tested, and compared two models, focusing on meteorological variable selection for a more accurate PM2.5 prediction. These models were extreme gradient boosting (XGBoost) and a fully connected neural network. They used a dataset consisting of measurements from two stations in Hunan Province in central China, which were taken over the course of 2021. Each sample consisted of the PM2.5 pollutant level and six other meteorological features (wind speed and direction, air temperature, humidity, atmospheric pressure, and rainfall). On the Hunan dataset, the boosting method achieved higher performance, with a coefficient of determination value of 0.761 on the testing data, increasing to 0.856 at night.

\citet{bai2019hourly} introduced a stacked autoencoder (SAE) architecture for predicting hourly PM2.5 levels. The following image shows the representation of the simple autoencoder (5.a), and next to it, there are autoencoders stacked on top of each other (5.b). The autoencoder learned to compress data and then rebuild it. In addition to these autoencoder layers, a fully connected layer was used to predict the final output. The training for the AE layers was done unsupervised, while the dense layer was trained using supervised learning at the end. They also created a different model for each season of the year, because this way, each model could focus on and better understand patterns specific to its seasonality. The model was trained and tested on data gathered from three air quality stations near Beijing.

\citet{qin2025sfdformer} presented a new transformer-based model for air pollution forecasting. They utilized a dataset comprising historical daily pollution (PM2.5, PM10, SO2, NO2, CO, O3) and meteorological data (temperature, precipitation, wind speed) from eight cities across China, spanning from October 2013 to May 2021. The proposed model combines time series decomposition (to understand both trends and seasonality) with the Fourier Transform (to convert time-series data into frequencies, where some high frequencies can be associated with noise and can be ignored). It also employed a sparse attention mechanism to identify the most relevant patterns, thereby reducing the time complexity from quadratic to linear. The SFDformer model was compared to other transformer architectures, such as Autoformer, Informer, and Reformer, and it achieved the best performance among them on three different metrics (MAE, MSE, and RMSE).","\citet{mohammadzadeh2024spatiotemporal} presented a hybrid architecture that combines two types of neural networks: a graph convolutional network (GCN) \cite{DBLP:conf/iclr/KipfW17} and an exogenous long short-term memory (LSTM) \cite{hochreiter1997long}.
GCN models extract insights about the graph structure and gather information from each node's neighbors to capture the graph topology and the attributes of the nodes \cite{zhang2019graph}. The multivariate LSTM took more past variables as inputs, not just the PM2.5 concentration, but also variables such as temperature, humidity, pressure, wind speed, and direction. This way, the model understood how external weather factors influence the PM2.5 levels. By combining these two architectures, the model captured more spatial and temporal data, resulting in improved performance. Their dataset comprised PM2.5 and weather data collected by several stations across the American state of Michigan over four years, from January 2015 to December 2019. The hybrid GCN E-LSTM architecture performed well because it could understand how its neighboring stations might influence pollution levels at one station due to factors such as wind speed and direction, which the GCN component could capture.

\citet{zajkeckaautomated} aimed to build models that forecast PM2.5 values over a 6-hour horizon using three deep learning methods: one based on feed-forward networks (MLP) and two others based on recurrent neural networks (LSTM and ESN). The models were trained with four different types of input features from historical datasets collected by stations near Krakow, a city in Poland. These four types of inputs contained: only past PM2.5 values, PM2.5 values and exogenous variables, data from meteorological forecasts, and data from nearby stations. In the first two cases, ESN got the best results, with LSTM and MLP lagging. The scenario with meteorological forecasts achieved the best overall results, while the simpler PM2.5-only LSTM model yielded the worst results, especially for longer horizons. ESNs (echo state networks) are a special type of RNNs that have three components: the input layer, the ""reservoir"" (which is a random, fixed recurrent neural network whose weights cannot be updated) and the output layer (which is the only component that is being trained, using linear regression on the reservoir states) \cite{jaeger2007echo}.

\citet{caceres2024analysis} used a hybrid Prophet-LSTM model for PM2.5 forecasting. They used a dataset consisting of daily measurements from January 2019 to June 2024 from seven different districts in Madrid. Prophet models seasonality, trends, and special events, such as holidays or significant known incidents. The LSTM captures the residuals that Prophet doesn't explain (noise, temporary episodes). One advantage of this architecture is that it combines short and long-term dependencies. Their results showed that the air pollution levels have fallen since 2019 in most districts. There were still some exceptions, such as in Carabanchel, where an increase in PM2.5 levels has been observed. However, the maximum values and standard deviation still showed that spikes occurred.

\citet{liu2025deep} proposed a hybrid architecture that combines a transformer with an LSTM layer to predict PM2.5 concentrations in cities situated in central and western parts of China. 

It combines the self-attention mechanism specific to transformer models with LSTM units to capture both long-term and global dependencies within sequences. The model also utilizes Particle Swarm Optimization (PSO) to adjust the batch size and learning rate automatically. Moreover, the model also addresses complex patterns that are not only evident on short horizons but also on long ones. Since the transformer doesn't natively handle time series data, positional encoding was used. The architecture employs two multi-head attention layers: the first is used solely for learning from previous data. At the same time, the second is used to understand how each measurement relates to the others in the sequence.
The hybrid model was tested and evaluated on input datasets from the Chinese cities of Wuhan and Nanchang. The proposed PSO-transformer-LSTM model outperformed the other two models with which it was compared (a vanilla LSTM and a PSO-optimized LSTM) on every evaluation metric for both cities.

\citet{peng2022machine} trained, fine-tuned, tested, and compared two models, focusing on meteorological variable selection for a more accurate PM2.5 prediction. These models were extreme gradient boosting (XGBoost) and a fully connected neural network. They used a dataset consisting of measurements from two stations in Hunan Province in central China, which were taken over the course of 2021. Each sample consisted of the PM2.5 pollutant level and six other meteorological features (wind speed and direction, air temperature, humidity, atmospheric pressure, and rainfall). On the Hunan dataset, the boosting method achieved higher performance, with a coefficient of determination value of 0.761 on the testing data, increasing to 0.856 at night.

\citet{bai2019hourly} introduced a stacked autoencoder (SAE) architecture for predicting hourly PM2.5 levels. The following image shows the representation of the simple autoencoder (5.a), and next to it, there are autoencoders stacked on top of each other (5.b). The autoencoder learned to compress data and then rebuild it. In addition to these autoencoder layers, a fully connected layer was used to predict the final output. The training for the AE layers was done unsupervised, while the dense layer was trained using supervised learning at the end. They also created a different model for each season of the year, because this way, each model could focus on and better understand patterns specific to its seasonality. The model was trained and tested on data gathered from three air quality stations near Beijing.

\citet{qin2025sfdformer} presented a new transformer-based model for air pollution forecasting. They utilized a dataset comprising historical daily pollution (PM2.5, PM10, SO2, NO2, CO, O3) and meteorological data (temperature, precipitation, wind speed) from eight cities across China, spanning from October 2013 to May 2021. The proposed model combines time series decomposition (to understand both trends and seasonality) with the Fourier Transform (to convert time-series data into frequencies, where some high frequencies can be associated with noise and can be ignored). It also employed a sparse attention mechanism to identify the most relevant patterns, thereby reducing the time complexity from quadratic to linear. The SFDformer model was compared to other transformer architectures, such as Autoformer, Informer, and Reformer, and it achieved the best performance among them on three different metrics (MAE, MSE, and RMSE).",
2511.07585v1,http://arxiv.org/abs/2511.07585v1,2025-11-10 19:54:00+00:00,LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows,"Financial institutions deploy Large Language Models (LLMs) for reconciliations, regulatory reporting, and client communications, but nondeterministic outputs (output drift) undermine auditability and trust. We quantify drift across five model architectures (7B-120B parameters) on regulated financial tasks, revealing a stark inverse relationship: smaller models (Granite-3-8B, Qwen2.5-7B) achieve 100% output consistency at T=0.0, while GPT-OSS-120B exhibits only 12.5% consistency (95% CI: 3.5-36.0%) regardless of configuration (p<0.0001, Fisher's exact test). This finding challenges conventional assumptions that larger models are universally superior for production deployment.
  Our contributions include: (i) a finance-calibrated deterministic test harness combining greedy decoding (T=0.0), fixed seeds, and SEC 10-K structure-aware retrieval ordering; (ii) task-specific invariant checking for RAG, JSON, and SQL outputs using finance-calibrated materiality thresholds (plus or minus 5%) and SEC citation validation; (iii) a three-tier model classification system enabling risk-appropriate deployment decisions; and (iv) an audit-ready attestation system with dual-provider validation.
  We evaluated five models (Qwen2.5-7B via Ollama, Granite-3-8B via IBM watsonx.ai, Llama-3.3-70B, Mistral-Medium-2505, and GPT-OSS-120B) across three regulated financial tasks. Across 480 runs (n=16 per condition), structured tasks (SQL) remain stable even at T=0.2, while RAG tasks show drift (25-75%), revealing task-dependent sensitivity. Cross-provider validation confirms deterministic behavior transfers between local and cloud deployments. We map our framework to Financial Stability Board (FSB), Bank for International Settlements (BIS), and Commodity Futures Trading Commission (CFTC) requirements, demonstrating practical pathways for compliance-ready AI deployments.","\subsection{Technical Foundations of Nondeterminism}

The breakthrough understanding of LLM nondeterminism comes from Thinking Machines Lab's September 2025 work \cite{thinkingmachines2025}, which identified batch-size variation-not floating-point arithmetic-as the primary cause. Their batch-invariant kernels achieve exact reproducibility by ensuring operations yield identical results regardless of batch composition. Recent work on SLM agents \cite{belcak2025slm} complements batch-invariant kernels by advocating smaller models for efficient, deterministic agentic inference. Complementary research on deterministic decoding methods \cite{shi2024decoding} and hardware-level approaches including HPC reproducibility studies \cite{antunes2021reproducibility} provides additional foundations. This finding shows that determinism is achievable through engineering rather than fundamentally impossible.

Baldwin et al. \cite{baldwin2024} quantified the problem's severity, demonstrating accuracy variations up to 15\% across runs even at zero temperature, with performance gaps reaching 70\% between best and worst outcomes. They introduced the Total Agreement Rate (TAR) metric, providing the quantitative framework we extend in our financial context evaluation.

Recent research identifies additional practical nondeterminism sources at \Temp{0.0}, including non-batch-invariant matrix multiplication and attention kernels causing numeric divergence under varying loads. Tokenizer drift—where text normalization changes inflate token counts by up to 112\% and enable command injection vulnerabilities—further exacerbates output variability and operational costs \cite{tokenizer-drift}.

Analysis of model drift in crisis response contexts \cite{model-drift-crises} shows subtle output shifts can alter narratives, paralleling financial compliance requirements for stable, auditable responses. State-of-the-art LLM observability tools \cite{llm-observability} emphasize real-time monitoring of prompt-response pairs to detect such issues.

The vLLM serving infrastructure \cite{kwon2023} with PagedAttention provides efficient batching that can be extended with deterministic kernels. Production deployments of models like Qwen with vLLM \cite{redhat2025} demonstrate the framework's maturity, though as we demonstrate, efficient serving alone doesn't guarantee consistency-explicit determinism controls are required.

\subsection{Financial AI Requirements and Benchmarks}

Recent financial AI benchmarks-FinBen \cite{xie2024finben}, SEC-QA \cite{agarwal2024}, and DocFinQA \cite{zhu2024}-focus on accuracy metrics while overlooking reproducibility. While FinBen overlooks reproducibility, SLM agent frameworks \cite{belcak2025slm} suggest fine-tuned small models could enhance consistency in financial QA. SEC-QA's 1\% margin tolerance for numerical answers and DocFinQA's 123,000-word contexts test reasoning capability but not output stability.

Existing financial AI benchmarks evaluate \textit{accuracy} but not \textit{reproducibility}—a fundamental gap for regulated deployments. A model achieving 95\% accuracy on SEC-QA but exhibiting 25\% output variance across runs fails regulatory requirements despite strong performance metrics. Our work addresses this overlooked dimension, demonstrating that model selection for compliance requires determinism evaluation alongside traditional accuracy benchmarks.

The regulatory landscape demands more. The Financial Stability Board \cite{fsb2024,fsb2023ai} requires ``consistent and traceable decision-making,'' the Bank for International Settlements \cite{bis2024,bis2023ml} mandates ``clear accountability across the AI lifecycle,'' and CFTC guidance \cite{cftc2024,cftc2024ai} demands ``proper documentation of AI system outcomes.'' Additional regulatory oversight comes from the Federal Reserve \cite{fed2024ai} and the Office of the Comptroller of the Currency \cite{occ2024ai}. Our work bridges the gap between these requirements and technical capabilities, incorporating reproducibility guidance from PyTorch and vLLM~\cite{pytorchrepro,vllmrepro}.
\paragraph{What changed since 2024?}
Greedy decoding at \Temp{0.0} alone was not sufficient in prior runs due to retrieval order nondeterminism and batching variance. Our harness adds: (1) DeterministicRetriever implementing multi-key ordering (score↓, section\_priority↑, snippet\_id↑, chunk\_idx↑) that encodes SEC 10-K disclosure precedence rules, treating retrieval order as a \textit{compliance requirement} under Basel III explainability mandates rather than merely a performance optimization, (2) fixed seeds and sampler params in manifests, and (3) schema/invariant checks (SQL) with finance-calibrated ±5\% tolerance thresholds to constrain decoding freedom. These domain-specific adaptations—mapping document structure to regulatory obligations—eliminate residual drift at \Temp{0.0} across concurrencies.","\subsection{Technical Foundations of Nondeterminism}

The breakthrough understanding of LLM nondeterminism comes from Thinking Machines Lab's September 2025 work \cite{thinkingmachines2025}, which identified batch-size variation-not floating-point arithmetic-as the primary cause. Their batch-invariant kernels achieve exact reproducibility by ensuring operations yield identical results regardless of batch composition. Recent work on SLM agents \cite{belcak2025slm} complements batch-invariant kernels by advocating smaller models for efficient, deterministic agentic inference. Complementary research on deterministic decoding methods \cite{shi2024decoding} and hardware-level approaches including HPC reproducibility studies \cite{antunes2021reproducibility} provides additional foundations. This finding shows that determinism is achievable through engineering rather than fundamentally impossible.

Baldwin et al. \cite{baldwin2024} quantified the problem's severity, demonstrating accuracy variations up to 15\

Recent research identifies additional practical nondeterminism sources at \Temp{0.0}, including non-batch-invariant matrix multiplication and attention kernels causing numeric divergence under varying loads. Tokenizer drift—where text normalization changes inflate token counts by up to 112\

Analysis of model drift in crisis response contexts \cite{model-drift-crises} shows subtle output shifts can alter narratives, paralleling financial compliance requirements for stable, auditable responses. State-of-the-art LLM observability tools \cite{llm-observability} emphasize real-time monitoring of prompt-response pairs to detect such issues.

The vLLM serving infrastructure \cite{kwon2023} with PagedAttention provides efficient batching that can be extended with deterministic kernels. Production deployments of models like Qwen with vLLM \cite{redhat2025} demonstrate the framework's maturity, though as we demonstrate, efficient serving alone doesn't guarantee consistency-explicit determinism controls are required.

\subsection{Financial AI Requirements and Benchmarks}

Recent financial AI benchmarks-FinBen \cite{xie2024finben}, SEC-QA \cite{agarwal2024}, and DocFinQA \cite{zhu2024}-focus on accuracy metrics while overlooking reproducibility. While FinBen overlooks reproducibility, SLM agent frameworks \cite{belcak2025slm} suggest fine-tuned small models could enhance consistency in financial QA. SEC-QA's 1\

Existing financial AI benchmarks evaluate \textit{accuracy} but not \textit{reproducibility}—a fundamental gap for regulated deployments. A model achieving 95\

The regulatory landscape demands more. The Financial Stability Board \cite{fsb2024,fsb2023ai} requires ``consistent and traceable decision-making,'' the Bank for International Settlements \cite{bis2024,bis2023ml} mandates ``clear accountability across the AI lifecycle,'' and CFTC guidance \cite{cftc2024,cftc2024ai} demands ``proper documentation of AI system outcomes.'' Additional regulatory oversight comes from the Federal Reserve \cite{fed2024ai} and the Office of the Comptroller of the Currency \cite{occ2024ai}. Our work bridges the gap between these requirements and technical capabilities, incorporating reproducibility guidance from PyTorch and vLLM~\cite{pytorchrepro,vllmrepro}.
\paragraph{What changed since 2024?}
Greedy decoding at \Temp{0.0} alone was not sufficient in prior runs due to retrieval order nondeterminism and batching variance. Our harness adds: (1) DeterministicRetriever implementing multi-key ordering (score↓, section\_priority↑, snippet\_id↑, chunk\_idx↑) that encodes SEC 10-K disclosure precedence rules, treating retrieval order as a \textit{compliance requirement} under Basel III explainability mandates rather than merely a performance optimization, (2) fixed seeds and sampler params in manifests, and (3) schema/invariant checks (SQL) with finance-calibrated ±5\",
2511.09677v1,http://arxiv.org/abs/2511.09677v1,2025-11-12 19:30:11+00:00,Boosted GFlowNets: Improving Exploration via Sequential Learning,"Generative Flow Networks (GFlowNets) are powerful samplers for compositional objects that, by design, sample proportionally to a given non-negative reward. Nonetheless, in practice, they often struggle to explore the reward landscape evenly: trajectories toward easy-to-reach regions dominate training, while hard-to-reach modes receive vanishing or uninformative gradients, leading to poor coverage of high-reward areas. We address this imbalance with Boosted GFlowNets, a method that sequentially trains an ensemble of GFlowNets, each optimizing a residual reward that compensates for the mass already captured by previous models. This residual principle reactivates learning signals in underexplored regions and, under mild assumptions, ensures a monotone non-degradation property: adding boosters cannot worsen the learned distribution and typically improves it. Empirically, Boosted GFlowNets achieve substantially better exploration and sample diversity on multimodal synthetic benchmarks and peptide design tasks, while preserving the stability and simplicity of standard trajectory-balance training.","\vspace{-5 pt}


GFlowNets face well-documented exploration bottlenecks on large, multimodal targets. Practical remedies span three main angles: \emph{(i) auxiliary exploration policies}, e.g., sibling-augmented schemes that collect novelty-seeking trajectories, often via intrinsic rewards such as Random Network Distillation, and re-label them off-policy for training the main learner \citep{madan2025-sa-gfn}; \emph{(ii) adaptive curricula}, where teacher--student mechanisms steer sampling toward under-covered regions identified by high loss or discrepancy signals \citep{kim2025-adaptive-teacher}; and \emph{(iii) loss-design perspectives}, where alternative regression losses (zero-avoiding families) are motivated through divergence analyses to bias training away from mode collapse and toward broader coverage \citep{hu2025-beyond-squared}. Scaling-oriented approaches decompose the state graph and train subgraphs asynchronously to enlarge visited regions under fixed budgets \citep{silva2025generalization}. Complementary theoretical works study conditions for correctness and distributional mismatch in finite-budget regimes, clarifying when coverage failures arise in practice \citep{silva2025when}.

Boosting Variational Inference \citep[BVI;][]{guo2016boosting, miller17boosting, locatello18boosting} generalizes classical boosting to probabilistic inference by iteratively constructing a mixture approximation to the target posterior. At each round, a new variational component is added to reduce the overall Kullback–Leibler divergence between the mixture and the true distribution. This procedure can be interpreted as performing functional gradient descent in the space of probability measures, where each component incrementally corrects the residual approximation error of the current mixture. Under mild smoothness and compactness assumptions on the variational family, BVI enjoys explicit convergence guarantees and systematically improves multimodal coverage by expanding the support of the approximation.

Boosted GFlowNets extend this idea to the domain of flow-based generative policies. Rather than updating a mixture density, each new GFlowNet is trained to correct the part of the target reward distribution that previous models have underrepresented. This iterative scheme redistributes probability mass from saturated regions toward underexplored modes, improving coverage while preserving the standard trajectory-balance formulation. By performing additive updates directly in flow space, BGFNs achieve monotone improvement in exploration and require no auxiliary policies or teacher modules.","\vspace{-5 pt}


GFlowNets face well-documented exploration bottlenecks on large, multimodal targets. Practical remedies span three main angles: \emph{(i) auxiliary exploration policies}, e.g., sibling-augmented schemes that collect novelty-seeking trajectories, often via intrinsic rewards such as Random Network Distillation, and re-label them off-policy for training the main learner \citep{madan2025-sa-gfn}; \emph{(ii) adaptive curricula}, where teacher--student mechanisms steer sampling toward under-covered regions identified by high loss or discrepancy signals \citep{kim2025-adaptive-teacher}; and \emph{(iii) loss-design perspectives}, where alternative regression losses (zero-avoiding families) are motivated through divergence analyses to bias training away from mode collapse and toward broader coverage \citep{hu2025-beyond-squared}. Scaling-oriented approaches decompose the state graph and train subgraphs asynchronously to enlarge visited regions under fixed budgets \citep{silva2025generalization}. Complementary theoretical works study conditions for correctness and distributional mismatch in finite-budget regimes, clarifying when coverage failures arise in practice \citep{silva2025when}.

Boosting Variational Inference \citep[BVI;][]{guo2016boosting, miller17boosting, locatello18boosting} generalizes classical boosting to probabilistic inference by iteratively constructing a mixture approximation to the target posterior. At each round, a new variational component is added to reduce the overall Kullback–Leibler divergence between the mixture and the true distribution. This procedure can be interpreted as performing functional gradient descent in the space of probability measures, where each component incrementally corrects the residual approximation error of the current mixture. Under mild smoothness and compactness assumptions on the variational family, BVI enjoys explicit convergence guarantees and systematically improves multimodal coverage by expanding the support of the approximation.

Boosted GFlowNets extend this idea to the domain of flow-based generative policies. Rather than updating a mixture density, each new GFlowNet is trained to correct the part of the target reward distribution that previous models have underrepresented. This iterative scheme redistributes probability mass from saturated regions toward underexplored modes, improving coverage while preserving the standard trajectory-balance formulation. By performing additive updates directly in flow space, BGFNs achieve monotone improvement in exploration and require no auxiliary policies or teacher modules.",
2511.02144v1,http://arxiv.org/abs/2511.02144v1,2025-11-04 00:29:23+00:00,Fast Measuring Pavement Crack Width by Cascading Principal Component Analysis,"Accurate quantification of pavement crack width plays a pivotal role in assessing structural integrity and guiding maintenance interventions. However, achieving precise crack width measurements presents significant challenges due to: (1) the complex, non-uniform morphology of crack boundaries, which limits the efficacy of conventional approaches, and (2) the demand for rapid measurement capabilities from arbitrary pixel locations to facilitate comprehensive pavement condition evaluation. To overcome these limitations, this study introduces a cascaded framework integrating Principal Component Analysis (PCA) and Robust PCA (RPCA) for efficient crack width extraction from digital images. The proposed methodology comprises three sequential stages: (1) initial crack segmentation using established detection algorithms to generate a binary representation, (2) determination of the primary orientation axis for quasi-parallel cracks through PCA, and (3) extraction of the Main Propagation Axis (MPA) for irregular crack geometries using RPCA. Comprehensive evaluations were conducted across three publicly available datasets, demonstrating that the proposed approach achieves superior performance in both computational efficiency and measurement accuracy compared to existing state-of-the-art techniques.","\label{sec:relatedwork}

%Calculating the crack width is based on the pixel-wise crack detection. Therefore, we firstly review the pixel-wise crack detection and then the measurement crack width for pavement.
%已改
\subsection{Pixel-Wise Crack Detection}

Algorithms for pixel-level crack detection operate by assigning a categorical label to every pixel within an image, thereby achieving a comprehensive, pixel-by-pixel detection map. This task is fundamentally an application of semantic segmentation methodologies. Notable examples include the utilization of U-Net~\cite{Ronneberger2015Biomedical} by Jenkins \textit{et al.}\cite{David2018Semantic} and the introduction of DeepCrack~\cite{Qin2018DeepCrack} specifically for this purpose. Both approaches leverage an encoder-decoder architecture, which classifies pixels as either 'crack' or 'background'. This is achieved by using multiple down-sampling stages to capture the latent crack structure and multiple up-sampling stages to facilitate feature fusion. In parallel, \cite{Yang2017Estimation} conducted an experimental evaluation on the influence of U-Net's architectural parameters—such as layer depth, convolution kernel dimensions, and feature map quantity—on both segmentation accuracy and computational speed. Their findings offer a valuable reference for optimizing network design.

Moreover, within the domain of semantic segmentation, feature fusion has been identified as a critical component for performance enhancement. This technique allows for the integration of contextual information derived from disparate feature levels, culminating in more refined and accurate crack detection outcomes~\cite{Qu2021Pavement}~\cite{Xiang2022concrete}~\cite{Song2020automated}~\cite{liu2019deepcrack}.

Nevertheless, the inherent spatial complexity of crack structures presents a significant challenge for pixel-level segmentation. The manual, pixel-by-pixel annotation required during pre-processing is highly dependent on the subjective judgment of human annotators. This annotation process is notoriously resource-intensive, being costly, laborious, and susceptible to errors such as mislabeling or omissions. From a practical standpoint, the industry places high importance on these subtle, fine-grained cracks, as their early-stage repair is more economical than addressing severe degradation. This context necessitates that any subsequent crack width measurement technique must exhibit robustness, performing reliably regardless of variations in the underlying segmentation method used to generate its input.

\subsection{Crack Width Measurement}

 %<A hybrid method for pavement crack width measurement>
%已改
Methodologies for crack quantification are broadly classified as either semi-automatic or fully automatic. Semi-automatic approaches typically couple an image acquisition system with subsequent manual analysis, where operators assess the crack severity. For instance, while straightforward, techniques like the crack scale method~\cite{Yamaguchi2010Practical} are known to be time-consuming. Conversely, automatic methods depend on image processing algorithms to perform the analysis, identification, and assessment of cracks~\cite{WeixingWangA2019Pavement}. These automatic approaches can be differentiated into three primary research streams.

The first stream relies on geometric simplification, modeling a crack segment as a rectangle-like shape. For example,~\cite{liu2021crackformer} defined width as the shortest distance from the crack's skeleton to its boundaries. \cite{Benz2021Vision} adopted a method that measured width by transforming the grayscale pixel distribution of the crack into a rectangle of equivalent area. The fundamental drawback of these methods is their reliance on this rectangular assumption to identify the MPA. This simplification fails for complex geometries like alligator cracks, often resulting in an underestimation of the true width.

The second research avenue focuses on leveraging the crack's skeleton to ascertain its propagation direction. Illustratively,~\cite{Takafumi2011Concrete} applied cardinal spline interpolation to the skeleton and then utilized the derivative of the interpolated curve to determine orientation. ~\cite{Wang2017Methodology} introduced an orthogonal projection method, which measures width by simplifying the crack geometry using contours formed from boundary points. However, reliably extracting a meaningful skeleton from alligator cracks is a significant challenge for these techniques, as both spline interpolation and orthogonal projection struggle with such complex topologies.

The third research line operates on the assumption that small crack segments possess parallel boundaries, enabling width measurement based on this local geometry~\cite{Weng2019Segment}~\cite{Ni2019Zernike}. Examples include finding the Euclidean distance along a normal line intersecting the boundary pixels~\cite{Kim2019Image-based}, or determining orientation using a central difference scheme on skeleton points~\cite{Payab2019Review}. \cite{Wang2018PavementCrack} posited a direct correspondence between pixels on opposing boundaries. This ""parallel boundary"" hypothesis, however, rarely holds true for intricate criss-cross patterns or the complex peripheries of alligator cracks. As discussed by~\cite{Wang2017Methodology}, the presence of non-parallel boundaries or regions of high curvature can, in fact, lead to an overestimation of the crack width.","\subsection{Pixel-Wise Crack Detection}

Algorithms for pixel-level crack detection operate by assigning a categorical label to every pixel within an image, thereby achieving a comprehensive, pixel-by-pixel detection map. This task is fundamentally an application of semantic segmentation methodologies. Notable examples include the utilization of U-Net~\cite{Ronneberger2015Biomedical} by Jenkins \textit{et al.}\cite{David2018Semantic} and the introduction of DeepCrack~\cite{Qin2018DeepCrack} specifically for this purpose. Both approaches leverage an encoder-decoder architecture, which classifies pixels as either 'crack' or 'background'. This is achieved by using multiple down-sampling stages to capture the latent crack structure and multiple up-sampling stages to facilitate feature fusion. In parallel, \cite{Yang2017Estimation} conducted an experimental evaluation on the influence of U-Net's architectural parameters—such as layer depth, convolution kernel dimensions, and feature map quantity—on both segmentation accuracy and computational speed. Their findings offer a valuable reference for optimizing network design.

Moreover, within the domain of semantic segmentation, feature fusion has been identified as a critical component for performance enhancement. This technique allows for the integration of contextual information derived from disparate feature levels, culminating in more refined and accurate crack detection outcomes~\cite{Qu2021Pavement}~\cite{Xiang2022concrete}~\cite{Song2020automated}~\cite{liu2019deepcrack}.

Nevertheless, the inherent spatial complexity of crack structures presents a significant challenge for pixel-level segmentation. The manual, pixel-by-pixel annotation required during pre-processing is highly dependent on the subjective judgment of human annotators. This annotation process is notoriously resource-intensive, being costly, laborious, and susceptible to errors such as mislabeling or omissions. From a practical standpoint, the industry places high importance on these subtle, fine-grained cracks, as their early-stage repair is more economical than addressing severe degradation. This context necessitates that any subsequent crack width measurement technique must exhibit robustness, performing reliably regardless of variations in the underlying segmentation method used to generate its input.

\subsection{Crack Width Measurement}

 

Methodologies for crack quantification are broadly classified as either semi-automatic or fully automatic. Semi-automatic approaches typically couple an image acquisition system with subsequent manual analysis, where operators assess the crack severity. For instance, while straightforward, techniques like the crack scale method~\cite{Yamaguchi2010Practical} are known to be time-consuming. Conversely, automatic methods depend on image processing algorithms to perform the analysis, identification, and assessment of cracks~\cite{WeixingWangA2019Pavement}. These automatic approaches can be differentiated into three primary research streams.

The first stream relies on geometric simplification, modeling a crack segment as a rectangle-like shape. For example,~\cite{liu2021crackformer} defined width as the shortest distance from the crack's skeleton to its boundaries. \cite{Benz2021Vision} adopted a method that measured width by transforming the grayscale pixel distribution of the crack into a rectangle of equivalent area. The fundamental drawback of these methods is their reliance on this rectangular assumption to identify the MPA. This simplification fails for complex geometries like alligator cracks, often resulting in an underestimation of the true width.

The second research avenue focuses on leveraging the crack's skeleton to ascertain its propagation direction. Illustratively,~\cite{Takafumi2011Concrete} applied cardinal spline interpolation to the skeleton and then utilized the derivative of the interpolated curve to determine orientation. ~\cite{Wang2017Methodology} introduced an orthogonal projection method, which measures width by simplifying the crack geometry using contours formed from boundary points. However, reliably extracting a meaningful skeleton from alligator cracks is a significant challenge for these techniques, as both spline interpolation and orthogonal projection struggle with such complex topologies.

The third research line operates on the assumption that small crack segments possess parallel boundaries, enabling width measurement based on this local geometry~\cite{Weng2019Segment}~\cite{Ni2019Zernike}. Examples include finding the Euclidean distance along a normal line intersecting the boundary pixels~\cite{Kim2019Image-based}, or determining orientation using a central difference scheme on skeleton points~\cite{Payab2019Review}. \cite{Wang2018PavementCrack} posited a direct correspondence between pixels on opposing boundaries. This ""parallel boundary"" hypothesis, however, rarely holds true for intricate criss-cross patterns or the complex peripheries of alligator cracks. As discussed by~\cite{Wang2017Methodology}, the presence of non-parallel boundaries or regions of high curvature can, in fact, lead to an overestimation of the crack width.",
2511.07997v1,http://arxiv.org/abs/2511.07997v1,2025-11-11 09:00:51+00:00,PrAda-GAN: A Private Adaptive Generative Adversarial Network with Bayes Network Structure,"We revisit the problem of generating synthetic data under differential privacy. To address the core limitations of marginal-based methods, we propose the Private Adaptive Generative Adversarial Network with Bayes Network Structure (PrAda-GAN), which integrates the strengths of both GAN-based and marginal-based approaches. Our method adopts a sequential generator architecture to capture complex dependencies among variables, while adaptively regularizing the learned structure to promote sparsity in the underlying Bayes network. Theoretically, we establish diminishing bounds on the parameter distance, variable selection error, and Wasserstein distance. Our analysis shows that leveraging dependency sparsity leads to significant improvements in convergence rates. Empirically, experiments on both synthetic and real-world datasets demonstrate that PrAda-GAN outperforms existing tabular data synthesis methods in terms of the privacy-utility trade-off.","\label{sec:related work} 


\paragraph{GAN Based Methods.}
Given the remarkable success of GANs \citep{goodfellow2014generative}, a growing body of works have explored their applications to differentially private data synthesis
\citep{jordon2018pate, liu2019ppgan, chen2020gs, long2021g, bie2023private,ma2023rdp}.
A common approach to privatizing GANs is to apply differentially private stochastic gradient descent (DPSGD) \citep{abadi2016deep} when updating the discriminator, a method known as DPGAN
\citep{xie2018differentially, zhang2018differentially, torkzadehmahani2019dp, zhao2024ctab}.
This technique is shown to be effective for generating private synthetic data across various domains.

\paragraph{Marginal-based Methods.}
Low-order marginals are widely adopted in tabular data synthesis due to their ability to capture essential low-dimensional structures while exhibiting low sensitivity under DP \citep{hu2024sok}. Marginal-based approaches typically select a set of marginals, inject calibrated noise, and reconstruct the joint distribution to generate synthetic data. A promising line of work employs Bayes networks to model conditional dependencies through a directed acyclic graph (DAG). Representative examples include \texttt{PrivBayes} \citep{zhang2017privbayes}, which learns the network structure from data and perturbs the conditional distributions, and AIM \citep{mckenna2022aim}, which enhances utility by tailoring the network structure to the available private marginals. Recent work established statistical foundations for marginal-based private synthesis \citep{li2023statistical}. 

An alternative line of research models data distributions using Markov random fields (MRFs)—undirected graphical models that capture symmetric relationships among variables. These methods estimate noisy low-order marginals and reconstruct the global distribution using inference techniques such as Gibbs sampling \citep{chen2015differentially, mckenna2019graphical, cai2021data}. By leveraging local Markov properties, MRF-based approaches are capable of representing complex dependencies while maintaining scalability and privacy.","\paragraph{GAN Based Methods.}
Given the remarkable success of GANs \citep{goodfellow2014generative}, a growing body of works have explored their applications to differentially private data synthesis
\citep{jordon2018pate, liu2019ppgan, chen2020gs, long2021g, bie2023private,ma2023rdp}.
A common approach to privatizing GANs is to apply differentially private stochastic gradient descent (DPSGD) \citep{abadi2016deep} when updating the discriminator, a method known as DPGAN
\citep{xie2018differentially, zhang2018differentially, torkzadehmahani2019dp, zhao2024ctab}.
This technique is shown to be effective for generating private synthetic data across various domains.

\paragraph{Marginal-based Methods.}
Low-order marginals are widely adopted in tabular data synthesis due to their ability to capture essential low-dimensional structures while exhibiting low sensitivity under DP \citep{hu2024sok}. Marginal-based approaches typically select a set of marginals, inject calibrated noise, and reconstruct the joint distribution to generate synthetic data. A promising line of work employs Bayes networks to model conditional dependencies through a directed acyclic graph (DAG). Representative examples include \texttt{PrivBayes} \citep{zhang2017privbayes}, which learns the network structure from data and perturbs the conditional distributions, and AIM \citep{mckenna2022aim}, which enhances utility by tailoring the network structure to the available private marginals. Recent work established statistical foundations for marginal-based private synthesis \citep{li2023statistical}. 

An alternative line of research models data distributions using Markov random fields (MRFs)—undirected graphical models that capture symmetric relationships among variables. These methods estimate noisy low-order marginals and reconstruct the global distribution using inference techniques such as Gibbs sampling \citep{chen2015differentially, mckenna2019graphical, cai2021data}. By leveraging local Markov properties, MRF-based approaches are capable of representing complex dependencies while maintaining scalability and privacy.",
2511.02345v1,http://arxiv.org/abs/2511.02345v1,2025-11-04 08:08:00+00:00,Reducing normalizing flow complexity for MCMC preconditioning,"Preconditioning is a key component of MCMC algorithms that improves sampling efficiency by facilitating exploration of geometrically complex target distributions through an invertible map. While linear preconditioners are often sufficient for moderately complex target distributions, recent work has explored nonlinear preconditioning with invertible neural networks as components of normalizing flows (NFs). However, empirical and theoretical studies show that overparameterized NF preconditioners can degrade sampling efficiency and fit quality. Moreover, existing NF-based approaches do not adapt their architectures to the target distribution. Related work outside of MCMC similarly finds that suitably parameterized NFs can achieve comparable or superior performance with substantially less training time or data. We propose a factorized preconditioning architecture that reduces NF complexity by combining a linear component with a conditional NF, improving adaptability to target geometry. The linear preconditioner is applied to dimensions that are approximately Gaussian, as estimated from warmup samples, while the conditional NF models more complex dimensions. Our method yields significantly better tail samples on two complex synthetic distributions and consistently better performance on a sparse logistic regression posterior across varying likelihood and prior strengths. It also achieves higher effective sample sizes on hierarchical Bayesian model posteriors with weak likelihoods and strong funnel geometries. This approach is particularly relevant for hierarchical Bayesian model analyses with limited data and could inform current theoretical and software strides in neural MCMC design.","% On NF preconditioning for MCMC
The importance of nonlinear preconditioning in MCMC stems from the non-Gaussianity of various target distributions.
In HBMs,~\cite{neal_slice_2003} demonstrated the inefficiency of the Gibbs sampler on a synthetic funnel distribution.
\cite{betancourt_hamiltonian_2015} further discussed that such geometries are very prevalent in HBMs with sparse data and thus strong priors.
They suggest designing non-centered parameterizations~\citep{papaspiliopoulos_general_2007} to improve posterior geometries for HMC sampling.

Since then, many works suggested automated or learned model reparameterization~\citep{parno_transport_2018, gorinova_automatic_2020} with~\cite{hoffman_neutralizing_2019} suggesting nonlinear MCMC preconditioning based on NFs.
There have been several further advancements connecting NF preconditioning or global NF proposals with models in cosmology~\citep{gabrie_efficient_2021,karamanis_accelerating_2022, grumitt_sequential_2024}, field theory~\citep{gabrie_adaptive_2022}, molecular dynamics~\citep{gabrie_adaptive_2022}, chaotic systems~\citep{grumitt_flow_2024}, as well as HBMs and general sampling~\citep{arbel_annealed_2021, grumitt_deterministic_2022, matthews_continual_2022}.
The methods show improved sampling efficiency on target distributions with funnel geometries and multimodality, as well as ill-conditioned distributions.

There exists less systematic research regarding suitable NF architectures for MCMC preconditioning, though several recent works address different aspects of the problem.
In a recent empirical investigation,~\cite{nabergoj_empirical_2024} found that using NFs with $10^3$ to $10^4$ trainable parameters achieved better results than bigger models on targets with fewer than 1000 model parameters, indicating that overparametrization could be hindering the preconditioning performance of NFs.
\cite{grenioux_sampling_2023a} observed a reduction in sliced total variation and Kolmogorov-Smirnov distances between MCMC samples and the target distribution as the NF fit better matches the target distribution.
Outside of MCMC, several works proposed injecting domain knowledge into NFs via probabilistic graphical models~\citep[PGMs;][]{weilbach_structured_2020, wehenkel_graphical_2021, mouton_graphical_2022a}.
\cite{wehenkel_graphical_2021} found this approach to substantially outperform NF architectures without any injected knowledge on density estimation tasks.
Moreover, they found that such NFs only require a single layer to match the density estimation performance of architectures with multiple layers.
These ideas have also been explored in structured equation modeling and structured causal modeling~\citep{khemakhem_causal_2021, balgi_personalized_2022, javaloy_causal_2023a} with \cite{khemakhem_causal_2021} similarly reporting high efficiency at small sample sizes.
While inferring PGMs or causal graphs may not be directly applicable for arbitrary target distributions in MCMC, these works highlight the promise of designing an NF with a more explicit treatment of geometrically nontrivial dimensions, including a simplified treatment of approximately Gaussian dimensions.
Further,~\cite{wehenkel_graphical_2021} describe an approach for automatic determination of PGMs, and thus demonstrate that some degree of knowledge can be injected into NFs without user input.
This further motivates automatically identifying information that can inform preconditioner parameterization, such as approximate Gaussianity, as investigated in this work.

Our approach to tailoring preconditioner complexity to the target distribution can be related to Gibbs sampling~\citep{geman_stochastic_1984}.
A Gibbs step consists of updating a subset of state dimensions, then updating the remaining dimensions conditional on the former.
Our method similarly factorizes the preconditioner into a linear and a nonlinear component, each appropriately transforming it corresponding dimensions into an approximately Gaussian latent space.
However, rather than restricting ourselves to sampling with fixed dimension subsets as in Gibbs, we allow joint sampling of the entire distribution with an arbitrary MCMC sampler.
This allows MCMC to implicitly correct for suboptimal subset choices, while simplifying the geometry of each subset.

Funnel-shaped target distributions represent a relevant test case for our work, as successfully identifying approximately Gaussian dimensions can lead to a highly efficient reparameterization of the funnel geometry.
Several works already approached this problem, including slice sampling~\citep{neal_slice_2003}, which generalizes Gibbs updates by sampling under the multidimensional target density plot conditional on a slice variable; and Riemannian HMC~\citep{girolami_riemann_2011} which replaces the constant mass matrix (equivalent to preconditioning) with a position-specific metric based on a Riemannian manifold.
Recently,~\cite{gundersen_escaping_2025} proposed sampling from a generalized higher-dimensional HBM that reduces funnel sharpness, then numerically marginalizing out the parameters of the generalized model to recover samples from the target HBM.
The drawback of Riemannian HMC is the apparent numerical instability in the proposed Hamiltonian.
Slice sampling and generalized HBMs focus on the sampling strategy, but do not consider improving the underlying preconditioner.
Our direct focus in this work is reducing NF complexity for general MCMC preconditioning, potentially including non-HBM target distributions.","The importance of nonlinear preconditioning in MCMC stems from the non-Gaussianity of various target distributions.
In HBMs,~\cite{neal_slice_2003} demonstrated the inefficiency of the Gibbs sampler on a synthetic funnel distribution.
\cite{betancourt_hamiltonian_2015} further discussed that such geometries are very prevalent in HBMs with sparse data and thus strong priors.
They suggest designing non-centered parameterizations~\citep{papaspiliopoulos_general_2007} to improve posterior geometries for HMC sampling.

Since then, many works suggested automated or learned model reparameterization~\citep{parno_transport_2018, gorinova_automatic_2020} with~\cite{hoffman_neutralizing_2019} suggesting nonlinear MCMC preconditioning based on NFs.
There have been several further advancements connecting NF preconditioning or global NF proposals with models in cosmology~\citep{gabrie_efficient_2021,karamanis_accelerating_2022, grumitt_sequential_2024}, field theory~\citep{gabrie_adaptive_2022}, molecular dynamics~\citep{gabrie_adaptive_2022}, chaotic systems~\citep{grumitt_flow_2024}, as well as HBMs and general sampling~\citep{arbel_annealed_2021, grumitt_deterministic_2022, matthews_continual_2022}.
The methods show improved sampling efficiency on target distributions with funnel geometries and multimodality, as well as ill-conditioned distributions.

There exists less systematic research regarding suitable NF architectures for MCMC preconditioning, though several recent works address different aspects of the problem.
In a recent empirical investigation,~\cite{nabergoj_empirical_2024} found that using NFs with $10^3$ to $10^4$ trainable parameters achieved better results than bigger models on targets with fewer than 1000 model parameters, indicating that overparametrization could be hindering the preconditioning performance of NFs.
\cite{grenioux_sampling_2023a} observed a reduction in sliced total variation and Kolmogorov-Smirnov distances between MCMC samples and the target distribution as the NF fit better matches the target distribution.
Outside of MCMC, several works proposed injecting domain knowledge into NFs via probabilistic graphical models~\citep[PGMs;][]{weilbach_structured_2020, wehenkel_graphical_2021, mouton_graphical_2022a}.
\cite{wehenkel_graphical_2021} found this approach to substantially outperform NF architectures without any injected knowledge on density estimation tasks.
Moreover, they found that such NFs only require a single layer to match the density estimation performance of architectures with multiple layers.
These ideas have also been explored in structured equation modeling and structured causal modeling~\citep{khemakhem_causal_2021, balgi_personalized_2022, javaloy_causal_2023a} with \cite{khemakhem_causal_2021} similarly reporting high efficiency at small sample sizes.
While inferring PGMs or causal graphs may not be directly applicable for arbitrary target distributions in MCMC, these works highlight the promise of designing an NF with a more explicit treatment of geometrically nontrivial dimensions, including a simplified treatment of approximately Gaussian dimensions.
Further,~\cite{wehenkel_graphical_2021} describe an approach for automatic determination of PGMs, and thus demonstrate that some degree of knowledge can be injected into NFs without user input.
This further motivates automatically identifying information that can inform preconditioner parameterization, such as approximate Gaussianity, as investigated in this work.

Our approach to tailoring preconditioner complexity to the target distribution can be related to Gibbs sampling~\citep{geman_stochastic_1984}.
A Gibbs step consists of updating a subset of state dimensions, then updating the remaining dimensions conditional on the former.
Our method similarly factorizes the preconditioner into a linear and a nonlinear component, each appropriately transforming it corresponding dimensions into an approximately Gaussian latent space.
However, rather than restricting ourselves to sampling with fixed dimension subsets as in Gibbs, we allow joint sampling of the entire distribution with an arbitrary MCMC sampler.
This allows MCMC to implicitly correct for suboptimal subset choices, while simplifying the geometry of each subset.

Funnel-shaped target distributions represent a relevant test case for our work, as successfully identifying approximately Gaussian dimensions can lead to a highly efficient reparameterization of the funnel geometry.
Several works already approached this problem, including slice sampling~\citep{neal_slice_2003}, which generalizes Gibbs updates by sampling under the multidimensional target density plot conditional on a slice variable; and Riemannian HMC~\citep{girolami_riemann_2011} which replaces the constant mass matrix (equivalent to preconditioning) with a position-specific metric based on a Riemannian manifold.
Recently,~\cite{gundersen_escaping_2025} proposed sampling from a generalized higher-dimensional HBM that reduces funnel sharpness, then numerically marginalizing out the parameters of the generalized model to recover samples from the target HBM.
The drawback of Riemannian HMC is the apparent numerical instability in the proposed Hamiltonian.
Slice sampling and generalized HBMs focus on the sampling strategy, but do not consider improving the underlying preconditioner.
Our direct focus in this work is reducing NF complexity for general MCMC preconditioning, potentially including non-HBM target distributions.",
2511.09845v1,http://arxiv.org/abs/2511.09845v1,2025-11-13 00:59:20+00:00,Bridging Constraints and Stochasticity: A Fully First-Order Method for Stochastic Bilevel Optimization with Linear Constraints,"This work provides the first finite-time convergence guarantees for linearly constrained stochastic bilevel optimization using only first-order methods, requiring solely gradient information without any Hessian computations or second-order derivatives. We address the unprecedented challenge of simultaneously handling linear constraints, stochastic noise, and finite-time analysis in bilevel optimization, a combination that has remained theoretically intractable until now. While existing approaches either require second-order information, handle only unconstrained stochastic problems, or provide merely asymptotic convergence results, our method achieves finite-time guarantees using gradient-based techniques alone. We develop a novel framework that constructs hypergradient approximations via smoothed penalty functions, using approximate primal and dual solutions to overcome the fundamental challenges posed by the interaction between linear constraints and stochastic noise. Our theoretical analysis provides explicit finite-time bounds on the bias and variance of the hypergradient estimator, demonstrating how approximation errors interact with stochastic perturbations. We prove that our first-order algorithm converges to $(δ, ε)$-Goldstein stationary points using $Θ(δ^{-1}ε^{-5})$ stochastic gradient evaluations, establishing the first finite-time complexity result for this challenging problem class and representing a significant theoretical breakthrough in constrained stochastic bilevel optimization.","\textbf{Penalty Methods and First-Order Reformulations.} To reduce the computational cost of second-order derivatives in bilevel optimization, recent works have proposed scalable, first-order algorithms based on penalty formulations~\cite{kwon2023,liu2022bome}. These techniques transform constrained bilevel problems into single-level surrogates that can be solved efficiently with convergence guarantees, where deterministic, partially stochastic, and fully stochastic bilevel optimization can achieve $\epsilon$-stationary point in $O(\epsilon^{-2})$, $O(\epsilon^{-4})$, $O(\epsilon^{-6})$ gradient calls, respectively~\cite{chen2024optimal}. The convergence rate of the deterministic case can be further improved to $O(\epsilon^{-1.5})$ by momentum-based method~\cite{yang2023achieving}.

\paragraph{Bilevel Optimization with Linear Constraints.}
Due to the nonsmoothness of constrained bilevel optimization problems, ~\cite{kornowski2024} focuses on Goldstein stationary~\cite{goldstein1977} and designs a new penalty method to achieve a zero-th order algorithm with $O(\delta^{-1} \epsilon^{-3})$ convergence and a first-order algorithm with $O(\delta^{-1} \epsilon^{-4})$ convergence to a ($\epsilon,\delta$)-Goldstein stationary point.
On the other hand, \cite{yao2024constrained,lu2024first} consider a different stationarity using $\epsilon$-KKT stationary, where \cite{lu2024first} achieves a $O(\epsilon^{-7})$ convergence rate, and \cite{yao2024constrained} achieves a $O(\epsilon^{-2})$ rate under a stronger assumption of access to projection operators.

% The use of quadratic penalties for handling linear constraints has been particularly effective in deterministic settings, where dimension-free rates can be established. Complementary developments in generalized Nash equilibrium and constraint handling further illustrate the power of smooth penalty functions~\cite{gebken2024analyzing}.

% \textbf{Stochastic Bilevel Optimization.} Extending bilevel optimization to the stochastic setting introduces additional challenges due to gradient noise from simulation or sampling. Recent methods address these challenges in the unconstrained setting~\cite{ghadimi2018approximation,kwon2023}, where differentiability and smoothness assumptions are often preserved. However, when the lower-level problem includes constraints, new difficulties arise due to active-set changes and resulting nonsmoothness.

\textbf{Nonsmooth and nonconvex optimization.} The nonsmooth and nonconvex structure of bilevel optimization with constraints makes its analysis closely related to nonsmooth nonconvex optimization. The best-known convergence result is given by \cite{zhang2020complexity}, which establishes optimal convergence rates of $O(\delta^{-1} \epsilon^{-3})$ for the deterministic case and $O(\delta^{-1} \epsilon^{-4})$ for the stochastic case. Our result of $\tilde{O}(\delta^{-1}\epsilon^{-5})$ is one factor of $\epsilon$ away from the optimal stochastic rate, indicating potential room for improvement.


% Nonsmooth bilevel optimization presents structural instability due to discontinuous changes in the active constraint set. Analytical tools such as calmness and exact penalization~\cite{burke1991} have been proposed to ensure solution regularity under weaker assumptions than full differentiability. In stochastic environments, recent work has developed model-based minimization techniques for weakly convex objectives~\cite{davis2019stochastic}, and advanced variance-reduction tools such as SPIDER~\cite{fang2018spider} and momentum-based estimators~\cite{cutkosky2019momentum} improve the stability of gradient approximations.","\textbf{Penalty Methods and First-Order Reformulations.} To reduce the computational cost of second-order derivatives in bilevel optimization, recent works have proposed scalable, first-order algorithms based on penalty formulations~\cite{kwon2023,liu2022bome}. These techniques transform constrained bilevel problems into single-level surrogates that can be solved efficiently with convergence guarantees, where deterministic, partially stochastic, and fully stochastic bilevel optimization can achieve $\epsilon$-stationary point in $O(\epsilon^{-2})$, $O(\epsilon^{-4})$, $O(\epsilon^{-6})$ gradient calls, respectively~\cite{chen2024optimal}. The convergence rate of the deterministic case can be further improved to $O(\epsilon^{-1.5})$ by momentum-based method~\cite{yang2023achieving}.

\paragraph{Bilevel Optimization with Linear Constraints.}
Due to the nonsmoothness of constrained bilevel optimization problems, ~\cite{kornowski2024} focuses on Goldstein stationary~\cite{goldstein1977} and designs a new penalty method to achieve a zero-th order algorithm with $O(\delta^{-1} \epsilon^{-3})$ convergence and a first-order algorithm with $O(\delta^{-1} \epsilon^{-4})$ convergence to a ($\epsilon,\delta$)-Goldstein stationary point.
On the other hand, \cite{yao2024constrained,lu2024first} consider a different stationarity using $\epsilon$-KKT stationary, where \cite{lu2024first} achieves a $O(\epsilon^{-7})$ convergence rate, and \cite{yao2024constrained} achieves a $O(\epsilon^{-2})$ rate under a stronger assumption of access to projection operators.





\textbf{Nonsmooth and nonconvex optimization.} The nonsmooth and nonconvex structure of bilevel optimization with constraints makes its analysis closely related to nonsmooth nonconvex optimization. The best-known convergence result is given by \cite{zhang2020complexity}, which establishes optimal convergence rates of $O(\delta^{-1} \epsilon^{-3})$ for the deterministic case and $O(\delta^{-1} \epsilon^{-4})$ for the stochastic case. Our result of $\tilde{O}(\delta^{-1}\epsilon^{-5})$ is one factor of $\epsilon$ away from the optimal stochastic rate, indicating potential room for improvement.",
2511.06641v1,http://arxiv.org/abs/2511.06641v1,2025-11-10 02:38:27+00:00,Neyman-Pearson Classification under Both Null and Alternative Distributions Shift,"We consider the problem of transfer learning in Neyman-Pearson classification, where the objective is to minimize the error w.r.t. a distribution $μ_1$, subject to the constraint that the error w.r.t. a distribution $μ_0$ remains below a prescribed threshold. While transfer learning has been extensively studied in traditional classification, transfer learning in imbalanced classification such as Neyman-Pearson classification has received much less attention. This setting poses unique challenges, as both types of errors must be simultaneously controlled. Existing works address only the case of distribution shift in $μ_1$, whereas in many practical scenarios shifts may occur in both $μ_0$ and $μ_1$. We derive an adaptive procedure that not only guarantees improved Type-I and Type-II errors when the source is informative, but also automatically adapt to situations where the source is uninformative, thereby avoiding negative transfer. In addition to such statistical guarantees, the procedures is efficient, as shown via complementary computational guarantees.","Transfer learning has been extensively studied in traditional classification and regression \citep{li2022transfer,cai2021transfer,kpotufe2021marginal,tripuraneni2020theory,mousavi2020minimax,ben2010theory,ben2006analysis,mansour2009domain}. In the context of binary classification over a hypothesis class, \citep{hanneke2019value} introduce the notion of transfer exponent, which translates the performance of a classifier from the source domain to the target domain, and achieve the minimax rate adaptively in balanced classification. However, none of these works address imbalanced classification, where the central challenge is to simultaneously control both Type-I and Type-II errors.

The problem of NP classification when the underlying distributions are unknown except through samples was first formulated by \citet{cannon2002learning, scott2005neyman}, who considered empirical risk minimization with respect to one class while constraining the empirical error on the other below a pre-specified threshold. \citet{rigollet2011neyman} studied the same problem under a surrogate convex loss. \citet{tong2013plug} studied a nonparametric NP classification framework and established rates of convergence for a plug-in approach based on estimating the class distributions. More recently, \citet{kalan2024distribution} derived distribution-free minimax rates for NP classification and showed that, unlike traditional classification, the problem exhibits a dichotomy between fast and slow rates.

Related to this work, in the context of transfer learning in NP classification, \citet{kalantight} derived minimax rates for $0$-$1$ loss. Building on this, \citet{kalan2025transfer} introduced an implementable transfer learning procedure for NP classification using a surrogate loss and derived upper bound guarantees. Both \citet{kalantight,kalan2025transfer} focused on the restricted setting where the source and target share the class-$0$ distribution $\mu_0$ and the shift occurs only in $\mu_1$. Moreover, they established only statistical guarantees without addressing computational aspects. However, in this work, we consider the general setting where distribution shifts may occur in both $\mu_0$ and $\mu_1$. We propose an adaptive procedure with statistical guarantees that also recovers the special case in \citet{kalantight,kalan2025transfer}. On the computational side, we reformulate the learning problem within a two-stage convex programming framework and develop a concrete stochastic optimization procedure that outputs a model with the desired excess-risk guarantee with a bound on the gradient complexity of the procedure.

%Related to this work in the context of transfer learning in NP classification, \citep{kalantight} derived minimax rate for transfer learning in NP classification without proposing an implemtable procedure. Following this work, \citep{kalan2025transfer} proposed an implemetbale  transfer learning procedure in NP classifcation using a surrogate loss and dervied upper bound guarntees. Both of \cite{kalantight,kalan2025transfer} only considered the restricted case where source and target share class 0 distribution $\mu_0$ and there is only a shift in $\mu_1$. Furtherore, \cite{kalantight,kalan2025transfer} only derived statistical garuantees wihtout any computational garantees. In this work, we consider the most general case where there could be distribution shift in both $\mu_0$ and $\mu_1$. We propose an adaptive procedure along with its statsitical ganratees which recover the special case in \cite{kalantight,kalan2025transfer}. Furthermore, computationally, we reformulate the learning problem into two stage convex programs, and provide a concrete stochastic optimization procedure that returns the model with desired excess risk guarantee. We also show that the stochastic gradient complexity scales linearly only with the number of source and target class 1 samples.




%The problem of NP classification when the distributions are unknown except through samples initiated by \citep{cannon2002learning,scott2005neyman} which consider empriical risk minimization with respect to one class while the empirical error w.r.t. other class is less than a threshold. \citep{rigollet2011neyman} studies the same problem but under a surrogate convex loss. \citep{tong2013plug} considers a nonparametric NP classification and analyzes the rate of plug-in approach where estimates the distribitons of each class. \citep{kalan2024distribution} derives distribution free minimax rates of NP classification and shows unlike traditional there is dichotomy of fast and slow rates.







%Transfer Learning has been studied vastly in traditional classification and regression \citep{li2022transfer,ben2010theory,ben2006analysis,mansour2009domain,mousavi2020minimax}. \citep{cai2021transfer} introduces a notion of transferability using the regression functions in the source and target in nonparamteric setting for binary classification and achives the minimax rate adaptively without prior knowledge of relatedness between source and target. More related to this work, \citep{hanneke2019value} introduces the notion of transfer exponent over a hypothesis class which translates the performance of a classifer from source to target and adaptively achives the minimax rate in classfication. However, none of these work consider imbalacned classfication where the challenge is that both Type-I and Type-II errors must be controlled simultaneasly.
\vspace{-2mm}","Transfer learning has been extensively studied in traditional classification and regression \citep{li2022transfer,cai2021transfer,kpotufe2021marginal,tripuraneni2020theory,mousavi2020minimax,ben2010theory,ben2006analysis,mansour2009domain}. In the context of binary classification over a hypothesis class, \citep{hanneke2019value} introduce the notion of transfer exponent, which translates the performance of a classifier from the source domain to the target domain, and achieve the minimax rate adaptively in balanced classification. However, none of these works address imbalanced classification, where the central challenge is to simultaneously control both Type-I and Type-II errors.

The problem of NP classification when the underlying distributions are unknown except through samples was first formulated by \citet{cannon2002learning, scott2005neyman}, who considered empirical risk minimization with respect to one class while constraining the empirical error on the other below a pre-specified threshold. \citet{rigollet2011neyman} studied the same problem under a surrogate convex loss. \citet{tong2013plug} studied a nonparametric NP classification framework and established rates of convergence for a plug-in approach based on estimating the class distributions. More recently, \citet{kalan2024distribution} derived distribution-free minimax rates for NP classification and showed that, unlike traditional classification, the problem exhibits a dichotomy between fast and slow rates.

Related to this work, in the context of transfer learning in NP classification, \citet{kalantight} derived minimax rates for $0$-$1$ loss. Building on this, \citet{kalan2025transfer} introduced an implementable transfer learning procedure for NP classification using a surrogate loss and derived upper bound guarantees. Both \citet{kalantight,kalan2025transfer} focused on the restricted setting where the source and target share the class-$0$ distribution $\mu_0$ and the shift occurs only in $\mu_1$. Moreover, they established only statistical guarantees without addressing computational aspects. However, in this work, we consider the general setting where distribution shifts may occur in both $\mu_0$ and $\mu_1$. We propose an adaptive procedure with statistical guarantees that also recovers the special case in \citet{kalantight,kalan2025transfer}. On the computational side, we reformulate the learning problem within a two-stage convex programming framework and develop a concrete stochastic optimization procedure that outputs a model with the desired excess-risk guarantee with a bound on the gradient complexity of the procedure.















\vspace{-2mm}",
2511.07270v1,http://arxiv.org/abs/2511.07270v1,2025-11-10 16:17:16+00:00,High-Dimensional Asymptotics of Differentially Private PCA,"In differential privacy, statistics of a sensitive dataset are privatized by introducing random noise. Most privacy analyses provide privacy bounds specifying a noise level sufficient to achieve a target privacy guarantee. Sometimes, these bounds are pessimistic and suggest adding excessive noise, which overwhelms the meaningful signal. It remains unclear if such high noise levels are truly necessary or a limitation of the proof techniques. This paper explores whether we can obtain sharp privacy characterizations that identify the smallest noise level required to achieve a target privacy level for a given mechanism. We study this problem in the context of differentially private principal component analysis, where the goal is to privatize the leading principal components (PCs) of a dataset with n samples and p features. We analyze the exponential mechanism for this problem in a model-free setting and provide sharp utility and privacy characterizations in the high-dimensional limit ($p\rightarrow\infty$). Our privacy result shows that, in high dimensions, detecting the presence of a target individual in the dataset using the privatized PCs is exactly as hard as distinguishing two Gaussians with slightly different means, where the mean difference depends on certain spectral properties of the dataset. Our privacy analysis combines the hypothesis-testing formulation of privacy guarantees proposed by Dong, Roth, and Su (2022) with classical contiguity arguments due to Le Cam to obtain sharp high-dimensional privacy characterizations.","%Our analysis of the  exponential mechanism (\Cref{alg:ExpM}) adopts the differential privacy framework and relies on various asymptotic  properties of the Gibbs distribution (\Cref{def:Gibbs}). 
The problem of differentially private PCA has been studied by many different works, which we discuss below. We will use the notations $\tilde{O}(\cdot)$ and $\tilde{\Omega}(\cdot)$ to suppress logarithmic factors while discussing prior work. 

\paragraph{Model-Free Differentially Private PCA.} Several works \citep{chaudhuri2013near, kapralov2013differentially, amin2019differentially, leake2021sampling, mangoubi2022re, dwork2014analyze, wei2016analysis, gonem2018smooth, hardt2013beyond, hardt2012beating, hardt2014noisy, tran2025spectral,d2025tight} have studied differentially private PCA in the model-free setting of this paper, obtaining non-asymptotic bounds on the privacy and utility of various mechanisms. Specifically, under (pure) $\epsilon$-DP \cite{dwork2006calibrating}, several mechanisms \citep{chaudhuri2013near,kapralov2013differentially,amin2019differentially,leake2021sampling,mangoubi2022re} give accurate approximations to the true PCs using $\ssize = \tilde{O}(\dim^2)$ samples, with a lower bound due to \citet{chaudhuri2013near} showing that this is optimal in the sense that any $\epsilon$-DP mechanism must use $\ssize = \tilde{\Omega}(\dim^2)$ samples to achieve low error (assuming a constant spectral gap). In contrast, under the more relaxed privacy notion of $(\epsilon,\delta)$-DP \citep{dwork2006our}, some works \citep{dwork2014analyze,gonem2018smooth,tran2025spectral} construct private approximations to PCs using $\ssize = \tilde{O}(\dim^{3/2})$ samples. This is the optimal sample complexity for $(\epsilon,\delta)$-DP mechanisms for sufficiently small $\delta$ \citep{dwork2014analyze}. Notably, our work also considers the $\ssize \asymp \dim^{3/2}$ regime, which is information-theoretically optimal for $(\epsilon,\delta)$-DP (up to logarithmic factors). However, some caution is needed since the privacy guarantees proved in our work correspond to asymptotic analogs of Gaussian \citep{dong2022gaussian} and Rényi Differential Privacy \citep{mironov2017renyi}, and it is not immediately clear whether existing lower bounds extend to these privacy notions.
% {\color{magenta}
% \begin{itemize}
%     \item We're unsure whether to mention \cite{he2025differentially} since it considers a different task.
% \end{itemize}}

\paragraph{Model-Based Differentially Private PCA.}   A different line of work \citep{cai2024optimal,liu2022dp,singhal2021privately,dungler2025iterative} has studied the problem of privatizing PCs in a model-based setting. Here, one assumes that the given dataset $X$ consists of $\ssize$ samples drawn i.i.d. from some distribution with population covariance matrix $\Sigma_\star$. The state-of-the-art results \citep{cai2024optimal,liu2022dp,dungler2025iterative} in the model-based setting construct consistent and $(\epsilon,\delta$)-DP estimators for the population PCs (top $\rnk$ eigenvectors of $\mSigma_\star$) using $\ssize = \tilde{O}(\dim)$ samples. This is the optimal sample complexity for $(\epsilon, \delta)$-DP algorithm for $\delta \leq e^{-\Omega(\dim)}$  \citep{cai2024optimal,liu2022dp}. Notice that the optimal sample complexity in the model-based setting is smaller than in the model-free setting. This difference can, in part, be attributed to the additional assumptions made on the dataset in the model-based setting.

\paragraph{High-dimensional Asymptotics in Differential Privacy.} More recently, \citet{dwork2024differentially} and \citet{bombari2025better} have studied the problem of differentially private regression in the high-dimensional asymptotic setting. In this problem, one observes feature vectors $\mX = \{x_1, \dotsc, x_\ssize\} \subset \R^\dim$ and responses $y = \{y_1, \dotsc, y_\ssize\} \subset \R$. The feature vectors are drawn independently from either a (possibly correlated) Gaussian distribution or a sub-Gaussian product distribution. The responses are generated via the linear model $y_i = \ip{x_i}{\theta_\star} + \epsilon_i$, where $\theta_\star$ is the unknown coefficient vector and $\epsilon_1,\dotsc, \epsilon_\ssize$ represent measurement noise.  Under these model assumptions, these works characterize the precise asymptotic estimation error for popular differentially private estimators of $\theta_\star$ in the proportional high-dimensional asymptotic setting $\dim \rightarrow \infty, \ssize/\dim \rightarrow \delta$ for some $\delta \in (0,\infty)$. The focus of these papers is primarily to obtain sharp characterizations of the \emph{estimation error}. The privacy guarantees in these works are worst-case non-asymptotic privacy \emph{bounds}. The focus of this paper is different and complementary. We were particularly interested in obtaining \emph{sharp privacy characterizations} that specify the minimal noise level required to achieve a target privacy level in the high-dimensional limit $\dim \rightarrow \infty$ (in addition to sharp utility characterizations). Moreover, both our privacy and utility results are model-free and apply to deterministic datasets. 

\paragraph{Spherical Integrals and Spin Glasses.} The Gibbs distribution in \eqref{eq:gibbs-intro} has also been studied in statistical physics and probability theory \cite{kosterlitz1976spherical,guionnet2005fourier,baik2016fluctuations, baik2017fluctuations, baik2021spherical,baik2018ferromagnetic,landon2020fluctuations,guionnet2021asymptotics,husson2025spherical}, where it is referred to as the spherical Sherrington–Kirkpatrick model \citep{kosterlitz1976spherical}, and its normalizing constant is called a spherical integral \citep{guionnet2005fourier}. In particular, our approach to analyzing this Gibbs distribution builds on the work of \citet{guionnet2005fourier}, and a subsequent refinement of their result by \citet{guionnet2021asymptotics} plays an important role in our proofs. A series of works \cite{hoff2009simulation,kume2006sampling,kapralov2013differentially,kent2018new,leake2021sampling,kent2004simulation} have also studied the problem of sampling from this Gibbs distribution. We have found that the Gibbs sampler developed by \citet{hoff2009simulation} works particularly well in practice, and we use this sampler for all our experimental results. Unfortunately, a mixing time bound for this sampler is not known. The problem of designing samplers for this Gibbs distribution with polynomial mixing time guarantees has also been investigated \citep{kapralov2013differentially,ge2021efficient,leake2021sampling}. Unfortunately, these results are not applicable in our setting, as they either focus on the $k=1$ case \citep{kapralov2013differentially,ge2021efficient} or study a complex analog of the Gibbs distribution \citep{leake2021sampling}, which is supported on $\mathbb{U}(\dim,\rnk)$, the set of complex $ p\times k$ column-orthogonal matrices.","The problem of differentially private PCA has been studied by many different works, which we discuss below. We will use the notations $\tilde{O}(\cdot)$ and $\tilde{\Omega}(\cdot)$ to suppress logarithmic factors while discussing prior work. 

\paragraph{Model-Free Differentially Private PCA.} Several works \citep{chaudhuri2013near, kapralov2013differentially, amin2019differentially, leake2021sampling, mangoubi2022re, dwork2014analyze, wei2016analysis, gonem2018smooth, hardt2013beyond, hardt2012beating, hardt2014noisy, tran2025spectral,d2025tight} have studied differentially private PCA in the model-free setting of this paper, obtaining non-asymptotic bounds on the privacy and utility of various mechanisms. Specifically, under (pure) $\epsilon$-DP \cite{dwork2006calibrating}, several mechanisms \citep{chaudhuri2013near,kapralov2013differentially,amin2019differentially,leake2021sampling,mangoubi2022re} give accurate approximations to the true PCs using $\ssize = \tilde{O}(\dim^2)$ samples, with a lower bound due to \citet{chaudhuri2013near} showing that this is optimal in the sense that any $\epsilon$-DP mechanism must use $\ssize = \tilde{\Omega}(\dim^2)$ samples to achieve low error (assuming a constant spectral gap). In contrast, under the more relaxed privacy notion of $(\epsilon,\delta)$-DP \citep{dwork2006our}, some works \citep{dwork2014analyze,gonem2018smooth,tran2025spectral} construct private approximations to PCs using $\ssize = \tilde{O}(\dim^{3/2})$ samples. This is the optimal sample complexity for $(\epsilon,\delta)$-DP mechanisms for sufficiently small $\delta$ \citep{dwork2014analyze}. Notably, our work also considers the $\ssize \asymp \dim^{3/2}$ regime, which is information-theoretically optimal for $(\epsilon,\delta)$-DP (up to logarithmic factors). However, some caution is needed since the privacy guarantees proved in our work correspond to asymptotic analogs of Gaussian \citep{dong2022gaussian} and Rényi Differential Privacy \citep{mironov2017renyi}, and it is not immediately clear whether existing lower bounds extend to these privacy notions.





\paragraph{Model-Based Differentially Private PCA.}   A different line of work \citep{cai2024optimal,liu2022dp,singhal2021privately,dungler2025iterative} has studied the problem of privatizing PCs in a model-based setting. Here, one assumes that the given dataset $X$ consists of $\ssize$ samples drawn i.i.d. from some distribution with population covariance matrix $\Sigma_\star$. The state-of-the-art results \citep{cai2024optimal,liu2022dp,dungler2025iterative} in the model-based setting construct consistent and $(\epsilon,\delta$)-DP estimators for the population PCs (top $\rnk$ eigenvectors of $\mSigma_\star$) using $\ssize = \tilde{O}(\dim)$ samples. This is the optimal sample complexity for $(\epsilon, \delta)$-DP algorithm for $\delta \leq e^{-\Omega(\dim)}$  \citep{cai2024optimal,liu2022dp}. Notice that the optimal sample complexity in the model-based setting is smaller than in the model-free setting. This difference can, in part, be attributed to the additional assumptions made on the dataset in the model-based setting.

\paragraph{High-dimensional Asymptotics in Differential Privacy.} More recently, \citet{dwork2024differentially} and \citet{bombari2025better} have studied the problem of differentially private regression in the high-dimensional asymptotic setting. In this problem, one observes feature vectors $\mX = \{x_1, \dotsc, x_\ssize\} \subset \R^\dim$ and responses $y = \{y_1, \dotsc, y_\ssize\} \subset \R$. The feature vectors are drawn independently from either a (possibly correlated) Gaussian distribution or a sub-Gaussian product distribution. The responses are generated via the linear model $y_i = \ip{x_i}{\theta_\star} + \epsilon_i$, where $\theta_\star$ is the unknown coefficient vector and $\epsilon_1,\dotsc, \epsilon_\ssize$ represent measurement noise.  Under these model assumptions, these works characterize the precise asymptotic estimation error for popular differentially private estimators of $\theta_\star$ in the proportional high-dimensional asymptotic setting $\dim \rightarrow \infty, \ssize/\dim \rightarrow \delta$ for some $\delta \in (0,\infty)$. The focus of these papers is primarily to obtain sharp characterizations of the \emph{estimation error}. The privacy guarantees in these works are worst-case non-asymptotic privacy \emph{bounds}. The focus of this paper is different and complementary. We were particularly interested in obtaining \emph{sharp privacy characterizations} that specify the minimal noise level required to achieve a target privacy level in the high-dimensional limit $\dim \rightarrow \infty$ (in addition to sharp utility characterizations). Moreover, both our privacy and utility results are model-free and apply to deterministic datasets. 

\paragraph{Spherical Integrals and Spin Glasses.} The Gibbs distribution in \eqref{eq:gibbs-intro} has also been studied in statistical physics and probability theory \cite{kosterlitz1976spherical,guionnet2005fourier,baik2016fluctuations, baik2017fluctuations, baik2021spherical,baik2018ferromagnetic,landon2020fluctuations,guionnet2021asymptotics,husson2025spherical}, where it is referred to as the spherical Sherrington–Kirkpatrick model \citep{kosterlitz1976spherical}, and its normalizing constant is called a spherical integral \citep{guionnet2005fourier}. In particular, our approach to analyzing this Gibbs distribution builds on the work of \citet{guionnet2005fourier}, and a subsequent refinement of their result by \citet{guionnet2021asymptotics} plays an important role in our proofs. A series of works \cite{hoff2009simulation,kume2006sampling,kapralov2013differentially,kent2018new,leake2021sampling,kent2004simulation} have also studied the problem of sampling from this Gibbs distribution. We have found that the Gibbs sampler developed by \citet{hoff2009simulation} works particularly well in practice, and we use this sampler for all our experimental results. Unfortunately, a mixing time bound for this sampler is not known. The problem of designing samplers for this Gibbs distribution with polynomial mixing time guarantees has also been investigated \citep{kapralov2013differentially,ge2021efficient,leake2021sampling}. Unfortunately, these results are not applicable in our setting, as they either focus on the $k=1$ case \citep{kapralov2013differentially,ge2021efficient} or study a complex analog of the Gibbs distribution \citep{leake2021sampling}, which is supported on $\mathbb{U}(\dim,\rnk)$, the set of complex $ p\times k$ column-orthogonal matrices.",
2511.06790v1,http://arxiv.org/abs/2511.06790v1,2025-11-10 07:27:08+00:00,Robust Causal Discovery under Imperfect Structural Constraints,"Robust causal discovery from observational data under imperfect prior knowledge remains a significant and largely unresolved challenge. Existing methods typically presuppose perfect priors or can only handle specific, pre-identified error types. And their performance degrades substantially when confronted with flawed constraints of unknown location and type. This decline arises because most of them rely on inflexible and biased thresholding strategies that may conflict with the data distribution. To overcome these limitations, we propose to harmonizes knowledge and data through prior alignment and conflict resolution. First, we assess the credibility of imperfect structural constraints through a surrogate model, which then guides a sparse penalization term measuring the loss between the learned and constrained adjacency matrices. We theoretically prove that, under ideal assumption, the knowledge-driven objective aligns with the data-driven objective. Furthermore, to resolve conflicts when this assumption is violated, we introduce a multi-task learning framework optimized via multi-gradient descent, jointly minimizing both objectives. Our proposed method is robust to both linear and nonlinear settings. Extensive experiments, conducted under diverse noise conditions and structural equation model types, demonstrate the effectiveness and efficiency of our method under imperfect structural constraints.","\paragraph{Causal discovery under structural constraints} 
For combinatorial-based methods, integrating edge constraints is relatively straightforward, typically by restricting the search space \cite{de2007bayesian, colombo2014order,constantinou2023impact}. However, path constraints, which are weaker and non-decomposable, need the graphical search space or specialized data structures to entail \cite{chen2016learning, wang2021learning, wang2025large}. A key limitation of these approaches is their reliance on the assumption that all provided constraints are perfect and error-free. For continuous-based approaches, perfect edge constraints are often handled in two ways: either enforced as hard constraints that are optimized simultaneously with the acyclicity constraint \cite{hasan2022kcrl,sun2023nts,wang2024incorporating}, or by directly modifying the gradients of the adjacency matrix to steer the search \cite{bello2022dagma}. Imperfect priors are typically handled via soft penalties, where constraints are formulated as differentiable terms, such as a cross-entropy loss measuring constraint violation \cite{li2024weakly, chen2025continuous}. To handle path constraints, this paradigm involves employing partial order-based optimization strategies \cite{bandifferentiable}.

More recently, a nascent line of work has explored using Large Language Models (LLMs) as a proxy for domain experts \cite{kiciman2023causal}. LLMs have been used to generate initial graphs \cite{ban2025integrating}, suggest post-hoc adjustments \cite{khatibi2024alcm}, or fuse structural priors from text \cite{zhou2024causalbench, ban2025llm}.

For a broader survey of general causal discovery methods, we refer the reader to Appendix A.

\paragraph{Multi-task Learning} MTL is quite a hot topic in the machine learning community \cite{zhang2021survey}. MTL can improve the generalization and reduce the cost of learned models, thus it is widely applied in many scenarios \cite{zhao2022inherent}. Key research in MTL involves designing shared architectures and managing conflicting task objectives \cite{lin2023libmtl}. Our work concentrates on the latter, employing multi-objective optimization (MOO) to mitigate the conflict between data-driven and knowledge-driven objectives for causal discovery.

MOO solvers can be broadly categorized into two families \cite{zhang2024libmoon}. The first, aggregation-based methods, transforms the multi-objective problem into a single-objective one by aggregating individual loss functions, such as Linear scalarization \cite{miettinen1999nonlinear}, the Tchebycheff method \cite{zhang2007moea}, Smooth TCH \cite{lin2024smooth}. The second family, gradient-manipulation-based methods, operates directly on the gradients of each task to find a descent direction that improves all objectives. Prominent examples include the MGDA \cite{sener2018multi}, its preference-based extensions \cite{lin2019pareto}, and normalization version \cite{chen2018gradnorm}.","\paragraph{Causal discovery under structural constraints} 
For combinatorial-based methods, integrating edge constraints is relatively straightforward, typically by restricting the search space \cite{de2007bayesian, colombo2014order,constantinou2023impact}. However, path constraints, which are weaker and non-decomposable, need the graphical search space or specialized data structures to entail \cite{chen2016learning, wang2021learning, wang2025large}. A key limitation of these approaches is their reliance on the assumption that all provided constraints are perfect and error-free. For continuous-based approaches, perfect edge constraints are often handled in two ways: either enforced as hard constraints that are optimized simultaneously with the acyclicity constraint \cite{hasan2022kcrl,sun2023nts,wang2024incorporating}, or by directly modifying the gradients of the adjacency matrix to steer the search \cite{bello2022dagma}. Imperfect priors are typically handled via soft penalties, where constraints are formulated as differentiable terms, such as a cross-entropy loss measuring constraint violation \cite{li2024weakly, chen2025continuous}. To handle path constraints, this paradigm involves employing partial order-based optimization strategies \cite{bandifferentiable}.

More recently, a nascent line of work has explored using Large Language Models (LLMs) as a proxy for domain experts \cite{kiciman2023causal}. LLMs have been used to generate initial graphs \cite{ban2025integrating}, suggest post-hoc adjustments \cite{khatibi2024alcm}, or fuse structural priors from text \cite{zhou2024causalbench, ban2025llm}.

For a broader survey of general causal discovery methods, we refer the reader to Appendix A.

\paragraph{Multi-task Learning} MTL is quite a hot topic in the machine learning community \cite{zhang2021survey}. MTL can improve the generalization and reduce the cost of learned models, thus it is widely applied in many scenarios \cite{zhao2022inherent}. Key research in MTL involves designing shared architectures and managing conflicting task objectives \cite{lin2023libmtl}. Our work concentrates on the latter, employing multi-objective optimization (MOO) to mitigate the conflict between data-driven and knowledge-driven objectives for causal discovery.

MOO solvers can be broadly categorized into two families \cite{zhang2024libmoon}. The first, aggregation-based methods, transforms the multi-objective problem into a single-objective one by aggregating individual loss functions, such as Linear scalarization \cite{miettinen1999nonlinear}, the Tchebycheff method \cite{zhang2007moea}, Smooth TCH \cite{lin2024smooth}. The second family, gradient-manipulation-based methods, operates directly on the gradients of each task to find a descent direction that improves all objectives. Prominent examples include the MGDA \cite{sener2018multi}, its preference-based extensions \cite{lin2019pareto}, and normalization version \cite{chen2018gradnorm}.",
2511.04361v1,http://arxiv.org/abs/2511.04361v1,2025-11-06 13:45:15+00:00,Causal Regime Detection in Energy Markets With Augmented Time Series Structural Causal Models,"Energy markets exhibit complex causal relationships between weather patterns, generation technologies, and price formation, with regime changes occurring continuously rather than at discrete break points. Current approaches model electricity prices without explicit causal interpretation or counterfactual reasoning capabilities. We introduce Augmented Time Series Causal Models (ATSCM) for energy markets, extending counterfactual reasoning frameworks to multivariate temporal data with learned causal structure. Our approach models energy systems through interpretable factors (weather, generation mix, demand patterns), rich grid dynamics, and observable market variables. We integrate neural causal discovery to learn time-varying causal graphs without requiring ground truth DAGs. Applied to real-world electricity price data, ATSCM enables novel counterfactual queries such as ""What would prices be under different renewable generation scenarios?"".","\textbf{Causal Inference in Financial Markets.} Causal data science has been applied to financial stress testing and risk management \cite{gao2018causal, rigana2024navigating}. However, energy markets present unique challenges due to physical constraints, storage limitations, and renewable intermittency that require specialized treatment.

\textbf{Electricity Price Forecasting.} Traditional approaches focus on statistical methods (ARIMA, VAR) or machine learning models (LSTM, gradient boosting) for price prediction \cite{lago2021forecasting, ziel2018day}. While achieving good predictive performance, these methods lack explicit causal interpretation and cannot answer counterfactual queries.

\textbf{Counterfactual Reasoning.} ASCM frameworks \cite{pan2024counterfactual, pan2025counterfactual} enable counterfactual reasoning in high-dimensional spaces but assume static causal structures. Time series causal methods like TNCM-VAE \cite{thumm2025towards},  SCIGAN \cite{bica2020estimatingthe} and Causal Transformer \cite{melnychuk2022causal} address temporal dynamics but do not handle regime changes in learned causal graphs.","\textbf{Causal Inference in Financial Markets.} Causal data science has been applied to financial stress testing and risk management \cite{gao2018causal, rigana2024navigating}. However, energy markets present unique challenges due to physical constraints, storage limitations, and renewable intermittency that require specialized treatment.

\textbf{Electricity Price Forecasting.} Traditional approaches focus on statistical methods (ARIMA, VAR) or machine learning models (LSTM, gradient boosting) for price prediction \cite{lago2021forecasting, ziel2018day}. While achieving good predictive performance, these methods lack explicit causal interpretation and cannot answer counterfactual queries.

\textbf{Counterfactual Reasoning.} ASCM frameworks \cite{pan2024counterfactual, pan2025counterfactual} enable counterfactual reasoning in high-dimensional spaces but assume static causal structures. Time series causal methods like TNCM-VAE \cite{thumm2025towards},  SCIGAN \cite{bica2020estimatingthe} and Causal Transformer \cite{melnychuk2022causal} address temporal dynamics but do not handle regime changes in learned causal graphs.",
2511.03749v1,http://arxiv.org/abs/2511.03749v1,2025-11-04 11:43:52+00:00,Applying Time Series Deep Learning Models to Forecast the Growth of Perennial Ryegrass in Ireland,"Grasslands, constituting the world's second-largest terrestrial carbon sink, play a crucial role in biodiversity and the regulation of the carbon cycle. Currently, the Irish dairy sector, a significant economic contributor, grapples with challenges related to profitability and sustainability. Presently, grass growth forecasting relies on impractical mechanistic models. In response, we propose deep learning models tailored for univariate datasets, presenting cost-effective alternatives. Notably, a temporal convolutional network designed for forecasting Perennial Ryegrass growth in Cork exhibits high performance, leveraging historical grass height data with RMSE of 2.74 and MAE of 3.46. Validation across a comprehensive dataset spanning 1,757 weeks over 34 years provides insights into optimal model configurations. This study enhances our understanding of model behavior, thereby improving reliability in grass growth forecasting and contributing to the advancement of sustainable dairy farming practices.","\label{sec:relatedwork}

Several studies have leveraged information technologies in agriculture to enhance crop yield, as exemplified by \cite{Ngo:2021}, \cite{Ngo:2023}, and \cite{Benedict:2023}. In \cite{Ngo:2021}, the authors introduced an Electronic Farming Record (EFR) and utilized agricultural Big Data analytics to determine optimal quantities of various factors, including soil properties (texture and pH), soil nutrients, seed rates, herbicides, insecticides, fungicides, and adjuvants for the 12 most popular crops in Europe. Meanwhile, \cite{Ngo:2023} employed data warehousing and statistical techniques \citep{ngo2019designing} to propose recommended quantities of fertilizer components (such as nitrogen, phosphorus, and potassium) for Barley, Dried Beans, Linseed, Rye, and Wheat, considering a broad spectrum of environmental and crop management conditions. In \cite{Benedict:2023}, the authors studied exotic annual grass (EAG) in western U.S. rangeland during the active growing season. They used a normalized difference vegetation index (NDVI) threshold-based interpolation technique to understand the links between weather conditions, EAG phenology, and the potential impact of weed grass competition on crop yield and quality. However, these papers did not employ time series models, DL models, or provide forecasts for grass yield.


Several papers have utilized machine learning (ML)/DL models for identifying information related to grass various countries, such as \cite{Sapkota:2020}, \cite{Holtgrave:2023}, and \cite{Defalque:2024}. In the study by \cite{Sapkota:2020}, the authors implemented a deep neural network based on an unmanned aerial systems-based remote sensing approach, incorporating color-transformed features and vegetation indices. This amalgamation significantly improved the detection and mapping of Italian ryegrass in wheat. In the study by \citep{Holtgrave:2023}, the authors applied Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) models, utilizing optical, synthetic aperture radar, and weather time series data. Their objective was to identify grassland mowing events in Germany. In the study conducted by \citep{Defalque:2024}, ML models, namely the Xtreme Gradient Boosting Regressor and Support Vector Regressor, were formulated and implemented. These models considered a range of factors, encompassing cattle parameters, environmental conditions, and spectral data within the Brazilian study area. The primary aim of these models was to estimate biomass and dry matter in grazing systems. However, these three papers did not use time series ML/DL models nor made forecasts regarding grass height.


Some papers applied ML/DL algorithms to forecast grass growth/height, such as \cite{KRGS19}, \cite{MBLJ20} and \cite{PBADGQ21}. In the study by \cite{KRGS19}, a case-based reasoning system was employed for predicting grass growth, integrating a Bayesian model to account for climatic variability and data uncertainty. The methodology included employing a gold standard dataset as a model to eliminate noise from the working dataset. Cases were generated using features such as the average growth rate since the previous grass cover, the recorded week, month, and season. Additionally, the study incorporated weather data, encompassing maximum temperature, average soil temperature, and average global radiation. In the work by \cite{MBLJ20}, the authors introduced a multi-layered mapping methodological framework aimed at addressing challenges in precision agriculture. This framework serves to enhance transparency and explainability in decision support systems. The paper conducted a preliminary exploratory statistical case study analysis on grass growth data, specifically focusing on Northern Ireland. The analysis aimed to unveil patterns and identify the key factors influencing grass growth in the region. In the study by \cite{PBADGQ21}, the authors utilized ML algorithms to improve the accuracy of predicting dry matter yield for perennial ryegrass. They applied Partial Least Squares Regression, Random Forest, and Support Vector Machines to 468 field plots, incorporating both structural data from a consumer-grade RGB camera and spectral information from a Multispectral camera system. The results revealed that the optimal performance was achieved through the combination of Multispectral information and the Random Forest algorithm. However, these three papers did not employ time series models for forecasting. 















%%%%%%%%%%%%%%%%%%%%%%%%%
There are some papers applied time series techniques in agriculture, such as \cite{Yoo:2020},  \cite{Yuan:2023} and \cite{Quan:2023}. The authors in \cite{Yoo:2020} introduced a LSTM model designed to forecast the sales of agricultural products, aiming to stabilize supply and demand. The model incorporated seasonality attributes such as week, month, and quarter as additional inputs to historical time-series data. The evaluation focused on 3000 items, including crops like Onion, Lettuce, Mallow, and Tomato. These sales records were collected from the point-of-sale system of a local food retail store in Wanju, South Korea, spanning the period from June 2014 to December 2019. In \cite{Yuan:2023}, the authors  introduced contrastive learning as a methodology for extracting unified representations in the context of crop classification, utilizing both optical and synthetic aperture radar satellite image time series. They innovatively devised an enhanced feature-level fusion network with selectively shared weights among branches, aiming to mitigate model complexity. In \cite{Quan:2023}, the authors utilized multi-temporal remote sensing images that integrated both spectral and lidar data. The dataset employed in their research encompasses five distinct periods of maize growth in Harbin, China, during the year 2021. Employing sophisticated deep learning techniques, they conducted a spatial assessment of weed competitiveness across various stages of maize growth. The paper presented the dynamic responses of spectral and lidar information during prolonged weed competition. However, the study's emphasis was on image processing and the time series data covered only a one-year period. However, \cite{Yoo:2020} studied non-grass agricultural products using a dataset spanning only approximately five years. While, \cite{Yuan:2023} worked on image processing and classification. In \cite{Quan:2023}, the study's emphasized image processing, with the time series data covering only a one-year period.






%From this review of existing literature, previous attempts at forecasting grass growth require the collection of multiple causal variables such as climatic conditions and soil type, an exercise that may not be feasible at scale in any more than a handful of farms. Therefore, the development of methods capable of generating reliable grass growth forecasts with minimal human effort is of significant importance.

%---------------------------------------------","Several studies have leveraged information technologies in agriculture to enhance crop yield, as exemplified by \cite{Ngo:2021}, \cite{Ngo:2023}, and \cite{Benedict:2023}. In \cite{Ngo:2021}, the authors introduced an Electronic Farming Record (EFR) and utilized agricultural Big Data analytics to determine optimal quantities of various factors, including soil properties (texture and pH), soil nutrients, seed rates, herbicides, insecticides, fungicides, and adjuvants for the 12 most popular crops in Europe. Meanwhile, \cite{Ngo:2023} employed data warehousing and statistical techniques \citep{ngo2019designing} to propose recommended quantities of fertilizer components (such as nitrogen, phosphorus, and potassium) for Barley, Dried Beans, Linseed, Rye, and Wheat, considering a broad spectrum of environmental and crop management conditions. In \cite{Benedict:2023}, the authors studied exotic annual grass (EAG) in western U.S. rangeland during the active growing season. They used a normalized difference vegetation index (NDVI) threshold-based interpolation technique to understand the links between weather conditions, EAG phenology, and the potential impact of weed grass competition on crop yield and quality. However, these papers did not employ time series models, DL models, or provide forecasts for grass yield.


Several papers have utilized machine learning (ML)/DL models for identifying information related to grass various countries, such as \cite{Sapkota:2020}, \cite{Holtgrave:2023}, and \cite{Defalque:2024}. In the study by \cite{Sapkota:2020}, the authors implemented a deep neural network based on an unmanned aerial systems-based remote sensing approach, incorporating color-transformed features and vegetation indices. This amalgamation significantly improved the detection and mapping of Italian ryegrass in wheat. In the study by \citep{Holtgrave:2023}, the authors applied Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) models, utilizing optical, synthetic aperture radar, and weather time series data. Their objective was to identify grassland mowing events in Germany. In the study conducted by \citep{Defalque:2024}, ML models, namely the Xtreme Gradient Boosting Regressor and Support Vector Regressor, were formulated and implemented. These models considered a range of factors, encompassing cattle parameters, environmental conditions, and spectral data within the Brazilian study area. The primary aim of these models was to estimate biomass and dry matter in grazing systems. However, these three papers did not use time series ML/DL models nor made forecasts regarding grass height.


Some papers applied ML/DL algorithms to forecast grass growth/height, such as \cite{KRGS19}, \cite{MBLJ20} and \cite{PBADGQ21}. In the study by \cite{KRGS19}, a case-based reasoning system was employed for predicting grass growth, integrating a Bayesian model to account for climatic variability and data uncertainty. The methodology included employing a gold standard dataset as a model to eliminate noise from the working dataset. Cases were generated using features such as the average growth rate since the previous grass cover, the recorded week, month, and season. Additionally, the study incorporated weather data, encompassing maximum temperature, average soil temperature, and average global radiation. In the work by \cite{MBLJ20}, the authors introduced a multi-layered mapping methodological framework aimed at addressing challenges in precision agriculture. This framework serves to enhance transparency and explainability in decision support systems. The paper conducted a preliminary exploratory statistical case study analysis on grass growth data, specifically focusing on Northern Ireland. The analysis aimed to unveil patterns and identify the key factors influencing grass growth in the region. In the study by \cite{PBADGQ21}, the authors utilized ML algorithms to improve the accuracy of predicting dry matter yield for perennial ryegrass. They applied Partial Least Squares Regression, Random Forest, and Support Vector Machines to 468 field plots, incorporating both structural data from a consumer-grade RGB camera and spectral information from a Multispectral camera system. The results revealed that the optimal performance was achieved through the combination of Multispectral information and the Random Forest algorithm. However, these three papers did not employ time series models for forecasting. 
















There are some papers applied time series techniques in agriculture, such as \cite{Yoo:2020},  \cite{Yuan:2023} and \cite{Quan:2023}. The authors in \cite{Yoo:2020} introduced a LSTM model designed to forecast the sales of agricultural products, aiming to stabilize supply and demand. The model incorporated seasonality attributes such as week, month, and quarter as additional inputs to historical time-series data. The evaluation focused on 3000 items, including crops like Onion, Lettuce, Mallow, and Tomato. These sales records were collected from the point-of-sale system of a local food retail store in Wanju, South Korea, spanning the period from June 2014 to December 2019. In \cite{Yuan:2023}, the authors  introduced contrastive learning as a methodology for extracting unified representations in the context of crop classification, utilizing both optical and synthetic aperture radar satellite image time series. They innovatively devised an enhanced feature-level fusion network with selectively shared weights among branches, aiming to mitigate model complexity. In \cite{Quan:2023}, the authors utilized multi-temporal remote sensing images that integrated both spectral and lidar data. The dataset employed in their research encompasses five distinct periods of maize growth in Harbin, China, during the year 2021. Employing sophisticated deep learning techniques, they conducted a spatial assessment of weed competitiveness across various stages of maize growth. The paper presented the dynamic responses of spectral and lidar information during prolonged weed competition. However, the study's emphasis was on image processing and the time series data covered only a one-year period. However, \cite{Yoo:2020} studied non-grass agricultural products using a dataset spanning only approximately five years. While, \cite{Yuan:2023} worked on image processing and classification. In \cite{Quan:2023}, the study's emphasized image processing, with the time series data covering only a one-year period.",
2511.09577v1,http://arxiv.org/abs/2511.09577v1,2025-11-12 07:47:46+00:00,Siegel Neural Networks,"Riemannian symmetric spaces (RSS) such as hyperbolic spaces and symmetric positive definite (SPD) manifolds have become popular spaces for representation learning. In this paper, we propose a novel approach for building discriminative neural networks on Siegel spaces, a family of RSS that is largely unexplored in machine learning tasks. For classification applications, one focus of recent works is the construction of multiclass logistic regression (MLR) and fully-connected (FC) layers for hyperbolic and SPD neural networks. Here we show how to build such layers for Siegel neural networks. Our approach relies on the quotient structure of those spaces and the notation of vector-valued distance on RSS. We demonstrate the relevance of our approach on two applications, i.e., radar clutter classification and node classification. Our results successfully demonstrate state-of-the-art performance across all datasets.","\label{sec:related_work}

Existing MLR models on Riemannian manifolds are generally built on either SPD manifolds~\cite{chen2024riemannian,NguyenGyroMatMans23} and their low-rank counterparts~\cite{NguyenICLR24} or 
hyperbolic spaces~\cite{BdeirFullyHnnCv24,NEURIPS2018_dbab2adc,LebanonMarginClassifierICML04,shimizu2021hyperbolic}. 
Many of them~\cite{NEURIPS2018_dbab2adc,NguyenGyroMatMans23,NguyenICLR24,shimizu2021hyperbolic} leverage the gyro-structures of the Poincar\'e ball and SPD manifolds. 
The work in~\cite{NguyenICLR25} proposes MLR and FC layers for neural network on RSS %which are applicable to Siegel spaces. 
which rely on the construction of Busemann functions. 
The work in~\cite{Sonoda2022FCRidgele} analyzes some existing hyperbolic and SPD neural networks from the perspective of harmonic analysis on RSS. It mainly concerns with a constructive proof of the universal approximation property of finite neural networks on RSS. 
Our method in Section~\ref{sec:hyperplanes_quotient_structure} is inspired by the works 
in~\cite{NEURIPS2018_dbab2adc,NguyenGyroMatMans23,NguyenICLR24} and focuses on Siegel spaces. 
Our method in Sections~\ref{sec:hyperplanes_vvd} explores the connection between the point-to-hyperplane distance and the vector-valued distance which has not been investigated in previous works. 

%-----------------------------------------------------------------------------------------------------------------------------------","Existing MLR models on Riemannian manifolds are generally built on either SPD manifolds~\cite{chen2024riemannian,NguyenGyroMatMans23} and their low-rank counterparts~\cite{NguyenICLR24} or 
hyperbolic spaces~\cite{BdeirFullyHnnCv24,NEURIPS2018_dbab2adc,LebanonMarginClassifierICML04,shimizu2021hyperbolic}. 
Many of them~\cite{NEURIPS2018_dbab2adc,NguyenGyroMatMans23,NguyenICLR24,shimizu2021hyperbolic} leverage the gyro-structures of the Poincar\'e ball and SPD manifolds. 
The work in~\cite{NguyenICLR25} proposes MLR and FC layers for neural network on RSS 
which rely on the construction of Busemann functions. 
The work in~\cite{Sonoda2022FCRidgele} analyzes some existing hyperbolic and SPD neural networks from the perspective of harmonic analysis on RSS. It mainly concerns with a constructive proof of the universal approximation property of finite neural networks on RSS. 
Our method in Section~\ref{sec:hyperplanes_quotient_structure} is inspired by the works 
in~\cite{NEURIPS2018_dbab2adc,NguyenGyroMatMans23,NguyenICLR24} and focuses on Siegel spaces. 
Our method in Sections~\ref{sec:hyperplanes_vvd} explores the connection between the point-to-hyperplane distance and the vector-valued distance which has not been investigated in previous works.",
2511.05640v1,http://arxiv.org/abs/2511.05640v1,2025-11-07 16:27:59+00:00,Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games,"Inverse Game Theory (IGT) methods based on the entropy-regularized Quantal Response Equilibrium (QRE) offer a tractable approach for competitive settings, but critically assume the agents' rationality parameter (temperature $τ$) is known a priori. When $τ$ is unknown, a fundamental scale ambiguity emerges that couples $τ$ with the reward parameters ($θ$), making them statistically unidentifiable. We introduce Blind-IGT, the first statistical framework to jointly recover both $θ$ and $τ$ from observed behavior. We analyze this bilinear inverse problem and establish necessary and sufficient conditions for unique identification by introducing a normalization constraint that resolves the scale ambiguity. We propose an efficient Normalized Least Squares (NLS) estimator and prove it achieves the optimal $\mathcal{O}(N^{-1/2})$ convergence rate for joint parameter recovery. When strong identifiability conditions fail, we provide partial identification guarantees through confidence set construction. We extend our framework to Markov games and demonstrate optimal convergence rates with strong empirical performance even when transition dynamics are unknown.","Our work lies at the intersection of inverse optimization, inverse reinforcement learning, and the statistical analysis of entropy-regularized games.

\textbf{Inverse Optimization (IO) and Inverse Reinforcement Learning (IRL).} IO aims to infer objective functions from observed decisions~\citep{Ahuja&Orlin2001, Chan2022}. IRL specifically focuses on recovering reward functions in sequential decision-making~\citep{Ng&Russell2000}. A prominent approach is Maximum Entropy IRL (MaxEnt-IRL)~\citep{Ziebart2008, Wulfmeier2016, Finn2016}.

Recent theoretical work has analyzed reward identifiability in entropy-regularized settings. \citet{Cao2021} and \citet{Rolland2022} demonstrated improved identifiability in single-agent scenarios. Crucially, this prior work addresses an \textit{additive} ambiguity inherent in the Bellman equation (potential shaping). In contrast, our work tackles a \textit{multiplicative} scale ambiguity arising from the unknown temperature in a competitive setting. These represent distinct mathematical challenges.

\textbf{Inverse Game Theory (IGT).} IGT extends IRL to multi-agent systems, aiming to recover the underlying payoff structures that rationalize observed strategic interactions~\citep{Vorobeychik2007, Arora&Doshi2021}. The predominant approach in the literature assumes that the observed behavior corresponds to a standard equilibrium concept, typically the Nash Equilibrium (NE)~\citep{Lin2014, Kuleshov&Schrijvers2015} or the Correlated Equilibrium (CE)~\citep{Waugh2011}. However, these approaches often struggle with non-uniqueness and computational intractability, frequently relying on complex bilevel optimization as discussed in existing work \citep{Wang&Klabjan2018, Wu2022, Konstantakopoulos2017}.

\textbf{Entropy Regularization and QRE.} The Quantal Response Equilibrium (QRE)~\citep{McKelvey&Palfrey1995} models agents playing noisy best responses. Unlike the idealized Nash Equilibrium (NE), QRE provides smooth, unique equilibrium predictions, which facilitates tractable analysis~\citep{Mertikopoulos&Sandholm2016} and often better fits empirical data from human behavior~\citep{Goeree2016, Camerer2003}. The temperature parameter $\tau$ explicitly controls the degree of bounded rationality, allowing QRE to interpolate between uniform random play (as $\tau \to \infty$) and the NE (as $\tau \to 0$). This connection makes QRE a central concept in modern multi-agent reinforcement learning (MARL), where entropy regularization is widely employed to stabilize training and encourage exploration~\citep{Haarnoja2018, Cen2021, Guan2021, Ahmed2019, Zhan2023}.

\paragraph{Relation to prior QRE-based IGT.}
The work most closely related to ours is \citet{Liao2025}, which established a framework for IGT in entropy-regularized zero-sum games based on the linearized QRE constraints we utilize. Our work directly addresses the fundamental limitation of their analysis (the assumption of a known $\tau$) by developing a ""blind"" framework. We provide a methodology (NLS) for unique point identification of both rewards and the unknown temperature via normalization. Furthermore, we extend the partial identification analysis of \citet{Liao2025} to the bilinear setting where $\tau$ is unknown.

\textbf{Bilinear Inverse Problems.} The structure of Blind-IGT is a bilinear inverse problem, where we seek two vectors whose product explains the observations. Such problems arise in areas like blind deconvolution \citep{Ahmed2014} and self-calibration~\citep{Ling&Strohmer2015}. They are generally non-convex. Our approach leverages the specific structure of the QRE constraints and the normalization constraint to derive an efficient and provably correct estimation method.","Our work lies at the intersection of inverse optimization, inverse reinforcement learning, and the statistical analysis of entropy-regularized games.

\textbf{Inverse Optimization (IO) and Inverse Reinforcement Learning (IRL).} IO aims to infer objective functions from observed decisions~\citep{Ahuja&Orlin2001, Chan2022}. IRL specifically focuses on recovering reward functions in sequential decision-making~\citep{Ng&Russell2000}. A prominent approach is Maximum Entropy IRL (MaxEnt-IRL)~\citep{Ziebart2008, Wulfmeier2016, Finn2016}.

Recent theoretical work has analyzed reward identifiability in entropy-regularized settings. \citet{Cao2021} and \citet{Rolland2022} demonstrated improved identifiability in single-agent scenarios. Crucially, this prior work addresses an \textit{additive} ambiguity inherent in the Bellman equation (potential shaping). In contrast, our work tackles a \textit{multiplicative} scale ambiguity arising from the unknown temperature in a competitive setting. These represent distinct mathematical challenges.

\textbf{Inverse Game Theory (IGT).} IGT extends IRL to multi-agent systems, aiming to recover the underlying payoff structures that rationalize observed strategic interactions~\citep{Vorobeychik2007, Arora&Doshi2021}. The predominant approach in the literature assumes that the observed behavior corresponds to a standard equilibrium concept, typically the Nash Equilibrium (NE)~\citep{Lin2014, Kuleshov&Schrijvers2015} or the Correlated Equilibrium (CE)~\citep{Waugh2011}. However, these approaches often struggle with non-uniqueness and computational intractability, frequently relying on complex bilevel optimization as discussed in existing work \citep{Wang&Klabjan2018, Wu2022, Konstantakopoulos2017}.

\textbf{Entropy Regularization and QRE.} The Quantal Response Equilibrium (QRE)~\citep{McKelvey&Palfrey1995} models agents playing noisy best responses. Unlike the idealized Nash Equilibrium (NE), QRE provides smooth, unique equilibrium predictions, which facilitates tractable analysis~\citep{Mertikopoulos&Sandholm2016} and often better fits empirical data from human behavior~\citep{Goeree2016, Camerer2003}. The temperature parameter $\tau$ explicitly controls the degree of bounded rationality, allowing QRE to interpolate between uniform random play (as $\tau \to \infty$) and the NE (as $\tau \to 0$). This connection makes QRE a central concept in modern multi-agent reinforcement learning (MARL), where entropy regularization is widely employed to stabilize training and encourage exploration~\citep{Haarnoja2018, Cen2021, Guan2021, Ahmed2019, Zhan2023}.

\paragraph{Relation to prior QRE-based IGT.}
The work most closely related to ours is \citet{Liao2025}, which established a framework for IGT in entropy-regularized zero-sum games based on the linearized QRE constraints we utilize. Our work directly addresses the fundamental limitation of their analysis (the assumption of a known $\tau$) by developing a ""blind"" framework. We provide a methodology (NLS) for unique point identification of both rewards and the unknown temperature via normalization. Furthermore, we extend the partial identification analysis of \citet{Liao2025} to the bilinear setting where $\tau$ is unknown.

\textbf{Bilinear Inverse Problems.} The structure of Blind-IGT is a bilinear inverse problem, where we seek two vectors whose product explains the observations. Such problems arise in areas like blind deconvolution \citep{Ahmed2014} and self-calibration~\citep{Ling&Strohmer2015}. They are generally non-convex. Our approach leverages the specific structure of the QRE constraints and the normalization constraint to derive an efficient and provably correct estimation method.",
