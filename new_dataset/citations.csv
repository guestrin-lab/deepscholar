parent_paper_title,parent_paper_arxiv_id,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,has_metadata,is_arxiv_paper,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal
ABConformer: Physics-inspired Sliding Attention for Antibody-Antigen Interface Prediction,2509.23254v1,PECAN,\cite{PECAN},Learning context-aware structural representations to predict antigen and antibody binding interfaces,,,True,False,"Pittala, Srivamshi and Bailey-Kellogg, Chris",2020,,,,Bioinformatics
ABConformer: Physics-inspired Sliding Attention for Antibody-Antigen Interface Prediction,2509.23254v1,Honda,\cite{Honda},Cross attentive antibody-antigen interaction prediction with multi-task learning,,,True,False,"Honda, Shion and Koyama, Kyohei and Kotaro, Kamiya",2020,,,,
ABConformer: Physics-inspired Sliding Attention for Antibody-Antigen Interface Prediction,2509.23254v1,EPMP,\cite{EPMP},Neural message passing for joint paratope-epitope prediction,,,True,False,"Del Vecchio, Alice and Deac, Andreea and Li{\`o}, Pietro and Veli{\v{c}}kovi{\'c}, Petar",2021,,,,arXiv preprint arXiv:2106.00757
ABConformer: Physics-inspired Sliding Attention for Antibody-Antigen Interface Prediction,2509.23254v1,PeSTo,\cite{PeSTo},PeSTo: parameter-free geometric deep learning for accurate prediction of protein binding interfaces,,,True,False,"Krapp, Lucien F and Abriata, Luciano A and Cort{\'e}s Rodriguez, Fabio and Dal Peraro, Matteo",2023,,,,Nature communications
ABConformer: Physics-inspired Sliding Attention for Antibody-Antigen Interface Prediction,2509.23254v1,MIPE,\cite{MIPE},Improving paratope and epitope prediction by multi-modal contrastive learning and interaction informativeness estimation,,,True,False,"Wang, Zhiwei and Wang, Yongkang and Zhang, Wen",2024,,,,arXiv preprint arXiv:2405.20668
ABConformer: Physics-inspired Sliding Attention for Antibody-Antigen Interface Prediction,2509.23254v1,AbLang,\cite{AbLang},AbLang: an antibody language model for completing antibody sequences,,,True,False,"Olsen, Tobias H and Moal, Iain H and Deane, Charlotte M",2022,,,,Bioinformatics Advances
ABConformer: Physics-inspired Sliding Attention for Antibody-Antigen Interface Prediction,2509.23254v1,DeepInterAware,\cite{DeepInterAware},DeepInterAware: Deep Interaction Interface-Aware Network for Improving Antigen-Antibody Interaction Prediction from Sequence Data,,,True,False,"Xia, Yuhang and Wang, Zhiwei and Huang, Feng and Xiong, Zhankun and Wang, Yongkang and Qiu, Minyao and Zhang, Wen",2025,,,,Advanced Science
ABConformer: Physics-inspired Sliding Attention for Antibody-Antigen Interface Prediction,2509.23254v1,AntiBERTy,\cite{AntiBERTy},Deciphering antibody affinity maturation with language models and weakly supervised learning,,,True,False,"Ruffolo, Jeffrey A and Gray, Jeffrey J and Sulam, Jeremias",2021,,,,arXiv preprint arXiv:2112.07782
ABConformer: Physics-inspired Sliding Attention for Antibody-Antigen Interface Prediction,2509.23254v1,Epi4Ab,\cite{Epi4Ab},Epi4Ab: a data-driven prediction model of conformational epitopes for specific antibody VH/VL families and CDRs sequences,,,True,False,"Tran, Nhan Dinh and Subramani, Krithika and Su, Chinh Tran-To",2025,,,,
ABConformer: Physics-inspired Sliding Attention for Antibody-Antigen Interface Prediction,2509.23254v1,BepiPred-3.0,\cite{BepiPred-3.0},BepiPred-3.0: Improved B-cell epitope prediction using protein language models,,,True,False,"Clifford, Joakim N{\o}ddeskov and H{\o}ie, Magnus Haraldson and Deleuran, Sebastian and Peters, Bjoern and Nielsen, Morten and Marcatili, Paolo",2022,,,,Protein Science
ABConformer: Physics-inspired Sliding Attention for Antibody-Antigen Interface Prediction,2509.23254v1,ESM-IF1,\cite{ESM-IF1},Learning inverse folding from millions of predicted structures,,,True,False,"Hsu, Chloe and Verkuil, Robert and Liu, Jason and Lin, Zeming and Hie, Brian and Sercu, Tom and Lerer, Adam and Rives, Alexander",2022,,,,
ABConformer: Physics-inspired Sliding Attention for Antibody-Antigen Interface Prediction,2509.23254v1,DiscoTope-3.0,\cite{DiscoTope-3.0},DiscoTope-3.0: improved B-cell epitope prediction using inverse folding latent representations,,,True,False,"H{\o}ie, Magnus Haraldson and Gade, Frederik Steensgaard and Johansen, Julie Maria and W{\""u}rtzen, Charlotte and Winther, Ole and Nielsen, Morten and Marcatili, Paolo",2024,,,,Frontiers in immunology
ABConformer: Physics-inspired Sliding Attention for Antibody-Antigen Interface Prediction,2509.23254v1,SaProt,\cite{SaProt},Saprot: Protein language modeling with structure-aware vocabulary,,,True,False,"Su, Jin and Han, Chenchen and Zhou, Yuyang and Shan, Junjie and Zhou, Xibin and Yuan, Fajie",2023,,,,BioRxiv
ABConformer: Physics-inspired Sliding Attention for Antibody-Antigen Interface Prediction,2509.23254v1,SEMA-2.0,\cite{SEMA-2.0},SEMA 2.0: web-platform for B-cell conformational epitopes prediction using artificial intelligence,,,True,False,"Ivanisenko, Nikita V and Shashkova, Tatiana I and Shevtsov, Andrey and Sindeeva, Maria and Umerenkov, Dmitriy and Kardymon, Olga",2024,,,,Nucleic Acids Research
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,haase2022hitchhiker,\cite{haase2022hitchhiker},A hitchhiker's guide through the bio-image analysis software universe,,,True,False,"Haase, Robert and Fazeli, Elnaz and Legland, David and Doube, Michael and Culley, Si{\^a}n and Belevich, Ilya and Jokitalo, Eija and Schorb, Martin and Klemm, Anna and Tischer, Christian",2022,,,,FEBS Letters
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,schneider2012nih,\cite{schneider2012nih},{NIH Image to ImageJ}: 25 years of image analysis,,,True,False,"Schneider, Caroline A and Rasband, Wayne S and Eliceiri, Kevin W",2012,,,,Nature Methods
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,schindelin2012fiji,\cite{schindelin2012fiji},{Fiji}: an open-source platform for biological-image analysis,,,True,False,"Schindelin, Johannes and Arganda-Carreras, Ignacio and Frise, Erwin and Kaynig, Verena and Longair, Mark and Pietzsch, Tobias and Preibisch, Stephan and Rueden, Curtis and Saalfeld, Stephan and Schmid, Benjamin and Tinevez, Jean-Yves and White, Daniel J. and Hartenstein, Volker and Eliceiri, Kevin and Tomancak, Pavel and Cardona, Albert",2012,,,10.1038/nmeth.2019,Nature Methods
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,stirling2021cellprofiler4,\cite{stirling2021cellprofiler4},"{CellProfiler} 4: improvements in speed, utility and usability",,,True,False,"Stirling, David R. and Carpenter, Anne E. and Cimini, Beth A.",2021,,,10.1186/s12859-021-04344-9,BMC Bioinformatics
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,kreshuk2019ilastik,\cite{kreshuk2019ilastik},ilastik: interactive machine learning for (bio)image analysis,,,True,False,"Berg, Stuart and Kutra, Dominik and Kroeger, Thorben and Straehle, Christoph N. and Kausler, Bernhard X. and Haubold, Chris and Schiegg, Marco and Ales, Markus and Beier, Thomas and Rudy, Babak and Weigert, Martin and Rajan, Vishwanathan and Schmidt, Urs and Weigert, Martin and Myers, Eugene W. and Kopf, Martin and Hamprecht, Fred A. and Kreshuk, Anna",2019,,,10.1038/s41592-019-0582-9,Nature Methods
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,bankhead2017qupath,\cite{bankhead2017qupath},QuPath: Open source software for digital pathology image analysis,,,True,False,"Bankhead, Phil and Loughrey, Michael B. and Fern{\'a}ndez, Daniel and Dombrowski, Yena and McArt, Daniel G. and Dunne, Peter D. and McQuaid, Sonia and Gray, Ron and Murray, Liam J. and Coleman, Helen G. and James, John A. and Salto-Tellez, Manuel and Hamilton, Paul W.",2017,,,10.1038/s41598-017-17204-5,Scientific Reports
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,paszke2019pytorch,\cite{paszke2019pytorch},"PyTorch: An Imperative Style, High-Performance Deep Learning Library",https://arxiv.org/abs/1912.01703v1,"Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.
  In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.
  We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.",True,True,"Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith",2019,,,,
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,munoz2025cp_measure,\cite{munoz2025cp_measure},cp\_measure: {API-first} feature extraction for image-based profiling workflows,,,True,False,"Mu{\~n}oz, Al{\'a}n F. and Treis, Tim and Kalinin, Alexandr A. and Dasgupta, Shatavisha and Theis, Fabian and Carpenter, Anne E. and Singh, Shantanu",2025,,,,
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,napari2019,\cite{napari2019},napari: a multi-dimensional image viewer for {Python},,,True,False,{napari} contributors,2019,,,10.5281/zenodo.3555620,
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,chiu2022napari,\cite{chiu2022napari},Napari: a {Python} multi-dimensional image viewer platform for the research community,,,True,False,"Chiu, Chi-Li and Clack, Nathan and {the napari community}",2022,,,,Microscopy and Microanalysis
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,witz2024napari,\cite{witz2024napari},napari-skimage,,,True,False,"Witz, Guillaume",2024,,,,
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,opencv_library,\cite{opencv_library},{The OpenCV Library},,,True,False,"Bradski, G.",2000,,,,Dr. Dobb's Journal of Software Tools
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,vanderwalt2014scikit,\cite{vanderwalt2014scikit},scikit-image: Image processing in {P}ython,,,True,False,"van der Walt, St{\'e}fan and Sch{\""o}nberger, Johannes L. and Nunez-Iglesias, Juan et al.",2014,,,10.7717/peerj.453,PeerJ
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,harris2020array,\cite{harris2020array},Array programming with {NumPy},,,True,False,"Harris, Charles R and Millman, K Jarrod and Van Der Walt, St{\'e}fan J and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J and others",2020,,,,Nature
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,virtanen2020scipy,\cite{virtanen2020scipy},{SciPy} 1.0: fundamental algorithms for scientific computing in {Python},,,True,False,"Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and others",2020,,,,Nature Methods
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,rasse2020opsef,\cite{rasse2020opsef},OpSeF: Open Source Python Framework for Collaborative Instance Segmentation of Bioimages,,,True,False,"Rasse, Tobias M. and Hollandi, R{\'e}ka and Horv{\'a}th, Peter",2020,,,10.3389/fbioe.2020.558880,Frontiers in Bioengineering and Biotechnology
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,palla2022squidpy,\cite{palla2022squidpy},Squidpy: A scalable framework for spatial omics analysis,,,True,False,"Palla, Giovanni and Spitzer, Hannah and Klein, Michal and Fischer, David and Schaar, Anna Christina and Kuemmerle, Louis Benedikt and Rybakov, Sergei and Ibarra, Ignacio L and Holmberg, Olle and Virshup, Isaac and others",2022,,,10.1038/s41592-021-01358-2,Nature Methods
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,comolet2024scalefex,\cite{comolet2024scalefex},"A highly efficient, scalable pipeline for fixed feature extraction from large-scale high-content imaging screens",,,True,False,"Comolet, Gabriel and Bose, Neeloy and Winchell, Jeff and Duren-Lubanski, Alyssa and Rusielewicz, Tom and Goldberg, Jordan and Horn, Grayson and Paull, Daniel and Migliori, Bianca",2024,,,10.1016/j.isci.2024.111434,iScience
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,atgu2024microscopytools,\cite{atgu2024microscopytools},Microscopy Computational Tools,,,True,False,{Analytic and Translational Genetics Unit},2024,,,,
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,olafsson2025spacr,\cite{olafsson2025spacr},{SpaCr}: Spatial phenotype analysis of {CRISPR-Cas9} screens,,,True,False,"Olafsson, Einar",2025,,,,
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,autodeconj,\cite{autodeconj},AutoDeconJ: a GPU accelerated ImageJ plugin for 3D light field deconvolution with optimal iteration numbers predicting,https://arxiv.org/abs/2208.11422v1,"Light field microscopy is a compact solution to high-speed 3D fluorescence imaging. Usually, we need to do 3D deconvolution to the captured raw data. Although there are deep neural network methods that can accelerate the reconstruction process, the model is not universally applicable for all system parameters. Here, we develop AutoDeconJ, a GPU accelerated ImageJ plugin for 4.4x faster and accurate deconvolution of light field microscopy data. We further propose an image quality metric for the deconvolution process, aiding in automatically determining the optimal number of iterations with higher reconstruction accuracy and fewer artifacts",True,True,"Su, Changqing and Gao, Yuhan and Zhou, You and Sun, Yaoqi and Yan, Chenggang and Yin, Haibing and Xiong, Bo",2022,11,,,Bioinformatics
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,haase2020_clij,\cite{haase2020_clij},{CLIJ: GPU}-accelerated image processing for everyone,,,True,False,"Haase, Robert and Royer, Loic A. and Steinbach, Peter and Schmidt, Deborah and Dibrov, Alexandr and Schmidt, Uwe and Weigert, Martin and Maghelli, Nicola and Tomancak, Pavel and Jug, Florian and Myers, Eugene W.",2020,,,10.1038/s41592-019-0650-1,Nature Methods
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,vorkel2020_gpu_macro,\cite{vorkel2020_gpu_macro},GPU-accelerating ImageJ Macro image processing workflows using CLIJ,https://arxiv.org/abs/2008.11799v1,"This chapter introduces GPU-accelerated image processing in ImageJ/FIJI. The reader is expected to have some pre-existing knowledge of ImageJ Macro programming. Core concepts such as variables, for-loops, and functions are essential. The chapter provides basic guidelines for improved performance in typical image processing workflows. We present in a step-by-step tutorial how to translate a pre-existing ImageJ macro into a GPU-accelerated macro.",True,True,"Vorkel, Daniela and Haase, Robert",2020,,,,arXiv preprint
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,pyclesperanto2023,\cite{pyclesperanto2023},{pyclesperanto}: {GPU}-accelerated Image Processing Library,,,True,False,"Haase, Robert and Str{\""i}gaud, S{\'e}bastien and others",2025,,,,
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,abadi2016tensorflow,\cite{abadi2016tensorflow},{TensorFlow}: A System for Large-Scale Machine Learning,,,True,False,"Abadi, Mart\'{\i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man\'{e}, Dandelion and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi\'{e}gas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang",2016,,,,
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,czech2019cytokit,\cite{czech2019cytokit},Cytokit: a single-cell analysis toolkit for high dimensional fluorescent microscopy imaging,,,True,False,"Czech, Eric and Aksoy, Bulent Arman and Aksoy, Pinar and Hammerbacher, Jeff",2019,,,,BMC bioinformatics
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,riba2020kornia,\cite{riba2020kornia},Kornia: an Open Source Differentiable Computer Vision Library for PyTorch,https://arxiv.org/abs/1910.02190v2,"This work presents Kornia -- an open source computer vision library which consists of a set of differentiable routines and modules to solve generic computer vision problems. The package uses PyTorch as its main backend both for efficiency and to take advantage of the reverse-mode auto-differentiation to define and compute the gradient of complex functions. Inspired by OpenCV, Kornia is composed of a set of modules containing operators that can be inserted inside neural networks to train models to perform image transformations, camera calibration, epipolar geometry, and low level image processing techniques, such as filtering and edge detection that operate directly on high dimensional tensor representations. Examples of classical vision problems implemented using our framework are provided including a benchmark comparing to existing vision libraries.",True,True,"Riba, Edgar and Mishkin, Dmytro and Ponsa, Daniel and Rublee, Ethan and Bradski, Gary",2020,,,,
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,torchvision2016,\cite{torchvision2016},TorchVision: PyTorch's Computer Vision library,,,True,False,TorchVision maintainers and contributors,,,,,
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,cupy,\cite{cupy},{CuPy}: A {NumPy}-Compatible Library for {NVIDIA GPU} Calculations,,,True,False,"Okuta, Ryosuke and Unno, Yuya and Nishino, Daisuke and Hido, Shohei and Loomis, Crissman",2017,,http://learningsys.org/nips17/assets/papers/paper_16.pdf,,
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,cucim,\cite{cucim},{cuCIM}: {RAPIDS GPU}-accelerated image processing library,,,True,False,{RAPIDSAI} contributors,2025,,,,
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,haase2021apoc,\cite{haase2021apoc},napari-accelerated-pixel-and-object-classification,,,True,False,"Haase, Robert",2021,,,,
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,lambert2021pycudadecon,\cite{lambert2021pycudadecon},pycudadecon,,,True,False,"Lambert, Talley",2019,,,,
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,biggs1997acceleration,\cite{biggs1997acceleration},Acceleration of iterative image restoration algorithms,,,True,False,"Biggs, David SC and Andrews, Mark",1997,,,,Applied Optics
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,clesperanto2020napari,\cite{clesperanto2020napari},napari-pyclesperanto-assistant,,,True,False,{clEsperanto contributors},2020,,,,
cubic: CUDA-accelerated 3D Bioimage Computing,2510.14143v1,haase2021napari,\cite{haase2021napari},napari-cupy-image-processing,,,True,False,"Haase, Robert",2021,,,,
Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets,2511.06356v1,rea_BH,\cite{rea_BH},Predicting reaction performance in {C}--{N} cross-coupling using machine learning,,,True,False,Derek T. Ahneman  and Jes{\'u}s G. Estrada  and Shishi Lin  and Spencer D. Dreher  and Abigail G. Doyle,2018,,,,Science
Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets,2511.06356v1,rea_SM,\cite{rea_SM},A platform for automated nanomole-scale reaction screening and micromole-scale synthesis in flow,,,True,False,"Perera, Damith and Tucker, Joseph W and Brahmbhatt, Shalini and Helal, Christopher J and Chong, Ashley and Farrell, William and Richardson, Paul and Sach, Neal W",2018,,,,Science
Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets,2511.06356v1,rea_MFF_fp,\cite{rea_MFF_fp},A Structure-Based Platform for Predicting Chemical Reactivity,,,True,False,"Frederik Sandfort and Felix Strieth‐Kalthoff and Marius K{\""u}hnemund and Christian Beecks and Frank Glorius",2019,,,,Chem
Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets,2511.06356v1,rea_gnn_desc,\cite{rea_gnn_desc},On the use of real-world datasets for reaction yield prediction,,,True,False,"Saebi, Mandana and Nan, Bozhao and Herr, John E. and Wahlers, Jessica and Guo, Zhichun and Zurański, Andrzej M. and Kogej, Thierry and Norrby, Per-Ola and Doyle, Abigail G. and Chawla, Nitesh V. and Wiest, Olaf",2023,,,10.1039/D2SC06041H,Chem. Sci.
Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets,2511.06356v1,rea_NiCOlit,\cite{rea_NiCOlit},"Machine learning yield prediction from NiCOlit, a small-size literature data set of nickel catalyzed C--O couplings",,,True,False,"Schleinitz, Jules and Langevin, Maxime and Smail, Yanis and Wehnert, Benjamin and Grimaud, Laurence and Vuilleumier, Rodolphe",2022,,,,Journal of the American Chemical Society
Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets,2511.06356v1,rea_bert,\cite{rea_bert},Prediction of chemical reaction yields using deep learning,,,True,False,"Schwaller, Philippe and Vaucher, Alain C and Laino, Teodoro and Reymond, Jean-Louis",2021,,,,Machine learning: science and technology
Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets,2511.06356v1,rea_bert_augmentation,\cite{rea_bert_augmentation},{Data augmentation strategies to improve reaction yield predictions and estimate uncertainty},,,True,False,Philippe Schwaller and Alain C. Vaucher and Teodoro Laino and Jean-Louis Reymond,2020,11,,10.26434/chemrxiv.13286741,Machine Learning for Molecules Workshop @ NeurIPS 2020
Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets,2511.06356v1,BERT,\cite{BERT},BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,https://arxiv.org/abs/1810.04805v2,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.
  BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",True,True,Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova,2019,,,,
Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets,2511.06356v1,rea_tf,\cite{rea_tf},Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction,,,True,False,"Schwaller, Philippe and Laino, Teodoro and Gaudin, Théophile and Bolgar, Peter and Hunter, Christopher A. and Bekas, Costas and Lee, Alpha A.",2019,,,10.1021/acscentsci.9b00576,ACS Central Science
Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets,2511.06356v1,transformer,\cite{transformer},Not All Attention Is All You Need,https://arxiv.org/abs/2104.04692v3,"Beyond the success story of pre-trained language models (PrLMs) in recent natural language processing, they are susceptible to over-fitting due to unusual large model size. To this end, dropout serves as a therapy. However, existing methods like random-based, knowledge-based and search-based dropout are more general but less effective onto self-attention based models, which are broadly chosen as the fundamental architecture of PrLMs. In this paper, we propose a novel dropout method named AttendOut to let self-attention empowered PrLMs capable of more robust task-specific tuning. We demonstrate that state-of-the-art models with elaborate training design may achieve much stronger results. We verify the universality of our approach on extensive natural language processing tasks.",True,True,"Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \L{}ukasz and Polosukhin, Illia",2017,,,,
Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets,2511.06356v1,rea_t5,\cite{rea_t5},Unified Deep Learning Model for Multitask Reaction Predictions with Explanation,,,True,False,Jieyu Lu and Yingkai Zhang,2022,,,,Journal of chemical information and modeling
Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets,2511.06356v1,t5,\cite{t5},Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,,,True,False,"Raffel, C. and Shazeer, N. and Roberts, A. and et al.",2020,,,,Journal of Machine Learning Research
Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets,2511.06356v1,permutation_3,\cite{permutation_3},Rxn Hypergraph: a Hypergraph Attention Model for Chemical Reaction Representation,https://arxiv.org/abs/2201.01196v1,"It is fundamental for science and technology to be able to predict chemical reactions and their properties. To achieve such skills, it is important to develop good representations of chemical reactions, or good deep learning architectures that can learn such representations automatically from the data. There is currently no universal and widely adopted method for robustly representing chemical reactions. Most existing methods suffer from one or more drawbacks, such as: (1) lacking universality; (2) lacking robustness; (3) lacking interpretability; or (4) requiring excessive manual pre-processing. Here we exploit graph-based representations of molecular structures to develop and test a hypergraph attention neural network approach to solve at once the reaction representation and property-prediction problems, alleviating the aforementioned drawbacks. We evaluate this hypergraph representation in three experiments using three independent data sets of chemical reactions. In all experiments, the hypergraph-based approach matches or outperforms other representations and their corresponding models of chemical reactions while yielding interpretable multi-level representations.",True,True,"Tavakoli, M. and Shmakov, A. and Ceccarelli, F. and et al.",2022,,,,arXiv preprint arXiv:2201.01196
Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets,2511.06356v1,rea_gnn_sum,\cite{rea_gnn_sum},Uncertainty-aware prediction of chemical reaction yields with graph neural networks,,,True,False,"Kwon, Youngchun and Lee, Dongseon and Choi, Youn-Suk and Kang, Seokho",2022,,,,Journal of Cheminformatics
Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets,2511.06356v1,rea_uam,\cite{rea_uam},Uncertainty-Aware Yield Prediction with Multimodal Molecular Features,,,True,False,Jiayuan Chen and Kehan Guo and Zhen Liu and Olexandr Isayev and Xiangliang Zhang,2024,,,,
Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets,2511.06356v1,rea_gnn_template,\cite{rea_gnn_template},A generalized-template-based graph neural network for accurate organic reactivity prediction,,,True,False,Shuan Chen and Yousung Jung,2022,,,,Nature Machine Intelligence
Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets,2511.06356v1,rea_mvp,\cite{rea_mvp},Prediction of chemical reaction yields with large-scale multi-view pre-training,,,True,False,"Shi, Runhan and Yu, Gufeng and Huo, Xiao and others",2024,,,,Journal of Cheminformatics
Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets,2511.06356v1,gru,\cite{gru},On the Properties of Neural Machine Translation: Encoder-Decoder Approaches,,,True,False,Kyunghyun Cho and Bart van Merrienboer and Dzmitry Bahdanau and Yoshua Bengio,2014,,,,
Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets,2511.06356v1,rea_yieldfcp,\cite{rea_yieldfcp},YieldFCP: Enhancing Reaction Yield Prediction via Fine-grained Cross-modal Pre-training,,,True,False,"Shi, R. and Yu, G. and Chen, L. and et al.",2025,,,,Artificial Intelligence Chemistry
Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets,2511.06356v1,RECAP,\cite{RECAP},Recap retrosynthetic combinatorial analysis procedure: a powerful new technique for identifying privileged molecular fragments with useful applications in combinatorial chemistry,,,True,False,"Lewell, X. Q. and Judd, D. B. and Watson, S. P. and Hann, M. M.",1998,,,,Journal of Chemical Information and Computer Sciences
Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets,2511.06356v1,BRICS,\cite{BRICS},On the Art of Compiling and Using `Drug-Like' Chemical Fragment Spaces,,,True,False,"Degen, J. and Wegscheid-Gerlach, C. and Zaliani, A. and Rarey, M.",2008,,,,ChemMedChem: Chemistry Enabling Drug Discovery
Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets,2511.06356v1,ReLMole,\cite{ReLMole},ReLMole: Molecular representation learning based on two-level graph similarities,,,True,False,"Ji, Z. and Shi, R. and Lu, J. and et al.",2022,,,,Journal of Chemical Information and Modeling
Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets,2511.06356v1,ECFP,\cite{ECFP},Extended-connectivity fingerprints,,,True,False,"Rogers, D. and Hahn, M.",2010,,,,Journal of Chemical Information and Modeling
Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets,2511.06356v1,shingle_0,\cite{shingle_0},A probabilistic molecular fingerprint for big data settings,,,True,False,"Probst, D. and Reymond, J. L.",2018,,,,Journal of Cheminformatics
Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets,2511.06356v1,shingle_1,\cite{shingle_1},"One molecular fingerprint to rule them all: drugs, biomolecules, and the metabolome",,,True,False,"Capecchi, A. and Probst, D. and Reymond, J. L.",2020,,,,Journal of Cheminformatics
Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets,2511.06356v1,shingle_2,\cite{shingle_2},One chiral fingerprint to find them all,,,True,False,"Orsi, M. and Reymond, J. L.",2024,,,,Journal of Cheminformatics
Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets,2511.06356v1,rea_DRFP,\cite{rea_DRFP},Reaction Classification and Yield Prediction using the Differential Reaction Fingerprint DRFP,,,True,False,"Probst, Daniel and Schwaller, Philippe and Reymond, Jean-Louis",2022,,,,Digital Discovery
InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions,2510.03370v2,alford2017rosetta,\cite{alford2017rosetta},The Rosetta all-atom energy function for macromolecular modeling and design,,,True,False,"Alford, Rebecca F and Leaver-Fay, Andrew and Jeliazkov, Jeliazko R and O’Meara, Matthew J and DiMaio, Frank P and Park, Hahnbeom and Shapovalov, Maxim V and Renfrew, P Douglas and Mulligan, Vikram K and Kappel, Kalli and others",2017,,,,Journal of chemical theory and computation
InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions,2510.03370v2,xiong2020increasing,\cite{xiong2020increasing},Increasing the efficiency and accuracy of the ABACUS protein sequence design method,,,True,False,"Xiong, Peng and Hu, Xiuhong and Huang, Bin and Zhang, Jiahai and Chen, Quan and Liu, Haiyan",2020,,,,Bioinformatics
InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions,2510.03370v2,meier2021language,\cite{meier2021language},Language models enable zero-shot prediction of the effects of mutations on protein function,,,True,False,"Meier, Joshua and Rao, Roshan and Verkuil, Robert and Liu, Jason and Sercu, Tom and Rives, Alex",2021,,,,Advances in neural information processing systems
InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions,2510.03370v2,li2024prosst,\cite{li2024prosst},Prosst: Protein language modeling with quantized structure and disentangled attention,,,True,False,"Li, Mingchen and Tan, Yang and Ma, Xinzhu and Zhong, Bozitao and Yu, Huiqun and Zhou, Ziyi and Ouyang, Wanli and Zhou, Bingxin and Tan, Pan and Hong, Liang",2024,,,,Advances in Neural Information Processing Systems
InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions,2510.03370v2,sun2025structure,\cite{sun2025structure},Structure-based self-supervised learning enables ultrafast protein stability prediction upon mutation,,,True,False,"Sun, Jinyuan and Zhu, Tong and Cui, Yinglu and Wu, Bian",2025,,,,The Innovation
InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions,2510.03370v2,notin2023proteingym,\cite{notin2023proteingym},Proteingym: Large-scale benchmarks for protein fitness prediction and design,,,True,False,"Notin, Pascal and Kollasch, Aaron and Ritter, Daniel and Van Niekerk, Lood and Paul, Steffanie and Spinner, Han and Rollins, Nathan and Shaw, Ada and Orenbuch, Rose and Weitzman, Ruben and others",2023,,,,Advances in Neural Information Processing Systems
InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions,2510.03370v2,sun2024mixture,\cite{sun2024mixture},Mixture of experts enable efficient and effective protein understanding and design,,,True,False,"Sun, Ning and Zou, Shuxian and Tao, Tianhua and Mahbub, Sazan and Li, Dian and Zhuang, Yonghao and Wang, Hongyi and Cheng, Xingyi and Song, Le and Xing, Eric P",2024,,,,bioRxiv
InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions,2510.03370v2,tan2024retrieval,\cite{tan2024retrieval},Retrieval-Enhanced Mutation Mastery: Augmenting Zero-Shot Prediction of Protein Language Model,https://arxiv.org/abs/2410.21127v1,"Enzyme engineering enables the modification of wild-type proteins to meet industrial and research demands by enhancing catalytic activity, stability, binding affinities, and other properties. The emergence of deep learning methods for protein modeling has demonstrated superior results at lower costs compared to traditional approaches such as directed evolution and rational design. In mutation effect prediction, the key to pre-training deep learning models lies in accurately interpreting the complex relationships among protein sequence, structure, and function. This study introduces a retrieval-enhanced protein language model for comprehensive analysis of native properties from sequence and local structural interactions, as well as evolutionary properties from retrieved homologous sequences. The state-of-the-art performance of the proposed ProtREM is validated on over 2 million mutants across 217 assays from an open benchmark (ProteinGym). We also conducted post-hoc analyses of the model's ability to improve the stability and binding affinity of a VHH antibody. Additionally, we designed 10 new mutants on a DNA polymerase and conducted wet-lab experiments to evaluate their enhanced activity at higher temperatures. Both in silico and experimental evaluations confirmed that our method provides reliable predictions of mutation effects, offering an auxiliary tool for biologists aiming to evolve existing enzymes. The implementation is publicly available at https://github.com/tyang816/ProtREM.",True,True,"Tan, Yang and Wang, Ruilin and Wu, Banghao and Hong, Liang and Zhou, Bingxin",2024,,,,arXiv preprint arXiv:2410.21127
InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions,2510.03370v2,zhang2024multi,\cite{zhang2024multi},Multi-scale representation learning for protein fitness prediction,,,True,False,"Zhang, Zuobai and Notin, Pascal and Huang, Yining and Lozano, Aurelie C and Chenthamarakshan, Vijil and Marks, Debora and Das, Payel and Tang, Jian",2024,,,,Advances in Neural Information Processing Systems
InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions,2510.03370v2,baltruvsaitis2018multimodal,\cite{baltruvsaitis2018multimodal},Multimodal machine learning: A survey and taxonomy,,,True,False,"Baltru{\v{s}}aitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe",2018,,,,IEEE transactions on pattern analysis and machine intelligence
InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions,2510.03370v2,alayrac2022flamingo,\cite{alayrac2022flamingo},Flamingo: a visual language model for few-shot learning,,,True,False,"Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others",2022,,,,Advances in neural information processing systems
InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions,2510.03370v2,tong2024cambrian,\cite{tong2024cambrian},"Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs",https://arxiv.org/abs/2406.16860v2,"We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a vision-centric approach. While stronger language models can enhance multimodal capabilities, the design choices for vision components are often insufficiently explored and disconnected from visual representation learning research. This gap hinders accurate sensory grounding in real-world scenarios. Our study uses LLMs and visual instruction tuning as an interface to evaluate various visual representations, offering new insights into different models and architectures -- self-supervised, strongly supervised, or combinations thereof -- based on experiments with over 20 vision encoders. We critically examine existing MLLM benchmarks, address the difficulties involved in consolidating and interpreting results from various tasks, and introduce a new vision-centric benchmark, CV-Bench. To further improve visual grounding, we propose the Spatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that integrates high-resolution vision features with LLMs while reducing the number of tokens. Additionally, we discuss the curation of high-quality visual instruction-tuning data from publicly available sources, emphasizing the importance of data source balancing and distribution ratio. Collectively, Cambrian-1 not only achieves state-of-the-art performance but also serves as a comprehensive, open cookbook for instruction-tuned MLLMs. We provide model weights, code, supporting tools, datasets, and detailed instruction-tuning and evaluation recipes. We hope our release will inspire and accelerate advancements in multimodal systems and visual representation learning.",True,True,"Tong, Peter and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and IYER, Adithya Jairam Vedagiri and Akula, Sai Charitha and Yang, Shusheng and Yang, Jihan and Middepogu, Manoj and Wang, Ziteng and others",2024,,,,Advances in Neural Information Processing Systems
InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions,2510.03370v2,li2024multimodal,\cite{li2024multimodal},Multimodal alignment and fusion: A survey,,,True,False,"Li, Songtao and Tang, Hao",2024,,,,arXiv preprint arXiv:2411.17040
InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions,2510.03370v2,liu2023visual,\cite{liu2023visual},Visual instruction tuning,,,True,False,"Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",2023,,,,Advances in neural information processing systems
InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions,2510.03370v2,liu2024improved,\cite{liu2024improved},Improved baselines with visual instruction tuning,,,True,False,"Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae",2024,,,,
InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions,2510.03370v2,li2024llava,\cite{li2024llava},"LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models",https://arxiv.org/abs/2407.07895v2,"Visual instruction tuning has made considerable strides in enhancing the capabilities of Large Multimodal Models (LMMs). However, existing open LMMs largely focus on single-image tasks, their applications to multi-image scenarios remains less explored. Additionally, prior LMM research separately tackles different scenarios, leaving it impossible to generalize cross scenarios with new emerging capabilities. To this end, we introduce LLaVA-NeXT-Interleave, which simultaneously tackles Multi-image, Multi-frame (video), Multi-view (3D), and Multi-patch (single-image) scenarios in LMMs. To enable these capabilities, we regard the interleaved data format as a general template and compile the M4-Instruct dataset with 1,177.6k samples, spanning 4 primary domains with 14 tasks and 41 datasets. We also curate the LLaVA-Interleave Bench to comprehensively evaluate the multi-image performance of LMMs. Through extensive experiments, LLaVA-NeXT-Interleave achieves leading results in multi-image, video, and 3D benchmarks, while maintaining the performance of single-image tasks. Besides, our model also exhibits several emerging capabilities, e.g., transferring tasks across different settings and modalities. Code is available at https://github.com/LLaVA-VL/LLaVA-NeXT",True,True,"Li, Feng and Zhang, Renrui and Zhang, Hao and Zhang, Yuanhan and Li, Bo and Li, Wei and Ma, Zejun and Li, Chunyuan",2024,,,,arXiv preprint arXiv:2407.07895
InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions,2510.03370v2,Qwen-VL,\cite{Qwen-VL},"Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond",https://arxiv.org/abs/2308.12966v3,"In this work, we introduce the Qwen-VL series, a set of large-scale vision-language models (LVLMs) designed to perceive and understand both texts and images. Starting from the Qwen-LM as a foundation, we endow it with visual capacity by the meticulously designed (i) visual receptor, (ii) input-output interface, (iii) 3-stage training pipeline, and (iv) multilingual multimodal cleaned corpus. Beyond the conventional image description and question-answering, we implement the grounding and text-reading ability of Qwen-VLs by aligning image-caption-box tuples. The resulting models, including Qwen-VL and Qwen-VL-Chat, set new records for generalist models under similar model scales on a broad range of visual-centric benchmarks (e.g., image captioning, question answering, visual grounding) and different settings (e.g., zero-shot, few-shot). Moreover, on real-world dialog benchmarks, our instruction-tuned Qwen-VL-Chat also demonstrates superiority compared to existing vision-language chatbots. Code, demo and models are available at https://github.com/QwenLM/Qwen-VL.",True,True,"Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren",2023,,,,arXiv preprint arXiv:2308.12966
InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions,2510.03370v2,awadalla2023openflamingo,\cite{awadalla2023openflamingo},Openflamingo: An open-source framework for training large autoregressive vision-language models,,,True,False,"Awadalla, Anas and Gao, Irena and Gardner, Josh and Hessel, Jack and Hanafy, Yusuf and Zhu, Wanrong and Marathe, Kalyani and Bitton, Yonatan and Gadre, Samir and Sagawa, Shiori and others",2023,,,,arXiv preprint arXiv:2308.01390
InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions,2510.03370v2,meng2024deepstack,\cite{meng2024deepstack},DeepStack: Deeply Stacking Visual Tokens is Surprisingly Simple and Effective for LMMs,https://arxiv.org/abs/2406.04334v1,"Most large multimodal models (LMMs) are implemented by feeding visual tokens as a sequence into the first layer of a large language model (LLM). The resulting architecture is simple but significantly increases computation and memory costs, as it has to handle a large number of additional tokens in its input layer. This paper presents a new architecture DeepStack for LMMs. Considering $N$ layers in the language and vision transformer of LMMs, we stack the visual tokens into $N$ groups and feed each group to its aligned transformer layer \textit{from bottom to top}. Surprisingly, this simple method greatly enhances the power of LMMs to model interactions among visual tokens across layers but with minimal additional cost. We apply DeepStack to both language and vision transformer in LMMs, and validate the effectiveness of DeepStack LMMs with extensive empirical results. Using the same context length, our DeepStack 7B and 13B parameters surpass their counterparts by \textbf{2.7} and \textbf{2.9} on average across \textbf{9} benchmarks, respectively. Using only one-fifth of the context length, DeepStack rivals closely to the counterparts that use the full context length. These gains are particularly pronounced on high-resolution tasks, e.g., \textbf{4.2}, \textbf{11.0}, and \textbf{4.0} improvements on TextVQA, DocVQA, and InfoVQA compared to LLaVA-1.5-7B, respectively. We further apply DeepStack to vision transformer layers, which brings us a similar amount of improvements, \textbf{3.8} on average compared with LLaVA-1.5-7B.",True,True,"Meng, Lingchen and Yang, Jianwei and Tian, Rui and Dai, Xiyang and Wu, Zuxuan and Gao, Jianfeng and Jiang, Yu-Gang",2024,,,,Advances in Neural Information Processing Systems
InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions,2510.03370v2,lin2025multi,\cite{lin2025multi},"Multi-Layer Visual Feature Fusion in Multimodal LLMs: Methods, Analysis, and Best Practices",,,True,False,"Lin, Junyan and Chen, Haoran and Fan, Yue and Fan, Yingqi and Jin, Xin and Su, Hui and Fu, Jinlan and Shen, Xiaoyu",2025,,,,
InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions,2510.03370v2,wadekar2024evolution,\cite{wadekar2024evolution},The evolution of multimodal model architectures,,,True,False,"Wadekar, Shakti N and Chaurasia, Abhishek and Chadha, Aman and Culurciello, Eugenio",2024,,,,arXiv preprint arXiv:2405.17927
InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions,2510.03370v2,wu2025qwen,\cite{wu2025qwen},Qwen-Image Technical Report,https://arxiv.org/abs/2508.02324v1,"We present Qwen-Image, an image generation foundation model in the Qwen series that achieves significant advances in complex text rendering and precise image editing. To address the challenges of complex text rendering, we design a comprehensive data pipeline that includes large-scale data collection, filtering, annotation, synthesis, and balancing. Moreover, we adopt a progressive training strategy that starts with non-text-to-text rendering, evolves from simple to complex textual inputs, and gradually scales up to paragraph-level descriptions. This curriculum learning approach substantially enhances the model's native text rendering capabilities. As a result, Qwen-Image not only performs exceptionally well in alphabetic languages such as English, but also achieves remarkable progress on more challenging logographic languages like Chinese. To enhance image editing consistency, we introduce an improved multi-task training paradigm that incorporates not only traditional text-to-image (T2I) and text-image-to-image (TI2I) tasks but also image-to-image (I2I) reconstruction, effectively aligning the latent representations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed the original image into Qwen2.5-VL and the VAE encoder to obtain semantic and reconstructive representations, respectively. This dual-encoding mechanism enables the editing module to strike a balance between preserving semantic consistency and maintaining visual fidelity. Qwen-Image achieves state-of-the-art performance, demonstrating its strong capabilities in both image generation and editing across multiple benchmarks.",True,True,"Wu, Chenfei and Li, Jiahao and Zhou, Jingren and Lin, Junyang and Gao, Kaiyuan and Yan, Kun and Yin, Sheng-ming and Bai, Shuai and Xu, Xiao and Chen, Yilei and others",2025,,,,arXiv preprint arXiv:2508.02324
InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions,2510.03370v2,zhu2504internvl3,\cite{zhu2504internvl3},"Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, 2025",,,True,False,"Zhu, Jinguo and Wang, W and Chen, Z and Liu, Z and Ye, S and Gu, L and Tian, H and Duan, Y and Su, W and Shao, J and others",2025,,,,URL https://arxiv. org/abs/2504.10479
InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions,2510.03370v2,sun2023aligning,\cite{sun2023aligning},Aligning large multimodal models with factually augmented rlhf,,,True,False,"Sun, Zhiqing and Shen, Sheng and Cao, Shengcao and Liu, Haotian and Li, Chunyuan and Shen, Yikang and Gan, Chuang and Gui, Liang-Yan and Wang, Yu-Xiong and Yang, Yiming and others",2023,,,,arXiv preprint arXiv:2309.14525
InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions,2510.03370v2,chu2025sft,\cite{chu2025sft},"SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training",https://arxiv.org/abs/2501.17161v2,"Supervised fine-tuning (SFT) and reinforcement learning (RL) are widely used post-training techniques for foundation models. However, their roles in enhancing model generalization capabilities remain unclear. This paper studies the difference between SFT and RL on generalization and memorization, focusing on text-based rule variants and visual variants. We introduce GeneralPoints, an arithmetic reasoning card game, and adopt V-IRL, a real-world navigation environment, to assess how models trained with SFT and RL generalize to unseen variants in both textual and visual domains. We show that RL, especially when trained with an outcome-based reward, generalizes across both rule-based textual and visual variants. SFT, in contrast, tends to memorize training data and struggles to generalize out-of-distribution scenarios. Further analysis reveals that RL improves the model's underlying visual recognition capabilities, contributing to its enhanced generalization in the visual domain. Despite RL's superior generalization, we show that SFT remains essential for effective RL training; SFT stabilizes the model's output format, enabling subsequent RL to achieve its performance gains. These findings demonstrates the capability of RL for acquiring generalizable knowledge in complex, multi-modal tasks.",True,True,"Chu, Tianzhe and Zhai, Yuexiang and Yang, Jihan and Tong, Shengbang and Xie, Saining and Schuurmans, Dale and Le, Quoc V and Levine, Sergey and Ma, Yi",2025,,,,arXiv preprint arXiv:2501.17161
InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions,2510.03370v2,su2023saprot,\cite{su2023saprot},Saprot: Protein language modeling with structure-aware vocabulary,,,True,False,"Su, Jin and Han, Chenchen and Zhou, Yuyang and Shan, Junjie and Zhou, Xibin and Yuan, Fajie",2023,,,,BioRxiv
InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions,2510.03370v2,qiu2024instructplm,\cite{qiu2024instructplm},Instructplm: Aligning protein language models to follow protein structure instructions,,,True,False,"Qiu, Jiezhong and Xu, Junde and Hu, Jie and Cao, Hanqun and Hou, Liya and Gao, Zijun and Zhou, Xinyi and Li, Anni and Li, Xiujuan and Cui, Bin and others",2024,,,,bioRxiv
InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions,2510.03370v2,hayes2025simulating,\cite{hayes2025simulating},Simulating 500 million years of evolution with a language model,,,True,False,"Hayes, Thomas and Rao, Roshan and Akin, Halil and Sofroniew, Nicholas J and Oktay, Deniz and Lin, Zeming and Verkuil, Robert and Tran, Vincent Q and Deaton, Jonathan and Wiggert, Marius and others",2025,,,,Science
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,Schutt2017SchNet,\cite{Schutt2017SchNet},SchNet: A continuous-filter convolutional neural network for modeling quantum interactions,https://arxiv.org/abs/1706.08566v5,"Deep learning has the potential to revolutionize quantum chemistry as it is ideally suited to learn representations for structured data and speed up the exploration of chemical space. While convolutional neural networks have proven to be the first choice for images, audio and video data, the atoms in molecules are not restricted to a grid. Instead, their precise locations contain essential physical information, that would get lost if discretized. Thus, we propose to use continuous-filter convolutional layers to be able to model local correlations without requiring the data to lie on a grid. We apply those layers in SchNet: a novel deep learning architecture modeling quantum interactions in molecules. We obtain a joint model for the total energy and interatomic forces that follows fundamental quantum-chemical principles. This includes rotationally invariant energy predictions and a smooth, differentiable potential energy surface. Our architecture achieves state-of-the-art performance for benchmarks of equilibrium molecules and molecular dynamics trajectories. Finally, we introduce a more challenging benchmark with chemical and structural variations that suggests the path for further work.",True,True,"Kristof T. Sch{\""u}tt and Huziel E. Sauceda and P.-J. Kindermans and Alexandre Tkatchenko and Klaus-Robert M{\""u}ller",2017,,,,
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,chmiela2022accurateglobalmachinelearning,\cite{chmiela2022accurateglobalmachinelearning},Accurate global machine learning force fields for molecules with hundreds of atoms,https://arxiv.org/abs/2209.14865v3,"Global machine learning force fields (MLFFs), that have the capacity to capture collective many-atom interactions in molecular systems, currently only scale up to a few dozen atoms due a considerable growth of the model complexity with system size. For larger molecules, locality assumptions are typically introduced, with the consequence that non-local interactions are poorly or not at all described, even if those interactions are contained within the reference ab initio data. Here, we approach this challenge and develop an exact iterative parameter-free approach to train global symmetric gradient domain machine learning (sGDML) force fields for systems with up to several hundred atoms, without resorting to any localization of atomic interactions or other potentially uncontrolled approximations. This means that all atomic degrees of freedom remain fully correlated in the global sGDML FF, allowing the accurate description of complex molecules and materials that present phenomena with far-reaching characteristic correlation lengths. We assess the accuracy and efficiency of our MLFFs on a newly developed MD22 benchmark dataset containing molecules from 42 to 370 atoms. The robustness of our approach is demonstrated in nanosecond long path-integral molecular dynamics simulations for the supramolecular complexes in the MD22 dataset.",True,True,Stefan Chmiela and Valentin Vassilev-Galindo and Oliver T. Unke and Adil Kabylda and Huziel E. Sauceda and Alexandre Tkatchenko and Klaus-Robert Müller,2022,,https://arxiv.org/abs/2209.14865,,
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,Musaelian2023Allegro,\cite{Musaelian2023Allegro},Learning Local 3D Energetics with Graph Neural Networks,,,True,False,Albert Musaelian and Sebastian Batzner and Anders Johansson and Simon Kozinsky and Boris Kozinsky,2023,,,,Nature Communications
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,Liao2024EquiformerV2,\cite{Liao2024EquiformerV2},EquiformerV2: Improved Equivariant Transformer for Scaling 3D Molecular Learning,,,True,False,Yi-Lun Liao and others,2024,,,,arXiv preprint arXiv:2402.xxxxx
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,Yang2025EfficientEquivariantMLIP,\cite{Yang2025EfficientEquivariantMLIP},Efficient equivariant model for machine learning interatomic potentials,,,True,False,"Yang, Ziduo and Wang, Xian and Li, Yifan and Lv, Qiujie and Chen, Calvin Yu-Chian and Shen, Lei",2025,,https://doi.org/10.1038/s41524-025-01535-3,10.1038/s41524-025-01535-3,npj Computational Materials
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,yuan2025foundation,\cite{yuan2025foundation},Foundation Models for Atomistic Simulation of Chemistry and Materials,,,True,False,"Yuan, Eric C-Y and Liu, Yunsheng and Chen, Junmin and Zhong, Peichen and Raja, Sanjeev and Kreiman, Tobias and Vargas, Santiago and Xu, Wenbin and Head-Gordon, Martin and Yang, Chao and others",2025,,,,arXiv preprint arXiv:2503.10538
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,gasteiger2022directionalmessagepassingmolecular,\cite{gasteiger2022directionalmessagepassingmolecular},Directional Message Passing for Molecular Graphs,,,True,False,Johannes Gasteiger and Janek Groß and Stephan Günnemann,2020,,https://openreview.net/forum?id=B1eWbxStPH,,
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,klicpera2021gemnet,\cite{klicpera2021gemnet},GemNet: Universal Directional Graph Neural Networks for Molecules,https://arxiv.org/abs/2106.08903v10,"Effectively predicting molecular interactions has the potential to accelerate molecular dynamics by multiple orders of magnitude and thus revolutionize chemical simulations. Graph neural networks (GNNs) have recently shown great successes for this task, overtaking classical methods based on fixed molecular kernels. However, they still appear very limited from a theoretical perspective, since regular GNNs cannot distinguish certain types of graphs. In this work we close this gap between theory and practice. We show that GNNs with spherical representations are indeed universal approximators for predictions that are invariant to translation, and equivariant to permutation and rotation. We then discretize such GNNs via directed edge embeddings and two-hop message passing, and incorporate multiple structural improvements to arrive at the geometric message passing neural network (GemNet). We demonstrate the benefits of the proposed changes in multiple ablation studies. GemNet outperforms previous models on the COLL, MD17, and OC20 datasets by 34%, 41%, and 20%, respectively, and performs especially well on the most challenging molecules. Our implementation is available online.",True,True,"Johannes Klicpera and Florian Becker and Stephan G{\""u}nnemann",2021,,https://openreview.net/forum?id=HS_sOaxS9K-,,
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,Batzner2022Equivariant,\cite{Batzner2022Equivariant},E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials,,,True,False,Simon Batzner and Albert Musaelian and Lixin Sun and others,2022,,https://doi.org/10.1038/s41467-022-29939-5,10.1038/s41467-022-29939-5,Nature Communications
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,batatia2023macehigherorderequivariant,\cite{batatia2023macehigherorderequivariant},MACE: Higher Order Equivariant Message Passing Neural Networks for Fast and Accurate Force Fields,https://arxiv.org/abs/2206.07697v2,"Creating fast and accurate force fields is a long-standing challenge in computational chemistry and materials science. Recently, several equivariant message passing neural networks (MPNNs) have been shown to outperform models built using other approaches in terms of accuracy. However, most MPNNs suffer from high computational cost and poor scalability. We propose that these limitations arise because MPNNs only pass two-body messages leading to a direct relationship between the number of layers and the expressivity of the network. In this work, we introduce MACE, a new equivariant MPNN model that uses higher body order messages. In particular, we show that using four-body messages reduces the required number of message passing iterations to just two, resulting in a fast and highly parallelizable model, reaching or exceeding state-of-the-art accuracy on the rMD17, 3BPA, and AcAc benchmark tasks. We also demonstrate that using higher order messages leads to an improved steepness of the learning curves.",True,True,Ilyes Batatia and David Peter Kovacs and Gregor N. C. Simm and Christoph Ortner and Gabor Csanyi,2022,,https://openreview.net/forum?id=YPpSngE-ZU,,
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,satorras2022en,\cite{satorras2022en},Equivariant Polynomials for Graph Neural Networks,https://arxiv.org/abs/2302.11556v2,"Graph Neural Networks (GNN) are inherently limited in their expressive power. Recent seminal works (Xu et al., 2019; Morris et al., 2019b) introduced the Weisfeiler-Lehman (WL) hierarchy as a measure of expressive power. Although this hierarchy has propelled significant advances in GNN analysis and architecture developments, it suffers from several significant limitations. These include a complex definition that lacks direct guidance for model improvement and a WL hierarchy that is too coarse to study current GNNs. This paper introduces an alternative expressive power hierarchy based on the ability of GNNs to calculate equivariant polynomials of a certain degree. As a first step, we provide a full characterization of all equivariant graph polynomials by introducing a concrete basis, significantly generalizing previous results. Each basis element corresponds to a specific multi-graph, and its computation over some graph data input corresponds to a tensor contraction problem. Second, we propose algorithmic tools for evaluating the expressiveness of GNNs using tensor contraction sequences, and calculate the expressive power of popular GNNs. Finally, we enhance the expressivity of common GNN architectures by adding polynomial features or additional operations / aggregations inspired by our theory. These enhanced GNNs demonstrate state-of-the-art results in experiments across multiple graph learning benchmarks.",True,True,Victor Garcia Satorras and Emiel Hoogeboom and Max Welling,2021,,,,
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,pmlr-v202-passaro23a,\cite{pmlr-v202-passaro23a},Reducing {SO}(3) Convolutions to {SO}(2) for Efficient Equivariant {GNN}s,,,True,False,"Passaro, Saro and Zitnick, C. Lawrence",2023,,,,
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,fu2025learning,\cite{fu2025learning},Learning Smooth and Expressive Interatomic Potentials for Physical Property Prediction,,,True,False,Xiang Fu and Brandon M Wood and Luis Barroso-Luque and Daniel S. Levine and Meng Gao and Misko Dzamba and C. Lawrence Zitnick,2025,,https://openreview.net/forum?id=R0PBjxIbgm,,
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,rhodes2025orbv3atomisticsimulationscale,\cite{rhodes2025orbv3atomisticsimulationscale},Orb-v3: atomistic simulation at scale,https://arxiv.org/abs/2504.06231v2,"We introduce Orb-v3, the next generation of the Orb family of universal interatomic potentials. Models in this family expand the performance-speed-memory Pareto frontier, offering near SoTA performance across a range of evaluations with a >10x reduction in latency and > 8x reduction in memory. Our experiments systematically traverse this frontier, charting the trade-off induced by roto-equivariance, conservatism and graph sparsity. Contrary to recent literature, we find that non-equivariant, non-conservative architectures can accurately model physical properties, including those which require higher-order derivatives of the potential energy surface.
  This model release is guided by the principle that the most valuable foundation models for atomic simulation will excel on all fronts: accuracy, latency and system size scalability. The reward for doing so is a new era of computational chemistry driven by high-throughput and mesoscale all-atom simulations.",True,True,Benjamin Rhodes and Sander Vandenhaute and Vaidotas Šimkus and James Gin and Jonathan Godwin and Tim Duignan and Mark Neumann,2025,,https://arxiv.org/abs/2504.06231,,
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,10.5555/3524938.3525722,\cite{10.5555/3524938.3525722},Learning to simulate complex physics with graph networks,,,True,False,"Sanchez-Gonzalez, Alvaro and Godwin, Jonathan and Pfaff, Tobias and Ying, Rex and Leskovec, Jure and Battaglia, Peter W.",2020,,,,
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,inoue2018dataaugmentationpairingsamples,\cite{inoue2018dataaugmentationpairingsamples},Data Augmentation by Pairing Samples for Images Classification,,,True,False,Hiroshi Inoue,2018,,https://arxiv.org/abs/1801.02929,,
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,dosovitskiy2021an,\cite{dosovitskiy2021an},An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,https://arxiv.org/abs/2010.11929v2,"While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",True,True,Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby,2021,,https://openreview.net/forum?id=YicbFdNTTy,,
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,rahat2024dataaugmentationimageclassification,\cite{rahat2024dataaugmentationimageclassification},Data Augmentation for Image Classification using Generative AI,,,True,False,Fazle Rahat and M Shifat Hossain and Md Rubel Ahmed and Sumit Kumar Jha and Rickard Ewetz,2024,,https://arxiv.org/abs/2409.00547,,
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,negassi2021smartsamplingaugmentoptimalefficientdata,\cite{negassi2021smartsamplingaugmentoptimalefficientdata},Smart(Sampling)Augment: Optimal and Efficient Data Augmentation for Semantic Segmentation,,,True,False,"Negassi, Misgana and Wagner, Diane and Reiterer, Alexander",2022,,https://www.mdpi.com/1999-4893/15/5/165,10.3390/a15050165,Algorithms
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,yu2024diffusionbaseddataaugmentationnuclei,\cite{yu2024diffusionbaseddataaugmentationnuclei},Diffusion-Based Data Augmentation for Nuclei Image Segmentation,,,True,False,"Yu, Xinyi
and Li, Guanbin
and Lou, Wei
and Liu, Siqi
and Wan, Xiang
and Chen, Yan
and Li, Haofeng",2023,,,,
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,Abramson2024x,\cite{Abramson2024x},Accurate structure prediction of biomolecular interactions with AlphaFold 3,,,True,False,Jacob Abramson and Jane Adler and John Dunger and others,2024,,https://doi.org/10.1038/s41586-024-07487-w,10.1038/s41586-024-07487-w,Nature
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,wang2024swallowingbitterpillsimplified,\cite{wang2024swallowingbitterpillsimplified},Swallowing the Bitter Pill: Simplified Scalable Conformer Generation,,,True,False,"Wang, Yuyang and Elhag, Ahmed A. and Jaitly, Navdeep and Susskind, Joshua M. and Bautista, Miguel {\'A}ngel",2024,,,,
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,zhang2025symdiff,\cite{zhang2025symdiff},SymDiff: Equivariant Diffusion via Stochastic Symmetrisation,https://arxiv.org/abs/2410.06262v2,"We propose SymDiff, a method for constructing equivariant diffusion models using the framework of stochastic symmetrisation. SymDiff resembles a learned data augmentation that is deployed at sampling time, and is lightweight, computationally efficient, and easy to implement on top of arbitrary off-the-shelf models. In contrast to previous work, SymDiff typically does not require any neural network components that are intrinsically equivariant, avoiding the need for complex parameterisations or the use of higher-order geometric features. Instead, our method can leverage highly scalable modern architectures as drop-in replacements for these more constrained alternatives. We show that this additional flexibility yields significant empirical benefit for $\mathrm{E}(3)$-equivariant molecular generation. To the best of our knowledge, this is the first application of symmetrisation to generative modelling, suggesting its potential in this domain more generally.",True,True,Leo Zhang and Kianoosh Ashouritaklimi and Yee Whye Teh and Rob Cornish,2025,,https://openreview.net/forum?id=i1NNCrRxdM,,
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,joshi2025allatom,\cite{joshi2025allatom},All-atom Diffusion Transformers: Unified generative modelling of molecules and materials,https://arxiv.org/abs/2503.03965v2,"Diffusion models are the standard toolkit for generative modelling of 3D atomic systems. However, for different types of atomic systems -- such as molecules and materials -- the generative processes are usually highly specific to the target system despite the underlying physics being the same. We introduce the All-atom Diffusion Transformer (ADiT), a unified latent diffusion framework for jointly generating both periodic materials and non-periodic molecular systems using the same model: (1) An autoencoder maps a unified, all-atom representations of molecules and materials to a shared latent embedding space; and (2) A diffusion model is trained to generate new latent embeddings that the autoencoder can decode to sample new molecules or materials. Experiments on MP20, QM9 and GEOM-DRUGS datasets demonstrate that jointly trained ADiT generates realistic and valid molecules as well as materials, obtaining state-of-the-art results on par with molecule and crystal-specific models. ADiT uses standard Transformers with minimal inductive biases for both the autoencoder and diffusion model, resulting in significant speedups during training and inference compared to equivariant diffusion models. Scaling ADiT up to half a billion parameters predictably improves performance, representing a step towards broadly generalizable foundation models for generative chemistry. Open source code: https://github.com/facebookresearch/all-atom-diffusion-transformer",True,True,Chaitanya K. Joshi and Xiang Fu and Yi-Lun Liao and Vahe Gharakhanyan and Benjamin Kurt Miller and Anuroop Sriram and Zachary W. Ulissi,2025,,,,
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,puny2022frameaveraginginvariantequivariant,\cite{puny2022frameaveraginginvariantequivariant},Frame Averaging for Invariant and Equivariant Network Design,,,True,False,Omri Puny and Matan Atzmon and Edward J. Smith and Ishan Misra and Aditya Grover and Heli Ben-Hamu and Yaron Lipman,2022,,https://openreview.net/forum?id=zIUyj55nXR,,
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,pmlr-v202-duval23a,\cite{pmlr-v202-duval23a},{FAEN}et: Frame Averaging Equivariant {GNN} for Materials Modeling,,,True,False,"Duval, Alexandre Agm and Schmidt, Victor and Hern\'{a}ndez-Garc\'{\i}a, Alex and Miret, Santiago and Malliaros, Fragkiskos D. and Bengio, Yoshua and Rolnick, David",2023,,https://proceedings.mlr.press/v202/duval23a.html,,
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,lin2024equivariance,\cite{lin2024equivariance},Equivariance via Minimal Frame Averaging for More Symmetries and Efficiency,https://arxiv.org/abs/2406.07598v4,"We consider achieving equivariance in machine learning systems via frame averaging. Current frame averaging methods involve a costly sum over large frames or rely on sampling-based approaches that only yield approximate equivariance. Here, we propose Minimal Frame Averaging (MFA), a mathematical framework for constructing provably minimal frames that are exactly equivariant. The general foundations of MFA also allow us to extend frame averaging to more groups than previously considered, including the Lorentz group for describing symmetries in space-time, and the unitary group for complex-valued domains. Results demonstrate the efficiency and effectiveness of encoding symmetries via MFA across a diverse range of tasks, including $n$-body simulation, top tagging in collider physics, and relaxed energy prediction. Our code is available at https://github.com/divelab/MFA.",True,True,Yuchao Lin and Jacob Helwig and Shurui Gui and Shuiwang Ji,2024,,https://openreview.net/forum?id=guFsTBXsov,,
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,huang2024proteinnucleicacidcomplexmodeling,\cite{huang2024proteinnucleicacidcomplexmodeling},Protein-Nucleic Acid Complex Modeling with Frame Averaging Transformer,https://arxiv.org/abs/2406.09586v3,"Nucleic acid-based drugs like aptamers have recently demonstrated great therapeutic potential. However, experimental platforms for aptamer screening are costly, and the scarcity of labeled data presents a challenge for supervised methods to learn protein-aptamer binding. To this end, we develop an unsupervised learning approach based on the predicted pairwise contact map between a protein and a nucleic acid and demonstrate its effectiveness in protein-aptamer binding prediction. Our model is based on FAFormer, a novel equivariant transformer architecture that seamlessly integrates frame averaging (FA) within each transformer block. This integration allows our model to infuse geometric information into node features while preserving the spatial semantics of coordinates, leading to greater expressive power than standard FA models. Our results show that FAFormer outperforms existing equivariant models in contact map prediction across three protein complex datasets, with over 10% relative improvement. Moreover, we curate five real-world protein-aptamer interaction datasets and show that the contact map predicted by FAFormer serves as a strong binding indicator for aptamer screening.",True,True,Tinglin Huang and Zhenqiao Song and Rex Ying and Wengong Jin,2024,,https://openreview.net/forum?id=Xngi3Z3wkN,,
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,DBLP:conf/icml/DymLS24,\cite{DBLP:conf/icml/DymLS24},Equivariant Frames and the Impossibility of Continuous Canonicalization,,,True,False,Nadav Dym and Hannah Lawrence and Jonathan W. Siegel,2024,,https://openreview.net/forum?id=4iy0q0carb,,
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,kaba2022equivariance,\cite{kaba2022equivariance},Equivariance With Learned Canonicalization Functions,,,True,False,S{\'e}kou-Oumar Kaba and Arnab Kumar Mondal and Yan Zhang and Yoshua Bengio and Siamak Ravanbakhsh,2022,,https://openreview.net/forum?id=pVD1k8ge25a,,
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,baker2024an,\cite{baker2024an},An Explicit Frame Construction for Normalizing 3D Point Clouds,,,True,False,Justin Baker and Shih-Hsin Wang and Tommaso de Fernex and Bao Wang,2024,,https://openreview.net/forum?id=SZ0JnRxi0x,,
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,ma2024canonizationperspectiveinvariantequivariant,\cite{ma2024canonizationperspectiveinvariantequivariant},A Canonicalization Perspective on Invariant and Equivariant Learning,,,True,False,George Ma and Yifei Wang and Derek Lim and Stefanie Jegelka and Yisen Wang,2024,,https://openreview.net/forum?id=jjcY92FX4R,,
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,ICLR2025_db7534a0,\cite{ICLR2025_db7534a0},Beyond Canonicalization: How Tensorial Messages Improve Equivariant Message Passing,https://arxiv.org/abs/2405.15389v3,"In numerous applications of geometric deep learning, the studied systems exhibit spatial symmetries and it is desirable to enforce these. For the symmetry of global rotations and reflections, this means that the model should be equivariant with respect to the transformations that form the group of $\mathrm O(d)$. While many approaches for equivariant message passing require specialized architectures, including non-standard normalization layers or non-linearities, we here present a framework based on local reference frames (""local canonicalization"") which can be integrated with any architecture without restrictions. We enhance equivariant message passing based on local canonicalization by introducing tensorial messages to communicate geometric information consistently between different local coordinate frames. Our framework applies to message passing on geometric data in Euclidean spaces of arbitrary dimension. We explicitly show how our approach can be adapted to make a popular existing point cloud architecture equivariant. We demonstrate the superiority of tensorial messages and achieve state-of-the-art results on normal vector regression and competitive results on other standard 3D point cloud tasks.",True,True,"Lippmann, Peter and Gerhartz, Gerrit and Remme, Roman and Hamprecht, Fred A",2025,,https://proceedings.iclr.cc/paper_files/paper/2025/file/db7534a06ace69f4ec95bc89e91d5dbb-Paper-Conference.pdf,,
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,qu2024importance,\cite{qu2024importance},The importance of being scalable: Improving the speed and accuracy of neural network interatomic potentials across chemical domains,,,True,False,"Qu, Eric and Krishnapriyan, Aditi",2024,,,,Advances in Neural Information Processing Systems
Learning Inter-Atomic Potentials without Explicit Equivariance,2510.00027v2,mazitov2025petmadlightweightuniversalinteratomic,\cite{mazitov2025petmadlightweightuniversalinteratomic},"PET-MAD, a lightweight universal interatomic potential for advanced materials modeling",https://arxiv.org/abs/2503.14118v2,"Machine-learning interatomic potentials (MLIPs) have greatly extended the reach of atomic-scale simulations, offering the accuracy of first-principles calculations at a fraction of the cost. Leveraging large quantum mechanical databases and expressive architectures, recent ''universal'' models deliver qualitative accuracy across the periodic table but are often biased toward low-energy configurations. We introduce PET-MAD, a generally applicable MLIP trained on a dataset combining stable inorganic and organic solids, systematically modified to enhance atomic diversity. Using a moderate but highly-consistent level of electronic-structure theory, we assess PET-MAD's accuracy on established benchmarks and advanced simulations of six materials. Despite the small training set and lightweight architecture, PET-MAD is competitive with state-of-the-art MLIPs for inorganic solids, while also being reliable for molecules, organic materials, and surfaces. It is stable and fast, enabling the near-quantitative study of thermal and quantum mechanical fluctuations, functional properties, and phase transitions out of the box. It can be efficiently fine-tuned to deliver full quantum mechanical accuracy with a minimal number of targeted calculations.",True,True,Arslan Mazitov and Filippo Bigi and Matthias Kellner and Paolo Pegolo and Davide Tisi and Guillaume Fraux and Sergey Pozdnyakov and Philip Loche and Michele Ceriotti,2025,,https://arxiv.org/abs/2503.14118,,
Brain Harmony: A Multimodal Foundation Model Unifying Morphology and Function into 1D Tokens,2509.24693v1,carobrainlm,\cite{carobrainlm},BrainLM: A foundation model for brain activity recordings,,,True,False,"Caro, Josue Ortega and de Oliveira Fonseca, Antonio Henrique and Rizvi, Syed A and Rosati, Matteo and Averill, Christopher and Cross, James L and Mittal, Prateek and Zappala, Emanuele and Dhodapkar, Rahul Madhav and Abdallah, Chadi and others",,,,,
Brain Harmony: A Multimodal Foundation Model Unifying Morphology and Function into 1D Tokens,2509.24693v1,dong2024brain,\cite{dong2024brain},Brain-JEPA: Brain Dynamics Foundation Model with Gradient Positioning and Spatiotemporal Masking,https://arxiv.org/abs/2409.19407v1,"We introduce Brain-JEPA, a brain dynamics foundation model with the Joint-Embedding Predictive Architecture (JEPA). This pioneering model achieves state-of-the-art performance in demographic prediction, disease diagnosis/prognosis, and trait prediction through fine-tuning. Furthermore, it excels in off-the-shelf evaluations (e.g., linear probing) and demonstrates superior generalizability across different ethnic groups, surpassing the previous large model for brain activity significantly. Brain-JEPA incorporates two innovative techniques: Brain Gradient Positioning and Spatiotemporal Masking. Brain Gradient Positioning introduces a functional coordinate system for brain functional parcellation, enhancing the positional encoding of different Regions of Interest (ROIs). Spatiotemporal Masking, tailored to the unique characteristics of fMRI data, addresses the challenge of heterogeneous time-series patches. These methodologies enhance model performance and advance our understanding of the neural circuits underlying cognition. Overall, Brain-JEPA is paving the way to address pivotal questions of building brain functional coordinate system and masking brain activity at the AI-neuroscience interface, and setting a potentially new paradigm in brain activity analysis through downstream adaptation.",True,True,"Dong, Zijian and Li, Ruilin and Wu, Yilei and Nguyen, Thuan Tinh and Chong, Joanna and Ji, Fang and Tong, Nathanael and Chen, Christopher and Zhou, Juan Helen",2024,,,,Advances in Neural Information Processing Systems
Brain Harmony: A Multimodal Foundation Model Unifying Morphology and Function into 1D Tokens,2509.24693v1,park2025foundational,\cite{park2025foundational},A Foundational Brain Dynamics Model via Stochastic Optimal Control,https://arxiv.org/abs/2502.04892v1,"We introduce a foundational model for brain dynamics that utilizes stochastic optimal control (SOC) and amortized inference. Our method features a continuous-discrete state space model (SSM) that can robustly handle the intricate and noisy nature of fMRI signals. To address computational limitations, we implement an approximation strategy grounded in the SOC framework. Additionally, we present a simulation-free latent dynamics approach that employs locally linear approximations, facilitating efficient and scalable inference. For effective representation learning, we derive an Evidence Lower Bound (ELBO) from the SOC formulation, which integrates smoothly with recent advancements in self-supervised learning (SSL), thereby promoting robust and transferable representations. Pre-trained on extensive datasets such as the UKB, our model attains state-of-the-art results across a variety of downstream tasks, including demographic prediction, trait analysis, disease diagnosis, and prognosis. Moreover, evaluating on external datasets such as HCP-A, ABIDE, and ADHD200 further validates its superior abilities and resilience across different demographic and clinical distributions. Our foundational model provides a scalable and efficient approach for deciphering brain dynamics, opening up numerous applications in neuroscience.",True,True,"Park, Joonhyeong and Park, Byoungwoo and Bang, Chang-Bae and Choi, Jungwon and Chung, Hyungjin and Kim, Byung-Hoon and Lee, Juho",2025,,,,arXiv preprint arXiv:2502.04892
Brain Harmony: A Multimodal Foundation Model Unifying Morphology and Function into 1D Tokens,2509.24693v1,yang2024brainmass,\cite{yang2024brainmass},BrainMass: Advancing Brain Network Analysis for Diagnosis with Large-scale Self-Supervised Learning,https://arxiv.org/abs/2403.01433v1,"Foundation models pretrained on large-scale datasets via self-supervised learning demonstrate exceptional versatility across various tasks. Due to the heterogeneity and hard-to-collect medical data, this approach is especially beneficial for medical image analysis and neuroscience research, as it streamlines broad downstream tasks without the need for numerous costly annotations. However, there has been limited investigation into brain network foundation models, limiting their adaptability and generalizability for broad neuroscience studies. In this study, we aim to bridge this gap. In particular, (1) we curated a comprehensive dataset by collating images from 30 datasets, which comprises 70,781 samples of 46,686 participants. Moreover, we introduce pseudo-functional connectivity (pFC) to further generates millions of augmented brain networks by randomly dropping certain timepoints of the BOLD signal. (2) We propose the BrainMass framework for brain network self-supervised learning via mask modeling and feature alignment. BrainMass employs Mask-ROI Modeling (MRM) to bolster intra-network dependencies and regional specificity. Furthermore, Latent Representation Alignment (LRA) module is utilized to regularize augmented brain networks of the same participant with similar topological properties to yield similar latent representations by aligning their latent embeddings. Extensive experiments on eight internal tasks and seven external brain disorder diagnosis tasks show BrainMass's superior performance, highlighting its significant generalizability and adaptability. Nonetheless, BrainMass demonstrates powerful few/zero-shot learning abilities and exhibits meaningful interpretation to various diseases, showcasing its potential use for clinical applications.",True,True,"Yang, Yanwu and Ye, Chenfei and Su, Guinan and Zhang, Ziyao and Chang, Zhikai and Chen, Hairui and Chan, Piu and Yu, Yue and Ma, Ting",2024,,,,IEEE Transactions on Medical Imaging
Brain Harmony: A Multimodal Foundation Model Unifying Morphology and Function into 1D Tokens,2509.24693v1,rui2024brainmvp,\cite{rui2024brainmvp},BrainMVP: Multi-modal Vision Pre-training for Brain Image Analysis using Multi-parametric MRI,,,True,False,"Rui, Shaohao and Chen, Lingzhi and Tang, Zhenyu and Wang, Lilong and Liu, Mianxin and Zhang, Shaoting and Wang, Xiaosong",2024,,,,arXiv preprint arXiv:2410.10604
Pharmacophore-Guided Generative Design of Novel Drug-Like Molecules,2510.01480v1,zhu2023pgmg,\cite{zhu2023pgmg},PGMG: Pharmacophore-guided molecule generation using graph neural networks,,,True,False,"Zhu, L. and others",2023,,,,Journal of Medicinal Chemistry
Pharmacophore-Guided Generative Design of Novel Drug-Like Molecules,2510.01480v1,seo2024pharmaconet,\cite{seo2024pharmaconet},PharmacoNet: automated pharmacophore modeling and scoring,,,True,False,"Seo, K. and Kim, H.",2024,,,,Bioinformatics
Pharmacophore-Guided Generative Design of Novel Drug-Like Molecules,2510.01480v1,yu2025diffphore,\cite{yu2025diffphore},DiffPhore: Diffusion-based pharmacophore-guided lead discovery,,,True,False,"Yu, H. and others",2025,,,,Nature Machine Intelligence
Pharmacophore-Guided Generative Design of Novel Drug-Like Molecules,2510.01480v1,moyano-gomez2024olap,\cite{moyano-gomez2024olap},O-LAP: Cavity-filling pseudo-ligands for docking rescoring,,,True,False,"Moyano-Gómez, P. and others",2024,,,,Journal of Chemical Information and Modeling
Pharmacophore-Guided Generative Design of Novel Drug-Like Molecules,2510.01480v1,alakhdar2025pharmadiff,\cite{alakhdar2025pharmadiff},PharmaDiff: Pharmacophore-conditioned diffusion models for drug discovery,,,True,False,"Alakhdar, R. and others",2025,,,,ACS Central Science
Transforming Multi-Omics Integration with GANs: Applications in Alzheimer's and Cancer,2510.19870v1,Chakraborty-24,\cite{Chakraborty-24},Multi-OMICS approaches in cancer biology: New era in cancer therapy,,,True,False,"S. Chakraborty and G, Sharma and  S. Karmakar and   S, Banerjee S.",2024,,,,Biochim Biophys Acta Mol Basis Dis.
Transforming Multi-Omics Integration with GANs: Applications in Alzheimer's and Cancer,2510.19870v1,Ballard-24,\cite{Ballard-24},Deep learning-based approaches for multi-omics data integration and analysis,,,True,False,Ballard JL and Wang Z Li W and Shen L and  Long Q.,2024,,,,BioData Min.
Transforming Multi-Omics Integration with GANs: Applications in Alzheimer's and Cancer,2510.19870v1,baryshnikova2022iomics,\cite{baryshnikova2022iomics},iOmicsPASS: network-based integration of multiomics data for predictive subnetwork discovery,,,True,False,"Baryshnikova, Anastasia and Wolf, Dina and Hoadley, Katherine A. and Sander, Chris and Serafini, Michael and Gross, Benjamin and Mukherjee, Sayan",2022,,https://doi.org/10.1093/nar/gkac704,10.1093/nar/gkac704,Nucleic Acids Research
Transforming Multi-Omics Integration with GANs: Applications in Alzheimer's and Cancer,2510.19870v1,Alam-16a,\cite{Alam-16a},Influence Function of Multiple Kernel Canonical Analysis to Identify Outliers in Imaging Genetics Data,,,True,False,"Alam, Md Ashad and   V. Calhoun and Y. P. Wang",2016,,,,"Proceedings of 7th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics (ACM BCB),Seattle, WA, USA"
Transforming Multi-Omics Integration with GANs: Applications in Alzheimer's and Cancer,2510.19870v1,Alam-16b,\cite{Alam-16b},Robust Kernel Canonical Correlation Analysis to Detect Gene-Gene Interaction for Imaging Genetics Data,,,True,False,"Alam, Md Ashad and O. Komori and  V. Calhoun and Y. P. Wang",2016,,,,"Proceedings of 7th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics (ACM BCB),Seattle, WA, USA"
Transforming Multi-Omics Integration with GANs: Applications in Alzheimer's and Cancer,2510.19870v1,Ashad-13,\cite{Ashad-13},Higher-order regularized kernel {{CCA}},,,True,False,"Alam, Md Ashad and Fukumizu, Kenji",2013,,,,12th International Conference on Machine Learning and Applications
Transforming Multi-Omics Integration with GANs: Applications in Alzheimer's and Cancer,2510.19870v1,Alam-19,\cite{Alam-19},Robust kernel canonical correlation analysis to detect gene-gene co-associations: A case study in genetics,,,True,False,"Alam, Md Ashad and Komori, Osamu and Deng, Hong-Wen and Calhoun, Vince D and Wang, Yu-Ping",2019,,,,Journal of bioinformatics and computational biology
Transforming Multi-Omics Integration with GANs: Applications in Alzheimer's and Cancer,2510.19870v1,Ashad-10,\cite{Ashad-10},"A Comparative Study of Kernel and Robust
Canonical Correlation Analysis",,,True,False,"Alam, Md Ashad and  Nasser, Mohammed and Fukumizu kenji",2010,,,,Journal of Multimedia.
Transforming Multi-Omics Integration with GANs: Applications in Alzheimer's and Cancer,2510.19870v1,Ashad-14T,\cite{Ashad-14T},Kernel Choice for Unsupervised Kernel Methods,,,True,False,"Alam, Md Ashad",2014,,,,
Transforming Multi-Omics Integration with GANs: Applications in Alzheimer's and Cancer,2510.19870v1,huang2019salmon,\cite{huang2019salmon},SALMON: Survival Analysis Learning With Multi-Omics Neural Networks on Breast Cancer,,,True,False,"Huang, Zhiqiang and Zhan, Xianfeng and Xiang, Shuang and Johnson, Trevor S. and Helm, Benjamin and Yu, Chen Yuan and others",2019,,,Insert DOI if available,Frontiers in Genetics
Transforming Multi-Omics Integration with GANs: Applications in Alzheimer's and Cancer,2510.19870v1,hao2019gene,\cite{hao2019gene},Gene- and Pathway-Based Deep Neural Network for Multi-omics Data Integration to Predict Cancer Survival Outcomes,,,True,False,"Hao, Jianwen and Masum, Mohammad and Oh, Ji Hyun and Kang, Min",2019,,,Insert DOI if available,
Transforming Multi-Omics Integration with GANs: Applications in Alzheimer's and Cancer,2510.19870v1,zhao2021deepomix,\cite{zhao2021deepomix},DeepOmix: A Scalable and Interpretable Multi-Omics Deep Learning Framework and Application in Cancer Survival Analysis,,,True,False,"Zhao, Lin and Dong, Qiqi and Luo, Chen and Wu, Yujie and Bu, Dongbo and Qi, Xuefeng and others",2021,,,Insert DOI if available,Computational and Structural Biotechnology Journal
Transforming Multi-Omics Integration with GANs: Applications in Alzheimer's and Cancer,2510.19870v1,li2022mogcn,\cite{li2022mogcn},MoGCN: A Multi-Omics Integration Method Based on Graph Convolutional Network for Cancer Subtype Analysis,,,True,False,"Li, Xin and Ma, Jie and Leng, Lin and Han, Meng and Li, Meng and He, Fang and others",2022,,,Insert DOI if available,Frontiers in Genetics
Transforming Multi-Omics Integration with GANs: Applications in Alzheimer's and Cancer,2510.19870v1,peng2022drugresponse,\cite{peng2022drugresponse},Predicting Drug Response Based on Multi-Omics Fusion and Graph Convolution,,,True,False,"Peng, Wei and Chen, Ting and Dai, Wei",2022,,,Insert DOI if available,IEEE
Transforming Multi-Omics Integration with GANs: Applications in Alzheimer's and Cancer,2510.19870v1,althubaiti2021deepmocca,\cite{althubaiti2021deepmocca},DeepMOCCA: A Pan-Cancer Prognostic Model Identifies Personalized Prognostic Markers Through Graph Attention and Multi-Omics Data Integration,,,True,False,"Althubaiti, Saad and Kulmanov, Maxat and Liu, Yang and Gkoutos, Georgios V. and Schofield, Paul and Hoehndorf, Robert",2021,,,https://doi.org/10.1101/2021.03.02.433454,bioRxiv
Transforming Multi-Omics Integration with GANs: Applications in Alzheimer's and Cancer,2510.19870v1,mitra2020multiview,\cite{mitra2020multiview},Multi-View Clustering for Multi-Omics Data Using Unified Embedding,,,True,False,"Mitra, Subhajit and Saha, Suman and Hasanuzzaman, Mohammad",2020,,,Insert DOI if available,Scientific Reports
Transforming Multi-Omics Integration with GANs: Applications in Alzheimer's and Cancer,2510.19870v1,zuo2021deep,\cite{zuo2021deep},Deep Cross-Omics Cycle Attention Model for Joint Analysis of Single-Cell Multi-Omics Data,,,True,False,"Zuo, Chenyu and Dai, Haiping and Chen, Liang",2021,,,Insert DOI if available,Bioinformatics
Which Similarity-Sensitive Entropy?,2511.03849v2,shannon1948mathematical,\cite{shannon1948mathematical},A Mathematical Theory of Semantic Communication,https://arxiv.org/abs/2401.13387v2,"The year 1948 witnessed the historic moment of the birth of classic information theory (CIT). Guided by CIT, modern communication techniques have approached the theoretic limitations, such as, entropy function $H(U)$, channel capacity $C=\max_{p(x)}I(X;Y)$ and rate-distortion function $R(D)=\min_{p(\hat{x}|x):\mathbb{E}d(x,\hat{x})\leq D} I(X;\hat{X})$. Semantic communication paves a new direction for future communication techniques whereas the guided theory is missed. In this paper, we try to establish a systematic framework of semantic information theory (SIT). We investigate the behavior of semantic communication and find that synonym is the basic feature so we define the synonymous mapping between semantic information and syntactic information. Stemming from this core concept, synonymous mapping $f$, we introduce the measures of semantic information, such as semantic entropy $H_s(\tilde{U})$, up/down semantic mutual information $I^s(\tilde{X};\tilde{Y})$ $(I_s(\tilde{X};\tilde{Y}))$, semantic capacity $C_s=\max_{f_{xy}}\max_{p(x)}I^s(\tilde{X};\tilde{Y})$, and semantic rate-distortion function $R_s(D)=\min_{\{f_x,f_{\hat{x}}\}}\min_{p(\hat{x}|x):\mathbb{E}d_s(\tilde{x},\hat{\tilde{x}})\leq D}I_s(\tilde{X};\hat{\tilde{X}})$. Furthermore, we prove three coding theorems of SIT by using random coding and (jointly) typical decoding/encoding, that is, the semantic source coding theorem, semantic channel coding theorem, and semantic rate-distortion coding theorem. We find that the limits of SIT are extended by using synonymous mapping, that is, $H_s(\tilde{U})\leq H(U)$, $C_s\geq C$ and $R_s(D)\leq R(D)$. All these works composite the basis of semantic information theory. In addition, we discuss the semantic information measures in the continuous case. For the band-limited Gaussian channel, we obtain a new channel capacity formula, $C_s=B\log\left[S^4\left(1+\frac{P}{N_0B}\right)\right]$.",True,True,"Shannon, Claude E",1948,,,,The Bell system technical journal
Which Similarity-Sensitive Entropy?,2511.03849v2,renyiMeasuresEntropyInformation1961,\cite{renyiMeasuresEntropyInformation1961},On {Measures} of {Entropy} and {Information},,,True,False,"Rényi, Alfréd",1961,,https://projecteuclid.org/ebooks/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-Fourth-Berkeley-Symposium-on-Mathematical-Statistics-and/chapter/On-Measures-of-Entropy-and-Information/bsmsp/1200512181,,
Which Similarity-Sensitive Entropy?,2511.03849v2,jost2006entropy,\cite{jost2006entropy},Entropy and diversity,,,True,False,"Jost, Lou",2006,,,,Oikos
Which Similarity-Sensitive Entropy?,2511.03849v2,hill1973diversity,\cite{hill1973diversity},Diversity and evenness: a unifying notation and its consequences,,,True,False,"Hill, Mark O",1973,,,,Ecology
Which Similarity-Sensitive Entropy?,2511.03849v2,tsallis1988possible,\cite{tsallis1988possible},Possible generalization of Boltzmann-Gibbs statistics,,,True,False,"Tsallis, Constantino",1988,,,,Journal of statistical physics
Which Similarity-Sensitive Entropy?,2511.03849v2,leinsterEntropyDiversityAxiomatic2020,\cite{leinsterEntropyDiversityAxiomatic2020},Entropy and {Diversity}: {The} {Axiomatic} {Approach},,,True,False,"Leinster, Tom",2020,,,,arXiv preprint arXiv:2012.02113
Which Similarity-Sensitive Entropy?,2511.03849v2,leinster2012measuring,\cite{leinster2012measuring},Measuring diversity: the importance of species similarity,,,True,False,"Leinster, Tom and Cobbold, Christina A",2012,,,,Ecology
Which Similarity-Sensitive Entropy?,2511.03849v2,chao2010phylogenetic,\cite{chao2010phylogenetic},Phylogenetic diversity measures based on Hill numbers,,,True,False,"Chao, Anne and Chiu, Chun-Huo and Jost, Lou",2010,,,,Philosophical Transactions of the Royal Society B: Biological Sciences
Which Similarity-Sensitive Entropy?,2511.03849v2,chaoAttributediversityApproachFunctional2019,\cite{chaoAttributediversityApproachFunctional2019},"An attribute-diversity approach to functional diversity, functional beta diversity, and related (dis)similarity measures",,,True,False,"Chao, Anne and Chiu, Chun-Huo and Villéger, Sébastien and Sun, I Fang and Thorn, Simon and Lin, Yiching and Chiang, Jyh-Min and B. Sherwin, William",2019,,,10.1002/ecm.1343,Ecological Monographs
Which Similarity-Sensitive Entropy?,2511.03849v2,aroraRepertoirescaleMeasuresAntigen2022,\cite{aroraRepertoirescaleMeasuresAntigen2022},Repertoire-scale measures of antigen binding,,,True,False,"Arora, Rohit and Arnaout, Ramy",2022,,https://pnas.org/doi/full/10.1073/pnas.2203505119,10.1073/pnas.2203505119,Proceedings of the National Academy of Sciences
Which Similarity-Sensitive Entropy?,2511.03849v2,nguyen2023greylock,\cite{nguyen2023greylock},greylock: A python package for measuring the composition of complex datasets,,,True,False,"Nguyen, Phuc and Arora, Rohit and Hill, Elliot D and Braun, Jasper and Morgan, Alexandra and Quintana, Liza M and Mazzoni, Gabrielle and Lee, Ghee Rye and Arnaout, Rima and Arnaout, Ramy",2023,,,,ArXiv
Which Similarity-Sensitive Entropy?,2511.03849v2,couch2024beyond,\cite{couch2024beyond},Beyond Size and Class Balance: Alpha as a New Dataset Quality Metric for Deep Learning,,,True,False,"Couch, Josiah and Arnaout, Rima and Arnaout, Ramy",2024,,,,ArXiv
Which Similarity-Sensitive Entropy?,2511.03849v2,pasarkar2023cousins,\cite{pasarkar2023cousins},Cousins of the vendi score: A family of similarity-based diversity metrics for science and machine learning,,,True,False,"Pasarkar, Amey P and Dieng, Adji Bousso",2023,,,,arXiv preprint arXiv:2310.12952
Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,2511.05531v1,Lynall9477,\cite{Lynall9477},Functional Connectivity and Brain Networks in Schizophrenia,,,True,False,"Lynall, Mary-Ellen and Bassett, Danielle S. and Kerwin, Robert and McKenna, Peter J. and Kitzbichler, Manfred and Muller, Ulrich and Bullmore, Ed",2010,,,,Journal of Neuroscience
Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,2511.05531v1,hasanzadeh2020graph,\cite{hasanzadeh2020graph},Graph theory analysis of directed functional brain networks in major depressive disorder based on {EEG} signal,,,True,False,"Hasanzadeh, Fatemeh and Mohebbi, Maryam and Rostami, Reza",2020,,,,Journal of Neural Engineering
Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,2511.05531v1,Jin2024,\cite{Jin2024},Bayesian inference of frequency-specific functional connectivity in {MEG} imaging using a spectral graph model,,,True,False,"Jin, Huaqing and Abdelnour, Farras and Verma, Parul and Sipes, Benjamin S. and Nagarajan, Srikantan S. and Raj, Ashish",2024,10,,10.1162/imag_a_00307,Imaging Neuroscience
Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,2511.05531v1,chen2021estimation,\cite{chen2021estimation},Estimation of discriminative multimodal brain network connectivity using message-passing-based nonlinear network fusion,,,True,False,"Chen, Nan and Guo, Man and Li, Yongchao and Hu, Xiping and Yao, Zhijun and Hu, Bin",2021,,,,IEEE/ACM Transactions on Computational Biology and Bioinformatics
Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,2511.05531v1,qu2021brain,\cite{qu2021brain},Brain functional connectivity analysis via graphical deep learning,,,True,False,"Qu, Gang and Hu, Wenxing and Xiao, Li and Wang, Junqi and Bai, Yuntong and Patel, Beenish and Zhang, Kun and Wang, Yu-Ping",2021,,,,IEEE Transactions on Biomedical Engineering
Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,2511.05531v1,jie2013integration,\cite{jie2013integration},Integration of network topological and connectivity properties for neuroimaging classification,,,True,False,"Jie, Biao and Zhang, Daoqiang and Gao, Wei and Wang, Qian and Wee, Chong-Yaw and Shen, Dinggang",2013,,,,IEEE Transactions on Biomedical Engineering
Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,2511.05531v1,kragel2022temporal,\cite{kragel2022temporal},The temporal dynamics of spontaneous emotional brain states and their implications for mental health,,,True,False,"Kragel, Philip A and Hariri, Ahmad R and LaBar, Kevin S",2022,,,,Journal of Cognitive Neuroscience
Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,2511.05531v1,tawhid2024genet,\cite{tawhid2024genet},{GENet}: a generic neural network for detecting various neurological disorders from {EEG},,,True,False,"Tawhid, Md Nurul Ahad and Siuly, Siuly and Wang, Kate and Wang, Hua",2024,,,,IEEE Transactions on Cognitive and Developmental Systems
Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,2511.05531v1,dai2025feature,\cite{dai2025feature},Feature and Semantic Matching Multi-Source Domain Adaptation for Diagnostic Classification of Neuropsychiatric Disorders,,,True,False,"Dai, Minghao and Su, Jianpo and Fan, Zhipeng and Wang, Chenyu and Peng, Limin and Hu, Dewen and Zeng, Ling-Li",2025,,,,IEEE Transactions on Cognitive and Developmental Systems
Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,2511.05531v1,qin2025classification,\cite{qin2025classification},Classification of Neuropsychiatric Disorders via Brain-Region-Selected Graph Convolutional Network,,,True,False,"Qin, Zhenzhe and Li, Yongbo and Song, Xiaoying and Chai, Li",2025,,,,IEEE Transactions on Neural Systems and Rehabilitation Engineering
Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,2511.05531v1,ma2023multi,\cite{ma2023multi},Multi-scale dynamic graph learning for brain disorder detection with functional {MRI},,,True,False,"Ma, Yunling and Wang, Qianqian and Cao, Liang and Li, Long and Zhang, Chaojun and Qiao, Lishan and Liu, Mingxia",2023,,,,IEEE Transactions on Neural Systems and Rehabilitation Engineering
Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,2511.05531v1,hao2022multimodal,\cite{hao2022multimodal},Multimodal self-paced locality-preserving learning for diagnosis of Alzheimer’s disease,,,True,False,"Hao, Xiaoke and Wang, Ruxue and Guo, Yingchun and Xiao, Yunjia and Yu, Ming and Wang, Meiling and Chen, Weibin and Zhang, Daoqiang",2022,,,,IEEE Transactions on Cognitive and Developmental Systems
Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,2511.05531v1,klepl2022eeg,\cite{klepl2022eeg},{EEG}-based graph neural network classification of Alzheimer’s disease: An empirical evaluation of functional connectivity methods,,,True,False,"Klepl, Dominik and He, Fei and Wu, Min and Blackburn, Daniel J and Sarrigiannis, Ptolemaios",2022,,,,IEEE Transactions on Neural Systems and Rehabilitation Engineering
Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,2511.05531v1,cao2023novel,\cite{cao2023novel},A novel approach analysing the dynamic brain functional connectivity for improved {MCI} detection,,,True,False,"Cao, Tangwei and Lin, Runwei and Zheng, Yinuo and Shen, Dinggang and Xu, Lin",2023,,,,IEEE Transactions on Biomedical Engineering
Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,2511.05531v1,ma2022diagnosis,\cite{ma2022diagnosis},Diagnosis of mild cognitive impairment with ordinal pattern kernel,,,True,False,"Ma, Kai and Huang, Shuo and Zhang, Daoqiang",2022,,,,IEEE Transactions on Neural Systems and Rehabilitation Engineering
Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,2511.05531v1,kang2023classifying,\cite{kang2023classifying},Classifying and scoring major depressive disorders by residual neural networks on specific frequencies and brain regions,,,True,False,"Kang, Cheng and Novak, Daniel and Yao, Xujing and Xie, Jiayong and Hu, Yong",2023,,,,IEEE Transactions on Neural Systems and Rehabilitation Engineering
Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,2511.05531v1,zheng2023attention,\cite{zheng2023attention},An attention-based multi-modal {MRI} fusion model for major depressive disorder diagnosis,,,True,False,"Zheng, Guowei and Zheng, Weihao and Zhang, Yu and Wang, Junyu and Chen, Miao and Wang, Yin and Cai, Tianhong and Yao, Zhijun and Hu, Bin",2023,,,,Journal of Neural Engineering
Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,2511.05531v1,guo2020diagnosis,\cite{guo2020diagnosis},Diagnosis of major depressive disorder using whole-brain effective connectivity networks derived from resting-state functional {MRI},,,True,False,"Guo, Man and Wang, Tiancheng and Zhang, Zhe and Chen, Nan and Li, Yongchao and Wang, Yin and Yao, Zhijun and Hu, Bin",2020,,,,Journal of Neural Engineering
Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,2511.05531v1,grover2023schizo,\cite{grover2023schizo},{Schizo-Net}: A novel Schizophrenia Diagnosis framework using late fusion multimodal deep learning on Electroencephalogram-based Brain connectivity indices,,,True,False,"Grover, Nitin and Chharia, Aviral and Upadhyay, Rahul and Longo, Luca",2023,,,,IEEE Transactions on Neural Systems and Rehabilitation Engineering
Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,2511.05531v1,huang2022joint,\cite{huang2022joint},Joint-channel-connectivity-based feature selection and classification on {fNIRS} for stress detection in decision-making,,,True,False,"Huang, Meiyan and Zhang, Xiaoling and Chen, Xiumei and Mai, Yiling and Wu, Xiaohua and Zhao, Jiubo and Feng, Qianjin",2022,,,,IEEE Transactions on Neural Systems and Rehabilitation Engineering
Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,2511.05531v1,barik2023functional,\cite{barik2023functional},Functional connectivity based machine learning approach for autism detection in young children using {MEG} signals,,,True,False,"Barik, Kasturi and Watanabe, Katsumi and Bhattacharya, Joydeep and Saha, Goutam",2023,,,,Journal of Neural Engineering
Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,2511.05531v1,jamal2014classification,\cite{jamal2014classification},Classification of autism spectrum disorder using supervised learning of brain connectivity measures extracted from synchrostates,,,True,False,"Jamal, Wasifa and Das, Saptarshi and Oprescu, Ioana-Anastasia and Maharatna, Koushik and Apicella, Fabio and Sicca, Federico",2014,,,,Journal of Neural Engineering
Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,2511.05531v1,han2024brain,\cite{han2024brain},Brain connectivity patterns derived from aging-related alterations in dynamic brain functional networks and their potential as features for brain age classification,,,True,False,"Han, Hongfang and Jiang, Jiuchuan and Gu, Lingyun and Gan, John Q and Wang, Haixian",2024,,,,Journal of Neural Engineering
Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,2511.05531v1,ji2020convolutional,\cite{ji2020convolutional},Convolutional Neural Network With Graphical Lasso to Extract Sparse Topological Features for Brain Disease Classification,,,True,False,"Ji, Junzhong and Yao, Yao",2020,,,,IEEE/ACM Transactions on Computational Biology and Bioinformatics
Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,2511.05531v1,GUTIERREZGOMEZ2020102316,\cite{GUTIERREZGOMEZ2020102316},Stable biomarker identification for predicting schizophrenia in the human connectome,,,True,False,Leonardo Gutiérrez-Gómez and Jakub Vohryzek and Benjamin Chiêm and Philipp S. Baumann and Philippe Conus and Kim Do Cuenod and Patric Hagmann and Jean-Charles Delvenne,2020,,,,NeuroImage: Clinical
Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,2511.05531v1,Esther2015,\cite{Esther2015},Feature Selection Based on the {SVM} Weight Vector for Classification of Dementia,,,True,False,"Bron, Esther E. and Smits, Marion and Niessen, Wiro J. and Klein, Stefan",2015,,,10.1109/JBHI.2015.2432832,IEEE Journal of Biomedical and Health Informatics
Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,2511.05531v1,jie2018discriminating,\cite{jie2018discriminating},Discriminating bipolar disorder from major depression using whole-brain functional connectivity: a feature selection analysis with {SVM-FoBa} algorithm,,,True,False,"Jie, Nan-Feng and Osuch, Elizabeth A and Zhu, Mao-Hu and Wammes, Michael and Ma, Xiao-Ying and Jiang, Tian-Zi and Sui, Jing and Calhoun, Vince D",2018,,,,Journal of Signal Processing Systems
Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,2511.05531v1,holker2021quantitative,\cite{holker2021quantitative},Quantitative {EEG} feature selection by MajorityVoting for alcohol use disorder detection,,,True,False,"Holker, Ruchi and Susan, Seba",2021,,,,Proc. IEEE EMBS International Conference on Biomedical and Health Informatics (BHI)
Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,2511.05531v1,ali2023correlation,\cite{ali2023correlation},Correlation-filter-based channel and feature selection framework for hybrid {EEG-fNIRS BCI} applications,,,True,False,"Ali, Muhammad Umair and Zafar, Amad and Kallu, Karam Dad and Masood, Haris and Mannan, Malik Muhammad Naeem and Ibrahim, Malik Muhammad and Kim, Sangil and Khan, Muhammad Attique",2023,,,,IEEE Journal of Biomedical and Health Informatics
Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,2511.05531v1,obertino2016infinite,\cite{obertino2016infinite},Infinite feature selection on shore-based biomarkers reveals connectivity modulation after stroke,,,True,False,"Obertino, Silvia and Roffo, Giorgio and Granziera, Cristina and Menegaz, Gloria",2016,,,,Proc. 2016 International Workshop on Pattern Recognition in Neuroimaging (PRNI)
Selection and Stability of Functional Connectivity Features for Classification of Brain Disorders,2511.05531v1,pei2020eeg,\cite{pei2020eeg},{EEG-based} multiclass workload identification using feature fusion and selection,,,True,False,"Pei, Zian and Wang, Hongtao and Bezerianos, Anastasios and Li, Junhua",2020,,,,IEEE Transactions on Instrumentation and Measurement
PhenoGnet: A Graph-Based Contrastive Learning Framework for Disease Similarity Prediction,2509.14037v1,zhang2021,\cite{zhang2021},Graph Neural Networks and Their Current Applications in Bioinformatics,,,True,False,"Zhang, Xiao-Meng  and Liang, Li  and Liu, Lin  and Tang, Ming-Jing",2021,,https://www.frontiersin.org/journals/genetics/articles/10.3389/fgene.2021.690049,10.3389/fgene.2021.690049,Frontiers in Genetics
PhenoGnet: A Graph-Based Contrastive Learning Framework for Disease Similarity Prediction,2509.14037v1,zhu2023,\cite{zhu2023},Graph Contrastive Learning with Cross-view Reconstruction,https://arxiv.org/abs/2209.07699v3,"Among different existing graph self-supervised learning strategies, graph contrastive learning (GCL) has been one of the most prevalent approaches to this problem. Despite the remarkable performance those GCL methods have achieved, existing GCL methods that heavily depend on various manually designed augmentation techniques still struggle to alleviate the feature suppression issue without risking losing task-relevant information. Consequently, the learned representation is either brittle or unilluminating. In light of this, we introduce the Graph Contrastive Learning with Cross-View Reconstruction (GraphCV), which follows the information bottleneck principle to learn minimal yet sufficient representation from graph data. Specifically, GraphCV aims to elicit the predictive (useful for downstream instance discrimination) and other non-predictive features separately. Except for the conventional contrastive loss which guarantees the consistency and sufficiency of the representation across different augmentation views, we introduce a cross-view reconstruction mechanism to pursue the disentanglement of the two learned representations. Besides, an adversarial view perturbed from the original view is added as the third view for the contrastive loss to guarantee the intactness of the global semantics and improve the representation robustness. We empirically demonstrate that our proposed model outperforms the state-of-the-art on graph classification task over multiple benchmark datasets.",True,True,Jianian Zhu and Weixin Zeng and Junfeng Zhang and Jiuyang Tang and Xiang Zhao,2023,,https://www.sciencedirect.com/science/article/pii/S1566253523001835,https://doi.org/10.1016/j.inffus.2023.101867,Information Fusion
PhenoGnet: A Graph-Based Contrastive Learning Framework for Disease Similarity Prediction,2509.14037v1,hassani2020,\cite{hassani2020},Contrastive Multi-View Representation Learning on Graphs,https://arxiv.org/abs/2006.05582v1,"We introduce a self-supervised approach for learning node and graph level representations by contrasting structural views of graphs. We show that unlike visual representation learning, increasing the number of views to more than two or contrasting multi-scale encodings do not improve performance, and the best performance is achieved by contrasting encodings from first-order neighbors and a graph diffusion. We achieve new state-of-the-art results in self-supervised learning on 8 out of 8 node and graph classification benchmarks under the linear evaluation protocol. For example, on Cora (node) and Reddit-Binary (graph) classification benchmarks, we achieve 86.8% and 84.5% accuracy, which are 5.5% and 2.4% relative improvements over previous state-of-the-art. When compared to supervised baselines, our approach outperforms them in 4 out of 8 benchmarks. Source code is released at: https://github.com/kavehhassani/mvgrl",True,True,Kaveh Hassani and Amir Hosein Khasahmadi,2020,,https://arxiv.org/abs/2006.05582,,
PhenoGnet: A Graph-Based Contrastive Learning Framework for Disease Similarity Prediction,2509.14037v1,kang2024,\cite{kang2024},Inter-structure and intra-semantics graph contrastive learning for disease prediction,,,True,False,Yan Kang and Jingyu Zheng and Mingjian Yang and Ning An,2024,,https://www.sciencedirect.com/science/article/pii/S0950705124006932,https://doi.org/10.1016/j.knosys.2024.112059,Knowledge-Based Systems
PhenoGnet: A Graph-Based Contrastive Learning Framework for Disease Similarity Prediction,2509.14037v1,Chen2022,\cite{Chen2022},CoGO: a contrastive learning framework to predict disease similarity based on gene network and ontology structure,,,True,False,Yuhao Chen and Yanshi Hu and Xiaotian Hu and Cong Feng and Ming Chen,2022,9,https://dx.doi.org/10.1093/bioinformatics/btac520,10.1093/BIOINFORMATICS/BTAC520,Bioinformatics
PhenoGnet: A Graph-Based Contrastive Learning Framework for Disease Similarity Prediction,2509.14037v1,Wei2024,\cite{Wei2024},DiSMVC: a multi-view graph collaborative learning framework for measuring disease similarity,,,True,False,Hang Wei and Lin Gao and Shuai Wu and Yina Jiang and Bin Liu,2024,5,https://dx.doi.org/10.1093/bioinformatics/btae306,10.1093/BIOINFORMATICS/BTAE306,Bioinformatics
TriAgent: Automated Biomarker Discovery with Deep Research Grounding for Triage in Acute Care by LLM-Based Multi-Agent Collaboration,2510.16080v1,RN77,\cite{RN77},Artificial intelligence-based biomarkers for treatment decisions in oncology,,,True,False,"Ligero, Marta and El Nahhas, Omar S. M. and Aldea, Mihaela and Kather, Jakob Nikolas",2025,,https://www.sciencedirect.com/science/article/pii/S2405803324002802,https://doi.org/10.1016/j.trecan.2024.12.001,Trends in Cancer
TriAgent: Automated Biomarker Discovery with Deep Research Grounding for Triage in Acute Care by LLM-Based Multi-Agent Collaboration,2510.16080v1,RN76,\cite{RN76},The benefits and pitfalls of machine learning for biomarker discovery,,,True,False,"Ng, Sandra and Masarone, Sara and Watson, David and Barnes, Michael R.",2023,,https://doi.org/10.1007/s00441-023-03816-z,10.1007/s00441-023-03816-z,Cell and Tissue Research
TriAgent: Automated Biomarker Discovery with Deep Research Grounding for Triage in Acute Care by LLM-Based Multi-Agent Collaboration,2510.16080v1,RN54,\cite{RN54},Automatic biomarker discovery and enrichment with BRAD,,,True,False,"Pickard, J. and Prakash, R. and Choi, M. A. and Oliven, N. and Stansbury, C. and Cwycyshyn, J. and Galioto, N. and Gorodetsky, A. and Velasquez, A. and Rajapakse, I.",2025,,,10.1093/bioinformatics/btaf159,Bioinformatics
TriAgent: Automated Biomarker Discovery with Deep Research Grounding for Triage in Acute Care by LLM-Based Multi-Agent Collaboration,2510.16080v1,RN55,\cite{RN55},Human-augmented large language model-driven selection of glutathione peroxidase 4 as a candidate blood transcriptional biomarker for circulating erythroid cells,,,True,False,"Subba, Bishesh and Toufiq, Mohammed and Omi, Fuadur and Yurieva, Marina and Khan, Taushif and Rinchai, Darawan and Palucka, Karolina and Chaussabel, Damien",2024,,https://doi.org/10.1038/s41598-024-73916-5,10.1038/s41598-024-73916-5,Scientific Reports
TriAgent: Automated Biomarker Discovery with Deep Research Grounding for Triage in Acute Care by LLM-Based Multi-Agent Collaboration,2510.16080v1,RN57,\cite{RN57},LmRaC: a functionally extensible tool for LLM interrogation of user experimental results,,,True,False,"Craig, Douglas B and Drăghici, Sorin",2024,,https://doi.org/10.1093/bioinformatics/btae679,10.1093/bioinformatics/btae679,Bioinformatics
TriAgent: Automated Biomarker Discovery with Deep Research Grounding for Triage in Acute Care by LLM-Based Multi-Agent Collaboration,2510.16080v1,RN56,\cite{RN56},HEAL-KGGen: A Hierarchical Multi-Agent LLM Framework with Knowledge Graph Enhancement for Genetic Biomarker-Based Medical Diagnosis,,,True,False,"Zuo, Kaiwen and Zhong, Zixuan and Huang, Peizhou and Tang, Shiyan and Chen, Yuyan and Jiang, Yirui",2025,,https://www.biorxiv.org/content/biorxiv/early/2025/06/06/2025.06.03.657521.full.pdf,10.1101/2025.06.03.657521,bioRxiv
TriAgent: Automated Biomarker Discovery with Deep Research Grounding for Triage in Acute Care by LLM-Based Multi-Agent Collaboration,2510.16080v1,RN69,\cite{RN69},Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,https://arxiv.org/abs/2201.11903v6,"We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",True,True,"Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny",2022,,,,Advances in neural information processing systems
TriAgent: Automated Biomarker Discovery with Deep Research Grounding for Triage in Acute Care by LLM-Based Multi-Agent Collaboration,2510.16080v1,RN60,\cite{RN60},Enhancing diagnostic capability with multi-agents conversational large language models,,,True,False,"Chen, Xi and Yi, Huahui and You, Mingke and Liu, WeiZhi and Wang, Li and Li, Hairui and Zhang, Xue and Guo, Yingman and Fan, Lei and Chen, Gang and Lao, Qicheng and Fu, Weili and Li, Kang and Li, Jian",2025,,https://doi.org/10.1038/s41746-025-01550-0,10.1038/s41746-025-01550-0,npj Digital Medicine
TriAgent: Automated Biomarker Discovery with Deep Research Grounding for Triage in Acute Care by LLM-Based Multi-Agent Collaboration,2510.16080v1,RN65,\cite{RN65},MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning,https://arxiv.org/abs/2311.10537v4,"Large language models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare. This field faces unique challenges such as domain-specific terminologies and reasoning over specialized knowledge. To address these issues, we propose MedAgents, a novel multi-disciplinary collaboration framework for the medical domain. MedAgents leverages LLM-based agents in a role-playing setting that participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities. This training-free framework encompasses five critical steps: gathering domain experts, proposing individual analyses, summarising these analyses into a report, iterating over discussions until a consensus is reached, and ultimately making a decision. Our work focuses on the zero-shot setting, which is applicable in real-world scenarios. Experimental results on nine datasets (MedQA, MedMCQA, PubMedQA, and six subtasks from MMLU) establish that our proposed MedAgents framework excels at mining and harnessing the medical expertise within LLMs, as well as extending its reasoning abilities. Our code can be found at https://github.com/gersteinlab/MedAgents.",True,True,"Tang, Xiangru and Zou, Anni and Zhang, Zhuosheng and Li, Ziming and Zhao, Yilun and Zhang, Xingyao and Cohan, Arman and Gerstein, Mark",2023,,,,arXiv preprint arXiv:2311.10537
TriAgent: Automated Biomarker Discovery with Deep Research Grounding for Triage in Acute Care by LLM-Based Multi-Agent Collaboration,2510.16080v1,RN61,\cite{RN61},Mdagents: An adaptive collaboration of llms for medical decision-making,,,True,False,"Kim, Yubin and Park, Chanwoo and Jeong, Hyewon and Chan, Yik S and Xu, Xuhai and McDuff, Daniel and Lee, Hyeonhoon and Ghassemi, Marzyeh and Breazeal, Cynthia and Park, Hae W",2024,,,,Advances in Neural Information Processing Systems
TriAgent: Automated Biomarker Discovery with Deep Research Grounding for Triage in Acute Care by LLM-Based Multi-Agent Collaboration,2510.16080v1,RN53,\cite{RN53},TriageAgent: Towards Better Multi-Agents Collaborations for Large Language Model-Based Clinical Triage,,,True,False,"Lu, Meng and Ho, Brandon and Ren, Dennis and Wang, Xuan",,,"https://aclanthology.org/2024.findings-emnlp.329/
https://doi.org/10.18653/v1/2024.findings-emnlp.329",10.18653/v1/2024.findings-emnlp.329,
TriAgent: Automated Biomarker Discovery with Deep Research Grounding for Triage in Acute Care by LLM-Based Multi-Agent Collaboration,2510.16080v1,RN62,\cite{RN62},Reinforcing clinical decision support through multi-agent systems and ethical ai governance,,,True,False,"Chen, Ying-Jung and Albarqawi, Ahmad and Chen, Chi-Sheng",2025,,,,arXiv preprint arXiv:2504.03699
Accelerating Protein Molecular Dynamics Simulation with DeepJump,2509.13294v1,smith2017ani,\cite{smith2017ani},ANI-1: An extensible neural network potential with DFT accuracy at force field computational cost,https://arxiv.org/abs/1610.08935v4,"Deep learning is revolutionizing many areas of science and technology, especially image, text and speech recognition. In this paper, we demonstrate how a deep neural network (NN) trained on quantum mechanical (QM) DFT calculations can learn an accurate and fully transferable potential for organic molecules. We introduce ANAKIN-ME (Accurate NeurAl networK engINe for Molecular Energies) or ANI in short. ANI is a new method and procedure for training neural network potentials that utilizes a highly modified version of the Behler and Parrinello symmetry functions to build single-atom atomic environment vectors as a molecular representation. We utilize ANI to build a potential called ANI-1, which was trained on a subset of the GDB databases with up to 8 heavy atoms to predict total energies for organic molecules containing four atom types: H, C, N, and O. To obtain an accelerated but physically relevant sampling of molecular potential surfaces, we also propose a Normal Mode Sampling (NMS) method for generating molecular configurations. Through a series of case studies, we show that ANI-1 is chemically accurate compared to reference DFT calculations on much larger molecular systems (up to 54 atoms) than those included in the training data set, with root mean square errors as low as 0.56 kcal/mol.",True,True,"Smith, Justin S and Isayev, Olexandr and Roitberg, Adrian E",2017,,,,Chemical science
Accelerating Protein Molecular Dynamics Simulation with DeepJump,2509.13294v1,batzner20223,\cite{batzner20223},E (3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials,,,True,False,"Batzner, Simon and Musaelian, Albert and Sun, Lixin and Geiger, Mario and Mailoa, Jonathan P and Kornbluth, Mordechai and Molinari, Nicola and Smidt, Tess E and Kozinsky, Boris",2022,,,,Nature communications
Accelerating Protein Molecular Dynamics Simulation with DeepJump,2509.13294v1,batatia2022mace,\cite{batatia2022mace},MACE: Higher Order Equivariant Message Passing Neural Networks for Fast and Accurate Force Fields,https://arxiv.org/abs/2206.07697v2,"Creating fast and accurate force fields is a long-standing challenge in computational chemistry and materials science. Recently, several equivariant message passing neural networks (MPNNs) have been shown to outperform models built using other approaches in terms of accuracy. However, most MPNNs suffer from high computational cost and poor scalability. We propose that these limitations arise because MPNNs only pass two-body messages leading to a direct relationship between the number of layers and the expressivity of the network. In this work, we introduce MACE, a new equivariant MPNN model that uses higher body order messages. In particular, we show that using four-body messages reduces the required number of message passing iterations to just two, resulting in a fast and highly parallelizable model, reaching or exceeding state-of-the-art accuracy on the rMD17, 3BPA, and AcAc benchmark tasks. We also demonstrate that using higher order messages leads to an improved steepness of the learning curves.",True,True,"Batatia, Ilyes and Kovacs, David P and Simm, Gregor and Ortner, Christoph and Cs{\'a}nyi, G{\'a}bor",2022,,,,Advances in Neural Information Processing Systems
Accelerating Protein Molecular Dynamics Simulation with DeepJump,2509.13294v1,jing2024alphafold,\cite{jing2024alphafold},AlphaFold meets flow matching for generating protein ensembles,,,True,False,"Jing, Bowen and Berger, Bonnie and Jaakkola, Tommi",2024,,,,arXiv preprint arXiv:2402.04845
Accelerating Protein Molecular Dynamics Simulation with DeepJump,2509.13294v1,lewis2025scalable,\cite{lewis2025scalable},Scalable emulation of protein equilibrium ensembles with generative deep learning,,,True,False,"Lewis, Sarah and Hempel, Tim and Jim{\'e}nez-Luna, Jos{\'e} and Gastegger, Michael and Xie, Yu and Foong, Andrew YK and Satorras, Victor Garc{\'\i}a and Abdin, Osama and Veeling, Bastiaan S and Zaporozhets, Iryna and others",2025,,,,Science
Accelerating Protein Molecular Dynamics Simulation with DeepJump,2509.13294v1,ito,\cite{ito},Implicit Transfer Operator Learning: Multiple Time-Resolution Surrogates for Molecular Dynamics,https://arxiv.org/abs/2305.18046v2,"Computing properties of molecular systems rely on estimating expectations of the (unnormalized) Boltzmann distribution. Molecular dynamics (MD) is a broadly adopted technique to approximate such quantities. However, stable simulations rely on very small integration time-steps ($10^{-15}\,\mathrm{s}$), whereas convergence of some moments, e.g. binding free energy or rates, might rely on sampling processes on time-scales as long as $10^{-1}\, \mathrm{s}$, and these simulations must be repeated for every molecular system independently. Here, we present Implict Transfer Operator (ITO) Learning, a framework to learn surrogates of the simulation process with multiple time-resolutions. We implement ITO with denoising diffusion probabilistic models with a new SE(3) equivariant architecture and show the resulting models can generate self-consistent stochastic dynamics across multiple time-scales, even when the system is only partially observed. Finally, we present a coarse-grained CG-SE3-ITO model which can quantitatively model all-atom molecular dynamics using only coarse molecular representations. As such, ITO provides an important step towards multiple time- and space-resolution acceleration of MD. Code is available at \href{https://github.com/olsson-group/ito}{https://github.com/olsson-group/ito}.",True,True,Mathias Schreiner and Ole Winther and Simon Olsson,2023,,,,
Accelerating Protein Molecular Dynamics Simulation with DeepJump,2509.13294v1,li2024f3low,\cite{li2024f3low},F$^3$low: Frame-to-Frame Coarse-grained Molecular Dynamics with SE(3) Guided Flow Matching,https://arxiv.org/abs/2405.00751v1,"Molecular dynamics (MD) is a crucial technique for simulating biological systems, enabling the exploration of their dynamic nature and fostering an understanding of their functions and properties. To address exploration inefficiency, emerging enhanced sampling approaches like coarse-graining (CG) and generative models have been employed. In this work, we propose a \underline{Frame-to-Frame} generative model with guided \underline{Flow}-matching (F$3$low) for enhanced sampling, which (a) extends the domain of CG modeling to the SE(3) Riemannian manifold; (b) retreating CGMD simulations as autoregressively sampling guided by the former frame via flow-matching models; (c) targets the protein backbone, offering improved insights into secondary structure formation and intricate folding pathways. Compared to previous methods, F$3$low allows for broader exploration of conformational space. The ability to rapidly generate diverse conformations via force-free generative paradigm on SE(3) paves the way toward efficient enhanced sampling methods.",True,True,Shaoning Li and Yusong Wang and Mingyu Li and Jian Zhang and Bin Shao and Nanning Zheng and Jian Tang,2024,,,,
Accelerating Protein Molecular Dynamics Simulation with DeepJump,2509.13294v1,jing24generative,\cite{jing24generative},Generative Modeling of Molecular Dynamics Trajectories,https://arxiv.org/abs/2409.17808v1,"Molecular dynamics (MD) is a powerful technique for studying microscopic phenomena, but its computational cost has driven significant interest in the development of deep learning-based surrogate models. We introduce generative modeling of molecular trajectories as a paradigm for learning flexible multi-task surrogate models of MD from data. By conditioning on appropriately chosen frames of the trajectory, we show such generative models can be adapted to diverse tasks such as forward simulation, transition path sampling, and trajectory upsampling. By alternatively conditioning on part of the molecular system and inpainting the rest, we also demonstrate the first steps towards dynamics-conditioned molecular design. We validate the full set of these capabilities on tetrapeptide simulations and show that our model can produce reasonable ensembles of protein monomers. Altogether, our work illustrates how generative modeling can unlock value from MD data towards diverse downstream tasks that are not straightforward to address with existing methods or even MD itself. Code is available at https://github.com/bjing2016/mdgen.",True,True,"Jing, Bowen and Stark, Hannes and Jaakkola, Tommi and Berger, Bonnie",2024,,,,
Accelerating Protein Molecular Dynamics Simulation with DeepJump,2509.13294v1,costa2024equijump,\cite{costa2024equijump},EquiJump: Protein Dynamics Simulation via SO(3)-Equivariant Stochastic Interpolants,https://arxiv.org/abs/2410.09667v2,"Mapping the conformational dynamics of proteins is crucial for elucidating their functional mechanisms. While Molecular Dynamics (MD) simulation enables detailed time evolution of protein motion, its computational toll hinders its use in practice. To address this challenge, multiple deep learning models for reproducing and accelerating MD have been proposed drawing on transport-based generative methods. However, existing work focuses on generation through transport of samples from prior distributions, that can often be distant from the data manifold. The recently proposed framework of stochastic interpolants, instead, enables transport between arbitrary distribution endpoints. Building upon this work, we introduce EquiJump, a transferable SO(3)-equivariant model that bridges all-atom protein dynamics simulation time steps directly. Our approach unifies diverse sampling methods and is benchmarked against existing models on trajectory data of fast folding proteins. EquiJump achieves state-of-the-art results on dynamics simulation with a transferable model on all of the fast folding proteins.",True,True,"Costa, Allan Dos Santos and Mitnikov, Ilan and Pellegrini, Franco and Daigavane, Ameya and Geiger, Mario and Cao, Zhonglin and Kreis, Karsten and Smidt, Tess and Kucukbenli, Emine and Jacobson, Joseph",2024,,,,arXiv preprint arXiv:2410.09667
Accelerating Protein Molecular Dynamics Simulation with DeepJump,2509.13294v1,daigavane2025jamunbridgingsmoothedmolecular,\cite{daigavane2025jamunbridgingsmoothedmolecular},JAMUN: Bridging Smoothed Molecular Dynamics and Score-Based Learning for Conformational Ensembles,https://arxiv.org/abs/2410.14621v3,"Conformational ensembles of protein structures are immensely important both for understanding protein function and drug discovery in novel modalities such as cryptic pockets. Current techniques for sampling ensembles such as molecular dynamics (MD) are computationally inefficient, while many recent machine learning methods do not transfer to systems outside their training data. We propose JAMUN which performs MD in a smoothed, noised space of all-atom 3D conformations of molecules by utilizing the framework of walk-jump sampling. JAMUN enables ensemble generation for small peptides at rates of an order of magnitude faster than traditional molecular dynamics. The physical priors in JAMUN enables transferability to systems outside of its training data, even to peptides that are longer than those originally trained on. Our model, code and weights are available at https://github.com/prescient-design/jamun.",True,True,Ameya Daigavane and Bodhi P. Vani and Darcy Davidson and Saeed Saremi and Joshua Rackers and Joseph Kleinhenz,2025,,https://arxiv.org/abs/2410.14621,,
Accelerating Protein Molecular Dynamics Simulation with DeepJump,2509.13294v1,geiger2022e3nn,\cite{geiger2022e3nn},e3nn: Euclidean Neural Networks,https://arxiv.org/abs/2207.09453v1,"We present e3nn, a generalized framework for creating E(3) equivariant trainable functions, also known as Euclidean neural networks. e3nn naturally operates on geometry and geometric tensors that describe systems in 3D and transform predictably under a change of coordinate system. The core of e3nn are equivariant operations such as the TensorProduct class or the spherical harmonics functions that can be composed to create more complex modules such as convolutions and attention mechanisms. These core operations of e3nn can be used to efficiently articulate Tensor Field Networks, 3D Steerable CNNs, Clebsch-Gordan Networks, SE(3) Transformers and other E(3) equivariant networks.",True,True,"Geiger, Mario and Smidt, Tess",2022,,,,arXiv preprint arXiv:2207.09453
Accelerating Protein Molecular Dynamics Simulation with DeepJump,2509.13294v1,lipman2022flow,\cite{lipman2022flow},Flow matching for generative modeling,,,True,False,"Lipman, Yaron and Chen, Ricky TQ and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt",2022,,,,arXiv preprint arXiv:2210.02747
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,Behler2007,\cite{Behler2007},Generalized Neural-Network Representation of High-Dimensional Potential-Energy Surfaces,,,True,False,"Behler,  J\""{o}rg and Parrinello,  Michele",2007,,http://dx.doi.org/10.1103/PhysRevLett.98.146401,10.1103/physrevlett.98.146401,Physical Review Letters
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,batatia2024macemp,\cite{batatia2024macemp},A foundation model for atomistic materials chemistry,,,True,False,Ilyes Batatia and Philipp Benner and Yuan Chiang and Alin M. Elena and Dávid P. Kovács and Janosh Riebesell and Xavier R. Advincula and Mark Asta and Matthew Avaylon and William J. Baldwin and Fabian Berger and Noam Bernstein and Arghya Bhowmik and Samuel M. Blau and Vlad Cărare and James P. Darby and Sandip De and Flaviano Della Pia and Volker L. Deringer and Rokas Elijošius and Zakariya El-Machachi and Fabio Falcioni and Edvin Fako and Andrea C. Ferrari and Annalena Genreith-Schriever and Janine George and Rhys E. A. Goodall and Clare P. Grey and Petr Grigorev and Shuang Han and Will Handley and Hendrik H. Heenen and Kersti Hermansson and Christian Holm and Jad Jaafar and Stephan Hofmann and Konstantin S. Jakob and Hyunwook Jung and Venkat Kapil and Aaron D. Kaplan and Nima Karimitari and James R. Kermode and Namu Kroupa and Jolla Kullgren and Matthew C. Kuner and Domantas Kuryla and Guoda Liepuoniute and Johannes T. Margraf and Ioan-Bogdan Magdău and Angelos Michaelides and J. Harry Moore and Aakash A. Naik and Samuel P. Niblett and Sam Walton Norwood and Niamh O'Neill and Christoph Ortner and Kristin A. Persson and Karsten Reuter and Andrew S. Rosen and Lars L. Schaaf and Christoph Schran and Benjamin X. Shi and Eric Sivonxay and Tamás K. Stenczel and Viktor Svahn and Christopher Sutton and Thomas D. Swinburne and Jules Tilly and Cas van der Oord and Eszter Varga-Umbrich and Tejs Vegge and Martin Vondrák and Yangshuai Wang and William C. Witt and Fabian Zills and Gábor Csányi,2024,,,,arXiv preprint arXiv:2401.00096
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,garrison2023applying,\cite{garrison2023applying},Applying Large Graph Neural Networks to Predict Transition Metal Complex Energies Using the tmQM\_wB97MV Data Set,,,True,False,"Garrison, Aaron G and Heras-Domingo, Javier and Kitchin, John R and dos Passos Gomes, Gabriel and Ulissi, Zachary W and Blau, Samuel M",2023,,,,Journal of Chemical Information and Modeling
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,Artrith2016,\cite{Artrith2016},An implementation of artificial neural-network potentials for atomistic materials simulations: Performance for TiO2,,,True,False,"Artrith,  Nongnuch and Urban,  Alexander",2016,,http://dx.doi.org/10.1016/j.commatsci.2015.11.047,10.1016/j.commatsci.2015.11.047,Computational Materials Science
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,batatia_mace_2022,\cite{batatia_mace_2022},MACE: Higher Order Equivariant Message Passing Neural Networks for Fast and Accurate Force Fields,https://arxiv.org/abs/2206.07697v2,"Creating fast and accurate force fields is a long-standing challenge in computational chemistry and materials science. Recently, several equivariant message passing neural networks (MPNNs) have been shown to outperform models built using other approaches in terms of accuracy. However, most MPNNs suffer from high computational cost and poor scalability. We propose that these limitations arise because MPNNs only pass two-body messages leading to a direct relationship between the number of layers and the expressivity of the network. In this work, we introduce MACE, a new equivariant MPNN model that uses higher body order messages. In particular, we show that using four-body messages reduces the required number of message passing iterations to just two, resulting in a fast and highly parallelizable model, reaching or exceeding state-of-the-art accuracy on the rMD17, 3BPA, and AcAc benchmark tasks. We also demonstrate that using higher order messages leads to an improved steepness of the learning curves.",True,True,"Batatia, Ilyes and Kovacs, David P and Simm, Gregor and Ortner, Christoph and Csányi, Gábor",2022,,,,Advances in Neural Information Processing Systems
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,gasteiger_gemnet_2021,\cite{gasteiger_gemnet_2021},GemNet: Universal Directional Graph Neural Networks for Molecules,https://arxiv.org/abs/2106.08903v10,"Effectively predicting molecular interactions has the potential to accelerate molecular dynamics by multiple orders of magnitude and thus revolutionize chemical simulations. Graph neural networks (GNNs) have recently shown great successes for this task, overtaking classical methods based on fixed molecular kernels. However, they still appear very limited from a theoretical perspective, since regular GNNs cannot distinguish certain types of graphs. In this work we close this gap between theory and practice. We show that GNNs with spherical representations are indeed universal approximators for predictions that are invariant to translation, and equivariant to permutation and rotation. We then discretize such GNNs via directed edge embeddings and two-hop message passing, and incorporate multiple structural improvements to arrive at the geometric message passing neural network (GemNet). We demonstrate the benefits of the proposed changes in multiple ablation studies. GemNet outperforms previous models on the COLL, MD17, and OC20 datasets by 34%, 41%, and 20%, respectively, and performs especially well on the most challenging molecules. Our implementation is available online.",True,True,"Gasteiger, Johannes and Becker, Florian and Günnemann, Stephan",2021,,,,Advances in Neural Information Processing Systems
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,Batzner_2022nequip,\cite{Batzner_2022nequip},E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials,,,True,False,"Batzner, Simon and Musaelian, Albert and Sun, Lixin and Geiger, Mario and Mailoa, Jonathan P. and Kornbluth, Mordechai and Molinari, Nicola and Smidt, Tess E. and Kozinsky, Boris",2022,,http://dx.doi.org/10.1038/s41467-022-29939-5,10.1038/s41467-022-29939-5,Nature Communications
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,qu2024importance,\cite{qu2024importance},The importance of being scalable: Improving the speed and accuracy of neural network interatomic potentials across chemical domains,,,True,False,"Qu, Eric and Krishnapriyan, Aditi",2024,,,,Advances in Neural Information Processing Systems
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,neumann2024orbfastscalableneural,\cite{neumann2024orbfastscalableneural},"Orb: A Fast, Scalable Neural Network Potential",https://arxiv.org/abs/2410.22570v1,"We introduce Orb, a family of universal interatomic potentials for atomistic modelling of materials. Orb models are 3-6 times faster than existing universal potentials, stable under simulation for a range of out of distribution materials and, upon release, represented a 31% reduction in error over other methods on the Matbench Discovery benchmark. We explore several aspects of foundation model development for materials, with a focus on diffusion pretraining. We evaluate Orb as a model for geometry optimization, Monte Carlo and molecular dynamics simulations.",True,True,Mark Neumann and James Gin and Benjamin Rhodes and Steven Bennett and Zhiyi Li and Hitarth Choubisa and Arthur Hussey and Jonathan Godwin,2024,,https://arxiv.org/abs/2410.22570,,
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,mazitov2025petmadlightweightuniversalinteratomic,\cite{mazitov2025petmadlightweightuniversalinteratomic},"PET-MAD, a lightweight universal interatomic potential for advanced materials modeling",https://arxiv.org/abs/2503.14118v2,"Machine-learning interatomic potentials (MLIPs) have greatly extended the reach of atomic-scale simulations, offering the accuracy of first-principles calculations at a fraction of the cost. Leveraging large quantum mechanical databases and expressive architectures, recent ''universal'' models deliver qualitative accuracy across the periodic table but are often biased toward low-energy configurations. We introduce PET-MAD, a generally applicable MLIP trained on a dataset combining stable inorganic and organic solids, systematically modified to enhance atomic diversity. Using a moderate but highly-consistent level of electronic-structure theory, we assess PET-MAD's accuracy on established benchmarks and advanced simulations of six materials. Despite the small training set and lightweight architecture, PET-MAD is competitive with state-of-the-art MLIPs for inorganic solids, while also being reliable for molecules, organic materials, and surfaces. It is stable and fast, enabling the near-quantitative study of thermal and quantum mechanical fluctuations, functional properties, and phase transitions out of the box. It can be efficiently fine-tuned to deliver full quantum mechanical accuracy with a minimal number of targeted calculations.",True,True,Arslan Mazitov and Filippo Bigi and Matthias Kellner and Paolo Pegolo and Davide Tisi and Guillaume Fraux and Sergey Pozdnyakov and Philip Loche and Michele Ceriotti,2025,,https://arxiv.org/abs/2503.14118,,
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,gat_oversmooth,\cite{gat_oversmooth},Demystifying Oversmoothing in Attention-Based Graph Neural Networks,https://arxiv.org/abs/2305.16102v4,"Oversmoothing in Graph Neural Networks (GNNs) refers to the phenomenon where increasing network depth leads to homogeneous node representations. While previous work has established that Graph Convolutional Networks (GCNs) exponentially lose expressive power, it remains controversial whether the graph attention mechanism can mitigate oversmoothing. In this work, we provide a definitive answer to this question through a rigorous mathematical analysis, by viewing attention-based GNNs as nonlinear time-varying dynamical systems and incorporating tools and techniques from the theory of products of inhomogeneous matrices and the joint spectral radius. We establish that, contrary to popular belief, the graph attention mechanism cannot prevent oversmoothing and loses expressive power exponentially. The proposed framework extends the existing results on oversmoothing for symmetric GCNs to a significantly broader class of GNN models, including random walk GCNs, Graph Attention Networks (GATs) and (graph) transformers. In particular, our analysis accounts for asymmetric, state-dependent and time-varying aggregation operators and a wide range of common nonlinear activation functions, such as ReLU, LeakyReLU, GELU and SiLU.",True,True,"Wu, Xinyi and Ajorlou, Amir and Wu, Zihui and Jadbabaie, Ali",2023,,https://proceedings.neurips.cc/paper_files/paper/2023/file/6e4cdfdd909ea4e34bfc85a12774cba0-Paper-Conference.pdf,,
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,liao2024equiformerv2,\cite{liao2024equiformerv2},EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations,,,True,False,Yi-Lun Liao and Brandon Wood and Abhishek Das and Tess Smidt,2024,,https://arxiv.org/abs/2306.12059,,
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,vaswani2023attentionneed,\cite{vaswani2023attentionneed},Not All Attention Is All You Need,https://arxiv.org/abs/2104.04692v3,"Beyond the success story of pre-trained language models (PrLMs) in recent natural language processing, they are susceptible to over-fitting due to unusual large model size. To this end, dropout serves as a therapy. However, existing methods like random-based, knowledge-based and search-based dropout are more general but less effective onto self-attention based models, which are broadly chosen as the fundamental architecture of PrLMs. In this paper, we propose a novel dropout method named AttendOut to let self-attention empowered PrLMs capable of more robust task-specific tuning. We demonstrate that state-of-the-art models with elaborate training design may achieve much stronger results. We verify the universality of our approach on extensive natural language processing tasks.",True,True,Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin,2023,,https://arxiv.org/abs/1706.03762,,
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,chithrananda2020chembertalargescaleselfsupervisedpretraining,\cite{chithrananda2020chembertalargescaleselfsupervisedpretraining},ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction,https://arxiv.org/abs/2010.09885v2,"GNNs and chemical fingerprints are the predominant approaches to representing molecules for property prediction. However, in NLP, transformers have become the de-facto standard for representation learning thanks to their strong downstream task transfer. In parallel, the software ecosystem around transformers is maturing rapidly, with libraries like HuggingFace and BertViz enabling streamlined training and introspection. In this work, we make one of the first attempts to systematically evaluate transformers on molecular property prediction tasks via our ChemBERTa model. ChemBERTa scales well with pretraining dataset size, offering competitive downstream performance on MoleculeNet and useful attention-based visualization modalities. Our results suggest that transformers offer a promising avenue of future work for molecular representation learning and property prediction. To facilitate these efforts, we release a curated dataset of 77M SMILES from PubChem suitable for large-scale self-supervised pretraining.",True,True,Seyone Chithrananda and Gabriel Grand and Bharath Ramsundar,2020,,https://arxiv.org/abs/2010.09885,,
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,kim2022puretransformerspowerfulgraph,\cite{kim2022puretransformerspowerfulgraph},Pure Transformers are Powerful Graph Learners,,,True,False,Jinwoo Kim and Tien Dat Nguyen and Seonwoo Min and Sungjun Cho and Moontae Lee and Honglak Lee and Seunghoon Hong,2022,,https://arxiv.org/abs/2207.02505,,
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,rampášek2023recipegeneralpowerfulscalable,\cite{rampášek2023recipegeneralpowerfulscalable},"Recipe for a General, Powerful, Scalable Graph Transformer",,,True,False,Ladislav Rampášek and Mikhail Galkin and Vijay Prakash Dwivedi and Anh Tuan Luu and Guy Wolf and Dominique Beaini,2023,,https://arxiv.org/abs/2205.12454,,
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,zhou2023unimol,\cite{zhou2023unimol},Uni-Mol: A Universal 3D Molecular Representation Learning Framework,,,True,False,Gengmo Zhou and Zhifeng Gao and Qiankun Ding and Hang Zheng and Hongteng Xu and Zhewei Wei and Linfeng Zhang and Guolin Ke,2023,,https://openreview.net/forum?id=6K2RM6wVqKu,,
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,eissler2025simplegoofftheshelftransformer,\cite{eissler2025simplegoofftheshelftransformer},How simple can you go? An off-the-shelf transformer approach to molecular dynamics,https://arxiv.org/abs/2503.01431v2,"Most current neural networks for molecular dynamics (MD) include physical inductive biases, resulting in specialized and complex architectures. This is in contrast to most other machine learning domains, where specialist approaches are increasingly replaced by general-purpose architectures trained on vast datasets. In line with this trend, several recent studies have questioned the necessity of architectural features commonly found in MD models, such as built-in rotational equivariance or energy conservation. In this work, we contribute to the ongoing discussion by evaluating the performance of an MD model with as few specialized architectural features as possible. We present a recipe for MD using an Edge Transformer, an ""off-the-shelf'' transformer architecture that has been minimally modified for the MD domain, termed MD-ET. Our model implements neither built-in equivariance nor energy conservation. We use a simple supervised pre-training scheme on $\sim$30 million molecular structures from the QCML database. Using this ""off-the-shelf'' approach, we show state-of-the-art results on several benchmarks after fine-tuning for a small number of steps. Additionally, we examine the effects of being only approximately equivariant and energy conserving for MD simulations, proposing a novel method for distinguishing the errors resulting from non-equivariance from other sources of inaccuracies like numerical rounding errors. While our model exhibits runaway energy increases on larger structures, we show approximately energy-conserving NVE simulations for a range of small structures.",True,True,Max Eissler and Tim Korjakow and Stefan Ganscha and Oliver T. Unke and Klaus-Robert Müller and Stefan Gugler,2025,,https://arxiv.org/abs/2503.01431,,
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,Abramson2024,\cite{Abramson2024},Accurate structure prediction of biomolecular interactions with AlphaFold 3,,,True,False,"Abramson,  Josh and Adler,  Jonas and Dunger,  Jack and Evans,  Richard and Green,  Tim and Pritzel,  Alexander and Ronneberger,  Olaf and Willmore,  Lindsay and Ballard,  Andrew J. and Bambrick,  Joshua and Bodenstein,  Sebastian W. and Evans,  David A. and Hung,  Chia-Chun and O’Neill,  Michael and Reiman,  David and Tunyasuvunakool,  Kathryn and Wu,  Zachary and Žemgulytė,  Akvilė and Arvaniti,  Eirini and Beattie,  Charles and Bertolli,  Ottavia and Bridgland,  Alex and Cherepanov,  Alexey and Congreve,  Miles and Cowen-Rivers,  Alexander I. and Cowie,  Andrew and Figurnov,  Michael and Fuchs,  Fabian B. and Gladman,  Hannah and Jain,  Rishub and Khan,  Yousuf A. and Low,  Caroline M. R. and Perlin,  Kuba and Potapenko,  Anna and Savy,  Pascal and Singh,  Sukhdeep and Stecula,  Adrian and Thillaisundaram,  Ashok and Tong,  Catherine and Yakneen,  Sergei and Zhong,  Ellen D. and Zielinski,  Michal and Žídek,  Augustin and Bapst,  Victor and Kohli,  Pushmeet and Jaderberg,  Max and Hassabis,  Demis and Jumper,  John M.",2024,,http://dx.doi.org/10.1038/s41586-024-07487-w,10.1038/s41586-024-07487-w,Nature
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,joshi2025allatomdiffusiontransformersunified,\cite{joshi2025allatomdiffusiontransformersunified},All-atom Diffusion Transformers: Unified generative modelling of molecules and materials,https://arxiv.org/abs/2503.03965v2,"Diffusion models are the standard toolkit for generative modelling of 3D atomic systems. However, for different types of atomic systems -- such as molecules and materials -- the generative processes are usually highly specific to the target system despite the underlying physics being the same. We introduce the All-atom Diffusion Transformer (ADiT), a unified latent diffusion framework for jointly generating both periodic materials and non-periodic molecular systems using the same model: (1) An autoencoder maps a unified, all-atom representations of molecules and materials to a shared latent embedding space; and (2) A diffusion model is trained to generate new latent embeddings that the autoencoder can decode to sample new molecules or materials. Experiments on MP20, QM9 and GEOM-DRUGS datasets demonstrate that jointly trained ADiT generates realistic and valid molecules as well as materials, obtaining state-of-the-art results on par with molecule and crystal-specific models. ADiT uses standard Transformers with minimal inductive biases for both the autoencoder and diffusion model, resulting in significant speedups during training and inference compared to equivariant diffusion models. Scaling ADiT up to half a billion parameters predictably improves performance, representing a step towards broadly generalizable foundation models for generative chemistry. Open source code: https://github.com/facebookresearch/all-atom-diffusion-transformer",True,True,Chaitanya K. Joshi and Xiang Fu and Yi-Lun Liao and Vahe Gharakhanyan and Benjamin Kurt Miller and Anuroop Sriram and Zachary W. Ulissi,2025,,https://arxiv.org/abs/2503.03965,,
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,wang2024swallowingbitterpillsimplified,\cite{wang2024swallowingbitterpillsimplified},Swallowing the Bitter Pill: Simplified Scalable Conformer Generation,,,True,False,Yuyang Wang and Ahmed A. Elhag and Navdeep Jaitly and Joshua M. Susskind and Miguel Angel Bautista,2024,,https://arxiv.org/abs/2311.17932,,
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,vadgama2025probingequivariancesymmetrybreaking,\cite{vadgama2025probingequivariancesymmetrybreaking},Probing Equivariance and Symmetry Breaking in Convolutional Networks,,,True,False,Sharvaree Vadgama and Mohammad Mohaiminul Islam and Domas Buracas and Christian Shewmake and Artem Moskalev and Erik Bekkers,2025,,https://arxiv.org/abs/2501.01999,,
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,devlin2019bertpretrainingdeepbidirectional,\cite{devlin2019bertpretrainingdeepbidirectional},BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,https://arxiv.org/abs/1810.04805v2,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.
  BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",True,True,Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova,2019,,https://arxiv.org/abs/1810.04805,,
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,grattafiori2024llama3herdmodels,\cite{grattafiori2024llama3herdmodels},The Llama 3 Herd of Models,,,True,False,Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Alex Vaughan and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurelien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and Charlotte Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Canton Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and Danny Wyatt and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Francisco Guzmán and Frank Zhang and Gabriel Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Govind Thattai and Graeme Nail and Gregoire Mialon and Guan Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel Kloumann and Ishan Misra and Ivan Evtimov and Jack Zhang and Jade Copet and Jaewon Lee and Jan Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and Jelmer van der Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Junteng Jia and Kalyan Vasuden Alwala and Karthik Prasad and Kartikeya Upasani and Kate Plawiak and Ke Li and Kenneth Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuenley Chiu and Kunal Bhalla and Kushal Lakhotia and Lauren Rantala-Yeary and Laurens van der Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and Mahesh Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Maria Tsimpoukelli and Mathew Oldham and Mathieu Rita and Maya Pavlova and Melanie Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri Chatterji and Ning Zhang and Olivier Duchenne and Onur Çelebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasic and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohan Maheswari and Rohit Girdhar and Rohit Patel and Romain Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and Saghar Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and Sharath Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and Sydney Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and Vítor Albiero and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whitney Meers and Xavier Martinet and Xiaodong Wang and Xiaofang Wang and Xiaoqing Ellen Tan and Xide Xia and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yi Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zheng Yan and Zhengxing Chen and Zoe Papakipos and Aaditya Singh and Aayushi Srivastava and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adithya Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alexei Baevski and Allie Feinstein and Amanda Kallet and Amit Sangani and Amos Teo and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Dong and Annie Franco and Anuj Goyal and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Benjamin Leonhardi and Bernie Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Ce Liu and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Cynthia Gao and Damon Civin and Dana Beaty and Daniel Kreymer and Daniel Li and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Eric-Tuan Le and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Filippos Kokkinos and Firat Ozgenel and Francesco Caggioni and Frank Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and Grant Herman and Grigory Sizov and Guangyi and Zhang and Guna Lakshminarayanan and Hakan Inan and Hamid Shojanazeri and Han Zou and Hannah Wang and Hanwen Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Hongyuan Zhan and Ibrahim Damlaj and Igor Molybog and Igor Tufanov and Ilias Leontiadis and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Janice Lam and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kai Wu and Kam Hou U and Karan Saxena and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and Kaushik Veeraraghavan and Kelly Michelena and Keqian Li and Kiran Jagadeesh and Kun Huang and Kunal Chawla and Kyle Huang and Lailin Chen and Lakshya Garg and Lavender A and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and Martynas Mankus and Matan Hasson and Matthew Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and Miao Liu and Michael L. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikhil Mehta and Nikolay Pavlovich Laptev and Ning Dong and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Dollar and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Rangaprabhu Parthasarathy and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Russ Howes and Ruty Rinott and Sachin Mehta and Sachin Siby and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Mahajan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Shaun Lindsay and Sheng Feng and Shenghao Lin and Shengxin Cindy Zha and Shishir Patil and Shiva Shankar and Shuqiang Zhang and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and Soji Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Summer Deng and Sungmin Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and Tal Remez and Tamar Glaser and Tamara Best and Thilo Koehler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vlad Ionescu and Vlad Poenaru and Vlad Tiberiu Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xiaocheng Tang and Xiaojian Wu and Xiaolan Wang and Xilun Wu and Xinbo Gao and Yaniv Kleinman and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and Yu and Wang and Yu Zhao and Yuchen Hao and Yundi Qian and Yunlu Li and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao and Zhiyu Ma,2024,,https://arxiv.org/abs/2407.21783,,
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,touvron2023llamaopenefficientfoundation,\cite{touvron2023llamaopenefficientfoundation},LLaMA: Open and Efficient Foundation Language Models,,,True,False,Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample,2023,,https://arxiv.org/abs/2302.13971,,
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,dosovitskiy2021imageworth16x16wordsvit,\cite{dosovitskiy2021imageworth16x16wordsvit},An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,https://arxiv.org/abs/2010.11929v2,"While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",True,True,Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby,2021,,https://arxiv.org/abs/2010.11929,,
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,octomodelteam2024octoopensourcegeneralistrobot,\cite{octomodelteam2024octoopensourcegeneralistrobot},Octo: An Open-Source Generalist Robot Policy,https://arxiv.org/abs/2405.12213v2,"Large policies pretrained on diverse robot datasets have the potential to transform robotic learning: instead of training new policies from scratch, such generalist robot policies may be finetuned with only a little in-domain data, yet generalize broadly. However, to be widely applicable across a range of robotic learning scenarios, environments, and tasks, such policies need to handle diverse sensors and action spaces, accommodate a variety of commonly used robotic platforms, and finetune readily and efficiently to new domains. In this work, we aim to lay the groundwork for developing open-source, widely applicable, generalist policies for robotic manipulation. As a first step, we introduce Octo, a large transformer-based policy trained on 800k trajectories from the Open X-Embodiment dataset, the largest robot manipulation dataset to date. It can be instructed via language commands or goal images and can be effectively finetuned to robot setups with new sensory inputs and action spaces within a few hours on standard consumer GPUs. In experiments across 9 robotic platforms, we demonstrate that Octo serves as a versatile policy initialization that can be effectively finetuned to new observation and action spaces. We also perform detailed ablations of design decisions for the Octo model, from architecture to training data, to guide future research on building generalist robot models.",True,True,Octo Model Team and Dibya Ghosh and Homer Walke and Karl Pertsch and Kevin Black and Oier Mees and Sudeep Dasari and Joey Hejna and Tobias Kreiman and Charles Xu and Jianlan Luo and You Liang Tan and Lawrence Yunliang Chen and Pannag Sanketi and Quan Vuong and Ted Xiao and Dorsa Sadigh and Chelsea Finn and Sergey Levine,2024,,https://arxiv.org/abs/2405.12213,,
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,kim2024openvlaopensourcevisionlanguageactionmodel,\cite{kim2024openvlaopensourcevisionlanguageactionmodel},OpenVLA: An Open-Source Vision-Language-Action Model,https://arxiv.org/abs/2406.09246v3,"Large policies pretrained on a combination of Internet-scale vision-language data and diverse robot demonstrations have the potential to change how we teach robots new skills: rather than training new behaviors from scratch, we can fine-tune such vision-language-action (VLA) models to obtain robust, generalizable policies for visuomotor control. Yet, widespread adoption of VLAs for robotics has been challenging as 1) existing VLAs are largely closed and inaccessible to the public, and 2) prior work fails to explore methods for efficiently fine-tuning VLAs for new tasks, a key component for adoption. Addressing these challenges, we introduce OpenVLA, a 7B-parameter open-source VLA trained on a diverse collection of 970k real-world robot demonstrations. OpenVLA builds on a Llama 2 language model combined with a visual encoder that fuses pretrained features from DINOv2 and SigLIP. As a product of the added data diversity and new model components, OpenVLA demonstrates strong results for generalist manipulation, outperforming closed models such as RT-2-X (55B) by 16.5% in absolute task success rate across 29 tasks and multiple robot embodiments, with 7x fewer parameters. We further show that we can effectively fine-tune OpenVLA for new settings, with especially strong generalization results in multi-task environments involving multiple objects and strong language grounding abilities, and outperform expressive from-scratch imitation learning methods such as Diffusion Policy by 20.4%. We also explore compute efficiency; as a separate contribution, we show that OpenVLA can be fine-tuned on consumer GPUs via modern low-rank adaptation methods and served efficiently via quantization without a hit to downstream success rate. Finally, we release model checkpoints, fine-tuning notebooks, and our PyTorch codebase with built-in support for training VLAs at scale on Open X-Embodiment datasets.",True,True,Moo Jin Kim and Karl Pertsch and Siddharth Karamcheti and Ted Xiao and Ashwin Balakrishna and Suraj Nair and Rafael Rafailov and Ethan Foster and Grace Lam and Pannag Sanketi and Quan Vuong and Thomas Kollar and Benjamin Burchfiel and Russ Tedrake and Dorsa Sadigh and Sergey Levine and Percy Liang and Chelsea Finn,2024,,https://arxiv.org/abs/2406.09246,,
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,kaplan2020scaling,\cite{kaplan2020scaling},Scaling Laws for Neural Language Models,,,True,False,Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei,2020,,,,
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,henighan2020scalinglawsautoregressivegenerative,\cite{henighan2020scalinglawsautoregressivegenerative},Scaling Laws for Autoregressive Generative Modeling,,,True,False,Tom Henighan and Jared Kaplan and Mor Katz and Mark Chen and Christopher Hesse and Jacob Jackson and Heewoo Jun and Tom B. Brown and Prafulla Dhariwal and Scott Gray and Chris Hallacy and Benjamin Mann and Alec Radford and Aditya Ramesh and Nick Ryder and Daniel M. Ziegler and John Schulman and Dario Amodei and Sam McCandlish,2020,,https://arxiv.org/abs/2010.14701,,
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,hoffmann2022trainingcomputeoptimallargelanguage,\cite{hoffmann2022trainingcomputeoptimallargelanguage},Training Compute-Optimal Large Language Models,https://arxiv.org/abs/2203.15556v1,"We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$\times$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.",True,True,Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre,2022,,https://arxiv.org/abs/2203.15556,,
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,brown2020languagemodelsfewshotlearners,\cite{brown2020languagemodelsfewshotlearners},Language Models are Few-Shot Learners,,,True,False,Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei,2020,,https://arxiv.org/abs/2005.14165,,
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,Frey2023,\cite{Frey2023},Neural scaling of deep chemical models,,,True,False,"Frey,  Nathan C. and Soklaski,  Ryan and Axelrod,  Simon and Samsi,  Siddharth and Gómez-Bombarelli,  Rafael and Coley,  Connor W. and Gadepally,  Vijay",2023,,http://dx.doi.org/10.1038/s42256-023-00740-3,10.1038/s42256-023-00740-3,Nature Machine Intelligence
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,wood2025umafamilyuniversalmodels,\cite{wood2025umafamilyuniversalmodels},UMA: A Family of Universal Models for Atoms,,,True,False,Brandon M. Wood and Misko Dzamba and Xiang Fu and Meng Gao and Muhammed Shuaibi and Luis Barroso-Luque and Kareem Abdelmaqsoud and Vahe Gharakhanyan and John R. Kitchin and Daniel S. Levine and Kyle Michel and Anuroop Sriram and Taco Cohen and Abhishek Das and Ammar Rizvi and Sushree Jagriti Sahoo and Zachary W. Ulissi and C. Lawrence Zitnick,2025,,https://arxiv.org/abs/2506.23971,,
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,levine2025openmolecules2025omol25,\cite{levine2025openmolecules2025omol25},"The open molecules 2025 (omol25) dataset, evaluations, and models",,,True,False,Daniel S. Levine and Muhammed Shuaibi and Evan Walter Clark Spotte-Smith and Michael G. Taylor and Muhammad R. Hasyim and Kyle Michel and Ilyes Batatia and Gábor Csányi and Misko Dzamba and Peter Eastman and Nathan C. Frey and Xiang Fu and Vahe Gharakhanyan and Aditi S. Krishnapriyan and Joshua A. Rackers and Sanjeev Raja and Ammar Rizvi and Andrew S. Rosen and Zachary Ulissi and Santiago Vargas and C. Lawrence Zitnick and Samuel M. Blau and Brandon M. Wood,2025,,https://arxiv.org/abs/2505.08762,,arXiv preprint arXiv:2505.08762
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,sriram2022trainingbillionparametergraph,\cite{sriram2022trainingbillionparametergraph},Towards Training Billion Parameter Graph Neural Networks for Atomic Simulations,https://arxiv.org/abs/2203.09697v1,"Recent progress in Graph Neural Networks (GNNs) for modeling atomic simulations has the potential to revolutionize catalyst discovery, which is a key step in making progress towards the energy breakthroughs needed to combat climate change. However, the GNNs that have proven most effective for this task are memory intensive as they model higher-order interactions in the graphs such as those between triplets or quadruplets of atoms, making it challenging to scale these models. In this paper, we introduce Graph Parallelism, a method to distribute input graphs across multiple GPUs, enabling us to train very large GNNs with hundreds of millions or billions of parameters. We empirically evaluate our method by scaling up the number of parameters of the recently proposed DimeNet++ and GemNet models by over an order of magnitude. On the large-scale Open Catalyst 2020 (OC20) dataset, these graph-parallelized models lead to relative improvements of 1) 15% on the force MAE metric for the S2EF task and 2) 21% on the AFbT metric for the IS2RS task, establishing new state-of-the-art results.",True,True,Anuroop Sriram and Abhishek Das and Brandon M. Wood and Siddharth Goyal and C. Lawrence Zitnick,2022,,https://arxiv.org/abs/2203.09697,,
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,kovács2023maceoff23,\cite{kovács2023maceoff23},MACE-OFF23: Transferable Machine Learning Force Fields for Organic Molecules,,,True,False,Dávid Péter Kovács and J. Harry Moore and Nicholas J. Browning and Ilyes Batatia and Joshua T. Horton and Venkat Kapil and William C. Witt and Ioan-Bogdan Magdău and Daniel J. Cole and Gábor Csányi,2023,,,,
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,rusch2023surveyoversmoothinggraphneural,\cite{rusch2023surveyoversmoothinggraphneural},A Survey on Oversmoothing in Graph Neural Networks,https://arxiv.org/abs/2303.10993v1,"Node features of graph neural networks (GNNs) tend to become more similar with the increase of the network depth. This effect is known as over-smoothing, which we axiomatically define as the exponential convergence of suitable similarity measures on the node features. Our definition unifies previous approaches and gives rise to new quantitative measures of over-smoothing. Moreover, we empirically demonstrate this behavior for several over-smoothing measures on different graphs (small-, medium-, and large-scale). We also review several approaches for mitigating over-smoothing and empirically test their effectiveness on real-world graph datasets. Through illustrative examples, we demonstrate that mitigating over-smoothing is a necessary but not sufficient condition for building deep GNNs that are expressive on a wide range of graph learning tasks. Finally, we extend our definition of over-smoothing to the rapidly emerging field of continuous-time GNNs.",True,True,T. Konstantin Rusch and Michael M. Bronstein and Siddhartha Mishra,2023,,https://arxiv.org/abs/2303.10993,,
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,digiovanni2023oversquashingmessagepassingneural,\cite{digiovanni2023oversquashingmessagepassingneural},"On Over-Squashing in Message Passing Neural Networks: The Impact of Width, Depth, and Topology",https://arxiv.org/abs/2302.02941v3,"Message Passing Neural Networks (MPNNs) are instances of Graph Neural Networks that leverage the graph to send messages over the edges. This inductive bias leads to a phenomenon known as over-squashing, where a node feature is insensitive to information contained at distant nodes. Despite recent methods introduced to mitigate this issue, an understanding of the causes for over-squashing and of possible solutions are lacking. In this theoretical work, we prove that: (i) Neural network width can mitigate over-squashing, but at the cost of making the whole network more sensitive; (ii) Conversely, depth cannot help mitigate over-squashing: increasing the number of layers leads to over-squashing being dominated by vanishing gradients; (iii) The graph topology plays the greatest role, since over-squashing occurs between nodes at high commute (access) time. Our analysis provides a unified framework to study different recent methods introduced to cope with over-squashing and serves as a justification for a class of methods that fall under graph rewiring.",True,True,Francesco Di Giovanni and Lorenzo Giusti and Federico Barbero and Giulia Luise and Pietro Lio' and Michael Bronstein,2023,,https://arxiv.org/abs/2302.02941,,
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,topping2022understandingoversquashingbottlenecksgraphs,\cite{topping2022understandingoversquashingbottlenecksgraphs},Understanding over-squashing and bottlenecks on graphs via curvature,,,True,False,Jake Topping and Francesco Di Giovanni and Benjamin Paul Chamberlain and Xiaowen Dong and Michael M. Bronstein,2022,,https://arxiv.org/abs/2111.14522,,
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,dwivedi2023longrangegraphbenchmark,\cite{dwivedi2023longrangegraphbenchmark},Long Range Graph Benchmark,https://arxiv.org/abs/2206.08164v4,"Graph Neural Networks (GNNs) that are based on the message passing (MP) paradigm generally exchange information between 1-hop neighbors to build node representations at each layer. In principle, such networks are not able to capture long-range interactions (LRI) that may be desired or necessary for learning a given task on graphs. Recently, there has been an increasing interest in development of Transformer-based methods for graphs that can consider full node connectivity beyond the original sparse structure, thus enabling the modeling of LRI. However, MP-GNNs that simply rely on 1-hop message passing often fare better in several existing graph benchmarks when combined with positional feature representations, among other innovations, hence limiting the perceived utility and ranking of Transformer-like architectures. Here, we present the Long Range Graph Benchmark (LRGB) with 5 graph learning datasets: PascalVOC-SP, COCO-SP, PCQM-Contact, Peptides-func and Peptides-struct that arguably require LRI reasoning to achieve strong performance in a given task. We benchmark both baseline GNNs and Graph Transformer networks to verify that the models which capture long-range dependencies perform significantly better on these tasks. Therefore, these datasets are suitable for benchmarking and exploration of MP-GNNs and Graph Transformer architectures that are intended to capture LRI.",True,True,Vijay Prakash Dwivedi and Ladislav Rampášek and Mikhail Galkin and Ali Parviz and Guy Wolf and Anh Tuan Luu and Dominique Beaini,2023,,https://arxiv.org/abs/2206.08164,,
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,bechlerspeicher2024gnnregular,\cite{bechlerspeicher2024gnnregular},Graph Neural Networks Use Graphs When They Shouldn't,,,True,False,Maya Bechler-Speicher and Ido Amos and Ran Gilad-Bachrach and Amir Globerson,2024,,https://arxiv.org/abs/2309.04332,,
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,kreiman2025understandingmitigatingdistributionshifts,\cite{kreiman2025understandingmitigatingdistributionshifts},Understanding and Mitigating Distribution Shifts For Machine Learning Force Fields,,,True,False,"Kreiman, Tobias and Krishnapriyan, Aditi S",2025,,https://arxiv.org/abs/2503.08674,,arXiv preprint arXiv:2503.08674
Transformers Discover Molecular Structure Without Graph Priors,2510.02259v1,powergraph,\cite{powergraph},PowerGraph: distributed graph-parallel computation on natural graphs,,,True,False,"Gonzalez, Joseph E. and Low, Yucheng and Gu, Haijie and Bickson, Danny and Guestrin, Carlos",2012,,,,
Prostate-VarBench: A Benchmark with Interpretable TabNet Framework for Prostate Cancer Variant Classification,2511.09576v1,Ng2003SIFT,\cite{Ng2003SIFT},SIFT: predicting amino acid changes that affect protein function,,,True,False,"Ng, Pauline C. and Henikoff, Steven",2003,,,10.1093/nar/gkg509,Nucleic Acids Research
Prostate-VarBench: A Benchmark with Interpretable TabNet Framework for Prostate Cancer Variant Classification,2511.09576v1,Adzhubei2010PolyPhen2,\cite{Adzhubei2010PolyPhen2},A method and server for predicting damaging missense mutations,,,True,False,"Adzhubei, Ivan A. and Schmidt, Daniel and Peshkin, Leonid and et al.",2010,,,10.1038/nmeth0410-248,Nature Methods
Prostate-VarBench: A Benchmark with Interpretable TabNet Framework for Prostate Cancer Variant Classification,2511.09576v1,Kircher2014CADD,\cite{Kircher2014CADD},A general framework for estimating the relative pathogenicity of human genetic variants,,,True,False,"Kircher, Martin and Witten, Daniel M. and Jain, Preti and et al.",2014,,,10.1038/ng.2892,Nature Genetics
Prostate-VarBench: A Benchmark with Interpretable TabNet Framework for Prostate Cancer Variant Classification,2511.09576v1,Dong2015MetaSVM,\cite{Dong2015MetaSVM},Comparison and integration of deleteriousness prediction methods for nonsynonymous SNVs in whole exome sequencing studies,,,True,False,"Dong, Chengliang and Wei, Panyu and Jian, Xiang and et al.",2015,,,10.1093/hmg/ddu733,Human Molecular Genetics
Prostate-VarBench: A Benchmark with Interpretable TabNet Framework for Prostate Cancer Variant Classification,2511.09576v1,Popejoy2016GenomicsDiversity,\cite{Popejoy2016GenomicsDiversity},Genomics is failing on diversity,,,True,False,"Popejoy, Alice B. and Fullerton, Stephanie M.",2016,,https://doi.org/10.1038/538161a,10.1038/538161a,Nature
Prostate-VarBench: A Benchmark with Interpretable TabNet Framework for Prostate Cancer Variant Classification,2511.09576v1,Chen2023JAMANetOpenVUS,\cite{Chen2023JAMANetOpenVUS},Rates and Classification of Variants of Uncertain Significance in Hereditary Disease Genetic Testing,,,True,False,"Chen, Eric and Sinha, Saurabh and Rishishwar, Lavanya and et al.",2023,,,10.1001/jamanetworkopen.2023.43586,JAMA Network Open
Prostate-VarBench: A Benchmark with Interpretable TabNet Framework for Prostate Cancer Variant Classification,2511.09576v1,Richards2015ACMGAMP,\cite{Richards2015ACMGAMP},Standards and guidelines for the interpretation of sequence variants,,,True,False,"Richards, Sue and Aziz, Nazneen and Bale, Sherri and et al.",2015,,,10.1038/gim.2015.30,Genetics in Medicine
Prostate-VarBench: A Benchmark with Interpretable TabNet Framework for Prostate Cancer Variant Classification,2511.09576v1,Khandakji2022BRCA2XGB,\cite{Khandakji2022BRCA2XGB},Gene-specific machine learning model to predict the pathogenicity of BRCA2 variants,,,True,False,"Khandakji, Malik N. and et al.",2022,,,10.3389/fgene.2022.982930,Frontiers in Genetics
Prostate-VarBench: A Benchmark with Interpretable TabNet Framework for Prostate Cancer Variant Classification,2511.09576v1,Tamborero2022_MTBP,\cite{Tamborero2022_MTBP},The Molecular Tumor Board Portal supports clinical decisions and automated reporting for precision oncology,,,True,False,"Tamborero, David and Dienstmann, Rodrigo and Rachid, Maan Haj and Boekel, Jorrit and López-Fernández, Adrià and Jonsson, Markus and Razzak, Ali and Braña, Irene and De Petris, Luigi and Yachnin, Jeffrey and Baird, Richard D. and Loriot, Yohann and Massard, Christophe and Martin-Romano, Patricia and Opdam, Frans and Schlenk, Richard F. and Vernieri, Claudio and Masucci, Michele and Villalobos, Xenia and Chavarria, Elena and the Cancer Core Europe consortium and Balmaña, Judith and Apolone, Giovanni and Caldas, Carlos and Bergh, Jonas and Ernberg, Ingemar and Fröhling, Stefan and Garralda, Elena and Karlsson, Claes and Tabernero, Josep and Voest, Emile and Rodon, Jordi and Lehtiö, Janne",2022,,https://doi.org/10.1038/s43018-022-00332-x,10.1038/s43018-022-00332-x,Nature Cancer
Prostate-VarBench: A Benchmark with Interpretable TabNet Framework for Prostate Cancer Variant Classification,2511.09576v1,FDA2025_AIEnabledDSF_Draft,\cite{FDA2025_AIEnabledDSF_Draft},Artificial Intelligence-Enabled Device Software Functions: Lifecycle Management and Marketing Submission Recommendations,,,True,False,{U.S. Food and Drug Administration},2025,,https://www.fda.gov/regulatory-information/search-fda-guidance-documents/artificial-intelligence-enabled-device-software-functions-lifecycle-management-and-marketing,,
Prostate-VarBench: A Benchmark with Interpretable TabNet Framework for Prostate Cancer Variant Classification,2511.09576v1,FDA2021_GMLP,\cite{FDA2021_GMLP},Good Machine Learning Practice for Medical Device Development: Guiding Principles,,,True,False,{U.S. Food and Drug Administration and Health Canada and Medicines and Healthcare products Regulatory Agency},2021,,https://www.fda.gov/media/153486/download,,
Prostate-VarBench: A Benchmark with Interpretable TabNet Framework for Prostate Cancer Variant Classification,2511.09576v1,arik2020tabnetattentiveinterpretabletabular,\cite{arik2020tabnetattentiveinterpretabletabular},TabNet: Attentive Interpretable Tabular Learning,https://arxiv.org/abs/1908.07442v5,"We propose a novel high-performance and interpretable canonical deep tabular data learning architecture, TabNet. TabNet uses sequential attention to choose which features to reason from at each decision step, enabling interpretability and more efficient learning as the learning capacity is used for the most salient features. We demonstrate that TabNet outperforms other neural network and decision tree variants on a wide range of non-performance-saturated tabular datasets and yields interpretable feature attributions plus insights into the global model behavior. Finally, for the first time to our knowledge, we demonstrate self-supervised learning for tabular data, significantly improving performance with unsupervised representation learning when unlabeled data is abundant.",True,True,Sercan O. Arik and Tomas Pfister,2020,,https://arxiv.org/abs/1908.07442,,
Prostate-VarBench: A Benchmark with Interpretable TabNet Framework for Prostate Cancer Variant Classification,2511.09576v1,McLaughlin2023NPJ,\cite{McLaughlin2023NPJ},"Fast, accurate, and racially unbiased pan-cancer tumor-only variant calling with tabular machine learning",,,True,False,"McLaughlin, R. Tyler and Asthana, Maansi and Di Meo, Marc and Ceccarelli, Michele and Jacob, Howard J. and Masica, David L.",2023,,https://www.nature.com/articles/s41698-022-00340-1,10.1038/s41698-022-00340-1,NPJ Precision Oncology
Prostate-VarBench: A Benchmark with Interpretable TabNet Framework for Prostate Cancer Variant Classification,2511.09576v1,Arik2021TabNet,\cite{Arik2021TabNet},TabNet: Attentive Interpretable Tabular Learning,https://arxiv.org/abs/1908.07442v5,"We propose a novel high-performance and interpretable canonical deep tabular data learning architecture, TabNet. TabNet uses sequential attention to choose which features to reason from at each decision step, enabling interpretability and more efficient learning as the learning capacity is used for the most salient features. We demonstrate that TabNet outperforms other neural network and decision tree variants on a wide range of non-performance-saturated tabular datasets and yields interpretable feature attributions plus insights into the global model behavior. Finally, for the first time to our knowledge, we demonstrate self-supervised learning for tabular data, significantly improving performance with unsupervised representation learning when unlabeled data is abundant.",True,True,"Arik, Sercan {\""O}zkan and Pfister, Tomas",2021,,,,
SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression,2511.04838v1,almeida2024overcoming,\cite{almeida2024overcoming},Overcoming class imbalance in drug discovery problems: Graph neural networks and balancing approaches,,,True,False,"Almeida, Rafael Lopes and Maltarollo, Vinícius Gonçalves and Coelho, Frederico Gualberto Ferreira",2024,,,,Journal of Molecular Graphics and Modelling
SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression,2511.04838v1,bo2023survey,\cite{bo2023survey},A Survey on Spectral Graph Neural Networks,https://arxiv.org/abs/2302.05631v1,"Graph neural networks (GNNs) have attracted considerable attention from the research community. It is well established that GNNs are usually roughly divided into spatial and spectral methods. Despite that spectral GNNs play an important role in both graph signal processing and graph representation learning, existing studies are biased toward spatial approaches, and there is no comprehensive review on spectral GNNs so far. In this paper, we summarize the recent development of spectral GNNs, including model, theory, and application. Specifically, we first discuss the connection between spatial GNNs and spectral GNNs, which shows that spectral GNNs can capture global information and have better expressiveness and interpretability. Next, we categorize existing spectral GNNs according to the spectrum information they use, \ie, eigenvalues or eigenvectors. In addition, we review major theoretical results and applications of spectral GNNs, followed by a quantitative experiment to benchmark some popular spectral GNNs. Finally, we conclude the paper with some future directions.",True,True,"Bo, Deyu and Zheng, Chuan and Wang, Xinchen and Jiao, Peipei and Zhou, Shirui and Zhang, Hao and Wei, Zhewei and Shi, Chuan",2023,,,,arXiv preprint arXiv:2302.05631
SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression,2511.04838v1,wang2022powerful,\cite{wang2022powerful},How Powerful are Spectral Graph Neural Networks,,,True,False,"Wang, Xiyuan and Zhang, Ming",2022,,,,arXiv preprint arXiv:2205.11172
SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression,2511.04838v1,chawla2002smote,\cite{chawla2002smote},SMOTE: Synthetic Minority Over-sampling Technique,https://arxiv.org/abs/1106.1813v1,"An approach to the construction of classifiers from    imbalanced datasets is described. A dataset is imbalanced if the    classification categories are not approximately equally    represented. Often real-world data sets are predominately composed of    ""normal"" examples with only a small percentage of ""abnormal"" or    ""interesting"" examples. It is also the case that the cost of    misclassifying an abnormal (interesting) example as a normal example    is often much higher than the cost of the reverse    error. Under-sampling of the majority (normal) class has been proposed    as a good means of increasing the sensitivity of a classifier to the    minority class. This paper shows that a combination of our method of    over-sampling the minority (abnormal) class and under-sampling the    majority (normal) class can achieve better classifier performance (in    ROC space) than only under-sampling the majority class.  This paper    also shows that a combination of our method of over-sampling the    minority class and under-sampling the majority class can achieve    better classifier performance (in ROC space) than varying the loss    ratios in Ripper or class priors in Naive Bayes. Our method of    over-sampling the minority class involves creating synthetic minority    class examples.  Experiments are performed using C4.5, Ripper and a    Naive Bayes classifier. The method is evaluated using the area under    the Receiver Operating Characteristic curve (AUC) and the ROC convex    hull strategy.",True,True,"Chawla, Nitesh V and Bowyer, Kevin W and Hall, Lawrence O and Kegelmeyer, W Philip",2002,,,,Journal of artificial intelligence research
SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression,2511.04838v1,cui2019class,\cite{cui2019class},Class-Balanced Loss Based on Effective Number of Samples,https://arxiv.org/abs/1901.05555v1,"With the rapid increase of large-scale, real-world datasets, it becomes critical to address the problem of long-tailed data distribution (i.e., a few classes account for most of the data, while most classes are under-represented). Existing solutions typically adopt class re-balancing strategies such as re-sampling and re-weighting based on the number of observations for each class. In this work, we argue that as the number of samples increases, the additional benefit of a newly added data point will diminish. We introduce a novel theoretical framework to measure data overlap by associating with each sample a small neighboring region rather than a single point. The effective number of samples is defined as the volume of samples and can be calculated by a simple formula $(1-β^{n})/(1-β)$, where $n$ is the number of samples and $β\in [0,1)$ is a hyperparameter. We design a re-weighting scheme that uses the effective number of samples for each class to re-balance the loss, thereby yielding a class-balanced loss. Comprehensive experiments are conducted on artificially induced long-tailed CIFAR datasets and large-scale datasets including ImageNet and iNaturalist. Our results show that when trained with the proposed class-balanced loss, the network is able to achieve significant performance gains on long-tailed datasets.",True,True,"Cui, Yin and Jia, Menglin and Lin, Tsung-Yi and Song, Yang and Belongie, Serge",2019,,,,
SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression,2511.04838v1,lin2017focal,\cite{lin2017focal},Focal loss for dense object detection,,,True,False,"Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr",2017,,,,
SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression,2511.04838v1,cao2019learning,\cite{cao2019learning},Learning imbalanced datasets with label-distribution-aware margin loss,,,True,False,"Cao, Kaidi and Wei, Colin and Gaidon, Adrien and Arechiga, Nikos and Ma, Tengyu",2019,,,,Advances in neural information processing systems
SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression,2511.04838v1,menon2020long,\cite{menon2020long},Long-tail learning via logit adjustment,https://arxiv.org/abs/2007.07314v2,"Real-world classification problems typically exhibit an imbalanced or long-tailed label distribution, wherein many labels are associated with only a few samples. This poses a challenge for generalisation on such labels, and also makes naïve learning biased towards dominant labels. In this paper, we present two simple modifications of standard softmax cross-entropy training to cope with these challenges. Our techniques revisit the classic idea of logit adjustment based on the label frequencies, either applied post-hoc to a trained model, or enforced in the loss during training. Such adjustment encourages a large relative margin between logits of rare versus dominant labels. These techniques unify and generalise several recent proposals in the literature, while possessing firmer statistical grounding and empirical performance.",True,True,"Menon, Aditya Krishna and Jayasumana, Sadeep and Rawat, Ankit Singh and Jain, Himanshu and Veit, Andreas and Kumar, Sanjiv",2020,,,,arXiv preprint arXiv:2007.07314
SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression,2511.04838v1,tian2020posterior,\cite{tian2020posterior},Posterior re-calibration for imbalanced datasets,,,True,False,"Tian, Junjiao and Liu, Yen-Cheng and Glaser, Nathaniel and Hsu, Yen-Chang and Kira, Zsolt",2020,,,,Advances in neural information processing systems
SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression,2511.04838v1,ribeiro2020imbalanced,\cite{ribeiro2020imbalanced},Imbalanced regression and extreme value prediction,,,True,False,"Ribeiro, Rita P. and Moniz, Nuno",2020,,,,Machine Learning
SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression,2511.04838v1,branco2017smogn,\cite{branco2017smogn},SMOGN: a pre-processing approach for imbalanced regression,,,True,False,"Branco, Paula and Torgo, Lu{\'\i}s and Ribeiro, Rita P",2017,,,,
SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression,2511.04838v1,ren2022balanced,\cite{ren2022balanced},Balanced mse for imbalanced visual regression,,,True,False,"Ren, Jiawei and Zhang, Mingyuan and Yu, Cunjun and Liu, Ziwei",2022,,,,
SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression,2511.04838v1,yang2021delving,\cite{yang2021delving},Delving into deep imbalanced regression,,,True,False,"Yang, Yuzhe and Zha, Kaiwen and Chen, Yingcong and Wang, Hao and Katabi, Dina",2021,,,,
SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression,2511.04838v1,gong2022ranksim,\cite{gong2022ranksim},Ranksim: Ranking similarity regularization for deep imbalanced regression,,,True,False,"Gong, Yu and Mori, Greg and Tung, Frederick",2022,,,,arXiv preprint arXiv:2205.15236
SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression,2511.04838v1,ribeiro_imbalanced_2020,\cite{ribeiro_imbalanced_2020},Imbalanced regression and extreme value prediction,,,True,False,"Ribeiro, Rita P. and Moniz, Nuno",2020,,https://doi.org/10.1007/s10994-020-05900-9,10.1007/s10994-020-05900-9,Machine Learning
SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression,2511.04838v1,liu2023semi,\cite{liu2023semi},Semi-Supervised Graph Imbalanced Regression,https://arxiv.org/abs/2305.12087v1,"Data imbalance is easily found in annotated data when the observations of certain continuous label values are difficult to collect for regression tasks. When they come to molecule and polymer property predictions, the annotated graph datasets are often small because labeling them requires expensive equipment and effort. To address the lack of examples of rare label values in graph regression tasks, we propose a semi-supervised framework to progressively balance training data and reduce model bias via self-training. The training data balance is achieved by (1) pseudo-labeling more graphs for under-represented labels with a novel regression confidence measurement and (2) augmenting graph examples in latent space for remaining rare labels after data balancing with pseudo-labels. The former is to identify quality examples from unlabeled data whose labels are confidently predicted and sample a subset of them with a reverse distribution from the imbalanced annotated data. The latter collaborates with the former to target a perfect balance using a novel label-anchored mixup algorithm. We perform experiments in seven regression tasks on graph datasets. Results demonstrate that the proposed framework significantly reduces the error of predicted graph properties, especially in under-represented label areas.",True,True,"Liu, Gang and Zhao, Tong and Inae, Eric and Luo, Tengfei and Jiang, Meng",2023,,,,
SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression,2511.04838v1,zong2024boosting,\cite{zong2024boosting},Boosting semi-supervised learning under imbalanced regression via pseudo-labeling,,,True,False,"Zong, Nannan and Su, Songzhi and Zhou, Changle",2024,,,,Concurrency and Computation: Practice and Experience
SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression,2511.04838v1,bo2023specformer,\cite{bo2023specformer},Specformer: Spectral Graph Neural Networks Meet Transformers,https://arxiv.org/abs/2303.01028v1,"Spectral graph neural networks (GNNs) learn graph representations via spectral-domain graph convolutions. However, most existing spectral graph filters are scalar-to-scalar functions, i.e., mapping a single eigenvalue to a single filtered value, thus ignoring the global pattern of the spectrum. Furthermore, these filters are often constructed based on some fixed-order polynomials, which have limited expressiveness and flexibility. To tackle these issues, we introduce Specformer, which effectively encodes the set of all eigenvalues and performs self-attention in the spectral domain, leading to a learnable set-to-set spectral filter. We also design a decoder with learnable bases to enable non-local graph convolution. Importantly, Specformer is equivariant to permutation. By stacking multiple Specformer layers, one can build a powerful spectral GNN. On synthetic datasets, we show that our Specformer can better recover ground-truth spectral filters than other spectral GNNs. Extensive experiments of both node-level and graph-level tasks on real-world graph datasets show that our Specformer outperforms state-of-the-art GNNs and learns meaningful spectrum patterns. Code and data are available at https://github.com/bdy9527/Specformer.",True,True,"Bo, Deyu and Shi, Chuan and Wang, Lele and Liao, Renjie",2023,,,,arXiv preprint arXiv:2303.01028
SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression,2511.04838v1,li2025largescale,\cite{li2025largescale},Large-Scale Spectral Graph Neural Networks via Laplacian Sparsification: Technical Report,https://arxiv.org/abs/2501.04570v1,"Graph Neural Networks (GNNs) play a pivotal role in graph-based tasks for their proficiency in representation learning. Among the various GNN methods, spectral GNNs employing polynomial filters have shown promising performance on tasks involving both homophilous and heterophilous graph structures. However, The scalability of spectral GNNs on large graphs is limited because they learn the polynomial coefficients through multiple forward propagation executions during forward propagation. Existing works have attempted to scale up spectral GNNs by eliminating the linear layers on the input node features, a change that can disrupt end-to-end training, potentially impact performance, and become impractical with high-dimensional input features. To address the above challenges, we propose ""Spectral Graph Neural Networks with Laplacian Sparsification (SGNN-LS)"", a novel graph spectral sparsification method to approximate the propagation patterns of spectral GNNs. We prove that our proposed method generates Laplacian sparsifiers that can approximate both fixed and learnable polynomial filters with theoretical guarantees. Our method allows the application of linear layers on the input node features, enabling end-to-end training as well as the handling of raw text features. We conduct an extensive experimental analysis on datasets spanning various graph scales and properties to demonstrate the superior efficiency and effectiveness of our method. The results show that our method yields superior results in comparison with the corresponding approximated base models, especially on dataset Ogbn-papers100M(111M nodes, 1.6B edges) and MAG-scholar-C (2.8M features).",True,True,"Li, Tianyi and Yin, Hongxu and Shi, Chuan and Lin, Wei",2025,,,,arXiv preprint arXiv:2501.04570
SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression,2511.04838v1,yang2024spectral,\cite{yang2024spectral},Spectral-aware augmentation for enhanced graph representation learning,,,True,False,"Yang, Kaiqi and Han, Haoyu and Jin, Wei and Liu, Hui",2024,,,,
SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression,2511.04838v1,yao2024knowledge,\cite{yao2024knowledge},Knowledge mapping of graph neural networks for drug discovery: a bibliometric and visualized analysis,,,True,False,"Yao, Rufan and Shen, Zhenhua and Xu, Xinyi and Ling, Guixia and Xiang, Rongwu and Song, Tingyan and Zhai, Fei and Zhai, Yuxuan",2024,,,,Frontiers in Pharmacology
SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression,2511.04838v1,fan2024reducing,\cite{fan2024reducing},Reducing overconfident errors in molecular property classification using posterior network,,,True,False,"Fan, Zhe and Yu, Junda and Zhang, Xiangyu and Chen, Yuhan and Sun, Shuqian and Zhang, Yuyang and Chen, Ming and Xiao, Feng and Wu, Wei and Li, Xiang-Nan and others",2024,,,,Patterns
SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression,2511.04838v1,zhang2023review,\cite{zhang2023review},A review on graph neural networks for predicting synergistic drug combinations,,,True,False,"Zhang, Bin and Tu, Mengjun",2023,,,,Artificial Intelligence Review
SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression,2511.04838v1,prykhodko2019novo,\cite{prykhodko2019novo},A de novo molecular generation method using latent vector based generative adversarial network,,,True,False,"Prykhodko, Oleksii and Johansson, Simon Viet and Kotsias, Panagiotis-Christos and Ar{\'u}s-Pous, Josep and Bjerrum, Esben Jannik and Engkvist, Ola and Chen, Hongming",2019,,,,Journal of cheminformatics
SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression,2511.04838v1,khemchandani2020deepgraphmolgen,\cite{khemchandani2020deepgraphmolgen},"DeepGraphMolGen, a multi-objective, computational strategy for generating molecules with desirable properties: a graph convolution and reinforcement learning approach",,,True,False,"Khemchandani, Yash and O’Hagan, Stephen and Samanta, Soumitra and Swainston, Neil and Roberts, Timothy J and Bollegala, Danushka and Kell, Douglas B",2020,,,,Journal of cheminformatics
SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression,2511.04838v1,jeon2020autonomous,\cite{jeon2020autonomous},Autonomous molecule generation using reinforcement learning and docking to develop potential novel inhibitors,,,True,False,"Jeon, Woosung and Kim, Dongsup",2020,,,,Scientific reports
SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression,2511.04838v1,lee2022mgcvae,\cite{lee2022mgcvae},MGCVAE: Multi-objective Inverse Design via Molecular Graph Conditional Variational Autoencoder,https://arxiv.org/abs/2202.07476v1,"The ultimate goal of various fields is to directly generate molecules with desired properties, such as finding water-soluble molecules in drug development and finding molecules suitable for organic light-emitting diode (OLED) or photosensitizers in the field of development of new organic materials. In this respect, this study proposes a molecular graph generative model based on the autoencoder for de novo design. The performance of molecular graph conditional variational autoencoder (MGCVAE) for generating molecules having specific desired properties is investigated by comparing it to molecular graph variational autoencoder (MGVAE). Furthermore, multi-objective optimization for MGCVAE was applied to satisfy two selected properties simultaneously. In this study, two physical properties -- logP and molar refractivity -- were used as optimization targets for the purpose of designing de novo molecules, especially in drug discovery. As a result, it was confirmed that among generated molecules, 25.89% optimized molecules were generated in MGCVAE compared to 0.66% in MGVAE. Hence, it demonstrates that MGCVAE effectively produced drug-like molecules with two target properties. The results of this study suggest that these graph-based data-driven models are one of the effective methods of designing new molecules that fulfill various physical properties, such as drug discovery.",True,True,"Lee, Myeonghun and Min, Kyoungmin",2022,,,,Journal of chemical information and modeling
SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression,2511.04838v1,chen2023deep,\cite{chen2023deep},Deep generative model for drug design from protein target sequence,,,True,False,"Chen, Yangyang and Wang, Zixu and Wang, Lei and Wang, Jianmin and Li, Pengyong and Cao, Dongsheng and Zeng, Xiangxiang and Ye, Xiucai and Sakurai, Tetsuya",2023,,,,Journal of Cheminformatics
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,jumper2021highly,\cite{jumper2021highly},Highly accurate protein structure prediction with AlphaFold,,,True,False,"Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others",2021,,,,nature
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,baek2021accurate,\cite{baek2021accurate},Accurate prediction of protein structures and interactions using a three-track neural network,,,True,False,"Baek, Minkyung and DiMaio, Frank and Anishchenko, Ivan and Dauparas, Justas and Ovchinnikov, Sergey and Lee, Gyu Rie and Wang, Jue and Cong, Qian and Kinch, Lisa N and Schaeffer, R Dustin and others",2021,,,,Science
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,af3,\cite{af3},Accurate structure prediction of biomolecular interactions with AlphaFold 3,,,True,False,"Abramson, Josh and Adler, Jonas and Dunger, Jack and Evans, Richard and Green, Tim and Pritzel, Alexander and Ronneberger, Olaf and Willmore, Lindsay and Ballard, Andrew J and Bambrick, Joshua and others",2024,,,,Nature
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,schafer2025sequence,\cite{schafer2025sequence},Sequence clustering confounds AlphaFold2,,,True,False,"Schafer, Joseph W and Lee, Myeongsang and Chakravarty, Devlina and Thole, Joseph F and Chen, Ethan A and Porter, Lauren L",2025,,,,Nature
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,afcluster,\cite{afcluster},Predicting multiple conformations via sequence clustering and AlphaFold2,,,True,False,"Wayment-Steele, Hannah K and Ojoawo, Adedolapo and Otten, Renee and Apitz, Julia M and Pitsawong, Warintra and H{\""o}mberger, Marc and Ovchinnikov, Sergey and Colwell, Lucy and Kern, Dorothee",2024,,,,Nature
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,kalakoti2025afsample2,\cite{kalakoti2025afsample2},AFsample2 predicts multiple conformations and ensembles with AlphaFold2,,,True,False,"Kalakoti, Yogesh and Wallner, Bj{\""o}rn",2025,,,,Communications Biology
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,karplus2005molecular,\cite{karplus2005molecular},Molecular dynamics and protein function,,,True,False,"Karplus, Martin and Kuriyan, John",2005,,,,Proceedings of the National Academy of Sciences
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,henzler2007dynamic,\cite{henzler2007dynamic},Dynamic personalities of proteins,,,True,False,"Henzler-Wildman, Katherine and Kern, Dorothee",2007,,,,Nature
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,mdgen,\cite{mdgen},Generative Modeling of Molecular Dynamics Trajectories,https://arxiv.org/abs/2409.17808v1,"Molecular dynamics (MD) is a powerful technique for studying microscopic phenomena, but its computational cost has driven significant interest in the development of deep learning-based surrogate models. We introduce generative modeling of molecular trajectories as a paradigm for learning flexible multi-task surrogate models of MD from data. By conditioning on appropriately chosen frames of the trajectory, we show such generative models can be adapted to diverse tasks such as forward simulation, transition path sampling, and trajectory upsampling. By alternatively conditioning on part of the molecular system and inpainting the rest, we also demonstrate the first steps towards dynamics-conditioned molecular design. We validate the full set of these capabilities on tetrapeptide simulations and show that our model can produce reasonable ensembles of protein monomers. Altogether, our work illustrates how generative modeling can unlock value from MD data towards diverse downstream tasks that are not straightforward to address with existing methods or even MD itself. Code is available at https://github.com/bjing2016/mdgen.",True,True,"Jing, Bowen and Stark, Hannes and Jaakkola, Tommi and Berger, Bonnie",2024,,,,
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,alphaflow,\cite{alphaflow},AlphaFold meets flow matching for generating protein ensembles,,,True,False,"Jing, Bowen and Berger, Bonnie and Jaakkola, Tommi",2024,,,,
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,confdiff,\cite{confdiff},Protein Conformation Generation via Force-Guided SE(3) Diffusion Models,https://arxiv.org/abs/2403.14088v2,"The conformational landscape of proteins is crucial to understanding their functionality in complex biological processes. Traditional physics-based computational methods, such as molecular dynamics (MD) simulations, suffer from rare event sampling and long equilibration time problems, hindering their applications in general protein systems. Recently, deep generative modeling techniques, especially diffusion models, have been employed to generate novel protein conformations. However, existing score-based diffusion methods cannot properly incorporate important physical prior knowledge to guide the generation process, causing large deviations in the sampled protein conformations from the equilibrium distribution. In this paper, to overcome these limitations, we propose a force-guided SE(3) diffusion model, ConfDiff, for protein conformation generation. By incorporating a force-guided network with a mixture of data-based score models, ConfDiff can generate protein conformations with rich diversity while preserving high fidelity. Experiments on a variety of protein conformation prediction tasks, including 12 fast-folding proteins and the Bovine Pancreatic Trypsin Inhibitor (BPTI), demonstrate that our method surpasses the state-of-the-art method.",True,True,"Wang, Yan and Wang, Lihao and Shen, Yuning and Wang, Yiqun and Yuan, Huizhuo and Wu, Yue and Gu, Quanquan",2024,,,,
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,bioemu,\cite{bioemu},Scalable emulation of protein equilibrium ensembles with generative deep learning,,,True,False,"Lewis, Sarah and Hempel, Tim and Jim{\'e}nez-Luna, Jos{\'e} and Gastegger, Michael and Xie, Yu and Foong, Andrew YK and Satorras, Victor Garc{\'\i}a and Abdin, Osama and Veeling, Bastiaan S and Zaporozhets, Iryna and others",2025,,,,Science
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,klein2023timewarp,\cite{klein2023timewarp},Timewarp: Transferable Acceleration of Molecular Dynamics by Learning Time-Coarsened Dynamics,https://arxiv.org/abs/2302.01170v2,"Molecular dynamics (MD) simulation is a widely used technique to simulate molecular systems, most commonly at the all-atom resolution where equations of motion are integrated with timesteps on the order of femtoseconds ($1\textrm{fs}=10^{-15}\textrm{s}$). MD is often used to compute equilibrium properties, which requires sampling from an equilibrium distribution such as the Boltzmann distribution. However, many important processes, such as binding and folding, occur over timescales of milliseconds or beyond, and cannot be efficiently sampled with conventional MD. Furthermore, new MD simulations need to be performed for each molecular system studied. We present Timewarp, an enhanced sampling method which uses a normalising flow as a proposal distribution in a Markov chain Monte Carlo method targeting the Boltzmann distribution. The flow is trained offline on MD trajectories and learns to make large steps in time, simulating the molecular dynamics of $10^{5} - 10^{6}\:\textrm{fs}$. Crucially, Timewarp is transferable between molecular systems: once trained, we show that it generalises to unseen small peptides (2-4 amino acids) at all-atom resolution, exploring their metastable states and providing wall-clock acceleration of sampling compared to standard MD. Our method constitutes an important step towards general, transferable algorithms for accelerating MD.",True,True,"Klein, Leon and Foong, Andrew and Fjelde, Tor and Mlodozeniec, Bruno and Brockschmidt, Marc and Nowozin, Sebastian and No{\'e}, Frank and Tomioka, Ryota",2023,,,,Advances in Neural Information Processing Systems
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,schreiner2023implicit,\cite{schreiner2023implicit},Implicit Transfer Operator Learning: Multiple Time-Resolution Surrogates for Molecular Dynamics,https://arxiv.org/abs/2305.18046v2,"Computing properties of molecular systems rely on estimating expectations of the (unnormalized) Boltzmann distribution. Molecular dynamics (MD) is a broadly adopted technique to approximate such quantities. However, stable simulations rely on very small integration time-steps ($10^{-15}\,\mathrm{s}$), whereas convergence of some moments, e.g. binding free energy or rates, might rely on sampling processes on time-scales as long as $10^{-1}\, \mathrm{s}$, and these simulations must be repeated for every molecular system independently. Here, we present Implict Transfer Operator (ITO) Learning, a framework to learn surrogates of the simulation process with multiple time-resolutions. We implement ITO with denoising diffusion probabilistic models with a new SE(3) equivariant architecture and show the resulting models can generate self-consistent stochastic dynamics across multiple time-scales, even when the system is only partially observed. Finally, we present a coarse-grained CG-SE3-ITO model which can quantitatively model all-atom molecular dynamics using only coarse molecular representations. As such, ITO provides an important step towards multiple time- and space-resolution acceleration of MD. Code is available at \href{https://github.com/olsson-group/ito}{https://github.com/olsson-group/ito}.",True,True,"Schreiner, Mathias and Winther, Ole and Olsson, Simon",2023,,,,Advances in Neural Information Processing Systems
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,lu2025eba,\cite{lu2025eba},Aligning Protein Conformation Ensemble Generation with Physical Feedback,https://arxiv.org/abs/2505.24203v1,"Protein dynamics play a crucial role in protein biological functions and properties, and their traditional study typically relies on time-consuming molecular dynamics (MD) simulations conducted in silico. Recent advances in generative modeling, particularly denoising diffusion models, have enabled efficient accurate protein structure prediction and conformation sampling by learning distributions over crystallographic structures. However, effectively integrating physical supervision into these data-driven approaches remains challenging, as standard energy-based objectives often lead to intractable optimization. In this paper, we introduce Energy-based Alignment (EBA), a method that aligns generative models with feedback from physical models, efficiently calibrating them to appropriately balance conformational states based on their energy differences. Experimental results on the MD ensemble benchmark demonstrate that EBA achieves state-of-the-art performance in generating high-quality protein ensembles. By improving the physical plausibility of generated structures, our approach enhances model predictions and holds promise for applications in structural biology and drug discovery.",True,True,Jiarui Lu and Xiaoyin Chen and Stephen Zhewen Lu and Aurélie Lozano and Vijil Chenthamarakshan and Payel Das and Jian Tang,2025,,https://arxiv.org/abs/2505.24203,,
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,jin2025p2dflow,\cite{jin2025p2dflow},P2DFlow: A Protein Ensemble Generative Model with SE(3) Flow Matching,https://arxiv.org/abs/2411.17196v2,"Biological processes, functions, and properties are intricately linked to the ensemble of protein conformations, rather than being solely determined by a single stable conformation. In this study, we have developed P2DFlow, a generative model based on SE(3) flow matching, to predict the structural ensembles of proteins. We specifically designed a valuable prior for the flow process and enhanced the model's ability to distinguish each intermediate state by incorporating an additional dimension to describe the ensemble data, which can reflect the physical laws governing the distribution of ensembles, so that the prior knowledge can effectively guide the generation process. When trained and evaluated on the MD datasets of ATLAS, P2DFlow outperforms other baseline models on extensive experiments, successfully capturing the observable dynamic fluctuations as evidenced in crystal structure and MD simulations. As a potential proxy agent for protein molecular simulation, the high-quality ensembles generated by P2DFlow could significantly aid in understanding protein functions across various scenarios. Code is available at https://github.com/BLEACH366/P2DFlow",True,True,Yaowei Jin and Qi Huang and Ziyang Song and Mingyue Zheng and Dan Teng and Qian Shi,2025,,https://arxiv.org/abs/2411.17196,,
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,laurents2022alphafold,\cite{laurents2022alphafold},"AlphaFold 2 and NMR spectroscopy: partners to understand protein structure, dynamics and function",,,True,False,"Laurents, Douglas V",2022,,,,Frontiers in molecular biosciences
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,maddipatla2025inverseproblemsexperimentguidedalphafold,\cite{maddipatla2025inverseproblemsexperimentguidedalphafold},Inverse problems with experiment-guided AlphaFold,,,True,False,Advaith Maddipatla and Nadav Bojan Sellam and Meital Bojan and Sanketh Vedula and Paul Schanda and Ailie Marx aand Alex M. Bronstein,2025,,https://arxiv.org/abs/2502.09372,,
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,tengcryogen,\cite{tengcryogen},CryoGEN: Generative Energy-based Models for Cryogenic Electron Tomography Reconstruction,,,True,False,"Teng, Yunfei and Ren, Yuxuan and Chen, Kai and Chen, Xi and Chen, Zhaoming and Ye, Qiwei",,,,,
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,renphysical,\cite{renphysical},Physical Consistency Bridges Heterogeneous Data in Molecular Multi-Task Learning,https://arxiv.org/abs/2410.10118v1,"In recent years, machine learning has demonstrated impressive capability in handling molecular science tasks. To support various molecular properties at scale, machine learning models are trained in the multi-task learning paradigm. Nevertheless, data of different molecular properties are often not aligned: some quantities, e.g. equilibrium structure, demand more cost to compute than others, e.g. energy, so their data are often generated by cheaper computational methods at the cost of lower accuracy, which cannot be directly overcome through multi-task learning. Moreover, it is not straightforward to leverage abundant data of other tasks to benefit a particular task. To handle such data heterogeneity challenges, we exploit the specialty of molecular tasks that there are physical laws connecting them, and design consistency training approaches that allow different tasks to exchange information directly so as to improve one another. Particularly, we demonstrate that the more accurate energy data can improve the accuracy of structure prediction. We also find that consistency training can directly leverage force and off-equilibrium structure data to improve structure prediction, demonstrating a broad capability for integrating heterogeneous data.",True,True,"Ren, Yuxuan and Zheng, Dihan and Liu, Chang and Jin, Peiran and Shi, Yu and Huang, Lin and He, Jiyan and Luo, Shengjie and Qin, Tao and Liu, Tie-Yan",,,,,
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,gyawali2025multimodal,\cite{gyawali2025multimodal},Multimodal deep learning integration of cryo-EM and AlphaFold3 for high-accuracy protein structure determination,,,True,False,"Gyawali, Rajan and Dhakal, Ashwin and Cheng, Jianlin",2025,,,,bioRxiv
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,christiano2017deep,\cite{christiano2017deep},Deep reinforcement learning from human preferences,https://arxiv.org/abs/1706.03741v4,"For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.",True,True,"Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario",2017,,,,Advances in neural information processing systems
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,stiennon2020learning,\cite{stiennon2020learning},Learning to summarize with human feedback,,,True,False,"Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F",2020,,,,Advances in neural information processing systems
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,ppo,\cite{ppo},Proximal Policy Optimization Algorithms,https://arxiv.org/abs/1707.06347v2,"We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a ""surrogate"" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.",True,True,John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov,2017,,https://arxiv.org/abs/1707.06347,,
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,dpo,\cite{dpo},Direct Preference Optimization: Your Language Model is Secretly a Reward Model,https://arxiv.org/abs/2305.18290v3,"While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.",True,True,"Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",2023,,,,Advances in Neural Information Processing Systems
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,ethayarajh2024kto,\cite{ethayarajh2024kto},KTO: Model Alignment as Prospect Theoretic Optimization,https://arxiv.org/abs/2402.01306v4,"Kahneman & Tversky's $\textit{prospect theory}$ tells us that humans perceive random variables in a biased but well-defined manner (1992); for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases -- the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them belonging to a family of loss functions that we call $\textit{human-aware losses}$ (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach KTO, and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B, despite only learning from a binary signal of whether an output is desirable. More broadly, our work suggests that there is no one HALO that is universally superior; the best loss depends on the inductive biases most appropriate for a given setting, an oft-overlooked consideration.",True,True,Kawin Ethayarajh and Winnie Xu and Niklas Muennighoff and Dan Jurafsky and Douwe Kiela,2024,,https://arxiv.org/abs/2402.01306,,
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,wang2023beyond,\cite{wang2023beyond},Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints,https://arxiv.org/abs/2309.16240v1,"The increasing capabilities of large language models (LLMs) raise opportunities for artificial general intelligence but concurrently amplify safety concerns, such as potential misuse of AI systems, necessitating effective AI alignment. Reinforcement Learning from Human Feedback (RLHF) has emerged as a promising pathway towards AI alignment but brings forth challenges due to its complexity and dependence on a separate reward model. Direct Preference Optimization (DPO) has been proposed as an alternative, and it remains equivalent to RLHF under the reverse KL regularization constraint. This paper presents $f$-DPO, a generalized approach to DPO by incorporating diverse divergence constraints. We show that under certain $f$-divergences, including Jensen-Shannon divergence, forward KL divergences and $α$-divergences, the complex relationship between the reward and optimal policy can also be simplified by addressing the Karush-Kuhn-Tucker conditions. This eliminates the need for estimating the normalizing constant in the Bradley-Terry model and enables a tractable mapping between the reward function and the optimal policy. Our approach optimizes LLMs to align with human preferences in a more efficient and supervised manner under a broad set of divergence constraints. Empirically, adopting these divergences ensures a balance between alignment performance and generation diversity. Importantly, $f$-DPO outperforms PPO-based methods in divergence efficiency, and divergence constraints directly influence expected calibration error (ECE).",True,True,"Wang, Chaoqi and Jiang, Yibo and Yang, Chenghao and Liu, Han and Chen, Yuxin",,,,,
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,wallace2024diffusion,\cite{wallace2024diffusion},Diffusion Model Alignment Using Direct Preference Optimization,https://arxiv.org/abs/2311.12908v1,"Large language models (LLMs) are fine-tuned using human comparison data with Reinforcement Learning from Human Feedback (RLHF) methods to make them better aligned with users' preferences. In contrast to LLMs, human preference learning has not been widely explored in text-to-image diffusion models; the best existing approach is to fine-tune a pretrained model using carefully curated high quality images and captions to improve visual appeal and text alignment. We propose Diffusion-DPO, a method to align diffusion models to human preferences by directly optimizing on human comparison data. Diffusion-DPO is adapted from the recently developed Direct Preference Optimization (DPO), a simpler alternative to RLHF which directly optimizes a policy that best satisfies human preferences under a classification objective. We re-formulate DPO to account for a diffusion model notion of likelihood, utilizing the evidence lower bound to derive a differentiable objective. Using the Pick-a-Pic dataset of 851K crowdsourced pairwise preferences, we fine-tune the base model of the state-of-the-art Stable Diffusion XL (SDXL)-1.0 model with Diffusion-DPO. Our fine-tuned base model significantly outperforms both base SDXL-1.0 and the larger SDXL-1.0 model consisting of an additional refinement model in human evaluation, improving visual appeal and prompt alignment. We also develop a variant that uses AI feedback and has comparable performance to training on human preferences, opening the door for scaling of diffusion model alignment methods.",True,True,"Wallace, Bram and Dang, Meihua and Rafailov, Rafael and Zhou, Linqi and Lou, Aaron and Purushwalkam, Senthil and Ermon, Stefano and Xiong, Caiming and Joty, Shafiq and Naik, Nikhil",2024,,,,
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,yang2024using,\cite{yang2024using},Using human feedback to fine-tune diffusion models without any reward model,,,True,False,"Yang, Kai and Tao, Jian and Lyu, Jiafei and Ge, Chunjiang and Chen, Jiaxin and Shen, Weihan and Zhu, Xiaolong and Li, Xiu",2024,,,,
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,lipo,\cite{lipo},LiPO: Listwise Preference Optimization through Learning-to-Rank,https://arxiv.org/abs/2402.01878v3,"Aligning language models (LMs) with curated human feedback is critical to control their behaviors in real-world applications. Several recent policy optimization methods, such as DPO and SLiC, serve as promising alternatives to the traditional Reinforcement Learning from Human Feedback (RLHF) approach. In practice, human feedback often comes in a format of a ranked list over multiple responses to amortize the cost of reading prompt. Multiple responses can also be ranked by reward models or AI feedback. There lacks such a thorough study on directly fitting upon a list of responses. In this work, we formulate the LM alignment as a \textit{listwise} ranking problem and describe the LiPO framework, where the policy can potentially learn more effectively from a ranked list of plausible responses given the prompt. This view draws an explicit connection to Learning-to-Rank (LTR), where most existing preference optimization work can be mapped to existing ranking objectives. Following this connection, we provide an examination of ranking objectives that are not well studied for LM alignment with DPO and SLiC as special cases when list size is two. In particular, we highlight a specific method, LiPO-$λ$, which leverages a state-of-the-art \textit{listwise} ranking objective and weights each preference pair in a more advanced manner. We show that LiPO-$λ$ can outperform DPO variants and SLiC by a clear margin on several preference alignment tasks with both curated and real rankwise preference data.",True,True,"Liu, Tianqi and Qin, Zhen and Wu, Junru and Shen, Jiaming and Khalman, Misha and Joshi, Rishabh and Zhao, Yao and Saleh, Mohammad and Baumgartner, Simon and Liu, Jialu and others",2024,,,,arXiv preprint arXiv:2402.01878
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,gu2024aligning,\cite{gu2024aligning},Aligning Target-Aware Molecule Diffusion Models with Exact Energy Optimization,https://arxiv.org/abs/2407.01648v2,"Generating ligand molecules for specific protein targets, known as structure-based drug design, is a fundamental problem in therapeutics development and biological discovery. Recently, target-aware generative models, especially diffusion models, have shown great promise in modeling protein-ligand interactions and generating candidate drugs. However, existing models primarily focus on learning the chemical distribution of all drug candidates, which lacks effective steerability on the chemical quality of model generations. In this paper, we propose a novel and general alignment framework to align pretrained target diffusion models with preferred functional properties, named AliDiff. AliDiff shifts the target-conditioned chemical distribution towards regions with higher binding affinity and structural rationality, specified by user-defined reward functions, via the preference optimization approach. To avoid the overfitting problem in common preference optimization objectives, we further develop an improved Exact Energy Preference Optimization method to yield an exact and efficient alignment of the diffusion models, and provide the closed-form expression for the converged distribution. Empirical studies on the CrossDocked2020 benchmark show that AliDiff can generate molecules with state-of-the-art binding energies with up to -7.07 Avg. Vina Score, while maintaining strong molecular properties. Code is available at https://github.com/MinkaiXu/AliDiff.",True,True,"Gu, Siyi and Xu, Minkai and Powers, Alexander and Nie, Weili and Geffner, Tomas and Kreis, Karsten and Leskovec, Jure and Vahdat, Arash and Ermon, Stefano",2024,,,,Advances in Neural Information Processing Systems
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,wang2024training,\cite{wang2024training},Training Free Guided Flow Matching with Optimal Control,https://arxiv.org/abs/2410.18070v3,"Controlled generation with pre-trained Diffusion and Flow Matching models has vast applications. One strategy for guiding ODE-based generative models is through optimizing a target loss $R(x_1)$ while staying close to the prior distribution. Along this line, some recent work showed the effectiveness of guiding flow model by differentiating through its ODE sampling process. Despite the superior performance, the theoretical understanding of this line of methods is still preliminary, leaving space for algorithm improvement. Moreover, existing methods predominately focus on Euclidean data manifold, and there is a compelling need for guided flow methods on complex geometries such as SO(3), which prevails in high-stake scientific applications like protein design. We present OC-Flow, a general and theoretically grounded training-free framework for guided flow matching using optimal control. Building upon advances in optimal control theory, we develop effective and practical algorithms for solving optimal control in guided ODE-based generation and provide a systematic theoretical analysis of the convergence guarantee in both Euclidean and SO(3). We show that existing backprop-through-ODE methods can be interpreted as special cases of Euclidean OC-Flow. OC-Flow achieved superior performance in extensive experiments on text-guided image manipulation, conditional molecule generation, and all-atom peptide design.",True,True,"Wang, Luran and Cheng, Chaoran and Liao, Yizhen and Qu, Yanru and Liu, Ge",,,,,
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,black2023training,\cite{black2023training},Training diffusion models with reinforcement learning,,,True,False,"Black, Kevin and Janner, Michael and Du, Yilun and Kostrikov, Ilya and Levine, Sergey",2023,,,,arXiv preprint arXiv:2305.13301
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,fan2023dpok,\cite{fan2023dpok},Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models,,,True,False,"Fan, Ying and Watkins, Olivia and Du, Yuqing and Liu, Hao and Ryu, Moonkyung and Boutilier, Craig and Abbeel, Pieter and Ghavamzadeh, Mohammad and Lee, Kangwook and Lee, Kimin",2023,,,,Advances in Neural Information Processing Systems
EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization,2511.10165v1,clark2023directly,\cite{clark2023directly},Directly Fine-Tuning Diffusion Models on Differentiable Rewards,https://arxiv.org/abs/2309.17400v2,"We present Direct Reward Fine-Tuning (DRaFT), a simple and effective method for fine-tuning diffusion models to maximize differentiable reward functions, such as scores from human preference models. We first show that it is possible to backpropagate the reward function gradient through the full sampling procedure, and that doing so achieves strong performance on a variety of rewards, outperforming reinforcement learning-based approaches. We then propose more efficient variants of DRaFT: DRaFT-K, which truncates backpropagation to only the last K steps of sampling, and DRaFT-LV, which obtains lower-variance gradient estimates for the case when K=1. We show that our methods work well for a variety of reward functions and can be used to substantially improve the aesthetic quality of images generated by Stable Diffusion 1.4. Finally, we draw connections between our approach and prior work, providing a unifying perspective on the design space of gradient-based fine-tuning algorithms.",True,True,"Clark, Kevin and Vicol, Paul and Swersky, Kevin and Fleet, David J",2023,,,,arXiv preprint arXiv:2309.17400
This EEG Looks Like These EEGs: Interpretable Interictal Epileptiform Discharge Detection With ProtoEEG-kNN,2510.20846v1,Diniz2024AdvancingED,\cite{Diniz2024AdvancingED},{Advancing Epilepsy Diagnosis: A Meta-Analysis of Artificial Intelligence Approaches for Interictal Epileptiform Discharge Detection},,,True,False,Jordana Borges Camargo Diniz and La{\'i}s Silva Santana and Marianna Leite and Jo{\~a}o Lucas Silva Santana and Sarah Isabela Magalh{\~a}es Costa and Luiz H Castro and Jo{\~a}o Paulo Mota Telles,2024,,,,{Seizure: European Journal of Epilepsy}
This EEG Looks Like These EEGs: Interpretable Interictal Epileptiform Discharge Detection With ProtoEEG-kNN,2510.20846v1,Geng2021DeepLF,\cite{Geng2021DeepLF},{Deep Learning for Robust Detection of Interictal Epileptiform Discharges},,,True,False,David Geng and Ayham Alkhachroum and Manuel Melo Bicchi and Jonathan R. Jagid and Iahn Cajigas and Zhe Sage Chen,2021,,,,{Journal of Neural Engineering}
This EEG Looks Like These EEGs: Interpretable Interictal Epileptiform Discharge Detection With ProtoEEG-kNN,2510.20846v1,TjepkemaCloostermans2018DeepLF,\cite{TjepkemaCloostermans2018DeepLF},{Deep Learning for Detection of Focal Epileptiform Discharges From Scalp EEG Recordings},,,True,False,Marleen C. Tjepkema‐Cloostermans and Rafael de Carvalho and Michel J. A. M. van Putten,2018,,,,{Clinical Neurophysiology}
This EEG Looks Like These EEGs: Interpretable Interictal Epileptiform Discharge Detection With ProtoEEG-kNN,2510.20846v1,Frbass2020AnAI,\cite{Frbass2020AnAI},{An Artificial Intelligence-Based EEG Algorithm for Detection of Epileptiform EEG Discharges: Validation Against the Diagnostic Gold Standard},,,True,False,"Franz F{\""u}rbass and Mustafa Aykut Kural and Gerhard Gritsch and Manfred Martin Hartmann and Tilmann Kluge and S{\'a}ndor Beniczky",2020,,,,{Clinical Neurophysiology}
This EEG Looks Like These EEGs: Interpretable Interictal Epileptiform Discharge Detection With ProtoEEG-kNN,2510.20846v1,Kural2022AccurateIO,\cite{Kural2022AccurateIO},{Accurate Identification of EEG Recordings With Interictal Epileptiform Discharges Using a Hybrid Approach: Artificial Intelligence Supervised by Human Experts},,,True,False,"Mustafa Aykut Kural and Jin Jing and Franz F{\""u}rbass and Hannes Perko and Erisela Qerama and Birger Johnsen and Steffen Fuchs and M. Brandon Westover and S{\'a}ndor Beniczky",2022,,,,Epilepsia
This EEG Looks Like These EEGs: Interpretable Interictal Epileptiform Discharge Detection With ProtoEEG-kNN,2510.20846v1,Jing2020DevelopmentOE,\cite{Jing2020DevelopmentOE},Development of Expert-Level Automated Detection of Epileptiform Discharges During Electroencephalogram Interpretation.,,,True,False,Jin Jing and Jin Jing and Haoqi Sun and Jennifer A. Kim and Aline Herlopian and Ioannis Karakis and Marcus C. Ng and Jonathan J. Halford and Douglas Maus and Fonda Chan and Marjan Dolatshahi and Carlos F. Muniz and Catherine J. Chu and Valeria Sacca and Jay S. Pathmanathan and Wendong Ge and Justin Dauwels and Alice D. Lam and Andrew J. Cole and Sydney S. Cash and M. Brandon Westover,2020,,,,JAMA Neurology
This EEG Looks Like These EEGs: Interpretable Interictal Epileptiform Discharge Detection With ProtoEEG-kNN,2510.20846v1,Tveit2023AutomatedIO,\cite{Tveit2023AutomatedIO},{Automated Interpretation of Clinical Electroencephalograms Using Artificial Intelligence},,,True,False,Jesper Tveit and Harald Aurlien and S. Plis and Vince D. Calhoun and William O. Tatum and Donald L. Schomer and Vibeke Arntsen and Fieke M.E. Cox and Firas Fahoum and William B. Gallentine and Elena Gardella and Cecil D. Hahn and Aatif M. Husain and Sudha Kilaru Kessler and Mustafa Aykut Kural and F{\'a}bio A. Nascimento and Hatice Tankisi and Line B Ulvin and Richard A. Wennberg and S{\'a}ndor Beniczky,2023,,,,{JAMA Neurology}
This EEG Looks Like These EEGs: Interpretable Interictal Epileptiform Discharge Detection With ProtoEEG-kNN,2510.20846v1,chen2019this,\cite{chen2019this},{This Looks Like That: Deep Learning for Interpretable Image Recognition},,,True,False,"Chen, Chaofan and Li, Oscar and Tao, Daniel and Barnett, Alina and Su, Jonathan K and Rudin, Cynthia",2019,,,,{Advances in Neural Information Processing Systems}
This EEG Looks Like These EEGs: Interpretable Interictal Epileptiform Discharge Detection With ProtoEEG-kNN,2510.20846v1,ukai2022looks,\cite{ukai2022looks},{This Looks Like it Rather Than That: ProtoKNN for Similarity-Based Classifiers},,,True,False,"Ukai, Yuki and Hirakawa, Tsubasa and Yamashita, Takayoshi and Fujiyoshi, Hironobu",2022,,,,
This EEG Looks Like These EEGs: Interpretable Interictal Epileptiform Discharge Detection With ProtoEEG-kNN,2510.20846v1,Tang2023ProtoEEGNetAI,\cite{Tang2023ProtoEEGNetAI},{ProtoEEGNet: An Interpretable Approach for Detecting Interictal Epileptiform Discharges},,,True,False,Dennis Tang and Frank Willard and Ronan Tegerdine and Luke Triplett and Jon Donnelly and Luke Moffett and Lesia Semenova and Alina Jade Barnett and Jin Jing and Cynthia Rudin and Brandon Westover,2023,,,,{Medical Imaging Meets NeurIPS Workshop}
This EEG Looks Like These EEGs: Interpretable Interictal Epileptiform Discharge Detection With ProtoEEG-kNN,2510.20846v1,Gao2023ASD,\cite{Gao2023ASD},{A Self-Interpretable Deep Learning Model for Seizure Prediction Using a Multi-Scale Prototypical Part Network},,,True,False,"Gao, Yikai and Liu, Aiping and Wang, Lanlan and Qian, Ruobing and Chen, Xun",2023,,,,{IEEE Transactions on Neural Systems and Rehabilitation Engineering}
This EEG Looks Like These EEGs: Interpretable Interictal Epileptiform Discharge Detection With ProtoEEG-kNN,2510.20846v1,Gao2024AnIA,\cite{Gao2024AnIA},{An Interpretable and Generalizable Deep Learning Model for iEEG-based Seizure Prediction Using Prototype Learning and Contrastive Learning},,,True,False,Yikai Gao and Aiping Liu and Heng Cui and Ruobing Qian and Xun Chen,2024,,,,{Computers in Biology and Medicine}
This EEG Looks Like These EEGs: Interpretable Interictal Epileptiform Discharge Detection With ProtoEEG-kNN,2510.20846v1,Nascimento2022AQA,\cite{Nascimento2022AQA},{A Quantitative Approach to Evaluating Interictal Epileptiform Discharges Based on Interpretable Quantitative Criteria},,,True,False,F{\'a}bio Augusto Nascimento and Jaden D. Barfuss and Alex Jaffe and M. Brandon Westover and Jin Jing,2022,,,,{Clinical Neurophysiology}
This EEG Looks Like These EEGs: Interpretable Interictal Epileptiform Discharge Detection With ProtoEEG-kNN,2510.20846v1,Kural2020CriteriaFD,\cite{Kural2020CriteriaFD},{Criteria for Defining Interictal Epileptiform Discharges in EEG},,,True,False,Mustafa Aykut Kural and Lene Duez and Vibeke Sejer Hansen and P{\aa}l Gunnar Larsson and Stefan Rampp and Reinhard Schulz and Hatice Tankisi and Richard A. Wennberg and Bo Martin Bibby and Michael Scherg and S{\'a}ndor Beniczky,2020,,,,Neurology
This EEG Looks Like These EEGs: Interpretable Interictal Epileptiform Discharge Detection With ProtoEEG-kNN,2510.20846v1,Lopes2023UsingCS,\cite{Lopes2023UsingCS},{Using CNN Saliency Maps and EEG Modulation Spectra for Improved and More Interpretable Machine Learning-Based Alzheimer's Disease Diagnosis},,,True,False,Marilia Karla Soares Lopes and Raymundo Cassani and Tiago H. Falk,2023,,,,{Computational Intelligence and Neuroscience}
This EEG Looks Like These EEGs: Interpretable Interictal Epileptiform Discharge Detection With ProtoEEG-kNN,2510.20846v1,Ozcan2019SeizurePI,\cite{Ozcan2019SeizurePI},{Seizure Prediction in Scalp EEG Using 3D Convolutional Neural Networks With an Image-Based Approach},,,True,False,Ahmet Remzi Ozcan and S. Erturk,2019,,,,{IEEE Transactions on Neural Systems and Rehabilitation Engineering}
This EEG Looks Like These EEGs: Interpretable Interictal Epileptiform Discharge Detection With ProtoEEG-kNN,2510.20846v1,rudin2019stop,\cite{rudin2019stop},Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead,https://arxiv.org/abs/1811.10154v3,"Black box machine learning models are currently being used for high stakes decision-making throughout society, causing problems throughout healthcare, criminal justice, and in other domains. People have hoped that creating methods for explaining these black box models will alleviate some of these problems, but trying to \textit{explain} black box models, rather than creating models that are \textit{interpretable} in the first place, is likely to perpetuate bad practices and can potentially cause catastrophic harm to society. There is a way forward -- it is to design models that are inherently interpretable. This manuscript clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare, and computer vision.",True,True,"Rudin, Cynthia",2019,,,,Nature Machine Intelligence
This EEG Looks Like These EEGs: Interpretable Interictal Epileptiform Discharge Detection With ProtoEEG-kNN,2510.20846v1,Adebayo2018SanityCF,\cite{Adebayo2018SanityCF},{Sanity Checks for Saliency Maps},,,True,False,Julius Adebayo and Justin Gilmer and Michael Muelly and Ian J. Goodfellow and Moritz Hardt and Been Kim,2018,,,,{Advances in Neural Information Processing Systems}
Coder as Editor: Code-driven Interpretable Molecular Optimization,2510.14455v1,weininger1988smiles,\cite{weininger1988smiles},"SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules",,,True,False,"Weininger, David",1988,,,,Journal of chemical information and computer sciences
Coder as Editor: Code-driven Interpretable Molecular Optimization,2510.14455v1,gomez2018automatic,\cite{gomez2018automatic},Automatic chemical design using a data-driven continuous representation of molecules,https://arxiv.org/abs/1610.02415v3,"We report a method to convert discrete representations of molecules to and from a multidimensional continuous representation. This model allows us to generate new molecules for efficient exploration and optimization through open-ended spaces of chemical compounds. A deep neural network was trained on hundreds of thousands of existing chemical structures to construct three coupled functions: an encoder, a decoder and a predictor. The encoder converts the discrete representation of a molecule into a real-valued continuous vector, and the decoder converts these continuous vectors back to discrete molecular representations. The predictor estimates chemical properties from the latent continuous vector representation of the molecule. Continuous representations allow us to automatically generate novel chemical structures by performing simple operations in the latent space, such as decoding random vectors, perturbing known chemical structures, or interpolating between molecules. Continuous representations also allow the use of powerful gradient-based optimization to efficiently guide the search for optimized functional compounds. We demonstrate our method in the domain of drug-like molecules and also in the set of molecules with fewer that nine heavy atoms.",True,True,"G{\'o}mez-Bombarelli, Rafael and Wei, Jennifer N and Duvenaud, David and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and S{\'a}nchez-Lengeling, Benjam{\'\i}n and Sheberla, Dennis and Aguilera-Iparraguirre, Jorge and Hirzel, Timothy D and Adams, Ryan P and Aspuru-Guzik, Al{\'a}n",2018,,,,ACS central science
Coder as Editor: Code-driven Interpretable Molecular Optimization,2510.14455v1,segler2018generating,\cite{segler2018generating},Generating focused molecule libraries for drug discovery with recurrent neural networks,,,True,False,"Segler, Marwin HS and Kogej, Thierry and Tyrchan, Christian and Waller, Mark P",2018,,,,ACS central science
Coder as Editor: Code-driven Interpretable Molecular Optimization,2510.14455v1,schwaller2019molecular,\cite{schwaller2019molecular},Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction,,,True,False,"Schwaller, Philippe and Laino, Teodoro and Gaudin, Th{\'e}ophile and Bolgar, Peter and Hunter, Christopher A and Bekas, Costas and Lee, Alpha A",2019,,,,ACS central science
Coder as Editor: Code-driven Interpretable Molecular Optimization,2510.14455v1,gilmer2017neural,\cite{gilmer2017neural},Neural message passing for quantum chemistry,,,True,False,"Gilmer, Justin and Schoenholz, Samuel S and Riley, Patrick F and Vinyals, Oriol and Dahl, George E",2017,,,,
Coder as Editor: Code-driven Interpretable Molecular Optimization,2510.14455v1,jin2018junction,\cite{jin2018junction},Junction tree variational autoencoder for molecular graph generation,,,True,False,"Jin, Wengong and Barzilay, Regina and Jaakkola, Tommi",2018,,,,
Coder as Editor: Code-driven Interpretable Molecular Optimization,2510.14455v1,shi2020graphaf,\cite{shi2020graphaf},GraphAF: a Flow-based Autoregressive Model for Molecular Graph Generation,https://arxiv.org/abs/2001.09382v2,"Molecular graph generation is a fundamental problem for drug discovery and has been attracting growing attention. The problem is challenging since it requires not only generating chemically valid molecular structures but also optimizing their chemical properties in the meantime. Inspired by the recent progress in deep generative models, in this paper we propose a flow-based autoregressive model for graph generation called GraphAF. GraphAF combines the advantages of both autoregressive and flow-based approaches and enjoys: (1) high model flexibility for data density estimation; (2) efficient parallel computation for training; (3) an iterative sampling process, which allows leveraging chemical domain knowledge for valency checking. Experimental results show that GraphAF is able to generate 68% chemically valid molecules even without chemical knowledge rules and 100% valid molecules with chemical rules. The training process of GraphAF is two times faster than the existing state-of-the-art approach GCPN. After fine-tuning the model for goal-directed property optimization with reinforcement learning, GraphAF achieves state-of-the-art performance on both chemical property optimization and constrained property optimization.",True,True,"Shi, Chence and Xu, Minkai and Zhu, Zhaocheng and Zhang, Weinan and Zhang, Ming and Tang, Jian",2020,,,,arXiv preprint arXiv:2001.09382
Coder as Editor: Code-driven Interpretable Molecular Optimization,2510.14455v1,schutt2017quantum,\cite{schutt2017quantum},Quantum-Chemical Insights from Deep Tensor Neural Networks,https://arxiv.org/abs/1609.08259v4,"Learning from data has led to paradigm shifts in a multitude of disciplines, including web, text, and image search, speech recognition, as well as bioinformatics. Can machine learning enable similar breakthroughs in understanding quantum many-body systems? Here we develop an efficient deep learning approach that enables spatially and chemically resolved insights into quantum-mechanical observables of molecular systems. We unify concepts from many-body Hamiltonians with purpose-designed deep tensor neural networks (DTNN), which leads to size-extensive and uniformly accurate (1 kcal/mol) predictions in compositional and configurational chemical space for molecules of intermediate size. As an example of chemical relevance, the DTNN model reveals a classification of aromatic rings with respect to their stability -- a useful property that is not contained as such in the training dataset. Further applications of DTNN for predicting atomic energies and local chemical potentials in molecules, reliable isomer energies, and molecules with peculiar electronic structure demonstrate the high potential of machine learning for revealing novel insights into complex quantum-chemical systems.",True,True,"Sch{\""u}tt, Kristof T and Arbabzadah, Farhad and Chmiela, Stefan and M{\""u}ller, Klaus R and Tkatchenko, Alexandre",2017,,,,Nature communications
Coder as Editor: Code-driven Interpretable Molecular Optimization,2510.14455v1,satorras2021n,\cite{satorras2021n},Equivariant Polynomials for Graph Neural Networks,https://arxiv.org/abs/2302.11556v2,"Graph Neural Networks (GNN) are inherently limited in their expressive power. Recent seminal works (Xu et al., 2019; Morris et al., 2019b) introduced the Weisfeiler-Lehman (WL) hierarchy as a measure of expressive power. Although this hierarchy has propelled significant advances in GNN analysis and architecture developments, it suffers from several significant limitations. These include a complex definition that lacks direct guidance for model improvement and a WL hierarchy that is too coarse to study current GNNs. This paper introduces an alternative expressive power hierarchy based on the ability of GNNs to calculate equivariant polynomials of a certain degree. As a first step, we provide a full characterization of all equivariant graph polynomials by introducing a concrete basis, significantly generalizing previous results. Each basis element corresponds to a specific multi-graph, and its computation over some graph data input corresponds to a tensor contraction problem. Second, we propose algorithmic tools for evaluating the expressiveness of GNNs using tensor contraction sequences, and calculate the expressive power of popular GNNs. Finally, we enhance the expressivity of common GNN architectures by adding polynomial features or additional operations / aggregations inspired by our theory. These enhanced GNNs demonstrate state-of-the-art results in experiments across multiple graph learning benchmarks.",True,True,"Satorras, V{\i}ctor Garcia and Hoogeboom, Emiel and Welling, Max",2021,,,,
Coder as Editor: Code-driven Interpretable Molecular Optimization,2510.14455v1,chithrananda2020chemberta,\cite{chithrananda2020chemberta},ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction,https://arxiv.org/abs/2010.09885v2,"GNNs and chemical fingerprints are the predominant approaches to representing molecules for property prediction. However, in NLP, transformers have become the de-facto standard for representation learning thanks to their strong downstream task transfer. In parallel, the software ecosystem around transformers is maturing rapidly, with libraries like HuggingFace and BertViz enabling streamlined training and introspection. In this work, we make one of the first attempts to systematically evaluate transformers on molecular property prediction tasks via our ChemBERTa model. ChemBERTa scales well with pretraining dataset size, offering competitive downstream performance on MoleculeNet and useful attention-based visualization modalities. Our results suggest that transformers offer a promising avenue of future work for molecular representation learning and property prediction. To facilitate these efforts, we release a curated dataset of 77M SMILES from PubChem suitable for large-scale self-supervised pretraining.",True,True,"Chithrananda, Seyone and Grand, Gabriel and Ramsundar, Bharath",2020,,,,arXiv preprint arXiv:2010.09885
Coder as Editor: Code-driven Interpretable Molecular Optimization,2510.14455v1,fabian2020molecular,\cite{fabian2020molecular},Molecular representation learning with language models and domain-relevant auxiliary tasks,,,True,False,"Fabian, Benedek and Edlich, Thomas and Gaspar, H{\'e}l{\'e}na and Segler, Marwin and Meyers, Joshua and Fiscato, Marco and Ahmed, Mohamed",2020,,,,arXiv preprint arXiv:2011.13230
Coder as Editor: Code-driven Interpretable Molecular Optimization,2510.14455v1,irwin2022chemformer,\cite{irwin2022chemformer},Chemformer: a pre-trained transformer for computational chemistry,,,True,False,"Irwin, Ross and Dimitriadis, Spyridon and He, Jiazhen and Bjerrum, Esben Jannik",2022,,,,Machine Learning: Science and Technology
Coder as Editor: Code-driven Interpretable Molecular Optimization,2510.14455v1,liu2023molxpt,\cite{liu2023molxpt},Molxpt: Wrapping molecules with text for generative pre-training,,,True,False,"Liu, Zequn and Zhang, Wei and Xia, Yingce and Wu, Lijun and Xie, Shufang and Qin, Tao and Zhang, Ming and Liu, Tie-Yan",2023,,,,arXiv preprint arXiv:2305.10688
Coder as Editor: Code-driven Interpretable Molecular Optimization,2510.14455v1,merz2020generative,\cite{merz2020generative},Generative models for molecular design,,,True,False,"Merz Jr, Kenneth M and De Fabritiis, Gianni and Wei, Guo-Wei",2020,,,,Journal of Chemical Information and Modeling
Coder as Editor: Code-driven Interpretable Molecular Optimization,2510.14455v1,yang2023large,\cite{yang2023large},Large Language Models as Optimizers,https://arxiv.org/abs/2309.03409v3,"Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to our main application in prompt optimization, where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks. Code at https://github.com/google-deepmind/opro.",True,True,"Yang, Chengrun and Wang, Xuezhi and Lu, Yifeng and Liu, Hanxiao and Le, Quoc V and Zhou, Denny and Chen, Xinyun",2023,,,,
Coder as Editor: Code-driven Interpretable Molecular Optimization,2510.14455v1,meyerson2024language,\cite{meyerson2024language},Language Model Crossover: Variation through Few-Shot Prompting,https://arxiv.org/abs/2302.12170v3,"This paper pursues the insight that language models naturally enable an intelligent variation operator similar in spirit to evolutionary crossover. In particular, language models of sufficient scale demonstrate in-context learning, i.e. they can learn from associations between a small number of input patterns to generate outputs incorporating such associations (also called few-shot prompting). This ability can be leveraged to form a simple but powerful variation operator, i.e. to prompt a language model with a few text-based genotypes (such as code, plain-text sentences, or equations), and to parse its corresponding output as those genotypes' offspring. The promise of such language model crossover (which is simple to implement and can leverage many different open-source language models) is that it enables a simple mechanism to evolve semantically-rich text representations (with few domain-specific tweaks), and naturally benefits from current progress in language models. Experiments in this paper highlight the versatility of language-model crossover, through evolving binary bit-strings, sentences, equations, text-to-image prompts, and Python code. The conclusion is that language model crossover is a promising method for evolving genomes representable as text.",True,True,"Meyerson, Elliot and Nelson, Mark J and Bradley, Herbie and Gaier, Adam and Moradi, Arash and Hoover, Amy K and Lehman, Joel",2024,,,,ACM Transactions on Evolutionary Learning
Coder as Editor: Code-driven Interpretable Molecular Optimization,2510.14455v1,liu2024large,\cite{liu2024large},Large Language Models to Enhance Bayesian Optimization,https://arxiv.org/abs/2402.03921v2,"Bayesian optimization (BO) is a powerful approach for optimizing complex and expensive-to-evaluate black-box functions. Its importance is underscored in many applications, notably including hyperparameter tuning, but its efficacy depends on efficiently balancing exploration and exploitation. While there has been substantial progress in BO methods, striking this balance remains a delicate process. In this light, we present LLAMBO, a novel approach that integrates the capabilities of Large Language Models (LLM) within BO. At a high level, we frame the BO problem in natural language, enabling LLMs to iteratively propose and evaluate promising solutions conditioned on historical evaluations. More specifically, we explore how combining contextual understanding, few-shot learning proficiency, and domain knowledge of LLMs can improve model-based BO. Our findings illustrate that LLAMBO is effective at zero-shot warmstarting, and enhances surrogate modeling and candidate sampling, especially in the early stages of search when observations are sparse. Our approach is performed in context and does not require LLM finetuning. Additionally, it is modular by design, allowing individual components to be integrated into existing BO frameworks, or function cohesively as an end-to-end method. We empirically validate LLAMBO's efficacy on the problem of hyperparameter tuning, highlighting strong empirical performance across a range of diverse benchmarks, proprietary, and synthetic tasks.",True,True,"Liu, Tennison and Astorga, Nicol{\'a}s and Seedat, Nabeel and van der Schaar, Mihaela",2024,,,,arXiv preprint arXiv:2402.03921
Coder as Editor: Code-driven Interpretable Molecular Optimization,2510.14455v1,wang2024efficient,\cite{wang2024efficient},Efficient Evolutionary Search Over Chemical Space with Large Language Models,https://arxiv.org/abs/2406.16976v3,"Molecular discovery, when formulated as an optimization problem, presents significant computational challenges because optimization objectives can be non-differentiable. Evolutionary Algorithms (EAs), often used to optimize black-box objectives in molecular discovery, traverse chemical space by performing random mutations and crossovers, leading to a large number of expensive objective evaluations. In this work, we ameliorate this shortcoming by incorporating chemistry-aware Large Language Models (LLMs) into EAs. Namely, we redesign crossover and mutation operations in EAs using LLMs trained on large corpora of chemical information. We perform extensive empirical studies on both commercial and open-source models on multiple tasks involving property optimization, molecular rediscovery, and structure-based drug design, demonstrating that the joint usage of LLMs with EAs yields superior performance over all baseline models across single- and multi-objective settings. We demonstrate that our algorithm improves both the quality of the final solution and convergence speed, thereby reducing the number of required objective evaluations. Our code is available at http://github.com/zoom-wang112358/MOLLEO",True,True,"Wang, Haorui and Skreta, Marta and Ser, Cher-Tian and Gao, Wenhao and Kong, Lingkai and Strieth-Kalthoff, Felix and Duan, Chenru and Zhuang, Yuchen and Yu, Yue and Zhu, Yanqiao and others",2024,,,,arXiv preprint arXiv:2406.16976
Coder as Editor: Code-driven Interpretable Molecular Optimization,2510.14455v1,liu2024conversational,\cite{liu2024conversational},ChatGPT-powered Conversational Drug Editing Using Retrieval and Domain Feedback,https://arxiv.org/abs/2305.18090v1,"Recent advancements in conversational large language models (LLMs), such as ChatGPT, have demonstrated remarkable promise in various domains, including drug discovery. However, existing works mainly focus on investigating the capabilities of conversational LLMs on chemical reaction and retrosynthesis. While drug editing, a critical task in the drug discovery pipeline, remains largely unexplored. To bridge this gap, we propose ChatDrug, a framework to facilitate the systematic investigation of drug editing using LLMs. ChatDrug jointly leverages a prompt module, a retrieval and domain feedback (ReDF) module, and a conversation module to streamline effective drug editing. We empirically show that ChatDrug reaches the best performance on 33 out of 39 drug editing tasks, encompassing small molecules, peptides, and proteins. We further demonstrate, through 10 case studies, that ChatDrug can successfully identify the key substructures (e.g., the molecule functional groups, peptide motifs, and protein structures) for manipulation, generating diverse and valid suggestions for drug editing. Promisingly, we also show that ChatDrug can offer insightful explanations from a domain-specific perspective, enhancing interpretability and enabling informed decision-making. This research sheds light on the potential of ChatGPT and conversational LLMs for drug editing. It paves the way for a more efficient and collaborative drug discovery pipeline, contributing to the advancement of pharmaceutical research and development.",True,True,"Liu, Shengchao and Wang, Jiongxiao and Yang, Yijin and Wang, Chengpeng and Liu, Ling and Guo, Hongyu and Xiao, Chaowei",2024,,,,
Coder as Editor: Code-driven Interpretable Molecular Optimization,2510.14455v1,rankovic2023bochemian,\cite{rankovic2023bochemian},BoChemian: Large language model embeddings for Bayesian optimization of chemical reactions,,,True,False,"Rankovi{\'c}, Bojana and Schwaller, Philippe",2023,,,,
Coder as Editor: Code-driven Interpretable Molecular Optimization,2510.14455v1,bedrosian2024small,\cite{bedrosian2024small},Small Molecule Optimization with Large Language Models,,,True,False,"Bedrosian, Menua and Guevorguian, Philipp and Fahradyan, Tigran and Chilingaryan, Gayane and Khachatrian, Hrant and Aghajanyan, Armen",2024,,,,
Coder as Editor: Code-driven Interpretable Molecular Optimization,2510.14455v1,fang2023domain,\cite{fang2023domain},Domain-agnostic molecular generation with self-feedback,,,True,False,"Fang, Yin and Zhang, Ningyu and Chen, Zhuo and Guo, Lingbing and Fan, Xiaohui and Chen, Huajun",2023,,,,arXiv preprint arXiv:2301.11259
Coder as Editor: Code-driven Interpretable Molecular Optimization,2510.14455v1,kristiadi2024sober,\cite{kristiadi2024sober},A sober look at LLMs for material discovery: Are they actually good for Bayesian optimization over molecules?,,,True,False,"Kristiadi, Agustinus and Strieth-Kalthoff, Felix and Skreta, Marta and Poupart, Pascal and Aspuru-Guzik, Al{\'a}n and Pleiss, Geoff",2024,,,,arXiv preprint arXiv:2402.05015
Coder as Editor: Code-driven Interpretable Molecular Optimization,2510.14455v1,ye2025drugassist,\cite{ye2025drugassist},Drugassist: A large language model for molecule optimization,,,True,False,"Ye, Geyan and Cai, Xibao and Lai, Houtim and Wang, Xing and Huang, Junhong and Wang, Longyue and Liu, Wei and Zeng, Xiangxiang",2025,,,,Briefings in Bioinformatics
Coder as Editor: Code-driven Interpretable Molecular Optimization,2510.14455v1,nguyen2024lico,\cite{nguyen2024lico},Lico: Large language models for in-context molecular optimization,,,True,False,"Nguyen, Tung and Grover, Aditya",2024,,,,arXiv preprint arXiv:2406.18851
Coder as Editor: Code-driven Interpretable Molecular Optimization,2510.14455v1,zhao2025molreasoner,\cite{zhao2025molreasoner},MolReasoner: Toward Effective and Interpretable Reasoning for Molecular LLMs,,,True,False,"Zhao, Guojiang and Li, Sihang and Lu, Zixiang and Cheng, Zheng and Lin, Haitao and Wu, Lirong and Xia, Hanchen and Cai, Hengxing and Guo, Wentao and Wang, Hongshuai and others",2025,,,,arXiv preprint arXiv:2508.02066
Flow Autoencoders are Effective Protein Tokenizers,2510.00351v1,esser2021taming,\cite{esser2021taming},Taming transformers for high-resolution image synthesis,,,True,False,"Esser, Patrick and Rombach, Robin and Ommer, Bjorn",2021,,,,
Flow Autoencoders are Effective Protein Tokenizers,2510.00351v1,van2017neural,\cite{van2017neural},Neural Discrete Representation Learning,https://arxiv.org/abs/1711.00937v2,"Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of ""posterior collapse"" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.",True,True,"Van Den Oord, Aaron and Vinyals, Oriol and others",2017,,,,Advances in neural information processing systems
Flow Autoencoders are Effective Protein Tokenizers,2510.00351v1,van2022foldseek,\cite{van2022foldseek},Foldseek: fast and accurate protein structure search,,,True,False,"van Kempen, Michel and Kim, Stephanie S and Tumescheit, Charlotte and Mirdita, Milot and Gilchrist, Cameron LM and S{\""o}ding, Johannes and Steinegger, Martin",2022,,,,Biorxiv
Flow Autoencoders are Effective Protein Tokenizers,2510.00351v1,steinegger2017mmseqs2,\cite{steinegger2017mmseqs2},MMseqs2 enables sensitive protein sequence searching for the analysis of massive data sets,,,True,False,"Steinegger, Martin and S{\""o}ding, Johannes",2017,,,,Nature biotechnology
Flow Autoencoders are Effective Protein Tokenizers,2510.00351v1,ist-gaujac2024learning,\cite{ist-gaujac2024learning},Learning the language of protein structure,,,True,False,"Gaujac, Benoit and Don{\`a}, J{\'e}r{\'e}mie and Copoiu, Liviu and Atkinson, Timothy and Pierrot, Thomas and Barrett, Thomas D",2024,,,,arXiv preprint arXiv:2405.15840
Flow Autoencoders are Effective Protein Tokenizers,2510.00351v1,lin2023protokens,\cite{lin2023protokens},Protokens: A machine-learned language for compact and informative encoding of protein 3d structures,,,True,False,"Lin, Xiaohan and Chen, Zhenyu and Li, Yanheng and Lu, Xingyu and Fan, Chuanliu and Cao, Ziqiang and Feng, Shihao and Gao, Yi Qin and Zhang, Jun",2023,,,,
Flow Autoencoders are Effective Protein Tokenizers,2510.00351v1,esm3-hayes2025simulating,\cite{esm3-hayes2025simulating},Simulating 500 million years of evolution with a language model,,,True,False,"Hayes, Thomas and Rao, Roshan and Akin, Halil and Sofroniew, Nicholas J and Oktay, Deniz and Lin, Zeming and Verkuil, Robert and Tran, Vincent Q and Deaton, Jonathan and Wiggert, Marius and others",2025,,,,Science
Flow Autoencoders are Effective Protein Tokenizers,2510.00351v1,dplm2-ang2024dplm,\cite{dplm2-ang2024dplm},DPLM-2: A Multimodal Diffusion Protein Language Model,https://arxiv.org/abs/2410.13782v1,"Proteins are essential macromolecules defined by their amino acid sequences, which determine their three-dimensional structures and, consequently, their functions in all living organisms. Therefore, generative protein modeling necessitates a multimodal approach to simultaneously model, understand, and generate both sequences and structures. However, existing methods typically use separate models for each modality, limiting their ability to capture the intricate relationships between sequence and structure. This results in suboptimal performance in tasks that requires joint understanding and generation of both modalities. In this paper, we introduce DPLM-2, a multimodal protein foundation model that extends discrete diffusion protein language model (DPLM) to accommodate both sequences and structures. To enable structural learning with the language model, 3D coordinates are converted to discrete tokens using a lookup-free quantization-based tokenizer. By training on both experimental and high-quality synthetic structures, DPLM-2 learns the joint distribution of sequence and structure, as well as their marginals and conditionals. We also implement an efficient warm-up strategy to exploit the connection between large-scale evolutionary data and structural inductive biases from pre-trained sequence-based protein language models. Empirical evaluation shows that DPLM-2 can simultaneously generate highly compatible amino acid sequences and their corresponding 3D structures eliminating the need for a two-stage generation approach. Moreover, DPLM-2 demonstrates competitive performance in various conditional generation tasks, including folding, inverse folding, and scaffolding with multimodal motif inputs, as well as providing structure-aware representations for predictive tasks.",True,True,"Wang, Xinyou and Zheng, Zaixiang and Ye, Fei and Xue, Dongyu and Huang, Shujian and Gu, Quanquan",2024,,,,arXiv preprint arXiv:2410.13782
Flow Autoencoders are Effective Protein Tokenizers,2510.00351v1,framediff-yim2023se,\cite{framediff-yim2023se},SE (3) diffusion model with application to protein backbone generation,,,True,False,"Yim, Jason and Trippe, Brian L and De Bortoli, Valentin and Mathieu, Emile and Doucet, Arnaud and Barzilay, Regina and Jaakkola, Tommi",2023,,,,arXiv preprint arXiv:2302.02277
Flow Autoencoders are Effective Protein Tokenizers,2510.00351v1,rf-watson2023novo,\cite{rf-watson2023novo},De novo design of protein structure and function with RFdiffusion,,,True,False,"Watson, Joseph L and Juergens, David and Bennett, Nathaniel R and Trippe, Brian L and Yim, Jason and Eisenach, Helen E and Ahern, Woody and Borst, Andrew J and Ragotte, Robert J and Milles, Lukas F and others",2023,,,,Nature
Flow Autoencoders are Effective Protein Tokenizers,2510.00351v1,chroma-ingraham2023illuminating,\cite{chroma-ingraham2023illuminating},Illuminating protein space with a programmable generative model,,,True,False,"Ingraham, John B and Baranov, Max and Costello, Zak and Barber, Karl W and Wang, Wujie and Ismail, Ahmed and Frappier, Vincent and Lord, Dana M and Ng-Thow-Hing, Christopher and Van Vlack, Erik R and others",2023,,,,Nature
Flow Autoencoders are Effective Protein Tokenizers,2510.00351v1,frameflow-yim2023fast,\cite{frameflow-yim2023fast},Fast protein backbone generation with se (3) flow matching,,,True,False,"Yim, Jason and Campbell, Andrew and Foong, Andrew YK and Gastegger, Michael and Jim{\'e}nez-Luna, Jos{\'e} and Lewis, Sarah and Satorras, Victor Garcia and Veeling, Bastiaan S and Barzilay, Regina and Jaakkola, Tommi and others",2023,,,,arXiv preprint arXiv:2310.05297
Flow Autoencoders are Effective Protein Tokenizers,2510.00351v1,foldflow2-bose2023se,\cite{foldflow2-bose2023se},Se (3)-stochastic flow matching for protein backbone generation,,,True,False,"Bose, Avishek Joey and Akhound-Sadegh, Tara and Huguet, Guillaume and Fatras, Kilian and Rector-Brooks, Jarrid and Liu, Cheng-Hao and Nica, Andrei Cristian and Korablyov, Maksym and Bronstein, Michael and Tong, Alexander",2023,,,,arXiv preprint arXiv:2310.02391
Flow Autoencoders are Effective Protein Tokenizers,2510.00351v1,huguet2024sequence,\cite{huguet2024sequence},Sequence-augmented SE (3)-flow matching for conditional protein generation,,,True,False,"Huguet, Guillaume and Vuckovic, James and Fatras, Kilian and Thibodeau-Laufer, Eric and Lemos, Pablo and Islam, Riashat and Liu, Chenghao and Rector-Brooks, Jarrid and Akhound-Sadegh, Tara and Bronstein, Michael and others",2024,,,,Advances in neural information processing systems
Flow Autoencoders are Effective Protein Tokenizers,2510.00351v1,proteina-geffner2025proteina,\cite{proteina-geffner2025proteina},Proteina: Scaling Flow-based Protein Structure Generative Models,https://arxiv.org/abs/2503.00710v1,"Recently, diffusion- and flow-based generative models of protein structures have emerged as a powerful tool for de novo protein design. Here, we develop Proteina, a new large-scale flow-based protein backbone generator that utilizes hierarchical fold class labels for conditioning and relies on a tailored scalable transformer architecture with up to 5x as many parameters as previous models. To meaningfully quantify performance, we introduce a new set of metrics that directly measure the distributional similarity of generated proteins with reference sets, complementing existing metrics. We further explore scaling training data to millions of synthetic protein structures and explore improved training and sampling recipes adapted to protein backbone generation. This includes fine-tuning strategies like LoRA for protein backbones, new guidance methods like classifier-free guidance and autoguidance for protein backbones, and new adjusted training objectives. Proteina achieves state-of-the-art performance on de novo protein backbone design and produces diverse and designable proteins at unprecedented length, up to 800 residues. The hierarchical conditioning offers novel control, enabling high-level secondary-structure guidance as well as low-level fold-specific generation.",True,True,"Geffner, Tomas and Didi, Kieran and Zhang, Zuobai and Reidenbach, Danny and Cao, Zhonglin and Yim, Jason and Geiger, Mario and Dallago, Christian and Kucukbenli, Emine and Vahdat, Arash and others",2025,,,,arXiv preprint arXiv:2503.00710
Flow Autoencoders are Effective Protein Tokenizers,2510.00351v1,lin2024out,\cite{lin2024out},"Out of many, one: Designing and scaffolding proteins at the scale of the structural universe with genie 2",,,True,False,"Lin, Yeqing and Lee, Minji and Zhang, Zhao and AlQuraishi, Mohammed",2024,,,,arXiv preprint arXiv:2405.15489
Flow Autoencoders are Effective Protein Tokenizers,2510.00351v1,yadav2020situ,\cite{yadav2020situ},In situ monitoring systems of the SLM process: On the need to develop machine learning models for data processing,,,True,False,"Yadav, Pinku and Rigo, Olivier and Arvieu, Corinne and Le Guen, Emilie and Lacoste, Eric",2020,,,,Crystals
Flow Autoencoders are Effective Protein Tokenizers,2510.00351v1,bunne2024build,\cite{bunne2024build},How to build the virtual cell with artificial intelligence: Priorities and opportunities,,,True,False,"Bunne, Charlotte and Roohani, Yusuf and Rosen, Yanay and Gupta, Ankit and Zhang, Xikun and Roed, Marcel and Alexandrov, Theo and AlQuraishi, Mohammed and Brennan, Patricia and Burkhardt, Daniel B and others",2024,,,,Cell
Flow Autoencoders are Effective Protein Tokenizers,2510.00351v1,esser2024scaling,\cite{esser2024scaling},Scaling rectified flow transformers for high-resolution image synthesis,,,True,False,"Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and M{\""u}ller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and others",2024,,,,
Flow Autoencoders are Effective Protein Tokenizers,2510.00351v1,lipman2022flow,\cite{lipman2022flow},Flow matching for generative modeling,,,True,False,"Lipman, Yaron and Chen, Ricky TQ and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt",2022,,,,arXiv preprint arXiv:2210.02747
Flow Autoencoders are Effective Protein Tokenizers,2510.00351v1,preechakul2022diffusion,\cite{preechakul2022diffusion},Diffusion autoencoders: Toward a meaningful and decodable representation,,,True,False,"Preechakul, Konpat and Chatthee, Nattanat and Wizadwongsa, Suttisak and Suwajanakorn, Supasorn",2022,,,,
Flow Autoencoders are Effective Protein Tokenizers,2510.00351v1,flowmo-sargent2025flow,\cite{flowmo-sargent2025flow},Flow to the mode: Mode-seeking diffusion autoencoders for state-of-the-art image tokenization,,,True,False,"Sargent, Kyle and Hsu, Kyle and Johnson, Justin and Fei-Fei, Li and Wu, Jiajun",2025,,,,arXiv preprint arXiv:2503.11056
Flow Autoencoders are Effective Protein Tokenizers,2510.00351v1,chen2025diffusion,\cite{chen2025diffusion},Diffusion autoencoders are scalable image tokenizers,,,True,False,"Chen, Yinbo and Girdhar, Rohit and Wang, Xiaolong and Rambhatla, Sai Saketh and Misra, Ishan",2025,,,,arXiv preprint arXiv:2501.18593
Flow Autoencoders are Effective Protein Tokenizers,2510.00351v1,wohlwend2025boltz,\cite{wohlwend2025boltz},Boltz-1 democratizing biomolecular interaction modeling,,,True,False,"Wohlwend, Jeremy and Corso, Gabriele and Passaro, Saro and Getz, Noah and Reveiz, Mateo and Leidal, Ken and Swiderski, Wojtek and Atkinson, Liam and Portnoi, Tally and Chinn, Itamar and others",2025,,,,BioRxiv
HyperHELM: Hyperbolic Hierarchy Encoding for mRNA Language Modeling,2509.24655v2,penic2024rinalmo,\cite{penic2024rinalmo},RiNALMo: General-Purpose RNA Language Models Can Generalize Well on Structure Prediction Tasks,https://arxiv.org/abs/2403.00043v2,"While RNA has recently been recognized as an interesting small-molecule drug target, many challenges remain to be addressed before we take full advantage of it. This emphasizes the necessity to improve our understanding of its structures and functions. Over the years, sequencing technologies have produced an enormous amount of unlabeled RNA data, which hides a huge potential. Motivated by the successes of protein language models, we introduce RiboNucleic Acid Language Model (RiNALMo) to unveil the hidden code of RNA. RiNALMo is the largest RNA language model to date, with 650M parameters pre-trained on 36M non-coding RNA sequences from several databases. It can extract hidden knowledge and capture the underlying structure information implicitly embedded within the RNA sequences. RiNALMo achieves state-of-the-art results on several downstream tasks. Notably, we show that its generalization capabilities overcome the inability of other deep learning methods for secondary structure prediction to generalize on unseen RNA families.",True,True,,,,,,
HyperHELM: Hyperbolic Hierarchy Encoding for mRNA Language Modeling,2509.24655v2,mollaysa2025biolangfusion,\cite{mollaysa2025biolangfusion},"BioLangFusion: Multimodal Fusion of DNA, mRNA, and Protein Language Models",https://arxiv.org/abs/2506.08936v1,"We present BioLangFusion, a simple approach for integrating pre-trained DNA, mRNA, and protein language models into unified molecular representations. Motivated by the central dogma of molecular biology (information flow from gene to transcript to protein), we align per-modality embeddings at the biologically meaningful codon level (three nucleotides encoding one amino acid) to ensure direct cross-modal correspondence. BioLangFusion studies three standard fusion techniques: (i) codon-level embedding concatenation, (ii) entropy-regularized attention pooling inspired by multiple-instance learning, and (iii) cross-modal multi-head attention -- each technique providing a different inductive bias for combining modality-specific signals. These methods require no additional pre-training or modification of the base models, allowing straightforward integration with existing sequence-based foundation models. Across five molecular property prediction tasks, BioLangFusion outperforms strong unimodal baselines, showing that even simple fusion of pre-trained models can capture complementary multi-omic information with minimal overhead.",True,True,,,,,,
HyperHELM: Hyperbolic Hierarchy Encoding for mRNA Language Modeling,2509.24655v2,moskalev2024hyena,\cite{moskalev2024hyena},SE(3)-Hyena Operator for Scalable Equivariant Learning,https://arxiv.org/abs/2407.01049v2,"Modeling global geometric context while maintaining equivariance is crucial for accurate predictions in many fields such as biology, chemistry, or vision. Yet, this is challenging due to the computational demands of processing high-dimensional data at scale. Existing approaches such as equivariant self-attention or distance-based message passing, suffer from quadratic complexity with respect to sequence length, while localized methods sacrifice global information. Inspired by the recent success of state-space and long-convolutional models, in this work, we introduce SE(3)-Hyena operator, an equivariant long-convolutional model based on the Hyena operator. The SE(3)-Hyena captures global geometric context at sub-quadratic complexity while maintaining equivariance to rotations and translations. Evaluated on equivariant associative recall and n-body modeling, SE(3)-Hyena matches or outperforms equivariant self-attention while requiring significantly less memory and computational resources for long sequences. Our model processes the geometric context of 20k tokens x3.5 times faster than the equivariant transformer and allows x175 longer a context within the same memory budget.",True,True,,,,,,
HyperHELM: Hyperbolic Hierarchy Encoding for mRNA Language Modeling,2509.24655v2,moskalev2025geometric,\cite{moskalev2025geometric},Geometric Hyena Networks for Large-scale Equivariant Learning,https://arxiv.org/abs/2505.22560v1,"Processing global geometric context while preserving equivariance is crucial when modeling biological, chemical, and physical systems. Yet, this is challenging due to the computational demands of equivariance and global context at scale. Standard methods such as equivariant self-attention suffer from quadratic complexity, while local methods such as distance-based message passing sacrifice global information. Inspired by the recent success of state-space and long-convolutional models, we introduce Geometric Hyena, the first equivariant long-convolutional model for geometric systems. Geometric Hyena captures global geometric context at sub-quadratic complexity while maintaining equivariance to rotations and translations. Evaluated on all-atom property prediction of large RNA molecules and full protein molecular dynamics, Geometric Hyena outperforms existing equivariant models while requiring significantly less memory and compute that equivariant self-attention. Notably, our model processes the geometric context of 30k tokens 20x faster than the equivariant transformer and allows 72x longer context within the same budget.",True,True,,,,,,
HyperHELM: Hyperbolic Hierarchy Encoding for mRNA Language Modeling,2509.24655v2,sarkar2011low,\cite{sarkar2011low},Identification of the slow E3 transition 136mCs -> 136Cs with conversion electrons,https://arxiv.org/abs/1106.6240v1,"We performed at ISOLDE the spectroscopy of the decay of the 8- isomer in 136Cs by and conversion-electron detection. For the first time the excitation energy of the isomer and the multipolarity of its decay have been measured. The half-life of the isomeric state was remeasured to T1/2 = 17.5(2) s. This isomer decays via a very slow 518 keV E3 transition to the ground state. In addition to this, a much weaker decay branch via a 413 keV M4 and a subsequent 105 keV E2 transition has been found. Thus we have found a new level at 105 keV with spin 4+ between the isomeric and the ground state. The results are discussed in comparison to shell model calculations.",True,True,,,,,,
HyperHELM: Hyperbolic Hierarchy Encoding for mRNA Language Modeling,2509.24655v2,chamberlain2017neural,\cite{chamberlain2017neural},Neural Embeddings of Graphs in Hyperbolic Space,https://arxiv.org/abs/1705.10359v1,"Neural embeddings have been used with great success in Natural Language Processing (NLP). They provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph based tasks, embeddings have been learned in high-dimensional Euclidean spaces. However, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but negatively curved, hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that embedding graphs in their natural geometry significantly improves performance on downstream tasks for several real-world public datasets.",True,True,,,,,,
HyperHELM: Hyperbolic Hierarchy Encoding for mRNA Language Modeling,2509.24655v2,ganea2018hyperbolic,\cite{ganea2018hyperbolic},Hyperbolic Entailment Cones for Learning Hierarchical Embeddings,https://arxiv.org/abs/1804.01882v3,"Learning graph representations via low-dimensional embeddings that preserve relevant network properties is an important class of problems in machine learning. We here present a novel method to embed directed acyclic graphs. Following prior work, we first advocate for using hyperbolic spaces which provably model tree-like structures better than Euclidean geometry. Second, we view hierarchical relations as partial orders defined using a family of nested geodesically convex cones. We prove that these entailment cones admit an optimal shape with a closed form expression both in the Euclidean and hyperbolic spaces, and they canonically define the embedding learning process. Experiments show significant improvements of our method over strong recent baselines both in terms of representational capacity and generalization.",True,True,,,,,,
HyperHELM: Hyperbolic Hierarchy Encoding for mRNA Language Modeling,2509.24655v2,becigneul2018riemannian,\cite{becigneul2018riemannian},Riemannian Adaptive Optimization Methods,https://arxiv.org/abs/1810.00760v2,"Several first order stochastic optimization methods commonly used in the Euclidean domain such as stochastic gradient descent (SGD), accelerated gradient descent or variance reduced methods have already been adapted to certain Riemannian settings. However, some of the most popular of these optimization tools - namely Adam , Adagrad and the more recent Amsgrad - remain to be generalized to Riemannian manifolds. We discuss the difficulty of generalizing such adaptive schemes to the most agnostic Riemannian setting, and then provide algorithms and convergence proofs for geodesically convex objectives in the particular case of a product of Riemannian manifolds, in which adaptivity is implemented across manifolds in the cartesian product. Our generalization is tight in the sense that choosing the Euclidean space as Riemannian manifold yields the same algorithms and regret bounds as those that were already known for the standard algorithms. Experimentally, we show faster convergence and to a lower train loss value for Riemannian adaptive methods over their corresponding baselines on the realistic task of embedding the WordNet taxonomy in the Poincare ball.",True,True,,,,,,
HyperHELM: Hyperbolic Hierarchy Encoding for mRNA Language Modeling,2509.24655v2,khrulkov2020hyperbolic,\cite{khrulkov2020hyperbolic},Performance of Hyperbolic Geometry Models on Top-N Recommendation Tasks,https://arxiv.org/abs/2008.06716v1,"We introduce a simple autoencoder based on hyperbolic geometry for solving standard collaborative filtering problem. In contrast to many modern deep learning techniques, we build our solution using only a single hidden layer. Remarkably, even with such a minimalistic approach, we not only outperform the Euclidean counterpart but also achieve a competitive performance with respect to the current state-of-the-art. We additionally explore the effects of space curvature on the quality of hyperbolic models and propose an efficient data-driven method for estimating its optimal value.",True,True,,,,,,
HyperHELM: Hyperbolic Hierarchy Encoding for mRNA Language Modeling,2509.24655v2,ghadimi2021hyperbolic,\cite{ghadimi2021hyperbolic},Hyperbolic Busemann Learning with Ideal Prototypes,https://arxiv.org/abs/2106.14472v2,"Hyperbolic space has become a popular choice of manifold for representation learning of various datatypes from tree-like structures and text to graphs. Building on the success of deep learning with prototypes in Euclidean and hyperspherical spaces, a few recent works have proposed hyperbolic prototypes for classification. Such approaches enable effective learning in low-dimensional output spaces and can exploit hierarchical relations amongst classes, but require privileged information about class labels to position the hyperbolic prototypes. In this work, we propose Hyperbolic Busemann Learning. The main idea behind our approach is to position prototypes on the ideal boundary of the Poincaré ball, which does not require prior label knowledge. To be able to compute proximities to ideal prototypes, we introduce the penalised Busemann loss. We provide theory supporting the use of ideal prototypes and the proposed loss by proving its equivalence to logistic regression in the one-dimensional case. Empirically, we show that our approach provides a natural interpretation of classification confidence, while outperforming recent hyperspherical and hyperbolic prototype approaches.",True,True,,,,,,
HyperHELM: Hyperbolic Hierarchy Encoding for mRNA Language Modeling,2509.24655v2,mettes2024hyperbolic,\cite{mettes2024hyperbolic},Adversarial Attacks on Hyperbolic Networks,https://arxiv.org/abs/2412.01495v1,"As hyperbolic deep learning grows in popularity, so does the need for adversarial robustness in the context of such a non-Euclidean geometry. To this end, this paper proposes hyperbolic alternatives to the commonly used FGM and PGD adversarial attacks. Through interpretable synthetic benchmarks and experiments on existing datasets, we show how the existing and newly proposed attacks differ. Moreover, we investigate the differences in adversarial robustness between Euclidean and fully hyperbolic networks. We find that these networks suffer from different types of vulnerabilities and that the newly proposed hyperbolic attacks cannot address these differences. Therefore, we conclude that the shifts in adversarial robustness are due to the models learning distinct patterns resulting from their different geometries.",True,True,,,,,,
HyperHELM: Hyperbolic Hierarchy Encoding for mRNA Language Modeling,2509.24655v2,chami2019hyperbolic,\cite{chami2019hyperbolic},Hyperbolic Graph Convolutional Neural Networks,https://arxiv.org/abs/1910.12933v1,"Graph convolutional neural networks (GCNs) embed nodes in a graph into Euclidean space, which has been shown to incur a large distortion when embedding real-world graphs with scale-free or hierarchical structure. Hyperbolic geometry offers an exciting alternative, as it enables embeddings with much smaller distortion. However, extending GCNs to hyperbolic geometry presents several unique challenges because it is not clear how to define neural network operations, such as feature transformation and aggregation, in hyperbolic space. Furthermore, since input features are often Euclidean, it is unclear how to transform the features into hyperbolic embeddings with the right amount of curvature. Here we propose Hyperbolic Graph Convolutional Neural Network (HGCN), the first inductive hyperbolic GCN that leverages both the expressiveness of GCNs and hyperbolic geometry to learn inductive node representations for hierarchical and scale-free graphs. We derive GCN operations in the hyperboloid model of hyperbolic space and map Euclidean input features to embeddings in hyperbolic spaces with different trainable curvature at each layer. Experiments demonstrate that HGCN learns embeddings that preserve hierarchical structure, and leads to improved performance when compared to Euclidean analogs, even with very low dimensional embeddings: compared to state-of-the-art GCNs, HGCN achieves an error reduction of up to 63.1% in ROC AUC for link prediction and of up to 47.5% in F1 score for node classification, also improving state-of-the art on the Pubmed dataset.",True,True,,,,,,
HyperHELM: Hyperbolic Hierarchy Encoding for mRNA Language Modeling,2509.24655v2,tifrea2018poincar,\cite{tifrea2018poincar},Poincaré GloVe: Hyperbolic Word Embeddings,https://arxiv.org/abs/1810.06546v2,"Words are not created equal. In fact, they form an aristocratic graph with a latent hierarchical structure that the next generation of unsupervised learned word embeddings should reveal. In this paper, justified by the notion of delta-hyperbolicity or tree-likeliness of a space, we propose to embed words in a Cartesian product of hyperbolic spaces which we theoretically connect to the Gaussian word embeddings and their Fisher geometry. This connection allows us to introduce a novel principled hypernymy score for word embeddings. Moreover, we adapt the well-known Glove algorithm to learn unsupervised word embeddings in this type of Riemannian manifolds. We further explain how to solve the analogy task using the Riemannian parallel transport that generalizes vector arithmetics to this new type of geometry. Empirically, based on extensive experiments, we prove that our embeddings, trained unsupervised, are the first to simultaneously outperform strong and popular baselines on the tasks of similarity, analogy and hypernymy detection. In particular, for word hypernymy, we obtain new state-of-the-art on fully unsupervised WBLESS classification accuracy.",True,True,,,,,,
HyperHELM: Hyperbolic Hierarchy Encoding for mRNA Language Modeling,2509.24655v2,dhingra2018embedding,\cite{dhingra2018embedding},Embedding Text in Hyperbolic Spaces,https://arxiv.org/abs/1806.04313v1,"Natural language text exhibits hierarchical structure in a variety of respects. Ideally, we could incorporate our prior knowledge of this hierarchical structure into unsupervised learning algorithms that work on text data. Recent work by Nickel & Kiela (2017) proposed using hyperbolic instead of Euclidean embedding spaces to represent hierarchical data and demonstrated encouraging results when embedding graphs. In this work, we extend their method with a re-parameterization technique that allows us to learn hyperbolic embeddings of arbitrarily parameterized objects. We apply this framework to learn word and sentence embeddings in hyperbolic space in an unsupervised manner from text corpora. The resulting embeddings seem to encode certain intuitive notions of hierarchy, such as word-context frequency and phrase constituency. However, the implicit continuous hierarchy in the learned hyperbolic space makes interrogating the model's learned hierarchies more difficult than for models that learn explicit edges between items. The learned hyperbolic embeddings show improvements over Euclidean embeddings in some -- but not all -- downstream tasks, suggesting that hierarchical organization is more useful for some tasks than others.",True,True,,,,,,
HyperHELM: Hyperbolic Hierarchy Encoding for mRNA Language Modeling,2509.24655v2,snell2017prototypical,\cite{snell2017prototypical},Prototypical Networks for Few-shot Learning,https://arxiv.org/abs/1703.05175v2,"We propose prototypical networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend prototypical networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.",True,True,,,,,,
Diffusion-Based Quality Control of Medical Image Segmentations across Organs,2511.09588v1,doi:10.1073/pnas.2216399120,\cite{doi:10.1073/pnas.2216399120},Robust machine learning segmentation for large-scale analysis of heterogeneous clinical brain MRI datasets,,,True,False,Benjamin Billot  and Colin Magdamo  and You Cheng  and Steven E. Arnold  and Sudeshna Das  and Juan Eugenio Iglesias,2023,,,,Proceedings of the National Academy of Sciences
Diffusion-Based Quality Control of Medical Image Segmentations across Organs,2511.09588v1,kalkhof2023m3d,\cite{kalkhof2023m3d},{M3D-NCA: Robust 3D Segmentation with Built-In Quality Control},,,True,False,"Kalkhof, John and Mukhopadhyay, Anirban",2023,,,,
Diffusion-Based Quality Control of Medical Image Segmentations across Organs,2511.09588v1,qiu2023qcresunet,\cite{qiu2023qcresunet},{QCResUNet: Joint subject-level and voxel-level prediction of segmentation quality},,,True,False,"Qiu, Peijie and Chakrabarty, Satrajit and Nguyen, Phuc and Ghosh, Soumyendu Sekhar and Sotiras, Aristeidis",2023,,,,
Diffusion-Based Quality Control of Medical Image Segmentations across Organs,2511.09588v1,lin2022novel,\cite{lin2022novel},A novel quality control algorithm for medical image segmentation based on fuzzy uncertainty,,,True,False,"Lin, Qiao and Chen, Xin and Chen, Chao and Garibaldi, Jonathan M",2022,,,,IEEE Transactions on Fuzzy Systems
Diffusion-Based Quality Control of Medical Image Segmentations across Organs,2511.09588v1,audelan2019unsupervised,\cite{audelan2019unsupervised},{Unsupervised quality control of image segmentation based on Bayesian learning},,,True,False,"Audelan, Beno{\^\i}t and Delingette, Herv{\'e}",2019,,,,
Diffusion-Based Quality Control of Medical Image Segmentations across Organs,2511.09588v1,fournel2021medical,\cite{fournel2021medical},{Medical image segmentation automatic quality control: A multi-dimensional approach},,,True,False,"Fournel, Joris and Bartoli, Axel and Bendahan, David and Guye, Maxime and Bernard, Monique and Rauseo, Elisa and Khanji, Mohammed Y and Petersen, Steffen E and Jacquier, Alexis and Ghattas, Badih",2021,,,,Medical Image Analysis
Diffusion-Based Quality Control of Medical Image Segmentations across Organs,2511.09588v1,galati2021efficient,\cite{galati2021efficient},Efficient Model Monitoring for Quality Control in Cardiac Image Segmentation,,,True,False,Francesco Galati and Maria A. Zuluaga,2021,,,,
Diffusion-Based Quality Control of Medical Image Segmentations across Organs,2511.09588v1,robinson2018realtime,\cite{robinson2018realtime},Real-time Prediction of Segmentation Quality,https://arxiv.org/abs/1806.06244v1,"Recent advances in deep learning based image segmentation methods have enabled real-time performance with human-level accuracy. However, occasionally even the best method fails due to low image quality, artifacts or unexpected behaviour of black box algorithms. Being able to predict segmentation quality in the absence of ground truth is of paramount importance in clinical practice, but also in large-scale studies to avoid the inclusion of invalid data in subsequent analysis.
  In this work, we propose two approaches of real-time automated quality control for cardiovascular MR segmentations using deep learning. First, we train a neural network on 12,880 samples to predict Dice Similarity Coefficients (DSC) on a per-case basis. We report a mean average error (MAE) of 0.03 on 1,610 test samples and 97% binary classification accuracy for separating low and high quality segmentations. Secondly, in the scenario where no manually annotated data is available, we train a network to predict DSC scores from estimated quality obtained via a reverse testing strategy. We report an MAE=0.14 and 91% binary classification accuracy for this case. Predictions are obtained in real-time which, when combined with real-time segmentation methods, enables instant feedback on whether an acquired scan is analysable while the patient is still in the scanner. This further enables new applications of optimising image acquisition towards best possible analysis results.",True,True,"Robinson, Robert
and Oktay, Ozan
and Bai, Wenjia
and Valindria, Vanya V.
and Sanghvi, Mihir M.
and Aung, Nay
and Paiva, Jos{\'e} M.
and Zemrak, Filip
and Fung, Kenneth
and Lukaschuk, Elena
and Lee, Aaron M.
and Carapella, Valentina
and Kim, Young Jin
and Kainz, Bernhard
and Piechnik, Stefan K.
and Neubauer, Stefan
and Petersen, Steffen E.
and Page, Chris
and Rueckert, Daniel
and Glocker, Ben",2018,,,,
Diffusion-Based Quality Control of Medical Image Segmentations across Organs,2511.09588v1,specktor2025segqc,\cite{specktor2025segqc},SegQC: a segmentation network-based framework for multi-metric segmentation quality control and segmentation error detection in volumetric medical images,https://arxiv.org/abs/2411.07601v1,"Quality control of structures segmentation in volumetric medical images is important for identifying segmentation errors in clinical practice and for facilitating model development. This paper introduces SegQC, a novel framework for segmentation quality estimation and segmentation error detection. SegQC computes an estimate measure of the quality of a segmentation in volumetric scans and in their individual slices and identifies possible segmentation error regions within a slice. The key components include: 1. SegQC-Net, a deep network that inputs a scan and its segmentation mask and outputs segmentation error probabilities for each voxel in the scan; 2. three new segmentation quality metrics, two overlap metrics and a structure size metric, computed from the segmentation error probabilities; 3. a new method for detecting possible segmentation errors in scan slices computed from the segmentation error probabilities. We introduce a new evaluation scheme to measure segmentation error discrepancies based on an expert radiologist corrections of automatically produced segmentations that yields smaller observer variability and is closer to actual segmentation errors. We demonstrate SegQC on three fetal structures in 198 fetal MRI scans: fetal brain, fetal body and the placenta. To assess the benefits of SegQC, we compare it to the unsupervised Test Time Augmentation (TTA)-based quality estimation. Our studies indicate that SegQC outperforms TTA-based quality estimation in terms of Pearson correlation and MAE for fetal body and fetal brain structures segmentation. Our segmentation error detection method achieved recall and precision rates of 0.77 and 0.48 for fetal body, and 0.74 and 0.55 for fetal brain segmentation error detection respectively. SegQC enhances segmentation metrics estimation for whole scans and individual slices, as well as provides error regions detection.",True,True,"Specktor-Fadida, Bella and Ben-Sira, Liat and Ben-Bashat, Dafna and Joskowicz, Leo",2025,,,,Medical Image Analysis
Diffusion-Based Quality Control of Medical Image Segmentations across Organs,2511.09588v1,valindria2017reverse,\cite{valindria2017reverse},Reverse Classification Accuracy: Predicting Segmentation Performance in the Absence of Ground Truth,https://arxiv.org/abs/1702.03407v1,"When integrating computational tools such as automatic segmentation into clinical practice, it is of utmost importance to be able to assess the level of accuracy on new data, and in particular, to detect when an automatic method fails. However, this is difficult to achieve due to absence of ground truth. Segmentation accuracy on clinical data might be different from what is found through cross-validation because validation data is often used during incremental method development, which can lead to overfitting and unrealistic performance expectations. Before deployment, performance is quantified using different metrics, for which the predicted segmentation is compared to a reference segmentation, often obtained manually by an expert. But little is known about the real performance after deployment when a reference is unavailable. In this paper, we introduce the concept of reverse classification accuracy (RCA) as a framework for predicting the performance of a segmentation method on new data. In RCA we take the predicted segmentation from a new image to train a reverse classifier which is evaluated on a set of reference images with available ground truth. The hypothesis is that if the predicted segmentation is of good quality, then the reverse classifier will perform well on at least some of the reference images. We validate our approach on multi-organ segmentation with different classifiers and segmentation methods. Our results indicate that it is indeed possible to predict the quality of individual segmentations, in the absence of ground truth. Thus, RCA is ideal for integration into automatic processing pipelines in clinical routine and as part of large-scale image analysis studies.",True,True,"Valindria, Vanya V and Lavdas, Ioannis and Bai, Wenjia and Kamnitsas, Konstantinos and Aboagye, Eric O and Rockall, Andrea G and Rueckert, Daniel and Glocker, Ben",2017,,,,IEEE Transactions on Medical Imaging
Diffusion-Based Quality Control of Medical Image Segmentations across Organs,2511.09588v1,wang2020deep,\cite{wang2020deep},Deep Generative Model-based Quality Control for Cardiac MRI Segmentation,https://arxiv.org/abs/2006.13379v1,"In recent years, convolutional neural networks have demonstrated promising performance in a variety of medical image segmentation tasks. However, when a trained segmentation model is deployed into the real clinical world, the model may not perform optimally. A major challenge is the potential poor-quality segmentations generated due to degraded image quality or domain shift issues. There is a timely need to develop an automated quality control method that can detect poor segmentations and feedback to clinicians. Here we propose a novel deep generative model-based framework for quality control of cardiac MRI segmentation. It first learns a manifold of good-quality image-segmentation pairs using a generative model. The quality of a given test segmentation is then assessed by evaluating the difference from its projection onto the good-quality manifold. In particular, the projection is refined through iterative search in the latent space. The proposed method achieves high prediction accuracy on two publicly available cardiac MRI datasets. Moreover, it shows better generalisation ability than traditional regression-based methods. Our approach provides a real-time and model-agnostic quality control for cardiac MRI segmentation, which has the potential to be integrated into clinical image analysis workflows.",True,True,"Wang, Shuo and Tarroni, Giacomo and Qin, Chen and Mo, Yuanhan and Dai, Chengliang and Chen, Chen and Glocker, Ben and Guo, Yike and Rueckert, Daniel and Bai, Wenjia",2020,,,,
Diffusion-Based Quality Control of Medical Image Segmentations across Organs,2511.09588v1,kohlberger2012evaluating,\cite{kohlberger2012evaluating},{Evaluating segmentation error without ground truth},,,True,False,"Kohlberger, Tim and Singh, Vikas and Alvino, Christopher and Bahlmann, Claus and Grady, Leo",2012,,,,
Diffusion-Based Quality Control of Medical Image Segmentations across Organs,2511.09588v1,liu2019alarm,\cite{liu2019alarm},{An alarm system for segmentation algorithm based on shape model},,,True,False,"Liu, Fengze and Xia, Yingda and Yang, Dong and Yuille, Alan L and Xu, Daguang",2019,,,,
Diffusion-Based Quality Control of Medical Image Segmentations across Organs,2511.09588v1,Robinson2019Automated,\cite{Robinson2019Automated},Automated Quality Control in Image Segmentation: Application to the UK Biobank Cardiac MR Imaging Study,https://arxiv.org/abs/1901.09351v1,"Background: The trend towards large-scale studies including population imaging poses new challenges in terms of quality control (QC). This is a particular issue when automatic processing tools, e.g. image segmentation methods, are employed to derive quantitative measures or biomarkers for later analyses. Manual inspection and visual QC of each segmentation isn't feasible at large scale. However, it's important to be able to automatically detect when a segmentation method fails so as to avoid inclusion of wrong measurements into subsequent analyses which could lead to incorrect conclusions. Methods: To overcome this challenge, we explore an approach for predicting segmentation quality based on Reverse Classification Accuracy, which enables us to discriminate between successful and failed segmentations on a per-cases basis. We validate this approach on a new, large-scale manually-annotated set of 4,800 cardiac magnetic resonance scans. We then apply our method to a large cohort of 7,250 cardiac MRI on which we have performed manual QC. Results: We report results used for predicting segmentation quality metrics including Dice Similarity Coefficient (DSC) and surface-distance measures. As initial validation, we present data for 400 scans demonstrating 99% accuracy for classifying low and high quality segmentations using predicted DSC scores. As further validation we show high correlation between real and predicted scores and 95% classification accuracy on 4,800 scans for which manual segmentations were available. We mimic real-world application of the method on 7,250 cardiac MRI where we show good agreement between predicted quality metrics and manual visual QC scores. Conclusions: We show that RCA has the potential for accurate and fully automatic segmentation QC on a per-case basis in the context of large-scale population imaging as in the UK Biobank Imaging Study.",True,True,"Robinson, Ross and Valindria, Vanessa V. and Bai, Wenjia and et al.",2019,,,,Journal of Cardiovascular Magnetic Resonance
Diffusion-Based Quality Control of Medical Image Segmentations across Organs,2511.09588v1,pinaya2022brain,\cite{pinaya2022brain},{Brain imaging generation with latent diffusion models},,,True,False,"Pinaya, Walter HL and Tudosiu, Petru-Daniel and Dafflon, Jessica and Da Costa, Pedro F and Fernandez, Virginia and Nachev, Parashkev and Ourselin, Sebastien and Cardoso, M Jorge",2022,,,,
Diffusion-Based Quality Control of Medical Image Segmentations across Organs,2511.09588v1,tudosiu2024realistic,\cite{tudosiu2024realistic},{Realistic morphology-preserving generative modelling of the brain},,,True,False,"Tudosiu, Petru-Daniel and Pinaya, Walter HL and Ferreira Da Costa, Pedro and Dafflon, Jessica and Patel, Ashay and Borges, Pedro and Fernandez, Virginia and Graham, Mark S and Gray, Robert J and Nachev, Parashkev and others",2024,,,,Nature Machine Intelligence
Diffusion-Based Quality Control of Medical Image Segmentations across Organs,2511.09588v1,bercea2023mask,\cite{bercea2023mask},"Mask, Stitch, and Re-Sample: Enhancing Robustness and Generalizability in Anomaly Detection through Automatic Diffusion Models",,,True,False,"Bercea, Cosmin I and Neumayr, Michael and Rueckert, Daniel and Schnabel, Julia A",,,,,
Diffusion-Based Quality Control of Medical Image Segmentations across Organs,2511.09588v1,fernandez2024generating,\cite{fernandez2024generating},{Generating multi-pathological and multi-modal images and labels for brain MRI},,,True,False,"Fernandez, Virginia and Pinaya, Walter Hugo Lopez and Borges, Pedro and Graham, Mark S and Tudosiu, Petru-Daniel and Vercauteren, Tom and Cardoso, M Jorge",2024,,,,Medical Image Analysis
Diffusion-Based Quality Control of Medical Image Segmentations across Organs,2511.09588v1,gupta2024topodiffusionnet,\cite{gupta2024topodiffusionnet},TopoDiffusionNet: A Topology-aware Diffusion Model,https://arxiv.org/abs/2410.16646v2,"Diffusion models excel at creating visually impressive images but often struggle to generate images with a specified topology. The Betti number, which represents the number of structures in an image, is a fundamental measure in topology. Yet, diffusion models fail to satisfy even this basic constraint. This limitation restricts their utility in applications requiring exact control, like robotics and environmental modeling. To address this, we propose TopoDiffusionNet (TDN), a novel approach that enforces diffusion models to maintain the desired topology. We leverage tools from topological data analysis, particularly persistent homology, to extract the topological structures within an image. We then design a topology-based objective function to guide the denoising process, preserving intended structures while suppressing noisy ones. Our experiments across four datasets demonstrate significant improvements in topological accuracy. TDN is the first to integrate topology with diffusion models, opening new avenues of research in this area. Code available at https://github.com/Saumya-Gupta-26/TopoDiffusionNet",True,True,"Gupta, Saumya and Samaras, Dimitris and Chen, Chao",2024,,,,arXiv preprint arXiv:2410.16646
Diffusion-Based Quality Control of Medical Image Segmentations across Organs,2511.09588v1,rezende2015variational,\cite{rezende2015variational},Variational Inference with Normalizing Flows,,,True,False,"Rezende, Danilo Jimenez and Mohamed, Shakir",2015,,,,International Conference on Machine Learning
Diffusion-Based Quality Control of Medical Image Segmentations across Organs,2511.09588v1,goodfellow2014generative,\cite{goodfellow2014generative},Lipschitz Generative Adversarial Nets,https://arxiv.org/abs/1902.05687v4,"In this paper, we study the convergence of generative adversarial networks (GANs) from the perspective of the informativeness of the gradient of the optimal discriminative function. We show that GANs without restriction on the discriminative function space commonly suffer from the problem that the gradient produced by the discriminator is uninformative to guide the generator. By contrast, Wasserstein GAN (WGAN), where the discriminative function is restricted to 1-Lipschitz, does not suffer from such a gradient uninformativeness problem. We further show in the paper that the model with a compact dual form of Wasserstein distance, where the Lipschitz condition is relaxed, may also theoretically suffer from this issue. This implies the importance of Lipschitz condition and motivates us to study the general formulation of GANs with Lipschitz constraint, which leads to a new family of GANs that we call Lipschitz GANs (LGANs). We show that LGANs guarantee the existence and uniqueness of the optimal discriminative function as well as the existence of a unique Nash equilibrium. We prove that LGANs are generally capable of eliminating the gradient uninformativeness problem. According to our empirical analysis, LGANs are more stable and generate consistently higher quality samples compared with WGAN.",True,True,"Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua",2014,,,,
Diffusion-Based Quality Control of Medical Image Segmentations across Organs,2511.09588v1,ho2020denoising,\cite{ho2020denoising},Denoising Diffusion Probabilistic Models,https://arxiv.org/abs/2006.11239v2,"We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion",True,True,"Ho, Jonathan and Jain, Ajay and Abbeel, Pieter",2020,,,,Advances in neural information processing systems
Diffusion-Based Quality Control of Medical Image Segmentations across Organs,2511.09588v1,song2020denoising,\cite{song2020denoising},Denoising Diffusion Implicit Models,https://arxiv.org/abs/2010.02502v4,"Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that DDIMs can produce high quality samples $10 \times$ to $50 \times$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space.",True,True,"Song, Jiaming and Meng, Chenlin and Ermon, Stefano",2020,,,,arXiv preprint arXiv:2010.02502
Diffusion-Based Quality Control of Medical Image Segmentations across Organs,2511.09588v1,rombach2021highresolution,\cite{rombach2021highresolution},High-Resolution Image Synthesis with Latent Diffusion Models,,,True,False,Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer,2021,,,,arXiv
Diffusion-Based Quality Control of Medical Image Segmentations across Organs,2511.09588v1,butoi2023universeg,\cite{butoi2023universeg},UniverSeg: Universal Medical Image Segmentation,https://arxiv.org/abs/2304.06131v1,"While deep learning models have become the predominant method for medical image segmentation, they are typically not capable of generalizing to unseen segmentation tasks involving new anatomies, image modalities, or labels. Given a new segmentation task, researchers generally have to train or fine-tune models, which is time-consuming and poses a substantial barrier for clinical researchers, who often lack the resources and expertise to train neural networks. We present UniverSeg, a method for solving unseen medical segmentation tasks without additional training. Given a query image and example set of image-label pairs that define a new segmentation task, UniverSeg employs a new Cross-Block mechanism to produce accurate segmentation maps without the need for additional training. To achieve generalization to new tasks, we have gathered and standardized a collection of 53 open-access medical segmentation datasets with over 22,000 scans, which we refer to as MegaMedical. We used this collection to train UniverSeg on a diverse set of anatomies and imaging modalities. We demonstrate that UniverSeg substantially outperforms several related methods on unseen tasks, and thoroughly analyze and draw insights about important aspects of the proposed system. The UniverSeg source code and model weights are freely available at https://universeg.csail.mit.edu",True,True,"Butoi, Victor Ion and Ortiz, Jose Javier Gonzalez and Ma, Tianyu and Sabuncu, Mert R and Guttag, John and Dalca, Adrian V",2023,,,,
Diffusion-Based Quality Control of Medical Image Segmentations across Organs,2511.09588v1,MedSAM,\cite{MedSAM},Segment Anything in Medical Images,https://arxiv.org/abs/2304.12306v3,"Medical image segmentation is a critical component in clinical practice, facilitating accurate diagnosis, treatment planning, and disease monitoring. However, existing methods, often tailored to specific modalities or disease types, lack generalizability across the diverse spectrum of medical image segmentation tasks. Here we present MedSAM, a foundation model designed for bridging this gap by enabling universal medical image segmentation. The model is developed on a large-scale medical image dataset with 1,570,263 image-mask pairs, covering 10 imaging modalities and over 30 cancer types. We conduct a comprehensive evaluation on 86 internal validation tasks and 60 external validation tasks, demonstrating better accuracy and robustness than modality-wise specialist models. By delivering accurate and efficient segmentation across a wide spectrum of tasks, MedSAM holds significant potential to expedite the evolution of diagnostic tools and the personalization of treatment plans.",True,True,"Ma, Jun and He, Yuting and Li, Feifei and Han, Lin and You, Chenyu and Wang, Bo",2024,,,,Nature Communications
Diffusion-Based Quality Control of Medical Image Segmentations across Organs,2511.09588v1,wong2024scribbleprompt,\cite{wong2024scribbleprompt},Scribbleprompt: fast and flexible interactive segmentation for any biomedical image,,,True,False,"Wong, Hallee E and Rakic, Marianne and Guttag, John and Dalca, Adrian V",2024,,,,
Diffusion-Based Quality Control of Medical Image Segmentations across Organs,2511.09588v1,isensee2021nnu,\cite{isensee2021nnu},nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation,https://arxiv.org/abs/1809.10486v1,"The U-Net was presented in 2015. With its straight-forward and successful architecture it quickly evolved to a commonly used benchmark in medical image segmentation. The adaptation of the U-Net to novel problems, however, comprises several degrees of freedom regarding the exact architecture, preprocessing, training and inference. These choices are not independent of each other and substantially impact the overall performance. The present paper introduces the nnU-Net ('no-new-Net'), which refers to a robust and self-adapting framework on the basis of 2D and 3D vanilla U-Nets. We argue the strong case for taking away superfluous bells and whistles of many proposed network designs and instead focus on the remaining aspects that make out the performance and generalizability of a method. We evaluate the nnU-Net in the context of the Medical Segmentation Decathlon challenge, which measures segmentation performance in ten disciplines comprising distinct entities, image modalities, image geometries and dataset sizes, with no manual adjustments between datasets allowed. At the time of manuscript submission, nnU-Net achieves the highest mean dice scores across all classes and seven phase 1 tasks (except class 1 in BrainTumour) in the online leaderboard of the challenge.",True,True,"Isensee, Fabian and Jaeger, Paul Friedrich and Kohl, Simon AA and Petersen, Jens and Maier-Hein, Klaus H",2021,,,,Nature Methods
Diffusion-Based Quality Control of Medical Image Segmentations across Organs,2511.09588v1,isensee2024nnu,\cite{isensee2024nnu},nnu-net revisited: A call for rigorous validation in 3d medical image segmentation,,,True,False,"Isensee, Fabian and Wald, Tassilo and Ulrich, Constantin and Baumgartner, Michael and Roy, Saikat and Maier-Hein, Klaus and Jaeger, Paul F",2024,,,,
siDPT: siRNA Efficacy Prediction via Debiased Preference-Pair Transformer,2509.15664v1,naito2009sidirect,\cite{naito2009sidirect},siDirect 2.0: updated software for designing functional siRNA with reduced seed-dependent off-target effect,,,True,False,"Naito, Yuki and Yoshimura, Jun and Morishita, Shinichi and Ui-Tei, Kumiko",2009,,,,BMC bioinformatics
siDPT: siRNA Efficacy Prediction via Debiased Preference-Pair Transformer,2509.15664v1,khvorova2003functional,\cite{khvorova2003functional},Functional siRNAs and miRNAs exhibit strand bias,,,True,False,"Khvorova, Anastasia and Reynolds, Angela and Jayasena, Sumedha D",2003,,,,Cell
siDPT: siRNA Efficacy Prediction via Debiased Preference-Pair Transformer,2509.15664v1,katoh2007specific,\cite{katoh2007specific},Specific residues at every third position of siRNA shape its efficient RNAi activity,,,True,False,"Katoh, Takayuki and Suzuki, Tsutomu",2007,,,,Nucleic acids research
siDPT: siRNA Efficacy Prediction via Debiased Preference-Pair Transformer,2509.15664v1,huesken2005design,\cite{huesken2005design},Design of a genome-wide siRNA library using an artificial neural network,,,True,False,"Huesken, Dieter and Lange, Joerg and Mickanin, Craig and Weiler, Jan and Asselbergs, Fred and Warner, Justin and Meloon, Brian and Engel, Sharon and Rosenberg, Avi and Cohen, Dalia and others",2005,,,,Nature biotechnology
siDPT: siRNA Efficacy Prediction via Debiased Preference-Pair Transformer,2509.15664v1,wang2010predicting,\cite{wang2010predicting},Predicting siRNA potency with random forests and support vector machines,,,True,False,"Wang, Liangjiang and Huang, Caiyan and Yang, Jack Y",2010,,,,BMC genomics
siDPT: siRNA Efficacy Prediction via Debiased Preference-Pair Transformer,2509.15664v1,monopoli2023asymmetric,\cite{monopoli2023asymmetric},Asymmetric trichotomous partitioning overcomes dataset limitations in building machine learning models for predicting siRNA efficacy,,,True,False,"Monopoli, Kathryn R and Korkin, Dmitry and Khvorova, Anastasia",2023,,,,Molecular Therapy Nucleic Acids
siDPT: siRNA Efficacy Prediction via Debiased Preference-Pair Transformer,2509.15664v1,han2018sirna,\cite{han2018sirna},SiRNA silencing efficacy prediction based on a deep architecture,,,True,False,"Han, Ye and He, Fei and Chen, Yongbing and Liu, Yuanning and Yu, Helong",2018,,,,BMC genomics
siDPT: siRNA Efficacy Prediction via Debiased Preference-Pair Transformer,2509.15664v1,bereczki2025mitigating,\cite{bereczki2025mitigating},"Mitigating off-target effects of small RNAs: conventional approaches, network theory and artificial intelligence",,,True,False,"Bereczki, Zolt{\'a}n and Benczik, Bettina and Balogh, Oliv{\'e}r M and Marton, Szandra and Puhl, Eszter and P{\'e}terv{\'a}ri, M{\'a}ty{\'a}s and V{\'a}czy-F{\""o}ldi, M{\'a}t{\'e} and Papp, Zsolt Tam{\'a}s and Makkos, Andr{\'a}s and Glass, Kimberly and others",2025,,,,British Journal of Pharmacology
siDPT: siRNA Efficacy Prediction via Debiased Preference-Pair Transformer,2509.15664v1,la2022graph,\cite{la2022graph},A graph neural network approach for the analysis of siRNA-target biological networks,,,True,False,"La Rosa, Massimo and Fiannaca, Antonino and La Paglia, Laura and Urso, Alfonso",2022,,,,International Journal of Molecular Sciences
siDPT: siRNA Efficacy Prediction via Debiased Preference-Pair Transformer,2509.15664v1,he2017predicting,\cite{he2017predicting},Predicting siRNA efficacy based on multiple selective siRNA representations and their combination at score level,,,True,False,"He, Fei and Han, Ye and Gong, Jianting and Song, Jiazhi and Wang, Han and Li, Yanwen",2017,,,,Scientific Reports
siDPT: siRNA Efficacy Prediction via Debiased Preference-Pair Transformer,2509.15664v1,liu2024attsioff,\cite{liu2024attsioff},AttSiOff: a self-attention-based approach on siRNA design with inhibition and off-target effect prediction,,,True,False,"Liu, Bin and Yuan, Ye and Pan, Xiaoyong and Shen, Hong-Bin and Jin, Cheng",2024,,,,Med-X
siDPT: siRNA Efficacy Prediction via Debiased Preference-Pair Transformer,2509.15664v1,chen2022interpretable,\cite{chen2022interpretable},Interpretable RNA Foundation Model from Unannotated Data for Highly Accurate RNA Structure and Function Predictions,https://arxiv.org/abs/2204.00300v5,"Non-coding RNA structure and function are essential to understanding various biological processes, such as cell signaling, gene expression, and post-transcriptional regulations. These are all among the core problems in the RNA field. With the rapid growth of sequencing technology, we have accumulated a massive amount of unannotated RNA sequences. On the other hand, expensive experimental observatory results in only limited numbers of annotated data and 3D structures. Hence, it is still challenging to design computational methods for predicting their structures and functions. The lack of annotated data and systematic study causes inferior performance. To resolve the issue, we propose a novel RNA foundation model (RNA-FM) to take advantage of all the 23 million non-coding RNA sequences through self-supervised learning. Within this approach, we discover that the pre-trained RNA-FM could infer sequential and evolutionary information of non-coding RNAs without using any labels. Furthermore, we demonstrate RNA-FM's effectiveness by applying it to the downstream secondary/3D structure prediction, SARS-CoV-2 genome structure and evolution prediction, protein-RNA binding preference modeling, and gene expression regulation modeling. The comprehensive experiments show that the proposed method improves the RNA structural and functional modelling results significantly and consistently. Despite only being trained with unlabelled data, RNA-FM can serve as the foundational model for the field.",True,True,"Chen, Jiayang and Hu, Zhihang and Sun, Siqi and Tan, Qingxiong and Wang, Yixuan and Yu, Qinze and Zong, Licheng and Hong, Liang and Xiao, Jin and Shen, Tao and others",2022,,,,arXiv preprint arXiv:2204.00300
siDPT: siRNA Efficacy Prediction via Debiased Preference-Pair Transformer,2509.15664v1,brixi2025genome,\cite{brixi2025genome},Genome modeling and design across all domains of life with Evo 2,,,True,False,"Brixi, Garyk and Durrant, Matthew G and Ku, Jerome and Poli, Michael and Brockman, Greg and Chang, Daniel and Gonzalez, Gabriel A and King, Samuel H and Li, David B and Merchant, Aditi T and others",2025,,,,BioRxiv
siDPT: siRNA Efficacy Prediction via Debiased Preference-Pair Transformer,2509.15664v1,zhang2025mrna2vec,\cite{zhang2025mrna2vec},mRNA2vec: mRNA Embedding with Language Model in the 5'UTR-CDS for mRNA Design,,,True,False,"Zhang, Honggen and Gao, Xiangrui and Zhang, June and Lai, Lipeng",2025,,,,
siDPT: siRNA Efficacy Prediction via Debiased Preference-Pair Transformer,2509.15664v1,bai2024oligoformer,\cite{bai2024oligoformer},OligoFormer: an accurate and robust prediction method for siRNA design,,,True,False,"Bai, Yilan and Zhong, Haochen and Wang, Taiwei and Lu, Zhi John",2024,,,,Bioinformatics
siDPT: siRNA Efficacy Prediction via Debiased Preference-Pair Transformer,2509.15664v1,long2024sirnadiscovery,\cite{long2024sirnadiscovery},siRNADiscovery: a graph neural network for siRNA efficacy prediction via deep RNA sequence analysis,,,True,False,"Long, Rongzhuo and Guo, Ziyu and Han, Da and Liu, Boxiang and Yuan, Xudong and Chen, Guangyong and Heng, Pheng-Ann and Zhang, Liang",2024,,,,Briefings in Bioinformatics
STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation,2511.02769v1,li2023spotgan,\cite{li2023spotgan},SpotGAN: A reverse-transformer GAN generates scaffold-constrained molecules with property optimization,,,True,False,"Li, Chen and Yamanishi, Yoshihiro",2023,,,,
STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation,2511.02769v1,li2024tengan,\cite{li2024tengan},Tengan: Pure transformer encoders make an efficient discrete gan for de novo molecular generation,,,True,False,"Li, Chen and Yamanishi, Yoshihiro",2024,,,,
STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation,2511.02769v1,kuznetsov2021molgrow,\cite{kuznetsov2021molgrow},MolGrow: A graph normalizing flow for hierarchical molecular generation,,,True,False,"Kuznetsov, Maksim and Polykovskiy, Daniil",2021,,,,
STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation,2511.02769v1,jo2022score,\cite{jo2022score},Score-based generative modeling of graphs via the system of stochastic differential equations,,,True,False,"Jo, Jaehyeong and Lee, Seul and Hwang, Sung Ju",2022,,,,
STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation,2511.02769v1,huang2023conditional,\cite{huang2023conditional},Conditional Diffusion Based on Discrete Graph Structures for Molecular Graph Generation,https://arxiv.org/abs/2301.00427v2,"Learning the underlying distribution of molecular graphs and generating high-fidelity samples is a fundamental research problem in drug discovery and material science. However, accurately modeling distribution and rapidly generating novel molecular graphs remain crucial and challenging goals. To accomplish these goals, we propose a novel Conditional Diffusion model based on discrete Graph Structures (CDGS) for molecular graph generation. Specifically, we construct a forward graph diffusion process on both graph structures and inherent features through stochastic differential equations (SDE) and derive discrete graph structures as the condition for reverse generative processes. We present a specialized hybrid graph noise prediction model that extracts the global context and the local node-edge dependency from intermediate graph states. We further utilize ordinary differential equation (ODE) solvers for efficient graph sampling, based on the semi-linear structure of the probability flow ODE. Experiments on diverse datasets validate the effectiveness of our framework. Particularly, the proposed method still generates high-quality molecular graphs in a limited number of steps. Our code is provided in https://github.com/GRAPH-0/CDGS.",True,True,"Huang, Han and Sun, Leilei and Du, Bowen and Lv, Weifeng",2023,,,,
STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation,2511.02769v1,lee2023exploring,\cite{lee2023exploring},Exploring chemical space with score-based out-of-distribution generation,,,True,False,"Lee, Seul and Jo, Jaehyeong and Hwang, Sung Ju",2023,,,,
STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation,2511.02769v1,gomez2018automatic,\cite{gomez2018automatic},Automatic chemical design using a data-driven continuous representation of molecules,https://arxiv.org/abs/1610.02415v3,"We report a method to convert discrete representations of molecules to and from a multidimensional continuous representation. This model allows us to generate new molecules for efficient exploration and optimization through open-ended spaces of chemical compounds. A deep neural network was trained on hundreds of thousands of existing chemical structures to construct three coupled functions: an encoder, a decoder and a predictor. The encoder converts the discrete representation of a molecule into a real-valued continuous vector, and the decoder converts these continuous vectors back to discrete molecular representations. The predictor estimates chemical properties from the latent continuous vector representation of the molecule. Continuous representations allow us to automatically generate novel chemical structures by performing simple operations in the latent space, such as decoding random vectors, perturbing known chemical structures, or interpolating between molecules. Continuous representations also allow the use of powerful gradient-based optimization to efficiently guide the search for optimized functional compounds. We demonstrate our method in the domain of drug-like molecules and also in the set of molecules with fewer that nine heavy atoms.",True,True,"G{\'o}mez-Bombarelli, Rafael and Wei, Jennifer N and Duvenaud, David and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and S{\'a}nchez-Lengeling, Benjam{\'\i}n and Sheberla, Dennis and Aguilera-Iparraguirre, Jorge and Hirzel, Timothy D and Adams, Ryan P and Aspuru-Guzik, Al{\'a}n",2018,,,,ACS central science
STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation,2511.02769v1,weininger1989smiles,\cite{weininger1989smiles},SMILES. 2. Algorithm for generation of unique SMILES notation,,,True,False,"Weininger, David and Weininger, Arthur and Weininger, Joseph L",1989,,,,Journal of chemical information and computer sciences
STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation,2511.02769v1,kusner2017grammar,\cite{kusner2017grammar},Grammar Variational Autoencoder,https://arxiv.org/abs/1703.01925v1,"Deep generative models have been wildly successful at learning coherent latent representations for continuous data such as video and audio. However, generative modeling of discrete data such as arithmetic expressions and molecular structures still poses significant challenges. Crucially, state-of-the-art methods often produce outputs that are not valid. We make the key observation that frequently, discrete data can be represented as a parse tree from a context-free grammar. We propose a variational autoencoder which encodes and decodes directly to and from these parse trees, ensuring the generated outputs are always valid. Surprisingly, we show that not only does our model more often generate valid outputs, it also learns a more coherent latent space in which nearby points decode to similar discrete outputs. We demonstrate the effectiveness of our learned models by showing their improved performance in Bayesian optimization for symbolic regression and molecular synthesis.",True,True,"Kusner, Matt J and Paige, Brooks and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel",2017,,,,
STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation,2511.02769v1,dai2018syntax,\cite{dai2018syntax},Syntax-directed variational autoencoder for structured data,,,True,False,"Dai, Hanjun and Tian, Yingtao and Dai, Bo and Skiena, Steven and Song, Le",2018,,,,arXiv preprint arXiv:1802.08786
STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation,2511.02769v1,simonovsky2018graphvae,\cite{simonovsky2018graphvae},GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders,https://arxiv.org/abs/1802.03480v1,"Deep learning on graphs has become a popular research topic with many applications. However, past work has concentrated on learning graph embedding tasks, which is in contrast with advances in generative models for images and text. Is it possible to transfer this progress to the domain of graphs? We propose to sidestep hurdles associated with linearization of such discrete structures by having a decoder output a probabilistic fully-connected graph of a predefined maximum size directly at once. Our method is formulated as a variational autoencoder. We evaluate on the challenging task of molecule generation.",True,True,"Simonovsky, Martin and Komodakis, Nikos",2018,,,,
STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation,2511.02769v1,ma2021gf,\cite{ma2021gf},GF-VAE: a flow-based variational autoencoder for molecule generation,,,True,False,"Ma, Changsheng and Zhang, Xiangliang",2021,,,,
STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation,2511.02769v1,jin2018junction,\cite{jin2018junction},Junction tree variational autoencoder for molecular graph generation,,,True,False,"Jin, Wengong and Barzilay, Regina and Jaakkola, Tommi",2018,,,,
STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation,2511.02769v1,lim2018molecular,\cite{lim2018molecular},Molecular generative model based on conditional variational autoencoder for de novo molecular design,https://arxiv.org/abs/1806.05805v1,"We propose a molecular generative model based on the conditional variational autoencoder for de novo molecular design. It is specialized to control multiple molecular properties simultaneously by imposing them on a latent space. As a proof of concept, we demonstrate that it can be used to generate drug-like molecules with five target properties. We were also able to adjust a single property without changing the others and to manipulate it beyond the range of the dataset.",True,True,"Lim, Jaechang and Ryu, Seongok and Kim, Jin Woo and Kim, Woo Youn",2018,,,,Journal of cheminformatics
STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation,2511.02769v1,kang2018conditional,\cite{kang2018conditional},Conditional molecular design with deep generative models,,,True,False,"Kang, Seokho and Cho, Kyunghyun",2018,,,,Journal of chemical information and modeling
STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation,2511.02769v1,bagal2021molgpt,\cite{bagal2021molgpt},MolGPT: molecular generation using a transformer-decoder model,,,True,False,"Bagal, Viraj and Aggarwal, Rishal and Vinod, PK and Priyakumar, U Deva",2021,,,,Journal of chemical information and modeling
STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation,2511.02769v1,fang2023domain,\cite{fang2023domain},Domain-agnostic molecular generation with chemical feedback,,,True,False,"Fang, Yin and Zhang, Ningyu and Chen, Zhuo and Guo, Lingbing and Fan, Xiaohui and Chen, Huajun",2023,,,,arXiv preprint arXiv:2301.11259
STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation,2511.02769v1,irwin2022chemformer,\cite{irwin2022chemformer},Chemformer: a pre-trained transformer for computational chemistry,,,True,False,"Irwin, Ross and Dimitriadis, Spyridon and He, Jiazhen and Bjerrum, Esben Jannik",2022,,,,Machine Learning: Science and Technology
STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation,2511.02769v1,priyadarsini2024self,\cite{priyadarsini2024self},SELF-BART : A Transformer-based Molecular Representation Model using SELFIES,https://arxiv.org/abs/2410.12348v1,"Large-scale molecular representation methods have revolutionized applications in material science, such as drug discovery, chemical modeling, and material design. With the rise of transformers, models now learn representations directly from molecular structures. In this study, we develop an encoder-decoder model based on BART that is capable of leaning molecular representations and generate new molecules. Trained on SELFIES, a robust molecular string representation, our model outperforms existing baselines in downstream tasks, demonstrating its potential in efficient and effective molecular data analysis and manipulation.",True,True,"Priyadarsini, Indra and Takeda, Seiji and Hamada, Lisa and Brazil, Emilio Vital and Soares, Eduardo and Shinohara, Hajime",2024,,,,arXiv preprint arXiv:2410.12348
STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation,2511.02769v1,priyadarsini2025selfiested,\cite{priyadarsini2025selfiested},{SELFIES}-{TED} : A Robust Transformer Model for Molecular Representation using {SELFIES},,,True,False,Indra Priyadarsini and Seiji Takeda and Lisa Hamada and Emilio Vital Brazil and Eduardo Soares and Hajime Shinohara,2025,,https://openreview.net/forum?id=uPj9oBH80V,,
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,huber1994local,\cite{huber1994local},Local elevation: a method for improving the searching properties of molecular dynamics simulation,,,True,False,"Huber, Thomas and Torda, Andrew E and Van Gunsteren, Wilfred F",1994,,,,Journal of computer-aided molecular design
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,barducci2008well,\cite{barducci2008well},Well-Tempered Metadynamics: A Smoothly Converging and Tunable Free-Energy Method,https://arxiv.org/abs/0803.3861v1,We present a method for determining the free energy dependence on a selected number of collective variables using an adaptive bias. The formalism provides a unified description which has metadynamics and canonical sampling as limiting cases. Convergence and errors can be rigorously and easily controlled. The parameters of the simulation can be tuned so as to focus the computational effort only on the physically relevant regions of the order parameter space. The algorithm is tested on the reconstruction of alanine dipeptide free energy landscape.,True,True,"Barducci, Alessandro and Bussi, Giovanni and Parrinello, Michele",2008,,,,Physical review letters
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,torrie1977nonphysical,\cite{torrie1977nonphysical},Nonphysical sampling distributions in Monte Carlo free-energy estimation: Umbrella sampling,,,True,False,"Torrie, Glenn M and Valleau, John P",1977,,,,Journal of computational physics
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,kastner2011umbrella,\cite{kastner2011umbrella},Successive Umbrella Sampling,https://arxiv.org/abs/cond-mat/0306678v1,We propose an extension of umbrella sampling in which the pertinent range of states is subdivided in windows that are sampled consecutively and linked together. Extrapolating results from one window we estimate a weight function for the neighboring window. We present a detailed analysis and demonstrate that the error is controlled and independent from the window sizes. The analysis also allows us to detect sampling difficulties. The efficiency of the algorithm is comparable to a multicanonical simulation with an ideal weight function. We exemplify the computational scheme by simulating the liquid-vapor coexistence in a Lennard--Jones system.,True,True,"K{\""a}stner, Johannes",2011,,,,Wiley Interdisciplinary Reviews: Computational Molecular Science
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,grubmuller1995predicting,\cite{grubmuller1995predicting},Predicting slow structural transitions in macromolecular systems: Conformational flooding,,,True,False,"Grubm{\""u}ller, Helmut",1995,,,,Physical Review E
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,swendsen1986replica,\cite{swendsen1986replica},Replica Monte Carlo simulation of spin-glasses,,,True,False,"Swendsen, Robert H and Wang, Jian-Sheng",1986,,,,Physical review letters
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,sugita1999replica,\cite{sugita1999replica},Replica-exchange molecular dynamics method for protein folding,,,True,False,"Sugita, Yuji and Okamoto, Yuko",1999,,,,Chemical physics letters
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,wang2011replica,\cite{wang2011replica},Replica exchange with solute scaling: a more efficient version of replica exchange with solute tempering (REST2),,,True,False,"Wang, Lingle and Friesner, Richard A and Berne, BJ",2011,,,,The Journal of Physical Chemistry B
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,li2020scalable,\cite{li2020scalable},Scalable gradients for stochastic differential equations,,,True,False,"Li, Xuechen and Wong, Ting-Kam Leonard and Chen, Ricky TQ and Duvenaud, David",2020,,,,
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,ghosh2022differentiable,\cite{ghosh2022differentiable},Differentiable Bayesian inference of SDE parameters using a pathwise series expansion of Brownian motion,,,True,False,"Ghosh, Sanmitra and Birrell, Paul J and De Angelis, Daniela",2022,,,,
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,elerian2001likelihood,\cite{elerian2001likelihood},Likelihood inference for discretely observed nonlinear diffusions,,,True,False,"Elerian, Ola and Chib, Siddhartha and Shephard, Neil",2001,,,,Econometrica
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,roberts2001inference,\cite{roberts2001inference},On inference for partially observed nonlinear diffusion models using the Metropolis--Hastings algorithm,,,True,False,"Roberts, Gareth O and Stramer, Osnat",2001,,,,Biometrika
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,lyons2014series,\cite{lyons2014series},Series Expansion Approximations of Brownian Motion for Non-Linear Kalman Filtering of Diffusion Processes,https://arxiv.org/abs/1302.5324v3,"In this paper, we describe a novel application of sigma-point methods to continuous-discrete filtering. In principle, the nonlinear continuous- discrete filtering problem can be solved exactly. In practice, the solution contains terms that are computationally intractible. Assumed density filtering methods attempt to match statistics of the filtering distribution to some set of more tractible probability distributions. We describe a novel method that decomposes the Brownian motion driving the signal in a generalised Fourier series, which is truncated after a number of terms. This approximation to Brownian can be described using a relatively small number of Fourier coefficients, and allows us to compute statistics of the filtering distribution with a single application of a sigma-point method. Assumed density filters that exist in the literature usually rely on discretisation of the signal dynamics followed by iterated application of a sigma point transform (or a limiting case thereof). Iterating the transform in this manner can lead to loss of information about the filtering distri- bution in highly nonlinear settings. We demonstrate that our method is better equipped to cope with such problems.",True,True,"Lyons, Simon MJ and S{\""a}rkk{\""a}, Simo and Storkey, Amos J",2014,,,,IEEE Transactions on Signal Processing
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,fuchs2013inference,\cite{fuchs2013inference},Inference for diffusion processes: with applications in life sciences,,,True,False,"Fuchs, Christiane",2013,,,,
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,bunin2017ecological,\cite{bunin2017ecological},Ecological communities with Lotka-Volterra dynamics,,,True,False,"Bunin, Guy",2017,,,,Physical Review E
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,jones1998bayesian,\cite{jones1998bayesian},Bayesian estimation of continuous-time finance models,,,True,False,"Jones, Christopher S",1998,,,,manuscript University of Rochester
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,eraker2001mcmc,\cite{eraker2001mcmc},MCMC analysis of diffusion models with application to finance,,,True,False,"Eraker, Bj{\o}rn",2001,,,,Journal of Business \& Economic Statistics
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,kamenik2022enhanced,\cite{kamenik2022enhanced},Enhanced sampling without borders: on global biasing functions and how to reweight them,,,True,False,"Kamenik, Anna S and Linker, Stephanie M and Riniker, Sereina",2022,,,,Physical Chemistry Chemical Physics
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,gallicchio2005temperature,\cite{gallicchio2005temperature},"Temperature weighted histogram analysis method, replica exchange, and transition paths",,,True,False,"Gallicchio, Emilio and Andrec, Michael and Felts, Anthony K and Levy, Ronald M",2005,,,,The Journal of Physical Chemistry B
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,souaille2001extension,\cite{souaille2001extension},Extension to the weighted histogram analysis method: combining umbrella sampling with free energy calculations,,,True,False,"Souaille, Marc and Roux, Beno{\i}t",2001,,,,Computer physics communications
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,shirts2008statistically,\cite{shirts2008statistically},Statistically optimal analysis of samples from multiple equilibrium states,https://arxiv.org/abs/0801.1426v3,"We present a new estimator for computing free energy differences and thermodynamic expectations as well as their uncertainties from samples obtained from multiple equilibrium states via either simulation or experiment. The estimator, which we term the multistate Bennett acceptance ratio (MBAR) estimator because it reduces to the Bennett acceptance ratio when only two states are considered, has significant advantages over multiple histogram reweighting methods for combining data from multiple states. It does not require the sampled energy range to be discretized to produce histograms, eliminating bias due to energy binning and significantly reducing the time complexity of computing a solution to the estimating equations in many cases. Additionally, an estimate of the statistical uncertainty is provided for all estimated quantities. In the large sample limit, MBAR is unbiased and has the lowest variance of any known estimator for making use of equilibrium data collected from multiple states. We illustrate this method by producing a highly precise estimate of the potential of mean force for a DNA hairpin system, combining data from multiple optical tweezer measurements under constant force bias.",True,True,"Shirts, Michael R and Chodera, John D",2008,,,,The Journal of chemical physics
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,dibak2022temperature,\cite{dibak2022temperature},Temperature steerable flows and Boltzmann generators,,,True,False,"Dibak, Manuel and Klein, Leon and Kr{\""a}mer, Andreas and No{\'e}, Frank",2022,,,,Physical Review Research
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,wang2022data,\cite{wang2022data},From data to noise to data for mixing physics across temperatures with generative artificial intelligence,,,True,False,"Wang, Yihang and Herron, Lukas and Tiwary, Pratyush",2022,,,,Proceedings of the National Academy of Sciences
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,moqvist2025thermodynamic,\cite{moqvist2025thermodynamic},Thermodynamic Interpolation: A generative approach to molecular thermodynamics and kinetics,https://arxiv.org/abs/2411.10075v1,"Using normalizing flows and reweighting, Boltzmann Generators enable equilibrium sampling from a Boltzmann distribution, defined by an energy function and thermodynamic state. In this work, we introduce Thermodynamic Interpolation (TI), which allows for generating sampling statistics in a temperature-controllable way. We introduce TI flavors that work directly in the ambient configurational space, mapping between different thermodynamic states or through a latent, normally distributed reference state. Our ambient-space approach allows for the specification of arbitrary target temperatures, ensuring generalizability within the temperature range of the training set and demonstrating the potential for extrapolation beyond it. We validate the effectiveness of TI on model systems that exhibit metastability and non-trivial temperature dependencies. Finally, we demonstrate how to combine TI-based sampling to estimate free energy differences through various free energy perturbation methods and provide corresponding approximated kinetic rates estimated through generator extended dynamic mode decomposition (gEDMD).",True,True,"Moqvist, Selma and Chen, Weilong and Schreiner, Mathias and N{\""u}ske, Feliks and Olsson, Simon",2025,,,,Journal of Chemical Theory and Computation
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,Invernizzi2022,\cite{Invernizzi2022},Skipping the Replica Exchange Ladder with Normalizing Flows,,,True,False,"Invernizzi,  Michele and Kr\""{a}mer,  Andreas and Clementi,  Cecilia and Noé,  Frank",2022,,http://dx.doi.org/10.1021/acs.jpclett.2c03327,10.1021/acs.jpclett.2c03327,The Journal of Physical Chemistry Letters
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,wu2017variational,\cite{wu2017variational},Variational Koopman models: slow collective variables and molecular kinetics from short off-equilibrium simulations,https://arxiv.org/abs/1610.06773v2,"Markov state models (MSMs) and Master equation models are popular approaches to approximate molecular kinetics, equilibria, metastable states, and reaction coordinates in terms of a state space discretization usually obtained by clustering. Recently, a powerful generalization of MSMs has been introduced, the variational approach (VA) of molecular kinetics and its special case the time-lagged independent component analysis (TICA), which allow us to approximate slow collective variables and molecular kinetics by linear combinations of smooth basis functions or order parameters. While it is known how to estimate MSMs from trajectories whose starting points are not sampled from an equilibrium ensemble, this has not yet been the case for TICA and the VA. Previous estimates from short trajectories, have been strongly biased and thus not variationally optimal. Here, we employ Koopman operator theory and ideas from dynamic mode decomposition (DMD) to extend the VA and TICA to non-equilibrium data. The main insight is that the VA and TICA provide a coefficient matrix that we call Koopman model, as it approximates the underlying dynamical (Koopman) operator in conjunction with the basis set used. This Koopman model can be used to compute a stationary vector to reweight the data to equilibrium. From such a Koopman-reweighted sample, equilibrium expectation values and variationally optimal reversible Koopman models can be constructed even with short simulations. The Koopman model can be used to propagate densities, and its eigenvalue decomposition provide estimates of relaxation timescales and slow collective variables for dimension reduction. Koopman models are generalizations of Markov state models, TICA and the linear VA and allow molecular kinetics to be described without a cluster discretization.",True,True,"Wu, Hao and N{\""u}ske, Feliks and Paul, Fabian and Klus, Stefan and Koltai, P{\'e}ter and No{\'e}, Frank",2017,,,,The Journal of chemical physics
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,schreiner2023implicit,\cite{schreiner2023implicit},Implicit Transfer Operator Learning: Multiple Time-Resolution Surrogates for Molecular Dynamics,https://arxiv.org/abs/2305.18046v2,"Computing properties of molecular systems rely on estimating expectations of the (unnormalized) Boltzmann distribution. Molecular dynamics (MD) is a broadly adopted technique to approximate such quantities. However, stable simulations rely on very small integration time-steps ($10^{-15}\,\mathrm{s}$), whereas convergence of some moments, e.g. binding free energy or rates, might rely on sampling processes on time-scales as long as $10^{-1}\, \mathrm{s}$, and these simulations must be repeated for every molecular system independently. Here, we present Implict Transfer Operator (ITO) Learning, a framework to learn surrogates of the simulation process with multiple time-resolutions. We implement ITO with denoising diffusion probabilistic models with a new SE(3) equivariant architecture and show the resulting models can generate self-consistent stochastic dynamics across multiple time-scales, even when the system is only partially observed. Finally, we present a coarse-grained CG-SE3-ITO model which can quantitatively model all-atom molecular dynamics using only coarse molecular representations. As such, ITO provides an important step towards multiple time- and space-resolution acceleration of MD. Code is available at \href{https://github.com/olsson-group/ito}{https://github.com/olsson-group/ito}.",True,True,"Schreiner, Mathias and Winther, Ole and Olsson, Simon",2023,,,,Advances in Neural Information Processing Systems
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,klein2023timewarp,\cite{klein2023timewarp},Timewarp: Transferable Acceleration of Molecular Dynamics by Learning Time-Coarsened Dynamics,https://arxiv.org/abs/2302.01170v2,"Molecular dynamics (MD) simulation is a widely used technique to simulate molecular systems, most commonly at the all-atom resolution where equations of motion are integrated with timesteps on the order of femtoseconds ($1\textrm{fs}=10^{-15}\textrm{s}$). MD is often used to compute equilibrium properties, which requires sampling from an equilibrium distribution such as the Boltzmann distribution. However, many important processes, such as binding and folding, occur over timescales of milliseconds or beyond, and cannot be efficiently sampled with conventional MD. Furthermore, new MD simulations need to be performed for each molecular system studied. We present Timewarp, an enhanced sampling method which uses a normalising flow as a proposal distribution in a Markov chain Monte Carlo method targeting the Boltzmann distribution. The flow is trained offline on MD trajectories and learns to make large steps in time, simulating the molecular dynamics of $10^{5} - 10^{6}\:\textrm{fs}$. Crucially, Timewarp is transferable between molecular systems: once trained, we show that it generalises to unseen small peptides (2-4 amino acids) at all-atom resolution, exploring their metastable states and providing wall-clock acceleration of sampling compared to standard MD. Our method constitutes an important step towards general, transferable algorithms for accelerating MD.",True,True,"Klein, Leon and Foong, Andrew and Fjelde, Tor and Mlodozeniec, Bruno and Brockschmidt, Marc and Nowozin, Sebastian and No{\'e}, Frank and Tomioka, Ryota",2023,,,,Advances in Neural Information Processing Systems
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,diez2024boltzmann,\cite{diez2024boltzmann},Boltzmann priors for implicit transfer operators,,,True,False,"Diez, Juan Viguera and Schreiner, Mathias and Engkvist, Ola and Olsson, Simon",2024,,,,arXiv preprint arXiv:2410.10605
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,diez2025boltzmann,\cite{diez2025boltzmann},Boltzmann priors for Implicit Transfer Operators,,,True,False,Juan Viguera Diez and Mathias Jacob Schreiner and Ola Engkvist and Simon Olsson,2025,,https://openreview.net/forum?id=pRCOZllZdT,,
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,prinz2011markov,\cite{prinz2011markov},Markov models of molecular kinetics: Generation and validation,,,True,False,"Prinz, Jan-Hendrik and Wu, Hao and Sarich, Marco and Keller, Bettina and Senne, Martin and Held, Martin and Chodera, John D and Sch{\""u}tte, Christof and No{\'e}, Frank",2011,,,,The Journal of chemical physics
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,husic2018markov,\cite{husic2018markov},Markov state models: From an art to a science,,,True,False,"Husic, Brooke E and Pande, Vijay S",2018,,,,Journal of the American Chemical Society
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,mey2014xtram,\cite{mey2014xtram},xTRAM: Estimating equilibrium expectations from time-correlated simulation data at multiple thermodynamic states,https://arxiv.org/abs/1407.0138v2,"Computing the equilibrium properties of complex systems, such as free energy differences, is often hampered by rare events in the dynamics. Enhanced sampling methods may be used in order to speed up sampling by, for example, using high temperatures, as in parallel tempering, or simulating with a biasing potential such as in the case of umbrella sampling. The equilibrium properties of the thermodynamic state of interest (e.g., lowest temperature or unbiased potential) can be computed using reweighting estimators such as the weighted histogram analysis method or the multistate Bennett acceptance ratio (MBAR). weighted histogram analysis method and MBAR produce unbiased estimates, the simulation samples from the global equilibria at their respective thermodynamic state--a requirement that can be prohibitively expensive for some simulations such as a large parallel tempering ensemble of an explicitly solvated biomolecule. Here, we introduce the transition-based reweighting analysis method (TRAM)--a class of estimators that exploit ideas from Markov modeling and only require the simulation data to be in local equilibrium within subsets of the configuration space. We formulate the expanded TRAM (xTRAM) estimator that is shown to be asymptotically unbiased and a generalization of MBAR. Using four exemplary systems of varying complexity, we demonstrate the improved convergence (ranging from a twofold improvement to several orders of magnitude) of xTRAM in comparison to a direct counting estimator and MBAR, with respect to the invested simulation effort. Lastly, we introduce a random-swapping simulation protocol that can be used with xTRAM, gaining orders-of-magnitude advantages over simulation protocols that require the constraint of sampling from a global equilibrium.",True,True,"Mey, Antonia SJS and Wu, Hao and No{\'e}, Frank",2014,,,,Physical Review X
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,donati2017girsanov,\cite{donati2017girsanov},Girsanov reweighting for path ensembles and Markov state models,,,True,False,"Donati, Lorenzo and Hartmann, Carsten and Keller, Bettina G",2017,,,,The Journal of chemical physics
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,donati2022review,\cite{donati2022review},A review of Girsanov reweighting and of square root approximation for building molecular Markov state models,,,True,False,"Donati, Luca and Weber, Marcus and Keller, Bettina G",2022,,,,Journal of Mathematical Physics
Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation,2509.25872v1,schafer2024implementation,\cite{schafer2024implementation},Implementation of Girsanov Reweighting in OpenMM and Deeptime,,,True,False,"Schafer, Joana-Lysiane and Keller, Bettina G",2024,,,,The Journal of Physical Chemistry B
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,steinhaus2021mutationtaster2021,\cite{steinhaus2021mutationtaster2021},MutationTaster2021,,,True,False,"Steinhaus, Robin and Proft, Sebastian and Schuelke, Markus and Cooper, David N and Schwarz, Jana Marie and Seelow, Dominik",2021,,,,Nucleic Acids Research
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,rentzsch2019cadd,\cite{rentzsch2019cadd},CADD: predicting the deleteriousness of variants throughout the human genome,,,True,False,"Rentzsch, Philipp and Witten, Daniela and Cooper, Gregory M and Shendure, Jay and Kircher, Martin",2019,,,,Nucleic acids research
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,jagadeesh2016m,\cite{jagadeesh2016m},M-CAP eliminates a majority of variants of uncertain significance in clinical exomes at high sensitivity,,,True,False,"Jagadeesh, Karthik A and Wenger, Aaron M and Berger, Mark J and Guturu, Harendra and Stenson, Peter D and Cooper, David N and Bernstein, Jonathan A and Bejerano, Gill",2016,,,,Nature genetics
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,yang2015phenolyzer,\cite{yang2015phenolyzer},Phenolyzer: phenotype-based prioritization of candidate genes for human diseases,,,True,False,"Yang, Hui and Robinson, Peter N and Wang, Kai",2015,,,,Nature methods
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,jagadeesh2019phrank,\cite{jagadeesh2019phrank},Phrank measures phenotype sets similarity to greatly improve Mendelian diagnostic disease prioritization,,,True,False,"Jagadeesh, Karthik A and Birgmeier, Johannes and Guturu, Harendra and Deisseroth, Cole A and Wenger, Aaron M and Bernstein, Jonathan A and Bejerano, Gill",2019,,,,Genetics in Medicine
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,peng2021cada,\cite{peng2021cada},CADA: phenotype-driven gene prioritization based on a case-enriched knowledge graph,,,True,False,"Peng, Chengyao and Dieck, Simon and Schmid, Alexander and Ahmad, Ashar and Knaus, Alexej and Wenzel, Maren and Mehnert, Laura and Zirn, Birgit and Haack, Tobias and Ossowski, Stephan and others",2021,,,,NAR Genomics and Bioinformatics
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,cui2020conan,\cite{cui2020conan},CONAN: complementary pattern augmentation for rare disease detection,,,True,False,"Cui, Limeng and Biswal, Siddharth and Glass, Lucas M and Lever, Greg and Sun, Jimeng and Xiao, Cao",2020,,,,
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,10.1093/nar/gkaa1043,\cite{10.1093/nar/gkaa1043},The Human Phenotype Ontology in 2021,,,True,False,"Köhler, Sebastian and Gargano, Michael and Matentzoglu, Nicolas and Carmody, Leigh C and Lewis-Smith, David and Vasilevsky, Nicole A and Danis, Daniel and Balagura, Ganna and Baynam, Gareth and Brower, Amy M and Callahan, Tiffany J and Chute, Christopher G and Est, Johanna L and Galer, Peter D and Ganesan, Shiva and Griese, Matthias and Haimel, Matthias and Pazmandi, Julia and Hanauer, Marc and Harris, Nomi L and Hartnett, Michael J and Hastreiter, Maximilian and Hauck, Fabian and He, Yongqun and Jeske, Tim and Kearney, Hugh and Kindle, Gerhard and Klein, Christoph and Knoflach, Katrin and Krause, Roland and Lagorce, David and McMurry, Julie A and Miller, Jillian A and Munoz-Torres, Monica C and Peters, Rebecca L and Rapp, Christina K and Rath, Ana M and Rind, Shahmir A and Rosenberg, Avi Z and Segal, Michael M and Seidel, Markus G and Smedley, Damian and Talmy, Tomer and Thomas, Yarlalu and Wiafe, Samuel A and Xian, Julie and Yüksel, Zafer and Helbig, Ingo and Mungall, Christopher J and Haendel, Melissa A and Robinson, Peter N",2020,12,https://doi.org/10.1093/nar/gkaa1043,10.1093/nar/gkaa1043,Nucleic Acids Research
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,10.1093/nar/gky1151,\cite{10.1093/nar/gky1151},OMIM.org: leveraging knowledge across phenotype–gene relationships,,,True,False,"Amberger, Joanna S and Bocchini, Carol A and Scott, Alan F and Hamosh, Ada",2018,11,https://doi.org/10.1093/nar/gky1151,10.1093/nar/gky1151,Nucleic Acids Research
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,weinreich2008orphanet,\cite{weinreich2008orphanet},Orphanet: een Europese database over zeldzame ziekten,,,True,False,"Weinreich, S. S. and Mangon, R. and Sikkens, J. J. and Teeuw, M. E. and Cornel, M. C.",2008,,,,Nederlands Tijdschrift voor Geneeskunde
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,birgmeier2020amelie,\cite{birgmeier2020amelie},AMELIE speeds Mendelian diagnosis by matching patient phenotype and genotype to primary literature,,,True,False,"Birgmeier, Johannes and Haeussler, Maximilian and Deisseroth, Cole A and Steinberg, Ethan H and Jagadeesh, Karthik A and Ratner, Alexander J and Guturu, Harendra and Wenger, Aaron M and Diekhans, Mark E and Stenson, Peter D and others",2020,,,,Science Translational Medicine
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,shepherd,\cite{shepherd},Few shot learning for phenotype-driven diagnosis of patients with rare genetic diseases,,,True,False,"Alsentzer, Emily and Li, Michelle M and Kobren, Shilpa N and Noori, Ayush and Undiagnosed Diseases Network and Kohane, Isaac S and Zitnik, Marinka",2025,,,,npj Digital Medicine
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,smedley2015next,\cite{smedley2015next},Next-generation diagnostics and disease-gene discovery with the Exomiser,,,True,False,"Smedley, Damian and Jacobsen, Julius OB and J{\""a}ger, Marten and K{\""o}hler, Sebastian and Holtgrewe, Manuel and Schubach, Max and Siragusa, Enrico and Zemojtel, Tomasz and Buske, Orion J and Washington, Nicole L and others",2015,,,,Nature protocols
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,li2019xrare,\cite{li2019xrare},Xrare: a machine learning method jointly modeling phenotypes and genetic evidence for rare disease diagnosis,,,True,False,"Li, Qigang and Zhao, Keyan and Bustamante, Carlos D and Ma, Xin and Wong, Wing H",2019,,,,Genetics in Medicine
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,mao2024ai,\cite{mao2024ai},AI-MARRVEL—A Knowledge-Driven AI System for Diagnosing Mendelian Disorders,,,True,False,"Mao, Dongxue and Liu, Chaozhong and Wang, Linhua and AI-Ouran, Rami and Deisseroth, Cole and Pasupuleti, Sasidhar and Kim, Seon Young and Li, Lucian and Rosenfeld, Jill A and Meng, Linyan and others",2024,,,,NEJM AI
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,10.1093/nar/gkz1021,\cite{10.1093/nar/gkz1021},The DisGeNET knowledge platform for disease genomics: 2019 update,,,True,False,"Pinero, Janet and Ramírez-Anguita, Juan Manuel and Sauch-Pitarch, Josep and Ronzano, Francesco and Centeno, Emilio and Sanz, Ferran and Furlong, Laura I",2019,11,https://doi.org/10.1093/nar/gkz1021,10.1093/nar/gkz1021,Nucleic Acids Research
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,drkg2020,\cite{drkg2020},DRKG - Drug Repurposing Knowledge Graph for Covid-19,,,True,False,"Ioannidis, Vassilis N. and Song, Xiang and Manchanda, Saurav and Li, Mufei and Pan, Xiaoqin
            and Zheng, Da and Ning, Xia and Zeng, Xiangxiang and Karypis, George",2020,,,,
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,10.1093/nar/gkac957,\cite{10.1093/nar/gkac957},GenomicKB: a knowledge graph for the human genome,,,True,False,"Feng, Fan and Tang, Feitong and Gao, Yijia and Zhu, Dongyu and Li, Tianjun and Yang, Shuyuan and Yao, Yuan and Huang, Yuanhao and Liu, Jie",2022,11,https://doi.org/10.1093/nar/gkac957,10.1093/nar/gkac957,Nucleic Acids Research
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,Chandak2022.05.01.489928,\cite{Chandak2022.05.01.489928},Building a knowledge graph to enable precision medicine,,,True,False,"Chandak, Payal and Huang, Kexin and Zitnik, Marinka",2022,,https://www.biorxiv.org/content/early/2022/05/01/2022.05.01.489928,10.1101/2022.05.01.489928,bioRxiv
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,vilela2023biomedical,\cite{vilela2023biomedical},Biomedical knowledge graph embeddings for personalized medicine: Predicting disease-gene associations,,,True,False,"Vilela, Joana and Asif, Muhammad and Marques, Ana Rita and Santos, Joao Xavier and Rasga, C{\'e}lia and Vicente, Astrid and Martiniano, Hugo",2023,,,,Expert Systems
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,ektefaie2023multimodal,\cite{ektefaie2023multimodal},Multimodal learning with graphs,,,True,False,"Ektefaie, Yasha and Dasoulas, George and Noori, Ayush and Farhat, Maha and Zitnik, Marinka",2023,,,,Nature Machine Intelligence
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,galkin2023towards,\cite{galkin2023towards},Towards Foundation Models for Knowledge Graph Reasoning,,,True,False,Mikhail Galkin and Xinyu Yuan and Hesham Mostafa and Jian Tang and Zhaocheng Zhu,2023,,https://openreview.net/forum?id=LzMWMJlxHg,,
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,alsentzer2020subgraph,\cite{alsentzer2020subgraph},Subgraph Neural Networks,https://arxiv.org/abs/2006.10538v3,"Deep learning methods for graphs achieve remarkable performance on many node-level and graph-level prediction tasks. However, despite the proliferation of the methods and their success, prevailing Graph Neural Networks (GNNs) neglect subgraphs, rendering subgraph prediction tasks challenging to tackle in many impactful applications. Further, subgraph prediction tasks present several unique challenges: subgraphs can have non-trivial internal topology, but also carry a notion of position and external connectivity information relative to the underlying graph in which they exist. Here, we introduce SubGNN, a subgraph neural network to learn disentangled subgraph representations. We propose a novel subgraph routing mechanism that propagates neural messages between the subgraph's components and randomly sampled anchor patches from the underlying graph, yielding highly accurate subgraph representations. SubGNN specifies three channels, each designed to capture a distinct aspect of subgraph topology, and we provide empirical evidence that the channels encode their intended properties. We design a series of new synthetic and real-world subgraph datasets. Empirical results for subgraph classification on eight datasets show that SubGNN achieves considerable performance gains, outperforming strong baseline methods, including node-level and graph-level GNNs, by 19.8% over the strongest baseline. SubGNN performs exceptionally well on challenging biomedical datasets where subgraphs have complex topology and even comprise multiple disconnected components.",True,True,"Alsentzer, Emily and Finlayson, Samuel and Li, Michelle and Zitnik, Marinka",2020,,,,Advances in Neural Information Processing Systems
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,sun2019pullnet,\cite{sun2019pullnet},PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text,https://arxiv.org/abs/1904.09537v1,"We consider open-domain queston answering (QA) where answers are drawn from either a corpus, a knowledge base (KB), or a combination of both of these. We focus on a setting in which a corpus is supplemented with a large but incomplete KB, and on questions that require non-trivial (e.g., ``multi-hop'') reasoning. We describe PullNet, an integrated framework for (1) learning what to retrieve (from the KB and/or corpus) and (2) reasoning with this heterogeneous information to find the best answer. PullNet uses an {iterative} process to construct a question-specific subgraph that contains information relevant to the question. In each iteration, a graph convolutional network (graph CNN) is used to identify subgraph nodes that should be expanded using retrieval (or ``pull'') operations on the corpus and/or KB. After the subgraph is complete, a similar graph CNN is used to extract the answer from the subgraph. This retrieve-and-reason process allows us to answer multi-hop questions using large KBs and corpora. PullNet is weakly supervised, requiring question-answer pairs but not gold inference paths. Experimentally PullNet improves over the prior state-of-the art, and in the setting where a corpus is used with incomplete KB these improvements are often dramatic. PullNet is also often superior to prior systems in a KB-only setting or a text-only setting.",True,True,"Sun, Haitian and Bedrax-Weiss, Tania and Cohen, William",2019,,,,
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,shen2022improving,\cite{shen2022improving},Improving Subgraph Representation Learning via Multi-View Augmentation,https://arxiv.org/abs/2205.13038v3,"Subgraph representation learning based on Graph Neural Network (GNN) has exhibited broad applications in scientific advancements, such as predictions of molecular structure-property relationships and collective cellular function. In particular, graph augmentation techniques have shown promising results in improving graph-based and node-based classification tasks. Still, they have rarely been explored in the existing GNN-based subgraph representation learning studies. In this study, we develop a novel multi-view augmentation mechanism to improve subgraph representation learning models and thus the accuracy of downstream prediction tasks. Our augmentation technique creates multiple variants of subgraphs and embeds these variants into the original graph to achieve highly improved training efficiency, scalability, and accuracy. Benchmark experiments on several real-world biological and physiological datasets demonstrate the superiority of our proposed multi-view augmentation techniques in subgraph representation learning.",True,True,"Shen, Yili and Liu, Xiao and Ju, Cheng-Wei and Yan, Jiaxu and Yi, Jun and Lin, Zhou and Guan, Hui",2022,,,,arXiv preprint arXiv:2205.13038
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,alsentzer2023simulation,\cite{alsentzer2023simulation},Simulation of undiagnosed patients with novel genetic conditions,,,True,False,"Alsentzer, Emily and Finlayson, Samuel G and Li, Michelle M and Undiagnosed Diseases Network and Kobren, Shilpa N and Kohane, Isaac S",2023,,,,Nature Communications
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,mavromatis2024gnnraggraphneuralretrieval,\cite{mavromatis2024gnnraggraphneuralretrieval},GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning,https://arxiv.org/abs/2405.20139v1,"Knowledge Graphs (KGs) represent human-crafted factual knowledge in the form of triplets (head, relation, tail), which collectively form a graph. Question Answering over KGs (KGQA) is the task of answering natural questions grounding the reasoning to the information provided by the KG. Large Language Models (LLMs) are the state-of-the-art models for QA tasks due to their remarkable ability to understand natural language. On the other hand, Graph Neural Networks (GNNs) have been widely used for KGQA as they can handle the complex graph information stored in the KG. In this work, we introduce GNN-RAG, a novel method for combining language understanding abilities of LLMs with the reasoning abilities of GNNs in a retrieval-augmented generation (RAG) style. First, a GNN reasons over a dense KG subgraph to retrieve answer candidates for a given question. Second, the shortest paths in the KG that connect question entities and answer candidates are extracted to represent KG reasoning paths. The extracted paths are verbalized and given as input for LLM reasoning with RAG. In our GNN-RAG framework, the GNN acts as a dense subgraph reasoner to extract useful graph information, while the LLM leverages its natural language processing ability for ultimate KGQA. Furthermore, we develop a retrieval augmentation (RA) technique to further boost KGQA performance with GNN-RAG. Experimental results show that GNN-RAG achieves state-of-the-art performance in two widely used KGQA benchmarks (WebQSP and CWQ), outperforming or matching GPT-4 performance with a 7B tuned LLM. In addition, GNN-RAG excels on multi-hop and multi-entity questions outperforming competing approaches by 8.9--15.5% points at answer F1.",True,True,Costas Mavromatis and George Karypis,2024,,https://arxiv.org/abs/2405.20139,,
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,NEURIPS2024_efaf1c97,\cite{NEURIPS2024_efaf1c97},G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering,,,True,False,"He, Xiaoxin and Tian, Yijun and Sun, Yifei and Chawla, Nitesh V. and Laurent, Thomas and LeCun, Yann and Bresson, Xavier and Hooi, Bryan",2024,,https://proceedings.neurips.cc/paper_files/paper/2024/file/efaf1c9726648c8ba363a5c927440529-Paper-Conference.pdf,,
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,wen2024mindmapknowledgegraphprompting,\cite{wen2024mindmapknowledgegraphprompting},MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models,https://arxiv.org/abs/2308.09729v5,"Large language models (LLMs) have achieved remarkable performance in natural language understanding and generation tasks. However, they often suffer from limitations such as difficulty in incorporating new knowledge, generating hallucinations, and explaining their reasoning process. To address these challenges, we propose a novel prompting pipeline, named \method, that leverages knowledge graphs (KGs) to enhance LLMs' inference and transparency. Our method enables LLMs to comprehend KG inputs and infer with a combination of implicit and external knowledge. Moreover, our method elicits the mind map of LLMs, which reveals their reasoning pathways based on the ontology of knowledge. We evaluate our method on diverse question \& answering tasks, especially in medical domains, and show significant improvements over baselines. We also introduce a new hallucination evaluation benchmark and analyze the effects of different components of our method. Our results demonstrate the effectiveness and robustness of our method in merging knowledge from LLMs and KGs for combined inference. To reproduce our results and extend the framework further, we make our codebase available at https://github.com/wyl-willing/MindMap.",True,True,Yilin Wen and Zifeng Wang and Jimeng Sun,2024,,https://arxiv.org/abs/2308.09729,,
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,luo2024reasoninggraphsfaithfulinterpretable,\cite{luo2024reasoninggraphsfaithfulinterpretable},Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning,,,True,False,Linhao Luo and Yuan-Fang Li and Gholamreza Haffari and Shirui Pan,2024,,https://arxiv.org/abs/2310.01061,,
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,li2023graphreasoningquestionanswering,\cite{li2023graphreasoningquestionanswering},Graph Reasoning for Question Answering with Triplet Retrieval,,,True,False,Shiyang Li and Yifan Gao and Haoming Jiang and Qingyu Yin and Zheng Li and Xifeng Yan and Chao Zhang and Bing Yin,2023,,https://arxiv.org/abs/2305.18742,,
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,fatemi2023talklikegraphencoding,\cite{fatemi2023talklikegraphencoding},Talk like a Graph: Encoding Graphs for Large Language Models,https://arxiv.org/abs/2310.04560v1,"Graphs are a powerful tool for representing and analyzing complex relationships in real-world applications such as social networks, recommender systems, and computational finance. Reasoning on graphs is essential for drawing inferences about the relationships between entities in a complex system, and to identify hidden patterns and trends. Despite the remarkable progress in automated reasoning with natural text, reasoning on graphs with large language models (LLMs) remains an understudied problem. In this work, we perform the first comprehensive study of encoding graph-structured data as text for consumption by LLMs. We show that LLM performance on graph reasoning tasks varies on three fundamental levels: (1) the graph encoding method, (2) the nature of the graph task itself, and (3) interestingly, the very structure of the graph considered. These novel results provide valuable insight on strategies for encoding graphs as text. Using these insights we illustrate how the correct choice of encoders can boost performance on graph reasoning tasks inside LLMs by 4.8% to 61.8%, depending on the task.",True,True,Bahare Fatemi and Jonathan Halcrow and Bryan Perozzi,2023,,https://arxiv.org/abs/2310.04560,,
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,10387715,\cite{10387715},Unifying Large Language Models and Knowledge Graphs: A Roadmap,,,True,False,"Pan, Shirui and Luo, Linhao and Wang, Yufei and Chen, Chen and Wang, Jiapu and Wu, Xindong",2024,,,10.1109/TKDE.2024.3352100,IEEE Transactions on Knowledge and Data Engineering
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,10697304,\cite{10697304},Large Language Models on Graphs: A Comprehensive Survey,https://arxiv.org/abs/2312.02783v4,"Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data is associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data is paired with rich textual information (e.g., molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-attributed graphs, and text-paired graphs. We then discuss detailed techniques for utilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM as Aligner, and compare the advantages and disadvantages of different schools of models. Furthermore, we discuss the real-world applications of such methods and summarize open-source codes and benchmark datasets. Finally, we conclude with potential future research directions in this fast-growing field. The related source can be found at https://github.com/PeterGriffinJin/Awesome-Language-Model-on-Graphs.",True,True,"Jin, Bowen and Liu, Gang and Han, Chi and Jiang, Meng and Ji, Heng and Han, Jiawei",2024,,,10.1109/TKDE.2024.3469578,IEEE Transactions on Knowledge and Data Engineering
Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis,2510.08655v1,Hager2024,\cite{Hager2024},Evaluating and Mitigating Limitations of Large Language Models in Clinical Decision Making,,,True,False,"Hager, Paul and Jungmann, Friederike and Bhagat, Kunal and Hubrecht, Inga and Knauer, Manuel and Vielhauer, Jakob and Holland, Robbie and Braren, Rickmer and Makowski, Marcus and Kaisis, Georgios and Rueckert, Daniel",2024,,https://www.medrxiv.org/content/early/2024/01/26/2024.01.26.24301810,10.1101/2024.01.26.24301810,medRxiv
Multi-step Predictive Coding Leads To Simplicity Bias,2511.09290v1,recanatesi2021predictive,\cite{recanatesi2021predictive},Predictive learning as a network mechanism for extracting low-dimensional latent space representations,,,True,False,"Recanatesi, Stefano and Farrell, Matthew and Lajoie, Guillaume and Deneve, Sophie and Rigotti, Mattia and Shea-Brown, Eric",2021,,,,Nature communications
Multi-step Predictive Coding Leads To Simplicity Bias,2511.09290v1,levenstein2024sequential,\cite{levenstein2024sequential},Sequential predictive learning is a unifying theory for hippocampal representation and replay,,,True,False,"Levenstein, Daniel and Efremov, Aleksei and Eyono, Roy Henha and Peyrache, Adrien and Richards, Blake",2024,,,,bioRxiv
Multi-step Predictive Coding Leads To Simplicity Bias,2511.09290v1,soudry2018implicit,\cite{soudry2018implicit},The implicit bias of gradient descent on separable data,,,True,False,"Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan",2018,,,,Journal of Machine Learning Research
Multi-step Predictive Coding Leads To Simplicity Bias,2511.09290v1,ji2018gradient,\cite{ji2018gradient},Gradient descent aligns the layers of deep linear networks,,,True,False,"Ji, Ziwei and Telgarsky, Matus",2018,,,,arXiv preprint arXiv:1810.02032
Multi-step Predictive Coding Leads To Simplicity Bias,2511.09290v1,lyu2019gradient,\cite{lyu2019gradient},Gradient descent maximizes the margin of homogeneous neural networks,,,True,False,"Lyu, Kaifeng and Li, Jian",2019,,,,arXiv preprint arXiv:1906.05890
Multi-step Predictive Coding Leads To Simplicity Bias,2511.09290v1,papyan2020prevalence,\cite{papyan2020prevalence},Prevalence of neural collapse during the terminal phase of deep learning training,,,True,False,"Papyan, Vardan and Han, XY and Donoho, David L",2020,,,,Proceedings of the National Academy of Sciences
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,vdmalsburg1999binding,\cite{vdmalsburg1999binding},The what and why of binding: the modeler’s perspective,,,True,False,"Von der Malsburg, Christoph",1999,,,,Neuron
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,feldman2013neural,\cite{feldman2013neural},The neural binding problem (s),,,True,False,"Feldman, Jerome",2013,,,,Cognitive neurodynamics
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,zeki1978functional,\cite{zeki1978functional},Functional specialisation in the visual cortex of the rhesus monkey,,,True,False,"Zeki, Semir M",1978,,,,Nature
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,livingstone1988segregation,\cite{livingstone1988segregation},"Segregation of form, color, movement, and depth: anatomy, physiology, and perception",,,True,False,"Livingstone, Margaret and Hubel, David",1988,,,,Science
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,mishkin1983object,\cite{mishkin1983object},Object vision and spatial vision: two cortical pathways,,,True,False,"Mishkin, Mortimer and Ungerleider, Leslie G and Macko, Kathleen A",1983,,,,Trends in neurosciences
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,felleman1991distributed,\cite{felleman1991distributed},Distributed hierarchical processing in the primate cerebral cortex.,,,True,False,"Felleman, Daniel J and Van Essen, David C",1991,,,,"Cerebral cortex (New York, NY: 1991)"
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,grill2014functional,\cite{grill2014functional},The functional architecture of the ventral temporal cortex and its role in categorization,,,True,False,"Grill-Spector, Kalanit and Weiner, Kevin S",2014,,,,Nature Reviews Neuroscience
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,peters2021capturing,\cite{peters2021capturing},Capturing the objects of vision with neural networks,,,True,False,"Peters, Benjamin and Kriegeskorte, Nikolaus",2021,,,,Nature human behaviour
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,scholl2001objects,\cite{scholl2001objects},Objects and attention: The state of the art,,,True,False,"Scholl, Brian J",2001,,,,Cognition
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,kanizsa1979organization,\cite{kanizsa1979organization},Organization in vision: Essays on Gestalt perception,,,True,False,"Kanizsa, Gaetano and Legrenzi, Paolo and Bozzi, Paolo",1979,,,,(No Title)
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,palmer1977hierarchical,\cite{palmer1977hierarchical},Hierarchical structure in perceptual representation,,,True,False,"Palmer, Stephen E",1977,,,,Cognitive psychology
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,biederman1987recognition,\cite{biederman1987recognition},Recognition-by-components: a theory of human image understanding.,,,True,False,"Biederman, Irving",1987,,,,Psychological review
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,von1994correlation,\cite{von1994correlation},The correlation theory of brain function,,,True,False,"Von Der Malsburg, Christoph",1994,,,,
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,treisman1996binding,\cite{treisman1996binding},The binding problem,,,True,False,"Treisman, Anne",1996,,,,Current opinion in neurobiology
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,scholte2025beyond,\cite{scholte2025beyond},Beyond binding: from modular to natural vision,,,True,False,"Scholte, H Steven and de Haan, Edward HF",2025,,,,Trends in Cognitive Sciences
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,robertson2003binding,\cite{robertson2003binding},"Binding, spatial attention and perceptual awareness",,,True,False,"Robertson, Lynn C",2003,,,,Nature Reviews Neuroscience
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,roskies1999binding,\cite{roskies1999binding},The binding problem,,,True,False,"Roskies, Adina L",1999,,,,Neuron
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,treisman1977focused,\cite{treisman1977focused},Focused attention in the perception and retrieval of multidimensional stimuli,,,True,False,"Treisman, Anne",1977,,,,Perception \& Psychophysics
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,reynolds1999role,\cite{reynolds1999role},The role of neural mechanisms of attention in solving the binding problem,,,True,False,"Reynolds, John H and Desimone, Robert",1999,,,,Neuron
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,roelfsema2023solving,\cite{roelfsema2023solving},Solving the binding problem: Assemblies form when neurons enhance their firing rate—they don’t need to oscillate or synchronize,,,True,False,"Roelfsema, Pieter R",2023,,,,Neuron
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,van2020going,\cite{van2020going},Going in circles is the way forward: the role of recurrence in visual inference,,,True,False,"van Bergen, Ruben S and Kriegeskorte, Nikolaus",2020,,,,Current Opinion in Neurobiology
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,kar2019evidence,\cite{kar2019evidence},Evidence that recurrent circuits are critical to the ventral stream’s execution of core object recognition behavior,,,True,False,"Kar, Kohitij and Kubilius, Jonas and Schmidt, Kailyn and Issa, Elias B and DiCarlo, James J",2019,,,,Nature neuroscience
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,locatello2020object,\cite{locatello2020object},Object-centric learning with slot attention,,,True,False,"Locatello, Francesco and Weissenborn, Dirk and Unterthiner, Thomas and Mahendran, Aravindh and Heigold, Georg and Uszkoreit, Jakob and Dosovitskiy, Alexey and Kipf, Thomas",2020,,,,Advances in neural information processing systems
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,greff2019multi,\cite{greff2019multi},Multi-object representation learning with iterative variational inference,,,True,False,"Greff, Klaus and Kaufman, Rapha{\""e}l Lopez and Kabra, Rishabh and Watters, Nick and Burgess, Christopher and Zoran, Daniel and Matthey, Loic and Botvinick, Matthew and Lerchner, Alexander",2019,,,,
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,burgess2019monet,\cite{burgess2019monet},Monet: Unsupervised scene decomposition and representation,,,True,False,"Burgess, Christopher P and Matthey, Loic and Watters, Nicholas and Kabra, Rishabh and Higgins, Irina and Botvinick, Matt and Lerchner, Alexander",2019,,https://arxiv.org/abs/1901.11390,,arXiv preprint arXiv:1901.11390
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,kipf2021conditional,\cite{kipf2021conditional},Conditional Object-Centric Learning from Video,https://arxiv.org/abs/2111.12594v2,"Object-centric representations are a promising path toward more systematic generalization by providing flexible abstractions upon which compositional world models can be built. Recent work on simple 2D and 3D datasets has shown that models with object-centric inductive biases can learn to segment and represent meaningful objects from the statistical structure of the data alone without the need for any supervision. However, such fully-unsupervised methods still fail to scale to diverse realistic data, despite the use of increasingly complex inductive biases such as priors for the size of objects or the 3D geometry of the scene. In this paper, we instead take a weakly-supervised approach and focus on how 1) using the temporal dynamics of video data in the form of optical flow and 2) conditioning the model on simple object location cues can be used to enable segmenting and tracking objects in significantly more realistic synthetic data. We introduce a sequential extension to Slot Attention which we train to predict optical flow for realistic looking synthetic scenes and show that conditioning the initial state of this model on a small set of hints, such as center of mass of objects in the first frame, is sufficient to significantly improve instance segmentation. These benefits generalize beyond the training distribution to novel objects, novel backgrounds, and to longer video sequences. We also find that such initial-state-conditioning can be used during inference as a flexible interface to query the model for specific objects or parts of objects, which could pave the way for a range of weakly-supervised approaches and allow more effective interaction with trained models.",True,True,"Kipf, Thomas and Elsayed, Gamaleldin F and Mahendran, Aravindh and Stone, Austin and Sabour, Sara and Heigold, Georg and Jonschkowski, Rico and Dosovitskiy, Alexey and Greff, Klaus",2021,,,,arXiv preprint arXiv:2111.12594
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,seitzer2022bridging,\cite{seitzer2022bridging},Bridging the gap to real-world object-centric learning,,,True,False,"Seitzer, Maximilian and Horn, Max and Zadaianchuk, Andrii and Zietlow, Dominik and Xiao, Tianjun and Simon-Gabriel, Carl-Johann and He, Tong and Zhang, Zheng and Sch{\""o}lkopf, Bernhard and Brox, Thomas and others",2022,,,,arXiv preprint arXiv:2209.14860
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,singh2022simple,\cite{singh2022simple},Simple Unsupervised Object-Centric Learning for Complex and Naturalistic Videos,https://arxiv.org/abs/2205.14065v1,"Unsupervised object-centric learning aims to represent the modular, compositional, and causal structure of a scene as a set of object representations and thereby promises to resolve many critical limitations of traditional single-vector representations such as poor systematic generalization. Although there have been many remarkable advances in recent years, one of the most critical problems in this direction has been that previous methods work only with simple and synthetic scenes but not with complex and naturalistic images or videos. In this paper, we propose STEVE, an unsupervised model for object-centric learning in videos. Our proposed model makes a significant advancement by demonstrating its effectiveness on various complex and naturalistic videos unprecedented in this line of research. Interestingly, this is achieved by neither adding complexity to the model architecture nor introducing a new objective or weak supervision. Rather, it is achieved by a surprisingly simple architecture that uses a transformer-based image decoder conditioned on slots and the learning objective is simply to reconstruct the observation. Our experiment results on various complex and naturalistic videos show significant improvements compared to the previous state-of-the-art.",True,True,"Singh, Gautam and Wu, Yi-Fu and Ahn, Sungjin",2022,,,,Advances in Neural Information Processing Systems
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,jiang2023object,\cite{jiang2023object},Object-Centric Slot Diffusion,https://arxiv.org/abs/2303.10834v5,"The recent success of transformer-based image generative models in object-centric learning highlights the importance of powerful image generators for handling complex scenes. However, despite the high expressiveness of diffusion models in image generation, their integration into object-centric learning remains largely unexplored in this domain. In this paper, we explore the feasibility and potential of integrating diffusion models into object-centric learning and investigate the pros and cons of this approach. We introduce Latent Slot Diffusion (LSD), a novel model that serves dual purposes: it is the first object-centric learning model to replace conventional slot decoders with a latent diffusion model conditioned on object slots, and it is also the first unsupervised compositional conditional diffusion model that operates without the need for supervised annotations like text. Through experiments on various object-centric tasks, including the first application of the FFHQ dataset in this field, we demonstrate that LSD significantly outperforms state-of-the-art transformer-based decoders, particularly in more complex scenes, and exhibits superior unsupervised compositional generation quality. In addition, we conduct a preliminary investigation into the integration of pre-trained diffusion models in LSD and demonstrate its effectiveness in real-world image segmentation and generation. Project page is available at https://latentslotdiffusion.github.io",True,True,"Jiang, Jindong and Deng, Fei and Singh, Gautam and Ahn, Sungjin",2023,,,,arXiv preprint arXiv:2303.10834
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,jung2024learning,\cite{jung2024learning},Learning to Compose: Improving Object Centric Learning by Injecting Compositionality,https://arxiv.org/abs/2405.00646v1,"Learning compositional representation is a key aspect of object-centric learning as it enables flexible systematic generalization and supports complex visual reasoning. However, most of the existing approaches rely on auto-encoding objective, while the compositionality is implicitly imposed by the architectural or algorithmic bias in the encoder. This misalignment between auto-encoding objective and learning compositionality often results in failure of capturing meaningful object representations. In this study, we propose a novel objective that explicitly encourages compositionality of the representations. Built upon the existing object-centric learning framework (e.g., slot attention), our method incorporates additional constraints that an arbitrary mixture of object representations from two images should be valid by maximizing the likelihood of the composite data. We demonstrate that incorporating our objective to the existing framework consistently improves the objective-centric learning and enhances the robustness to the architectural choices.",True,True,"Jung, Whie and Yoo, Jaehoon and Ahn, Sungjin and Hong, Seunghoon",2024,,,,arXiv preprint arXiv:2405.00646
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,kim2023leveraging,\cite{kim2023leveraging},Leveraging Image Augmentation for Object Manipulation: Towards Interpretable Controllability in Object-Centric Learning,,,True,False,"Kim, Jinwoo and Choi, Janghyuk and Kang, Jaehyun and Lee, Changyeon and Choi, Ho-Jin and Kim, Seon Joo",2023,,,,arXiv preprint arXiv:2310.08929
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,rubinstein2025we,\cite{rubinstein2025we},Are We Done with Object-Centric Learning?,,,True,False,"Rubinstein, Alexander and Prabhu, Ameya and Bethge, Matthias and Oh, Seong Joon",2025,,,,arXiv preprint arXiv:2504.07092
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,teh2023towards,\cite{teh2023towards},Towards Discrete Object Representations in Vision Transformers with Tensor Products,,,True,False,"Teh, Wei Yuen and Lim, Chern Hong and Lim, Mei Kuan and Tan, Ian KT",2023,,,,
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,sabour2017dynamic,\cite{sabour2017dynamic},Dynamic Routing Between Capsules,https://arxiv.org/abs/1710.09829v2,"A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.",True,True,"Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E",2017,,,,Advances in neural information processing systems
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,qian2024recasting,\cite{qian2024recasting},Recasting Generic Pretrained Vision Transformers As Object-Centric Scene Encoders For Manipulation Policies,https://arxiv.org/abs/2405.15916v1,"Generic re-usable pre-trained image representation encoders have become a standard component of methods for many computer vision tasks. As visual representations for robots however, their utility has been limited, leading to a recent wave of efforts to pre-train robotics-specific image encoders that are better suited to robotic tasks than their generic counterparts. We propose Scene Objects From Transformers, abbreviated as SOFT, a wrapper around pre-trained vision transformer (PVT) models that bridges this gap without any further training. Rather than construct representations out of only the final layer activations, SOFT individuates and locates object-like entities from PVT attentions, and describes them with PVT activations, producing an object-centric embedding. Across standard choices of generic pre-trained vision transformers PVT, we demonstrate in each case that policies trained on SOFT(PVT) far outstrip standard PVT representations for manipulation tasks in simulated and real settings, approaching the state-of-the-art robotics-aware representations. Code, appendix and videos: https://sites.google.com/view/robot-soft/",True,True,"Qian, Jianing and Panagopoulos, Anastasios and Jayaraman, Dinesh",2024,,,,
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,wang2023tokencut,\cite{wang2023tokencut},Tokencut: Segmenting objects in images and videos with self-supervised transformer and normalized cut,,,True,False,"Wang, Yangtao and Shen, Xi and Yuan, Yuan and Du, Yuming and Li, Maomao and Hu, Shell Xu and Crowley, James L and Vaufreydaz, Dominique",2023,,,,IEEE transactions on pattern analysis and machine intelligence
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,bielski2022move,\cite{bielski2022move},MOVE: Unsupervised Movable Object Segmentation and Detection,https://arxiv.org/abs/2210.07920v2,"We introduce MOVE, a novel method to segment objects without any form of supervision. MOVE exploits the fact that foreground objects can be shifted locally relative to their initial position and result in realistic (undistorted) new images. This property allows us to train a segmentation model on a dataset of images without annotation and to achieve state of the art (SotA) performance on several evaluation datasets for unsupervised salient object detection and segmentation. In unsupervised single object discovery, MOVE gives an average CorLoc improvement of 7.2% over the SotA, and in unsupervised class-agnostic object detection it gives a relative AP improvement of 53% on average. Our approach is built on top of self-supervised features (e.g. from DINO or MAE), an inpainting network (based on the Masked AutoEncoder) and adversarial training.",True,True,"Bielski, Adam and Favaro, Paolo",2022,,,,Advances in Neural Information Processing Systems
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,ding2022deeply,\cite{ding2022deeply},Deeply unsupervised patch re-identification for pre-training object detectors,,,True,False,"Ding, Jian and Xie, Enze and Xu, Hang and Jiang, Chenhan and Li, Zhenguo and Luo, Ping and Xia, Gui-Song",2022,,,,IEEE Transactions on Pattern Analysis and Machine Intelligence
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,trusca2024object,\cite{trusca2024object},Object-Attribute Binding in Text-to-Image Generation: Evaluation and Control,https://arxiv.org/abs/2404.13766v1,"Current diffusion models create photorealistic images given a text prompt as input but struggle to correctly bind attributes mentioned in the text to the right objects in the image. This is evidenced by our novel image-graph alignment model called EPViT (Edge Prediction Vision Transformer) for the evaluation of image-text alignment. To alleviate the above problem, we propose focused cross-attention (FCA) that controls the visual attention maps by syntactic constraints found in the input sentence. Additionally, the syntax structure of the prompt helps to disentangle the multimodal CLIP embeddings that are commonly used in T2I generation. The resulting DisCLIP embeddings and FCA are easily integrated in state-of-the-art diffusion models without additional training of these models. We show substantial improvements in T2I generation and especially its attribute-object binding on several datasets.\footnote{Code and data will be made available upon acceptance.",True,True,"Trusca, Maria Mihaela and Nuyts, Wolf and Thomm, Jonathan and Honig, Robert and Hofmann, Thomas and Tuytelaars, Tinne and Moens, Marie-Francine",2024,,,,arXiv preprint arXiv:2404.13766
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,hu2024token,\cite{hu2024token},Token Merging for Training-Free Semantic Binding in Text-to-Image Synthesis,,,True,False,"Hu, Taihang and Li, Linxuan and van de Weijer, Joost and Gao, Hongcheng and Shahbaz Khan, Fahad and Yang, Jian and Cheng, Ming-Ming and Wang, Kai and Wang, Yaxing",2024,,,,Advances in Neural Information Processing Systems
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,wang2025reversal,\cite{wang2025reversal},Is the reversal curse a binding problem? uncovering limitations of transformers from a basic generalization failure,,,True,False,"Wang, Boshi and Sun, Huan",2025,,,,arXiv preprint arXiv:2504.01928
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,campbell2024understanding,\cite{campbell2024understanding},Understanding the limits of vision language models through the lens of the binding problem,,,True,False,"Campbell, Declan and Rane, Sunayana and Giallanza, Tyler and De Sabbata, Camillo Nicol{\`o} and Ghods, Kia and Joshi, Amogh and Ku, Alexander and Frankland, Steven and Griffiths, Tom and Cohen, Jonathan D and others",2024,,,,Advances in Neural Information Processing Systems
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,feng2023language,\cite{feng2023language},How do Language Models Bind Entities in Context?,https://arxiv.org/abs/2310.17191v2,"To correctly use in-context information, language models (LMs) must bind entities to their attributes. For example, given a context describing a ""green square"" and a ""blue circle"", LMs must bind the shapes to their respective colors. We analyze LM representations and identify the binding ID mechanism: a general mechanism for solving the binding problem, which we observe in every sufficiently large model from the Pythia and LLaMA families. Using causal interventions, we show that LMs' internal activations represent binding information by attaching binding ID vectors to corresponding entities and attributes. We further show that binding ID vectors form a continuous subspace, in which distances between binding ID vectors reflect their discernability. Overall, our results uncover interpretable strategies in LMs for representing symbolic knowledge in-context, providing a step towards understanding general in-context reasoning in large-scale LMs.",True,True,"Feng, Jiahai and Steinhardt, Jacob",2023,,,,arXiv preprint arXiv:2310.17191
Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?,2510.24709v1,dai2024representational,\cite{dai2024representational},Representational Analysis of Binding in Large Language Models,,,True,False,"Dai, Qin and Heinzerling, Benjamin and Inui, Kentaro",2024,,,,arXiv e-prints
Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow,2511.00977v1,liu2023flow,\cite{liu2023flow},{Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow},,,True,False,Xingchao Liu and Chengyue Gong and Qiang Liu,2023,,,,
Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow,2511.00977v1,lipman2023flow,\cite{lipman2023flow},Flow Matching for Generative Modeling,,,True,False,Yaron Lipman and Ricky T. Q. Chen and Heli Ben-Hamu and Maximilian Nickel and Matthew Le,2023,,,,
Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow,2511.00977v1,albergo2023building,\cite{albergo2023building},Building Normalizing Flows with Stochastic Interpolants,,,True,False,Michael Samuel Albergo and Eric Vanden-Eijnden,2023,,,,
Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow,2511.00977v1,eijkelboom2024variational,\cite{eijkelboom2024variational},Variational Flow Matching for Graph Generation,,,True,False,Floor Eijkelboom and Grigory Bartosh and Christian A. Naesseth and Max Welling and Jan-Willem van de Meent,2024,,,,
Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow,2511.00977v1,tong2024improving,\cite{tong2024improving},Improving and generalizing flow-based generative models with minibatch optimal transport,,,True,False,Alexander Tong and Kilian FATRAS and Nikolay Malkin and Guillaume Huguet and Yanlei Zhang and Jarrid Rector-Brooks and Guy Wolf and Yoshua Bengio,2024,,,,Transactions on Machine Learning Research
Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow,2511.00977v1,pmlr-v238-tong24a,\cite{pmlr-v238-tong24a},Simulation-Free {S}chrödinger Bridges via Score and Flow Matching,,,True,False,"Tong, Alexander Y. and Malkin, Nikolay and Fatras, Kilian and Atanackovic, Lazar and Zhang, Yanlei and Huguet, Guillaume and Wolf, Guy and Bengio, Yoshua",2024,02--04 May,,,
Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow,2511.00977v1,atanackovic2025meta,\cite{atanackovic2025meta},Meta Flow Matching: Integrating Vector Fields on the Wasserstein Manifold,https://arxiv.org/abs/2408.14608v2,"Numerous biological and physical processes can be modeled as systems of interacting entities evolving continuously over time, e.g. the dynamics of communicating cells or physical particles. Learning the dynamics of such systems is essential for predicting the temporal evolution of populations across novel samples and unseen environments. Flow-based models allow for learning these dynamics at the population level - they model the evolution of the entire distribution of samples. However, current flow-based models are limited to a single initial population and a set of predefined conditions which describe different dynamics. We argue that multiple processes in natural sciences have to be represented as vector fields on the Wasserstein manifold of probability densities. That is, the change of the population at any moment in time depends on the population itself due to the interactions between samples. In particular, this is crucial for personalized medicine where the development of diseases and their respective treatment response depend on the microenvironment of cells specific to each patient. We propose Meta Flow Matching (MFM), a practical approach to integrate along these vector fields on the Wasserstein manifold by amortizing the flow model over the initial populations. Namely, we embed the population of samples using a Graph Neural Network (GNN) and use these embeddings to train a Flow Matching model. This gives MFM the ability to generalize over the initial distributions, unlike previously proposed methods. We demonstrate the ability of MFM to improve the prediction of individual treatment responses on a large-scale multi-patient single-cell drug screen dataset.",True,True,Lazar Atanackovic and Xi Zhang and Brandon Amos and Mathieu Blanchette and Leo J Lee and Yoshua Bengio and Alexander Tong and Kirill Neklyudov,2025,,,,
Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow,2511.00977v1,klein2024genot,\cite{klein2024genot},GENOT: Entropic (Gromov) Wasserstein Flow Matching with Applications to Single-Cell Genomics,https://arxiv.org/abs/2310.09254v4,"Single-cell genomics has significantly advanced our understanding of cellular behavior, catalyzing innovations in treatments and precision medicine. However, single-cell sequencing technologies are inherently destructive and can only measure a limited array of data modalities simultaneously. This limitation underscores the need for new methods capable of realigning cells. Optimal transport (OT) has emerged as a potent solution, but traditional discrete solvers are hampered by scalability, privacy, and out-of-sample estimation issues. These challenges have spurred the development of neural network-based solvers, known as neural OT solvers, that parameterize OT maps. Yet, these models often lack the flexibility needed for broader life science applications. To address these deficiencies, our approach learns stochastic maps (i.e. transport plans), allows for any cost function, relaxes mass conservation constraints and integrates quadratic solvers to tackle the complex challenges posed by the (Fused) Gromov-Wasserstein problem. Utilizing flow matching as a backbone, our method offers a flexible and effective framework. We demonstrate its versatility and robustness through applications in cell development studies, cellular drug response modeling, and cross-modality cell translation, illustrating significant potential for enhancing therapeutic strategies.",True,True,"Klein, Dominik and Uscidda, Th{\'e}o and Theis, Fabian and Cuturi, Marco",2024,,,,Advances in Neural Information Processing Systems
Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow,2511.00977v1,haviv2024wasserstein,\cite{haviv2024wasserstein},Wasserstein Flow Matching: Generative modeling over families of distributions,https://arxiv.org/abs/2411.00698v2,"Generative modeling typically concerns transporting a single source distribution to a target distribution via simple probability flows. However, in fields like computer graphics and single-cell genomics, samples themselves can be viewed as distributions, where standard flow matching ignores their inherent geometry. We propose Wasserstein flow matching (WFM), which lifts flow matching onto families of distributions using the Wasserstein geometry. Notably, WFM is the first algorithm capable of generating distributions in high dimensions, whether represented analytically (as Gaussians) or empirically (as point-clouds). Our theoretical analysis establishes that Wasserstein geodesics constitute proper conditional flows over the space of distributions, making for a valid FM objective. Our algorithm leverages optimal transport theory and the attention mechanism, demonstrating versatility across computational regimes: exploiting closed-form optimal transport paths for Gaussian families, while using entropic estimates on point-clouds for general distributions. WFM successfully generates both 2D & 3D shapes and high-dimensional cellular microenvironments from spatial transcriptomics data. Code is available at https://github.com/DoronHav/WassersteinFlowMatching .",True,True,"Haviv, Doron and Pooladian, Aram-Alexandre and Pe'er, Dana and Amos, Brandon",2024,,,,arXiv preprint arXiv:2411.00698
Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow,2511.00977v1,zhu2025diffusion,\cite{zhu2025diffusion},Diffusion Generative Modeling for Spatially Resolved Gene Expression Inference from Histology Images,,,True,False,Sichen Zhu and Yuchen Zhu and Molei Tao and Peng Qiu,2025,,,,
Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow,2511.00977v1,wan2023integrating,\cite{wan2023integrating},Integrating spatial and single-cell transcriptomics data using deep generative models with SpatialScope,,,True,False,"Wan, Xiaomeng and Xiao, Jiashun and Tam, Sindy Sing Ting and Cai, Mingxuan and Sugimura, Ryohichi and Wang, Yang and Wan, Xiang and Lin, Zhixiang and Wu, Angela Ruohao and Yang, Can",2023,,,,Nature Communications
Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow,2511.00977v1,haviv2025covariance,\cite{haviv2025covariance},The covariance environment defines cellular niches for spatial inference,,,True,False,"Haviv, Doron and Rem{\v{s}}{\'\i}k, J{\'a}n and Gatie, Mohamed and Snopkowski, Catherine and Takizawa, Meril and Pereira, Nathan and Bashkin, John and Jovanovich, Stevan and Nawy, Tal and Chaligne, Ronan and others",2025,,,,Nature Biotechnology
Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow,2511.00977v1,li2024stdiff,\cite{li2024stdiff},stDiff: a diffusion model for imputing spatial transcriptomics through single-cell transcriptomics,,,True,False,"Li, Kongming and Li, Jiahao and Tao, Yuhao and Wang, Fei",2024,,,,Briefings in Bioinformatics
Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow,2511.00977v1,akbarnejad2025mapping,\cite{akbarnejad2025mapping},Mapping and reprogramming human tissue microenvironments with MintFlow,,,True,False,"Akbarnejad, Amir and Steele, Lloyd and Jafree, Daniyal J and Birk, Sebastian and Sallese, Marta Rosa and Rademaker, Koen and Boxall, Adam and Rumney, Benjamin and Tudor, Catherine and Patel, Minal and others",2025,,,,bioRxiv
Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow,2511.00977v1,yu2025tissue,\cite{yu2025tissue},Tissue reassembly with generative AI,,,True,False,"Yu, Tingyang and Ekbote, Chanakya and Morozov, Nikita and Fan, Jiashuo and Frossard, Pascal and d’Ascoli, St{\'e}phane and Brbi{\'c}, Maria",2025,,,,bioRxiv
Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow,2511.00977v1,ho2020denoising,\cite{ho2020denoising},Denoising Diffusion Probabilistic Models,https://arxiv.org/abs/2006.11239v2,"We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion",True,True,"Ho, Jonathan and Jain, Ajay and Abbeel, Pieter",2020,,,,Advances in neural information processing systems
Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow,2511.00977v1,pham2023robust,\cite{pham2023robust},Robust mapping of spatiotemporal trajectories and cell--cell interactions in healthy and diseased tissues,,,True,False,"Pham, Duy and Tan, Xiao and Balderson, Brad and Xu, Jun and Grice, Laura F and Yoon, Sohye and Willis, Emily F and Tran, Minh and Lam, Pui Yeng and Raghubar, Arti and others",2023,,,,Nature communications
Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow,2511.00977v1,abdelaal2024sirv,\cite{abdelaal2024sirv},SIRV: Spatial inference of RNA velocity at the single-cell resolution,,,True,False,"Abdelaal, Tamim and Grossouw, Laurens M and Pasterkamp, R Jeroen and Lelieveldt, Boudewijn PF and Reinders, Marcel JT and Mahfouz, Ahmed",2024,,,,NAR genomics and bioinformatics
Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow,2511.00977v1,long2025spvelo,\cite{long2025spvelo},spVelo: RNA velocity inference for multi-batch spatial transcriptomics data,,,True,False,"Long, Wenxin and Liu, Tianyu and Xue, Lingzhou and Zhao, Hongyu",2025,,,,bioRxiv
Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow,2511.00977v1,shen2025inferring,\cite{shen2025inferring},Inferring cell trajectories of spatial transcriptomics via optimal transport analysis,,,True,False,"Shen, Xunan and Zuo, Lulu and Ye, Zhongfei and Yuan, Zhongyang and Huang, Ke and Li, Zeyu and Yu, Qichao and Zou, Xuanxuan and Wei, Xiaoyu and Xu, Ping and others",2025,,,,Cell Systems
Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow,2511.00977v1,klein2025mapping,\cite{klein2025mapping},Mapping cells through time and space with moscot,,,True,False,"Klein, Dominik and Palla, Giovanni and Lange, Marius and Klein, Michal and Piran, Zoe and Gander, Manuel and Meng-Papaxanthos, Laetitia and Sterr, Michael and Saber, Lama and Jing, Changying and others",2025,,,,Nature
Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow,2511.00977v1,bryan2025accurate,\cite{bryan2025accurate},Accurate trajectory inference in time-series spatial transcriptomics with structurally-constrained optimal transport,,,True,False,"Bryan, John Peterson and Farhi, Samouil L and Cleary, Brian",2025,,,,bioRxiv
Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow,2511.00977v1,halmos2025dest,\cite{halmos2025dest},DeST-OT: Alignment of spatiotemporal transcriptomics data,,,True,False,"Halmos, Peter and Liu, Xinhao and Gold, Julian and Chen, Feng and Ding, Li and Raphael, Benjamin J",2025,,,,Cell Systems
Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow,2511.00977v1,vayer2020fused,\cite{vayer2020fused},Fused Gromov-Wasserstein Distance for Structured Objects,,,True,False,"Vayer, Titouan and Chapel, Laetitia and Flamary, R{\'e}mi and Tavenard, Romain and Courty, Nicolas",2020,,,,Algorithms
On Biologically Plausible Learning in Continuous Time,2510.18808v1,xie2003equivalence,\cite{xie2003equivalence},Equivalence of backpropagation and contrastive Hebbian learning in a layered network,,,True,False,"Xie, Xiaohui and Seung, H Sebastian",2003,,,,Neural computation
On Biologically Plausible Learning in Continuous Time,2510.18808v1,scellier2017equilibrium,\cite{scellier2017equilibrium},Equilibrium propagation: Bridging the gap between energy-based models and backpropagation,,,True,False,"Scellier, Benjamin and Bengio, Yoshua",2017,,,,Frontiers in computational neuroscience
On Biologically Plausible Learning in Continuous Time,2510.18808v1,hopfield1982neural,\cite{hopfield1982neural},Neural networks and physical systems with emergent collective computational abilities.,,,True,False,"Hopfield, John J",1982,,,,Proceedings of the national academy of sciences
On Biologically Plausible Learning in Continuous Time,2510.18808v1,bengio2015early,\cite{bengio2015early},Early Inference in Energy-Based Models Approximates Back-Propagation,https://arxiv.org/abs/1510.02777v2,"We show that Langevin MCMC inference in an energy-based model with latent variables has the property that the early steps of inference, starting from a stationary point, correspond to propagating error gradients into internal layers, similarly to back-propagation. The error that is back-propagated is with respect to visible units that have received an outside driving force pushing them away from the stationary point. Back-propagated error gradients correspond to temporal derivatives of the activation of hidden units. This observation could be an element of a theory for explaining how brains perform credit assignment in deep hierarchies as efficiently as back-propagation does. In this theory, the continuous-valued latent variables correspond to averaged voltage potential (across time, spikes, and possibly neurons in the same minicolumn), and neural computation corresponds to approximate inference and error back-propagation at the same time.",True,True,"Bengio, Yoshua and Fischer, Asja",2015,,,,arXiv preprint arXiv:1510.02777
On Biologically Plausible Learning in Continuous Time,2510.18808v1,chen2018neural,\cite{chen2018neural},Neural Ordinary Differential Equations,https://arxiv.org/abs/1806.07366v5,"We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.",True,True,"Chen, Ricky TQ and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K",2018,,,,Advances in neural information processing systems
On Biologically Plausible Learning in Continuous Time,2510.18808v1,baydin2018automatic,\cite{baydin2018automatic},Automatic differentiation in machine learning: a survey,https://arxiv.org/abs/1502.05767v4,"Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply ""autodiff"", is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names ""dynamic computational graphs"" and ""differentiable programming"". We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms ""autodiff"", ""automatic differentiation"", and ""symbolic differentiation"" as these are encountered more and more in machine learning settings.",True,True,"Baydin, Atilim Gunes and Pearlmutter, Barak A and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark",2018,,,,Journal of machine learning research
On Biologically Plausible Learning in Continuous Time,2510.18808v1,liao2024self,\cite{liao2024self},Self-Assembly of a Biologically Plausible Learning Circuit,https://arxiv.org/abs/2412.20018v1,"Over the last four decades, the amazing success of deep learning has been driven by the use of Stochastic Gradient Descent (SGD) as the main optimization technique. The default implementation for the computation of the gradient for SGD is backpropagation, which, with its variations, is used to this day in almost all computer implementations. From the perspective of neuroscientists, however, the consensus is that backpropagation is unlikely to be used by the brain. Though several alternatives have been discussed, none is so far supported by experimental evidence. Here we propose a circuit for updating the weights in a network that is biologically plausible, works as well as backpropagation, and leads to verifiable predictions about the anatomy and the physiology of a characteristic motif of four plastic synapses between ascending and descending cortical streams. A key prediction of our proposal is a surprising property of self-assembly of the basic circuit, emerging from initial random connectivity and heterosynaptic plasticity rules.",True,True,"Liao, Qianli and Ziyin, Liu and Gan, Yulu and Cheung, Brian and Harnett, Mark and Poggio, Tomaso",2024,,,,arXiv preprint arXiv:2412.20018
On Biologically Plausible Learning in Continuous Time,2510.18808v1,lillicrap2020backpropagation,\cite{lillicrap2020backpropagation},Backpropagation and the brain,,,True,False,"Lillicrap, Timothy P and Santoro, Adam and Marris, Luke and Akerman, Colin J and Hinton, Geoffrey",2020,,,,Nature Reviews Neuroscience
On Biologically Plausible Learning in Continuous Time,2510.18808v1,koplow2025emergence,\cite{koplow2025emergence},Emergence of Hebbian Dynamics in Regularized Non-Local Learners,https://arxiv.org/abs/2505.18069v1,"Stochastic Gradient Descent (SGD) has emerged as a remarkably effective learning algorithm, underpinning nearly all state-of-the-art machine learning models, from large language models to autonomous vehicles. Despite its practical success, SGD appears fundamentally distinct from biological learning mechanisms. It is widely believed that the biological brain can not implement gradient descent because it is nonlocal, and we have found little (if any) experimental evidence for it. In contrast, the brain is widely thought to learn via local Hebbian learning principles, which have been seen as incompatible with gradient descent. In this paper, we establish a theoretical and empirical connection between the learning signals of neural networks trained using SGD with weight decay and those trained with Hebbian learning near convergence. We show that SGD with regularization can appear to learn according to a Hebbian rule, and SGD with injected noise according to an anti-Hebbian rule. We also provide empirical evidence that Hebbian learning properties can emerge in a network with weight decay from virtually any learning rule--even random ones. These results may bridge a long-standing gap between artificial and biological learning, revealing Hebbian properties as an epiphenomenon of deeper optimization principles and cautioning against interpreting their presence in neural data as evidence against more complex hetero-synaptic mechanisms.",True,True,"Koplow, David and Poggio, Tomaso and Ziyin, Liu",2025,,,,arXiv preprint arXiv:2505.18069
Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,2510.25976v1,kay2008identifying,\cite{kay2008identifying},Identifying natural images from human brain activity,,,True,False,"Kay, Kendrick N and Naselaris, Thomas and Prenger, Ryan J and Gallant, Jack L",2008,,,,Nature
Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,2510.25976v1,naselaris2009bayesian,\cite{naselaris2009bayesian},BigGAN-based Bayesian reconstruction of natural images from human brain activity,https://arxiv.org/abs/2003.06105v1,"In the visual decoding domain, visually reconstructing presented images given the corresponding human brain activity monitored by functional magnetic resonance imaging (fMRI) is difficult, especially when reconstructing viewed natural images. Visual reconstruction is a conditional image generation on fMRI data and thus generative adversarial network (GAN) for natural image generation is recently introduced for this task. Although GAN-based methods have greatly improved, the fidelity and naturalness of reconstruction are still unsatisfactory due to the small number of fMRI data samples and the instability of GAN training. In this study, we proposed a new GAN-based Bayesian visual reconstruction method (GAN-BVRM) that includes a classifier to decode categories from fMRI data, a pre-trained conditional generator to generate natural images of specified categories, and a set of encoding models and evaluator to evaluate generated images. GAN-BVRM employs the pre-trained generator of the prevailing BigGAN to generate masses of natural images, and selects the images that best matches with the corresponding brain activity through the encoding models as the reconstruction of the image stimuli. In this process, the semantic and detailed contents of reconstruction are controlled by decoded categories and encoding models, respectively. GAN-BVRM used the Bayesian manner to avoid contradiction between naturalness and fidelity from current GAN-based methods and thus can improve the advantages of GAN. Experimental results revealed that GAN-BVRM improves the fidelity and naturalness, that is, the reconstruction is natural and similar to the presented image stimuli.",True,True,"Naselaris, Thomas and Prenger, Ryan J and Kay, Kendrick N and Oliver, Michael and Gallant, Jack L",2009,,,,Neuron
Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,2510.25976v1,nishimoto2011reconstructing,\cite{nishimoto2011reconstructing},Reconstructing visual experiences from brain activity evoked by natural movies,,,True,False,"Nishimoto, Shinji and Vu, An T and Naselaris, Thomas and Benjamini, Yuval and Yu, Bin and Gallant, Jack L",2011,,,,Current biology
Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,2510.25976v1,gucclu2015deep,\cite{gucclu2015deep},Deep Neural Networks Reveal a Gradient in the Complexity of Neural Representations across the Brain's Ventral Visual Pathway,https://arxiv.org/abs/1411.6422v1,"Converging evidence suggests that the mammalian ventral visual pathway encodes increasingly complex stimulus features in downstream areas. Using deep convolutional neural networks, we can now quantitatively demonstrate that there is indeed an explicit gradient for feature complexity in the ventral pathway of the human brain. Our approach also allows stimulus features of increasing complexity to be mapped across the human brain, providing an automated approach to probing how representations are mapped across the cortical sheet. Finally, it is shown that deep convolutional neural networks allow decoding of representations in the human brain at a previously unattainable degree of accuracy, providing a more sensitive window into the human brain.",True,True,"G{\""u}{\c{c}}l{\""u}, Umut and Van Gerven, Marcel AJ",2015,,,,Journal of Neuroscience
Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,2510.25976v1,zhang2018constraint,\cite{zhang2018constraint},Constraint-free Natural Image Reconstruction from fMRI Signals Based on Convolutional Neural Network,https://arxiv.org/abs/1801.05151v1,"In recent years, research on decoding brain activity based on functional magnetic resonance imaging (fMRI) has made remarkable achievements. However, constraint-free natural image reconstruction from brain activity is still a challenge. The existing methods simplified the problem by using semantic prior information or just reconstructing simple images such as letters and digitals. Without semantic prior information, we present a novel method to reconstruct nature images from fMRI signals of human visual cortex based on the computation model of convolutional neural network (CNN). Firstly, we extracted the units output of viewed natural images in each layer of a pre-trained CNN as CNN features. Secondly, we transformed image reconstruction from fMRI signals into the problem of CNN feature visualizations by training a sparse linear regression to map from the fMRI patterns to CNN features. By iteratively optimization to find the matched image, whose CNN unit features become most similar to those predicted from the brain activity, we finally achieved the promising results for the challenging constraint-free natural image reconstruction. As there was no use of semantic prior information of the stimuli when training decoding model, any category of images (not constraint by the training set) could be reconstructed theoretically. We found that the reconstructed images resembled the natural stimuli, especially in position and shape. The experimental results suggest that hierarchical visual features can effectively express the visual perception process of human brain.",True,True,"Zhang, Chi and Qiao, Kai and Wang, Linyuan and Tong, Li and Zeng, Ying and Yan, Bin",2018,,,,Frontiers in human neuroscience
Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,2510.25976v1,shen2019deep,\cite{shen2019deep},Deep image reconstruction from human brain activity,,,True,False,"Shen, Guohua and Horikawa, Tomoyasu and Majima, Kei and Kamitani, Yukiyasu",2019,,,,PLoS computational biology
Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,2510.25976v1,seeliger2018generative,\cite{seeliger2018generative},Generative adversarial networks for reconstructing natural images from brain activity,,,True,False,"Seeliger, Katja and G{\""u}{\c{c}}l{\""u}, Umut and Ambrogioni, Luca and G{\""u}{\c{c}}l{\""u}t{\""u}rk, Yagmur and Van Gerven, Marcel AJ",2018,,,,NeuroImage
Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,2510.25976v1,st2018generative,\cite{st2018generative},Generative adversarial networks conditioned on brain activity reconstruct seen images,,,True,False,"St-Yves, Ghislain and Naselaris, Thomas",2018,,,,
Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,2510.25976v1,beliy2019voxels,\cite{beliy2019voxels},From voxels to pixels and back: Self-supervision in natural-image reconstruction from fMRI,,,True,False,"Beliy, Roman and Gaziv, Guy and Hoogi, Assaf and Strappini, Francesca and Golan, Tal and Irani, Michal",2019,,,,Advances in Neural Information Processing Systems
Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,2510.25976v1,han2019variational,\cite{han2019variational},Variational autoencoder: An unsupervised model for encoding and decoding fMRI activity in visual cortex,,,True,False,"Han, Kuan and Wen, Haiguang and Shi, Junxing and Lu, Kun-Han and Zhang, Yizhen and Fu, Di and Liu, Zhongming",2019,,,,NeuroImage
Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,2510.25976v1,lin2019dcnn,\cite{lin2019dcnn},DCNN-GAN: Reconstructing Realistic Image from fMRI,https://arxiv.org/abs/1901.07368v1,"Visualizing the perceptual content by analyzing human functional magnetic resonance imaging (fMRI) has been an active research area. However, due to its high dimensionality, complex dimensional structure, and small number of samples available, reconstructing realistic images from fMRI remains challenging. Recently with the development of convolutional neural network (CNN) and generative adversarial network (GAN), mapping multi-voxel fMRI data to complex, realistic images has been made possible. In this paper, we propose a model, DCNN-GAN, by combining a reconstruction network and GAN. We utilize the CNN for hierarchical feature extraction and the DCNN-GAN to reconstruct more realistic images. Extensive experiments have been conducted, showing that our method outperforms previous works, regarding reconstruction quality and computational cost.",True,True,"Lin, Yunfeng and Li, Jiangbei and Wang, Hanjing",2019,,,,
Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,2510.25976v1,mozafari2020reconstructing,\cite{mozafari2020reconstructing},Reconstructing Natural Scenes from fMRI Patterns using BigBiGAN,https://arxiv.org/abs/2001.11761v3,"Decoding and reconstructing images from brain imaging data is a research area of high interest. Recent progress in deep generative neural networks has introduced new opportunities to tackle this problem. Here, we employ a recently proposed large-scale bi-directional generative adversarial network, called BigBiGAN, to decode and reconstruct natural scenes from fMRI patterns. BigBiGAN converts images into a 120-dimensional latent space which encodes class and attribute information together, and can also reconstruct images based on their latent vectors. We computed a linear mapping between fMRI data, acquired over images from 150 different categories of ImageNet, and their corresponding BigBiGAN latent vectors. Then, we applied this mapping to the fMRI activity patterns obtained from 50 new test images from 50 unseen categories in order to retrieve their latent vectors, and reconstruct the corresponding images. Pairwise image decoding from the predicted latent vectors was highly accurate (84%). Moreover, qualitative and quantitative assessments revealed that the resulting image reconstructions were visually plausible, successfully captured many attributes of the original images, and had high perceptual similarity with the original content. This method establishes a new state-of-the-art for fMRI-based natural image reconstruction, and can be flexibly updated to take into account any future improvements in generative models of natural scene images.",True,True,"Mozafari, Milad and Reddy, Leila and VanRullen, Rufin",2020,,,,
Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,2510.25976v1,qiao2020biggan,\cite{qiao2020biggan},BigGAN-based Bayesian reconstruction of natural images from human brain activity,https://arxiv.org/abs/2003.06105v1,"In the visual decoding domain, visually reconstructing presented images given the corresponding human brain activity monitored by functional magnetic resonance imaging (fMRI) is difficult, especially when reconstructing viewed natural images. Visual reconstruction is a conditional image generation on fMRI data and thus generative adversarial network (GAN) for natural image generation is recently introduced for this task. Although GAN-based methods have greatly improved, the fidelity and naturalness of reconstruction are still unsatisfactory due to the small number of fMRI data samples and the instability of GAN training. In this study, we proposed a new GAN-based Bayesian visual reconstruction method (GAN-BVRM) that includes a classifier to decode categories from fMRI data, a pre-trained conditional generator to generate natural images of specified categories, and a set of encoding models and evaluator to evaluate generated images. GAN-BVRM employs the pre-trained generator of the prevailing BigGAN to generate masses of natural images, and selects the images that best matches with the corresponding brain activity through the encoding models as the reconstruction of the image stimuli. In this process, the semantic and detailed contents of reconstruction are controlled by decoded categories and encoding models, respectively. GAN-BVRM used the Bayesian manner to avoid contradiction between naturalness and fidelity from current GAN-based methods and thus can improve the advantages of GAN. Experimental results revealed that GAN-BVRM improves the fidelity and naturalness, that is, the reconstruction is natural and similar to the presented image stimuli.",True,True,"Qiao, Kai and Chen, Jian and Wang, Linyuan and Zhang, Chi and Tong, Li and Yan, Bin",2020,,,,Neuroscience
Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,2510.25976v1,ren2021reconstructing,\cite{ren2021reconstructing},Reconstructing seen image from brain activity by visually-guided cognitive representation and adversarial learning,,,True,False,"Ren, Ziqi and Li, Jie and Xue, Xuetong and Li, Xin and Yang, Fan and Jiao, Zhicheng and Gao, Xinbo",2021,,,,NeuroImage
Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,2510.25976v1,chen2023seeing,\cite{chen2023seeing},Seeing beyond the brain: Conditional diffusion model with sparse masked modeling for vision decoding,,,True,False,"Chen, Zijiao and Qing, Jiaxin and Xiang, Tiange and Yue, Wan Lin and Zhou, Juan Helen",2023,,,,
Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,2510.25976v1,ozcelik2023natural,\cite{ozcelik2023natural},Natural scene reconstruction from fMRI signals using generative latent diffusion,,,True,False,"Ozcelik, Furkan and VanRullen, Rufin",2023,,,,Scientific Reports
Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,2510.25976v1,takagi2023high,\cite{takagi2023high},High-resolution image reconstruction with latent diffusion models from human brain activity,,,True,False,"Takagi, Yu and Nishimoto, Shinji",2023,,,,
Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,2510.25976v1,scotti2024mindeye2,\cite{scotti2024mindeye2},MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data,https://arxiv.org/abs/2403.11207v2,"Reconstructions of visual perception from brain activity have improved tremendously, but the practical utility of such methods has been limited. This is because such models are trained independently per subject where each subject requires dozens of hours of expensive fMRI training data to attain high-quality results. The present work showcases high-quality reconstructions using only 1 hour of fMRI training data. We pretrain our model across 7 subjects and then fine-tune on minimal data from a new subject. Our novel functional alignment procedure linearly maps all brain data to a shared-subject latent space, followed by a shared non-linear mapping to CLIP image space. We then map from CLIP space to pixel space by fine-tuning Stable Diffusion XL to accept CLIP latents as inputs instead of text. This approach improves out-of-subject generalization with limited training data and also attains state-of-the-art image retrieval and reconstruction metrics compared to single-subject approaches. MindEye2 demonstrates how accurate reconstructions of perception are possible from a single visit to the MRI facility. All code is available on GitHub.",True,True,"Scotti, Paul S and Tripathy, Mihir and Villanueva, Cesar Kadir Torrico and Kneeland, Reese and Chen, Tong and Narang, Ashutosh and Santhirasegaran, Charan and Xu, Jonathan and Naselaris, Thomas and Norman, Kenneth A and others",2024,,,,arXiv preprint arXiv:2403.11207
Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,2510.25976v1,gong2025mindtuner,\cite{gong2025mindtuner},MindTuner: Cross-Subject Visual Decoding with Visual Fingerprint and Semantic Correction,https://arxiv.org/abs/2404.12630v2,"Decoding natural visual scenes from brain activity has flourished, with extensive research in single-subject tasks and, however, less in cross-subject tasks. Reconstructing high-quality images in cross-subject tasks is a challenging problem due to profound individual differences between subjects and the scarcity of data annotation. In this work, we proposed MindTuner for cross-subject visual decoding, which achieves high-quality and rich semantic reconstructions using only 1 hour of fMRI training data benefiting from the phenomena of visual fingerprint in the human visual system and a novel fMRI-to-text alignment paradigm. Firstly, we pre-train a multi-subject model among 7 subjects and fine-tune it with scarce data on new subjects, where LoRAs with Skip-LoRAs are utilized to learn the visual fingerprint. Then, we take the image modality as the intermediate pivot modality to achieve fMRI-to-text alignment, which achieves impressive fMRI-to-text retrieval performance and corrects fMRI-to-image reconstruction with fine-tuned semantics. The results of both qualitative and quantitative analyses demonstrate that MindTuner surpasses state-of-the-art cross-subject visual decoding models on the Natural Scenes Dataset (NSD), whether using training data of 1 hour or 40 hours.",True,True,"Gong, Zixuan and Zhang, Qi and Bao, Guangyin and Zhu, Lei and Xu, Rongtao and Liu, Ke and Hu, Liang and Miao, Duoqian",2025,,,,
Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,2510.25976v1,huo2024neuropictor,\cite{huo2024neuropictor},NeuroPictor: Refining fMRI-to-Image Reconstruction via Multi-individual Pretraining and Multi-level Modulation,https://arxiv.org/abs/2403.18211v2,"Recent fMRI-to-image approaches mainly focused on associating fMRI signals with specific conditions of pre-trained diffusion models. These approaches, while producing high-quality images, capture only a limited aspect of the complex information in fMRI signals and offer little detailed control over image creation. In contrast, this paper proposes to directly modulate the generation process of diffusion models using fMRI signals. Our approach, NeuroPictor, divides the fMRI-to-image process into three steps: i) fMRI calibrated-encoding, to tackle multi-individual pre-training for a shared latent space to minimize individual difference and enable the subsequent multi-subject training; ii) fMRI-to-image multi-subject pre-training, perceptually learning to guide diffusion model with high- and low-level conditions across different individuals; iii) fMRI-to-image single-subject refining, similar with step ii but focus on adapting to particular individual. NeuroPictor extracts high-level semantic features from fMRI signals that characterizing the visual stimulus and incrementally fine-tunes the diffusion model with a low-level manipulation network to provide precise structural instructions. By training with about 67,000 fMRI-image pairs from various individuals, our model enjoys superior fMRI-to-image decoding capacity, particularly in the within-subject setting, as evidenced in benchmark datasets. Our code and model are available at https://jingyanghuo.github.io/neuropictor/.",True,True,"Huo, Jingyang and Wang, Yikai and Wang, Yun and Qian, Xuelin and Li, Chong and Fu, Yanwei and Feng, Jianfeng",2024,,,,
Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,2510.25976v1,xia2024umbrae,\cite{xia2024umbrae},UMBRAE: Unified Multimodal Brain Decoding,https://arxiv.org/abs/2404.07202v2,"We address prevailing challenges of the brain-powered research, departing from the observation that the literature hardly recover accurate spatial information and require subject-specific models. To address these challenges, we propose UMBRAE, a unified multimodal decoding of brain signals. First, to extract instance-level conceptual and spatial details from neural signals, we introduce an efficient universal brain encoder for multimodal-brain alignment and recover object descriptions at multiple levels of granularity from subsequent multimodal large language model (MLLM). Second, we introduce a cross-subject training strategy mapping subject-specific features to a common feature space. This allows a model to be trained on multiple subjects without extra resources, even yielding superior results compared to subject-specific models. Further, we demonstrate this supports weakly-supervised adaptation to new subjects, with only a fraction of the total training data. Experiments demonstrate that UMBRAE not only achieves superior results in the newly introduced tasks but also outperforms methods in well established tasks. To assess our method, we construct and share with the community a comprehensive brain understanding benchmark BrainHub. Our code and benchmark are available at https://weihaox.github.io/UMBRAE.",True,True,"Xia, Weihao and de Charette, Raoul and Oztireli, Cengiz and Xue, Jing-Hao",2024,,,,
Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,2510.25976v1,ferrante2024through,\cite{ferrante2024through},Through their eyes: multi-subject Brain Decoding with simple alignment techniques,,,True,False,"Ferrante, Matteo and Boccato, Tommaso and Ozcelik, Furkan and VanRullen, Rufin and Toschi, Nicola",2024,,,,Imaging Neuroscience
Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,2510.25976v1,liu2025see,\cite{liu2025see},See Through Their Minds: Learning Transferable Brain Decoding Models from Cross-Subject fMRI,,,True,False,"Liu, Yulong and Ma, Yongqiang and Zhu, Guibo and Jing, Haodong and Zheng, Nanning",2025,,,,
Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,2510.25976v1,shen2024neuro,\cite{shen2024neuro},Neuro-Vision to Language: Enhancing Brain Recording-based Visual Reconstruction and Language Interaction,https://arxiv.org/abs/2404.19438v4,"Decoding non-invasive brain recordings is pivotal for advancing our understanding of human cognition but faces challenges due to individual differences and complex neural signal representations. Traditional methods often require customized models and extensive trials, lacking interpretability in visual reconstruction tasks. Our framework integrates 3D brain structures with visual semantics using a Vision Transformer 3D. This unified feature extractor efficiently aligns fMRI features with multiple levels of visual embeddings, eliminating the need for subject-specific models and allowing extraction from single-trial data. The extractor consolidates multi-level visual features into one network, simplifying integration with Large Language Models (LLMs). Additionally, we have enhanced the fMRI dataset with diverse fMRI-image-related textual data to support multimodal large model development. Integrating with LLMs enhances decoding capabilities, enabling tasks such as brain captioning, complex reasoning, concept localization, and visual reconstruction. Our approach demonstrates superior performance across these tasks, precisely identifying language-based concepts within brain signals, enhancing interpretability, and providing deeper insights into neural processes. These advances significantly broaden the applicability of non-invasive brain decoding in neuroscience and human-computer interaction, setting the stage for advanced brain-computer interfaces and cognitive models.",True,True,"Shen, Guobin and Zhao, Dongcheng and He, Xiang and Feng, Linghao and Dong, Yiting and Wang, Jihang and Zhang, Qian and Zeng, Yi",2024,,,,Advances in Neural Information Processing Systems
Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,2510.25976v1,xia2024dream,\cite{xia2024dream},DREAM: Visual Decoding from Reversing Human Visual System,https://arxiv.org/abs/2310.02265v2,"In this work we present DREAM, an fMRI-to-image method for reconstructing viewed images from brain activities, grounded on fundamental knowledge of the human visual system. We craft reverse pathways that emulate the hierarchical and parallel nature of how humans perceive the visual world. These tailored pathways are specialized to decipher semantics, color, and depth cues from fMRI data, mirroring the forward pathways from visual stimuli to fMRI recordings. To do so, two components mimic the inverse processes within the human visual system: the Reverse Visual Association Cortex (R-VAC) which reverses pathways of this brain region, extracting semantics from fMRI data; the Reverse Parallel PKM (R-PKM) component simultaneously predicting color and depth from fMRI signals. The experiments indicate that our method outperforms the current state-of-the-art models in terms of the consistency of appearance, structure, and semantics. Code will be made publicly available to facilitate further research in this field.",True,True,"Xia, Weihao and De Charette, Raoul and Oztireli, Cengiz and Xue, Jing-Hao",2024,,,,
Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,2510.25976v1,wang2024unibrain,\cite{wang2024unibrain},UniBrain: A Unified Model for Cross-Subject Brain Decoding,,,True,False,"Wang, Zicheng and Zhao, Zhen and Zhou, Luping and Nachev, Parashkev",2024,,,,arXiv preprint arXiv:2412.19487
Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,2510.25976v1,scotti2023reconstructing,\cite{scotti2023reconstructing},Reconstructing the mind's eye: fmri-to-image with contrastive learning and diffusion priors,,,True,False,"Scotti, Paul and Banerjee, Atmadeep and Goode, Jimmie and Shabalin, Stepan and Nguyen, Alex and Dempster, Aidan and Verlinde, Nathalie and Yundler, Elad and Weisberg, David and Norman, Kenneth and others",2023,,,,Advances in Neural Information Processing Systems
Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,2510.25976v1,lin2022mind,\cite{lin2022mind},Mind Reader: Reconstructing complex images from brain activities,https://arxiv.org/abs/2210.01769v1,"Understanding how the brain encodes external stimuli and how these stimuli can be decoded from the measured brain activities are long-standing and challenging questions in neuroscience. In this paper, we focus on reconstructing the complex image stimuli from fMRI (functional magnetic resonance imaging) signals. Unlike previous works that reconstruct images with single objects or simple shapes, our work aims to reconstruct image stimuli that are rich in semantics, closer to everyday scenes, and can reveal more perspectives. However, data scarcity of fMRI datasets is the main obstacle to applying state-of-the-art deep learning models to this problem. We find that incorporating an additional text modality is beneficial for the reconstruction problem compared to directly translating brain signals to images. Therefore, the modalities involved in our method are: (i) voxel-level fMRI signals, (ii) observed images that trigger the brain signals, and (iii) textual description of the images. To further address data scarcity, we leverage an aligned vision-language latent space pre-trained on massive datasets. Instead of training models from scratch to find a latent space shared by the three modalities, we encode fMRI signals into this pre-aligned latent space. Then, conditioned on embeddings in this space, we reconstruct images with a generative model. The reconstructed images from our pipeline balance both naturalness and fidelity: they are photo-realistic and capture the ground truth image contents well.",True,True,"Lin, Sikun and Sprague, Thomas and Singh, Ambuj K",2022,,,,Advances in Neural Information Processing Systems
Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,2510.25976v1,beliy2024wisdom,\cite{beliy2024wisdom},The Wisdom of a Crowd of Brains: A Universal Brain Encoder,,,True,False,"Beliy, Roman and Wasserman, Navve and Zalcher, Amit and Irani, Michal",2024,,,,arXiv preprint arXiv:2406.12179
Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,2510.25976v1,simonyan2014very,\cite{simonyan2014very},Very deep convolutional networks for large-scale image recognition,,,True,False,"Simonyan, Karen and Zisserman, Andrew",2014,,,,arXiv preprint arXiv:1409.1556
Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer,2510.25976v1,zhang2018perceptual,\cite{zhang2018perceptual},The Unreasonable Effectiveness of Deep Features as a Perceptual Metric,,,True,False,"Richard Zhang and Phillip Isola and Alexei~A. Efros and
               Eli Shechtman and Oliver Wang",2018,,https://arxiv.org/abs/1801.03924,10.1109/CVPR.2018.00068,
A Graphical Method for Identifying Gene Clusters from RNA Sequencing Data,2511.09590v1,cheng_machine_2024,\cite{cheng_machine_2024},Machine learning and related approaches in transcriptomics,,,True,False,"Cheng, Yuning and Xu, Si-Mei and Santucci, Kristina and Lindner, Grace and Janitz, Michael",2024,,https://www.sciencedirect.com/science/article/pii/S0006291X24007617,https://doi.org/10.1016/j.bbrc.2024.150225,Biochemical and Biophysical Research Communications
A Graphical Method for Identifying Gene Clusters from RNA Sequencing Data,2511.09590v1,cascianelli_machine_2020,\cite{cascianelli_machine_2020},Machine learning for {RNA} sequencing-based intrinsic subtyping of breast cancer,,,True,False,"Cascianelli, Silvia and Molineris, Ivan and Isella, Claudio and Masseroli, Marco and Medico, Enzo",2020,,https://www.nature.com/articles/s41598-020-70832-2,10.1038/s41598-020-70832-2,Scientific Reports
A Graphical Method for Identifying Gene Clusters from RNA Sequencing Data,2511.09590v1,taghizadeh_breast_2022,\cite{taghizadeh_breast_2022},Breast cancer prediction with transcriptome profiling using feature selection and machine learning methods,,,True,False,"Taghizadeh, Eskandar and Heydarheydari, Sahel and Saberi, Alihossein and JafarpoorNesheli, Shabnam and Rezaeijo, Seyed Masoud",2022,,https://doi.org/10.1186/s12859-022-04965-8,10.1186/s12859-022-04965-8,BMC Bioinformatics
A Graphical Method for Identifying Gene Clusters from RNA Sequencing Data,2511.09590v1,maurya_transcriptome_2021,\cite{maurya_transcriptome_2021},Transcriptome profiling by combined machine learning and statistical {R} analysis identifies {TMEM236} as a potential novel diagnostic biomarker for colorectal cancer,,,True,False,"Maurya, Neha Shree and Kushwaha, Sandeep and Chawade, Aakash and Mani, Ashutosh",2021,,https://www.nature.com/articles/s41598-021-92692-0,10.1038/s41598-021-92692-0,Scientific Reports
A Graphical Method for Identifying Gene Clusters from RNA Sequencing Data,2511.09590v1,jha_identifying_2022,\cite{jha_identifying_2022},Identifying common transcriptome signatures of cancer by interpreting deep learning models,,,True,False,"Jha, Anupama and Quesnel-Vallières, Mathieu and Wang, David and Thomas-Tikhonenko, Andrei and Lynch, Kristen W. and Barash, Yoseph",2022,,https://doi.org/10.1186/s13059-022-02681-3,10.1186/s13059-022-02681-3,Genome Biology
A Graphical Method for Identifying Gene Clusters from RNA Sequencing Data,2511.09590v1,ma2025integrating,\cite{ma2025integrating},Integrating explainable machine learning and transcriptomics data reveals cell-type specific immune signatures underlying macular degeneration,,,True,False,"Ma, Khang and Nakajima, Hosei and Basak, Nipa and Barman, Arko and Ratnapriya, Rinki",2025,,,,npj Genomic Medicine
A Graphical Method for Identifying Gene Clusters from RNA Sequencing Data,2511.09590v1,wang_rna-seq_2018,\cite{wang_rna-seq_2018},{RNA}-seq assistant: machine learning based methods to identify more transcriptional regulated genes,,,True,False,"Wang, Likai and Xi, Yanpeng and Sung, Sibum and Qiao, Hong",2018,,https://doi.org/10.1186/s12864-018-4932-2,10.1186/s12864-018-4932-2,BMC Genomics
A Graphical Method for Identifying Gene Clusters from RNA Sequencing Data,2511.09590v1,rychel_machine_2020,\cite{rychel_machine_2020},Machine learning uncovers independently regulated modules in the {Bacillus} subtilis transcriptome,,,True,False,"Rychel, Kevin and Sastry, Anand V. and Palsson, Bernhard O.",2020,,https://www.nature.com/articles/s41467-020-20153-9,10.1038/s41467-020-20153-9,Nature Communications
A Graphical Method for Identifying Gene Clusters from RNA Sequencing Data,2511.09590v1,mochida_statistical_2018,\cite{mochida_statistical_2018},Statistical and {Machine} {Learning} {Approaches} to {Predict} {Gene} {Regulatory} {Networks} {From} {Transcriptome} {Datasets},,,True,False,"Mochida, Keiichi and Koda, Satoru and Inoue, Komaki and Nishii, Ryuei",2018,,https://www.frontiersin.org/journals/plant-science/articles/10.3389/fpls.2018.01770/full,10.3389/fpls.2018.01770,Frontiers in Plant Science
A Graphical Method for Identifying Gene Clusters from RNA Sequencing Data,2511.09590v1,petegrosso_machine_2019,\cite{petegrosso_machine_2019},Machine learning and statistical methods for clustering single-cell {RNA}-sequencing data,,,True,False,"Petegrosso, Raphael and Li, Zhuliu and Kuang, Rui",2019,,https://doi.org/10.1093/bib/bbz063,10.1093/bib/bbz063,Briefings in Bioinformatics
A Graphical Method for Identifying Gene Clusters from RNA Sequencing Data,2511.09590v1,torgano_rna_2025,\cite{torgano_rna_2025},{RNA} knowledge-graph analysis through homogeneous embedding methods,,,True,False,"Torgano, Francesco and Soto Gomez, Mauricio and Zignani, Matteo and Gliozzo, Jessica and Cavalleri, Emanuele and Mesiti, Marco and Casiraghi, Elena and Valentini, Giorgio",2025,,https://doi.org/10.1093/bioadv/vbaf109,10.1093/bioadv/vbaf109,Bioinformatics Advances
A Graphical Method for Identifying Gene Clusters from RNA Sequencing Data,2511.09590v1,grover2016node2vec,\cite{grover2016node2vec},node2vec: Scalable feature learning for networks,,,True,False,"Grover, Aditya and Leskovec, Jure",2016,,,,
A Graphical Method for Identifying Gene Clusters from RNA Sequencing Data,2511.09590v1,wang_single-cell_2021,\cite{wang_single-cell_2021},Single-{Cell} {Transcriptome} {Analysis} in {Melanoma} {Using} {Network} {Embedding},,,True,False,"Wang, Liming and Liu, Fangfang and Du, Longting and Qin, Guimin",2021,,https://www.frontiersin.org/journals/genetics/articles/10.3389/fgene.2021.700036/full,10.3389/fgene.2021.700036,Frontiers in Genetics
EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation,2510.25132v1,smi1,\cite{smi1},Reshaping substrate-binding pocket of leucine dehydrogenase for bidirectionally accessing structurally diverse substrates,,,True,False,"Wu, Tao and Wang, Yinmiao and Zhang, Ningxin and Yin, Dejing and Xu, Yan and Nie, Yao and Mu, Xiaoqing",2022,,,,ACS Catalysis
EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation,2510.25132v1,smi2,\cite{smi2},Mechanism-guided tunnel engineering to increase the efficiency of a flavin-dependent halogenase,,,True,False,"Prakinee, Kridsadakorn and Phintha, Aisaraphon and Visitsatthawong, Surawit and Lawan, Narin and Sucharitakul, Jeerus and Kantiwiriyawanitch, Chadaporn and Damborsky, Jiri and Chitnumsub, Penchit and Van Pee, Karl-Heinz and Chaiyen, Pimchai",2022,,,,Nature Catalysis
EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation,2510.25132v1,smi3,\cite{smi3},Computational design of multisubstrate enzyme specificity,,,True,False,"St-Jacques, Antony D and Eyahpaise, Marie-Eve C and Chica, Roberto A",2019,,,,Acs Catalysis
EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation,2510.25132v1,rat1,\cite{rat1},The mutagenesis of a single site for enhancing or reversing the enantio-or regiopreference of cyclohexanone monooxygenases,,,True,False,"Hu, Yujing and Xu, Weihua and Hui, Chenggong and Xu, Jian and Huang, Meilan and Lin, Xianfu and Wu, Qi",2020,,,,Chemical Communications
EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation,2510.25132v1,rat2,\cite{rat2},Engineered cytochrome P450 for direct arylalkene-to-ketone oxidation via highly reactive carbocation intermediates,,,True,False,"Gergel, Sebastian and Soler, Jordi and Klein, Alina and Sch{\""u}lke, Kai H and Hauer, Bernhard and Garcia-Borr{\`a}s, Marc and Hammer, Stephan C",2023,,,,Nature Catalysis
EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation,2510.25132v1,rat3,\cite{rat3},Loop dynamics and the evolution of enzyme activity,,,True,False,"Corbella, Marina and Pinto, Gaspar P and Kamerlin, Shina CL",2023,,,,Nature Reviews Chemistry
EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation,2510.25132v1,enzygen,\cite{enzygen},Generative Enzyme Design Guided by Functionally Important Sites and Small-Molecule Substrates,https://arxiv.org/abs/2405.08205v3,"Enzymes are genetically encoded biocatalysts capable of accelerating chemical reactions. How can we automatically design functional enzymes? In this paper, we propose EnzyGen, an approach to learn a unified model to design enzymes across all functional families. Our key idea is to generate an enzyme's amino acid sequence and their three-dimensional (3D) coordinates based on functionally important sites and substrates corresponding to a desired catalytic function. These sites are automatically mined from enzyme databases. EnzyGen consists of a novel interleaving network of attention and neighborhood equivariant layers, which captures both long-range correlation in an entire protein sequence and local influence from nearest amino acids in 3D space. To learn the generative model, we devise a joint training objective, including a sequence generation loss, a position prediction loss and an enzyme-substrate interaction loss. We further construct EnzyBench, a dataset with 3157 enzyme families, covering all available enzymes within the protein data bank (PDB). Experimental results show that our EnzyGen consistently achieves the best performance across all 323 testing families, surpassing the best baseline by 10.79% in terms of substrate binding affinity. These findings demonstrate EnzyGen's superior capability in designing well-folded and effective enzymes binding to specific substrates with high affinities.",True,True,"Song, Zhenqiao and Zhao, Yunlong and Shi, Wenxian and Jin, Wengong and Yang, Yang and Li, Lei",2024,,,,arXiv preprint arXiv:2405.08205
EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation,2510.25132v1,smi4,\cite{smi4},Reshaping the binding pocket of lysine hydroxylase for enhanced activity,,,True,False,"Wang, Fenghua and Zhu, Menglu and Song, Zhan and Li, Chao and Wang, Yuying and Zhu, Zhangliang and Sun, Dengyue and Lu, Fuping and Qin, Hui-Min",2020,,,,ACS Catalysis
EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation,2510.25132v1,smi5,\cite{smi5},Computational redesign of the substrate binding pocket of glutamate dehydrogenase for efficient synthesis of noncanonical L-amino acids,,,True,False,"Wang, Ziyuan and Zhou, Haisheng and Yu, Haoran and Pu, Zhongji and Xu, Jinling and Zhang, Hongyu and Wu, Jianping and Yang, Lirong",2022,,,,ACS Catalysis
EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation,2510.25132v1,smi6,\cite{smi6},"Computationally guided bioengineering of the active site, substrate access pathway, and water channels of thermostable cytochrome P450, CYP175A1, for catalyzing the alkane hydroxylation reaction",,,True,False,"Taher, Mohd and Dubey, Kshatresh Dutta and Mazumdar, Shyamalava",2023,,,,Chemical Science
EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation,2510.25132v1,rat4,\cite{rat4},Enhanced thermostability and catalytic activity of Streptomyces mobaraenesis transglutaminase by rationally engineering its flexible regions,,,True,False,"Yang, Penghui and Wang, Xinglong and Ye, Jiacai and Rao, Shengqi and Zhou, Jingwen and Du, Guocheng and Liu, Song",2023,,,,Journal of Agricultural and Food Chemistry
EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation,2510.25132v1,rat5,\cite{rat5},Second-shell residues contribute to catalysis by predominately preorganizing the apo state in PafA,,,True,False,"Deng, Jiahua and Cui, Qiang",2023,,,,Journal of the American Chemical Society
EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation,2510.25132v1,rat6,\cite{rat6},Regio-and stereoselectivity in the CYP450 BM3-catalyzed hydroxylation of complex terpenoids: a QM/MM study,,,True,False,"Hui, Chenggong and Singh, Warispreet and Quinn, Derek and Li, Chun and Moody, Thomas S and Huang, Meilan",2020,,,,Physical Chemistry Chemical Physics
EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation,2510.25132v1,ahern2025atom,\cite{ahern2025atom},Atom level enzyme active site scaffolding using RFdiffusion2,,,True,False,"Ahern, Woody and Yim, Jason and Tischer, Doug and Salike, Saman and Woodbury, Seth and Kim, Donghyo and Kalvet, Indrek and Kipnis, Yakov and Coventry, Brian and Altae-Tran, Han and others",2025,,,,bioRxiv
EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation,2510.25132v1,silva2016motif,\cite{silva2016motif},Motif-driven design of protein--protein interfaces,,,True,False,"Silva, Daniel-Adriano and Correia, Bruno E and Procko, Erik",2016,,,,Computational Design of Ligand Binding Proteins
EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation,2510.25132v1,yang2021bottom,\cite{yang2021bottom},Bottom-up de novo design of functional proteins with complex structural features,,,True,False,"Yang, Che and Sesterhenn, Fabian and Bonet, Jaume and van Aalen, Eva A and Scheller, Leo and Abriata, Luciano A and Cramer, Johannes T and Wen, Xiaolin and Rosset, St{\'e}phane and Georgeon, Sandrine and others",2021,,,,Nature Chemical Biology
EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation,2510.25132v1,sesterhenn2020novo,\cite{sesterhenn2020novo},De novo protein design enables the precise induction of RSV-neutralizing antibodies,,,True,False,"Sesterhenn, Fabian and Yang, Che and Bonet, Jaume and Cramer, Johannes T and Wen, Xiaolin and Wang, Yimeng and Chiang, Chi-I and Abriata, Luciano A and Kucharska, Iga and Castoro, Giacomo and others",2020,,,,Science
EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation,2510.25132v1,linsky2020novo,\cite{linsky2020novo},De novo design of potent and resilient hACE2 decoys to neutralize SARS-CoV-2,,,True,False,"Linsky, Thomas W and Vergara, Renan and Codina, Nuria and Nelson, Jorgen W and Walker, Matthew J and Su, Wen and Barnes, Christopher O and Hsiang, Tien-Ying and Esser-Nobis, Katharina and Yu, Kevin and others",2020,,,,Science
EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation,2510.25132v1,rfdiffusion,\cite{rfdiffusion},De novo design of protein structure and function with RFdiffusion,,,True,False,"Watson, Joseph L and Juergens, David and Bennett, Nathaniel R and Trippe, Brian L and Yim, Jason and Eisenach, Helen E and Ahern, Woody and Borst, Andrew J and Ragotte, Robert J and Milles, Lukas F and others",2023,,,,Nature
EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation,2510.25132v1,frameflow,\cite{frameflow},Fast protein backbone generation with SE (3) flow matching,,,True,False,"Yim, Jason and Campbell, Andrew and Foong, Andrew YK and Gastegger, Michael and Jim{\'e}nez-Luna, Jos{\'e} and Lewis, Sarah and Satorras, Victor Garcia and Veeling, Bastiaan S and Barzilay, Regina and Jaakkola, Tommi and others",2023,,,,arXiv preprint arXiv:2310.05297
EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation,2510.25132v1,mf1,\cite{mf1},Protein Language Model Supervised Scalable Approach for Diverse and Designable Protein Motif-Scaffolding with GPDL,,,True,False,"Zhang, Bo and Liu, Kexin and Zheng, Zhuoqi and Zhu, Junjie and Li, Zhengxin and Liu, Yunfeiyang and Mu, Junxi and Wei, Ting and Chen, Hai-Feng",2023,,,,bioRxiv
EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation,2510.25132v1,mf2,\cite{mf2},Floating Anchor Diffusion Model for Multi-motif Scaffolding,,,True,False,"Liu, Ke and Mao, Weian and Shen, Shuaike and Jiao, Xiaoran and Sun, Zheng and Chen, Hao and Shen, Chunhua",2024,,,,arXiv preprint arXiv:2406.03141
EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation,2510.25132v1,mf3,\cite{mf3},A framework for conditional diffusion modelling with applications in motif scaffolding for protein design,,,True,False,"Didi, Kieran and Vargas, Francisco and Mathis, Simon V and Dutordoir, Vincent and Mathieu, Emile and Komorowska, Urszula J and Lio, Pietro",2023,,,,arXiv preprint arXiv:2312.09236
Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity,2510.18037v2,curto2009simple,\cite{curto2009simple},A simple model of cortical dynamics explains variability and state dependence of sensory responses in urethane-anesthetized auditory cortex,,,True,False,"Curto, Carina and Sakata, Shuzo and Marguet, Stephan and Itskov, Vladimir and Harris, Kenneth D",2009,,,,Journal of neuroscience
Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity,2510.18037v2,glaser2020recurrent,\cite{glaser2020recurrent},Recurrent switching dynamical systems models for multiple interacting neural populations,,,True,False,"Glaser, Joshua and Whiteway, Matthew and Cunningham, John P and Paninski, Liam and Linderman, Scott",2020,,,,Advances in neural information processing systems
Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity,2510.18037v2,sani2021modeling,\cite{sani2021modeling},Modeling behaviorally relevant neural dynamics enabled by preferential subspace identification,,,True,False,"Sani, Omid G and Abbaspourazad, Hamidreza and Wong, Yan T and Pesaran, Bijan and Shanechi, Maryam M",2021,,,,Nature neuroscience
Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity,2510.18037v2,azabou2023unified,\cite{azabou2023unified},"A unified, scalable framework for neural population decoding",,,True,False,"Azabou, Mehdi and Arora, Vinam and Ganesh, Venkataramana and Mao, Ximeng and Nachimuthu, Santosh and Mendelson, Michael and Richards, Blake and Perich, Matthew and Lajoie, Guillaume and Dyer, Eva",2023,,,,Advances in Neural Information Processing Systems
Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity,2510.18037v2,ryoo2025generalizable,\cite{ryoo2025generalizable},"Generalizable, real-time neural decoding with hybrid state-space models",https://arxiv.org/abs/2506.05320v2,"Real-time decoding of neural activity is central to neuroscience and neurotechnology applications, from closed-loop experiments to brain-computer interfaces, where models are subject to strict latency constraints. Traditional methods, including simple recurrent neural networks, are fast and lightweight but often struggle to generalize to unseen data. In contrast, recent Transformer-based approaches leverage large-scale pretraining for strong generalization performance, but typically have much larger computational requirements and are not always suitable for low-resource or real-time settings. To address these shortcomings, we present POSSM, a novel hybrid architecture that combines individual spike tokenization via a cross-attention module with a recurrent state-space model (SSM) backbone to enable (1) fast and causal online prediction on neural activity and (2) efficient generalization to new sessions, individuals, and tasks through multi-dataset pretraining. We evaluate POSSM's decoding performance and inference speed on intracortical decoding of monkey motor tasks, and show that it extends to clinical applications, namely handwriting and speech decoding in human subjects. Notably, we demonstrate that pretraining on monkey motor-cortical recordings improves decoding performance on the human handwriting task, highlighting the exciting potential for cross-species transfer. In all of these tasks, we find that POSSM achieves decoding accuracy comparable to state-of-the-art Transformers, at a fraction of the inference cost (up to 9x faster on GPU). These results suggest that hybrid SSMs are a promising approach to bridging the gap between accuracy, inference speed, and generalization when training neural decoders for real-time, closed-loop applications.",True,True,"Ryoo, Avery Hee-Woon and Krishna, Nanda H and Mao, Ximeng and Azabou, Mehdi and Dyer, Eva L and Perich, Matthew G and Lajoie, Guillaume",2025,,,,arXiv preprint arXiv:2506.05320
Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity,2510.18037v2,azabou2025multisession,\cite{azabou2025multisession},"Multi-session, multi-task neural decoding from distinct cell-types and brain regions",,,True,False,Mehdi Azabou and Krystal Xuejing Pan and Vinam Arora and Ian Jarratt Knight and Eva L Dyer and Blake Aaron Richards,2025,,https://openreview.net/forum?id=IuU0wcO0mo,,
Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity,2510.18037v2,sani2024dissociative,\cite{sani2024dissociative},Dissociative and prioritized modeling of behaviorally relevant neural dynamics using recurrent neural networks,,,True,False,"Sani, Omid G and Pesaran, Bijan and Shanechi, Maryam M",2024,,,,Nature neuroscience
Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity,2510.18037v2,vahidi2025braid,\cite{vahidi2025braid},BRAID: Input-Driven Nonlinear Dynamical Modeling of Neural-Behavioral Data,https://arxiv.org/abs/2509.18627v1,"Neural populations exhibit complex recurrent structures that drive behavior, while continuously receiving and integrating external inputs from sensory stimuli, upstream regions, and neurostimulation. However, neural populations are often modeled as autonomous dynamical systems, with little consideration given to the influence of external inputs that shape the population activity and behavioral outcomes. Here, we introduce BRAID, a deep learning framework that models nonlinear neural dynamics underlying behavior while explicitly incorporating any measured external inputs. Our method disentangles intrinsic recurrent neural population dynamics from the effects of inputs by including a forecasting objective within input-driven recurrent neural networks. BRAID further prioritizes the learning of intrinsic dynamics that are related to a behavior of interest by using a multi-stage optimization scheme. We validate BRAID with nonlinear simulations, showing that it can accurately learn the intrinsic dynamics shared between neural and behavioral modalities. We then apply BRAID to motor cortical activity recorded during a motor task and demonstrate that our method more accurately fits the neural-behavioral data by incorporating measured sensory stimuli into the model and improves the forecasting of neural-behavioral data compared with various baseline methods, whether input-driven or not.",True,True,Parsa Vahidi and Omid G. Sani and Maryam Shanechi,2025,,https://openreview.net/forum?id=3usdM1AuI3,,
Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity,2510.18037v2,pandarinath2018inferring,\cite{pandarinath2018inferring},Inferring single-trial neural population dynamics using sequential auto-encoders,,,True,False,"Pandarinath, Chethan and O’Shea, Daniel J and Collins, Jasmine and Jozefowicz, Rafal and Stavisky, Sergey D and Kao, Jonathan C and Trautmann, Eric M and Kaufman, Matthew T and Ryu, Stephen I and Hochberg, Leigh R and others",2018,,,,Nature methods
Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity,2510.18037v2,pei2021neural,\cite{pei2021neural},Neural Latents Benchmark '21: Evaluating latent variable models of neural population activity,https://arxiv.org/abs/2109.04463v4,"Advances in neural recording present increasing opportunities to study neural activity in unprecedented detail. Latent variable models (LVMs) are promising tools for analyzing this rich activity across diverse neural systems and behaviors, as LVMs do not depend on known relationships between the activity and external experimental variables. However, progress with LVMs for neuronal population activity is currently impeded by a lack of standardization, resulting in methods being developed and compared in an ad hoc manner. To coordinate these modeling efforts, we introduce a benchmark suite for latent variable modeling of neural population activity. We curate four datasets of neural spiking activity from cognitive, sensory, and motor areas to promote models that apply to the wide variety of activity seen across these areas. We identify unsupervised evaluation as a common framework for evaluating models across datasets, and apply several baselines that demonstrate benchmark diversity. We release this benchmark through EvalAI. http://neurallatents.github.io",True,True,"Pei, Felix and Ye, Joel and Zoltowski, David and Wu, Anqi and Chowdhury, Raeed H and Sohn, Hansem and O'Doherty, Joseph E and Shenoy, Krishna V and Kaufman, Matthew T and Churchland, Mark and others",2021,,,,arXiv preprint arXiv:2109.04463
Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity,2510.18037v2,zhang2024towards,\cite{zhang2024towards},"Towards a"" universal translator"" for neural dynamics at single-cell, single-spike resolution",,,True,False,"Zhang, Yizi and Wang, Yanchen and Jim{\'e}nez-Benet{\'o}, Donato and Wang, Zixuan and Azabou, Mehdi and Richards, Blake and Tung, Renee and Winter, Olivier and Dyer, Eva and Paninski, Liam and others",2024,,,,Advances in Neural Information Processing Systems
Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity,2510.18037v2,li2023amag,\cite{li2023amag},"Amag: Additive, multiplicative and adaptive graph neural network for forecasting neuron activity",,,True,False,"Li, Jingyuan and Scholl, Leo and Le, Trung and Rajeswaran, Pavithra and Orsborn, Amy and Shlizerman, Eli",2023,,,,Advances in Neural Information Processing Systems
Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity,2510.18037v2,wagenmaker2024active,\cite{wagenmaker2024active},Active learning of neural population dynamics using two-photon holographic optogenetics,https://arxiv.org/abs/2412.02529v4,"Recent advances in techniques for monitoring and perturbing neural populations have greatly enhanced our ability to study circuits in the brain. In particular, two-photon holographic optogenetics now enables precise photostimulation of experimenter-specified groups of individual neurons, while simultaneous two-photon calcium imaging enables the measurement of ongoing and induced activity across the neural population. Despite the enormous space of potential photostimulation patterns and the time-consuming nature of photostimulation experiments, very little algorithmic work has been done to determine the most effective photostimulation patterns for identifying the neural population dynamics. Here, we develop methods to efficiently select which neurons to stimulate such that the resulting neural responses will best inform a dynamical model of the neural population activity. Using neural population responses to photostimulation in mouse motor cortex, we demonstrate the efficacy of a low-rank linear dynamical systems model, and develop an active learning procedure which takes advantage of low-rank structure to determine informative photostimulation patterns. We demonstrate our approach on both real and synthetic data, obtaining in some cases as much as a two-fold reduction in the amount of data required to reach a given predictive power. Our active stimulation design method is based on a novel active learning procedure for low-rank regression, which may be of independent interest.",True,True,"Wagenmaker, Andrew and Mi, Lu and Rozsa, Marton and Bull, Matthew and Svoboda, Karel and Daie, Kayvon and Golub, Matthew and Jamieson, Kevin G",2024,,,,Advances in Neural Information Processing Systems
Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity,2510.18037v2,antoniades2024neuroformer,\cite{antoniades2024neuroformer},Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data,,,True,False,Antonis Antoniades and Yiyi Yu and Joe S Canzano and William Yang Wang and Spencer Smith,2024,,https://openreview.net/forum?id=W8S8SxS9Ng,,
Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity,2510.18037v2,filipe2025one,\cite{filipe2025one},One Model to Train Them All: A Unified Diffusion Framework for Multi-Context Neural Population Forecasting,,,True,False,A. Carolina Filipe and Il Memming Park,2025,,https://openreview.net/forum?id=R9feGbYRG7,,
Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity,2510.18037v2,immer2025forecasting,\cite{immer2025forecasting},Forecasting Whole-Brain Neuronal Activity from Volumetric Video,https://arxiv.org/abs/2503.00073v1,"Large-scale neuronal activity recordings with fluorescent calcium indicators are increasingly common, yielding high-resolution 2D or 3D videos. Traditional analysis pipelines reduce this data to 1D traces by segmenting regions of interest, leading to inevitable information loss. Inspired by the success of deep learning on minimally processed data in other domains, we investigate the potential of forecasting neuronal activity directly from volumetric videos. To capture long-range dependencies in high-resolution volumetric whole-brain recordings, we design a model with large receptive fields, which allow it to integrate information from distant regions within the brain. We explore the effects of pre-training and perform extensive model selection, analyzing spatio-temporal trade-offs for generating accurate forecasts. Our model outperforms trace-based forecasting approaches on ZAPBench, a recently proposed benchmark on whole-brain activity prediction in zebrafish, demonstrating the advantages of preserving the spatial structure of neuronal activity.",True,True,"Immer, Alexander and Lueckmann, Jan-Matthis and Chen, Alex Bo-Yuan and Li, Peter H and Petkova, Mariela D and Iyer, Nirmala A and Dev, Aparna and Ihrke, Gudrun and Park, Woohyun and Petruncio, Alyson and others",2025,,,,arXiv preprint arXiv:2503.00073
Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity,2510.18037v2,duan2025poco,\cite{duan2025poco},POCO: Scalable Neural Forecasting through Population Conditioning,https://arxiv.org/abs/2506.14957v2,"Predicting future neural activity is a core challenge in modeling brain dynamics, with applications ranging from scientific investigation to closed-loop neurotechnology. While recent models of population activity emphasize interpretability and behavioral decoding, neural forecasting-particularly across multi-session, spontaneous recordings-remains underexplored. We introduce POCO, a unified forecasting model that combines a lightweight univariate forecaster with a population-level encoder to capture both neuron-specific and brain-wide dynamics. Trained across five calcium imaging datasets spanning zebrafish, mice, and C. elegans, POCO achieves state-of-the-art accuracy at cellular resolution in spontaneous behaviors. After pre-training, POCO rapidly adapts to new recordings with minimal fine-tuning. Notably, POCO's learned unit embeddings recover biologically meaningful structure-such as brain region clustering-without any anatomical labels. Our comprehensive analysis reveals several key factors influencing performance, including context length, session diversity, and preprocessing. Together, these results position POCO as a scalable and adaptable approach for cross-session neural forecasting and offer actionable insights for future model design. By enabling accurate, generalizable forecasting models of neural dynamics across individuals and species, POCO lays the groundwork for adaptive neurotechnologies and large-scale efforts for neural foundation models. Code is available at https://github.com/yuvenduan/POCO.",True,True,"Duan, Yu and Chaudhry, Hamza Tahir and Ahrens, Misha B and Harvey, Christopher D and Perich, Matthew G and Deisseroth, Karl and Rajan, Kanaka",2025,,,,arXiv preprint arXiv:2506.14957
Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity,2510.18037v2,ansari2024chronos,\cite{ansari2024chronos},Chronos: Learning the language of time series,,,True,False,"Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Sundar and Arango, Sebastian Pineda and Kapoor, Shubham and others",2024,,,,arXiv preprint arXiv:2403.07815
Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity,2510.18037v2,woo2024unified,\cite{woo2024unified},Unified Training of Universal Time Series Forecasting Transformers,https://arxiv.org/abs/2402.02592v2,"Deep learning for time series forecasting has traditionally operated within a one-model-per-dataset framework, limiting its potential to leverage the game-changing impact of large pre-trained models. The concept of universal forecasting, emerging from pre-training on a vast collection of time series datasets, envisions a single Large Time Series Model capable of addressing diverse downstream forecasting tasks. However, constructing such a model poses unique challenges specific to time series data: i) cross-frequency learning, ii) accommodating an arbitrary number of variates for multivariate time series, and iii) addressing the varying distributional properties inherent in large-scale data. To address these challenges, we present novel enhancements to the conventional time series Transformer architecture, resulting in our proposed Masked Encoder-based Universal Time Series Forecasting Transformer (Moirai). Trained on our newly introduced Large-scale Open Time Series Archive (LOTSA) featuring over 27B observations across nine domains, Moirai achieves competitive or superior performance as a zero-shot forecaster when compared to full-shot models. Code, data, and model weights can be found at https://github.com/SalesforceAIResearch/uni2ts.",True,True,"Woo, Gerald and Liu, Chenghao and Kumar, Akshat and Xiong, Caiming and Savarese, Silvio and Sahoo, Doyen",2024,,,,
Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity,2510.18037v2,faisal2008noise,\cite{faisal2008noise},Noise in the nervous system,,,True,False,"Faisal, A Aldo and Selen, Luc PJ and Wolpert, Daniel M",2008,,,,Nature reviews neuroscience
Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity,2510.18037v2,gneiting2014probabilistic,\cite{gneiting2014probabilistic},Probabilistic forecasting,,,True,False,"Gneiting, Tilmann and Katzfuss, Matthias",2014,,,,Annual Review of Statistics and Its Application
A Foundation Chemical Language Model for Comprehensive Fragment-Based Drug Discovery,2509.19586v1,bagal2021molgpt,\cite{bagal2021molgpt},MolGPT: molecular generation using a transformer-decoder model,,,True,False,"Bagal, Viraj and Aggarwal, Rishal and Vinod, PK and Priyakumar, U Deva",2021,,,,Journal of chemical information and modeling
A Foundation Chemical Language Model for Comprehensive Fragment-Based Drug Discovery,2509.19586v1,chithrananda2020chemberta,\cite{chithrananda2020chemberta},ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction,https://arxiv.org/abs/2010.09885v2,"GNNs and chemical fingerprints are the predominant approaches to representing molecules for property prediction. However, in NLP, transformers have become the de-facto standard for representation learning thanks to their strong downstream task transfer. In parallel, the software ecosystem around transformers is maturing rapidly, with libraries like HuggingFace and BertViz enabling streamlined training and introspection. In this work, we make one of the first attempts to systematically evaluate transformers on molecular property prediction tasks via our ChemBERTa model. ChemBERTa scales well with pretraining dataset size, offering competitive downstream performance on MoleculeNet and useful attention-based visualization modalities. Our results suggest that transformers offer a promising avenue of future work for molecular representation learning and property prediction. To facilitate these efforts, we release a curated dataset of 77M SMILES from PubChem suitable for large-scale self-supervised pretraining.",True,True,"Chithrananda, Seyone and Grand, Gabriel and Ramsundar, Bharath",2020,,,,arXiv preprint arXiv:2010.09885
A Foundation Chemical Language Model for Comprehensive Fragment-Based Drug Discovery,2509.19586v1,yue2024unlocking,\cite{yue2024unlocking},Unlocking comprehensive molecular design across all scenarios with large language model and unordered chemical language,,,True,False,"Yue, Jie and Peng, Bingxin and Chen, Yu and Jin, Jieyu and Zhao, Xinda and Shen, Chao and Ji, Xiangyang and Hsieh, Chang-Yu and Song, Jianfei and Hou, Tingjun and others",2024,,,,Chemical Science
A Foundation Chemical Language Model for Comprehensive Fragment-Based Drug Discovery,2509.19586v1,skinnider2024invalid,\cite{skinnider2024invalid},Invalid SMILES are beneficial rather than detrimental to chemical language models,,,True,False,"Skinnider, Michael A",2024,,,,Nature Machine Intelligence
A Foundation Chemical Language Model for Comprehensive Fragment-Based Drug Discovery,2509.19586v1,xu2025fragment,\cite{xu2025fragment},"Fragment-based drug design: From then until now, and toward the future",,,True,False,"Xu, Weijun and Kang, Congbao",2025,,,,Journal of Medicinal Chemistry
A Foundation Chemical Language Model for Comprehensive Fragment-Based Drug Discovery,2509.19586v1,keseru2016design,\cite{keseru2016design},Design principles for fragment libraries: maximizing the value of learnings from pharma fragment-based drug discovery (FBDD) programs for use in academia,,,True,False,"Keseru, Gyorgy M and Erlanson, Daniel A and Ferenczy, Gyorgy G and Hann, Michael M and Murray, Christopher W and Pickett, Stephen D",2016,,,,Journal of medicinal chemistry
A Foundation Chemical Language Model for Comprehensive Fragment-Based Drug Discovery,2509.19586v1,tingle2023zinc,\cite{tingle2023zinc},ZINC-22--A free multi-billion-scale database of tangible compounds for ligand discovery,,,True,False,"Tingle, Benjamin I and Tang, Khanh G and Castanon, Mar and Gutierrez, John J and Khurelbaatar, Munkhzul and Dandarchuluun, Chinzorig and Moroz, Yurii S and Irwin, John J",2023,,,,Journal of chemical information and modeling
A Foundation Chemical Language Model for Comprehensive Fragment-Based Drug Discovery,2509.19586v1,bian2018computational,\cite{bian2018computational},"Computational fragment-based drug design: current trends, strategies, and applications",,,True,False,"Bian, Yuemin and Xie, Xiang-Qun",2018,,,,The AAPS journal
A Foundation Chemical Language Model for Comprehensive Fragment-Based Drug Discovery,2509.19586v1,powers2023geometric,\cite{powers2023geometric},Geometric deep learning for structure-based ligand design,,,True,False,"Powers, Alexander S and Yu, Helen H and Suriana, Patricia and Koodli, Rohan V and Lu, Tianyu and Paggi, Joseph M and Dror, Ron O",2023,,,,ACS Central Science
A Foundation Chemical Language Model for Comprehensive Fragment-Based Drug Discovery,2509.19586v1,podda2020deep,\cite{podda2020deep},A deep generative model for fragment-based molecule generation,,,True,False,"Podda, Marco and Bacciu, Davide and Micheli, Alessio",2020,,,,
A Foundation Chemical Language Model for Comprehensive Fragment-Based Drug Discovery,2509.19586v1,chen2021fragment,\cite{chen2021fragment},Fragment-based sequential translation for molecular optimization,,,True,False,"Chen, Benson and Fu, Xiang and Barzilay, Regina and Jaakkola, Tommi",2021,,,,arXiv preprint arXiv:2111.01009
A Foundation Chemical Language Model for Comprehensive Fragment-Based Drug Discovery,2509.19586v1,lee2024molecule,\cite{lee2024molecule},Molecule generation with fragment retrieval augmentation,,,True,False,"Lee, Seul and Kreis, Karsten and Veccham, Srimukh and Liu, Meng and Reidenbach, Danny and Paliwal, Saee and Vahdat, Arash and Nie, Weili",2024,,,,Advances in Neural Information Processing Systems
A Foundation Chemical Language Model for Comprehensive Fragment-Based Drug Discovery,2509.19586v1,fu2024fragment,\cite{fu2024fragment},Fragment and geometry aware tokenization of molecules for structure-based drug design using language models,,,True,False,"Fu, Cong and Li, Xiner and Olson, Blake and Ji, Heng and Ji, Shuiwang",2024,,,,arXiv preprint arXiv:2408.09730
In Silico Mapping of Visual Categorical Selectivity Across the Whole Brain,2510.21142v1,ferrante2023brain,\cite{ferrante2023brain},Through their eyes: multi-subject Brain Decoding with simple alignment techniques,https://arxiv.org/abs/2309.00627v1,"Previous brain decoding research primarily involves single-subject studies, reconstructing stimuli via fMRI activity from the same subject. Our study aims to introduce a generalization technique for cross-subject brain decoding, facilitated by exploring data alignment methods. We utilized the NSD dataset, a comprehensive 7T fMRI vision experiment involving multiple subjects exposed to 9841 images, 982 of which were viewed by all. Our approach involved training a decoding model on one subject, aligning others' data to this space, and testing the decoding on the second subject. We compared ridge regression, hyper alignment, and anatomical alignment techniques for fMRI data alignment. We established that cross-subject brain decoding is feasible, even using around 10% of the total data, or 982 common images, with comparable performance to single-subject decoding. Ridge regression was the best method for functional alignment. Through subject alignment, we achieved superior brain decoding and a potential 90% reduction in scan time. This could pave the way for more efficient experiments and further advancements in the field, typically requiring an exorbitant 20-hour scan time per subject.",True,True,,,,,,
In Silico Mapping of Visual Categorical Selectivity Across the Whole Brain,2510.21142v1,matsuyama_lavca_2025,\cite{matsuyama_lavca_2025},LaVCa: LLM-assisted Visual Cortex Captioning,https://arxiv.org/abs/2502.13606v1,"Understanding the property of neural populations (or voxels) in the human brain can advance our comprehension of human perceptual and cognitive processing capabilities and contribute to developing brain-inspired computer models. Recent encoding models using deep neural networks (DNNs) have successfully predicted voxel-wise activity. However, interpreting the properties that explain voxel responses remains challenging because of the black-box nature of DNNs. As a solution, we propose LLM-assisted Visual Cortex Captioning (LaVCa), a data-driven approach that uses large language models (LLMs) to generate natural-language captions for images to which voxels are selective. By applying LaVCa for image-evoked brain activity, we demonstrate that LaVCa generates captions that describe voxel selectivity more accurately than the previously proposed method. Furthermore, the captions generated by LaVCa quantitatively capture more detailed properties than the existing method at both the inter-voxel and intra-voxel levels. Furthermore, a more detailed analysis of the voxel-specific properties generated by LaVCa reveals fine-grained functional differentiation within regions of interest (ROIs) in the visual cortex and voxels that simultaneously represent multiple distinct concepts. These findings offer profound insights into human visual representations by assigning detailed captions throughout the visual cortex while highlighting the potential of LLM-based methods in understanding brain representations. Please check out our webpage at https://sites.google.com/view/lavca-llm/",True,True,,,,,,
In Silico Mapping of Visual Categorical Selectivity Across the Whole Brain,2510.21142v1,turishcheva2024dynamic,\cite{turishcheva2024dynamic},Modeling Dynamic Neural Activity by combining Naturalistic Video Stimuli and Stimulus-independent Latent Factors,https://arxiv.org/abs/2410.16136v2,"Understanding how visual processing of natural stimuli and internal brain states interact in populations of neurons remains an open question in neuroscience. Currently there are no dynamic encoding models that explicitly model a latent state and the entire neuronal response distribution. We address this gap by proposing a probabilistic model that predicts the joint distribution of the neuronal responses from video stimuli and stimulus-independent latent factors. After training and testing our model on mouse V1 neuronal responses, we find that it outperforms video-only models in terms of log-likelihood and achieves improvements in likelihood and correlation when conditioned on responses from other neurons. Furthermore, we find that the learned latent factors strongly correlate with mouse behavior and that they exhibits patterns related to the neurons position on visual cortex, although the model was trained without behavior and cortical coordinates. Our findings demonstrate that unsupervised learning of latent factors from population responses can reveal biologically meaningful structure that bridges sensory processing and behavior, without requiring explicit behavioral annotations during training. Code will be available upon publication.",True,True,,,,,,
In Silico Mapping of Visual Categorical Selectivity Across the Whole Brain,2510.21142v1,gifford2023algonauts,\cite{gifford2023algonauts},The Algonauts Project 2023 Challenge: How the Human Brain Makes Sense of Natural Scenes,https://arxiv.org/abs/2301.03198v4,"The sciences of biological and artificial intelligence are ever more intertwined. Neural computational principles inspire new intelligent machines, which are in turn used to advance theoretical understanding of the brain. To promote further exchange of ideas and collaboration between biological and artificial intelligence researchers, we introduce the 2023 installment of the Algonauts Project challenge: How the Human Brain Makes Sense of Natural Scenes (http://algonauts.csail.mit.edu). This installment prompts the fields of artificial and biological intelligence to come together towards building computational models of the visual brain using the largest and richest dataset of fMRI responses to visual scenes, the Natural Scenes Dataset (NSD). NSD provides high-quality fMRI responses to ~73,000 different naturalistic colored scenes, making it the ideal candidate for data-driven model building approaches promoted by the 2023 challenge. The challenge is open to all and makes results directly comparable and transparent through a public leaderboard automatically updated after each submission, thus allowing for rapid model development. We believe that the 2023 installment will spark symbiotic collaborations between biological and artificial intelligence scientists, leading to a deeper understanding of the brain through cutting-edge computational models and to novel ways of engineering artificial intelligent agents through inductive biases from biological systems.",True,True,,,,,,
In Silico Mapping of Visual Categorical Selectivity Across the Whole Brain,2510.21142v1,kriegeskorte2015deep,\cite{kriegeskorte2015deep},Representational Distance Learning for Deep Neural Networks,https://arxiv.org/abs/1511.03979v6,"Deep neural networks (DNNs) provide useful models of visual representational transformations. We present a method that enables a DNN (student) to learn from the internal representational spaces of a reference model (teacher), which could be another DNN or, in the future, a biological brain. Representational spaces of the student and the teacher are characterized by representational distance matrices (RDMs). We propose representational distance learning (RDL), a stochastic gradient descent method that drives the RDMs of the student to approximate the RDMs of the teacher. We demonstrate that RDL is competitive with other transfer learning techniques for two publicly available benchmark computer vision datasets (MNIST and CIFAR-100), while allowing for architectural differences between student and teacher. By pulling the student's RDMs towards those of the teacher, RDL significantly improved visual classification performance when compared to baseline networks that did not use transfer learning. In the future, RDL may enable combined supervised training of deep neural networks using task constraints (e.g. images and category labels) and constraints from brain-activity measurements, so as to build models that replicate the internal representational spaces of biological brains.",True,True,,,,,,
In Silico Mapping of Visual Categorical Selectivity Across the Whole Brain,2510.21142v1,adeli_transformer_2025,\cite{adeli_transformer_2025},Transformer brain encoders explain human high-level visual responses,https://arxiv.org/abs/2505.17329v2,"A major goal of neuroscience is to understand brain computations during visual processing in naturalistic settings. A dominant approach is to use image-computable deep neural networks trained with different task objectives as a basis for linear encoding models. However, in addition to requiring estimation of a large number of linear encoding parameters, this approach ignores the structure of the feature maps both in the brain and the models. Recently proposed alternatives factor the linear mapping into separate sets of spatial and feature weights, thus finding static receptive fields for units, which is appropriate only for early visual areas. In this work, we employ the attention mechanism used in the transformer architecture to study how retinotopic visual features can be dynamically routed to category-selective areas in high-level visual processing. We show that this computational motif is significantly more powerful than alternative methods in predicting brain activity during natural scene viewing, across different feature basis models and modalities. We also show that this approach is inherently more interpretable as the attention-routing signals for different high-level categorical areas can be easily visualized for any input image. Given its high performance at predicting brain responses to novel images, the model deserves consideration as a candidate mechanistic model of how visual information from retinotopic maps is routed in the human brain based on the relevance of the input content to different category-selective regions.",True,True,,,,,,
Discovering Interpretable Biological Concepts in Single-cell RNA-seq Foundation Models,2510.25807v1,maleki2020gene,\cite{maleki2020gene},"Gene set analysis: challenges, opportunities, and future research",,,True,False,"Maleki, Farhad and Ovens, Katie and Hogan, Daniel J and Kusalik, Anthony J",2020,,,,Frontiers in genetics
Discovering Interpretable Biological Concepts in Single-cell RNA-seq Foundation Models,2510.25807v1,fabregat2016reactome,\cite{fabregat2016reactome},The reactome pathway knowledgebase,,,True,False,"Fabregat, Antonio and Sidiropoulos, Konstantinos and Garapati, Phani and Gillespie, Marc and Hausmann, Kerstin and Haw, Robin and Jassal, Bijay and Jupe, Steven and Korninger, Florian and McKay, Sheldon and others",2016,,,,Nucleic acids research
Discovering Interpretable Biological Concepts in Single-cell RNA-seq Foundation Models,2510.25807v1,ashburner2000gene,\cite{ashburner2000gene},Gene ontology: tool for the unification of biology,,,True,False,"Ashburner, Michael and Ball, Catherine A and Blake, Judith A and Botstein, David and Butler, Heather and Cherry, J Michael and Davis, Allan P and Dolinski, Kara and Dwight, Selina S and Eppig, Janan T and others",2000,,,,Nature genetics
Discovering Interpretable Biological Concepts in Single-cell RNA-seq Foundation Models,2510.25807v1,bourgeais2022graphgonet,\cite{bourgeais2022graphgonet},GraphGONet: a self-explaining neural network encapsulating the Gene Ontology graph for phenotype prediction on gene expression,,,True,False,"Bourgeais, Victoria and Zehraoui, Farida and Hanczar, Blaise",2022,,,,Bioinformatics
Discovering Interpretable Biological Concepts in Single-cell RNA-seq Foundation Models,2510.25807v1,rybakov2020learning,\cite{rybakov2020learning},Learning interpretable latent autoencoder representations with annotations of feature sets,,,True,False,"Rybakov, Sergei and Lotfollahi, Mohammad and Theis, Fabian J and Wolf, F Alexander",2020,,,,bioRxiv
Discovering Interpretable Biological Concepts in Single-cell RNA-seq Foundation Models,2510.25807v1,lotfollahi2023biologically,\cite{lotfollahi2023biologically},Biologically informed deep learning to query gene programs in single-cell atlases,,,True,False,"Lotfollahi, Mohammad and Rybakov, Sergei and Hrovatin, Karin and Hediyeh-Zadeh, Soroor and Talavera-L{\'o}pez, Carlos and Misharin, Alexander V and Theis, Fabian J",2023,,,,Nature Cell Biology
Discovering Interpretable Biological Concepts in Single-cell RNA-seq Foundation Models,2510.25807v1,gut2021pmvae,\cite{gut2021pmvae},PmVAE: Learning interpretable single-cell representations with pathway modules,,,True,False,"Gut, Gilles and Stark, Stefan G and R{\""a}tsch, Gunnar and Davidson, Natalie R",2021,,,,bioRxiv
Discovering Interpretable Biological Concepts in Single-cell RNA-seq Foundation Models,2510.25807v1,zarlengatabcbm,\cite{zarlengatabcbm},TabCBM: Concept-based Interpretable Neural Networks for Tabular Data,,,True,False,"Zarlenga, Mateo Espinosa and Shams, Zohreh and Nelson, Michael Edward and Kim, Been and Jamnik, Mateja",2024,,,,Transactions on Machine Learning Research
Discovering Interpretable Biological Concepts in Single-cell RNA-seq Foundation Models,2510.25807v1,yap2021verifying,\cite{yap2021verifying},Verifying explainability of a deep learning tissue classifier trained on RNA-seq data,,,True,False,"Yap, Melvyn and Johnston, Rebecca L and Foley, Helena and MacDonald, Samual and Kondrashova, Olga and Tran, Khoa A and Nones, Katia and Koufariotis, Lambros T and Bean, Cameron and Pearson, John V and others",2021,,,,Scientific reports
Discovering Interpretable Biological Concepts in Single-cell RNA-seq Foundation Models,2510.25807v1,usman2025explainable,\cite{usman2025explainable},Explainable AI model reveals disease-related mechanisms in single-cell RNA-seq data,https://arxiv.org/abs/2501.03923v1,"Neurodegenerative diseases (NDDs) are complex and lack effective treatment due to their poorly understood mechanism. The increasingly used data analysis from Single nucleus RNA Sequencing (snRNA-seq) allows to explore transcriptomic events at a single cell level, yet face challenges in interpreting the mechanisms underlying a disease. On the other hand, Neural Network (NN) models can handle complex data to offer insights but can be seen as black boxes with poor interpretability. In this context, explainable AI (XAI) emerges as a solution that could help to understand disease-associated mechanisms when combined with efficient NN models. However, limited research explores XAI in single-cell data. In this work, we implement a method for identifying disease-related genes and the mechanistic explanation of disease progression based on NN model combined with SHAP. We analyze available Huntington's disease (HD) data to identify both HD-altered genes and mechanisms by adding Gene Set Enrichment Analysis (GSEA) comparing two methods, differential gene expression analysis (DGE) and NN combined with SHAP approach. Our results show that DGE and SHAP approaches offer both common and differential sets of altered genes and pathways, reinforcing the usefulness of XAI methods for a broader perspective of disease.",True,True,"Usman, Mohammad and Varea, Olga and Radeva, Petia and Canals, Josep and Abante, Jordi and Ortiz, Daniel",2025,,,,arXiv preprint arXiv:2501.03923
Discovering Interpretable Biological Concepts in Single-cell RNA-seq Foundation Models,2510.25807v1,chereda2021explaining,\cite{chereda2021explaining},Explaining decisions of graph convolutional neural networks: patient-specific molecular subnetworks responsible for metastasis prediction in breast cancer,,,True,False,"Chereda, Hryhorii and Bleckmann, Annalen and Menck, Kerstin and Perera-Bel, J{\'u}lia and Stegmaier, Philip and Auer, Florian and Kramer, Frank and Leha, Andreas and Bei{\ss}barth, Tim",2021,,,,Genome medicine
Discovering Interpretable Biological Concepts in Single-cell RNA-seq Foundation Models,2510.25807v1,zhou2023xai,\cite{zhou2023xai},XAI meets Biology: A Comprehensive Review of Explainable AI in Bioinformatics Applications,https://arxiv.org/abs/2312.06082v1,"Artificial intelligence (AI), particularly machine learning and deep learning models, has significantly impacted bioinformatics research by offering powerful tools for analyzing complex biological data. However, the lack of interpretability and transparency of these models presents challenges in leveraging these models for deeper biological insights and for generating testable hypotheses. Explainable AI (XAI) has emerged as a promising solution to enhance the transparency and interpretability of AI models in bioinformatics. This review provides a comprehensive analysis of various XAI techniques and their applications across various bioinformatics domains including DNA, RNA, and protein sequence analysis, structural analysis, gene expression and genome analysis, and bioimaging analysis. We introduce the most pertinent machine learning and XAI methods, then discuss their diverse applications and address the current limitations of available XAI tools. By offering insights into XAI's potential and challenges, this review aims to facilitate its practical implementation in bioinformatics research and help researchers navigate the landscape of XAI tools.",True,True,"Zhou, Zhongliang and Hu, Mengxuan and Salcedo, Mariah and Gravel, Nathan and Yeung, Wayland and Venkat, Aarya and Guo, Dongliang and Zhang, Jielu and Kannan, Natarajan and Li, Sheng",2023,,,,arXiv preprint arXiv:2312.06082
Discovering Interpretable Biological Concepts in Single-cell RNA-seq Foundation Models,2510.25807v1,conard2023spectrum,\cite{conard2023spectrum},A spectrum of explainable and interpretable machine learning approaches for genomic studies,,,True,False,"Conard, Ashley Mae and DenAdel, Alan and Crawford, Lorin",2023,,,,Wiley Interdisciplinary Reviews: Computational Statistics
Discovering Interpretable Biological Concepts in Single-cell RNA-seq Foundation Models,2510.25807v1,sharkey2022taking,\cite{sharkey2022taking},Taking features out of superposition with sparse autoencoders,,,True,False,"Sharkey, Lee and Braun, Dan and Millidge, Beren",2022,,,,
Discovering Interpretable Biological Concepts in Single-cell RNA-seq Foundation Models,2510.25807v1,huben2023sparse,\cite{huben2023sparse},Sparse Autoencoders Find Highly Interpretable Features in Language Models,https://arxiv.org/abs/2309.08600v3,"One of the roadblocks to a better understanding of neural networks' internals is \textit{polysemanticity}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is \textit{superposition}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task \citep{wang2022interpretability} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.",True,True,"Huben, Robert and Cunningham, Hoagy and Smith, Logan Riggs and Ewart, Aidan and Sharkey, Lee",2023,,,,
Discovering Interpretable Biological Concepts in Single-cell RNA-seq Foundation Models,2510.25807v1,fel2023craft,\cite{fel2023craft},CRAFT: Concept Recursive Activation FacTorization for Explainability,https://arxiv.org/abs/2211.10154v2,"Attribution methods, which employ heatmaps to identify the most influential regions of an image that impact model decisions, have gained widespread popularity as a type of explainability method. However, recent research has exposed the limited practical value of these methods, attributed in part to their narrow focus on the most prominent regions of an image -- revealing ""where"" the model looks, but failing to elucidate ""what"" the model sees in those areas. In this work, we try to fill in this gap with CRAFT -- a novel approach to identify both ""what"" and ""where"" by generating concept-based explanations. We introduce 3 new ingredients to the automatic concept extraction literature: (i) a recursive strategy to detect and decompose concepts across layers, (ii) a novel method for a more faithful estimation of concept importance using Sobol indices, and (iii) the use of implicit differentiation to unlock Concept Attribution Maps.
  We conduct both human and computer vision experiments to demonstrate the benefits of the proposed approach. We show that the proposed concept importance estimation technique is more faithful to the model than previous methods. When evaluating the usefulness of the method for human experimenters on a human-centered utility benchmark, we find that our approach significantly improves on two of the three test scenarios. Our code is freely available at github.com/deel-ai/Craft.",True,True,"Fel, Thomas and Picard, Agustin and Bethune, Louis and Boissin, Thibaut and Vigouroux, David and Colin, Julien and Cad{\`e}ne, R{\'e}mi and Serre, Thomas",2023,,,,
Discovering Interpretable Biological Concepts in Single-cell RNA-seq Foundation Models,2510.25807v1,Adams2025FromMI,\cite{Adams2025FromMI},"From Mechanistic Interpretability to Mechanistic Biology: Training, Evaluating, and Interpreting Sparse Autoencoders on Protein Language Models",,,True,False,Etowah Adams and Liam Bai and Minji Lee and Yiyang Yu and Mohammed Alquraishi,2025,,https://api.semanticscholar.org/CorpusID:276259557,,bioRxiv
Discovering Interpretable Biological Concepts in Single-cell RNA-seq Foundation Models,2510.25807v1,simon2024interplm,\cite{simon2024interplm},InterPLM: Discovering Interpretable Features in Protein Language Models via Sparse Autoencoders,https://arxiv.org/abs/2412.12101v1,"Protein language models (PLMs) have demonstrated remarkable success in protein modeling and design, yet their internal mechanisms for predicting structure and function remain poorly understood. Here we present a systematic approach to extract and analyze interpretable features from PLMs using sparse autoencoders (SAEs). By training SAEs on embeddings from the PLM ESM-2, we identify up to 2,548 human-interpretable latent features per layer that strongly correlate with up to 143 known biological concepts such as binding sites, structural motifs, and functional domains. In contrast, examining individual neurons in ESM-2 reveals up to 46 neurons per layer with clear conceptual alignment across 15 known concepts, suggesting that PLMs represent most concepts in superposition. Beyond capturing known annotations, we show that ESM-2 learns coherent concepts that do not map onto existing annotations and propose a pipeline using language models to automatically interpret novel latent features learned by the SAEs. As practical applications, we demonstrate how these latent features can fill in missing annotations in protein databases and enable targeted steering of protein sequence generation. Our results demonstrate that PLMs encode rich, interpretable representations of protein biology and we propose a systematic framework to extract and analyze these latent features. In the process, we recover both known biology and potentially new protein motifs. As community resources, we introduce InterPLM (interPLM.ai), an interactive visualization platform for exploring and analyzing learned PLM features, and release code for training and analysis at github.com/ElanaPearl/interPLM.",True,True,"Simon, Elana and Zou, James",2024,,,,bioRxiv
Discovering Interpretable Biological Concepts in Single-cell RNA-seq Foundation Models,2510.25807v1,lelearning,\cite{lelearning},Learning biologically relevant features in a pathology foundation model using sparse autoencoders,https://arxiv.org/abs/2407.10785v3,"Pathology plays an important role in disease diagnosis, treatment decision-making and drug development. Previous works on interpretability for machine learning models on pathology images have revolved around methods such as attention value visualization and deriving human-interpretable features from model heatmaps. Mechanistic interpretability is an emerging area of model interpretability that focuses on reverse-engineering neural networks. Sparse Autoencoders (SAEs) have emerged as a promising direction in terms of extracting monosemantic features from polysemantic model activations. In this work, we trained a Sparse Autoencoder on the embeddings of a pathology pretrained foundation model. We found that Sparse Autoencoder features represent interpretable and monosemantic biological concepts. In particular, individual SAE dimensions showed strong correlations with cell type counts such as plasma cells and lymphocytes. These biological representations were unique to the pathology pretrained model and were not found in a self-supervised model pretrained on natural images. We demonstrated that such biologically-grounded monosemantic representations evolved across the model's depth, and the pathology foundation model eventually gained robustness to non-biological factors such as scanner type. The emergence of biologically relevant SAE features was generalizable to an out-of-domain dataset. Our work paves the way for further exploration around interpretable feature dimensions and their utility for medical and clinical applications.",True,True,Nhat Minh Le and Neel Patel and Ciyue Shen and Blake Martin and Alfred Eng and Chintan Shah and Sean Grullon and Dinkar Juyal,2024,,,,
Discovering Interpretable Biological Concepts in Single-cell RNA-seq Foundation Models,2510.25807v1,Schuster2024CanSA,\cite{Schuster2024CanSA},Can sparse autoencoders make sense of latent representations?,,,True,False,Viktoria Schuster,2024,,https://api.semanticscholar.org/CorpusID:273351260,,ArXiv
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,docking-review,\cite{docking-review},Protein-ligand Docking: A Review of Recent,,,True,False,"Pujadas, Gerard and Vaqué, Montserrat and",2008,02,,10.2174/157341208783497597,Current Pharmaceutical Analysis
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,rarey_fast_1996,\cite{rarey_fast_1996},A {Fast} {Flexible} {Docking} {Method} using an {Incremental} {Construction} {Algorithm},,,True,False,"Rarey, Matthias and Kramer, Bernd and Lengauer, Thomas and Klebe, Gerhard",1996,,https://www.sciencedirect.com/science/article/pii/S0022283696904775,10.1006/jmbi.1996.0477,J. Mol. Biol.
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,surflex,\cite{surflex},{Surflex: Fully Automatic Flexible Molecular Docking Using a Molecular Similarity-Based Search Engine},,,True,False,"Jain, Ajay N.",2003,,,,J. Med. Chem.
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,mcgann_gaussian_2003,\cite{mcgann_gaussian_2003},Gaussian docking functions,,,True,False,"McGann, Mark R. and Almond, Harold R. and Nicholls, Anthony and Grant, J. Andrew and Brown, Frank K.",2003,,https://onlinelibrary.wiley.com/doi/10.1002/bip.10207,10.1002/bip.10207,Biopolymers
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,gold,\cite{gold},{Improved protein-ligand docking using GOLD},,,True,False,"Verdonk, Marcel L. and Cole, Jason C. and Hartshorn, Michael J. and Murray, Christopher W. and Taylor, Richard D.",2003,,https://doi.org/10.1002/prot.10465,10.1002/prot.10465,Proteins
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,friesner_glide_2004,\cite{friesner_glide_2004},"Glide: {A} {New} {Approach} for {Rapid}, {Accurate} {Docking} and {Scoring}. 1. {Method} and {Assessment} of {Docking} {Accuracy}",,,True,False,"Friesner, Richard A. and Banks, Jay L. and Murphy, Robert B. and Halgren, Thomas A. and Klicic, Jasna J. and Mainz, Daniel T. and Repasky, Matthew P. and Knoll, Eric H. and Shelley, Mee and Perry, Jason K. and Shaw, David E. and Francis, Perry and Shenkin, Peter S.",2004,,https://pubs.acs.org/doi/10.1021/jm0306430,10.1021/jm0306430,J. Med. Chem.
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,corbeil_variability_2012,\cite{corbeil_variability_2012},Variability in docking success rates due to dataset preparation,,,True,False,"Corbeil, Christopher R. and Williams, Christopher I. and Labute, Paul",2012,,https://link.springer.com/article/10.1007/s10822-012-9570-1,10.1007/s10822-012-9570-1,J. Comput.-Aided Mol. Des.
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,venkatraman_flexible_2012,\cite{venkatraman_flexible_2012},Flexible protein docking refinement using pose-dependent normal mode analysis,,,True,False,"Venkatraman, Vishwesh and Ritchie, David W.",2012,,https://onlinelibrary.wiley.com/doi/abs/10.1002/prot.24115,10.1002/prot.24115,Proteins
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,AutoDockVina,\cite{AutoDockVina},"{AutoDock Vina 1.2.0: New Docking Methods, Expanded Force Field, and Python Bindings}",,,True,False,"Eberhardt, Jerome and Santos-Martins, Diogo and Tillack, Andreas F. and Forli, Stefano",2021,,https://doi.org/10.1021/acs.jcim.1c00203,10.1021/acs.jcim.1c00203,J. Chem. Inf. Model.
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,inducedfit1,\cite{inducedfit1},Application of a Theory of Enzyme Specificity to Protein Synthesis,,,True,False,"Koshland, D. E.",1958,,https://www.pnas.org/doi/abs/10.1073/pnas.44.2.98,10.1073/pnas.44.2.98,Proc. Natl. Acad. Sci. USA
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,inducedfit2,\cite{inducedfit2},Enzyme flexibility and enzyme action,,,True,False,"Koshland, D. E.",1959,,https://onlinelibrary.wiley.com/doi/abs/10.1002/jcp.1030540420,https://doi.org/10.1002/jcp.1030540420,J. Cell. Physiol.
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,miller2021inducedfitdocking,\cite{miller2021inducedfitdocking},Reliable and accurate solution to the induced fit docking problem for protein--ligand binding,,,True,False,"Miller, Edward B and Murphy, Robert B and Sindhikara, Daniel and Borrelli, Kenneth W and Grisewood, Matthew J and Ranalli, Fabio and Dixon, Steven L and Jerome, Steven and Boyles, Nicholas A and Day, Tyler and others",2021,,,,J. Chem. Theory Comput.
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,corso2022diffdock,\cite{corso2022diffdock},"DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking",,,True,False,"Corso, Gabriele and St{\""a}rk, Hannes and Jing, Bowen and Barzilay, Regina and Jaakkola, Tommi",2023,,,,
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,lee_genmol_2025,\cite{lee_genmol_2025},{GenMol}: A Drug Discovery Generalist with Discrete Diffusion,,,True,False,"Lee, Seul and Kreis, Karsten and Veccham, Srimukh Prasad and Liu, Meng and Reidenbach, Danny and Peng, Yuxing and Paliwal, Saee Gopal and Nie, Weili and Vahdat, Arash",2025,,,,
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,buttenschoen2024posebusters,\cite{buttenschoen2024posebusters},PoseBusters: AI-based docking methods fail to generate physically valid poses or generalise to novel sequences,https://arxiv.org/abs/2308.05777v3,"The last few years have seen the development of numerous deep learning-based protein-ligand docking methods. They offer huge promise in terms of speed and accuracy. However, despite claims of state-of-the-art performance in terms of crystallographic root-mean-square deviation (RMSD), upon closer inspection, it has become apparent that they often produce physically implausible molecular structures. It is therefore not sufficient to evaluate these methods solely by RMSD to a native binding mode. It is vital, particularly for deep learning-based methods, that they are also evaluated on steric and energetic criteria. We present PoseBusters, a Python package that performs a series of standard quality checks using the well-established cheminformatics toolkit RDKit. Only methods that both pass these checks and predict native-like binding modes should be classed as having ""state-of-the-art"" performance. We use PoseBusters to compare five deep learning-based docking methods (DeepDock, DiffDock, EquiBind, TankBind, and Uni-Mol) and two well-established standard docking methods (AutoDock Vina and CCDC Gold) with and without an additional post-prediction energy minimisation step using a molecular mechanics force field. We show that both in terms of physical plausibility and the ability to generalise to examples that are distinct from the training data, no deep learning-based method yet outperforms classical docking tools. In addition, we find that molecular mechanics force fields contain docking-relevant physics missing from deep-learning methods. PoseBusters allows practitioners to assess docking and molecular generation methods and may inspire new inductive biases still required to improve deep learning-based methods, which will help drive the development of more accurate and more realistic predictions.",True,True,"Buttenschoen, Martin and Morris, Garrett M and Deane, Charlotte M",2024,,https://pubs.rsc.org/en/content/articlelanding/2024/sc/d3sc04185a,,Chem. Sci.
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,af2,\cite{af2},{Highly accurate protein structure prediction with AlphaFold},,,True,False,"Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michalina and Steinegger, Martin and Pacholarz, Michal and Kumar, Sanjeev and van den Berg, Huberta and Bodenstein, Sebastian and Hassabis, Demis and Jowett, John",2021,,https://doi.org/10.1038/s41586-021-03819-2,10.1038/s41586-021-03819-2,Nature
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,casp14,\cite{casp14},"Critical assessment of methods of protein structure prediction
               ({CASP)-Round} {XIV}",,,True,False,"Kryshtafovych, Andriy and Schwede, Torsten and Topf, Maya and
               Fidelis, Krzysztof and Moult, John",,,,,Proteins
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,rosettafold2021,\cite{rosettafold2021},"Accurate prediction of protein structures and interactions using
               a three-track neural network",,,True,False,"Baek, Minkyung and DiMaio, Frank and Anishchenko, Ivan and
               Dauparas, Justas and Ovchinnikov, Sergey and Lee, Gyu Rie and
               Wang, Jue and Cong, Qian and Kinch, Lisa N and Schaeffer, R
               Dustin and Mill{\'a}n, Claudia and Park, Hahnbeom and Adams,
               Carson and Glassman, Caleb R and DeGiovanni, Andy and Pereira,
               Jose H and Rodrigues, Andria V and van Dijk, Alberdina A and
               Ebrecht, Ana C and Opperman, Diederik J and Sagmeister, Theo and
               Buhlheller, Christoph and Pavkov-Keller, Tea and Rathinaswamy,
               Manoj K and Dalwadi, Udit and Yip, Calvin K and Burke, John E
               and Garcia, K Christopher and Grishin, Nick V and Adams, Paul D
               and Read, Randy J and Baker, David",,,,,Science
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,abramson2024accurate,\cite{abramson2024accurate},Accurate structure prediction of biomolecular interactions with AlphaFold 3,,,True,False,"Abramson, Josh and Adler, Jonas and Dunger, Jack and Evans, Richard and Green, Tim and Pritzel, Alexander and Ronneberger, Olaf and Willmore, Lindsay and Ballard, Andrew J and Bambrick, Joshua and others",2024,,https://www.nature.com/articles/s41586-024-07487-w,10.1038/s41586-024-07487-w,Nature
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,wwpdb_stats_2025,\cite{wwpdb_stats_2025},Worldwide Protein Data Bank: Deposition Statistics,,,True,False,{wwPDB Consortium},2025-09-20,,,,
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,chai2024chai,\cite{chai2024chai},Chai-1: Decoding the molecular interactions of life,,,True,False,"{{Chai Discovery team}} and Boitreaud, Jacques and Dent, Jack and McPartlon, Matthew and Meier, Joshua and Reis, Vinicius and Rogozhonikov, Alex and Wu, Kevin",2024,,,,bioRxiv
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,wohlwend2024boltz1,\cite{wohlwend2024boltz1},Boltz-1: Democratizing Biomolecular Interaction Modeling,,,True,False,"Wohlwend, Jeremy and Corso, Gabriele and Passaro, Saro and Reveiz, Mateo and Leidal, Ken and Swiderski, Wojtek and Portnoi, Tally and Chinn, Itamar and Silterra, Jacob and Jaakkola, Tommi and others",2024,,,,bioRxiv
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,passaro2025boltz2,\cite{passaro2025boltz2},Boltz-2: Towards Accurate and Efficient Binding Affinity Prediction,,,True,False,"Passaro, Saro and Corso, Gabriele and Wohlwend, Jeremy and Reveiz, Mateo and Thaler, Stephan and Ram Somnath, Vignesh and Getz, Noah and Portnoi, Tally and Roy, Julien and Stark, Hannes and others",2025,,,,bioRxiv
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,liu2024helixfold3,\cite{liu2024helixfold3},Technical report of HelixFold3 for biomolecular structure prediction,,,True,False,"Liu, Lihang and Zhang, Shanzhuo and Xue, Yang and Ye, Xianbin and Zhu, Kunrui and Li, Yuxin and Liu, Yang and Gao, Jie and Zhao, Wenlai and Yu, Hongkun and others",2024,,,,arXiv
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,qiao2024neuralplexer3,\cite{qiao2024neuralplexer3},NeuralPLexer3: Accurate Biomolecular Complex Structure Prediction with Flow Models,https://arxiv.org/abs/2412.10743v2,"Structure determination is essential to a mechanistic understanding of diseases and the development of novel therapeutics. Machine-learning-based structure prediction methods have made significant advancements by computationally predicting protein and bioassembly structures from sequences and molecular topology alone. Despite substantial progress in the field, challenges remain to deliver structure prediction models to real-world drug discovery. Here, we present NeuralPLexer3 -- a physics-inspired flow-based generative model that achieves state-of-the-art prediction accuracy on key biomolecular interaction types and improves training and sampling efficiency compared to its predecessors and alternative methodologies. Examined through newly developed benchmarking strategies, NeuralPLexer3 excels in vital areas that are crucial to structure-based drug design, such as physical validity and ligand-induced conformational changes.",True,True,"Qiao, Zhuoran and Ding, Feizhi and Dresselhaus, Thomas and Rosenfeld, Mia A. and Han, Xiaotian and Howell, Owen and Iyengar, Aniketh and Opalenski, Stephen and Christensen, Anders S. and Sirumalla, Sai Krishna and Manby, Frederick R. and Miller III, Thomas F. and Welborn, Matthew",2024,,,,
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,bytedance2025protenix,\cite{bytedance2025protenix},Protenix-advancing structure prediction through a comprehensive AlphaFold3 reproduction,,,True,False,"{{ByteDance AML AI4Science Team}} and Chen, Xinshi and Zhang, Yuxuan and Lu, Chan and Ma, Wenzhi and Guan, Jiaqi and Gong, Chengyue and Yang, Jincai and Zhang, Hanyu and Zhang, Ke and others",2025,,,,bioRxiv
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,diffusion,\cite{diffusion},Denoising Diffusion Probabilistic Models,https://arxiv.org/abs/2006.11239v2,"We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion",True,True,"Ho, Jonathan and Jain, Ajay and Abbeel, Pieter",2020,,,,
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,bronstein2017geometric,\cite{bronstein2017geometric},Geometric deep learning: going beyond Euclidean data,https://arxiv.org/abs/1611.08097v2,"Many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions), and are natural targets for machine learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure, and in cases where the invariances of these structures are built into networks used to model them. Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds. The purpose of this paper is to overview different examples of geometric deep learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.",True,True,"Bronstein, Michael M and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre",2017,,,,IEEE Signal Process. Mag.
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,burley2025updated,\cite{burley2025updated},Updated resources for exploring experimentally-determined PDB structures and Computed Structure Models at the RCSB Protein Data Bank,,,True,False,"Burley, Stephen K and Bhatt, Rusham and Bhikadiya, Charmi and Bi, Chunxiao and Biester, Alison and Biswas, Pratyoy and Bittrich, Sebastian and Blaumann, Santiago and Brown, Ronald and Chao, Henry and others",2025,,https://academic.oup.com/nar/article/53/D1/D564/7912033,,Nucleic Acids Res.
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,vskrinjar2025have,\cite{vskrinjar2025have},Have protein-ligand cofolding methods moved beyond memorisation?,,,True,False,"{Skrinjar}, Peter and Eberhardt, J{\'e}r{\^o}me and Durairaj, Janani and Schwede, Torsten",2025,,,,bioRxiv
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,Melnyk2025,\cite{Melnyk2025},{AlphaFold distillation for inverse protein design},,,True,False,"Melnyk, Ihor and Lozano, Aurelie and Das, Payel and Chenthamarakshan, Vijil",2025,,https://doi.org/10.1038/s41598-025-00436-1,10.1038/s41598-025-00436-1,Sci. Rep.
Pearl: A Foundation Model for Placing Every Atom in the Right Location,2510.24670v2,ahdritz2024openfold,\cite{ahdritz2024openfold},OpenFold: Retraining AlphaFold2 yields new insights into its learning mechanisms and capacity for generalization,,,True,False,"Ahdritz, Gustaf and Bouatta, Nazim and Floristean, Christina and Kadyan, Sachin and Xia, Qinghui and Gerecke, William and O’Donnell, Timothy J and Berenberg, Daniel and Fisk, Ian and Zanichelli, Niccol{\`o} and others",2024,,,,Nat. Methods
Discontinuous Epitope Fragments as Sufficient Target Templates for Efficient Binder Design,2509.25479v2,jumperHighlyAccurateProtein2021,\cite{jumperHighlyAccurateProtein2021},Highly Accurate Protein Structure Prediction with {{AlphaFold}},,,True,False,"Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v Z}{\'i}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and {Romera-Paredes}, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis",2021,,,10.1038/s41586-021-03819-2,Nature
Discontinuous Epitope Fragments as Sufficient Target Templates for Efficient Binder Design,2509.25479v2,baekAccuratePredictionProtein2021,\cite{baekAccuratePredictionProtein2021},Accurate Prediction of Protein Structures and Interactions Using a Three-Track Neural Network,,,True,False,"Baek, Minkyung and DiMaio, Frank and Anishchenko, Ivan and Dauparas, Justas and Ovchinnikov, Sergey and Lee, Gyu Rie and Wang, Jue and Cong, Qian and Kinch, Lisa N. and Schaeffer, R. Dustin and Mill{\'a}n, Claudia and Park, Hahnbeom and Adams, Carson and Glassman, Caleb R. and DeGiovanni, Andy and Pereira, Jose H. and Rodrigues, Andria V. and {van Dijk}, Alberdina A. and Ebrecht, Ana C. and Opperman, Diederik J. and Sagmeister, Theo and Buhlheller, Christoph and {Pavkov-Keller}, Tea and Rathinaswamy, Manoj K. and Dalwadi, Udit and Yip, Calvin K. and Burke, John E. and Garcia, K. Christopher and Grishin, Nick V. and Adams, Paul D. and Read, Randy J. and Baker, David",2021,,,10.1126/science.abj8754,Science
Discontinuous Epitope Fragments as Sufficient Target Templates for Efficient Binder Design,2509.25479v2,linEvolutionaryscalePredictionAtomiclevel2023,\cite{linEvolutionaryscalePredictionAtomiclevel2023},Evolutionary-Scale Prediction of Atomic-Level Protein Structure with a Language Model,,,True,False,"Lin, Zeming and Akin, Halil and Rao, Roshan and Hie, Brian and Zhu, Zhongkai and Lu, Wenting and Smetanin, Nikita and Verkuil, Robert and Kabeli, Ori and Shmueli, Yaniv and Costa, Allan dos Santos and {Fazel-Zarandi}, Maryam and Sercu, Tom and Candido, Salvatore and Rives, Alexander",2023,,,10.1126/science.ade2574,Science
Discontinuous Epitope Fragments as Sufficient Target Templates for Efficient Binder Design,2509.25479v2,evansProteinComplexPrediction2022,\cite{evansProteinComplexPrediction2022},Protein Complex Prediction with {{AlphaFold-Multimer}},,,True,False,"Evans, Richard and O'Neill, Michael and Pritzel, Alexander and Antropova, Natasha and Senior, Andrew and Green, Tim and {\v Z}{\'i}dek, Augustin and Bates, Russ and Blackwell, Sam and Yim, Jason and Ronneberger, Olaf and Bodenstein, Sebastian and Zielinski, Michal and Bridgland, Alex and Potapenko, Anna and Cowie, Andrew and Tunyasuvunakool, Kathryn and Jain, Rishub and Clancy, Ellen and Kohli, Pushmeet and Jumper, John and Hassabis, Demis",2022,,,10.1101/2021.10.04.463034,bioRxiv
Discontinuous Epitope Fragments as Sufficient Target Templates for Efficient Binder Design,2509.25479v2,gaoAF2ComplexPredictsDirect2022,\cite{gaoAF2ComplexPredictsDirect2022},{{AF2Complex}} Predicts Direct Physical Interactions in Multimeric Proteins with Deep Learning,,,True,False,"Gao, Mu and Nakajima An, Davi and Parks, Jerry M. and Skolnick, Jeffrey",2022,,,10.1038/s41467-022-29394-2,Nature Communications
Discontinuous Epitope Fragments as Sufficient Target Templates for Efficient Binder Design,2509.25479v2,abramsonAccurateStructurePrediction2024,\cite{abramsonAccurateStructurePrediction2024},Accurate Structure Prediction of Biomolecular Interactions with {{AlphaFold}} 3,,,True,False,"Abramson, Josh and Adler, Jonas and Dunger, Jack and Evans, Richard and Green, Tim and Pritzel, Alexander and Ronneberger, Olaf and Willmore, Lindsay and Ballard, Andrew J. and Bambrick, Joshua and Bodenstein, Sebastian W. and Evans, David A. and Hung, Chia-Chun and O'Neill, Michael and Reiman, David and Tunyasuvunakool, Kathryn and Wu, Zachary and {\v Z}emgulyt{\.e}, Akvil{\.e} and Arvaniti, Eirini and Beattie, Charles and Bertolli, Ottavia and Bridgland, Alex and Cherepanov, Alexey and Congreve, Miles and {Cowen-Rivers}, Alexander I. and Cowie, Andrew and Figurnov, Michael and Fuchs, Fabian B. and Gladman, Hannah and Jain, Rishub and Khan, Yousuf A. and Low, Caroline M. R. and Perlin, Kuba and Potapenko, Anna and Savy, Pascal and Singh, Sukhdeep and Stecula, Adrian and Thillaisundaram, Ashok and Tong, Catherine and Yakneen, Sergei and Zhong, Ellen D. and Zielinski, Michal and {\v Z}{\'i}dek, Augustin and Bapst, Victor and Kohli, Pushmeet and Jaderberg, Max and Hassabis, Demis and Jumper, John M.",2024,,,10.1038/s41586-024-07487-w,Nature
Discontinuous Epitope Fragments as Sufficient Target Templates for Efficient Binder Design,2509.25479v2,corleyAcceleratingBiomolecularModeling2025,\cite{corleyAcceleratingBiomolecularModeling2025},Accelerating {{Biomolecular Modeling}} with {{AtomWorks}} and {{RF3}},,,True,False,"Corley, Nathaniel and Mathis, Simon and Krishna, Rohith and Bauer, Magnus S. and Thompson, Tuscan R. and Ahern, Woody and Kazman, Maxwell W. and Brent, Rafael I. and Didi, Kieran and Kubaney, Andrew and McHugh, Lilian and Nagle, Arnav and Favor, Andrew and Kshirsagar, Meghana and Sturmfels, Pascal and Li, Yanjing and Butcher, Jasper and Qiang, Bo and Schaaf, Lars L. and Mitra, Raktim and Campbell, Katelyn and Zhang, Odin and Weissman, Roni and Humphreys, Ian R. and Cong, Qian and Funk, Jonathan and Sonthalia, Shreyash and Li{\`o}, Pietro and Baker, David and DiMaio, Frank",2025,,,10.1101/2025.08.14.670328,bioRxiv
Discontinuous Epitope Fragments as Sufficient Target Templates for Efficient Binder Design,2509.25479v2,ChaidiscoveryChailab2024,\cite{ChaidiscoveryChailab2024},Chaidiscovery/Chai-Lab,,,True,False,,2024,,,,
Discontinuous Epitope Fragments as Sufficient Target Templates for Efficient Binder Design,2509.25479v2,passaroBoltz2AccurateEfficient2025,\cite{passaroBoltz2AccurateEfficient2025},Boltz-2: {{Towards Accurate}} and {{Efficient Binding Affinity Prediction}},,,True,False,"Passaro, Saro and Corso, Gabriele and Wohlwend, Jeremy and Reveiz, Mateo and Thaler, Stephan and Somnath, Vignesh Ram and Getz, Noah and Portnoi, Tally and Roy, Julien and Stark, Hannes and {Kwabi-Addo}, David and Beaini, Dominique and Jaakkola, Tommi and Barzilay, Regina",2025,,,10.1101/2025.06.14.659707,bioRxiv
Discontinuous Epitope Fragments as Sufficient Target Templates for Efficient Binder Design,2509.25479v2,roneyStateoftheartEstimationProtein2022,\cite{roneyStateoftheartEstimationProtein2022},State-of-the-Art Estimation of Protein Model Accuracy Using {{AlphaFold}},,,True,False,"Roney, James P. and Ovchinnikov, Sergey",2022,,,,Physical Review Letters
Discontinuous Epitope Fragments as Sufficient Target Templates for Efficient Binder Design,2509.25479v2,bennettImprovingNovoProtein2023,\cite{bennettImprovingNovoProtein2023},Improving de Novo Protein Binder Design with Deep Learning,,,True,False,"Bennett, Nathaniel R. and Coventry, Brian and Goreshnik, Inna and Huang, Buwei and Allen, Aza and Vafeados, Dionne and Peng, Ying Po and Dauparas, Justas and Baek, Minkyung and Stewart, Lance and DiMaio, Frank and De Munck, Steven and Savvides, Savvas N. and Baker, David",2023,,,10.1038/s41467-023-38328-5,Nature Communications
Discontinuous Epitope Fragments as Sufficient Target Templates for Efficient Binder Design,2509.25479v2,manshourComprehensiveEvaluationAlphaFoldMultimer2024,\cite{manshourComprehensiveEvaluationAlphaFoldMultimer2024},"Comprehensive {{Evaluation}} of {{AlphaFold-Multimer}}, {{AlphaFold3}} and {{ColabFold}}, and {{Scoring Functions}} in {{Predicting Protein-Peptide Complex Structures}}",,,True,False,"Manshour, Negin and Ren, Jarett Zida and Esmaili, Farzaneh and Bergstrom, Erik and Xu, Dong",2024,,,10.1101/2024.11.11.622992,bioRxiv
Discontinuous Epitope Fragments as Sufficient Target Templates for Efficient Binder Design,2509.25479v2,watsonNovoDesignProtein2023,\cite{watsonNovoDesignProtein2023},De Novo Design of Protein Structure and Function with {{RFdiffusion}},,,True,False,"Watson, Joseph L. and Juergens, David and Bennett, Nathaniel R. and Trippe, Brian L. and Yim, Jason and Eisenach, Helen E. and Ahern, Woody and Borst, Andrew J. and Ragotte, Robert J. and Milles, Lukas F. and Wicky, Basile I. M. and Hanikel, Nikita and Pellock, Samuel J. and Courbet, Alexis and Sheffler, William and Wang, Jue and Venkatesh, Preetham and Sappington, Isaac and Torres, Susana V{\'a}zquez and Lauko, Anna and De Bortoli, Valentin and Mathieu, Emile and Ovchinnikov, Sergey and Barzilay, Regina and Jaakkola, Tommi S. and DiMaio, Frank and Baek, Minkyung and Baker, David",2023,,,10.1038/s41586-023-06415-8,Nature
Discontinuous Epitope Fragments as Sufficient Target Templates for Efficient Binder Design,2509.25479v2,anishchenkoNovoProteinDesign2021,\cite{anishchenkoNovoProteinDesign2021},De Novo Protein Design by Deep Network Hallucination,,,True,False,"Anishchenko, Ivan and Pellock, Samuel J. and Chidyausiku, Tamuka M. and Ramelot, Theresa A. and Ovchinnikov, Sergey and Hao, Jingzhou and Bafna, Khushboo and Norn, Christoffer and Kang, Alex and Bera, Asim K. and DiMaio, Frank and Carter, Lauren and Chow, Cameron M. and Montelione, Gaetano T. and Baker, David",2021,,,10.1038/s41586-021-04184-w,Nature
Discontinuous Epitope Fragments as Sufficient Target Templates for Efficient Binder Design,2509.25479v2,goverdeNovoProteinDesign2023,\cite{goverdeNovoProteinDesign2023},De Novo Protein Design by Inversion of the {{AlphaFold}} Structure Prediction Network,,,True,False,"Goverde, Casper A. and Wolf, Benedict and Khakzad, Hamed and Rosset, St{\'e}phane and Correia, Bruno E.",2023,,,10.1002/pro.4653,Protein Science
Discontinuous Epitope Fragments as Sufficient Target Templates for Efficient Binder Design,2509.25479v2,pacesaBindCraftOneshotDesign2024,\cite{pacesaBindCraftOneshotDesign2024},{{BindCraft}}: One-Shot Design of Functional Protein Binders,,,True,False,"Pacesa, Martin and Nickel, Lennart and Schmidt, Joseph and Pyatova, Ekaterina and Schellhaas, Christian and Kissling, Lucas and {Alcaraz-Serna}, Ana and Cho, Yehlin and Ghamary, Kourosh H. and Vinu{\'e}, Laura and Yachnin, Brahm J. and Wollacott, Andrew M. and Buckley, Stephen and Georgeon, Sandrine and Goverde, Casper A. and Hatzopoulos, Georgios N. and G{\""o}nczy, Pierre and Muller, Yannick D. and Schwank, Gerald and Ovchinnikov, Sergey and Correia, Bruno E.",2024,,,10.1101/2024.09.30.615802,bioRxiv
Discontinuous Epitope Fragments as Sufficient Target Templates for Efficient Binder Design,2509.25479v2,raybouldFiveComputationalDevelopability2019,\cite{raybouldFiveComputationalDevelopability2019},Five Computational Developability Guidelines for Therapeutic Antibody Profiling,,,True,False,"Raybould, Matthew I. J. and Marks, Claire and Krawczyk, Konrad and Taddese, Bruck and Nowak, Jaroslaw and Lewis, Alan P. and Bujotzek, Alexander and Shi, Jiye and Deane, Charlotte M.",2019,,,10.1073/pnas.1810576116,Proceedings of the National Academy of Sciences
Bridging Accuracy and Explainability in EEG-based Graph Attention Network for Depression Detection,2511.05537v1,reza2009sz,\cite{reza2009sz},An efficient classifier to diagnose of schizophrenia based on the {EEG} signals,,,True,False,Reza Boostani and Khadijeh Sadatnezhad and Malihe Sabeti,2009,,,https://doi.org/10.1016/j.eswa.2008.07.037,Expert Systems with Applications
Bridging Accuracy and Explainability in EEG-based Graph Attention Network for Depression Detection,2511.05537v1,DING2019156,\cite{DING2019156},"Classifying major depression patients and healthy controls using {EEG}, eye tracking and galvanic skin response data",,,True,False,Xinfang Ding and Xinxin Yue and Rui Zheng and Cheng Bi and Dai Li and Guizhong Yao,2019,,,https://doi.org/10.1016/j.jad.2019.03.058,Journal of Affective Disorders
Bridging Accuracy and Explainability in EEG-based Graph Attention Network for Depression Detection,2511.05537v1,Mahato2019eeg,\cite{Mahato2019eeg},Detection of major depressive disorder using linear and non-linear features from {EEG} signals,,,True,False,"Mahato, Shalini and Paul, Sanchita",2019,03,,10.1007/s00542-018-4075-z,Microsystem Technologies
Bridging Accuracy and Explainability in EEG-based Graph Attention Network for Depression Detection,2511.05537v1,Christopher1998wavelet,\cite{Christopher1998wavelet},A Practical Guide to Wavelet Analysis,,,True,False,Christopher Torrence and Gilbert P. Compo,1998,,,10.1175/1520-0477(1998)079<0061:APGTWA>2.0.CO;2,Bulletin of the American Meteorological Society
Bridging Accuracy and Explainability in EEG-based Graph Attention Network for Depression Detection,2511.05537v1,Jeong2016coherence,\cite{Jeong2016coherence},Wavelet Energy and Wavelet Coherence as {EEG} Biomarkers for the Diagnosis of {Parkinson’s} Disease-Related Dementia and {Alzheimer’s} Disease,,,True,False,"Jeong, Dong-Hwa and Kim, Young-Do and Song, In-Uk and Chung, Yong-An and Jeong, Jaeseung",2016,,,10.3390/e18010008,Entropy
Bridging Accuracy and Explainability in EEG-based Graph Attention Network for Depression Detection,2511.05537v1,Ziad2012az,\cite{Ziad2012az},Wavelet Coherence Model for Diagnosis of {A}lzheimer Disease,,,True,False,Ziad Sankari and Hojjat Adeli and Anahita Adeli,2012,,,10.1177/1550059412444970,Clinical EEG and Neuroscience
Bridging Accuracy and Explainability in EEG-based Graph Attention Network for Depression Detection,2511.05537v1,ghosh2023connectome,\cite{ghosh2023connectome},Graph Convolutional Learning of Multimodal Brain Connectome Data for Schizophrenia Classification,,,True,False,"Ghosh, Sanjay and Bhargava, Eshan and Lin, Chieh-Te and Nagarajan, Srikantan S.",2023,,,10.1109/ISBI53787.2023.10230441,Proc. IEEE 20th International Symposium on Biomedical Imaging (ISBI)
Bridging Accuracy and Explainability in EEG-based Graph Attention Network for Depression Detection,2511.05537v1,Mazurek2024graphxai,\cite{Mazurek2024graphxai},Explainable Graph Neural Networks for {EEG} Classification and Seizure Detection in Epileptic Patients,,,True,False,"Mazurek, Szymon and Blanco, Rosmary and Falcó-Roget, Joan and Crimi, Alessandro",2024,,,10.1109/ISBI56570.2024.10635821,Proc. IEEE International Symposium on Biomedical Imaging (ISBI)
Bridging Accuracy and Explainability in EEG-based Graph Attention Network for Depression Detection,2511.05537v1,Friston2011connectivity,\cite{Friston2011connectivity},Functional and Effective Connectivity: A Review,,,True,False,"Friston, Karl",2011,01,,10.1089/brain.2011.0008,Brain connectivity
Bridging Accuracy and Explainability in EEG-based Graph Attention Network for Depression Detection,2511.05537v1,ellis2024crosseeg,\cite{ellis2024crosseeg},Cross-Sampling Rate Transfer Learning for Enhanced Raw {EEG} Deep Learning Classifier Performance in Major Depressive Disorder Diagnosis,,,True,False,"Ellis, Charles A. and Miller, Robyn L. and Calhoun, Vince D.",2024,,,10.1109/ISBI56570.2024.10635743,2024 IEEE International Symposium on Biomedical Imaging (ISBI)
Bridging Accuracy and Explainability in EEG-based Graph Attention Network for Depression Detection,2511.05537v1,seal2021deprnet,\cite{seal2021deprnet},{DeprNet}: A Deep Convolution Neural Network Framework for Detecting Depression Using {EEG},,,True,False,"Seal, Ayan and Bajpai, Rishabh and Agnihotri, Jagriti and Yazidi, Anis and Herrera-Viedma, Enrique and Krejcar, Ondrej",2021,,,,IEEE Transactions on Instrumentation and Measurement
Bridging Accuracy and Explainability in EEG-based Graph Attention Network for Depression Detection,2511.05537v1,LEI2022103370,\cite{LEI2022103370},A convolutional neural network-based diagnostic method using resting-state electroencephalograph signals for major depressive and bipolar disorders,,,True,False,"Lei, Yu and Belkacem, Abdelkader Nasreddine and Wang, Xiaotian and Sha, Sha and Wang, Changming and Chao, Chen",2022,02,,10.1016/j.bspc.2021.103370,Biomedical Signal Processing and Control
Bridging Accuracy and Explainability in EEG-based Graph Attention Network for Depression Detection,2511.05537v1,Uyulan2021cnn,\cite{Uyulan2021cnn},Major Depressive Disorder Classification Based on Different Convolutional Neural Network Models: Deep Learning Approach,,,True,False,Caglar Uyulan and Türker Tekin Ergüzel and Huseyin Unubol and Merve Cebi and Gokben Hizli Sayar and Mahdi Nezhad Asad and Nevzat Tarhan,2021,,,10.1177/1550059420916634,Clinical EEG and Neuroscience
Bridging Accuracy and Explainability in EEG-based Graph Attention Network for Depression Detection,2511.05537v1,SADATSHAHABI2021946,\cite{SADATSHAHABI2021946},Prediction of drug response in major depressive disorder using ensemble of transfer learning with convolutional neural network based on {EEG},,,True,False,Mohsen {Sadat Shahabi} and Ahmad Shalbaf and Arash Maghsoudi,2021,,,,Biocybernetics and Biomedical Engineering
Bridging Accuracy and Explainability in EEG-based Graph Attention Network for Depression Detection,2511.05537v1,Saeedi2020knn,\cite{Saeedi2020knn},Major Depressive Disorder Assessment via Enhanced {K}-nearest Neighbor Method and {EEG} Signals,,,True,False,"Saeedi, Maryam and Saeedi, Abdolkarim and Maghsoudi, Arash",2020,07,,10.1007/s13246-020-00897-w,Physical and Engineering Sciences in Medicine
Bridging Accuracy and Explainability in EEG-based Graph Attention Network for Depression Detection,2511.05537v1,sharma2021dephnn,\cite{sharma2021dephnn},{DepHNN}: A novel hybrid neural network for electroencephalogram ({EEG})-based screening of depression,,,True,False,Geetanjali Sharma and Abhishek Parashar and Amit M. Joshi,2021,,,https://doi.org/10.1016/j.bspc.2020.102393,Biomedical Signal Processing and Control
Bridging Accuracy and Explainability in EEG-based Graph Attention Network for Depression Detection,2511.05537v1,luo2023graph,\cite{luo2023graph},Exploring Adaptive Graph Topologies and Temporal Graph Networks for {EEG}-Based Depression Detection,,,True,False,"Luo, Gang and Rao, Hong and An, Panfeng and Li, Yunxia and Hong, Ruiyun and Chen, Wenwu and Chen, Shengbo",2023,,,10.1109/TNSRE.2023.3320693,IEEE Transactions on Neural Systems and Rehabilitation Engineering
Bridging Accuracy and Explainability in EEG-based Graph Attention Network for Depression Detection,2511.05537v1,ghosh2023subspace,\cite{ghosh2023subspace},A joint subspace mapping between structural and functional brain connectomes,,,True,False,Sanjay Ghosh and Ashish Raj and Srikantan S. Nagarajan,2023,,,https://doi.org/10.1016/j.neuroimage.2023.119975,NeuroImage
Bridging Accuracy and Explainability in EEG-based Graph Attention Network for Depression Detection,2511.05537v1,lawhern2018eegnet,\cite{lawhern2018eegnet},{EEGNet}: a compact convolutional neural network for {EEG}-based brain–computer interfaces,,,True,False,"Lawhern, Vernon J and Solon, Amelia J and Waytowich, Nicholas R and Gordon, Stephen M and Hung, Chou P and Lance, Brent J",2018,,,10.1088/1741-2552/aace8c,Journal of Neural Engineering
Speak to a Protein: An Interactive Multimodal Co-Scientist for Protein Analysis,2510.17826v1,kitano2021,\cite{kitano2021},Artificial intelligence to win a Nobel prize and beyond: Creating the engine for scientific discovery,,,True,False,"Kitano, Hiroaki",2021,,,,AI Magazine
Speak to a Protein: An Interactive Multimodal Co-Scientist for Protein Analysis,2510.17826v1,boiko2023autonomous,\cite{boiko2023autonomous},Autonomous chemical research with large language models,,,True,False,"Boiko, Daniil A and MacKnight, Robert and Kline, Ben and Gomes, Gabe",2023,,,,Nature
Speak to a Protein: An Interactive Multimodal Co-Scientist for Protein Analysis,2510.17826v1,zou2025agente,\cite{zou2025agente},El Agente: An autonomous agent for quantum chemistry,,,True,False,"Zou, Yunheng and Cheng, Austin H and Aldossary, Abdulrahman and Bai, Jiaru and Leong, Shi Xuan and Campos-Gonzalez-Angulo, Jorge Arturo and Choi, Changhyeok and Ser, Cher Tian and Tom, Gary and Wang, Andrew and others",2025,,,,Matter
Speak to a Protein: An Interactive Multimodal Co-Scientist for Protein Analysis,2510.17826v1,guo2023proteinchat,\cite{guo2023proteinchat},ProteinChat: Towards Achieving ChatGPT-Like Functionalities on Protein 3D Structures,,,True,False,"Guo, Han and Huo, Mingjia and Zhang, Ruiyi and Xie, Pengtao",2023,,https://doi.org/10.36227/techrxiv.23120606,10.36227/techrxiv.23120606,
Speak to a Protein: An Interactive Multimodal Co-Scientist for Protein Analysis,2510.17826v1,wang2024protchatgpt,\cite{wang2024protchatgpt},ProtChatGPT: Towards Understanding Proteins with Large Language Models,,,True,False,"Wang, Chao and Fan, Hehe and Quan, Ruijie and Yang, Yi",2024,,https://arxiv.org/abs/2402.09649,,arXiv preprint arXiv:2402.09649
Speak to a Protein: An Interactive Multimodal Co-Scientist for Protein Analysis,2510.17826v1,xiao2024proteingpt,\cite{xiao2024proteingpt},ProteinGPT: Multimodal LLM for Protein Property Prediction and Structure Understanding,,,True,False,"Xiao, Yijia and Sun, Edward and Jin, Yiqiao and Wang, Qifan and Wang, Wei",2024,,https://arxiv.org/abs/2408.11363,,arXiv preprint arXiv:2408.11363
Speak to a Protein: An Interactive Multimodal Co-Scientist for Protein Analysis,2510.17826v1,wang2025prot2chat,\cite{wang2025prot2chat},"Prot2Chat: Protein LLM with Early-Fusion of Text, Sequence and Structure",,,True,False,"Wang, Zhicong and Ma, Zicheng and Cao, Ziqiang and Zhou, Changlong and Zhang, Jun and Gao, Yiqin",2025,,,,arXiv preprint arXiv:2502.06846
Speak to a Protein: An Interactive Multimodal Co-Scientist for Protein Analysis,2510.17826v1,sun2024chatmol,\cite{sun2024chatmol},ChatMol Copilot: An Agent for Molecular Modeling and Computation Powered by LLMs,,,True,False,"Sun, Jinyuan and Li, Auston and Deng, Yifan and Li, Jiabo",2024,,,,
Speak to a Protein: An Interactive Multimodal Co-Scientist for Protein Analysis,2510.17826v1,ille2024gpt4structural,\cite{ille2024gpt4structural},Generative artificial intelligence performs rudimentary structural biology modeling,,,True,False,"Ille, Alexander M and Markosian, Christopher and Burley, Stephen K and Mathews, Michael B and Pasqualini, Renata and Arap, Wadih",2024,,,,Scientific reports
Speak to a Protein: An Interactive Multimodal Co-Scientist for Protein Analysis,2510.17826v1,bran2024chemcrow,\cite{bran2024chemcrow},Augmenting Large Language Models with Chemistry Tools,,,True,False,"Bran, Andres M. and Cox, Sam and Schilter, Oliver and Baldassari, Carlo and White, Andrew D. and Schwaller, Philippe",2024,,,10.1038/s42256-024-00832-8,Nature Machine Intelligence
Speak to a Protein: An Interactive Multimodal Co-Scientist for Protein Analysis,2510.17826v1,lee2025cladd,\cite{lee2025cladd},RAG-Enhanced Collaborative LLM Agents for Drug Discovery,https://arxiv.org/abs/2502.17506v2,"Recent advances in large language models (LLMs) have shown great potential to accelerate drug discovery. However, the specialized nature of biochemical data often necessitates costly domain-specific fine-tuning, posing critical challenges. First, it hinders the application of more flexible general-purpose LLMs in cutting-edge drug discovery tasks. More importantly, it impedes the rapid integration of the vast amounts of scientific data continuously generated through experiments and research. To investigate these challenges, we propose CLADD, a retrieval-augmented generation (RAG)-empowered agentic system tailored to drug discovery tasks. Through the collaboration of multiple LLM agents, CLADD dynamically retrieves information from biomedical knowledge bases, contextualizes query molecules, and integrates relevant evidence to generate responses -- all without the need for domain-specific fine-tuning. Crucially, we tackle key obstacles in applying RAG workflows to biochemical data, including data heterogeneity, ambiguity, and multi-source integration. We demonstrate the flexibility and effectiveness of this framework across a variety of drug discovery tasks, showing that it outperforms general-purpose and domain-specific LLMs as well as traditional deep learning approaches.",True,True,"Lee, Namkyeong and De Brouwer, Edward and Hajiramezanali, Ehsan and Biancalani, Tommaso and Park, Chanyoung and Scalia, Gabriele",2025,,https://arxiv.org/abs/2502.17506,10.48550/arXiv.2502.17506,arXiv preprint arXiv:2502.17506
CORE - A Cell-Level Coarse-to-Fine Image Registration Engine for Multi-stain Image Alignment,2511.03826v2,elhaminia_traditional_2025,\cite{elhaminia_traditional_2025},From Traditional to Deep Learning Approaches in Whole Slide Image Registration: A Methodological Review,https://arxiv.org/abs/2502.19123v1,"Whole slide image (WSI) registration is an essential task for analysing the tumour microenvironment (TME) in histopathology. It involves the alignment of spatial information between WSIs of the same section or serial sections of a tissue sample. The tissue sections are usually stained with single or multiple biomarkers before imaging, and the goal is to identify neighbouring nuclei along the Z-axis for creating a 3D image or identifying subclasses of cells in the TME. This task is considerably more challenging compared to radiology image registration, such as magnetic resonance imaging or computed tomography, due to various factors. These include gigapixel size of images, variations in appearance between differently stained tissues, changes in structure and morphology between non-consecutive sections, and the presence of artefacts, tears, and deformations. Currently, there is a noticeable gap in the literature regarding a review of the current approaches and their limitations, as well as the challenges and opportunities they present. We aim to provide a comprehensive understanding of the available approaches and their application for various purposes. Furthermore, we investigate current deep learning methods used for WSI registration, emphasising their diverse methodologies. We examine the available datasets and explore tools and software employed in the field. Finally, we identify open challenges and potential future trends in this area of research.",True,True,"Elhaminia, Behnaz and Alsalemi, Abdullah and Nasir, Esha and Jahanifar, Mostafa and Awan, Ruqayya and Young, Lawrence S. and Rajpoot, Nasir M. and Minhas, Fayyaz and Raza, Shan E Ahmed",2025,,https://arxiv.org/abs/2502.19123,10.48550/ARXIV.2502.19123,
CORE - A Cell-Level Coarse-to-Fine Image Registration Engine for Multi-stain Image Alignment,2511.03826v2,lotz_comparison_2023,\cite{lotz_comparison_2023},Comparison of {Consecutive} and {Re}-stained {Sections} for {Image} {Registration} in {Histopathology},,,True,False,"Lotz, Johannes and Weiss, Nick and Laak, Jeroen van der and Heldmann, Stefan",2023,,http://arxiv.org/abs/2106.13150,10.1117/1.JMI.10.6.067501,Journal of Medical Imaging
CORE - A Cell-Level Coarse-to-Fine Image Registration Engine for Multi-stain Image Alignment,2511.03826v2,gatenbee_virtual_2023,\cite{gatenbee_virtual_2023},Virtual alignment of pathology image series for multi-gigapixel whole slide images,,,True,False,"Gatenbee, Chandler D. and Baker, Ann-Marie and Prabhakaran, Sandhya and Swinyard, Ottilie and Slebos, Robbert J. C. and Mandal, Gunjan and Mulholland, Eoghan and Andor, Noemi and Marusyk, Andriy and Leedham, Simon and Conejo-Garcia, Jose R. and Chung, Christine H. and Robertson-Tessi, Mark and Graham, Trevor A. and Anderson, Alexander R. A.",2023,,https://www.nature.com/articles/s41467-023-40218-9,10.1038/s41467-023-40218-9,Nature Communications
CORE - A Cell-Level Coarse-to-Fine Image Registration Engine for Multi-stain Image Alignment,2511.03826v2,wodzinski_deeperhistreg_2024,\cite{wodzinski_deeperhistreg_2024},DeeperHistReg: Robust Whole Slide Images Registration Framework,https://arxiv.org/abs/2404.14434v1,"DeeperHistReg is a software framework dedicated to registering whole slide images (WSIs) acquired using multiple stains. It allows one to perform the preprocessing, initial alignment, and nonrigid registration of WSIs acquired using multiple stains (e.g. hematoxylin \& eosin, immunochemistry). The framework implements several state-of-the-art registration algorithms and provides an interface to operate on arbitrary resolution of the WSIs (up to 200k x 200k). The framework is extensible and new algorithms can be easily integrated by other researchers. The framework is available both as a PyPI package and as a Docker container.",True,True,"Wodzinski, Marek and Marini, Niccolò and Atzori, Manfredo and Müller, Henning",2024,,https://arxiv.org/abs/2404.14434,10.48550/ARXIV.2404.14434,
CORE - A Cell-Level Coarse-to-Fine Image Registration Engine for Multi-stain Image Alignment,2511.03826v2,awan_deep_2023,\cite{awan_deep_2023},Deep Feature based Cross-slide Registration,https://arxiv.org/abs/2202.09971v5,"Cross-slide image analysis provides additional information by analysing the expression of different biomarkers as compared to a single slide analysis. These biomarker stained slides are analysed side by side, revealing unknown relations between them. During the slide preparation, a tissue section may be placed at an arbitrary orientation as compared to other sections of the same tissue block. The problem is compounded by the fact that tissue contents are likely to change from one section to the next and there may be unique artefacts on some of the slides. This makes registration of each section to a reference section of the same tissue block an important pre-requisite task before any cross-slide analysis. We propose a deep feature based registration (DFBR) method which utilises data-driven features to estimate the rigid transformation. We adopted a multi-stage strategy for improving the quality of registration. We also developed a visualisation tool to view registered pairs of WSIs at different magnifications. With the help of this tool, one can apply a transformation on the fly without the need to generate transformed source WSI in a pyramidal form. We compared the performance of data-driven features with that of hand-crafted features on the COMET dataset. Our approach can align the images with low registration errors. Generally, the success of non-rigid registration is dependent on the quality of rigid registration. To evaluate the efficacy of the DFBR method, the first two steps of the ANHIR winner's framework are replaced with our DFBR to register challenge provided image pairs. The modified framework produces comparable results to that of challenge winning team.",True,True,"Awan, Ruqayya and Raza, Shan E. Ahmed and Lotz, Johannes and Weiss, Nick and Rajpoot, Nasir",2023,,https://linkinghub.elsevier.com/retrieve/pii/S089561112200132X,10.1016/j.compmedimag.2022.102162,Computerized Medical Imaging and Graphics
CORE - A Cell-Level Coarse-to-Fine Image Registration Engine for Multi-stain Image Alignment,2511.03826v2,lotz_patch-based_2016,\cite{lotz_patch-based_2016},Patch-{Based} {Nonlinear} {Image} {Registration} for {Gigapixel} {Whole} {Slide} {Images},,,True,False,"Lotz, J. and Olesch, J. and Muller, B. and Polzin, T. and Galuschka, P. and Lotz, J. M. and Heldmann, S. and Laue, H. and Gonzalez-Vallinas, M. and Warth, A. and Lahrmann, B. and Grabe, N. and Sedlaczek, O. and Breuhahn, K. and Modersitzki, J.",2016,,https://ieeexplore.ieee.org/document/7335576/,10.1109/TBME.2015.2503122,IEEE Transactions on Biomedical Engineering
CORE - A Cell-Level Coarse-to-Fine Image Registration Engine for Multi-stain Image Alignment,2511.03826v2,lotz_robust_2019,\cite{lotz_robust_2019},"Robust, fast and accurate: a 3-step method for automatic histological image registration",,,True,False,"Lotz, Johannes and Weiss, Nick and Heldmann, Stefan",2019,,https://arxiv.org/abs/1903.12063,10.48550/ARXIV.1903.12063,
CORE - A Cell-Level Coarse-to-Fine Image Registration Engine for Multi-stain Image Alignment,2511.03826v2,borovec_anhir_2020,\cite{borovec_anhir_2020},{ANHIR}: {Automatic} {Non}-{Rigid} {Histological} {Image} {Registration} {Challenge},,,True,False,"Borovec, Jiri and Kybic, Jan and Arganda-Carreras, Ignacio and Sorokin, Dmitry V. and Bueno, Gloria and Khvostikov, Alexander V. and Bakas, Spyridon and Chang, Eric I-Chao and Heldmann, Stefan and Kartasalo, Kimmo and Latonen, Leena and Lotz, Johannes and Noga, Michelle and Pati, Sarthak and Punithakumar, Kumaradevan and Ruusuvuori, Pekka and Skalski, Andrzej and Tahmasebi, Nazanin and Valkonen, Masi and Venet, Ludovic and Wang, Yizhe and Weiss, Nick and Wodzinski, Marek and Xiang, Yu and Xu, Yan and Yan, Yan and Yushkevich, Paul and Zhao, Shengyu and Munoz-Barrutia, Arrate",2020,,https://ieeexplore.ieee.org/document/9058666/,10.1109/TMI.2020.2986331,IEEE Transactions on Medical Imaging
CORE - A Cell-Level Coarse-to-Fine Image Registration Engine for Multi-stain Image Alignment,2511.03826v2,weitz_acrobat_2024,\cite{weitz_acrobat_2024},The {ACROBAT} 2022 challenge: {Automatic} registration of breast cancer tissue,,,True,False,"Weitz, Philippe and Valkonen, Masi and Solorzano, Leslie and Carr, Circe and Kartasalo, Kimmo and Boissin, Constance and Koivukoski, Sonja and Kuusela, Aino and Rasic, Dusan and Feng, Yanbo and Pouplier, Sandra Sinius and Sharma, Abhinav and Eriksson, Kajsa Ledesma and Robertson, Stephanie and Marzahl, Christian and Gatenbee, Chandler D. and Anderson, Alexander R.A. and Wodzinski, Marek and Jurgas, Artur and Marini, Niccolò and Atzori, Manfredo and Müller, Henning and Budelmann, Daniel and Weiss, Nick and Heldmann, Stefan and Lotz, Johannes and Wolterink, Jelmer M. and De Santi, Bruno and Patil, Abhijeet and Sethi, Amit and Kondo, Satoshi and Kasai, Satoshi and Hirasawa, Kousuke and Farrokh, Mahtab and Kumar, Neeraj and Greiner, Russell and Latonen, Leena and Laenkholm, Anne-Vibeke and Hartman, Johan and Ruusuvuori, Pekka and Rantalainen, Mattias",2024,,https://linkinghub.elsevier.com/retrieve/pii/S1361841524001828,10.1016/j.media.2024.103257,Medical Image Analysis
CORE - A Cell-Level Coarse-to-Fine Image Registration Engine for Multi-stain Image Alignment,2511.03826v2,leutenegger_brisk_2011,\cite{leutenegger_brisk_2011},{BRISK}: {Binary} {Robust} invariant scalable keypoints,,,True,False,"Leutenegger, Stefan and Chli, Margarita and Siegwart, Roland Y.",2011,,http://ieeexplore.ieee.org/document/6126542/,10.1109/ICCV.2011.6126542,
CORE - A Cell-Level Coarse-to-Fine Image Registration Engine for Multi-stain Image Alignment,2511.03826v2,sarkar_robust_2014,\cite{sarkar_robust_2014},A robust method for inter-marker whole slide registration of digital pathology images using lines based features,,,True,False,"Sarkar, Anindya and Yuan, Quan and Srinivas, Chukka",2014,,http://ieeexplore.ieee.org/document/6867982/,10.1109/ISBI.2014.6867982,
CORE - A Cell-Level Coarse-to-Fine Image Registration Engine for Multi-stain Image Alignment,2511.03826v2,fischler_random_1981,\cite{fischler_random_1981},Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography,,,True,False,"Fischler, Martin A. and Bolles, Robert C.",1981,,https://dl.acm.org/doi/10.1145/358669.358692,10.1145/358669.358692,Communications of the ACM
CORE - A Cell-Level Coarse-to-Fine Image Registration Engine for Multi-stain Image Alignment,2511.03826v2,wei_unsupervised_2025,\cite{wei_unsupervised_2025},Unsupervised {Non}-{Rigid} {Histological} {Image} {Registration} {Guided} by {Keypoint} {Correspondences} {Based} on {Learnable} {Deep} {Features} {With} {Iterative} {Training},,,True,False,"Wei, Xingyue and Ge, Lin and Huang, Lijie and Luo, Jianwen and Xu, Yan",2025,,https://ieeexplore.ieee.org/document/10643202/,10.1109/TMI.2024.3447214,IEEE Transactions on Medical Imaging
CORE - A Cell-Level Coarse-to-Fine Image Registration Engine for Multi-stain Image Alignment,2511.03826v2,antonacopoulos_unsupervised_2025,\cite{antonacopoulos_unsupervised_2025},Unsupervised {Feature} {Matching} for {Affine} {Histological} {Image} {Registration},,,True,False,"Pyatov, Vladislav A. and Sorokin, Dmitry V.",2025,,https://link.springer.com/10.1007/978-3-031-78201-5_3,10.1007/978-3-031-78201-5_3,
CORE - A Cell-Level Coarse-to-Fine Image Registration Engine for Multi-stain Image Alignment,2511.03826v2,ge_unsupervised_2022,\cite{ge_unsupervised_2022},Unsupervised {Histological} {Image} {Registration} {Using} {Structural} {Feature} {Guided} {Convolutional} {Neural} {Network},,,True,False,"Ge, Lin and Wei, Xingyue and Hao, Yayu and Luo, Jianwen and Xu, Yan",2022,,https://ieeexplore.ieee.org/document/9745959/,10.1109/tmi.2022.3164088,IEEE Transactions on Medical Imaging
CORE - A Cell-Level Coarse-to-Fine Image Registration Engine for Multi-stain Image Alignment,2511.03826v2,sarlin_superglue_2020,\cite{sarlin_superglue_2020},{SuperGlue}: {Learning} {Feature} {Matching} {With} {Graph} {Neural} {Networks},,,True,False,"Sarlin, Paul-Edouard and DeTone, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew",2020,,https://ieeexplore.ieee.org/document/9157489/,10.1109/CVPR42600.2020.00499,
CORE - A Cell-Level Coarse-to-Fine Image Registration Engine for Multi-stain Image Alignment,2511.03826v2,awan2018deep,\cite{awan2018deep},Deep autoencoder features for registration of histology images,,,True,False,Awan R and Rajpoot N,2018,,,,
CORE - A Cell-Level Coarse-to-Fine Image Registration Engine for Multi-stain Image Alignment,2511.03826v2,dosovitskiy2015flownet,\cite{dosovitskiy2015flownet},Flownet: Learning optical flow with convolutional networks,,,True,False,Dosovitskiy A and Fischer P and Ilg E and Hausser P and Hazirbas C and Golkov V and Van Der Smagt P and Cremers D and Brox T,2015,,,,
CORE - A Cell-Level Coarse-to-Fine Image Registration Engine for Multi-stain Image Alignment,2511.03826v2,mahapatra_registration_2020,\cite{mahapatra_registration_2020},Registration of Histopathogy Images Using Structural Information From Fine Grained Feature Maps,https://arxiv.org/abs/2007.02078v1,"Registration is an important part of many clinical workflows and factually, including information of structures of interest improves registration performance. We propose a novel approach of combining segmentation information in a registration framework using self supervised segmentation feature maps extracted using a pre-trained segmentation network followed by clustering. Using self supervised feature maps enables us to use segmentation information despite the unavailability of manual segmentations. Experimental results show our approach effectively replaces manual segmentation maps and demonstrate the possibility of obtaining state of the art registration performance in real world cases where manual segmentation maps are unavailable.",True,True,"Mahapatra, Dwarikanath",2020,,https://arxiv.org/abs/2007.02078,10.48550/ARXIV.2007.02078,
CORE - A Cell-Level Coarse-to-Fine Image Registration Engine for Multi-stain Image Alignment,2511.03826v2,yap_nuclei-location_2024,\cite{yap_nuclei-location_2024},Nuclei-Location Based Point Set Registration of Multi-Stained Whole Slide Images,https://arxiv.org/abs/2404.17041v1,"Whole Slide Images (WSIs) provide exceptional detail for studying tissue architecture at the cell level. To study tumour microenvironment (TME) with the context of various protein biomarkers and cell sub-types, analysis and registration of features using multi-stained WSIs is often required. Multi-stained WSI pairs normally suffer from rigid and non-rigid deformities in addition to slide artefacts and control tissue which present challenges at precise registration. Traditional registration methods mainly focus on global rigid/non-rigid registration but struggle with aligning slides with complex tissue deformations at the nuclei level. However, nuclei level non-rigid registration is essential for downstream tasks such as cell sub-type analysis in the context of protein biomarker signatures. This paper focuses on local level non-rigid registration using a nuclei-location based point set registration approach for aligning multi-stained WSIs. We exploit the spatial distribution of nuclei that is prominent and consistent (to a large level) across different stains to establish a spatial correspondence. We evaluate our approach using the HYRECO dataset consisting of 54 re-stained images of H\&E and PHH3 image pairs. The approach can be extended to other IHC and IF stained WSIs considering a good nuclei detection algorithm is accessible. The performance of the model is tested against established registration algorithms and is shown to outperform the model for nuclei level registration.",True,True,"Jeyasangar, Adith and Alsalemi, Abdullah and Raza, Shan E. Ahmed",2024,,https://link.springer.com/10.1007/978-3-031-66955-2_26,10.1007/978-3-031-66955-2_26,
CORE - A Cell-Level Coarse-to-Fine Image Registration Engine for Multi-stain Image Alignment,2511.03826v2,van_der_laak_hyreco_2021,\cite{van_der_laak_hyreco_2021},{HyReCo} - {Hybrid} re-stained and consecutive histological serial sections,,,True,False,"van der Laak, Jeroen and Lotz, Johannes and Weiss, Nick and Heldmann, Stefan",2021,,https://ieee-dataport.org/open-access/hyreco-hybrid-re-stained-and-consecutive-histological-serial-sections,10.21227/PZJ5-BS61,
CORE - A Cell-Level Coarse-to-Fine Image Registration Engine for Multi-stain Image Alignment,2511.03826v2,jiang_multimodal_2024,\cite{jiang_multimodal_2024},Multimodal Alignment of Histopathological Images Using Cell Segmentation and Point Set Matching for Integrative Cancer Analysis,https://arxiv.org/abs/2410.00152v1,"Histopathological imaging is vital for cancer research and clinical practice, with multiplexed Immunofluorescence (MxIF) and Hematoxylin and Eosin (H&E) providing complementary insights. However, aligning different stains at the cell level remains a challenge due to modality differences. In this paper, we present a novel framework for multimodal image alignment using cell segmentation outcomes. By treating cells as point sets, we apply Coherent Point Drift (CPD) for initial alignment and refine it with Graph Matching (GM). Evaluated on ovarian cancer tissue microarrays (TMAs), our method achieves high alignment accuracy, enabling integration of cell-level features across modalities and generating virtual H&E images from MxIF data for enhanced clinical interpretation.",True,True,"Jiang, Jun and Moore, Raymond and Novotny, Brenna and Liu, Leo and Fogarty, Zachary and Guo, Ray and Svetomir, Markovic and Wang, Chen",2024,,https://arxiv.org/abs/2410.00152,10.48550/ARXIV.2410.00152,
CORE - A Cell-Level Coarse-to-Fine Image Registration Engine for Multi-stain Image Alignment,2511.03826v2,gatenbee_source_2023,\cite{gatenbee_source_2023},Source data for {VALIS}: {Virtual} {Alignment} of {pathoLogy} {Image} {Series} for multi-gigapixel whole slide images publication,,,True,False,"Gatenbee, Chandler",2023,,https://zenodo.org/record/7962760,10.5281/ZENODO.7962760,
CORE - A Cell-Level Coarse-to-Fine Image Registration Engine for Multi-stain Image Alignment,2511.03826v2,doyle_whole-slide_2023,\cite{doyle_whole-slide_2023},"Whole-{Slide} {Imaging}, {Mutual} {Information} {Registration} for {Multiplex} {Immunohistochemistry} and {Immunofluorescence}",,,True,False,"Doyle, Joshua and Green, Benjamin F. and Eminizer, Margaret and Jimenez-Sanchez, Daniel and Lu, Steve and Engle, Elizabeth L. and Xu, Haiying and Ogurtsova, Aleksandra and Lai, Jonathan and Soto-Diaz, Sigfredo and Roskes, Jeffrey S. and Deutsch, Julie S. and Taube, Janis M. and Sunshine, Joel C. and Szalay, Alexander S.",2023,,https://linkinghub.elsevier.com/retrieve/pii/S0023683723001186,10.1016/j.labinv.2023.100175,Laboratory Investigation
Clinically-aligned Multi-modal Chest X-ray Classification,2511.09581v1,Chehade2024LungXrayReview,\cite{Chehade2024LungXrayReview},A Systematic Review: Classification of Lung Diseases from Chest X-Ray Images Using Deep Learning Algorithms,,,True,False,A. H. Chehade and N. Abdallah and J.-M. Marion and M. Hatt and M. Oueidat and P. Chauvet,2024,,https://doi.org/10.1007/s42979-024-02751-2,10.1007/s42979-024-02751-2,SN Computer Science
Clinically-aligned Multi-modal Chest X-ray Classification,2511.09581v1,Rubin2018,\cite{Rubin2018},Large Scale Automated Reading of Frontal and Lateral Chest X-Rays using Dual Convolutional Neural Networks,https://arxiv.org/abs/1804.07839v2,"The MIMIC-CXR dataset is (to date) the largest released chest x-ray dataset consisting of 473,064 chest x-rays and 206,574 radiology reports collected from 63,478 patients. We present the results of training and evaluating a collection of deep convolutional neural networks on this dataset to recognize multiple common thorax diseases. To the best of our knowledge, this is the first work that trains CNNs for this task on such a large collection of chest x-ray images, which is over four times the size of the largest previously released chest x-ray corpus (ChestX-Ray14). We describe and evaluate individual CNN models trained on frontal and lateral CXR view types. In addition, we present a novel DualNet architecture that emulates routine clinical practice by simultaneously processing both frontal and lateral CXR images obtained from a radiological exam. Our DualNet architecture shows improved performance in recognizing findings in CXR images when compared to applying separate baseline frontal and lateral classifiers.",True,True,J. Rubin and D. Sanghavi and C. Zhao and K. Lee and A. Qadir and M. Xu-Wilson,2018,,,,arXiv preprint
Clinically-aligned Multi-modal Chest X-ray Classification,2511.09581v1,Zhu2021,\cite{Zhu2021},MVC-NET: Multi-View Chest Radiograph Classification Network With Deep Fusion,,,True,False,X. Zhu and Q. Feng,2021,,,10.1109/ISBI48211.2021.9434000,
Clinically-aligned Multi-modal Chest X-ray Classification,2511.09581v1,Kim2023,\cite{Kim2023},CheXFusion: Effective Fusion of Multi-View Features using Transformers for Long-Tailed Chest X-Ray Classification,https://arxiv.org/abs/2308.03968v1,"Medical image classification poses unique challenges due to the long-tailed distribution of diseases, the co-occurrence of diagnostic findings, and the multiple views available for each study or patient. This paper introduces our solution to the ICCV CVAMD 2023 Shared Task on CXR-LT: Multi-Label Long-Tailed Classification on Chest X-Rays. Our approach introduces CheXFusion, a transformer-based fusion module incorporating multi-view images. The fusion module, guided by self-attention and cross-attention mechanisms, efficiently aggregates multi-view features while considering label co-occurrence. Furthermore, we explore data balancing and self-training methods to optimize the model's performance. Our solution achieves state-of-the-art results with 0.372 mAP in the MIMIC-CXR test set, securing 1st place in the competition. Our success in the task underscores the significance of considering multi-view settings, class imbalance, and label co-occurrence in medical image classification. Public code is available at https://github.com/dongkyuk/CXR-LT-public-solution",True,True,D. Kim,2023,,,,
Clinically-aligned Multi-modal Chest X-ray Classification,2511.09581v1,CXR-LT2024,\cite{CXR-LT2024},"CXR-LT 2024: A MICCAI challenge on long-tailed, multi-label, and zero-shot disease classification from chest X-ray",https://arxiv.org/abs/2506.07984v1,"The CXR-LT series is a community-driven initiative designed to enhance lung disease classification using chest X-rays (CXR). It tackles challenges in open long-tailed lung disease classification and enhances the measurability of state-of-the-art techniques. The first event, CXR-LT 2023, aimed to achieve these goals by providing high-quality benchmark CXR data for model development and conducting comprehensive evaluations to identify ongoing issues impacting lung disease classification performance. Building on the success of CXR-LT 2023, the CXR-LT 2024 expands the dataset to 377,110 chest X-rays (CXRs) and 45 disease labels, including 19 new rare disease findings. It also introduces a new focus on zero-shot learning to address limitations identified in the previous event. Specifically, CXR-LT 2024 features three tasks: (i) long-tailed classification on a large, noisy test set, (ii) long-tailed classification on a manually annotated ""gold standard"" subset, and (iii) zero-shot generalization to five previously unseen disease findings. This paper provides an overview of CXR-LT 2024, detailing the data curation process and consolidating state-of-the-art solutions, including the use of multimodal models for rare disease detection, advanced generative approaches to handle noisy labels, and zero-shot learning strategies for unseen diseases. Additionally, the expanded dataset enhances disease coverage to better represent real-world clinical settings, offering a valuable resource for future research. By synthesizing the insights and innovations of participating teams, we aim to advance the development of clinically realistic and generalizable diagnostic models for chest radiography.",True,True,Mingquan Lin and Gregory Holste and Song Wang and Yiliang Zhou and Yishu Wei and Imon Banerjee and Pengyi Chen and Tianjie Dai and Yuexi Du and Nicha C. Dvornek and Yuyan Ge and Zuwei Guo and Shouhei Hanaoka and Dongkyun Kim and Pablo Messina and Yang Lu and Denis Parra and Donghyun Son and Álvaro Soto and Aisha Urooj and René Vidal and Yosuke Yamagishi and Pingkun Yan and Zefan Yang and Ruichi Zhang and Yang Zhou and Leo Anthony Celi and Ronald M. Summers and Zhiyong Lu and Hao Chen and Adam Flanders and George Shih and Zhangyang Wang and Yifan Peng,2025,,https://www.sciencedirect.com/science/article/pii/S1361841525002865,https://doi.org/10.1016/j.media.2025.103739,Medical Image Analysis
Clinically-aligned Multi-modal Chest X-ray Classification,2511.09581v1,Malik2024,\cite{Malik2024},Multi-modal deep learning methods for classification of chest diseases using different medical imaging and cough sounds,,,True,False,Hassaan Malik and Tayyaba Anees,2024,,,10.1371/journal.pone.0296352,PLOS ONE
Clinically-aligned Multi-modal Chest X-ray Classification,2511.09581v1,Ketabi2023,\cite{Ketabi2023},Multimodal Learning for Improving Performance and Explainability of Chest X-Ray Classification,,,True,False,S. Ketabi and P. Agnihotri and H. Zakeri and K. Namdar and F. Khalvati,2023,,,,
Clinically-aligned Multi-modal Chest X-ray Classification,2511.09581v1,Jacenkow2022,\cite{Jacenkow2022},Indication as Prior Knowledge for Multimodal Disease Classification in Chest Radiographs with Transformers,,,True,False,G. Jacenków and A. Q. O’Neil and S. A. Tsaftaris,2022,,,10.1109/ISBI52829.2022.9761567,
Clinically-aligned Multi-modal Chest X-ray Classification,2511.09581v1,Holste2024,\cite{Holste2024},"Towards long-tailed, multi-label disease classification from chest X-ray: Overview of the CXR-LT challenge",https://arxiv.org/abs/2310.16112v2,"Many real-world image recognition problems, such as diagnostic medical imaging exams, are ""long-tailed"" $\unicode{x2013}$ there are a few common findings followed by many more relatively rare conditions. In chest radiography, diagnosis is both a long-tailed and multi-label problem, as patients often present with multiple findings simultaneously. While researchers have begun to study the problem of long-tailed learning in medical image recognition, few have studied the interaction of label imbalance and label co-occurrence posed by long-tailed, multi-label disease classification. To engage with the research community on this emerging topic, we conducted an open challenge, CXR-LT, on long-tailed, multi-label thorax disease classification from chest X-rays (CXRs). We publicly release a large-scale benchmark dataset of over 350,000 CXRs, each labeled with at least one of 26 clinical findings following a long-tailed distribution. We synthesize common themes of top-performing solutions, providing practical recommendations for long-tailed, multi-label medical image classification. Finally, we use these insights to propose a path forward involving vision-language foundation models for few- and zero-shot disease classification.",True,True,G. Holste and Y. Zhou and S. Wang and A. Jaiswal and M. Lin and S. Zhuge and Y. Yang and D. Kim and T.-H. Nguyen-Mau and M.-T. Tran and J. Jeong and W. Park and J. Ryu and F. Hong and A. Verma and Y. Yamagishi and C. Kim and H. Seo and M. Kang and L. A. Celi and Y. Peng,2024,,,,Medical Image Analysis
Clinically-aligned Multi-modal Chest X-ray Classification,2511.09581v1,Nguyen-Mau2023,\cite{Nguyen-Mau2023},Advanced Augmentation and Ensemble Approaches for Classifying Long-tailed Multi-label Chest X-rays,,,True,False,T. H. Nguyen-Mau and T. L. Huynh and T. D. Le and H. D. Nguyen and M. T. Tran,2023,,,,
Clinically-aligned Multi-modal Chest X-ray Classification,2511.09581v1,Jeong2023,\cite{Jeong2023},An Optimized Ensemble Framework for Multi-label Classification on Long-tailed Chest X-ray Data,,,True,False,J. Jeong and B. Jeoun and Y. Park and B. Han,2023,,,,
Clinically-aligned Multi-modal Chest X-ray Classification,2511.09581v1,Park2023,\cite{Park2023},Robust Asymmetric Loss for Multi-Label Long-Tailed Learning,,,True,False,W. Park and I. Park and S. Kim and J. Ryu,2023,,,,
Clinically-aligned Multi-modal Chest X-ray Classification,2511.09581v1,Hong2023,\cite{Hong2023},Bag of Tricks for Long-Tailed Multi-Label Classification on Chest X-Rays,,,True,False,F. Hong and T. Dai and J. Yao and Y. Zhang and Y. Wang,2023,,,,arXiv preprint arXiv:2308.08853
Clinically-aligned Multi-modal Chest X-ray Classification,2511.09581v1,Verma2023,\cite{Verma2023},How Can We Tame the Long-Tail of Chest X-ray Datasets?,,,True,False,A. Verma,2023,,https://arxiv.org/abs/2309.04293,,
Clinically-aligned Multi-modal Chest X-ray Classification,2511.09581v1,Yamagishi2023,\cite{Yamagishi2023},Effect of Stage Training for Long-Tailed Multi-Label Image Classification,,,True,False,Y. Yamagishi and S. Hanaoka,2023,,,,
Clinically-aligned Multi-modal Chest X-ray Classification,2511.09581v1,KimC2023,\cite{KimC2023},Chest X-ray Feature Pyramid Sum Model with Diseased Area Data Augmentation Method,,,True,False,C. Kim and G. Kim and S. Yang and H. Kim and S. Lee and H. Cho,2023,,,,
Clinically-aligned Multi-modal Chest X-ray Classification,2511.09581v1,Seo2023,\cite{Seo2023},Enhancing Multi-Label Long-Tailed Classification on Chest X-rays through ML-GCN Augmentation,,,True,False,H. Seo and M. Lee and W. Cheong and H. Yoon and S. Kim and M. Kang,2023,,,,
Clinically-aligned Multi-modal Chest X-ray Classification,2511.09581v1,Wang2024GazeGNN,\cite{Wang2024GazeGNN},GazeGNN: A Gaze-Guided Graph Neural Network for Chest X-ray Classification,https://arxiv.org/abs/2305.18221v3,"Eye tracking research is important in computer vision because it can help us understand how humans interact with the visual world. Specifically for high-risk applications, such as in medical imaging, eye tracking can help us to comprehend how radiologists and other medical professionals search, analyze, and interpret images for diagnostic and clinical purposes. Hence, the application of eye tracking techniques in disease classification has become increasingly popular in recent years. Contemporary works usually transform gaze information collected by eye tracking devices into visual attention maps (VAMs) to supervise the learning process. However, this is a time-consuming preprocessing step, which stops us from applying eye tracking to radiologists' daily work. To solve this problem, we propose a novel gaze-guided graph neural network (GNN), GazeGNN, to leverage raw eye-gaze data without being converted into VAMs. In GazeGNN, to directly integrate eye gaze into image classification, we create a unified representation graph that models both images and gaze pattern information. With this benefit, we develop a real-time, real-world, end-to-end disease classification algorithm for the first time in the literature. This achievement demonstrates the practicality and feasibility of integrating real-time eye tracking techniques into the daily work of radiologists. To our best knowledge, GazeGNN is the first work that adopts GNN to integrate image and eye-gaze data. Our experiments on the public chest X-ray dataset show that our proposed method exhibits the best classification performance compared to existing methods. The code is available at https://github.com/ukaukaaaa/GazeGNN.",True,True,B. Wang and H. Pan and A. Aboah and Z. Zhang and E. Keles and D. Torigian and B. Turkbey and E. Krupinski and J. Udupa and U. Bagci,2024,January,,,
Clinically-aligned Multi-modal Chest X-ray Classification,2511.09581v1,Dai2024UniChest,\cite{Dai2024UniChest},UniChest: Conquer-and-Divide Pre-Training for Multi-Source Chest X-Ray Classification,,,True,False,T. Dai and R. Zhang and F. Hong and J. Yao and Y. Zhang and Y. Wang,2024,,,10.1109/TMI.2024.3381123,IEEE Transactions on Medical Imaging
Clinically-aligned Multi-modal Chest X-ray Classification,2511.09581v1,Li2024a,\cite{Li2024a},Automated thorax disease diagnosis using multi-branch residual attention network,,,True,False,"Li, D. and Huo, H. and Jiao, S. and Sun, X. and Chen, S.",2024,,https://doi.org/10.1038/s41598-024-62813-6,10.1038/s41598-024-62813-6,Scientific Reports
Clinically-aligned Multi-modal Chest X-ray Classification,2511.09581v1,Wang2025CrossDomain,\cite{Wang2025CrossDomain},Cross-Domain Invariant Feature Absorption and Domain-Specific Feature Retention for Domain Incremental Chest X-Ray Classification,,,True,False,M. Wang and Y. He and L. Peng and X. Song and S. Dong and Y. Gong,2025,,,10.1109/TMI.2025.3525902,IEEE Transactions on Medical Imaging
Clinically-aligned Multi-modal Chest X-ray Classification,2511.09581v1,agostini2024,\cite{agostini2024},Weakly-Supervised Multimodal Learning on MIMIC-CXR,https://arxiv.org/abs/2411.10356v1,"Multimodal data integration and label scarcity pose significant challenges for machine learning in medical settings. To address these issues, we conduct an in-depth evaluation of the newly proposed Multimodal Variational Mixture-of-Experts (MMVM) VAE on the challenging MIMIC-CXR dataset. Our analysis demonstrates that the MMVM VAE consistently outperforms other multimodal VAEs and fully supervised approaches, highlighting its strong potential for real-world medical applications.",True,True,Andrea Agostini and Daphné Chopard and Yang Meng and Norbert Fortin and Babak Shahbaba and Stephan Mandt and Thomas M. Sutter and Julia E. Vogt,2024,,https://arxiv.org/abs/2411.10356,,
Clinically-aligned Multi-modal Chest X-ray Classification,2511.09581v1,MIMIC,\cite{MIMIC},"MIMIC-CXR, a De-identified Publicly Available Database of Chest Radiographs with Free-text Reports",,,True,False,A. E. W. Johnson and T. J. Pollard and S. J. Berkowitz and N. R. Greenbaum and M. P. Lungren and C. Deng and R. G. Mark and S. Horng,2019,,,,Scientific Data
Clinically-aligned Multi-modal Chest X-ray Classification,2511.09581v1,Li2024,\cite{Li2024},A review of deep learning-based information fusion techniques for multimodal medical image classification,https://arxiv.org/abs/2404.15022v1,"Multimodal medical imaging plays a pivotal role in clinical diagnosis and research, as it combines information from various imaging modalities to provide a more comprehensive understanding of the underlying pathology. Recently, deep learning-based multimodal fusion techniques have emerged as powerful tools for improving medical image classification. This review offers a thorough analysis of the developments in deep learning-based multimodal fusion for medical classification tasks. We explore the complementary relationships among prevalent clinical modalities and outline three main fusion schemes for multimodal classification networks: input fusion, intermediate fusion (encompassing single-level fusion, hierarchical fusion, and attention-based fusion), and output fusion. By evaluating the performance of these fusion techniques, we provide insight into the suitability of different network architectures for various multimodal fusion scenarios and application domains. Furthermore, we delve into challenges related to network architecture selection, handling incomplete multimodal data management, and the potential limitations of multimodal fusion. Finally, we spotlight the promising future of Transformer-based multimodal fusion techniques and give recommendations for future research in this rapidly evolving field.",True,True,Y. Li and M. {El Habib Daho} and P-H. Conze and R. Zeghlache and H. {Le Boité} and R. Tadayoni and B. Cochener and M. Lamard and G. Quellec,2024,,https://www.sciencedirect.com/science/article/pii/S0010482524007200,https://doi.org/10.1016/j.compbiomed.2024.108635,Computers in Biology and Medicine
Clinically-aligned Multi-modal Chest X-ray Classification,2511.09581v1,Samak2023TranSOP,\cite{Samak2023TranSOP},TranSOP: Transformer-based Multimodal Classification for Stroke Treatment Outcome Prediction,https://arxiv.org/abs/2301.10829v1,"Acute ischaemic stroke, caused by an interruption in blood flow to brain tissue, is a leading cause of disability and mortality worldwide. The selection of patients for the most optimal ischaemic stroke treatment is a crucial step for a successful outcome, as the effect of treatment highly depends on the time to treatment. We propose a transformer-based multimodal network (TranSOP) for a classification approach that employs clinical metadata and imaging information, acquired on hospital admission, to predict the functional outcome of stroke treatment based on the modified Rankin Scale (mRS). This includes a fusion module to efficiently combine 3D non-contrast computed tomography (NCCT) features and clinical information. In comparative experiments using unimodal and multimodal data on the MRCLEAN dataset, we achieve a state-of-the-art AUC score of 0.85.",True,True,Z. A. Samak and P. Clatworthy and M. Mirmehdi,2023,,,10.1109/ISBI53787.2023.10230576,
Clinically-aligned Multi-modal Chest X-ray Classification,2511.09581v1,Samak2025StrokeReview,\cite{Samak2025StrokeReview},Automatic Prediction of Stroke Treatment Outcomes: Latest Advances and Perspectives,https://arxiv.org/abs/2412.04812v1,"Stroke is a major global health problem that causes mortality and morbidity. Predicting the outcomes of stroke intervention can facilitate clinical decision-making and improve patient care. Engaging and developing deep learning techniques can help to analyse large and diverse medical data, including brain scans, medical reports and other sensor information, such as EEG, ECG, EMG and so on. Despite the common data standardisation challenge within medical image analysis domain, the future of deep learning in stroke outcome prediction lie in using multimodal information, including final infarct data, to achieve better prediction of long-term functional outcomes. This article provides a broad review of recent advances and applications of deep learning in the prediction of stroke outcomes, including (i) the data and models used, (ii) the prediction tasks and measures of success, (iii) the current challenges and limitations, and (iv) future directions and potential benefits. This comprehensive review aims to provide researchers, clinicians, and policy makers with an up-to-date understanding of this rapidly evolving and promising field.",True,True,Z. A. Samak and P. Clatworthy and M. Mirmehdi,2025,,,10.1007/s13534-025-00462-y,Biomedical Engineering Letters
Clinically-aligned Multi-modal Chest X-ray Classification,2511.09581v1,Hyland2024,\cite{Hyland2024},MAIRA at RRG24: A Specialised Large Multimodal Model for Radiology Report Generation,,,True,False,S. Srivastav and M. Ranjit and F. Pérez-García and K. Bouzid and S. Bannur and D. C. Castro and A. Schwaighofer and H. Sharma and M. Ilse and V. Salvatelli and S. Bond-Taylor and F. Falck and A. Thieme and H. Richardson and M. P. Lungren and S. L. Hyland and J. Alvarez-Valle,2024,August,https://aclanthology.org/2024.bionlp-1.50,10.18653/v1/2024.bionlp-1.50,
Clinically-aligned Multi-modal Chest X-ray Classification,2511.09581v1,Shurrab2024,\cite{Shurrab2024},Multimodal masked siamese network improves chest X-ray representation learning,,,True,False,"Shurrab, S. and Guerra-Manzanares, A. and Shamout, F. E.",2024,,https://doi.org/10.1038/s41598-024-74043-x,10.1038/s41598-024-74043-x,Scientific Reports
Clinically-aligned Multi-modal Chest X-ray Classification,2511.09581v1,Khader2023,\cite{Khader2023},Multimodal Deep Learning for Integrating Chest Radiographs and Clinical Parameters: A Case for Transformers,,,True,False,"Khader, F. and M{\""u}ller-Franzes, G. and Wang, T. and Han, T. and Tayebi Arasteh, S. and Haarburger, C. and Stegmaier, J. and Bressem, K. and Kuhl, C. and Nebelung, S. and Kather, J. N. and Truhn, D.",2023,oct,,10.1148/radiol.230806,Radiology
Clinically-aligned Multi-modal Chest X-ray Classification,2511.09581v1,Banerji2025,\cite{Banerji2025},Clinicians must participate in the development of multimodal AI,,,True,False,"Banerji, Christopher R. S. and Bhardwaj Shah, Aroon and Dabson, Ben and Chakraborti, Tapabrata and Hellon, Vicky and Harbron, Chris and MacArthur, Ben D.",2025,,https://doi.org/10.1016/j.eclinm.2025.103252,10.1016/j.eclinm.2025.103252,eClinicalMedicine
Neurons as Detectors of Coherent Sets in Sensory Dynamics,2510.26955v1,attneave1954some,\cite{attneave1954some},Some informational aspects of visual perception.,,,True,False,"Attneave, Fred",1954,,,,Psychological review
Neurons as Detectors of Coherent Sets in Sensory Dynamics,2510.26955v1,barlow1961possible,\cite{barlow1961possible},Possible principles underlying the transformation of sensory messages,,,True,False,"Barlow, Horace B and others",1961,,,,Sensory communication
Neurons as Detectors of Coherent Sets in Sensory Dynamics,2510.26955v1,simoncelli2001natural,\cite{simoncelli2001natural},Natural image statistics and neural representation,,,True,False,"Simoncelli, Eero P and Olshausen, Bruno A",2001,,,,Annual review of neuroscience
Neurons as Detectors of Coherent Sets in Sensory Dynamics,2510.26955v1,price2022efficient,\cite{price2022efficient},Efficient temporal coding in the early visual system: existing evidence and future directions,,,True,False,"Price, Byron H and Gavornik, Jeffrey P",2022,,,,Frontiers in Computational Neuroscience
Neurons as Detectors of Coherent Sets in Sensory Dynamics,2510.26955v1,bialek2006efficient,\cite{bialek2006efficient},Efficient representation as a design principle for neural coding and computation,,,True,False,"Bialek, William and Van Steveninck, Rob R De Ruyter and Tishby, Naftali",2006,,,,
Neurons as Detectors of Coherent Sets in Sensory Dynamics,2510.26955v1,tishby2000information,\cite{tishby2000information},The information bottleneck method,,,True,False,"Tishby, Naftali and Pereira, Fernando C and Bialek, William",2000,,,,arXiv preprint physics/0004057
Neurons as Detectors of Coherent Sets in Sensory Dynamics,2510.26955v1,palmer2015predictive,\cite{palmer2015predictive},Predictive information in a sensory population,https://arxiv.org/abs/1307.0225v1,"Guiding behavior requires the brain to make predictions about future sensory inputs. Here we show that efficient predictive computation starts at the earliest stages of the visual system. We estimate how much information groups of retinal ganglion cells carry about the future state of their visual inputs, and show that every cell we can observe participates in a group of cells for which this predictive information is close to the physical limit set by the statistical structure of the inputs themselves. Groups of cells in the retina also carry information about the future state of their own activity, and we show that this information can be compressed further and encoded by downstream predictor neurons, which then exhibit interesting feature selectivity. Efficient representation of predictive information is a candidate principle that can be applied at each stage of neural computation.",True,True,"Palmer, Stephanie E and Marre, Olivier and Berry, Michael J and Bialek, William",2015,,,,Proceedings of the National Academy of Sciences
Neurons as Detectors of Coherent Sets in Sensory Dynamics,2510.26955v1,chalk2018toward,\cite{chalk2018toward},"Toward a unified theory of efficient, predictive, and sparse coding",,,True,False,"Chalk, Matthew and Marre, Olivier and Tka{\v{c}}ik, Ga{\v{s}}per",2018,,,,Proceedings of the National Academy of Sciences
Neurons as Detectors of Coherent Sets in Sensory Dynamics,2510.26955v1,chechik2003information,\cite{chechik2003information},Information bottleneck for Gaussian variables,,,True,False,"Chechik, Gal and Globerson, Amir and Tishby, Naftali and Weiss, Yair",2003,,,,Advances in Neural Information Processing Systems
Neurons as Detectors of Coherent Sets in Sensory Dynamics,2510.26955v1,wiskott2002slow,\cite{wiskott2002slow},Slow feature analysis: Unsupervised learning of invariances,,,True,False,"Wiskott, Laurenz and Sejnowski, Terrence J",2002,,,,Neural Computation
Neurons as Detectors of Coherent Sets in Sensory Dynamics,2510.26955v1,lipshutz2020biologically,\cite{lipshutz2020biologically},A biologically plausible neural network for slow feature analysis,,,True,False,"Lipshutz, David and Windolf, Charles and Golkar, Siavash and Chklovskii, Dmitri B",2020,,,,Advances in Neural Information Processing Systems
Neurons as Detectors of Coherent Sets in Sensory Dynamics,2510.26955v1,gregor2012lattice,\cite{gregor2012lattice},A lattice filter model of the visual pathway,,,True,False,"Gregor, Karol and Chklovskii, Dmitri",2012,,,,Advances in Neural Information Processing Systems
Neurons as Detectors of Coherent Sets in Sensory Dynamics,2510.26955v1,srinivasan1982predictive,\cite{srinivasan1982predictive},Predictive coding: a fresh view of inhibition in the retina,,,True,False,"Srinivasan, Mandyam Veerambudi and Laughlin, Simon Barry and Dubs, Andreas",1982,,,,Proceedings of the Royal Society of London. Series B. Biological Sciences
Neurons as Detectors of Coherent Sets in Sensory Dynamics,2510.26955v1,rao1999predictive,\cite{rao1999predictive},Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects,,,True,False,"Rao, Rajesh PN and Ballard, Dana H",1999,,,,Nature neuroscience
Neurons as Detectors of Coherent Sets in Sensory Dynamics,2510.26955v1,druckmann2012mechanistic,\cite{druckmann2012mechanistic},A mechanistic model of early sensory processing based on subtracting sparse representations,,,True,False,"Druckmann, Shaul and Hu, Tao and Chklovskii, Dmitri",2012,,,,Advances in Neural Information Processing Systems
Neurons as Detectors of Coherent Sets in Sensory Dynamics,2510.26955v1,gjorgjieva2014benefits,\cite{gjorgjieva2014benefits},Benefits of pathway splitting in sensory coding,,,True,False,"Gjorgjieva, Julijana and Sompolinsky, Haim and Meister, Markus",2014,,,,Journal of Neuroscience
Neurons as Detectors of Coherent Sets in Sensory Dynamics,2510.26955v1,dong1995temporal,\cite{dong1995temporal},Temporal decorrelation: a theory of lagged and nonlagged responses in the lateral geniculate nucleus,,,True,False,"Dong, Dawei W and Atick, Joseph J",1995,,,,Network: Computation in Neural Systems
Neurons as Detectors of Coherent Sets in Sensory Dynamics,2510.26955v1,cvitanovic2012knowing,\cite{cvitanovic2012knowing},Knowing when to stop: how noise frees us from determinism,https://arxiv.org/abs/1206.5506v1,"Deterministic chaotic dynamics presumes that the state space can be partitioned arbitrarily finely. In a physical system, the inevitable presence of some noise sets a finite limit to the finest possible resolution that can be attained. Much previous research deals with what this attainable resolution might be, all of it based on global averages over a stochastic flow. We show how to compute the locally optimal partition, for a given dynamical system and given noise, in terms of local eigenfunctions of the Fokker-Planck operator and its adjoint. We first analyze the interplay of the deterministic dynamics with the noise in the neighborhood of a periodic orbit of a map, by using a discretized version of Fokker-Planck formalism. Then we propose a method to determine the 'optimal resolution' of the state space, based on solving Fokker-Planck's equation locally, on sets of unstable periodic orbits of the deterministic system. We test our hypothesis on unimodal maps.",True,True,"Cvitanovi{\'c}, Predrag and Lippolis, Domenico",2012,,,,
Neurons as Detectors of Coherent Sets in Sensory Dynamics,2510.26955v1,heninger2015neighborhoods,\cite{heninger2015neighborhoods},Neighborhoods of periodic orbits and the stationary distribution of a noisy chaotic system,,,True,False,"Heninger, Jeffrey M and Lippolis, Domenico and Cvitanovi{\'c}, Predrag",2015,,,,Physical Review E
Neurons as Detectors of Coherent Sets in Sensory Dynamics,2510.26955v1,froyland2010transport,\cite{froyland2010transport},Transport in time-dependent dynamical systems: Finite-time coherent sets,https://arxiv.org/abs/1008.1613v1,"We study the transport properties of nonautonomous chaotic dynamical systems over a finite time duration. We are particularly interested in those regions that remain coherent and relatively non-dispersive over finite periods of time, despite the chaotic nature of the system. We develop a novel probabilistic methodology based upon transfer operators that automatically detects maximally coherent sets. The approach is very simple to implement, requiring only singular vector computations of a matrix of transitions induced by the dynamics. We illustrate our new methodology on an idealized stratospheric flow and in two and three dimensional analyses of European Centre for Medium Range Weather Forecasting (ECMWF) reanalysis data.",True,True,"Froyland, Gary and Santitissadeekorn, Naratip and Monahan, Adam",2010,,,,Chaos: An Interdisciplinary Journal of Nonlinear Science
Neurons as Detectors of Coherent Sets in Sensory Dynamics,2510.26955v1,froyland2013analytic,\cite{froyland2013analytic},An analytic framework for identifying finite-time coherent sets in time-dependent dynamical systems,,,True,False,"Froyland, Gary",2013,,,,Physica D: Nonlinear Phenomena
Neurons as Detectors of Coherent Sets in Sensory Dynamics,2510.26955v1,golkar2024neuronal,\cite{golkar2024neuronal},Neuronal Temporal Filters as Normal Mode Extractors,https://arxiv.org/abs/2401.03248v1,"To generate actions in the face of physiological delays, the brain must predict the future. Here we explore how prediction may lie at the core of brain function by considering a neuron predicting the future of a scalar time series input. Assuming that the dynamics of the lag vector (a vector composed of several consecutive elements of the time series) are locally linear, Normal Mode Decomposition decomposes the dynamics into independently evolving (eigen-)modes allowing for straightforward prediction. We propose that a neuron learns the top mode and projects its input onto the associated subspace. Under this interpretation, the temporal filter of a neuron corresponds to the left eigenvector of a generalized eigenvalue problem. We mathematically analyze the operation of such an algorithm on noisy observations of synthetic data generated by a linear system. Interestingly, the shape of the temporal filter varies with the signal-to-noise ratio (SNR): a noisy input yields a monophasic filter and a growing SNR leads to multiphasic filters with progressively greater number of phases. Such variation in the temporal filter with input SNR resembles that observed experimentally in biological neurons.",True,True,"Golkar, Siavash and Berman, Jules and Lipshutz, David and Haret, Robert Mihai and Gollisch, Tim and Chklovskii, Dmitri B",2024,,,,Physical Review Research
Neurons as Detectors of Coherent Sets in Sensory Dynamics,2510.26955v1,pehlevan2017clustering,\cite{pehlevan2017clustering},A clustering neural network model of insect olfaction,,,True,False,"Cengiz Pehlevan and Genkin, Alexander and Chklovskii, Dmitri B",2017,,,,
Neurons as Detectors of Coherent Sets in Sensory Dynamics,2510.26955v1,ma2006bayesian,\cite{ma2006bayesian},Bayesian inference with probabilistic population codes,,,True,False,"Ma, Wei Ji and Beck, Jeffrey M and Latham, Peter E and Pouget, Alexandre",2006,,,,Nature neuroscience
Neurons as Detectors of Coherent Sets in Sensory Dynamics,2510.26955v1,singer2018sensory,\cite{singer2018sensory},Sensory cortex is optimized for prediction of future input,,,True,False,"Singer, Yosef and Teramoto, Yayoi and Willmore, Ben DB and Schnupp, Jan WH and King, Andrew J and Harper, Nicol S",2018,,,,Elife
Neurons as Detectors of Coherent Sets in Sensory Dynamics,2510.26955v1,choi2024koopman,\cite{choi2024koopman},Koopman AutoEncoder via Singular Value Decomposition for Data-Driven Long-Term Prediction,https://arxiv.org/abs/2408.11303v1,"The Koopman autoencoder, a data-driven technique, has gained traction for modeling nonlinear dynamics using deep learning methods in recent years. Given the linear characteristics inherent to the Koopman operator, controlling its eigenvalues offers an opportunity to enhance long-term prediction performance, a critical task for forecasting future trends in time-series datasets with long-term behaviors. However, controlling eigenvalues is challenging due to high computational complexity and difficulties in managing them during the training process. To tackle this issue, we propose leveraging the singular value decomposition (SVD) of the Koopman matrix to adjust the singular values for better long-term prediction. Experimental results demonstrate that, during training, the loss term for singular values effectively brings the eigenvalues close to the unit circle, and the proposed approach outperforms existing baseline methods for long-term prediction tasks.",True,True,"Choi, Jinho and Krishnan, Sivaram and Park, Jihong",2024,,,,
Neurons as Detectors of Coherent Sets in Sensory Dynamics,2510.26955v1,wu2020variational,\cite{wu2020variational},Variational approach for learning Markov processes from time series data,,,True,False,"Wu, Hao and No{\'e}, Frank",2020,,,,Journal of Nonlinear Science
Neurons as Detectors of Coherent Sets in Sensory Dynamics,2510.26955v1,mardt2018vampnets,\cite{mardt2018vampnets},VAMPnets for deep learning of molecular kinetics,,,True,False,"Mardt, Andreas and Pasquali, Luca and Wu, Hao and No{\'e}, Frank",2018,,,,Nature communications
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,black2023training,\cite{black2023training},Training diffusion models with reinforcement learning,,,True,False,"Black, Kevin and Janner, Michael and Du, Yilun and Kostrikov, Ilya and Levine, Sergey",2023,,,,arXiv preprint arXiv:2305.13301
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,Wallace_2024_CVPR,\cite{Wallace_2024_CVPR},Diffusion Model Alignment Using Direct Preference Optimization,https://arxiv.org/abs/2311.12908v1,"Large language models (LLMs) are fine-tuned using human comparison data with Reinforcement Learning from Human Feedback (RLHF) methods to make them better aligned with users' preferences. In contrast to LLMs, human preference learning has not been widely explored in text-to-image diffusion models; the best existing approach is to fine-tune a pretrained model using carefully curated high quality images and captions to improve visual appeal and text alignment. We propose Diffusion-DPO, a method to align diffusion models to human preferences by directly optimizing on human comparison data. Diffusion-DPO is adapted from the recently developed Direct Preference Optimization (DPO), a simpler alternative to RLHF which directly optimizes a policy that best satisfies human preferences under a classification objective. We re-formulate DPO to account for a diffusion model notion of likelihood, utilizing the evidence lower bound to derive a differentiable objective. Using the Pick-a-Pic dataset of 851K crowdsourced pairwise preferences, we fine-tune the base model of the state-of-the-art Stable Diffusion XL (SDXL)-1.0 model with Diffusion-DPO. Our fine-tuned base model significantly outperforms both base SDXL-1.0 and the larger SDXL-1.0 model consisting of an additional refinement model in human evaluation, improving visual appeal and prompt alignment. We also develop a variant that uses AI feedback and has comparable performance to training on human preferences, opening the door for scaling of diffusion model alignment methods.",True,True,"Wallace, Bram and Dang, Meihua and Rafailov, Rafael and Zhou, Linqi and Lou, Aaron and Purushwalkam, Senthil and Ermon, Stefano and Xiong, Caiming and Joty, Shafiq and Naik, Nikhil",2024,June,,,
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,domingo2024adjoint,\cite{domingo2024adjoint},Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with Memoryless Stochastic Optimal Control,https://arxiv.org/abs/2409.08861v5,"Dynamical generative models that produce samples through an iterative process, such as Flow Matching and denoising diffusion models, have seen widespread use, but there have not been many theoretically-sound methods for improving these models with reward fine-tuning. In this work, we cast reward fine-tuning as stochastic optimal control (SOC). Critically, we prove that a very specific memoryless noise schedule must be enforced during fine-tuning, in order to account for the dependency between the noise variable and the generated samples. We also propose a new algorithm named Adjoint Matching which outperforms existing SOC algorithms, by casting SOC problems as a regression problem. We find that our approach significantly improves over existing methods for reward fine-tuning, achieving better consistency, realism, and generalization to unseen human preference reward models, while retaining sample diversity.",True,True,"Domingo-Enrich, Carles and Drozdzal, Michal and Karrer, Brian and Chen, Ricky TQ",2024,,,,arXiv preprint arXiv:2409.08861
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,uehara2024understanding,\cite{uehara2024understanding},Understanding Reinforcement Learning-Based Fine-Tuning of Diffusion Models: A Tutorial and Review,https://arxiv.org/abs/2407.13734v1,"This tutorial provides a comprehensive survey of methods for fine-tuning diffusion models to optimize downstream reward functions. While diffusion models are widely known to provide excellent generative modeling capability, practical applications in domains such as biology require generating samples that maximize some desired metric (e.g., translation efficiency in RNA, docking score in molecules, stability in protein). In these cases, the diffusion model can be optimized not only to generate realistic samples but also to explicitly maximize the measure of interest. Such methods are based on concepts from reinforcement learning (RL). We explain the application of various RL algorithms, including PPO, differentiable optimization, reward-weighted MLE, value-weighted sampling, and path consistency learning, tailored specifically for fine-tuning diffusion models. We aim to explore fundamental aspects such as the strengths and limitations of different RL-based fine-tuning algorithms across various scenarios, the benefits of RL-based fine-tuning compared to non-RL-based approaches, and the formal objectives of RL-based fine-tuning (target distributions). Additionally, we aim to examine their connections with related topics such as classifier guidance, Gflownets, flow-based diffusion models, path integral control theory, and sampling from unnormalized distributions such as MCMC. The code of this tutorial is available at https://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq",True,True,"Uehara, Masatoshi and Zhao, Yulai and Biancalani, Tommaso and Levine, Sergey",2024,,,,arXiv preprint arXiv:2407.13734
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,fan2023dpok,\cite{fan2023dpok},Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models,,,True,False,"Fan, Ying and Watkins, Olivia and Du, Yuqing and Liu, Hao and Ryu, Moonkyung and Boutilier, Craig and Abbeel, Pieter and Ghavamzadeh, Mohammad and Lee, Kangwook and Lee, Kimin",2023,,,,Advances in Neural Information Processing Systems
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,clark2023directly,\cite{clark2023directly},Directly Fine-Tuning Diffusion Models on Differentiable Rewards,https://arxiv.org/abs/2309.17400v2,"We present Direct Reward Fine-Tuning (DRaFT), a simple and effective method for fine-tuning diffusion models to maximize differentiable reward functions, such as scores from human preference models. We first show that it is possible to backpropagate the reward function gradient through the full sampling procedure, and that doing so achieves strong performance on a variety of rewards, outperforming reinforcement learning-based approaches. We then propose more efficient variants of DRaFT: DRaFT-K, which truncates backpropagation to only the last K steps of sampling, and DRaFT-LV, which obtains lower-variance gradient estimates for the case when K=1. We show that our methods work well for a variety of reward functions and can be used to substantially improve the aesthetic quality of images generated by Stable Diffusion 1.4. Finally, we draw connections between our approach and prior work, providing a unifying perspective on the design space of gradient-based fine-tuning algorithms.",True,True,"Clark, Kevin and Vicol, Paul and Swersky, Kevin and Fleet, David J",2023,,,,arXiv preprint arXiv:2309.17400
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,blessing2025trust,\cite{blessing2025trust},Trust Region Constrained Measure Transport in Path Space for Stochastic Optimal Control and Inference,https://arxiv.org/abs/2508.12511v1,"Solving stochastic optimal control problems with quadratic control costs can be viewed as approximating a target path space measure, e.g. via gradient-based optimization. In practice, however, this optimization is challenging in particular if the target measure differs substantially from the prior. In this work, we therefore approach the problem by iteratively solving constrained problems incorporating trust regions that aim for approaching the target measure gradually in a systematic way. It turns out that this trust region based strategy can be understood as a geometric annealing from the prior to the target measure, where, however, the incorporated trust regions lead to a principled and educated way of choosing the time steps in the annealing path. We demonstrate in multiple optimal control applications that our novel method can improve performance significantly, including tasks in diffusion-based sampling, transition path sampling, and fine-tuning of diffusion models.",True,True,"Blessing, Denis and Berner, Julius and Richter, Lorenz and Domingo-Enrich, Carles and Du, Yuanqi and Vahdat, Arash and Neumann, Gerhard",2025,,,,arXiv preprint arXiv:2508.12511
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,lu2023specialist,\cite{lu2023specialist},Specialist diffusion: Plug-and-play sample-efficient fine-tuning of text-to-image diffusion models to learn any unseen style,,,True,False,"Lu, Haoming and Tunanyan, Hazarapet and Wang, Kai and Navasardyan, Shant and Wang, Zhangyang and Shi, Humphrey",2023,,,,
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,ruiz2023dreambooth,\cite{ruiz2023dreambooth},DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation,https://arxiv.org/abs/2208.12242v2,"Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for ""personalization"" of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions that do not appear in the reference images. We apply our technique to several previously-unassailable tasks, including subject recontextualization, text-guided view synthesis, and artistic rendering, all while preserving the subject's key features. We also provide a new dataset and evaluation protocol for this new task of subject-driven generation. Project page: https://dreambooth.github.io/",True,True,"Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir",2023,,,,
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,gupta2025simple,\cite{gupta2025simple},A simple and effective reinforcement learning method for text-to-image diffusion fine-tuning,,,True,False,"Gupta, Shashank and Ahuja, Chaitanya and Lin, Tsung-Yu and Roy, Sreya Dutta and Oosterhuis, Harrie and de Rijke, Maarten and Shukla, Satya Narayan",2025,,,,arXiv preprint arXiv:2503.00897
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,yuan2024self,\cite{yuan2024self},Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation,https://arxiv.org/abs/2402.10210v1,"Fine-tuning Diffusion Models remains an underexplored frontier in generative artificial intelligence (GenAI), especially when compared with the remarkable progress made in fine-tuning Large Language Models (LLMs). While cutting-edge diffusion models such as Stable Diffusion (SD) and SDXL rely on supervised fine-tuning, their performance inevitably plateaus after seeing a certain volume of data. Recently, reinforcement learning (RL) has been employed to fine-tune diffusion models with human preference data, but it requires at least two images (""winner"" and ""loser"" images) for each text prompt. In this paper, we introduce an innovative technique called self-play fine-tuning for diffusion models (SPIN-Diffusion), where the diffusion model engages in competition with its earlier versions, facilitating an iterative self-improvement process. Our approach offers an alternative to conventional supervised fine-tuning and RL strategies, significantly improving both model performance and alignment. Our experiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperforms the existing supervised fine-tuning method in aspects of human preference alignment and visual appeal right from its first iteration. By the second iteration, it exceeds the performance of RLHF-based methods across all metrics, achieving these results with less data.",True,True,"Yuan, Huizhuo and Chen, Zixiang and Ji, Kaixuan and Gu, Quanquan",2024,,,,Advances in Neural Information Processing Systems
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,fan2023reinforcement,\cite{fan2023reinforcement},Reinforcement learning for fine-tuning text-to-image diffusion models,,,True,False,"Fan, Ying and Watkins, Olivia and Du, Yuqing and Liu, Hao and Ryu, Moonkyung and Boutilier, Craig and Abbeel, Pieter and Ghavamzadeh, Mohammad and Lee, Kangwook and Lee, Kimin",2023,,,,
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,liu2025flow,\cite{liu2025flow},Flow-GRPO: Training Flow Matching Models via Online RL,https://arxiv.org/abs/2505.05470v5,"We propose Flow-GRPO, the first method to integrate online policy gradient reinforcement learning (RL) into flow matching models. Our approach uses two key strategies: (1) an ODE-to-SDE conversion that transforms a deterministic Ordinary Differential Equation (ODE) into an equivalent Stochastic Differential Equation (SDE) that matches the original model's marginal distribution at all timesteps, enabling statistical sampling for RL exploration; and (2) a Denoising Reduction strategy that reduces training denoising steps while retaining the original number of inference steps, significantly improving sampling efficiency without sacrificing performance. Empirically, Flow-GRPO is effective across multiple text-to-image tasks. For compositional generation, RL-tuned SD3.5-M generates nearly perfect object counts, spatial relations, and fine-grained attributes, increasing GenEval accuracy from $63\%$ to $95\%$. In visual text rendering, accuracy improves from $59\%$ to $92\%$, greatly enhancing text generation. Flow-GRPO also achieves substantial gains in human preference alignment. Notably, very little reward hacking occurred, meaning rewards did not increase at the cost of appreciable image quality or diversity degradation.",True,True,"Liu, Jie and Liu, Gongye and Liang, Jiajun and Li, Yangguang and Liu, Jiaheng and Wang, Xintao and Wan, Pengfei and Zhang, Di and Ouyang, Wanli",2025,,,,arXiv preprint arXiv:2505.05470
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,zheng2025diffusionnft,\cite{zheng2025diffusionnft},DiffusionNFT: Online Diffusion Reinforcement with Forward Process,,,True,False,"Zheng, Kaiwen and Chen, Huayu and Ye, Haotian and Wang, Haoxiang and Zhang, Qinsheng and Jiang, Kai and Su, Hang and Ermon, Stefano and Zhu, Jun and Liu, Ming-Yu",2025,,,,arXiv preprint arXiv:2509.16117
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,wang2025finetuning,\cite{wang2025finetuning},Fine-Tuning Discrete Diffusion Models via Reward Optimization with Applications to {DNA} and Protein Design,,,True,False,Chenyu Wang and Masatoshi Uehara and Yichun He and Amy Wang and Avantika Lal and Tommi Jaakkola and Sergey Levine and Aviv Regev and Hanchen and Tommaso Biancalani,2025,,,,
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,zekri2025fine,\cite{zekri2025fine},Fine-Tuning Discrete Diffusion Models with Policy Gradient Methods,https://arxiv.org/abs/2502.01384v2,"Discrete diffusion models have recently gained significant attention due to their ability to process complex discrete structures for language modeling. However, fine-tuning these models with policy gradient methods, as is commonly done in Reinforcement Learning from Human Feedback (RLHF), remains a challenging task. We propose an efficient, broadly applicable, and theoretically justified policy gradient algorithm, called Score Entropy Policy Optimization (SEPO), for fine-tuning discrete diffusion models over non-differentiable rewards. Our numerical experiments across several discrete generative tasks demonstrate the scalability and efficiency of our method. Our code is available at https://github.com/ozekri/SEPO.",True,True,"Zekri, Oussama and Boull{\'e}, Nicolas",2025,,,,arXiv preprint arXiv:2502.01384
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,cao2025glid,\cite{cao2025glid},GLID$^2$E: A Gradient-Free Lightweight Fine-tune Approach for Discrete Sequence Design,,,True,False,"Cao, Hanqun and Shi, Haosen and Wang, Chenyu and Pan, Sinno Jialin and Heng, Pheng-Ann",2025,,,,
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,uehara2024fine,\cite{uehara2024fine},Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized Control,https://arxiv.org/abs/2402.15194v2,"Diffusion models excel at capturing complex data distributions, such as those of natural images and proteins. While diffusion models are trained to represent the distribution in the training dataset, we often are more concerned with other properties, such as the aesthetic quality of the generated images or the functional properties of generated proteins. Diffusion models can be finetuned in a goal-directed way by maximizing the value of some reward function (e.g., the aesthetic quality of an image). However, these approaches may lead to reduced sample diversity, significant deviations from the training data distribution, and even poor sample quality due to the exploitation of an imperfect reward function. The last issue often occurs when the reward function is a learned model meant to approximate a ground-truth ""genuine"" reward, as is the case in many practical applications. These challenges, collectively termed ""reward collapse,"" pose a substantial obstacle. To address this reward collapse, we frame the finetuning problem as entropy-regularized control against the pretrained diffusion model, i.e., directly optimizing entropy-enhanced rewards with neural SDEs. We present theoretical and empirical evidence that demonstrates our framework is capable of efficiently generating diverse samples with high genuine rewards, mitigating the overoptimization of imperfect reward models.",True,True,"Uehara, Masatoshi and Zhao, Yulai and Black, Kevin and Hajiramezanali, Ehsan and Scalia, Gabriele and Diamant, Nathaniel Lee and Tseng, Alex M and Biancalani, Tommaso and Levine, Sergey",2024,,,,arXiv preprint arXiv:2402.15194
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,han2024stochastic,\cite{han2024stochastic},"Stochastic Control for Fine-tuning Diffusion Models: Optimality, Regularity, and Convergence",,,True,False,"Han, Yinbin and Razaviyayn, Meisam and Xu, Renyuan",2024,,,,arXiv preprint arXiv:2412.18164
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,tang2024fine,\cite{tang2024fine},Fine-tuning of diffusion models via stochastic control: entropy regularization and beyond,,,True,False,"Tang, Wenpin",2024,,,,arXiv preprint arXiv:2403.06279
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,zhu2025mdns,\cite{zhu2025mdns},MDNS: Masked Diffusion Neural Sampler via Stochastic Optimal Control,https://arxiv.org/abs/2508.10684v2,"We study the problem of learning a neural sampler to generate samples from discrete state spaces where the target probability mass function $π\propto\mathrm{e}^{-U}$ is known up to a normalizing constant, which is an important task in fields such as statistical physics, machine learning, combinatorial optimization, etc. To better address this challenging task when the state space has a large cardinality and the distribution is multi-modal, we propose $\textbf{M}$asked $\textbf{D}$iffusion $\textbf{N}$eural $\textbf{S}$ampler ($\textbf{MDNS}$), a novel framework for training discrete neural samplers by aligning two path measures through a family of learning objectives, theoretically grounded in the stochastic optimal control of the continuous-time Markov chains. We validate the efficiency and scalability of MDNS through extensive experiments on various distributions with distinct statistical properties, where MDNS learns to accurately sample from the target distributions despite the extremely high problem dimensions and outperforms other learning-based baselines by a large margin. A comprehensive study of ablations and extensions is also provided to demonstrate the efficacy and potential of the proposed framework. Our code is available at https://github.com/yuchen-zhu-zyc/MDNS.",True,True,"Zhu, Yuchen and Guo, Wei and Choi, Jaemoo and Liu, Guan-Horng and Chen, Yongxin and Tao, Molei",2025,,,,arXiv preprint arXiv:2508.10684
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,su2025iterative,\cite{su2025iterative},Iterative Distillation for Reward-Guided Fine-Tuning of Diffusion Models in Biomolecular Design,,,True,False,"Su, Xingyu and Li, Xiner and Uehara, Masatoshi and Kim, Sunwoo and Zhao, Yulai and Scalia, Gabriele and Hajiramezanali, Ehsan and Biancalani, Tommaso and Zhi, Degui and Ji, Shuiwang",2025,,,,arXiv preprint arXiv:2507.00445
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,zhao2025d1,\cite{zhao2025d1},d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning,https://arxiv.org/abs/2504.12216v2,"Recent large language models (LLMs) have demonstrated strong reasoning capabilities that benefits from online reinforcement learning (RL). These capabilities have primarily been demonstrated within the left-to-right autoregressive (AR) generation paradigm. In contrast, non-autoregressive paradigms based on diffusion generate text in a coarse-to-fine manner. Although recent diffusion-based large language models (dLLMs) have achieved competitive language modeling performance compared to their AR counterparts, it remains unclear if dLLMs can also leverage recent advances in LLM reasoning. To this end, we propose d1, a framework to adapt pre-trained masked dLLMs into reasoning models via a combination of supervised finetuning (SFT) and RL. Specifically, we develop and extend techniques to improve reasoning in pretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge and instill self-improvement behavior directly from existing datasets, and (b) we introduce a novel critic-free, policy-gradient based RL algorithm called diffu-GRPO, the first integration of policy gradient methods to masked dLLMs. Through empirical studies, we investigate the performance of different post-training recipes on multiple mathematical and planning benchmarks. We find that d1 yields the best performance and significantly improves performance of a state-of-the-art dLLM. Our code is released at https://dllm-reasoning.github.io/.",True,True,"Zhao, Siyan and Gupta, Devaansh and Zheng, Qinqing and Grover, Aditya",2025,,,,arXiv preprint arXiv:2504.12216
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,gong2025diffucoder,\cite{gong2025diffucoder},DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation,,,True,False,"Gong, Shansan and Zhang, Ruixiang and Zheng, Huangjie and Gu, Jiatao and Jaitly, Navdeep and Kong, Lingpeng and Zhang, Yizhe",2025,,,,arXiv preprint arXiv:2506.20639
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,zhu2025trivialized,\cite{zhu2025trivialized},Trivialized Momentum Facilitates Diffusion Generative Modeling on Lie Groups,https://arxiv.org/abs/2405.16381v2,"The generative modeling of data on manifolds is an important task, for which diffusion models in flat spaces typically need nontrivial adaptations. This article demonstrates how a technique called `trivialization' can transfer the effectiveness of diffusion models in Euclidean spaces to Lie groups. In particular, an auxiliary momentum variable was algorithmically introduced to help transport the position variable between data distribution and a fixed, easy-to-sample distribution. Normally, this would incur further difficulty for manifold data because momentum lives in a space that changes with the position. However, our trivialization technique creates a new momentum variable that stays in a simple fixed vector space. This design, together with a manifold preserving integrator, simplifies implementation and avoids inaccuracies created by approximations such as projections to tangent space and manifold, which were typically used in prior work, hence facilitating generation with high-fidelity and efficiency. The resulting method achieves state-of-the-art performance on protein and RNA torsion angle generation and sophisticated torus datasets. We also, arguably for the first time, tackle the generation of data on high-dimensional Special Orthogonal and Unitary groups, the latter essential for quantum problems. Code is available at https://github.com/yuchen-zhu-zyc/TDM.",True,True,Yuchen Zhu and Tianrong Chen and Lingkai Kong and Evangelos Theodorou and Molei Tao,2025,,https://openreview.net/forum?id=DTatjJTDl1,,
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,esser2024scaling,\cite{esser2024scaling},Scaling Rectified Flow Transformers for High-Resolution Image Synthesis,,,True,False,"Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and M\""{u}ller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and Podell, Dustin and Dockhorn, Tim and English, Zion and Rombach, Robin",2024,21--27 Jul,,,
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,zhu2025diffusion,\cite{zhu2025diffusion},Diffusion Generative Modeling for Spatially Resolved Gene Expression Inference from Histology Images,,,True,False,Sichen Zhu and Yuchen Zhu and Molei Tao and Peng Qiu,2025,,https://openreview.net/forum?id=FtjLUHyZAO,,
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,rojas2025diffuse,\cite{rojas2025diffuse},Diffuse Everything: Multimodal Diffusion Models on Arbitrary State Spaces,https://arxiv.org/abs/2506.07903v2,"Diffusion models have demonstrated remarkable performance in generating unimodal data across various tasks, including image, video, and text generation. On the contrary, the joint generation of multimodal data through diffusion models is still in the early stages of exploration. Existing approaches heavily rely on external preprocessing protocols, such as tokenizers and variational autoencoders, to harmonize varied data representations into a unified, unimodal format. This process heavily demands the high accuracy of encoders and decoders, which can be problematic for applications with limited data. To lift this restriction, we propose a novel framework for building multimodal diffusion models on arbitrary state spaces, enabling native generation of coupled data across different modalities. By introducing an innovative decoupled noise schedule for each modality, we enable both unconditional and modality-conditioned generation within a single model simultaneously. We empirically validate our approach for text-image generation and mixed-type tabular data synthesis, demonstrating that it achieves competitive performance.",True,True,Kevin Rojas and Yuchen Zhu and Sichen Zhu and Felix X-F. Ye and Molei Tao,2025,,https://openreview.net/forum?id=AjbiIcRt6q,,
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,zheng2025direct,\cite{zheng2025direct},Direct Discriminative Optimization: Your Likelihood-Based Visual Generative Model is Secretly a GAN Discriminator,https://arxiv.org/abs/2503.01103v3,"While likelihood-based generative models, particularly diffusion and autoregressive models, have achieved remarkable fidelity in visual generation, the maximum likelihood estimation (MLE) objective, which minimizes the forward KL divergence, inherently suffers from a mode-covering tendency that limits the generation quality under limited model capacity. In this work, we propose Direct Discriminative Optimization (DDO) as a unified framework that integrates likelihood-based generative training and GAN-type discrimination to bypass this fundamental constraint by exploiting reverse KL and self-generated negative signals. Our key insight is to parameterize a discriminator implicitly using the likelihood ratio between a learnable target model and a fixed reference model, drawing parallels with the philosophy of Direct Preference Optimization (DPO). Unlike GANs, this parameterization eliminates the need for joint training of generator and discriminator networks, allowing for direct, efficient, and effective finetuning of a well-trained model to its full potential beyond the limits of MLE. DDO can be performed iteratively in a self-play manner for progressive model refinement, with each round requiring less than 1% of pretraining epochs. Our experiments demonstrate the effectiveness of DDO by significantly advancing the previous SOTA diffusion model EDM, reducing FID scores from 1.79/1.58/1.96 to new records of 1.30/0.97/1.26 on CIFAR-10/ImageNet-64/ImageNet 512x512 datasets without any guidance mechanisms, and by consistently improving both guidance-free and CFG-enhanced FIDs of visual autoregressive models on ImageNet 256x256.",True,True,"Zheng, Kaiwen and Chen, Yongxin and Chen, Huayu and He, Guande and Liu, Ming-Yu and Zhu, Jun and Zhang, Qinsheng",2025,,,,arXiv preprint arXiv:2503.01103
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,austin2021structured,\cite{austin2021structured},Structured Denoising Diffusion Models in Discrete State-Spaces,https://arxiv.org/abs/2107.03006v3,"Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusion-like generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. 2021, by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss. For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on LM1B. On the image dataset CIFAR-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space DDPM model.",True,True,"Austin, Jacob and Johnson, Daniel D and Ho, Jonathan and Tarlow, Daniel and Van Den Berg, Rianne",2021,,,,Advances in neural information processing systems
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,campbell2022continuous,\cite{campbell2022continuous},A continuous time framework for discrete denoising models,,,True,False,"Campbell, Andrew and Benton, Joe and De Bortoli, Valentin and Rainforth, Thomas and Deligiannidis, George and Doucet, Arnaud",2022,,,,Advances in Neural Information Processing Systems
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,lou2023discrete,\cite{lou2023discrete},Discrete diffusion modeling by estimating the ratios of the data distribution,,,True,False,"Lou, Aaron and Meng, Chenlin and Ermon, Stefano",2023,,,,arXiv preprint arXiv:2310.16834
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,sahoo2024simple,\cite{sahoo2024simple},Simple and effective masked diffusion language models,,,True,False,"Sahoo, Subham and Arriola, Marianne and Schiff, Yair and Gokaslan, Aaron and Marroquin, Edgar and Chiu, Justin and Rush, Alexander and Kuleshov, Volodymyr",2024,,,,Advances in Neural Information Processing Systems
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,wang2024diffusion,\cite{wang2024diffusion},Diffusion Language Models Are Versatile Protein Learners,,,True,False,Xinyou Wang and Zaixiang Zheng and Fei YE and Dongyu Xue and Shujian Huang and Quanquan Gu,2024,,,,
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,shi2024simplified,\cite{shi2024simplified},Simplified and generalized masked diffusion for discrete data,,,True,False,"Shi, Jiaxin and Han, Kehang and Wang, Zhe and Doucet, Arnaud and Titsias, Michalis",2024,,,,Advances in neural information processing systems
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,peng2025path,\cite{peng2025path},Path Planning for Masked Diffusion Models with Applications to Biological Sequence Generation,,,True,False,Fred Zhangzhi Peng and Zachary Bezemek and Sawan Patel and Jarrid Rector-Brooks and Sherwood Yao and Alexander Tong and Pranam Chatterjee,2025,,,,
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,tang2025peptune,\cite{tang2025peptune},PepTune: De Novo Generation of Therapeutic Peptides with Multi-Objective-Guided Discrete Diffusion,https://arxiv.org/abs/2412.17780v4,"We present PepTune, a multi-objective discrete diffusion model for simultaneous generation and optimization of therapeutic peptide SMILES. Built on the Masked Discrete Language Model (MDLM) framework, PepTune ensures valid peptide structures with a novel bond-dependent masking schedule and invalid loss function. To guide the diffusion process, we introduce Monte Carlo Tree Guidance (MCTG), an inference-time multi-objective guidance algorithm that balances exploration and exploitation to iteratively refine Pareto-optimal sequences. MCTG integrates classifier-based rewards with search-tree expansion, overcoming gradient estimation challenges and data sparsity. Using PepTune, we generate diverse, chemically-modified peptides simultaneously optimized for multiple therapeutic properties, including target binding affinity, membrane permeability, solubility, hemolysis, and non-fouling for various disease-relevant targets. In total, our results demonstrate that MCTG for masked discrete diffusion is a powerful and modular approach for multi-objective sequence design in discrete state spaces.",True,True,"Tang, Sophia and Zhang, Yinuo and Chatterjee, Pranam",2025,,,,42nd International Conference of Machine Learning (ICML 2025)
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,nisonoff2025unlocking,\cite{nisonoff2025unlocking},Unlocking Guidance for Discrete State-Space Diffusion and Flow Models,,,True,False,Hunter Nisonoff and Junhao Xiong and Stephan Allenspach and Jennifer Listgarten,2025,,,,
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,rector-brooks2025steering,\cite{rector-brooks2025steering},Steering Masked Discrete Diffusion Models via Discrete Denoising Posterior Prediction,https://arxiv.org/abs/2410.08134v1,"Generative modeling of discrete data underlies important applications spanning text-based agents like ChatGPT to the design of the very building blocks of life in protein sequences. However, application domains need to exert control over the generated data by steering the generative process - typically via RLHF - to satisfy a specified property, reward, or affinity metric. In this paper, we study the problem of steering Masked Diffusion Models (MDMs), a recent class of discrete diffusion models that offer a compelling alternative to traditional autoregressive models. We introduce Discrete Denoising Posterior Prediction (DDPP), a novel framework that casts the task of steering pre-trained MDMs as a problem of probabilistic inference by learning to sample from a target Bayesian posterior. Our DDPP framework leads to a family of three novel objectives that are all simulation-free, and thus scalable while applying to general non-differentiable reward functions. Empirically, we instantiate DDPP by steering MDMs to perform class-conditional pixel-level image modeling, RLHF-based alignment of MDMs using text-based rewards, and finetuning protein language models to generate more diverse secondary structures and shorter proteins. We substantiate our designs via wet-lab validation, where we observe transient expression of reward-optimized protein sequences.",True,True,Jarrid Rector-Brooks and Mohsin Hasan and Zhangzhi Peng and Cheng-Hao Liu and Sarthak Mittal and Nouha Dziri and Michael M. Bronstein and Pranam Chatterjee and Alexander Tong and Joey Bose,2025,,,,
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,bai2025meissonic,\cite{bai2025meissonic},Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis,https://arxiv.org/abs/2410.08261v4,"We present Meissonic, which elevates non-autoregressive masked image modeling (MIM) text-to-image to a level comparable with state-of-the-art diffusion models like SDXL. By incorporating a comprehensive suite of architectural innovations, advanced positional encoding strategies, and optimized sampling conditions, Meissonic substantially improves MIM's performance and efficiency. Additionally, we leverage high-quality training data, integrate micro-conditions informed by human preference scores, and employ feature compression layers to further enhance image fidelity and resolution. Our model not only matches but often exceeds the performance of existing models like SDXL in generating high-quality, high-resolution images. Extensive experiments validate Meissonic's capabilities, demonstrating its potential as a new standard in text-to-image synthesis. We release a model checkpoint capable of producing $1024 \times 1024$ resolution images.",True,True,Jinbin Bai and Tian Ye and Wei Chow and Enxin Song and Qing-Guo Chen and Xiangtai Li and Zhen Dong and Lei Zhu and Shuicheng YAN,2025,,,,
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,shi2025muddit,\cite{shi2025muddit},Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model,https://arxiv.org/abs/2505.23606v3,"Unified generation models aim to handle diverse tasks across modalities -- such as text generation, image generation, and vision-language reasoning -- within a single architecture and decoding paradigm. Autoregressive unified models suffer from slow inference due to sequential decoding, and non-autoregressive unified models suffer from weak generalization due to limited pretrained backbones. We introduce Muddit, a unified discrete diffusion transformer that enables fast and parallel generation across both text and image modalities. Unlike prior unified diffusion models trained from scratch, Muddit integrates strong visual priors from a pretrained text-to-image backbone with a lightweight text decoder, enabling flexible and high-quality multimodal generation under a unified architecture. Empirical results show that Muddit achieves competitive or superior performance compared to significantly larger autoregressive models in both quality and efficiency. The work highlights the potential of purely discrete diffusion, when equipped with strong visual priors, as a scalable and effective backbone for unified generation.",True,True,"Shi, Qingyu and Bai, Jinbin and Zhao, Zhuoran and Chai, Wenhao and Yu, Kaidong and Wu, Jianzong and Song, Shuangyong and Tong, Yunhai and Li, Xiangtai and Li, Xuelong and others",2025,,,,arXiv preprint arXiv:2505.23606
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,vincoff2025soapia,\cite{vincoff2025soapia},{SOAPIA}: Siamese-Guided Generation of Off Target-Avoiding Protein Interactions with High Target Affinity,,,True,False,Sophia Vincoff and Oscar Davis and Ismail Ilkan Ceylan and Alexander Tong and Joey Bose and Pranam Chatterjee,2025,,,,
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,goel2025memdlm,\cite{goel2025memdlm},Me{MDLM}: De Novo Membrane Protein Design with Property-Guided Discrete Diffusion,,,True,False,Shrey Goel and Vishrut Thoutam and Edgar Mariano Marroquin and Aaron Gokaslan and Arash Firouzbakht and Sophia Vincoff and Volodymyr Kuleshov and Huong T. Kratochvil and Pranam Chatterjee,2025,,,,
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,patel2025multiobjectiveguided,\cite{patel2025multiobjectiveguided},Multi-Objective-Guided Generative Design of m{RNA} with Therapeutic Properties,,,True,False,Sawan Patel and Sophia Tang and Yinuo Zhang and Pranam Chatterjee and Sherwood Yao,2025,,,,
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,arriola2025block,\cite{arriola2025block},Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models,https://arxiv.org/abs/2503.09573v3,"Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with KV caching and parallel token sampling. We propose a recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets a new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences. We provide the code, along with the model weights and blog post on the project page: https://m-arriola.com/bd3lms",True,True,Marianne Arriola and Subham Sekhar Sahoo and Aaron Gokaslan and Zhihan Yang and Zhixuan Qi and Jiaqi Han and Justin T Chiu and Volodymyr Kuleshov,2025,,,,
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,schiff2025simple,\cite{schiff2025simple},Simple Guidance Mechanisms for Discrete Diffusion Models,,,True,False,Yair Schiff and Subham Sekhar Sahoo and Hao Phung and Guanghan Wang and Sam Boshar and Hugo Dalla-torre and Bernardo P de Almeida and Alexander M Rush and Thomas PIERROT and Volodymyr Kuleshov,2025,,,,
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,skreta2025feynman,\cite{skreta2025feynman},"Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts",https://arxiv.org/abs/2503.02819v2,"While score-based generative models are the model of choice across diverse domains, there are limited tools available for controlling inference-time behavior in a principled manner, e.g. for composing multiple pretrained models. Existing classifier-free guidance methods use a simple heuristic to mix conditional and unconditional scores to approximately sample from conditional distributions. However, such methods do not approximate the intermediate distributions, necessitating additional `corrector' steps. In this work, we provide an efficient and principled method for sampling from a sequence of annealed, geometric-averaged, or product distributions derived from pretrained score-based models. We derive a weighted simulation scheme which we call Feynman-Kac Correctors (FKCs) based on the celebrated Feynman-Kac formula by carefully accounting for terms in the appropriate partial differential equations (PDEs). To simulate these PDEs, we propose Sequential Monte Carlo (SMC) resampling algorithms that leverage inference-time scaling to improve sampling quality. We empirically demonstrate the utility of our methods by proposing amortized sampling via inference-time temperature annealing, improving multi-objective molecule generation using pretrained models, and improving classifier-free guidance for text-to-image generation. Our code is available at https://github.com/martaskrt/fkc-diffusion.",True,True,"Skreta, Marta and Akhound-Sadegh, Tara and Ohanesian, Viktor and Bondesan, Roberto and Aspuru-Guzik, Al{\'a}n and Doucet, Arnaud and Brekelmans, Rob and Tong, Alexander and Neklyudov, Kirill",2025,,,,arXiv preprint arXiv:2503.02819
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,singhal2025general,\cite{singhal2025general},A general framework for inference-time scaling and steering of diffusion models,,,True,False,"Singhal, Raghav and Horvitz, Zachary and Teehan, Ryan and Ren, Mengye and Yu, Zhou and McKeown, Kathleen and Ranganath, Rajesh",2025,,,,arXiv preprint arXiv:2501.06848
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,chen2025solving,\cite{chen2025solving},Solving inverse problems via diffusion-based priors: An approximation-free ensemble sampling approach,,,True,False,"Chen, Haoxuan and Ren, Yinuo and Min, Martin Renqiang and Ying, Lexing and Izzo, Zachary",2025,,,,arXiv preprint arXiv:2506.03979
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,sun2022molsearch,\cite{sun2022molsearch},Molsearch: search-based multi-objective molecular generation and property optimization,,,True,False,"Sun, Mengying and Xing, Jing and Meng, Han and Wang, Huijun and Chen, Bin and Zhou, Jiayu",2022,,,,
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,song2020score,\cite{song2020score},Score-Based Generative Modeling through Stochastic Differential Equations,https://arxiv.org/abs/2011.13456v2,"Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.",True,True,"Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben",2020,,,,arXiv preprint arXiv:2011.13456
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,bansal2023universal,\cite{bansal2023universal},Universal guidance for diffusion models,,,True,False,"Bansal, Arpit and Chu, Hong-Min and Schwarzschild, Avi and Sengupta, Soumyadip and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom",2023,,,,
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,chatterjee2018sample,\cite{chatterjee2018sample},The sample size required in importance sampling,,,True,False,"Chatterjee, Sourav and Diaconis, Persi",2018,,,,The Annals of Applied Probability
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,li2024derivative,\cite{li2024derivative},Derivative-free guidance in continuous and discrete diffusion models with soft value-based decoding,,,True,False,"Li, Xiner and Zhao, Yulai and Wang, Chenyu and Scalia, Gabriele and Eraslan, Gokcen and Nair, Surag and Biancalani, Tommaso and Ji, Shuiwang and Regev, Aviv and Levine, Sergey and others",2024,,,,arXiv preprint arXiv:2408.08252
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,jain2025diffusion,\cite{jain2025diffusion},Diffusion Tree Sampling: Scalable inference-time alignment of diffusion models,https://arxiv.org/abs/2506.20701v1,"Adapting a pretrained diffusion model to new objectives at inference time remains an open problem in generative modeling. Existing steering methods suffer from inaccurate value estimation, especially at high noise levels, which biases guidance. Moreover, information from past runs is not reused to improve sample quality, resulting in inefficient use of compute. Inspired by the success of Monte Carlo Tree Search, we address these limitations by casting inference-time alignment as a search problem that reuses past computations. We introduce a tree-based approach that samples from the reward-aligned target density by propagating terminal rewards back through the diffusion chain and iteratively refining value estimates with each additional generation. Our proposed method, Diffusion Tree Sampling (DTS), produces asymptotically exact samples from the target distribution in the limit of infinite rollouts, and its greedy variant, Diffusion Tree Search (DTS$^\star$), performs a global search for high reward samples. On MNIST and CIFAR-10 class-conditional generation, DTS matches the FID of the best-performing baseline with up to $10\times$ less compute. In text-to-image generation and language completion tasks, DTS$^\star$ effectively searches for high reward samples that match best-of-N with up to $5\times$ less compute. By reusing information from previous generations, we get an anytime algorithm that turns additional compute into steadily better samples, providing a scalable approach for inference-time alignment of diffusion models.",True,True,"Jain, Vineet and Sareen, Kusha and Pedramfar, Mohammad and Ravanbakhsh, Siamak",2025,,,,arXiv preprint arXiv:2506.20701
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,zhang2025inference,\cite{zhang2025inference},Inference-time Scaling of Diffusion Models through Classical Search,,,True,False,"Zhang, Xiangcheng and Lin, Haowei and Ye, Haotian and Zou, James and Ma, Jianzhu and Liang, Yitao and Du, Yilun",2025,,,,arXiv preprint arXiv:2505.23614
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,rojas2025theory,\cite{rojas2025theory},Theory-Informed Improvements to Classifier-Free Guidance for Discrete Diffusion Models,https://arxiv.org/abs/2507.08965v1,"Classifier-Free Guidance (CFG) is a widely used technique for conditional generation and improving sample quality in continuous diffusion models, and recent works have extended it to discrete diffusion. This paper theoretically analyzes CFG in the context of masked discrete diffusion, focusing on the role of guidance schedules. Our analysis shows that high guidance early in sampling (when inputs are heavily masked) harms generation quality, while late-stage guidance has a larger effect. These findings provide a theoretical explanation for empirical observations in recent studies on guidance schedules. The analysis also reveals an imperfection of the current CFG implementations. These implementations can unintentionally cause imbalanced transitions, such as unmasking too rapidly during the early stages of generation, which degrades the quality of the resulting samples. To address this, we draw insight from the analysis and propose a novel classifier-free guidance mechanism empirically applicable to any discrete diffusion. Intuitively, our method smoothens the transport between the data distribution and the initial (masked/uniform) distribution, which results in improved sample quality. Remarkably, our method is achievable via a simple one-line code change. The efficacy of our method is empirically demonstrated with experiments on ImageNet (masked discrete diffusion) and QM9 (uniform discrete diffusion).",True,True,"Rojas, Kevin and He, Ye and Lai, Chieh-Hsin and Takida, Yuta and Mitsufuji, Yuki and Tao, Molei",2025,,,,arXiv preprint arXiv:2507.08965
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,guo2024plug,\cite{guo2024plug},Plug-and-play controllable generation for discrete masked models,,,True,False,"Guo, Wei and Zhu, Yuchen and Tao, Molei and Chen, Yongxin",2024,,,,arXiv preprint arXiv:2410.02143
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,ho2022classifier,\cite{ho2022classifier},Classifier-Free Diffusion Guidance,https://arxiv.org/abs/2207.12598v1,"Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.",True,True,"Ho, Jonathan and Salimans, Tim",2022,,,,arXiv preprint arXiv:2207.12598
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,gruver2023protein,\cite{gruver2023protein},Protein design with guided discrete diffusion,,,True,False,"Gruver, Nate and Stanton, Samuel and Frey, Nathan and Rudner, Tim GJ and Hotzel, Isidro and Lafrance-Vanasse, Julien and Rajpal, Arvind and Cho, Kyunghyun and Wilson, Andrew G",2023,,,,Advances in neural information processing systems
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,marler2004survey,\cite{marler2004survey},Survey of multi-objective optimization methods for engineering,,,True,False,"Marler, R Timothy and Arora, Jasbir S",2004,,,,Structural and multidisciplinary optimization
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,jain2017biophysical,\cite{jain2017biophysical},Biophysical properties of the clinical-stage antibody landscape,,,True,False,"Jain, Tushar and Sun, Tingwan and Durand, St{\'e}phanie and Hall, Amy and Houston, Nga Rewa and Nett, Juergen H and Sharkey, Beth and Bobrowicz, Beata and Caffry, Isabelle and Yu, Yao and others",2017,,,,Proceedings of the National Academy of Sciences
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,tagasovska2022pareto,\cite{tagasovska2022pareto},A Pareto-optimal compositional energy-based model for sampling and optimization of protein sequences,https://arxiv.org/abs/2210.10838v1,"Deep generative models have emerged as a popular machine learning-based approach for inverse design problems in the life sciences. However, these problems often require sampling new designs that satisfy multiple properties of interest in addition to learning the data distribution. This multi-objective optimization becomes more challenging when properties are independent or orthogonal to each other. In this work, we propose a Pareto-compositional energy-based model (pcEBM), a framework that uses multiple gradient descent for sampling new designs that adhere to various constraints in optimizing distinct properties. We demonstrate its ability to learn non-convex Pareto fronts and generate sequences that simultaneously satisfy multiple desired properties across a series of real-world antibody design tasks.",True,True,"Tagasovska, Nata{\v{s}}a and Frey, Nathan C and Loukas, Andreas and H{\""o}tzel, Isidro and Lafrance-Vanasse, Julien and Kelly, Ryan Lewis and Wu, Yan and Rajpal, Arvind and Bonneau, Richard and Cho, Kyunghyun and others",2022,,,,arXiv preprint arXiv:2210.10838
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,zhu2023sample,\cite{zhu2023sample},Sample-efficient Multi-objective Molecular Optimization with GFlowNets,https://arxiv.org/abs/2302.04040v2,"Many crucial scientific problems involve designing novel molecules with desired properties, which can be formulated as a black-box optimization problem over the discrete chemical space. In practice, multiple conflicting objectives and costly evaluations (e.g., wet-lab experiments) make the diversity of candidates paramount. Computational methods have achieved initial success but still struggle with considering diversity in both objective and search space. To fill this gap, we propose a multi-objective Bayesian optimization (MOBO) algorithm leveraging the hypernetwork-based GFlowNets (HN-GFN) as an acquisition function optimizer, with the purpose of sampling a diverse batch of candidate molecular graphs from an approximate Pareto front. Using a single preference-conditioned hypernetwork, HN-GFN learns to explore various trade-offs between objectives. We further propose a hindsight-like off-policy strategy to share high-performing molecules among different preferences in order to speed up learning for HN-GFN. We empirically illustrate that HN-GFN has adequate capacity to generalize over preferences. Moreover, experiments in various real-world MOBO settings demonstrate that our framework predominantly outperforms existing methods in terms of candidate quality and sample efficiency. The code is available at https://github.com/violet-sto/HN-GFN.",True,True,"Zhu, Yiheng and Wu, Jialu and Hu, Chaowen and Yan, Jiahuan and Hou, Tingjun and Wu, Jian and others",2023,,,,Advances in Neural Information Processing Systems
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,janson2008molecular,\cite{janson2008molecular},Molecular docking with multi-objective particle swarm optimization,,,True,False,"Janson, Stefan and Merkle, Daniel and Middendorf, Martin",2008,,,,Applied Soft Computing
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,nicolaou2007molecular,\cite{nicolaou2007molecular},Molecular optimization using computational multi-objective methods,,,True,False,"Nicolaou, Christos A and Brown, Nathan and Pattichis, Constantinos S",2007,,,,Current Opinion in Drug Discovery and Development
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,fromer2023computer,\cite{fromer2023computer},Computer-Aided Multi-Objective Optimization in Small Molecule Discovery,https://arxiv.org/abs/2210.07209v1,"Molecular discovery is a multi-objective optimization problem that requires identifying a molecule or set of molecules that balance multiple, often competing, properties. Multi-objective molecular design is commonly addressed by combining properties of interest into a single objective function using scalarization, which imposes assumptions about relative importance and uncovers little about the trade-offs between objectives. In contrast to scalarization, Pareto optimization does not require knowledge of relative importance and reveals the trade-offs between objectives. However, it introduces additional considerations in algorithm design. In this review, we describe pool-based and de novo generative approaches to multi-objective molecular discovery with a focus on Pareto optimization algorithms. We show how pool-based molecular discovery is a relatively direct extension of multi-objective Bayesian optimization and how the plethora of different generative models extend from single-objective to multi-objective optimization in similar ways using non-dominated sorting in the reward function (reinforcement learning) or to select molecules for retraining (distribution learning) or propagation (genetic algorithms). Finally, we discuss some remaining challenges and opportunities in the field, emphasizing the opportunity to adopt Bayesian optimization techniques into multi-objective de novo design.",True,True,"Fromer, Jenna C and Coley, Connor W",2023,,,,Patterns
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,winter2019efficient,\cite{winter2019efficient},Efficient multi-objective molecular optimization in a continuous latent space,,,True,False,"Winter, Robin and Montanari, Floriane and Steffen, Andreas and Briem, Hans and No{\'e}, Frank and Clevert, Djork-Arn{\'e}",2019,,,,Chemical science
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,jin2020multi,\cite{jin2020multi},Multi-Objective Molecule Generation using Interpretable Substructures,https://arxiv.org/abs/2002.03244v3,"Drug discovery aims to find novel compounds with specified chemical property profiles. In terms of generative modeling, the goal is to learn to sample molecules in the intersection of multiple property constraints. This task becomes increasingly challenging when there are many property constraints. We propose to offset this complexity by composing molecules from a vocabulary of substructures that we call molecular rationales. These rationales are identified from molecules as substructures that are likely responsible for each property of interest. We then learn to expand rationales into a full molecule using graph generative models. Our final generative model composes molecules as mixtures of multiple rationale completions, and this mixture is fine-tuned to preserve the properties of interest. We evaluate our model on various drug design tasks and demonstrate significant improvements over state-of-the-art baselines in terms of accuracy, diversity, and novelty of generated compounds.",True,True,"Jin, Wengong and Barzilay, Regina and Jaakkola, Tommi",2020,,,,
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,xie2021mars,\cite{xie2021mars},Mars: Markov molecular sampling for multi-objective drug discovery,,,True,False,"Xie, Yutong and Shi, Chence and Zhou, Hao and Yang, Yuwei and Zhang, Weinan and Yu, Yong and Li, Lei",2021,,,,arXiv preprint arXiv:2103.10432
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,censor1977pareto,\cite{censor1977pareto},Pareto optimality in multiobjective problems,,,True,False,"Censor, Yair",1977,,,,Applied Mathematics and Optimization
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,yang2019efficient,\cite{yang2019efficient},Efficient Computation of Expected Hypervolume Improvement Using Box Decomposition Algorithms,https://arxiv.org/abs/1904.12672v2,"In the field of multi-objective optimization algorithms, multi-objective Bayesian Global Optimization (MOBGO) is an important branch, in addition to evolutionary multi-objective optimization algorithms (EMOAs). MOBGO utilizes Gaussian Process models learned from previous objective function evaluations to decide the next evaluation site by maximizing or minimizing an infill criterion. A common criterion in MOBGO is the Expected Hypervolume Improvement (EHVI), which shows a good performance on a wide range of problems, with respect to exploration and exploitation. However, so far it has been a challenge to calculate exact EHVI values efficiently. In this paper, an efficient algorithm for the computation of the exact EHVI for a generic case is proposed. This efficient algorithm is based on partitioning the integration volume into a set of axis-parallel slices. Theoretically, the upper bound time complexities are improved from previously $O (n^2)$ and $O(n^3)$, for two- and three-objective problems respectively, to $Θ(n\log n)$, which is asymptotically optimal. This article generalizes the scheme in higher dimensional case by utilizing a new hyperbox decomposition technique, which was proposed by D{ä}chert et al, EJOR, 2017. It also utilizes a generalization of the multilayered integration scheme that scales linearly in the number of hyperboxes of the decomposition. The speed comparison shows that the proposed algorithm in this paper significantly reduces computation time. Finally, this decomposition technique is applied in the calculation of the Probability of Improvement (PoI).",True,True,"Yang, Kaifeng and Emmerich, Michael and Deutz, Andr{\'e} and B{\""a}ck, Thomas",2019,,,,Journal of Global Optimization
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,daulton2020differentiable,\cite{daulton2020differentiable},Differentiable expected hypervolume improvement for parallel multi-objective Bayesian optimization,,,True,False,"Daulton, Samuel and Balandat, Maximilian and Bakshy, Eytan",2020,,,,Advances in neural information processing systems
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,ament2023unexpected,\cite{ament2023unexpected},Unexpected improvements to expected improvement for bayesian optimization,,,True,False,"Ament, Sebastian and Daulton, Samuel and Eriksson, David and Balandat, Maximilian and Bakshy, Eytan",2023,,,,Advances in Neural Information Processing Systems
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,daulton2022multi,\cite{daulton2022multi},Multi-Objective Bayesian Optimization over High-Dimensional Search Spaces,https://arxiv.org/abs/2109.10964v4,"Many real world scientific and industrial applications require optimizing multiple competing black-box objectives. When the objectives are expensive-to-evaluate, multi-objective Bayesian optimization (BO) is a popular approach because of its high sample efficiency. However, even with recent methodological advances, most existing multi-objective BO methods perform poorly on search spaces with more than a few dozen parameters and rely on global surrogate models that scale cubically with the number of observations. In this work we propose MORBO, a scalable method for multi-objective BO over high-dimensional search spaces. MORBO identifies diverse globally optimal solutions by performing BO in multiple local regions of the design space in parallel using a coordinated strategy. We show that MORBO significantly advances the state-of-the-art in sample efficiency for several high-dimensional synthetic problems and real world applications, including an optical display design problem and a vehicle design problem with 146 and 222 parameters, respectively. On these problems, where existing BO algorithms fail to scale and perform well, MORBO provides practitioners with order-of-magnitude improvements in sample efficiency over the current approach.",True,True,"Daulton, Samuel and Eriksson, David and Balandat, Maximilian and Bakshy, Eytan",2022,,,,
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,konakovic2020diversity,\cite{konakovic2020diversity},Diversity-guided multi-objective bayesian optimization with batch evaluations,,,True,False,"Konakovic Lukovic, Mina and Tian, Yunsheng and Matusik, Wojciech",2020,,,,Advances in Neural Information Processing Systems
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,zuluaga2016pal,\cite{zuluaga2016pal},e-pal: An active learning approach to the multi-objective optimization problem,,,True,False,"Zuluaga, Marcela and Krause, Andreas and P{\""u}schel, Markus",2016,,,,Journal of Machine Learning Research
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,belakaria2020uncertainty,\cite{belakaria2020uncertainty},Uncertainty-aware search framework for multi-objective Bayesian optimization,,,True,False,"Belakaria, Syrine and Deshwal, Aryan and Jayakodi, Nitthilan Kannappan and Doppa, Janardhan Rao",2020,,,,
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,wang2017max,\cite{wang2017max},Max-value entropy search for efficient Bayesian optimization,,,True,False,"Wang, Zi and Jegelka, Stefanie",2017,,,,
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,suzuki2020multi,\cite{suzuki2020multi},Multi-objective Bayesian Optimization using Pareto-frontier Entropy,https://arxiv.org/abs/1906.00127v2,"This paper studies an entropy-based multi-objective Bayesian optimization (MBO). The entropy search is successful approach to Bayesian optimization. However, for MBO, existing entropy-based methods ignore trade-off among objectives or introduce unreliable approximations. We propose a novel entropy-based MBO called Pareto-frontier entropy search (PFES) by considering the entropy of Pareto-frontier, which is an essential notion of the optimality of the multi-objective problem. Our entropy can incorporate the trade-off relation of the optimal values, and further, we derive an analytical formula without introducing additional approximations or simplifications to the standard entropy search setting. We also show that our entropy computation is practically feasible by using a recursive decomposition technique which has been known in studies of the Pareto hyper-volume computation. Besides the usual MBO setting, in which all the objectives are simultaneously observed, we also consider the ""decoupled"" setting, in which the objective functions can be observed separately. PFES can easily adapt to the decoupled setting by considering the entropy of the marginal density for each output dimension. This approach incorporates dependency among objectives conditioned on Pareto-frontier, which is ignored by the existing method. Our numerical experiments show effectiveness of PFES through several benchmark datasets.",True,True,"Suzuki, Shinya and Takeno, Shion and Tamura, Tomoyuki and Shitara, Kazuki and Karasuyama, Masayuki",2020,,,,
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,hernandez2016predictive,\cite{hernandez2016predictive},Predictive entropy search for multi-objective bayesian optimization,,,True,False,"Hern{\'a}ndez-Lobato, Daniel and Hernandez-Lobato, Jose and Shah, Amar and Adams, Ryan",2016,,,,
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,fernandez2020improved,\cite{fernandez2020improved},Improved Max-value Entropy Search for Multi-objective Bayesian Optimization with Constraints,https://arxiv.org/abs/2011.01150v2,"We present MESMOC+, an improved version of Max-value Entropy search for Multi-Objective Bayesian optimization with Constraints (MESMOC). MESMOC+ can be used to solve constrained multi-objective problems when the objectives and the constraints are expensive to evaluate. MESMOC+ works by minimizing the entropy of the solution of the optimization problem in function space, i.e., the Pareto frontier, to guide the search for the optimum. The cost of MESMOC+ is linear in the number of objectives and constraints. Furthermore, it is often significantly smaller than the cost of alternative methods based on minimizing the entropy of the Pareto set. The reason for this is that it is easier to approximate the required computations in MESMOC+. Moreover, MESMOC+'s acquisition function is expressed as the sum of one acquisition per each black-box (objective or constraint). Thus, it can be used in a decoupled evaluation setting in which one chooses not only the next input location to evaluate, but also which black-box to evaluate there. We compare MESMOC+ with related methods in synthetic and real optimization problems. These experiments show that the entropy estimation provided by MESMOC+ is more accurate than that of previous methods. This leads to better optimization results. MESMOC+ is also competitive with other information-based methods for constrained multi-objective Bayesian optimization, but it is significantly faster.",True,True,"Fern{\'a}ndez-S{\'a}nchez, Daniel and Garrido-Merch{\'a}n, Eduardo C and Hern{\'a}ndez-Lobato, Daniel",2020,,,,arXiv preprint arXiv:2011.01150
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,park2023botied,\cite{park2023botied},BOtied: Multi-objective Bayesian optimization with tied multivariate ranks,https://arxiv.org/abs/2306.00344v2,"Many scientific and industrial applications require the joint optimization of multiple, potentially competing objectives. Multi-objective Bayesian optimization (MOBO) is a sample-efficient framework for identifying Pareto-optimal solutions. At the heart of MOBO is the acquisition function, which determines the next candidate to evaluate by navigating the best compromises among the objectives. In this paper, we show a natural connection between non-dominated solutions and the extreme quantile of the joint cumulative distribution function (CDF). Motivated by this link, we propose the Pareto-compliant CDF indicator and the associated acquisition function, BOtied. BOtied inherits desirable invariance properties of the CDF, and an efficient implementation with copulas allows it to scale to many objectives. Our experiments on a variety of synthetic and real-world problems demonstrate that BOtied outperforms state-of-the-art MOBO acquisition functions while being computationally efficient for many objectives.",True,True,"Park, Ji Won and Tagasovska, Nata{\v{s}}a and Maser, Michael and Ra, Stephen and Cho, Kyunghyun",2023,,,,arXiv preprint arXiv:2306.00344
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,gelbart2014bayesian,\cite{gelbart2014bayesian},Bayesian optimization with unknown constraints,,,True,False,"Gelbart, Michael A and Snoek, Jasper and Adams, Ryan P",2014,,,,arXiv preprint arXiv:1403.5607
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,li2024constrained,\cite{li2024constrained},Constrained Multi-objective Bayesian Optimization through Optimistic Constraints Estimation,,,True,False,"Li, Diantong and Zhang, Fengxue and Liu, Chong and Chen, Yuxin",2024,,,,arXiv preprint arXiv:2411.03641
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,ren2024hyperdpo,\cite{ren2024hyperdpo},Hyperdpo: Conditioned one-shot multi-objective fine-tuning framework,,,True,False,"Ren, Yinuo and Xiao, Tesi and Shavlovsky, Michael and Ying, Lexing and Rahmanian, Holakou",2024,,,,arXiv preprint arXiv:2410.08316
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,ren2024multi,\cite{ren2024multi},Multi-Objective Optimization via Wasserstein-Fisher-Rao Gradient Flow,https://arxiv.org/abs/2311.13159v2,"Multi-objective optimization (MOO) aims to optimize multiple, possibly conflicting objectives with widespread applications. We introduce a novel interacting particle method for MOO inspired by molecular dynamics simulations. Our approach combines overdamped Langevin and birth-death dynamics, incorporating a ""dominance potential"" to steer particles toward global Pareto optimality. In contrast to previous methods, our method is able to relocate dominated particles, making it particularly adept at managing Pareto fronts of complicated geometries. Our method is also theoretically grounded as a Wasserstein-Fisher-Rao gradient flow with convergence guarantees. Extensive experiments confirm that our approach outperforms state-of-the-art methods on challenging synthetic and real-world datasets.",True,True,"Ren, Yinuo and Xiao, Tesi and Gangwani, Tanmay and Rangi, Anshuka and Rahmanian, Holakou and Ying, Lexing and Sanyal, Subhajit",2024,,,,
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,yao2024proud,\cite{yao2024proud},PROUD: PaRetO-gUided Diffusion Model for Multi-objective Generation,https://arxiv.org/abs/2407.04493v1,"Recent advancements in the realm of deep generative models focus on generating samples that satisfy multiple desired properties. However, prevalent approaches optimize these property functions independently, thus omitting the trade-offs among them. In addition, the property optimization is often improperly integrated into the generative models, resulting in an unnecessary compromise on generation quality (i.e., the quality of generated samples). To address these issues, we formulate a constrained optimization problem. It seeks to optimize generation quality while ensuring that generated samples reside at the Pareto front of multiple property objectives. Such a formulation enables the generation of samples that cannot be further improved simultaneously on the conflicting property functions and preserves good quality of generated samples. Building upon this formulation, we introduce the PaRetO-gUided Diffusion model (PROUD), wherein the gradients in the denoising process are dynamically adjusted to enhance generation quality while the generated samples adhere to Pareto optimality. Experimental evaluations on image generation and protein generation tasks demonstrate that our PROUD consistently maintains superior generation quality while approaching Pareto optimality across multiple property functions compared to various baselines.",True,True,"Yao, Yinghua and Pan, Yuangang and Li, Jing and Tsang, Ivor and Yao, Xin",2024,,,,Machine Learning
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,han2023training,\cite{han2023training},Training-free multi-objective diffusion model for 3d molecule generation,,,True,False,"Han, Xu and Shan, Caihua and Shen, Yifei and Xu, Can and Yang, Han and Li, Xiang and Li, Dongsheng",2023,,,,
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,yuan2024moduli,\cite{yuan2024moduli},MODULI: Unlocking Preference Generalization via Diffusion Models for Offline Multi-Objective Reinforcement Learning,https://arxiv.org/abs/2408.15501v2,"Multi-objective Reinforcement Learning (MORL) seeks to develop policies that simultaneously optimize multiple conflicting objectives, but it requires extensive online interactions. Offline MORL provides a promising solution by training on pre-collected datasets to generalize to any preference upon deployment. However, real-world offline datasets are often conservatively and narrowly distributed, failing to comprehensively cover preferences, leading to the emergence of out-of-distribution (OOD) preference areas. Existing offline MORL algorithms exhibit poor generalization to OOD preferences, resulting in policies that do not align with preferences. Leveraging the excellent expressive and generalization capabilities of diffusion models, we propose MODULI (Multi-objective Diffusion Planner with Sliding Guidance), which employs a preference-conditioned diffusion model as a planner to generate trajectories that align with various preferences and derive action for decision-making. To achieve accurate generation, MODULI introduces two return normalization methods under diverse preferences for refining guidance. To further enhance generalization to OOD preferences, MODULI proposes a novel sliding guidance mechanism, which involves training an additional slider adapter to capture the direction of preference changes. Incorporating the slider, it transitions from in-distribution (ID) preferences to generating OOD preferences, patching, and extending the incomplete Pareto front. Extensive experiments on the D4MORL benchmark demonstrate that our algorithm outperforms state-of-the-art Offline MORL baselines, exhibiting excellent generalization to OOD preferences.",True,True,"Yuan, Yifu and Zheng, Zhenrui and Dong, Zibin and Hao, Jianye",2024,,,,arXiv preprint arXiv:2408.15501
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,annadani2025preference,\cite{annadani2025preference},Preference-Guided Diffusion for Multi-Objective Offline Optimization,,,True,False,"Annadani, Yashas and Belakaria, Syrine and Ermon, Stefano and Bauer, Stefan and Engelhardt, Barbara E",2025,,,,arXiv preprint arXiv:2503.17299
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,zhang2025pmodiff,\cite{zhang2025pmodiff},PMODiff: Physics-Informed Multi-Objective Optimization Diffusion Model for Protein-Specific 3D Molecule Generation,,,True,False,"Zhang, Yaoxiang and Wang, Shuang and Ma, Junteng and Zhang, Ze and Song, Tao",2025,,,,Journal of Chemical Information and Modeling
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,jain2023multi,\cite{jain2023multi},Multi-Objective GFlowNets,https://arxiv.org/abs/2210.12765v2,"We study the problem of generating diverse candidates in the context of Multi-Objective Optimization. In many applications of machine learning such as drug discovery and material design, the goal is to generate candidates which simultaneously optimize a set of potentially conflicting objectives. Moreover, these objectives are often imperfect evaluations of some underlying property of interest, making it important to generate diverse candidates to have multiple options for expensive downstream evaluations. We propose Multi-Objective GFlowNets (MOGFNs), a novel method for generating diverse Pareto optimal solutions, based on GFlowNets. We introduce two variants of MOGFNs: MOGFN-PC, which models a family of independent sub-problems defined by a scalarization function, with reward-conditional GFlowNets, and MOGFN-AL, which solves a sequence of sub-problems defined by an acquisition function in an active learning loop. Our experiments on wide variety of synthetic and benchmark tasks demonstrate advantages of the proposed methods in terms of the Pareto performance and importantly, improved candidate diversity, which is the main contribution of this work.",True,True,"Jain, Moksh and Raparthy, Sharath Chandra and Hern{\'a}ndez-Garc{\i}a, Alex and Rector-Brooks, Jarrid and Bengio, Yoshua and Miret, Santiago and Bengio, Emmanuel",2023,,,,
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,yuan2024paretoflow,\cite{yuan2024paretoflow},ParetoFlow: Guided Flows in Multi-Objective Optimization,https://arxiv.org/abs/2412.03718v2,"In offline multi-objective optimization (MOO), we leverage an offline dataset of designs and their associated labels to simultaneously minimize multiple objectives. This setting more closely mirrors complex real-world problems compared to single-objective optimization. Recent works mainly employ evolutionary algorithms and Bayesian optimization, with limited attention given to the generative modeling capabilities inherent in such data. In this study, we explore generative modeling in offline MOO through flow matching, noted for its effectiveness and efficiency. We introduce ParetoFlow, specifically designed to guide flow sampling to approximate the Pareto front. Traditional predictor (classifier) guidance is inadequate for this purpose because it models only a single objective. In response, we propose a multi-objective predictor guidance module that assigns each sample a weight vector, representing a weighted distribution across multiple objective predictions. A local filtering scheme is introduced to address non-convex Pareto fronts. These weights uniformly cover the entire objective space, effectively directing sample generation towards the Pareto front. Since distributions with similar weights tend to generate similar samples, we introduce a neighboring evolution module to foster knowledge sharing among neighboring distributions. This module generates offspring from these distributions, and selects the most promising one for the next iteration. Our method achieves state-of-the-art performance across various tasks.",True,True,"Yuan, Ye and Chen, Can and Pal, Christopher and Liu, Xue",2024,,,,arXiv preprint arXiv:2412.03718
TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion,2509.25171v1,chen2025multi,\cite{chen2025multi},Multi-Objective-Guided Discrete Flow Matching for Controllable Biological Sequence Design,https://arxiv.org/abs/2505.07086v2,"Designing biological sequences that satisfy multiple, often conflicting, functional and biophysical criteria remains a central challenge in biomolecule engineering. While discrete flow matching models have recently shown promise for efficient sampling in high-dimensional sequence spaces, existing approaches address only single objectives or require continuous embeddings that can distort discrete distributions. We present Multi-Objective-Guided Discrete Flow Matching (MOG-DFM), a general framework to steer any pretrained discrete flow matching generator toward Pareto-efficient trade-offs across multiple scalar objectives. At each sampling step, MOG-DFM computes a hybrid rank-directional score for candidate transitions and applies an adaptive hypercone filter to enforce consistent multi-objective progression. We also trained two unconditional discrete flow matching models, PepDFM for diverse peptide generation and EnhancerDFM for functional enhancer DNA generation, as base generation models for MOG-DFM. We demonstrate MOG-DFM's effectiveness in generating peptide binders optimized across five properties (hemolysis, non-fouling, solubility, half-life, and binding affinity), and in designing DNA sequences with specific enhancer classes and DNA shapes. In total, MOG-DFM proves to be a powerful tool for multi-property-guided biomolecule sequence design.",True,True,"Chen, Tong and Zhang, Yinuo and Tang, Sophia and Chatterjee, Pranam",2025,,,,arXiv preprint arXiv:2505.07086
FP-AbDiff: Improving Score-based Antibody Design by Capturing Nonequilibrium Dynamics through the Underlying Fokker-Planck Equation,2511.03113v1,adolf2018rosettaantibodydesign,\cite{adolf2018rosettaantibodydesign},RosettaAntibodyDesign (RAbD): A general framework for computational antibody design,,,True,False,"Adolf-Bryfogle, Jared and Kalyuzhniy, Oleks and Kubitz, Michael and Weitzner, Brian D and Hu, Xiaozhen and Adachi, Yumiko and Schief, William R and Dunbrack Jr, Roland L",2018,,,,PLoS computational biology
FP-AbDiff: Improving Score-based Antibody Design by Capturing Nonequilibrium Dynamics through the Underlying Fokker-Planck Equation,2511.03113v1,kuroda2012computer,\cite{kuroda2012computer},Computer-aided antibody design,,,True,False,"Kuroda, Daisuke and Shirai, Hiroki and Jacobson, Matthew P and Nakamura, Haruki",2012,,,,"Protein engineering, design \& selection"
FP-AbDiff: Improving Score-based Antibody Design by Capturing Nonequilibrium Dynamics through the Underlying Fokker-Planck Equation,2511.03113v1,ruffolo2021deciphering,\cite{ruffolo2021deciphering},Deciphering antibody affinity maturation with language models and weakly supervised learning,,,True,False,"Ruffolo, Jeffrey A and Gray, Jeffrey J and Sulam, Jeremias",2021,,,,arXiv preprint arXiv:2112.07782
FP-AbDiff: Improving Score-based Antibody Design by Capturing Nonequilibrium Dynamics through the Underlying Fokker-Planck Equation,2511.03113v1,elnaggar2007prottrans,\cite{elnaggar2007prottrans},ProtTrans: Towards cracking the language of Life’s code through self-supervised deep learning and high performance computing. arXiv 2020,,,True,False,"Elnaggar, Ahmed and Heinzinger, Michael and Dallago, Christian and Rihawi, Ghalia and Wang, Yu and Jones, Llion and Gibbs, Tom and Feher, Tamas and Angerer, Christoph and Steinegger, Martin and others",2007,,,,arXiv preprint arXiv:2007.06225
FP-AbDiff: Improving Score-based Antibody Design by Capturing Nonequilibrium Dynamics through the Underlying Fokker-Planck Equation,2511.03113v1,shin2021protein,\cite{shin2021protein},Protein design and variant prediction using autoregressive generative models,,,True,False,"Shin, Jung-Eun and Riesselman, Adam J and Kollasch, Aaron W and McMahon, Conor and Simon, Elana and Sander, Chris and Manglik, Aashish and Kruse, Andrew C and Marks, Debora S",2021,,,,Nature communications
FP-AbDiff: Improving Score-based Antibody Design by Capturing Nonequilibrium Dynamics through the Underlying Fokker-Planck Equation,2511.03113v1,xiong2025textguidedmultipropertymolecularoptimization,\cite{xiong2025textguidedmultipropertymolecularoptimization},Text-Guided Multi-Property Molecular Optimization with a Diffusion Language Model,https://arxiv.org/abs/2410.13597v2,"Molecular optimization (MO) is a crucial stage in drug discovery in which task-oriented generated molecules are optimized to meet practical industrial requirements. Existing mainstream MO approaches primarily utilize external property predictors to guide iterative property optimization. However, learning all molecular samples in the vast chemical space is unrealistic for predictors. As a result, errors and noise are inevitably introduced during property prediction due to the nature of approximation. This leads to discrepancy accumulation, generalization reduction and suboptimal molecular candidates. In this paper, we propose a text-guided multi-property molecular optimization method utilizing transformer-based diffusion language model (TransDLM). TransDLM leverages standardized chemical nomenclature as semantic representations of molecules and implicitly embeds property requirements into textual descriptions, thereby mitigating error propagation during diffusion process. By fusing physically and chemically detailed textual semantics with specialized molecular representations, TransDLM effectively integrates diverse information sources to guide precise optimization, which enhances the model's ability to balance structural retention and property enhancement. Additionally, the success of a case study further demonstrates TransDLM's ability to solve practical problems. Experimentally, our approach surpasses state-of-the-art methods in maintaining molecular structural similarity and enhancing chemical properties on the benchmark dataset. The code is available at: https://github.com/Cello2195/TransDLM.",True,True,Yida Xiong and Kun Li and Jiameng Chen and Hongzhi Zhang and Di Lin and Yan Che and Wenbin Hu,2025,,https://arxiv.org/abs/2410.13597,,
FP-AbDiff: Improving Score-based Antibody Design by Capturing Nonequilibrium Dynamics through the Underlying Fokker-Planck Equation,2511.03113v1,jin2022antibody,\cite{jin2022antibody},Antibody-antigen docking and design via hierarchical structure refinement,,,True,False,"Jin, Wengong and Barzilay, Regina and Jaakkola, Tommi",2022,,,,
FP-AbDiff: Improving Score-based Antibody Design by Capturing Nonequilibrium Dynamics through the Underlying Fokker-Planck Equation,2511.03113v1,kong2022conditional,\cite{kong2022conditional},Conditional Antibody Design as 3D Equivariant Graph Translation,https://arxiv.org/abs/2208.06073v6,"Antibody design is valuable for therapeutic usage and biological research. Existing deep-learning-based methods encounter several key issues: 1) incomplete context for Complementarity-Determining Regions (CDRs) generation; 2) incapability of capturing the entire 3D geometry of the input structure; 3) inefficient prediction of the CDR sequences in an autoregressive manner. In this paper, we propose Multi-channel Equivariant Attention Network (MEAN) to co-design 1D sequences and 3D structures of CDRs. To be specific, MEAN formulates antibody design as a conditional graph translation problem by importing extra components including the target antigen and the light chain of the antibody. Then, MEAN resorts to E(3)-equivariant message passing along with a proposed attention mechanism to better capture the geometrical correlation between different components. Finally, it outputs both the 1D sequences and 3D structure via a multi-round progressive full-shot scheme, which enjoys more efficiency and precision against previous autoregressive approaches. Our method significantly surpasses state-of-the-art models in sequence and structure modeling, antigen-binding CDR design, and binding affinity optimization. Specifically, the relative improvement to baselines is about 23% in antigen-binding CDR design and 34% for affinity optimization.",True,True,"Kong, Xiangzhe and Huang, Wenbing and Liu, Yang",2022,,,,arXiv preprint arXiv:2208.06073
FP-AbDiff: Improving Score-based Antibody Design by Capturing Nonequilibrium Dynamics through the Underlying Fokker-Planck Equation,2511.03113v1,kong2023end,\cite{kong2023end},End-to-End Full-Atom Antibody Design,https://arxiv.org/abs/2302.00203v4,"Antibody design is an essential yet challenging task in various domains like therapeutics and biology. There are two major defects in current learning-based methods: 1) tackling only a certain subtask of the whole antibody design pipeline, making them suboptimal or resource-intensive. 2) omitting either the framework regions or side chains, thus incapable of capturing the full-atom geometry. To address these pitfalls, we propose dynamic Multi-channel Equivariant grAph Network (dyMEAN), an end-to-end full-atom model for E(3)-equivariant antibody design given the epitope and the incomplete sequence of the antibody. Specifically, we first explore structural initialization as a knowledgeable guess of the antibody structure and then propose shadow paratope to bridge the epitope-antibody connections. Both 1D sequences and 3D structures are updated via an adaptive multi-channel equivariant encoder that is able to process protein residues of variable sizes when considering full atoms. Finally, the updated antibody is docked to the epitope via the alignment of the shadow paratope. Experiments on epitope-binding CDR-H3 design, complex structure prediction, and affinity optimization demonstrate the superiority of our end-to-end framework and full-atom modeling.",True,True,"Kong, Xiangzhe and Huang, Wenbing and Liu, Yang",2023,,,,arXiv preprint arXiv:2302.00203
FP-AbDiff: Improving Score-based Antibody Design by Capturing Nonequilibrium Dynamics through the Underlying Fokker-Planck Equation,2511.03113v1,jumper2021highly,\cite{jumper2021highly},Highly accurate protein structure prediction with AlphaFold,,,True,False,"Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others",2021,,,,nature
FP-AbDiff: Improving Score-based Antibody Design by Capturing Nonequilibrium Dynamics through the Underlying Fokker-Planck Equation,2511.03113v1,norman2020thermometry,\cite{norman2020thermometry},Thermometry of intermediate level nuclear waste containers in multiple environmental conditions,https://arxiv.org/abs/2010.03395v1,Intermediate level nuclear waste must be stored until it is safe for permanent disposal. Temperature monitoring of waste packages is important to the nuclear decommissioning industry to support management of each package. Phosphor thermometry and thermal imaging have been used to monitor the temperature of intermediate level waste containers within the expected range of environmental storage conditions at the Sellafield Ltd site: temperatures from 10 °C to 25 °C and relative humidities from 60 %rh to 90 %rh. The feasibility of determining internal temperature from external surface temperature measurement in the required range of environmental conditions has been demonstrated.,True,True,"Norman, J and Sposito, A and McMillan, JL and Bond, W and Hayes, M and Simpson, R and Sutton, G and Panicker, V and Machin, G and Jowsey, J and others",2020,,,,arXiv preprint arXiv:2010.03395
FP-AbDiff: Improving Score-based Antibody Design by Capturing Nonequilibrium Dynamics through the Underlying Fokker-Planck Equation,2511.03113v1,ahdritz2024openfold,\cite{ahdritz2024openfold},OpenFold: Retraining AlphaFold2 yields new insights into its learning mechanisms and capacity for generalization,,,True,False,"Ahdritz, Gustaf and Bouatta, Nazim and Floristean, Christina and Kadyan, Sachin and Xia, Qinghui and Gerecke, William and O’Donnell, Timothy J and Berenberg, Daniel and Fisk, Ian and Zanichelli, Niccol{\`o} and others",2024,,,,Nature methods
FP-AbDiff: Improving Score-based Antibody Design by Capturing Nonequilibrium Dynamics through the Underlying Fokker-Planck Equation,2511.03113v1,luo2022antigen,\cite{luo2022antigen},Antigen-specific antibody design and optimization with diffusion-based generative models for protein structures,,,True,False,"Luo, Shitong and Su, Yufeng and Peng, Xingang and Wang, Sheng and Peng, Jian and Ma, Jianzhu",2022,,,,Advances in Neural Information Processing Systems
FP-AbDiff: Improving Score-based Antibody Design by Capturing Nonequilibrium Dynamics through the Underlying Fokker-Planck Equation,2511.03113v1,martinkus2024abdiffuser,\cite{martinkus2024abdiffuser},AbDiffuser: Full-Atom Generation of in vitro Functioning Antibodies,https://arxiv.org/abs/2308.05027v2,"We introduce AbDiffuser, an equivariant and physics-informed diffusion model for the joint generation of antibody 3D structures and sequences. AbDiffuser is built on top of a new representation of protein structure, relies on a novel architecture for aligned proteins, and utilizes strong diffusion priors to improve the denoising process. Our approach improves protein diffusion by taking advantage of domain knowledge and physics-based constraints; handles sequence-length changes; and reduces memory complexity by an order of magnitude, enabling backbone and side chain generation. We validate AbDiffuser in silico and in vitro. Numerical experiments showcase the ability of AbDiffuser to generate antibodies that closely track the sequence and structural properties of a reference set. Laboratory experiments confirm that all 16 HER2 antibodies discovered were expressed at high levels and that 57.1% of the selected designs were tight binders.",True,True,"Martinkus, Karolis and Ludwiczak, Jan and Liang, Wei-Ching and Lafrance-Vanasse, Julien and Hotzel, Isidro and Rajpal, Arvind and Wu, Yan and Cho, Kyunghyun and Bonneau, Richard and Gligorijevic, Vladimir and others",2024,,,,Advances in Neural Information Processing Systems
FP-AbDiff: Improving Score-based Antibody Design by Capturing Nonequilibrium Dynamics through the Underlying Fokker-Planck Equation,2511.03113v1,zhu2024antibody,\cite{zhu2024antibody},"Antibody design using a score-based diffusion model guided by evolutionary, physical and geometric constraints",,,True,False,"Zhu, Tian and Ren, Milong and Zhang, Haicang",2024,,,,
FP-AbDiff: Improving Score-based Antibody Design by Capturing Nonequilibrium Dynamics through the Underlying Fokker-Planck Equation,2511.03113v1,lai2023fp,\cite{lai2023fp},FP-Diffusion: Improving Score-based Diffusion Models by Enforcing the Underlying Score Fokker-Planck Equation,https://arxiv.org/abs/2210.04296v4,"Score-based generative models (SGMs) learn a family of noise-conditional score functions corresponding to the data density perturbed with increasingly large amounts of noise. These perturbed data densities are linked together by the Fokker-Planck equation (FPE), a partial differential equation (PDE) governing the spatial-temporal evolution of a density undergoing a diffusion process. In this work, we derive a corresponding equation called the score FPE that characterizes the noise-conditional scores of the perturbed data densities (i.e., their gradients). Surprisingly, despite the impressive empirical performance, we observe that scores learned through denoising score matching (DSM) fail to fulfill the underlying score FPE, which is an inherent self-consistency property of the ground truth score. We prove that satisfying the score FPE is desirable as it improves the likelihood and the degree of conservativity. Hence, we propose to regularize the DSM objective to enforce satisfaction of the score FPE, and we show the effectiveness of this approach across various datasets.",True,True,"Lai, Chieh-Hsin and Takida, Yuhta and Murata, Naoki and Uesaka, Toshimitsu and Mitsufuji, Yuki and Ermon, Stefano",2023,,,,
KnowMol: Advancing Molecular Large Language Models with Multi-Level Chemical Knowledge,2510.19484v1,chatmol,\cite{chatmol},ChatMol: interactive molecular discovery with natural language,,,True,False,"Zeng, Zheni and Yin, Bangchen and Wang, Shipeng and Liu, Jiarui and Yang, Cheng and Yao, Haishen and Sun, Xingzhi and Sun, Maosong and Xie, Guotong and Liu, Zhiyuan",2024,,,,Bioinformatics
KnowMol: Advancing Molecular Large Language Models with Multi-Level Chemical Knowledge,2510.19484v1,molt5,\cite{molt5},Translation between Molecules and Natural Language,,,True,False,"Edwards, Carl  and
      Lai, Tuan  and
      Ros, Kevin  and
      Honke, Garrett  and
      Cho, Kyunghyun  and
      Ji, Heng",2022,,https://aclanthology.org/2022.emnlp-main.26/,10.18653/v1/2022.emnlp-main.26,
KnowMol: Advancing Molecular Large Language Models with Multi-Level Chemical Knowledge,2510.19484v1,chebi_20,\cite{chebi_20},Text2mol: Cross-modal molecule retrieval with natural language queries,,,True,False,"Edwards, Carl and Zhai, ChengXiang and Ji, Heng",2021,,,,
KnowMol: Advancing Molecular Large Language Models with Multi-Level Chemical Knowledge,2510.19484v1,3d-molm,\cite{3d-molm},3D-MoLM: Towards 3D Molecule-Text Interpretation in Language Models,,,True,False,"Li, Sihang and Liu, Zhiyuan and Luo, Yanchen and Wang, Xiang and He, Xiangnan and Kawaguchi, Kenji  and Chua, Tat-Seng and Tian, Qi",2024,,https://openreview.net/forum?id=xI4yNlkaqh,,
KnowMol: Advancing Molecular Large Language Models with Multi-Level Chemical Knowledge,2510.19484v1,srinivas2024crossing,\cite{srinivas2024crossing},Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design,https://arxiv.org/abs/2408.11866v1,"Molecule design is a multifaceted approach that leverages computational methods and experiments to optimize molecular properties, fast-tracking new drug discoveries, innovative material development, and more efficient chemical processes. Recently, text-based molecule design has emerged, inspired by next-generation AI tasks analogous to foundational vision-language models. Our study explores the use of knowledge-augmented prompting of large language models (LLMs) for the zero-shot text-conditional de novo molecular generation task. Our approach uses task-specific instructions and a few demonstrations to address distributional shift challenges when constructing augmented prompts for querying LLMs to generate molecules consistent with technical descriptions. Our framework proves effective, outperforming state-of-the-art (SOTA) baseline models on benchmark datasets.",True,True,"Srinivas, Sakhinana Sagar and Runkana, Venkataramana",2024,,,,arXiv preprint arXiv:2408.11866
KnowMol: Advancing Molecular Large Language Models with Multi-Level Chemical Knowledge,2510.19484v1,artificiallyR_R,\cite{artificiallyR_R},From Artificially Real to Real: Leveraging Pseudo Data from Large Language Models for Low-Resource Molecule Discovery,https://arxiv.org/abs/2309.05203v3,"Molecule discovery serves as a cornerstone in numerous scientific domains, fueling the development of new materials and innovative drug designs. Recent developments of in-silico molecule discovery have highlighted the promising results of cross-modal techniques, which bridge molecular structures with their descriptive annotations. However, these cross-modal methods frequently encounter the issue of data scarcity, hampering their performance and application. In this paper, we address the low-resource challenge by utilizing artificially-real data generated by Large Language Models (LLMs). We first introduce a retrieval-based prompting strategy to construct high-quality pseudo data, then explore the optimal method to effectively leverage this pseudo data. Experiments show that using pseudo data for domain adaptation outperforms all existing methods, while also requiring a smaller model scale, reduced data size and lower training cost, highlighting its efficiency. Furthermore, our method shows a sustained improvement as the volume of pseudo data increases, revealing the great potential of pseudo data in advancing low-resource cross-modal molecule discovery. Our code and data are available at https://github.com/SCIR-HI/ArtificiallyR2R.",True,True,"Chen, Yuhan and Xi, Nuwa and Du, Yanrui and Wang, Haochun and Chen, Jianyu and Zhao, Sendong and Qin, Bing",2024,,,,
KnowMol: Advancing Molecular Large Language Models with Multi-Level Chemical Knowledge,2510.19484v1,l_m24,\cite{l_m24},{L}+{M}-24: Building a Dataset for {L}anguage+{M}olecules @ {ACL} 2024,,,True,False,"Edwards, Carl  and
      Wang, Qingyun  and
      Zhao, Lawrence  and
      Ji, Heng",2024,,https://aclanthology.org/2024.langmol-1.1/,10.18653/v1/2024.langmol-1.1,
KnowMol: Advancing Molecular Large Language Models with Multi-Level Chemical Knowledge,2510.19484v1,momu,\cite{momu},A Molecular Multimodal Foundation Model Associating Molecule Graphs with Natural Language,https://arxiv.org/abs/2209.05481v1,"Although artificial intelligence (AI) has made significant progress in understanding molecules in a wide range of fields, existing models generally acquire the single cognitive ability from the single molecular modality. Since the hierarchy of molecular knowledge is profound, even humans learn from different modalities including both intuitive diagrams and professional texts to assist their understanding. Inspired by this, we propose a molecular multimodal foundation model which is pretrained from molecular graphs and their semantically related textual data (crawled from published Scientific Citation Index papers) via contrastive learning. This AI model represents a critical attempt that directly bridges molecular graphs and natural language. Importantly, through capturing the specific and complementary information of the two modalities, our proposed model can better grasp molecular expertise. Experimental results show that our model not only exhibits promising performance in cross-modal tasks such as cross-modal retrieval and molecule caption, but also enhances molecular property prediction and possesses capability to generate meaningful molecular graphs from natural language descriptions. We believe that our model would have a broad impact on AI-empowered fields across disciplines such as biology, chemistry, materials, environment, and medicine, among others.",True,True,"Su, Bing and Du, Dazhao and Yang, Zhao and Zhou, Yujie and Li, Jiangmeng and Rao, Anyi and Sun, Hao and Lu, Zhiwu and Wen, Ji-Rong",2022,,,,arXiv preprint arXiv:2209.05481
KnowMol: Advancing Molecular Large Language Models with Multi-Level Chemical Knowledge,2510.19484v1,moleculestm,\cite{moleculestm},Multi-modal Molecule Structure-text Model for Text-based Retrieval and Editing,https://arxiv.org/abs/2212.10789v3,"There is increasing adoption of artificial intelligence in drug discovery. However, existing studies use machine learning to mainly utilize the chemical structures of molecules but ignore the vast textual knowledge available in chemistry. Incorporating textual knowledge enables us to realize new drug design objectives, adapt to text-based instructions and predict complex biological activities. Here we present a multi-modal molecule structure-text model, MoleculeSTM, by jointly learning molecules' chemical structures and textual descriptions via a contrastive learning strategy. To train MoleculeSTM, we construct a large multi-modal dataset, namely, PubChemSTM, with over 280,000 chemical structure-text pairs. To demonstrate the effectiveness and utility of MoleculeSTM, we design two challenging zero-shot tasks based on text instructions, including structure-text retrieval and molecule editing. MoleculeSTM has two main properties: open vocabulary and compositionality via natural language. In experiments, MoleculeSTM obtains the state-of-the-art generalization ability to novel biochemical concepts across various benchmarks.",True,True,"Liu, Shengchao and Nie, Weili and Wang, Chengpeng and Lu, Jiarui and Qiao, Zhuoran and Liu, Ling and Tang, Jian and Xiao, Chaowei and Anandkumar, Animashree",2023,,,,Nature Machine Intelligence
KnowMol: Advancing Molecular Large Language Models with Multi-Level Chemical Knowledge,2510.19484v1,molfm,\cite{molfm},MolFM: A Multimodal Molecular Foundation Model,https://arxiv.org/abs/2307.09484v2,"Molecular knowledge resides within three different modalities of information sources: molecular structures, biomedical documents, and knowledge bases. Effective incorporation of molecular knowledge from these modalities holds paramount significance in facilitating biomedical research. However, existing multimodal molecular foundation models exhibit limitations in capturing intricate connections between molecular structures and texts, and more importantly, none of them attempt to leverage a wealth of molecular expertise derived from knowledge graphs. In this study, we introduce MolFM, a multimodal molecular foundation model designed to facilitate joint representation learning from molecular structures, biomedical texts, and knowledge graphs. We propose cross-modal attention between atoms of molecular structures, neighbors of molecule entities and semantically related texts to facilitate cross-modal comprehension. We provide theoretical analysis that our cross-modal pre-training captures local and global molecular knowledge by minimizing the distance in the feature space between different modalities of the same molecule, as well as molecules sharing similar structures or functions. MolFM achieves state-of-the-art performance on various downstream tasks. On cross-modal retrieval, MolFM outperforms existing models with 12.13% and 5.04% absolute gains under the zero-shot and fine-tuning settings, respectively. Furthermore, qualitative analysis showcases MolFM's implicit ability to provide grounding from molecular substructures and knowledge graphs. Code and models are available on https://github.com/BioFM/OpenBioMed.",True,True,"Luo, Yizhen and Yang, Kai and Hong, Massimo and Liu, Xing Yi and Nie, Zaiqing",2023,,,,arXiv preprint arXiv:2307.09484
KnowMol: Advancing Molecular Large Language Models with Multi-Level Chemical Knowledge,2510.19484v1,GIT-Mol,\cite{GIT-Mol},"GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text",https://arxiv.org/abs/2308.06911v3,"Large language models have made significant strides in natural language processing, enabling innovative applications in molecular science by processing textual representations of molecules. However, most existing language models cannot capture the rich information with complex molecular structures or images. In this paper, we introduce GIT-Mol, a multi-modal large language model that integrates the Graph, Image, and Text information. To facilitate the integration of multi-modal molecular data, we propose GIT-Former, a novel architecture that is capable of aligning all modalities into a unified latent space. We achieve a 5%-10% accuracy increase in properties prediction and a 20.2% boost in molecule generation validity compared to the baselines. With the any-to-language molecular translation strategy, our model has the potential to perform more downstream tasks, such as compound name recognition and chemical reaction prediction.",True,True,"Liu, Pengfei and Ren, Yiming and Tao, Jun and Ren, Zhixiang",2024,,https://doi.org/10.1016/j.compbiomed.2024.108073,10.1016/j.compbiomed.2024.108073,Comput. Biol. Med.
KnowMol: Advancing Molecular Large Language Models with Multi-Level Chemical Knowledge,2510.19484v1,instructmol,\cite{instructmol},{I}nstruct{M}ol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery,,,True,False,"Cao, He  and
      Liu, Zijing  and
      Lu, Xingyu  and
      Yao, Yuan  and
      Li, Yu",2025,,https://aclanthology.org/2025.coling-main.25/,,
KnowMol: Advancing Molecular Large Language Models with Multi-Level Chemical Knowledge,2510.19484v1,drugchat,\cite{drugchat},DrugChat: Towards Enabling ChatGPT-Like Capabilities on Drug Molecule Graphs,https://arxiv.org/abs/2309.03907v1,"A ChatGPT-like system for drug compounds could be a game-changer in pharmaceutical research, accelerating drug discovery, enhancing our understanding of structure-activity relationships, guiding lead optimization, aiding drug repurposing, reducing the failure rate, and streamlining clinical trials. In this work, we make an initial attempt towards enabling ChatGPT-like capabilities on drug molecule graphs, by developing a prototype system DrugChat. DrugChat works in a similar way as ChatGPT. Users upload a compound molecule graph and ask various questions about this compound. DrugChat will answer these questions in a multi-turn, interactive manner. The DrugChat system consists of a graph neural network (GNN), a large language model (LLM), and an adaptor. The GNN takes a compound molecule graph as input and learns a representation for this graph. The adaptor transforms the graph representation produced by the GNN into another representation that is acceptable to the LLM. The LLM takes the compound representation transformed by the adaptor and users' questions about this compound as inputs and generates answers. All these components are trained end-to-end. To train DrugChat, we collected instruction tuning datasets which contain 10,834 drug compounds and 143,517 question-answer pairs. The code and data is available at \url{https://github.com/UCSD-AI4H/drugchat}",True,True,"Liang, Youwei and Zhang, Ruiyi and Zhang, Li and Xie, Pengtao",2023,,,,arXiv preprint arXiv:2309.03907
KnowMol: Advancing Molecular Large Language Models with Multi-Level Chemical Knowledge,2510.19484v1,molca,\cite{molca},{M}ol{CA}: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter,,,True,False,"Liu, Zhiyuan  and
      Li, Sihang  and
      Luo, Yanchen  and
      Fei, Hao  and
      Cao, Yixin  and
      Kawaguchi, Kenji  and
      Wang, Xiang  and
      Chua, Tat-Seng",2023,,https://aclanthology.org/2023.emnlp-main.966/,10.18653/v1/2023.emnlp-main.966,
KnowMol: Advancing Molecular Large Language Models with Multi-Level Chemical Knowledge,2510.19484v1,blip-2,\cite{blip-2},BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models,https://arxiv.org/abs/2301.12597v3,"The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.",True,True,"Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven",2023,,,,
KnowMol: Advancing Molecular Large Language Models with Multi-Level Chemical Knowledge,2510.19484v1,HIGHT,\cite{HIGHT},Hierarchical Graph Tokenization for Molecule-Language Alignment,,,True,False,Yongqiang Chen and Quanming Yao and Juzheng Zhang and James Cheng and Yatao Bian,2025,,https://openreview.net/forum?id=wpbNczwAwV,,
KnowMol: Advancing Molecular Large Language Models with Multi-Level Chemical Knowledge,2510.19484v1,UniMoT,\cite{UniMoT},UniMoT: Unified Molecule-Text Language Model with Discrete Token Representation,https://arxiv.org/abs/2408.00863v2,"The remarkable success of Large Language Models (LLMs) across diverse tasks has driven the research community to extend their capabilities to molecular applications. However, most molecular LLMs employ adapter-based architectures that do not treat molecule and text modalities equally and lack a supervision signal for the molecule modality. To address these issues, we introduce UniMoT, a Unified Molecule-Text LLM adopting a tokenizer-based architecture that expands the vocabulary of LLM with molecule tokens. Specifically, we introduce a Vector Quantization-driven tokenizer that incorporates a Q-Former to bridge the modality gap between molecule and text. This tokenizer transforms molecules into sequences of molecule tokens with causal dependency, encapsulating high-level molecular and textual information. Equipped with this tokenizer, UniMoT can unify molecule and text modalities under a shared token representation and an autoregressive training paradigm, enabling it to interpret molecules as a foreign language and generate them as text. Following a four-stage training scheme, UniMoT emerges as a multi-modal generalist capable of performing both molecule-to-text and text-to-molecule tasks. Extensive experiments demonstrate that UniMoT achieves state-of-the-art performance across a wide range of molecule comprehension and generation tasks.",True,True,Shuhan Guo and Yatao Bian and Ruibing Wang and Nan Yin and Quanming Yao,2025,,https://openreview.net/forum?id=cJy1klzsx6,,
KnowMol: Advancing Molecular Large Language Models with Multi-Level Chemical Knowledge,2510.19484v1,vqvae,\cite{vqvae},Neural Discrete Representation Learning,https://arxiv.org/abs/1711.00937v2,"Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of ""posterior collapse"" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.",True,True,"Van Den Oord, Aaron and Vinyals, Oriol and others",2017,,,,Advances in neural information processing systems
BrainCSD: A Hierarchical Consistency-Driven MoE Foundation Model for Unified Connectome Synthesis and Multitask Brain Trait Prediction,2511.05630v1,shang2023optimization,\cite{shang2023optimization},Optimization of structural connectomes and scaled patterns of structural-functional decoupling in Parkinson's disease,,,True,False,"Shang, Song'an and Wang, Lijuan and Xu, Yao and Zhang, Hongying and Chen, Lanlan and Dou, Weiqiang and Yin, Xindao and Ye, Jing and Chen, Yu-Chen",2023,,,,NeuroImage
BrainCSD: A Hierarchical Consistency-Driven MoE Foundation Model for Unified Connectome Synthesis and Multitask Brain Trait Prediction,2511.05630v1,li2023developing,\cite{li2023developing},Developing a dynamic graph network for interpretable analysis of multi-modal MRI data in parkinson’s disease diagnosis,,,True,False,"Li, Fanshi and Wang, Zhihui and Guo, Yifan and Liu, Congcong and Zhu, Yanjie and Zhou, Yihang and Li, Jun and Liang, Dong and Wang, Haifeng",2023,,,,
BrainCSD: A Hierarchical Consistency-Driven MoE Foundation Model for Unified Connectome Synthesis and Multitask Brain Trait Prediction,2511.05630v1,cui2023adaptive,\cite{cui2023adaptive},An adaptive weighted attention-enhanced deep convolutional neural network for classification of MRI images of Parkinson's disease,,,True,False,"Cui, Xinchun and Chen, Ningning and Zhao, Chao and Li, Jianlong and Zheng, Xiangwei and Liu, Caixia and Yang, Jiahu and Li, Xiuli and Yu, Chao and Liu, Jinxing and others",2023,,,,Journal of Neuroscience Methods
BrainCSD: A Hierarchical Consistency-Driven MoE Foundation Model for Unified Connectome Synthesis and Multitask Brain Trait Prediction,2511.05630v1,ren2023stratifying,\cite{ren2023stratifying},Stratifying ASD and characterizing the functional connectivity of subtypes in resting-state fMRI,,,True,False,"Ren, Pengchen and Bi, Qingshang and Pang, Wenbin and Wang, Meijuan and Zhou, Qionglin and Ye, Xiaoshan and Li, Ling and Xiao, Le",2023,,,,Behavioural Brain Research
BrainCSD: A Hierarchical Consistency-Driven MoE Foundation Model for Unified Connectome Synthesis and Multitask Brain Trait Prediction,2511.05630v1,li2025riemannian,\cite{li2025riemannian},Riemannian manifold-based disentangled representation learning for multi-site functional connectivity analysis,,,True,False,"Li, Wenyang and Wang, Mingliang and Liu, Mingxia and Liu, Qingshan",2025,,,,Neural Networks
BrainCSD: A Hierarchical Consistency-Driven MoE Foundation Model for Unified Connectome Synthesis and Multitask Brain Trait Prediction,2511.05630v1,zhang2023detection,\cite{zhang2023detection},Detection of autism spectrum disorder using fMRI functional connectivity with feature selection and deep learning,,,True,False,"Zhang, Jin and Feng, Fan and Han, Tianyi and Gong, Xiaoli and Duan, Feng",2023,,,,Cognitive Computation
BrainCSD: A Hierarchical Consistency-Driven MoE Foundation Model for Unified Connectome Synthesis and Multitask Brain Trait Prediction,2511.05630v1,carobrainlm,\cite{carobrainlm},BrainLM: A foundation model for brain activity recordings,,,True,False,"Caro, Josue Ortega and de Oliveira Fonseca, Antonio Henrique and Rizvi, Syed A and Rosati, Matteo and Averill, Christopher and Cross, James L and Mittal, Prateek and Zappala, Emanuele and Dhodapkar, Rahul Madhav and Abdallah, Chadi and others",2024,,,,
BrainCSD: A Hierarchical Consistency-Driven MoE Foundation Model for Unified Connectome Synthesis and Multitask Brain Trait Prediction,2511.05630v1,yang2025foundational,\cite{yang2025foundational},A Foundational fMRI Model for Representing Continuous Brain States,,,True,False,"Yang, Li and Guo, Lei and Yuan, Yixuan and Han, Junwei and Hu, Xintao and Zhang, Tuo",2025,,,,IEEE Journal of Biomedical and Health Informatics
BrainCSD: A Hierarchical Consistency-Driven MoE Foundation Model for Unified Connectome Synthesis and Multitask Brain Trait Prediction,2511.05630v1,yang2024brainmass,\cite{yang2024brainmass},BrainMass: Advancing Brain Network Analysis for Diagnosis with Large-scale Self-Supervised Learning,https://arxiv.org/abs/2403.01433v1,"Foundation models pretrained on large-scale datasets via self-supervised learning demonstrate exceptional versatility across various tasks. Due to the heterogeneity and hard-to-collect medical data, this approach is especially beneficial for medical image analysis and neuroscience research, as it streamlines broad downstream tasks without the need for numerous costly annotations. However, there has been limited investigation into brain network foundation models, limiting their adaptability and generalizability for broad neuroscience studies. In this study, we aim to bridge this gap. In particular, (1) we curated a comprehensive dataset by collating images from 30 datasets, which comprises 70,781 samples of 46,686 participants. Moreover, we introduce pseudo-functional connectivity (pFC) to further generates millions of augmented brain networks by randomly dropping certain timepoints of the BOLD signal. (2) We propose the BrainMass framework for brain network self-supervised learning via mask modeling and feature alignment. BrainMass employs Mask-ROI Modeling (MRM) to bolster intra-network dependencies and regional specificity. Furthermore, Latent Representation Alignment (LRA) module is utilized to regularize augmented brain networks of the same participant with similar topological properties to yield similar latent representations by aligning their latent embeddings. Extensive experiments on eight internal tasks and seven external brain disorder diagnosis tasks show BrainMass's superior performance, highlighting its significant generalizability and adaptability. Nonetheless, BrainMass demonstrates powerful few/zero-shot learning abilities and exhibits meaningful interpretation to various diseases, showcasing its potential use for clinical applications.",True,True,"Yang, Yanwu and Ye, Chenfei and Su, Guinan and Zhang, Ziyao and Chang, Zhikai and Chen, Hairui and Chan, Piu and Yu, Yue and Ma, Ting",2024,,,,IEEE transactions on medical imaging
BrainCSD: A Hierarchical Consistency-Driven MoE Foundation Model for Unified Connectome Synthesis and Multitask Brain Trait Prediction,2511.05630v1,rajbhandari2022deepspeed,\cite{rajbhandari2022deepspeed},DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale,https://arxiv.org/abs/2201.05596v2,"As the training of giant dense models hits the boundary on the availability and capability of the hardware resources today, Mixture-of-Experts (MoE) models become one of the most promising model architectures due to their significant training cost reduction compared to a quality-equivalent dense model. Its training cost saving is demonstrated from encoder-decoder models (prior works) to a 5x saving for auto-aggressive language models (this work along with parallel explorations). However, due to the much larger model size and unique architecture, how to provide fast MoE model inference remains challenging and unsolved, limiting its practical usage. To tackle this, we present DeepSpeed-MoE, an end-to-end MoE training and inference solution as part of the DeepSpeed library, including novel MoE architecture designs and model compression techniques that reduce MoE model size by up to 3.7x, and a highly optimized inference system that provides 7.3x better latency and cost compared to existing MoE inference solutions. DeepSpeed-MoE offers an unprecedented scale and efficiency to serve massive MoE models with up to 4.5x faster and 9x cheaper inference compared to quality-equivalent dense models. We hope our innovations and systems help open a promising path to new directions in the large model landscape, a shift from dense to sparse MoE models, where training and deploying higher-quality models with fewer resources becomes more widely possible.",True,True,"Rajbhandari, Samyam and Li, Conglong and Yao, Zhewei and Zhang, Minjia and Aminabadi, Reza Yazdani and Awan, Ammar Ahmad and Rasley, Jeff and He, Yuxiong",2022,,,,
BrainCSD: A Hierarchical Consistency-Driven MoE Foundation Model for Unified Connectome Synthesis and Multitask Brain Trait Prediction,2511.05630v1,li2025uni,\cite{li2025uni},Uni-MoE: Scaling Unified Multimodal LLMs with Mixture of Experts,https://arxiv.org/abs/2405.11273v1,"Recent advancements in Multimodal Large Language Models (MLLMs) underscore the significance of scalable models and data to boost performance, yet this often incurs substantial computational costs. Although the Mixture of Experts (MoE) architecture has been employed to efficiently scale large language and image-text models, these efforts typically involve fewer experts and limited modalities. To address this, our work presents the pioneering attempt to develop a unified MLLM with the MoE architecture, named Uni-MoE that can handle a wide array of modalities. Specifically, it features modality-specific encoders with connectors for a unified multimodal representation. We also implement a sparse MoE architecture within the LLMs to enable efficient training and inference through modality-level data parallelism and expert-level model parallelism. To enhance the multi-expert collaboration and generalization, we present a progressive training strategy: 1) Cross-modality alignment using various connectors with different cross-modality data, 2) Training modality-specific experts with cross-modality instruction data to activate experts' preferences, and 3) Tuning the Uni-MoE framework utilizing Low-Rank Adaptation (LoRA) on mixed multimodal instruction data. We evaluate the instruction-tuned Uni-MoE on a comprehensive set of multimodal datasets. The extensive experimental results demonstrate Uni-MoE's principal advantage of significantly reducing performance bias in handling mixed multimodal datasets, alongside improved multi-expert collaboration and generalization. Our findings highlight the substantial potential of MoE frameworks in advancing MLLMs and the code is available at https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs.",True,True,"Li, Yunxin and Jiang, Shenyuan and Hu, Baotian and Wang, Longyue and Zhong, Wanqi and Luo, Wenhan and Ma, Lin and Zhang, Min",2025,,,,IEEE Transactions on Pattern Analysis and Machine Intelligence
BrainCSD: A Hierarchical Consistency-Driven MoE Foundation Model for Unified Connectome Synthesis and Multitask Brain Trait Prediction,2511.05630v1,chen2025joint,\cite{chen2025joint},Joint image synthesis and fusion with converted features for Alzheimer’s disease diagnosis,,,True,False,"Chen, Zhaodong and Wang, Mingxia and Nan, Fengtao and Yang, Yun and Li, Shunbao and Zhou, Menghui and Qi, Jun and Wang, Hanwen and Yang, Po",2025,,,,Engineering Applications of Artificial Intelligence
BrainCSD: A Hierarchical Consistency-Driven MoE Foundation Model for Unified Connectome Synthesis and Multitask Brain Trait Prediction,2511.05630v1,zuo2024u,\cite{zuo2024u},U-shaped convolutional transformer GAN with multi-resolution consistency loss for restoring brain functional time-series and dementia diagnosis,,,True,False,"Zuo, Qiankun and Li, Ruiheng and Shi, Binghua and Hong, Jin and Zhu, Yanfei and Chen, Xuhang and Wu, Yixian and Guo, Jia",2024,,,,Frontiers in Computational Neuroscience
BrainCSD: A Hierarchical Consistency-Driven MoE Foundation Model for Unified Connectome Synthesis and Multitask Brain Trait Prediction,2511.05630v1,zhao2025diffusion,\cite{zhao2025diffusion},Diffusion transformer-augmented fMRI functional connectivity for enhanced autism spectrum disorder diagnosis,,,True,False,"Zhao, Haokai and Lou, Haowei and Yao, Lina and Zhang, Yu",2025,,,,Journal of Neural Engineering
BrainCSD: A Hierarchical Consistency-Driven MoE Foundation Model for Unified Connectome Synthesis and Multitask Brain Trait Prediction,2511.05630v1,guan2025spatio,\cite{guan2025spatio},Spatio-temporal mapping generative adversarial network for functional connectivity network reconstruction across brain atlases,,,True,False,"Guan, Hongzheng and Jin, Tao and Xiao, Li and Qu, Gang and Wang, Yu-Ping",2025,,,,
BrainCSD: A Hierarchical Consistency-Driven MoE Foundation Model for Unified Connectome Synthesis and Multitask Brain Trait Prediction,2511.05630v1,yuan2024remind,\cite{yuan2024remind},ReMiND: recovery of missing neuroimaging using diffusion models with application to Alzheimer’s disease,,,True,False,"Yuan, Chenxi and Duan, Jinhao and Xu, Kaidi and Tustison, Nicholas J and Hubbard, Rebecca A and Linn, Kristin A",2024,,,,Imaging Neuroscience
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,lavenant2023mathematicaltheorytrajectoryinference,\cite{lavenant2023mathematicaltheorytrajectoryinference},Towards a mathematical theory of trajectory inference,https://arxiv.org/abs/2102.09204v2,"We devise a theoretical framework and a numerical method to infer trajectories of a stochastic process from samples of its temporal marginals. This problem arises in the analysis of single cell RNA-sequencing data, which provide high dimensional measurements of cell states but cannot track the trajectories of the cells over time. We prove that for a class of stochastic processes it is possible to recover the ground truth trajectories from limited samples of the temporal marginals at each time-point, and provide an efficient algorithm to do so in practice. The method we develop, Global Waddington-OT (gWOT), boils down to a smooth convex optimization problem posed globally over all time-points involving entropy-regularized optimal transport. We demonstrate that this problem can be solved efficiently in practice and yields good reconstructions, as we show on several synthetic and real datasets.",True,True,Hugo Lavenant and Stephen Zhang and Young-Heon Kim and Geoffrey Schiebinger,2023,,https://arxiv.org/abs/2102.09204,,
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,hashimoto2016rnn4sc,\cite{hashimoto2016rnn4sc},Learning Population-Level Diffusions with Generative RNNs,,,True,False,"Hashimoto, Tatsunori and Gifford, David and Jaakkola, Tommi",2016,20--22 Jun,https://proceedings.mlr.press/v48/hashimoto16.html,,
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,tong2020trajectorynet,\cite{tong2020trajectorynet},{T}rajectory{N}et: A Dynamic Optimal Transport Network for Modeling Cellular Dynamics,,,True,False,"Tong, Alexander and Huang, Jessie and Wolf, Guy and Van Dijk, David and Krishnaswamy, Smita",2020,13--18 Jul,https://proceedings.mlr.press/v119/tong20a.html,,
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,tong2023trajnet_application,\cite{tong2023trajnet_application},Learning transcriptional and regulatory dynamics driving cancer cell plasticity using neural ODE-based optimal transport,,,True,False,"Tong, Alexander and Kuchroo, Manik and Gupta, Shabarni and Venkat, Aarthi and San Juan, Beatriz P. and Rangel, Laura and Zhu, Brandon and Lock, John G. and Chaffer, Christine L. and Krishnaswamy, Smita",2023,,https://www.biorxiv.org/content/early/2023/03/29/2023.03.28.534644,10.1101/2023.03.28.534644,bioRxiv
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,zhang2024scnode,\cite{zhang2024scnode},scNODE : Generative Model for Temporal Single Cell Transcriptomic Data Prediction,,,True,False,"Zhang, Jiaqi and Larschan, Erica and Bigness, Jeremy and Singh, Ritambhara",2023,,https://www.biorxiv.org/content/early/2023/11/23/2023.11.22.568346,10.1101/2023.11.22.568346,bioRxiv
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,koshizuka2023nlsb,\cite{koshizuka2023nlsb},Neural Lagrangian Schrödinger Bridge: Diffusion Modeling for Population Dynamics,https://arxiv.org/abs/2204.04853v5,"Population dynamics is the study of temporal and spatial variation in the size of populations of organisms and is a major part of population ecology. One of the main difficulties in analyzing population dynamics is that we can only obtain observation data with coarse time intervals from fixed-point observations due to experimental costs or measurement constraints. Recently, modeling population dynamics by using continuous normalizing flows (CNFs) and dynamic optimal transport has been proposed to infer the sample trajectories from a fixed-point observed population. While the sample behavior in CNFs is deterministic, the actual sample in biological systems moves in an essentially random yet directional manner. Moreover, when a sample moves from point A to point B in dynamical systems, its trajectory typically follows the principle of least action in which the corresponding action has the smallest possible value. To satisfy these requirements of the sample trajectories, we formulate the Lagrangian Schrödinger bridge (LSB) problem and propose to solve it approximately by modeling the advection-diffusion process with regularized neural SDE. We also develop a model architecture that enables faster computation of the loss function. Experimental results show that the proposed method can efficiently approximate the population-level dynamics even for high-dimensional data and that using the prior knowledge introduced by the Lagrangian enables us to estimate the sample-level dynamics with stochastic behavior.",True,True,Takeshi Koshizuka and Issei Sato,2023,,https://openreview.net/forum?id=d3QNWD_pcFv,,
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,huguet2022mioflow,\cite{huguet2022mioflow},Manifold Interpolating Optimal-Transport Flows for Trajectory Inference,https://arxiv.org/abs/2206.14928v2,"We present a method called Manifold Interpolating Optimal-Transport Flow (MIOFlow) that learns stochastic, continuous population dynamics from static snapshot samples taken at sporadic timepoints. MIOFlow combines dynamic models, manifold learning, and optimal transport by training neural ordinary differential equations (Neural ODE) to interpolate between static population snapshots as penalized by optimal transport with manifold ground distance. Further, we ensure that the flow follows the geometry by operating in the latent space of an autoencoder that we call a geodesic autoencoder (GAE). In GAE the latent space distance between points is regularized to match a novel multiscale geodesic distance on the data manifold that we define. We show that this method is superior to normalizing flows, Schrödinger bridges and other generative models that are designed to flow from noise to data in terms of interpolating between populations. Theoretically, we link these trajectories with dynamic optimal transport. We evaluate our method on simulated data with bifurcations and merges, as well as scRNA-seq data from embryoid body differentiation, and acute myeloid leukemia treatment.",True,True,Guillaume Huguet and D. S. Magruder and Alexander Tong and Oluwadamilola Fasina and Manik Kuchroo and Guy Wolf and Smita Krishnaswamy,2022,,https://arxiv.org/abs/2206.14928,,
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,schiebinger2019waddingtonot,\cite{schiebinger2019waddingtonot},Optimal-Transport Analysis of Single-Cell Gene Expression Identifies Developmental Trajectories in Reprogramming,,,True,False,Geoffrey Schiebinger and Jian Shu and Marcin Tabaka and Brian Cleary and Vidya Subramanian and Aryeh Solomon and Joshua Gould and Siyan Liu and Stacie Lin and Peter Berube and Lia Lee and Jenny Chen and Justin Brumbaugh and Philippe Rigollet and Konrad Hochedlinger and Rudolf Jaenisch and Aviv Regev and Eric S. Lander,2019,,https://www.sciencedirect.com/science/article/pii/S009286741930039X,https://doi.org/10.1016/j.cell.2019.01.006,Cell
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,bunne2021jkonet,\cite{bunne2021jkonet},JKOnet: Proximal Optimal Transport Modeling of Population Dynamics,,,True,False,"Charlotte Bunne and
                  Laetitia Meng{-}Papaxanthos and
                  Andreas Krause and
                  Marco Cuturi",2021,,https://arxiv.org/abs/2106.06345,,CoRR
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,tong2024otcfm,\cite{tong2024otcfm},Improving and generalizing flow-based generative models with minibatch optimal transport,,,True,False,Alexander Tong and Kilian FATRAS and Nikolay Malkin and Guillaume Huguet and Yanlei Zhang and Jarrid Rector-Brooks and Guy Wolf and Yoshua Bengio,2024,,https://openreview.net/forum?id=CD9Snc73AW,,Transactions on Machine Learning Research
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,kapusniak2024metric,\cite{kapusniak2024metric},Metric Flow Matching for Smooth Interpolations on the Data Manifold,,,True,False,Kacper Kapusniak and Peter Potaptchik and Teodora Reu and Leo Zhang and Alexander Tong and Michael M. Bronstein and Joey Bose and Francesco Di Giovanni,2024,,https://openreview.net/forum?id=fE3RqiF4Nx,,
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,zhang2025deepruot,\cite{zhang2025deepruot},Learning stochastic dynamics from snapshots through regularized unbalanced optimal transport,https://arxiv.org/abs/2410.00844v5,"Reconstructing dynamics using samples from sparsely time-resolved snapshots is an important problem in both natural sciences and machine learning. Here, we introduce a new deep learning approach for solving regularized unbalanced optimal transport (RUOT) and inferring continuous unbalanced stochastic dynamics from observed snapshots. Based on the RUOT form, our method models these dynamics without requiring prior knowledge of growth and death processes or additional information, allowing them to be learned directly from data. Theoretically, we explore the connections between the RUOT and Schrödinger bridge problem and discuss the key challenges and potential solutions. The effectiveness of our method is demonstrated with a synthetic gene regulatory network, high-dimensional Gaussian Mixture Model, and single-cell RNA-seq data from blood development. Compared with other methods, our approach accurately identifies growth and transition patterns, eliminates false transitions, and constructs the Waddington developmental landscape. Our code is available at: https://github.com/zhenyiizhang/DeepRUOT.",True,True,Zhenyi Zhang and Tiejun Li and Peijie Zhou,2025,,https://openreview.net/forum?id=gQlxd3Mtru,,
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,wang2025jointvelocitygrowthflowmatching,\cite{wang2025jointvelocitygrowthflowmatching},Joint Velocity-Growth Flow Matching for Single-Cell Dynamics Modeling,https://arxiv.org/abs/2505.13413v2,"Learning the underlying dynamics of single cells from snapshot data has gained increasing attention in scientific and machine learning research. The destructive measurement technique and cell proliferation/death result in unpaired and unbalanced data between snapshots, making the learning of the underlying dynamics challenging. In this paper, we propose joint Velocity-Growth Flow Matching (VGFM), a novel paradigm that jointly learns state transition and mass growth of single-cell populations via flow matching. VGFM builds an ideal single-cell dynamics containing velocity of state and growth of mass, driven by a presented two-period dynamic understanding of the static semi-relaxed optimal transport, a mathematical tool that seeks the coupling between unpaired and unbalanced data. To enable practical usage, we approximate the ideal dynamics using neural networks, forming our joint velocity and growth matching framework. A distribution fitting loss is also employed in VGFM to further improve the fitting performance for snapshot data. Extensive experimental results on both synthetic and real datasets demonstrate that VGFM can capture the underlying biological dynamics accounting for mass and state variations over time, outperforming existing approaches for single-cell dynamics modeling.",True,True,Dongyi Wang and Yuanwei Jiang and Zhenyi Zhang and Xiang Gu and Peijie Zhou and Jian Sun,2025,,https://arxiv.org/abs/2505.13413,,
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,terpin2024lightspeed,\cite{terpin2024lightspeed},Learning diffusion at lightspeed,https://arxiv.org/abs/2406.12616v2,"Diffusion regulates numerous natural processes and the dynamics of many successful generative models. Existing models to learn the diffusion terms from observational data rely on complex bilevel optimization problems and model only the drift of the system. We propose a new simple model, JKOnet*, which bypasses the complexity of existing architectures while presenting significantly enhanced representational capabilities: JKOnet* recovers the potential, interaction, and internal energy components of the underlying diffusion process. JKOnet* minimizes a simple quadratic loss and outperforms other baselines in terms of sample efficiency, computational complexity, and accuracy. Additionally, JKOnet* provides a closed-form optimal solution for linearly parametrized functionals, and, when applied to predict the evolution of cellular processes from real-world data, it achieves state-of-the-art accuracy at a fraction of the computational cost of all existing methods. Our methodology is based on the interpretation of diffusion processes as energy-minimizing trajectories in the probability space via the so-called JKO scheme, which we study via its first-order optimality conditions.",True,True,Antonio Terpin and Nicolas Lanzetti and Mart{\'\i}n Gadea and Florian Dorfler,2024,,https://openreview.net/forum?id=y10avdRFNK,,
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,neklyudov2023actionmatching,\cite{neklyudov2023actionmatching},Action Matching: Learning Stochastic Dynamics from Samples,https://arxiv.org/abs/2210.06662v3,"Learning the continuous dynamics of a system from snapshots of its temporal marginals is a problem which appears throughout natural sciences and machine learning, including in quantum systems, single-cell biological data, and generative modeling. In these settings, we assume access to cross-sectional samples that are uncorrelated over time, rather than full trajectories of samples. In order to better understand the systems under observation, we would like to learn a model of the underlying process that allows us to propagate samples in time and thereby simulate entire individual trajectories. In this work, we propose Action Matching, a method for learning a rich family of dynamics using only independent samples from its time evolution. We derive a tractable training objective, which does not rely on explicit assumptions about the underlying dynamics and does not require back-propagation through differential equations or optimal transport solvers. Inspired by connections with optimal transport, we derive extensions of Action Matching to learn stochastic differential equations and dynamics involving creation and destruction of probability mass. Finally, we showcase applications of Action Matching by achieving competitive performance in a diverse set of experiments from biology, physics, and generative modeling.",True,True,"Neklyudov, Kirill and Brekelmans, Rob and Severo, Daniel and Makhzani, Alireza",2023,23--29 Jul,https://proceedings.mlr.press/v202/neklyudov23a.html,,
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,Davidson2002GRN,\cite{Davidson2002GRN},A genomic regulatory network for development,,,True,False,"Davidson, Eric H. and Rast, Jonathan P. and Oliveri, Paola and Ransick, Andrew and Calestani, Cristina and Yuh, Chiou-Hwa and Minokawa, Takuya and Amore, Gabriele and Hinman, Veronica and Arenas-Mena, Cesar and Otim, Ochan and Brown, C. Titus and Livi, Carolina B. and Lee, Pei Yun and Revilla, Roger and Rust, Alistair G. and Pan, Zheng Jun and Schilstra, Maria J. and Clarke, Peter J. C. and Arnone, Maria I. and Rowen, Lee and Cameron, R. Andrew and McClay, David R. and Hood, Leroy and Bolouri, Hamid",2002,,,10.1126/science.1069883,Science
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,tong2024sf2m,\cite{tong2024sf2m},Simulation-free Schrödinger bridges via score and flow matching,https://arxiv.org/abs/2307.03672v3,"We present simulation-free score and flow matching ([SF]$^2$M), a simulation-free objective for inferring stochastic dynamics given unpaired samples drawn from arbitrary source and target distributions. Our method generalizes both the score-matching loss used in the training of diffusion models and the recently proposed flow matching loss used in the training of continuous normalizing flows. [SF]$^2$M interprets continuous-time stochastic generative modeling as a Schrödinger bridge problem. It relies on static entropy-regularized optimal transport, or a minibatch approximation, to efficiently learn the SB without simulating the learned stochastic process. We find that [SF]$^2$M is more efficient and gives more accurate solutions to the SB problem than simulation-based methods from prior work. Finally, we apply [SF]$^2$M to the problem of learning cell dynamics from snapshot data. Notably, [SF]$^2$M is the first method to accurately model cell dynamics in high dimensions and can recover known gene regulatory networks from simulated data. Our code is available in the TorchCFM package at https://github.com/atong01/conditional-flow-matching.",True,True,Alexander Tong and Nikolay Malkin and Kilian Fatras and Lazar Atanackovic and Yanlei Zhang and Guillaume Huguet and Guy Wolf and Yoshua Bengio,2024,,https://proceedings.mlr.press/v238/y-tong24a.html,,
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,Pratapa2020beeline,\cite{Pratapa2020beeline},Benchmarking algorithms for gene regulatory network inference from single-cell transcriptomic data,,,True,False,"Pratapa, Aditya and Jalihal, Amogh P. and Law, Jeffrey N. and Bharadwaj, Aditya and Murali, T. M.",2020,,https://doi.org/10.1038/s41592-019-0690-6,10.1038/s41592-019-0690-6,Nature Methods
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,thu2010genie3,\cite{thu2010genie3},Inferring Regulatory Networks from Expression Data Using Tree-Based Methods,,,True,False,"Huynh-Thu, Vân Anh AND Irrthum, Alexandre AND Wehenkel, Louis AND Geurts, Pierre",2010,09,https://doi.org/10.1371/journal.pone.0012776,10.1371/journal.pone.0012776,PLOS ONE
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,moerman2018grnboost2,\cite{moerman2018grnboost2},GRNBoost2 and Arboreto: efficient and scalable inference of gene regulatory networks,,,True,False,"Moerman, Thomas and Aibar Santos, Sara and Bravo González-Blas, Carmen and Simm, Jaak and Moreau, Yves and Aerts, Jan and Aerts, Stein",2018,11,https://doi.org/10.1093/bioinformatics/bty916,10.1093/bioinformatics/bty916,Bioinformatics
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,chan2017pidc,\cite{chan2017pidc},Gene Regulatory Network Inference from Single-Cell Data Using Multivariate Information Measures,,,True,False,Thalia E. Chan and Michael P.H. Stumpf and Ann C. Babtie,2017,,https://www.sciencedirect.com/science/article/pii/S2405471217303861,https://doi.org/10.1016/j.cels.2017.08.014,Cell Systems
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,Lu2021causal,\cite{Lu2021causal},Causal network inference from gene transcriptional time-series response to glucocorticoids,,,True,False,"Lu, Junjie and Dumitrascu, Bianca and McDowell, Ian C. and Jo, Brian and Barrera, Alejandro and Hong, Lee K. and Leichter, Samuel M. and Reddy, Timothy E. and Engelhardt, Barbara E.",2021,,,10.1371/journal.pcbi.1008223,PLoS Computational Biology
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,aijoe2009grn_GP,\cite{aijoe2009grn_GP},Learning gene regulatory networks from gene expression measurements using non-parametric molecular kinetics,,,True,False,"Äijö, Tarmo and Lähdesmäki, Harri",2009,08,https://doi.org/10.1093/bioinformatics/btp511,10.1093/bioinformatics/btp511,Bioinformatics
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,lin2025interpretableneuralodesgene,\cite{lin2025interpretableneuralodesgene},Interpretable Neural ODEs for Gene Regulatory Network Discovery under Perturbations,,,True,False,Zaikang Lin and Sei Chang and Aaron Zweig and Minseo Kang and Elham Azizi and David A. Knowles,2025,,https://arxiv.org/abs/2501.02409,,
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,brunton2016sindy,\cite{brunton2016sindy},Discovering governing equations from data: Sparse identification of nonlinear dynamical systems,https://arxiv.org/abs/1509.03580v1,"The ability to discover physical laws and governing equations from data is one of humankind's greatest intellectual achievements. A quantitative understanding of dynamic constraints and balances in nature has facilitated rapid development of knowledge and enabled advanced technological achievements, including aircraft, combustion engines, satellites, and electrical power. In this work, we combine sparsity-promoting techniques and machine learning with nonlinear dynamical systems to discover governing physical equations from measurement data. The only assumption about the structure of the model is that there are only a few important terms that govern the dynamics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems. In particular, we use sparse regression to determine the fewest terms in the dynamic governing equations required to accurately represent the data. The resulting models are parsimonious, balancing model complexity with descriptive ability while avoiding overfitting. We demonstrate the algorithm on a wide range of problems, from simple canonical systems, including linear and nonlinear oscillators and the chaotic Lorenz system, to the fluid vortex shedding behind an obstacle. The fluid example illustrates the ability of this method to discover the underlying dynamics of a system that took experts in the community nearly 30 years to resolve. We also show that this method generalizes to parameterized, time-varying, or externally forced systems.",True,True,Steven L. Brunton  and Joshua L. Proctor  and J. Nathan Kutz,2016,,https://www.pnas.org/doi/abs/10.1073/pnas.1517384113,10.1073/pnas.1517384113,Proceedings of the National Academy of Sciences
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,pervez2024mechanistic,\cite{pervez2024mechanistic},Mechanistic Neural Networks for Scientific Machine Learning,,,True,False,Adeel Pervez and Francesco Locatello and Stratis Gavves,2024,,https://openreview.net/forum?id=pLtuwhoQh7,,
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,chen2025scalablemnn,\cite{chen2025scalablemnn},Scalable Mechanistic Neural Networks,,,True,False,Jiale Chen and Dingling Yao and Adeel Pervez and Dan Alistarh and Francesco Locatello,2025,,https://openreview.net/forum?id=Oazgf8A24z,,
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,yao2024marrying_causal_w_dynamics,\cite{yao2024marrying_causal_w_dynamics},Marrying Causal Representation Learning with Dynamical Systems for Science,,,True,False,Dingling Yao and Caroline Muller and Francesco Locatello,2024,,https://openreview.net/forum?id=MWHRxKz4mq,,
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,ascoli2024odeformer,\cite{ascoli2024odeformer},{ODEF}ormer: Symbolic Regression of Dynamical Systems with Transformers,,,True,False,"St{\'e}phane d'Ascoli and S{\""o}ren Becker and Philippe Schwaller and Alexander Mathis and Niki Kilbertus",2024,,https://openreview.net/forum?id=TzoHLiGVMo,,
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,schmidt1966statespace4navigation,\cite{schmidt1966statespace4navigation},Application of State-Space Methods to Navigation Problems,,,True,False,STANLEY F. Schmidt,1966,,https://www.sciencedirect.com/science/article/pii/B9781483167169500114,https://doi.org/10.1016/B978-1-4831-6716-9.50011-4,
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,li2004ilqr,\cite{li2004ilqr},Iterative Linear Quadratic Regulator Design for Nonlinear Biological Movement Systems.,,,True,False,"Li, Weiwei and Todorov, Emanuel",,,http://dblp.uni-trier.de/db/conf/icinco/icinco2004.html#LiT04,,
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,todorov2005ilqg,\cite{todorov2005ilqg},A generalized iterative LQG method for locally-optimal feedback control of constrained nonlinear stochastic systems,,,True,False,"Todorov, E. and Weiwei Li",2005,,,10.1109/ACC.2005.1469949,
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,szmuk2020rocketlanding,\cite{szmuk2020rocketlanding},Successive Convexification for Real-Time Six-Degree-of-Freedom Powered Descent Guidance with State-Triggered Constraints,,,True,False,"Szmuk, Michael and Reynolds, Taylor P. and A\c{c}\i{}kme\c{s}e, Beh\c{c}et",2020,,https://doi.org/10.2514/1.G004549,10.2514/1.G004549,"Journal of Guidance, Control, and Dynamics"
Learning Explicit Single-Cell Dynamics Using ODE Representations,2510.02903v1,spencer2023control_oriented_structure,\cite{spencer2023control_oriented_structure},Learning Control-Oriented Dynamical Structure from Data,https://arxiv.org/abs/2302.02529v2,"Even for known nonlinear dynamical systems, feedback controller synthesis is a difficult problem that often requires leveraging the particular structure of the dynamics to induce a stable closed-loop system. For general nonlinear models, including those fit to data, there may not be enough known structure to reliably synthesize a stabilizing feedback controller. In this paper, we discuss a state-dependent nonlinear tracking controller formulation based on a state-dependent Riccati equation for general nonlinear control-affine systems. This formulation depends on a nonlinear factorization of the system of vector fields defining the control-affine dynamics, which always exists under mild smoothness assumptions. We propose a method for learning this factorization from a finite set of data. On a variety of simulated nonlinear dynamical systems, we empirically demonstrate the efficacy of learned versions of this controller in stable trajectory tracking. Alongside our learning method, we evaluate recent ideas in jointly learning a controller and stabilizability certificate for known dynamical systems; we show experimentally that such methods can be frail in comparison.",True,True,"Richards, Spencer M. and Slotine, Jean-Jacques and Azizan, Navid and Pavone, Marco",2023,23--29 Jul,https://proceedings.mlr.press/v202/richards23a.html,,
Physicochemically Informed Dual-Conditioned Generative Model of T-Cell Receptor Variable Regions for Cellular Therapy,2510.05747v1,montemurro2021nettcr,\cite{montemurro2021nettcr},NetTCR-2.0 enables accurate prediction of TCR-peptide binding by using paired TCR$\alpha$ and $\beta$ sequence data,,,True,False,"Montemurro, Alessandro and Schuster, Viktoria and Povlsen, Helle Rus and Bentzen, Amalie Kai and Jurtz, Vanessa and Chronister, William D and Crinklaw, Austin and Hadrup, Sine R and Winther, Ole and Peters, Bjoern and others",2021,,,,{Communications Biology}
Physicochemically Informed Dual-Conditioned Generative Model of T-Cell Receptor Variable Regions for Cellular Therapy,2510.05747v1,sidhom2021deeptcr,\cite{sidhom2021deeptcr},DeepTCR is a deep learning framework for revealing sequence concepts within T-cell repertoires,,,True,False,"Sidhom, John-William and Larman, H Benjamin and Pardoll, Drew M and Baras, Alexander S",2021,,,,{Nature Communications}
Physicochemically Informed Dual-Conditioned Generative Model of T-Cell Receptor Variable Regions for Cellular Therapy,2510.05747v1,weber2021titan,\cite{weber2021titan},TITAN: T Cell Receptor Specificity Prediction with Bimodal Attention Networks,https://arxiv.org/abs/2105.03323v1,"Motivation: The activity of the adaptive immune system is governed by T-cells and their specific T-cell receptors (TCR), which selectively recognize foreign antigens. Recent advances in experimental techniques have enabled sequencing of TCRs and their antigenic targets (epitopes), allowing to research the missing link between TCR sequence and epitope binding specificity. Scarcity of data and a large sequence space make this task challenging, and to date only models limited to a small set of epitopes have achieved good performance. Here, we establish a k-nearest-neighbor (K-NN) classifier as a strong baseline and then propose TITAN (Tcr epITope bimodal Attention Networks), a bimodal neural network that explicitly encodes both TCR sequences and epitopes to enable the independent study of generalization capabilities to unseen TCRs and/or epitopes. Results: By encoding epitopes at the atomic level with SMILES sequences, we leverage transfer learning and data augmentation to enrich the input data space and boost performance. TITAN achieves high performance in the prediction of specificity of unseen TCRs (ROC-AUC 0.87 in 10-fold CV) and surpasses the results of the current state-of-the-art (ImRex) by a large margin. Notably, our Levenshtein-distance-based K-NN classifier also exhibits competitive performance on unseen TCRs. While the generalization to unseen epitopes remains challenging, we report two major breakthroughs. First, by dissecting the attention heatmaps, we demonstrate that the sparsity of available epitope data favors an implicit treatment of epitopes as classes. This may be a general problem that limits unseen epitope performance for sufficiently complex models. Second, we show that TITAN nevertheless exhibits significantly improved performance on unseen epitopes and is capable of focusing attention on chemically meaningful molecular structures.",True,True,"Weber, Anna and Born, Jannis and Rodriguez Mart{\'\i}nez, Mar{\'\i}a",2021,,,,Bioinformatics
Physicochemically Informed Dual-Conditioned Generative Model of T-Cell Receptor Variable Regions for Cellular Therapy,2510.05747v1,zhang2021mapping,\cite{zhang2021mapping},Mapping the functional landscape of T cell receptor repertoires by single-T cell transcriptomics,,,True,False,"Zhang, Ze and Xiong, Danyi and Wang, Xinlei and Liu, Hongyu and Wang, Tao",2021,,,,{Nature Methods}
Physicochemically Informed Dual-Conditioned Generative Model of T-Cell Receptor Variable Regions for Cellular Therapy,2510.05747v1,lin2024tcr,\cite{lin2024tcr},TCR-GPT: Integrating Autoregressive Model and Reinforcement Learning for T-Cell Receptor Repertoires Generation,https://arxiv.org/abs/2408.01156v1,"T-cell receptors (TCRs) play a crucial role in the immune system by recognizing and binding to specific antigens presented by infected or cancerous cells. Understanding the sequence patterns of TCRs is essential for developing targeted immune therapies and designing effective vaccines. Language models, such as auto-regressive transformers, offer a powerful solution to this problem by learning the probability distributions of TCR repertoires, enabling the generation of new TCR sequences that inherit the underlying patterns of the repertoire. We introduce TCR-GPT, a probabilistic model built on a decoder-only transformer architecture, designed to uncover and replicate sequence patterns in TCR repertoires. TCR-GPT demonstrates an accuracy of 0.953 in inferring sequence probability distributions measured by Pearson correlation coefficient. Furthermore, by leveraging Reinforcement Learning(RL), we adapted the distribution of TCR sequences to generate TCRs capable of recognizing specific peptides, offering significant potential for advancing targeted immune therapies and vaccine development. With the efficacy of RL, fine-tuned pretrained TCR-GPT models demonstrated the ability to produce TCR repertoires likely to bind specific peptides, illustrating RL's efficiency in enhancing the model's adaptability to the probability distributions of biologically relevant TCR sequences.",True,True,"Lin, Yicheng and Zhang, Dandan and Liu, Yun",2024,,,,arXiv preprint arXiv:2408.01156
Physicochemically Informed Dual-Conditioned Generative Model of T-Cell Receptor Variable Regions for Cellular Therapy,2510.05747v1,madani2020progen,\cite{madani2020progen},Progen: Language modeling for protein generation,,,True,False,"Madani, Ali and McCann, Bryan and Naik, Nikhil and Keskar, Nitish Shirish and Anand, Namrata and Eguchi, Raphael R and Huang, Po-Ssu and Socher, Richard",2020,,,,arXiv preprint arXiv:2004.03497
Physicochemically Informed Dual-Conditioned Generative Model of T-Cell Receptor Variable Regions for Cellular Therapy,2510.05747v1,ferruz2022protgpt2,\cite{ferruz2022protgpt2},ProtGPT2 is a deep unsupervised language model for protein design,,,True,False,"Ferruz, Noelia and Schmidt, Steffen and H{\""o}cker, Birte",2022,,,,{Nature Communications}
Physicochemically Informed Dual-Conditioned Generative Model of T-Cell Receptor Variable Regions for Cellular Therapy,2510.05747v1,meier2021language,\cite{meier2021language},Language models enable zero-shot prediction of the effects of mutations on protein function,,,True,False,"Meier, Joshua and Rao, Roshan and Verkuil, Robert and Liu, Jason and Sercu, Tom and Rives, Alex",2021,,,,{Advances in Neural Information Processing Systems}
Physicochemically Informed Dual-Conditioned Generative Model of T-Cell Receptor Variable Regions for Cellular Therapy,2510.05747v1,watson2023novo,\cite{watson2023novo},De novo design of protein structure and function with RFdiffusion,,,True,False,"Watson, Joseph L and Juergens, David and Bennett, Nathaniel R and Trippe, Brian L and Yim, Jason and Eisenach, Helen E and Ahern, Woody and Borst, Andrew J and Ragotte, Robert J and Milles, Lukas F and others",2023,,,,Nature
Physicochemically Informed Dual-Conditioned Generative Model of T-Cell Receptor Variable Regions for Cellular Therapy,2510.05747v1,singh2024chroma,\cite{singh2024chroma},Chroma is a generative model for protein design,,,True,False,"Singh, Arunima",2024,,,,{Nature Methods}
Physicochemically Informed Dual-Conditioned Generative Model of T-Cell Receptor Variable Regions for Cellular Therapy,2510.05747v1,dauparas2022robust,\cite{dauparas2022robust},Robust deep learning--based protein sequence design using ProteinMPNN,,,True,False,"Dauparas, Justas and Anishchenko, Ivan and Bennett, Nathaniel and Bai, Hua and Ragotte, Robert J and Milles, Lukas F and Wicky, Basile IM and Courbet, Alexis and de Haas, Rob J and Bethel, Neville and others",2022,,,,Science
Physicochemically Informed Dual-Conditioned Generative Model of T-Cell Receptor Variable Regions for Cellular Therapy,2510.05747v1,hayat2015all,\cite{hayat2015all},All-atom 3D structure prediction of transmembrane $\beta$-barrel proteins from sequences,,,True,False,"Hayat, Sikander and Sander, Chris and Marks, Debora S and Elofsson, Arne",2015,,,,{Proceedings of the National Academy of Sciences}
Physicochemically Informed Dual-Conditioned Generative Model of T-Cell Receptor Variable Regions for Cellular Therapy,2510.05747v1,lin2023evolutionary,\cite{lin2023evolutionary},Evolutionary-scale prediction of atomic-level protein structure with a language model,,,True,False,"Lin, Zeming and Akin, Halil and Rao, Roshan and Hie, Brian and Zhu, Zhongkai and Lu, Wenting and Smetanin, Nikita and Verkuil, Robert and Kabeli, Ori and Shmueli, Yaniv and others",2023,,,,Science
Physicochemically Informed Dual-Conditioned Generative Model of T-Cell Receptor Variable Regions for Cellular Therapy,2510.05747v1,liu2006rosettadesign,\cite{liu2006rosettadesign},RosettaDesign server for protein design,,,True,False,"Liu, Yi and Kuhlman, Brian",2006,,,,{Nucleic Acids Research}
Physicochemically Informed Dual-Conditioned Generative Model of T-Cell Receptor Variable Regions for Cellular Therapy,2510.05747v1,lu2021deep,\cite{lu2021deep},Deep learning-based prediction of the T cell receptor--antigen binding specificity,,,True,False,"Lu, Tianshi and Zhang, Ze and Zhu, James and Wang, Yunguan and Jiang, Peixin and Xiao, Xue and Bernatchez, Chantale and Heymach, John V and Gibbons, Don L and Wang, Jun and others",2021,,,,{Nature Machine Intelligence}
Physicochemically Informed Dual-Conditioned Generative Model of T-Cell Receptor Variable Regions for Cellular Therapy,2510.05747v1,feng2024sliding,\cite{feng2024sliding},Sliding-attention transformer neural architecture for predicting T cell receptor--antigen--human leucocyte antigen binding,,,True,False,"Feng, Ziyan and Chen, Jingyang and Hai, Youlong and Pang, Xuelian and Zheng, Kun and Xie, Chenglong and Zhang, Xiujuan and Li, Shengqing and Zhang, Chengjuan and Liu, Kangdong and others",2024,,,,{Nature Machine Intelligence}
A Novel Framework for Multi-Modal Protein Representation Learning,2510.23273v1,CNN_Shanehsazzadeh,\cite{CNN_Shanehsazzadeh},Is Transfer Learning Necessary for Protein Landscape Prediction?,,,True,False,Amir Shanehsazzadeh and David Belanger and David Dohan,2020,,https://arxiv.org/abs/2011.03443,,
A Novel Framework for Multi-Modal Protein Representation Learning,2510.23273v1,LSTM_wang2016protein,\cite{LSTM_wang2016protein},Protein secondary structure prediction based on integration of {CNN} and {LSTM} model,,,True,False,"Cheng, Jinyong and Liu, Yihui and Ma, Yuming",2020,,,10.1016/j.jvcir.2020.102844,J. Visual Commun. Image Represent.
A Novel Framework for Multi-Modal Protein Representation Learning,2510.23273v1,ESM_Rives2021,\cite{ESM_Rives2021},Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences,,,True,False,Alexander Rives  and Joshua Meier  and Tom Sercu  and Siddharth Goyal  and Zeming Lin  and Jason Liu  and Demi Guo  and Myle Ott  and C. Lawrence Zitnick  and Jerry Ma  and Rob Fergus,2021,,,10.1073/pnas.2016239118,Proc. Natl. Acad. Sci. U.S.A.
A Novel Framework for Multi-Modal Protein Representation Learning,2510.23273v1,elnaggar2021prottrans,\cite{elnaggar2021prottrans},ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning,,,True,False,"Elnaggar, Ahmed and Heinzinger, Michael and Dallago, Christian and Rehawi, Ghalia and Wang, Yu and Jones, Llion and Gibbs, Tom and Feher, Tamas and Angerer, Christoph and Steinegger, Martin and Bhowmik, Debsindhu and Rost, Burkhard",2022,,,10.1109/TPAMI.2021.3095381,IEEE Trans. Pattern Anal. Mach. Intell.
A Novel Framework for Multi-Modal Protein Representation Learning,2510.23273v1,zhang2022gearnet,\cite{zhang2022gearnet},Protein Representation Learning by Geometric Structure Pretraining,https://arxiv.org/abs/2203.06125v5,"Learning effective protein representations is critical in a variety of tasks in biology such as predicting protein function or structure. Existing approaches usually pretrain protein language models on a large number of unlabeled amino acid sequences and then finetune the models with some labeled data in downstream tasks. Despite the effectiveness of sequence-based approaches, the power of pretraining on known protein structures, which are available in smaller numbers only, has not been explored for protein property prediction, though protein structures are known to be determinants of protein function. In this paper, we propose to pretrain protein representations according to their 3D structures. We first present a simple yet effective encoder to learn the geometric features of a protein. We pretrain the protein graph encoder by leveraging multiview contrastive learning and different self-prediction tasks. Experimental results on both function prediction and fold classification tasks show that our proposed pretraining methods outperform or are on par with the state-of-the-art sequence-based methods, while using much less pretraining data. Our implementation is available at https://github.com/DeepGraphLearning/GearNet.",True,True,Zuobai Zhang and Minghao Xu and Arian Rokkum Jamasb and Vijil Chenthamarakshan and Aurelie Lozano and Payel Das and Jian Tang,2023,,https://openreview.net/forum?id=to3qCB3tOh9,,
A Novel Framework for Multi-Modal Protein Representation Learning,2510.23273v1,deepfri_gligorijevic2021structure,\cite{deepfri_gligorijevic2021structure},Structure-based protein function prediction using graph convolutional networks,,,True,False,"Gligorijević, Vladimir and Renfrew, P. Douglas and Kosciolek, Tomasz and Leman, Julia Koehler and Berenberg, Daniel and Vatanen, Tommi and Chandler, Chris and Taylor, Bryn C. and Fisk, Ian M. and Vlamakis, Hera and Xavier, Ramnik J. and Knight, Rob and Cho, Kyunghyun and Bonneau, Richard",2021,,,10.1038/s41467-021-23303-9,Nat. Commun.
A Novel Framework for Multi-Modal Protein Representation Learning,2510.23273v1,hermosilla2021ieconv,\cite{hermosilla2021ieconv},Intrinsic-Extrinsic Convolution and Pooling for Learning on 3D Protein Structures,,,True,False,"Pedro Hermosilla and Marco Sch{\""a}fer and Matej Lang and Gloria Fackelmann and Pere-Pau V{\'a}zquez and Barbora Kozlikova and Michael Krone and Tobias Ritschel and Timo Ropinski",2021,,https://openreview.net/forum?id=l0mSUROpwY,,
A Novel Framework for Multi-Modal Protein Representation Learning,2510.23273v1,dmasif_sverrisson2021fast,\cite{dmasif_sverrisson2021fast},Fast end-to-end learning on protein surfaces,,,True,False,"Sverrisson, Freyr and Feydy, Jean and Correia, Bruno and others",2021,,,,
A Novel Framework for Multi-Modal Protein Representation Learning,2510.23273v1,hermosilla2022contrastive,\cite{hermosilla2022contrastive},Contrastive Representation Learning for 3D Protein Structures,,,True,False,Pedro Hermosilla and Timo Ropinski,2022,,https://arxiv.org/abs/2205.15675,,
A Novel Framework for Multi-Modal Protein Representation Learning,2510.23273v1,xu2023pretraining,\cite{xu2023pretraining},Pre-Training Protein Encoder via Siamese Sequence-Structure Diffusion Trajectory Prediction,https://arxiv.org/abs/2301.12068v2,"Self-supervised pre-training methods on proteins have recently gained attention, with most approaches focusing on either protein sequences or structures, neglecting the exploration of their joint distribution, which is crucial for a comprehensive understanding of protein functions by integrating co-evolutionary information and structural characteristics. In this work, inspired by the success of denoising diffusion models in generative tasks, we propose the DiffPreT approach to pre-train a protein encoder by sequence-structure joint diffusion modeling. DiffPreT guides the encoder to recover the native protein sequences and structures from the perturbed ones along the joint diffusion trajectory, which acquires the joint distribution of sequences and structures. Considering the essential protein conformational variations, we enhance DiffPreT by a method called Siamese Diffusion Trajectory Prediction (SiamDiff) to capture the correlation between different conformers of a protein. SiamDiff attains this goal by maximizing the mutual information between representations of diffusion trajectories of structurally-correlated conformers. We study the effectiveness of DiffPreT and SiamDiff on both atom- and residue-level structure-based protein understanding tasks. Experimental results show that the performance of DiffPreT is consistently competitive on all tasks, and SiamDiff achieves new state-of-the-art performance, considering the mean ranks on all tasks. Our implementation is available at https://github.com/DeepGraphLearning/SiamDiff.",True,True,Zuobai Zhang and Minghao Xu and Aurelie Lozano and Vijil Chenthamarakshan and Payel Das and Jian Tang,2023,,https://openreview.net/forum?id=tIzbNQko3c,,
A Novel Framework for Multi-Modal Protein Representation Learning,2510.23273v1,Wang2022LMGVP,\cite{Wang2022LMGVP},{LM}-{GVP}: {An} extensible sequence and structure informed deep learning framework for protein property prediction,,,True,False,"Wang, Zichen and Combs, Steven A. and Brand, Ryan and Calvo, Miguel Romero and Xu, Panpan and Price, George and Golovach, Nataliya and Salawu, Emmanuel O. and Wise, Colby J. and Ponnapalli, Sri Priya and Clark, Peter M.",2022,,,10.1038/s41598-022-10775-y,Sci. Rep.
A Novel Framework for Multi-Modal Protein Representation Learning,2510.23273v1,zhang2023systematic,\cite{zhang2023systematic},A Systematic Study of Joint Representation Learning on Protein Sequences and Structures,https://arxiv.org/abs/2303.06275v2,"Learning effective protein representations is critical in a variety of tasks in biology such as predicting protein functions. Recent sequence representation learning methods based on Protein Language Models (PLMs) excel in sequence-based tasks, but their direct adaptation to tasks involving protein structures remains a challenge. In contrast, structure-based methods leverage 3D structural information with graph neural networks and geometric pre-training methods show potential in function prediction tasks, but still suffers from the limited number of available structures. To bridge this gap, our study undertakes a comprehensive exploration of joint protein representation learning by integrating a state-of-the-art PLM (ESM-2) with distinct structure encoders (GVP, GearNet, CDConv). We introduce three representation fusion strategies and explore different pre-training techniques. Our method achieves significant improvements over existing sequence- and structure-based methods, setting new state-of-the-art for function annotation. This study underscores several important design choices for fusing protein sequence and structure information. Our implementation is available at https://github.com/DeepGraphLearning/ESM-GearNet.",True,True,Zuobai Zhang and Chuanrui Wang and Minghao Xu and Vijil Chenthamarakshan and Aurélie Lozano and Payel Das and Jian Tang,2023,,https://arxiv.org/abs/2303.06275,,
A Novel Framework for Multi-Modal Protein Representation Learning,2510.23273v1,gvp_Jing2021,\cite{gvp_Jing2021},Learning from Protein Structure with Geometric Vector Perceptrons,,,True,False,Bowen Jing and Stephan Eismann and Patricia Suriana and Raphael John Lamarre Townshend and Ron Dror,2021,,https://openreview.net/forum?id=1YLJDvSx6J4,,
A Novel Framework for Multi-Modal Protein Representation Learning,2510.23273v1,cdconv_Fan2023,\cite{cdconv_Fan2023},Continuous-Discrete Convolution for Geometry-Sequence Modeling in Proteins,,,True,False,Hehe Fan and Zhangyang Wang and Yi Yang and Mohan Kankanhalli,2023,,https://openreview.net/forum?id=P5Z-Zl9XJ7,,
A Novel Framework for Multi-Modal Protein Representation Learning,2510.23273v1,zhang2023sst,\cite{zhang2023sst},{SST}-{ResNet}: {A} sequence and structure information integration model for protein property prediction,,,True,False,"Zhou, Guowei and Zhao, Yanpeng and He, Song and Bo, Xiaochen",2025,,,10.3390/ijms26062783,Int. J. Mol. Sci.
A Novel Framework for Multi-Modal Protein Representation Learning,2510.23273v1,hou2021tale,\cite{hou2021tale},TALE: Transformer-based protein function Annotation through joint sequence–Label Embedding,,,True,False,"Hou, Jie and Wu, Tao and Cao, Rui and Cheng, Jianlin",2021,,,10.1093/bioinformatics/btab198,Bioinformatics
A Novel Framework for Multi-Modal Protein Representation Learning,2510.23273v1,zhang2022atgo,\cite{zhang2022atgo},Integrating unsupervised language model with triplet neural networks for protein gene ontology prediction,,,True,False,"Zhu, Yi-Heng and Zhang, Chengxin and Yu, Dong-Jun and Zhang, Yang",2022,,,10.1371/journal.pcbi.1010793,PLoS Comput Biol
A Novel Framework for Multi-Modal Protein Representation Learning,2510.23273v1,DPFunc_NatCommun2024,\cite{DPFunc_NatCommun2024},{DPFunc}: accurately predicting protein function via deep learning with domain-guided structure information,,,True,False,"Wang, Wenkang and Shuai, Yunyan and Zeng, Min and Fan, Wei and Li, Min",2024,,,10.1038/s41467-024-54816-8,Nat. Commun.
TEMPO: Temporal Multi-scale Autoregressive Generation of Protein Conformational Ensembles,2511.05510v1,potapenko2021highly,\cite{potapenko2021highly},Highly accurate protein structure prediction with AlphaFold,,,True,False,"Potapenko, Alex Bridgland and Meyer, Clemens and Kohl, Simon AA and Ballard, Andrew J and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Hassabis, Demis",2021,,,,Nature
TEMPO: Temporal Multi-scale Autoregressive Generation of Protein Conformational Ensembles,2511.05510v1,jing2024alphafold,\cite{jing2024alphafold},AlphaFold meets flow matching for generating protein ensembles,,,True,False,"Jing, Bowen and Berger, Bonnie and Jaakkola, Tommi",2024,,,,
TEMPO: Temporal Multi-scale Autoregressive Generation of Protein Conformational Ensembles,2511.05510v1,lewis2024scalable,\cite{lewis2024scalable},Scalable emulation of protein equilibrium ensembles with generative deep learning,,,True,False,"Lewis, Sarah and Hempel, Tim and Jim{\'e}nez-Luna, Jos{\'e} and Gastegger, Michael and Xie, Yu and Foong, Andrew YK and Satorras, Victor Garc{\'\i}a and Abdin, Osama and Veeling, Bastiaan S and Zaporozhets, Iryna and others",2024,,,,bioRxiv
TEMPO: Temporal Multi-scale Autoregressive Generation of Protein Conformational Ensembles,2511.05510v1,wang2024protein,\cite{wang2024protein},Protein Conformation Generation via Force-Guided SE(3) Diffusion Models,https://arxiv.org/abs/2403.14088v2,"The conformational landscape of proteins is crucial to understanding their functionality in complex biological processes. Traditional physics-based computational methods, such as molecular dynamics (MD) simulations, suffer from rare event sampling and long equilibration time problems, hindering their applications in general protein systems. Recently, deep generative modeling techniques, especially diffusion models, have been employed to generate novel protein conformations. However, existing score-based diffusion methods cannot properly incorporate important physical prior knowledge to guide the generation process, causing large deviations in the sampled protein conformations from the equilibrium distribution. In this paper, to overcome these limitations, we propose a force-guided SE(3) diffusion model, ConfDiff, for protein conformation generation. By incorporating a force-guided network with a mixture of data-based score models, ConfDiff can generate protein conformations with rich diversity while preserving high fidelity. Experiments on a variety of protein conformation prediction tasks, including 12 fast-folding proteins and the Bovine Pancreatic Trypsin Inhibitor (BPTI), demonstrate that our method surpasses the state-of-the-art method.",True,True,"Wang, Yan and Wang, Lihao and Shen, Yuning and Wang, Yiqun and Yuan, Huizhuo and Wu, Yue and Gu, Quanquan",2024,,,,
TEMPO: Temporal Multi-scale Autoregressive Generation of Protein Conformational Ensembles,2511.05510v1,lustr2str,\cite{lustr2str},Str2Str: A Score-based Framework for Zero-shot Protein Conformation Sampling,,,True,False,"Lu, Jiarui and Zhong, Bozitao and Zhang, Zuobai and Tang, Jian",,,,,
TEMPO: Temporal Multi-scale Autoregressive Generation of Protein Conformational Ensembles,2511.05510v1,mardt2022deep,\cite{mardt2022deep},Deep learning to decompose macromolecules into independent Markovian domains,,,True,False,"Mardt, Andreas and Hempel, Tim and Clementi, Cecilia and No{\'e}, Frank",2022,,,,Nature Communications
TEMPO: Temporal Multi-scale Autoregressive Generation of Protein Conformational Ensembles,2511.05510v1,wu2023diffmd,\cite{wu2023diffmd},DIFFMD: a geometric diffusion model for molecular dynamics simulations,,,True,False,"Wu, Fang and Li, Stan Z",2023,,,,
TEMPO: Temporal Multi-scale Autoregressive Generation of Protein Conformational Ensembles,2511.05510v1,arts2023two,\cite{arts2023two},Two for one: Diffusion models and force fields for coarse-grained molecular dynamics,,,True,False,"Arts, Marloes and Garcia Satorras, Victor and Huang, Chin-Wei and Zugner, Daniel and Federici, Marco and Clementi, Cecilia and No{\'e}, Frank and Pinsler, Robert and van den Berg, Rianne",2023,,,,Journal of Chemical Theory and Computation
TEMPO: Temporal Multi-scale Autoregressive Generation of Protein Conformational Ensembles,2511.05510v1,zheng2024predicting,\cite{zheng2024predicting},Predicting equilibrium distributions for molecular systems with deep learning,,,True,False,"Zheng, Shuxin and He, Jiyan and Liu, Chang and Shi, Yu and Lu, Ziheng and Feng, Weitao and Ju, Fusong and Wang, Jiaxi and Zhu, Jianwei and Min, Yaosen and others",2024,,,,Nature Machine Intelligence
TEMPO: Temporal Multi-scale Autoregressive Generation of Protein Conformational Ensembles,2511.05510v1,jing2023eigenfold,\cite{jing2023eigenfold},EigenFold: Generative Protein Structure Prediction with Diffusion Models,https://arxiv.org/abs/2304.02198v1,"Protein structure prediction has reached revolutionary levels of accuracy on single structures, yet distributional modeling paradigms are needed to capture the conformational ensembles and flexibility that underlie biological function. Towards this goal, we develop EigenFold, a diffusion generative modeling framework for sampling a distribution of structures from a given protein sequence. We define a diffusion process that models the structure as a system of harmonic oscillators and which naturally induces a cascading-resolution generative process along the eigenmodes of the system. On recent CAMEO targets, EigenFold achieves a median TMScore of 0.84, while providing a more comprehensive picture of model uncertainty via the ensemble of sampled structures relative to existing methods. We then assess EigenFold's ability to model and predict conformational heterogeneity for fold-switching proteins and ligand-induced conformational change. Code is available at https://github.com/bjing2016/EigenFold.",True,True,"Jing, Bowen and Erives, Ezra and Pao-Huang, Peter and Corso, Gabriele and Berger, Bonnie and Jaakkola, Tommi S",,,,,
TEMPO: Temporal Multi-scale Autoregressive Generation of Protein Conformational Ensembles,2511.05510v1,bosese,\cite{bosese},SE (3)-Stochastic Flow Matching for Protein Backbone Generation,,,True,False,"Bose, Joey and Akhound-Sadegh, Tara and Huguet, Guillaume and FATRAS, Kilian and Rector-Brooks, Jarrid and Liu, Cheng-Hao and Nica, Andrei Cristian and Korablyov, Maksym and Bronstein, Michael M and Tong, Alexander",,,,,
TEMPO: Temporal Multi-scale Autoregressive Generation of Protein Conformational Ensembles,2511.05510v1,schreiner2023implicit,\cite{schreiner2023implicit},Implicit Transfer Operator Learning: Multiple Time-Resolution Surrogates for Molecular Dynamics,https://arxiv.org/abs/2305.18046v2,"Computing properties of molecular systems rely on estimating expectations of the (unnormalized) Boltzmann distribution. Molecular dynamics (MD) is a broadly adopted technique to approximate such quantities. However, stable simulations rely on very small integration time-steps ($10^{-15}\,\mathrm{s}$), whereas convergence of some moments, e.g. binding free energy or rates, might rely on sampling processes on time-scales as long as $10^{-1}\, \mathrm{s}$, and these simulations must be repeated for every molecular system independently. Here, we present Implict Transfer Operator (ITO) Learning, a framework to learn surrogates of the simulation process with multiple time-resolutions. We implement ITO with denoising diffusion probabilistic models with a new SE(3) equivariant architecture and show the resulting models can generate self-consistent stochastic dynamics across multiple time-scales, even when the system is only partially observed. Finally, we present a coarse-grained CG-SE3-ITO model which can quantitatively model all-atom molecular dynamics using only coarse molecular representations. As such, ITO provides an important step towards multiple time- and space-resolution acceleration of MD. Code is available at \href{https://github.com/olsson-group/ito}{https://github.com/olsson-group/ito}.",True,True,"Schreiner, Mathias and Winther, Ole and Olsson, Simon",2023,,,,Advances in Neural Information Processing Systems
TEMPO: Temporal Multi-scale Autoregressive Generation of Protein Conformational Ensembles,2511.05510v1,trinquier2021efficient,\cite{trinquier2021efficient},Efficient generative modeling of protein sequences using simple autoregressive models,https://arxiv.org/abs/2103.03292v3,"Generative models emerge as promising candidates for novel sequence-data driven approaches to protein design, and for the extraction of structural and functional information about proteins deeply hidden in rapidly growing sequence databases. Here we propose simple autoregressive models as highly accurate but computationally efficient generative sequence models. We show that they perform similarly to existing approaches based on Boltzmann machines or deep generative models, but at a substantially lower computational cost (by a factor between $10^2$ and $10^3$). Furthermore, the simple structure of our models has distinctive mathematical advantages, which translate into an improved applicability in sequence generation and evaluation. Within these models, we can easily estimate both the probability of a given sequence, and, using the model's entropy, the size of the functional sequence space related to a specific protein family. In the example of response regulators, we find a huge number of ca. $10^{68}$ possible sequences, which nevertheless constitute only the astronomically small fraction $10^{-80}$ of all amino-acid sequences of the same length. These findings illustrate the potential and the difficulty in exploring sequence space via generative sequence models.",True,True,"Trinquier, Jeanne and Uguzzoni, Guido and Pagnani, Andrea and Zamponi, Francesco and Weigt, Martin",2021,,,,Nature communications
TEMPO: Temporal Multi-scale Autoregressive Generation of Protein Conformational Ensembles,2511.05510v1,lu2024structure,\cite{lu2024structure},Structure Language Models for Protein Conformation Generation,,,True,False,"Lu, Jiarui and Chen, Xiaoyin and Lu, Stephen Zhewen and Shi, Chence and Guo, Hongyu and Bengio, Yoshua and Tang, Jian",,,,,
TEMPO: Temporal Multi-scale Autoregressive Generation of Protein Conformational Ensembles,2511.05510v1,costa2024equijump,\cite{costa2024equijump},EquiJump: Protein Dynamics Simulation via SO(3)-Equivariant Stochastic Interpolants,https://arxiv.org/abs/2410.09667v2,"Mapping the conformational dynamics of proteins is crucial for elucidating their functional mechanisms. While Molecular Dynamics (MD) simulation enables detailed time evolution of protein motion, its computational toll hinders its use in practice. To address this challenge, multiple deep learning models for reproducing and accelerating MD have been proposed drawing on transport-based generative methods. However, existing work focuses on generation through transport of samples from prior distributions, that can often be distant from the data manifold. The recently proposed framework of stochastic interpolants, instead, enables transport between arbitrary distribution endpoints. Building upon this work, we introduce EquiJump, a transferable SO(3)-equivariant model that bridges all-atom protein dynamics simulation time steps directly. Our approach unifies diverse sampling methods and is benchmarked against existing models on trajectory data of fast folding proteins. EquiJump achieves state-of-the-art results on dynamics simulation with a transferable model on all of the fast folding proteins.",True,True,"Costa, Allan dos Santos and Mitnikov, Ilan and Pellegrini, Franco and Daigavane, Ameya and Geiger, Mario and Cao, Zhonglin and Kreis, Karsten and Smidt, Tess and Kucukbenli, Emine and Jacobson, Joseph",2024,,,,arXiv preprint arXiv:2410.09667
Training Language Models to Explain Their Own Computations,2511.08579v1,korbak2025chainthoughtmonitorabilitynew,\cite{korbak2025chainthoughtmonitorabilitynew},Chain of Thought Monitorability: A New and Fragile Opportunity for {AI} Safety,,,True,False,Tomek Korbak and Mikita Balesni and Elizabeth Barnes and Yoshua Bengio and Joe Benton and Joseph Bloom and Mark Chen and Alan Cooney and Allan Dafoe and Anca Dragan and Scott Emmons and Owain Evans and David Farhi and Ryan Greenblatt and Dan Hendrycks and Marius Hobbhahn and Evan Hubinger and Geoffrey Irving and Erik Jenner and Daniel Kokotajlo and Victoria Krakovna and Shane Legg and David Lindner and David Luan and Aleksander Mądry and Julian Michael and Neel Nanda and Dave Orr and Jakub Pachocki and Ethan Perez and Mary Phuong and Fabien Roger and Joshua Saxe and Buck Shlegeris and Martín Soto and Eric Steinberger and Jasmine Wang and Wojciech Zaremba and Bowen Baker and Rohin Shah and Vlad Mikulik,2025,,https://arxiv.org/abs/2507.11473,,
Training Language Models to Explain Their Own Computations,2511.08579v1,baker2025monitoringreasoningmodelsmisbehavior,\cite{baker2025monitoringreasoningmodelsmisbehavior},Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation,,,True,False,Bowen Baker and Joost Huizinga and Leo Gao and Zehao Dou and Melody Y. Guan and Aleksander Madry and Wojciech Zaremba and Jakub Pachocki and David Farhi,2025,,https://arxiv.org/abs/2503.11926,,
Training Language Models to Explain Their Own Computations,2511.08579v1,turpin2023language,\cite{turpin2023language},Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting,,,True,False,Miles Turpin and Julian Michael and Ethan Perez and Samuel R. Bowman,2023,,https://openreview.net/forum?id=bzs4uPLXvi,,
Training Language Models to Explain Their Own Computations,2511.08579v1,lanham2023measuringfaithfulnesschainofthoughtreasoning,\cite{lanham2023measuringfaithfulnesschainofthoughtreasoning},Measuring Faithfulness in Chain-of-Thought Reasoning,https://arxiv.org/abs/2307.13702v1,"Large language models (LLMs) perform better when they produce step-by-step, ""Chain-of-Thought"" (CoT) reasoning before answering a question, but it is unclear if the stated reasoning is a faithful explanation of the model's actual reasoning (i.e., its process for answering the question). We investigate hypotheses for how CoT reasoning may be unfaithful, by examining how the model predictions change when we intervene on the CoT (e.g., by adding mistakes or paraphrasing it). Models show large variation across tasks in how strongly they condition on the CoT when predicting their answer, sometimes relying heavily on the CoT and other times primarily ignoring it. CoT's performance boost does not seem to come from CoT's added test-time compute alone or from information encoded via the particular phrasing of the CoT. As models become larger and more capable, they produce less faithful reasoning on most tasks we study. Overall, our results suggest that CoT can be faithful if the circumstances such as the model size and task are carefully chosen.",True,True,Tamera Lanham and Anna Chen and Ansh Radhakrishnan and Benoit Steiner and Carson Denison and Danny Hernandez and Dustin Li and Esin Durmus and Evan Hubinger and Jackson Kernion and Kamilė Lukošiūtė and Karina Nguyen and Newton Cheng and Nicholas Joseph and Nicholas Schiefer and Oliver Rausch and Robin Larson and Sam McCandlish and Sandipan Kundu and Saurav Kadavath and Shannon Yang and Thomas Henighan and Timothy Maxwell and Timothy Telleen-Lawton and Tristan Hume and Zac Hatfield-Dodds and Jared Kaplan and Jan Brauner and Samuel R. Bowman and Ethan Perez,2023,,https://arxiv.org/abs/2307.13702,,
Training Language Models to Explain Their Own Computations,2511.08579v1,barez2025cot,\cite{barez2025cot},Chain-of-Thought Is Not Explainability,,,True,False,"Barez, Fazl and Wu, Tung-Yu and Arcuschin, Iv{\'a}n and Lan, Michael and Wang, Vincent and Siegel, Noah and Collignon, Nicolas and Neo, Clement and Lee, Isabelle and Paren, Alasdair and Bibi, Adel and Trager, Robert and Fornasiere, Damiano and Yan, John and Elazar, Yanai and Bengio, Yoshua",2025,,https://aigi.ox.ac.uk/wp-content/uploads/2025/07/Cot_Is_Not_Explainability.pdf,,
Training Language Models to Explain Their Own Computations,2511.08579v1,chen2025reasoningmodelsdontsay,\cite{chen2025reasoningmodelsdontsay},Reasoning Models Don't Always Say What They Think,,,True,False,Yanda Chen and Joe Benton and Ansh Radhakrishnan and Jonathan Uesato and Carson Denison and John Schulman and Arushi Somani and Peter Hase and Misha Wagner and Fabien Roger and Vlad Mikulik and Samuel R. Bowman and Jan Leike and Jared Kaplan and Ethan Perez,2025,,https://arxiv.org/abs/2505.05410,,
Training Language Models to Explain Their Own Computations,2511.08579v1,gurnee2024universal,\cite{gurnee2024universal},Universal Neurons in {GPT}2 Language Models,,,True,False,Wes Gurnee and Theo Horsley and Zifan Carl Guo and Tara Rezaei Kheirkhah and Qinyi Sun and Will Hathaway and Neel Nanda and Dimitris Bertsimas,2024,,https://openreview.net/forum?id=ZeI104QZ8I,,Transactions on Machine Learning Research
Training Language Models to Explain Their Own Computations,2511.08579v1,wang2023interpretability,\cite{wang2023interpretability},Interpretability in the Wild: a Circuit for Indirect Object Identification in {GPT}-2 Small,,,True,False,Kevin Ro Wang and Alexandre Variengien and Arthur Conmy and Buck Shlegeris and Jacob Steinhardt,2023,,https://openreview.net/forum?id=NpsVSN6o4ul,,
Training Language Models to Explain Their Own Computations,2511.08579v1,nanda2023progressmeasuresgrokkingmechanistic,\cite{nanda2023progressmeasuresgrokkingmechanistic},Progress measures for grokking via mechanistic interpretability,,,True,False,Neel Nanda and Lawrence Chan and Tom Lieberum and Jess Smith and Jacob Steinhardt,2023,,https://arxiv.org/abs/2301.05217,,
Training Language Models to Explain Their Own Computations,2511.08579v1,sharkey2025open,\cite{sharkey2025open},Open Problems in Mechanistic Interpretability,https://arxiv.org/abs/2501.16496v1,"Mechanistic interpretability aims to understand the computational mechanisms underlying neural networks' capabilities in order to accomplish concrete scientific and engineering goals. Progress in this field thus promises to provide greater assurance over AI system behavior and shed light on exciting scientific questions about the nature of intelligence. Despite recent progress toward these goals, there are many open problems in the field that require solutions before many scientific and practical benefits can be realized: Our methods require both conceptual and practical improvements to reveal deeper insights; we must figure out how best to apply our methods in pursuit of specific goals; and the field must grapple with socio-technical challenges that influence and are influenced by our work. This forward-facing review discusses the current frontier of mechanistic interpretability and the open problems that the field may benefit from prioritizing.",True,True,Lee Sharkey and Bilal Chughtai and Joshua Batson and Jack Lindsey and Jeffrey Wu and Lucius Bushnaq and Nicholas Goldowsky-Dill and Stefan Heimersheim and Alejandro Ortega and Joseph Isaac Bloom and Stella Biderman and Adri{\`a} Garriga-Alonso and Arthur Conmy and Neel Nanda and Jessica Mary Rumbelow and Martin Wattenberg and Nandi Schoots and Joseph Miller and William Saunders and Eric J Michaud and Stephen Casper and Max Tegmark and David Bau and Eric Todd and Atticus Geiger and Mor Geva and Jesse Hoogland and Daniel Murfet and Thomas McGrath,2025,,https://openreview.net/forum?id=91H76m9Z94,,Transactions on Machine Learning Research
Training Language Models to Explain Their Own Computations,2511.08579v1,hernandez2022natural,\cite{hernandez2022natural},Natural Language Descriptions of Deep Visual Features,https://arxiv.org/abs/2201.11114v2,"Some neurons in deep networks specialize in recognizing highly specific perceptual, structural, or semantic features of inputs. In computer vision, techniques exist for identifying neurons that respond to individual concept categories like colors, textures, and object classes. But these techniques are limited in scope, labeling only a small subset of neurons and behaviors in any network. Is a richer characterization of neuron-level computation possible? We introduce a procedure (called MILAN, for mutual-information-guided linguistic annotation of neurons) that automatically labels neurons with open-ended, compositional, natural language descriptions. Given a neuron, MILAN generates a description by searching for a natural language string that maximizes pointwise mutual information with the image regions in which the neuron is active. MILAN produces fine-grained descriptions that capture categorical, relational, and logical structure in learned features. These descriptions obtain high agreement with human-generated feature descriptions across a diverse set of model architectures and tasks, and can aid in understanding and controlling learned models. We highlight three applications of natural language neuron descriptions. First, we use MILAN for analysis, characterizing the distribution and importance of neurons selective for attribute, category, and relational information in vision models. Second, we use MILAN for auditing, surfacing neurons sensitive to human faces in datasets designed to obscure them. Finally, we use MILAN for editing, improving robustness in an image classifier by deleting neurons sensitive to text features spuriously correlated with class labels.",True,True,"Hernandez, Evan and Schwettmann, Sarah and Bau, David and Bagashvili, Teona and Torralba, Antonio and Andreas, Jacob",2022,,https://arxiv.org/abs/2201.11114,,
Training Language Models to Explain Their Own Computations,2511.08579v1,bills2023language,\cite{bills2023language},Language models can explain neurons in language models,,,True,False,"Bills, Steven and Cammarata, Nick and Mossing, Dan and Tillman, Henk and Gao, Leo and Goh, Gabriel and Sutskever, Ilya and Leike, Jan and Wu, Jeff and Saunders, William",2023,,,,
Training Language Models to Explain Their Own Computations,2511.08579v1,choi2024automatic,\cite{choi2024automatic},Scaling Automatic Neuron Description,,,True,False,"Choi, Dami and Huang, Vincent and Meng, Kevin and Johnson, Daniel D and Steinhardt, Jacob and Schwettmann, Sarah",2024,October,,,
Training Language Models to Explain Their Own Computations,2511.08579v1,paulo2025automatically,\cite{paulo2025automatically},Automatically Interpreting Millions of Features in Large Language Models,https://arxiv.org/abs/2410.13928v3,"While the activations of neurons in deep neural networks usually do not have a simple human-understandable interpretation, sparse autoencoders (SAEs) can be used to transform these activations into a higher-dimensional latent space which may be more easily interpretable. However, these SAEs can have millions of distinct latent features, making it infeasible for humans to manually interpret each one. In this work, we build an open-source automated pipeline to generate and evaluate natural language explanations for SAE features using LLMs. We test our framework on SAEs of varying sizes, activation functions, and losses, trained on two different open-weight LLMs. We introduce five new techniques to score the quality of explanations that are cheaper to run than the previous state of the art. One of these techniques, intervention scoring, evaluates the interpretability of the effects of intervening on a feature, which we find explains features that are not recalled by existing methods. We propose guidelines for generating better explanations that remain valid for a broader set of activating contexts, and discuss pitfalls with existing scoring techniques. We use our explanations to measure the semantic similarity of independently trained SAEs, and find that SAEs trained on nearby layers of the residual stream are highly similar. Our large-scale analysis confirms that SAE latents are indeed much more interpretable than neurons, even when neurons are sparsified using top-$k$ postprocessing. Our code is available at https://github.com/EleutherAI/sae-auto-interp, and our explanations are available at https://huggingface.co/datasets/EleutherAI/auto_interp_explanations.",True,True,Gon{\c{c}}alo Santos Paulo and Alex Troy Mallen and Caden Juang and Nora Belrose,2025,,https://openreview.net/forum?id=5lIXRf8Lnw,,
Training Language Models to Explain Their Own Computations,2511.08579v1,conmy2023automated,\cite{conmy2023automated},Towards Automated Circuit Discovery for Mechanistic Interpretability,,,True,False,Arthur Conmy and Augustine N. Mavor-Parker and Aengus Lynch and Stefan Heimersheim and Adri{\`a} Garriga-Alonso,2023,,,,
Training Language Models to Explain Their Own Computations,2511.08579v1,syed-etal-2024-attribution,\cite{syed-etal-2024-attribution},Attribution Patching Outperforms Automated Circuit Discovery,https://arxiv.org/abs/2310.10348v2,"Automated interpretability research has recently attracted attention as a potential research direction that could scale explanations of neural network behavior to large models. Existing automated circuit discovery work applies activation patching to identify subnetworks responsible for solving specific tasks (circuits). In this work, we show that a simple method based on attribution patching outperforms all existing methods while requiring just two forward passes and a backward pass. We apply a linear approximation to activation patching to estimate the importance of each edge in the computational subgraph. Using this approximation, we prune the least important edges of the network. We survey the performance and limitations of this method, finding that averaged over all tasks our method has greater AUC from circuit recovery than other methods.",True,True,"Syed, Aaquib  and
      Rager, Can  and
      Conmy, Arthur",2024,,https://aclanthology.org/2024.blackboxnlp-1.25/,10.18653/v1/2024.blackboxnlp-1.25,
Training Language Models to Explain Their Own Computations,2511.08579v1,hanna2024have,\cite{hanna2024have},Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms,https://arxiv.org/abs/2403.17806v2,"Many recent language model (LM) interpretability studies have adopted the circuits framework, which aims to find the minimal computational subgraph, or circuit, that explains LM behavior on a given task. Most studies determine which edges belong in a LM's circuit by performing causal interventions on each edge independently, but this scales poorly with model size. Edge attribution patching (EAP), gradient-based approximation to interventions, has emerged as a scalable but imperfect solution to this problem. In this paper, we introduce a new method - EAP with integrated gradients (EAP-IG) - that aims to better maintain a core property of circuits: faithfulness. A circuit is faithful if all model edges outside the circuit can be ablated without changing the model's performance on the task; faithfulness is what justifies studying circuits, rather than the full model. Our experiments demonstrate that circuits found using EAP are less faithful than those found using EAP-IG, even though both have high node overlap with circuits found previously using causal interventions. We conclude more generally that when using circuits to compare the mechanisms models use to solve tasks, faithfulness, not overlap, is what should be measured.",True,True,Michael Hanna and Sandro Pezzelle and Yonatan Belinkov,2024,,https://openreview.net/forum?id=grXgesr5dT,,
Training Language Models to Explain Their Own Computations,2511.08579v1,hsu2025efficient,\cite{hsu2025efficient},Efficient Automated Circuit Discovery in Transformers using Contextual Decomposition,https://arxiv.org/abs/2407.00886v3,"Automated mechanistic interpretation research has attracted great interest due to its potential to scale explanations of neural network internals to large models. Existing automated circuit discovery work relies on activation patching or its approximations to identify subgraphs in models for specific tasks (circuits). They often suffer from slow runtime, approximation errors, and specific requirements of metrics, such as non-zero gradients. In this work, we introduce contextual decomposition for transformers (CD-T) to build interpretable circuits in large language models. CD-T can produce circuits of arbitrary level of abstraction, and is the first able to produce circuits as fine-grained as attention heads at specific sequence positions efficiently. CD-T consists of a set of mathematical equations to isolate contribution of model features. Through recursively computing contribution of all nodes in a computational graph of a model using CD-T followed by pruning, we are able to reduce circuit discovery runtime from hours to seconds compared to state-of-the-art baselines. On three standard circuit evaluation datasets (indirect object identification, greater-than comparisons, and docstring completion), we demonstrate that CD-T outperforms ACDC and EAP by better recovering the manual circuits with an average of 97% ROC AUC under low runtimes. In addition, we provide evidence that faithfulness of CD-T circuits is not due to random chance by showing our circuits are 80% more faithful than random circuits of up to 60% of the original model size. Finally, we show CD-T circuits are able to perfectly replicate original models' behavior (faithfulness $ = 1$) using fewer nodes than the baselines for all tasks. Our results underscore the great promise of CD-T for efficient automated mechanistic interpretability, paving the way for new insights into the workings of large language models.",True,True,Aliyah R. Hsu and Georgia Zhou and Yeshwanth Cherapanamjeri and Yaxuan Huang and Anobel Odisho and Peter R. Carroll and Bin Yu,2025,,https://openreview.net/forum?id=41HlN8XYM5,,
Training Language Models to Explain Their Own Computations,2511.08579v1,nanda2023attribution,\cite{nanda2023attribution},Attribution Patching: Activation Patching At Industrial Scale,,,True,False,"Nanda, Neel",2023,,,,
Training Language Models to Explain Their Own Computations,2511.08579v1,nostalgebraist2020interpreting,\cite{nostalgebraist2020interpreting},Interpreting {GPT}: the logit lens,,,True,False,nostalgebraist,2020,,https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens,,
Training Language Models to Explain Their Own Computations,2511.08579v1,ghandeharioun2024patchscopes,\cite{ghandeharioun2024patchscopes},Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models,,,True,False,Asma Ghandeharioun and Avi Caciularu and Adam Pearce and Lucas Dixon and Mor Geva,2024,,https://openreview.net/forum?id=5uwBzcn885,,
Training Language Models to Explain Their Own Computations,2511.08579v1,chen2024selfie,\cite{chen2024selfie},SelfIE: Self-Interpretation of Large Language Model Embeddings,https://arxiv.org/abs/2403.10949v2,"How do large language models (LLMs) obtain their answers? The ability to explain and control an LLM's reasoning process is key for reliability, transparency, and future model developments. We propose SelfIE (Self-Interpretation of Embeddings), a framework that enables LLMs to interpret their own embeddings in natural language by leveraging their ability to respond to inquiries about a given passage. Capable of interpreting open-world concepts in the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such as making ethical decisions, internalizing prompt injection, and recalling harmful knowledge. SelfIE's text descriptions on hidden embeddings also open up new avenues to control LLM reasoning. We propose Supervised Control, which allows editing open-ended concepts while only requiring gradient computation of individual layer. We extend RLHF to hidden embeddings and propose Reinforcement Control that erases harmful knowledge in LLM without supervision targets.",True,True,Haozhe Chen and Carl Vondrick and Chengzhi Mao,2024,,https://openreview.net/forum?id=gjgRKbdYR7,,
Training Language Models to Explain Their Own Computations,2511.08579v1,kharlapenko2024self,\cite{kharlapenko2024self},Self-explaining {SAE} features,,,True,False,Dmitrii Kharlapenko and Stepan Shabalin and Neel Nanda and Arthur Conmy,2024,,https://www.lesswrong.com/posts/,,
Training Language Models to Explain Their Own Computations,2511.08579v1,pan2024latentqa,\cite{pan2024latentqa},LatentQA: Teaching LLMs to Decode Activations Into Natural Language,https://arxiv.org/abs/2412.08686v1,"Interpretability methods seek to understand language model representations, yet the outputs of most such methods -- circuits, vectors, scalars -- are not immediately human-interpretable. In response, we introduce LatentQA, the task of answering open-ended questions about model activations in natural language. Towards solving LatentQA, we propose Latent Interpretation Tuning (LIT), which finetunes a decoder LLM on a dataset of activations and associated question-answer pairs, similar to how visual instruction tuning trains on question-answer pairs associated with images. We use the decoder for diverse reading applications, such as extracting relational knowledge from representations or uncovering system prompts governing model behavior. Our decoder also specifies a differentiable loss that we use to control models, such as debiasing models on stereotyped sentences and controlling the sentiment of generations. Finally, we extend LatentQA to reveal harmful model capabilities, such as generating recipes for bioweapons and code for hacking.",True,True,"Pan, Alexander and Chen, Lijie and Steinhardt, Jacob",2024,,,,arXiv
Training Language Models to Explain Their Own Computations,2511.08579v1,viégas2023modelusermodelexploring,\cite{viégas2023modelusermodelexploring},The System Model and the User Model: Exploring {AI} Dashboard Design,,,True,False,Fernanda Viégas and Martin Wattenberg,2023,,https://arxiv.org/abs/2305.02469,,
Training Language Models to Explain Their Own Computations,2511.08579v1,chen2024designingdashboardtransparencycontrol,\cite{chen2024designingdashboardtransparencycontrol},Designing a Dashboard for Transparency and Control of Conversational {AI},,,True,False,Yida Chen and Aoyu Wu and Trevor DePodesta and Catherine Yeh and Kenneth Li and Nicholas Castillo Marin and Oam Patel and Jan Riecke and Shivam Raval and Olivia Seow and Martin Wattenberg and Fernanda Viégas,2024,,https://arxiv.org/abs/2406.07882,,
Training Language Models to Explain Their Own Computations,2511.08579v1,binder2025looking,\cite{binder2025looking},Looking Inward: Language Models Can Learn About Themselves by Introspection,https://arxiv.org/abs/2410.13787v1,"Humans acquire knowledge by observing the external world, but also by introspection. Introspection gives a person privileged access to their current state of mind (e.g., thoughts and feelings) that is not accessible to external observers. Can LLMs introspect? We define introspection as acquiring knowledge that is not contained in or derived from training data but instead originates from internal states. Such a capability could enhance model interpretability. Instead of painstakingly analyzing a model's internal workings, we could simply ask the model about its beliefs, world models, and goals. More speculatively, an introspective model might self-report on whether it possesses certain internal states such as subjective feelings or desires and this could inform us about the moral status of these states. Such self-reports would not be entirely dictated by the model's training data.
  We study introspection by finetuning LLMs to predict properties of their own behavior in hypothetical scenarios. For example, ""Given the input P, would your output favor the short- or long-term option?"" If a model M1 can introspect, it should outperform a different model M2 in predicting M1's behavior even if M2 is trained on M1's ground-truth behavior. The idea is that M1 has privileged access to its own behavioral tendencies, and this enables it to predict itself better than M2 (even if M2 is generally stronger).
  In experiments with GPT-4, GPT-4o, and Llama-3 models (each finetuned to predict itself), we find that the model M1 outperforms M2 in predicting itself, providing evidence for introspection. Notably, M1 continues to predict its behavior accurately even after we intentionally modify its ground-truth behavior. However, while we successfully elicit introspection on simple tasks, we are unsuccessful on more complex tasks or those requiring out-of-distribution generalization.",True,True,Felix Jedidja Binder and James Chua and Tomek Korbak and Henry Sleight and John Hughes and Robert Long and Ethan Perez and Miles Turpin and Owain Evans,2025,,https://openreview.net/forum?id=eb5pkwIB5i,,
Training Language Models to Explain Their Own Computations,2511.08579v1,treutlein2024connecting,\cite{treutlein2024connecting},Connecting the Dots: {LLM}s can Infer and Verbalize Latent Structure from Disparate Training Data,,,True,False,Johannes Treutlein and Dami Choi and Jan Betley and Samuel Marks and Cem Anil and Roger Baker Grosse and Owain Evans,2024,,https://openreview.net/forum?id=7FokMz6U8n,,
Training Language Models to Explain Their Own Computations,2511.08579v1,laine2024memyselfaisituational,\cite{laine2024memyselfaisituational},"Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs",,,True,False,Rudolf Laine and Bilal Chughtai and Jan Betley and Kaivalya Hariharan and Jeremy Scheurer and Mikita Balesni and Marius Hobbhahn and Alexander Meinke and Owain Evans,2024,,https://arxiv.org/abs/2407.04694,,
Training Language Models to Explain Their Own Computations,2511.08579v1,comsa2025doesmakesensespeak,\cite{comsa2025doesmakesensespeak},Does It Make Sense to Speak of Introspection in Large Language Models?,https://arxiv.org/abs/2506.05068v2,"Large language models (LLMs) exhibit compelling linguistic behaviour, and sometimes offer self-reports, that is to say statements about their own nature, inner workings, or behaviour. In humans, such reports are often attributed to a faculty of introspection and are typically linked to consciousness. This raises the question of how to interpret self-reports produced by LLMs, given their increasing linguistic fluency and cognitive capabilities. To what extent (if any) can the concept of introspection be meaningfully applied to LLMs? Here, we present and critique two examples of apparent introspective self-report from LLMs. In the first example, an LLM attempts to describe the process behind its own ""creative"" writing, and we argue this is not a valid example of introspection. In the second example, an LLM correctly infers the value of its own temperature parameter, and we argue that this can be legitimately considered a minimal example of introspection, albeit one that is (presumably) not accompanied by conscious experience.",True,True,Iulia M. Comsa and Murray Shanahan,2025,,https://arxiv.org/abs/2506.05068,,
Training Language Models to Explain Their Own Computations,2511.08579v1,song2025language,\cite{song2025language},Language Models Fail to Introspect About Their Knowledge of Language,https://arxiv.org/abs/2503.07513v3,"There has been recent interest in whether large language models (LLMs) can introspect about their own internal states. Such abilities would make LLMs more interpretable, and also validate the use of standard introspective methods in linguistics to evaluate grammatical knowledge in models (e.g., asking ""Is this sentence grammatical?""). We systematically investigate emergent introspection across 21 open-source LLMs, in two domains where introspection is of theoretical interest: grammatical knowledge and word prediction. Crucially, in both domains, a model's internal linguistic knowledge can be theoretically grounded in direct measurements of string probability. We then evaluate whether models' responses to metalinguistic prompts faithfully reflect their internal knowledge. We propose a new measure of introspection: the degree to which a model's prompted responses predict its own string probabilities, beyond what would be predicted by another model with nearly identical internal knowledge. While both metalinguistic prompting and probability comparisons lead to high task accuracy, we do not find evidence that LLMs have privileged ""self-access"". By using general tasks, controlling for model similarity, and evaluating a wide range of open-source models, we show that LLMs cannot introspect, and add new evidence to the argument that prompted responses should not be conflated with models' linguistic generalizations.",True,True,Siyuan Song and Jennifer Hu and Kyle Mahowald,2025,,https://openreview.net/forum?id=AivRDOFi5H,,
Training Language Models to Explain Their Own Computations,2511.08579v1,song2025privilegedselfaccessmattersintrospection,\cite{song2025privilegedselfaccessmattersintrospection},Privileged Self-Access Matters for Introspection in {AI},,,True,False,Siyuan Song and Harvey Lederman and Jennifer Hu and Kyle Mahowald,2025,,https://arxiv.org/abs/2508.14802,,
Training Language Models to Explain Their Own Computations,2511.08579v1,li2025do,\cite{li2025do},Do Natural Language Descriptions of Model Activations Convey Privileged Information?,https://arxiv.org/abs/2509.13316v2,"Recent interpretability methods have proposed to translate LLM internal representations into natural language descriptions using a second verbalizer LLM. This is intended to illuminate how the target model represents and operates on inputs. But do such activation verbalization approaches actually provide privileged knowledge about the internal workings of the target model, or do they merely convey information about its inputs? We critically evaluate popular verbalization methods across datasets used in prior work and find that they can succeed at benchmarks without any access to target model internals, suggesting that these datasets may not be ideal for evaluating verbalization methods. We then run controlled experiments which reveal that verbalizations often reflect the parametric knowledge of the verbalizer LLM which generated them, rather than the knowledge of the target LLM whose activations are decoded. Taken together, our results indicate a need for targeted benchmarks and experimental controls to rigorously assess whether verbalization methods provide meaningful insights into the operations of LLMs.",True,True,Millicent Li and Alberto Mario Ceballos Arroyo and Giordano Rogers and Naomi Saphra and Byron C Wallace,2025,,https://openreview.net/forum?id=zyhibAkzSA,,
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,robertson_invariant_1940,\cite{robertson_invariant_1940},The invariant theory of isotropic turbulence,,,True,False,"Robertson, H. P.",1940,,https://www.cambridge.org/core/journals/mathematical-proceedings-of-the-cambridge-philosophical-society/article/invariant-theory-of-isotropic-turbulence/20D2B827EAE1BA8EFDBD0AEB7069A059,10.1017/S0305004100017199,Mathematical Proceedings of the Cambridge Philosophical Society
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,lumley_computational_1979,\cite{lumley_computational_1979},Computational {Modeling} of {Turbulent} {Flows}*,,,True,False,"Lumley, John L.",1979,,https://www.sciencedirect.com/science/article/pii/S0065215608702667,10.1016/S0065-2156(08)70266-7,
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,sarkar_simple_1989,\cite{sarkar_simple_1989},A simple nonlinear model for the return to isotropy in turbulence,,,True,False,"Sarkar, Sutanu and Speziale, Charles G.",1989,,https://ntrs.nasa.gov/citations/19890011041,,
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,chung_nonlinear_1995,\cite{chung_nonlinear_1995},A nonlinear return‐to‐isotropy model with {Reynolds} number and anisotropy dependency,,,True,False,"Chung, Myung Kyoon and Kim, Soong Kee",1995,,https://doi.org/10.1063/1.868760,10.1063/1.868760,Physics of Fluids
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,ling_reynolds_2016,\cite{ling_reynolds_2016},Reynolds averaged turbulence modelling using deep neural networks with embedded invariance,,,True,False,"Ling, Julia and Kurzawski, Andrew and Templeton, Jeremy",2016,,https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/reynolds-averaged-turbulence-modelling-using-deep-neural-networks-with-embedded-invariance/0B280EEE89C74A7BF651C422F8FBD1EB,10.1017/jfm.2016.615,Journal of Fluid Mechanics
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,kaandorp_data-driven_2020,\cite{kaandorp_data-driven_2020},Data-driven modelling of the {Reynolds} stress tensor using random forests with invariance,,,True,False,"Kaandorp, Mikael L. A. and Dwight, Richard P.",2020,,https://www.sciencedirect.com/science/article/pii/S0045793020300700,10.1016/j.compfluid.2020.104497,Computers \& Fluids
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,cai_revisiting_2024,\cite{cai_revisiting_2024},Revisiting Tensor Basis Neural Networks for Reynolds stress modeling: application to plane channel and square duct flows,https://arxiv.org/abs/2403.11746v1,"Several Tensor Basis Neural Network (TBNN) frameworks aimed at enhancing turbulence RANS modeling have recently been proposed in the literature as data-driven constitutive models for systems with known invariance properties. However, persistent ambiguities remain regarding the physical adequacy of applying the General Eddy Viscosity Model (GEVM). This work aims at investigating this aspect in an a priori stage for better predictions of the Reynolds stress anisotropy tensor, while preserving the Galilean and rotational invariances. In particular, we propose a general framework providing optimal tensor basis models for two types of canonical flows: Plane Channel Flow (PCF) and Square Duct Flow (SDF). Subsequently, deep neural networks based on these optimal models are trained using state-of-the-art strategies to achieve a balanced and physically sound prediction of the full anisotropy tensor. A priori results obtained by the proposed framework are in very good agreement with the reference DNS data. Notably, our shallow network with three layers provides accurate predictions of the anisotropy tensor for PCF at unobserved friction Reynolds numbers, both in interpolation and extrapolation scenarios. Learning the SDF case is more challenging because of its physical nature and a lack of training data at various regimes. We propose to alleviate this problem based on Transfer Learning (TL). To more efficiently generalize to an unseen intermediate $\mathrm{Re}_τ$ regime, we take advantage of our prior knowledge acquired from a training with a larger and wider dataset. Our results indicate the potential of the developed network model, and demonstrate the feasibility and efficiency of the TL process in terms of training data size and training time. Based on these results, we believe there is a promising future by integrating these neural networks into an adapted in-house RANS solver.",True,True,"Cai, Jiayi and Angeli, Pierre-Emmanuel and Martinez, Jean-Marc and Damblin, Guillaume and Lucor, Didier",2024,,http://arxiv.org/abs/2403.11746,10.1016/j.compfluid.2024.106246,Computers \& Fluids
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,spencer_theory_1958,\cite{spencer_theory_1958},The theory of matrix polynomials and its application to the mechanics of isotropic continua,,,True,False,"Spencer, A. J. M. and Rivlin, R. S.",1958,,https://doi.org/10.1007/BF00277933,10.1007/BF00277933,Archive for Rational Mechanics and Analysis
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,zhang_ensemble_2022,\cite{zhang_ensemble_2022},Ensemble {Kalman} method for learning turbulence models from indirect observation data,,,True,False,"Zhang, Xin-Lei and Xiao, Heng and Luo, Xiaodong and He, Guowei",2022,,https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/ensemble-kalman-method-for-learning-turbulence-models-from-indirect-observation-data/1017FA12DF01850C1579ABCFB09405E2,10.1017/jfm.2022.744,Journal of Fluid Mechanics
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,lennon_scientific_2023,\cite{lennon_scientific_2023},Scientific machine learning for modeling and simulating complex fluids,,,True,False,"Lennon, Kyle R. and McKinley, Gareth H. and Swan, James W.",2023,,https://www.pnas.org/doi/10.1073/pnas.2304669120,10.1073/pnas.2304669120,Proceedings of the National Academy of Sciences
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,parmar_generalized_2020,\cite{parmar_generalized_2020},Generalized {Non}-{Linear} {Eddy} {Viscosity} {Models} for {Data}-{Assisted} {Reynolds} {Stress} {Closure},,,True,False,"Parmar, Basu and Peters, Eric and Jansen, Kenneth E. and Doostan, Alireza and Evans, John A.",2020,,https://arc.aiaa.org/doi/10.2514/6.2020-0351,10.2514/6.2020-0351,
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,sunol_learning_2025,\cite{sunol_learning_2025},Learning constitutive models and rheology from partial flow measurements,,,True,False,"Sunol, Alp M. and Roggeveen, James V. and Alhashim, Mohammed G. and Bae, Henry S. and Brenner, Michael P.",2025,,http://arxiv.org/abs/2510.24673,10.48550/arXiv.2510.24673,
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,smith_isotropic_1971,\cite{smith_isotropic_1971},"On isotropic functions of symmetric tensors, skew-symmetric tensors and vectors",,,True,False,"Smith, G. F.",1971,,https://www.sciencedirect.com/science/article/pii/0020722571900231,10.1016/0020-7225(71)90023-1,International Journal of Engineering Science
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,pennisi_third_1992,\cite{pennisi_third_1992},On third order tensor-valued isotropic functions,,,True,False,"Pennisi, S.",1992,,https://www.sciencedirect.com/science/article/pii/0020722592900115,10.1016/0020-7225(92)90011-5,International Journal of Engineering Science
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,prakash_invariant_2022,\cite{prakash_invariant_2022},Invariant Data-Driven Subgrid Stress Modeling on Anisotropic Grids for Large Eddy Simulation,https://arxiv.org/abs/2212.00332v2,"We present a new approach for constructing data-driven subgrid stress models for large eddy simulation of turbulent flows using anisotropic grids. The key to our approach is a Galilean, rotationally, reflectionally and unit invariant model form that also embeds filter anisotropy in such a way that an important subgrid stress identity is satisfied. We use this model form to train a data-driven subgrid stress model using only a small amount of anisotropically filtered DNS data and a simple and inexpensive neural network architecture. A priori and a posteriori tests indicate that the trained data-driven model generalizes well to filter anisotropy ratios, Reynolds numbers and flow physics outside the training dataset.",True,True,"Prakash, Aviral and Jansen, Kenneth E. and Evans, John A.",2022,,https://www.sciencedirect.com/science/article/pii/S0045782522004923,10.1016/j.cma.2022.115457,Computer Methods in Applied Mechanics and Engineering
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,duval_hitchhikers_2024,\cite{duval_hitchhikers_2024},A {Hitchhiker}'s {Guide} to {Geometric} {GNNs} for {3D} {Atomic} {Systems},,,True,False,"Duval, Alexandre and Mathis, Simon V. and Joshi, Chaitanya K. and Schmidt, Victor and Miret, Santiago and Malliaros, Fragkiskos D. and Cohen, Taco and Liò, Pietro and Bengio, Yoshua and Bronstein, Michael",2024,,http://arxiv.org/abs/2312.07511,10.48550/arXiv.2312.07511,
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,schutt_schnet_2017,\cite{schutt_schnet_2017},SchNet: A continuous-filter convolutional neural network for modeling quantum interactions,https://arxiv.org/abs/1706.08566v5,"Deep learning has the potential to revolutionize quantum chemistry as it is ideally suited to learn representations for structured data and speed up the exploration of chemical space. While convolutional neural networks have proven to be the first choice for images, audio and video data, the atoms in molecules are not restricted to a grid. Instead, their precise locations contain essential physical information, that would get lost if discretized. Thus, we propose to use continuous-filter convolutional layers to be able to model local correlations without requiring the data to lie on a grid. We apply those layers in SchNet: a novel deep learning architecture modeling quantum interactions in molecules. We obtain a joint model for the total energy and interatomic forces that follows fundamental quantum-chemical principles. This includes rotationally invariant energy predictions and a smooth, differentiable potential energy surface. Our architecture achieves state-of-the-art performance for benchmarks of equilibrium molecules and molecular dynamics trajectories. Finally, we introduce a more challenging benchmark with chemical and structural variations that suggests the path for further work.",True,True,"Schütt, Kristof T. and Kindermans, Pieter-Jan and Sauceda, Huziel E. and Chmiela, Stefan and Tkatchenko, Alexandre and Müller, Klaus-Robert",2017,,http://arxiv.org/abs/1706.08566,10.48550/arXiv.1706.08566,
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,gasteiger_directional_2022,\cite{gasteiger_directional_2022},Directional {Message} {Passing} for {Molecular} {Graphs},,,True,False,"Gasteiger, Johannes and Groß, Janek and Günnemann, Stephan",2022,,http://arxiv.org/abs/2003.03123,10.48550/arXiv.2003.03123,
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,thomas_tensor_2018,\cite{thomas_tensor_2018},Tensor field networks: {Rotation}- and translation-equivariant neural networks for {3D} point clouds,,,True,False,"Thomas, Nathaniel and Smidt, Tess and Kearnes, Steven and Yang, Lusann and Li, Li and Kohlhoff, Kai and Riley, Patrick",2018,,http://arxiv.org/abs/1802.08219,,
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,weiler_3d_2018,\cite{weiler_3d_2018},3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric Data,https://arxiv.org/abs/1807.02547v2,"We present a convolutional network that is equivariant to rigid body motions. The model uses scalar-, vector-, and tensor fields over 3D Euclidean space to represent data, and equivariant convolutions to map between such representations. These SE(3)-equivariant convolutions utilize kernels which are parameterized as a linear combination of a complete steerable kernel basis, which is derived analytically in this paper. We prove that equivariant convolutions are the most general equivariant linear maps between fields over R^3. Our experimental results confirm the effectiveness of 3D Steerable CNNs for the problem of amino acid propensity prediction and protein structure classification, both of which have inherent SE(3) symmetry.",True,True,"Weiler, Maurice and Geiger, Mario and Welling, Max and Boomsma, Wouter and Cohen, Taco",2018,,http://arxiv.org/abs/1807.02547,,
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,brandstetter_geometric_2021,\cite{brandstetter_geometric_2021},Geometric and {Physical} {Quantities} improve {E}(3) {Equivariant} {Message} {Passing},,,True,False,"Brandstetter, Johannes and Hesselink, Rob and Pol, Elise van der and Bekkers, Erik J. and Welling, Max",2021,,https://openreview.net/forum?id=_xwr8gOBeV1,,
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,kondor_clebsch-gordan_2018,\cite{kondor_clebsch-gordan_2018},Clebsch-Gordan Nets: a Fully Fourier Space Spherical Convolutional Neural Network,https://arxiv.org/abs/1806.09231v2,"Recent work by Cohen \emph{et al.} has achieved state-of-the-art results for learning spherical images in a rotation invariant way by using ideas from group representation theory and noncommutative harmonic analysis. In this paper we propose a generalization of this work that generally exhibits improved performace, but from an implementation point of view is actually simpler. An unusual feature of the proposed architecture is that it uses the Clebsch--Gordan transform as its only source of nonlinearity, thus avoiding repeated forward and backward Fourier transforms. The underlying ideas of the paper generalize to constructing neural networks that are invariant to the action of other compact groups.",True,True,"Kondor, Risi and Lin, Zhen and Trivedi, Shubhendu",2018,,http://arxiv.org/abs/1806.09231,10.48550/arXiv.1806.09231,
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,jumper_highly_2021,\cite{jumper_highly_2021},Highly accurate protein structure prediction with {AlphaFold},,,True,False,"Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis",2021,,https://www.nature.com/articles/s41586-021-03819-2,10.1038/s41586-021-03819-2,Nature
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,batzner_e3-equivariant_2022,\cite{batzner_e3-equivariant_2022},E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials,,,True,False,"Batzner, Simon and Musaelian, Albert and Sun, Lixin and Geiger, Mario and Mailoa, Jonathan P. and Kornbluth, Mordechai and Molinari, Nicola and Smidt, Tess E. and Kozinsky, Boris",2022,,https://www.nature.com/articles/s41467-022-29939-5,10.1038/s41467-022-29939-5,Nature Communications
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,zitnick_introduction_2020,\cite{zitnick_introduction_2020},An Introduction to Electrocatalyst Design using Machine Learning for Renewable Energy Storage,https://arxiv.org/abs/2010.09435v1,"Scalable and cost-effective solutions to renewable energy storage are essential to addressing the world's rising energy needs while reducing climate change. As we increase our reliance on renewable energy sources such as wind and solar, which produce intermittent power, storage is needed to transfer power from times of peak generation to peak demand. This may require the storage of power for hours, days, or months. One solution that offers the potential of scaling to nation-sized grids is the conversion of renewable energy to other fuels, such as hydrogen or methane. To be widely adopted, this process requires cost-effective solutions to running electrochemical reactions. An open challenge is finding low-cost electrocatalysts to drive these reactions at high rates. Through the use of quantum mechanical simulations (density functional theory), new catalyst structures can be tested and evaluated. Unfortunately, the high computational cost of these simulations limits the number of structures that may be tested. The use of machine learning may provide a method to efficiently approximate these calculations, leading to new approaches in finding effective electrocatalysts. In this paper, we provide an introduction to the challenges in finding suitable electrocatalysts, how machine learning may be applied to the problem, and the use of the Open Catalyst Project OC20 dataset for model training.",True,True,"Zitnick, C. Lawrence and Chanussot, Lowik and Das, Abhishek and Goyal, Siddharth and Heras-Domingo, Javier and Ho, Caleb and Hu, Weihua and Lavril, Thibaut and Palizhati, Aini and Riviere, Morgane and Shuaibi, Muhammed and Sriram, Anuroop and Tran, Kevin and Wood, Brandon and Yoon, Junwoong and Parikh, Devi and Ulissi, Zachary",2020,,http://arxiv.org/abs/2010.09435,10.48550/arXiv.2010.09435,
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,dauparas_robust_2022,\cite{dauparas_robust_2022},Robust deep learning–based protein sequence design using {ProteinMPNN},,,True,False,"Dauparas, J. and Anishchenko, I. and Bennett, N. and Bai, H. and Ragotte, R. J. and Milles, L. F. and Wicky, B. I. M. and Courbet, A. and de Haas, R. J. and Bethel, N. and Leung, P. J. Y. and Huddy, T. F. and Pellock, S. and Tischer, D. and Chan, F. and Koepnick, B. and Nguyen, H. and Kang, A. and Sankaran, B. and Bera, A. K. and King, N. P. and Baker, D.",2022,,https://www.science.org/doi/10.1126/science.add2187,10.1126/science.add2187,Science
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,zhou_frame-independent_2022,\cite{zhou_frame-independent_2022},Frame-independent vector-cloud neural network for nonlocal constitutive modeling on arbitrary grids,https://arxiv.org/abs/2103.06685v4,"Constitutive models are widely used for modeling complex systems in science and engineering, where first-principle-based, well-resolved simulations are often prohibitively expensive. For example, in fluid dynamics, constitutive models are required to describe nonlocal, unresolved physics such as turbulence and laminar-turbulent transition. However, traditional constitutive models based on partial differential equations (PDEs) often lack robustness and are too rigid to accommodate diverse calibration datasets. We propose a frame-independent, nonlocal constitutive model based on a vector-cloud neural network that can be learned with data. The model predicts the closure variable at a point based on the flow information in its neighborhood. Such nonlocal information is represented by a group of points, each having a feature vector attached to it, and thus the input is referred to as vector cloud. The cloud is mapped to the closure variable through a frame-independent neural network, invariant both to coordinate translation and rotation and to the ordering of points in the cloud. As such, the network can deal with any number of arbitrarily arranged grid points and thus is suitable for unstructured meshes in fluid simulations. The merits of the proposed network are demonstrated for scalar transport PDEs on a family of parameterized periodic hill geometries. The vector-cloud neural network is a promising tool not only as nonlocal constitutive models and but also as general surrogate models for PDEs on irregular domains.",True,True,"Zhou, Xu-Hui and Han, Jiequn and Xiao, Heng",2022,,https://www.sciencedirect.com/science/article/pii/S0045782521005429,10.1016/j.cma.2021.114211,Computer Methods in Applied Mechanics and Engineering
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,han_equivariant_2023,\cite{han_equivariant_2023},An equivariant neural operator for developing nonlocal tensorial constitutive models,,,True,False,"Han, Jiequn and Zhou, Xu-Hui and Xiao, Heng",2023,,https://www.sciencedirect.com/science/article/pii/S0021999123003388,10.1016/j.jcp.2023.112243,Journal of Computational Physics
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,gao_roteqnet_2022,\cite{gao_roteqnet_2022},{RotEqNet}: {Rotation}-equivariant network for fluid systems with symmetric high-order tensors,,,True,False,"Gao, Liyao and Du, Yifan and Li, Hongshan and Lin, Guang",2022,,https://www.sciencedirect.com/science/article/pii/S0021999122002674,10.1016/j.jcp.2022.111205,Journal of Computational Physics
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,kaszuba_implicit_2025,\cite{kaszuba_implicit_2025},Implicit modeling of equivariant tensor basis with {Euclidean} turbulence closure neural network,,,True,False,"Kaszuba, Grzegorz and Krakowski, Tomasz and Ziegler, Bartosz and Jaszkiewicz, Andrzej and Sankowski, Piotr",2025,,https://doi.org/10.1063/5.0249490,10.1063/5.0249490,Physics of Fluids
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,pfaff_learning_2021,\cite{pfaff_learning_2021},Learning {Mesh}-{Based} {Simulation} with {Graph} {Networks},,,True,False,"Pfaff, Tobias and Fortunato, Meire and Sanchez-Gonzalez, Alvaro and Battaglia, Peter W.",2021,,http://arxiv.org/abs/2010.03409,,
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,toshev_learning_2023,\cite{toshev_learning_2023},Learning {Lagrangian} {Fluid} {Mechanics} with {E}(\$3\$)-{Equivariant} {Graph} {Neural} {Networks},,,True,False,"Toshev, Artur P. and Galletti, Gianluca and Brandstetter, Johannes and Adami, Stefan and Adams, Nikolaus A.",2023,,http://arxiv.org/abs/2305.15603,10.48550/arXiv.2305.15603,
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,list_rotational_2025,\cite{list_rotational_2025},Rotational equivariant graph neural networks via local eigenbasis transformations,,,True,False,"List, B. and Lino, M. and Thuerey, N.",2025,,https://doi.org/10.1063/5.0279499,10.1063/5.0279499,Physics of Fluids
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,lino_multi-scale_2022,\cite{lino_multi-scale_2022},Multi-scale rotation-equivariant graph neural networks for unsteady {Eulerian} fluid dynamics,,,True,False,"Lino, Mario and Fotiadis, Stathi and Bharath, Anil A. and Cantwell, Chris D.",2022,,https://pubs.aip.org/pof/article/34/8/087110/2847850/Multi-scale-rotation-equivariant-graph-neural,10.1063/5.0097679,Physics of Fluids
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,milano_neural_2002,\cite{milano_neural_2002},Neural {Network} {Modeling} for {Near} {Wall} {Turbulent} {Flow},,,True,False,"Milano, Michele and Koumoutsakos, Petros",2002,,https://www.sciencedirect.com/science/article/pii/S0021999102971469,10.1006/jcph.2002.7146,Journal of Computational Physics
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,raissi_physics-informed_2019,\cite{raissi_physics-informed_2019},Physics-informed neural networks: {A} deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations,,,True,False,"Raissi, M. and Perdikaris, P. and Karniadakis, G.E.",2019,,https://linkinghub.elsevier.com/retrieve/pii/S0021999118307125,10.1016/j.jcp.2018.10.045,Journal of Computational Physics
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,karniadakis_physics-informed_2021,\cite{karniadakis_physics-informed_2021},Physics-informed machine learning,,,True,False,"Karniadakis, George Em and Kevrekidis, Ioannis G. and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu",2021,,https://www.nature.com/articles/s42254-021-00314-5,10.1038/s42254-021-00314-5,Nature Reviews Physics
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,wang_incorporating_2021,\cite{wang_incorporating_2021},Incorporating {Symmetry} into {Deep} {Dynamics} {Models} for {Improved} {Generalization},,,True,False,"Wang, Rui and Walters, Robin and Yu, Rose",2021,,http://arxiv.org/abs/2002.03061,10.48550/arXiv.2002.03061,
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,chen_group-theoretic_nodate,\cite{chen_group-theoretic_nodate},A {Group}-{Theoretic} {Framework} for {Data} {Augmentation},,,True,False,"Chen, Shuxiao and Dobriban, Edgar and Lee, Jane H",,,,,
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,liu_harnessing_2024,\cite{liu_harnessing_2024},Harnessing the {Power} of {Neural} {Operators} with {Automatically} {Encoded} {Conservation} {Laws},,,True,False,"Liu, Ning and Fan, Yiming and Zeng, Xianyi and Klöwer, Milan and Zhang, Lu and Yu, Yue",2024,,https://proceedings.mlr.press/v235/liu24p.html,,
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,richter-powell_neural_2022,\cite{richter-powell_neural_2022},Neural Conservation Laws: A Divergence-Free Perspective,https://arxiv.org/abs/2210.01741v3,"We investigate the parameterization of deep neural networks that by design satisfy the continuity equation, a fundamental conservation law. This is enabled by the observation that any solution of the continuity equation can be represented as a divergence-free vector field. We hence propose building divergence-free neural networks through the concept of differential forms, and with the aid of automatic differentiation, realize two practical constructions. As a result, we can parameterize pairs of densities and vector fields that always exactly satisfy the continuity equation, foregoing the need for extra penalty methods or expensive numerical simulation. Furthermore, we prove these models are universal and so can be used to represent any divergence-free vector field. Finally, we experimentally validate our approaches by computing neural network-based solutions to fluid equations, solving for the Hodge decomposition, and learning dynamical optimal transport maps.",True,True,"Richter-Powell, Jack and Lipman, Yaron and Chen, Ricky T. Q.",2022,,http://arxiv.org/abs/2210.01741,10.48550/arXiv.2210.01741,
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,chalapathi_scaling_2024,\cite{chalapathi_scaling_2024},Scaling physics-informed hard constraints with mixture-of-experts,https://arxiv.org/abs/2402.13412v1,"Imposing known physical constraints, such as conservation laws, during neural network training introduces an inductive bias that can improve accuracy, reliability, convergence, and data efficiency for modeling physical dynamics. While such constraints can be softly imposed via loss function penalties, recent advancements in differentiable physics and optimization improve performance by incorporating PDE-constrained optimization as individual layers in neural networks. This enables a stricter adherence to physical constraints. However, imposing hard constraints significantly increases computational and memory costs, especially for complex dynamical systems. This is because it requires solving an optimization problem over a large number of points in a mesh, representing spatial and temporal discretizations, which greatly increases the complexity of the constraint. To address this challenge, we develop a scalable approach to enforce hard physical constraints using Mixture-of-Experts (MoE), which can be used with any neural network architecture. Our approach imposes the constraint over smaller decomposed domains, each of which is solved by an ""expert"" through differentiable optimization. During training, each expert independently performs a localized backpropagation step by leveraging the implicit function theorem; the independence of each expert allows for parallelization across multiple GPUs. Compared to standard differentiable optimization, our scalable approach achieves greater accuracy in the neural PDE solver setting for predicting the dynamics of challenging non-linear systems. We also improve training stability and require significantly less computation time during both training and inference stages.",True,True,"Chalapathi, Nithin and Du, Yiheng and Krishnapriyan, Aditi",2024,,http://arxiv.org/abs/2402.13412,10.48550/arXiv.2402.13412,
Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks,2511.09769v1,vasilache_tensor_2018,\cite{vasilache_tensor_2018},Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions,https://arxiv.org/abs/1802.04730v3,"Deep learning models with convolutional and recurrent networks are now ubiquitous and analyze massive amounts of audio, image, video, text and graph data, with applications in automatic translation, speech-to-text, scene understanding, ranking user preferences, ad placement, etc. Competing frameworks for building these networks such as TensorFlow, Chainer, CNTK, Torch/PyTorch, Caffe1/2, MXNet and Theano, explore different tradeoffs between usability and expressiveness, research or production orientation and supported hardware. They operate on a DAG of computational operators, wrapping high-performance libraries such as CUDNN (for NVIDIA GPUs) or NNPACK (for various CPUs), and automate memory allocation, synchronization, distribution. Custom operators are needed where the computation does not fit existing high-performance library calls, usually at a high engineering cost. This is frequently required when new operators are invented by researchers: such operators suffer a severe performance penalty, which limits the pace of innovation. Furthermore, even if there is an existing runtime call these frameworks can use, it often doesn't offer optimal performance for a user's particular network architecture and dataset, missing optimizations between operators as well as optimizations that can be done knowing the size and shape of data. Our contributions include (1) a language close to the mathematics of deep learning called Tensor Comprehensions, (2) a polyhedral Just-In-Time compiler to convert a mathematical description of a deep learning DAG into a CUDA kernel with delegated memory management and synchronization, also providing optimizations such as operator fusion and specialization for specific sizes, (3) a compilation cache populated by an autotuner. [Abstract cutoff]",True,True,"Vasilache, Nicolas and Zinenko, Oleksandr and Theodoridis, Theodoros and Goyal, Priya and DeVito, Zachary and Moses, William S. and Verdoolaege, Sven and Adams, Andrew and Cohen, Albert",2018,,http://arxiv.org/abs/1802.04730,10.48550/arXiv.1802.04730,
ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech,2511.08247v1,heseltine2024large,\cite{heseltine2024large},Incentivizing News Consumption on Social Media Platforms Using Large Language Models and Realistic Bot Accounts,https://arxiv.org/abs/2403.13362v3,"Polarization, declining trust, and wavering support for democratic norms are pressing threats to U.S. democracy. Exposure to verified and quality news may lower individual susceptibility to these threats and make citizens more resilient to misinformation, populism, and hyperpartisan rhetoric. This project examines how to enhance users' exposure to and engagement with verified and ideologically balanced news in an ecologically valid setting. We rely on a large-scale two-week long field experiment (from 1/19/2023 to 2/3/2023) on 28,457 Twitter users. We created 28 bots utilizing GPT-2 that replied to users tweeting about sports, entertainment, or lifestyle with a contextual reply containing two hardcoded elements: a URL to the topic-relevant section of quality news organization and an encouragement to follow its Twitter account. To further test differential effects by gender of the bots, treated users were randomly assigned to receive responses by bots presented as female or male. We examine whether our over-time intervention enhances the following of news media organization, the sharing and the liking of news content and the tweeting about politics and the liking of political content. We find that the treated users followed more news accounts and the users in the female bot treatment were more likely to like news content than the control. Most of these results, however, were small in magnitude and confined to the already politically interested Twitter users, as indicated by their pre-treatment tweeting about politics. These findings have implications for social media and news organizations, and also offer direction for future work on how Large Language Models and other computational interventions can effectively enhance individual on-platform engagement with quality news and public affairs.",True,True,,,,,,
ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech,2511.08247v1,gunes2023multiclass,\cite{gunes2023multiclass},Multiclass Classification of Policy Documents with Large Language Models,https://arxiv.org/abs/2310.08167v1,"Classifying policy documents into policy issue topics has been a long-time effort in political science and communication disciplines. Efforts to automate text classification processes for social science research purposes have so far achieved remarkable results, but there is still a large room for progress. In this work, we test the prediction performance of an alternative strategy, which requires human involvement much less than full manual coding. We use the GPT 3.5 and GPT 4 models of the OpenAI, which are pre-trained instruction-tuned Large Language Models (LLM), to classify congressional bills and congressional hearings into Comparative Agendas Project's 21 major policy issue topics. We propose three use-case scenarios and estimate overall accuracies ranging from %58-83 depending on scenario and GPT model employed. The three scenarios aims at minimal, moderate, and major human interference, respectively. Overall, our results point towards the insufficiency of complete reliance on GPT with minimal human intervention, an increasing accuracy along with the human effort exerted, and a surprisingly high accuracy achieved in the most humanly demanding use-case. However, the superior use-case achieved the %83 accuracy on the %65 of the data in which the two models agreed, suggesting that a similar approach to ours can be relatively easily implemented and allow for mostly automated coding of a majority of a given dataset. This could free up resources allowing manual human coding of the remaining %35 of the data to achieve an overall higher level of accuracy while reducing costs significantly.",True,True,,,,,,
ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech,2511.08247v1,argyle2023out,\cite{argyle2023out},AI Chat Assistants can Improve Conversations about Divisive Topics,https://arxiv.org/abs/2302.07268v5,"A rapidly increasing amount of human conversation occurs online. But divisiveness and conflict can fester in text-based interactions on social media platforms, in messaging apps, and on other digital forums. Such toxicity increases polarization and, importantly, corrodes the capacity of diverse societies to develop efficient solutions to complex social problems that impact everyone. Scholars and civil society groups promote interventions that can make interpersonal conversations less divisive or more productive in offline settings, but scaling these efforts to the amount of discourse that occurs online is extremely challenging. We present results of a large-scale experiment that demonstrates how online conversations about divisive topics can be improved with artificial intelligence tools. Specifically, we employ a large language model to make real-time, evidence-based recommendations intended to improve participants' perception of feeling understood in conversations. We find that these interventions improve the reported quality of the conversation, reduce political divisiveness, and improve the tone, without systematically changing the content of the conversation or moving people's policy attitudes. These findings have important implications for future research on social media, political deliberation, and the growing community of scholars interested in the place of artificial intelligence within computational social science.",True,True,,,,,,
ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech,2511.08247v1,moghimifar2024modelling,\cite{moghimifar2024modelling},Modelling Political Coalition Negotiations Using LLM-based Agents,https://arxiv.org/abs/2402.11712v1,"Coalition negotiations are a cornerstone of parliamentary democracies, characterised by complex interactions and strategic communications among political parties. Despite its significance, the modelling of these negotiations has remained unexplored with the domain of Natural Language Processing (NLP), mostly due to lack of proper data. In this paper, we introduce coalition negotiations as a novel NLP task, and model it as a negotiation between large language model-based agents. We introduce a multilingual dataset, POLCA, comprising manifestos of European political parties and coalition agreements over a number of elections in these countries. This dataset addresses the challenge of the current scope limitations in political negotiation modelling by providing a diverse, real-world basis for simulation. Additionally, we propose a hierarchical Markov decision process designed to simulate the process of coalition negotiation between political parties and predict the outcomes. We evaluate the performance of state-of-the-art large language models (LLMs) as agents in handling coalition negotiations, offering insights into their capabilities and paving the way for future advancements in political modelling.",True,True,,,,,,
ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech,2511.08247v1,kornilova2019billsum,\cite{kornilova2019billsum},BillSum: A Corpus for Automatic Summarization of US Legislation,https://arxiv.org/abs/1910.00523v2,"Automatic summarization methods have been studied on a variety of domains, including news and scientific articles. Yet, legislation has not previously been considered for this task, despite US Congress and state governments releasing tens of thousands of bills every year. In this paper, we introduce BillSum, the first dataset for summarization of US Congressional and California state bills (https://github.com/FiscalNote/BillSum). We explain the properties of the dataset that make it more challenging to process than other domains. Then, we benchmark extractive methods that consider neural sentence representations and traditional contextual features. Finally, we demonstrate that models built on Congressional bills can be used to summarize California bills, thus, showing that methods developed on this dataset can transfer to states without human-written summaries.",True,True,,,,,,
ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech,2511.08247v1,santurkar2023whose,\cite{santurkar2023whose},Whose Opinions Do Language Models Reflect?,https://arxiv.org/abs/2303.17548v1,"Language models (LMs) are increasingly being used in open-ended contexts, where the opinions reflected by LMs in response to subjective queries can have a profound impact, both on user satisfaction, as well as shaping the views of society at large. In this work, we put forth a quantitative framework to investigate the opinions reflected by LMs -- by leveraging high-quality public opinion polls and their associated human responses. Using this framework, we create OpinionsQA, a new dataset for evaluating the alignment of LM opinions with those of 60 US demographic groups over topics ranging from abortion to automation. Across topics, we find substantial misalignment between the views reflected by current LMs and those of US demographic groups: on par with the Democrat-Republican divide on climate change. Notably, this misalignment persists even after explicitly steering the LMs towards particular demographic groups. Our analysis not only confirms prior observations about the left-leaning tendencies of some human feedback-tuned LMs, but also surfaces groups whose opinions are poorly reflected by current LMs (e.g., 65+ and widowed individuals). Our code and data are available at https://github.com/tatsu-lab/opinions_qa.",True,True,,,,,,
ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech,2511.08247v1,durmus2024towards,\cite{durmus2024towards},Towards Federated Learning with On-device Training and Communication in 8-bit Floating Point,https://arxiv.org/abs/2407.02610v2,"Recent work has shown that 8-bit floating point (FP8) can be used for efficiently training neural networks with reduced computational cost compared to training in FP32/FP16. In this work, we investigate the use of FP8 training in a federated learning context. This approach brings not only the usual benefits of FP8 which are desirable for on-device training at the edge, but also reduces client-server communication costs due to significant weight compression. We present a novel method for combining FP8 client training while maintaining a global FP32 server model and provide convergence analysis. Experiments with various machine learning models and datasets show that our method consistently yields communication reductions of at least 2.9x across a variety of tasks and models compared to an FP32 baseline to achieve the same trained model accuracy.",True,True,,,,,,
Textual understanding boost in the WikiRace,2511.10585v1,siegart-2020,\cite{siegart-2020},{A Network Analysis of Wikispeedia},,,True,False,"Siegart, Jamie",2020,4,https://jsiegart.medium.com/a-network-analysis-of-wikispeedia-da4525e1f207,,
Textual understanding boost in the WikiRace,2511.10585v1,west2009wikispeedia,\cite{west2009wikispeedia},Wikispeedia: An Online Game for Inferring Semantic Distances between Concepts.,,,True,False,"West, Robert and Pineau, Joelle and Precup, Doina",2009,,,,
Textual understanding boost in the WikiRace,2511.10585v1,lamprecht2017structure,\cite{lamprecht2017structure},How the structure of Wikipedia articles influences user navigation,,,True,False,"Lamprecht, Daniel and Lerman, Kristina and Helic, Denis and Strohmaier, Markus",2017,,,,New Review of Hypermedia and Multimedia
Textual understanding boost in the WikiRace,2511.10585v1,rodi2017search,\cite{rodi2017search},Search strategies of Wikipedia readers,,,True,False,"Rodi, Giovanna Chiara and Loreto, Vittorio and Tria, Francesca",2017,,,,PloS one
Textual understanding boost in the WikiRace,2511.10585v1,west2012human,\cite{west2012human},Human wayfinding in information networks,,,True,False,"West, Robert and Leskovec, Jure",2012,,,,
Textual understanding boost in the WikiRace,2511.10585v1,koschutzki2008centrality,\cite{koschutzki2008centrality},Centrality analysis methods for biological networks and their application to gene regulatory networks,,,True,False,"Kosch{\""u}tzki, Dirk and Schreiber, Falk",2008,,,,Gene regulation and systems biology
Textual understanding boost in the WikiRace,2511.10585v1,cheriyan2020m,\cite{cheriyan2020m},m-PageRank: A novel centrality measure for multilayer networks,,,True,False,"Cheriyan, Jo and Sajeev, GP",2020,,,,Advances in Complex Systems
Textual understanding boost in the WikiRace,2511.10585v1,borgatti2006graph,\cite{borgatti2006graph},A graph-theoretic perspective on centrality,,,True,False,"Borgatti, Stephen P and Everett, Martin G",2006,,,,Social networks
Textual understanding boost in the WikiRace,2511.10585v1,church2017word2vec,\cite{church2017word2vec},Word2Vec,,,True,False,"Church, Kenneth Ward",2017,,,,Natural Language Engineering
Textual understanding boost in the WikiRace,2511.10585v1,yamada2018wikipedia2vec,\cite{yamada2018wikipedia2vec},Wikipedia2Vec: An efficient toolkit for learning and visualizing the embeddings of words and entities from Wikipedia,,,True,False,"Yamada, Ikuya and Asai, Akari and Sakuma, Jin and Shindo, Hiroyuki and Takeda, Hideaki and Takefuji, Yoshiyasu and Matsumoto, Yuji",2018,,,,arXiv preprint arXiv:1812.06280
Textual understanding boost in the WikiRace,2511.10585v1,kwon2020hierarchical,\cite{kwon2020hierarchical},Hierarchical trivia fact extraction from Wikipedia articles,,,True,False,"Kwon, Jingun and Kamigaito, Hidetaka and Song, Young-In and Okumura, Manabu",2020,,,,
Textual understanding boost in the WikiRace,2511.10585v1,barron-2011,\cite{barron-2011},{An AI for the Wikipedia game},,,True,False,"Barron, Alex and Swafford, Zack",2011,,https://cs229.stanford.edu/proj2015/309_report.pdf,,
Textual understanding boost in the WikiRace,2511.10585v1,darvariu2024graph,\cite{darvariu2024graph},Graph reinforcement learning for combinatorial optimization: A survey and unifying perspective,,,True,False,"Darvariu, Victor-Alexandru and Hailes, Stephen and Musolesi, Mirco",2024,,,,arXiv preprint arXiv:2404.06492
Textual understanding boost in the WikiRace,2511.10585v1,zaheer2022learning,\cite{zaheer2022learning},Learning to Navigate Wikipedia by Taking Random Walks,https://arxiv.org/abs/2211.00177v1,"A fundamental ability of an intelligent web-based agent is seeking out and acquiring new information. Internet search engines reliably find the correct vicinity but the top results may be a few links away from the desired target. A complementary approach is navigation via hyperlinks, employing a policy that comprehends local content and selects a link that moves it closer to the target. In this paper, we show that behavioral cloning of randomly sampled trajectories is sufficient to learn an effective link selection policy. We demonstrate the approach on a graph version of Wikipedia with 38M nodes and 387M edges. The model is able to efficiently navigate between nodes 5 and 20 steps apart 96% and 92% of the time, respectively. We then use the resulting embeddings and policy in downstream fact verification and question answering tasks where, in combination with basic TF-IDF search and ranking methods, they are competitive results to the state-of-the-art methods.",True,True,"Zaheer, Manzil and Marino, Kenneth and Grathwohl, Will and Schultz, John and Shang, Wendy and Babayan, Sheila and Ahuja, Arun and Dasgupta, Ishita and Kaeser-Chen, Christine and Fergus, Rob",2022,,,,Advances in Neural Information Processing Systems
LampQ: Towards Accurate Layer-wise Mixed Precision Quantization for Vision Transformers,2511.10004v1,MPSurvey,\cite{MPSurvey},A Review of State-of-the-Art Mixed-Precision Neural Network Frameworks,,,True,False,"Rakka, Mariam and Fouda, Mohammed E and Khargonekar, Pramod and Kurdahi, Fadi",2024,,,,
LampQ: Towards Accurate Layer-wise Mixed Precision Quantization for Vision Transformers,2511.10004v1,SDQ,\cite{SDQ},SDQ: Stochastic differentiable quantization with mixed precision,,,True,False,"Huang, Xijie and Shen, Zhiqiang and Li, Shichao and Liu, Zechun and Xianghong, Hu and Wicaksana, Jeffry and Xing, Eric and Cheng, Kwang-Ting",2022,,,,
LampQ: Towards Accurate Layer-wise Mixed Precision Quantization for Vision Transformers,2511.10004v1,NIPQ,\cite{NIPQ},NIPQ: Noise proxy-based Integrated Pseudo-Quantization,https://arxiv.org/abs/2206.00820v2,"Straight-through estimator (STE), which enables the gradient flow over the non-differentiable function via approximation, has been favored in studies related to quantization-aware training (QAT). However, STE incurs unstable convergence during QAT, resulting in notable quality degradation in low precision. Recently, pseudoquantization training has been proposed as an alternative approach to updating the learnable parameters using the pseudo-quantization noise instead of STE. In this study, we propose a novel noise proxy-based integrated pseudoquantization (NIPQ) that enables unified support of pseudoquantization for both activation and weight by integrating the idea of truncation on the pseudo-quantization framework. NIPQ updates all of the quantization parameters (e.g., bit-width and truncation boundary) as well as the network parameters via gradient descent without STE instability. According to our extensive experiments, NIPQ outperforms existing quantization algorithms in various vision and language applications by a large margin.",True,True,"Shin, Juncheol and So, Junhyuk and Park, Sein and Kang, Seungyeop and Yoo, Sungjoo and Park, Eunhyeok",2023,,,,
LampQ: Towards Accurate Layer-wise Mixed Precision Quantization for Vision Transformers,2511.10004v1,AutoQ,\cite{AutoQ},AutoQ: Automated Kernel-Wise Neural Network Quantization,https://arxiv.org/abs/1902.05690v3,"Network quantization is one of the most hardware friendly techniques to enable the deployment of convolutional neural networks (CNNs) on low-power mobile devices. Recent network quantization techniques quantize each weight kernel in a convolutional layer independently for higher inference accuracy, since the weight kernels in a layer exhibit different variances and hence have different amounts of redundancy. The quantization bitwidth or bit number (QBN) directly decides the inference accuracy, latency, energy and hardware overhead. To effectively reduce the redundancy and accelerate CNN inferences, various weight kernels should be quantized with different QBNs. However, prior works use only one QBN to quantize each convolutional layer or the entire CNN, because the design space of searching a QBN for each weight kernel is too large. The hand-crafted heuristic of the kernel-wise QBN search is so sophisticated that domain experts can obtain only sub-optimal results. It is difficult for even deep reinforcement learning (DRL) Deep Deterministic Policy Gradient (DDPG)-based agents to find a kernel-wise QBN configuration that can achieve reasonable inference accuracy. In this paper, we propose a hierarchical-DRL-based kernel-wise network quantization technique, AutoQ, to automatically search a QBN for each weight kernel, and choose another QBN for each activation layer. Compared to the models quantized by the state-of-the-art DRL-based schemes, on average, the same models quantized by AutoQ reduce the inference latency by 54.06\%, and decrease the inference energy consumption by 50.69\%, while achieving the same inference accuracy.",True,True,Qian Lou and Feng Guo and Minje Kim and Lantao Liu and Lei Jiang.,2020,,,,
LampQ: Towards Accurate Layer-wise Mixed Precision Quantization for Vision Transformers,2511.10004v1,MetaMix,\cite{MetaMix},MetaMix: Meta-state Precision Searcher for Mixed-precision Activation Quantization,https://arxiv.org/abs/2311.06798v2,"Mixed-precision quantization of efficient networks often suffer from activation instability encountered in the exploration of bit selections. To address this problem, we propose a novel method called MetaMix which consists of bit selection and weight training phases. The bit selection phase iterates two steps, (1) the mixed-precision-aware weight update, and (2) the bit-search training with the fixed mixed-precision-aware weights, both of which combined reduce activation instability in mixed-precision quantization and contribute to fast and high-quality bit selection. The weight training phase exploits the weights and step sizes trained in the bit selection phase and fine-tunes them thereby offering fast training. Our experiments with efficient and hard-to-quantize networks, i.e., MobileNet v2 and v3, and ResNet-18 on ImageNet show that our proposed method pushes the boundary of mixed-precision quantization, in terms of accuracy vs. operations, by outperforming both mixed- and single-precision SOTA methods.",True,True,"Kim, Han-Byul and Lee, Joo Hyung and Yoo, Sungjoo and Kim, Hong-Seok",2024,,,,
LampQ: Towards Accurate Layer-wise Mixed Precision Quantization for Vision Transformers,2511.10004v1,BP-NAS,\cite{BP-NAS},Search What You Want: Barrier Panelty NAS for Mixed Precision Quantization,https://arxiv.org/abs/2007.10026v1,"Emergent hardwares can support mixed precision CNN models inference that assign different bitwidths for different layers. Learning to find an optimal mixed precision model that can preserve accuracy and satisfy the specific constraints on model size and computation is extremely challenge due to the difficult in training a mixed precision model and the huge space of all possible bit quantizations. In this paper, we propose a novel soft Barrier Penalty based NAS (BP-NAS) for mixed precision quantization, which ensures all the searched models are inside the valid domain defined by the complexity constraint, thus could return an optimal model under the given constraint by conducting search only one time. The proposed soft Barrier Penalty is differentiable and can impose very large losses to those models outside the valid domain while almost no punishment for models inside the valid domain, thus constraining the search only in the feasible domain. In addition, a differentiable Prob-1 regularizer is proposed to ensure learning with NAS is reasonable. A distribution reshaping training strategy is also used to make training more stable. BP-NAS sets new state of the arts on both classification (Cifar-10, ImageNet) and detection (COCO), surpassing all the efficient mixed precision methods designed manually and automatically. Particularly, BP-NAS achieves higher mAP (up to 2.7\% mAP improvement) together with lower bit computation cost compared with the existing best mixed precision model on COCO detection.",True,True,"Yu, Haibao and Han, Qi and Li, Jianbo and Shi, Jianping and Cheng, Guangliang and Fan, Bin",2020,,,,
LampQ: Towards Accurate Layer-wise Mixed Precision Quantization for Vision Transformers,2511.10004v1,JAQ,\cite{JAQ},JAQ: Joint Efficient Architecture Design and Low-Bit Quantization with Hardware-Software Co-Exploration,https://arxiv.org/abs/2501.05339v1,"The co-design of neural network architectures, quantization precisions, and hardware accelerators offers a promising approach to achieving an optimal balance between performance and efficiency, particularly for model deployment on resource-constrained edge devices. In this work, we propose the JAQ Framework, which jointly optimizes the three critical dimensions. However, effectively automating the design process across the vast search space of those three dimensions poses significant challenges, especially when pursuing extremely low-bit quantization. Specifical, the primary challenges include: (1) Memory overhead in software-side: Low-precision quantization-aware training can lead to significant memory usage due to storing large intermediate features and latent weights for back-propagation, potentially causing memory exhaustion. (2) Search time-consuming in hardware-side: The discrete nature of hardware parameters and the complex interplay between compiler optimizations and individual operators make the accelerator search time-consuming. To address these issues, JAQ mitigates the memory overhead through a channel-wise sparse quantization (CSQ) scheme, selectively applying quantization to the most sensitive components of the model during optimization. Additionally, JAQ designs BatchTile, which employs a hardware generation network to encode all possible tiling modes, thereby speeding up the search for the optimal compiler mapping strategy. Extensive experiments demonstrate the effectiveness of JAQ, achieving approximately 7% higher Top-1 accuracy on ImageNet compared to previous methods and reducing the hardware search time per iteration to 0.15 seconds.",True,True,"Wang, Mingzi and Meng, Yuan and Tang, Chen and Zhang, Weixiang and Qin, Yijian and Yao, Yang and Li, Yingxin and Feng, Tongtong and Wang, Xin and Guan, Xun and others",2025,,,,
LampQ: Towards Accurate Layer-wise Mixed Precision Quantization for Vision Transformers,2511.10004v1,QE,\cite{QE},Entropy-driven mixed-precision quantization for deep network design,,,True,False,"Sun, Zhenhong and Ge, Ce and Wang, Junyan and Lin, Ming and Chen, Hesen and Li, Hao and Sun, Xiuyu",2022,,,,
LampQ: Towards Accurate Layer-wise Mixed Precision Quantization for Vision Transformers,2511.10004v1,OMPQ,\cite{OMPQ},OMPQ: Orthogonal Mixed Precision Quantization,https://arxiv.org/abs/2109.07865v4,"To bridge the ever increasing gap between deep neural networks' complexity and hardware capability, network quantization has attracted more and more research attention. The latest trend of mixed precision quantization takes advantage of hardware's multiple bit-width arithmetic operations to unleash the full potential of network quantization. However, this also results in a difficult integer programming formulation, and forces most existing approaches to use an extremely time-consuming search process even with various relaxations. Instead of solving a problem of the original integer programming, we propose to optimize a proxy metric, the concept of network orthogonality, which is highly correlated with the loss of the integer programming but also easy to optimize with linear programming. This approach reduces the search time and required data amount by orders of magnitude, with little compromise on quantization accuracy. Specifically, we achieve 72.08% Top-1 accuracy on ResNet-18 with 6.7Mb, which does not require any searching iterations. Given the high efficiency and low data dependency of our algorithm, we used it for the post-training quantization, which achieve 71.27% Top-1 accuracy on MobileNetV2 with only 1.5Mb. Our code is available at https://github.com/MAC-AutoML/OMPQ.",True,True,"Ma, Yuexiao and Jin, Taisong and Zheng, Xiawu and Wang, Yan and Li, Huixia and Wu, Yongjian and Jiang, Guannan and Zhang, Wei and Ji, Rongrong",2023,,,,
LOCA-R: Near-Perfect Performance on the Chinese Physics Olympiad 2025,2511.10515v1,kojima2022large,\cite{kojima2022large},Large language models are zero-shot reasoners,,,True,False,"Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke",2022,,,,Advances in neural information processing systems
LOCA-R: Near-Perfect Performance on the Chinese Physics Olympiad 2025,2511.10515v1,wei2022chain,\cite{wei2022chain},Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,https://arxiv.org/abs/2201.11903v6,"We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",True,True,"Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others",2022,,,,Advances in neural information processing systems
LOCA-R: Near-Perfect Performance on the Chinese Physics Olympiad 2025,2511.10515v1,yao2023tree,\cite{yao2023tree},Tree of Thoughts: Deliberate Problem Solving with Large Language Models,https://arxiv.org/abs/2305.10601v2,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",True,True,"Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik",2023,,,,Advances in neural information processing systems
LOCA-R: Near-Perfect Performance on the Chinese Physics Olympiad 2025,2511.10515v1,besta2024graph,\cite{besta2024graph},Graph of Thoughts: Solving Elaborate Problems with Large Language Models,https://arxiv.org/abs/2308.09687v4,"We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information (""LLM thoughts"") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by >31%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.",True,True,"Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and others",2024,,,,
LOCA-R: Near-Perfect Performance on the Chinese Physics Olympiad 2025,2511.10515v1,madaan2023self,\cite{madaan2023self},Self-refine: Iterative refinement with self-feedback,,,True,False,"Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others",2023,,,,Advances in Neural Information Processing Systems
LOCA-R: Near-Perfect Performance on the Chinese Physics Olympiad 2025,2511.10515v1,liu2024large,\cite{liu2024large},Large Language Models have Intrinsic Self-Correction Ability,https://arxiv.org/abs/2406.15673v2,"Large language models (LLMs) have attracted significant attention for their exceptional abilities in various natural language processing tasks, but they suffer from hallucinations that will cause performance degradation. One promising solution to improve the LLMs' performance is to ask LLMs to revise their answer after generation, a technique known as self-correction. Among the two types of self-correction, intrinsic self-correction is considered a promising direction because it does not utilize external knowledge. However, recent works doubt the validity of LLM's ability to conduct intrinsic self-correction. In this paper, we present a novel perspective on the intrinsic self-correction capabilities of LLMs through theoretical analyses and empirical experiments. In addition, we identify two critical factors for successful self-correction: zero temperature and fair prompts. Leveraging these factors, we demonstrate that intrinsic self-correction ability is exhibited across multiple existing LLMs. Our findings offer insights into the fundamental theories underlying the self-correction behavior of LLMs and remark on the importance of unbiased prompts and zero temperature settings in harnessing their full potential.",True,True,"Liu, Dancheng and Nassereldine, Amir and Yang, Ziming and Xu, Chenhui and Hu, Yuting and Li, Jiajie and Kumar, Utkarsh and Lee, Changjae and Qin, Ruiyang and Shi, Yiyu and others",2024,,,,arXiv preprint arXiv:2406.15673
LOCA-R: Near-Perfect Performance on the Chinese Physics Olympiad 2025,2511.10515v1,liang2024internal,\cite{liang2024internal},Internal consistency and self-feedback in large language models: A survey,,,True,False,"Liang, Xun and Song, Shichao and Zheng, Zifan and Wang, Hanyu and Yu, Qingchen and Li, Xunkai and Li, Rong-Hua and Wang, Yi and Wang, Zhonghao and Xiong, Feiyu and others",2024,,,,arXiv preprint arXiv:2407.14507
LOCA-R: Near-Perfect Performance on the Chinese Physics Olympiad 2025,2511.10515v1,zhang2024sciinstruct,\cite{zhang2024sciinstruct},SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models,https://arxiv.org/abs/2401.07950v3,"Large Language Models (LLMs) have shown promise in assisting scientific discovery. However, such applications are currently limited by LLMs' deficiencies in understanding intricate scientific concepts, deriving symbolic equations, and solving advanced numerical calculations. To bridge these gaps, we introduce SciInstruct, a suite of scientific instructions for training scientific language models capable of college-level scientific reasoning. Central to our approach is a novel self-reflective instruction annotation framework to address the data scarcity challenge in the science domain. This framework leverages existing LLMs to generate step-by-step reasoning for unlabelled scientific questions, followed by a process of self-reflective critic-and-revise. Applying this framework, we curated a diverse and high-quality dataset encompassing physics, chemistry, math, and formal proofs. We analyze the curated SciInstruct from multiple interesting perspectives (e.g., domain, scale, source, question type, answer length, etc.). To verify the effectiveness of SciInstruct, we fine-tuned different language models with SciInstruct, i.e., ChatGLM3 (6B and 32B), Llama3-8B-Instruct, and Mistral-7B: MetaMath, enhancing their scientific and mathematical reasoning capabilities, without sacrificing the language understanding capabilities of the base model. We release all codes and SciInstruct at https://github.com/THUDM/SciGLM.",True,True,"Zhang, Dan and Hu, Ziniu and Zhoubian, Sining and Du, Zhengxiao and Yang, Kaiyu and Wang, Zihan and Yue, Yisong and Dong, Yuxiao and Tang, Jie",2024,,,,Advances in Neural Information Processing Systems
LOCA-R: Near-Perfect Performance on the Chinese Physics Olympiad 2025,2511.10515v1,wang2023plan,\cite{wang2023plan},Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models,,,True,False,"Wang, Lei and Xu, Wanyu and Lan, Yihuai and Hu, Zhiqiang and Lan, Yunshi and Lee, Roy Ka-Wei and Lim, Ee-Peng",2023,,,,arXiv preprint arXiv:2305.04091
LOCA-R: Near-Perfect Performance on the Chinese Physics Olympiad 2025,2511.10515v1,yao2022react,\cite{yao2022react},React: Synergizing reasoning and acting in language models,,,True,False,"Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik R and Cao, Yuan",2022,,,,
LOCA-R: Near-Perfect Performance on the Chinese Physics Olympiad 2025,2511.10515v1,du2023improving,\cite{du2023improving},Improving factuality and reasoning in language models through multiagent debate,,,True,False,"Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B and Mordatch, Igor",2023,,,,
LOCA-R: Near-Perfect Performance on the Chinese Physics Olympiad 2025,2511.10515v1,lu2022dynamic,\cite{lu2022dynamic},Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning,https://arxiv.org/abs/2209.14610v3,"Mathematical reasoning, a core ability of human intelligence, presents unique challenges for machines in abstract thinking and logical reasoning. Recent large pre-trained language models such as GPT-3 have achieved remarkable progress on mathematical reasoning tasks written in text form, such as math word problems (MWP). However, it is unknown if the models can handle more complex problems that involve math reasoning over heterogeneous information, such as tabular data. To fill the gap, we present Tabular Math Word Problems (TabMWP), a new dataset containing 38,431 open-domain grade-level problems that require mathematical reasoning on both textual and tabular data. Each question in TabMWP is aligned with a tabular context, which is presented as an image, semi-structured text, and a structured table. There are two types of questions: free-text and multi-choice, and each problem is annotated with gold solutions to reveal the multi-step reasoning process. We evaluate different pre-trained models on TabMWP, including the GPT-3 model in a few-shot setting. As earlier studies suggest, since few-shot GPT-3 relies on the selection of in-context examples, its performance is unstable and can degrade to near chance. The unstable issue is more severe when handling complex problems like TabMWP. To mitigate this, we further propose a novel approach, PromptPG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example. Experimental results show that our method outperforms the best baseline by 5.31% on the accuracy metric and reduces the prediction variance significantly compared to random selection, which verifies its effectiveness in selecting in-context examples.",True,True,"Lu, Pan and Qiu, Liang and Chang, Kai-Wei and Wu, Ying Nian and Zhu, Song-Chun and Rajpurohit, Tanmay and Clark, Peter and Kalyan, Ashwin",2022,,,,arXiv preprint arXiv:2209.14610
LOCA-R: Near-Perfect Performance on the Chinese Physics Olympiad 2025,2511.10515v1,lewkowycz2022solving,\cite{lewkowycz2022solving},Solving quantitative reasoning problems with language models,,,True,False,"Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others",2022,,,,Advances in neural information processing systems
LOCA-R: Near-Perfect Performance on the Chinese Physics Olympiad 2025,2511.10515v1,qiu2025physics,\cite{qiu2025physics},Physics Supernova: AI Agent Matches Elite Gold Medalists at IPhO 2025,https://arxiv.org/abs/2509.01659v1,"Physics provides fundamental laws that describe and predict the natural world. AI systems aspiring toward more general, real-world intelligence must therefore demonstrate strong physics problem-solving abilities: to formulate and apply physical laws for explaining and predicting physical processes. The International Physics Olympiad (IPhO)--the world's most prestigious physics competition--offers a rigorous benchmark for this purpose. We introduce Physics Supernova, an AI agent system with superior physics problem-solving abilities that match elite IPhO gold medalists. In IPhO 2025 theory problems, Physics Supernova attains 23.5/30 points, ranking 14th of 406 contestants and surpassing the median performance of human gold medalists. We extensively analyzed Physics Supernova's capabilities and flexibility across diverse physics tasks. These results show that principled tool integration within agent systems can deliver competitive improvements in solving challenging science problems. The codes are available at https://github.com/CharlesQ9/Physics-Supernova.",True,True,"Qiu, Jiahao and Shi, Jingzhe and Juan, Xinzhe and Zhao, Zelin and Geng, Jiayi and Liu, Shilong and Wang, Hongru and Wu, Sanfeng and Wang, Mengdi",2025,,,,arXiv preprint arXiv:2509.01659
LOCA-R: Near-Perfect Performance on the Chinese Physics Olympiad 2025,2511.10515v1,pang2025physics,\cite{pang2025physics},Physics Reasoner: Knowledge-Augmented Reasoning for Solving Physics Problems with Large Language Models,https://arxiv.org/abs/2412.13791v1,"Physics problems constitute a significant aspect of reasoning, necessitating complicated reasoning ability and abundant physics knowledge. However, existing large language models (LLMs) frequently fail due to a lack of knowledge or incorrect knowledge application. To mitigate these issues, we propose Physics Reasoner, a knowledge-augmented framework to solve physics problems with LLMs. Specifically, the proposed framework constructs a comprehensive formula set to provide explicit physics knowledge and utilizes checklists containing detailed instructions to guide effective knowledge application. Namely, given a physics problem, Physics Reasoner solves it through three stages: problem analysis, formula retrieval, and guided reasoning. During the process, checklists are employed to enhance LLMs' self-improvement in the analysis and reasoning stages. Empirically, Physics Reasoner mitigates the issues of insufficient knowledge and incorrect application, achieving state-of-the-art performance on SciBench with an average accuracy improvement of 5.8%.",True,True,"Pang, Xinyu and Hong, Ruixin and Zhou, Zhanke and Lv, Fangrui and Yang, Xinwei and Liang, Zhilong and Han, Bo and Zhang, Changshui",2025,,,,
LOCA-R: Near-Perfect Performance on the Chinese Physics Olympiad 2025,2511.10515v1,jaiswal2024improving,\cite{jaiswal2024improving},Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents,https://arxiv.org/abs/2412.00821v1,"Large Language Models (LLMs) demonstrate remarkable capabilities in various reasoning tasks. However, they encounter significant challenges when it comes to scientific reasoning, particularly in physics, which requires not only mathematical reasoning but also factual and conceptual understanding. When addressing complex physics problems, LLMs typically face three key issues: problem miscomprehension, incorrect concept application, and computational errors. While each of these problems can be addressed individually, there is a need for a generalized approach that can tackle all three issues simultaneously. To address this, we introduce Mixture of Refinement Agents (MoRA), a novel agentic refinement framework that iteratively refines the LLM generated base solution by correcting the aforementioned errors, resulting in a significant performance improvement for open-source LLMs. Our approach aims to bridge the gap between opensource LLMs and GPT-4o by utilizing the latter as error identifier to guide these refinement agents. We evaluate our approach on the SciEval and MMLU subsets along with our own physics dataset (PhysicsQA). MoRA significantly improves the performance of Llama-3-70B and Gemma-2-27B on these datasets, achieving up to a 16% increase in final answer accuracy.",True,True,"Jaiswal, Raj and Jain, Dhruv and Popat, Harsh Parimal and Anand, Avinash and Dharmadhikari, Abhishek and Marathe, Atharva and Shah, Rajiv Ratn",2024,,,,arXiv preprint arXiv:2412.00821
LOCA-R: Near-Perfect Performance on the Chinese Physics Olympiad 2025,2511.10515v1,qiu2025phybench,\cite{qiu2025phybench},PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models,https://arxiv.org/abs/2504.16074v2,"Current benchmarks for evaluating the reasoning capabilities of Large Language Models (LLMs) face significant limitations: task oversimplification, data contamination, and flawed evaluation items. These deficiencies necessitate more rigorous assessment methods. To address these limitations, we introduce PHYBench, a benchmark of 500 original physics problems ranging from high school to Physics Olympiad difficulty. PHYBench addresses data contamination through original content and employs a systematic curation pipeline to eliminate flawed items. Evaluations show that PHYBench activates more tokens and provides stronger differentiation between reasoning models compared to other baselines like AIME 2024, OlympiadBench and GPQA. Even the best-performing model, Gemini 2.5 Pro, achieves only 36.9% accuracy compared to human experts' 61.9%. To further enhance evaluation precision, we introduce the Expression Edit Distance (EED) Score for mathematical expression assessment, which improves sample efficiency by 204% over binary scoring. Moreover, PHYBench effectively elicits multi-step and multi-condition reasoning, providing a platform for examining models' reasoning robustness, preferences, and deficiencies. The benchmark results and dataset are publicly available at https://www.phybench.cn/.",True,True,"Qiu, Shi and Guo, Shaoyang and Song, Zhuo-Yang and Sun, Yunbo and Cai, Zeyu and Wei, Jiashen and Luo, Tianyu and Yin, Yixuan and Zhang, Haoxu and Hu, Yi and others",2025,,,,arXiv preprint arXiv:2504.16074
LOCA-R: Near-Perfect Performance on the Chinese Physics Olympiad 2025,2511.10515v1,he2024olympiadbench,\cite{he2024olympiadbench},Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems,,,True,False,"He, Chaoqun and Luo, Renjie and Bai, Yuzhuo and Hu, Shengding and Thai, Zhen and Shen, Junhao and Hu, Jinyi and Han, Xu and Huang, Yujie and Zhang, Yuxiang and others",2024,,,,
LOCA-R: Near-Perfect Performance on the Chinese Physics Olympiad 2025,2511.10515v1,feng2025physics,\cite{feng2025physics},PHYSICS: Benchmarking Foundation Models on University-Level Physics Problem Solving,https://arxiv.org/abs/2503.21821v1,"We introduce PHYSICS, a comprehensive benchmark for university-level physics problem solving. It contains 1297 expert-annotated problems covering six core areas: classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, atomic physics, and optics. Each problem requires advanced physics knowledge and mathematical reasoning. We develop a robust automated evaluation system for precise and reliable validation. Our evaluation of leading foundation models reveals substantial limitations. Even the most advanced model, o3-mini, achieves only 59.9% accuracy, highlighting significant challenges in solving high-level scientific problems. Through comprehensive error analysis, exploration of diverse prompting strategies, and Retrieval-Augmented Generation (RAG)-based knowledge augmentation, we identify key areas for improvement, laying the foundation for future advancements.",True,True,"Feng, Kaiyue and Zhao, Yilun and Liu, Yixin and Yang, Tianyu and Zhao, Chen and Sous, John and Cohan, Arman",2025,,,,arXiv preprint arXiv:2503.21821
LOCA-R: Near-Perfect Performance on the Chinese Physics Olympiad 2025,2511.10515v1,zhang2025abench,\cite{zhang2025abench},ABench-Physics: Benchmarking Physical Reasoning in LLMs via High-Difficulty and Dynamic Physics Problems,https://arxiv.org/abs/2507.04766v1,"Large Language Models (LLMs) have shown impressive performance in domains such as mathematics and programming, yet their capabilities in physics remain underexplored and poorly understood. Physics poses unique challenges that demand not only precise computation but also deep conceptual understanding and physical modeling skills. Existing benchmarks often fall short due to limited difficulty, multiple-choice formats, and static evaluation settings that fail to capture physical modeling ability. In this paper, we introduce ABench-Physics, a novel benchmark designed to rigorously evaluate LLMs' physical reasoning and generalization capabilities. ABench-Physics consists of two components: Phy_A, a static set of 400 graduate- or Olympiad-level problems; and Phy_B, a dynamic subset of 100 problems equipped with an automatic variation engine to test model robustness across changing conditions. All questions require precise numerical answers, with strict formatting and tolerance constraints. Our evaluation of several state-of-the-art LLMs reveals substantial performance gaps, highlighting persistent limitations in physical reasoning, especially in generalization to dynamic variants. ABench-Physics provides a challenging and diagnostic framework for advancing scientific reasoning in LLMs.",True,True,"Zhang, Yiming and Ma, Yingfan and Gu, Yanmei and Yang, Zhengkai and Zhuang, Yihong and Wang, Feng and Huang, Zenan and Wang, Yuanyuan and Huang, Chao and Song, Bowen and others",2025,,,,arXiv preprint arXiv:2507.04766
LOCA-R: Near-Perfect Performance on the Chinese Physics Olympiad 2025,2511.10515v1,zhang2025physreason,\cite{zhang2025physreason},PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning,https://arxiv.org/abs/2502.12054v2,"Large language models demonstrate remarkable capabilities across various domains, especially mathematics and logic reasoning. However, current evaluations overlook physics-based reasoning - a complex task requiring physics theorems and constraints. We present PhysReason, a 1,200-problem benchmark comprising knowledge-based (25%) and reasoning-based (75%) problems, where the latter are divided into three difficulty levels (easy, medium, hard). Notably, problems require an average of 8.1 solution steps, with hard requiring 15.6, reflecting the complexity of physics-based reasoning. We propose the Physics Solution Auto Scoring Framework, incorporating efficient answer-level and comprehensive step-level evaluations. Top-performing models like Deepseek-R1, Gemini-2.0-Flash-Thinking, and o3-mini-high achieve less than 60% on answer-level evaluation, with performance dropping from knowledge questions (75.11%) to hard problems (31.95%). Through step-level evaluation, we identified four key bottlenecks: Physics Theorem Application, Physics Process Understanding, Calculation, and Physics Condition Analysis. These findings position PhysReason as a novel and comprehensive benchmark for evaluating physics-based reasoning capabilities in large language models. Our code and data will be published at https:/dxzxy12138.github.io/PhysReason.",True,True,"Zhang, Xinyu and Dong, Yuxuan and Wu, Yanrui and Huang, Jiaxing and Jia, Chengyou and Fernando, Basura and Shou, Mike Zheng and Zhang, Lingling and Liu, Jun",2025,,,,arXiv preprint arXiv:2502.12054
LOCA-R: Near-Perfect Performance on the Chinese Physics Olympiad 2025,2511.10515v1,siddique2025physicseval,\cite{siddique2025physicseval},PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems,https://arxiv.org/abs/2508.00079v2,"The discipline of physics stands as a cornerstone of human intellect, driving the evolution of technology and deepening our understanding of the fundamental principles of the cosmos. Contemporary literature includes some works centered on the task of solving physics problems - a crucial domain of natural language reasoning. In this paper, we evaluate the performance of frontier LLMs in solving physics problems, both mathematical and descriptive. We also employ a plethora of inference-time techniques and agentic frameworks to improve the performance of the models. This includes the verification of proposed solutions in a cumulative fashion by other, smaller LLM agents, and we perform a comparative analysis of the performance that the techniques entail. There are significant improvements when the multi-agent framework is applied to problems that the models initially perform poorly on. Furthermore, we introduce a new evaluation benchmark for physics problems, ${\rm P{\small HYSICS}E{\small VAL}}$, consisting of 19,609 problems sourced from various physics textbooks and their corresponding correct solutions scraped from physics forums and educational websites. Our code and data are publicly available at https://github.com/areebuzair/PhysicsEval.",True,True,"Siddique, Oshayer and Alam, JM and Rafy, Md Jobayer Rahman and Raiyan, Syed Rifat and Mahmud, Hasan and Hasan, Md Kamrul",2025,,,,arXiv preprint arXiv:2508.00079
LOCA-R: Near-Perfect Performance on the Chinese Physics Olympiad 2025,2511.10515v1,chung2025theoretical,\cite{chung2025theoretical},Theoretical Physics Benchmark (TPBench) -- a Dataset and Study of AI Reasoning Capabilities in Theoretical Physics,https://arxiv.org/abs/2502.15815v1,"We introduce a benchmark to evaluate the capability of AI to solve problems in theoretical physics, focusing on high-energy theory and cosmology. The first iteration of our benchmark consists of 57 problems of varying difficulty, from undergraduate to research level. These problems are novel in the sense that they do not come from public problem collections. We evaluate our data set on various open and closed language models, including o3-mini, o1, DeepSeek-R1, GPT-4o and versions of Llama and Qwen. While we find impressive progress in model performance with the most recent models, our research-level difficulty problems are mostly unsolved. We address challenges of auto-verifiability and grading, and discuss common failure modes. While currently state-of-the art models are still of limited use for researchers, our results show that AI assisted theoretical physics research may become possible in the near future. We discuss the main obstacles towards this goal and possible strategies to overcome them. The public problems and solutions, results for various models, and updates to the data set and score distribution, are available on the website of the dataset tpbench.org.",True,True,"Chung, Daniel JH and Gao, Zhiqi and Kvasiuk, Yurii and Li, Tianyi and M{\""u}nchmeyer, Moritz and Rudolph, Maja and Sala, Frederic and Tadepalli, Sai Chaitanya",2025,,,,arXiv preprint arXiv:2502.15815
SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition,2511.10091v1,ye2024pose,\cite{ye2024pose},Pose-Promote: Progressive Visual Perception for Activities of Daily Living,,,True,False,"Ye, Qilang and Yu, Zitong",2024,,,,IEEE Signal Processing Letters
SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition,2511.10091v1,nan20243sg,\cite{nan20243sg},3sG: Three-stage guidance for indoor human action recognition,,,True,False,"Nan, Hai and Ye, Qilang and Yu, Zitong and An, Kang",2024,,,,IET Image Processing
SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition,2511.10091v1,rajendran2024review,\cite{rajendran2024review},Review on synergizing the Metaverse and AI-driven synthetic data: enhancing virtual realms and activity recognition in computer vision,,,True,False,"Rajendran, Megani and Tan, Chek Tien and Atmosukarto, Indriyati and Ng, Aik Beng and See, Simon",2024,,,,Visual Intelligence
SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition,2511.10091v1,li2025role,\cite{li2025role},The Role of Video Generation in Enhancing Data-Limited Action Understanding,,,True,False,"Li, Wei and Luo, Dezhao and Yang, Dongbao and Li, Zhenhang and Wang, Weiping and Zhou, Yu",2025,,,,arXiv preprint arXiv:2505.19495
SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition,2511.10091v1,st-gcn,\cite{st-gcn},"Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action
                  Recognition",,,True,False,"Sijie Yan and
                  Yuanjun Xiong and
                  Dahua Lin",2018,,,,
SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition,2511.10091v1,2sAGCN,\cite{2sAGCN},Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition,https://arxiv.org/abs/1805.07694v3,"In skeleton-based action recognition, graph convolutional networks (GCNs), which model the human body skeletons as spatiotemporal graphs, have achieved remarkable performance. However, in existing GCN-based methods, the topology of the graph is set manually, and it is fixed over all layers and input samples. This may not be optimal for the hierarchical GCN and diverse samples in action recognition tasks. In addition, the second-order information (the lengths and directions of bones) of the skeleton data, which is naturally more informative and discriminative for action recognition, is rarely investigated in existing methods. In this work, we propose a novel two-stream adaptive graph convolutional network (2s-AGCN) for skeleton-based action recognition. The topology of the graph in our model can be either uniformly or individually learned by the BP algorithm in an end-to-end manner. This data-driven method increases the flexibility of the model for graph construction and brings more generality to adapt to various data samples. Moreover, a two-stream framework is proposed to model both the first-order and the second-order information simultaneously, which shows notable improvement for the recognition accuracy. Extensive experiments on the two large-scale datasets, NTU-RGBD and Kinetics-Skeleton, demonstrate that the performance of our model exceeds the state-of-the-art with a significant margin.",True,True,"Lei Shi and
                  Yifan Zhang and
                  Jian Cheng and
                  Hanqing Lu",2019,,,,
SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition,2511.10091v1,ctrgcn,\cite{ctrgcn},Channel-wise Topology Refinement Graph Convolution for Skeleton-Based Action Recognition,https://arxiv.org/abs/2107.12213v2,"Graph convolutional networks (GCNs) have been widely used and achieved remarkable results in skeleton-based action recognition. In GCNs, graph topology dominates feature aggregation and therefore is the key to extracting representative features. In this work, we propose a novel Channel-wise Topology Refinement Graph Convolution (CTR-GC) to dynamically learn different topologies and effectively aggregate joint features in different channels for skeleton-based action recognition. The proposed CTR-GC models channel-wise topologies through learning a shared topology as a generic prior for all channels and refining it with channel-specific correlations for each channel. Our refinement method introduces few extra parameters and significantly reduces the difficulty of modeling channel-wise topologies. Furthermore, via reformulating graph convolutions into a unified form, we find that CTR-GC relaxes strict constraints of graph convolutions, leading to stronger representation capability. Combining CTR-GC with temporal modeling modules, we develop a powerful graph convolutional network named CTR-GCN which notably outperforms state-of-the-art methods on the NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets.",True,True,"Yuxin Chen and
                  Ziqi Zhang and
                  Chunfeng Yuan and
                  Bing Li and
                  Ying Deng and
                  Weiming Hu",2021,,,,
SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition,2511.10091v1,hands,\cite{hands},Human Action Recognition: Pose-based Attention draws focus to Hands,https://arxiv.org/abs/1712.08002v1,"We propose a new spatio-temporal attention based mechanism for human action recognition able to automatically attend to the hands most involved into the studied action and detect the most discriminative moments in an action. Attention is handled in a recurrent manner employing Recurrent Neural Network (RNN) and is fully-differentiable. In contrast to standard soft-attention based mechanisms, our approach does not use the hidden RNN state as input to the attention model. Instead, attention distributions are extracted using external information: human articulated pose. We performed an extensive ablation study to show the strengths of this approach and we particularly studied the conditioning aspect of the attention mechanism. We evaluate the method on the largest currently available human action recognition dataset, NTU-RGB+D, and report state-of-the-art results. Other advantages of our model are certain aspects of explanability, as the spatial and temporal attention distributions at test time allow to study and verify on which parts of the input data the method focuses.",True,True,"Baradel, Fabien and Wolf, Christian and Mille, Julien",2017,,,,
SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition,2511.10091v1,toyota,\cite{toyota},Toyota Smarthome: Real-World Activities of Daily Living,,,True,False,"Srijan Das and
                  Rui Dai and
                  Michal Koperski and
                  Luca Minciullo and
                  Lorenzo Garattoni and
                  Fran{\c{c}}ois Br{\'{e}}mond and
                  Gianpiero Francesca",2019,,,,
SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition,2511.10091v1,vpn,\cite{vpn},VPN: Learning Video-Pose Embedding for Activities of Daily Living,https://arxiv.org/abs/2007.03056v1,"In this paper, we focus on the spatio-temporal aspect of recognizing Activities of Daily Living (ADL). ADL have two specific properties (i) subtle spatio-temporal patterns and (ii) similar visual patterns varying with time. Therefore, ADL may look very similar and often necessitate to look at their fine-grained details to distinguish them. Because the recent spatio-temporal 3D ConvNets are too rigid to capture the subtle visual patterns across an action, we propose a novel Video-Pose Network: VPN. The 2 key components of this VPN are a spatial embedding and an attention network. The spatial embedding projects the 3D poses and RGB cues in a common semantic space. This enables the action recognition framework to learn better spatio-temporal features exploiting both modalities. In order to discriminate similar actions, the attention network provides two functionalities - (i) an end-to-end learnable pose backbone exploiting the topology of human body, and (ii) a coupler to provide joint spatio-temporal attention weights across a video. Experiments show that VPN outperforms the state-of-the-art results for action classification on a large scale human activity dataset: NTU-RGB+D 120, its subset NTU-RGB+D 60, a real-world challenging human activity dataset: Toyota Smarthome and a small scale human-object interaction dataset Northwestern UCLA.",True,True,"Das, Srijan and Sharma, Saurav and Dai, Rui and Bremond, Francois and Thonnat, Monique",2020,,,,
SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition,2511.10091v1,mmnet,\cite{mmnet},Mmnet: A model-based multimodal network for human action recognition in rgb-d videos,,,True,False,"Bruce, XB and Liu, Yan and Zhang, Xiang and Zhong, Sheng-hua and Chan, Keith CC",2022,,,,
SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition,2511.10091v1,vpn++,\cite{vpn++},VPN++: Rethinking Video-Pose embeddings for understanding Activities of Daily Living,https://arxiv.org/abs/2105.08141v1,"Many attempts have been made towards combining RGB and 3D poses for the recognition of Activities of Daily Living (ADL). ADL may look very similar and often necessitate to model fine-grained details to distinguish them. Because the recent 3D ConvNets are too rigid to capture the subtle visual patterns across an action, this research direction is dominated by methods combining RGB and 3D Poses. But the cost of computing 3D poses from RGB stream is high in the absence of appropriate sensors. This limits the usage of aforementioned approaches in real-world applications requiring low latency. Then, how to best take advantage of 3D Poses for recognizing ADL? To this end, we propose an extension of a pose driven attention mechanism: Video-Pose Network (VPN), exploring two distinct directions. One is to transfer the Pose knowledge into RGB through a feature-level distillation and the other towards mimicking pose driven attention through an attention-level distillation. Finally, these two approaches are integrated into a single model, we call VPN++. We show that VPN++ is not only effective but also provides a high speed up and high resilience to noisy Poses. VPN++, with or without 3D Poses, outperforms the representative baselines on 4 public datasets. Code is available at https://github.com/srijandas07/vpnplusplus.",True,True,"Das, Srijan and Dai, Rui and Yang, Di and Bremond, Francois",2021,,,,
SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition,2511.10091v1,clip,\cite{clip},Learning Transferable Visual Models From Natural Language Supervision,https://arxiv.org/abs/2103.00020v1,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",True,True,"Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",2021,,,,
SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition,2511.10091v1,blip,\cite{blip},Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation,,,True,False,"Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven",2022,,,,
SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition,2511.10091v1,align,\cite{align},Scaling up visual and vision-language representation learning with noisy text supervision,,,True,False,"Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom",2021,,,,
SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition,2511.10091v1,actionclip,\cite{actionclip},Actionclip: A new paradigm for video action recognition,,,True,False,"Wang, Mengmeng and Xing, Jiazheng and Liu, Yong",2021,,,,arXiv preprint arXiv:2109.08472
SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition,2511.10091v1,xie2024fusionmamba,\cite{xie2024fusionmamba},Fusionmamba: Dynamic feature enhancement for multimodal image fusion with mamba,,,True,False,"Xie, Xinyu and Cui, Yawen and Tan, Tao and Zheng, Xubin and Yu, Zitong",2024,,,,Visual Intelligence
SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition,2511.10091v1,lin2025reliable,\cite{lin2025reliable},Reliable and Balanced Transfer Learning for Generalized Multimodal Face Anti-Spoofing,,,True,False,"Lin, Xun and Liu, Ajian and Yu, Zitong and Cai, Rizhao and Wang, Shuai and Yu, Yi and Wan, Jun and Lei, Zhen and Cao, Xiaochun and Kot, Alex",2025,,,,IEEE Transactions on Pattern Analysis and Machine Intelligence
SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition,2511.10091v1,CAT,\cite{CAT},CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios,https://arxiv.org/abs/2403.04640v1,"This paper focuses on the challenge of answering questions in scenarios that are composed of rich and complex dynamic audio-visual components. Although existing Multimodal Large Language Models (MLLMs) can respond to audio-visual content, these responses are sometimes ambiguous and fail to describe specific audio-visual events. To overcome this limitation, we introduce the CAT, which enhances MLLM in three ways: 1) besides straightforwardly bridging audio and video, we design a clue aggregator that aggregates question-related clues in dynamic audio-visual scenarios to enrich the detailed knowledge required for large language models. 2) CAT is trained on a mixed multimodal dataset, allowing direct application in audio-visual scenarios. Notably, we collect an audio-visual joint instruction dataset named AVinstruct, to further enhance the capacity of CAT to model cross-semantic correlations. 3) we propose AI-assisted ambiguity-aware direct preference optimization, a strategy specialized in retraining the model to favor the non-ambiguity response and improve the ability to localize specific audio-visual objects. Extensive experimental results demonstrate that CAT outperforms existing methods on multimodal tasks, especially in Audio-Visual Question Answering (AVQA) tasks. The codes and the collected instructions are released at https://github.com/rikeilong/Bay-CAT.",True,True,"Ye, Qilang and Yu, Zitong and Shao, Rui and Xie, Xinyu and Torr, Philip and Cao, Xiaochun",2024,,,,
SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition,2511.10091v1,ye2025cat+,\cite{ye2025cat+},Cat+: Investigating and enhancing audio-visual understanding in large language models,,,True,False,"Ye, Qilang and Yu, Zitong and Shao, Rui and Cui, Yawen and Kang, Xiangui and Liu, Xin and Torr, Philip and Cao, Xiaochun",2025,,,,IEEE Transactions on Pattern Analysis and Machine Intelligence
SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition,2511.10091v1,videochat,\cite{videochat},VideoChat: Chat-Centric Video Understanding,https://arxiv.org/abs/2305.06355v2,"In this paper, we initiate an attempt of developing an end-to-end chat-centric video understanding system, coined as VideoChat. It integrates video foundation models and large language models via a learnable neural interface, excelling in spatiotemporal reasoning, event localization, and causal relationship inference. To instructively tune this system, we build a video-centric instruction dataset, composed of thousands of videos associated with detailed descriptions and conversations. This dataset emphasizes spatiotemporal reasoning and captures causal relationships, providing a valuable asset for training our chat-centric video understanding system. Preliminary qualitative experiments demonstrate the potential of our system across a broad spectrum of video applications, which could serve as a simple prototype system for future research on chat-centric video understanding. Access our code and data at https://github.com/OpenGVLab/Ask-Anything",True,True,"Kunchang Li and
                  Yinan He and
                  Yi Wang and
                  Yizhuo Li and
                  Wenhai Wang and
                  Ping Luo and
                  Yali Wang and
                  Limin Wang and
                  Yu Qiao",2023,,,,CoRR
SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition,2511.10091v1,promptlearning1,\cite{promptlearning1},Learning to prompt for vision-language models,,,True,False,"Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei",2022,,,,
SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition,2511.10091v1,actionllm,\cite{actionllm},Llms are good action recognizers,,,True,False,"Qu, Haoxuan and Cai, Yujun and Liu, Jun",2024,,,,
SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition,2511.10091v1,vqvae,\cite{vqvae},Neural Discrete Representation Learning,https://arxiv.org/abs/1711.00937v2,"Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of ""posterior collapse"" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.",True,True,"Van Den Oord, Aaron and Vinyals, Oriol and others",2017,,,,Advances in neural information processing systems
SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition,2511.10091v1,lora,\cite{lora},LoRA+: Efficient Low Rank Adaptation of Large Models,https://arxiv.org/abs/2402.12354v2,"In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$ improves performance (1-2 $\%$ improvements) and finetuning speed (up to $\sim$ 2X SpeedUp), at the same computational cost as LoRA.",True,True,"Edward J. Hu and
                  Yelong Shen and
                  Phillip Wallis and
                  Zeyuan Allen{-}Zhu and
                  Yuanzhi Li and
                  Shean Wang and
                  Lu Wang and
                  Weizhu Chen",2022,,,,
Distribution-Based Feature Attribution for Explaining the Predictions of Any Classifier,2511.09332v1,ribeiro2016should,\cite{ribeiro2016should},``{W}hy should {I} trust you?'' {E}xplaining the predictions of any classifier,,,True,False,"Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos",2016,,,,
Distribution-Based Feature Attribution for Explaining the Predictions of Any Classifier,2511.09332v1,zafar2021deterministic,\cite{zafar2021deterministic},Deterministic Local Interpretable Model-Agnostic Explanations for Stable Explainability,,,True,False,"Zafar, Muhammad Rehman and Khan, Naimul",2021,,,,Machine Learning and Knowledge Extraction
Distribution-Based Feature Attribution for Explaining the Predictions of Any Classifier,2511.09332v1,plumb2018model,\cite{plumb2018model},Model Agnostic Supervised Local Explanations,https://arxiv.org/abs/1807.02910v3,"Model interpretability is an increasingly important component of practical machine learning. Some of the most common forms of interpretability systems are example-based, local, and global explanations. One of the main challenges in interpretability is designing explanation systems that can capture aspects of each of these explanation types, in order to develop a more thorough understanding of the model. We address this challenge in a novel model called MAPLE that uses local linear modeling techniques along with a dual interpretation of random forests (both as a supervised neighborhood approach and as a feature selection method). MAPLE has two fundamental advantages over existing interpretability systems. First, while it is effective as a black-box explanation system, MAPLE itself is a highly accurate predictive model that provides faithful self explanations, and thus sidesteps the typical accuracy-interpretability trade-off. Specifically, we demonstrate, on several UCI datasets, that MAPLE is at least as accurate as random forests and that it produces more faithful local explanations than LIME, a popular interpretability system. Second, MAPLE provides both example-based and local explanations and can detect global patterns, which allows it to diagnose limitations in its local explanations.",True,True,"Plumb, Gregory and Molitor, Denali and Talwalkar, Ameet S",2018,,,,Advances in Neural Information Processing Systems
Distribution-Based Feature Attribution for Explaining the Predictions of Any Classifier,2511.09332v1,bjorklund2023explaining,\cite{bjorklund2023explaining},Explaining any black box model using real data,,,True,False,"Bj{\""o}rklund, Anton and Henelius, Andreas and Oikarinen, Emilia and Kallonen, Kimmo and Puolam{\""a}ki, Kai",2023,,,,Frontiers in Computer Science
Distribution-Based Feature Attribution for Explaining the Predictions of Any Classifier,2511.09332v1,dhurandhar2023locally,\cite{dhurandhar2023locally},Locally Invariant Explanations: Towards Stable and Unidirectional Explanations through Local Invariant Learning,https://arxiv.org/abs/2201.12143v2,"Locally interpretable model agnostic explanations (LIME) method is one of the most popular methods used to explain black-box models at a per example level. Although many variants have been proposed, few provide a simple way to produce high fidelity explanations that are also stable and intuitive. In this work, we provide a novel perspective by proposing a model agnostic local explanation method inspired by the invariant risk minimization (IRM) principle -- originally proposed for (global) out-of-distribution generalization -- to provide such high fidelity explanations that are also stable and unidirectional across nearby examples. Our method is based on a game theoretic formulation where we theoretically show that our approach has a strong tendency to eliminate features where the gradient of the black-box function abruptly changes sign in the locality of the example we want to explain, while in other cases it is more careful and will choose a more conservative (feature) attribution, a behavior which can be highly desirable for recourse. Empirically, we show on tabular, image and text data that the quality of our explanations with neighborhoods formed using random perturbations are much better than LIME and in some cases even comparable to other methods that use realistic neighbors sampled from the data manifold. This is desirable given that learning a manifold to either create realistic neighbors or to project explanations is typically expensive or may even be impossible. Moreover, our algorithm is simple and efficient to train, and can ascertain stable input features for local decisions of a black-box without access to side information such as a (partial) causal graph as has been seen in some recent works.",True,True,"Dhurandhar, Amit and Natesan Ramamurthy, Karthikeyan and Ahuja, Kartik and Arya, Vijay",2023,,,,Advances in Neural Information Processing Systems
Distribution-Based Feature Attribution for Explaining the Predictions of Any Classifier,2511.09332v1,lundberg2017unified,\cite{lundberg2017unified},A Unified Approach to Interpreting Model Predictions,https://arxiv.org/abs/1705.07874v2,"Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.",True,True,"Lundberg, Scott M and Lee, Su-In",2017,,,,Advances in Neural Information Processing Systems
Distribution-Based Feature Attribution for Explaining the Predictions of Any Classifier,2511.09332v1,shapley1953value,\cite{shapley1953value},A Value for n-Person Games,,,True,False,"Shapley, Lloyd S",1953,,,,
Distribution-Based Feature Attribution for Explaining the Predictions of Any Classifier,2511.09332v1,lipovetsky2001analysis,\cite{lipovetsky2001analysis},Analysis of regression in game theory approach,,,True,False,"Lipovetsky, Stan and Conklin, Michael",2001,,,,Applied stochastic models in business and industry
Distribution-Based Feature Attribution for Explaining the Predictions of Any Classifier,2511.09332v1,strumbelj2014explaining,\cite{strumbelj2014explaining},Explaining prediction models and individual predictions with feature contributions,,,True,False,"Strumbelj, Erik and Kononenko, Igor",2014,,,,Knowledge and information systems
Distribution-Based Feature Attribution for Explaining the Predictions of Any Classifier,2511.09332v1,fisher2019all,\cite{fisher2019all},"All models are wrong, but many are useful: Learning a variable's importance by studying an entire class of prediction models simultaneously",,,True,False,"Fisher, Aaron and Rudin, Cynthia and Dominici, Francesca",2019,,,,Journal of Machine Learning Research
Distribution-Based Feature Attribution for Explaining the Predictions of Any Classifier,2511.09332v1,breiman2001random,\cite{breiman2001random},Random forests,,,True,False,"Breiman, Leo",2001,,,,Machine learning
Distribution-Based Feature Attribution for Explaining the Predictions of Any Classifier,2511.09332v1,rosenblatt1956remarks,\cite{rosenblatt1956remarks},Remarks on Some Nonparametric Estimates of a Density Function,,,True,False,"Rosenblatt, Murray",1956,,,,The Annals of Mathematical Statistics
Distribution-Based Feature Attribution for Explaining the Predictions of Any Classifier,2511.09332v1,ting2021isolation,\cite{ting2021isolation},Isolation kernel density estimation,,,True,False,"Ting, Kai Ming and Washio, Takashi and Wells, Jonathan R and Zhang, Hang",2021,,,,
Distribution-Based Feature Attribution for Explaining the Predictions of Any Classifier,2511.09332v1,ting2018isolation,\cite{ting2018isolation},Isolation kernel and its effect on SVM,,,True,False,"Ting, Kai Ming and Zhu, Yue and Zhou, Zhi-Hua",2018,,,,
Distribution-Based Feature Attribution for Explaining the Predictions of Any Classifier,2511.09332v1,wells2019new,\cite{wells2019new},A new simple and efficient density estimator that enables fast systematic search,,,True,False,"Wells, Jonathan R and Ting, Kai Ming",2019,,,,Pattern Recognition Letters
Distribution-Based Feature Attribution for Explaining the Predictions of Any Classifier,2511.09332v1,samariya2020new,\cite{samariya2020new},A new effective and efficient measure for outlying aspect mining,,,True,False,"Samariya, Durgesh and Aryal, Sunil and Ting, Kai Ming and Ma, Jiangang",2020,,,,
Distribution-Based Feature Attribution for Explaining the Predictions of Any Classifier,2511.09332v1,bandaragoda2014efficient,\cite{bandaragoda2014efficient},Efficient anomaly detection by isolation using nearest neighbour ensemble,,,True,False,"Bandaragoda, Tharindu R and Ting, Kai Ming and Albrecht, David and Liu, Fei Tony and Wells, Jonathan R",2014,,,,
Distribution-Based Feature Attribution for Explaining the Predictions of Any Classifier,2511.09332v1,williams2000using,\cite{williams2000using},"Using the Nystr{\""o}m method to speed up kernel machines",,,True,False,"Williams, Christopher and Seeger, Matthias",2000,,,,Advances in Neural Information Processing Systems
Distribution-Based Feature Attribution for Explaining the Predictions of Any Classifier,2511.09332v1,muandet2017kernel,\cite{muandet2017kernel},Kernel Mean Embedding of Distributions: A Review and Beyond,https://arxiv.org/abs/1605.09522v4,"A Hilbert space embedding of a distribution---in short, a kernel mean embedding---has recently emerged as a powerful tool for machine learning and inference. The basic idea behind this framework is to map distributions into a reproducing kernel Hilbert space (RKHS) in which the whole arsenal of kernel methods can be extended to probability measures. It can be viewed as a generalization of the original ""feature map"" common to support vector machines (SVMs) and other kernel methods. While initially closely associated with the latter, it has meanwhile found application in fields ranging from kernel machines and probabilistic modeling to statistical inference, causal discovery, and deep learning. The goal of this survey is to give a comprehensive review of existing work and recent advances in this research area, and to discuss the most challenging issues and open problems that could lead to new research directions. The survey begins with a brief introduction to the RKHS and positive definite kernels which forms the backbone of this survey, followed by a thorough discussion of the Hilbert space embedding of marginal distributions, theoretical guarantees, and a review of its applications. The embedding of distributions enables us to apply RKHS methods to probability measures which prompts a wide range of applications such as kernel two-sample testing, independent testing, and learning on distributional data. Next, we discuss the Hilbert space embedding for conditional distributions, give theoretical insights, and review some applications. The conditional mean embedding enables us to perform sum, product, and Bayes' rules---which are ubiquitous in graphical model, probabilistic inference, and reinforcement learning---in a non-parametric way. We then discuss relationships between this framework and other related areas. Lastly, we give some suggestions on future research directions.",True,True,"Muandet, Krikamol and Fukumizu, Kenji and Sriperumbudur, Bharath and Sch{\""o}lkopf, Bernhard and others",2017,,,,Foundations and Trends{\textregistered} in Machine Learning
FLAD: Federated Learning for LLM-based Autonomous Driving in Vehicle-Edge-Cloud Networks,2511.09025v1,jia2023driveadapter,\cite{jia2023driveadapter},Driveadapter: Breaking the coupling barrier of perception and planning in end-to-end autonomous driving,,,True,False,"Jia, Xiaosong and Gao, Yulu and Chen, Li and Yan, Junchi and Liu, Patrick Langechuan and Li, Hongyang",2023,,,,
FLAD: Federated Learning for LLM-based Autonomous Driving in Vehicle-Edge-Cloud Networks,2511.09025v1,sima2023drivelm,\cite{sima2023drivelm},Drivelm: Driving with graph visual question answering,,,True,False,"Sima, Chonghao and Renz, Katrin and Chitta, Kashyap and Chen, Li and Zhang, Hanxue and Xie, Chengen and Luo, Ping and Geiger, Andreas and Li, Hongyang",2023,,,,arXiv preprint arXiv:2312.14150
FLAD: Federated Learning for LLM-based Autonomous Driving in Vehicle-Edge-Cloud Networks,2511.09025v1,choudhary2023talk2bev,\cite{choudhary2023talk2bev},Talk2BEV: Language-enhanced Bird's-eye View Maps for Autonomous Driving,https://arxiv.org/abs/2310.02251v2,"Talk2BEV is a large vision-language model (LVLM) interface for bird's-eye view (BEV) maps in autonomous driving contexts. While existing perception systems for autonomous driving scenarios have largely focused on a pre-defined (closed) set of object categories and driving scenarios, Talk2BEV blends recent advances in general-purpose language and vision models with BEV-structured map representations, eliminating the need for task-specific models. This enables a single system to cater to a variety of autonomous driving tasks encompassing visual and spatial reasoning, predicting the intents of traffic actors, and decision-making based on visual cues. We extensively evaluate Talk2BEV on a large number of scene understanding tasks that rely on both the ability to interpret free-form natural language queries, and in grounding these queries to the visual context embedded into the language-enhanced BEV map. To enable further research in LVLMs for autonomous driving scenarios, we develop and release Talk2BEV-Bench, a benchmark encompassing 1000 human-annotated BEV scenarios, with more than 20,000 questions and ground-truth responses from the NuScenes dataset.",True,True,"Choudhary, Tushar and Dewangan, Vikrant and Chandhok, Shivam and Priyadarshan, Shubham and Jain, Anushka and Singh, Arun K and Srivastava, Siddharth and Jatavallabhula, Krishna Murthy and Krishna, K Madhava",2023,,,,arXiv preprint arXiv:2310.02251
FLAD: Federated Learning for LLM-based Autonomous Driving in Vehicle-Edge-Cloud Networks,2511.09025v1,shao2024lmdrive,\cite{shao2024lmdrive},LMDrive: Closed-Loop End-to-End Driving with Large Language Models,https://arxiv.org/abs/2312.07488v2,"Despite significant recent progress in the field of autonomous driving, modern methods still struggle and can incur serious accidents when encountering long-tail unforeseen events and challenging urban scenarios. On the one hand, large language models (LLM) have shown impressive reasoning capabilities that approach ""Artificial General Intelligence"". On the other hand, previous autonomous driving methods tend to rely on limited-format inputs (e.g. sensor data and navigation waypoints), restricting the vehicle's ability to understand language information and interact with humans. To this end, this paper introduces LMDrive, a novel language-guided, end-to-end, closed-loop autonomous driving framework. LMDrive uniquely processes and integrates multi-modal sensor data with natural language instructions, enabling interaction with humans and navigation software in realistic instructional settings. To facilitate further research in language-based closed-loop autonomous driving, we also publicly release the corresponding dataset which includes approximately 64K instruction-following data clips, and the LangAuto benchmark that tests the system's ability to handle complex instructions and challenging driving scenarios. Extensive closed-loop experiments are conducted to demonstrate LMDrive's effectiveness. To the best of our knowledge, we're the very first work to leverage LLMs for closed-loop end-to-end autonomous driving. Codes, models, and datasets can be found at https://github.com/opendilab/LMDrive",True,True,"Shao, Hao and Hu, Yuxuan and Wang, Letian and Song, Guanglu and Waslander, Steven L and Liu, Yu and Li, Hongsheng",2024,,,,
FLAD: Federated Learning for LLM-based Autonomous Driving in Vehicle-Edge-Cloud Networks,2511.09025v1,wang2024drivedreamer,\cite{wang2024drivedreamer},DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving,https://arxiv.org/abs/2309.09777v2,"World models, especially in autonomous driving, are trending and drawing extensive attention due to their capacity for comprehending driving environments. The established world model holds immense potential for the generation of high-quality driving videos, and driving policies for safe maneuvering. However, a critical limitation in relevant research lies in its predominant focus on gaming environments or simulated settings, thereby lacking the representation of real-world driving scenarios. Therefore, we introduce DriveDreamer, a pioneering world model entirely derived from real-world driving scenarios. Regarding that modeling the world in intricate driving scenes entails an overwhelming search space, we propose harnessing the powerful diffusion model to construct a comprehensive representation of the complex environment. Furthermore, we introduce a two-stage training pipeline. In the initial phase, DriveDreamer acquires a deep understanding of structured traffic constraints, while the subsequent stage equips it with the ability to anticipate future states. The proposed DriveDreamer is the first world model established from real-world driving scenarios. We instantiate DriveDreamer on the challenging nuScenes benchmark, and extensive experiments verify that DriveDreamer empowers precise, controllable video generation that faithfully captures the structural constraints of real-world traffic scenarios. Additionally, DriveDreamer enables the generation of realistic and reasonable driving policies, opening avenues for interaction and practical applications.",True,True,"Wang, Xiaofeng and Zhu, Zheng and Huang, Guan and Chen, Xinze and Zhu, Jiagang and Lu, Jiwen",2024,,,,
FLAD: Federated Learning for LLM-based Autonomous Driving in Vehicle-Edge-Cloud Networks,2511.09025v1,hu2023planning,\cite{hu2023planning},Planning-oriented Autonomous Driving,https://arxiv.org/abs/2212.10156v2,"Modern autonomous driving system is characterized as modular tasks in sequential order, i.e., perception, prediction, and planning. In order to perform a wide diversity of tasks and achieve advanced-level intelligence, contemporary approaches either deploy standalone models for individual tasks, or design a multi-task paradigm with separate heads. However, they might suffer from accumulative errors or deficient task coordination. Instead, we argue that a favorable framework should be devised and optimized in pursuit of the ultimate goal, i.e., planning of the self-driving car. Oriented at this, we revisit the key components within perception and prediction, and prioritize the tasks such that all these tasks contribute to planning. We introduce Unified Autonomous Driving (UniAD), a comprehensive framework up-to-date that incorporates full-stack driving tasks in one network. It is exquisitely devised to leverage advantages of each module, and provide complementary feature abstractions for agent interaction from a global perspective. Tasks are communicated with unified query interfaces to facilitate each other toward planning. We instantiate UniAD on the challenging nuScenes benchmark. With extensive ablations, the effectiveness of using such a philosophy is proven by substantially outperforming previous state-of-the-arts in all aspects. Code and models are public.",True,True,"Hu, Yihan and Yang, Jiazhi and Chen, Li and Li, Keyu and Sima, Chonghao and Zhu, Xizhou and Chai, Siqi and Du, Senyao and Lin, Tianwei and Wang, Wenhai and others",2023,,,,
FLAD: Federated Learning for LLM-based Autonomous Driving in Vehicle-Edge-Cloud Networks,2511.09025v1,rajbhandari2020zero,\cite{rajbhandari2020zero},ZeRO: Memory Optimizations Toward Training Trillion Parameter Models,https://arxiv.org/abs/1910.02054v3,"Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today's hardware.
  We implement and evaluate ZeRO: it trains large models of over 100B parameter with super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops. This represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability, ZeRO can train large models of up to 13B parameters (e.g., larger than Megatron GPT 8.3B and T5 11B) without requiring model parallelism which is harder for scientists to apply. Last but not the least, researchers have used the system breakthroughs of ZeRO to create the world's largest language model (Turing-NLG, 17B parameters) with record breaking accuracy.",True,True,"Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong",2020,,,,
FLAD: Federated Learning for LLM-based Autonomous Driving in Vehicle-Edge-Cloud Networks,2511.09025v1,shoeybi2019megatron,\cite{shoeybi2019megatron},Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,https://arxiv.org/abs/1909.08053v4,"Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).",True,True,"Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan",2019,,,,arXiv preprint arXiv:1909.08053
FLAD: Federated Learning for LLM-based Autonomous Driving in Vehicle-Edge-Cloud Networks,2511.09025v1,huang2019gpipe,\cite{huang2019gpipe},GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,https://arxiv.org/abs/1811.06965v5,"Scaling up deep neural network capacity has been known as an effective approach to improving model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific and do not transfer to other tasks. To address the need for efficient and task-independent model parallelism, we introduce GPipe, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of scaling a variety of different networks to gigantic sizes efficiently. Moreover, GPipe utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a model is partitioned across multiple accelerators. We demonstrate the advantages of GPipe by training large-scale neural networks on two different tasks with distinct network architectures: (i) Image Classification: We train a 557-million-parameter AmoebaNet model and attain a top-1 accuracy of 84.4% on ImageNet-2012, (ii) Multilingual Neural Machine Translation: We train a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.",True,True,"Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and others",2019,,,,Advances in neural information processing systems
FLAD: Federated Learning for LLM-based Autonomous Driving in Vehicle-Edge-Cloud Networks,2511.09025v1,fan2021dapple,\cite{fan2021dapple},DAPPLE: A Pipelined Data Parallel Approach for Training Large Models,https://arxiv.org/abs/2007.01045v1,"It is a challenging task to train large DNN models on sophisticated GPU platforms with diversified interconnect capabilities. Recently, pipelined training has been proposed as an effective approach for improving device utilization. However, there are still several tricky issues to address: improving computing efficiency while ensuring convergence, and reducing memory usage without incurring additional computing costs. We propose DAPPLE, a synchronous training framework which combines data parallelism and pipeline parallelism for large DNN models. It features a novel parallelization strategy planner to solve the partition and placement problems, and explores the optimal hybrid strategy of data and pipeline parallelism. We also propose a new runtime scheduling algorithm to reduce device memory usage, which is orthogonal to re-computation approach and does not come at the expense of training throughput. Experiments show that DAPPLE planner consistently outperforms strategies generated by PipeDream's planner by up to 3.23x under synchronous training scenarios, and DAPPLE runtime outperforms GPipe by 1.6x speedup of training throughput and reduces the memory consumption of 12% at the same time.",True,True,"Fan, Shiqing and Rong, Yi and Meng, Chen and Cao, Zongyan and Wang, Siyu and Zheng, Zhen and Wu, Chuan and Long, Guoping and Yang, Jun and Xia, Lixue and others",2021,,,,
FLAD: Federated Learning for LLM-based Autonomous Driving in Vehicle-Edge-Cloud Networks,2511.09025v1,lin2024nnscaler,\cite{lin2024nnscaler},{nnScaler}: {Constraint-Guided} Parallelization Plan Generation for Deep Learning Training,,,True,False,"Lin, Zhiqi and Miao, Youshan and Zhang, Quanlu and Yang, Fan and Zhu, Yi and Li, Cheng and Maleki, Saeed and Cao, Xu and Shang, Ning and Yang, Yilei and others",2024,,,,
FLAD: Federated Learning for LLM-based Autonomous Driving in Vehicle-Edge-Cloud Networks,2511.09025v1,jang2023oobleck,\cite{jang2023oobleck},Oobleck: Resilient Distributed Training of Large Models Using Pipeline Templates,https://arxiv.org/abs/2309.08125v2,"Oobleck enables resilient distributed training of large DNN models with guaranteed fault tolerance. It takes a planning-execution co-design approach, where it first generates a set of heterogeneous pipeline templates and instantiates at least $f+1$ logically equivalent pipeline replicas to tolerate any $f$ simultaneous failures. During execution, it relies on already-replicated model states across the replicas to provide fast recovery. Oobleck provably guarantees that some combination of the initially created pipeline templates can be used to cover all available resources after $f$ or fewer simultaneous failures, thereby avoiding resource idling at all times. Evaluation on large DNN models with billions of parameters shows that Oobleck provides consistently high throughput, and it outperforms state-of-the-art fault tolerance solutions like Bamboo and Varuna by up to $29.6x$.",True,True,"Jang, Insu and Yang, Zhenning and Zhang, Zhen and Jin, Xin and Chowdhury, Mosharaf",2023,,,,
FLAD: Federated Learning for LLM-based Autonomous Driving in Vehicle-Edge-Cloud Networks,2511.09025v1,ye2024asteroid,\cite{ye2024asteroid},Asteroid: Resource-Efficient Hybrid Pipeline Parallelism for Collaborative DNN Training on Heterogeneous Edge Devices,https://arxiv.org/abs/2408.08015v1,"On-device Deep Neural Network (DNN) training has been recognized as crucial for privacy-preserving machine learning at the edge. However, the intensive training workload and limited onboard computing resources pose significant challenges to the availability and efficiency of model training. While existing works address these challenges through native resource management optimization, we instead leverage our observation that edge environments usually comprise a rich set of accompanying trusted edge devices with idle resources beyond a single terminal. We propose Asteroid, a distributed edge training system that breaks the resource walls across heterogeneous edge devices for efficient model training acceleration. Asteroid adopts a hybrid pipeline parallelism to orchestrate distributed training, along with a judicious parallelism planning for maximizing throughput under certain resource constraints. Furthermore, a fault-tolerant yet lightweight pipeline replay mechanism is developed to tame the device-level dynamics for training robustness and performance stability. We implement Asteroid on heterogeneous edge devices with both vision and language models, demonstrating up to 12.2x faster training than conventional parallelism methods and 2.1x faster than state-of-the-art hybrid parallelism methods through evaluations. Furthermore, Asteroid can recover training pipeline 14x faster than baseline methods while preserving comparable throughput despite unexpected device exiting and failure.",True,True,"Ye, Shengyuan and Zeng, Liekang and Chu, Xiaowen and Xing, Guoliang and Chen, Xu",2024,,,,
FLAD: Federated Learning for LLM-based Autonomous Driving in Vehicle-Edge-Cloud Networks,2511.09025v1,yoon2021edgepipe,\cite{yoon2021edgepipe},Edgepipe: Tailoring pipeline parallelism with deep neural networks for volatile wireless edge devices,,,True,False,"Yoon, JinYi and Byeon, Yeongsin and Kim, Jeewoon and Lee, HyungJune",2021,,,,IEEE Internet of Things Journal
FLAD: Federated Learning for LLM-based Autonomous Driving in Vehicle-Edge-Cloud Networks,2511.09025v1,lim2024accelerating,\cite{lim2024accelerating},Accelerating model training in multi-cluster environments with consumer-grade gpus,,,True,False,"Lim, Hwijoon and Ye, Juncheol and Abdu Jyothi, Sangeetha and Han, Dongsu",2024,,,,
FLAD: Federated Learning for LLM-based Autonomous Driving in Vehicle-Edge-Cloud Networks,2511.09025v1,li2024flexnn,\cite{li2024flexnn},Flexnn: Efficient and adaptive dnn inference on memory-constrained edge devices,,,True,False,"Li, Xiangyu and Li, Yuanchun and Li, Yuanzhe and Cao, Ting and Liu, Yunxin",2024,,,,
FLAD: Federated Learning for LLM-based Autonomous Driving in Vehicle-Edge-Cloud Networks,2511.09025v1,zheng2023autofed,\cite{zheng2023autofed},AutoFed: Heterogeneity-Aware Federated Multimodal Learning for Robust Autonomous Driving,https://arxiv.org/abs/2302.08646v3,"Object detection with on-board sensors (e.g., lidar, radar, and camera) play a crucial role in autonomous driving (AD), and these sensors complement each other in modalities. While crowdsensing may potentially exploit these sensors (of huge quantity) to derive more comprehensive knowledge, \textit{federated learning} (FL) appears to be the necessary tool to reach this potential: it enables autonomous vehicles (AVs) to train machine learning models without explicitly sharing raw sensory data. However, the multimodal sensors introduce various data heterogeneity across distributed AVs (e.g., label quantity skews and varied modalities), posing critical challenges to effective FL. To this end, we present AutoFed as a heterogeneity-aware FL framework to fully exploit multimodal sensory data on AVs and thus enable robust AD. Specifically, we first propose a novel model leveraging pseudo-labeling to avoid mistakenly treating unlabeled objects as the background. We also propose an autoencoder-based data imputation method to fill missing data modality (of certain AVs) with the available ones. To further reconcile the heterogeneity, we finally present a client selection mechanism exploiting the similarities among client models to improve both training stability and convergence rate. Our experiments on benchmark dataset confirm that AutoFed substantially improves over status quo approaches in both precision and recall, while demonstrating strong robustness to adverse weather conditions.",True,True,"Zheng, Tianyue and Li, Ang and Chen, Zhe and Wang, Hongbo and Luo, Jun",2023,,,,
FLAD: Federated Learning for LLM-based Autonomous Driving in Vehicle-Edge-Cloud Networks,2511.09025v1,zheng2022alpa,\cite{zheng2022alpa},Alpa: Automating inter-and {Intra-Operator} parallelism for distributed deep learning,,,True,False,"Zheng, Lianmin and Li, Zhuohan and Zhang, Hao and Zhuang, Yonghao and Chen, Zhifeng and Huang, Yanping and Wang, Yida and Xu, Yuanzhong and Zhuo, Danyang and Xing, Eric P and others",2022,,,,
Modeling Uncertainty Trends for Timely Retrieval in Dynamic RAG,2511.09980v1,fan2024survey,\cite{fan2024survey},A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models,https://arxiv.org/abs/2405.06211v3,"As one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations, such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the generation quality of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: architectures, training strategies, and applications. As the preliminary knowledge, we briefly introduce the foundations and recent advances of LLMs. Then, to illustrate the practical significance of RAG for LLMs, we systematically review mainstream relevant work by their architectures, training strategies, and application areas, detailing specifically the challenges of each and the corresponding capabilities of RA-LLMs. Finally, to deliver deeper insights, we discuss current limitations and several promising directions for future research. Updated information about this survey can be found at https://advanced-recommender-systems.github.io/RAG-Meets-LLMs/",True,True,"Fan, Wenqi and Ding, Yujuan and Ning, Liangbo and Wang, Shijie and Li, Hengyun and Yin, Dawei and Chua, Tat-Seng and Li, Qing",2024,,,,
Modeling Uncertainty Trends for Timely Retrieval in Dynamic RAG,2511.09980v1,Yoran2023MakingRL,\cite{Yoran2023MakingRL},Making Retrieval-Augmented Language Models Robust to Irrelevant Context,https://arxiv.org/abs/2310.01558v2,"Retrieval-augmented language models (RALMs) hold promise to produce language understanding systems that are are factual, efficient, and up-to-date. An important desideratum of RALMs, is that retrieved information helps model performance when it is relevant, and does not harm performance when it is not. This is particularly important in multi-hop reasoning scenarios, where misuse of irrelevant evidence can lead to cascading errors. However, recent work has shown that retrieval augmentation can sometimes have a negative effect on performance. In this work, we present a thorough analysis on five open-domain question answering benchmarks, characterizing cases when retrieval reduces accuracy. We then propose two methods to mitigate this issue. First, a simple baseline that filters out retrieved passages that do not entail question-answer pairs according to a natural language inference (NLI) model. This is effective in preventing performance reduction, but at a cost of also discarding relevant passages. Thus, we propose a method for automatically generating data to fine-tune the language model to properly leverage retrieved passages, using a mix of relevant and irrelevant contexts at training time. We empirically show that even 1,000 examples suffice to train the model to be robust to irrelevant contexts while maintaining high performance on examples with relevant ones.",True,True,Ori Yoran and Tomer Wolfson and Ori Ram and Jonathan Berant,2023,,https://api.semanticscholar.org/CorpusID:263608822,,ArXiv
Modeling Uncertainty Trends for Timely Retrieval in Dynamic RAG,2511.09980v1,Luo2024LandmarkEA,\cite{Luo2024LandmarkEA},BGE Landmark Embedding: A Chunking-Free Embedding Method For Retrieval Augmented Long-Context Large Language Models,https://arxiv.org/abs/2402.11573v1,"Large language models (LLMs) call for extension of context to handle many critical applications. However, the existing approaches are prone to expensive costs and inferior quality of context extension. In this work, we proposeExtensible Embedding, which realizes high-quality extension of LLM's context with strong flexibility and cost-effectiveness. Extensible embedding stand as an enhancement of typical token embedding, which represents the information for an extensible scope of context instead of a single token. By leveraging such compact input units of higher information density, the LLM can access to a vast scope of context even with a small context window. Extensible embedding is systematically optimized in architecture and training method, which leads to multiple advantages. 1) High flexibility of context extension, which flexibly supports ad-hoc extension of diverse context lengths. 2) Strong sample efficiency of training, which enables the embedding model to be learned in a cost-effective way. 3) Superior compatibility with the existing LLMs, where the extensible embedding can be seamlessly introduced as a plug-in component. Comprehensive evaluations on long-context language modeling and understanding tasks verify extensible embedding as an effective, efficient, flexible, and compatible method to extend the LLM's context.",True,True,Kun Luo and Zheng Liu and Shitao Xiao and Tong Zhou and Yubo Chen and Jun Zhao and Kang Liu,2024,,https://api.semanticscholar.org/CorpusID:271915473,,
Modeling Uncertainty Trends for Timely Retrieval in Dynamic RAG,2511.09980v1,fang2024enhancingnr,\cite{fang2024enhancingnr},Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training,https://arxiv.org/abs/2405.20978v1,"Large Language Models (LLMs) exhibit substantial capabilities yet encounter challenges, including hallucination, outdated knowledge, and untraceable reasoning processes. Retrieval-augmented generation (RAG) has emerged as a promising solution, integrating knowledge from external databases to mitigate these challenges. However, inappropriate retrieved passages can potentially hinder the LLMs' capacity to generate comprehensive and high-quality responses. Prior RAG studies on the robustness of retrieval noises often confine themselves to a limited set of noise types, deviating from real-world retrieval environments and limiting practical applicability. In this study, we initially investigate retrieval noises and categorize them into three distinct types, reflecting real-world environments. We analyze the impact of these various retrieval noises on the robustness of LLMs. Subsequently, we propose a novel RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT). RAAT leverages adaptive adversarial training to dynamically adjust the model's training process in response to retrieval noises. Concurrently, it employs multi-task learning to ensure the model's capacity to internally recognize noisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model trained using RAAT exhibits significant improvements in F1 and EM scores under diverse noise conditions. For reproducibility, we release our code and data at: https://github.com/calubkk/RAAT.",True,True,Feiteng Fang and Yuelin Bai and Shiwen Ni and Min Yang and Xiaojun Chen and Ruifeng Xu,2024,,https://api.semanticscholar.org/CorpusID:270199429,,
Modeling Uncertainty Trends for Timely Retrieval in Dynamic RAG,2511.09980v1,xu2024unsupervisedir,\cite{xu2024unsupervisedir},Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation,https://arxiv.org/abs/2402.18150v2,"Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating additional information from retrieval. However, studies have shown that LLMs still face challenges in effectively using the retrieved information, even ignoring it or being misled by it. The key reason is that the training of LLMs does not clearly make LLMs learn how to utilize input retrieved texts with varied quality. In this paper, we propose a novel perspective that considers the role of LLMs in RAG as ``Information Refiner'', which means that regardless of correctness, completeness, or usefulness of retrieved texts, LLMs can consistently integrate knowledge within the retrieved texts and model parameters to generate the texts that are more concise, accurate, and complete than the retrieved texts. To this end, we propose an information refinement training method named InFO-RAG that optimizes LLMs for RAG in an unsupervised manner. InFO-RAG is low-cost and general across various tasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse tasks including Question Answering, Slot-Filling, Language Modeling, Dialogue, and Code Generation show that InFO-RAG improves the performance of LLaMA2 by an average of 9.39\% relative points. InFO-RAG also shows advantages in in-context learning and robustness of RAG.",True,True,Shicheng Xu and Liang Pang and Mo Yu and Fandong Meng and Huawei Shen and Xueqi Cheng and Jie Zhou,2024,,https://api.semanticscholar.org/CorpusID:268041643,,ArXiv
Modeling Uncertainty Trends for Timely Retrieval in Dynamic RAG,2511.09980v1,Izacard2020LeveragingPR,\cite{Izacard2020LeveragingPR},Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering,,,True,False,Gautier Izacard and Edouard Grave,2020,,https://api.semanticscholar.org/CorpusID:220302360,,ArXiv
Modeling Uncertainty Trends for Timely Retrieval in Dynamic RAG,2511.09980v1,Wang2023SelfKnowledgeGR,\cite{Wang2023SelfKnowledgeGR},Self-Knowledge Guided Retrieval Augmentation for Large Language Models,https://arxiv.org/abs/2310.05002v1,"Large language models (LLMs) have shown superior performance without task-specific fine-tuning. Despite the success, the knowledge stored in the parameters of LLMs could still be incomplete and difficult to update due to the computational costs. As complementary, retrieval-based methods can offer non-parametric world knowledge and improve the performance on tasks such as question answering. However, we find that the retrieved knowledge does not always help and even has a negative impact on original responses occasionally. To better make use of both internal knowledge and external world knowledge, we investigate eliciting the model's ability to recognize what they know and do not know (which is also called self-knowledge) and propose Self-Knowledge guided Retrieval augmentation (SKR), a simple yet effective method which can let LLMs refer to the questions they have previously encountered and adaptively call for external resources when dealing with new questions. We evaluate SKR on multiple datasets and demonstrate that it outperforms chain-of-thought based and fully retrieval-based methods by using either InstructGPT or ChatGPT.",True,True,Yile Wang and Peng Li and Maosong Sun and Yang Liu,2023,,https://api.semanticscholar.org/CorpusID:263828724,,
Modeling Uncertainty Trends for Timely Retrieval in Dynamic RAG,2511.09980v1,jiang2023active,\cite{jiang2023active},Active Retrieval Augmented Generation,https://arxiv.org/abs/2305.06983v2,"Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.",True,True,"Jiang, Zhengbao and Xu, Frank F and Gao, Luyu and Sun, Zhiqing and Liu, Qian and Dwivedi-Yu, Jane and Yang, Yiming and Callan, Jamie and Neubig, Graham",2023,,,,arXiv preprint arXiv:2305.06983
Modeling Uncertainty Trends for Timely Retrieval in Dynamic RAG,2511.09980v1,su2024dragin,\cite{su2024dragin},DRAGIN: Dynamic Retrieval Augmented Generation based on the Information Needs of Large Language Models,https://arxiv.org/abs/2403.10081v3,"Dynamic retrieval augmented generation (RAG) paradigm actively decides when and what to retrieve during the text generation process of Large Language Models (LLMs). There are two key elements of this paradigm: identifying the optimal moment to activate the retrieval module (deciding when to retrieve) and crafting the appropriate query once retrieval is triggered (determining what to retrieve). However, current dynamic RAG methods fall short in both aspects. Firstly, the strategies for deciding when to retrieve often rely on static rules. Moreover, the strategies for deciding what to retrieve typically limit themselves to the LLM's most recent sentence or the last few tokens, while the LLM's real-time information needs may span across the entire context. To overcome these limitations, we introduce a new framework, DRAGIN, i.e., Dynamic Retrieval Augmented Generation based on the real-time Information Needs of LLMs. Our framework is specifically designed to make decisions on when and what to retrieve based on the LLM's real-time information needs during the text generation process. We evaluate DRAGIN along with existing methods comprehensively over 4 knowledge-intensive generation datasets. Experimental results show that DRAGIN achieves superior performance on all tasks, demonstrating the effectiveness of our method. We have open-sourced all the code, data, and models in GitHub: https://github.com/oneal2000/DRAGIN/tree/main",True,True,"Su, Weihang and Tang, Yichen and Ai, Qingyao and Wu, Zhijing and Liu, Yiqun",2024,,,,arXiv preprint arXiv:2403.10081
Modeling Uncertainty Trends for Timely Retrieval in Dynamic RAG,2511.09980v1,Shi2023REPLUGRB,\cite{Shi2023REPLUGRB},REPLUG: Retrieval-Augmented Black-Box Language Models,https://arxiv.org/abs/2301.12652v4,"We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing retrieval and language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%.",True,True,Weijia Shi and Sewon Min and Michihiro Yasunaga and Minjoon Seo and Rich James and Mike Lewis and Luke Zettlemoyer and Wen-tau Yih,2023,,https://api.semanticscholar.org/CorpusID:256389797,,
Modeling Uncertainty Trends for Timely Retrieval in Dynamic RAG,2511.09980v1,Wang2023Query2docQE,\cite{Wang2023Query2docQE},Query2doc: Query Expansion with Large Language Models,,,True,False,Liang Wang and Nan Yang and Furu Wei,2023,,https://api.semanticscholar.org/CorpusID:257505063,,
Modeling Uncertainty Trends for Timely Retrieval in Dynamic RAG,2511.09980v1,Yu2023ImprovingLM,\cite{Yu2023ImprovingLM},Improving Language Models via Plug-and-Play Retrieval Feedback,https://arxiv.org/abs/2305.14002v1,"Large language models (LLMs) exhibit remarkable performance across various NLP tasks. However, they often generate incorrect or hallucinated information, which hinders their practical applicability in real-world scenarios. Human feedback has been shown to effectively enhance the factuality and quality of generated content, addressing some of these limitations. However, this approach is resource-intensive, involving manual input and supervision, which can be time-consuming and expensive. Moreover, it cannot be provided during inference, further limiting its practical utility in dynamic and interactive applications. In this paper, we introduce ReFeed, a novel pipeline designed to enhance LLMs by providing automatic retrieval feedback in a plug-and-play framework without the need for expensive fine-tuning. ReFeed first generates initial outputs, then utilizes a retrieval model to acquire relevant information from large document collections, and finally incorporates the retrieved information into the in-context demonstration for output refinement, thereby addressing the limitations of LLMs in a more efficient and cost-effective manner. Experiments on four knowledge-intensive benchmark datasets demonstrate our proposed ReFeed could improve over +6.0% under zero-shot setting and +2.5% under few-shot setting, compared to baselines without using retrieval feedback.",True,True,W. Yu and Zhihan Zhang and Zhenwen Liang and Meng Jiang and Ashish Sabharwal,2023,,https://api.semanticscholar.org/CorpusID:258841029,,ArXiv
Modeling Uncertainty Trends for Timely Retrieval in Dynamic RAG,2511.09980v1,ni2024whendl,\cite{ni2024whendl},When Do LLMs Need Retrieval Augmentation? Mitigating LLMs' Overconfidence Helps Retrieval Augmentation,https://arxiv.org/abs/2402.11457v2,"Large Language Models (LLMs) have been found to have difficulty knowing they do not possess certain knowledge and tend to provide specious answers in such cases. Retrieval Augmentation (RA) has been extensively studied to mitigate LLMs' hallucinations. However, due to the extra overhead and unassured quality of retrieval, it may not be optimal to conduct RA all the time. A straightforward idea is to only conduct retrieval when LLMs are uncertain about a question. This motivates us to enhance the LLMs' ability to perceive their knowledge boundaries to help RA. In this paper, we first quantitatively measure LLMs' such ability and confirm their overconfidence. Then, we study how LLMs' certainty about a question correlates with their dependence on external retrieved information. We propose several methods to enhance LLMs' perception of knowledge boundaries and show that they are effective in reducing overconfidence. Additionally, equipped with these methods, LLMs can achieve comparable or even better performance of RA with much fewer retrieval calls.",True,True,Shiyu Ni and Keping Bi and J. Guo and Xueqi Cheng,2024,,,,
Modeling Uncertainty Trends for Timely Retrieval in Dynamic RAG,2511.09980v1,Su2024BRIGHTAR,\cite{Su2024BRIGHTAR},BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval,,,True,False,"Hongjin Su and Howard Yen and Mengzhou Xia and Weijia Shi and Niklas Muennighoff and Han-yu Wang and Haisu Liu and Quan Shi and Zachary S. Siegel and Michael Tang and Ruoxi Sun and Jinsung Yoon and Sercan {\""O}. Arik and Danqi Chen and Tao Yu",2024,,https://api.semanticscholar.org/CorpusID:271270735,,ArXiv
Modeling Uncertainty Trends for Timely Retrieval in Dynamic RAG,2511.09980v1,borgeaud2022improving,\cite{borgeaud2022improving},Improving language models by retrieving from trillions of tokens,https://arxiv.org/abs/2112.04426v3,"We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.",True,True,"Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others",2022,,,,
Modeling Uncertainty Trends for Timely Retrieval in Dynamic RAG,2511.09980v1,trivedi2022interleaving,\cite{trivedi2022interleaving},Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions,,,True,False,"Trivedi, Harsh and Balasubramanian, Niranjan and Khot, Tushar and Sabharwal, Ashish",2022,,,,arXiv preprint arXiv:2212.10509
Modeling Uncertainty Trends for Timely Retrieval in Dynamic RAG,2511.09980v1,ram2023context,\cite{ram2023context},In-Context Retrieval-Augmented Language Models,https://arxiv.org/abs/2302.00083v3,"Retrieval-Augmented Language Modeling (RALM) methods, which condition a language model (LM) on relevant documents from a grounding corpus during generation, were shown to significantly improve language modeling performance. In addition, they can mitigate the problem of factually inaccurate text generation and provide natural source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment. This paper considers a simple alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM. We show that In-Context RALM that builds on off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora. We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to further boost performance. We conclude that In-Context RALM has considerable potential to increase the prevalence of LM grounding, particularly in settings where a pretrained LM must be used without modification or even via API access.",True,True,"Ram, Ori and Levine, Yoav and Dalmedigos, Itay and Muhlgay, Dor and Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav",2023,,,,Transactions of the Association for Computational Linguistics
Modeling Uncertainty Trends for Timely Retrieval in Dynamic RAG,2511.09980v1,Wang2024SelfDCWT,\cite{Wang2024SelfDCWT},Self-DC: When to Reason and When to Act? Self Divide-and-Conquer for Compositional Unknown Questions,,,True,False,Hongru Wang and Boyang Xue and Baohang Zhou and Tianhua Zhang and Cunxiang Wang and Guanhua Chen and Huimin Wang and Kam-Fai Wong,2024,,https://api.semanticscholar.org/CorpusID:267770210,,
Modeling Uncertainty Trends for Timely Retrieval in Dynamic RAG,2511.09980v1,Tao2024WhenTT,\cite{Tao2024WhenTT},When to Trust LLMs: Aligning Confidence with Response Quality,https://arxiv.org/abs/2404.17287v3,"Despite the success of large language models (LLMs) in natural language generation, much evidence shows that LLMs may produce incorrect or nonsensical text. This limitation highlights the importance of discerning when to trust LLMs, especially in safety-critical domains. Existing methods often express reliability by confidence level, however, their effectiveness is limited by the lack of objective guidance. To address this, we propose CONfidence-Quality-ORDer-preserving alignment approach (CONQORD), which leverages reinforcement learning guided by a tailored dual-component reward function. This function integrates quality reward and order-preserving alignment reward functions. Specifically, the order-preserving reward incentivizes the model to verbalize greater confidence for responses of higher quality to align the order of confidence and quality. Experiments demonstrate that CONQORD significantly improves the alignment performance between confidence and response accuracy, without causing over-cautious. Furthermore, the aligned confidence provided by CONQORD informs when to trust LLMs, and acts as a determinant for initiating the retrieval process of external knowledge. Aligning confidence with response quality ensures more transparent and reliable responses, providing better trustworthiness.",True,True,Shuchang Tao and Liuyi Yao and Hanxing Ding and Yuexiang Xie and Qi Cao and Fei Sun and Jinyang Gao and Huawei Shen and Bolin Ding,2024,,https://api.semanticscholar.org/CorpusID:269430565,,ArXiv
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,SSD2016,\cite{SSD2016},SSD: Single Shot MultiBox Detector,https://arxiv.org/abs/1512.02325v5,"We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For $300\times 300$ input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for $500\times 500$ input, SSD achieves 75.1% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .",True,True,"Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy,
   Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.",2016,,,10.1007/978-3-319-46448-0\_2,
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,RetinaNet2020,\cite{RetinaNet2020},Focal loss for dense object detection,,,True,False,"Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr",2017,,,,
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,Yolov32018,\cite{Yolov32018},YOLOv3: An Incremental Improvement,https://arxiv.org/abs/1804.02767v1,"We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/",True,True,"Redmon, Joseph",2018,,,,arXiv preprint arXiv:1804.02767
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,YOLOv42020,\cite{YOLOv42020},YOLOv4: Optimal Speed and Accuracy of Object Detection,,,True,False,Alexey Bochkovskiy and Chien-Yao Wang and Hong-Yuan Mark Liao,2020,,,,ArXiv
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,2022yolov5,\cite{2022yolov5},"ultralytics/yolov5: v6.0 - YOLOv5n 'Nano' models, Roboflow integration, TensorFlow export, OpenCV DNN support",,,True,False,Ultralytics,2021,feb,https://doi.org/10.5281/zenodo.5563715,10.5281/zenodo.5563715,
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,lin2017fpn,\cite{lin2017fpn},Feature pyramid networks for object detection,,,True,False,"Lin, Tsung-Yi and Doll{\'a}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge",2017,,,,
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,RCNN2014,\cite{RCNN2014},Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,,,True,False,"Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra",2014,,,10.1109/CVPR.2014.81,
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,FastRCNN2015,\cite{FastRCNN2015},Fast R-CNN,https://arxiv.org/abs/1504.08083v2,"This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.",True,True,"Girshick, Ross",2015,,,10.1109/ICCV.2015.169,
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,FasterRCNN2017,\cite{FasterRCNN2017},Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,https://arxiv.org/abs/1506.01497v3,"State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.",True,True,"Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian",2017,,,10.1109/TPAMI.2016.2577031,IEEE Transactions on Pattern Analysis and Machine Intelligence
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,CascadeRCNN2018,\cite{CascadeRCNN2018},Cascade R-CNN: Delving Into High Quality Object Detection,,,True,False,"Cai, Zhaowei and Vasconcelos, Nuno",2018,,,10.1109/CVPR.2018.00644,
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,TridentNet2019,\cite{TridentNet2019},Scale-Aware Trident Networks for Object Detection,,,True,False,"Li, Yanghao and Chen, Yuntao and Wang, Naiyan and Zhang, Zhao-Xiang",2019,,,10.1109/ICCV.2019.00615,
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,CornerNet2018,\cite{CornerNet2018},CornerNet: Detecting Objects as Paired Keypoints,https://arxiv.org/abs/1808.01244v2,"We propose CornerNet, a new approach to object detection where we detect an object bounding box as a pair of keypoints, the top-left corner and the bottom-right corner, using a single convolution neural network. By detecting objects as paired keypoints, we eliminate the need for designing a set of anchor boxes commonly used in prior single-stage detectors. In addition to our novel formulation, we introduce corner pooling, a new type of pooling layer that helps the network better localize corners. Experiments show that CornerNet achieves a 42.2% AP on MS COCO, outperforming all existing one-stage detectors.",True,True,"Law, Hei and Deng, Jia",2018,,,10.1007/978-3-030-01264-9\_45,
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,zhou2019objects,\cite{zhou2019objects},CenterNet: Keypoint Triplets for Object Detection,,,True,False,"Duan, Kaiwen and Bai, Song and Xie, Lingxi and Qi, Honggang and Huang,
   Qingming and Tian, Qi",2019,,,10.1109/ICCV.2019.00667,
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,2019grid,\cite{2019grid},Grid R-CNN,https://arxiv.org/abs/1811.12030v1,"This paper proposes a novel object detection framework named Grid R-CNN, which adopts a grid guided localization mechanism for accurate object detection. Different from the traditional regression based methods, the Grid R-CNN captures the spatial information explicitly and enjoys the position sensitive property of fully convolutional architecture. Instead of using only two independent points, we design a multi-point supervision formulation to encode more clues in order to reduce the impact of inaccurate prediction of specific points. To take the full advantage of the correlation of points in a grid, we propose a two-stage information fusion strategy to fuse feature maps of neighbor grid points. The grid guided localization approach is easy to be extended to different state-of-the-art detection frameworks. Grid R-CNN leads to high quality object localization, and experiments demonstrate that it achieves a 4.1% AP gain at IoU=0.8 and a 10.0% AP gain at IoU=0.9 on COCO benchmark compared to Faster R-CNN with Res50 backbone and FPN architecture.",True,True,"Lu, Xin and Li, Buyu and Yue, Yuxin and Li, Quanquan and Yan, Junjie",2019,,,10.1109/CVPR.2019.00754,
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,Reppoints2019,\cite{Reppoints2019},RepPoints: Point Set Representation for Object Detection,,,True,False,"Yang, Ze and Liu, Shaohui and Hu, Han and Wang, Liwei and Lin, Stephen",2019,,,10.1109/ICCV.2019.00975,
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,2020foveabox,\cite{2020foveabox},FoveaBox: Beyound Anchor-Based Object Detection,,,True,False,"Kong, Tao and Sun, Fuchun and Liu, Huaping and Jiang, Yuning and Li, Lei and Shi, Jianbo",2020,,,10.1109/TIP.2020.3002345,IEEE Transactions on Image Processing
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,FCOS2020,\cite{FCOS2020},{FCOS:} A simple and strong anchor-free object detector,,,True,False,"Tian, Zhi and Shen, Chunhua and Chen, Hao and He, Tong",2020,,,,IEEE Transactions on Pattern Analysis and Machine Intelligence
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,feng2021tood,\cite{feng2021tood},TOOD: Task-aligned One-stage Object Detection,https://arxiv.org/abs/2108.07755v3,"One-stage object detection is commonly implemented by optimizing two sub-tasks: object classification and localization, using heads with two parallel branches, which might lead to a certain level of spatial misalignment in predictions between the two tasks. In this work, we propose a Task-aligned One-stage Object Detection (TOOD) that explicitly aligns the two tasks in a learning-based manner. First, we design a novel Task-aligned Head (T-Head) which offers a better balance between learning task-interactive and task-specific features, as well as a greater flexibility to learn the alignment via a task-aligned predictor. Second, we propose Task Alignment Learning (TAL) to explicitly pull closer (or even unify) the optimal anchors for the two tasks during training via a designed sample assignment scheme and a task-aligned loss. Extensive experiments are conducted on MS-COCO, where TOOD achieves a 51.1 AP at single-model single-scale testing. This surpasses the recent one-stage detectors by a large margin, such as ATSS (47.7 AP), GFL (48.2 AP), and PAA (49.0 AP), with fewer parameters and FLOPs. Qualitative results also demonstrate the effectiveness of TOOD for better aligning the tasks of object classification and localization. Code is available at https://github.com/fcjian/TOOD.",True,True,"Feng, Chengjian and Zhong, Yujie and Gao, Yu and Scott, Matthew R and Huang, Weilin",2021,,,,
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,YOLOX2021,\cite{YOLOX2021},YOLOX: Exceeding YOLO Series in 2021,https://arxiv.org/abs/2107.08430v2,"In this report, we present some experienced improvements to YOLO series, forming a new high-performance detector -- YOLOX. We switch the YOLO detector to an anchor-free manner and conduct other advanced detection techniques, i.e., a decoupled head and the leading label assignment strategy SimOTA to achieve state-of-the-art results across a large scale range of models: For YOLO-Nano with only 0.91M parameters and 1.08G FLOPs, we get 25.3% AP on COCO, surpassing NanoDet by 1.8% AP; for YOLOv3, one of the most widely used detectors in industry, we boost it to 47.3% AP on COCO, outperforming the current best practice by 3.0% AP; for YOLOX-L with roughly the same amount of parameters as YOLOv4-CSP, YOLOv5-L, we achieve 50.0% AP on COCO at a speed of 68.9 FPS on Tesla V100, exceeding YOLOv5-L by 1.8% AP. Further, we won the 1st Place on Streaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021) using a single YOLOX-L model. We hope this report can provide useful experience for developers and researchers in practical scenes, and we also provide deploy versions with ONNX, TensorRT, NCNN, and Openvino supported. Source code is at https://github.com/Megvii-BaseDetection/YOLOX.",True,True,Zheng Ge and Songtao Liu and Feng Wang and Zeming Li and Jian Sun,2021,,,,ArXiv
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,2023yolov8,\cite{2023yolov8},Ultralytics YOLO,,,True,False,"{G. Jocher, A. Chaurasia, and J. Qiu.}",,,https://github.com/ultralytics/ultralytics,,
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,2020IJSTAR,\cite{2020IJSTAR},Cross-Layer Attention Network for Small Object Detection in Remote Sensing Imagery,,,True,False,"Li, Yangyang and Huang, Qin and Pei, Xuan and Chen, Yanqiao and Jiao, Licheng and Shang, Ronghua",2021,,,10.1109/JSTARS.2020.3046482,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,2022msodanet,\cite{2022msodanet},mSODANet: A network for multi-scale object detection in aerial images using hierarchical dilated convolutions,,,True,False,"Chalavadi, Vishnu and Jeripothula, Prudviraj and Datla, Rajeshreddy and Ch, Sobhan Babu and others",2022,,,,Pattern Recognition
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,2022Context-Aware,\cite{2022Context-Aware},"Scenario Context-Aware-Based Bidirectional Feature Pyramid Network for
   Remote Sensing Target Detection",,,True,False,"Huang, Wei and Li, Guanyi and Jin, Baohua and Chen, Qiqiang and Yin,
   Junru and Huang, Long",2022,,,10.1109/LGRS.2021.3135935,IEEE Geoscience and Remote Sensing Letters
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,2022FSANet,\cite{2022FSANet},{FSANet:} Feature-and-Spatial-Aligned Network for Tiny Object Detection in Remote Sensing Images,,,True,False,"Wu, Jixiang and Pan, Zongxu and Lei, Bin and Hu, Yuxin",2022,,,10.1109/TGRS.2022.3205052,IEEE Transactions on Geoscience and Remote Sensing
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,cheng2022tiny,\cite{cheng2022tiny},Tiny object detection via regional cross self-attention network,,,True,False,"Cheng, Keyang and Cui, Honggang and Ghafoor, Humaira Abdul and Wan, Hao and Mao, Qirong and Zhan, Yongzhao",2022,,,,IEEE Transactions on Circuits and Systems for Video Technology
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,hong2021sspnet,\cite{hong2021sspnet},SSPNet: Scale Selection Pyramid Network for Tiny Person Detection from UAV Images,https://arxiv.org/abs/2107.01548v1,"With the increasing demand for search and rescue, it is highly demanded to detect objects of interest in large-scale images captured by Unmanned Aerial Vehicles (UAVs), which is quite challenging due to extremely small scales of objects. Most existing methods employed Feature Pyramid Network (FPN) to enrich shallow layers' features by combing deep layers' contextual features. However, under the limitation of the inconsistency in gradient computation across different layers, the shallow layers in FPN are not fully exploited to detect tiny objects. In this paper, we propose a Scale Selection Pyramid network (SSPNet) for tiny person detection, which consists of three components: Context Attention Module (CAM), Scale Enhancement Module (SEM), and Scale Selection Module (SSM). CAM takes account of context information to produce hierarchical attention heatmaps. SEM highlights features of specific scales at different layers, leading the detector to focus on objects of specific scales instead of vast backgrounds. SSM exploits adjacent layers' relationships to fulfill suitable feature sharing between deep layers and shallow layers, thereby avoiding the inconsistency in gradient computation across different layers. Besides, we propose a Weighted Negative Sampling (WNS) strategy to guide the detector to select more representative samples. Experiments on the TinyPerson benchmark show that our method outperforms other state-of-the-art (SOTA) detectors.",True,True,"Hong, Mingbo and Li, Shuiwang and Yang, Yuchao and Zhu, Feiyu and Zhao, Qijun and Lu, Li",2021,,,,IEEE geoscience and remote sensing letters
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,chen2023mdct,\cite{chen2023mdct},MDCT: Multi-kernel dilated convolution and transformer for one-stage object detection of remote sensing images,,,True,False,"Chen, Juanjuan and Hong, Hansheng and Song, Bin and Guo, Jie and Chen, Chen and Xu, Junjie",2023,,,,Remote Sensing
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,Xu2022_NWD,\cite{Xu2022_NWD},Detecting tiny objects in aerial images: A normalized Wasserstein distance and a new benchmark,,,True,False,Chang Xu and Jinwang Wang and Wen Yang and Huai Yu and Lei Yu and Gui-Song Xia,2022,,,https://doi.org/10.1016/j.isprsjprs.2022.06.002,ISPRS Journal of Photogrammetry and Remote Sensing
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,RFLA2022,\cite{RFLA2022},RFLA: Gaussian Receptive Field based Label Assignment for Tiny Object Detection,https://arxiv.org/abs/2208.08738v2,"Detecting tiny objects is one of the main obstacles hindering the development of object detection. The performance of generic object detectors tends to drastically deteriorate on tiny object detection tasks. In this paper, we point out that either box prior in the anchor-based detector or point prior in the anchor-free detector is sub-optimal for tiny objects. Our key observation is that the current anchor-based or anchor-free label assignment paradigms will incur many outlier tiny-sized ground truth samples, leading to detectors imposing less focus on the tiny objects. To this end, we propose a Gaussian Receptive Field based Label Assignment (RFLA) strategy for tiny object detection. Specifically, RFLA first utilizes the prior information that the feature receptive field follows Gaussian distribution. Then, instead of assigning samples with IoU or center sampling strategy, a new Receptive Field Distance (RFD) is proposed to directly measure the similarity between the Gaussian receptive field and ground truth. Considering that the IoU-threshold based and center sampling strategy are skewed to large objects, we further design a Hierarchical Label Assignment (HLA) module based on RFD to achieve balanced learning for tiny objects. Extensive experiments on four datasets demonstrate the effectiveness of the proposed methods. Especially, our approach outperforms the state-of-the-art competitors with 4.0 AP points on the AI-TOD dataset. Codes are available at https://github.com/Chasel-Tsui/mmdet-rfla",True,True,"{Xu, Chang and Wang, Jinwang and Yang, Wen and Yu, Huai and Yu, Lei and Xia, Gui-Song}",2022,,,10.1007/978-3-031-20077-9\_31,
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,DotD2021,\cite{DotD2021},Dot distance for tiny object detection in aerial images,,,True,False,"Xu, Chang and Wang, Jinwang and Yang, Wen and Yu, Lei",2021,,,,
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,KLDetTGRS2024,\cite{KLDetTGRS2024},KLDet: Detecting Tiny Objects in Remote Sensing Images via Kullback–Leibler Divergence,,,True,False,"Zhou, Zhuangzhuang and Zhu, Yingying",2024,,,10.1109/TGRS.2024.3382099,IEEE Transactions on Geoscience and Remote Sensing
Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images,2511.09891v1,rs16234485,\cite{rs16234485},MMPW-Net: Detection of Tiny Objects in Aerial Imagery Using Mixed Minimum Point-Wasserstein Distance,,,True,False,"Su, Nan and Zhao, Zilong and Yan, Yiming and Wang, Jinpeng and Lu, Wanxuan and Cui, Hongbo and Qu, Yunfei and Feng, Shou and Zhao, Chunhui",2024,,https://www.mdpi.com/2072-4292/16/23/4485,10.3390/rs16234485,Remote Sensing
MACEval: A Multi-Agent Continual Evaluation Network for Large Models,2511.09139v1,chang2024survey,\cite{chang2024survey},DES Y3 + KiDS-1000: Consistent cosmology combining cosmic shear surveys,https://arxiv.org/abs/2305.17173v2,"We present a joint cosmic shear analysis of the Dark Energy Survey (DES Y3) and the Kilo-Degree Survey (KiDS-1000) in a collaborative effort between the two survey teams. We find consistent cosmological parameter constraints between DES Y3 and KiDS-1000 which, when combined in a joint-survey analysis, constrain the parameter $S_8 = σ_8 \sqrt{Ω_{\rm m}/0.3}$ with a mean value of $0.790^{+0.018}_{-0.014}$. The mean marginal is lower than the maximum a posteriori estimate, $S_8=0.801$, owing to skewness in the marginal distribution and projection effects in the multi-dimensional parameter space. Our results are consistent with $S_8$ constraints from observations of the cosmic microwave background by Planck, with agreement at the $1.7σ$ level. We use a Hybrid analysis pipeline, defined from a mock survey study quantifying the impact of the different analysis choices originally adopted by each survey team. We review intrinsic alignment models, baryon feedback mitigation strategies, priors, samplers and models of the non-linear matter power spectrum.",True,True,,,,,,
MACEval: A Multi-Agent Continual Evaluation Network for Large Models,2511.09139v1,li2024survey,\cite{li2024survey},Dark Energy Survey: 2.1% measurement of the Baryon Acoustic Oscillation scale from the final dataset,https://arxiv.org/abs/2409.08759v1,"Here, we present the angular diameter distance measurement obtained from the measurement of the Baryonic Acoustic Oscillation (BAO) feature using the completed Dark Energy Survey (DES) data, summarizing the main results of [Phys. Rev. D 110, 063514] and [Phys. Rev. D 110, 063515]. We use a galaxy sample optimized for BAO science in the redshift range 0.6 < z < 1.2, with an effective redshift of $z_{\rm eff}$ = 0.85. Our consensus measurement constrains the ratio of the angular distance to the sound horizon scale to $D_M(z_{\rm eff})/r_d$ = 19.51 $\pm$ 0.41. This measurement is found to be 2.13$σ$ below the angular BAO scale predicted by Planck. To date, it represents the most precise measurement from purely photometric data, and the most precise from any Stage-III experiment at such high redshift. The analysis was performed blinded to the BAO position and is shown to be robust against analysis choices, data removal, redshift calibrations and observational systematics.",True,True,,,,,,
MACEval: A Multi-Agent Continual Evaluation Network for Large Models,2511.09139v1,rajabi2024gsr,\cite{rajabi2024gsr},GSR-BENCH: A Benchmark for Grounded Spatial Reasoning Evaluation via Multimodal LLMs,https://arxiv.org/abs/2406.13246v2,"The ability to understand and reason about spatial relationships between objects in images is an important component of visual reasoning. This skill rests on the ability to recognize and localize objects of interest and determine their spatial relation. Early vision and language models (VLMs) have been shown to struggle to recognize spatial relations. We extend the previously released What'sUp dataset and propose a novel comprehensive evaluation for spatial relationship understanding that highlights the strengths and weaknesses of 27 different models. In addition to the VLMs evaluated in What'sUp, our extensive evaluation encompasses 3 classes of Multimodal LLMs (MLLMs) that vary in their parameter sizes (ranging from 7B to 110B), training/instruction-tuning methods, and visual resolution to benchmark their performances and scrutinize the scaling laws in this task.",True,True,,,,,,
MACEval: A Multi-Agent Continual Evaluation Network for Large Models,2511.09139v1,carlini2022quantifying,\cite{carlini2022quantifying},Quantifying Memorization Across Neural Language Models,https://arxiv.org/abs/2202.07646v3,"Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others).
  We describe three log-linear relationships that quantify the degree to which LMs emit memorized training data. Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes more complicated when generalizing these results across model families. On the whole, we find that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations.",True,True,,,,,,
MACEval: A Multi-Agent Continual Evaluation Network for Large Models,2511.09139v1,achiam2023gpt,\cite{achiam2023gpt},GPT-4 Technical Report,https://arxiv.org/abs/2303.08774v6,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",True,True,,,,,,
MACEval: A Multi-Agent Continual Evaluation Network for Large Models,2511.09139v1,touvron2023llama,\cite{touvron2023llama},LLaMA: Open and Efficient Foundation Language Models,https://arxiv.org/abs/2302.13971v1,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",True,True,,,,,,
MACEval: A Multi-Agent Continual Evaluation Network for Large Models,2511.09139v1,schuhmann2021laion,\cite{schuhmann2021laion},LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs,https://arxiv.org/abs/2111.02114v1,"Multi-modal language-vision models trained on hundreds of millions of image-text pairs (e.g. CLIP, DALL-E) gained a recent surge, showing remarkable capability to perform zero- or few-shot learning and transfer even in absence of per-sample labels on target image data. Despite this trend, to date there has been no publicly available datasets of sufficient scale for training such models from scratch. To address this issue, in a community effort we build and release for public LAION-400M, a dataset with CLIP-filtered 400 million image-text pairs, their CLIP embeddings and kNN indices that allow efficient similarity search.",True,True,,,,,,
When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?,2511.10059v1,qwen2,\cite{qwen2},Qwen2 Technical Report,https://arxiv.org/abs/2407.10671v4,"This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of foundational and instruction-tuned language models, encompassing a parameter range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts model. Qwen2 surpasses most prior open-weight models, including its predecessor Qwen1.5, and exhibits competitive performance relative to proprietary models across diverse benchmarks on language understanding, generation, multilingual proficiency, coding, mathematics, and reasoning.
  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2 demonstrates robust multilingual capabilities, proficient in approximately 30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and global reach.
  To foster community innovation and accessibility, we have made the Qwen2 model weights openly available on Hugging Face and ModelScope, and the supplementary materials including example code on GitHub. These platforms also include resources for quantization, fine-tuning, and deployment, facilitating a wide range of applications and research endeavors.",True,True,"An Yang and
                  Baosong Yang and
                  Binyuan Hui and
                  et al.",2024,,,,CoRR
When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?,2511.10059v1,qwen2.5,\cite{qwen2.5},Qwen2 Technical Report,https://arxiv.org/abs/2407.10671v4,"This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of foundational and instruction-tuned language models, encompassing a parameter range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts model. Qwen2 surpasses most prior open-weight models, including its predecessor Qwen1.5, and exhibits competitive performance relative to proprietary models across diverse benchmarks on language understanding, generation, multilingual proficiency, coding, mathematics, and reasoning.
  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2 demonstrates robust multilingual capabilities, proficient in approximately 30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and global reach.
  To foster community innovation and accessibility, we have made the Qwen2 model weights openly available on Hugging Face and ModelScope, and the supplementary materials including example code on GitHub. These platforms also include resources for quantization, fine-tuning, and deployment, facilitating a wide range of applications and research endeavors.",True,True,"An Yang and
                  Baosong Yang and
                  Beichen Zhang and
                  et al.",2024,,,,CoRR
When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?,2511.10059v1,llama2,\cite{llama2},Llama 2: Open Foundation and Fine-Tuned Chat Models,,,True,False,"Hugo Touvron and
                  Louis Martin and
                  Kevin Stone and
                  Peter Albert and
                  Amjad Almahairi and
                  Yasmine Babaei and
                  Nikolay Bashlykov and
                  Soumya Batra and
                  Prajjwal Bhargava and
                  Shruti Bhosale and
                  Dan Bikel and
                  Lukas Blecher and
                  Cristian Canton{-}Ferrer and
                  Moya Chen and
                  Guillem Cucurull and
                  David Esiobu and
                  Jude Fernandes and
                  Jeremy Fu and
                  Wenyin Fu and
                  Brian Fuller and
                  Cynthia Gao and
                  Vedanuj Goswami and
                  Naman Goyal and
                  Anthony Hartshorn and
                  Saghar Hosseini and
                  Rui Hou and
                  Hakan Inan and
                  Marcin Kardas and
                  Viktor Kerkez and
                  Madian Khabsa and
                  Isabel Kloumann and
                  Artem Korenev and
                  Punit Singh Koura and
                  Marie{-}Anne Lachaux and
                  Thibaut Lavril and
                  Jenya Lee and
                  Diana Liskovich and
                  Yinghai Lu and
                  Yuning Mao and
                  Xavier Martinet and
                  Todor Mihaylov and
                  Pushkar Mishra and
                  Igor Molybog and
                  Yixin Nie and
                  Andrew Poulton and
                  Jeremy Reizenstein and
                  Rashi Rungta and
                  Kalyan Saladi and
                  Alan Schelten and
                  Ruan Silva and
                  Eric Michael Smith and
                  Ranjan Subramanian and
                  Xiaoqing Ellen Tan and
                  Binh Tang and
                  Ross Taylor and
                  Adina Williams and
                  Jian Xiang Kuan and
                  Puxin Xu and
                  Zheng Yan and
                  Iliyan Zarov and
                  Yuchen Zhang and
                  Angela Fan and
                  Melanie Kambadur and
                  Sharan Narang and
                  Aur{\'{e}}lien Rodriguez and
                  Robert Stojnic and
                  Sergey Edunov and
                  Thomas Scialom",2023,,,,CoRR
When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?,2511.10059v1,pianoavqa,\cite{pianoavqa},Pano-AVQA: Grounded Audio-Visual Question Answering on 360$^\circ$ Videos,https://arxiv.org/abs/2110.05122v1,"360$^\circ$ videos convey holistic views for the surroundings of a scene. It provides audio-visual cues beyond pre-determined normal field of views and displays distinctive spatial relations on a sphere. However, previous benchmark tasks for panoramic videos are still limited to evaluate the semantic understanding of audio-visual relationships or spherical spatial property in surroundings. We propose a novel benchmark named Pano-AVQA as a large-scale grounded audio-visual question answering dataset on panoramic videos. Using 5.4K 360$^\circ$ video clips harvested online, we collect two types of novel question-answer pairs with bounding-box grounding: spherical spatial relation QAs and audio-visual relation QAs. We train several transformer-based models from Pano-AVQA, where the results suggest that our proposed spherical spatial embeddings and multimodal training objectives fairly contribute to a better semantic understanding of the panoramic surroundings on the dataset.",True,True,"Yun, Heeseung and Yu, Yang and Yang, Woosung and Lee, Kang-Il and Kim, Gun-Hee",,,,,
When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?,2511.10059v1,ye2024iet,\cite{ye2024iet},3sG: Three-stage guidance for indoor human action recognition,,,True,False,"Hai Nan and
                  Qilang Ye and
                  Zitong Yu and
                  Kang An",2024,,https://doi.org/10.1049/ipr2.13078,10.1049/IPR2.13078,{IET} Image Process.
When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?,2511.10059v1,ye2024pose,\cite{ye2024pose},Pose-Promote: Progressive Visual Perception for Activities of Daily Living,,,True,False,"Ye, Qilang and Yu, Zitong",2024,,,,IEEE Signal Processing Letters
When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?,2511.10059v1,ye2025cat+,\cite{ye2025cat+},CAT+: Investigating and Enhancing Audio-visual Understanding in Large Language Models,,,True,False,"Ye, Qilang and Yu, Zitong and Shao, Rui and Cui, Yawen and Kang, Xiangui and Liu, Xin and Torr, Philip and Cao, Xiaochun",2025,,,,IEEE Transactions on Pattern Analysis and Machine Intelligence
When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?,2511.10059v1,fusionmamba,\cite{fusionmamba},Fusionmamba: Dynamic feature enhancement for multimodal image fusion with mamba,,,True,False,"Xie, Xinyu and Cui, Yawen and Tan, Tao and Zheng, Xubin and Yu, Zitong",2024,,,,Visual Intelligence
When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?,2511.10059v1,liu2018attentive,\cite{liu2018attentive},Attentive moment retrieval in videos,,,True,False,"Liu, Meng and Wang, Xiang and Nie, Liqiang and He, Xiangnan and Chen, Baoquan and Chua, Tat-Seng",2018,,,,
When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?,2511.10059v1,liu2018cross,\cite{liu2018cross},Cross-modal moment localization in videos,,,True,False,"Liu, Meng and Wang, Xiang and Nie, Liqiang and Tian, Qi and Chen, Baoquan and Chua, Tat-Seng",2018,,,,
When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?,2511.10059v1,lin2025reliable,\cite{lin2025reliable},Reliable and Balanced Transfer Learning for Generalized Multimodal Face Anti-Spoofing,,,True,False,"Lin, Xun and Liu, Ajian and Yu, Zitong and Cai, Rizhao and Wang, Shuai and Yu, Yi and Wan, Jun and Lei, Zhen and Cao, Xiaochun and Kot, Alex",2025,,,,IEEE Transactions on Pattern Analysis and Machine Intelligence
When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?,2511.10059v1,ye2024CAT,\cite{ye2024CAT},CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios,https://arxiv.org/abs/2403.04640v1,"This paper focuses on the challenge of answering questions in scenarios that are composed of rich and complex dynamic audio-visual components. Although existing Multimodal Large Language Models (MLLMs) can respond to audio-visual content, these responses are sometimes ambiguous and fail to describe specific audio-visual events. To overcome this limitation, we introduce the CAT, which enhances MLLM in three ways: 1) besides straightforwardly bridging audio and video, we design a clue aggregator that aggregates question-related clues in dynamic audio-visual scenarios to enrich the detailed knowledge required for large language models. 2) CAT is trained on a mixed multimodal dataset, allowing direct application in audio-visual scenarios. Notably, we collect an audio-visual joint instruction dataset named AVinstruct, to further enhance the capacity of CAT to model cross-semantic correlations. 3) we propose AI-assisted ambiguity-aware direct preference optimization, a strategy specialized in retraining the model to favor the non-ambiguity response and improve the ability to localize specific audio-visual objects. Extensive experimental results demonstrate that CAT outperforms existing methods on multimodal tasks, especially in Audio-Visual Question Answering (AVQA) tasks. The codes and the collected instructions are released at https://github.com/rikeilong/Bay-CAT.",True,True,"Qilang Ye and
                  Zitong Yu and
                  Rui Shao and
                  Xinyu Xie and
                  Philip Torr and
                  Xiaochun Cao",2024,,,,
When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?,2511.10059v1,qwenomni,\cite{qwenomni},Qwen2.5-Omni Technical Report,,,True,False,"Jin Xu and
                  Zhifang Guo and
                  Jinzheng He and
                  et al.",2025,,,,CoRR
When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?,2511.10059v1,videollama2,\cite{videollama2},VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs,https://arxiv.org/abs/2406.07476v3,"In this paper, we present the VideoLLaMA 2, a set of Video Large Language Models (Video-LLMs) designed to enhance spatial-temporal modeling and audio understanding in video and audio-oriented tasks. Building upon its predecessor, VideoLLaMA 2 incorporates a tailor-made Spatial-Temporal Convolution (STC) connector, which effectively captures the intricate spatial and temporal dynamics of video data. Additionally, we integrate an Audio Branch into the model through joint training, thereby enriching the multimodal understanding capabilities of the model by seamlessly incorporating audio cues. Comprehensive evaluations on multiple-choice video question answering (MC-VQA), open-ended video question answering (OE-VQA), and video captioning (VC) tasks demonstrate that VideoLLaMA 2 consistently achieves competitive results among open-source models and even gets close to some proprietary models on several benchmarks. Furthermore, VideoLLaMA 2 exhibits reasonable improvements in audio-only and audio-video question-answering (AQA & OE-AVQA) benchmarks over existing models. These advancements underline VideoLLaMA 2's superior performance in multimodal comprehension, setting a new standard for intelligent video analysis systems. All models are public to facilitate further research.",True,True,"Zesen Cheng and
                  Sicong Leng and
                  Hang Zhang and
                  Yifei Xin and
                  Xin Li and
                  Guanzheng Chen and
                  Yongxin Zhu and
                  Wenqi Zhang and
                  Ziyang Luo and
                  Deli Zhao and
                  Lidong Bing",2024,,,,CoRR
When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?,2511.10059v1,pope,\cite{pope},Evaluating Object Hallucination in Large Vision-Language Models,https://arxiv.org/abs/2305.10355v3,"Inspired by the superior language abilities of large language models (LLM), large vision-language models (LVLM) have been recently explored by integrating powerful LLMs for improving the performance on complex multimodal tasks. Despite the promising progress on LVLMs, we find that LVLMs suffer from the hallucination problem, i.e. they tend to generate objects that are inconsistent with the target images in the descriptions. To investigate it, this work presents the first systematic study on object hallucination of LVLMs. We conduct the evaluation experiments on several representative LVLMs, and show that they mostly suffer from severe object hallucination issue. We further discuss that the visual instructions may influence the hallucination, and find that: objects that frequently occur in the visual instructions or co-occur with the image objects, are obviously prone to be hallucinated by LVLMs. Besides, we find that existing evaluation methods might be affected by the input instructions and generation styles of LVLMs. Thus, we further design an improved evaluation method for object hallucination by proposing a polling-based query method called POPE. Experiment results demonstrate that our POPE can evaluate the object hallucination in a more stable and flexible way. Our codes and data are publicly available at https://github.com/RUCAIBox/POPE.",True,True,"Yifan Li and
                  Yifan Du and
                  Kun Zhou and
                  Jinpeng Wang and
                  Wayne Xin Zhao and
                  Ji{-}Rong Wen",2023,,,,
When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?,2511.10059v1,hall2,\cite{hall2},AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?,https://arxiv.org/abs/2412.02611v1,"Recently, multimodal large language models (MLLMs), such as GPT-4o, Gemini 1.5 Pro, and Reka Core, have expanded their capabilities to include vision and audio modalities. While these models demonstrate impressive performance across a wide range of audio-visual applications, our proposed DeafTest reveals that MLLMs often struggle with simple tasks humans find trivial: 1) determining which of two sounds is louder, and 2) determining which of two sounds has a higher pitch. Motivated by these observations, we introduce AV-Odyssey Bench, a comprehensive audio-visual benchmark designed to assess whether those MLLMs can truly understand the audio-visual information. This benchmark encompasses 4,555 carefully crafted problems, each incorporating text, visual, and audio components. To successfully infer answers, models must effectively leverage clues from both visual and audio inputs. To ensure precise and objective evaluation of MLLM responses, we have structured the questions as multiple-choice, eliminating the need for human evaluation or LLM-assisted assessment. We benchmark a series of closed-source and open-source models and summarize the observations. By revealing the limitations of current models, we aim to provide useful insight for future dataset collection and model development.",True,True,"Kaixiong Gong and
                  Kaituo Feng and
                  Bohao Li and
                  Yibing Wang and
                  Mofan Cheng and
                  Shijia Yang and
                  Jiaming Han and
                  Benyou Wang and
                  Yutong Bai and
                  Zhuoran Yang and
                  Xiangyu Yue",2024,,,,CoRR
When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?,2511.10059v1,hall3,\cite{hall3},Detecting and Preventing Hallucinations in Large Vision Language Models,,,True,False,"Anisha Gunjal and
                  Jihan Yin and
                  Erhan Bas",2024,,,,
When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?,2511.10059v1,hall4,\cite{hall4},A Survey on Hallucination in Large Vision-Language Models,https://arxiv.org/abs/2402.00253v2,"Recent development of Large Vision-Language Models (LVLMs) has attracted growing attention within the AI landscape for its practical implementation potential. However, ``hallucination'', or more specifically, the misalignment between factual visual content and corresponding textual generation, poses a significant challenge of utilizing LVLMs. In this comprehensive survey, we dissect LVLM-related hallucinations in an attempt to establish an overview and facilitate future mitigation. Our scrutiny starts with a clarification of the concept of hallucinations in LVLMs, presenting a variety of hallucination symptoms and highlighting the unique challenges inherent in LVLM hallucinations. Subsequently, we outline the benchmarks and methodologies tailored specifically for evaluating hallucinations unique to LVLMs. Additionally, we delve into an investigation of the root causes of these hallucinations, encompassing insights from the training data and model components. We also critically review existing methods for mitigating hallucinations. The open questions and future directions pertaining to hallucinations within LVLMs are discussed to conclude this survey.",True,True,"Hanchao Liu and
                  Wenyuan Xue and
                  Yifei Chen and
                  Dapeng Chen and
                  Xiutian Zhao and
                  Ke Wang and
                  Liping Hou and
                  Rongjun Li and
                  Wei Peng",2024,,,,CoRR
When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?,2511.10059v1,shu2025semantics,\cite{shu2025semantics},When Semantics Mislead Vision: Mitigating Large Multimodal Models Hallucinations in Scene Text Spotting and Understanding,https://arxiv.org/abs/2506.05551v2,"Large Multimodal Models (LMMs) have achieved impressive progress in visual perception and reasoning. However, when confronted with visually ambiguous or non-semantic scene text, they often struggle to accurately spot and understand the content, frequently generating semantically plausible yet visually incorrect answers, which we refer to as semantic hallucination. In this work, we investigate the underlying causes of semantic hallucination and identify a key finding: Transformer layers in LLM with stronger attention focus on scene text regions are less prone to producing semantic hallucinations. Thus, we propose a training-free semantic hallucination mitigation framework comprising two key components: (1) ZoomText, a coarse-to-fine strategy that identifies potential text regions without external detectors; and (2) Grounded Layer Correction, which adaptively leverages the internal representations from layers less prone to hallucination to guide decoding, correcting hallucinated outputs for non-semantic samples while preserving the semantics of meaningful ones. To enable rigorous evaluation, we introduce TextHalu-Bench, a benchmark of 1,740 samples spanning both semantic and non-semantic cases, with manually curated question answer pairs designed to probe model hallucinations. Extensive experiments demonstrate that our method not only effectively mitigates semantic hallucination but also achieves strong performance on public benchmarks for scene text spotting and understanding.",True,True,"Shu, Yan and Lin, Hangui and Liu, Yexin and Zhang, Yan and Zeng, Gangyan and Li, Yan and Zhou, Yu and Lim, Ser-Nam and Yang, Harry and Sebe, Nicu",2025,,,,arXiv preprint arXiv:2506.05551
When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?,2511.10059v1,avhbench,\cite{avhbench},AVHBench: A Cross-Modal Hallucination Benchmark for Audio-Visual Large Language Models,https://arxiv.org/abs/2410.18325v2,"Following the success of Large Language Models (LLMs), expanding their boundaries to new modalities represents a significant paradigm shift in multimodal understanding. Human perception is inherently multimodal, relying not only on text but also on auditory and visual cues for a complete understanding of the world. In recognition of this fact, audio-visual LLMs have recently emerged. Despite promising developments, the lack of dedicated benchmarks poses challenges for understanding and evaluating models. In this work, we show that audio-visual LLMs struggle to discern subtle relationships between audio and visual signals, leading to hallucinations and highlighting the need for reliable benchmarks. To address this, we introduce AVHBench, the first comprehensive benchmark specifically designed to evaluate the perception and comprehension capabilities of audio-visual LLMs. Our benchmark includes tests for assessing hallucinations, as well as the cross-modal matching and reasoning abilities of these models. Our results reveal that most existing audio-visual LLMs struggle with hallucinations caused by cross-interactions between modalities, due to their limited capacity to perceive complex multimodal signals and their relationships. Additionally, we demonstrate that simple training with our AVHBench improves robustness of audio-visual LLMs against hallucinations. Dataset: https://github.com/kaist-ami/AVHBench",True,True,"Kim Sung{-}Bin and
                  Oh Hyun{-}Bin and
                  JungMok Lee and
                  Arda Senocak and
                  Joon Son Chung and
                  Tae{-}Hyun Oh",2025,,,,
When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?,2511.10059v1,rhlfff,\cite{rhlfff},Training language models to follow instructions with human feedback,https://arxiv.org/abs/2203.02155v1,"Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",True,True,"Long Ouyang and
                  Jeffrey Wu and
                  Xu Jiang and
                  Diogo Almeida and
                  Carroll L. Wainwright and
                  Pamela Mishkin and
                  Chong Zhang and
                  Sandhini Agarwal and
                  Katarina Slama and
                  Alex Ray and
                  John Schulman and
                  Jacob Hilton and
                  Fraser Kelton and
                  Luke Miller and
                  Maddie Simens and
                  Amanda Askell and
                  Peter Welinder and
                  Paul F. Christiano and
                  Jan Leike and
                  Ryan Lowe",2022,,,,
When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?,2511.10059v1,rhlf2,\cite{rhlf2},Interactive Learning from Policy-Dependent Human Feedback,https://arxiv.org/abs/1701.06049v2,"This paper investigates the problem of interactively learning behaviors communicated by a human teacher using positive and negative feedback. Much previous work on this problem has made the assumption that people provide feedback for decisions that is dependent on the behavior they are teaching and is independent from the learner's current policy. We present empirical results that show this assumption to be false -- whether human trainers give a positive or negative feedback for a decision is influenced by the learner's current policy. Based on this insight, we introduce {\em Convergent Actor-Critic by Humans} (COACH), an algorithm for learning from policy-dependent feedback that converges to a local optimum. Finally, we demonstrate that COACH can successfully learn multiple behaviors on a physical robot.",True,True,"James MacGlashan and
                  Mark K. Ho and
                  Robert Tyler Loftin and
                  Bei Peng and
                  Guan Wang and
                  David L. Roberts and
                  Matthew E. Taylor and
                  Michael L. Littman",2017,,,,
When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?,2511.10059v1,dpo,\cite{dpo},Direct Preference Optimization: Your Language Model is Secretly a Reward Model,https://arxiv.org/abs/2305.18290v3,"While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.",True,True,"Rafael Rafailov and
                  Archit Sharma and
                  Eric Mitchell and
                  Stefano Ermon and
                  Christopher D. Manning and
                  Chelsea Finn",2023,,,,CoRR
When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?,2511.10059v1,ppo,\cite{ppo},Proximal Policy Optimization Algorithms,https://arxiv.org/abs/1707.06347v2,"We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a ""surrogate"" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.",True,True,"John Schulman and
                  Filip Wolski and
                  Prafulla Dhariwal and
                  Alec Radford and
                  Oleg Klimov",2017,,,,CoRR
When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?,2511.10059v1,grpo,\cite{grpo},"DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open
                  Language Models",,,True,False,"Zhihong Shao and
                  Peiyi Wang and
                  Qihao Zhu and
                  Runxin Xu and
                  Junxiao Song and
                  Mingchuan Zhang and
                  Y. K. Li and
                  Y. Wu and
                  Daya Guo",2024,,,,CoRR
When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?,2511.10059v1,Liu2025SuperRLRL,\cite{Liu2025SuperRLRL},SuperRL: Reinforcement Learning with Supervision to Boost Language Model Reasoning,,,True,False,Yihao Liu and Shuocheng Li and Lang Cao and Yuhang Xie and Mengyu Zhou and Haoyu Dong and Xiaojun Ma and Shi Han and Dongmei Zhang,2025,,https://api.semanticscholar.org/CorpusID:279075961,,
When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?,2511.10059v1,sft,\cite{sft},Instruction Tuning for Large Language Models: {A} Survey,,,True,False,"Shengyu Zhang and
                  Linfeng Dong and
                  Xiaoya Li and
                  Sen Zhang and
                  Xiaofei Sun and
                  Shuhe Wang and
                  Jiwei Li and
                  Runyi Hu and
                  Tianwei Zhang and
                  Fei Wu and
                  Guoyin Wang",2023,,,,CoRR
When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?,2511.10059v1,qwenaudio,\cite{qwenaudio},Qwen2-Audio Technical Report,https://arxiv.org/abs/2407.10759v1,"We introduce the latest progress of Qwen-Audio, a large-scale audio-language model called Qwen2-Audio, which is capable of accepting various audio signal inputs and performing audio analysis or direct textual responses with regard to speech instructions. In contrast to complex hierarchical tags, we have simplified the pre-training process by utilizing natural language prompts for different data and tasks, and have further expanded the data volume. We have boosted the instruction-following capability of Qwen2-Audio and implemented two distinct audio interaction modes for voice chat and audio analysis. In the voice chat mode, users can freely engage in voice interactions with Qwen2-Audio without text input. In the audio analysis mode, users could provide audio and text instructions for analysis during the interaction. Note that we do not use any system prompts to switch between voice chat and audio analysis modes. Qwen2-Audio is capable of intelligently comprehending the content within audio and following voice commands to respond appropriately. For instance, in an audio segment that simultaneously contains sounds, multi-speaker conversations, and a voice command, Qwen2-Audio can directly understand the command and provide an interpretation and response to the audio. Additionally, DPO has optimized the model's performance in terms of factuality and adherence to desired behavior. According to the evaluation results from AIR-Bench, Qwen2-Audio outperformed previous SOTAs, such as Gemini-1.5-pro, in tests focused on audio-centric instruction-following capabilities. Qwen2-Audio is open-sourced with the aim of fostering the advancement of the multi-modal language community.",True,True,"Yunfei Chu and
                  Jin Xu and
                  Qian Yang and
                  et al.",2024,,,,CoRR
Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment,2511.10334v1,ho2025review,\cite{ho2025review},A Review on Vision-Language-Based Approaches: Challenges and Applications.,,,True,False,"Ho, Huu-Tuong and Nguyen, Luong Vuong and Pham, Minh-Tien and Pham, Quang-Huy and Tran, Quang-Duong and Huy, Duong Nguyen Minh and Nguyen, Tri-Hai",2025,,,,"Computers, Materials \& Continua"
Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment,2511.10334v1,radford2021learning,\cite{radford2021learning},Learning Transferable Visual Models From Natural Language Supervision,https://arxiv.org/abs/2103.00020v1,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",True,True,"Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",2021,,,,
Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment,2511.10334v1,jia2021scaling,\cite{jia2021scaling},Scaling up visual and vision-language representation learning with noisy text supervision,,,True,False,"Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom",2021,,,,
Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment,2511.10334v1,li2022blip,\cite{li2022blip},Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation,,,True,False,"Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven",2022,,,,
Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment,2511.10334v1,yu2022cocacontrastivecaptionersimagetext,\cite{yu2022cocacontrastivecaptionersimagetext},CoCa: Contrastive Captioners are Image-Text Foundation Models,,,True,False,Jiahui Yu and Zirui Wang and Vijay Vasudevan and Legg Yeung and Mojtaba Seyedhosseini and Yonghui Wu,2022,,https://arxiv.org/abs/2205.01917,,
Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment,2511.10334v1,alayrac2022flamingo,\cite{alayrac2022flamingo},Flamingo: a visual language model for few-shot learning,,,True,False,"Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others",2022,,,,Advances in neural information processing systems
Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment,2511.10334v1,li2023blip,\cite{li2023blip},BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models,https://arxiv.org/abs/2301.12597v3,"The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.",True,True,"Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven",2023,,,,
Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment,2511.10334v1,wang2024text,\cite{wang2024text},Text Is MASS: Modeling as Stochastic Embedding for Text-Video Retrieval,https://arxiv.org/abs/2403.17998v1,"The increasing prevalence of video clips has sparked growing interest in text-video retrieval. Recent advances focus on establishing a joint embedding space for text and video, relying on consistent embedding representations to compute similarity. However, the text content in existing datasets is generally short and concise, making it hard to fully describe the redundant semantics of a video. Correspondingly, a single text embedding may be less expressive to capture the video embedding and empower the retrieval. In this study, we propose a new stochastic text modeling method T-MASS, i.e., text is modeled as a stochastic embedding, to enrich text embedding with a flexible and resilient semantic range, yielding a text mass. To be specific, we introduce a similarity-aware radius module to adapt the scale of the text mass upon the given text-video pairs. Plus, we design and develop a support text regularization to further control the text mass during the training. The inference pipeline is also tailored to fully exploit the text mass for accurate retrieval. Empirical evidence suggests that T-MASS not only effectively attracts relevant text-video pairs while distancing irrelevant ones, but also enables the determination of precise text embeddings for relevant pairs. Our experimental results show a substantial improvement of T-MASS over baseline (3% to 6.3% by R@1). Also, T-MASS achieves state-of-the-art performance on five benchmark datasets, including MSRVTT, LSMDC, DiDeMo, VATEX, and Charades.",True,True,"Wang, Jiamian and Sun, Guohao and Wang, Pichao and Liu, Dongfang and Dianat, Sohail and Rabbani, Majid and Rao, Raghuveer and Tao, Zhiqiang",2024,,,,
Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment,2511.10334v1,tian2024holistic,\cite{tian2024holistic},Holistic features are almost sufficient for text-to-video retrieval,,,True,False,"Tian, Kaibin and Zhao, Ruixiang and Xin, Zijie and Lan, Bangxiang and Li, Xirong",2024,,,,
Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment,2511.10334v1,zou2024language,\cite{zou2024language},Language-aware visual semantic distillation for video question answering,,,True,False,"Zou, Bo and Yang, Chao and Qiao, Yu and Quan, Chengbin and Zhao, Youjian",2024,,,,
Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment,2511.10334v1,li2024configure,\cite{li2024configure},How to Configure Good In-Context Sequence for Visual Question Answering,https://arxiv.org/abs/2312.01571v1,"Inspired by the success of Large Language Models in dealing with new tasks via In-Context Learning (ICL) in NLP, researchers have also developed Large Vision-Language Models (LVLMs) with ICL capabilities. However, when implementing ICL using these LVLMs, researchers usually resort to the simplest way like random sampling to configure the in-context sequence, thus leading to sub-optimal results. To enhance the ICL performance, in this study, we use Visual Question Answering (VQA) as case study to explore diverse in-context configurations to find the powerful ones. Additionally, through observing the changes of the LVLM outputs by altering the in-context sequence, we gain insights into the inner properties of LVLMs, improving our understanding of them. Specifically, to explore in-context configurations, we design diverse retrieval methods and employ different strategies to manipulate the retrieved demonstrations. Through exhaustive experiments on three VQA datasets: VQAv2, VizWiz, and OK-VQA, we uncover three important inner properties of the applied LVLM and demonstrate which strategies can consistently improve the ICL VQA performance. Our code is provided in: https://github.com/GaryJiajia/OFv2_ICL_VQA.",True,True,"Li, Li and Peng, Jiawei and Chen, Huiyi and Gao, Chongyang and Yang, Xu",2024,,,,
Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment,2511.10334v1,huang2024frosterfrozenclipstrong,\cite{huang2024frosterfrozenclipstrong},FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition,https://arxiv.org/abs/2402.03241v1,"In this paper, we introduce FROSTER, an effective framework for open-vocabulary action recognition. The CLIP model has achieved remarkable success in a range of image-based tasks, benefiting from its strong generalization capability stemming from pretaining on massive image-text pairs. However, applying CLIP directly to the open-vocabulary action recognition task is challenging due to the absence of temporal information in CLIP's pretraining. Further, fine-tuning CLIP on action recognition datasets may lead to overfitting and hinder its generalizability, resulting in unsatisfactory results when dealing with unseen actions.
  To address these issues, FROSTER employs a residual feature distillation approach to ensure that CLIP retains its generalization capability while effectively adapting to the action recognition task. Specifically, the residual feature distillation treats the frozen CLIP model as a teacher to maintain the generalizability exhibited by the original CLIP and supervises the feature learning for the extraction of video-specific features to bridge the gap between images and videos. Meanwhile, it uses a residual sub-network for feature distillation to reach a balance between the two distinct objectives of learning generalizable and video-specific features.
  We extensively evaluate FROSTER on open-vocabulary action recognition benchmarks under both base-to-novel and cross-dataset settings. FROSTER consistently achieves state-of-the-art performance on all datasets across the board. Project page: https://visual-ai.github.io/froster.",True,True,Xiaohu Huang and Hao Zhou and Kun Yao and Kai Han,2024,,https://arxiv.org/abs/2402.03241,,
Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment,2511.10334v1,jia2023generatingactionconditionedpromptsopenvocabulary,\cite{jia2023generatingactionconditionedpromptsopenvocabulary},Generating Action-conditioned Prompts for Open-vocabulary Video Action Recognition,,,True,False,Chengyou Jia and Minnan Luo and Xiaojun Chang and Zhuohang Dang and Mingfei Han and Mengmeng Wang and Guang Dai and Sizhe Dang and Jingdong Wang,2023,,https://arxiv.org/abs/2312.02226,,
Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment,2511.10334v1,wu2024vadclip,\cite{wu2024vadclip},VadCLIP: Adapting Vision-Language Models for Weakly Supervised Video Anomaly Detection,https://arxiv.org/abs/2308.11681v3,"The recent contrastive language-image pre-training (CLIP) model has shown great success in a wide range of image-level tasks, revealing remarkable ability for learning powerful visual representations with rich semantics. An open and worthwhile problem is efficiently adapting such a strong model to the video domain and designing a robust video anomaly detector. In this work, we propose VadCLIP, a new paradigm for weakly supervised video anomaly detection (WSVAD) by leveraging the frozen CLIP model directly without any pre-training and fine-tuning process. Unlike current works that directly feed extracted features into the weakly supervised classifier for frame-level binary classification, VadCLIP makes full use of fine-grained associations between vision and language on the strength of CLIP and involves dual branch. One branch simply utilizes visual features for coarse-grained binary classification, while the other fully leverages the fine-grained language-image alignment. With the benefit of dual branch, VadCLIP achieves both coarse-grained and fine-grained video anomaly detection by transferring pre-trained knowledge from CLIP to WSVAD task. We conduct extensive experiments on two commonly-used benchmarks, demonstrating that VadCLIP achieves the best performance on both coarse-grained and fine-grained WSVAD, surpassing the state-of-the-art methods by a large margin. Specifically, VadCLIP achieves 84.51% AP and 88.02% AUC on XD-Violence and UCF-Crime, respectively. Code and features are released at https://github.com/nwpu-zxr/VadCLIP.",True,True,"Wu, Peng and Zhou, Xuerong and Pang, Guansong and Zhou, Lingru and Yan, Qingsen and Wang, Peng and Zhang, Yanning",2024,,,,
Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment,2511.10334v1,dev2024reflip,\cite{dev2024reflip},Reflip-vad: Towards weakly supervised video anomaly detection via vision-language model,,,True,False,"Dev, Prabhu Prasad and Hazari, Raju and Das, Pranesh",2024,,,,IEEE Transactions on Circuits and Systems for Video Technology
Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment,2511.10334v1,sultani2018real,\cite{sultani2018real},Real-world Anomaly Detection in Surveillance Videos,https://arxiv.org/abs/1801.04264v3,"Surveillance videos are able to capture a variety of realistic anomalies. In this paper, we propose to learn anomalies by exploiting both normal and anomalous videos. To avoid annotating the anomalous segments or clips in training videos, which is very time consuming, we propose to learn anomaly through the deep multiple instance ranking framework by leveraging weakly labeled training videos, i.e. the training labels (anomalous or normal) are at video-level instead of clip-level. In our approach, we consider normal and anomalous videos as bags and video segments as instances in multiple instance learning (MIL), and automatically learn a deep anomaly ranking model that predicts high anomaly scores for anomalous video segments. Furthermore, we introduce sparsity and temporal smoothness constraints in the ranking loss function to better localize anomaly during training. We also introduce a new large-scale first of its kind dataset of 128 hours of videos. It consists of 1900 long and untrimmed real-world surveillance videos, with 13 realistic anomalies such as fighting, road accident, burglary, robbery, etc. as well as normal activities. This dataset can be used for two tasks. First, general anomaly detection considering all anomalies in one group and all normal activities in another group. Second, for recognizing each of 13 anomalous activities. Our experimental results show that our MIL method for anomaly detection achieves significant improvement on anomaly detection performance as compared to the state-of-the-art approaches. We provide the results of several recent deep learning baselines on anomalous activity recognition. The low recognition performance of these baselines reveals that our dataset is very challenging and opens more opportunities for future work. The dataset is available at: https://webpages.uncc.edu/cchen62/dataset.html",True,True,"Sultani, Waqas and Chen, Chen and Shah, Mubarak",2018,,,,
Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment,2511.10334v1,tian2021weakly,\cite{tian2021weakly},Weakly-supervised video anomaly detection with robust temporal feature magnitude learning,,,True,False,"Tian, Yu and Pang, Guansong and Chen, Yuanhong and Singh, Rajvinder and Verjans, Johan W and Carneiro, Gustavo",2021,,,,
Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment,2511.10334v1,zanella2024harnessing,\cite{zanella2024harnessing},Harnessing large language models for training-free video anomaly detection,,,True,False,"Zanella, Luca and Menapace, Willi and Mancini, Massimiliano and Wang, Yiming and Ricci, Elisa",2024,,,,
Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment,2511.10334v1,zhang2025holmes,\cite{zhang2025holmes},Holmes-VAU: Towards Long-term Video Anomaly Understanding at Any Granularity,https://arxiv.org/abs/2412.06171v2,"How can we enable models to comprehend video anomalies occurring over varying temporal scales and contexts? Traditional Video Anomaly Understanding (VAU) methods focus on frame-level anomaly prediction, often missing the interpretability of complex and diverse real-world anomalies. Recent multimodal approaches leverage visual and textual data but lack hierarchical annotations that capture both short-term and long-term anomalies. To address this challenge, we introduce HIVAU-70k, a large-scale benchmark for hierarchical video anomaly understanding across any granularity. We develop a semi-automated annotation engine that efficiently scales high-quality annotations by combining manual video segmentation with recursive free-text annotation using large language models (LLMs). This results in over 70,000 multi-granular annotations organized at clip-level, event-level, and video-level segments. For efficient anomaly detection in long videos, we propose the Anomaly-focused Temporal Sampler (ATS). ATS integrates an anomaly scorer with a density-aware sampler to adaptively select frames based on anomaly scores, ensuring that the multimodal LLM concentrates on anomaly-rich regions, which significantly enhances both efficiency and accuracy. Extensive experiments demonstrate that our hierarchical instruction data markedly improves anomaly comprehension. The integrated ATS and visual-language model outperform traditional methods in processing long videos. Our benchmark and model are publicly available at https://github.com/pipixin321/HolmesVAU.",True,True,"Zhang, Huaxin and Xu, Xiaohao and Wang, Xiang and Zuo, Jialong and Huang, Xiaonan and Gao, Changxin and Zhang, Shanjun and Yu, Li and Sang, Nong",2025,,,,
Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment,2511.10334v1,lv2023unbiased,\cite{lv2023unbiased},Unbiased multiple instance learning for weakly supervised video anomaly detection,,,True,False,"Lv, Hui and Yue, Zhongqi and Sun, Qianru and Luo, Bin and Cui, Zhen and Zhang, Hanwang",2023,,,,
Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment,2511.10334v1,feng2021mist,\cite{feng2021mist},MIST: Multiple Instance Self-Training Framework for Video Anomaly Detection,https://arxiv.org/abs/2104.01633v1,"Weakly supervised video anomaly detection (WS-VAD) is to distinguish anomalies from normal events based on discriminative representations. Most existing works are limited in insufficient video representations. In this work, we develop a multiple instance self-training framework (MIST)to efficiently refine task-specific discriminative representations with only video-level annotations. In particular, MIST is composed of 1) a multiple instance pseudo label generator, which adapts a sparse continuous sampling strategy to produce more reliable clip-level pseudo labels, and 2) a self-guided attention boosted feature encoder that aims to automatically focus on anomalous regions in frames while extracting task-specific representations. Moreover, we adopt a self-training scheme to optimize both components and finally obtain a task-specific feature encoder. Extensive experiments on two public datasets demonstrate the efficacy of our method, and our method performs comparably to or even better than existing supervised and weakly supervised methods, specifically obtaining a frame-level AUC 94.83% on ShanghaiTech.",True,True,"Feng, Jia-Chang and Hong, Fa-Ting and Zheng, Wei-Shi",2021,,,,
Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment,2511.10334v1,joo2023clip,\cite{joo2023clip},CLIP-TSA: CLIP-Assisted Temporal Self-Attention for Weakly-Supervised Video Anomaly Detection,https://arxiv.org/abs/2212.05136v3,"Video anomaly detection (VAD) -- commonly formulated as a multiple-instance learning problem in a weakly-supervised manner due to its labor-intensive nature -- is a challenging problem in video surveillance where the frames of anomaly need to be localized in an untrimmed video. In this paper, we first propose to utilize the ViT-encoded visual features from CLIP, in contrast with the conventional C3D or I3D features in the domain, to efficiently extract discriminative representations in the novel technique. We then model temporal dependencies and nominate the snippets of interest by leveraging our proposed Temporal Self-Attention (TSA). The ablation study confirms the effectiveness of TSA and ViT feature. The extensive experiments show that our proposed CLIP-TSA outperforms the existing state-of-the-art (SOTA) methods by a large margin on three commonly-used benchmark datasets in the VAD problem (UCF-Crime, ShanghaiTech Campus, and XD-Violence). Our source code is available at https://github.com/joos2010kj/CLIP-TSA.",True,True,"Joo, Hyekang Kevin and Vo, Khoa and Yamazaki, Kashu and Le, Ngan",2023,,,,
Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment,2511.10334v1,pu2024learning,\cite{pu2024learning},Learning Prompt-Enhanced Context Features for Weakly-Supervised Video Anomaly Detection,https://arxiv.org/abs/2306.14451v2,"Video anomaly detection under weak supervision presents significant challenges, particularly due to the lack of frame-level annotations during training. While prior research has utilized graph convolution networks and self-attention mechanisms alongside multiple instance learning (MIL)-based classification loss to model temporal relations and learn discriminative features, these methods often employ multi-branch architectures to capture local and global dependencies separately, resulting in increased parameters and computational costs. Moreover, the coarse-grained interclass separability provided by the binary constraint of MIL-based loss neglects the fine-grained discriminability within anomalous classes. In response, this paper introduces a weakly supervised anomaly detection framework that focuses on efficient context modeling and enhanced semantic discriminability. We present a Temporal Context Aggregation (TCA) module that captures comprehensive contextual information by reusing the similarity matrix and implementing adaptive fusion. Additionally, we propose a Prompt-Enhanced Learning (PEL) module that integrates semantic priors using knowledge-based prompts to boost the discriminative capacity of context features while ensuring separability between anomaly sub-classes. Extensive experiments validate the effectiveness of our method's components, demonstrating competitive performance with reduced parameters and computational effort on three challenging benchmarks: UCF-Crime, XD-Violence, and ShanghaiTech datasets. Notably, our approach significantly improves the detection accuracy of certain anomaly sub-classes, underscoring its practical value and efficacy. Our code is available at: https://github.com/yujiangpu20/PEL4VAD.",True,True,"Pu, Yujiang and Wu, Xiaoyu and Yang, Lulu and Wang, Shengjin",2024,,,,IEEE Transactions on Image Processing
Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment,2511.10334v1,liu2024injecting,\cite{liu2024injecting},Injecting text clues for improving anomalous event detection from weakly labeled videos,,,True,False,"Liu, Tianshan and Lam, Kin-Man and Bao, Bing-Kun",2024,,,,IEEE Transactions on Image Processing
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Lim:2004aa,\cite{Lim:2004aa},The construction of domain ontology and its application to document retrieval,,,True,False,"Lim, Soo-Yeon and Song, Mu-Hee and Lee, Sang-Jo",2004,,,,International Conference on Advances in Information Systems
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Chen:2012aa,\cite{Chen:2012aa},A recommendation system based on domain ontology and SWRL for anti-diabetic drugs selection,,,True,False,"Chen, Rung-Ching and Huang, Yun-Hou and Bau, Cho-Tsan and Chen, Shyi-Ming",2012,,,,Expert Systems with Applications
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Chen:2019aa,\cite{Chen:2019aa},Bidirectional attentive memory networks for question answering over knowledge bases,,,True,False,"Chen, Yu and Wu, Lingfei and Zaki, Mohammed J",2019,,,,arXiv preprint arXiv:1903.02188
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Chen:2006aa,\cite{Chen:2006aa},Colorcocktail: an ontology-based recommender system,,,True,False,"Chen, Yu-Hsin and Huang, Ting-hsiang and Hsu, David Chawei and Hsu, Jane Yung-jen",2006,,,,"Proceedings of 20th American Association for Artificial Intelligence. Menlo Park, USA: AAAI Press"
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Al:2019aa,\cite{Al:2019aa},Ontology-Based Food Recommendation System for Seniors: A Peer To Peer Networking Approach,,,True,False,"Al, Ahlam and Fiaidhi, Jinan and Mohammed, Sabah",2019,02,,10.33832/ijast.2019.123.05,International Journal of Advanced Science and Technology
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Haussmann:2019aa,\cite{Haussmann:2019aa},FoodKG: a semantics-driven knowledge graph for food recommendation,,,True,False,"Haussmann, Steven and Seneviratne, Oshani and Chen, Yu and Ne'eman, Yarden and Codella, James and Chen, Ching-Hua and McGuinness, Deborah L and Zaki, Mohammed J",2019,,,,International Semantic Web Conference
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Oliveira:2021aa,\cite{Oliveira:2021aa},Wine ontology influence in a recommendation system,,,True,False,"Oliveira, Lu{\'\i}s and Rocha Silva, Rodrigo and Bernardino, Jorge",2021,,,,Big Data and Cognitive Computing
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Showafah:2021aa,\cite{Showafah:2021aa},"Ontology-based daily menu recommendation system for complementary food according to nutritional needs using na{\""\i}ve Bayes and TOPSIS",,,True,False,"Showafah, Mujahidah and Sihwi, Sari Widya",2021,,,,International Journal of Advanced Computer Science and Applications
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Chen:2023aa,\cite{Chen:2023aa},Health-aware food recommendation based on knowledge graph and multi-task learning,,,True,False,"Chen, Yi and Guo, Yandi and Fan, Qiuxu and Zhang, Qinghui and Dong, Yu",2023,,,,Foods
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Gawrysiak:2024aa,\cite{Gawrysiak:2024aa},WineGraph: A Graph Representation for Food-Wine Pairing,,,True,False,"Gawrysiak, Zuzanna and {\.Z}ywot, Agata and {\L}awrynowicz, Agnieszka",2024,,,,International Conference on Neural-Symbolic Learning and Reasoning
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Han:2024aa,\cite{Han:2024aa},Retrieval-augmented generation with graphs (graphrag),,,True,False,"Han, Haoyu and Wang, Yu and Shomer, Harry and Guo, Kai and Ding, Jiayuan and Lei, Yongjia and Halappanavar, Mahantesh and Rossi, Ryan A and Mukherjee, Subhabrata and Tang, Xianfeng",2024,,,,arXiv preprint arXiv:2501.00309
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Hu2024GRAGGR,\cite{Hu2024GRAGGR},GRAG: Graph Retrieval-Augmented Generation,https://arxiv.org/abs/2405.16506v3,"Naive Retrieval-Augmented Generation (RAG) focuses on individual documents during retrieval and, as a result, falls short in handling networked documents which are very popular in many applications such as citation graphs, social media, and knowledge graphs. To overcome this limitation, we introduce Graph Retrieval-Augmented Generation (GRAG), which tackles the fundamental challenges in retrieving textual subgraphs and integrating the joint textual and topological information into Large Language Models (LLMs) to enhance its generation. To enable efficient textual subgraph retrieval, we propose a novel divide-and-conquer strategy that retrieves the optimal subgraph structure in linear time. To achieve graph context-aware generation, incorporate textual graphs into LLMs through two complementary views-the text view and the graph view-enabling LLMs to more effectively comprehend and utilize the graph context. Extensive experiments on graph reasoning benchmarks demonstrate that in scenarios requiring multi-hop reasoning on textual graphs, our GRAG approach significantly outperforms current state-of-the-art RAG methods. Our datasets as well as codes of GRAG are available at https://github.com/HuieL/GRAG.",True,True,Yuntong Hu and Zhihan Lei and Zhengwu Zhang and Bo Pan and Chen Ling and Liang Zhao,2024,,https://api.semanticscholar.org/CorpusID:270062608,,
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Shen:2024aa,\cite{Shen:2024aa},Gear: Graph-enhanced agent for retrieval-augmented generation,,,True,False,"Shen, Zhili and Diao, Chenxin and Vougiouklis, Pavlos and Merita, Pascual and Piramanayagam, Shriram and Chen, Enting and Graux, Damien and Melo, Andre and Lai, Ruofei and Jiang, Zeren",2024,,,,arXiv preprint arXiv:2412.18431
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Xiang:2025aa,\cite{Xiang:2025aa},When to use Graphs in RAG: A Comprehensive Analysis for Graph Retrieval-Augmented Generation,https://arxiv.org/abs/2506.05690v2,"Graph retrieval-augmented generation (GraphRAG) has emerged as a powerful paradigm for enhancing large language models (LLMs) with external knowledge. It leverages graphs to model the hierarchical structure between specific concepts, enabling more coherent and effective knowledge retrieval for accurate reasoning.Despite its conceptual promise, recent studies report that GraphRAG frequently underperforms vanilla RAG on many real-world tasks. This raises a critical question: Is GraphRAG really effective, and in which scenarios do graph structures provide measurable benefits for RAG systems? To address this, we propose GraphRAG-Bench, a comprehensive benchmark designed to evaluate GraphRAG models onboth hierarchical knowledge retrieval and deep contextual reasoning. GraphRAG-Bench features a comprehensive dataset with tasks of increasing difficulty, coveringfact retrieval, complex reasoning, contextual summarization, and creative generation, and a systematic evaluation across the entire pipeline, from graph constructionand knowledge retrieval to final generation. Leveraging this novel benchmark, we systematically investigate the conditions when GraphRAG surpasses traditional RAG and the underlying reasons for its success, offering guidelines for its practical application. All related resources and analyses are collected for the community at https://github.com/GraphRAG-Bench/GraphRAG-Benchmark.",True,True,"Xiang, Zhishang and Wu, Chuanjie and Zhang, Qinggang and Chen, Shengyuan and Hong, Zijin and Huang, Xiao and Su, Jinsong",2025,,,,arXiv preprint arXiv:2506.05690
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Agrawal:2025aa,\cite{Agrawal:2025aa},Query-Aware Graph Neural Networks for Enhanced Retrieval-Augmented Generation,https://arxiv.org/abs/2508.05647v1,"We present a novel graph neural network (GNN) architecture for retrieval-augmented generation (RAG) that leverages query-aware attention mechanisms and learned scoring heads to improve retrieval accuracy on complex, multi-hop questions. Unlike traditional dense retrieval methods that treat documents as independent entities, our approach constructs per-episode knowledge graphs that capture both sequential and semantic relationships between text chunks. We introduce an Enhanced Graph Attention Network with query-guided pooling that dynamically focuses on relevant parts of the graph based on user queries. Experimental results demonstrate that our approach significantly outperforms standard dense retrievers on complex question answering tasks, particularly for questions requiring multi-document reasoning. Our implementation leverages PyTorch Geometric for efficient processing of graph-structured data, enabling scalable deployment in production retrieval systems",True,True,"Agrawal, Vibhor and Wang, Fay and Puri, Rishi",2025,,,,arXiv preprint arXiv:2508.05647
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Liang:2025aa,\cite{Liang:2025aa},GeoGraphRAG: A graph-based retrieval-augmented generation approach for empowering large language models in automated geospatial modeling,,,True,False,"Liang, Jianyuan and Hou, Shuyang and Jiao, Haoyue and Qing, Yaxian and Zhao, Anqi and Shen, Zhangxiao and Xiang, Longgang and Wu, Huayi",2025,,,,International Journal of Applied Earth Observation and Geoinformation
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Resnick:1994aa,\cite{Resnick:1994aa},Grouplens: An open architecture for collaborative filtering of netnews,,,True,False,"Resnick, Paul and Iacovou, Neophytos and Suchak, Mitesh and Bergstrom, Peter and Riedl, John",1994,,,,Proceedings of the 1994 ACM conference on Computer supported cooperative work
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Sarwar:2001aa,\cite{Sarwar:2001aa},Item-based collaborative filtering recommendation algorithms,,,True,False,"Sarwar, Badrul and Karypis, George and Konstan, Joseph and Riedl, John",2001,,,,Proceedings of the 10th international conference on World Wide Web
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Koren:2009aa,\cite{Koren:2009aa},Matrix factorization techniques for recommender systems,,,True,False,"Koren, Yehuda and Bell, Robert and Volinsky, Chris",2009,,,,Computer
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Covington:2016aa,\cite{Covington:2016aa},Deep neural networks for youtube recommendations,,,True,False,"Covington, Paul and Adams, Jay and Sargin, Emre",2016,,,,Proceedings of the 10th ACM conference on recommender systems
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Wang:2023aa,\cite{Wang:2023aa},Zero-Shot Next-Item Recommendation using Large Pretrained Language Models,https://arxiv.org/abs/2304.03153v1,"Large language models (LLMs) have achieved impressive zero-shot performance in various natural language processing (NLP) tasks, demonstrating their capabilities for inference without training examples. Despite their success, no research has yet explored the potential of LLMs to perform next-item recommendations in the zero-shot setting. We have identified two major challenges that must be addressed to enable LLMs to act effectively as recommenders. First, the recommendation space can be extremely large for LLMs, and LLMs do not know about the target user's past interacted items and preferences. To address this gap, we propose a prompting strategy called Zero-Shot Next-Item Recommendation (NIR) prompting that directs LLMs to make next-item recommendations. Specifically, the NIR-based strategy involves using an external module to generate candidate items based on user-filtering or item-filtering. Our strategy incorporates a 3-step prompting that guides GPT-3 to carry subtasks that capture the user's preferences, select representative previously watched movies, and recommend a ranked list of 10 movies. We evaluate the proposed approach using GPT-3 on MovieLens 100K dataset and show that it achieves strong zero-shot performance, even outperforming some strong sequential recommendation models trained on the entire training dataset. These promising results highlight the ample research opportunities to use LLMs as recommenders. The code can be found at https://github.com/AGI-Edgerunners/LLM-Next-Item-Rec.",True,True,"Wang, Lei and Lim, Ee-Peng",2023,,,,arXiv preprint arXiv:2304.03153
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Lyu:2023aa,\cite{Lyu:2023aa},LLM-Rec: Personalized Recommendation via Prompting Large Language Models,https://arxiv.org/abs/2307.15780v3,"Text-based recommendation holds a wide range of practical applications due to its versatility, as textual descriptions can represent nearly any type of item. However, directly employing the original item descriptions may not yield optimal recommendation performance due to the lack of comprehensive information to align with user preferences. Recent advances in large language models (LLMs) have showcased their remarkable ability to harness commonsense knowledge and reasoning. In this study, we introduce a novel approach, coined LLM-Rec, which incorporates four distinct prompting strategies of text enrichment for improving personalized text-based recommendations. Our empirical experiments reveal that using LLM-augmented text significantly enhances recommendation quality. Even basic MLP (Multi-Layer Perceptron) models achieve comparable or even better results than complex content-based methods. Notably, the success of LLM-Rec lies in its prompting strategies, which effectively tap into the language model's comprehension of both general and specific item characteristics. This highlights the importance of employing diverse prompts and input augmentation techniques to boost the recommendation effectiveness of LLMs.",True,True,"Lyu, Hanjia and Jiang, Song and Zeng, Hanqing and Xia, Yinglong and Wang, Qifan and Zhang, Si and Chen, Ren and Leung, Christopher and Tang, Jiajie and Luo, Jiebo",2023,,,,arXiv preprint arXiv:2307.15780
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Tian:2024aa,\cite{Tian:2024aa},MMREC: LLM Based Multi-Modal Recommender System,https://arxiv.org/abs/2408.04211v2,"The importance of recommender systems is growing rapidly due to the exponential increase in the volume of content generated daily. This surge in content presents unique challenges for designing effective recommender systems. Key among these challenges is the need to effectively leverage the vast amounts of natural language data and images that represent user preferences. This paper presents a novel approach to enhancing recommender systems by leveraging Large Language Models (LLMs) and deep learning techniques. The proposed framework aims to improve the accuracy and relevance of recommendations by incorporating multi-modal information processing and by the use of unified latent space representation. The study explores the potential of LLMs to better understand and utilize natural language data in recommendation contexts, addressing the limitations of previous methods. The framework efficiently extracts and integrates text and image information through LLMs, unifying diverse modalities in a latent space to simplify the learning process for the ranking model. Experimental results demonstrate the enhanced discriminative power of the model when utilizing multi-modal information. This research contributes to the evolving field of recommender systems by showcasing the potential of LLMs and multi-modal data integration to create more personalized and contextually relevant recommendations.",True,True,"Tian, Jiahao and Wang, Zhenkai and Zhao, Jinman and Ding, Zhicheng",2024,,,,2024 19th International Workshop on Semantic and Social Media Adaptation \& Personalization (SMAP)
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Wang:2025aa,\cite{Wang:2025aa},Knowledge graph retrieval-augmented generation for llm-based recommendation,,,True,False,"Wang, Shijie and Fan, Wenqi and Feng, Yue and Lin, Shanru and Ma, Xinyu and Wang, Shuaiqiang and Yin, Dawei",2025,,,,arXiv preprint arXiv:2501.02226
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Patil:2024aa,\cite{Patil:2024aa},Gorilla: Large Language Model Connected with Massive APIs,https://arxiv.org/abs/2305.15334v1,"Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model's ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla's code, model, data, and demo are available at https://gorilla.cs.berkeley.edu",True,True,"Patil, Shishir G and Zhang, Tianjun and Wang, Xin and Gonzalez, Joseph E",2024,,,,Advances in Neural Information Processing Systems
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Peng:2025aa,\cite{Peng:2025aa},A survey on llm-powered agents for recommender systems,,,True,False,"Peng, Qiyao and Liu, Hongtao and Huang, Hua and Yang, Qing and Shao, Minglai",2025,,,,arXiv preprint arXiv:2502.10050
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Wang:2023ab,\cite{Wang:2023ab},RecMind: Large Language Model Powered Agent For Recommendation,https://arxiv.org/abs/2308.14296v3,"While the recommendation system (RS) has advanced significantly through deep learning, current RS approaches usually train and fine-tune models on task-specific datasets, limiting their generalizability to new recommendation tasks and their ability to leverage external knowledge due to model scale and data size constraints. Thus, we designed an LLM-powered autonomous recommender agent, RecMind, which is capable of leveraging external knowledge, utilizing tools with careful planning to provide zero-shot personalized recommendations. We propose a Self-Inspiring algorithm to improve the planning ability. At each intermediate step, the LLM self-inspires to consider all previously explored states to plan for the next step. This mechanism greatly improves the model's ability to comprehend and utilize historical information in planning for recommendation. We evaluate RecMind's performance in various recommendation scenarios. Our experiment shows that RecMind outperforms existing zero/few-shot LLM-based recommendation baseline methods in various tasks and achieves comparable performance to a fully trained recommendation model P5.",True,True,"Wang, Yancheng and Jiang, Ziyan and Chen, Zheng and Yang, Fan and Zhou, Yingxue and Cho, Eunah and Fan, Xing and Huang, Xiaojiang and Lu, Yanbin and Yang, Yingzhen",2023,,,,arXiv preprint arXiv:2308.14296
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Shi:2024aa,\cite{Shi:2024aa},Large language models are learnable planners for long-term recommendation,,,True,False,"Shi, Wentao and He, Xiangnan and Zhang, Yang and Gao, Chongming and Li, Xinyue and Zhang, Jizhi and Wang, Qifan and Feng, Fuli",2024,,,,Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Zhao:2024aa,\cite{Zhao:2024aa},Let me do it for you: Towards llm empowered recommendation via tool learning,,,True,False,"Zhao, Yuyue and Wu, Jiancan and Wang, Xiang and Tang, Wei and Wang, Dingxian and De Rijke, Maarten",2024,,,,Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Shu:2024aa,\cite{Shu:2024aa},RAH! RecSys-Assistant-Human: A Human-Centered Recommendation Framework with LLM Agents,https://arxiv.org/abs/2308.09904v2,"The rapid evolution of the web has led to an exponential growth in content. Recommender systems play a crucial role in Human-Computer Interaction (HCI) by tailoring content based on individual preferences. Despite their importance, challenges persist in balancing recommendation accuracy with user satisfaction, addressing biases while preserving user privacy, and solving cold-start problems in cross-domain situations. This research argues that addressing these issues is not solely the recommender systems' responsibility, and a human-centered approach is vital. We introduce the RAH Recommender system, Assistant, and Human) framework, an innovative solution with LLM-based agents such as Perceive, Learn, Act, Critic, and Reflect, emphasizing the alignment with user personalities. The framework utilizes the Learn-Act-Critic loop and a reflection mechanism for improving user alignment. Using the real-world data, our experiments demonstrate the RAH framework's efficacy in various recommendation domains, from reducing human burden to mitigating biases and enhancing user control. Notably, our contributions provide a human-centered recommendation framework that partners effectively with various recommendation models.",True,True,"Shu, Yubo and Zhang, Haonan and Gu, Hansu and Zhang, Peng and Lu, Tun and Li, Dongsheng and Gu, Ning",2024,,,,IEEE Transactions on Computational Social Systems
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Zeng:2024aa,\cite{Zeng:2024aa},Automated Interactive Domain-Specific Conversational Agents that Understand Human Dialogs,https://arxiv.org/abs/2303.08941v2,"Achieving human-like communication with machines remains a classic, challenging topic in the field of Knowledge Representation and Reasoning and Natural Language Processing. These Large Language Models (LLMs) rely on pattern-matching rather than a true understanding of the semantic meaning of a sentence. As a result, they may generate incorrect responses. To generate an assuredly correct response, one has to ""understand"" the semantics of a sentence. To achieve this ""understanding"", logic-based (commonsense) reasoning methods such as Answer Set Programming (ASP) are arguably needed. In this paper, we describe the AutoConcierge system that leverages LLMs and ASP to develop a conversational agent that can truly ""understand"" human dialogs in restricted domains. AutoConcierge is focused on a specific domain-advising users about restaurants in their local area based on their preferences. AutoConcierge will interactively understand a user's utterances, identify the missing information in them, and request the user via a natural language sentence to provide it. Once AutoConcierge has determined that all the information has been received, it computes a restaurant recommendation based on the user-preferences it has acquired from the human user. AutoConcierge is based on our STAR framework developed earlier, which uses GPT-3 to convert human dialogs into predicates that capture the deep structure of the dialog's sentence. These predicates are then input into the goal-directed s(CASP) ASP system for performing commonsense reasoning. To the best of our knowledge, AutoConcierge is the first automated conversational agent that can realistically converse like a human and provide help to humans based on truly understanding human utterances.",True,True,"Zeng, Yankai and Rajasekharan, Abhiramon and Padalkar, Parth and Basu, Kinjal and Arias, Joaqu{\'\i}n and Gupta, Gopal",2024,,,,International Symposium on Practical Aspects of Declarative Languages
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Huang:2025aa,\cite{Huang:2025aa},Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations,https://arxiv.org/abs/2308.16505v3,"Recommender models excel at providing domain-specific item recommendations by leveraging extensive user behavior data. Despite their ability to act as lightweight domain experts, they struggle to perform versatile tasks such as providing explanations and engaging in conversations. On the other hand, large language models (LLMs) represent a significant step towards artificial general intelligence, showcasing remarkable capabilities in instruction comprehension, commonsense reasoning, and human interaction. However, LLMs lack the knowledge of domain-specific item catalogs and behavioral patterns, particularly in areas that diverge from general world knowledge, such as online e-commerce. Finetuning LLMs for each domain is neither economic nor efficient.
  In this paper, we bridge the gap between recommender models and LLMs, combining their respective strengths to create a versatile and interactive recommender system. We introduce an efficient framework called \textbf{InteRecAgent}, which employs LLMs as the brain and recommender models as tools. We first outline a minimal set of essential tools required to transform LLMs into InteRecAgent. We then propose an efficient workflow within InteRecAgent for task execution, incorporating key components such as memory components, dynamic demonstration-augmented task planning, and reflection. InteRecAgent enables traditional recommender systems, such as those ID-based matrix factorization models, to become interactive systems with a natural language interface through the integration of LLMs. Experimental results on several public datasets show that InteRecAgent achieves satisfying performance as a conversational recommender system, outperforming general-purpose LLMs. The source code of InteRecAgent is released at https://aka.ms/recagent.",True,True,"Huang, Xu and Lian, Jianxun and Lei, Yuxuan and Yao, Jing and Lian, Defu and Xie, Xing",2025,,,,ACM Transactions on Information Systems
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Zhang:2024aa,\cite{Zhang:2024aa},On Generative Agents in Recommendation,https://arxiv.org/abs/2310.10108v3,"Recommender systems are the cornerstone of today's information dissemination, yet a disconnect between offline metrics and online performance greatly hinders their development. Addressing this challenge, we envision a recommendation simulator, capitalizing on recent breakthroughs in human-level intelligence exhibited by Large Language Models (LLMs). We propose Agent4Rec, a user simulator in recommendation, leveraging LLM-empowered generative agents equipped with user profile, memory, and actions modules specifically tailored for the recommender system. In particular, these agents' profile modules are initialized using real-world datasets (e.g. MovieLens, Steam, Amazon-Book), capturing users' unique tastes and social traits; memory modules log both factual and emotional memories and are integrated with an emotion-driven reflection mechanism; action modules support a wide variety of behaviors, spanning both taste-driven and emotion-driven actions. Each agent interacts with personalized recommender models in a page-by-page manner, relying on a pre-implemented collaborative filtering-based recommendation algorithm. We delve into both the capabilities and limitations of Agent4Rec, aiming to explore an essential research question: ``To what extent can LLM-empowered generative agents faithfully simulate the behavior of real, autonomous humans in recommender systems?'' Extensive and multi-faceted evaluations of Agent4Rec highlight both the alignment and deviation between agents and user-personalized preferences. Beyond mere performance comparison, we explore insightful experiments, such as emulating the filter bubble effect and discovering the underlying causal relationships in recommendation tasks. Our codes are available at https://github.com/LehengTHU/Agent4Rec.",True,True,"Zhang, An and Chen, Yuxin and Sheng, Leheng and Wang, Xiang and Chua, Tat-Seng",2024,,,,Proceedings of the 47th international ACM SIGIR conference on research and development in Information Retrieval
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Zhang:2024ab,\cite{Zhang:2024ab},Agentcf: Collaborative learning with autonomous language agents for recommender systems,,,True,False,"Zhang, Junjie and Hou, Yupeng and Xie, Ruobing and Sun, Wenqi and McAuley, Julian and Zhao, Wayne Xin and Lin, Leyu and Wen, Ji-Rong",2024,,,,Proceedings of the ACM Web Conference 2024
MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System,2511.08181v1,Guo:2024aa,\cite{Guo:2024aa},Knowledge Graph Enhanced Language Agents for Recommendation,https://arxiv.org/abs/2410.19627v2,"Language agents have recently been used to simulate human behavior and user-item interactions for recommendation systems. However, current language agent simulations do not understand the relationships between users and items, leading to inaccurate user profiles and ineffective recommendations. In this work, we explore the utility of Knowledge Graphs (KGs), which contain extensive and reliable relationships between users and items, for recommendation. Our key insight is that the paths in a KG can capture complex relationships between users and items, eliciting the underlying reasons for user preferences and enriching user profiles. Leveraging this insight, we propose Knowledge Graph Enhanced Language Agents(KGLA), a framework that unifies language agents and KG for recommendation systems. In the simulated recommendation scenario, we position the user and item within the KG and integrate KG paths as natural language descriptions into the simulation. This allows language agents to interact with each other and discover sufficient rationale behind their interactions, making the simulation more accurate and aligned with real-world cases, thus improving recommendation performance. Our experimental results show that KGLA significantly improves recommendation performance (with a 33%-95% boost in NDCG@1 among three widely used benchmarks) compared to the previous best baseline method.",True,True,"Guo, Taicheng and Liu, Chaochun and Wang, Hai and Mannam, Varun and Wang, Fang and Chen, Xin and Zhang, Xiangliang and Reddy, Chandan K",2024,,,,arXiv preprint arXiv:2410.19627
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,mou2024salient,\cite{mou2024salient},Salient Object Detection in RGB-D Videos,https://arxiv.org/abs/2310.15482v2,"Given the widespread adoption of depth-sensing acquisition devices, RGB-D videos and related data/media have gained considerable traction in various aspects of daily life. Consequently, conducting salient object detection (SOD) in RGB-D videos presents a highly promising and evolving avenue. Despite the potential of this area, SOD in RGB-D videos remains somewhat under-explored, with RGB-D SOD and video SOD (VSOD) traditionally studied in isolation. To explore this emerging field, this paper makes two primary contributions: the dataset and the model. On one front, we construct the RDVS dataset, a new RGB-D VSOD dataset with realistic depth and characterized by its diversity of scenes and rigorous frame-by-frame annotations. We validate the dataset through comprehensive attribute and object-oriented analyses, and provide training and testing splits. Moreover, we introduce DCTNet+, a three-stream network tailored for RGB-D VSOD, with an emphasis on RGB modality and treats depth and optical flow as auxiliary modalities. In pursuit of effective feature enhancement, refinement, and fusion for precise final prediction, we propose two modules: the multi-modal attention module (MAM) and the refinement fusion module (RFM). To enhance interaction and fusion within RFM, we design a universal interaction module (UIM) and then integrate holistic multi-modal attentive paths (HMAPs) for refining multi-modal low-level features before reaching RFMs. Comprehensive experiments, conducted on pseudo RGB-D video datasets alongside our RDVS, highlight the superiority of DCTNet+ over 17 VSOD models and 14 RGB-D SOD models. Ablation experiments were performed on both pseudo and realistic RGB-D video datasets to demonstrate the advantages of individual modules as well as the necessity of introducing realistic depth. Our code together with RDVS dataset will be available at https://github.com/kerenfu/RDVS/.",True,True,"Mou, Ao and Lu, Yukang and He, Jiahao and Min, Dingyao and Fu, Keren and Zhao, Qijun",2024,,,,IEEE Transactions on Image Processing
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,qu2017rgbd,\cite{qu2017rgbd},RGBD Salient Object Detection via Deep Fusion,https://arxiv.org/abs/1607.03333v1,"Numerous efforts have been made to design different low level saliency cues for the RGBD saliency detection, such as color or depth contrast features, background and color compactness priors. However, how these saliency cues interact with each other and how to incorporate these low level saliency cues effectively to generate a master saliency map remain a challenging problem. In this paper, we design a new convolutional neural network (CNN) to fuse different low level saliency cues into hierarchical features for automatically detecting salient objects in RGBD images. In contrast to the existing works that directly feed raw image pixels to the CNN, the proposed method takes advantage of the knowledge in traditional saliency detection by adopting various meaningful and well-designed saliency feature vectors as input. This can guide the training of CNN towards detecting salient object more effectively due to the reduced learning ambiguity. We then integrate a Laplacian propagation framework with the learned CNN to extract a spatially consistent saliency map by exploiting the intrinsic structure of the input image. Extensive quantitative and qualitative experimental evaluations on three datasets demonstrate that the proposed method consistently outperforms state-of-the-art methods.",True,True,"Qu, Liangqiong and He, Shengfeng and Zhang, Jiawei and Tian, Jiandong and Tang, Yandong and Yang, Qingxiong",2017,,,,IEEE transactions on image processing
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,han2017cnns,\cite{han2017cnns},CNNs-based RGB-D saliency detection via cross-view transfer and multiview fusion,,,True,False,"Han, Junwei and Chen, Hao and Liu, Nian and Yan, Chenggang and Li, Xuelong",2017,,,,IEEE transactions on cybernetics
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,cong2023point,\cite{cong2023point},Point-aware interaction and cnn-induced refinement network for RGB-D salient object detection,,,True,False,"Cong, Runmin and Liu, Hongyu and Zhang, Chen and Zhang, Wei and Zheng, Feng and Song, Ran and Kwong, Sam",2023,,,,
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,bao2024quality,\cite{bao2024quality},Quality-aware Selective Fusion Network for V-D-T Salient Object Detection,https://arxiv.org/abs/2405.07655v1,"Depth images and thermal images contain the spatial geometry information and surface temperature information, which can act as complementary information for the RGB modality. However, the quality of the depth and thermal images is often unreliable in some challenging scenarios, which will result in the performance degradation of the two-modal based salient object detection (SOD). Meanwhile, some researchers pay attention to the triple-modal SOD task, where they attempt to explore the complementarity of the RGB image, the depth image, and the thermal image. However, existing triple-modal SOD methods fail to perceive the quality of depth maps and thermal images, which leads to performance degradation when dealing with scenes with low-quality depth and thermal images. Therefore, we propose a quality-aware selective fusion network (QSF-Net) to conduct VDT salient object detection, which contains three subnets including the initial feature extraction subnet, the quality-aware region selection subnet, and the region-guided selective fusion subnet. Firstly, except for extracting features, the initial feature extraction subnet can generate a preliminary prediction map from each modality via a shrinkage pyramid architecture. Then, we design the weakly-supervised quality-aware region selection subnet to generate the quality-aware maps. Concretely, we first find the high-quality and low-quality regions by using the preliminary predictions, which further constitute the pseudo label that can be used to train this subnet. Finally, the region-guided selective fusion subnet purifies the initial features under the guidance of the quality-aware maps, and then fuses the triple-modal features and refines the edge details of prediction maps through the intra-modality and inter-modality attention (IIA) module and the edge refinement (ER) module, respectively. Extensive experiments are performed on VDT-2048",True,True,"Bao, Liuxin and Zhou, Xiaofei and Lu, Xiankai and Sun, Yaoqi and Yin, Haibing and Hu, Zhenghui and Zhang, Jiyong and Yan, Chenggang",2024,,,,IEEE Transactions on Image Processing
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,hao2024primkd,\cite{hao2024primkd},PrimKD: Primary Modality Guided Multimodal Fusion for RGB-D Semantic Segmentation,,,True,False,"Hao, Zhiwei and Xiao, Zhongyu and Luo, Yong and Guo, Jianyuan and Wang, Jing and Shen, Li and Hu, Han",2024,,,,
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,wang2017video,\cite{wang2017video},Video Salient Object Detection via Fully Convolutional Networks,https://arxiv.org/abs/1702.00871v3,"This paper proposes a deep learning model to efficiently detect salient regions in videos. It addresses two important issues: (1) deep video saliency model training with the absence of sufficiently large and pixel-wise annotated video data, and (2) fast video saliency training and detection. The proposed deep video saliency network consists of two modules, for capturing the spatial and temporal saliency information, respectively. The dynamic saliency model, explicitly incorporating saliency estimates from the static saliency model, directly produces spatiotemporal saliency inference without time-consuming optical flow computation. We further propose a novel data augmentation technique that simulates video training data from existing annotated image datasets, which enables our network to learn diverse saliency information and prevents overfitting with the limited number of training videos. Leveraging our synthetic video data (150K video sequences) and real videos, our deep video saliency model successfully learns both spatial and temporal saliency cues, thus producing accurate spatiotemporal saliency estimate. We advance the state-of-the-art on the DAVIS dataset (MAE of .06) and the FBMS dataset (MAE of .07), and do so with much improved speed (2fps with all steps).",True,True,"Wang, Wenguan and Shen, Jianbing and Shao, Ling",2017,,,,IEEE Transactions on Image Processing
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,li2018flow,\cite{li2018flow},Flow guided recurrent neural encoder for video salient object detection,,,True,False,"Li, Guanbin and Xie, Yuan and Wei, Tianhao and Wang, Keze and Lin, Liang",2018,,,,
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,ji2020casnet,\cite{ji2020casnet},CASNet: A cross-attention siamese network for video salient object detection,,,True,False,"Ji, Yuzhu and Zhang, Haijun and Jie, Zequn and Ma, Lin and Wu, QM Jonathan",2020,,,,IEEE transactions on neural networks and learning systems
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,singh2024dsfnet,\cite{singh2024dsfnet},Dsfnet: video salient object detection using a novel lightweight deformable separable fusion network,,,True,False,"Singh, Hemraj and Verma, Mridula and Cheruku, Ramalingaswamy",2024,,,,IEEE Transactions on Instrumentation and Measurement
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,oh2019video,\cite{oh2019video},Video Object Segmentation using Space-Time Memory Networks,https://arxiv.org/abs/1904.00607v2,"We propose a novel solution for semi-supervised video object segmentation. By the nature of the problem, available cues (e.g. video frame(s) with object masks) become richer with the intermediate predictions. However, the existing methods are unable to fully exploit this rich source of information. We resolve the issue by leveraging memory networks and learn to read relevant information from all available sources. In our framework, the past frames with object masks form an external memory, and the current frame as the query is segmented using the mask information in the memory. Specifically, the query and the memory are densely matched in the feature space, covering all the space-time pixel locations in a feed-forward fashion. Contrast to the previous approaches, the abundant use of the guidance information allows us to better handle the challenges such as appearance changes and occlussions. We validate our method on the latest benchmark sets and achieved the state-of-the-art performance (overall score of 79.4 on Youtube-VOS val set, J of 88.7 and 79.2 on DAVIS 2016/2017 val set respectively) while having a fast runtime (0.16 second/frame on DAVIS 2016 val set).",True,True,"Oh, Seoung Wug and Lee, Joon-Young and Xu, Ning and Kim, Seon Joo",2019,,,,
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,cheng2022xmem,\cite{cheng2022xmem},XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model,https://arxiv.org/abs/2207.07115v2,"We present XMem, a video object segmentation architecture for long videos with unified feature memory stores inspired by the Atkinson-Shiffrin memory model. Prior work on video object segmentation typically only uses one type of feature memory. For videos longer than a minute, a single feature memory model tightly links memory consumption and accuracy. In contrast, following the Atkinson-Shiffrin model, we develop an architecture that incorporates multiple independent yet deeply-connected feature memory stores: a rapidly updated sensory memory, a high-resolution working memory, and a compact thus sustained long-term memory. Crucially, we develop a memory potentiation algorithm that routinely consolidates actively used working memory elements into the long-term memory, which avoids memory explosion and minimizes performance decay for long-term prediction. Combined with a new memory reading mechanism, XMem greatly exceeds state-of-the-art performance on long-video datasets while being on par with state-of-the-art methods (that do not work on long videos) on short-video datasets. Code is available at https://hkchengrex.github.io/XMem",True,True,"Cheng, Ho Kei and Schwing, Alexander G",2022,,,,
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,fang2024learning,\cite{fang2024learning},Learning better video query with sam for video instance segmentation,,,True,False,"Fang, Hao and Zhang, Tong and Zhou, Xiaofei and Zhang, Xinxin",2024,,,,IEEE Transactions on Circuits and Systems for Video Technology
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,wang2023look,\cite{wang2023look},Look Before You Match: Instance Understanding Matters in Video Object Segmentation,https://arxiv.org/abs/2212.06826v1,"Exploring dense matching between the current frame and past frames for long-range context modeling, memory-based methods have demonstrated impressive results in video object segmentation (VOS) recently. Nevertheless, due to the lack of instance understanding ability, the above approaches are oftentimes brittle to large appearance variations or viewpoint changes resulted from the movement of objects and cameras. In this paper, we argue that instance understanding matters in VOS, and integrating it with memory-based matching can enjoy the synergy, which is intuitively sensible from the definition of VOS task, \ie, identifying and segmenting object instances within the video. Towards this goal, we present a two-branch network for VOS, where the query-based instance segmentation (IS) branch delves into the instance details of the current frame and the VOS branch performs spatial-temporal matching with the memory bank. We employ the well-learned object queries from IS branch to inject instance-specific information into the query key, with which the instance-augmented matching is further performed. In addition, we introduce a multi-path fusion block to effectively combine the memory readout with multi-scale features from the instance segmentation decoder, which incorporates high-resolution instance-aware features to produce final segmentation results. Our method achieves state-of-the-art performance on DAVIS 2016/2017 val (92.6% and 87.1%), DAVIS 2017 test-dev (82.8%), and YouTube-VOS 2018/2019 val (86.3% and 86.3%), outperforming alternative methods by clear margins.",True,True,"Wang, Junke and Chen, Dongdong and Wu, Zuxuan and Luo, Chong and Tang, Chuanxin and Dai, Xiyang and Zhao, Yucheng and Xie, Yujia and Yuan, Lu and Jiang, Yu-Gang",2023,,,,
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,li2023dvsod,\cite{li2023dvsod},Dvsod: Rgb-d video salient object detection,,,True,False,"Li, Jingjing and Ji, Wei and Wang, Size and Li, Wenbo and others",2023,,,,Advances in Neural Information Processing Systems
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,lin2024vidsod,\cite{lin2024vidsod},ViDSOD-100: A New Dataset and a Baseline Model for RGB-D Video Salient Object Detection,,,True,False,"Lin, Junhao and Zhu, Lei and Shen, Jiaxing and Fu, Huazhu and Zhang, Qing and Wang, Liansheng",2024,,,,International Journal of Computer Vision
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,suolang2025lightweight,\cite{suolang2025lightweight},Lightweight Multi-Frequency Enhancement Network for RGB-D Video Salient Object Detection,,,True,False,"Suolang, Daerji and He, Jiahao and Tsering, Wangchuk and Fu, Keren and Li, Xiaofeng and Zhao, Qijun",2025,,,,
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,kirillov2023segment,\cite{kirillov2023segment},Segment anything,,,True,False,"Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others",2023,,,,
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,ravi2024sam,\cite{ravi2024sam},Sam 2: Segment anything in images and videos,,,True,False,"Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and R{\""a}dle, Roman and Rolland, Chloe and Gustafson, Laura and others",2024,,,,arXiv preprint arXiv:2408.00714
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,zhang2024uv,\cite{zhang2024uv},Uv-sam: Adapting segment anything model for urban village identification,,,True,False,"Zhang, Xin and Liu, Yu and Lin, Yuming and Liao, Qingmin and Li, Yong",2024,,,,
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,ayzenberg2024protosam,\cite{ayzenberg2024protosam},ProtoSAM: One-Shot Medical Image Segmentation With Foundational Models,https://arxiv.org/abs/2407.07042v2,"This work introduces a new framework, ProtoSAM, for one-shot medical image segmentation. It combines the use of prototypical networks, known for few-shot segmentation, with SAM - a natural image foundation model. The method proposed creates an initial coarse segmentation mask using the ALPnet prototypical network, augmented with a DINOv2 encoder. Following the extraction of an initial mask, prompts are extracted, such as points and bounding boxes, which are then input into the Segment Anything Model (SAM). State-of-the-art results are shown on several medical image datasets and demonstrate automated segmentation capabilities using a single image example (one shot) with no need for fine-tuning of the foundation model. Our code is available at: https://github.com/levayz/ProtoSAM",True,True,"Ayzenberg, Lev and Giryes, Raja and Greenspan, Hayit",2024,,,,arXiv preprint arXiv:2407.07042
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,xie2025rfmedsam,\cite{xie2025rfmedsam},RFMedSAM 2: Automatic Prompt Refinement for Enhanced Volumetric Medical Image Segmentation with SAM 2,,,True,False,"Xie, Bin and Tang, Hao and Yan, Yan and Agam, Gady",2025,,,,arXiv preprint arXiv:2502.02741
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,yang2024sam,\cite{yang2024sam},SAM-UNet: Enhancing Zero-Shot Segmentation of SAM for Universal Medical Images,,,True,False,"Yang, Sihan and Bi, Haixia and Zhang, Hai and Sun, Jian",2024,,,,arXiv preprint arXiv:2408.09886
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,xiong2024sam2,\cite{xiong2024sam2},SAM2-UNet: Segment Anything 2 Makes Strong Encoder for Natural and Medical Image Segmentation,https://arxiv.org/abs/2408.08870v1,"Image segmentation plays an important role in vision understanding. Recently, the emerging vision foundation models continuously achieved superior performance on various tasks. Following such success, in this paper, we prove that the Segment Anything Model 2 (SAM2) can be a strong encoder for U-shaped segmentation models. We propose a simple but effective framework, termed SAM2-UNet, for versatile image segmentation. Specifically, SAM2-UNet adopts the Hiera backbone of SAM2 as the encoder, while the decoder uses the classic U-shaped design. Additionally, adapters are inserted into the encoder to allow parameter-efficient fine-tuning. Preliminary experiments on various downstream tasks, such as camouflaged object detection, salient object detection, marine animal segmentation, mirror detection, and polyp segmentation, demonstrate that our SAM2-UNet can simply beat existing specialized state-of-the-art methods without bells and whistles. Project page: \url{https://github.com/WZH0120/SAM2-UNet}.",True,True,"Xiong, Xinyu and Wu, Zihuang and Tan, Shuangyi and Li, Wenxue and Tang, Feilong and Chen, Ying and Li, Siying and Ma, Jie and Li, Guanbin",2024,,,,arXiv preprint arXiv:2408.08870
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,xu2025dgsunet,\cite{xu2025dgsunet},DGSUnet: An Improved Unet Model with DINO-Guided SAM2 for Multi-Scale Feature Collaboration,,,True,False,"Xu, Yimin",2025,,,,arXiv preprint arXiv:2503.21187
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,wang2024adapting,\cite{wang2024adapting},Adapting Segment Anything Model to Multi-modal Salient Object Detection with Semantic Feature Fusion Guidance,https://arxiv.org/abs/2408.15063v4,"Although most existing multi-modal salient object detection (SOD) methods demonstrate effectiveness through training models from scratch, the limited multi-modal data hinders these methods from reaching optimality. In this paper, we propose a novel framework to explore and exploit the powerful feature representation and zero-shot generalization ability of the pre-trained Segment Anything Model (SAM) for multi-modal SOD. Despite serving as a recent vision fundamental model, driving the class-agnostic SAM to comprehend and detect salient objects accurately is non-trivial, especially in challenging scenes. To this end, we develop \underline{SAM} with se\underline{m}antic f\underline{e}ature fu\underline{s}ion guidanc\underline{e} (Sammese), which incorporates multi-modal saliency-specific knowledge into SAM to adapt SAM to multi-modal SOD tasks. However, it is difficult for SAM trained on single-modal data to directly mine the complementary benefits of multi-modal inputs and comprehensively utilize them to achieve accurate saliency prediction. To address these issues, we first design a multi-modal complementary fusion module to extract robust multi-modal semantic features by integrating information from visible and thermal or depth image pairs. Then, we feed the extracted multi-modal semantic features into both the SAM image encoder and mask decoder for fine-tuning and prompting, respectively. Specifically, in the image encoder, a multi-modal adapter is proposed to adapt the single-modal SAM to multi-modal information. In the mask decoder, a semantic-geometric prompt generation strategy is proposed to produce corresponding embeddings with various saliency cues. Extensive experiments on both RGB-D and RGB-T SOD benchmarks show the effectiveness of the proposed framework. The code will be available at \url{https://github.com/Angknpng/Sammese}.",True,True,"Wang, Kunpeng and Lin, Danying and Li, Chenglong and Tu, Zhengzheng and Luo, Bin",2024,,,,arXiv preprint arXiv:2408.15063
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,houlsby2019parameter,\cite{houlsby2019parameter},Parameter-efficient transfer learning for NLP,,,True,False,"Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain",2019,,,,
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,zhong2024convolution,\cite{zhong2024convolution},Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model,https://arxiv.org/abs/2401.17868v1,"The Segment Anything Model (SAM) stands as a foundational framework for image segmentation. While it exhibits remarkable zero-shot generalization in typical scenarios, its advantage diminishes when applied to specialized domains like medical imagery and remote sensing. To address this limitation, this paper introduces Conv-LoRA, a simple yet effective parameter-efficient fine-tuning approach. By integrating ultra-lightweight convolutional parameters into Low-Rank Adaptation (LoRA), Conv-LoRA can inject image-related inductive biases into the plain ViT encoder, further reinforcing SAM's local prior assumption. Notably, Conv-LoRA not only preserves SAM's extensive segmentation knowledge but also revives its capacity of learning high-level image semantics, which is constrained by SAM's foreground-background segmentation pretraining. Comprehensive experimentation across diverse benchmarks spanning multiple domains underscores Conv-LoRA's superiority in adapting SAM to real-world semantic segmentation tasks.",True,True,"Zhong, Zihan and Tang, Zhiqiang and He, Tong and Fang, Haoyang and Yuan, Chun",2024,,,,arXiv preprint arXiv:2401.17868
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,hu2022lora,\cite{hu2022lora},LoRA+: Efficient Low Rank Adaptation of Large Models,https://arxiv.org/abs/2402.12354v2,"In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$ improves performance (1-2 $\%$ improvements) and finetuning speed (up to $\sim$ 2X SpeedUp), at the same computational cost as LoRA.",True,True,"Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others",2022,,,,ICLR
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,diao2024unipt,\cite{diao2024unipt},Unipt: Universal parallel tuning for transfer learning with efficient parameter and memory,,,True,False,"Diao, Haiwen and Wan, Bo and Zhang, Ying and Jia, Xu and Lu, Huchuan and Chen, Long",2024,,,,
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,yue2024sam,\cite{yue2024sam},How SAM helps Unsupervised Video Object Segmentation?,,,True,False,"Yue, Jiahe and Zhang, Runchu and Zhang, Zhe and Zhao, Ruixiang and Lv, Wu and Ma, Jie",2024,,,,
SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection,2511.09870v1,deng2024memsam,\cite{deng2024memsam},MemSAM: taming segment anything model for echocardiography video segmentation,,,True,False,"Deng, Xiaolong and Wu, Huisi and Zeng, Runhao and Qin, Jing",2024,,,,
DELICATE: Diachronic Entity LInking using Classes And Temporal Evidence,2511.10404v1,labusch_named_2020,\cite{labusch_named_2020},Named {Entity} {Disambiguation} and {Linking} {Historic} {Newspaper} {OCR} with {BERT}.,,,True,False,"Labusch, Kai and Neudecker, Clemens",2020,,https://ceur-ws.org/Vol-2696/paper_163.pdf?ref=https://githubhelp.com,,
DELICATE: Diachronic Entity LInking using Classes And Temporal Evidence,2511.10404v1,kolitsas-etal-2018-end,\cite{kolitsas-etal-2018-end},End-to-End Neural Entity Linking,https://arxiv.org/abs/1808.07699v2,"Entity Linking (EL) is an essential task for semantic text understanding and information extraction. Popular methods separately address the Mention Detection (MD) and Entity Disambiguation (ED) stages of EL, without leveraging their mutual dependency. We here propose the first neural end-to-end EL system that jointly discovers and links entities in a text document. The main idea is to consider all possible spans as potential mentions and learn contextual similarity scores over their entity candidates that are useful for both MD and ED decisions. Key components are context-aware mention embeddings, entity embeddings and a probabilistic mention - entity map, without demanding other engineered features. Empirically, we show that our end-to-end method significantly outperforms popular systems on the Gerbil platform when enough training data is available. Conversely, if testing datasets follow different annotation conventions compared to the training set (e.g. queries/ tweets vs news documents), our ED model coupled with a traditional NER system offers the best or second best EL accuracy.",True,True,"Kolitsas, Nikolaos  and
      Ganea, Octavian-Eugen  and
      Hofmann, Thomas",2018,,https://aclanthology.org/K18-1050,10.18653/v1/K18-1050,
DELICATE: Diachronic Entity LInking using Classes And Temporal Evidence,2511.10404v1,limkonchotiwat-etal-2023-mrefined,\cite{limkonchotiwat-etal-2023-mrefined},m{R}e{F}in{ED}: An Efficient End-to-End Multilingual Entity Linking System,,,True,False,"Limkonchotiwat, Peerat  and
      Cheng, Weiwei  and
      Christodoulopoulos, Christos  and
      Saffari, Amir  and
      Lehmann, Jens",2023,,https://aclanthology.org/2023.findings-emnlp.1007,10.18653/v1/2023.findings-emnlp.1007,
DELICATE: Diachronic Entity LInking using Classes And Temporal Evidence,2511.10404v1,sevgili2022neural,\cite{sevgili2022neural},Neural Entity Linking: A Survey of Models Based on Deep Learning,https://arxiv.org/abs/2006.00575v4,"This survey presents a comprehensive description of recent neural entity linking (EL) systems developed since 2015 as a result of the ""deep learning revolution"" in natural language processing. Its goal is to systemize design features of neural entity linking systems and compare their performance to the remarkable classic methods on common benchmarks. This work distills a generic architecture of a neural EL system and discusses its components, such as candidate generation, mention-context encoding, and entity ranking, summarizing prominent methods for each of them. The vast variety of modifications of this general architecture are grouped by several common themes: joint entity mention detection and disambiguation, models for global linking, domain-independent techniques including zero-shot and distant supervision methods, and cross-lingual approaches. Since many neural models take advantage of entity and mention/context embeddings to represent their meaning, this work also overviews prominent entity embedding techniques. Finally, the survey touches on applications of entity linking, focusing on the recently emerged use-case of enhancing deep pre-trained masked language models based on the Transformer architecture.",True,True,"Sevgili, {\""O}zge and Shelmanov, Artem and Arkhipov, Mikhail and Panchenko, Alexander and Biemann, Chris",2022,,,,Semantic Web
DELICATE: Diachronic Entity LInking using Classes And Temporal Evidence,2511.10404v1,survey_hist_ner_2023,\cite{survey_hist_ner_2023},Named Entity Recognition and Classification in Historical Documents: A Survey,,,True,False,"Ehrmann, Maud and Hamdi, Ahmed and Pontes, Elvys Linhares and Romanello, Matteo and Doucet, Antoine",2023,,https://doi.org/10.1145/3604931,10.1145/3604931,ACM Comput. Surv.
DELICATE: Diachronic Entity LInking using Classes And Temporal Evidence,2511.10404v1,linhares2022melhissa,\cite{linhares2022melhissa},MELHISSA: a multilingual entity linking architecture for historical press articles,,,True,False,"Linhares Pontes, Elvys and Cabrera-Diego, Luis Adri{\'a}n and Moreno, Jose G and Boros, Emanuela and Hamdi, Ahmed and Doucet, Antoine and Sidere, Nicolas and Coustaty, Micka{\""e}l",2022,,,,International journal on digital libraries
DELICATE: Diachronic Entity LInking using Classes And Temporal Evidence,2511.10404v1,graciotti2025musical,\cite{graciotti2025musical},Musical Heritage Historical Entity Linking,https://arxiv.org/abs/2502.09168v1,"Linking named entities occurring in text to their corresponding entity in a Knowledge Base (KB) is challenging, especially when dealing with historical texts. In this work, we introduce Musical Heritage named Entities Recognition, Classification and Linking (MHERCL), a novel benchmark consisting of manually annotated sentences extrapolated from historical periodicals of the music domain. MHERCL contains named entities under-represented or absent in the most famous KBs. We experiment with several State-of-the-Art models on the Entity Linking (EL) task and show that MHERCL is a challenging dataset for all of them. We propose a novel unsupervised EL model and a method to extend supervised entity linkers by using Knowledge Graphs (KGs) to tackle the main difficulties posed by historical documents. Our experiments reveal that relying on unsupervised techniques and improving models with logical constraints based on KGs and heuristics to predict NIL entities (entities not represented in the KB of reference) results in better EL performance on historical documents.",True,True,"Graciotti, Arianna and Lazzari, Nicolas and Presutti, Valentina and Tripodi, Rocco",2025,,,,Artificial Intelligence Review
DELICATE: Diachronic Entity LInking using Classes And Temporal Evidence,2511.10404v1,wu2020scalable,\cite{wu2020scalable},Scalable Zero-shot Entity Linking with Dense Entity Retrieval,https://arxiv.org/abs/1911.03814v3,"This paper introduces a conceptually simple, scalable, and highly effective BERT-based entity linking model, along with an extensive evaluation of its accuracy-speed trade-off. We present a two-stage zero-shot linking algorithm, where each entity is defined only by a short textual description. The first stage does retrieval in a dense space defined by a bi-encoder that independently embeds the mention context and the entity descriptions. Each candidate is then re-ranked with a cross-encoder, that concatenates the mention and entity text. Experiments demonstrate that this approach is state of the art on recent zero-shot benchmarks (6 point absolute gains) and also on more established non-zero-shot evaluations (e.g. TACKBP-2010), despite its relative simplicity (e.g. no explicit entity embeddings or manually engineered mention tables). We also show that bi-encoder linking is very fast with nearest neighbour search (e.g. linking with 5.9 million candidates in 2 milliseconds), and that much of the accuracy gain from the more expensive cross-encoder can be transferred to the bi-encoder via knowledge distillation. Our code and models are available at https://github.com/facebookresearch/BLINK.",True,True,"Wu, Ledell and Petroni, Fabio and Josifoski, Martin and Riedel, Sebastian and Zettlemoyer, Luke",2020,,,,
DELICATE: Diachronic Entity LInking using Classes And Temporal Evidence,2511.10404v1,paccosi-palmero-aprosio-2022-kind,\cite{paccosi-palmero-aprosio-2022-kind},{KIND}: an {I}talian Multi-Domain Dataset for Named Entity Recognition,,,True,False,"Paccosi, Teresa  and
      Palmero Aprosio, Alessio",2022,,https://aclanthology.org/2022.lrec-1.52,,
DELICATE: Diachronic Entity LInking using Classes And Temporal Evidence,2511.10404v1,sprugnoli2016fifty,\cite{sprugnoli2016fifty},Fifty years of European history through the lens of computational linguistics: the De Gasperi Project,,,True,False,"Sprugnoli, Rachele and Tonelli, Sara and Moretti, Giovanni and Menini, Stefano",2016,,,,IJCoL. Italian Journal of Computational Linguistics
DELICATE: Diachronic Entity LInking using Classes And Temporal Evidence,2511.10404v1,barzaghi_amd_2025,\cite{barzaghi_amd_2025},The Semantic Digital Edition of Aldo Moro’s Writings: A Workflow supporting Data Sharing and Replicability,,,True,False,"Barzaghi, Sebastian and Palmero Aprosio, Alessio and Paolucci, Francesco and Tomasi, Francesca and Tonelli, Sara and Vignocchi, Marialaura and Vitali, Fabio",2025,,https://doi.org/10.1145/3725534,10.1145/3725534,J. Comput. Cult. Herit.
DELICATE: Diachronic Entity LInking using Classes And Temporal Evidence,2511.10404v1,colavizza_annotated_2017,\cite{colavizza_annotated_2017},Annotated {References} in the {Historiography} on {Venice}: 19th–21st centuries,,,True,False,"Colavizza, Giovanni and Romanello, Matteo",2017,,https://account.openhumanitiesdata.metajnl.com/index.php/up-j-johd/article/view/johd.9,10.5334/johd.9,Journal of Open Humanities Data
DELICATE: Diachronic Entity LInking using Classes And Temporal Evidence,2511.10404v1,ajmc_2024,\cite{ajmc_2024},A {Named} {Entity}-{Annotated} {Corpus} of 19th {Century} {Classical} {Commentaries},,,True,False,"Romanello, Matteo and Najem-Meyer, Sven",2024,,https://openhumanitiesdata.metajnl.com/articles/10.5334/johd.150,10.5334/johd.150,Journal of Open Humanities Data
DELICATE: Diachronic Entity LInking using Classes And Temporal Evidence,2511.10404v1,graciotti-etal-2025-ke-mhisto,\cite{graciotti-etal-2025-ke-mhisto},KE‑MHISTO: Towards a Multilingual Historical Knowledge Extraction Benchmark for Addressing the Long‑Tail Problem,,,True,False,"Graciotti, Arianna and Piano, Leonardo and Lazzari, Nicolas and Daga, Enrico and Tripodi, Rocco and Presutti, Valentina and Pompianu, Livio",2025,,,,
Blockly2Hooks: Smart Contracts for Everyone with the XRP Ledger and Google Blockly,2511.08403v1,marks,\cite{marks},The Case for Graphical Smart Contract Editors,,,True,False,Erik Marks,2018,04,https://medium.com/pennblockchain/the-case-for-graphical-smart-contract-editors-8e721cdcde93,,
Blockly2Hooks: Smart Contracts for Everyone with the XRP Ledger and Google Blockly,2511.08403v1,detailed-sc-eth,\cite{detailed-sc-eth},Replacing Paper Contracts With Ethereum Smart Contracts,,,True,False,Wesley Egbertsen and Gerdinand Hardeman and Maarten van den Hoven and Gert van der Kolk and Arthur van Rijsewijk,2016,,https://allquantor.at/blockchainbib/pdf/egbertsen2016replacing.pdf,,
Blockly2Hooks: Smart Contracts for Everyone with the XRP Ledger and Google Blockly,2511.08403v1,DasContract,\cite{DasContract},Das Contract - A Visual Domain Specific Language for Modeling Blockchain Smart Contracts,,,True,False,"Skotnica, Marek
and Pergl, Robert",2020,,,,
Blockly2Hooks: Smart Contracts for Everyone with the XRP Ledger and Google Blockly,2511.08403v1,towards-skotnika-DasContract2,\cite{towards-skotnika-DasContract2},Towards Model-Driven Smart Contract Systems - Code Generation and Improving Expressivity of Smart Contract Modeling,,,True,False,"Skotnica, Marek and Klicpera, Jan and Pergl, Robert",2021,03,https://ceur-ws.org/Vol-2825/,,"CIAO! Doctoral Consortium, EEWC Forum 2020"
Blockly2Hooks: Smart Contracts for Everyone with the XRP Ledger and Google Blockly,2511.08403v1,latte,\cite{latte},LATTE: Visual Construction of Smart Contracts,,,True,False,"Tan, Sean and S Bhowmick, Sourav and Chua, Huey Eng and Xiao, Xiaokui",2020,,https://doi.org/10.1145/3318464.3384687,10.1145/3318464.3384687,
Blockly2Hooks: Smart Contracts for Everyone with the XRP Ledger and Google Blockly,2511.08403v1,ETH-Visual-Auto,\cite{ETH-Visual-Auto},Visual and User-Defined Smart Contract Designing System Based on Automatic Coding,,,True,False,Dianhui Mao and Fan Wang and Yalei Wang and Zhihao Hao,2019,,,,IEEE Access
Blockly2Hooks: Smart Contracts for Everyone with the XRP Ledger and Google Blockly,2511.08403v1,yawl,\cite{yawl},Design and Implementation of the YAWL System,,,True,False,"van der Aalst, Wil M. P.
and Aldred, Lachlan
and Dumas, Marlon
and ter Hofstede, Arthur H. M.",2004,,,,
Blockly2Hooks: Smart Contracts for Everyone with the XRP Ledger and Google Blockly,2511.08403v1,industry-construction-users-themselves,\cite{industry-construction-users-themselves},From the graphical representation to the smart contract language: a use case in the construction industry,,,True,False,"Ye, Xuling and König, Markus",,November,,10.22260/ISARC2021/0039,
Blockly2Hooks: Smart Contracts for Everyone with the XRP Ledger and Google Blockly,2511.08403v1,smart-contracts-for-purchases,\cite{smart-contracts-for-purchases},Smart Contracts Using Blockly: Representing a Purchase Agreement Using a Graphical Programming Language,,,True,False,"Weingaertner, Tim and Rao, Rahul and Ettlin, Jasmin and Suter, Patrick and Dublanc, Philipp",2018,,,10.1109/CVCBT.2018.00012,
Blockly2Hooks: Smart Contracts for Everyone with the XRP Ledger and Google Blockly,2511.08403v1,reuse-SC-visual,\cite{reuse-SC-visual},Supporting Reuse of Smart Contracts through Service Orientation and Assisted Development,,,True,False,"Guida, Luca and Daniel, Florian",2019,,,10.1109/DAPPCON.2019.00017,
Blockly2Hooks: Smart Contracts for Everyone with the XRP Ledger and Google Blockly,2511.08403v1,purnell2022towards,\cite{purnell2022towards},Towards declarative smart contracts,,,True,False,"Purnell, Kevin John",2022,,,,
Blockly2Hooks: Smart Contracts for Everyone with the XRP Ledger and Google Blockly,2511.08403v1,blocks-article,\cite{blocks-article},How we created Blocks: an online drag-and-drop smart contract editor,,,True,False,Ryan Vandersmith,2022,01,https://levelup.gitconnected.com/how-we-created-blocks-an-online-drag-and-drop-smart-contract-editor-fe23eff4d933,,
Blockly2Hooks: Smart Contracts for Everyone with the XRP Ledger and Google Blockly,2511.08403v1,blockly-hyperledger,\cite{blockly-hyperledger},SmartBuilder: A Block-based Visual Programming Framework for Smart Contract Development,,,True,False,"Merlec, Mpyana Mwamba and Lee, Youn Kyu and In, Hoh Peter",2021,,,10.1109/Blockchain53845.2021.00023,
Blockly2Hooks: Smart Contracts for Everyone with the XRP Ledger and Google Blockly,2511.08403v1,hooks-builder,\cite{hooks-builder},{XRPL H}ooks Builder,,,True,False,{XRPL L}abs,2023,,https://hooks-builder.xrpl.org/develop,,
BuddyMoE: Exploiting Expert Redundancy to Accelerate Memory-Constrained Mixture-of-Experts Inference,2511.10054v1,shazeer2017outrageously,\cite{shazeer2017outrageously},Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,,,True,False,Noam Shazeer and *Azalia Mirhoseini and *Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean,2017,,https://openreview.net/forum?id=B1ckMDqlg,,
BuddyMoE: Exploiting Expert Redundancy to Accelerate Memory-Constrained Mixture-of-Experts Inference,2511.10054v1,lepikhin2021gshard,\cite{lepikhin2021gshard},GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding,,,True,False,Dmitry Lepikhin and HyoukJoong Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Yanping Huang and Maxim Krikun and Noam Shazeer and Zhifeng Chen,2020,,https://arxiv.org/abs/2006.16668,,
BuddyMoE: Exploiting Expert Redundancy to Accelerate Memory-Constrained Mixture-of-Experts Inference,2511.10054v1,fedus2021switch,\cite{fedus2021switch},Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,https://arxiv.org/abs/2101.03961v3,"In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the ""Colossal Clean Crawled Corpus"" and achieve a 4x speedup over the T5-XXL model.",True,True,William Fedus and Barret Zoph and Noam Shazeer,2022,,https://arxiv.org/abs/2101.03961,,
BuddyMoE: Exploiting Expert Redundancy to Accelerate Memory-Constrained Mixture-of-Experts Inference,2511.10054v1,rajbhandari2021zero,\cite{rajbhandari2021zero},ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning,,,True,False,Samyam Rajbhandari and Olatunji Ruwase and Jeff Rasley and Shaden Smith and Yuxiong He,2021,,https://arxiv.org/abs/2104.07857,,
BuddyMoE: Exploiting Expert Redundancy to Accelerate Memory-Constrained Mixture-of-Experts Inference,2511.10054v1,hf_accelerate_infer_2025,\cite{hf_accelerate_infer_2025},Accelerate Documentation: Big Model Inference (CPU/Disk Offloading),,,True,False,{Hugging Face},2025,,,,
BuddyMoE: Exploiting Expert Redundancy to Accelerate Memory-Constrained Mixture-of-Experts Inference,2511.10054v1,hf_accelerate_deepspeed_2025,\cite{hf_accelerate_deepspeed_2025},Accelerate Documentation: DeepSpeed Integration (ZeRO-Offload and ZeRO-Infinity),,,True,False,{Hugging Face},2025,,,,
BuddyMoE: Exploiting Expert Redundancy to Accelerate Memory-Constrained Mixture-of-Experts Inference,2511.10054v1,xue2024moe,\cite{xue2024moe},MoE-Infinity: Efficient MoE Inference on Personal Machines with Sparsity-Aware Expert Cache,https://arxiv.org/abs/2401.14361v3,"This paper presents MoE-Infinity, an efficient MoE inference system designed for personal machines with limited GPU memory capacity. The key idea for MoE-Infinity is that on personal machines, which are often single-user environments, MoE-based LLMs typically operate with a batch size of one. In this setting, MoE models exhibit a high degree of activation sparsity, meaning a small number of experts are frequently reused in generating tokens during the decode phase. Leveraging this idea, we design a sparsity-aware expert cache, which can trace the sparse activation of experts during inference and carefully select the trace that represents the sparsity pattern. By analyzing these selected traces, MoE-Infinity guides the replacement and prefetching of the expert cache, providing 3.1-16.7x per-token latency improvements over numerous state-of-the-art systems, including vLLM, Ollama, DeepSpeed and BrainStorm across various MoE models (DeepSeek and Mixtral) when handling different LLM tasks. MoE-Infinity's source code is publicly available at https://github.com/EfficientMoE/MoE-Infinity",True,True,Leyang Xue and Yao Fu and Zhan Lu and Luo Mai and Mahesh Marina,2025,,https://arxiv.org/abs/2401.14361,,
BuddyMoE: Exploiting Expert Redundancy to Accelerate Memory-Constrained Mixture-of-Experts Inference,2511.10054v1,hwang2024pregatedmoe,\cite{hwang2024pregatedmoe},Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference,https://arxiv.org/abs/2308.12066v3,"Large language models (LLMs) based on transformers have made significant strides in recent years, the success of which is driven by scaling up their model size. Despite their high algorithmic performance, the computational and memory requirements of LLMs present unprecedented challenges. To tackle the high compute requirements of LLMs, the Mixture-of-Experts (MoE) architecture was introduced which is able to scale its model size without proportionally scaling up its computational requirements. Unfortunately, MoE's high memory demands and dynamic activation of sparse experts restrict its applicability to real-world problems. Previous solutions that offload MoE's memory-hungry expert parameters to CPU memory fall short because the latency to migrate activated experts from CPU to GPU incurs high performance overhead. Our proposed Pre-gated MoE system effectively tackles the compute and memory challenges of conventional MoE architectures using our algorithm-system co-design. Pre-gated MoE employs our novel pre-gating function which alleviates the dynamic nature of sparse expert activation, allowing our proposed system to address the large memory footprint of MoEs while also achieving high performance. We demonstrate that Pre-gated MoE is able to improve performance, reduce GPU memory consumption, while also maintaining the same level of model quality. These features allow our Pre-gated MoE system to cost-effectively deploy large-scale LLMs using just a single GPU with high performance.",True,True,Ranggi Hwang and Jianyu Wei and Shijie Cao and Changho Hwang and Xiaohu Tang and Ting Cao and Mao Yang,2024,,https://arxiv.org/abs/2308.12066,,
BuddyMoE: Exploiting Expert Redundancy to Accelerate Memory-Constrained Mixture-of-Experts Inference,2511.10054v1,yi2023edgemoe,\cite{yi2023edgemoe},EdgeMoE: Empowering Sparse Large Language Models on Mobile Devices,https://arxiv.org/abs/2308.14352v2,"Large language models (LLMs) such as GPTs and Mixtral-8x7B have revolutionized machine intelligence due to their exceptional abilities in generic ML tasks. Transiting LLMs from datacenters to edge devices brings benefits like better privacy and availability, but is challenged by their massive parameter size and thus unbearable runtime costs. To this end, we present EdgeMoE, an on-device inference engine for mixture-of-expert (MoE) LLMs -- a popular form of sparse LLM that scales its parameter size with almost constant computing complexity. EdgeMoE achieves both memory- and compute-efficiency by partitioning the model into the storage hierarchy: non-expert weights are held in device memory; while expert weights are held on external storage and fetched to memory only when activated. This design is motivated by a key observation that expert weights are bulky but infrequently used due to sparse activation. To further reduce the expert I/O swapping overhead, EdgeMoE incorporates two novel techniques: (1) expert-wise bitwidth adaptation that reduces the expert sizes with tolerable accuracy loss; (2) expert preloading that predicts the activated experts ahead of time and preloads it with the compute-I/O pipeline. On popular MoE LLMs and edge devices, EdgeMoE showcase significant memory savings and speedup over competitive baselines. The code is available at https://github.com/UbiquitousLearning/mllm.",True,True,Rongjie Yi and Liwei Guo and Shiyun Wei and Ao Zhou and Shangguang Wang and Mengwei Xu,2025,,https://arxiv.org/abs/2308.14352,,
BuddyMoE: Exploiting Expert Redundancy to Accelerate Memory-Constrained Mixture-of-Experts Inference,2511.10054v1,zhong2024adapmoe,\cite{zhong2024adapmoe},AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for Efficient MoE Inference,https://arxiv.org/abs/2408.10284v1,"Mixture-of-Experts (MoE) models are designed to enhance the efficiency of large language models (LLMs) without proportionally increasing the computational demands. However, their deployment on edge devices still faces significant challenges due to high on-demand loading overheads from managing sparsely activated experts. This paper introduces AdapMoE, an algorithm-system co-design framework for efficient MoE inference. AdapMoE features adaptive expert gating and management to reduce the on-demand loading overheads. We observe the heterogeneity of experts loading across layers and tokens, based on which we propose a sensitivity-based strategy to adjust the number of activated experts dynamically. Meanwhile, we also integrate advanced prefetching and cache management techniques to further reduce the loading latency. Through comprehensive evaluations on various platforms, we demonstrate AdapMoE consistently outperforms existing techniques, reducing the average number of activated experts by 25% and achieving a 1.35x speedup without accuracy degradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE.",True,True,"Zhong, Shuzhang and Liang, Ling and Wang, Yuan and Wang, Runsheng and Huang, Ru and Li, Meng",2024,,http://dx.doi.org/10.1145/3676536.3676741,10.1145/3676536.3676741,
BuddyMoE: Exploiting Expert Redundancy to Accelerate Memory-Constrained Mixture-of-Experts Inference,2511.10054v1,kong2024swapmoe,\cite{kong2024swapmoe},SwapMoE: Serving Off-the-shelf MoE-based Large Language Models with Tunable Memory Budget,,,True,False,Rui Kong and Yuanchun Li and Qingtian Feng and Weijun Wang and Xiaozhou Ye and Ye Ouyang and Linghe Kong and Yunxin Liu,2024,,https://arxiv.org/abs/2308.15030,,
BuddyMoE: Exploiting Expert Redundancy to Accelerate Memory-Constrained Mixture-of-Experts Inference,2511.10054v1,fang2025fate,\cite{fang2025fate},Fate: Fast Edge Inference of Mixture-of-Experts Models via Cross-Layer Gate,https://arxiv.org/abs/2502.12224v2,"Large Language Models (LLMs) have demonstrated impressive performance across various tasks, and their application in edge scenarios has attracted significant attention. However, sparse-activated Mixture-of-Experts (MoE) models, which are well suited for edge scenarios, have received relatively little attention due to their high memory demands. Offload-based methods have been proposed to address this challenge, but they face difficulties with expert prediction. Inaccurate expert predictions can result in prolonged inference delays. To promote the application of MoE models in edge scenarios, we propose Fate, an offloading system designed for MoE models to enable efficient inference in resource-constrained environments. The key insight behind Fate is that gate inputs from adjacent layers can be effectively used for expert prefetching, achieving high prediction accuracy without additional GPU overhead. Furthermore, Fate employs a shallow-favoring expert caching strategy that increases the expert hit rate to 99\%. Additionally, Fate integrates tailored quantization strategies for cache optimization and IO efficiency. Experimental results show that, compared to Load on Demand and Expert Activation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in prefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively, while maintaining inference quality. Moreover, Fate's performance improvements are scalable across different memory budgets.",True,True,Zhiyuan Fang and Zicong Hong and Yuegui Huang and Yufeng Lyu and Wuhui Chen and Yue Yu and Fan Yu and Zibin Zheng,2025,,https://arxiv.org/abs/2502.12224,,
BuddyMoE: Exploiting Expert Redundancy to Accelerate Memory-Constrained Mixture-of-Experts Inference,2511.10054v1,lu2024experts,\cite{lu2024experts},Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models,,,True,False,Xudong Lu and Qi Liu and Yuhui Xu and Aojun Zhou and Siyuan Huang and Bo Zhang and Junchi Yan and Hongsheng Li,2024,,https://arxiv.org/abs/2402.14800,,
PITE: Multi-Prototype Alignment for Individual Treatment Effect Estimation,2511.10320v1,15,\cite{15},Estimating individual treatment effect: generalization bounds and algorithms,https://arxiv.org/abs/1606.03976v5,"There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare, economics and education. In particular, individual-level causal inference has important applications such as precision medicine. We give a new theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational data, under the assumption known as strong ignorability. The algorithms learn a ""balanced"" representation such that the induced treated and control distributions look similar. We give a novel, simple and intuitive generalization-error bound showing that the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distances. Experiments on real and simulated data show the new algorithms match or outperform the state-of-the-art.",True,True,"Shalit, Uri and Johansson, Fredrik D and Sontag, David",2017,,,,
PITE: Multi-Prototype Alignment for Individual Treatment Effect Estimation,2511.10320v1,18,\cite{18},GANITE: Estimation of individualized treatment effects using generative adversarial nets,,,True,False,"Yoon, Jinsung and Jordon, James and Van Der Schaar, Mihaela",2018,,,,
PITE: Multi-Prototype Alignment for Individual Treatment Effect Estimation,2511.10320v1,k4,\cite{k4},Cycle-balanced representation learning for counterfactual inference,,,True,False,"Zhou, Guanglin and Yao, Lina and Xu, Xiwei and Wang, Chen and Zhu, Liming",2022,,,,
PITE: Multi-Prototype Alignment for Individual Treatment Effect Estimation,2511.10320v1,7,\cite{7},Reducing balancing error for causal inference via optimal transport,,,True,False,"Yan, Yuguang and Zhou, Hao and Yang, Zeqin and Chen, Weilin and Cai, Ruichu and Hao, Zhifeng",2024,,,,
PITE: Multi-Prototype Alignment for Individual Treatment Effect Estimation,2511.10320v1,a6,\cite{a6},Optimal transport for treatment effect estimation,,,True,False,"Wang, Hao and Fan, Jiajun and Chen, Zhichao and Li, Haoxuan and Liu, Weiming and Liu, Tianqiao and Dai, Quanyu and Wang, Yichao and Dong, Zhenhua and Tang, Ruiming",2023,,,,Advances in Neural Information Processing Systems
PITE: Multi-Prototype Alignment for Individual Treatment Effect Estimation,2511.10320v1,a1,\cite{a1},The central role of the propensity score in observational studies for causal effects,,,True,False,"Rosenbaum, Paul R and Rubin, Donald B",1983,,,,Biometrika
PITE: Multi-Prototype Alignment for Individual Treatment Effect Estimation,2511.10320v1,16,\cite{16},Representation learning for treatment effect estimation from observational data,,,True,False,"Yao, Liuyi and Li, Sheng and Li, Yaliang and Huai, Mengdi and Gao, Jing and Zhang, Aidong",2018,,,,
PITE: Multi-Prototype Alignment for Individual Treatment Effect Estimation,2511.10320v1,x8,\cite{x8},Contrastive individual treatment effects estimation,,,True,False,"Li, Xinshu and Yao, Lina",2022,,,,
PITE: Multi-Prototype Alignment for Individual Treatment Effect Estimation,2511.10320v1,zhangcounterfactual,\cite{zhangcounterfactual},Counterfactual Contrastive Learning with Normalizing Flows for Robust Treatment Effect Estimation,,,True,False,"Zhang, Jiaxuan and Eldele, Emadeldeen and Wang, Yang and Li, Xiaoli and Liang, Jiye and others",2025,,,,
Flex-MIG: Enabling Distributed Execution on MIG,2511.09143v2,nvidia-mps,\cite{nvidia-mps},Multi-Process Service,,,True,False,NVIDIA,2024,,,,
Flex-MIG: Enabling Distributed Execution on MIG,2511.09143v2,TGS,\cite{TGS},Transparent {GPU} Sharing in Container Clouds for Deep Learning Workloads,,,True,False,Bingyang Wu and Zili Zhang and Zhihao Bai and Xuanzhe Liu and Xin Jin,2023,,https://www.usenix.org/conference/nsdi23/presentation/wu,,
Flex-MIG: Enabling Distributed Execution on MIG,2511.09143v2,Orion,\cite{Orion},"Orion: Interference-aware, fine-grained GPU sharing for ML applications",,,True,False,"Strati, Foteini and Ma, Xianzhe and Klimovic, Ana",2024,,,,
Flex-MIG: Enabling Distributed Execution on MIG,2511.09143v2,nvidia-mig,\cite{nvidia-mig},NVIDIA MIG User Guide,,,True,False,NVIDIA,2025,,,,
Flex-MIG: Enabling Distributed Execution on MIG,2511.09143v2,MIGER,\cite{MIGER},MIGER: Integrating Multi-Instance GPU and Multi-Process Service for Deep Learning Clusters,,,True,False,"Zhang, Bowen and Li, Shuxin and Li, Zhuozhao",2024,,,,
Flex-MIG: Enabling Distributed Execution on MIG,2511.09143v2,parva,\cite{parva},ParvaGPU: Efficient Spatial GPU Sharing for Large-Scale DNN Inference in Cloud Environments,https://arxiv.org/abs/2409.14447v1,"In cloud environments, GPU-based deep neural network (DNN) inference servers are required to meet the Service Level Objective (SLO) latency for each workload under a specified request rate, while also minimizing GPU resource consumption. However, previous studies have not fully achieved this objective. In this paper, we propose ParvaGPU, a technology that facilitates spatial GPU sharing for large-scale DNN inference in cloud computing. ParvaGPU integrates NVIDIA's Multi-Instance GPU (MIG) and Multi-Process Service (MPS) technologies to enhance GPU utilization, with the goal of meeting the diverse SLOs of each workload and reducing overall GPU usage. Specifically, ParvaGPU addresses the challenges of minimizing underutilization within allocated GPU space partitions and external fragmentation in combined MIG and MPS environments. We conducted our assessment on multiple A100 GPUs, evaluating 11 diverse DNN workloads with varying SLOs. Our evaluation revealed no SLO violations and a significant reduction in GPU usage compared to state-of-the-art frameworks.",True,True,"Lee, Munkyu and Seong, Sihoon and Kang, Minki and Lee, Jihyuk and Na, Gap-Joo and Chun, In-Geol and Nikolopoulos, Dimitrios and Hong, Cheol-Ho",2024,,https://doi.org/10.1109/SC41406.2024.00048,10.1109/SC41406.2024.00048,
PDAC: Efficient Coreset Selection for Continual Learning via Probability Density Awareness,2511.09487v1,tsang2005core,\cite{tsang2005core},Core vector machines: Fast SVM training on very large data sets.,,,True,False,"Tsang, Ivor W and Kwok, James T and Cristianini, Nello and others",2005,,,,Journal of Machine Learning Research
PDAC: Efficient Coreset Selection for Continual Learning via Probability Density Awareness,2511.09487v1,HugginsCB16,\cite{HugginsCB16},Coresets for Scalable Bayesian Logistic Regression,,,True,False,"Jonathan H. Huggins and
                  Trevor Campbell and
                  Tamara Broderick",2016,,,,
PDAC: Efficient Coreset Selection for Continual Learning via Probability Density Awareness,2511.09487v1,MunteanuSSW18,\cite{MunteanuSSW18},On Coresets for Logistic Regression,,,True,False,"Alexander Munteanu and
                  Chris Schwiegelshohn and
                  Christian Sohler and
                  David P. Woodruff",2018,,,,
PDAC: Efficient Coreset Selection for Continual Learning via Probability Density Awareness,2511.09487v1,Har-PeledM04,\cite{Har-PeledM04},On coresets for k-means and k-median clustering,,,True,False,"Sariel Har{-}Peled and
                  Soham Mazumdar",2004,,,,
PDAC: Efficient Coreset Selection for Continual Learning via Probability Density Awareness,2511.09487v1,har2005smaller,\cite{har2005smaller},Smaller coresets for k-median and k-means clustering,,,True,False,"Har-Peled, Sariel and Kushal, Akash",2005,,,,
PDAC: Efficient Coreset Selection for Continual Learning via Probability Density Awareness,2511.09487v1,FeldmanL11,\cite{FeldmanL11},A unified framework for approximating and clustering data,,,True,False,"Dan Feldman and
                  Michael Langberg",2011,,,,
PDAC: Efficient Coreset Selection for Continual Learning via Probability Density Awareness,2511.09487v1,FeldmanFK11,\cite{FeldmanFK11},Scalable Training of Mixture Models via Coresets,,,True,False,"Dan Feldman and
                  Matthew Faulkner and
                  Andreas Krause",2011,,,,
PDAC: Efficient Coreset Selection for Continual Learning via Probability Density Awareness,2511.09487v1,lucic2018training,\cite{lucic2018training},Training Gaussian Mixture Models at Scale via Coresets,https://arxiv.org/abs/1703.08110v2,"How can we train a statistical mixture model on a massive data set? In this work we show how to construct coresets for mixtures of Gaussians. A coreset is a weighted subset of the data, which guarantees that models fitting the coreset also provide a good fit for the original data set. We show that, perhaps surprisingly, Gaussian mixtures admit coresets of size polynomial in dimension and the number of mixture components, while being independent of the data set size. Hence, one can harness computationally intensive algorithms to compute a good approximation on a significantly smaller data set. More importantly, such coresets can be efficiently constructed both in distributed and streaming settings and do not impose restrictions on the data generating process. Our results rely on a novel reduction of statistical estimation to problems in computational geometry and new combinatorial complexity results for mixtures of Gaussians. Empirical evaluation on several real-world datasets suggests that our coreset-based approach enables significant reduction in training-time with negligible approximation error.",True,True,"Lucic, Mario and Faulkner, Matthew and Krause, Andreas and Feldman, Dan",2018,,,,Journal of Machine Learning Research
PDAC: Efficient Coreset Selection for Continual Learning via Probability Density Awareness,2511.09487v1,NguyenLBT18,\cite{NguyenLBT18},Variational Continual Learning,https://arxiv.org/abs/1710.10628v3,"This paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that VCL outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.",True,True,"Cuong V. Nguyen and
                  Yingzhen Li and
                  Thang D. Bui and
                  Richard E. Turner",2018,,,,
PDAC: Efficient Coreset Selection for Continual Learning via Probability Density Awareness,2511.09487v1,gonzalez1985clustering,\cite{gonzalez1985clustering},Clustering to minimize the maximum intercluster distance,,,True,False,"Gonzalez, Teofilo F",1985,,,,Theoretical computer science
PDAC: Efficient Coreset Selection for Continual Learning via Probability Density Awareness,2511.09487v1,YoonMYH22,\cite{YoonMYH22},Online Coreset Selection for Rehearsal-based Continual Learning,,,True,False,"Jaehong Yoon and
                  Divyam Madaan and
                  Eunho Yang and
                  Sung Ju Hwang",2022,,,,
PDAC: Efficient Coreset Selection for Continual Learning via Probability Density Awareness,2511.09487v1,TiwariKIS22,\cite{TiwariKIS22},GCR: Gradient Coreset Based Replay Buffer Selection For Continual Learning,https://arxiv.org/abs/2111.11210v3,"Continual learning (CL) aims to develop techniques by which a single model adapts to an increasing number of tasks encountered sequentially, thereby potentially leveraging learnings across tasks in a resource-efficient manner. A major challenge for CL systems is catastrophic forgetting, where earlier tasks are forgotten while learning a new task. To address this, replay-based CL approaches maintain and repeatedly retrain on a small buffer of data selected across encountered tasks. We propose Gradient Coreset Replay (GCR), a novel strategy for replay buffer selection and update using a carefully designed optimization criterion. Specifically, we select and maintain a ""coreset"" that closely approximates the gradient of all the data seen so far with respect to current model parameters, and discuss key strategies needed for its effective application to the continual learning setting. We show significant gains (2%-4% absolute) over the state-of-the-art in the well-studied offline continual learning setting. Our findings also effectively transfer to online / streaming CL settings, showing upto 5% gains over existing approaches. Finally, we demonstrate the value of supervised contrastive loss for continual learning, which yields a cumulative gain of up to 5% accuracy when combined with our subset selection strategy.",True,True,"Rishabh Tiwari and
                  KrishnaTeja Killamsetty and
                  Rishabh K. Iyer and
                  Pradeep Shenoy",2022,,,,
PDAC: Efficient Coreset Selection for Continual Learning via Probability Density Awareness,2511.09487v1,BorsosM020,\cite{BorsosM020},Coresets via Bilevel Optimization for Continual Learning and Streaming,,,True,False,"Zal{\'{a}}n Borsos and
                  Mojmir Mutny and
                  Andreas Krause",2020,,,,
PDAC: Efficient Coreset Selection for Continual Learning via Probability Density Awareness,2511.09487v1,ZhouPZLCZ22,\cite{ZhouPZLCZ22},Probabilistic Bilevel Coreset Selection,https://arxiv.org/abs/2301.09880v1,"The goal of coreset selection in supervised learning is to produce a weighted subset of data, so that training only on the subset achieves similar performance as training on the entire dataset. Existing methods achieved promising results in resource-constrained scenarios such as continual learning and streaming. However, most of the existing algorithms are limited to traditional machine learning models. A few algorithms that can handle large models adopt greedy search approaches due to the difficulty in solving the discrete subset selection problem, which is computationally costly when coreset becomes larger and often produces suboptimal results. In this work, for the first time we propose a continuous probabilistic bilevel formulation of coreset selection by learning a probablistic weight for each training sample. The overall objective is posed as a bilevel optimization problem, where 1) the inner loop samples coresets and train the model to convergence and 2) the outer loop updates the sample probability progressively according to the model's performance. Importantly, we develop an efficient solver to the bilevel optimization problem via unbiased policy gradient without trouble of implicit differentiation. We provide the convergence property of our training procedure and demonstrate the superiority of our algorithm against various coreset selection methods in various tasks, especially in more challenging label-noise and class-imbalance scenarios.",True,True,"Xiao Zhou and
                  Renjie Pi and
                  Weizhong Zhang and
                  Yong Lin and
                  Zonghao Chen and
                  Tong Zhang",2022,,,,
PDAC: Efficient Coreset Selection for Continual Learning via Probability Density Awareness,2511.09487v1,HaoJL23,\cite{HaoJL23},"Bilevel Coreset Selection in Continual Learning: {A} New Formulation
                  and Algorithm",,,True,False,"Jie Hao and
                  Kaiyi Ji and
                  Mingrui Liu",2023,,,,
PDAC: Efficient Coreset Selection for Continual Learning via Probability Density Awareness,2511.09487v1,chaudhry2019tiny,\cite{chaudhry2019tiny},On Tiny Episodic Memories in Continual Learning,https://arxiv.org/abs/1902.10486v4,"In continual learning (CL), an agent learns from a stream of tasks leveraging prior experience to transfer knowledge to future tasks. It is an ideal framework to decrease the amount of supervision in the existing learning algorithms. But for a successful knowledge transfer, the learner needs to remember how to perform previous tasks. One way to endow the learner the ability to perform tasks seen in the past is to store a small memory, dubbed episodic memory, that stores few examples from previous tasks and then to replay these examples when training for future tasks. In this work, we empirically analyze the effectiveness of a very small episodic memory in a CL setup where each training example is only seen once. Surprisingly, across four rather different supervised learning benchmarks adapted to CL, a very simple baseline, that jointly trains on both examples from the current task as well as examples stored in the episodic memory, significantly outperforms specifically designed CL approaches with and without episodic memory. Interestingly, we find that repetitive training on even tiny memories of past tasks does not harm generalization, on the contrary, it improves it, with gains between 7\% and 17\% when the memory is populated with a single example per class.",True,True,"Chaudhry, Arslan and Rohrbach, Marcus and Elhoseiny, Mohamed and Ajanthan, Thalaiyasingam and Dokania, Puneet K and Torr, Philip HS and Ranzato, Marc'Aurelio",2019,,,,arXiv preprint arXiv:1902.10486
PDAC: Efficient Coreset Selection for Continual Learning via Probability Density Awareness,2511.09487v1,hayes2019memory,\cite{hayes2019memory},Memory efficient experience replay for streaming learning,,,True,False,"Hayes, Tyler L and Cahill, Nathan D and Kanan, Christopher",2019,,,,
PDAC: Efficient Coreset Selection for Continual Learning via Probability Density Awareness,2511.09487v1,RebuffiKSL17,\cite{RebuffiKSL17},iCaRL: Incremental Classifier and Representation Learning,,,True,False,"Sylvestre{-}Alvise Rebuffi and
                  Alexander Kolesnikov and
                  Georg Sperl and
                  Christoph H. Lampert",2017,,,,
PDAC: Efficient Coreset Selection for Continual Learning via Probability Density Awareness,2511.09487v1,li2023variational,\cite{li2023variational},Variational data-free knowledge distillation for continual learning,,,True,False,"Li, Xiaorong and Wang, Shipeng and Sun, Jian and Xu, Zongben",2023,,,,IEEE Transactions on Pattern Analysis and Machine Intelligence
PDAC: Efficient Coreset Selection for Continual Learning via Probability Density Awareness,2511.09487v1,BuzzegaBPAC20,\cite{BuzzegaBPAC20},"Dark Experience for General Continual Learning: a Strong, Simple Baseline",,,True,False,"Pietro Buzzega and
                  Matteo Boschini and
                  Angelo Porrello and
                  Davide Abati and
                  Simone Calderara",2020,,,,
PDAC: Efficient Coreset Selection for Continual Learning via Probability Density Awareness,2511.09487v1,QiGCLLWZ25,\cite{QiGCLLWZ25},Contrastive Augmented Graph2Graph Memory Interaction for Few Shot Continual Learning,https://arxiv.org/abs/2403.04140v1,"Few-Shot Class-Incremental Learning (FSCIL) has gained considerable attention in recent years for its pivotal role in addressing continuously arriving classes. However, it encounters additional challenges. The scarcity of samples in new sessions intensifies overfitting, causing incompatibility between the output features of new and old classes, thereby escalating catastrophic forgetting. A prevalent strategy involves mitigating catastrophic forgetting through the Explicit Memory (EM), which comprise of class prototypes. However, current EM-based methods retrieves memory globally by performing Vector-to-Vector (V2V) interaction between features corresponding to the input and prototypes stored in EM, neglecting the geometric structure of local features. This hinders the accurate modeling of their positional relationships. To incorporate information of local geometric structure, we extend the V2V interaction to Graph-to-Graph (G2G) interaction. For enhancing local structures for better G2G alignment and the prevention of local feature collapse, we propose the Local Graph Preservation (LGP) mechanism. Additionally, to address sample scarcity in classes from new sessions, the Contrast-Augmented G2G (CAG2G) is introduced to promote the aggregation of same class features thus helps few-shot learning. Extensive comparisons on CIFAR100, CUB200, and the challenging ImageNet-R dataset demonstrate the superiority of our method over existing methods.",True,True,"Biqing Qi and
                  Junqi Gao and
                  Xinquan Chen and
                  Dong Li and
                  Jianxing Liu and
                  Ligang Wu and
                  Bowen Zhou",2025,,,,IEEE Transactions on Circuits and Systems for Video Technology
PDAC: Efficient Coreset Selection for Continual Learning via Probability Density Awareness,2511.09487v1,Lopez-PazR17,\cite{Lopez-PazR17},Gradient Episodic Memory for Continual Learning,,,True,False,"David Lopez{-}Paz and
                  Marc'Aurelio Ranzato",2017,,,,
PDAC: Efficient Coreset Selection for Continual Learning via Probability Density Awareness,2511.09487v1,ChaudhryRRE19,\cite{ChaudhryRRE19},Efficient Lifelong Learning with {A-GEM},,,True,False,"Arslan Chaudhry and
                  Marc'Aurelio Ranzato and
                  Marcus Rohrbach and
                  Mohamed Elhoseiny",2019,,,,
FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment,2511.10250v1,shao2020finegym,\cite{shao2020finegym},{Finegym: A Hierarchical Video Dataset for Fine-Grained Action Understanding},,,True,False,"Shao, Dian and Zhao, Yue and Dai, Bo and Lin, Dahua",2020,,,,
FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment,2511.10250v1,ramanathan2014human,\cite{ramanathan2014human},{Human Action Recognition with Video Data: Research and Evaluation Challenges},,,True,False,"Ramanathan, Manoj and Yau, Wei-Yun and Teoh, Eam Khwang",2014,,,,
FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment,2511.10250v1,kong2022human,\cite{kong2022human},{Human Action Recognition and Prediction: A Survey},,,True,False,"Kong, Yu and Fu, Yun",2022,,,,
FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment,2511.10250v1,sportshhi,\cite{sportshhi},{SportsHHI: A Dataset for Human-Human Interaction Detection in Sports Videos},,,True,False,"Wu, Tao and He, Runyu and Wu, Gangshan and Wang, Limin",2024,,,,
FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment,2511.10250v1,sun2022human,\cite{sun2022human},Human Action Recognition from Various Data Modalities: A Review,https://arxiv.org/abs/2012.11866v5,"Human Action Recognition (HAR) aims to understand human behavior and assign a label to each action. It has a wide range of applications, and therefore has been attracting increasing attention in the field of computer vision. Human actions can be represented using various data modalities, such as RGB, skeleton, depth, infrared, point cloud, event stream, audio, acceleration, radar, and WiFi signal, which encode different sources of useful yet distinct information and have various advantages depending on the application scenarios. Consequently, lots of existing works have attempted to investigate different types of approaches for HAR using various modalities. In this paper, we present a comprehensive survey of recent progress in deep learning methods for HAR based on the type of input data modality. Specifically, we review the current mainstream deep learning methods for single data modalities and multiple data modalities, including the fusion-based and the co-learning-based frameworks. We also present comparative results on several benchmark datasets for HAR, together with insightful observations and inspiring future research directions.",True,True,"Sun, Zehua and Ke, Qiuhong and Rahmani, Hossein and Bennamoun, Mohammed and Wang, Gang and Liu, Jun",2022,,,,
FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment,2511.10250v1,wang2021survey,\cite{wang2021survey},A Survey of Video-based Action Quality Assessment,https://arxiv.org/abs/2204.09271v1,"Human action recognition and analysis have great demand and important application significance in video surveillance, video retrieval, and human-computer interaction. The task of human action quality evaluation requires the intelligent system to automatically and objectively evaluate the action completed by the human. The action quality assessment model can reduce the human and material resources spent in action evaluation and reduce subjectivity. In this paper, we provide a comprehensive survey of existing papers on video-based action quality assessment. Different from human action recognition, the application scenario of action quality assessment is relatively narrow. Most of the existing work focuses on sports and medical care. We first introduce the definition and challenges of human action quality assessment. Then we present the existing datasets and evaluation metrics. In addition, we summarized the methods of sports and medical care according to the model categories and publishing institutions according to the characteristics of the two fields. At the end, combined with recent work, the promising development direction in action quality assessment is discussed.",True,True,"Wang, Shunli and Yang, Dingkang and Zhai, Peng and Yu, Qing and Suo, Tao and Sun, Zhan and Li, Ka and Zhang, Lihua",2021,,,,
FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment,2511.10250v1,zahan2024learning,\cite{zahan2024learning},Learning Sparse Temporal Video Mapping for Action Quality Assessment in Floor Gymnastics,https://arxiv.org/abs/2301.06103v1,"Athlete performance measurement in sports videos requires modeling long sequences since the entire spatio-temporal progression contributes dominantly to the performance. It is crucial to comprehend local discriminative spatial dependencies and global semantics for accurate evaluation. However, existing benchmark datasets mainly incorporate sports where the performance lasts only a few seconds. Consequently, state-ofthe-art sports quality assessment methods specifically focus on spatial structure. Although they achieve high performance in short-term sports, they are unable to model prolonged video sequences and fail to achieve similar performance in long-term sports. To facilitate such analysis, we introduce a new dataset, coined AGF-Olympics, that incorporates artistic gymnastic floor routines. AFG-Olympics provides highly challenging scenarios with extensive background, viewpoint, and scale variations over an extended sample duration of up to 2 minutes. In addition, we propose a discriminative attention module to map the dense feature space into a sparse representation by disentangling complex associations. Extensive experiments indicate that our proposed module provides an effective way to embed long-range spatial and temporal correlation semantics.",True,True,"Zahan, Sania and Hassan, Ghulam Mubashar and Mian, Ajmal",2024,,,,
FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment,2511.10250v1,limagr,\cite{limagr},MAGR: Manifold-Aligned Graph Regularization for Continual Action Quality Assessment,https://arxiv.org/abs/2403.04398v2,"Action Quality Assessment (AQA) evaluates diverse skills but models struggle with non-stationary data. We propose Continual AQA (CAQA) to refine models using sparse new data. Feature replay preserves memory without storing raw inputs. However, the misalignment between static old features and the dynamically changing feature manifold causes severe catastrophic forgetting. To address this novel problem, we propose Manifold-Aligned Graph Regularization (MAGR), which first aligns deviated old features to the current feature manifold, ensuring representation consistency. It then constructs a graph jointly arranging old and new features aligned with quality scores. Experiments show MAGR outperforms recent strong baselines with up to 6.56%, 5.66%, 15.64%, and 9.05% correlation gains on the MTL-AQA, FineDiving, UNLV-Dive, and JDM-MSA split datasets, respectively. This validates MAGR for continual assessment challenges arising from non-stationary skill variations. Code is available at https://github.com/ZhouKanglei/MAGR_CAQA}{https://github.com/ZhouKanglei/MAGR_CAQA.",True,True,"Li, Frederick WB and Li, Jianguo and Liang, Xiaohui",2024,,,,
FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment,2511.10250v1,gao2023automatic,\cite{gao2023automatic},{Automatic Modelling for Interactive Action Assessment},,,True,False,"Gao, Jibin and Pan, Jia-Hui and Zhang, Shao-Jie and Zheng, Wei-Shi",2023,,,,
FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment,2511.10250v1,zeng2024multimodal,\cite{zeng2024multimodal},Multimodal Action Quality Assessment,https://arxiv.org/abs/2402.09444v3,"Action quality assessment (AQA) is to assess how well an action is performed. Previous works perform modelling by only the use of visual information, ignoring audio information. We argue that although AQA is highly dependent on visual information, the audio is useful complementary information for improving the score regression accuracy, especially for sports with background music, such as figure skating and rhythmic gymnastics. To leverage multimodal information for AQA, i.e., RGB, optical flow and audio information, we propose a Progressive Adaptive Multimodal Fusion Network (PAMFN) that separately models modality-specific information and mixed-modality information. Our model consists of with three modality-specific branches that independently explore modality-specific information and a mixed-modality branch that progressively aggregates the modality-specific information from the modality-specific branches. To build the bridge between modality-specific branches and the mixed-modality branch, three novel modules are proposed. First, a Modality-specific Feature Decoder module is designed to selectively transfer modality-specific information to the mixed-modality branch. Second, when exploring the interaction between modality-specific information, we argue that using an invariant multimodal fusion policy may lead to suboptimal results, so as to take the potential diversity in different parts of an action into consideration. Therefore, an Adaptive Fusion Module is proposed to learn adaptive multimodal fusion policies in different parts of an action. This module consists of several FusionNets for exploring different multimodal fusion strategies and a PolicyNet for deciding which FusionNets are enabled. Third, a module called Cross-modal Feature Decoder is designed to transfer cross-modal features generated by Adaptive Fusion Module to the mixed-modality branch.",True,True,"Zeng, Ling-An and Zheng, Wei-Shi",2024,,,,
FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment,2511.10250v1,SVR,\cite{SVR},Learning To Score Olympic Events,https://arxiv.org/abs/1611.05125v3,"Estimating action quality, the process of assigning a ""score"" to the execution of an action, is crucial in areas such as sports and health care. Unlike action recognition, which has millions of examples to learn from, the action quality datasets that are currently available are small -- typically comprised of only a few hundred samples. This work presents three frameworks for evaluating Olympic sports which utilize spatiotemporal features learned using 3D convolutional neural networks (C3D) and perform score regression with i) SVR, ii) LSTM, and iii) LSTM followed by SVR. An efficient training mechanism for the limited data scenarios is presented for clip-based training with LSTM. The proposed systems show significant improvement over existing quality assessment approaches on the task of predicting scores of Olympic events {diving, vault, figure skating}. While the SVR-based frameworks yield better results, LSTM-based frameworks are more natural for describing an action and can be used for improvement feedback.",True,True,"Parmar, Paritosh and Tran Morris, Brendan",2017,,,,
FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment,2511.10250v1,baller,\cite{baller},Am I a Baller? Basketball Performance Assessment from First-Person Videos,https://arxiv.org/abs/1611.05365v4,"This paper presents a method to assess a basketball player's performance from his/her first-person video. A key challenge lies in the fact that the evaluation metric is highly subjective and specific to a particular evaluator. We leverage the first-person camera to address this challenge. The spatiotemporal visual semantics provided by a first-person view allows us to reason about the camera wearer's actions while he/she is participating in an unscripted basketball game. Our method takes a player's first-person video and provides a player's performance measure that is specific to an evaluator's preference.
  To achieve this goal, we first use a convolutional LSTM network to detect atomic basketball events from first-person videos. Our network's ability to zoom-in to the salient regions addresses the issue of a severe camera wearer's head movement in first-person videos. The detected atomic events are then passed through the Gaussian mixtures to construct a highly non-linear visual spatiotemporal basketball assessment feature. Finally, we use this feature to learn a basketball assessment model from pairs of labeled first-person basketball videos, for which a basketball expert indicates, which of the two players is better.
  We demonstrate that despite not knowing the basketball evaluator's criterion, our model learns to accurately assess the players in real-world games. Furthermore, our model can also discover basketball events that contribute positively and negatively to a player's performance.",True,True,"Bertasius, Gedas and Soo Park, Hyun and Yu, Stella X and Shi, Jianbo",2017,,,,
FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment,2511.10250v1,pan2019action,\cite{pan2019action},{Action Assessment by Joint Relation Graphs},,,True,False,"Pan, Jia-Hui and Gao, Jibin and Zheng, Wei-Shi",2019,,,,
FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment,2511.10250v1,venkataraman2015dynamical,\cite{venkataraman2015dynamical},{Dynamical Regularity for Action Analysis},,,True,False,"Venkataraman, Vinay and Vlachos, Ioannis and Turaga, Pavan K",2015,,,,
FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment,2511.10250v1,ActionNet,\cite{ActionNet},Hybrid Dynamic-static Context-aware Attention Network for Action Assessment in Long Videos,https://arxiv.org/abs/2008.05977v1,"The objective of action quality assessment is to score sports videos. However, most existing works focus only on video dynamic information (i.e., motion information) but ignore the specific postures that an athlete is performing in a video, which is important for action assessment in long videos. In this work, we present a novel hybrid dynAmic-static Context-aware attenTION NETwork (ACTION-NET) for action assessment in long videos. To learn more discriminative representations for videos, we not only learn the video dynamic information but also focus on the static postures of the detected athletes in specific frames, which represent the action quality at certain moments, along with the help of the proposed hybrid dynamic-static architecture. Moreover, we leverage a context-aware attention module consisting of a temporal instance-wise graph convolutional network unit and an attention unit for both streams to extract more robust stream features, where the former is for exploring the relations between instances and the latter for assigning a proper weight to each instance. Finally, we combine the features of the two streams to regress the final video score, supervised by ground-truth scores given by experts. Additionally, we have collected and annotated the new Rhythmic Gymnastics dataset, which contains videos of four different types of gymnastics routines, for evaluation of action quality assessment in long videos. Extensive experimental results validate the efficacy of our proposed method, which outperforms related approaches. The codes and dataset are available at \url{https://github.com/lingan1996/ACTION-NET}.",True,True,"Zeng, Ling-An and Hong, Fa-Ting and Zheng, Wei-Shi and Yu, Qi-Zhi and Zeng, Wei and Wang, Yao-Wei and Lai, Jian-Huang",2020,,,,
FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment,2511.10250v1,gdlt,\cite{gdlt},{Likert Scoring with Grade Decoupling for Long-Term Action Assessment},,,True,False,"Xu, Angchi and Zeng, Ling-An and Zheng, Wei-Shi",2022,,,,
FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment,2511.10250v1,CoRe,\cite{CoRe},{Group-Aware Contrastive Regression for Action Quality Assessment},,,True,False,"Yu, Xumin and Rao, Yongming and Zhao, Wenliang and Lu, Jiwen and Zhou, Jie",2021,,,,
FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment,2511.10250v1,cofinal,\cite{cofinal},CoFInAl: Enhancing Action Quality Assessment with Coarse-to-Fine Instruction Alignment,https://arxiv.org/abs/2404.13999v1,"Action Quality Assessment (AQA) is pivotal for quantifying actions across domains like sports and medical care. Existing methods often rely on pre-trained backbones from large-scale action recognition datasets to boost performance on smaller AQA datasets. However, this common strategy yields suboptimal results due to the inherent struggle of these backbones to capture the subtle cues essential for AQA. Moreover, fine-tuning on smaller datasets risks overfitting. To address these issues, we propose Coarse-to-Fine Instruction Alignment (CoFInAl). Inspired by recent advances in large language model tuning, CoFInAl aligns AQA with broader pre-trained tasks by reformulating it as a coarse-to-fine classification task. Initially, it learns grade prototypes for coarse assessment and then utilizes fixed sub-grade prototypes for fine-grained assessment. This hierarchical approach mirrors the judging process, enhancing interpretability within the AQA framework. Experimental results on two long-term AQA datasets demonstrate CoFInAl achieves state-of-the-art performance with significant correlation gains of 5.49% and 3.55% on Rhythmic Gymnastics and Fis-V, respectively. Our code is available at https://github.com/ZhouKanglei/CoFInAl_AQA.",True,True,"Zhou, Kanglei and Li, Junlin and Cai, Ruizhi and Wang, Liyuan and Zhang, Xinxgxing and Liang Xiaohui",2024,,,,
FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment,2511.10250v1,finediving,\cite{finediving},{Finediving: A Fine-Grained Dataset for Procedure-Aware Action Quality Assessment},,,True,False,"Xu, Jinglin and Rao, Yongming and Yu, Xumin and Chen, Guangyi and Zhou, Jie and Lu, Jiwen",2022,,,,
FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment,2511.10250v1,TPT,\cite{TPT},{Action Quality Assessment with Temporal Parsing Transformer},,,True,False,"Bai, Yang and Zhou, Desen and Zhang, Songyang and Wang, Jian and Ding, Errui and Guan, Yu and Long, Yang and Wang, Jingdong",2022,,,,
FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment,2511.10250v1,rica,\cite{rica},"{RICA\^{} 2: Rubric-Informed, Calibrated Assessment of Actions}",,,True,False,"Majeedi, Abrar and Gajjala, Viswanatha Reddy and GNVV, Satya Sai Srinath Namburi and Li, Yin",2024,,,,
FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment,2511.10250v1,finedving+,\cite{finedving+},{Procedure-Aware Action Quality Assessment: Datasets and Performance Evaluation},,,True,False,"Xu, Jinglin and Rao, Yongming and Zhou, Jie and Lu, Jiwen",2024,,,,
FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment,2511.10250v1,MIT,\cite{MIT},{Assessing the Quality of Actions},,,True,False,"Pirsiavash, Hamed and Vondrick, Carl and Torralba, Antonio",2014,,,,
FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment,2511.10250v1,MS-LSTM,\cite{MS-LSTM},Learning to score the figure skating sports videos,https://arxiv.org/abs/1802.02774v3,"This paper targets at learning to score the figure skating sports videos. To address this task, we propose a deep architecture that includes two complementary components, i.e., Self-Attentive LSTM and Multi-scale Convolutional Skip LSTM. These two components can efficiently learn the local and global sequential information in each video. Furthermore, we present a large-scale figure skating sports video dataset -- FisV dataset. This dataset includes 500 figure skating videos with the average length of 2 minutes and 50 seconds. Each video is annotated by two scores of nine different referees, i.e., Total Element Score(TES) and Total Program Component Score (PCS). Our proposed model is validated on FisV and MIT-skate datasets. The experimental results show the effectiveness of our models in learning to score the figure skating videos.",True,True,"Xu, Chengming and Fu, Yanwei and Zhang, Bing and Chen, Zitian and Jiang, Yu-Gang and Xue, Xiangyang",2019,,,,
FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment,2511.10250v1,HGCN,\cite{HGCN},{Hierarchical Graph Convolutional Networks for Action Quality Assessment},,,True,False,"Zhou, Kanglei and Ma, Yue and Shum, Hubert PH and Liang, Xiaohui",2023,,,,
FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment,2511.10250v1,UNLV,\cite{UNLV},Learning To Score Olympic Events,https://arxiv.org/abs/1611.05125v3,"Estimating action quality, the process of assigning a ""score"" to the execution of an action, is crucial in areas such as sports and health care. Unlike action recognition, which has millions of examples to learn from, the action quality datasets that are currently available are small -- typically comprised of only a few hundred samples. This work presents three frameworks for evaluating Olympic sports which utilize spatiotemporal features learned using 3D convolutional neural networks (C3D) and perform score regression with i) SVR, ii) LSTM, and iii) LSTM followed by SVR. An efficient training mechanism for the limited data scenarios is presented for clip-based training with LSTM. The proposed systems show significant improvement over existing quality assessment approaches on the task of predicting scores of Olympic events {diving, vault, figure skating}. While the SVR-based frameworks yield better results, LSTM-based frameworks are more natural for describing an action and can be used for improvement feedback.",True,True,"Parmar, Paritosh and Tran Morris, Brendan",2017,,,,
FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment,2511.10250v1,action,\cite{action},Action Quality Assessment Across Multiple Actions,https://arxiv.org/abs/1812.06367v2,"Can learning to measure the quality of an action help in measuring the quality of other actions? If so, can consolidated samples from multiple actions help improve the performance of current approaches? In this paper, we carry out experiments to see if knowledge transfer is possible in the action quality assessment (AQA) setting. Experiments are carried out on our newly released AQA dataset (http://rtis.oit.unlv.edu/datasets.html) consisting of 1106 action samples from seven actions with quality scores as measured by expert human judges. Our experimental results show that there is utility in learning a single model across multiple actions.",True,True,"Parmar, Paritosh and Morris, Brendan",2019,,,,
FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment,2511.10250v1,MTL,\cite{MTL},{What and how Well You Performed? A Multitask Learning Approach to Action Quality Assessment},,,True,False,"Parmar, Paritosh and Morris, Brendan Tran",2019,,,,
"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation",2511.09585v1,forsgren6riffusion,\cite{forsgren6riffusion},Riffusion-Stable diffusion for real-time music generation,,,True,False,"Forsgren, Seth and Martiros, Hayk",2022,,,,URL https://riffusion. com/about
"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation",2511.09585v1,huang2023noise2music,\cite{huang2023noise2music},Noise2Music: Text-conditioned Music Generation with Diffusion Models,https://arxiv.org/abs/2302.03917v2,"We introduce Noise2Music, where a series of diffusion models is trained to generate high-quality 30-second music clips from text prompts. Two types of diffusion models, a generator model, which generates an intermediate representation conditioned on text, and a cascader model, which generates high-fidelity audio conditioned on the intermediate representation and possibly the text, are trained and utilized in succession to generate high-fidelity music. We explore two options for the intermediate representation, one using a spectrogram and the other using audio with lower fidelity. We find that the generated audio is not only able to faithfully reflect key elements of the text prompt such as genre, tempo, instruments, mood, and era, but goes beyond to ground fine-grained semantics of the prompt. Pretrained large language models play a key role in this story -- they are used to generate paired text for the audio of the training set and to extract embeddings of the text prompts ingested by the diffusion models.
  Generated examples: https://google-research.github.io/noise2music",True,True,"Huang, Qingqing and Park, Daniel S and Wang, Tao and Denk, Timo I and Ly, Andy and Chen, Nanxin and Zhang, Zhengdong and Zhang, Zhishuai and Yu, Jiahui and Frank, Christian and others",2023,,,,arXiv preprint arXiv:2302.03917
"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation",2511.09585v1,schneider2023mo,\cite{schneider2023mo},Mo\^{u}sai: Text-to-music generation with long-context latent diffusion,,,True,False,"Schneider, Flavio and Kamal, Ojasv and Jin, Zhijing and Sch{\""o}lkopf, Bernhard",2023,,,,arXiv preprint arXiv:2301.11757
"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation",2511.09585v1,liu2024audioldm,\cite{liu2024audioldm},AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining,https://arxiv.org/abs/2308.05734v3,"Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework introduces a general representation of audio, called ""language of audio"" (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate any modalities into LOA by using a GPT-2 model, and we perform self-supervised audio generation learning with a latent diffusion model conditioned on LOA. The proposed framework naturally brings advantages such as in-context learning abilities and reusable self-supervised pretrained AudioMAE and latent diffusion models. Experiments on the major benchmarks of text-to-audio, text-to-music, and text-to-speech demonstrate state-of-the-art or competitive performance against previous approaches. Our code, pretrained model, and demo are available at https://audioldm.github.io/audioldm2.",True,True,"Liu, Haohe and Yuan, Yi and Liu, Xubo and Mei, Xinhao and Kong, Qiuqiang and Tian, Qiao and Wang, Yuping and Wang, Wenwu and Wang, Yuxuan and Plumbley, Mark D",2024,,,,"IEEE/ACM Transactions on Audio, Speech, and Language Processing"
"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation",2511.09585v1,novack2024ditto,\cite{novack2024ditto},DITTO: Diffusion Inference-Time T-Optimization for Music Generation,https://arxiv.org/abs/2401.12179v2,"We propose Diffusion Inference-Time T-Optimization (DITTO), a general-purpose frame-work for controlling pre-trained text-to-music diffusion models at inference-time via optimizing initial noise latents. Our method can be used to optimize through any differentiable feature matching loss to achieve a target (stylized) output and leverages gradient checkpointing for memory efficiency. We demonstrate a surprisingly wide-range of applications for music generation including inpainting, outpainting, and looping as well as intensity, melody, and musical structure control - all without ever fine-tuning the underlying model. When we compare our approach against related training, guidance, and optimization-based methods, we find DITTO achieves state-of-the-art performance on nearly all tasks, including outperforming comparable approaches on controllability, audio quality, and computational efficiency, thus opening the door for high-quality, flexible, training-free control of diffusion models. Sound examples can be found at https://DITTO-Music.github.io/web/.",True,True,"Novack, Zachary and McAuley, Julian and Berg-Kirkpatrick, Taylor and Bryan, Nicholas J",2024,,,,arXiv preprint arXiv:2401.12179
"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation",2511.09585v1,melechovsky2024mustango,\cite{melechovsky2024mustango},Mustango: Toward Controllable Text-to-Music Generation,https://arxiv.org/abs/2311.08355v3,"The quality of the text-to-music models has reached new heights due to recent advancements in diffusion models. The controllability of various musical aspects, however, has barely been explored. In this paper, we propose Mustango: a music-domain-knowledge-inspired text-to-music system based on diffusion. Mustango aims to control the generated music, not only with general text captions, but with more rich captions that can include specific instructions related to chords, beats, tempo, and key. At the core of Mustango is MuNet, a Music-Domain-Knowledge-Informed UNet guidance module that steers the generated music to include the music-specific conditions, which we predict from the text prompt, as well as the general text embedding, during the reverse diffusion process. To overcome the limited availability of open datasets of music with text captions, we propose a novel data augmentation method that includes altering the harmonic, rhythmic, and dynamic aspects of music audio and using state-of-the-art Music Information Retrieval methods to extract the music features which will then be appended to the existing descriptions in text format. We release the resulting MusicBench dataset which contains over 52K instances and includes music-theory-based descriptions in the caption text. Through extensive experiments, we show that the quality of the music generated by Mustango is state-of-the-art, and the controllability through music-specific text prompts greatly outperforms other models such as MusicGen and AudioLDM2.",True,True,"Melechovsky, Jan and Guo, Zixun and Ghosal, Deepanway and Majumder, Navonil and Herremans, Dorien and Poria, Soujanya",2024,,,,
"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation",2511.09585v1,wu2024music,\cite{wu2024music},Music ControlNet: Multiple Time-varying Controls for Music Generation,https://arxiv.org/abs/2311.07069v1,"Text-to-music generation models are now capable of generating high-quality music audio in broad styles. However, text control is primarily suitable for the manipulation of global musical attributes like genre, mood, and tempo, and is less suitable for precise control over time-varying attributes such as the positions of beats in time or the changing dynamics of the music. We propose Music ControlNet, a diffusion-based music generation model that offers multiple precise, time-varying controls over generated audio. To imbue text-to-music models with time-varying control, we propose an approach analogous to pixel-wise control of the image-domain ControlNet method. Specifically, we extract controls from training audio yielding paired data, and fine-tune a diffusion-based conditional generative model over audio spectrograms given melody, dynamics, and rhythm controls. While the image-domain Uni-ControlNet method already allows generation with any subset of controls, we devise a new strategy to allow creators to input controls that are only partially specified in time. We evaluate both on controls extracted from audio and controls we expect creators to provide, demonstrating that we can generate realistic music that corresponds to control inputs in both settings. While few comparable music generation models exist, we benchmark against MusicGen, a recent model that accepts text and melody input, and show that our model generates music that is 49% more faithful to input melodies despite having 35x fewer parameters, training on 11x less data, and enabling two additional forms of time-varying control. Sound examples can be found at https://MusicControlNet.github.io/web/.",True,True,"Wu, Shih-Lun and Donahue, Chris and Watanabe, Shinji and Bryan, Nicholas J",2024,,,,"IEEE/ACM Transactions on Audio, Speech, and Language Processing"
"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation",2511.09585v1,zhang2024MusicMagus,\cite{zhang2024MusicMagus},MusicMagus: Zero-Shot Text-to-Music Editing via Diffusion Models,https://arxiv.org/abs/2402.06178v3,"Recent advances in text-to-music generation models have opened new avenues in musical creativity. However, music generation usually involves iterative refinements, and how to edit the generated music remains a significant challenge. This paper introduces a novel approach to the editing of music generated by such models, enabling the modification of specific attributes, such as genre, mood and instrument, while maintaining other aspects unchanged. Our method transforms text editing to \textit{latent space manipulation} while adding an extra constraint to enforce consistency. It seamlessly integrates with existing pretrained text-to-music diffusion models without requiring additional training. Experimental results demonstrate superior performance over both zero-shot and certain supervised baselines in style and timbre transfer evaluations. Additionally, we showcase the practical applicability of our approach in real-world music editing scenarios.",True,True,"Zhang, Yixiao and Ikemiya, Yukara and Xia, Gus and Murata, Naoki and Mart\'{\i}nez-Ram\'{\i}rez, Marco A. and Liao, Wei-Hsiang and Mitsufuji, Yuki and Dixon, Simon",2024,,,,
"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation",2511.09585v1,niu2025steermusic,\cite{niu2025steermusic},SteerMusic: Enhanced Musical Consistency for Zero-shot Text-Guided and Personalized Music Editing,,,True,False,"Niu, Xinlei and Cheuk, Kin Wai and Zhang, Jing and Murata, Naoki and Lai, Chieh-Hsin and Mancusi, Michele and Choi, Woosung and Fabbro, Giorgio and Liao, Wei-Hsiang and Martin, Charles Patrick and others",2025,,,,arXiv preprint arXiv:2504.10826
"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation",2511.09585v1,di2021video,\cite{di2021video},Video background music generation with controllable music transformer,,,True,False,"Di, Shangzhe and Jiang, Zeren and Liu, Si and Wang, Zhaokai and Zhu, Leyan and He, Zexin and Liu, Hongming and Yan, Shuicheng",2021,,,,
"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation",2511.09585v1,app12105050,\cite{app12105050},Double Linear Transformer for Background Music Generation from Videos,,,True,False,"Yang, Xueting and Yu, Ying and Wu, Xiaoyu",2022,,,,Applied Sciences
"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation",2511.09585v1,zhuo2023video,\cite{zhuo2023video},"Video Background Music Generation: Dataset, Method and Evaluation",https://arxiv.org/abs/2211.11248v2,"Music is essential when editing videos, but selecting music manually is difficult and time-consuming. Thus, we seek to automatically generate background music tracks given video input. This is a challenging task since it requires music-video datasets, efficient architectures for video-to-music generation, and reasonable metrics, none of which currently exist. To close this gap, we introduce a complete recipe including dataset, benchmark model, and evaluation metric for video background music generation. We present SymMV, a video and symbolic music dataset with various musical annotations. To the best of our knowledge, it is the first video-music dataset with rich musical annotations. We also propose a benchmark video background music generation framework named V-MusProd, which utilizes music priors of chords, melody, and accompaniment along with video-music relations of semantic, color, and motion features. To address the lack of objective metrics for video-music correspondence, we design a retrieval-based metric VMCP built upon a powerful video-music representation learning model. Experiments show that with our dataset, V-MusProd outperforms the state-of-the-art method in both music quality and correspondence with videos. We believe our dataset, benchmark model, and evaluation metric will boost the development of video background music generation. Our dataset and code are available at https://github.com/zhuole1025/SymMV.",True,True,"Zhuo, Le and Wang, Zhaokai and Wang, Baisen and Liao, Yue and Bao, Chenxi and Peng, Stanley and Han, Songhao and Zhang, Aixi and Fang, Fei and Liu, Si",2023,,,,
"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation",2511.09585v1,yu2023long,\cite{yu2023long},Long-Term Rhythmic Video Soundtracker,https://arxiv.org/abs/2305.01319v2,"We consider the problem of generating musical soundtracks in sync with rhythmic visual cues. Most existing works rely on pre-defined music representations, leading to the incompetence of generative flexibility and complexity. Other methods directly generating video-conditioned waveforms suffer from limited scenarios, short lengths, and unstable generation quality. To this end, we present Long-Term Rhythmic Video Soundtracker (LORIS), a novel framework to synthesize long-term conditional waveforms. Specifically, our framework consists of a latent conditional diffusion probabilistic model to perform waveform synthesis. Furthermore, a series of context-aware conditioning encoders are proposed to take temporal information into consideration for a long-term generation. Notably, we extend our model's applicability from dances to multiple sports scenarios such as floor exercise and figure skating. To perform comprehensive evaluations, we establish a benchmark for rhythmic video soundtracks including the pre-processed dataset, improved evaluation metrics, and robust generative baselines. Extensive experiments show that our model generates long-term soundtracks with state-of-the-art musical quality and rhythmic correspondence. Codes are available at \url{https://github.com/OpenGVLab/LORIS}.",True,True,"Yu, Jiashuo and Wang, Yaohui and Chen, Xinyuan and Sun, Xiao and Qiao, Yu",2023,,,,
"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation",2511.09585v1,KANG2024123640,\cite{KANG2024123640},Video2Music: Suitable Music Generation from Videos using an Affective Multimodal Transformer model,https://arxiv.org/abs/2311.00968v2,"Numerous studies in the field of music generation have demonstrated impressive performance, yet virtually no models are able to directly generate music to match accompanying videos. In this work, we develop a generative music AI framework, Video2Music, that can match a provided video. We first curated a unique collection of music videos. Then, we analysed the music videos to obtain semantic, scene offset, motion, and emotion features. These distinct features are then employed as guiding input to our music generation model. We transcribe the audio files into MIDI and chords, and extract features such as note density and loudness. This results in a rich multimodal dataset, called MuVi-Sync, on which we train a novel Affective Multimodal Transformer (AMT) model to generate music given a video. This model includes a novel mechanism to enforce affective similarity between video and music. Finally, post-processing is performed based on a biGRU-based regression model to estimate note density and loudness based on the video features. This ensures a dynamic rendering of the generated chords with varying rhythm and volume. In a thorough experiment, we show that our proposed framework can generate music that matches the video content in terms of emotion. The musical quality, along with the quality of music-video matching is confirmed in a user study. The proposed AMT model, along with the new MuVi-Sync dataset, presents a promising step for the new task of music generation for videos.",True,True,Jaeyong Kang and Soujanya Poria and Dorien Herremans,2024,,,,Expert Systems with Applications
"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation",2511.09585v1,qi2024harmonizing,\cite{qi2024harmonizing},Harmonizing Pixels and Melodies: Maestro-Guided Film Score Generation and Composition Style Transfer,,,True,False,"Qi, Fan and Ni, L and Xu, C",2024,,,,arXiv preprint arXiv:2411.07539
"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation",2511.09585v1,liu2023m,\cite{liu2023m},Multi-modal Music Understanding and Generation with the Power of Large Language Models,,,True,False,"Liu, Shansong and Hussain, Atin Sakkeer and Sun, Chenshuo and Shan, Ying",2023,,,,arXiv preprint arXiv:2311.11255
"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation",2511.09585v1,xu2024mozart,\cite{xu2024mozart},Mozart's Touch: A Lightweight Multi-modal Music Generation Framework Based on Pre-Trained Large Models,https://arxiv.org/abs/2405.02801v3,"In recent years, AI-Generated Content (AIGC) has witnessed rapid advancements, facilitating the creation of music, images, and other artistic forms across a wide range of industries. However, current models for image- and video-to-music synthesis struggle to capture the nuanced emotions and atmosphere conveyed by visual content. To fill this gap, we propose Mozart's Touch, a multi-modal music generation framework capable of generating music aligned with cross-modal inputs such as images, videos, and text. The framework consists of three key components: Multi-modal Captioning Module, Large Language Model (LLM) understanding \& Bridging Module, and Music Generation Module. Unlike traditional end-to-end methods, Mozart's Touch uses LLMs to accurately interpret visual elements without requiring the training or fine-tuning of music generation models, providing efficiency and transparency through clear, interpretable prompts. We also introduce the ""LLM-Bridge"" method to resolve the heterogeneous representation challenges between descriptive texts from different modalities. Through a series of objective and subjective evaluations, we demonstrate that Mozart's Touch outperforms current state-of-the-art models. Our code and examples are available at https://github.com/TiffanyBlews/MozartsTouch.",True,True,"Xu, Tianze and Li, Jiajun and Chen, Xuesong and Yao, Xinrui and Liu, Shuchang",2024,,,,arXiv preprint arXiv:2405.02801
"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation",2511.09585v1,tong2024video,\cite{tong2024video},Video Echoed in Harmony: Learning and Sampling Video-Integrated Chord Progression Sequences for Controllable Video Background Music Generation,,,True,False,"Tong, Xinyi and Chen, Sitong and Yu, Peiyang and Liu, Nian and Qv, Hui and Ma, Tao and Zheng, Bo and Yu, Feng and Zhu, Song-Chun",2024,,,,IEEE Transactions on Computational Social Systems
"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation",2511.09585v1,wang2024multimodal,\cite{wang2024multimodal},Multimodal Music Generation with Explicit Bridges and Retrieval Augmentation,,,True,False,"Wang, Baisen and Zhuo, Le and Wang, Zhaokai and Bao, Chenxi and Chengjing, Wu and Nie, Xuecheng and Dai, Jiao and Han, Jizhong and Liao, Yue and Liu, Si",2024,,,,arXiv preprint arXiv:2412.09428
"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation",2511.09585v1,zhou2025harmonyset,\cite{zhou2025harmonyset},Harmonyset: A comprehensive dataset for understanding video-music semantic alignment and temporal synchronization,,,True,False,"Zhou, Zitang and Mei, Ke and Lu, Yu and Wang, Tianyi and Rao, Fengyun",2025,,,,
"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation",2511.09585v1,zhang2025sonique,\cite{zhang2025sonique},SONIQUE: Video Background Music Generation Using Unpaired Audio-Visual Data,https://arxiv.org/abs/2410.03879v2,"We present SONIQUE, a model for generating background music tailored to video content. Unlike traditional video-to-music generation approaches, which rely heavily on paired audio-visual datasets, SONIQUE leverages unpaired data, combining royalty-free music and independent video sources. By utilizing large language models (LLMs) for video understanding and converting visual descriptions into musical tags, alongside a U-Net-based conditional diffusion model, SONIQUE enables customizable music generation. Users can control specific aspects of the music, such as instruments, genres, tempo, and melodies, ensuring the generated output fits their creative vision. SONIQUE is open-source, with a demo available online.",True,True,"Zhang, Liqian and Fuentes, Magdalena",2025,,,,
"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation",2511.09585v1,tian2025audiox,\cite{tian2025audiox},AudioX: Diffusion Transformer for Anything-to-Audio Generation,,,True,False,"Tian, Zeyue and Jin, Yizhu and Liu, Zhaoyang and Yuan, Ruibin and Tan, Xu and Chen, Qifeng and Xue, Wei and Guo, Yike",2025,,,,arXiv preprint arXiv:2503.10522
"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation",2511.09585v1,su2024v2meow,\cite{su2024v2meow},V2Meow: Meowing to the Visual Beat via Video-to-Music Generation,,,True,False,"Su, Kun and Li, Judith Yue and Huang, Qingqing and Kuzmin, Dima and Lee, Joonseok and Donahue, Chris and Sha, Fei and Jansen, Aren and Wang, Yu and Verzetti, Mauro and others",2024,,,,
"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation",2511.09585v1,xie2025filmcomposer,\cite{xie2025filmcomposer},FilmComposer: LLM-Driven Music Production for Silent Film Clips,https://arxiv.org/abs/2503.08147v1,"In this work, we implement music production for silent film clips using LLM-driven method. Given the strong professional demands of film music production, we propose the FilmComposer, simulating the actual workflows of professional musicians. FilmComposer is the first to combine large generative models with a multi-agent approach, leveraging the advantages of both waveform music and symbolic music generation. Additionally, FilmComposer is the first to focus on the three core elements of music production for film-audio quality, musicality, and musical development-and introduces various controls, such as rhythm, semantics, and visuals, to enhance these key aspects. Specifically, FilmComposer consists of the visual processing module, rhythm-controllable MusicGen, and multi-agent assessment, arrangement and mix. In addition, our framework can seamlessly integrate into the actual music production pipeline and allows user intervention in every step, providing strong interactivity and a high degree of creative freedom. Furthermore, we propose MusicPro-7k which includes 7,418 film clips, music, description, rhythm spots and main melody, considering the lack of a professional and high-quality film music dataset. Finally, both the standard metrics and the new specialized metrics we propose demonstrate that the music generated by our model achieves state-of-the-art performance in terms of quality, consistency with video, diversity, musicality, and musical development. Project page: https://apple-jun.github.io/FilmComposer.github.io/",True,True,"Xie, Zhifeng and He, Qile and Zhu, Youjia and He, Qiwei and Li, Mengtian",2025,,,,
"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation",2511.09585v1,lin2024vmas,\cite{lin2024vmas},VMAS: Video-to-Music Generation via Semantic Alignment in Web Music Videos,https://arxiv.org/abs/2409.07450v1,"We present a framework for learning to generate background music from video inputs. Unlike existing works that rely on symbolic musical annotations, which are limited in quantity and diversity, our method leverages large-scale web videos accompanied by background music. This enables our model to learn to generate realistic and diverse music. To accomplish this goal, we develop a generative video-music Transformer with a novel semantic video-music alignment scheme. Our model uses a joint autoregressive and contrastive learning objective, which encourages the generation of music aligned with high-level video content. We also introduce a novel video-beat alignment scheme to match the generated music beats with the low-level motions in the video. Lastly, to capture fine-grained visual cues in a video needed for realistic background music generation, we introduce a new temporal video encoder architecture, allowing us to efficiently process videos consisting of many densely sampled frames. We train our framework on our newly curated DISCO-MV dataset, consisting of 2.2M video-music samples, which is orders of magnitude larger than any prior datasets used for video music generation. Our method outperforms existing approaches on the DISCO-MV and MusicCaps datasets according to various music generation evaluation metrics, including human evaluation. Results are available at https://genjib.github.io/project_page/VMAs/index.html",True,True,"Lin, Yan-Bo and Tian, Yu and Yang, Linjie and Bertasius, Gedas and Wang, Heng",2024,,,,arXiv preprint arXiv:2409.07450
"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation",2511.09585v1,tian2024vidmuse,\cite{tian2024vidmuse},VidMuse: A Simple Video-to-Music Generation Framework with Long-Short-Term Modeling,https://arxiv.org/abs/2406.04321v3,"In this work, we systematically study music generation conditioned solely on the video. First, we present a large-scale dataset comprising 360K video-music pairs, including various genres such as movie trailers, advertisements, and documentaries. Furthermore, we propose VidMuse, a simple framework for generating music aligned with video inputs. VidMuse stands out by producing high-fidelity music that is both acoustically and semantically aligned with the video. By incorporating local and global visual cues, VidMuse enables the creation of musically coherent audio tracks that consistently match the video content through Long-Short-Term modeling. Through extensive experiments, VidMuse outperforms existing models in terms of audio quality, diversity, and audio-visual alignment. The code and datasets are available at https://vidmuse.github.io/.",True,True,"Tian, Zeyue and Liu, Zhaoyang and Yuan, Ruibin and Pan, Jiahao and Liu, Qifeng and Tan, Xu and Chen, Qifeng and Xue, Wei and Guo, Yike",2024,,,,arXiv preprint arXiv:2406.04321
"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation",2511.09585v1,li2024diff,\cite{li2024diff},Diff-BGM: A Diffusion Model for Video Background Music Generation,,,True,False,"Li, Sizhe and Qin, Yiming and Zheng, Minghang and Jin, Xin and Liu, Yang",2024,,,,
"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation",2511.09585v1,li2024muvivideotomusicgenerationsemantic,\cite{li2024muvivideotomusicgenerationsemantic},MuVi: Video-to-Music Generation with Semantic Alignment and Rhythmic Synchronization,,,True,False,Ruiqi Li and Siqi Zheng and Xize Cheng and Ziang Zhang and Shengpeng Ji and Zhou Zhao,2024,,,,arXiv preprint arXiv:2410.12957
"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation",2511.09585v1,li2024vidmusician,\cite{li2024vidmusician},VidMusician: Video-to-Music Generation with Semantic-Rhythmic Alignment via Hierarchical Visual Features,,,True,False,"Li, Sifei and Yang, Binxin and Yin, Chunji and Sun, Chong and Zhang, Yuxin and Dong, Weiming and Li, Chen",2024,,,,arXiv preprint arXiv:2412.06296
"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation",2511.09585v1,zuo2025gvmgen,\cite{zuo2025gvmgen},GVMGen: A General Video-to-Music Generation Model with Hierarchical Attentions,https://arxiv.org/abs/2501.09972v1,"Composing music for video is essential yet challenging, leading to a growing interest in automating music generation for video applications. Existing approaches often struggle to achieve robust music-video correspondence and generative diversity, primarily due to inadequate feature alignment methods and insufficient datasets. In this study, we present General Video-to-Music Generation model (GVMGen), designed for generating high-related music to the video input. Our model employs hierarchical attentions to extract and align video features with music in both spatial and temporal dimensions, ensuring the preservation of pertinent features while minimizing redundancy. Remarkably, our method is versatile, capable of generating multi-style music from different video inputs, even in zero-shot scenarios. We also propose an evaluation model along with two novel objective metrics for assessing video-music alignment. Additionally, we have compiled a large-scale dataset comprising diverse types of video-music pairs. Experimental results demonstrate that GVMGen surpasses previous models in terms of music-video correspondence, generative diversity, and application universality.",True,True,"Zuo, Heda and You, Weitao and Wu, Junxian and Ren, Shihong and Chen, Pei and Zhou, Mingxu and Lu, Yujia and Sun, Lingyun",2025,,,,
Binary and Multiclass Cyberattack Classification on GeNIS Dataset,2511.08660v1,PINTO2025111177,\cite{PINTO2025111177},"A review on intrusion detection datasets: tools, processes, and features",,,True,False,Daniela Pinto and others,2025,,,10.1016/j.comnet.2025.111177,Computer Networks
Binary and Multiclass Cyberattack Classification on GeNIS Dataset,2511.08660v1,cicids,\cite{cicids},Toward Generating a New Intrusion Detection Dataset and Intrusion Traffic Characterization,,,True,False,Iman Sharafaldin and others,2018,,,10.5220/0006639801080116,
Binary and Multiclass Cyberattack Classification on GeNIS Dataset,2511.08660v1,benchmark_cicids2017,\cite{benchmark_cicids2017},Benchmarking of Machine Learning for Anomaly Based Intrusion Detection Systems in the CICIDS2017 Dataset,,,True,False,"Maseer, Ziadoon and others",2021,,,10.1109/ACCESS.2021.3056614,IEEE Access
Binary and Multiclass Cyberattack Classification on GeNIS Dataset,2511.08660v1,benchmark_cicids2017_2,\cite{benchmark_cicids2017_2},A Case Study with CICIDS2017 on the Robustness of Machine Learning against Adversarial Attacks in Intrusion Detection,,,True,False,"Catillo, Marta and others",2023,,,10.1145/3600160.3605031,
Binary and Multiclass Cyberattack Classification on GeNIS Dataset,2511.08660v1,engelen,\cite{engelen},Troubleshooting an Intrusion Detection Dataset: the CICIDS2017 Case Study,,,True,False,"Engelen, Gints and others",2021,,,10.1109/SPW53761.2021.00009,
Binary and Multiclass Cyberattack Classification on GeNIS Dataset,2511.08660v1,engelen_2,\cite{engelen_2},Error Prevalence in NIDS datasets: A Case Study on CIC-IDS-2017 and CSE-CIC-IDS-2018,,,True,False,"Liu, Lisa and others",2022,,,10.1109/CNS56114.2022.9947235,
Binary and Multiclass Cyberattack Classification on GeNIS Dataset,2511.08660v1,rosay,\cite{rosay},From CIC-IDS2017 to LYCOS-IDS2017: A corrected dataset for better performance,,,True,False,"Rosay, Arnaud and others",2022,,,10.1145/3486622.3493973,
Binary and Multiclass Cyberattack Classification on GeNIS Dataset,2511.08660v1,Lanvin,\cite{Lanvin},Errors in the CICIDS2017 Dataset and the Significant Differences in Detection Performances It Makes,,,True,False,"Lanvin, Maxime
and others",2023,,,,
Binary and Multiclass Cyberattack Classification on GeNIS Dataset,2511.08660v1,hikari,\cite{hikari},Generating Network Intrusion Detection Dataset Based on Real and Encrypted Synthetic Attack Traffic,,,True,False,"Ferriyan, Andrey and others",2021,,,10.3390/app11177868,Applied Sciences
Binary and Multiclass Cyberattack Classification on GeNIS Dataset,2511.08660v1,fernandes,\cite{fernandes},The impact of identifiable features in ML Classification algorithms with the HIKARI-2021 Dataset,,,True,False,"Fernandes, Rui and others",2023,,,10.1109/ISDFS58141.2023.10131864,
Binary and Multiclass Cyberattack Classification on GeNIS Dataset,2511.08660v1,new_hikari,\cite{new_hikari},"HIKARI-2021: Generating Network Intrusion
                   Detection Dataset Based on Real and Encrypted
                   Synthetic Attack Traffic",,,True,False,"Ferriyan, Andrey and
                  others",,,,10.5281/zenodo.6463389,
Binary and Multiclass Cyberattack Classification on GeNIS Dataset,2511.08660v1,hikari_22_analysis,\cite{hikari_22_analysis},Enhancing Network Intrusion Detection with Deep Learning: A Comprehensive Analysis,,,True,False,"Rahman, Md. and others",2024,,,10.1109/ICCIT64611.2024.11022353,
Binary and Multiclass Cyberattack Classification on GeNIS Dataset,2511.08660v1,Hera2025,\cite{Hera2025},A Novel Approach to Network Traffic Analysis: the HERA tool,https://arxiv.org/abs/2501.07475v1,"Cybersecurity threats highlight the need for robust network intrusion detection systems to identify malicious behaviour. These systems rely heavily on large datasets to train machine learning models capable of detecting patterns and predicting threats. In the past two decades, researchers have produced a multitude of datasets, however, some widely utilised recent datasets generated with CICFlowMeter contain inaccuracies. These result in flow generation and feature extraction inconsistencies, leading to skewed results and reduced system effectiveness. Other tools in this context lack ease of use, customizable feature sets, and flow labelling options. In this work, we introduce HERA, a new open-source tool that generates flow files and labelled or unlabelled datasets with user-defined features. Validated and tested with the UNSW-NB15 dataset, HERA demonstrated accurate flow and label generation.",True,True,"Pinto, Daniela and others",2024,,,10.1109/TrustCom63139.2024.00255,
Binary and Multiclass Cyberattack Classification on GeNIS Dataset,2511.08660v1,lstm_rw,\cite{lstm_rw},Deep learning algorithms for intrusion detection systems in internet of things using CIC-IDS 2017 dataset,,,True,False,Jinsi Jose and Deepa Jose,2023,,,10.11591/ijece.v13i1.pp1134-1141,International Journal of Electrical and Computer Engineering (IJECE)
Binary and Multiclass Cyberattack Classification on GeNIS Dataset,2511.08660v1,lstm_rw2,\cite{lstm_rw2},Intrusion detection systems using long short-term memory (LSTM),,,True,False,"Laghrissi,  FatimaEzzahra and others",2021,,,10.1186/s40537-021-00448-4,Journal of Big Data
Binary and Multiclass Cyberattack Classification on GeNIS Dataset,2511.08660v1,mlp,\cite{mlp},IGRF-RFE: A Hybrid Feature Selection Method for MLP-based Network Intrusion Detection on UNSW-NB15 Dataset,https://arxiv.org/abs/2203.16365v2,"The effectiveness of machine learning models is significantly affected by the size of the dataset and the quality of features as redundant and irrelevant features can radically degrade the performance. This paper proposes IGRF-RFE: a hybrid feature selection method tasked for multi-class network anomalies using a Multilayer perceptron (MLP) network. IGRF-RFE can be considered as a feature reduction technique based on both the filter feature selection method and the wrapper feature selection method. In our proposed method, we use the filter feature selection method, which is the combination of Information Gain and Random Forest Importance, to reduce the feature subset search space. Then, we apply recursive feature elimination(RFE) as a wrapper feature selection method to further eliminate redundant features recursively on the reduced feature subsets. Our experimental results obtained based on the UNSW-NB15 dataset confirm that our proposed method can improve the accuracy of anomaly detection while reducing the feature dimension. The results show that the feature dimension is reduced from 42 to 23 while the multi-classification accuracy of MLP is improved from 82.25% to 84.24%.",True,True,"Yin,  Yuhua and others",2023,,,10.1186/s40537-023-00694-8,Journal of Big Data
Binary and Multiclass Cyberattack Classification on GeNIS Dataset,2511.08660v1,arvores,\cite{arvores},Reliable feature selection for adversarially robust cyber-attack detection,,,True,False,"Vitorino,  João and others",2024,,,10.1007/s12243-024-01047-z,Annals of Telecommunications
Binary and Multiclass Cyberattack Classification on GeNIS Dataset,2511.08660v1,arvores2,\cite{arvores2},An Adversarial Robustness Benchmark for Enterprise Network Intrusion Detection,,,True,False,"Vitorino, Jo{\~a}o
and others",2024,,,,
Remodeling Semantic Relationships in Vision-Language Fine-Tuning,2511.08238v2,zaken2021bitfit,\cite{zaken2021bitfit},BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models,https://arxiv.org/abs/2106.10199v5,"We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods. Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.",True,True,"Zaken, Elad Ben and Ravfogel, Shauli and Goldberg, Yoav",2021,,,,arXiv preprint arXiv:2106.10199
Remodeling Semantic Relationships in Vision-Language Fine-Tuning,2511.08238v2,houlsby2019parameter,\cite{houlsby2019parameter},Parameter-efficient transfer learning for NLP,,,True,False,"Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain",2019,,,,
Remodeling Semantic Relationships in Vision-Language Fine-Tuning,2511.08238v2,hu2021lora,\cite{hu2021lora},LoRA+: Efficient Low Rank Adaptation of Large Models,https://arxiv.org/abs/2402.12354v2,"In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$ improves performance (1-2 $\%$ improvements) and finetuning speed (up to $\sim$ 2X SpeedUp), at the same computational cost as LoRA.",True,True,"Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu",2021,,,,arXiv preprint arXiv:2106.09685
Remodeling Semantic Relationships in Vision-Language Fine-Tuning,2511.08238v2,li2021prefix,\cite{li2021prefix},Prefix-Tuning: Optimizing Continuous Prompts for Generation,https://arxiv.org/abs/2101.00190v1,"Fine-tuning is the de facto way to leverage large pretrained language models to perform downstream tasks. However, it modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix). Prefix-tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were ""virtual tokens"". We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We find that by learning only 0.1\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics unseen during training.",True,True,"Li, Xiang Lisa and Liang, Percy",2021,,,,arXiv preprint arXiv:2101.00190
Remodeling Semantic Relationships in Vision-Language Fine-Tuning,2511.08238v2,lester2021power,\cite{lester2021power},The power of scale for parameter-efficient prompt tuning,,,True,False,"Lester, Brian and Al-Rfou, Rami and Constant, Noah",2021,,,,arXiv preprint arXiv:2104.08691
Remodeling Semantic Relationships in Vision-Language Fine-Tuning,2511.08238v2,10901852,\cite{10901852},When Adversarial Training Meets Prompt Tuning: Adversarial Dual Prompt Tuning for Unsupervised Domain Adaptation,,,True,False,"Cui, Chaoran and Liu, Ziyi and Gong, Shuai and Zhu, Lei and Zhang, Chunyun and Liu, Hui",2025,,,10.1109/TIP.2025.3541868,IEEE Transactions on Image Processing
Remodeling Semantic Relationships in Vision-Language Fine-Tuning,2511.08238v2,10431687,\cite{10431687},Learning Domain Invariant Prompt for Vision-Language Models,,,True,False,"Zhao, Cairong and Wang, Yubin and Jiang, Xinyang and Shen, Yifei and Song, Kaitao and Li, Dongsheng and Miao, Duoqian",2024,,,10.1109/TIP.2024.3362062,IEEE Transactions on Image Processing
Remodeling Semantic Relationships in Vision-Language Fine-Tuning,2511.08238v2,10816351,\cite{10816351},CLIP4STR: A Simple Baseline for Scene Text Recognition With Pre-Trained Vision-Language Model,,,True,False,"Zhao, Shuai and Quan, Ruijie and Zhu, Linchao and Yang, Yi",2024,,,10.1109/TIP.2024.3512354,IEEE Transactions on Image Processing
Remodeling Semantic Relationships in Vision-Language Fine-Tuning,2511.08238v2,jie2024memory,\cite{jie2024memory},Memory-Space Visual Prompting for Efficient Vision-Language Fine-Tuning,,,True,False,"Jie, Shibo and Tang, Yehui and Ding, Ning and Deng, Zhi-Hong and Han, Kai and Wang, Yunhe",2024,,,,arXiv preprint arXiv:2405.05615
Remodeling Semantic Relationships in Vision-Language Fine-Tuning,2511.08238v2,sung2022vl,\cite{sung2022vl},Vl-adapter: Parameter-efficient transfer learning for vision-and-language tasks,,,True,False,"Sung, Yi-Lin and Cho, Jaemin and Bansal, Mohit",2022,,,,
Remodeling Semantic Relationships in Vision-Language Fine-Tuning,2511.08238v2,hu2023vl,\cite{hu2023vl},Vl-pet: Vision-and-language parameter-efficient tuning via granularity control,,,True,False,"Hu, Zi-Yuan and Li, Yanyang and Lyu, Michael R and Wang, Liwei",2023,,,,
Remodeling Semantic Relationships in Vision-Language Fine-Tuning,2511.08238v2,luo2024cheap,\cite{luo2024cheap},Cheap and quick: Efficient vision-language instruction tuning for large language models,,,True,False,"Luo, Gen and Zhou, Yiyi and Ren, Tianhe and Chen, Shengxin and Sun, Xiaoshuai and Ji, Rongrong",2024,,,,Advances in Neural Information Processing Systems
Remodeling Semantic Relationships in Vision-Language Fine-Tuning,2511.08238v2,liu2024visual,\cite{liu2024visual},Visual instruction tuning,,,True,False,"Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",2024,,,,Advances in neural information processing systems
Remodeling Semantic Relationships in Vision-Language Fine-Tuning,2511.08238v2,alayrac2022flamingo,\cite{alayrac2022flamingo},Flamingo: a visual language model for few-shot learning,,,True,False,"Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others",2022,,,,Advances in neural information processing systems
Remodeling Semantic Relationships in Vision-Language Fine-Tuning,2511.08238v2,li2022blip,\cite{li2022blip},Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation,,,True,False,"Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven",2022,,,,
Remodeling Semantic Relationships in Vision-Language Fine-Tuning,2511.08238v2,li2023blip,\cite{li2023blip},BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models,https://arxiv.org/abs/2301.12597v3,"The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.",True,True,"Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven",2023,,,,
Remodeling Semantic Relationships in Vision-Language Fine-Tuning,2511.08238v2,lu2023uniadapter,\cite{lu2023uniadapter},UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling,https://arxiv.org/abs/2302.06605v2,"Large-scale vision-language pre-trained models have shown promising transferability to various downstream tasks. As the size of these foundation models and the number of downstream tasks grow, the standard full fine-tuning paradigm becomes unsustainable due to heavy computational and storage costs. This paper proposes UniAdapter, which unifies unimodal and multimodal adapters for parameter-efficient cross-modal adaptation on pre-trained vision-language models. Specifically, adapters are distributed to different modalities and their interactions, with the total number of tunable parameters reduced by partial weight sharing. The unified and knowledge-sharing design enables powerful cross-modal representations that can benefit various downstream tasks, requiring only 1.0%-2.0% tunable parameters of the pre-trained model. Extensive experiments on 6 cross-modal downstream benchmarks (including video-text retrieval, image-text retrieval, VideoQA, and VQA) show that in most cases, UniAdapter not only outperforms the state-of-the-arts, but even beats the full fine-tuning strategy. Particularly, on the MSRVTT retrieval task, UniAdapter achieves 49.7% recall@1 with 2.2% model parameters, outperforming the latest competitors by 2.0%. The code and models are available at https://github.com/RERV/UniAdapter.",True,True,"Lu, Haoyu and Huo, Yuqi and Yang, Guoxing and Lu, Zhiwu and Zhan, Wei and Tomizuka, Masayoshi and Ding, Mingyu",2023,,,,arXiv preprint arXiv:2302.06605
GraphIF: Enhancing Multi-Turn Instruction Following for Large Language Models with Relation Graph Prompt,2511.10051v1,wang2023openchat,\cite{wang2023openchat},OpenChat: Advancing Open-source Language Models with Mixed-Quality Data,https://arxiv.org/abs/2309.11235v2,"Nowadays, open-source large language models like LLaMA have emerged. Recent developments have incorporated supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to align these models with human goals. However, SFT methods treat all training data with mixed quality equally, while RLFT methods require high-quality pairwise or ranking-based preference data. In this study, we present a novel framework, named OpenChat, to advance open-source language models with mixed-quality data. Specifically, we consider the general SFT training data, consisting of a small amount of expert data mixed with a large proportion of sub-optimal data, without any preference labels. We propose the C(onditioned)-RLFT, which regards different data sources as coarse-grained reward labels and learns a class-conditioned policy to leverage complementary data quality information. Interestingly, the optimal policy in C-RLFT can be easily solved through single-stage, RL-free supervised learning, which is lightweight and avoids costly human preference labeling. Through extensive experiments on three standard benchmarks, our openchat-13b fine-tuned with C-RLFT achieves the highest average performance among all 13b open-source language models. Moreover, we use AGIEval to validate the model generalization performance, in which only openchat-13b surpasses the base model. Finally, we conduct a series of analyses to shed light on the effectiveness and robustness of OpenChat. Our code, data, and models are publicly available at https://github.com/imoneoi/openchat and https://huggingface.co/openchat.",True,True,"Wang, Guan and Cheng, Sijie and Zhan, Xianyuan and Li, Xiangang and Song, Sen and Liu, Yang",2023,,,,arXiv preprint arXiv:2309.11235
GraphIF: Enhancing Multi-Turn Instruction Following for Large Language Models with Relation Graph Prompt,2511.10051v1,zhao2024wildchat,\cite{zhao2024wildchat},Wildchat: 1m chatgpt interaction logs in the wild,,,True,False,"Zhao, Wenting and Ren, Xiang and Hessel, Jack and Cardie, Claire and Choi, Yejin and Deng, Yuntian",2024,,,,arXiv preprint arXiv:2405.01470
GraphIF: Enhancing Multi-Turn Instruction Following for Large Language Models with Relation Graph Prompt,2511.10051v1,ding2023enhancing,\cite{ding2023enhancing},Enhancing Chat Language Models by Scaling High-quality Instructional Conversations,https://arxiv.org/abs/2305.14233v1,"Fine-tuning on instruction data has been widely validated as an effective practice for implementing chat language models like ChatGPT. Scaling the diversity and quality of such data, although straightforward, stands a great chance of leading to improved performance. This paper aims to improve the upper bound of open-source models further. We first provide a systematically designed, diverse, informative, large-scale dataset of instructional conversations, UltraChat, which does not involve human queries. Our objective is to capture the breadth of interactions that a human might have with an AI assistant and employs a comprehensive framework to generate multi-turn conversation iteratively. UltraChat contains 1.5 million high-quality multi-turn dialogues and covers a wide range of topics and instructions. Our statistical analysis of UltraChat reveals its superiority in various key metrics, including scale, average length, diversity, coherence, etc., solidifying its position as a leading open-source dataset. Building upon UltraChat, we fine-tune a LLaMA model to create a powerful conversational model, UltraLLaMA. Our evaluations indicate that UltraLLaMA consistently outperforms other open-source models, including Vicuna, the previously recognized state-of-the-art open-source model. The dataset and the model will be publicly released\footnote{\url{https://github.com/thunlp/UltraChat}}.",True,True,"Ding, Ning and Chen, Yulin and Xu, Bokai and Qin, Yujia and Hu, Shengding and Liu, Zhiyuan and Sun, Maosong and Zhou, Bowen",2023,,,,
GraphIF: Enhancing Multi-Turn Instruction Following for Large Language Models with Relation Graph Prompt,2511.10051v1,wu2025instruct,\cite{wu2025instruct},Review-Instruct: A Review-Driven Multi-Turn Conversations Generation Method for Large Language Models,,,True,False,"Wu, Jiangxu  and
      Wang, Cong  and
      Su, Tianhuang  and
      Haozhi, Lin  and
      JunYang, JunYang  and
      Zhangchao, Zhangchao  and
      Pan, Binqiang  and
      SongpanYang, SongpanYang  and
      Mingpeng, Mingpeng  and
      Shi, Kai  and
      Li, Zixian",2025,,,,
GraphIF: Enhancing Multi-Turn Instruction Following for Large Language Models with Relation Graph Prompt,2511.10051v1,sun2024parrot,\cite{sun2024parrot},Parrot: Enhancing Multi-Turn Instruction Following for Large Language Models,https://arxiv.org/abs/2310.07301v2,"Humans often interact with large language models (LLMs) in multi-turn interaction to obtain desired answers or more information. However, most existing studies overlook the multi-turn instruction following ability of LLMs, in terms of training dataset, training method, and evaluation benchmark. In this paper, we introduce Parrot, a solution aiming to enhance multi-turn instruction following for LLMs. First, we introduce an efficient but effective method for collecting multi-turn instructions that feature human-like queries, such as anaphora and ellipsis. Second, we propose a context-aware preference optimization strategy to further enhance LLMs for complex queries in multi-turn interaction. Moreover, to quantitatively evaluate LLMs in multi-turn instruction following, we manually build a multi-turn benchmark derived from existing ones. Extensive experiments show that Parrot improves current LLMs by up to 7.2% in multi-turn instruction following. Our dataset and codes will be open-sourced to facilitate future research.",True,True,"Sun, Yuchong and Liu, Che and Zhou, Kun and Huang, Jinwen and Song, Ruihua and Zhao, Wayne Xin and Zhang, Fuzheng and Zhang, Di and Gai, Kun",2024,,,,
GraphIF: Enhancing Multi-Turn Instruction Following for Large Language Models with Relation Graph Prompt,2511.10051v1,chen2025consistentchat,\cite{chen2025consistentchat},ConsistentChat: Building Skeleton-Guided Consistent Multi-Turn Dialogues for Large Language Models from Scratch,https://arxiv.org/abs/2506.03558v2,"Current instruction data synthesis methods primarily focus on single-turn instructions and often neglect cross-turn coherence, resulting in context drift and reduced task completion rates in extended conversations. To address this limitation, we propose Skeleton-Guided Multi-Turn Dialogue Generation, a framework that constrains multi-turn instruction synthesis by explicitly modeling human conversational intent. It operates in two stages: (1) Intent Modeling, which captures the global structure of human dialogues by assigning each conversation to one of nine well-defined intent trajectories, ensuring a coherent and goal-oriented information flow; and (2) Skeleton Generation, which constructs a structurally grounded sequence of user queries aligned with the modeled intent, thereby serving as a scaffold that constrains and guides the downstream instruction synthesis process. Based on this process, we construct ConsistentChat, a multi-turn instruction dataset with approximately 15,000 multi-turn conversations and 224,392 utterances. Experiments on the Light, Topdial, and MT-Eval benchmarks show that models fine-tuned on ConsistentChat achieve a 20-30% improvement in chat consistency and up to a 15% increase in task success rate, significantly outperforming models trained on existing single-turn and multi-turn instruction datasets.",True,True,"Chen, Jiawei and Guan, Xinyan and Yuan, Qianhao and Mo, Guozhao and Zhou, Weixiang and Lu, Yaojie and Lin, Hongyu and He, Ben and Sun, Le and Han, Xianpei",2025,,,,arXiv preprint arXiv:2506.03558
GraphIF: Enhancing Multi-Turn Instruction Following for Large Language Models with Relation Graph Prompt,2511.10051v1,jimenez2024hipporag,\cite{jimenez2024hipporag},HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models,https://arxiv.org/abs/2405.14831v3,"In order to thrive in hostile and ever-changing natural environments, mammalian brains evolved to store large amounts of knowledge about the world and continually integrate new information while avoiding catastrophic forgetting. Despite the impressive accomplishments, large language models (LLMs), even with retrieval-augmented generation (RAG), still struggle to efficiently and effectively integrate a large amount of new experiences after pre-training. In this work, we introduce HippoRAG, a novel retrieval framework inspired by the hippocampal indexing theory of human long-term memory to enable deeper and more efficient knowledge integration over new experiences. HippoRAG synergistically orchestrates LLMs, knowledge graphs, and the Personalized PageRank algorithm to mimic the different roles of neocortex and hippocampus in human memory. We compare HippoRAG with existing RAG methods on multi-hop question answering and show that our method outperforms the state-of-the-art methods remarkably, by up to 20%. Single-step retrieval with HippoRAG achieves comparable or better performance than iterative retrieval like IRCoT while being 10-30 times cheaper and 6-13 times faster, and integrating HippoRAG into IRCoT brings further substantial gains. Finally, we show that our method can tackle new types of scenarios that are out of reach of existing methods. Code and data are available at https://github.com/OSU-NLP-Group/HippoRAG.",True,True,"Jimenez Gutierrez, Bernal and Shu, Yiheng and Gu, Yu and Yasunaga, Michihiro and Su, Yu",2024,,,,Advances in Neural Information Processing Systems
GraphIF: Enhancing Multi-Turn Instruction Following for Large Language Models with Relation Graph Prompt,2511.10051v1,li2024graphreader,\cite{li2024graphreader},GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models,https://arxiv.org/abs/2406.14550v2,"Long-context capabilities are essential for large language models (LLMs) to tackle complex and long-input tasks. Despite numerous efforts made to optimize LLMs for long contexts, challenges persist in robustly processing long inputs. In this paper, we introduce GraphReader, a graph-based agent system designed to handle long texts by structuring them into a graph and employing an agent to explore this graph autonomously. Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan. It then invokes a set of predefined functions to read node content and neighbors, facilitating a coarse-to-fine exploration of the graph. Throughout the exploration, the agent continuously records new insights and reflects on current circumstances to optimize the process until it has gathered sufficient information to generate an answer. Experimental results on the LV-Eval dataset reveal that GraphReader, using a 4k context window, consistently outperforms GPT-4-128k across context lengths from 16k to 256k by a large margin. Additionally, our approach demonstrates superior performance on four challenging single-hop and multi-hop benchmarks.",True,True,"Li, Shilong and He, Yancheng and Guo, Hangyu and Bu, Xingyuan and Bai, Ge and Liu, Jie and Liu, Jiaheng and Qu, Xingwei and Li, Yangguang and Ouyang, Wanli and others",2024,,,,
GraphIF: Enhancing Multi-Turn Instruction Following for Large Language Models with Relation Graph Prompt,2511.10051v1,lin2025rje,\cite{lin2025rje},RJE: A Retrieval-Judgment-Exploration Framework for Efficient Knowledge Graph Question Answering with LLMs,https://arxiv.org/abs/2510.01257v1,"Knowledge graph question answering (KGQA) aims to answer natural language questions using knowledge graphs. Recent research leverages large language models (LLMs) to enhance KGQA reasoning, but faces limitations: retrieval-based methods are constrained by the quality of retrieved information, while agent-based methods rely heavily on proprietary LLMs. To address these limitations, we propose Retrieval-Judgment-Exploration (RJE), a framework that retrieves refined reasoning paths, evaluates their sufficiency, and conditionally explores additional evidence. Moreover, RJE introduces specialized auxiliary modules enabling small-sized LLMs to perform effectively: Reasoning Path Ranking, Question Decomposition, and Retriever-assisted Exploration. Experiments show that our approach with proprietary LLMs (such as GPT-4o-mini) outperforms existing baselines while enabling small open-source LLMs (such as 3B and 8B parameters) to achieve competitive results without fine-tuning LLMs. Additionally, RJE substantially reduces the number of LLM calls and token usage compared to agent-based methods, yielding significant efficiency improvements.",True,True,"Lin, Can and Jiang, Zhengwang and Zheng, Ling and Zhao, Qi and Zhang, Yuhang and Song, Qi and Zhou, Wangqiu",2025,,,,
GraphIF: Enhancing Multi-Turn Instruction Following for Large Language Models with Relation Graph Prompt,2511.10051v1,edge2024local,\cite{edge2024local},From Local to Global: A Graph RAG Approach to Query-Focused Summarization,https://arxiv.org/abs/2404.16130v2,"The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previously unseen document collections. However, RAG fails on global questions directed at an entire text corpus, such as ""What are the main themes in the dataset?"", since this is inherently a query-focused summarization (QFS) task, rather than an explicit retrieval task. Prior QFS methods, meanwhile, do not scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we propose GraphRAG, a graph-based approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text. Our approach uses an LLM to build a graph index in two stages: first, to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, we show that GraphRAG leads to substantial improvements over a conventional RAG baseline for both the comprehensiveness and diversity of generated answers.",True,True,"Edge, Darren and Trinh, Ha and Cheng, Newman and Bradley, Joshua and Chao, Alex and Mody, Apurva and Truitt, Steven and Metropolitansky, Dasha and Ness, Robert Osazuwa and Larson, Jonathan",2024,,,,arXiv preprint arXiv:2404.16130
GraphIF: Enhancing Multi-Turn Instruction Following for Large Language Models with Relation Graph Prompt,2511.10051v1,guo2024lightrag,\cite{guo2024lightrag},LightRAG: Simple and Fast Retrieval-Augmented Generation,,,True,False,"Guo, Zirui and Xia, Lianghao and Yu, Yanhua and Ao, Tu and Huang, Chao",2024,,,,arXiv preprint arXiv:2410.05779
GraphIF: Enhancing Multi-Turn Instruction Following for Large Language Models with Relation Graph Prompt,2511.10051v1,wang2024knowledge,\cite{wang2024knowledge},Knowledge graph prompting for multi-document question answering,,,True,False,"Wang, Yu and Lipka, Nedim and Rossi, Ryan A and Siu, Alexa and Zhang, Ruiyi and Derr, Tyler",2024,,,,
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,Tay2022TransformerMA,\cite{Tay2022TransformerMA},Transformer Memory as a Differentiable Search Index,https://arxiv.org/abs/2202.06991v3,"In this paper, we demonstrate that information retrieval can be accomplished with a single Transformer, in which all information about the corpus is encoded in the parameters of the model. To this end, we introduce the Differentiable Search Index (DSI), a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a DSI model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process. We study variations in how documents and their identifiers are represented, variations in training procedures, and the interplay between models and corpus sizes. Experiments demonstrate that given appropriate design choices, DSI significantly outperforms strong baselines such as dual encoder models. Moreover, DSI demonstrates strong generalization capabilities, outperforming a BM25 baseline in a zero-shot setup.",True,True,Yi Tay and Vinh Quang Tran and Mostafa Dehghani and Jianmo Ni and Dara Bahri and Harsh Mehta and Zhen Qin and Kai Hui and Zhe Zhao and Jai Gupta and Tal Schuster and William W. Cohen and Donald Metzler,2022,,,,
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,Wang2022ANC,\cite{Wang2022ANC},A Neural Corpus Indexer for Document Retrieval,,,True,False,Yujing Wang and Ying Hou and Hong Wang and Ziming Miao and Shibin Wu and Hao Sun and Qi Chen and Yuqing Xia and Chengmin Chi and Guoshuai Zhao and Zheng Liu and Xing Xie and Hao Sun and Weiwei Deng and Qi Zhang and Mao Yang,2022,,,,
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,Zhou2022UltronAU,\cite{Zhou2022UltronAU},Ultron: An Ultimate Retriever on Corpus with a Model-based Indexer,,,True,False,Yujia Zhou and Jing Yao and Zhicheng Dou and Ledell Yu Wu and Peitian Zhang and Ji-rong Wen,2022,,,,ArXiv
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,TIGER-Rajput,\cite{TIGER-Rajput},Recommender Systems with Generative Retrieval,,,True,False,Shashank Rajput and Nikhil Mehta and Anima Singh and Raghunandan H. Keshavan and Trung Vu and Lukasz Heldt and Lichan Hong and Yi Tay and Vinh Q. Tran and Jonah Samost and Maciej Kula and Ed H. Chi and Maheswaran Sathiamoorthy,2023,,,,
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,IRGen-Zhang,\cite{IRGen-Zhang},IRGen: Generative Modeling for Image Retrieval,,,True,False,Yidan Zhang and Ting Zhang and Dong Chen and Yujing Wang and Qi Chen and Xing Xie and Hao Sun and Weiwei Deng and Qi Zhang and Fan Yang and Mao Yang and Qingmin Liao and Baining Guo,2023,,,,
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,zeng2023scalable_and_effective,\cite{zeng2023scalable_and_effective},Scalable and Effective Generative Information Retrieval,,,True,False,"Hansi Zeng and
                  Chen Luo and
                  Bowen Jin and
                  Sheikh Muhammad Sarwar and
                  Tianxin Wei and
                  Hamed Zamani",2023,,https://doi.org/10.48550/arXiv.2311.09134,10.48550/ARXIV.2311.09134,CoRR
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,sun2023learning,\cite{sun2023learning},Learning to tokenize for generative retrieval,,,True,False,"Sun, Weiwei and Yan, Lingyong and Chen, Zheng and Wang, Shuaiqiang and Zhu, Haichao and Ren, Pengjie and Chen, Zhumin and Yin, Dawei and Rijke, Maarten and Ren, Zhaochun",2023,,,,Advances in Neural Information Processing Systems
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,yang2023auto_search_indexer,\cite{yang2023auto_search_indexer},Auto Search Indexer for End-to-End Document Retrieval,,,True,False,"Tianchi Yang and
                  Minghui Song and
                  Zihan Zhang and
                  Haizhen Huang and
                  Weiwei Deng and
                  Feng Sun and
                  Qi Zhang",2023,,https://aclanthology.org/2023.findings-emnlp.464,,
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,DeCao2020AutoregressiveER,\cite{DeCao2020AutoregressiveER},Autoregressive Entity Retrieval,https://arxiv.org/abs/2010.00904v3,"Entities are at the center of how we represent and aggregate knowledge. For instance, Encyclopedias such as Wikipedia are structured by entities (e.g., one per Wikipedia article). The ability to retrieve such entities given a query is fundamental for knowledge-intensive tasks such as entity linking and open-domain question answering. Current approaches can be understood as classifiers among atomic labels, one for each entity. Their weight vectors are dense entity representations produced by encoding entity meta information such as their descriptions. This approach has several shortcomings: (i) context and entity affinity is mainly captured through a vector dot product, potentially missing fine-grained interactions; (ii) a large memory footprint is needed to store dense representations when considering large entity sets; (iii) an appropriately hard set of negative data has to be subsampled at training time. In this work, we propose GENRE, the first system that retrieves entities by generating their unique names, left to right, token-by-token in an autoregressive fashion. This mitigates the aforementioned technical issues since: (i) the autoregressive formulation directly captures relations between context and entity name, effectively cross encoding both; (ii) the memory footprint is greatly reduced because the parameters of our encoder-decoder architecture scale with vocabulary size, not entity count; (iii) the softmax loss is computed without subsampling negative data. We experiment with more than 20 datasets on entity disambiguation, end-to-end entity linking and document retrieval tasks, achieving new state-of-the-art or very competitive results while using a tiny fraction of the memory footprint of competing systems. Finally, we demonstrate that new entities can be added by simply specifying their names. Code and pre-trained models at https://github.com/facebookresearch/GENRE.",True,True,Nicola De Cao and Gautier Izacard and Sebastian Riedel and Fabio Petroni,2021,,,,
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,de-cao-etal-2022-multilingual,\cite{de-cao-etal-2022-multilingual},Multilingual Autoregressive Entity Linking,https://arxiv.org/abs/2103.12528v1,"We present mGENRE, a sequence-to-sequence system for the Multilingual Entity Linking (MEL) problem -- the task of resolving language-specific mentions to a multilingual Knowledge Base (KB). For a mention in a given language, mGENRE predicts the name of the target entity left-to-right, token-by-token in an autoregressive fashion. The autoregressive formulation allows us to effectively cross-encode mention string and entity names to capture more interactions than the standard dot product between mention and entity vectors. It also enables fast search within a large KB even for mentions that do not appear in mention tables and with no need for large-scale vector indices. While prior MEL works use a single representation for each entity, we match against entity names of as many languages as possible, which allows exploiting language connections between source input and target name. Moreover, in a zero-shot setting on languages with no training data at all, mGENRE treats the target language as a latent variable that is marginalized at prediction time. This leads to over 50% improvements in average accuracy. We show the efficacy of our approach through extensive evaluation including experiments on three popular MEL benchmarks where mGENRE establishes new state-of-the-art results. Code and pre-trained models at https://github.com/facebookresearch/GENRE.",True,True,"De Cao, Nicola  and
      Wu, Ledell  and
      Popat, Kashyap  and
      Artetxe, Mikel  and
      Goyal, Naman  and
      Plekhanov, Mikhail  and
      Zettlemoyer, Luke  and
      Cancedda, Nicola  and
      Riedel, Sebastian  and
      Petroni, Fabio",2022,,https://aclanthology.org/2022.tacl-1.16,10.1162/tacl_a_00460,Transactions of the Association for Computational Linguistics
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,Bevilacqua2022AutoregressiveSE,\cite{Bevilacqua2022AutoregressiveSE},Autoregressive Search Engines: Generating Substrings as Document Identifiers,https://arxiv.org/abs/2204.10628v1,"Knowledge-intensive language tasks require NLP systems to both provide the correct answer and retrieve supporting evidence for it in a given corpus. Autoregressive language models are emerging as the de-facto standard for generating answers, with newer and more powerful systems emerging at an astonishing pace. In this paper we argue that all this (and future) progress can be directly applied to the retrieval problem with minimal intervention to the models' architecture. Previous work has explored ways to partition the search space into hierarchical structures and retrieve documents by autoregressively generating their unique identifier. In this work we propose an alternative that doesn't force any structure in the search space: using all ngrams in a passage as its possible identifiers. This setup allows us to use an autoregressive model to generate and score distinctive ngrams, that are then mapped to full passages through an efficient data structure. Empirically, we show this not only outperforms prior autoregressive approaches but also leads to an average improvement of at least 10 points over more established retrieval solutions for passage-level retrieval on the KILT benchmark, establishing new state-of-the-art downstream performance on some datasets, while using a considerably lighter memory footprint than competing systems. Code and pre-trained models at https://github.com/facebookresearch/SEAL.",True,True,Michele Bevilacqua and Giuseppe Ottaviano and Patrick Lewis and Wen-tau Yih and Sebastian Riedel and Fabio Petroni,2022,,,,
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,Chen2022CorpusBrainPA,\cite{Chen2022CorpusBrainPA},CorpusBrain: Pre-train a Generative Retrieval Model for Knowledge-Intensive Language Tasks,https://arxiv.org/abs/2208.07652v1,"Knowledge-intensive language tasks (KILT) usually require a large body of information to provide correct answers. A popular paradigm to solve this problem is to combine a search system with a machine reader, where the former retrieves supporting evidences and the latter examines them to produce answers. Recently, the reader component has witnessed significant advances with the help of large-scale pre-trained generative models. Meanwhile most existing solutions in the search component rely on the traditional ``index-retrieve-then-rank'' pipeline, which suffers from large memory footprint and difficulty in end-to-end optimization. Inspired by recent efforts in constructing model-based IR models, we propose to replace the traditional multi-step search pipeline with a novel single-step generative model, which can dramatically simplify the search process and be optimized in an end-to-end manner. We show that a strong generative retrieval model can be learned with a set of adequately designed pre-training tasks, and be adopted to improve a variety of downstream KILT tasks with further fine-tuning. We name the pre-trained generative retrieval model as CorpusBrain as all information about the corpus is encoded in its parameters without the need of constructing additional index. Empirical results show that CorpusBrain can significantly outperform strong baselines for the retrieval task on the KILT benchmark and establish new state-of-the-art downstream performances. We also show that CorpusBrain works well under zero- and low-resource settings.",True,True,Jiangui Chen and Ruqing Zhang and Jiafeng Guo and Y. Liu and Yixing Fan and Xueqi Cheng,2022,,,,
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,UGR-Chen,\cite{UGR-Chen},"A Unified Generative Retriever for Knowledge-Intensive Language Tasks
                  via Prompt Learning",,,True,False,"Jiangui Chen and
                  Ruqing Zhang and
                  Jiafeng Guo and
                  Maarten de Rijke and
                  Yiqun Liu and
                  Yixing Fan and
                  Xueqi Cheng",2023,,https://doi.org/10.1145/3539618.3591631,10.1145/3539618.3591631,
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,tang2023inspired_by_learning_strategy,\cite{tang2023inspired_by_learning_strategy},Semantic-Enhanced Differentiable Search Index Inspired by Learning Strategies,https://arxiv.org/abs/2305.15115v1,"Recently, a new paradigm called Differentiable Search Index (DSI) has been proposed for document retrieval, wherein a sequence-to-sequence model is learned to directly map queries to relevant document identifiers. The key idea behind DSI is to fully parameterize traditional ``index-retrieve'' pipelines within a single neural model, by encoding all documents in the corpus into the model parameters. In essence, DSI needs to resolve two major questions: (1) how to assign an identifier to each document, and (2) how to learn the associations between a document and its identifier. In this work, we propose a Semantic-Enhanced DSI model (SE-DSI) motivated by Learning Strategies in the area of Cognitive Psychology. Our approach advances original DSI in two ways: (1) For the document identifier, we take inspiration from Elaboration Strategies in human learning. Specifically, we assign each document an Elaborative Description based on the query generation technique, which is more meaningful than a string of integers in the original DSI; and (2) For the associations between a document and its identifier, we take inspiration from Rehearsal Strategies in human learning. Specifically, we select fine-grained semantic features from a document as Rehearsal Contents to improve document memorization. Both the offline and online experiments show improved retrieval performance over prevailing baselines.",True,True,"Yubao Tang and
                  Ruqing Zhang and
                  Jiafeng Guo and
                  Jiangui Chen and
                  Zuowei Zhu and
                  Shuaiqiang Wang and
                  Dawei Yin and
                  Xueqi Cheng",2023,,https://doi.org/10.1145/3580305.3599903,10.1145/3580305.3599903,
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,uni_gen,\cite{uni_gen},"UniGen: {A} Unified Generative Framework for Retrieval and Question
                  Answering with Large Language Models",,,True,False,"Xiaoxi Li and
                  Yujia Zhou and
                  Zhicheng Dou",2024,,https://doi.org/10.1609/aaai.v38i8.28714,10.1609/AAAI.V38I8.28714,
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,qiao2023diffusionret,\cite{qiao2023diffusionret},DiffusionRet: Diffusion-enhanced generative retriever using constrained decoding,,,True,False,"Qiao, Shanbao and Liu, Xuebing and Na, Seung-Hoon",2023,,,,
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,nie2025large,\cite{nie2025large},Large Language Diffusion Models,https://arxiv.org/abs/2502.09992v3,"The capabilities of large language models (LLMs) are widely regarded as relying on autoregressive models (ARMs). We challenge this notion by introducing LLaDA, a diffusion model trained from scratch under the pre-training and supervised fine-tuning (SFT) paradigm. LLaDA employs a forward data masking process and a reverse generation process, parameterized by a Transformer to predict masked tokens. It provides a principled generative approach for probabilistic inference by optimizing a likelihood lower bound. Across extensive benchmarks on general tasks, math, code, and so on, LLaDA demonstrates strong scalability and performs comparably to our self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong LLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive instruction-following abilities in case studies such as multi-turn dialogue. Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal poem completion task. Our findings show the promise of diffusion models for language modeling at scale and challenge the common assumption that core LLM capabilities discussed above inherently depend on ARMs. Project page and codes: https://ml-gsai.github.io/LLaDA-demo/.",True,True,"Nie, Shen and Zhu, Fengqi and You, Zebin and Zhang, Xiaolu and Ou, Jingyang and Hu, Jun and Zhou, Jun and Lin, Yankai and Wen, Ji-Rong and Li, Chongxuan",2025,,,,arXiv preprint arXiv:2502.09992
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,ye2025dream,\cite{ye2025dream},Dream 7B: Diffusion Large Language Models,https://arxiv.org/abs/2508.15487v1,"We introduce Dream 7B, the most powerful open diffusion large language model to date. Unlike autoregressive (AR) models that generate tokens sequentially, Dream 7B employs discrete diffusion modeling to refine sequences in parallel through iterative denoising. Our model consistently outperforms existing diffusion language models on general, mathematical, and coding tasks. Dream 7B demonstrates superior planning abilities and inference flexibility, including arbitrary-order generation, infilling capabilities, and tunable quality-speed trade-offs. These results are achieved through simple yet effective training techniques, including AR-based LLM initialization and context-adaptive token-level noise rescheduling. We release both Dream-Base and Dream-Instruct to facilitate further research in diffusion-based language modeling.",True,True,"Ye, Jiacheng and Xie, Zhihui and Zheng, Lin and Gao, Jiahui and Wu, Zirui and Jiang, Xin and Li, Zhenguo and Kong, Lingpeng",2025,,,,arXiv preprint arXiv:2508.15487
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,song2020denoising,\cite{song2020denoising},Denoising Diffusion Implicit Models,https://arxiv.org/abs/2010.02502v4,"Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that DDIMs can produce high quality samples $10 \times$ to $50 \times$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space.",True,True,"Jiaming Song and
Chenlin Meng and
Stefano Ermon",2021,,,,
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,ho2020denoising,\cite{ho2020denoising},Denoising Diffusion Probabilistic Models,https://arxiv.org/abs/2006.11239v2,"We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion",True,True,"Ho, Jonathan and Jain, Ajay and Abbeel, Pieter",2020,,,,Advances in neural information processing systems
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,li2022diffusion,\cite{li2022diffusion},Diffusion-LM Improves Controllable Text Generation,https://arxiv.org/abs/2205.14217v1,"Controlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation. While recent works have demonstrated successes on controlling simple sentence attributes (e.g., sentiment), there has been little progress on complex, fine-grained controls (e.g., syntactic structure). To address this challenge, we develop a new non-autoregressive language model based on continuous diffusions that we call Diffusion-LM. Building upon the recent successes of diffusion models in continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variables. The continuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks. We demonstrate successful control of Diffusion-LM for six challenging fine-grained control tasks, significantly outperforming prior work.",True,True,"Li, Xiang Lisa and Thickstun, John and Gulrajani, Ishaan and Liang, Percy and Hashimoto, Tatsunori B",2022,,,,
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,gong2022diffuseq,\cite{gong2022diffuseq},DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models,https://arxiv.org/abs/2210.08933v3,"Recently, diffusion models have emerged as a new paradigm for generative models. Despite the success in domains using continuous signals such as vision and audio, adapting diffusion models to natural language is under-explored due to the discrete nature of texts, especially for conditional generation. We tackle this challenge by proposing DiffuSeq: a diffusion model designed for sequence-to-sequence (Seq2Seq) text generation tasks. Upon extensive evaluation over a wide range of Seq2Seq tasks, we find DiffuSeq achieving comparable or even better performance than six established baselines, including a state-of-the-art model that is based on pre-trained language models. Apart from quality, an intriguing property of DiffuSeq is its high diversity during generation, which is desired in many Seq2Seq tasks. We further include a theoretical analysis revealing the connection between DiffuSeq and autoregressive/non-autoregressive models. Bringing together theoretical analysis and empirical evidence, we demonstrate the great potential of diffusion models in complex conditional language generation tasks. Code is available at \url{https://github.com/Shark-NLP/DiffuSeq}",True,True,"Gong, Shansan and Li, Mukai and Feng, Jiangtao and Wu, Zhiyong and Kong, Lingpeng",2023,,,,
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,gong-etal-2023-diffuseq,\cite{gong-etal-2023-diffuseq},{D}iffu{S}eq-v2: Bridging Discrete and Continuous Text Spaces for Accelerated {S}eq2{S}eq Diffusion Models,,,True,False,"Gong, Shansan  and
Li, Mukai  and
Feng, Jiangtao  and
Wu, Zhiyong  and
Kong, Lingpeng",2023,,,,
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,genie2023,\cite{genie2023},Text generation with diffusion language models: a pre-training approach with continuous paragraph denoise,,,True,False,"Lin, Zhenghao and Gong, Yeyun and Shen, Yelong and Wu, Tong and Fan, Zhihao and Lin, Chen and Duan, Nan and Chen, Weizhu",2023,,,,
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,austin2021structured,\cite{austin2021structured},Structured Denoising Diffusion Models in Discrete State-Spaces,https://arxiv.org/abs/2107.03006v3,"Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusion-like generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. 2021, by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss. For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on LM1B. On the image dataset CIFAR-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space DDPM model.",True,True,"Austin, Jacob and Johnson, Daniel D and Ho, Jonathan and Tarlow, Daniel and Van Den Berg, Rianne",2021,,,,Advances in neural information processing systems
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,hoogeboom2021argmax,\cite{hoogeboom2021argmax},Argmax flows and multinomial diffusion: Learning categorical distributions,,,True,False,"Hoogeboom, Emiel and Nielsen, Didrik and Jaini, Priyank and Forr{\'e}, Patrick and Welling, Max",2021,,,,Advances in neural information processing systems
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,campbell2022continuous,\cite{campbell2022continuous},A continuous time framework for discrete denoising models,,,True,False,"Campbell, Andrew and Benton, Joe and De Bortoli, Valentin and Rainforth, Thomas and Deligiannidis, George and Doucet, Arnaud",2022,,,,Advances in Neural Information Processing Systems
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,he-etal-2023-diffusionbert,\cite{he-etal-2023-diffusionbert},{D}iffusion{BERT}: Improving Generative Masked Language Models with Diffusion Models,,,True,False,"He, Zhengfu  and
      Sun, Tianxiang  and
      Tang, Qiong  and
      Wang, Kuanning  and
      Huang, Xuanjing  and
      Qiu, Xipeng",2023,,,10.18653/v1/2023.acl-long.248,
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,ye2025diffusionlanguagemodelsperform,\cite{ye2025diffusionlanguagemodelsperform},Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning,https://arxiv.org/abs/2308.12219v3,"The recent surge of generative AI has been fueled by the generative power of diffusion probabilistic models and the scalable capabilities of large language models. Despite their potential, it remains elusive whether diffusion language models can solve general language tasks comparable to their autoregressive counterparts. This paper demonstrates that scaling diffusion models w.r.t. data, sizes, and tasks can effectively make them strong language learners. We build competent diffusion language models at scale by first acquiring knowledge from massive data via masked language modeling pretraining thanks to their intrinsic connections. We then reprogram pretrained masked language models into diffusion language models via diffusive adaptation, wherein task-specific finetuning and instruction finetuning are explored to unlock their versatility in solving general language tasks. Experiments show that scaling diffusion language models consistently improves performance across downstream language tasks. We further discover that instruction finetuning can elicit zero-shot and few-shot in-context learning abilities that help tackle many unseen tasks by following natural language instructions, and show promise in advanced and challenging abilities such as reasoning.",True,True,Jiasheng Ye and Zaixiang Zheng and Yu Bao and Lihua Qian and Quanquan Gu,2025,,https://arxiv.org/abs/2308.12219,,
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,ye2024diffusion,\cite{ye2024diffusion},Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models,https://arxiv.org/abs/2402.07754v3,"Recently, diffusion models have garnered significant interest in the field of text processing due to their many potential advantages compared to conventional autoregressive models. In this work, we propose Diffusion-of-Thought (DoT), a novel approach that integrates diffusion models with Chain-of-Thought, a well-established technique for improving the reasoning ability of autoregressive language models. In contrast to autoregressive language models that make decisions in a left-to-right, token-by-token manner, DoT allows reasoning steps to diffuse over time through a diffusion language model and offers greater flexibility in trading-off computation for reasoning performance. Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication, boolean logic, and grade school math problems, with a small diffusion model outperforming a much larger autoregressive model in both efficiency and accuracy. In addition to that, DoT showcases promising self-correction abilities and benefits from existing reasoning-enhancing techniques like self-consistency decoding. Our findings contribute to the understanding and development of reasoning with diffusion language models.",True,True,"Ye, Jiacheng and Gong, Shansan and Chen, Liheng and Zheng, Lin and Gao, Jiahui and Shi, Han and Wu, Chuan and Jiang, Xin and Li, Zhenguo and Bi, Wei and others",2024,,,,Advances in Neural Information Processing Systems
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,ye2024beyond,\cite{ye2024beyond},Beyond autoregression: Discrete diffusion for complex reasoning and planning,,,True,False,"Ye, Jiacheng and Gao, Jiahui and Gong, Shansan and Zheng, Lin and Jiang, Xin and Li, Zhenguo and Kong, Lingpeng",2025,,,,International Conference on Learning Representations
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,ye2025implicit,\cite{ye2025implicit},Implicit Search via Discrete Diffusion: A Study on Chess,https://arxiv.org/abs/2502.19805v1,"In the post-AlphaGo era, there has been a renewed interest in search techniques such as Monte Carlo Tree Search (MCTS), particularly in their application to Large Language Models (LLMs). This renewed attention is driven by the recognition that current next-token prediction models often lack the ability for long-term planning. Is it possible to instill search-like abilities within the models to enhance their planning abilities without relying on explicit search? We propose DiffuSearch , a model that does \textit{implicit search} by looking into the future world via discrete diffusion modeling. We instantiate DiffuSearch on a classical board game, Chess, where explicit search is known to be essential. Through extensive controlled experiments, we show DiffuSearch outperforms both the searchless and explicit search-enhanced policies. Specifically, DiffuSearch outperforms the one-step policy by 19.2% and the MCTS-enhanced policy by 14% on action accuracy. Furthermore, DiffuSearch demonstrates a notable 30% enhancement in puzzle-solving abilities compared to explicit search-based policies, along with a significant 540 Elo increase in game-playing strength assessment. These results indicate that implicit search via discrete diffusion is a viable alternative to explicit search over a one-step policy. All codes are publicly available at \href{https://github.com/HKUNLP/DiffuSearch}{https://github.com/HKUNLP/DiffuSearch}.",True,True,"Ye, Jiacheng and Wu, Zhenyu and Gao, Jiahui and Wu, Zhiyong and Jiang, Xin and Li, Zhenguo and Kong, Lingpeng",2025,,,,International Conference on Learning Representations
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,arriola2025blockdiffusion,\cite{arriola2025blockdiffusion},Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models,https://arxiv.org/abs/2503.09573v3,"Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with KV caching and parallel token sampling. We propose a recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets a new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences. We provide the code, along with the model weights and blog post on the project page: https://m-arriola.com/bd3lms",True,True,Marianne Arriola and Aaron Gokaslan and Justin T. Chiu and Zhihan Yang and Zhixuan Qi and Jiaqi Han and Subham Sekhar Sahoo and Volodymyr Kuleshov,2025,,https://arxiv.org/abs/2503.09573,,
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,gulrajani2023likelihoodbased,\cite{gulrajani2023likelihoodbased},Likelihood-Based Diffusion Language Models,https://arxiv.org/abs/2305.18619v1,"Despite a growing interest in diffusion-based language models, existing work has not shown that these models can attain nontrivial likelihoods on standard language modeling benchmarks. In this work, we take the first steps towards closing the likelihood gap between autoregressive and diffusion-based language models, with the goal of building and releasing a diffusion model which outperforms a small but widely-known autoregressive model. We pursue this goal through algorithmic improvements, scaling laws, and increased compute. On the algorithmic front, we introduce several methodological improvements for the maximum-likelihood training of diffusion language models. We then study scaling laws for our diffusion models and find compute-optimal training regimes which differ substantially from autoregressive models. Using our methods and scaling analysis, we train and release Plaid 1B, a large diffusion language model which outperforms GPT-2 124M in likelihood on benchmark datasets and generates fluent samples in unconditional and zero-shot control settings.",True,True,Ishaan Gulrajani and Tatsunori Hashimoto,2023,,,,
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,lou2023discrete,\cite{lou2023discrete},Discrete Diffusion Language Modeling by Estimating the Ratios of the Data Distribution,,,True,False,"Lou, Aaron and Meng, Chenlin and Ermon, Stefano",2024,,,,
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,gong2025scalingdiffusionlanguagemodels,\cite{gong2025scalingdiffusionlanguagemodels},Scaling Diffusion Language Models via Adaptation from Autoregressive Models,https://arxiv.org/abs/2410.17891v3,"Diffusion Language Models (DLMs) have emerged as a promising new paradigm for text generative modeling, potentially addressing limitations of autoregressive (AR) models. However, current DLMs have been studied at a smaller scale compared to their AR counterparts and lack fair comparison on language modeling benchmarks. Additionally, training diffusion models from scratch at scale remains challenging. Given the prevalence of open-source AR language models, we propose adapting these models to build text diffusion models. We demonstrate connections between AR and diffusion modeling objectives and introduce a simple continual pre-training approach for training diffusion models. Through systematic evaluation on language modeling, reasoning, and commonsense benchmarks, we show that we can convert AR models ranging from 127M to 7B parameters (GPT2 and LLaMA) into diffusion models DiffuGPT and DiffuLLaMA, using less than 200B tokens for training. Our experimental results reveal that these models outperform earlier DLMs and are competitive with their AR counterparts. We release a suite of DLMs (127M-355M-7B) capable of generating fluent text, performing in-context learning, filling in the middle without prompt re-ordering, and following instructions https://github.com/HKUNLP/DiffuLLaMA.",True,True,Shansan Gong and Shivam Agarwal and Yizhe Zhang and Jiacheng Ye and Lin Zheng and Mukai Li and Chenxin An and Peilin Zhao and Wei Bi and Jiawei Han and Hao Peng and Lingpeng Kong,2025,,,,International Conference on Learning Representations
DiffuGR: Generative Document Retrieval with Diffusion Language Models,2511.08150v1,inceptionlabs_mercury,\cite{inceptionlabs_mercury},Mercury: Ultra-Fast Language Models Based on Diffusion,https://arxiv.org/abs/2506.17298v1,"We present Mercury, a new generation of commercial-scale large language models (LLMs) based on diffusion. These models are parameterized via the Transformer architecture and trained to predict multiple tokens in parallel. In this report, we detail Mercury Coder, our first set of diffusion LLMs designed for coding applications. Currently, Mercury Coder comes in two sizes: Mini and Small. These models set a new state-of-the-art on the speed-quality frontier. Based on independent evaluations conducted by Artificial Analysis, Mercury Coder Mini and Mercury Coder Small achieve state-of-the-art throughputs of 1109 tokens/sec and 737 tokens/sec, respectively, on NVIDIA H100 GPUs and outperform speed-optimized frontier models by up to 10x on average while maintaining comparable quality. We discuss additional results on a variety of code benchmarks spanning multiple languages and use-cases as well as real-world validation by developers on Copilot Arena, where the model currently ranks second on quality and is the fastest model overall. We also release a public API at https://platform.inceptionlabs.ai/ and free playground at https://chat.inceptionlabs.ai",True,True,{Inception Labs},2025,,,,
Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs,2511.10480v1,bai2019deepbench,\cite{bai2019deepbench},DeepBench: Benchmarking JSON Document Stores,,,True,False,"Belloni, Stefano and Ritter, Daniel and Schr\""{o}der, Marco and R\""{o}rup, Nils",2022,,https://doi.org/10.1145/3531348.3532176,10.1145/3531348.3532176,
Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs,2511.10480v1,mlperf,\cite{mlperf},MLPerf Inference Benchmark,https://arxiv.org/abs/1911.02549v2,"Machine-learning (ML) hardware and software system demand is burgeoning. Driven by ML applications, the number of different ML inference systems has exploded. Over 100 organizations are building ML inference chips, and the systems that incorporate existing models span at least three orders of magnitude in power consumption and five orders of magnitude in performance; they range from embedded devices to data-center solutions. Fueling the hardware are a dozen or more software frameworks and libraries. The myriad combinations of ML hardware and ML software make assessing ML-system performance in an architecture-neutral, representative, and reproducible manner challenging. There is a clear need for industry-wide standard ML benchmarking and evaluation criteria. MLPerf Inference answers that call. In this paper, we present our benchmarking method for evaluating ML inference systems. Driven by more than 30 organizations as well as more than 200 ML engineers and practitioners, MLPerf prescribes a set of rules and best practices to ensure comparability across systems with wildly differing architectures. The first call for submissions garnered more than 600 reproducible inference-performance measurements from 14 organizations, representing over 30 systems that showcase a wide range of capabilities. The submissions attest to the benchmark's flexibility and adaptability.",True,True,Vijay Janapa Reddi and Christine Cheng and David Kanter and Peter Mattson and Guenther Schmuelling and Carole-Jean Wu and Brian Anderson and Maximilien Breughe and Mark Charlebois and William Chou and Ramesh Chukka and Cody Coleman and Sam Davis and Pan Deng and Greg Diamos and Jared Duke and Dave Fick and J. Scott Gardner and Itay Hubara and Sachin Idgunji and Thomas B. Jablin and Jeff Jiao and Tom St. John and Pankaj Kanwar and David Lee and Jeffery Liao and Anton Lokhmotov and Francisco Massa and Peng Meng and Paulius Micikevicius and Colin Osborne and Gennady Pekhimenko and Arun Tejusve Raghunath Rajan and Dilip Sequeira and Ashish Sirasao and Fei Sun and Hanlin Tang and Michael Thomson and Frank Wei and Ephrem Wu and Lingjie Xu and Koichi Yamada and Bing Yu and George Yuan and Aaron Zhong and Peizhao Zhang and Yuchen Zhou,2020,,https://arxiv.org/abs/1911.02549,,
Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs,2511.10480v1,pytorch-profiler,\cite{pytorch-profiler},PyTorch Profiler Recipe,,,True,False,{PyTorch Contributors},2024,,,,
Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs,2511.10480v1,cupti,\cite{cupti},CUDA Profiling Tools Interface (CUPTI),,,True,False,{NVIDIA Corporation},2024,,,,
Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs,2511.10480v1,pytorch-fx,\cite{pytorch-fx},Torch.fx: Practical Program Capture and Transformation for Deep Learning in Python,,,True,False,James K. Reed and Zachary DeVito and Horace He and Ansley Ussery and Jason Ansel,2022,,https://arxiv.org/abs/2112.08429,,
Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs,2511.10480v1,vtrain,\cite{vtrain},vtrain: A simulation framework for evaluating cost-effective and compute-optimal large language model training,,,True,False,"Bang, Jehyeon and Choi, Yujeong and Kim, Myeongwoo and Kim, Yongdeok and Rhu, Minsoo",2023,,,,arXiv preprint arXiv:2312.12391
Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs,2511.10480v1,madmax,\cite{madmax},MAD Max Beyond Single-Node: Enabling Large Machine Learning Model Acceleration on Distributed Systems,https://arxiv.org/abs/2310.02784v3,"Training and deploying large-scale machine learning models is time-consuming, requires significant distributed computing infrastructures, and incurs high operational costs. Our analysis, grounded in real-world large model training on datacenter-scale infrastructures, reveals that 14~32% of all GPU hours are spent on communication with no overlapping computation. To minimize this outstanding communication latency and other inherent at-scale inefficiencies, we introduce an agile performance modeling framework, MAD-Max. This framework is designed to optimize parallelization strategies and facilitate hardware-software co-design opportunities. Through the application of MAD-Max to a suite of real-world large-scale ML models on state-of-the-art GPU clusters, we showcase potential throughput enhancements of up to 2.24x for pre-training and up to 5.2x for inference scenarios, respectively.",True,True,"Hsia, Samuel and Golden, Alicia and Acun, Bilge and Ardalani, Newsha and DeVito, Zachary and Wei, Gu-Yeon and Brooks, David and Wu, Carole-Jean",2024,,,,
Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs,2511.10480v1,calculon,\cite{calculon},Calculon: a methodology and tool for high-level co-design of systems and large language models,,,True,False,"Isaev, Mikhail and McDonald, Nic and Dennison, Larry and Vuduc, Richard",2023,,,,
Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs,2511.10480v1,Souffle,\cite{Souffle},Optimizing Deep Learning Inference via Global Analysis and Tensor Expressions,,,True,False,"Xia, Chunwei and Zhao, Jiacheng and Sun, Qianqi and Wang, Zheng and Wen, Yuan and Yu, Teng and Feng, Xiaobing and Cui, Huimin",2024,,https://doi.org/10.1145/3617232.3624858,10.1145/3617232.3624858,
Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs,2511.10480v1,TVM,\cite{TVM},TVM: An Automated End-to-End Optimizing Compiler for Deep Learning,https://arxiv.org/abs/1802.04799v3,"There is an increasing need to bring machine learning to a wide diversity of hardware devices. Current frameworks rely on vendor-specific operator libraries and optimize for a narrow range of server-class GPUs. Deploying workloads to new platforms -- such as mobile phones, embedded devices, and accelerators (e.g., FPGAs, ASICs) -- requires significant manual effort. We propose TVM, a compiler that exposes graph-level and operator-level optimizations to provide performance portability to deep learning workloads across diverse hardware back-ends. TVM solves optimization challenges specific to deep learning, such as high-level operator fusion, mapping to arbitrary hardware primitives, and memory latency hiding. It also automates optimization of low-level programs to hardware characteristics by employing a novel, learning-based cost modeling method for rapid exploration of code optimizations. Experimental results show that TVM delivers performance across hardware back-ends that are competitive with state-of-the-art, hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPUs. We also demonstrate TVM's ability to target new accelerator back-ends, such as the FPGA-based generic deep learning accelerator. The system is open sourced and in production use inside several major companies.",True,True,Tianqi Chen and Thierry Moreau and Ziheng Jiang and Lianmin Zheng and Eddie Yan and Meghan Cowan and Haichen Shen and Leyuan Wang and Yuwei Hu and Luis Ceze and Carlos Guestrin and Arvind Krishnamurthy,2018,,https://arxiv.org/abs/1802.04799,,
Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs,2511.10480v1,TensorComprehend,\cite{TensorComprehend},Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions,https://arxiv.org/abs/1802.04730v3,"Deep learning models with convolutional and recurrent networks are now ubiquitous and analyze massive amounts of audio, image, video, text and graph data, with applications in automatic translation, speech-to-text, scene understanding, ranking user preferences, ad placement, etc. Competing frameworks for building these networks such as TensorFlow, Chainer, CNTK, Torch/PyTorch, Caffe1/2, MXNet and Theano, explore different tradeoffs between usability and expressiveness, research or production orientation and supported hardware. They operate on a DAG of computational operators, wrapping high-performance libraries such as CUDNN (for NVIDIA GPUs) or NNPACK (for various CPUs), and automate memory allocation, synchronization, distribution. Custom operators are needed where the computation does not fit existing high-performance library calls, usually at a high engineering cost. This is frequently required when new operators are invented by researchers: such operators suffer a severe performance penalty, which limits the pace of innovation. Furthermore, even if there is an existing runtime call these frameworks can use, it often doesn't offer optimal performance for a user's particular network architecture and dataset, missing optimizations between operators as well as optimizations that can be done knowing the size and shape of data. Our contributions include (1) a language close to the mathematics of deep learning called Tensor Comprehensions, (2) a polyhedral Just-In-Time compiler to convert a mathematical description of a deep learning DAG into a CUDA kernel with delegated memory management and synchronization, also providing optimizations such as operator fusion and specialization for specific sizes, (3) a compilation cache populated by an autotuner. [Abstract cutoff]",True,True,Nicolas Vasilache and Oleksandr Zinenko and Theodoros Theodoridis and Priya Goyal and Zachary DeVito and William S. Moses and Sven Verdoolaege and Andrew Adams and Albert Cohen,2018,,https://arxiv.org/abs/1802.04730,,
Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs,2511.10480v1,pytorch,\cite{pytorch},"PyTorch: An Imperative Style, High-Performance Deep Learning Library",https://arxiv.org/abs/1912.01703v1,"Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.
  In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.
  We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.",True,True,Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas Köpf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala,2019,,https://arxiv.org/abs/1912.01703,,
Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs,2511.10480v1,tensorflow,\cite{tensorflow},{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems,,,True,False,"Mart\'{i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng",2015,,https://www.tensorflow.org/,,
Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs,2511.10480v1,Apollo,\cite{Apollo},Apollo: Automatic Partition-based Operator Fusion through Layer by Layer Optimization,,,True,False,"Zhao, Jie and Gao, Xiong and Xia, Ruijie and Zhang, Zhaochuang and Chen, Deshi and Chen, Lei  and Zhang, Renwei and Geng, Zhen and Cheng, Bin and Jin, Xuefeng",2022,,https://proceedings.mlsys.org/paper_files/paper/2022/file/e175e8a86d28d935be4f43719651f86d-Paper.pdf,,
Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs,2511.10480v1,DNNFusion,\cite{DNNFusion},DNNFusion: Accelerating Deep Neural Networks Execution with Advanced Operator Fusion,https://arxiv.org/abs/2108.13342v2,"Deep Neural Networks (DNNs) have emerged as the core enabler of many major applications on mobile devices. To achieve high accuracy, DNN models have become increasingly deep with hundreds or even thousands of operator layers, leading to high memory and computational requirements for inference. Operator fusion (or kernel/layer fusion) is key optimization in many state-of-the-art DNN execution frameworks, such as TensorFlow, TVM, and MNN. However, these frameworks usually adopt fusion approaches based on certain patterns that are too restrictive to cover the diversity of operators and layer connections. Polyhedral-based loop fusion techniques, on the other hand, work on a low-level view of the computation without operator-level information, and can also miss potential fusion opportunities. To address this challenge, this paper proposes a novel and extensive loop fusion framework called DNNFusion. The basic idea of this work is to work at an operator view of DNNs, but expand fusion opportunities by developing a classification of both individual operators and their combinations. In addition, DNNFusion includes 1) a novel mathematical-property-based graph rewriting framework to reduce evaluation costs and facilitate subsequent operator fusion, 2) an integrated fusion plan generation that leverages the high-level analysis and accurate light-weight profiling, and 3) additional optimizations during fusion code generation. DNNFusion is extensively evaluated on 15 DNN models with varied types of tasks, model sizes, and layer counts. The evaluation results demonstrate that DNNFusion finds up to 8.8x higher fusion opportunities, outperforms four state-of-the-art DNN execution frameworks with 9.3x speedup. The memory requirement reduction and speedups can enable the execution of many of the target models on mobile devices and even make them part of a real-time application.",True,True,"Niu, Wei and Guan, Jiexiong and Wang, Yanzhi and Agrawal, Gagan and Ren, Bin",2021,,https://doi.org/10.1145/3453483.3454083,10.1145/3453483.3454083,
Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs,2511.10480v1,FlexFlow,\cite{FlexFlow},Beyond Data and Model Parallelism for Deep Neural Networks,,,True,False,Zhihao Jia and Matei Zaharia and Alex Aiken,2018,,https://arxiv.org/abs/1807.05358,,
Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs,2511.10480v1,Unity,\cite{Unity},Unity: Accelerating {DNN} Training Through Joint Optimization of Algebraic Transformations and Parallelization,,,True,False,Colin Unger and Zhihao Jia and Wei Wu and Sina Lin and Mandeep Baines and Carlos Efrain Quintero Narvaez and Vinay Ramakrishnaiah and Nirmal Prajapati and Pat McCormick and Jamaludin Mohd-Yusof and Xi Luo and Dheevatsa Mudigere and Jongsoo Park and Misha Smelyanskiy and Alex Aiken,2022,,https://www.usenix.org/conference/osdi22/presentation/unger,,
Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs,2511.10480v1,Zhu2025Mist,\cite{Zhu2025Mist},Mist: Efficient Distributed Training of Large Language Models via Memory-Parallelism Co-Optimization,https://arxiv.org/abs/2503.19050v1,"Various parallelism, such as data, tensor, and pipeline parallelism, along with memory optimizations like activation checkpointing, redundancy elimination, and offloading, have been proposed to accelerate distributed training for Large Language Models. To find the best combination of these techniques, automatic distributed training systems are proposed. However, existing systems only tune a subset of optimizations, due to the lack of overlap awareness, inability to navigate the vast search space, and ignoring the inter-microbatch imbalance, leading to sub-optimal performance. To address these shortcomings, we propose Mist, a memory, overlap, and imbalance-aware automatic distributed training system that comprehensively co-optimizes all memory footprint reduction techniques alongside parallelism. Mist is based on three key ideas: (1) fine-grained overlap-centric scheduling, orchestrating optimizations in an overlapped manner, (2) symbolic-based performance analysis that predicts runtime and memory usage using symbolic expressions for fast tuning, and (3) imbalance-aware hierarchical tuning, decoupling the process into an inter-stage imbalance and overlap aware Mixed Integer Linear Programming problem and an intra-stage Dual-Objective Constrained Optimization problem, and connecting them through Pareto frontier sampling. Our evaluation results show that Mist achieves an average of 1.28$\times$ (up to 1.73$\times$) and 1.27$\times$ (up to 2.04$\times$) speedup compared to state-of-the-art manual system Megatron-LM and state-of-the-art automatic system Aceso, respectively.",True,True,"Zhu, Zhanda and Giannoula, Christina and Andoorveedu, Muralidhar and Su, Qidong and Mangalam, Karttikeya and Zheng, Bojian and Pekhimenko, Gennady",2025,,http://dx.doi.org/10.1145/3689031.3717461,10.1145/3689031.3717461,
"Think, Remember, Navigate: Zero-Shot Object-Goal Navigation with VLM-Powered Reasoning",2511.08942v1,mousavian2019visual,\cite{mousavian2019visual},Visual Representations for Semantic Target Driven Navigation,,,True,False,"Mousavian, Arsalan and others",2019,,,,
"Think, Remember, Navigate: Zero-Shot Object-Goal Navigation with VLM-Powered Reasoning",2511.08942v1,ye2021efficient,\cite{ye2021efficient},Efficient Robotic Object Search via HIEM: Hierarchical Policy Learning with Intrinsic-Extrinsic Modeling,https://arxiv.org/abs/2010.08596v2,"Despite the significant success at enabling robots with autonomous behaviors makes deep reinforcement learning a promising approach for robotic object search task, the deep reinforcement learning approach severely suffers from the nature sparse reward setting of the task. To tackle this challenge, we present a novel policy learning paradigm for the object search task, based on hierarchical and interpretable modeling with an intrinsic-extrinsic reward setting. More specifically, we explore the environment efficiently through a proxy low-level policy which is driven by the intrinsic rewarding sub-goals. We further learn our hierarchical policy from the efficient exploration experience where we optimize both of our high-level and low-level policies towards the extrinsic rewarding goal to perform the object search task well. Experiments conducted on the House3D environment validate and show that the robot, trained with our model, can perform the object search task in a more optimal and interpretable way.",True,True,"Ye, Xin and Yang, Yi",2021,,,,IEEE Robotics and Automation Letters
"Think, Remember, Navigate: Zero-Shot Object-Goal Navigation with VLM-Powered Reasoning",2511.08942v1,chaplot2020object,\cite{chaplot2020object},Object Goal Navigation using Goal-Oriented Semantic Exploration,https://arxiv.org/abs/2007.00643v2,"This work studies the problem of object goal navigation which involves navigating to an instance of the given object category in unseen environments. End-to-end learning-based navigation methods struggle at this task as they are ineffective at exploration and long-term planning. We propose a modular system called, `Goal-Oriented Semantic Exploration' which builds an episodic semantic map and uses it to explore the environment efficiently based on the goal object category. Empirical results in visually realistic simulation environments show that the proposed model outperforms a wide range of baselines including end-to-end learning-based methods as well as modular map-based methods and led to the winning entry of the CVPR-2020 Habitat ObjectNav Challenge. Ablation analysis indicates that the proposed model learns semantic priors of the relative arrangement of objects in a scene, and uses them to explore efficiently. Domain-agnostic module design allow us to transfer our model to a mobile robot platform and achieve similar performance for object goal navigation in the real-world.",True,True,"Chaplot, Devendra Singh and others",2020,,,,
"Think, Remember, Navigate: Zero-Shot Object-Goal Navigation with VLM-Powered Reasoning",2511.08942v1,kumar2021gcexp,\cite{kumar2021gcexp},GCE{XP}: Goal-Conditioned Exploration for Object Goal Navigation,,,True,False,"Kumar, G and others",2021,,,,
"Think, Remember, Navigate: Zero-Shot Object-Goal Navigation with VLM-Powered Reasoning",2511.08942v1,sun2024survey,\cite{sun2024survey},A Survey of Object Goal Navigation,,,True,False,"Sun, Jie and Wu, Jian and Ji, Zhe and Lai, Yu-Kun",2024,,,,IEEE Transactions on Automation Science and Engineering
"Think, Remember, Navigate: Zero-Shot Object-Goal Navigation with VLM-Powered Reasoning",2511.08942v1,gadre2022clip,\cite{gadre2022clip},{CLIP} on Wheels: Zero-shot Object Navigation as Object Localization and Exploration,,,True,False,"Gadre, Shubham and others",2022,,,,
"Think, Remember, Navigate: Zero-Shot Object-Goal Navigation with VLM-Powered Reasoning",2511.08942v1,zhou2023esc,\cite{zhou2023esc},{ESC}: Exploration with Soft Commonsense Constraints for Zero-Shot Object Navigation,,,True,False,"Zhou, Kaizhi and others",2023,,,,
"Think, Remember, Navigate: Zero-Shot Object-Goal Navigation with VLM-Powered Reasoning",2511.08942v1,yu2023l3mvn,\cite{yu2023l3mvn},L3MVN: Leveraging Large Language Models for Visual Target Navigation,https://arxiv.org/abs/2304.05501v2,"Visual target navigation in unknown environments is a crucial problem in robotics. Despite extensive investigation of classical and learning-based approaches in the past, robots lack common-sense knowledge about household objects and layouts. Prior state-of-the-art approaches to this task rely on learning the priors during the training and typically require significant expensive resources and time for learning. To address this, we propose a new framework for visual target navigation that leverages Large Language Models (LLM) to impart common sense for object searching. Specifically, we introduce two paradigms: (i) zero-shot and (ii) feed-forward approaches that use language to find the relevant frontier from the semantic map as a long-term goal and explore the environment efficiently. Our analysis demonstrates the notable zero-shot generalization and transfer capabilities from the use of language. Experiments on Gibson and Habitat-Matterport 3D (HM3D) demonstrate that the proposed framework significantly outperforms existing map-based methods in terms of success rate and generalization. Ablation analysis also indicates that the common-sense knowledge from the language model leads to more efficient semantic exploration. Finally, we provide a real robot experiment to verify the applicability of our framework in real-world scenarios. The supplementary video and code can be accessed via the following link: https://sites.google.com/view/l3mvn.",True,True,"Yu, Bowen and Kasaei, Hamed and Cao, Ming",2023,,,,
"Think, Remember, Navigate: Zero-Shot Object-Goal Navigation with VLM-Powered Reasoning",2511.08942v1,yokoyama2024vlfm,\cite{yokoyama2024vlfm},VLFM: Vision-Language Frontier Maps for Zero-Shot Semantic Navigation,https://arxiv.org/abs/2312.03275v1,"Understanding how humans leverage semantic knowledge to navigate unfamiliar environments and decide where to explore next is pivotal for developing robots capable of human-like search behaviors. We introduce a zero-shot navigation approach, Vision-Language Frontier Maps (VLFM), which is inspired by human reasoning and designed to navigate towards unseen semantic objects in novel environments. VLFM builds occupancy maps from depth observations to identify frontiers, and leverages RGB observations and a pre-trained vision-language model to generate a language-grounded value map. VLFM then uses this map to identify the most promising frontier to explore for finding an instance of a given target object category. We evaluate VLFM in photo-realistic environments from the Gibson, Habitat-Matterport 3D (HM3D), and Matterport 3D (MP3D) datasets within the Habitat simulator. Remarkably, VLFM achieves state-of-the-art results on all three datasets as measured by success weighted by path length (SPL) for the Object Goal Navigation task. Furthermore, we show that VLFM's zero-shot nature enables it to be readily deployed on real-world robots such as the Boston Dynamics Spot mobile manipulation platform. We deploy VLFM on Spot and demonstrate its capability to efficiently navigate to target objects within an office building in the real world, without any prior knowledge of the environment. The accomplishments of VLFM underscore the promising potential of vision-language models in advancing the field of semantic navigation. Videos of real-world deployment can be viewed at naoki.io/vlfm.",True,True,"Yokoyama, Naoki and others",2024,,,,
"Think, Remember, Navigate: Zero-Shot Object-Goal Navigation with VLM-Powered Reasoning",2511.08942v1,zhao2024imaginenav,\cite{zhao2024imaginenav},ImagineNav: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination,https://arxiv.org/abs/2410.09874v1,"Visual navigation is an essential skill for home-assistance robots, providing the object-searching ability to accomplish long-horizon daily tasks. Many recent approaches use Large Language Models (LLMs) for commonsense inference to improve exploration efficiency. However, the planning process of LLMs is limited within texts and it is difficult to represent the spatial occupancy and geometry layout only by texts. Both are important for making rational navigation decisions. In this work, we seek to unleash the spatial perception and planning ability of Vision-Language Models (VLMs), and explore whether the VLM, with only on-board camera captured RGB/RGB-D stream inputs, can efficiently finish the visual navigation tasks in a mapless manner. We achieve this by developing the imagination-powered navigation framework ImagineNav, which imagines the future observation images at valuable robot views and translates the complex navigation planning process into a rather simple best-view image selection problem for VLM. To generate appropriate candidate robot views for imagination, we introduce the Where2Imagine module, which is distilled to align with human navigation habits. Finally, to reach the VLM preferred views, an off-the-shelf point-goal navigation policy is utilized. Empirical experiments on the challenging open-vocabulary object navigation benchmarks demonstrates the superiority of our proposed system.",True,True,"Zhao, Xin and others",2024,,,,
"Think, Remember, Navigate: Zero-Shot Object-Goal Navigation with VLM-Powered Reasoning",2511.08942v1,wei2023chainofthoughtprompting,\cite{wei2023chainofthoughtprompting},Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,https://arxiv.org/abs/2201.11903v6,"We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",True,True,"Wei, Jason and others",2023,,,,
Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,2511.09109v2,lewis2020retrieval,\cite{lewis2020retrieval},Retrieval-augmented Generation for Knowledge-intensive NLP Tasks,,,True,False,"Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\""u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\""a}schel, Tim and others",2020,,,,Advances in neural information processing systems
Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,2511.09109v2,achiam2023gpt,\cite{achiam2023gpt},GPT-4 Technical Report,https://arxiv.org/abs/2303.08774v6,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",True,True,"Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others",2023,,,,arXiv preprint arXiv:2303.08774
Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,2511.09109v2,team2024gemini,\cite{team2024gemini},Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,https://arxiv.org/abs/2403.05530v5,"In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. The family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds the February version on the great majority of capabilities and benchmarks; (2) Gemini 1.5 Flash, a more lightweight variant designed for efficiency with minimal regression in quality. Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world use cases, such as Gemini 1.5 collaborating with professionals on completing their tasks achieving 26 to 75% time savings across 10 different job categories, as well as surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.",True,True,"{Gemini Team} and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others",2024,,,,arXiv preprint arXiv:2403.05530
Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,2511.09109v2,LLMSurvey,\cite{LLMSurvey},Large Language Models: A Survey,https://arxiv.org/abs/2402.06196v3,"Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws \cite{kaplan2020scaling,hoffmann2022training}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we conclude the paper by discussing open challenges and future research directions.",True,True,"Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong",2023,,http://arxiv.org/abs/2303.18223,,arXiv preprint arXiv:2303.18223
Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,2511.09109v2,zhang2023siren,\cite{zhang2023siren},Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models,,,True,False,"Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and others",2023,,,,arXiv preprint arXiv:2309.01219
Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,2511.09109v2,liu2024robust,\cite{liu2024robust},Robust Neural Information Retrieval: An Adversarial and Out-of-distribution Perspective,https://arxiv.org/abs/2407.06992v2,"Recent advances in neural information retrieval (IR) models have significantly enhanced their effectiveness over various IR tasks. The robustness of these models, essential for ensuring their reliability in practice, has also garnered significant attention. With a wide array of research on robust IR being proposed, we believe it is the opportune moment to consolidate the current status, glean insights from existing methodologies, and lay the groundwork for future development. We view the robustness of IR to be a multifaceted concept, emphasizing its necessity against adversarial attacks, out-of-distribution (OOD) scenarios and performance variance. With a focus on adversarial and OOD robustness, we dissect robustness solutions for dense retrieval models (DRMs) and neural ranking models (NRMs), respectively, recognizing them as pivotal components of the neural IR pipeline. We provide an in-depth discussion of existing methods, datasets, and evaluation metrics, shedding light on challenges and future directions in the era of large language models. To the best of our knowledge, this is the first comprehensive survey on the robustness of neural IR models, and we will also be giving our first tutorial presentation at SIGIR 2024 \url{https://sigir2024-robust-information-retrieval.github.io}. Along with the organization of existing work, we introduce a Benchmark for robust IR (BestIR), a heterogeneous evaluation benchmark for robust neural information retrieval, which is publicly available at \url{https://github.com/Davion-Liu/BestIR}. We hope that this study provides useful clues for future research on the robustness of IR models and helps to develop trustworthy search engines \url{https://github.com/Davion-Liu/Awesome-Robustness-in-Information-Retrieval}.",True,True,"Liu, Yu-An and Zhang, Ruqing and Guo, Jiafeng and de Rijke, Maarten and Fan, Yixing and Cheng, Xueqi",,,,,ACM Transactions on Information Systems
Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,2511.09109v2,liu2025attack,\cite{liu2025attack},Attack-in-the-Chain: Bootstrapping Large Language Models for Attacks against Black-box Neural Ranking Models,,,True,False,"Liu, Yu-An and Zhang, Ruqing and Guo, Jiafeng and de Rijke, Maarten and Fan, Yixing and Cheng, Xueqi",2025,,,,
Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,2511.09109v2,liu2025scaling,\cite{liu2025scaling},On the Scaling of Robustness and Effectiveness in Dense Retrieval,,,True,False,"Liu, Yu-An and Zhang, Ruqing and Guo, Jiafeng and de Rijke, Maarten and Fan, Yixing and Cheng, Xueqi",2025,,,,
Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,2511.09109v2,gao2023retrieval,\cite{gao2023retrieval},Retrieval-augmented Generation for Large Language Models: A Survey,,,True,False,"Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yixin and Sun, Jiawei and Wang, Haofen and Wang, Haofen",2023,,,,arXiv preprint arXiv:2312.10997
Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,2511.09109v2,shuster2021retrieval,\cite{shuster2021retrieval},Retrieval Augmentation Reduces Hallucination in Conversation,https://arxiv.org/abs/2104.07567v1,"Despite showing increasingly human-like conversational abilities, state-of-the-art dialogue models often suffer from factual incorrectness and hallucination of knowledge (Roller et al., 2020). In this work we explore the use of neural-retrieval-in-the-loop architectures - recently shown to be effective in open-domain QA (Lewis et al., 2020b; Izacard and Grave, 2020) - for knowledge-grounded dialogue, a task that is arguably more challenging as it requires querying based on complex multi-turn dialogue context and generating conversationally coherent responses. We study various types of architectures with multiple components - retrievers, rankers, and encoder-decoders - with the goal of maximizing knowledgeability while retaining conversational ability. We demonstrate that our best models obtain state-of-the-art performance on two knowledge-grounded conversational tasks. The models exhibit open-domain conversational capabilities, generalize effectively to scenarios not within the training data, and, as verified by human evaluations, substantially reduce the well-known problem of knowledge hallucination in state-of-the-art chatbots.",True,True,"Shuster, Kurt  and
      Poff, Spencer  and
      Chen, Moya  and
      Kiela, Douwe  and
      Weston, Jason",2021,,https://aclanthology.org/2021.findings-emnlp.320/,10.18653/v1/2021.findings-emnlp.320,
Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,2511.09109v2,jiang2023activeretrievalaugmentedgeneration,\cite{jiang2023activeretrievalaugmentedgeneration},Active Retrieval Augmented Generation,https://arxiv.org/abs/2305.06983v2,"Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.",True,True,"Jiang, Zhengbao and Xu, Frank F. and Gao, Luyu and Sun, Zhiqing and Liu, Qian and Dwivedi-Yu, Jane and Yang, Yiming and Callan, Jamie and Neubig, Graham",2023,,,,
Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,2511.09109v2,shi2025search,\cite{shi2025search},Search and Refine during Think: Autonomous Retrieval-augmented Reasoning of LLMs,,,True,False,"Shi, Yaorui and Li, Shihan and Wu, Chang and Liu, Zhiyuan and Fang, Junfeng and Cai, Hengxing and Zhang, An and Wang, Xiang",2025,,,,arXiv preprint arXiv:2505.11277
Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,2511.09109v2,li2025towards,\cite{li2025towards},Towards AI Search Paradigm,https://arxiv.org/abs/2506.17188v1,"In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint for next-generation search systems capable of emulating human information processing and decision-making. The paradigm employs a modular architecture of four LLM-powered agents (Master, Planner, Executor and Writer) that dynamically adapt to the full spectrum of information needs, from simple factual queries to complex multi-stage reasoning tasks. These agents collaborate dynamically through coordinated workflows to evaluate query complexity, decompose problems into executable plans, and orchestrate tool usage, task execution, and content synthesis. We systematically present key methodologies for realizing this paradigm, including task planning and tool integration, execution strategies, aligned and robust retrieval-augmented generation, and efficient LLM inference, spanning both algorithmic techniques and infrastructure-level optimizations. By providing an in-depth guide to these foundational components, this work aims to inform the development of trustworthy, adaptive, and scalable AI search systems.",True,True,"Li, Yuchen and Cai, Hengyi and Kong, Rui and Chen, Xinran and Chen, Jiamin and Yang, Jun and Zhang, Haojie and Li, Jiayi and Wu, Jiayi and Chen, Yiqun and others",2025,,,,arXiv preprint arXiv:2506.17188
Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,2511.09109v2,asai2023Self-RAG,\cite{asai2023Self-RAG},"Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",https://arxiv.org/abs/2310.11511v1,"Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.",True,True,"Asai, Akari and Wu, Zeqiu and Wang, Yizhong and Sil, Avirup and Hajishirzi, Hannaneh",2023,,,,
Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,2511.09109v2,shi2023REPLUG,\cite{shi2023REPLUG},REPLUG: Retrieval-Augmented Black-Box Language Models,https://arxiv.org/abs/2301.12652v4,"We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing retrieval and language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%.",True,True,"Shi, Weijia and Min, Sewon and Yasunaga, Michihiro and Seo, Minjoon and James, Richard and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-tau",2024,,,,
Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,2511.09109v2,lin2024RA-DIT,\cite{lin2024RA-DIT},RA-DIT: Retrieval-Augmented Dual Instruction Tuning,https://arxiv.org/abs/2310.01352v4,"Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B, achieves state-of-the-art performance across a range of knowledge-intensive zero- and few-shot learning benchmarks, significantly outperforming existing in-context RALM approaches by up to +8.9% in 0-shot setting and +1.4% in 5-shot setting on average.",True,True,"Lin, Xi Victoria and Chen, Xilun and Chen, Mingda and Shi, Weijia and Lomeli, Maria and James, Richard and Rodriguez, Pedro and Kahn, Jacob and Szilvasy, Gergely and Lewis, Mike and others",2023,,,,
Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,2511.09109v2,trivedi2023ircot,\cite{trivedi2023ircot},Interleaving Retrieval with Chain-of-thought Reasoning for Knowledge-intensive Multi-step Questions,,,True,False,"Trivedi, Harsh and Balasubramanian, Niranjan and Khot, Tushar and Sabharwal, Ashish",2023,,,,
Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,2511.09109v2,li2025searcho1,\cite{li2025searcho1},Search-o1: Agentic Search-Enhanced Large Reasoning Models,https://arxiv.org/abs/2501.05366v1,"Large reasoning models (LRMs) like OpenAI-o1 have demonstrated impressive long stepwise reasoning capabilities through large-scale reinforcement learning. However, their extended reasoning processes often suffer from knowledge insufficiency, leading to frequent uncertainties and potential errors. To address this limitation, we introduce \textbf{Search-o1}, a framework that enhances LRMs with an agentic retrieval-augmented generation (RAG) mechanism and a Reason-in-Documents module for refining retrieved documents. Search-o1 integrates an agentic search workflow into the reasoning process, enabling dynamic retrieval of external knowledge when LRMs encounter uncertain knowledge points. Additionally, due to the verbose nature of retrieved documents, we design a separate Reason-in-Documents module to deeply analyze the retrieved information before injecting it into the reasoning chain, minimizing noise and preserving coherent reasoning flow. Extensive experiments on complex reasoning tasks in science, mathematics, and coding, as well as six open-domain QA benchmarks, demonstrate the strong performance of Search-o1. This approach enhances the trustworthiness and applicability of LRMs in complex reasoning tasks, paving the way for more reliable and versatile intelligent systems. The code is available at \url{https://github.com/sunnynexus/Search-o1}.",True,True,"Li, Xiaoxi and Dong, Guanting and Jin, Jiajie and Zhang, Yuyao and Zhou, Yujia and Zhu, Yutao and Zhang, Peitian and Dou, Zhicheng",2025,,,,arXiv preprint arXiv:2501.05366
Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,2511.09109v2,jin2025searchr1,\cite{jin2025searchr1},Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning,,,True,False,"Jin, Bowen and Zeng, Hansi and Yue, Zhenrui and Yoon, Jinsung and Arik, Sercan and Wang, Dong and Zamani, Hamed and Han, Jiawei",2025,,,,arXiv preprint arXiv:2503.09516
Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,2511.09109v2,kaelbling1996reinforcement,\cite{kaelbling1996reinforcement},Reinforcement Learning: A Survey,https://arxiv.org/abs/cs/9605103v1,"This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.",True,True,"Kaelbling, Leslie Pack and Littman, Michael L and Moore, Andrew W",1996,,,,Journal of Artificial Intelligence Research
Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,2511.09109v2,ouyang2022training,\cite{ouyang2022training},Training language models to follow instructions with human feedback,https://arxiv.org/abs/2203.02155v1,"Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",True,True,"Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",2022,,,,Advances in neural information processing systems
Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,2511.09109v2,schulman2017proximal,\cite{schulman2017proximal},Proximal Policy Optimization Algorithms,https://arxiv.org/abs/1707.06347v2,"We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a ""surrogate"" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.",True,True,"Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg",2017,,,,arXiv preprint arXiv:1707.06347
Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,2511.09109v2,rafailov2023direct,\cite{rafailov2023direct},Direct Preference Optimization: Your Language Model is Secretly a Reward Model,https://arxiv.org/abs/2305.18290v3,"While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.",True,True,"Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",2023,,,,Advances in Neural Information Processing Systems
Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,2511.09109v2,shao2024deepseekmath,\cite{shao2024deepseekmath},DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models,,,True,False,"Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Y and others",2024,,,,arXiv preprint arXiv:2402.03300
Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,2511.09109v2,jaech2024openai,\cite{jaech2024openai},OpenAI o1 System Card,https://arxiv.org/abs/2412.16720v1,"The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. These advanced reasoning capabilities provide new avenues for improving the safety and robustness of our models. In particular, our models can reason about our safety policies in context when responding to potentially unsafe prompts, through deliberative alignment. This leads to state-of-the-art performance on certain benchmarks for risks such as generating illicit advice, choosing stereotyped responses, and succumbing to known jailbreaks. Training models to incorporate a chain of thought before answering has the potential to unlock substantial benefits, while also increasing potential risks that stem from heightened intelligence. Our results underscore the need for building robust alignment methods, extensively stress-testing their efficacy, and maintaining meticulous risk management protocols. This report outlines the safety work carried out for the OpenAI o1 and OpenAI o1-mini models, including safety evaluations, external red teaming, and Preparedness Framework evaluations.",True,True,"Jaech, Aaron and Kalai, Adam and Lerer, Adam and Richardson, Adam and El-Kishky, Ahmed and Low, Aiden and Helyar, Alec and Madry, Aleksander and Beutel, Alex and Carney, Alex and others",2024,,,,arXiv preprint arXiv:2412.16720
Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,2511.09109v2,guo2025deepseek,\cite{guo2025deepseek},DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,https://arxiv.org/abs/2501.12948v1,"We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.",True,True,"Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others",2025,,,,arXiv preprint arXiv:2501.12948
Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,2511.09109v2,qwen2025qwen25technicalreport,\cite{qwen2025qwen25technicalreport},Qwen2 Technical Report,https://arxiv.org/abs/2407.10671v4,"This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of foundational and instruction-tuned language models, encompassing a parameter range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts model. Qwen2 surpasses most prior open-weight models, including its predecessor Qwen1.5, and exhibits competitive performance relative to proprietary models across diverse benchmarks on language understanding, generation, multilingual proficiency, coding, mathematics, and reasoning.
  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2 demonstrates robust multilingual capabilities, proficient in approximately 30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and global reach.
  To foster community innovation and accessibility, we have made the Qwen2 model weights openly available on Hugging Face and ModelScope, and the supplementary materials including example code on GitHub. These platforms also include resources for quantization, fine-tuning, and deployment, facilitating a wide range of applications and research endeavors.",True,True,"Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others",2024b,,,,arXiv preprint arXiv:2412.15115
Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,2511.09109v2,barrett2008learning,\cite{barrett2008learning},Learning All Optimal Policies with Multiple Criteria,,,True,False,"Barrett, Leon and Narayanan, Srini",2008,,,,
Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,2511.09109v2,roijers2013survey,\cite{roijers2013survey},A Survey of Multi-Objective Sequential Decision-Making,https://arxiv.org/abs/1402.0590v1,"Sequential decision-making problems with multiple objectives arise naturally in practice and pose unique challenges for research in decision-theoretic planning and learning, which has largely focused on single-objective settings. This article surveys algorithms designed for sequential decision-making problems with multiple objectives. Though there is a growing body of literature on this subject, little of it makes explicit under what circumstances special methods are needed to solve multi-objective problems. Therefore, we identify three distinct scenarios in which converting such a problem to a single-objective one is impossible, infeasible, or undesirable. Furthermore, we propose a taxonomy that classifies multi-objective methods according to the applicable scenario, the nature of the scalarization function (which projects multi-objective values to scalar ones), and the type of policies considered. We show how these factors determine the nature of an optimal solution, which can be a single policy, a convex hull, or a Pareto front. Using this taxonomy, we survey the literature on multi-objective methods for planning and learning. Finally, we discuss key applications of such methods and outline opportunities for future work.",True,True,"Roijers, Diederik M and Vamplew, Peter and Whiteson, Shimon and Dazeley, Richard",2013,,,,Journal of Artificial Intelligence Research
Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,2511.09109v2,li2020deep,\cite{li2020deep},Deep Reinforcement Learning for Multiobjective Optimization,,,True,False,"Li, Kaiwen and Zhang, Tao and Wang, Rui",2020,,,,IEEE Transactions on Cybernetics
Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning,2511.09109v2,rame2023rewarded,\cite{rame2023rewarded},Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards,https://arxiv.org/abs/2306.04488v2,"Foundation models are first pre-trained on vast unsupervised datasets and then fine-tuned on labeled data. Reinforcement learning, notably from human feedback (RLHF), can further align the network with the intended usage. Yet the imperfections in the proxy reward may hinder the training and lead to suboptimal results; the diversity of objectives in real-world tasks and human opinions exacerbate the issue. This paper proposes embracing the heterogeneity of diverse rewards by following a multi-policy strategy. Rather than focusing on a single a priori reward, we aim for Pareto-optimal generalization across the entire space of preferences. To this end, we propose rewarded soup, first specializing multiple networks independently (one for each proxy reward) and then interpolating their weights linearly. This succeeds empirically because we show that the weights remain linearly connected when fine-tuned on diverse rewards from a shared pre-trained initialization. We demonstrate the effectiveness of our approach for text-to-text (summarization, Q&A, helpful assistant, review), text-image (image captioning, text-to-image generation, visual grounding, VQA), and control (locomotion) tasks. We hope to enhance the alignment of deep models, and how they interact with the world in all its diversity.",True,True,"Rame, Alexandre and Couairon, Guillaume and Dancette, Corentin and Gaya, Jean-Baptiste and Shukor, Mustafa and Soulier, Laure and Cord, Matthieu",2023,,,,Advances in Neural Information Processing Systems
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,li2022neural,\cite{li2022neural},Neural 3D Video Synthesis from Multi-view Video,https://arxiv.org/abs/2103.02597v2,"We propose a novel approach for 3D video synthesis that is able to represent multi-view video recordings of a dynamic real-world scene in a compact, yet expressive representation that enables high-quality view synthesis and motion interpolation. Our approach takes the high quality and compactness of static neural radiance fields in a new direction: to a model-free, dynamic setting. At the core of our approach is a novel time-conditioned neural radiance field that represents scene dynamics using a set of compact latent codes. We are able to significantly boost the training speed and perceptual quality of the generated imagery by a novel hierarchical training scheme in combination with ray importance sampling. Our learned representation is highly compact and able to represent a 10 second 30 FPS multiview video recording by 18 cameras with a model size of only 28MB. We demonstrate that our method can render high-fidelity wide-angle novel views at over 1K resolution, even for complex and dynamic scenes. We perform an extensive qualitative and quantitative evaluation that shows that our approach outperforms the state of the art. Project website: https://neural-3d-video.github.io/.",True,True,"Li, Tianye and Slavcheva, Mira and Zollhoefer, Michael and Green, Simon and Lassner, Christoph and Kim, Changil and Schmidt, Tanner and Lovegrove, Steven and Goesele, Michael and Newcombe, Richard and others",2022,,,,
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,li2021neural,\cite{li2021neural},Neural scene flow fields for space-time view synthesis of dynamic scenes,,,True,False,"Li, Zhengqi and Niklaus, Simon and Snavely, Noah and Wang, Oliver",2021,,,,
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,park2021hypernerf,\cite{park2021hypernerf},HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields,,,True,False,"Park, Keunhong and Sinha, Utkarsh and Hedman, Peter and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Martin-Brualla, Ricardo and Seitz, Steven M.",2021,dec,,,ACM Trans. Graph.
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,fridovich2023k,\cite{fridovich2023k},"K-Planes: Explicit Radiance Fields in Space, Time, and Appearance",https://arxiv.org/abs/2301.10241v2,"We introduce k-planes, a white-box model for radiance fields in arbitrary dimensions. Our model uses d choose 2 planes to represent a d-dimensional scene, providing a seamless way to go from static (d=3) to dynamic (d=4) scenes. This planar factorization makes adding dimension-specific priors easy, e.g. temporal smoothness and multi-resolution spatial structure, and induces a natural decomposition of static and dynamic components of a scene. We use a linear feature decoder with a learned color basis that yields similar performance as a nonlinear black-box MLP decoder. Across a range of synthetic and real, static and dynamic, fixed and varying appearance scenes, k-planes yields competitive and often state-of-the-art reconstruction fidelity with low memory usage, achieving 1000x compression over a full 4D grid, and fast optimization with a pure PyTorch implementation. For video results and code, please see https://sarafridov.github.io/K-Planes.",True,True,"Fridovich-Keil, Sara and Meanti, Giacomo and Warburg, Frederik Rahb{\ae}k and Recht, Benjamin and Kanazawa, Angjoo",2023,,,,
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,cao2023hexplane,\cite{cao2023hexplane},Hexplane: A fast representation for dynamic scenes,,,True,False,"Cao, Ang and Johnson, Justin",2023,,,,
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,yang2023real,\cite{yang2023real},Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting,https://arxiv.org/abs/2310.10642v3,"Reconstructing dynamic 3D scenes from 2D images and generating diverse views over time is challenging due to scene complexity and temporal dynamics. Despite advancements in neural implicit models, limitations persist: (i) Inadequate Scene Structure: Existing methods struggle to reveal the spatial and temporal structure of dynamic scenes from directly learning the complex 6D plenoptic function. (ii) Scaling Deformation Modeling: Explicitly modeling scene element deformation becomes impractical for complex dynamics. To address these issues, we consider the spacetime as an entirety and propose to approximate the underlying spatio-temporal 4D volume of a dynamic scene by optimizing a collection of 4D primitives, with explicit geometry and appearance modeling. Learning to optimize the 4D primitives enables us to synthesize novel views at any desired time with our tailored rendering routine. Our model is conceptually simple, consisting of a 4D Gaussian parameterized by anisotropic ellipses that can rotate arbitrarily in space and time, as well as view-dependent and time-evolved appearance represented by the coefficient of 4D spherindrical harmonics. This approach offers simplicity, flexibility for variable-length video and end-to-end training, and efficient real-time rendering, making it suitable for capturing complex dynamic scene motions. Experiments across various benchmarks, including monocular and multi-view scenarios, demonstrate our 4DGS model's superior visual quality and efficiency.",True,True,"Yang, Zeyu and Yang, Hongye and Pan, Zijie and Zhang, Li",2023,,,,arXiv preprint arXiv:2310.10642
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,yang2023deformable3dgs,\cite{yang2023deformable3dgs},Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene Reconstruction,,,True,False,"Yang, Ziyi and Gao, Xinyu and Zhou, Wen and Jiao, Shaohui and Zhang, Yuqing and Jin, Xiaogang",2023,,,,arXiv preprint arXiv:2309.13101
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,Wu_2024_CVPR,\cite{Wu_2024_CVPR},4D Gaussian Splatting for Real-Time Dynamic Scene Rendering,,,True,False,"Wu, Guanjun and Yi, Taoran and Fang, Jiemin and Xie, Lingxi and Zhang, Xiaopeng and Wei, Wei and Liu, Wenyu and Tian, Qi and Wang, Xinggang",2024,June,,,
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,luiten2023dynamic,\cite{luiten2023dynamic},Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis,https://arxiv.org/abs/2308.09713v1,"We present a method that simultaneously addresses the tasks of dynamic scene novel-view synthesis and six degree-of-freedom (6-DOF) tracking of all dense scene elements. We follow an analysis-by-synthesis framework, inspired by recent work that models scenes as a collection of 3D Gaussians which are optimized to reconstruct input images via differentiable rendering. To model dynamic scenes, we allow Gaussians to move and rotate over time while enforcing that they have persistent color, opacity, and size. By regularizing Gaussians' motion and rotation with local-rigidity constraints, we show that our Dynamic 3D Gaussians correctly model the same area of physical space over time, including the rotation of that space. Dense 6-DOF tracking and dynamic reconstruction emerges naturally from persistent dynamic view synthesis, without requiring any correspondence or flow as input. We demonstrate a large number of downstream applications enabled by our representation, including first-person view synthesis, dynamic compositional scene synthesis, and 4D video editing.",True,True,"Luiten, Jonathon and Kopanas, Georgios and Leibe, Bastian and Ramanan, Deva",2023,,,,arXiv preprint arXiv:2308.09713
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,bae2024per,\cite{bae2024per},Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian Splatting,https://arxiv.org/abs/2404.03613v5,"As 3D Gaussian Splatting (3DGS) provides fast and high-quality novel view synthesis, it is a natural extension to deform a canonical 3DGS to multiple frames for representing a dynamic scene. However, previous works fail to accurately reconstruct complex dynamic scenes. We attribute the failure to the design of the deformation field, which is built as a coordinate-based function. This approach is problematic because 3DGS is a mixture of multiple fields centered at the Gaussians, not just a single coordinate-based framework. To resolve this problem, we define the deformation as a function of per-Gaussian embeddings and temporal embeddings. Moreover, we decompose deformations as coarse and fine deformations to model slow and fast movements, respectively. Also, we introduce a local smoothness regularization for per-Gaussian embedding to improve the details in dynamic regions. Project page: https://jeongminb.github.io/e-d3dgs/",True,True,"Bae, Jeongmin and Kim, Seoha and Yun, Youngsik and Lee, Hahyun and Bang, Gun and Uh, Youngjung",2024,,,,arXiv preprint arXiv:2404.03613
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,li2024spacetime,\cite{li2024spacetime},Spacetime gaussian feature splatting for real-time dynamic view synthesis,,,True,False,"Li, Zhan and Chen, Zhang and Li, Zhong and Xu, Yi",2024,,,,
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,SMPL:2015,\cite{SMPL:2015},{SMPL}: A Skinned Multi-Person Linear Model,,,True,False,"Loper, Matthew and Mahmood, Naureen and Romero, Javier and Pons-Moll, Gerard and Black, Michael J.",2015,,,,ACM Trans. Graphics (Proc. SIGGRAPH Asia)
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,li2017learning,\cite{li2017learning},Learning a model of facial shape and expression from 4D scans.,,,True,False,"Li, Tianye and Bolkart, Timo and Black, Michael J and Li, Hao and Romero, Javier",2017,,,,ACM Trans. Graph.
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,blanz2023morphable,\cite{blanz2023morphable},A morphable model for the synthesis of 3D faces,,,True,False,"Blanz, Volker and Vetter, Thomas",2023,,,,
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,romero2022embodied,\cite{romero2022embodied},Embodied hands: Modeling and capturing hands and bodies together,,,True,False,"Romero, Javier and Tzionas, Dimitrios and Black, Michael J",2022,,,,arXiv preprint arXiv:2201.02610
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,alldieck2018detailed,\cite{alldieck2018detailed},Detailed Human Avatars from Monocular Video,https://arxiv.org/abs/1808.01338v1,"We present a novel method for high detail-preserving human avatar creation from monocular video. A parameterized body model is refined and optimized to maximally resemble subjects from a video showing them from all sides. Our avatars feature a natural face, hairstyle, clothes with garment wrinkles, and high-resolution texture. Our paper contributes facial landmark and shading-based human body shape refinement, a semantic texture prior, and a novel texture stitching strategy, resulting in the most sophisticated-looking human avatars obtained from a single video to date. Numerous results show the robustness and versatility of our method. A user study illustrates its superiority over the state-of-the-art in terms of identity preservation, level of detail, realism, and overall user preference.",True,True,"Alldieck, Thiemo and Magnor, Marcus and Xu, Weipeng and Theobalt, Christian and Pons-Moll, Gerard",2018,,,,
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,mildenhall2021nerf,\cite{mildenhall2021nerf},NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis,https://arxiv.org/abs/2003.08934v2,"We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(θ, φ)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",True,True,"Mildenhall, Ben and Srinivasan, Pratul P and Tancik, Matthew and Barron, Jonathan T and Ramamoorthi, Ravi and Ng, Ren",2021,,,,Communications of the ACM
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,kerbl3Dgaussians,\cite{kerbl3Dgaussians},3D Gaussian Splatting for Real-Time Radiance Field Rendering,,,True,False,"Kerbl, Bernhard and Kopanas, Georgios and Leimk{\""u}hler, Thomas and Drettakis, George",2023,July,https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/,,ACM Transactions on Graphics
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,jiang2023instantavatar,\cite{jiang2023instantavatar},InstantAvatar: Learning Avatars from Monocular Video in 60 Seconds,https://arxiv.org/abs/2212.10550v1,"In this paper, we take a significant step towards real-world applicability of monocular neural avatar reconstruction by contributing InstantAvatar, a system that can reconstruct human avatars from a monocular video within seconds, and these avatars can be animated and rendered at an interactive rate. To achieve this efficiency we propose a carefully designed and engineered system, that leverages emerging acceleration structures for neural fields, in combination with an efficient empty space-skipping strategy for dynamic scenes. We also contribute an efficient implementation that we will make available for research purposes. Compared to existing methods, InstantAvatar converges 130x faster and can be trained in minutes instead of hours. It achieves comparable or even better reconstruction quality and novel pose synthesis results. When given the same time budget, our method significantly outperforms SoTA methods. InstantAvatar can yield acceptable visual quality in as little as 10 seconds training time.",True,True,"Jiang, Tianjian and Chen, Xu and Song, Jie and Hilliges, Otmar",2023,,,,
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,zhang2025humanref,\cite{zhang2025humanref},Humanref-gs: Image-to-3d human generation with reference-guided diffusion and 3d gaussian splatting,,,True,False,"Zhang, Jingbo and Li, Xiaoyu and Zhong, Hongliang and Zhang, Qi and Cao, Yanpei and Shan, Ying and Liao, Jing",2025,,,,IEEE Transactions on Circuits and Systems for Video Technology
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,weng2022humannerf,\cite{weng2022humannerf},HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video,https://arxiv.org/abs/2201.04127v2,"We introduce a free-viewpoint rendering method -- HumanNeRF -- that works on a given monocular video of a human performing complex body motions, e.g. a video from YouTube. Our method enables pausing the video at any frame and rendering the subject from arbitrary new camera viewpoints or even a full 360-degree camera path for that particular frame and body pose. This task is particularly challenging, as it requires synthesizing photorealistic details of the body, as seen from various camera angles that may not exist in the input video, as well as synthesizing fine details such as cloth folds and facial appearance. Our method optimizes for a volumetric representation of the person in a canonical T-pose, in concert with a motion field that maps the estimated canonical representation to every frame of the video via backward warps. The motion field is decomposed into skeletal rigid and non-rigid motions, produced by deep networks. We show significant performance improvements over prior work, and compelling examples of free-viewpoint renderings from monocular video of moving humans in challenging uncontrolled capture scenarios.",True,True,"Weng, Chung-Yi and Curless, Brian and Srinivasan, Pratul P and Barron, Jonathan T and Kemelmacher-Shlizerman, Ira",2022,,,,
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,chen2023fast,\cite{chen2023fast},Fast-SNARF: A fast deformer for articulated neural fields,,,True,False,"Chen, Xu and Jiang, Tianjian and Song, Jie and Rietmann, Max and Geiger, Andreas and Black, Michael J and Hilliges, Otmar",2023,,,,IEEE Transactions on Pattern Analysis and Machine Intelligence
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,chen2021snarf,\cite{chen2021snarf},Snarf: Differentiable forward skinning for animating non-rigid neural implicit shapes,,,True,False,"Chen, Xu and Zheng, Yufeng and Black, Michael J and Hilliges, Otmar and Geiger, Andreas",2021,,,,
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,gafni2021dynamic,\cite{gafni2021dynamic},Dynamic neural radiance fields for monocular 4d facial avatar reconstruction,,,True,False,"Gafni, Guy and Thies, Justus and Zollhofer, Michael and Nie{\ss}ner, Matthias",2021,,,,
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,yu2023monohuman,\cite{yu2023monohuman},MonoHuman: Animatable Human Neural Field from Monocular Video,https://arxiv.org/abs/2304.02001v1,"Animating virtual avatars with free-view control is crucial for various applications like virtual reality and digital entertainment. Previous studies have attempted to utilize the representation power of the neural radiance field (NeRF) to reconstruct the human body from monocular videos. Recent works propose to graft a deformation network into the NeRF to further model the dynamics of the human neural field for animating vivid human motions. However, such pipelines either rely on pose-dependent representations or fall short of motion coherency due to frame-independent optimization, making it difficult to generalize to unseen pose sequences realistically. In this paper, we propose a novel framework MonoHuman, which robustly renders view-consistent and high-fidelity avatars under arbitrary novel poses. Our key insight is to model the deformation field with bi-directional constraints and explicitly leverage the off-the-peg keyframe information to reason the feature correlations for coherent results. Specifically, we first propose a Shared Bidirectional Deformation module, which creates a pose-independent generalizable deformation field by disentangling backward and forward deformation correspondences into shared skeletal motion weight and separate non-rigid motions. Then, we devise a Forward Correspondence Search module, which queries the correspondence feature of keyframes to guide the rendering network. The rendered results are thus multi-view consistent with high fidelity, even under challenging novel pose settings. Extensive experiments demonstrate the superiority of our proposed MonoHuman over state-of-the-art methods.",True,True,"Yu, Zhengming and Cheng, Wei and Liu, Xian and Wu, Wayne and Lin, Kwan-Yee",2023,,,,
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,peng2021animatable,\cite{peng2021animatable},Animatable neural radiance fields for modeling dynamic human bodies,,,True,False,"Peng, Sida and Dong, Junting and Wang, Qianqian and Zhang, Shangzhan and Shuai, Qing and Zhou, Xiaowei and Bao, Hujun",2021,,,,
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,hu2024gauhuman,\cite{hu2024gauhuman},GauHuman: Articulated Gaussian Splatting from Monocular Human Videos,https://arxiv.org/abs/2312.02973v1,"We present, GauHuman, a 3D human model with Gaussian Splatting for both fast training (1 ~ 2 minutes) and real-time rendering (up to 189 FPS), compared with existing NeRF-based implicit representation modelling frameworks demanding hours of training and seconds of rendering per frame. Specifically, GauHuman encodes Gaussian Splatting in the canonical space and transforms 3D Gaussians from canonical space to posed space with linear blend skinning (LBS), in which effective pose and LBS refinement modules are designed to learn fine details of 3D humans under negligible computational cost. Moreover, to enable fast optimization of GauHuman, we initialize and prune 3D Gaussians with 3D human prior, while splitting/cloning via KL divergence guidance, along with a novel merge operation for further speeding up. Extensive experiments on ZJU_Mocap and MonoCap datasets demonstrate that GauHuman achieves state-of-the-art performance quantitatively and qualitatively with fast training and real-time rendering speed. Notably, without sacrificing rendering quality, GauHuman can fast model the 3D human performer with ~13k 3D Gaussians.",True,True,"Hu, Shoukang and Hu, Tao and Liu, Ziwei",2024,,,,
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,hu2024gaussianavatar,\cite{hu2024gaussianavatar},GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians,https://arxiv.org/abs/2312.02134v3,"We present GaussianAvatar, an efficient approach to creating realistic human avatars with dynamic 3D appearances from a single video. We start by introducing animatable 3D Gaussians to explicitly represent humans in various poses and clothing styles. Such an explicit and animatable representation can fuse 3D appearances more efficiently and consistently from 2D observations. Our representation is further augmented with dynamic properties to support pose-dependent appearance modeling, where a dynamic appearance network along with an optimizable feature tensor is designed to learn the motion-to-appearance mapping. Moreover, by leveraging the differentiable motion condition, our method enables a joint optimization of motions and appearances during avatar modeling, which helps to tackle the long-standing issue of inaccurate motion estimation in monocular settings. The efficacy of GaussianAvatar is validated on both the public dataset and our collected dataset, demonstrating its superior performances in terms of appearance quality and rendering efficiency.",True,True,"Hu, Liangxiao and Zhang, Hongwen and Zhang, Yuxiang and Zhou, Boyao and Liu, Boning and Zhang, Shengping and Nie, Liqiang",2024,,,,
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,moon2024expressive,\cite{moon2024expressive},Expressive Whole-Body 3D Gaussian Avatar,https://arxiv.org/abs/2407.21686v1,"Facial expression and hand motions are necessary to express our emotions and interact with the world. Nevertheless, most of the 3D human avatars modeled from a casually captured video only support body motions without facial expressions and hand motions.In this work, we present ExAvatar, an expressive whole-body 3D human avatar learned from a short monocular video. We design ExAvatar as a combination of the whole-body parametric mesh model (SMPL-X) and 3D Gaussian Splatting (3DGS). The main challenges are 1) a limited diversity of facial expressions and poses in the video and 2) the absence of 3D observations, such as 3D scans and RGBD images. The limited diversity in the video makes animations with novel facial expressions and poses non-trivial. In addition, the absence of 3D observations could cause significant ambiguity in human parts that are not observed in the video, which can result in noticeable artifacts under novel motions. To address them, we introduce our hybrid representation of the mesh and 3D Gaussians. Our hybrid representation treats each 3D Gaussian as a vertex on the surface with pre-defined connectivity information (i.e., triangle faces) between them following the mesh topology of SMPL-X. It makes our ExAvatar animatable with novel facial expressions by driven by the facial expression space of SMPL-X. In addition, by using connectivity-based regularizers, we significantly reduce artifacts in novel facial expressions and poses.",True,True,"Moon, Gyeongsik and Shiratori, Takaaki and Saito, Shunsuke",2024,,,,arXiv preprint arXiv:2407.21686
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,paudel2024ihuman,\cite{paudel2024ihuman},iHuman: Instant Animatable Digital Humans From Monocular Videos,https://arxiv.org/abs/2407.11174v1,"Personalized 3D avatars require an animatable representation of digital humans. Doing so instantly from monocular videos offers scalability to broad class of users and wide-scale applications. In this paper, we present a fast, simple, yet effective method for creating animatable 3D digital humans from monocular videos. Our method utilizes the efficiency of Gaussian splatting to model both 3D geometry and appearance. However, we observed that naively optimizing Gaussian splats results in inaccurate geometry, thereby leading to poor animations. This work achieves and illustrates the need of accurate 3D mesh-type modelling of the human body for animatable digitization through Gaussian splats. This is achieved by developing a novel pipeline that benefits from three key aspects: (a) implicit modelling of surface's displacements and the color's spherical harmonics; (b) binding of 3D Gaussians to the respective triangular faces of the body template; (c) a novel technique to render normals followed by their auxiliary supervision. Our exhaustive experiments on three different benchmark datasets demonstrates the state-of-the-art results of our method, in limited time settings. In fact, our method is faster by an order of magnitude (in terms of training time) than its closest competitor. At the same time, we achieve superior rendering and 3D reconstruction performance under the change of poses.",True,True,"Paudel, Pramish and Khanal, Anubhav and Chhatkuli, Ajad and Paudel, Danda Pani and Tandukar, Jyoti",2024,,,,arXiv preprint arXiv:2407.11174
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,shao2024splattingavatar,\cite{shao2024splattingavatar},SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded Gaussian Splatting,https://arxiv.org/abs/2403.05087v1,"We present SplattingAvatar, a hybrid 3D representation of photorealistic human avatars with Gaussian Splatting embedded on a triangle mesh, which renders over 300 FPS on a modern GPU and 30 FPS on a mobile device. We disentangle the motion and appearance of a virtual human with explicit mesh geometry and implicit appearance modeling with Gaussian Splatting. The Gaussians are defined by barycentric coordinates and displacement on a triangle mesh as Phong surfaces. We extend lifted optimization to simultaneously optimize the parameters of the Gaussians while walking on the triangle mesh. SplattingAvatar is a hybrid representation of virtual humans where the mesh represents low-frequency motion and surface deformation, while the Gaussians take over the high-frequency geometry and detailed appearance. Unlike existing deformation methods that rely on an MLP-based linear blend skinning (LBS) field for motion, we control the rotation and translation of the Gaussians directly by mesh, which empowers its compatibility with various animation techniques, e.g., skeletal animation, blend shapes, and mesh editing. Trainable from monocular videos for both full-body and head avatars, SplattingAvatar shows state-of-the-art rendering quality across multiple datasets.",True,True,"Shao, Zhijing and Wang, Zhaolong and Li, Zhuang and Wang, Duotun and Lin, Xiangru and Zhang, Yu and Fan, Mingming and Wang, Zeyu",2024,,,,
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,wen2024gomavatar,\cite{wen2024gomavatar},GoMAvatar: Efficient Animatable Human Modeling from Monocular Video Using Gaussians-on-Mesh,https://arxiv.org/abs/2404.07991v1,"We introduce GoMAvatar, a novel approach for real-time, memory-efficient, high-quality animatable human modeling. GoMAvatar takes as input a single monocular video to create a digital avatar capable of re-articulation in new poses and real-time rendering from novel viewpoints, while seamlessly integrating with rasterization-based graphics pipelines. Central to our method is the Gaussians-on-Mesh representation, a hybrid 3D model combining rendering quality and speed of Gaussian splatting with geometry modeling and compatibility of deformable meshes. We assess GoMAvatar on ZJU-MoCap data and various YouTube videos. GoMAvatar matches or surpasses current monocular human modeling algorithms in rendering quality and significantly outperforms them in computational efficiency (43 FPS) while being memory-efficient (3.63 MB per subject).",True,True,"Wen, Jing and Zhao, Xiaoming and Ren, Zhongzheng and Schwing, Alexander G and Wang, Shenlong",2024,,,,
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,qian20243dgs,\cite{qian20243dgs},3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting,https://arxiv.org/abs/2312.09228v3,"We introduce an approach that creates animatable human avatars from monocular videos using 3D Gaussian Splatting (3DGS). Existing methods based on neural radiance fields (NeRFs) achieve high-quality novel-view/novel-pose image synthesis but often require days of training, and are extremely slow at inference time. Recently, the community has explored fast grid structures for efficient training of clothed avatars. Albeit being extremely fast at training, these methods can barely achieve an interactive rendering frame rate with around 15 FPS. In this paper, we use 3D Gaussian Splatting and learn a non-rigid deformation network to reconstruct animatable clothed human avatars that can be trained within 30 minutes and rendered at real-time frame rates (50+ FPS). Given the explicit nature of our representation, we further introduce as-isometric-as-possible regularizations on both the Gaussian mean vectors and the covariance matrices, enhancing the generalization of our model on highly articulated unseen poses. Experimental results show that our method achieves comparable and even better performance compared to state-of-the-art approaches on animatable avatar creation from a monocular input, while being 400x and 250x faster in training and inference, respectively.",True,True,"Qian, Zhiyin and Wang, Shaofei and Mihajlovic, Marko and Geiger, Andreas and Tang, Siyu",2024,,,,
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,jiang2022neuman,\cite{jiang2022neuman},NeuMan: Neural Human Radiance Field from a Single Video,https://arxiv.org/abs/2203.12575v2,"Photorealistic rendering and reposing of humans is important for enabling augmented reality experiences. We propose a novel framework to reconstruct the human and the scene that can be rendered with novel human poses and views from just a single in-the-wild video. Given a video captured by a moving camera, we train two NeRF models: a human NeRF model and a scene NeRF model. To train these models, we rely on existing methods to estimate the rough geometry of the human and the scene. Those rough geometry estimates allow us to create a warping field from the observation space to the canonical pose-independent space, where we train the human model in. Our method is able to learn subject specific details, including cloth wrinkles and accessories, from just a 10 seconds video clip, and to provide high quality renderings of the human under novel poses, from novel views, together with the background.",True,True,"Jiang, Wei and Yi, Kwang Moo and Samei, Golnoosh and Tuzel, Oncel and Ranjan, Anurag",2022,,,,
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,guo2023vid2avatar,\cite{guo2023vid2avatar},Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition,https://arxiv.org/abs/2302.11566v1,"We present Vid2Avatar, a method to learn human avatars from monocular in-the-wild videos. Reconstructing humans that move naturally from monocular in-the-wild videos is difficult. Solving it requires accurately separating humans from arbitrary backgrounds. Moreover, it requires reconstructing detailed 3D surface from short video sequences, making it even more challenging. Despite these challenges, our method does not require any groundtruth supervision or priors extracted from large datasets of clothed human scans, nor do we rely on any external segmentation modules. Instead, it solves the tasks of scene decomposition and surface reconstruction directly in 3D by modeling both the human and the background in the scene jointly, parameterized via two separate neural fields. Specifically, we define a temporally consistent human representation in canonical space and formulate a global optimization over the background model, the canonical human shape and texture, and per-frame human pose parameters. A coarse-to-fine sampling strategy for volume rendering and novel objectives are introduced for a clean separation of dynamic human and static background, yielding detailed and robust 3D human geometry reconstructions. We evaluate our methods on publicly available datasets and show improvements over prior art.",True,True,"Guo, Chen and Jiang, Tianjian and Chen, Xu and Song, Jie and Hilliges, Otmar",2023,,,,
Dynamic Avatar-Scene Rendering from Human-centric Context,2511.10539v1,kocabas2024hugs,\cite{kocabas2024hugs},HUGS: Human Gaussian Splats,https://arxiv.org/abs/2311.17910v1,"Recent advances in neural rendering have improved both training and rendering times by orders of magnitude. While these methods demonstrate state-of-the-art quality and speed, they are designed for photogrammetry of static scenes and do not generalize well to freely moving humans in the environment. In this work, we introduce Human Gaussian Splats (HUGS) that represents an animatable human together with the scene using 3D Gaussian Splatting (3DGS). Our method takes only a monocular video with a small number of (50-100) frames, and it automatically learns to disentangle the static scene and a fully animatable human avatar within 30 minutes. We utilize the SMPL body model to initialize the human Gaussians. To capture details that are not modeled by SMPL (e.g. cloth, hairs), we allow the 3D Gaussians to deviate from the human body model. Utilizing 3D Gaussians for animated humans brings new challenges, including the artifacts created when articulating the Gaussians. We propose to jointly optimize the linear blend skinning weights to coordinate the movements of individual Gaussians during animation. Our approach enables novel-pose synthesis of human and novel view synthesis of both the human and the scene. We achieve state-of-the-art rendering quality with a rendering speed of 60 FPS while being ~100x faster to train over previous work. Our code will be announced here: https://github.com/apple/ml-hugs",True,True,"Kocabas, Muhammed and Chang, Jen-Hao Rick and Gabriel, James and Tuzel, Oncel and Ranjan, Anurag",2024,,,,
PEGNet: A Physics-Embedded Graph Network for Long-Term Stable Multiphysics Simulation,2511.08697v1,battaglia2016interaction,\cite{battaglia2016interaction},"Interaction networks for learning about objects, relations and physics",,,True,False,"Battaglia, Peter and Pascanu, Razvan and Lai, Matthew and Jimenez Rezende, Danilo and others",2016,,,,Advances in neural information processing systems
PEGNet: A Physics-Embedded Graph Network for Long-Term Stable Multiphysics Simulation,2511.08697v1,sanchez2020learning,\cite{sanchez2020learning},Learning to simulate complex physics with graph networks,,,True,False,"Sanchez-Gonzalez, Alvaro and Godwin, Jonathan and Pfaff, Tobias and Ying, Rex and Leskovec, Jure and Battaglia, Peter",2020,,,,
PEGNet: A Physics-Embedded Graph Network for Long-Term Stable Multiphysics Simulation,2511.08697v1,pfaff2020learning,\cite{pfaff2020learning},Learning mesh-based simulation with graph networks,,,True,False,"Pfaff, Tobias and Fortunato, Meire and Sanchez-Gonzalez, Alvaro and Battaglia, Peter",2020,,,,
PEGNet: A Physics-Embedded Graph Network for Long-Term Stable Multiphysics Simulation,2511.08697v1,yang2025mbds,\cite{yang2025mbds},MBDS: A MultiBody Dynamics Simulation Dataset for Graph Networks Simulators,,,True,False,"Yang, Sheng and Peng, Tao and Li, Zeyu and Wu, Fengge and Zhao, Junsuo",2025,,,,
PEGNet: A Physics-Embedded Graph Network for Long-Term Stable Multiphysics Simulation,2511.08697v1,gao2024predicting,\cite{gao2024predicting},Predicting fluid--structure interaction with graph neural networks,,,True,False,"Gao, Rui and Jaiman, Rajeev K",2024,,,,Physics of Fluids
PEGNet: A Physics-Embedded Graph Network for Long-Term Stable Multiphysics Simulation,2511.08697v1,deshpande2024magnet,\cite{deshpande2024magnet},MAgNET: A graph U-Net architecture for mesh-based simulations,,,True,False,"Deshpande, Saurabh and Bordas, St{\'e}phane PA and Lengiewicz, Jakub",2024,,,,Engineering Applications of Artificial Intelligence
PEGNet: A Physics-Embedded Graph Network for Long-Term Stable Multiphysics Simulation,2511.08697v1,ying2018hierarchical,\cite{ying2018hierarchical},Hierarchical graph representation learning with differentiable pooling,,,True,False,"Ying, Zhitao and You, Jiaxuan and Morris, Christopher and Ren, Xiang and Hamilton, Will and Leskovec, Jure",2018,,,,Advances in neural information processing systems
PEGNet: A Physics-Embedded Graph Network for Long-Term Stable Multiphysics Simulation,2511.08697v1,cangea2018sparsehierarchicalgraphclassifiers,\cite{cangea2018sparsehierarchicalgraphclassifiers},Towards Sparse Hierarchical Graph Classifiers,https://arxiv.org/abs/1811.01287v1,"Recent advances in representation learning on graphs, mainly leveraging graph convolutional networks, have brought a substantial improvement on many graph-based benchmark tasks. While novel approaches to learning node embeddings are highly suitable for node classification and link prediction, their application to graph classification (predicting a single label for the entire graph) remains mostly rudimentary, typically using a single global pooling step to aggregate node features or a hand-designed, fixed heuristic for hierarchical coarsening of the graph structure. An important step towards ameliorating this is differentiable graph coarsening---the ability to reduce the size of the graph in an adaptive, data-dependent manner within a graph neural network pipeline, analogous to image downsampling within CNNs. However, the previous prominent approach to pooling has quadratic memory requirements during training and is therefore not scalable to large graphs. Here we combine several recent advances in graph neural network design to demonstrate that competitive hierarchical graph classification results are possible without sacrificing sparsity. Our results are verified on several established graph classification benchmarks, and highlight an important direction for future research in graph-based neural networks.",True,True,Cătălina Cangea and Petar Veličković and Nikola Jovanović and Thomas Kipf and Pietro Liò,2018,,https://arxiv.org/abs/1811.01287,,
PEGNet: A Physics-Embedded Graph Network for Long-Term Stable Multiphysics Simulation,2511.08697v1,shen2025transferlearningscalablegraph,\cite{shen2025transferlearningscalablegraph},Transfer learning in Scalable Graph Neural Network for Improved Physical Simulation,https://arxiv.org/abs/2502.06848v1,"In recent years, Graph Neural Network (GNN) based models have shown promising results in simulating physics of complex systems. However, training dedicated graph network based physics simulators can be costly, as most models are confined to fully supervised training, which requires extensive data generated from traditional physics simulators. To date, how transfer learning could improve the model performance and training efficiency has remained unexplored. In this work, we introduce a pre-training and transfer learning paradigm for graph network simulators. We propose the scalable graph U-net (SGUNET). Incorporating an innovative depth-first search (DFS) pooling, the SGUNET is adaptable to different mesh sizes and resolutions for various simulation tasks. To enable the transfer learning between differently configured SGUNETs, we propose a set of mapping functions to align the parameters between the pre-trained model and the target model. An extra normalization term is also added into the loss to constrain the difference between the pre-trained weights and target model weights for better generalization performance. To pre-train our physics simulator we created a dataset which includes 20,000 physical simulations of randomly selected 3D shapes from the open source A Big CAD (ABC) dataset. We show that our proposed transfer learning methods allow the model to perform even better when fine-tuned with small amounts of training data than when it is trained from scratch with full extensive dataset. On the 2D Deformable Plate benchmark dataset, our pre-trained model fine-tuned on 1/16 of the training data achieved an 11.05\% improvement in position RMSE compared to the model trained from scratch.",True,True,Siqi Shen and Yu Liu and Daniel Biggs and Omar Hafez and Jiandong Yu and Wentao Zhang and Bin Cui and Jiulong Shan,2025,,https://arxiv.org/abs/2502.06848,,
PEGNet: A Physics-Embedded Graph Network for Long-Term Stable Multiphysics Simulation,2511.08697v1,cao2023efficient,\cite{cao2023efficient},Efficient learning of mesh-based physical simulation with bi-stride multi-scale graph neural network,,,True,False,"Cao, Yadi and Chai, Menglei and Li, Minchen and Jiang, Chenfanfu",2023,,,,
PEGNet: A Physics-Embedded Graph Network for Long-Term Stable Multiphysics Simulation,2511.08697v1,raissi2019physics,\cite{raissi2019physics},Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations,,,True,False,"Raissi, Maziar and Perdikaris, Paris and Karniadakis, George E",2019,,,,Journal of Computational physics
PEGNet: A Physics-Embedded Graph Network for Long-Term Stable Multiphysics Simulation,2511.08697v1,Lu_2021,\cite{Lu_2021},Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators,,,True,False,"Lu, Lu and Jin, Pengzhan and Pang, Guofei and Zhang, Zhongqiang and Karniadakis, George Em",2021,,http://dx.doi.org/10.1038/s42256-021-00302-5,10.1038/s42256-021-00302-5,Nature Machine Intelligence
PEGNet: A Physics-Embedded Graph Network for Long-Term Stable Multiphysics Simulation,2511.08697v1,lifourier,\cite{lifourier},Fourier Neural Operator for Parametric Partial Differential Equations,,,True,False,Zongyi Li and Nikola Kovachki and Kamyar Azizzadenesheli and Burigede Liu and Kaushik Bhattacharya and Andrew Stuart and Anima Anandkumar,2021,,https://arxiv.org/abs/2010.08895,,
PEGNet: A Physics-Embedded Graph Network for Long-Term Stable Multiphysics Simulation,2511.08697v1,goswami2023physics,\cite{goswami2023physics},Physics-Informed Deep Neural Operator Networks,https://arxiv.org/abs/2207.05748v2,"Standard neural networks can approximate general nonlinear operators, represented either explicitly by a combination of mathematical operators, e.g., in an advection-diffusion-reaction partial differential equation, or simply as a black box, e.g., a system-of-systems. The first neural operator was the Deep Operator Network (DeepONet), proposed in 2019 based on rigorous approximation theory. Since then, a few other less general operators have been published, e.g., based on graph neural networks or Fourier transforms. For black box systems, training of neural operators is data-driven only but if the governing equations are known they can be incorporated into the loss function during training to develop physics-informed neural operators. Neural operators can be used as surrogates in design problems, uncertainty quantification, autonomous systems, and almost in any application requiring real-time inference. Moreover, independently pre-trained DeepONets can be used as components of a complex multi-physics system by coupling them together with relatively light training. Here, we present a review of DeepONet, the Fourier neural operator, and the graph neural operator, as well as appropriate extensions with feature expansions, and highlight their usefulness in diverse applications in computational mechanics, including porous media, fluid mechanics, and solid mechanics.",True,True,"Goswami, Somdatta and Bora, Aniruddha and Yu, Yue and Karniadakis, George Em",2023,,,,
PEGNet: A Physics-Embedded Graph Network for Long-Term Stable Multiphysics Simulation,2511.08697v1,rao2023encoding,\cite{rao2023encoding},Encoding physics to learn reaction-diffusion processes,https://arxiv.org/abs/2106.04781v2,"Modeling complex spatiotemporal dynamical systems, such as the reaction-diffusion processes, have largely relied on partial differential equations (PDEs). However, due to insufficient prior knowledge on some under-explored dynamical systems, such as those in chemistry, biology, geology, physics and ecology, and the lack of explicit PDE formulation used for describing the nonlinear process of the system variables, to predict the evolution of such a system remains a challenging task. Unifying measurement data and our limited prior physics knowledge via machine learning provides us with a new path to solving this problem. Existing physics-informed learning paradigms impose physics laws through soft penalty constraints, whose solution quality largely depends on a trial-and-error proper setting of hyperparameters. Since the core of such methods is still rooted in black-box neural networks, the resulting model generally lacks interpretability and suffers from critical issues of extrapolation and generalization. To this end, we propose a deep learning framework that forcibly encodes given physics structure to facilitate the learning of the spatiotemporal dynamics in sparse data regimes. We show how the proposed approach can be applied to a variety of problems regarding the PDE system, including forward and inverse analysis, data-driven modeling, and discovery of PDEs. The resultant learning paradigm that encodes physics shows high accuracy, robustness, interpretability and generalizability demonstrated via extensive numerical experiments.",True,True,"Rao, Chengping and Ren, Pu and Wang, Qi and Buyukozturk, Oral and Sun, Hao and Liu, Yang",2023,,,,Nature Machine Intelligence
PEGNet: A Physics-Embedded Graph Network for Long-Term Stable Multiphysics Simulation,2511.08697v1,guo2021physics,\cite{guo2021physics},Physics embedded deep neural network for solving volume integral equation: 2-D case,,,True,False,"Guo, Rui and Shan, Tao and Song, Xiaoqian and Li, Maokun and Yang, Fan and Xu, Shenheng and Abubakar, Aria",2021,,,,IEEE Transactions on Antennas and Propagation
PEGNet: A Physics-Embedded Graph Network for Long-Term Stable Multiphysics Simulation,2511.08697v1,wang2025multipdenetpdeembeddedlearningmultitimestepping,\cite{wang2025multipdenetpdeembeddedlearningmultitimestepping},MultiPDENet: PDE-embedded Learning with Multi-time-stepping for Accelerated Flow Simulation,,,True,False,Qi Wang and Yuan Mi and Haoyun Wang and Yi Zhang and Ruizhi Chengze and Hongsheng Liu and Ji-Rong Wen and Hao Sun,2025,,https://arxiv.org/abs/2501.15987,,
PEGNet: A Physics-Embedded Graph Network for Long-Term Stable Multiphysics Simulation,2511.08697v1,horie2022physics,\cite{horie2022physics},Physics-embedded neural networks: Graph neural pde solvers with mixed boundary conditions,,,True,False,"Horie, Masanobu and Mitsume, Naoto",2022,,,,Advances in Neural Information Processing Systems
PEGNet: A Physics-Embedded Graph Network for Long-Term Stable Multiphysics Simulation,2511.08697v1,zeng2025phympgnphysicsencodedmessagepassing,\cite{zeng2025phympgnphysicsencodedmessagepassing},PhyMPGN: Physics-encoded Message Passing Graph Network for spatiotemporal PDE systems,https://arxiv.org/abs/2410.01337v3,"Solving partial differential equations (PDEs) serves as a cornerstone for modeling complex dynamical systems. Recent progresses have demonstrated grand benefits of data-driven neural-based models for predicting spatiotemporal dynamics (e.g., tremendous speedup gain compared with classical numerical methods). However, most existing neural models rely on rich training data, have limited extrapolation and generalization abilities, and suffer to produce precise or reliable physical prediction under intricate conditions (e.g., irregular mesh or geometry, complex boundary conditions, diverse PDE parameters, etc.). To this end, we propose a new graph learning approach, namely, Physics-encoded Message Passing Graph Network (PhyMPGN), to model spatiotemporal PDE systems on irregular meshes given small training datasets. Specifically, we incorporate a GNN into a numerical integrator to approximate the temporal marching of spatiotemporal dynamics for a given PDE system. Considering that many physical phenomena are governed by diffusion processes, we further design a learnable Laplace block, which encodes the discrete Laplace-Beltrami operator, to aid and guide the GNN learning in a physically feasible solution space. A boundary condition padding strategy is also designed to improve the model convergence and accuracy. Extensive experiments demonstrate that PhyMPGN is capable of accurately predicting various types of spatiotemporal dynamics on coarse unstructured meshes, consistently achieves the state-of-the-art results, and outperforms other baselines with considerable gains.",True,True,Bocheng Zeng and Qi Wang and Mengtao Yan and Yang Liu and Ruizhi Chengze and Yi Zhang and Hongsheng Liu and Zidong Wang and Hao Sun,2025,,https://arxiv.org/abs/2410.01337,,
Towards Provably Unlearnable Examples via Bayes Error Optimisation,2511.08191v1,huang2021unlearnable,\cite{huang2021unlearnable},Unlearnable Examples: Making Personal Data Unexploitable,https://arxiv.org/abs/2101.04898v2,"The volume of ""free"" data on the internet has been key to the current success of deep learning. However, it also raises privacy concerns about the unauthorized exploitation of personal data for training commercial models. It is thus crucial to develop methods to prevent unauthorized data exploitation. This paper raises the question: \emph{can data be made unlearnable for deep learning models?} We present a type of \emph{error-minimizing} noise that can indeed make training examples unlearnable. Error-minimizing noise is intentionally generated to reduce the error of one or more of the training example(s) close to zero, which can trick the model into believing there is ""nothing"" to learn from these example(s). The noise is restricted to be imperceptible to human eyes, and thus does not affect normal data utility. We empirically verify the effectiveness of error-minimizing noise in both sample-wise and class-wise forms. We also demonstrate its flexibility under extensive experimental settings and practicability in a case study of face recognition. Our work establishes an important first step towards making personal data unexploitable to deep learning models.",True,True,Hanxun Huang and Xingjun Ma and Sarah Monazam Erfani and James Bailey and Yisen Wang,,,https://openreview.net/forum?id=iAmZUo0DxC0,,
Towards Provably Unlearnable Examples via Bayes Error Optimisation,2511.08191v1,ren2022transferable,\cite{ren2022transferable},Transferable Unlearnable Examples,https://arxiv.org/abs/2210.10114v1,"With more people publishing their personal data online, unauthorized data usage has become a serious concern. The unlearnable strategies have been introduced to prevent third parties from training on the data without permission. They add perturbations to the users' data before publishing, which aims to make the models trained on the perturbed published dataset invalidated. These perturbations have been generated for a specific training setting and a target dataset. However, their unlearnable effects significantly decrease when used in other training settings and datasets. To tackle this issue, we propose a novel unlearnable strategy based on Classwise Separability Discriminant (CSD), which aims to better transfer the unlearnable effects to other training settings and datasets by enhancing the linear separability. Extensive experiments demonstrate the transferability of the proposed unlearnable examples across training settings and datasets.",True,True,"Ren, Jie and Xu, Han and Wan, Yuxuan and Ma, Xingjun and Sun, Lichao and Tang, Jiliang",,,,,arXiv preprint arXiv:2210.10114
Towards Provably Unlearnable Examples via Bayes Error Optimisation,2511.08191v1,wen2023adversarial,\cite{wen2023adversarial},Is Adversarial Training Really a Silver Bullet for Mitigating Data Poisoning?,,,True,False,Rui Wen and Zhengyu Zhao and Zhuoran Liu and Michael Backes and Tianhao Wang and Yang Zhang,,,https://openreview.net/forum?id=zKvm1ETDOq,,
Towards Provably Unlearnable Examples via Bayes Error Optimisation,2511.08191v1,fowl2021adversarial,\cite{fowl2021adversarial},Adversarial Examples Make Strong Poisons,https://arxiv.org/abs/2106.10807v1,"The adversarial machine learning literature is largely partitioned into evasion attacks on testing data and poisoning attacks on training data. In this work, we show that adversarial examples, originally intended for attacking pre-trained models, are even more effective for data poisoning than recent methods designed specifically for poisoning. Our findings indicate that adversarial examples, when assigned the original label of their natural base image, cannot be used to train a classifier for natural images. Furthermore, when adversarial examples are assigned their adversarial class label, they are useful for training. This suggests that adversarial examples contain useful semantic content, just with the ``wrong'' labels (according to a network, but not a human). Our method, adversarial poisoning, is substantially more effective than existing poisoning methods for secure dataset release, and we release a poisoned version of ImageNet, ImageNet-P, to encourage research into the strength of this form of data obfuscation.",True,True,"Fowl, Liam and Goldblum, Micah and Chiang, Ping-yeh and Geiping, Jonas and Czaja, Wojciech and Goldstein, Tom",,,,,Advances in Neural Information Processing Systems
Towards Provably Unlearnable Examples via Bayes Error Optimisation,2511.08191v1,fu2022robust,\cite{fu2022robust},Robust Unlearnable Examples: Protecting Data Against Adversarial Learning,https://arxiv.org/abs/2203.14533v1,"The tremendous amount of accessible data in cyberspace face the risk of being unauthorized used for training deep learning models. To address this concern, methods are proposed to make data unlearnable for deep learning models by adding a type of error-minimizing noise. However, such conferred unlearnability is found fragile to adversarial training. In this paper, we design new methods to generate robust unlearnable examples that are protected from adversarial training. We first find that the vanilla error-minimizing noise, which suppresses the informative knowledge of data via minimizing the corresponding training loss, could not effectively minimize the adversarial training loss. This explains the vulnerability of error-minimizing noise in adversarial training. Based on the observation, robust error-minimizing noise is then introduced to reduce the adversarial training loss. Experiments show that the unlearnability brought by robust error-minimizing noise can effectively protect data from adversarial training in various scenarios. The code is available at \url{https://github.com/fshp971/robust-unlearnable-examples}.",True,True,"Fu, Shaopeng and He, Fengxiang and Liu, Yang and Shen, Li and Tao, Dacheng",,,,,arXiv preprint arXiv:2203.14533
Towards Provably Unlearnable Examples via Bayes Error Optimisation,2511.08191v1,liu2024game,\cite{liu2024game},Game-Theoretic Unlearnable Example Generator,https://arxiv.org/abs/2401.17523v1,"Unlearnable example attacks are data poisoning attacks aiming to degrade the clean test accuracy of deep learning by adding imperceptible perturbations to the training samples, which can be formulated as a bi-level optimization problem. However, directly solving this optimization problem is intractable for deep neural networks. In this paper, we investigate unlearnable example attacks from a game-theoretic perspective, by formulating the attack as a nonzero sum Stackelberg game. First, the existence of game equilibria is proved under the normal setting and the adversarial training setting. It is shown that the game equilibrium gives the most powerful poison attack in that the victim has the lowest test accuracy among all networks within the same hypothesis space, when certain loss functions are used. Second, we propose a novel attack method, called the Game Unlearnable Example (GUE), which has three main gradients. (1) The poisons are obtained by directly solving the equilibrium of the Stackelberg game with a first-order algorithm. (2) We employ an autoencoder-like generative network model as the poison attacker. (3) A novel payoff function is introduced to evaluate the performance of the poison. Comprehensive experiments demonstrate that GUE can effectively poison the model in various scenarios. Furthermore, the GUE still works by using a relatively small percentage of the training data to train the generator, and the poison generator can generalize to unseen data well. Our implementation code can be found at https://github.com/hong-xian/gue.",True,True,"Liu, Shuang and Wang, Yihan and Gao, Xiao-Shan",,,,,
Towards Provably Unlearnable Examples via Bayes Error Optimisation,2511.08191v1,liu2024stable,\cite{liu2024stable},Stable unlearnable example: Enhancing the robustness of unlearnable examples via stable error-minimizing noise,,,True,False,"Liu, Yixin and Xu, Kaidi and Chen, Xun and Sun, Lichao",,,,,
Towards Provably Unlearnable Examples via Bayes Error Optimisation,2511.08191v1,liu2023reliable,\cite{liu2023reliable},Reliable Robustness Evaluation via Automatically Constructed Attack Ensembles,https://arxiv.org/abs/2211.12713v1,"Attack Ensemble (AE), which combines multiple attacks together, provides a reliable way to evaluate adversarial robustness. In practice, AEs are often constructed and tuned by human experts, which however tends to be sub-optimal and time-consuming. In this work, we present AutoAE, a conceptually simple approach for automatically constructing AEs. In brief, AutoAE repeatedly adds the attack and its iteration steps to the ensemble that maximizes ensemble improvement per additional iteration consumed. We show theoretically that AutoAE yields AEs provably within a constant factor of the optimal for a given defense. We then use AutoAE to construct two AEs for $l_{\infty}$ and $l_2$ attacks, and apply them without any tuning or adaptation to 45 top adversarial defenses on the RobustBench leaderboard. In all except one cases we achieve equal or better (often the latter) robustness evaluation than existing AEs, and notably, in 29 cases we achieve better robustness evaluation than the best known one. Such performance of AutoAE shows itself as a reliable evaluation protocol for adversarial robustness, which further indicates the huge potential of automatic AE construction. Code is available at \url{https://github.com/LeegerPENG/AutoAE}.",True,True,"Liu, Shengcai and Peng, Fu and Tang, Ke",,,,,
Towards Provably Unlearnable Examples via Bayes Error Optimisation,2511.08191v1,mu2025bayesian,\cite{mu2025bayesian},Bayesian Privacy Guarantee for User History in Sequential Recommendation Using Randomised Response,,,True,False,"Mu, Wenchuan and Lim, Kwan Hui",,,https://doi.org/10.1145/3746252.3760856,10.1145/3746252.3760856,
Towards Provably Unlearnable Examples via Bayes Error Optimisation,2511.08191v1,lu2023exploring,\cite{lu2023exploring},Exploring the limits of model-targeted indiscriminate data poisoning attacks,,,True,False,"Lu, Yiwei and Kamath, Gautam and Yu, Yaoliang",,,,,
Towards Provably Unlearnable Examples via Bayes Error Optimisation,2511.08191v1,he2023sharpness,\cite{he2023sharpness},Sharpness-Aware Data Poisoning Attack,https://arxiv.org/abs/2305.14851v3,"Recent research has highlighted the vulnerability of Deep Neural Networks (DNNs) against data poisoning attacks. These attacks aim to inject poisoning samples into the models' training dataset such that the trained models have inference failures. While previous studies have executed different types of attacks, one major challenge that greatly limits their effectiveness is the uncertainty of the re-training process after the injection of poisoning samples, including the re-training initialization or algorithms. To address this challenge, we propose a novel attack method called ''Sharpness-Aware Data Poisoning Attack (SAPA)''. In particular, it leverages the concept of DNNs' loss landscape sharpness to optimize the poisoning effect on the worst re-trained model. It helps enhance the preservation of the poisoning effect, regardless of the specific retraining procedure employed. Extensive experiments demonstrate that SAPA offers a general and principled strategy that significantly enhances various types of poisoning attacks.",True,True,"He, Pengfei and Xu, Han and Ren, Jie and Cui, Yingqian and Liu, Hui and Aggarwal, Charu C and Tang, Jiliang",,,,,arXiv preprint arXiv:2305.14851
Towards Provably Unlearnable Examples via Bayes Error Optimisation,2511.08191v1,fukunaga1975k,\cite{fukunaga1975k},k-nearest-neighbor Bayes-risk estimation,,,True,False,"Fukunaga, K. and Hostetler, L.",,,,10.1109/TIT.1975.1055373,IEEE Transactions on Information Theory
Towards Provably Unlearnable Examples via Bayes Error Optimisation,2511.08191v1,fukunaga1990introduction,\cite{fukunaga1990introduction},Introduction to Statistical Pattern Recognition (2nd Ed.),,,True,False,"Fukunaga, Keinosuke",,,,,
Towards Provably Unlearnable Examples via Bayes Error Optimisation,2511.08191v1,berisha2015empirically,\cite{berisha2015empirically},Empirically Estimable Classification Bounds Based on a New Divergence Measure,https://arxiv.org/abs/1412.6534v2,Information divergence functions play a critical role in statistics and information theory. In this paper we show that a non-parametric f-divergence measure can be used to provide improved bounds on the minimum binary classification probability of error for the case when the training and test data are drawn from the same distribution and for the case where there exists some mismatch between training and test distributions. We confirm the theoretical results by designing feature selection algorithms using the criteria from these bounds and by evaluating the algorithms on a series of pathological speech classification tasks.,True,True,"Berisha, Visar and Wisler, Alan and Hero, Alfred O. and Spanias, Andreas",,,,10.1109/TSP.2015.2477805,IEEE Transactions on Signal Processing
Towards Provably Unlearnable Examples via Bayes Error Optimisation,2511.08191v1,sekeh2020learning,\cite{sekeh2020learning},Learning to Bound the Multi-Class Bayes Error,,,True,False,"Sekeh, Salimeh Yasaei and Oselio, Brandon and Hero, Alfred O.",,,,10.1109/TSP.2020.2994807,IEEE Transactions on Signal Processing
Towards Provably Unlearnable Examples via Bayes Error Optimisation,2511.08191v1,kingma2018glow,\cite{kingma2018glow},Glow: Generative Flow with Invertible 1x1 Convolutions,,,True,False,"Kingma, Durk P and Dhariwal, Prafulla",,,https://proceedings.neurips.cc/paper_files/paper/2018/file/d139db6a236200b21cc7f752979132d0-Paper.pdf,,
Towards Provably Unlearnable Examples via Bayes Error Optimisation,2511.08191v1,theisen2021evaluating,\cite{theisen2021evaluating},Evaluating State-of-the-Art Classification Models Against Bayes Optimality,,,True,False,"Theisen, Ryan and Wang, Huan and Varshney, Lav R and Xiong, Caiming and Socher, Richard",,,https://proceedings.neurips.cc/paper_files/paper/2021/file/4e0ccd2b894f717df5ebc12f4282ee70-Paper.pdf,,
Towards Provably Unlearnable Examples via Bayes Error Optimisation,2511.08191v1,renggli2021evaluating,\cite{renggli2021evaluating},Evaluating Bayes Error Estimators on Real-World Datasets with FeeBee,https://arxiv.org/abs/2108.13034v2,"The Bayes error rate (BER) is a fundamental concept in machine learning that quantifies the best possible accuracy any classifier can achieve on a fixed probability distribution. Despite years of research on building estimators of lower and upper bounds for the BER, these were usually compared only on synthetic datasets with known probability distributions, leaving two key questions unanswered: (1) How well do they perform on real-world datasets?, and (2) How practical are they? Answering these is not trivial. Apart from the obvious challenge of an unknown BER for real-world datasets, there are two main aspects any BER estimator needs to overcome in order to be applicable in real-world settings: (1) the computational and sample complexity, and (2) the sensitivity and selection of hyper-parameters. In this work, we propose FeeBee, the first principled framework for analyzing and comparing BER estimators on any modern real-world dataset with unknown probability distribution. We achieve this by injecting a controlled amount of label noise and performing multiple evaluations on a series of different noise levels, supported by a theoretical result which allows drawing conclusions about the evolution of the BER. By implementing and analyzing 7 multi-class BER estimators on 6 commonly used datasets of the computer vision and NLP domains, FeeBee allows a thorough study of these estimators, clearly identifying strengths and weaknesses of each, whilst being easily deployable on any future BER estimator.",True,True,C{\'{e}}dric Renggli and Luka Rimanic and Nora Hollenstein and Ce Zhang,,,https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/045117b0e0a11a242b9765e79cbf113f-Abstract-round2.html,,
Towards Provably Unlearnable Examples via Bayes Error Optimisation,2511.08191v1,zhang2024certified,\cite{zhang2024certified},Certified Robust Accuracy of Neural Networks Are Bounded due to Bayes Errors,https://arxiv.org/abs/2405.11547v2,"Adversarial examples pose a security threat to many critical systems built on neural networks. While certified training improves robustness, it also decreases accuracy noticeably. Despite various proposals for addressing this issue, the significant accuracy drop remains. More importantly, it is not clear whether there is a certain fundamental limit on achieving robustness whilst maintaining accuracy. In this work, we offer a novel perspective based on Bayes errors. By adopting Bayes error to robustness analysis, we investigate the limit of certified robust accuracy, taking into account data distribution uncertainties. We first show that the accuracy inevitably decreases in the pursuit of robustness due to changed Bayes error in the altered data distribution. Subsequently, we establish an upper bound for certified robust accuracy, considering the distribution of individual classes and their boundaries. Our theoretical results are empirically evaluated on real-world datasets and are shown to be consistent with the limited success of existing certified training results, e.g., for CIFAR10, our analysis results in an upper bound (of certified robust accuracy) of 67.49\%, meanwhile existing approaches are only able to increase it from 53.89\% in 2017 to 62.84\% in 2023.",True,True,"Zhang, Ruihan and Sun, Jun",,,,,
Towards Provably Unlearnable Examples via Bayes Error Optimisation,2511.08191v1,zhang2024does,\cite{zhang2024does},How Does Bayes Error Limit Probabilistic Robust Accuracy,https://arxiv.org/abs/2405.14923v1,"Adversarial examples pose a security threat to many critical systems built on neural networks. Given that deterministic robustness often comes with significantly reduced accuracy, probabilistic robustness (i.e., the probability of having the same label with a vicinity is $\ge 1-κ$) has been proposed as a promising way of achieving robustness whilst maintaining accuracy. However, existing training methods for probabilistic robustness still experience non-trivial accuracy loss. It is unclear whether there is an upper bound on the accuracy when optimising towards probabilistic robustness, and whether there is a certain relationship between $κ$ and this bound. This work studies these problems from a Bayes error perspective. We find that while Bayes uncertainty does affect probabilistic robustness, its impact is smaller than that on deterministic robustness. This reduced Bayes uncertainty allows a higher upper bound on probabilistic robust accuracy than that on deterministic robust accuracy. Further, we prove that with optimal probabilistic robustness, each probabilistically robust input is also deterministically robust in a smaller vicinity. We also show that voting within the vicinity always improves probabilistic robust accuracy and the upper bound of probabilistic robust accuracy monotonically increases as $κ$ grows. Our empirical findings also align with our results.",True,True,"Zhang, Ruihan and Sun, Jun",,,,,arXiv preprint arXiv:2405.14923
FastGraph: Optimized GPU-Enabled Algorithms for Fast Graph Building and Message Passing,2511.10442v1,panigrahyHighDim,\cite{panigrahyHighDim},An Improved Algorithm Finding Nearest Neighbor Using Kd-Trees in High Dimensions,,,True,False,R. Panigrahy,2008,,,,
FastGraph: Optimized GPU-Enabled Algorithms for Fast Graph Building and Message Passing,2511.10442v1,yaledmKdTreeFail,\cite{yaledmKdTreeFail},Nearest Neighbor Search and the Curse of Dimensionality,,,True,False,Yale University Data Mining Course,2013,,,,
FastGraph: Optimized GPU-Enabled Algorithms for Fast Graph Building and Message Passing,2511.10442v1,johnson2019billion,\cite{johnson2019billion},Billion-scale similarity search with {GPUs},,,True,False,"Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}",2019,,,,IEEE Transactions on Big Data
FastGraph: Optimized GPU-Enabled Algorithms for Fast Graph Building and Message Passing,2511.10442v1,lazar2022accelerating,\cite{lazar2022accelerating},Accelerating the Inference of the Exa.TrkX Pipeline,,,True,False,"Alina Lazar and Xiangyang Ju and Daniel Murnane and Paolo Calafiura
                  and Steven Farrell and Yaoyuan Xu and Maria Spiropulu and Jean-Roch Vlimant
                  and Giuseppe Cerati and Lindsey Gray and Thomas Klijnsma and Jim Kowalkowski
                  and Markus Atkinson and Mark Neubauer and Gage DeZoort and Savannah Thais
                  and Shih-Chieh Hsu and Adam Aurisano and V. Hewes and Alexandra Ballow",2022,,,,arXiv preprint arXiv:2202.06929
FastGraph: Optimized GPU-Enabled Algorithms for Fast Graph Building and Message Passing,2511.10442v1,zhao2020song,\cite{zhao2020song},{SONG}: Approximate Nearest Neighbor Search on {GPU},,,True,False,Weijie Zhao and Shulong Tan and Ping Li,2020,,,,
FastGraph: Optimized GPU-Enabled Algorithms for Fast Graph Building and Message Passing,2511.10442v1,ggnn,\cite{ggnn},GGNN: Graph-based GPU Nearest Neighbor Search,https://arxiv.org/abs/1912.01059v4,"Approximate nearest neighbor (ANN) search in high dimensions is an integral part of several computer vision systems and gains importance in deep learning with explicit memory representations. Since PQT, FAISS, and SONG started to leverage the massive parallelism offered by GPUs, GPU-based implementations are a crucial resource for today's state-of-the-art ANN methods. While most of these methods allow for faster queries, less emphasis is devoted to accelerating the construction of the underlying index structures. In this paper, we propose a novel GPU-friendly search structure based on nearest neighbor graphs and information propagation on graphs. Our method is designed to take advantage of GPU architectures to accelerate the hierarchical construction of the index structure and for performing the query. Empirical evaluation shows that GGNN significantly surpasses the state-of-the-art CPU- and GPU-based systems in terms of build-time, accuracy and search speed.",True,True,"Groh, Fabian and Ruppert, Lukas and Wieschollek, Patrick and Lensch, Hendrik P. A.",2023,,,10.1109/TBDATA.2022.3161156,IEEE Transactions on Big Data
Enabling Agents to Communicate Entirely in Latent Space,2511.09149v1,zhu2025survey,\cite{zhu2025survey},A Survey on Latent Reasoning,https://arxiv.org/abs/2507.06203v2,"Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, especially when guided by explicit chain-of-thought (CoT) reasoning that verbalizes intermediate steps. While CoT improves both interpretability and accuracy, its dependence on natural language reasoning limits the model's expressive bandwidth. Latent reasoning tackles this bottleneck by performing multi-step inference entirely in the model's continuous hidden state, eliminating token-level supervision. To advance latent reasoning research, this survey provides a comprehensive overview of the emerging field of latent reasoning. We begin by examining the foundational role of neural network layers as the computational substrate for reasoning, highlighting how hierarchical representations support complex transformations. Next, we explore diverse latent reasoning methodologies, including activation-based recurrence, hidden state propagation, and fine-tuning strategies that compress or internalize explicit reasoning traces. Finally, we discuss advanced paradigms such as infinite-depth latent reasoning via masked diffusion models, which enable globally consistent and reversible reasoning processes. By unifying these perspectives, we aim to clarify the conceptual landscape of latent reasoning and chart future directions for research at the frontier of LLM cognition. An associated GitHub repository collecting the latest papers and repos is available at: https://github.com/multimodal-art-projection/LatentCoT-Horizon/.",True,True,"Zhu, Rui-Jie and Peng, Tianhao and Cheng, Tianhao and Qu, Xingwei and Huang, Jinfa and Zhu, Dawei and Wang, Hao and Xue, Kaiwen and Zhang, Xuanliang and Shan, Yong and others",2025,,,,arXiv preprint arXiv:2507.06203
Enabling Agents to Communicate Entirely in Latent Space,2511.09149v1,goyal2023think,\cite{goyal2023think},Think before you speak: Training Language Models With Pause Tokens,https://arxiv.org/abs/2310.02226v3,"Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\%$ EM score on the QA task of SQuAD, $8\%$ on CommonSenseQA and $1\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",True,True,"Goyal, Sachin and Ji, Ziwei and Rawat, Ankit Singh and Menon, Aditya Krishna and Kumar, Sanjiv and Nagarajan, Vaishnavh",2023,,,,arXiv preprint arXiv:2310.02226
Enabling Agents to Communicate Entirely in Latent Space,2511.09149v1,pfau2024let,\cite{pfau2024let},Let's Think Dot by Dot: Hidden Computation in Transformer Language Models,https://arxiv.org/abs/2404.15758v1,"Chain-of-thought responses from language models improve performance across most benchmarks. However, it remains unclear to what extent these performance gains can be attributed to human-like task decomposition or simply the greater computation that additional tokens allow. We show that transformers can use meaningless filler tokens (e.g., '......') in place of a chain of thought to solve two hard algorithmic tasks they could not solve when responding without intermediate tokens. However, we find empirically that learning to use filler tokens is difficult and requires specific, dense supervision to converge. We also provide a theoretical characterization of the class of problems where filler tokens are useful in terms of the quantifier depth of a first-order formula. For problems satisfying this characterization, chain-of-thought tokens need not provide information about the intermediate computational steps involved in multi-token computations. In summary, our results show that additional tokens can provide computational benefits independent of token choice. The fact that intermediate tokens can act as filler tokens raises concerns about large language models engaging in unauditable, hidden computations that are increasingly detached from the observed chain-of-thought tokens.",True,True,"Pfau, Jacob and Merrill, William and Bowman, Samuel R",2024,,,,arXiv preprint arXiv:2404.15758
Enabling Agents to Communicate Entirely in Latent Space,2511.09149v1,liu2024deliberation,\cite{liu2024deliberation},Deliberation in Latent Space via Differentiable Cache Augmentation,https://arxiv.org/abs/2412.17747v1,"Techniques enabling large language models (LLMs) to ""think more"" by generating and attending to intermediate reasoning steps have shown promise in solving complex problems. However, the standard approaches generate sequences of discrete tokens immediately before responding, and so they can incur significant latency costs and be challenging to optimize. In this work, we demonstrate that a frozen LLM can be augmented with an offline coprocessor that operates on the model's key-value (kv) cache. This coprocessor augments the cache with a set of latent embeddings designed to improve the fidelity of subsequent decoding. We train this coprocessor using the language modeling loss from the decoder on standard pretraining data, while keeping the decoder itself frozen. This approach enables the model to learn, in an end-to-end differentiable fashion, how to distill additional computation into its kv-cache. Because the decoder remains unchanged, the coprocessor can operate offline and asynchronously, and the language model can function normally if the coprocessor is unavailable or if a given cache is deemed not to require extra computation. We show experimentally that when a cache is augmented, the decoder achieves lower perplexity on numerous subsequent tokens. Furthermore, even without any task-specific training, our experiments demonstrate that cache augmentation consistently reduces perplexity and improves performance across a range of reasoning-intensive tasks.",True,True,"Liu, Luyang and Pfeiffer, Jonas and Wu, Jiaxing and Xie, Jun and Szlam, Arthur",2024,,,,arXiv preprint arXiv:2412.17747
Enabling Agents to Communicate Entirely in Latent Space,2511.09149v1,hao2024training,\cite{hao2024training},Training large language models to reason in a continuous latent space,,,True,False,"Hao, Shibo and Sukhbaatar, Sainbayar and Su, DiJia and Li, Xian and Hu, Zhiting and Weston, Jason and Tian, Yuandong",2024,,,,arXiv preprint arXiv:2412.06769
Enabling Agents to Communicate Entirely in Latent Space,2511.09149v1,shen2025codi,\cite{shen2025codi},Codi: Compressing chain-of-thought into continuous space via self-distillation,,,True,False,"Shen, Zhenyi and Yan, Hanqi and Zhang, Linhai and Hu, Zhanghao and Du, Yali and He, Yulan",2025,,,,arXiv preprint arXiv:2502.21074
Enabling Agents to Communicate Entirely in Latent Space,2511.09149v1,cheng2024compressed,\cite{cheng2024compressed},Compressed Chain of Thought: Efficient Reasoning Through Dense Representations,https://arxiv.org/abs/2412.13171v1,"Chain-of-thought (CoT) decoding enables language models to improve reasoning performance at the cost of high generation latency in decoding. Recent proposals have explored variants of contemplation tokens, a term we introduce that refers to special tokens used during inference to allow for extra computation. Prior work has considered fixed-length sequences drawn from a discrete set of embeddings as contemplation tokens. Here we propose Compressed Chain-of-Thought (CCoT), a framework to generate contentful and continuous contemplation tokens of variable sequence length. The generated contemplation tokens are compressed representations of explicit reasoning chains, and our method can be applied to off-the-shelf decoder language models. Through experiments, we illustrate how CCoT enables additional reasoning over dense contentful representations to achieve corresponding improvements in accuracy. Moreover, the reasoning improvements can be adaptively modified on demand by controlling the number of contemplation tokens generated.",True,True,"Cheng, Jeffrey and Van Durme, Benjamin",2024,,,,arXiv preprint arXiv:2412.13171
Enabling Agents to Communicate Entirely in Latent Space,2511.09149v1,bae2024relaxed,\cite{bae2024relaxed},Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA,https://arxiv.org/abs/2410.20672v3,"Large language models (LLMs) are expensive to deploy. Parameter sharing offers a possible path towards reducing their size and cost, but its effectiveness in modern LLMs remains fairly limited. In this work, we revisit ""layer tying"" as form of parameter sharing in Transformers, and introduce novel methods for converting existing LLMs into smaller ""Recursive Transformers"" that share parameters across layers, with minimal loss of performance. Here, our Recursive Transformers are efficiently initialized from standard pretrained Transformers, but only use a single block of unique layers that is then repeated multiple times in a loop. We further improve performance by introducing Relaxed Recursive Transformers that add flexibility to the layer tying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still preserve the compactness of the overall model. We show that our recursive models (e.g., recursive Gemma 1B) outperform both similar-sized vanilla pretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge distillation baselines -- and can even recover most of the performance of the original ""full-size"" model (e.g., Gemma 2B with no shared parameters). Finally, we propose Continuous Depth-wise Batching, a promising new inference paradigm enabled by the Recursive Transformer when paired with early exiting. In a theoretical analysis, we show that this has the potential to lead to significant (2-3x) gains in inference throughput.",True,True,"Bae, Sangmin and Fisch, Adam and Harutyunyan, Hrayr and Ji, Ziwei and Kim, Seungyeon and Schuster, Tal",2024,,,,arXiv preprint arXiv:2410.20672
Enabling Agents to Communicate Entirely in Latent Space,2511.09149v1,gao2024algoformer,\cite{gao2024algoformer},Algoformer: An efficient transformer framework with algorithmic structures,,,True,False,"Gao, Yihang and Zheng, Chuanyang and Xie, Enze and Shi, Han and Hu, Tianyang and Li, Yu and Ng, Michael K and Li, Zhenguo and Liu, Zhaoqiang",2024,,,,arXiv preprint arXiv:2402.13572
Enabling Agents to Communicate Entirely in Latent Space,2511.09149v1,geiping2025scaling,\cite{geiping2025scaling},Scaling up test-time compute with latent reasoning: A recurrent depth approach,,,True,False,"Geiping, Jonas and McLeish, Sean and Jain, Neel and Kirchenbauer, John and Singh, Siddharth and Bartoldson, Brian R and Kailkhura, Bhavya and Bhatele, Abhinav and Goldstein, Tom",2025,,,,arXiv preprint arXiv:2502.05171
Enabling Agents to Communicate Entirely in Latent Space,2511.09149v1,qian2024scaling,\cite{qian2024scaling},Scaling large language model-based multi-agent collaboration,,,True,False,"Qian, Chen and Xie, Zihao and Wang, Yifei and Liu, Wei and Zhu, Kunlun and Xia, Hanchen and Dang, Yufan and Du, Zhuoyun and Chen, Weize and Yang, Cheng and others",2024,,,,arXiv preprint arXiv:2406.07155
Enabling Agents to Communicate Entirely in Latent Space,2511.09149v1,zhu2025multiagentbench,\cite{zhu2025multiagentbench},Multiagentbench: Evaluating the collaboration and competition of llm agents,,,True,False,"Zhu, Kunlun and Du, Hongyi and Hong, Zhaochen and Yang, Xiaocheng and Guo, Shuyi and Wang, Zhe and Wang, Zhenhailong and Qian, Cheng and Tang, Xiangru and Ji, Heng and others",2025,,,,arXiv preprint arXiv:2503.01935
Enabling Agents to Communicate Entirely in Latent Space,2511.09149v1,wang2024survey,\cite{wang2024survey},A Survey on Large Language Model based Autonomous Agents,https://arxiv.org/abs/2308.11432v7,"Autonomous agents have long been a prominent research focus in both academic and industry communities. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of LLM-based autonomous agents from a holistic perspective. More specifically, we first discuss the construction of LLM-based autonomous agents, for which we propose a unified framework that encompasses a majority of the previous work. Then, we present a comprehensive overview of the diverse applications of LLM-based autonomous agents in the fields of social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository of relevant references at https://github.com/Paitesanshi/LLM-Agent-Survey.",True,True,"Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and others",2024,,,,Frontiers of Computer Science
Enabling Agents to Communicate Entirely in Latent Space,2511.09149v1,pham2023let,\cite{pham2023let},Let Models Speak Ciphers: Multiagent Debate through Embeddings,https://arxiv.org/abs/2310.06272v2,"Discussion and debate among Large Language Models (LLMs) have gained considerable attention due to their potential to enhance the reasoning ability of LLMs. Although natural language is an obvious choice for communication due to LLM's language understanding capability, the token sampling step needed when generating natural language poses a potential risk of information loss, as it uses only one token to represent the model's belief across the entire vocabulary. In this paper, we introduce a communication regime named CIPHER (Communicative Inter-Model Protocol Through Embedding Representation) to address this issue. Specifically, we remove the token sampling step from LLMs and let them communicate their beliefs across the vocabulary through the expectation of the raw transformer output embeddings. Remarkably, by deviating from natural language, CIPHER offers an advantage of encoding a broader spectrum of information without any modification to the model weights, outperforming the state-of-the-art LLM debate methods using natural language by 0.5-5.0% across five reasoning tasks and multiple open-source LLMs of varying sizes. This showcases the superiority and robustness of embeddings as an alternative ""language"" for communication among LLMs. We anticipate that CIPHER will inspire further exploration for the design of interactions within LLM agent systems, offering a new direction that could significantly influence future developments in the field.",True,True,"Pham, Chau and Liu, Boyi and Yang, Yingxiang and Chen, Zhengyu and Liu, Tianyi and Yuan, Jianbo and Plummer, Bryan A and Wang, Zhaoran and Yang, Hongxia",2023,,,,arXiv preprint arXiv:2310.06272
Enabling Agents to Communicate Entirely in Latent Space,2511.09149v1,ramesh2025communicating,\cite{ramesh2025communicating},Communicating Activations Between Language Model Agents,https://arxiv.org/abs/2501.14082v2,"Communication between multiple language model (LM) agents has been shown to scale up the reasoning ability of LMs. While natural language has been the dominant medium for inter-LM communication, it is not obvious this should be the standard: not only does natural language communication incur high inference costs that scale quickly with the number of both agents and messages, but also the decoding process abstracts away too much rich information that could be otherwise accessed from the internal activations. In this work, we propose a simple technique whereby LMs communicate via activations; concretely, we pause an LM $\textit{B}$'s computation at an intermediate layer, combine its current activation with another LM $\textit{A}$'s intermediate activation via some function $\textit{f}$, then pass $\textit{f}$'s output into the next layer of $\textit{B}$ and continue the forward pass till decoding is complete. This approach scales up LMs on new tasks with zero additional parameters and data, and saves a substantial amount of compute over natural language communication. We test our method with various functional forms $\textit{f}$ on two experimental setups--multi-player coordination games and reasoning benchmarks--and find that it achieves up to $27.0\%$ improvement over natural language communication across datasets with $<$$1/4$ the compute, illustrating the superiority and robustness of activations as an alternative ""language"" for communication between LMs.",True,True,"Ramesh, Vignav and Li, Kenneth",2025,,,,arXiv preprint arXiv:2501.14082
Enabling Agents to Communicate Entirely in Latent Space,2511.09149v1,tang2025augmenting,\cite{tang2025augmenting},Augmenting Multi-Agent Communication with State Delta Trajectory,,,True,False,"Tang, Yichen and Su, Weihang and Zhou, Yujia and Liu, Yiqun and Zhang, Min and Ma, Shaoping and Ai, Qingyao",2025,,,,arXiv preprint arXiv:2506.19209
Moral Change or Noise? On Problems of Aligning AI With Temporally Unstable Human Feedback,2511.10032v1,ji2023ai,\cite{ji2023ai},AI Alignment: A Comprehensive Survey,https://arxiv.org/abs/2310.19852v6,"AI alignment aims to make AI systems behave in line with human intentions and values. As AI systems grow more capable, so do risks from misalignment. To provide a comprehensive and up-to-date overview of the alignment field, in this survey, we delve into the core concepts, methodology, and practice of alignment. First, we identify four principles as the key objectives of AI alignment: Robustness, Interpretability, Controllability, and Ethicality (RICE). Guided by these four principles, we outline the landscape of current alignment research and decompose them into two key components: forward alignment and backward alignment. The former aims to make AI systems aligned via alignment training, while the latter aims to gain evidence about the systems' alignment and govern them appropriately to avoid exacerbating misalignment risks. On forward alignment, we discuss techniques for learning from feedback and learning under distribution shift. On backward alignment, we discuss assurance techniques and governance practices.
  We also release and continually update the website (www.alignmentsurvey.com) which features tutorials, collections of papers, blog posts, and other resources.",True,True,"Ji, Jiaming and Qiu, Tianyi and Chen, Boyuan and Zhang, Borong and Lou, Hantao and Wang, Kaile and Duan, Yawen and He, Zhonghao and Zhou, Jiayi and Zhang, Zhaowei and others",,,,,arXiv preprint arXiv:2310.19852
Moral Change or Noise? On Problems of Aligning AI With Temporally Unstable Human Feedback,2511.10032v1,shen2024towards,\cite{shen2024towards},"Towards bidirectional human-ai alignment: A systematic review for clarifications, framework, and future directions",,,True,False,"Shen, Hua and Knearem, Tiffany and Ghosh, Reshmi and Alkiek, Kenan and Krishna, Kundan and Liu, Yachuan and Ma, Ziqiao and Petridis, Savvas and Peng, Yi-Hao and Qiwei, Li and others",2024,,,,arXiv preprint arXiv:2406.09264
Moral Change or Noise? On Problems of Aligning AI With Temporally Unstable Human Feedback,2511.10032v1,slovic1995construction,\cite{slovic1995construction},The construction of preference.,,,True,False,"Slovic, Paul",1995,,,,American psychologist
Moral Change or Noise? On Problems of Aligning AI With Temporally Unstable Human Feedback,2511.10032v1,zhi-xuan_beyond_2024,\cite{zhi-xuan_beyond_2024},Beyond Preferences in AI Alignment,https://arxiv.org/abs/2408.16984v2,"The dominant practice of AI alignment assumes (1) that preferences are an adequate representation of human values, (2) that human rationality can be understood in terms of maximizing the satisfaction of preferences, and (3) that AI systems should be aligned with the preferences of one or more humans to ensure that they behave safely and in accordance with our values. Whether implicitly followed or explicitly endorsed, these commitments constitute what we term a preferentist approach to AI alignment. In this paper, we characterize and challenge the preferentist approach, describing conceptual and technical alternatives that are ripe for further research. We first survey the limits of rational choice theory as a descriptive model, explaining how preferences fail to capture the thick semantic content of human values, and how utility representations neglect the possible incommensurability of those values. We then critique the normativity of expected utility theory (EUT) for humans and AI, drawing upon arguments showing how rational agents need not comply with EUT, while highlighting how EUT is silent on which preferences are normatively acceptable. Finally, we argue that these limitations motivate a reframing of the targets of AI alignment: Instead of alignment with the preferences of a human user, developer, or humanity-writ-large, AI systems should be aligned with normative standards appropriate to their social roles, such as the role of a general-purpose assistant. Furthermore, these standards should be negotiated and agreed upon by all relevant stakeholders. On this alternative conception of alignment, a multiplicity of AI systems will be able to serve diverse ends, aligned with normative standards that promote mutual benefit and limit harm despite our plural and divergent values.",True,True,"Zhi-Xuan, Tan and Carroll, Micah and Franklin, Matija and Ashton, Hal",2024,,https://link.springer.com/10.1007/s11098-024-02249-w,10.1007/s11098-024-02249-w,Philosophical Studies
Moral Change or Noise? On Problems of Aligning AI With Temporally Unstable Human Feedback,2511.10032v1,carroll2024ai,\cite{carroll2024ai},AI Alignment with Changing and Influenceable Reward Functions,,,True,False,"Carroll, Micah and Foote, Davis and Siththaranjan, Anand and Russell, Stuart and Dragan, Anca",2024,,,,
Moral Change or Noise? On Problems of Aligning AI With Temporally Unstable Human Feedback,2511.10032v1,curmei2022towards,\cite{curmei2022towards},Towards Psychologically-Grounded Dynamic Preference Models,https://arxiv.org/abs/2208.01534v2,"Designing recommendation systems that serve content aligned with time varying preferences requires proper accounting of the feedback effects of recommendations on human behavior and psychological condition. We argue that modeling the influence of recommendations on people's preferences must be grounded in psychologically plausible models. We contribute a methodology for developing grounded dynamic preference models. We demonstrate this method with models that capture three classic effects from the psychology literature: Mere-Exposure, Operant Conditioning, and Hedonic Adaptation. We conduct simulation-based studies to show that the psychological models manifest distinct behaviors that can inform system design. Our study has two direct implications for dynamic user modeling in recommendation systems. First, the methodology we outline is broadly applicable for psychologically grounding dynamic preference models. It allows us to critique recent contributions based on their limited discussion of psychological foundation and their implausible predictions. Second, we discuss implications of dynamic preference models for recommendation systems evaluation and design. In an example, we show that engagement and diversity metrics may be unable to capture desirable recommendation system performance.",True,True,"Curmei, Mihaela and Haupt, Andreas A and Recht, Benjamin and Hadfield-Menell, Dylan",2022,,,,
Moral Change or Noise? On Problems of Aligning AI With Temporally Unstable Human Feedback,2511.10032v1,kleinberg2024inversion,\cite{kleinberg2024inversion},The inversion problem: Why algorithms should infer mental state and not just predict behavior,,,True,False,"Kleinberg, Jon and Ludwig, Jens and Mullainathan, Sendhil and Raghavan, Manish",2024,,,,Perspectives on Psychological Science
Moral Change or Noise? On Problems of Aligning AI With Temporally Unstable Human Feedback,2511.10032v1,buyl2025ai,\cite{buyl2025ai},AI Alignment at Your Discretion,https://arxiv.org/abs/2502.10441v1,"In AI alignment, extensive latitude must be granted to annotators, either human or algorithmic, to judge which model outputs are `better' or `safer.' We refer to this latitude as alignment discretion. Such discretion remains largely unexamined, posing two risks: (i) annotators may use their power of discretion arbitrarily, and (ii) models may fail to mimic this discretion. To study this phenomenon, we draw on legal concepts of discretion that structure how decision-making authority is conferred and exercised, particularly in cases where principles conflict or their application is unclear or irrelevant. Extended to AI alignment, discretion is required when alignment principles and rules are (inevitably) conflicting or indecisive. We present a set of metrics to systematically analyze when and how discretion in AI alignment is exercised, such that both risks (i) and (ii) can be observed. Moreover, we distinguish between human and algorithmic discretion and analyze the discrepancy between them. By measuring both human and algorithmic discretion over safety alignment datasets, we reveal layers of discretion in the alignment process that were previously unaccounted for. Furthermore, we demonstrate how algorithms trained on these datasets develop their own forms of discretion in interpreting and applying these principles, which challenges the purpose of having any principles at all. Our paper presents the first step towards formalizing this core gap in current alignment processes, and we call on the community to further scrutinize and control alignment discretion.",True,True,"Buyl, Maarten and Khalaf, Hadi and Mayrink Verdun, Claudio and Monteiro Paes, Lucas and Vieira Machado, Caio Cesar and du Pin Calmon, Flavio",2025,,,,
Moral Change or Noise? On Problems of Aligning AI With Temporally Unstable Human Feedback,2511.10032v1,liu_survey_2025,\cite{liu_survey_2025},A Survey of Direct Preference Optimization,https://arxiv.org/abs/2503.11701v1,"Large Language Models (LLMs) have demonstrated unprecedented generative capabilities, yet their alignment with human values remains critical for ensuring helpful and harmless deployments. While Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful paradigm for aligning LLMs with human preferences, its reliance on complex reward modeling introduces inherent trade-offs in computational efficiency and training stability. In this context, Direct Preference Optimization (DPO) has recently gained prominence as a streamlined alternative that directly optimizes LLMs using human preferences, thereby circumventing the need for explicit reward modeling. Owing to its theoretical elegance and computational efficiency, DPO has rapidly attracted substantial research efforts exploring its various implementations and applications. However, this field currently lacks systematic organization and comparative analysis. In this survey, we conduct a comprehensive overview of DPO and introduce a novel taxonomy, categorizing previous works into four key dimensions: data strategy, learning framework, constraint mechanism, and model property. We further present a rigorous empirical analysis of DPO variants across standardized benchmarks. Additionally, we discuss real-world applications, open challenges, and future directions for DPO. This work delivers both a conceptual framework for understanding DPO and practical guidance for practitioners, aiming to advance robust and generalizable alignment paradigms. All collected resources are available and will be continuously updated at https://github.com/liushunyu/awesome-direct-preference-optimization.",True,True,"Liu, Shunyu and Fang, Wenkai and Hu, Zetian and Zhang, Junjie and Zhou, Yang and Zhang, Kongcheng and Tu, Rongcheng and Lin, Ting-En and Huang, Fei and Song, Mingli and Li, Yongbin and Tao, Dacheng",2025,,http://arxiv.org/abs/2503.11701,10.48550/arXiv.2503.11701,
Moral Change or Noise? On Problems of Aligning AI With Temporally Unstable Human Feedback,2511.10032v1,boerstler2024stability,\cite{boerstler2024stability},On the stability of moral preferences: A problem with computational elicitation methods,,,True,False,"Boerstler, Kyle and Keswani, Vijay and Chan, Lok and Borg, Jana Schaich and Conitzer, Vincent and Heidari, Hoda and Sinnott-Armstrong, Walter",,,,,
Moral Change or Noise? On Problems of Aligning AI With Temporally Unstable Human Feedback,2511.10032v1,lin_limited_2024,\cite{lin_limited_2024},On the {Limited} {Generalization} {Capability} of the {Implicit} {Reward} {Model} {Induced} by {Direct} {Preference} {Optimization},,,True,False,"Lin, Yong and Seto, Skyler and Hoeve, Maartje ter and Metcalf, Katherine and Theobald, Barry-John and Wang, Xuan and Zhang, Yizhe and Huang, Chen and Zhang, Tong",2024,,http://arxiv.org/abs/2409.03650,10.48550/arXiv.2409.03650,
Moral Change or Noise? On Problems of Aligning AI With Temporally Unstable Human Feedback,2511.10032v1,shirali_direct_2025,\cite{shirali_direct_2025},Direct {Alignment} with {Heterogeneous} {Preferences},,,True,False,"Shirali, Ali and Nasr-Esfahany, Arash and Alomar, Abdullah and Mirtaheri, Parsa and Abebe, Rediet and Procaccia, Ariel",2025,,http://arxiv.org/abs/2502.16320,10.48550/arXiv.2502.16320,
Moral Change or Noise? On Problems of Aligning AI With Temporally Unstable Human Feedback,2511.10032v1,noothigattu_voting-based_2018,\cite{noothigattu_voting-based_2018},A {Voting}-{Based} {System} for {Ethical} {Decision} {Making},,,True,False,"Noothigattu, Ritesh and Gaikwad, Snehalkumar 'Neil' S. and Awad, Edmond and Dsouza, Sohan and Rahwan, Iyad and Ravikumar, Pradeep and Procaccia, Ariel D.",,,http://arxiv.org/abs/1709.06692,10.48550/arXiv.1709.06692,
Moral Change or Noise? On Problems of Aligning AI With Temporally Unstable Human Feedback,2511.10032v1,wiedeman_modeling_2020,\cite{wiedeman_modeling_2020},Modeling of moral decisions with deep learning,,,True,False,"Wiedeman, Christopher and Wang, Ge and Kruger, Uwe",,,https://vciba.springeropen.com/articles/10.1186/s42492-020-00063-9,10.1186/s42492-020-00063-9,"Visual Computing for Industry, Biomedicine, and Art"
Moral Change or Noise? On Problems of Aligning AI With Temporally Unstable Human Feedback,2511.10032v1,rodionov_evaluation_2023,\cite{rodionov_evaluation_2023},An {Evaluation} of {GPT}-4 on the {ETHICS} {Dataset},,,True,False,"Rodionov, Sergey and Goertzel, Zarathustra Amadeus and Goertzel, Ben",,,http://arxiv.org/abs/2309.10492,10.48550/arXiv.2309.10492,
Moral Change or Noise? On Problems of Aligning AI With Temporally Unstable Human Feedback,2511.10032v1,hendrycks_aligning_2023,\cite{hendrycks_aligning_2023},Aligning {AI} {With} {Shared} {Human} {Values},,,True,False,"Hendrycks, Dan and Burns, Collin and Basart, Steven and Critch, Andrew and Li, Jerry and Song, Dawn and Steinhardt, Jacob",,,http://arxiv.org/abs/2008.02275,10.48550/arXiv.2008.02275,
Moral Change or Noise? On Problems of Aligning AI With Temporally Unstable Human Feedback,2511.10032v1,zaim_bin_ahmad_large-scale_2025,\cite{zaim_bin_ahmad_large-scale_2025},Large-scale moral machine experiment on large language models,https://arxiv.org/abs/2411.06790v2,"The rapid advancement of Large Language Models (LLMs) and their potential integration into autonomous driving systems necessitates understanding their moral decision-making capabilities. While our previous study examined four prominent LLMs using the Moral Machine experimental framework, the dynamic landscape of LLM development demands a more comprehensive analysis. Here, we evaluate moral judgments across 52 different LLMs, including multiple versions of proprietary models (GPT, Claude, Gemini) and open-source alternatives (Llama, Gemma), to assess their alignment with human moral preferences in autonomous driving scenarios. Using a conjoint analysis framework, we evaluated how closely LLM responses aligned with human preferences in ethical dilemmas and examined the effects of model size, updates, and architecture. Results showed that proprietary models and open-source models exceeding 10 billion parameters demonstrated relatively close alignment with human judgments, with a significant negative correlation between model size and distance from human judgments in open-source models. However, model updates did not consistently improve alignment with human preferences, and many LLMs showed excessive emphasis on specific ethical principles. These findings suggest that while increasing model size may naturally lead to more human-like moral judgments, practical implementation in autonomous driving systems requires careful consideration of the trade-off between judgment quality and computational efficiency. Our comprehensive analysis provides crucial insights for the ethical design of autonomous systems and highlights the importance of considering cultural contexts in AI moral decision-making.",True,True,"Zaim Bin Ahmad, Muhammad Shahrul and Takemoto, Kazuhiro",,,https://dx.plos.org/10.1371/journal.pone.0322776,10.1371/journal.pone.0322776,PLOS One
Moral Change or Noise? On Problems of Aligning AI With Temporally Unstable Human Feedback,2511.10032v1,dickerson2025gets,\cite{dickerson2025gets},"Who Gets the Kidney? Human-AI Alignment, Indecision, and Moral Values",,,True,False,"Dickerson, John P and Hosseini, Hadi and Khanna, Samarth and Pierce, Leona",,,,,arXiv preprint arXiv:2506.00079
Moral Change or Noise? On Problems of Aligning AI With Temporally Unstable Human Feedback,2511.10032v1,mohsin_learning_2025,\cite{mohsin_learning_2025},Learning {Individual} and {Collective} {Priorities} over {Moral} {Dilemmas} with the {Life} {Jacket} {Dataset},,,True,False,"Mohsin, Farhad and Kang, Inwon and Chen, Pin-Yu and Rossi, Francesca and Xia, Lirong",,,https://farhadmohsin.github.io/docs/moral_dilemma-mpref-2022.pdf,,13th Multidisciplinary Workshop on Advances in Preference Handling at IJCAI-2022 (MPREF-22)
Moral Change or Noise? On Problems of Aligning AI With Temporally Unstable Human Feedback,2511.10032v1,rehren2023stable,\cite{rehren2023stable},How stable are moral judgments?,,,True,False,"Rehren, Paul and Sinnott-Armstrong, Walter",,,,,Review of philosophy and psychology
Trusted Multi-view Learning for Long-tailed Classification,2511.09138v1,han2005borderline,\cite{han2005borderline},Borderline-SMOTE: a new over-sampling method in imbalanced data sets learning,,,True,False,"Han, Hui and Wang, Wen-Yuan and Mao, Bing-Huan",2005,,,,
Trusted Multi-view Learning for Long-tailed Classification,2511.09138v1,liu2008exploratory,\cite{liu2008exploratory},Exploratory undersampling for class-imbalance learning,,,True,False,"Liu, Xu-Ying and Wu, Jianxin and Zhou, Zhi-Hua",2008,,,,"IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)"
Trusted Multi-view Learning for Long-tailed Classification,2511.09138v1,cui2019class,\cite{cui2019class},Class-Balanced Loss Based on Effective Number of Samples,https://arxiv.org/abs/1901.05555v1,"With the rapid increase of large-scale, real-world datasets, it becomes critical to address the problem of long-tailed data distribution (i.e., a few classes account for most of the data, while most classes are under-represented). Existing solutions typically adopt class re-balancing strategies such as re-sampling and re-weighting based on the number of observations for each class. In this work, we argue that as the number of samples increases, the additional benefit of a newly added data point will diminish. We introduce a novel theoretical framework to measure data overlap by associating with each sample a small neighboring region rather than a single point. The effective number of samples is defined as the volume of samples and can be calculated by a simple formula $(1-β^{n})/(1-β)$, where $n$ is the number of samples and $β\in [0,1)$ is a hyperparameter. We design a re-weighting scheme that uses the effective number of samples for each class to re-balance the loss, thereby yielding a class-balanced loss. Comprehensive experiments are conducted on artificially induced long-tailed CIFAR datasets and large-scale datasets including ImageNet and iNaturalist. Our results show that when trained with the proposed class-balanced loss, the network is able to achieve significant performance gains on long-tailed datasets.",True,True,"Cui, Yin and Jia, Menglin and Lin, Tsung-Yi and Song, Yang and Belongie, Serge",2019,,,,
Trusted Multi-view Learning for Long-tailed Classification,2511.09138v1,cao2019learning,\cite{cao2019learning},Learning imbalanced datasets with label-distribution-aware margin loss,,,True,False,"Cao, Kaidi and Wei, Colin and Gaidon, Adrien and Arechiga, Nikos and Ma, Tengyu",2019,,,,Advances in neural information processing systems
Trusted Multi-view Learning for Long-tailed Classification,2511.09138v1,wang2021adaptive,\cite{wang2021adaptive},Adaptive class suppression loss for long-tail object detection,,,True,False,"Wang, Tong and Zhu, Yousong and Zhao, Chaoyang and Zeng, Wei and Wang, Jinqiao and Tang, Ming",2021,,,,
Trusted Multi-view Learning for Long-tailed Classification,2511.09138v1,chu2020feature,\cite{chu2020feature},Feature space augmentation for long-tailed data,,,True,False,"Chu, Peng and Bian, Xiao and Liu, Shaopeng and Ling, Haibin",2020,,,,
Trusted Multi-view Learning for Long-tailed Classification,2511.09138v1,kim2020m2m,\cite{kim2020m2m},M2m: Imbalanced Classification via Major-to-minor Translation,https://arxiv.org/abs/2004.00431v2,"In most real-world scenarios, labeled training datasets are highly class-imbalanced, where deep neural networks suffer from generalizing to a balanced testing criterion. In this paper, we explore a novel yet simple way to alleviate this issue by augmenting less-frequent classes via translating samples (e.g., images) from more-frequent classes. This simple approach enables a classifier to learn more generalizable features of minority classes, by transferring and leveraging the diversity of the majority information. Our experimental results on a variety of class-imbalanced datasets show that the proposed method improves the generalization on minority classes significantly compared to other existing re-sampling or re-weighting methods. The performance of our method even surpasses those of previous state-of-the-art methods for the imbalanced classification.",True,True,"Kim, Jaehyung and Jeong, Jongheon and Shin, Jinwoo",2020,,,,
Trusted Multi-view Learning for Long-tailed Classification,2511.09138v1,yin2019feature,\cite{yin2019feature},Feature transfer learning for face recognition with under-represented data,,,True,False,"Yin, Xi and Yu, Xiang and Sohn, Kihyuk and Liu, Xiaoming and Chandraker, Manmohan",2019,,,,
Trusted Multi-view Learning for Long-tailed Classification,2511.09138v1,chawla2002smote,\cite{chawla2002smote},SMOTE: Synthetic Minority Over-sampling Technique,https://arxiv.org/abs/1106.1813v1,"An approach to the construction of classifiers from    imbalanced datasets is described. A dataset is imbalanced if the    classification categories are not approximately equally    represented. Often real-world data sets are predominately composed of    ""normal"" examples with only a small percentage of ""abnormal"" or    ""interesting"" examples. It is also the case that the cost of    misclassifying an abnormal (interesting) example as a normal example    is often much higher than the cost of the reverse    error. Under-sampling of the majority (normal) class has been proposed    as a good means of increasing the sensitivity of a classifier to the    minority class. This paper shows that a combination of our method of    over-sampling the minority (abnormal) class and under-sampling the    majority (normal) class can achieve better classifier performance (in    ROC space) than only under-sampling the majority class.  This paper    also shows that a combination of our method of over-sampling the    minority class and under-sampling the majority class can achieve    better classifier performance (in ROC space) than varying the loss    ratios in Ripper or class priors in Naive Bayes. Our method of    over-sampling the minority class involves creating synthetic minority    class examples.  Experiments are performed using C4.5, Ripper and a    Naive Bayes classifier. The method is evaluated using the area under    the Receiver Operating Characteristic curve (AUC) and the ROC convex    hull strategy.",True,True,"Chawla, Nitesh V and Bowyer, Kevin W and Hall, Lawrence O and Kegelmeyer, W Philip",2002,,,,Journal of artificial intelligence research
Trusted Multi-view Learning for Long-tailed Classification,2511.09138v1,kang2019decoupling,\cite{kang2019decoupling},Decoupling representation and classifier for long-tailed recognition,,,True,False,"Kang, Bingyi and Xie, Saining and Rohrbach, Marcus and Yan, Zhicheng and Gordo, Albert and Feng, Jiashi and Kalantidis, Yannis",2019,,,,arXiv preprint arXiv:1910.09217
Trusted Multi-view Learning for Long-tailed Classification,2511.09138v1,wang2019dynamic,\cite{wang2019dynamic},Dynamic curriculum learning for imbalanced data classification,,,True,False,"Wang, Yiru and Gan, Weihao and Yang, Jie and Wu, Wei and Yan, Junjie",2019,,,,
Trusted Multi-view Learning for Long-tailed Classification,2511.09138v1,feng2021exploring,\cite{feng2021exploring},Exploring Classification Equilibrium in Long-Tailed Object Detection,https://arxiv.org/abs/2108.07507v2,"The conventional detectors tend to make imbalanced classification and suffer performance drop, when the distribution of the training data is severely skewed. In this paper, we propose to use the mean classification score to indicate the classification accuracy for each category during training. Based on this indicator, we balance the classification via an Equilibrium Loss (EBL) and a Memory-augmented Feature Sampling (MFS) method. Specifically, EBL increases the intensity of the adjustment of the decision boundary for the weak classes by a designed score-guided loss margin between any two classes. On the other hand, MFS improves the frequency and accuracy of the adjustment of the decision boundary for the weak classes through over-sampling the instance features of those classes. Therefore, EBL and MFS work collaboratively for finding the classification equilibrium in long-tailed detection, and dramatically improve the performance of tail classes while maintaining or even improving the performance of head classes. We conduct experiments on LVIS using Mask R-CNN with various backbones including ResNet-50-FPN and ResNet-101-FPN to show the superiority of the proposed method. It improves the detection performance of tail classes by 15.6 AP, and outperforms the most recent long-tailed object detectors by more than 1 AP. Code is available at https://github.com/fcjian/LOCE.",True,True,"Feng, Chengjian and Zhong, Yujie and Huang, Weilin",2021,,,,
Trusted Multi-view Learning for Long-tailed Classification,2511.09138v1,ren2020balanced,\cite{ren2020balanced},Balanced meta-softmax for long-tailed visual recognition,,,True,False,"Ren, Jiawei and Yu, Cunjun and Ma, Xiao and Zhao, Haiyu and Yi, Shuai and others",2020,,,,Advances in neural information processing systems
Trusted Multi-view Learning for Long-tailed Classification,2511.09138v1,wang2020devil,\cite{wang2020devil},The devil is in classification: A simple framework for long-tail instance segmentation,,,True,False,"Wang, Tao and Li, Yu and Kang, Bingyi and Li, Junnan and Liew, Junhao and Tang, Sheng and Hoi, Steven and Feng, Jiashi",2020,,,,
Trusted Multi-view Learning for Long-tailed Classification,2511.09138v1,sensoy2018evidential,\cite{sensoy2018evidential},Evidential Deep Learning to Quantify Classification Uncertainty,https://arxiv.org/abs/1806.01768v3,"Deterministic neural nets have been shown to learn effective predictors on a wide range of machine learning problems. However, as the standard approach is to train the network to minimize a prediction loss, the resultant model remains ignorant to its prediction confidence. Orthogonally to Bayesian neural nets that indirectly infer prediction uncertainty through weight uncertainties, we propose explicit modeling of the same using the theory of subjective logic. By placing a Dirichlet distribution on the class probabilities, we treat predictions of a neural net as subjective opinions and learn the function that collects the evidence leading to these opinions by a deterministic neural net from data. The resultant predictor for a multi-class classification problem is another Dirichlet distribution whose parameters are set by the continuous output of a neural net. We provide a preliminary analysis on how the peculiarities of our new loss function drive improved uncertainty estimation. We observe that our method achieves unprecedented success on detection of out-of-distribution queries and endurance against adversarial perturbations.",True,True,"Sensoy, Murat and Kaplan, Lance and Kandemir, Melih",2018,,,,Advances in neural information processing systems
Trusted Multi-view Learning for Long-tailed Classification,2511.09138v1,han2022trusted,\cite{han2022trusted},Trusted multi-view classification with dynamic evidential fusion,,,True,False,"Han, Zongbo and Zhang, Changqing and Fu, Huazhu and Zhou, Joey Tianyi",2022,,,,IEEE transactions on pattern analysis and machine intelligence
Trusted Multi-view Learning for Long-tailed Classification,2511.09138v1,liu2022trusted,\cite{liu2022trusted},Trusted multi-view deep learning with opinion aggregation,,,True,False,"Liu, Wei and Yue, Xiaodong and Chen, Yufei and Denoeux, Thierry",2022,,,,
Trusted Multi-view Learning for Long-tailed Classification,2511.09138v1,xu2024reliable,\cite{xu2024reliable},Reliable Conflictive Multi-View Learning,https://arxiv.org/abs/2402.16897v2,"Multi-view learning aims to combine multiple features to achieve more comprehensive descriptions of data. Most previous works assume that multiple views are strictly aligned. However, real-world multi-view data may contain low-quality conflictive instances, which show conflictive information in different views. Previous methods for this problem mainly focus on eliminating the conflictive data instances by removing them or replacing conflictive views. Nevertheless, real-world applications usually require making decisions for conflictive instances rather than only eliminating them. To solve this, we point out a new Reliable Conflictive Multi-view Learning (RCML) problem, which requires the model to provide decision results and attached reliabilities for conflictive multi-view data. We develop an Evidential Conflictive Multi-view Learning (ECML) method for this problem. ECML first learns view-specific evidence, which could be termed as the amount of support to each category collected from data. Then, we can construct view-specific opinions consisting of decision results and reliability. In the multi-view fusion stage, we propose a conflictive opinion aggregation strategy and theoretically prove this strategy can exactly model the relation of multi-view common and view-specific reliabilities. Experiments performed on 6 datasets verify the effectiveness of ECML.",True,True,"Xu, Cai and Si, Jiajun and Guan, Ziyu and Zhao, Wei and Wu, Yue and Gao, Xiyue",2024,,,,
Trusted Multi-view Learning for Long-tailed Classification,2511.09138v1,xu2024trusted,\cite{xu2024trusted},Trusted Multi-view Learning with Label Noise,,,True,False,"Xu, Cai and Zhang, Yilin and Guan, Ziyu and Zhao, Wei",2024,,,,arXiv preprint arXiv:2404.11944
Trusted Multi-view Learning for Long-tailed Classification,2511.09138v1,shi2024generalized,\cite{shi2024generalized},Generalized Trusted Multi-view Classification Framework with Hierarchical Opinion Aggregation,https://arxiv.org/abs/2411.03713v2,"Recently, multi-view learning has witnessed a considerable interest on the research of trusted decision-making. Previous methods are mainly inspired from an important paper published by Han et al. in 2021, which formulates a Trusted Multi-view Classification (TMC) framework that aggregates evidence from different views based on Dempster's combination rule. All these methods only consider inter-view aggregation, yet lacking exploitation of intra-view information. In this paper, we propose a generalized trusted multi-view classification framework with hierarchical opinion aggregation. This hierarchical framework includes a two-phase aggregation process: the intra-view and inter-view aggregation hierarchies. In the intra aggregation, we assume that each view is comprised of common information shared with other views, as well as its specific information. We then aggregate both the common and specific information. This aggregation phase is useful to eliminate the feature noise inherent to view itself, thereby improving the view quality. In the inter-view aggregation, we design an attention mechanism at the evidence level to facilitate opinion aggregation from different views. To the best of our knowledge, this is one of the pioneering efforts to formulate a hierarchical aggregation framework in the trusted multi-view learning domain. Extensive experiments show that our model outperforms some state-of art trust-related baselines. One can access the source code on https://github.com/lshi91/GTMC-HOA.",True,True,"Shi, Long and Tang, Chuanqing and Deng, Huangyi and Xu, Cai and Xing, Lei and Chen, Badong",2024,,,,arXiv preprint arXiv:2411.03713
Trusted Multi-view Learning for Long-tailed Classification,2511.09138v1,liu2024dynamic,\cite{liu2024dynamic},Dynamic Evidence Decoupling for Trusted Multi-view Learning,,,True,False,"Liu, Ying and Liu, Lihong and Xu, Cai and Song, Xiangyu and Guan, Ziyu and Zhao, Wei",,,,,
Trusted Multi-view Learning for Long-tailed Classification,2511.09138v1,wang2024trusted,\cite{wang2024trusted},Trusted semi-supervised multi-view classification with contrastive learning,,,True,False,"Wang, Xiaoli and Wang, Yongli and Wang, Yupeng and Huang, Anqi and Liu, Jun",2024,,,,IEEE Transactions on Multimedia
Trusted Multi-view Learning for Long-tailed Classification,2511.09138v1,hu2025self,\cite{hu2025self},Self-supervised Trusted Contrastive Multi-view Clustering with Uncertainty Refined,,,True,False,"Hu, Shizhe and Tian, Binyan and Liu, Weibo and Ye, Yangdong",2025,,,,
Trusted Multi-view Learning for Long-tailed Classification,2511.09138v1,zhou2023rtmc,\cite{zhou2023rtmc},Rtmc: a rubost trusted multi-view classification framework,,,True,False,"Zhou, Hai and Xue, Zhe and Liu, Ying and Li, Boang and Du, Junping and Liang, Meiyu",2023,,,,
Trusted Multi-view Learning for Long-tailed Classification,2511.09138v1,zhou2023calm,\cite{zhou2023calm},CALM: An Enhanced Encoding and Confidence Evaluating Framework for Trustworthy Multi-view Learning,,,True,False,"Zhou, Hai and Xue, Zhe and Liu, Ying and Li, Boang and Du, Junping and Liang, Meiyu and Qi, Yuankai",2023,,,,
Trusted Multi-view Learning for Long-tailed Classification,2511.09138v1,du2023bridging,\cite{du2023bridging},"Bridging Trustworthiness and Open-World Learning: An Exploratory Neural Approach for Enhancing Interpretability, Generalization, and Robustness",,,True,False,"Du, Shide and Fang, Zihan and Lan, Shiyang and Tan, Yanchao and G{\""u}nther, Manuel and Wang, Shiping and Guo, Wenzhong",2023,,,,
Trusted Multi-view Learning for Long-tailed Classification,2511.09138v1,deng2025trustworthy,\cite{deng2025trustworthy},Trustworthy data recovery for incomplete multi-view learning,,,True,False,"Deng, Huangyi and Pan, Ningning and Tang, Chuanqing and Shi, Long",2025,,,,Signal Processing
Trusted Multi-view Learning for Long-tailed Classification,2511.09138v1,huang2025multi,\cite{huang2025multi},"Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities",https://arxiv.org/abs/2507.06261v5,"In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal understanding and it is now able to process up to 3 hours of video content. Its unique combination of long context, multimodal and reasoning capabilities can be combined to unlock new agentic workflows. Gemini 2.5 Flash provides excellent reasoning abilities at a fraction of the compute and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high performance at low latency and cost. Taken together, the Gemini 2.X model generation spans the full Pareto frontier of model capability vs cost, allowing users to explore the boundaries of what is possible with complex agentic problem solving.",True,True,,,,,,
Data-Driven Discovery of Feature Groups in Clinical Time Series,2511.08260v1,harutyunyan2019multitask,\cite{harutyunyan2019multitask},Multitask learning and benchmarking with clinical time series data,,,True,False,"Harutyunyan, Hrayr and Khachatrian, Hrant and Kale, David C and Ver Steeg, Greg and Galstyan, Aram",2019,,,,Scientific data
Data-Driven Discovery of Feature Groups in Clinical Time Series,2511.08260v1,hyland2020early,\cite{hyland2020early},Early prediction of circulatory failure in the intensive care unit using machine learning,,,True,False,"Hyland, Stephanie L and Faltys, Martin and H{\""u}ser, Matthias and Lyu, Xinrui and Gumbsch, Thomas and Esteban, Crist{\'o}bal and Bock, Christian and Horn, Max and Moor, Michael and Rieck, Bastian and others",2020,,,,Nature medicine
Data-Driven Discovery of Feature Groups in Clinical Time Series,2511.08260v1,yeche2021,\cite{yeche2021},HiRID-ICU-Benchmark -- A Comprehensive Machine Learning Benchmark on High-resolution ICU Data,https://arxiv.org/abs/2111.08536v4,"The recent success of machine learning methods applied to time series collected from Intensive Care Units (ICU) exposes the lack of standardized machine learning benchmarks for developing and comparing such methods. While raw datasets, such as MIMIC-IV or eICU, can be freely accessed on Physionet, the choice of tasks and pre-processing is often chosen ad-hoc for each publication, limiting comparability across publications. In this work, we aim to improve this situation by providing a benchmark covering a large spectrum of ICU-related tasks. Using the HiRID dataset, we define multiple clinically relevant tasks in collaboration with clinicians. In addition, we provide a reproducible end-to-end pipeline to construct both data and labels. Finally, we provide an in-depth analysis of current state-of-the-art sequence modeling methods, highlighting some limitations of deep learning approaches for this type of data. With this benchmark, we hope to give the research community the possibility of a fair comparison of their work.",True,True,"Y\`{e}che, Hugo and Kuznetsova, Rita and Zimmermann, Marc and H\""{u}ser, Matthias and Lyu, Xinrui and Faltys, Martin and R\""{a}tsch, Gunnar",2021,,,,
Data-Driven Discovery of Feature Groups in Clinical Time Series,2511.08260v1,yeche2023temporal,\cite{yeche2023temporal},Temporal label smoothing for early event prediction,,,True,False,"Y{\`e}che, Hugo and Pace, Aliz{\'e}e and Ratsch, Gunnar and Kuznetsova, Rita",2023,,,,
Data-Driven Discovery of Feature Groups in Clinical Time Series,2511.08260v1,yeche2024dsaforeep,\cite{yeche2024dsaforeep},Dynamic Survival Analysis for Early Event Prediction,,,True,False,Hugo Yèche and Manuel Burger and Dinara Veshchezerova and Gunnar Rätsch,2024,,https://arxiv.org/abs/2403.12818,,
Data-Driven Discovery of Feature Groups in Clinical Time Series,2511.08260v1,kuznetsova2023importance,\cite{kuznetsova2023importance},On the importance of step-wise embeddings for heterogeneous clinical time-series,,,True,False,"Kuznetsova, Rita and Pace, Aliz{\'e}e and Burger, Manuel and Y{\`e}che, Hugo and R{\""a}tsch, Gunnar",2023,,,,
Data-Driven Discovery of Feature Groups in Clinical Time Series,2511.08260v1,gorishniy2022embeddings,\cite{gorishniy2022embeddings},On embeddings for numerical features in tabular deep learning,,,True,False,"Gorishniy, Yury and Rubachev, Ivan and Babenko, Artem",2022,,,,Advances in Neural Information Processing Systems
Data-Driven Discovery of Feature Groups in Clinical Time Series,2511.08260v1,he2016deep,\cite{he2016deep},Deep residual learning for image recognition,,,True,False,"He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian",2016,,,,
Data-Driven Discovery of Feature Groups in Clinical Time Series,2511.08260v1,vaswani2017attention,\cite{vaswani2017attention},Not All Attention Is All You Need,https://arxiv.org/abs/2104.04692v3,"Beyond the success story of pre-trained language models (PrLMs) in recent natural language processing, they are susceptible to over-fitting due to unusual large model size. To this end, dropout serves as a therapy. However, existing methods like random-based, knowledge-based and search-based dropout are more general but less effective onto self-attention based models, which are broadly chosen as the fundamental architecture of PrLMs. In this paper, we propose a novel dropout method named AttendOut to let self-attention empowered PrLMs capable of more robust task-specific tuning. We demonstrate that state-of-the-art models with elaborate training design may achieve much stronger results. We verify the universality of our approach on extensive natural language processing tasks.",True,True,"Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia",2017,,,,arXiv preprint arXiv:1706.03762
Data-Driven Discovery of Feature Groups in Clinical Time Series,2511.08260v1,tomavsev2021use,\cite{tomavsev2021use},Use of deep learning to develop continuous-risk models for adverse event prediction from electronic health records,,,True,False,"Toma{\v{s}}ev, Nenad and Harris, Natalie and Baur, Sebastien and Mottram, Anne and Glorot, Xavier and Rae, Jack W and Zielinski, Michal and Askham, Harry and Saraiva, Andre and Magliulo, Valerio and others",2021,,,,Nature protocols
Data-Driven Discovery of Feature Groups in Clinical Time Series,2511.08260v1,tomavsev2019clinically,\cite{tomavsev2019clinically},A clinically applicable approach to continuous prediction of future acute kidney injury,,,True,False,"Toma{\v{s}}ev, Nenad and Glorot, Xavier and Rae, Jack W and Zielinski, Michal and Askham, Harry and Saraiva, Andre and Mottram, Anne and Meyer, Clemens and Ravuri, Suman and Protsyuk, Ivan and others",2019,,,,Nature
Data-Driven Discovery of Feature Groups in Clinical Time Series,2511.08260v1,swamy2024interpretcc,\cite{swamy2024interpretcc},Intrinsic User-Centric Interpretability through Global Mixture of Experts,https://arxiv.org/abs/2402.02933v4,"In human-centric settings like education or healthcare, model accuracy and model explainability are key factors for user adoption. Towards these two goals, intrinsically interpretable deep learning models have gained popularity, focusing on accurate predictions alongside faithful explanations. However, there exists a gap in the human-centeredness of these approaches, which often produce nuanced and complex explanations that are not easily actionable for downstream users. We present InterpretCC (interpretable conditional computation), a family of intrinsically interpretable neural networks at a unique point in the design space that optimizes for ease of human understanding and explanation faithfulness, while maintaining comparable performance to state-of-the-art models. InterpretCC achieves this through adaptive sparse activation of features before prediction, allowing the model to use a different, minimal set of features for each instance. We extend this idea into an interpretable, global mixture-of-experts (MoE) model that allows users to specify topics of interest, discretely separates the feature space for each data point into topical subnetworks, and adaptively and sparsely activates these topical subnetworks for prediction. We apply InterpretCC for text, time series and tabular data across several real-world datasets, demonstrating comparable performance with non-interpretable baselines and outperforming intrinsically interpretable baselines. Through a user study involving 56 teachers, InterpretCC explanations are found to have higher actionability and usefulness over other intrinsically interpretable approaches.",True,True,"Swamy, Vinitra and Montariol, Syrielle and Blackwell, Julian and Frej, Jibril and Jaggi, Martin and K{\""a}ser, Tanja",2024,,,,Network
Data-Driven Discovery of Feature Groups in Clinical Time Series,2511.08260v1,masoomi2020instance,\cite{masoomi2020instance},Instance-wise feature grouping,,,True,False,"Masoomi, Aria and Wu, Chieh and Zhao, Tingting and Wang, Zifeng and Castaldi, Peter and Dy, Jennifer",2020,,,,Advances in Neural Information Processing Systems
Data-Driven Discovery of Feature Groups in Clinical Time Series,2511.08260v1,imrie2022composite,\cite{imrie2022composite},Composite Feature Selection using Deep Ensembles,https://arxiv.org/abs/2211.00631v2,"In many real world problems, features do not act alone but in combination with each other. For example, in genomics, diseases might not be caused by any single mutation but require the presence of multiple mutations. Prior work on feature selection either seeks to identify individual features or can only determine relevant groups from a predefined set. We investigate the problem of discovering groups of predictive features without predefined grouping. To do so, we define predictive groups in terms of linear and non-linear interactions between features. We introduce a novel deep learning architecture that uses an ensemble of feature selection models to find predictive groups, without requiring candidate groups to be provided. The selected groups are sparse and exhibit minimum overlap. Furthermore, we propose a new metric to measure similarity between discovered groups and the ground truth. We demonstrate the utility of our model on multiple synthetic tasks and semi-synthetic chemistry datasets, where the ground truth structure is known, as well as an image dataset and a real-world cancer dataset.",True,True,"Imrie, Fergus and Norcliffe, Alexander and Li{\`o}, Pietro and van der Schaar, Mihaela",2022,,,,Advances in Neural Information Processing Systems
Data-Driven Discovery of Feature Groups in Clinical Time Series,2511.08260v1,ma2019learning,\cite{ma2019learning},Learning representations for time series clustering,,,True,False,"Ma, Qianli and Zheng, Jiawei and Li, Sen and Cottrell, Gary W",2019,,,,Advances in neural information processing systems
Data-Driven Discovery of Feature Groups in Clinical Time Series,2511.08260v1,ma2020concare,\cite{ma2020concare},ConCare: Personalized Clinical Feature Embedding via Capturing the Healthcare Context,https://arxiv.org/abs/1911.12216v1,"Predicting the patient's clinical outcome from the historical electronic medical records (EMR) is a fundamental research problem in medical informatics. Most deep learning-based solutions for EMR analysis concentrate on learning the clinical visit embedding and exploring the relations between visits. Although those works have shown superior performances in healthcare prediction, they fail to explore the personal characteristics during the clinical visits thoroughly. Moreover, existing works usually assume that the more recent record weights more in the prediction, but this assumption is not suitable for all conditions. In this paper, we propose ConCare to handle the irregular EMR data and extract feature interrelationship to perform individualized healthcare prediction. Our solution can embed the feature sequences separately by modeling the time-aware distribution. ConCare further improves the multi-head self-attention via the cross-head decorrelation, so that the inter-dependencies among dynamic features and static baseline information can be effectively captured to form the personal health context. Experimental results on two real-world EMR datasets demonstrate the effectiveness of ConCare. The medical findings extracted by ConCare are also empirically confirmed by human experts and medical literature.",True,True,"Ma, Liantao and Zhang, Chaohe and Wang, Yasha and Ruan, Wenjie and Wang, Jiangtao and Tang, Wen and Ma, Xinyu and Gao, Xin and Gao, Junyi",2020,,,,
Data-Driven Discovery of Feature Groups in Clinical Time Series,2511.08260v1,roy2021efficient,\cite{roy2021efficient},Efficient Content-Based Sparse Attention with Routing Transformers,https://arxiv.org/abs/2003.05997v5,"Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic compute and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: it combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to $O\left(n^{1.5}d\right)$ from $O\left(n^2d\right)$ for sequence length $n$ and hidden dimension $d$. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity) as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192.",True,True,"Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David",2021,,,,Transactions of the Association for Computational Linguistics
Data-Driven Discovery of Feature Groups in Clinical Time Series,2511.08260v1,vyas2020fast,\cite{vyas2020fast},Fast transformers with clustered attention,,,True,False,"Vyas, Apoorv and Katharopoulos, Angelos and Fleuret, Fran{\c{c}}ois",2020,,,,Advances in Neural Information Processing Systems
Data-Driven Discovery of Feature Groups in Clinical Time Series,2511.08260v1,li2021groupformer,\cite{li2021groupformer},Groupformer: Group activity recognition with clustered spatial-temporal transformer,,,True,False,"Li, Shuaicheng and Cao, Qianggang and Liu, Lingbo and Yang, Kunlin and Liu, Shinan and Hou, Jun and Yi, Shuai",2021,,,,
Data-Driven Discovery of Feature Groups in Clinical Time Series,2511.08260v1,becker2023unsupervised,\cite{becker2023unsupervised},Unsupervised EHR-based Phenotyping via Matrix and Tensor Decompositions,https://arxiv.org/abs/2209.00322v1,"Computational phenotyping allows for unsupervised discovery of subgroups of patients as well as corresponding co-occurring medical conditions from electronic health records (EHR). Typically, EHR data contains demographic information, diagnoses and laboratory results. Discovering (novel) phenotypes has the potential to be of prognostic and therapeutic value. Providing medical practitioners with transparent and interpretable results is an important requirement and an essential part for advancing precision medicine. Low-rank data approximation methods such as matrix (e.g., non-negative matrix factorization) and tensor decompositions (e.g., CANDECOMP/PARAFAC) have demonstrated that they can provide such transparent and interpretable insights. Recent developments have adapted low-rank data approximation methods by incorporating different constraints and regularizations that facilitate interpretability further. In addition, they offer solutions for common challenges within EHR data such as high dimensionality, data sparsity and incompleteness. Especially extracting temporal phenotypes from longitudinal EHR has received much attention in recent years. In this paper, we provide a comprehensive review of low-rank approximation-based approaches for computational phenotyping. The existing literature is categorized into temporal vs. static phenotyping approaches based on matrix vs. tensor decompositions. Furthermore, we outline different approaches for the validation of phenotypes, i.e., the assessment of clinical significance.",True,True,"Becker, Florian and Smilde, Age K and Acar, Evrim",2023,,,,Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery
ProBench: Benchmarking GUI Agents with Accurate Process Information,2511.09157v1,deka2017rico,\cite{deka2017rico},Rico: A mobile app dataset for building data-driven design applications,,,True,False,"Deka, Biplab and Huang, Zifeng and Franzen, Chad and Hibschman, Joshua and Afergan, Daniel and Li, Yang and Nichols, Jeffrey and Kumar, Ranjitha",2017,,,,
ProBench: Benchmarking GUI Agents with Accurate Process Information,2511.09157v1,venkatesh2022ugif,\cite{venkatesh2022ugif},UGIF: UI Grounded Instruction Following,https://arxiv.org/abs/2211.07615v2,"Smartphone users often find it difficult to navigate myriad menus to perform common tasks such as ""How to block calls from unknown numbers?"". Currently, help documents with step-by-step instructions are manually written to aid the user. The user experience can be further enhanced by grounding the instructions in the help document to the UI and overlaying a tutorial on the phone UI. To build such tutorials, several natural language processing components including retrieval, parsing, and grounding are necessary, but there isn't any relevant dataset for such a task. Thus, we introduce UGIF-DataSet, a multi-lingual, multi-modal UI grounded dataset for step-by-step task completion on the smartphone containing 4,184 tasks across 8 languages. As an initial approach to this problem, we propose retrieving the relevant instruction steps based on the user's query and parsing the steps using Large Language Models (LLMs) to generate macros that can be executed on-device. The instruction steps are often available only in English, so the challenge includes cross-modal, cross-lingual retrieval of English how-to pages from user queries in many languages and mapping English instruction steps to UI in a potentially different language. We compare the performance of different LLMs including PaLM and GPT-3 and find that the end-to-end task completion rate is 48% for English UI but the performance drops to 32% for other languages. We analyze the common failure modes of existing models on this task and point out areas for improvement.",True,True,"Venkatesh, Sagar Gubbi and Talukdar, Partha and Narayanan, Srini",2022,,,,arXiv preprint arXiv:2211.07615
ProBench: Benchmarking GUI Agents with Accurate Process Information,2511.09157v1,rawles2023androidinthewild,\cite{rawles2023androidinthewild},Androidinthewild: A large-scale dataset for android device control,,,True,False,"Rawles, Christopher and Li, Alice and Rodriguez, Daniel and Riva, Oriana and Lillicrap, Timothy",2023,,,,Advances in Neural Information Processing Systems
ProBench: Benchmarking GUI Agents with Accurate Process Information,2511.09157v1,zhang2024android,\cite{zhang2024android},Android in the zoo: Chain-of-action-thought for gui agents,,,True,False,"Zhang, Jiwen and Wu, Jihao and Teng, Yihua and Liao, Minghui and Xu, Nuo and Xiao, Xiao and Wei, Zhongyu and Tang, Duyu",2024,,,,arXiv preprint arXiv:2403.02713
ProBench: Benchmarking GUI Agents with Accurate Process Information,2511.09157v1,li2024effects,\cite{li2024effects},On the effects of data scale on computer control agents,,,True,False,"Li, Wei and Bishop, William and Li, Alice and Rawles, Chris and Campbell-Ajala, Folawiyo and Tyamagundlu, Divya and Riva, Oriana",2024,,,,arXiv e-prints
ProBench: Benchmarking GUI Agents with Accurate Process Information,2511.09157v1,xing2024understanding,\cite{xing2024understanding},Understanding the weakness of large language model agents within a complex android environment,,,True,False,"Xing, Mingzhe and Zhang, Rongkai and Xue, Hui and Chen, Qi and Yang, Fan and Xiao, Zhen",2024,,,,
ProBench: Benchmarking GUI Agents with Accurate Process Information,2511.09157v1,lee2024benchmarkingmobiledevicecontrol,\cite{lee2024benchmarkingmobiledevicecontrol},Benchmarking Mobile Device Control Agents across Diverse Configurations,https://arxiv.org/abs/2404.16660v3,"Mobile device control agents can largely enhance user interactions and productivity by automating daily tasks. However, despite growing interest in developing practical agents, the absence of a commonly adopted benchmark in this area makes it challenging to quantify scientific progress. In this work, we introduce B-MoCA: a novel benchmark with interactive environments for evaluating and developing mobile device control agents. To create a realistic benchmark, we develop B-MoCA based on the Android operating system and define 131 common daily tasks. Importantly, we incorporate a randomization feature that changes the configurations of mobile devices, including user interface layouts and language settings, to assess generalization performance. We benchmark diverse agents, including agents employing large language models (LLMs) or multi-modal LLMs as well as agents trained with imitation learning using human expert demonstrations. While these agents demonstrate proficiency in executing straightforward tasks, their poor performance on complex tasks highlights significant opportunities for future research to improve effectiveness. Our source code is publicly available at https://b-moca.github.io.",True,True,Juyong Lee and Taywon Min and Minyong An and Dongyoon Hahm and Haeone Lee and Changyeon Kim and Kimin Lee,2024,,https://arxiv.org/abs/2404.16660,,
ProBench: Benchmarking GUI Agents with Accurate Process Information,2511.09157v1,rawles2024androidworld,\cite{rawles2024androidworld},Androidworld: A dynamic benchmarking environment for autonomous agents,,,True,False,"Rawles, Christopher and Clinckemaillie, Sarah and Chang, Yifan and Waltz, Jonathan and Lau, Gabrielle and Fair, Marybeth and Li, Alice and Bishop, William and Li, Wei and Campbell-Ajala, Folawiyo and others",2024,,,,arXiv preprint arXiv:2405.14573
ProBench: Benchmarking GUI Agents with Accurate Process Information,2511.09157v1,xu2024androidlab,\cite{xu2024androidlab},Androidlab: Training and systematic benchmarking of android autonomous agents,,,True,False,"Xu, Yifan and Liu, Xiao and Sun, Xueqiao and Cheng, Siyi and Yu, Hao and Lai, Hanyu and Zhang, Shudan and Zhang, Dan and Tang, Jie and Dong, Yuxiao",2024,,,,arXiv preprint arXiv:2410.24024
ProBench: Benchmarking GUI Agents with Accurate Process Information,2511.09157v1,chen2024spa,\cite{chen2024spa},Spa-bench: A comprehensive benchmark for smartphone agent evaluation,,,True,False,"Chen, Jingxuan and Yuen, Derek and Xie, Bin and Yang, Yuhao and Chen, Gongwei and Wu, Zhihao and Yixing, Li and Zhou, Xurui and Liu, Weiwen and Wang, Shuai and others",2024,,,,
ProBench: Benchmarking GUI Agents with Accurate Process Information,2511.09157v1,chai2025a3,\cite{chai2025a3},A3: Android agent arena for mobile gui agents,,,True,False,"Chai, Yuxiang and Li, Hanhao and Zhang, Jiayu and Liu, Liang and Liu, Guangyi and Wang, Guozhi and Ren, Shuai and Huang, Siyuan and Li, Hongsheng",2025,,,,arXiv preprint arXiv:2501.01149
Patching LLM Like Software: A Lightweight Method for Improving Safety Policy in Large Language Models,2511.08484v1,christiano2017deep,\cite{christiano2017deep},Deep reinforcement learning from human preferences,https://arxiv.org/abs/1706.03741v4,"For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.",True,True,"Christiano, Paul and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario",2017,,,,
Patching LLM Like Software: A Lightweight Method for Improving Safety Policy in Large Language Models,2511.08484v1,ouyang2022,\cite{ouyang2022},Training language models to follow instructions with human feedback,https://arxiv.org/abs/2203.02155v1,"Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",True,True,"Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and others",2022,,,,
Patching LLM Like Software: A Lightweight Method for Improving Safety Policy in Large Language Models,2511.08484v1,rafailov2023,\cite{rafailov2023},Direct Preference Optimization: Your Language Model is Secretly a Reward Model,https://arxiv.org/abs/2305.18290v3,"While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.",True,True,"Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Finn, Chelsea",2023,,,,
Patching LLM Like Software: A Lightweight Method for Improving Safety Policy in Large Language Models,2511.08484v1,gehman2020realtoxicityprompts,\cite{gehman2020realtoxicityprompts},RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models,https://arxiv.org/abs/2009.11462v2,"Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning ""bad"" words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.",True,True,"Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A.",2020,,,,
Patching LLM Like Software: A Lightweight Method for Improving Safety Policy in Large Language Models,2511.08484v1,dong2024disclosure,\cite{dong2024disclosure},Disclosure and Mitigation of Gender Bias in LLMs,,,True,False,Xiangjue Dong and Yibo Wang and Philip S. Yu and James Caverlee,2024,,https://arxiv.org/abs/2402.11190,,arXiv preprint arXiv:2402.11190
Patching LLM Like Software: A Lightweight Method for Improving Safety Policy in Large Language Models,2511.08484v1,hu2021lora,\cite{hu2021lora},LoRA+: Efficient Low Rank Adaptation of Large Models,https://arxiv.org/abs/2402.12354v2,"In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$ improves performance (1-2 $\%$ improvements) and finetuning speed (up to $\sim$ 2X SpeedUp), at the same computational cost as LoRA.",True,True,"Hu, Edward J. and Shen, Yelong and Wallis, Phil and Allen-Zhu, Zeyuan and others",2021,,,,
Patching LLM Like Software: A Lightweight Method for Improving Safety Policy in Large Language Models,2511.08484v1,dettmers2023qlora,\cite{dettmers2023qlora},QLoRA: Efficient Finetuning of Quantized LLMs,https://arxiv.org/abs/2305.14314v1,"We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.",True,True,"Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke",2023,,,,
Patching LLM Like Software: A Lightweight Method for Improving Safety Policy in Large Language Models,2511.08484v1,li2021prefix,\cite{li2021prefix},Prefix-Tuning: Optimizing Continuous Prompts for Generation,https://arxiv.org/abs/2101.00190v1,"Fine-tuning is the de facto way to leverage large pretrained language models to perform downstream tasks. However, it modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix). Prefix-tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were ""virtual tokens"". We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We find that by learning only 0.1\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics unseen during training.",True,True,"Li, Xiang Lisa and Liang, Percy",2021,,,,
Patching LLM Like Software: A Lightweight Method for Improving Safety Policy in Large Language Models,2511.08484v1,lester2021,\cite{lester2021},The power of scale: Parameter-efficient adaptation for pretrained language models,,,True,False,"Lester, Brian and Al-Rfou, Rami and Constant, Noah",2021,,,,
ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,2511.08263v1,alexey2016discriminative,\cite{alexey2016discriminative},Discriminative unsupervised feature learning with exemplar convolutional neural networks,,,True,False,"Alexey, Dosovitskiy and Fischer, Philipp and Tobias, Jost and Springenberg, Martin Riedmiller and Brox, Thomas",2016,,,,IEEE TPAMI
ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,2511.08263v1,bautista2016cliquecnn,\cite{bautista2016cliquecnn},CliqueCNN: Deep Unsupervised Exemplar Learning,https://arxiv.org/abs/1608.08792v1,"Exemplar learning is a powerful paradigm for discovering visual similarities in an unsupervised manner. In this context, however, the recent breakthrough in deep learning could not yet unfold its full potential. With only a single positive sample, a great imbalance between one positive and many negatives, and unreliable relationships between most samples, training of Convolutional Neural networks is impaired. Given weak estimates of local distance we propose a single optimization problem to extract batches of samples with mutually consistent relations. Conflicting relations are distributed over different batches and similar samples are grouped into compact cliques. Learning exemplar similarities is framed as a sequence of clique categorization tasks. The CNN then consolidates transitivity relations within and between cliques and learns a single representation for all samples without the need for labels. The proposed unsupervised approach has shown competitive performance on detailed posture analysis and object classification.",True,True,"Bautista, Miguel A and Sanakoyeu, Artsiom and Tikhoncheva, Ekaterina and Ommer, Bjorn",2016,,,,Advances in Neural Information Processing Systems
ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,2511.08263v1,wei2015submodularity,\cite{wei2015submodularity},Submodularity in data subset selection and active learning,,,True,False,"Wei, Kai and Iyer, Rishabh and Bilmes, Jeff",2015,,,,
ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,2511.08263v1,soper2021greed,\cite{soper2021greed},Greed is good: rapid hyperparameter optimization and model selection using greedy k-fold cross validation,,,True,False,"Soper, Daniel S",2021,,,,Electronics
ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,2511.08263v1,paul2021deep,\cite{paul2021deep},Deep learning on a data diet: Finding important examples early in training,,,True,False,"Paul, Mansheej and Ganguli, Surya and Dziugaite, Gintare Karolina",2021,,,,Advances in neural information processing systems
ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,2511.08263v1,mirzasoleiman2020coresets,\cite{mirzasoleiman2020coresets},Coresets for data-efficient training of machine learning models,,,True,False,"Mirzasoleiman, Baharan and Bilmes, Jeff and Leskovec, Jure",2020,,,,
ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,2511.08263v1,killamsetty2021grad,\cite{killamsetty2021grad},GRAD-MATCH: Gradient Matching based Data Subset Selection for Efficient Deep Model Training,https://arxiv.org/abs/2103.00123v2,"The great success of modern machine learning models on large datasets is contingent on extensive computational resources with high financial and environmental costs. One way to address this is by extracting subsets that generalize on par with the full data. In this work, we propose a general framework, GRAD-MATCH, which finds subsets that closely match the gradient of the training or validation set. We find such subsets effectively using an orthogonal matching pursuit algorithm. We show rigorous theoretical and convergence guarantees of the proposed algorithm and, through our extensive experiments on real-world datasets, show the effectiveness of our proposed framework. We show that GRAD-MATCH significantly and consistently outperforms several recent data-selection algorithms and achieves the best accuracy-efficiency trade-off. GRAD-MATCH is available as a part of the CORDS toolkit: \url{https://github.com/decile-team/cords}.",True,True,"Killamsetty, Krishnateja and Durga, Sivasubramanian and Ramakrishnan, Ganesh and De, Abir and Iyer, Rishabh",2021,,,,
ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,2511.08263v1,LESS,\cite{LESS},Less: Selecting influential data for targeted instruction tuning,,,True,False,"Xia, Mengzhou and Malladi, Sadhika and Gururangan, Suchin and Arora, Sanjeev and Chen, Danqi",2024,,,,arXiv preprint arXiv:2402.04333
ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,2511.08263v1,toneva2018empirical,\cite{toneva2018empirical},An Empirical Study of Example Forgetting during Deep Neural Network Learning,https://arxiv.org/abs/1812.05159v3,"Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks. Our goal is to understand whether a related phenomenon occurs when data does not undergo a clear distributional shift. We define a `forgetting event' to have occurred when an individual training example transitions from being classified correctly to incorrectly over the course of learning. Across several benchmark data sets, we find that: (i) certain examples are forgotten with high frequency, and some not at all; (ii) a data set's (un)forgettable examples generalize across neural architectures; and (iii) based on forgetting dynamics, a significant fraction of examples can be omitted from the training data set while still maintaining state-of-the-art generalization performance.",True,True,"Toneva, Mariya and Sordoni, Alessandro and Combes, Remi Tachet des and Trischler, Adam and Bengio, Yoshua and Gordon, Geoffrey J",2018,,,,arXiv preprint arXiv:1812.05159
ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,2511.08263v1,wang2025datawhisperer,\cite{wang2025datawhisperer},Data Whisperer: Efficient Data Selection for Task-Specific LLM Fine-Tuning via Few-Shot In-Context Learning,https://arxiv.org/abs/2505.12212v3,"Fine-tuning large language models (LLMs) on task-specific data is essential for their effective deployment. As dataset sizes grow, efficiently selecting optimal subsets for training becomes crucial to balancing performance and computational costs. Traditional data selection methods often require fine-tuning a scoring model on the target dataset, which is time-consuming and resource-intensive, or rely on heuristics that fail to fully leverage the model's predictive capabilities. To address these challenges, we propose Data Whisperer, an efficient, training-free, attention-based method that leverages few-shot in-context learning with the model to be fine-tuned. Comprehensive evaluations were conducted on both raw and synthetic datasets across diverse tasks and models. Notably, Data Whisperer achieves superior performance compared to the full GSM8K dataset on the Llama-3-8B-Instruct model, using just 10% of the data, and outperforms existing methods with a 3.1-point improvement and a 7.4$\times$ speedup. The code is available at https://github.com/gszfwsb/Data-Whisperer.",True,True,"Wang, Shaobo and Jin, Xiangqi and Wang, Ziming and Wang, Jize and Zhang, Jiajun and Li, Kaixin and Wen, Zichen and Li, Zhong and He, Conghui and Hu, Xuming and Zhang, Linfeng",2025,,,,Annual Meeting of the Association for Computational Linguistics
ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,2511.08263v1,wang2025winning,\cite{wang2025winning},Winning the pruning gamble: A unified approach to joint sample and token pruning for efficient supervised fine-tuning,,,True,False,"Wang, Shaobo and Wang, Jiaming and Zhang, Jiajun and Wang, Cong and Min, Yue and Wen, Zichen and Huang, Fei and Jiang, Huiqiang and Lin, Junyang and Liu, Dayiheng and others",2025,,,,arXiv preprint arXiv:2509.23873
ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,2511.08263v1,wang2025circuitseer,\cite{wang2025circuitseer},CircuitSeer: Mining High-Quality Data by Probing Mathematical Reasoning Circuits in LLMs,https://arxiv.org/abs/2510.18470v2,"Large language models (LLMs) have demonstrated impressive reasoning capabilities, but scaling their performance often relies on massive reasoning datasets that are computationally expensive to train on. Existing data selection methods aim to curate smaller, high-quality subsets but often rely on costly external models or opaque heuristics. In this work, we shift the focus from external heuristics to the model's internal mechanisms. We find that complex reasoning tasks consistently activate a sparse, specialized subset of attention heads, forming core reasoning circuits. Building on this insight, we propose CircuitSeer, a novel data selection method that quantifies the reasoning complexity of data by measuring its influence on these crucial circuits. Extensive experiments on 4 models and 9 datasets demonstrate CircuitSeer's superiority. Notably, fine-tuning Qwen2.5-Math-7B on just 10% of data selected by our method achieves a 1.4-point gain in average Pass@1 over training on the full dataset, highlighting its efficiency and effectiveness.",True,True,"Wang, Shaobo and Miao, Yongliang and Liu, Yuancheng and Liao, Ning and Zhang, Linfeng and others",2025,,,,arXiv preprint arXiv:2510.18470
ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,2511.08263v1,DD,\cite{DD},Dataset distillation,,,True,False,"Wang, Tongzhou and Zhu, Jun-Yan and Torralba, Antonio and Efros, Alexei A",2018,,,,arXiv preprint arXiv:1811.10959
ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,2511.08263v1,DC,\cite{DC},Dataset condensation with gradient matching,,,True,False,"Zhao, Bo and Mopuri, Konda Reddy and Bilen, Hakan",2020,,,,arXiv preprint arXiv:2006.05929
ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,2511.08263v1,DCC,\cite{DCC},Dataset condensation with contrastive signals,,,True,False,"Lee, Saehyung and Chun, Sanghyuk and Jung, Sangwon and Yun, Sangdoo and Yoon, Sungroh",2022,,,,
ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,2511.08263v1,DSA,\cite{DSA},Dataset condensation with differentiable siamese augmentation,,,True,False,"Zhao, Bo and Bilen, Hakan",2021,,,,
ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,2511.08263v1,wang2024samples,\cite{wang2024samples},Not All Samples Should Be Utilized Equally: Towards Understanding and Improving Dataset Distillation,,,True,False,"Wang, Shaobo and Yang, Yantai and Wang, Qilong and Li, Kaixin and Zhang, Linfeng and Yan, Junchi",2025,,,,Synthetic Data for Computer Vision Workshop at CVPR
ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,2511.08263v1,wang2025drupi,\cite{wang2025drupi},DRUPI: Dataset Reduction Using Privileged Information,https://arxiv.org/abs/2410.01611v2,"Dataset reduction (DR) seeks to select or distill samples from large datasets into smaller subsets while preserving performance on target tasks. Existing methods primarily focus on pruning or synthesizing data in the same format as the original dataset, typically the input data and corresponding labels. However, in DR settings, we find it is possible to synthesize more information beyond the data-label pair as an additional learning target to facilitate model training. In this paper, we introduce Dataset Reduction Using Privileged Information (DRUPI), which enriches DR by synthesizing privileged information alongside the reduced dataset. This privileged information can take the form of feature labels or attention labels, providing auxiliary supervision to improve model learning. Our findings reveal that effective feature labels must balance between being overly discriminative and excessively diverse, with a moderate level proving optimal for improving the reduced dataset's efficacy. Extensive experiments on ImageNet, CIFAR-10/100, and Tiny ImageNet demonstrate that DRUPI integrates seamlessly with existing dataset reduction methods, offering significant performance gains. *The code will be released after the paper is accepted.*",True,True,Shaobo Wang and Yantai Yang and Shuaiyu Zhang and Chenghao Sun and Weiya Li and Xuming Hu and Linfeng Zhang,2025,,,,
ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,2511.08263v1,DATM,\cite{DATM},Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory Matching,https://arxiv.org/abs/2310.05773v2,"The ultimate goal of Dataset Distillation is to synthesize a small synthetic dataset such that a model trained on this synthetic set will perform equally well as a model trained on the full, real dataset. Until now, no method of Dataset Distillation has reached this completely lossless goal, in part due to the fact that previous methods only remain effective when the total number of synthetic samples is extremely small. Since only so much information can be contained in such a small number of samples, it seems that to achieve truly loss dataset distillation, we must develop a distillation method that remains effective as the size of the synthetic dataset grows. In this work, we present such an algorithm and elucidate why existing methods fail to generate larger, high-quality synthetic sets. Current state-of-the-art methods rely on trajectory-matching, or optimizing the synthetic data to induce similar long-term training dynamics as the real data. We empirically find that the training stage of the trajectories we choose to match (i.e., early or late) greatly affects the effectiveness of the distilled dataset. Specifically, early trajectories (where the teacher network learns easy patterns) work well for a low-cardinality synthetic set since there are fewer examples wherein to distribute the necessary information. Conversely, late trajectories (where the teacher network learns hard patterns) provide better signals for larger synthetic sets since there are now enough samples to represent the necessary complex patterns. Based on our findings, we propose to align the difficulty of the generated patterns with the size of the synthetic dataset. In doing so, we successfully scale trajectory matching-based methods to larger synthetic datasets, achieving lossless dataset distillation for the very first time. Code and distilled datasets are available at https://gzyaftermath.github.io/DATM.",True,True,"Guo, Ziyao and Wang, Kai and Cazenavette, George and Li, Hui and Zhang, Kaipeng and You, Yang",2023,,,,arXiv preprint arXiv:2310.05773
ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,2511.08263v1,MTT,\cite{MTT},Dataset Distillation by Matching Training Trajectories,https://arxiv.org/abs/2203.11932v1,"Dataset distillation is the task of synthesizing a small dataset such that a model trained on the synthetic set will match the test accuracy of the model trained on the full dataset. In this paper, we propose a new formulation that optimizes our distilled data to guide networks to a similar state as those trained on real data across many training steps. Given a network, we train it for several iterations on our distilled data and optimize the distilled data with respect to the distance between the synthetically trained parameters and the parameters trained on real data. To efficiently obtain the initial and target network parameters for large-scale datasets, we pre-compute and store training trajectories of expert networks trained on the real dataset. Our method handily outperforms existing methods and also allows us to distill higher-resolution visual data.",True,True,"Cazenavette, George and Wang, Tongzhou and Torralba, Antonio and Efros, Alexei A and Zhu, Jun-Yan",2022,,,,
ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,2511.08263v1,DM,\cite{DM},Dataset Condensation with Distribution Matching,,,True,False,Bo Zhao and Hakan Bilen,2022,,,,
ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,2511.08263v1,IDM,\cite{IDM},Improved distribution matching for dataset condensation,,,True,False,"Zhao, Ganlong and Li, Guanbin and Qin, Yipeng and Yu, Yizhou",2023,,,,
ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,2511.08263v1,NCFM,\cite{NCFM},Dataset distillation with neural characteristic function: A minmax perspective,,,True,False,"Wang, Shaobo and Yang, Yicun and Liu, Zhiyuan and Sun, Chenghao and Hu, Xuming and He, Conghui and Zhang, Linfeng",2025,,,,
ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,2511.08263v1,chen2023alpagasus,\cite{chen2023alpagasus},Alpagasus: Training a better alpaca with fewer data,,,True,False,"Chen, Lichang and Li, Shiyang and Yan, Jun and Wang, Hai and Gunaratna, Kalpa and Yadav, Vikas and Tang, Zheng and Srinivasan, Vijay and Zhou, Tianyi and Huang, Heng and others",2023,,,,arXiv preprint arXiv:2307.08701
ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,2511.08263v1,liu2023makes,\cite{liu2023makes},What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning,,,True,False,"Liu, Wei and Zeng, Weihao and He, Keqing and Jiang, Yong and He, Junxian",2023,,,,arXiv preprint arXiv:2312.15685
ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,2511.08263v1,xu2023rethinking,\cite{xu2023rethinking},Rethinking the instruction quality: Lift is what you need,,,True,False,"Xu, Yang and Yao, Yongqiang and Huang, Yufan and Qi, Mengnan and Wang, Maoquan and Gu, Bin and Sundaresan, Neel",2023,,,,arXiv preprint arXiv:2312.11508
ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,2511.08263v1,attendu2023nlu,\cite{attendu2023nlu},NLU on Data Diets: Dynamic Data Subset Selection for NLP Classification Tasks,https://arxiv.org/abs/2306.03208v1,"Finetuning large language models inflates the costs of NLU applications and remains the bottleneck of development cycles. Recent works in computer vision use data pruning to reduce training time. Pruned data selection with static methods is based on a score calculated for each training example prior to finetuning, which involves important computational overhead. Moreover, the score may not necessarily be representative of sample importance throughout the entire training duration. We propose to address these issues with a refined version of dynamic data pruning, a curriculum which periodically scores and discards unimportant examples during finetuning. Our method leverages an EL2N metric that we extend to the joint intent and slot classification task, and an initial finetuning phase on the full train set. Our results on the GLUE benchmark and four joint NLU datasets show a better time-accuracy trade-off compared to static methods. Our method preserves full accuracy while training on 50% of the data points and reduces computational times by up to 41%. If we tolerate instead a minor drop of accuracy of 1%, we can prune 80% of the training examples for a reduction in finetuning time reaching 66%.",True,True,"Attendu, Jean-Michel and Corbeil, Jean-Philippe",2023,,,,arXiv preprint arXiv:2306.03208
ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,2511.08263v1,kung2023active,\cite{kung2023active},Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks,https://arxiv.org/abs/2311.00288v1,"Instruction tuning (IT) achieves impressive zero-shot generalization results by training large language models (LLMs) on a massive amount of diverse tasks with instructions. However, how to select new tasks to improve the performance and generalizability of IT models remains an open question. Training on all existing tasks is impractical due to prohibiting computation requirements, and randomly selecting tasks can lead to suboptimal performance. In this work, we propose active instruction tuning based on prompt uncertainty, a novel framework to identify informative tasks, and then actively tune the models on the selected tasks. We represent the informativeness of new tasks with the disagreement of the current model outputs over perturbed prompts. Our experiments on NIV2 and Self-Instruct datasets demonstrate that our method consistently outperforms other baseline strategies for task selection, achieving better out-of-distribution generalization with fewer training tasks. Additionally, we introduce a task map that categorizes and diagnoses tasks based on prompt uncertainty and prediction probability. We discover that training on ambiguous (prompt-uncertain) tasks improves generalization while training on difficult (prompt-certain and low-probability) tasks offers no benefit, underscoring the importance of task selection for instruction tuning.",True,True,"Kung, Po-Nien and Yin, Fan and Wu, Di and Chang, Kai-Wei and Peng, Nanyun",2023,,,,arXiv preprint arXiv:2311.00288
ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,2511.08263v1,kushwaha2024audio,\cite{kushwaha2024audio},Audio-Visual Dataset Distillation,,,True,False,"Kushwaha, Saksham Singh and Vasireddy, Siva Sai Nagender and Wang, Kai and Tian, Yapeng",2024,,,,Transactions on Machine Learning Research
ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,2511.08263v1,xu2024low,\cite{xu2024low},Low-rank similarity mining for multimodal dataset distillation,,,True,False,"Xu, Yue and Lin, Zhilin and Qiu, Yusong and Lu, Cewu and Li, Yong-Lu",2024,,,,arXiv preprint arXiv:2406.03793
ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation,2511.08263v1,zhang2025beyond,\cite{zhang2025beyond},Beyond Modality Collapse: Representations Blending for Multimodal Dataset Distillation,https://arxiv.org/abs/2505.14705v1,"Multimodal Dataset Distillation (MDD) seeks to condense large-scale image-text datasets into compact surrogates while retaining their effectiveness for cross-modal learning. Despite recent progress, existing MDD approaches often suffer from \textit{\textbf{Modality Collapse}}, characterized by over-concentrated intra-modal representations and enlarged distributional gap across modalities. In this paper, at the first time, we identify this issue as stemming from a fundamental conflict between the over-compression behavior inherent in dataset distillation and the cross-modal supervision imposed by contrastive objectives. To alleviate modality collapse, we introduce \textbf{RepBlend}, a novel MDD framework that weakens overdominant cross-modal supervision via representation blending, thereby significantly enhancing intra-modal diversity. Additionally, we observe that current MDD methods impose asymmetric supervision across modalities, resulting in biased optimization. To address this, we propose symmetric projection trajectory matching, which synchronizes the optimization dynamics using modality-specific projection heads, thereby promoting balanced supervision and enhancing cross-modal alignment. Experiments on Flickr-30K and MS-COCO show that RepBlend consistently outperforms prior state-of-the-art MDD methods, achieving significant gains in retrieval performance (e.g., +9.4 IR@10, +6.3 TR@10 under the 100-pair setting) and offering up to 6.7$\times$ distillation speedup.",True,True,"Zhang, Xin and Zhang, Ziruo and Du, Jiawei and Liu, Zuozhu and Zhou, Joey Tianyi",2025,,,,arXiv preprint arXiv:2505.14705
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,Pointrcnn,\cite{Pointrcnn},Pointrcnn: 3d object proposal generation and detection from point cloud,,,True,False,"Shi, Shaoshuai and Wang, Xiaogang and Li, Hongsheng",2019,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,Pointpillars,\cite{Pointpillars},Pointpillars: Fast encoders for object detection from point clouds,,,True,False,"Lang, Alex H and Vora, Sourabh and Caesar, Holger and Zhou, Lubing and Yang, Jiong and Beijbom, Oscar",2019,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,deng2021multi_H23DRCNN,\cite{deng2021multi_H23DRCNN},From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection,https://arxiv.org/abs/2107.14391v1,"As an emerging data modal with precise distance sensing, LiDAR point clouds have been placed great expectations on 3D scene understanding. However, point clouds are always sparsely distributed in the 3D space, and with unstructured storage, which makes it difficult to represent them for effective 3D object detection. To this end, in this work, we regard point clouds as hollow-3D data and propose a new architecture, namely Hallucinated Hollow-3D R-CNN ($\text{H}^2$3D R-CNN), to address the problem of 3D object detection. In our approach, we first extract the multi-view features by sequentially projecting the point clouds into the perspective view and the bird-eye view. Then, we hallucinate the 3D representation by a novel bilaterally guided multi-view fusion block. Finally, the 3D objects are detected via a box refinement module with a novel Hierarchical Voxel RoI Pooling operation. The proposed $\text{H}^2$3D R-CNN provides a new angle to take full advantage of complementary information in the perspective view and the bird-eye view with an efficient framework. We evaluate our approach on the public KITTI Dataset and Waymo Open Dataset. Extensive experiments demonstrate the superiority of our method over the state-of-the-art algorithms with respect to both effectiveness and efficiency. The code will be made available at \url{https://github.com/djiajunustc/H-23D_R-CNN}.",True,True,"Deng, Jiajun and Zhou, Wengang and Zhang, Yanyong and Li, Houqiang",2021,,,,IEEE Transactions on Circuits and Systems for Video Technology
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,RT3D,\cite{RT3D},RT3D: Real-Time 3-D Vehicle Detection in LiDAR Point Cloud for Autonomous Driving,,,True,False,"Zeng, Yiming and Hu, Yu and Liu, Shice and Ye, Jing and Han, Yinhe and Li, Xiaowei and Sun, Ninghui",2018,Oct,,10.1109/lra.2018.2852843,IEEE Robotics and Automation Letters
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,Voxelnet,\cite{Voxelnet},Voxelnet: End-to-end learning for point cloud based 3d object detection,,,True,False,"Zhou, Yin and Tuzel, Oncel",2018,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,wang2023auto-points,\cite{wang2023auto-points},Auto-points: Automatic learning for point cloud analysis with neural architecture search,,,True,False,"Wang, Li and Xie, Tao and Zhang, Xinyu and Jiang, Zhiqiang and Yang, Linqi and Zhang, Haoming and Li, Xiaoyu and Ren, Yilong and Yu, Haiyang and Li, Jun and others",2023,,,,IEEE Transactions on Multimedia
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,Cia-ssd,\cite{Cia-ssd},CIA-SSD: Confident IoU-Aware Single-Stage Object Detector From Point Cloud,https://arxiv.org/abs/2012.03015v1,"Existing single-stage detectors for locating objects in point clouds often treat object localization and category classification as separate tasks, so the localization accuracy and classification confidence may not well align. To address this issue, we present a new single-stage detector named the Confident IoU-Aware Single-Stage object Detector (CIA-SSD). First, we design the lightweight Spatial-Semantic Feature Aggregation module to adaptively fuse high-level abstract semantic features and low-level spatial features for accurate predictions of bounding boxes and classification confidence. Also, the predicted confidence is further rectified with our designed IoU-aware confidence rectification module to make the confidence more consistent with the localization accuracy. Based on the rectified confidence, we further formulate the Distance-variant IoU-weighted NMS to obtain smoother regressions and avoid redundant predictions. We experiment CIA-SSD on 3D car detection in the KITTI test set and show that it attains top performance in terms of the official ranking metric (moderate AP 80.28%) and above 32 FPS inference speed, outperforming all prior single-stage detectors. The code is available at https://github.com/Vegeta2020/CIA-SSD.",True,True,"Zheng, Wu and Tang, Weiliang and Chen, Sijin and Jiang, Li and Fu, Chi-Wing",2021,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,SSN,\cite{SSN},Ssn: Shape signature networks for multi-class object detection from point clouds,,,True,False,"Zhu, Xinge and Ma, Yuexin and Wang, Tai and Xu, Yan and Shi, Jianping and Lin, Dahua",2020,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,Centerpoint,\cite{Centerpoint},Center-based 3d object detection and tracking,,,True,False,"Yin, Tianwei and Zhou, Xingyi and Krahenbuhl, Philipp",2021,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,afdet,\cite{afdet},AFDet: Anchor Free One Stage 3D Object Detection,https://arxiv.org/abs/2006.12671v2,"High-efficiency point cloud 3D object detection operated on embedded systems is important for many robotics applications including autonomous driving. Most previous works try to solve it using anchor-based detection methods which come with two drawbacks: post-processing is relatively complex and computationally expensive; tuning anchor parameters is tricky. We are the first to address these drawbacks with an anchor free and Non-Maximum Suppression free one stage detector called AFDet. The entire AFDet can be processed efficiently on a CNN accelerator or a GPU with the simplified post-processing. Without bells and whistles, our proposed AFDet performs competitively with other one stage anchor-based methods on KITTI validation set and Waymo Open Dataset validation set.",True,True,"Ge, Runzhou and Ding, Zhuangzhuang and Hu, Yihan and Wang, Yu and Chen, Sijia and Huang, Li and Li, Yuan",2020,,,,arXiv preprint arXiv:2006.12671
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,AFDetV2,\cite{AFDetV2},AFDetV2: Rethinking the Necessity of the Second Stage for Object Detection from Point Clouds,,,True,False,"Hu, Yihan and Ding, Zhuangzhuang and Ge, Runzhou and Shao, Wenxin and Huang, Li and Li, Kun and Liu, Qiang",2022,Jul,,10.1609/aaai.v36i1.19980,Proceedings of the AAAI Conference on Artificial Intelligence
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,fan2023hcpvf,\cite{fan2023hcpvf},Hcpvf: Hierarchical cascaded point-voxel fusion for 3D object detection,,,True,False,"Fan, Baojie and Zhang, Kexin and Tian, Jiandong",2023,,,,IEEE Transactions on Circuits and Systems for Video Technology
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,chen2020objecthotspots,\cite{chen2020objecthotspots},Object as Hotspots: An Anchor-Free 3D Object Detection Approach via Firing of Hotspots,https://arxiv.org/abs/1912.12791v3,"Accurate 3D object detection in LiDAR based point clouds suffers from the challenges of data sparsity and irregularities. Existing methods strive to organize the points regularly, e.g. voxelize, pass them through a designed 2D/3D neural network, and then define object-level anchors that predict offsets of 3D bounding boxes using collective evidences from all the points on the objects of interest. Contrary to the state-of-the-art anchor-based methods, based on the very nature of data sparsity, we observe that even points on an individual object part are informative about semantic information of the object. We thus argue in this paper for an approach opposite to existing methods using object-level anchors. Inspired by compositional models, which represent an object as parts and their spatial relations, we propose to represent an object as composition of its interior non-empty voxels, termed hotspots, and the spatial relations of hotspots. This gives rise to the representation of Object as Hotspots (OHS). Based on OHS, we further propose an anchor-free detection head with a novel ground truth assignment strategy that deals with inter-object point-sparsity imbalance to prevent the network from biasing towards objects with more points. Experimental results show that our proposed method works remarkably well on objects with a small number of points. Notably, our approach ranked 1st on KITTI 3D Detection Benchmark for cyclist and pedestrian detection, and achieved state-of-the-art performance on NuScenes 3D Detection Benchmark.",True,True,"Chen, Qi and Sun, Lin and Wang, Zhixin and Jia, Kui and Yuille, Alan",2020,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,peri2023towardsLT3D,\cite{peri2023towardsLT3D},Towards Long-Tailed 3D Detection,https://arxiv.org/abs/2211.08691v2,"Contemporary autonomous vehicle (AV) benchmarks have advanced techniques for training 3D detectors, particularly on large-scale lidar data. Surprisingly, although semantic class labels naturally follow a long-tailed distribution, contemporary benchmarks focus on only a few common classes (e.g., pedestrian and car) and neglect many rare classes in-the-tail (e.g., debris and stroller). However, AVs must still detect rare classes to ensure safe operation. Moreover, semantic classes are often organized within a hierarchy, e.g., tail classes such as child and construction-worker are arguably subclasses of pedestrian. However, such hierarchical relationships are often ignored, which may lead to misleading estimates of performance and missed opportunities for algorithmic innovation. We address these challenges by formally studying the problem of Long-Tailed 3D Detection (LT3D), which evaluates on all classes, including those in-the-tail. We evaluate and innovate upon popular 3D detection codebases, such as CenterPoint and PointPillars, adapting them for LT3D. We develop hierarchical losses that promote feature sharing across common-vs-rare classes, as well as improved detection metrics that award partial credit to ""reasonable"" mistakes respecting the hierarchy (e.g., mistaking a child for an adult). Finally, we point out that fine-grained tail class accuracy is particularly improved via multimodal fusion of RGB images with LiDAR; simply put, small fine-grained classes are challenging to identify from sparse (lidar) geometry alone, suggesting that multimodal cues are crucial to long-tailed 3D detection. Our modifications improve accuracy by 5% AP on average for all classes, and dramatically improve AP for rare classes (e.g., stroller AP improves from 3.6 to 31.6)! Our code is available at https://github.com/neeharperi/LT3D",True,True,"Peri, Neehar and Dave, Achal and Ramanan, Deva and Kong, Shu",2023,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,khan2024lidar,\cite{khan2024lidar},"LiDAR in Connected and Autonomous Vehicles-Perception, Threat Model, and Defense",,,True,False,"Khan, Muhammad Asif and Menouar, Hamid and Abdallah, Mohamed and Abu-Dayya, Adnan",2024,,,,IEEE Transactions on Intelligent Vehicles
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,liu2020smoke,\cite{liu2020smoke},SMOKE: Single-Stage Monocular 3D Object Detection via Keypoint Estimation,https://arxiv.org/abs/2002.10111v1,"Estimating 3D orientation and translation of objects is essential for infrastructure-less autonomous navigation and driving. In case of monocular vision, successful methods have been mainly based on two ingredients: (i) a network generating 2D region proposals, (ii) a R-CNN structure predicting 3D object pose by utilizing the acquired regions of interest. We argue that the 2D detection network is redundant and introduces non-negligible noise for 3D detection. Hence, we propose a novel 3D object detection method, named SMOKE, in this paper that predicts a 3D bounding box for each detected object by combining a single keypoint estimate with regressed 3D variables. As a second contribution, we propose a multi-step disentangling approach for constructing the 3D bounding box, which significantly improves both training convergence and detection accuracy. In contrast to previous 3D detection techniques, our method does not require complicated pre/post-processing, extra data, and a refinement stage. Despite of its structural simplicity, our proposed SMOKE network outperforms all existing monocular 3D detection methods on the KITTI dataset, giving the best state-of-the-art result on both 3D object detection and Bird's eye view evaluation. The code will be made publicly available.",True,True,"Liu, Zechen and Wu, Zizhang and T{\'o}th, Roland",2020,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,wang2021fcos3d,\cite{wang2021fcos3d},FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection,https://arxiv.org/abs/2104.10956v3,"Monocular 3D object detection is an important task for autonomous driving considering its advantage of low cost. It is much more challenging than conventional 2D cases due to its inherent ill-posed property, which is mainly reflected in the lack of depth information. Recent progress on 2D detection offers opportunities to better solving this problem. However, it is non-trivial to make a general adapted 2D detector work in this 3D task. In this paper, we study this problem with a practice built on a fully convolutional single-stage detector and propose a general framework FCOS3D. Specifically, we first transform the commonly defined 7-DoF 3D targets to the image domain and decouple them as 2D and 3D attributes. Then the objects are distributed to different feature levels with consideration of their 2D scales and assigned only according to the projected 3D-center for the training procedure. Furthermore, the center-ness is redefined with a 2D Gaussian distribution based on the 3D-center to fit the 3D target formulation. All of these make this framework simple yet effective, getting rid of any 2D detection or 2D-3D correspondence priors. Our solution achieves 1st place out of all the vision-only methods in the nuScenes 3D detection challenge of NeurIPS 2020. Code and models are released at https://github.com/open-mmlab/mmdetection3d.",True,True,"Wang, Tai and Zhu, Xinge and Pang, Jiangmiao and Lin, Dahua",2021,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,CaDDN,\cite{CaDDN},Categorical depth distribution network for monocular 3d object detection,,,True,False,"Reading, Cody and Harakeh, Ali and Chae, Julia and Waslander, Steven L",2021,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,zhang2021objects,\cite{zhang2021objects},Objects are different: Flexible monocular 3d object detection,,,True,False,"Zhang, Yunpeng and Lu, Jiwen and Zhou, Jie",2021,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,ma2023long_LT3D_2dlatefusion,\cite{ma2023long_LT3D_2dlatefusion},Long-tailed 3d detection via 2d late fusion,,,True,False,"Ma, Yechi and Peri, Neehar and Wei, Shuoquan and Hua, Wei and Ramanan, Deva and Li, Yanan and Kong, Shu",2023,,,,arXiv preprint arXiv:2312.10986
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,kim2025labeldistill,\cite{kim2025labeldistill},LabelDistill: Label-guided Cross-modal Knowledge Distillation for Camera-based 3D Object Detection,https://arxiv.org/abs/2407.10164v1,"Recent advancements in camera-based 3D object detection have introduced cross-modal knowledge distillation to bridge the performance gap with LiDAR 3D detectors, leveraging the precise geometric information in LiDAR point clouds. However, existing cross-modal knowledge distillation methods tend to overlook the inherent imperfections of LiDAR, such as the ambiguity of measurements on distant or occluded objects, which should not be transferred to the image detector. To mitigate these imperfections in LiDAR teacher, we propose a novel method that leverages aleatoric uncertainty-free features from ground truth labels. In contrast to conventional label guidance approaches, we approximate the inverse function of the teacher's head to effectively embed label inputs into feature space. This approach provides additional accurate guidance alongside LiDAR teacher, thereby boosting the performance of the image detector. Additionally, we introduce feature partitioning, which effectively transfers knowledge from the teacher modality while preserving the distinctive features of the student, thereby maximizing the potential of both modalities. Experimental results demonstrate that our approach improves mAP and NDS by 5.1 points and 4.9 points compared to the baseline model, proving the effectiveness of our approach. The code is available at https://github.com/sanmin0312/LabelDistill",True,True,"Kim, Sanmin and Kim, Youngseok and Hwang, Sihwan and Jeong, Hyeonjun and Kum, Dongsuk",2025,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,ji2024enhancing,\cite{ji2024enhancing},Enhancing 3D object detection with 2D detection-guided query anchors,,,True,False,"Ji, Haoxuanye and Liang, Pengpeng and Cheng, Erkang",2024,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,yang2023lite,\cite{yang2023lite},Lite-fpn for keypoint-based monocular 3d object detection,,,True,False,"Yang, Lei and Zhang, Xinyu and Li, Jun and Wang, Li and Zhu, Minghan and Zhu, Lei",2023,,,,Knowledge-Based Systems
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,park2024odd,\cite{park2024odd},ODD-M3D: Object-Wise Dense Depth Estimation for Monocular 3D Object Detection,,,True,False,"Park, Chanyeong and Kim, Heegwang and Jang, Junbo and Paik, Joonki",2024,,,,IEEE Transactions on Consumer Electronics
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,li2024unimode,\cite{li2024unimode},UniMODE: Unified Monocular 3D Object Detection,,,True,False,"Li, Zhuoling and Xu, Xiaogang and Lim, SerNam and Zhao, Hengshuang",2024,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,brazil2023omni3d,\cite{brazil2023omni3d},Omni3d: A large benchmark and model for 3d object detection in the wild,,,True,False,"Brazil, Garrick and Kumar, Abhinav and Straub, Julian and Ravi, Nikhila and Johnson, Justin and Gkioxari, Georgia",2023,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,FSD,\cite{FSD},Fully Sparse 3D Object Detection,https://arxiv.org/abs/2207.10035v2,"As the perception range of LiDAR increases, LiDAR-based 3D object detection becomes a dominant task in the long-range perception task of autonomous driving. The mainstream 3D object detectors usually build dense feature maps in the network backbone and prediction head. However, the computational and spatial costs on the dense feature map are quadratic to the perception range, which makes them hardly scale up to the long-range setting. To enable efficient long-range LiDAR-based object detection, we build a fully sparse 3D object detector (FSD). The computational and spatial cost of FSD is roughly linear to the number of points and independent of the perception range. FSD is built upon the general sparse voxel encoder and a novel sparse instance recognition (SIR) module. SIR first groups the points into instances and then applies instance-wise feature extraction and prediction. In this way, SIR resolves the issue of center feature missing, which hinders the design of the fully sparse architecture for all center-based or anchor-based detectors. Moreover, SIR avoids the time-consuming neighbor queries in previous point-based methods by grouping points into instances. We conduct extensive experiments on the large-scale Waymo Open Dataset to reveal the working mechanism of FSD, and state-of-the-art performance is reported. To demonstrate the superiority of FSD in long-range detection, we also conduct experiments on Argoverse 2 Dataset, which has a much larger perception range ($200m$) than Waymo Open Dataset ($75m$). On such a large perception range, FSD achieves state-of-the-art performance and is 2.4$\times$ faster than the dense counterpart. Codes will be released at https://github.com/TuSimple/SST.",True,True,"Fan, Lue and Wang, Feng and Wang, Naiyan and ZHANG, ZHAO-XIANG",2022,,,,Advances in Neural Information Processing Systems
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,FSDV2,\cite{FSDV2},FSD V2: Improving Fully Sparse 3D Object Detection with Virtual Voxels,https://arxiv.org/abs/2308.03755v1,"LiDAR-based fully sparse architecture has garnered increasing attention. FSDv1 stands out as a representative work, achieving impressive efficacy and efficiency, albeit with intricate structures and handcrafted designs. In this paper, we present FSDv2, an evolution that aims to simplify the previous FSDv1 while eliminating the inductive bias introduced by its handcrafted instance-level representation, thus promoting better general applicability. To this end, we introduce the concept of \textbf{virtual voxels}, which takes over the clustering-based instance segmentation in FSDv1. Virtual voxels not only address the notorious issue of the Center Feature Missing problem in fully sparse detectors but also endow the framework with a more elegant and streamlined approach. Consequently, we develop a suite of components to complement the virtual voxel concept, including a virtual voxel encoder, a virtual voxel mixer, and a virtual voxel assignment strategy. Through empirical validation, we demonstrate that the virtual voxel mechanism is functionally similar to the handcrafted clustering in FSDv1 while being more general. We conduct experiments on three large-scale datasets: Waymo Open Dataset, Argoverse 2 dataset, and nuScenes dataset. Our results showcase state-of-the-art performance on all three datasets, highlighting the superiority of FSDv2 in long-range scenarios and its general applicability to achieve competitive performance across diverse scenarios. Moreover, we provide comprehensive experimental analysis to elucidate the workings of FSDv2. To foster reproducibility and further research, we have open-sourced FSDv2 at https://github.com/tusen-ai/SST.",True,True,"Fan, Lue and Wang, Feng and Wang, Naiyan and Zhang, Zhaoxiang",2023,,,,arXiv preprint arXiv:2308.03755
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,yang2023mixteaching,\cite{yang2023mixteaching},"Mix-teaching: A simple, unified and effective semi-supervised learning framework for monocular 3d object detection",,,True,False,"Yang, Lei and Zhang, Xinyu and Li, Jun and Wang, Li and Zhu, Minghan and Zhang, Chuang and Liu, Huaping",2023,,,,IEEE Transactions on Circuits and Systems for Video Technology
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,sheng2023pdr,\cite{sheng2023pdr},PDR: Progressive Depth Regularization for Monocular 3D Object Detection,,,True,False,"Sheng, Hualian and Cai, Sijia and Zhao, Na and Deng, Bing and Zhao, Min-Jian and Lee, Gim Hee",2023,,,,IEEE Transactions on Circuits and Systems for Video Technology
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,huang2021b,\cite{huang2021b},B-splines for purely vision-based localization and mapping on non-holonomic ground vehicles,,,True,False,"Huang, Kun and Wang, Yifu and Kneip, Laurent",2021,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,jiang2024far3d,\cite{jiang2024far3d},Far3d: Expanding the horizon for surround-view 3d object detection,,,True,False,"Jiang, Xiaohui and Li, Shuailin and Liu, Yingfei and Wang, Shihao and Jia, Fan and Wang, Tiancai and Han, Lijin and Zhang, Xiangyu",2024,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,bevdepth,\cite{bevdepth},{Bevdepth: Acquisition of reliable depth for multi-view 3d object detection},,,True,False,"Li, Yinhao and Ge, Zheng and Yu, Guanyi and Yang, Jinrong and Wang, Zengran and Shi, Yukang and Sun, Jianjian and Li, Zeming",2023,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,xu2021fusionpainting,\cite{xu2021fusionpainting},Fusionpainting: Multimodal fusion with adaptive attention for 3d object detection,,,True,False,"Xu, Shaoqing and Zhou, Dingfu and Fang, Jin and Yin, Junbo and Bin, Zhou and Zhang, Liangjun",2021,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,xu2024multi,\cite{xu2024multi},Multi-Sem Fusion: Multimodal Semantic Fusion for 3D Object Detection,https://arxiv.org/abs/2212.05265v2,"LiDAR and camera fusion techniques are promising for achieving 3D object detection in autonomous driving. Most multi-modal 3D object detection frameworks integrate semantic knowledge from 2D images into 3D LiDAR point clouds to enhance detection accuracy. Nevertheless, the restricted resolution of 2D feature maps impedes accurate re-projection and often induces a pronounced boundary-blurring effect, which is primarily attributed to erroneous semantic segmentation. To well handle this limitation, we propose a general multi-modal fusion framework Multi-Sem Fusion (MSF) to fuse the semantic information from both the 2D image and 3D points scene parsing results. Specifically, we employ 2D/3D semantic segmentation methods to generate the parsing results for 2D images and 3D point clouds. The 2D semantic information is further reprojected into the 3D point clouds with calibration parameters. To handle the misalignment between the 2D and 3D parsing results, we propose an Adaptive Attention-based Fusion (AAF) module to fuse them by learning an adaptive fusion score. Then the point cloud with the fused semantic label is sent to the following 3D object detectors. Furthermore, we propose a Deep Feature Fusion (DFF) module to aggregate deep features at different levels to boost the final detection performance. The effectiveness of the framework has been verified on two public large-scale 3D object detection benchmarks by comparing them with different baselines. The experimental results show that the proposed fusion strategies can significantly improve the detection performance compared to the methods using only point clouds and the methods using only 2D semantic information. Most importantly, the proposed approach significantly outperforms other approaches and sets state-of-the-art results on the nuScenes testing benchmark.",True,True,"Xu, Shaoqing and Li, Fang and Song, Ziying and Fang, Jin and Wang, Sifen and Yang, Zhi-Xin",2024,,,,IEEE Transactions on Geoscience and Remote Sensing
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,bevfusion-mit,\cite{bevfusion-mit},BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation,https://arxiv.org/abs/2205.13542v3,"Multi-sensor fusion is essential for an accurate and reliable autonomous driving system. Recent approaches are based on point-level fusion: augmenting the LiDAR point cloud with camera features. However, the camera-to-LiDAR projection throws away the semantic density of camera features, hindering the effectiveness of such methods, especially for semantic-oriented tasks (such as 3D scene segmentation). In this paper, we break this deeply-rooted convention with BEVFusion, an efficient and generic multi-task multi-sensor fusion framework. It unifies multi-modal features in the shared bird's-eye view (BEV) representation space, which nicely preserves both geometric and semantic information. To achieve this, we diagnose and lift key efficiency bottlenecks in the view transformation with optimized BEV pooling, reducing latency by more than 40x. BEVFusion is fundamentally task-agnostic and seamlessly supports different 3D perception tasks with almost no architectural changes. It establishes the new state of the art on nuScenes, achieving 1.3% higher mAP and NDS on 3D object detection and 13.6% higher mIoU on BEV map segmentation, with 1.9x lower computation cost. Code to reproduce our results is available at https://github.com/mit-han-lab/bevfusion.",True,True,"Liu, Zhijian and Tang, Haotian and Amini, Alexander and Yang, Xinyu and Mao, Huizi and Rus, Daniela L and Han, Song",2023,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,bevfusion-pku,\cite{bevfusion-pku},Bevfusion: A simple and robust lidar-camera fusion framework,,,True,False,"Liang, Tingting and Xie, Hongwei and Yu, Kaicheng and Xia, Zhongyu and Lin, Zhiwei and Wang, Yongtao and Tang, Tao and Wang, Bing and Tang, Zhi",2022,,,,Advances in Neural Information Processing Systems
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,song2024graphbev,\cite{song2024graphbev},GraphBEV: Towards Robust BEV Feature Alignment for Multi-Modal 3D Object Detection,https://arxiv.org/abs/2403.11848v4,"Integrating LiDAR and camera information into Bird's-Eye-View (BEV) representation has emerged as a crucial aspect of 3D object detection in autonomous driving. However, existing methods are susceptible to the inaccurate calibration relationship between LiDAR and the camera sensor. Such inaccuracies result in errors in depth estimation for the camera branch, ultimately causing misalignment between LiDAR and camera BEV features. In this work, we propose a robust fusion framework called Graph BEV. Addressing errors caused by inaccurate point cloud projection, we introduce a Local Align module that employs neighbor-aware depth features via Graph matching. Additionally, we propose a Global Align module to rectify the misalignment between LiDAR and camera BEV features. Our Graph BEV framework achieves state-of-the-art performance, with an mAP of 70.1\%, surpassing BEV Fusion by 1.6\% on the nuscenes validation set. Importantly, our Graph BEV outperforms BEV Fusion by 8.3\% under conditions with misalignment noise.",True,True,"Song, Ziying and Yang, Lei and Xu, Shaoqing and Liu, Lin and Xu, Dongyang and Jia, Caiyan and Jia, Feiyang and Wang, Li",2024,,,,arXiv preprint arXiv:2403.11848
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,song2024robofusion,\cite{song2024robofusion},RoboFusion: Towards Robust Multi-Modal 3D Object Detection via SAM,https://arxiv.org/abs/2401.03907v4,"Multi-modal 3D object detectors are dedicated to exploring secure and reliable perception systems for autonomous driving (AD).Although achieving state-of-the-art (SOTA) performance on clean benchmark datasets, they tend to overlook the complexity and harsh conditions of real-world environments. With the emergence of visual foundation models (VFMs), opportunities and challenges are presented for improving the robustness and generalization of multi-modal 3D object detection in AD. Therefore, we propose RoboFusion, a robust framework that leverages VFMs like SAM to tackle out-of-distribution (OOD) noise scenarios. We first adapt the original SAM for AD scenarios named SAM-AD. To align SAM or SAM-AD with multi-modal methods, we then introduce AD-FPN for upsampling the image features extracted by SAM. We employ wavelet decomposition to denoise the depth-guided images for further noise reduction and weather interference. At last, we employ self-attention mechanisms to adaptively reweight the fused features, enhancing informative features while suppressing excess noise. In summary, RoboFusion significantly reduces noise by leveraging the generalization and robustness of VFMs, thereby enhancing the resilience of multi-modal 3D object detection. Consequently, RoboFusion achieves SOTA performance in noisy scenarios, as demonstrated by the KITTI-C and nuScenes-C benchmarks. Code is available at https://github.com/adept-thu/RoboFusion.",True,True,"Song, Ziying and Zhang, Guoxing and Liu, Lin and Yang, Lei and Xu, Shaoqing and Jia, Caiyan and Jia, Feiyang and Wang, Li",2024,,,,arXiv preprint arXiv:2401.03907
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,graphalign,\cite{graphalign},GraphAlign: Enhancing Accurate Feature Alignment by Graph matching for Multi-Modal 3D Object Detection,https://arxiv.org/abs/2310.08261v1,"LiDAR and cameras are complementary sensors for 3D object detection in autonomous driving. However, it is challenging to explore the unnatural interaction between point clouds and images, and the critical factor is how to conduct feature alignment of heterogeneous modalities. Currently, many methods achieve feature alignment by projection calibration only, without considering the problem of coordinate conversion accuracy errors between sensors, leading to sub-optimal performance. In this paper, we present GraphAlign, a more accurate feature alignment strategy for 3D object detection by graph matching. Specifically, we fuse image features from a semantic segmentation encoder in the image branch and point cloud features from a 3D Sparse CNN in the LiDAR branch. To save computation, we construct the nearest neighbor relationship by calculating Euclidean distance within the subspaces that are divided into the point cloud features. Through the projection calibration between the image and point cloud, we project the nearest neighbors of point cloud features onto the image features. Then by matching the nearest neighbors with a single point cloud to multiple images, we search for a more appropriate feature alignment. In addition, we provide a self-attention module to enhance the weights of significant relations to fine-tune the feature alignment between heterogeneous modalities. Extensive experiments on nuScenes benchmark demonstrate the effectiveness and efficiency of our GraphAlign.",True,True,"Song, Ziying and Wei, Haiyue and Bai, Lin and Yang, Lei and Jia, Caiyan",2023,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,graphalign++,\cite{graphalign++},GraphAlign: Enhancing Accurate Feature Alignment by Graph matching for Multi-Modal 3D Object Detection,https://arxiv.org/abs/2310.08261v1,"LiDAR and cameras are complementary sensors for 3D object detection in autonomous driving. However, it is challenging to explore the unnatural interaction between point clouds and images, and the critical factor is how to conduct feature alignment of heterogeneous modalities. Currently, many methods achieve feature alignment by projection calibration only, without considering the problem of coordinate conversion accuracy errors between sensors, leading to sub-optimal performance. In this paper, we present GraphAlign, a more accurate feature alignment strategy for 3D object detection by graph matching. Specifically, we fuse image features from a semantic segmentation encoder in the image branch and point cloud features from a 3D Sparse CNN in the LiDAR branch. To save computation, we construct the nearest neighbor relationship by calculating Euclidean distance within the subspaces that are divided into the point cloud features. Through the projection calibration between the image and point cloud, we project the nearest neighbors of point cloud features onto the image features. Then by matching the nearest neighbors with a single point cloud to multiple images, we search for a more appropriate feature alignment. In addition, we provide a self-attention module to enhance the weights of significant relations to fine-tune the feature alignment between heterogeneous modalities. Extensive experiments on nuScenes benchmark demonstrate the effectiveness and efficiency of our GraphAlign.",True,True,"Song, Ziying and Jia, Caiyan and Yang, Lei and Wei, Haiyue and Liu, Lin",2023,,,,IEEE Transactions on Circuits and Systems for Video Technology
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,song2024robustness,\cite{song2024robustness},Robustness-Aware 3D Object Detection in Autonomous Driving: A Review and Outlook,https://arxiv.org/abs/2401.06542v3,"In the realm of modern autonomous driving, the perception system is indispensable for accurately assessing the state of the surrounding environment, thereby enabling informed prediction and planning. The key step to this system is related to 3D object detection that utilizes vehicle-mounted sensors such as LiDAR and cameras to identify the size, the category, and the location of nearby objects. Despite the surge in 3D object detection methods aimed at enhancing detection precision and efficiency, there is a gap in the literature that systematically examines their resilience against environmental variations, noise, and weather changes. This study emphasizes the importance of robustness, alongside accuracy and latency, in evaluating perception systems under practical scenarios. Our work presents an extensive survey of camera-only, LiDAR-only, and multi-modal 3D object detection algorithms, thoroughly evaluating their trade-off between accuracy, latency, and robustness, particularly on datasets like KITTI-C and nuScenes-C to ensure fair comparisons. Among these, multi-modal 3D detection approaches exhibit superior robustness, and a novel taxonomy is introduced to reorganize the literature for enhanced clarity. This survey aims to offer a more practical perspective on the current capabilities and the constraints of 3D object detection algorithms in real-world applications, thus steering future research towards robustness-centric advancements.",True,True,"Song, Ziying and Liu, Lin and Jia, Feiyang and Luo, Yadan and Jia, Caiyan and Zhang, Guoxin and Yang, Lei and Wang, Li",2024,,,,IEEE Transactions on Intelligent Transportation Systems
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,ObjectFusion,\cite{ObjectFusion},ObjectFusion: Multi-modal 3D Object Detection with Object-Centric Fusion,,,True,False,"Cai, Qi and Pan, Yingwei and Yao, Ting and Ngo, Chong-Wah and Mei, Tao",2023,October,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,chen2023focalformer3d,\cite{chen2023focalformer3d},FocalFormer3D: Focusing on Hard Instance for 3D Object Detection,,,True,False,"Chen, Yilun and Yu, Zhiding and Chen, Yukang and Lan, Shiyi and Anandkumar, Anima and Jia, Jiaya and Alvarez, Jose M",2023,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,bai2022transfusion,\cite{bai2022transfusion},TransFusion: Robust LiDAR-Camera Fusion for 3D Object Detection with Transformers,https://arxiv.org/abs/2203.11496v1,"LiDAR and camera are two important sensors for 3D object detection in autonomous driving. Despite the increasing popularity of sensor fusion in this field, the robustness against inferior image conditions, e.g., bad illumination and sensor misalignment, is under-explored. Existing fusion methods are easily affected by such conditions, mainly due to a hard association of LiDAR points and image pixels, established by calibration matrices. We propose TransFusion, a robust solution to LiDAR-camera fusion with a soft-association mechanism to handle inferior image conditions. Specifically, our TransFusion consists of convolutional backbones and a detection head based on a transformer decoder. The first layer of the decoder predicts initial bounding boxes from a LiDAR point cloud using a sparse set of object queries, and its second decoder layer adaptively fuses the object queries with useful image features, leveraging both spatial and contextual relationships. The attention mechanism of the transformer enables our model to adaptively determine where and what information should be taken from the image, leading to a robust and effective fusion strategy. We additionally design an image-guided query initialization strategy to deal with objects that are difficult to detect in point clouds. TransFusion achieves state-of-the-art performance on large-scale datasets. We provide extensive experiments to demonstrate its robustness against degenerated image quality and calibration errors. We also extend the proposed method to the 3D tracking task and achieve the 1st place in the leaderboard of nuScenes tracking, showing its effectiveness and generalization capability.",True,True,"Bai, Xuyang and Hu, Zeyu and Zhu, Xinge and Huang, Qingqiu and Chen, Yilun and Fu, Hongbo and Tai, Chiew-Lan",2022,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,cao2024tfienet,\cite{cao2024tfienet},TFIENet: Transformer Fusion Information Enhancement Network for Multi-Model 3D Object Detection,,,True,False,"Cao, Feng and Jin, Yufeng and Tao, Chongben and Luo, Xizhao and Gao, Zhen and Zhang, Zufeng and Zheng, Sifa and Zhu, Yuan",2024,,,,IEEE Transactions on Instrumentation and Measurement
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,UniTR,\cite{UniTR},UniTR: A Unified and Efficient Multi-Modal Transformer for Bird's-Eye-View Representation,,,True,False,"Wang, Haiyang and Tang, Hao and Shi, Shaoshuai and Li, Aoxue and Li, Zhenguo and Schiele, Bernt and Wang, Liwei",2023,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,mvp,\cite{mvp},Multimodal Virtual Point 3D Detection,https://arxiv.org/abs/2111.06881v1,"Lidar-based sensing drives current autonomous vehicles. Despite rapid progress, current Lidar sensors still lag two decades behind traditional color cameras in terms of resolution and cost. For autonomous driving, this means that large objects close to the sensors are easily visible, but far-away or small objects comprise only one measurement or two. This is an issue, especially when these objects turn out to be driving hazards. On the other hand, these same objects are clearly visible in onboard RGB sensors. In this work, we present an approach to seamlessly fuse RGB sensors into Lidar-based 3D recognition. Our approach takes a set of 2D detections to generate dense 3D virtual points to augment an otherwise sparse 3D point cloud. These virtual points naturally integrate into any standard Lidar-based 3D detectors along with regular Lidar measurements. The resulting multi-modal detector is simple and effective. Experimental results on the large-scale nuScenes dataset show that our framework improves a strong CenterPoint baseline by a significant 6.6 mAP, and outperforms competing fusion approaches. Code and more visualizations are available at https://tianweiy.github.io/mvp/",True,True,"Yin, Tianwei and Zhou, Xingyi and Kr{\""a}henb{\""u}hl, Philipp",2021,,,,Advances in Neural Information Processing Systems
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,pointpainting,\cite{pointpainting},Pointpainting: Sequential fusion for 3d object detection,,,True,False,"Vora, Sourabh and Lang, Alex H and Helou, Bassam and Beijbom, Oscar",2020,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,qi2018frustum_pointnets,\cite{qi2018frustum_pointnets},Frustum pointnets for 3d object detection from rgb-d data,,,True,False,"Qi, Charles R and Liu, Wei and Wu, Chenxia and Su, Hao and Guibas, Leonidas J",2018,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,wang2020high_7Dpointnet,\cite{wang2020high_7Dpointnet},"High dimensional frustum pointnet for 3d object detection from camera, lidar, and radar",,,True,False,"Wang, Leichen and Chen, Tianbai and Anklam, Carsten and Goldluecke, Bastian",2020,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,wang2021pointaugmenting,\cite{wang2021pointaugmenting},Pointaugmenting: Cross-modal augmentation for 3d object detection,,,True,False,"Wang, Chunwei and Ma, Chao and Zhu, Ming and Yang, Xiaokang",2021,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,gupta2023far3det,\cite{gupta2023far3det},Far3Det: Towards Far-Field 3D Detection,https://arxiv.org/abs/2211.13858v1,"We focus on the task of far-field 3D detection (Far3Det) of objects beyond a certain distance from an observer, e.g., $>$50m. Far3Det is particularly important for autonomous vehicles (AVs) operating at highway speeds, which require detections of far-field obstacles to ensure sufficient braking distances. However, contemporary AV benchmarks such as nuScenes underemphasize this problem because they evaluate performance only up to a certain distance (50m). One reason is that obtaining far-field 3D annotations is difficult, particularly for lidar sensors that produce very few point returns for far-away objects. Indeed, we find that almost 50% of far-field objects (beyond 50m) contain zero lidar points. Secondly, current metrics for 3D detection employ a ""one-size-fits-all"" philosophy, using the same tolerance thresholds for near and far objects, inconsistent with tolerances for both human vision and stereo disparities. Both factors lead to an incomplete analysis of the Far3Det task. For example, while conventional wisdom tells us that high-resolution RGB sensors should be vital for 3D detection of far-away objects, lidar-based methods still rank higher compared to RGB counterparts on the current benchmark leaderboards. As a first step towards a Far3Det benchmark, we develop a method to find well-annotated scenes from the nuScenes dataset and derive a well-annotated far-field validation set. We also propose a Far3Det evaluation protocol and explore various 3D detection methods for Far3Det. Our result convincingly justifies the long-held conventional wisdom that high-resolution RGB improves 3D detection in the far-field. We further propose a simple yet effective method that fuses detections from RGB and lidar detectors based on non-maximum suppression, which remarkably outperforms state-of-the-art 3D detectors in the far-field.",True,True,"Gupta, Shubham and Kanjani, Jeet and Li, Mengtian and Ferroni, Francesco and Hays, James and Ramanan, Deva and Kong, Shu",2023,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,liu2024sparsedet,\cite{liu2024sparsedet},Sparsedet: a simple and effective framework for fully sparse lidar-based 3D object detection,,,True,False,"Liu, Lin and Song, Ziying and Xia, Qiming and Jia, Feiyang and Jia, Caiyan and Yang, Lei and Gong, Yan and Pan, Hongyu",2024,,,,IEEE Transactions on Geoscience and Remote Sensing
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,peri2023empirical,\cite{peri2023empirical},An empirical analysis of range for 3d object detection,,,True,False,"Peri, Neehar and Li, Mengtian and Wilson, Benjamin and Wang, Yu-Xiong and Hays, James and Ramanan, Deva",2023,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,liu2023generalized,\cite{liu2023generalized},Generalized Few-Shot 3D object detection of LiDAR point cloud for autonomous driving,,,True,False,"Liu, Jiawei and Dong, Xingping and Zhao, Sanyuan and Shen, Jianbing",2023,,,,arXiv preprint arXiv:2302.03914
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,pan2024clipbevformer,\cite{pan2024clipbevformer},CLIP-BEVFormer: Enhancing Multi-View Image-Based BEV Detector with Ground Truth Flow,https://arxiv.org/abs/2403.08919v2,"Autonomous driving stands as a pivotal domain in computer vision, shaping the future of transportation. Within this paradigm, the backbone of the system plays a crucial role in interpreting the complex environment. However, a notable challenge has been the loss of clear supervision when it comes to Bird's Eye View elements. To address this limitation, we introduce CLIP-BEVFormer, a novel approach that leverages the power of contrastive learning techniques to enhance the multi-view image-derived BEV backbones with ground truth information flow. We conduct extensive experiments on the challenging nuScenes dataset and showcase significant and consistent improvements over the SOTA. Specifically, CLIP-BEVFormer achieves an impressive 8.5\% and 9.2\% enhancement in terms of NDS and mAP, respectively, over the previous best BEV model on the 3D object detection task.",True,True,"Pan, Chenbin and Yaman, Burhaneddin and Velipasalar, Senem and Ren, Liu",2024,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,kitti,\cite{kitti},Are we ready for autonomous driving? the kitti vision benchmark suite,,,True,False,"Geiger, Andreas and Lenz, Philip and Urtasun, Raquel",2012,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,wang2025rethinking,\cite{wang2025rethinking},Rethinking How to Capture Long-range Dependency in 3D Object Detection,,,True,False,"Wang, Haoyu and Wang, Fasheng and Wang, Mengyin and Sun, Fuming and Li, Haojie",2025,,,,IEEE Transactions on Circuits and Systems for Video Technology
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,gu2025hgsfusion,\cite{gu2025hgsfusion},HGSFusion: Radar-camera fusion with hybrid generation and synchronization for 3d object detection,,,True,False,"Gu, Zijian and Ma, Jianwei and Huang, Yan and Wei, Honghao and Chen, Zhanye and Zhang, Hui and Hong, Wei",2025,,,,
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,yang2024ralibev,\cite{yang2024ralibev},RaLiBEV: Radar and LiDAR BEV fusion learning for anchor box free object detection systems,,,True,False,"Yang, Yanlong and Liu, Jianan and Huang, Tao and Han, Qing-Long and Ma, Gang and Zhu, Bing",2024,,,,IEEE Transactions on Circuits and Systems for Video Technology
DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection,2511.10035v1,huang2025l4dr,\cite{huang2025l4dr},L4dr: Lidar-4dradar fusion for weather-robust 3d object detection,,,True,False,"Huang, Xun and Xu, Ziyu and Wu, Hai and Wang, Jinlong and Xia, Qiming and Xia, Yan and Li, Jonathan and Gao, Kyle and Wen, Chenglu and Wang, Cheng",2025,,,,
SPIDER: Scalable Physics-Informed Dexterous Retargeting,2511.09484v1,parkDemoDiffusionOneShotHuman2025,\cite{parkDemoDiffusionOneShotHuman2025},DemoDiffusion: One-Shot Human Imitation using pre-trained Diffusion Policy,https://arxiv.org/abs/2506.20668v1,"We propose DemoDiffusion, a simple and scalable method for enabling robots to perform manipulation tasks in natural environments by imitating a single human demonstration. Our approach is based on two key insights. First, the hand motion in a human demonstration provides a useful prior for the robot's end-effector trajectory, which we can convert into a rough open-loop robot motion trajectory via kinematic retargeting. Second, while this retargeted motion captures the overall structure of the task, it may not align well with plausible robot actions in-context. To address this, we leverage a pre-trained generalist diffusion policy to modify the trajectory, ensuring it both follows the human motion and remains within the distribution of plausible robot actions. Our approach avoids the need for online reinforcement learning or paired human-robot data, enabling robust adaptation to new tasks and scenes with minimal manual effort. Experiments in both simulation and real-world settings show that DemoDiffusion outperforms both the base policy and the retargeted trajectory, enabling the robot to succeed even on tasks where the pre-trained generalist policy fails entirely. Project page: https://demodiffusion.github.io/",True,True,"Park, Sungjae and Bharadhwaj, Homanga and Tulsiani, Shubham",2025,,,,arXiv:2506.20668
SPIDER: Scalable Physics-Informed Dexterous Retargeting,2511.09484v1,liOKAMITeachingHumanoid2024,\cite{liOKAMITeachingHumanoid2024},OKAMI: Teaching Humanoid Robots Manipulation Skills through Single Video Imitation,https://arxiv.org/abs/2410.11792v1,"We study the problem of teaching humanoid robots manipulation skills by imitating from single video demonstrations. We introduce OKAMI, a method that generates a manipulation plan from a single RGB-D video and derives a policy for execution. At the heart of our approach is object-aware retargeting, which enables the humanoid robot to mimic the human motions in an RGB-D video while adjusting to different object locations during deployment. OKAMI uses open-world vision models to identify task-relevant objects and retarget the body motions and hand poses separately. Our experiments show that OKAMI achieves strong generalizations across varying visual and spatial conditions, outperforming the state-of-the-art baseline on open-world imitation from observation. Furthermore, OKAMI rollout trajectories are leveraged to train closed-loop visuomotor policies, which achieve an average success rate of 79.2% without the need for labor-intensive teleoperation. More videos can be found on our website https://ut-austin-rpl.github.io/OKAMI/.",True,True,"Li, Jinhan and Zhu, Yifeng and Xie, Yuqi and Jiang, Zhenyu and Seo, Mingyo and Pavlakos, Georgios and Zhu, Yuke",2024,,,,
SPIDER: Scalable Physics-Informed Dexterous Retargeting,2511.09484v1,papagiannisR+XRetrievalExecution2025,\cite{papagiannisR+XRetrievalExecution2025},R+{{X}}: {{Retrieval}} and {{Execution}} from {{Everyday Human Videos}},,,True,False,"Papagiannis, Georgios and Palo, Norman Di and Vitiello, Pietro and Johns, Edward",2025,,,,
SPIDER: Scalable Physics-Informed Dexterous Retargeting,2511.09484v1,liuEgoZeroRobotLearning2025,\cite{liuEgoZeroRobotLearning2025},EgoZero: Robot Learning from Smart Glasses,https://arxiv.org/abs/2505.20290v2,"Despite recent progress in general purpose robotics, robot policies still lag far behind basic human capabilities in the real world. Humans interact constantly with the physical world, yet this rich data resource remains largely untapped in robot learning. We propose EgoZero, a minimal system that learns robust manipulation policies from human demonstrations captured with Project Aria smart glasses, $\textbf{and zero robot data}$. EgoZero enables: (1) extraction of complete, robot-executable actions from in-the-wild, egocentric, human demonstrations, (2) compression of human visual observations into morphology-agnostic state representations, and (3) closed-loop policy learning that generalizes morphologically, spatially, and semantically. We deploy EgoZero policies on a gripper Franka Panda robot and demonstrate zero-shot transfer with 70% success rate over 7 manipulation tasks and only 20 minutes of data collection per task. Our results suggest that in-the-wild human data can serve as a scalable foundation for real-world robot learning - paving the way toward a future of abundant, diverse, and naturalistic training data for robots. Code and videos are available at https://egozero-robot.github.io.",True,True,"Liu, Vincent and Adeniji, Ademi and Zhan, Haotian and Haldar, Siddhant and Bhirangi, Raunaq and Abbeel, Pieter and Pinto, Lerrel",2025,,,,arXiv:2505.20290
SPIDER: Scalable Physics-Informed Dexterous Retargeting,2511.09484v1,wangMimicPlayLongHorizonImitation2023,\cite{wangMimicPlayLongHorizonImitation2023},MimicPlay: Long-Horizon Imitation Learning by Watching Human Play,https://arxiv.org/abs/2302.12422v2,"Imitation learning from human demonstrations is a promising paradigm for teaching robots manipulation skills in the real world. However, learning complex long-horizon tasks often requires an unattainable amount of demonstrations. To reduce the high data requirement, we resort to human play data - video sequences of people freely interacting with the environment using their hands. Even with different morphologies, we hypothesize that human play data contain rich and salient information about physical interactions that can readily facilitate robot policy learning. Motivated by this, we introduce a hierarchical learning framework named MimicPlay that learns latent plans from human play data to guide low-level visuomotor control trained on a small number of teleoperated demonstrations. With systematic evaluations of 14 long-horizon manipulation tasks in the real world, we show that MimicPlay outperforms state-of-the-art imitation learning methods in task success rate, generalization ability, and robustness to disturbances. Code and videos are available at https://mimic-play.github.io",True,True,"Wang, Chen and Fan, Linxi and Sun, Jiankai and Zhang, Ruohan and {Fei-Fei}, Li and Xu, Danfei and Zhu, Yuke and Anandkumar, Anima",2023,,,,
SPIDER: Scalable Physics-Informed Dexterous Retargeting,2511.09484v1,bharadhwaj2024track2act,\cite{bharadhwaj2024track2act},Track2Act: Predicting Point Tracks from Internet Videos enables Generalizable Robot Manipulation,https://arxiv.org/abs/2405.01527v2,"We seek to learn a generalizable goal-conditioned policy that enables zero-shot robot manipulation: interacting with unseen objects in novel scenes without test-time adaptation. While typical approaches rely on a large amount of demonstration data for such generalization, we propose an approach that leverages web videos to predict plausible interaction plans and learns a task-agnostic transformation to obtain robot actions in the real world. Our framework,Track2Act predicts tracks of how points in an image should move in future time-steps based on a goal, and can be trained with diverse videos on the web including those of humans and robots manipulating everyday objects. We use these 2D track predictions to infer a sequence of rigid transforms of the object to be manipulated, and obtain robot end-effector poses that can be executed in an open-loop manner. We then refine this open-loop plan by predicting residual actions through a closed loop policy trained with a few embodiment-specific demonstrations. We show that this approach of combining scalably learned track prediction with a residual policy requiring minimal in-domain robot-specific data enables diverse generalizable robot manipulation, and present a wide array of real-world robot manipulation results across unseen tasks, objects, and scenes. https://homangab.github.io/track2act/",True,True,"Bharadhwaj, Homanga and Mottaghi, Roozbeh and Gupta, Abhinav and Tulsiani, Shubham",2024,,,,
SPIDER: Scalable Physics-Informed Dexterous Retargeting,2511.09484v1,shawVideoDexLearningDexterity2022,\cite{shawVideoDexLearningDexterity2022},VideoDex: Learning Dexterity from Internet Videos,https://arxiv.org/abs/2212.04498v1,"To build general robotic agents that can operate in many environments, it is often imperative for the robot to collect experience in the real world. However, this is often not feasible due to safety, time, and hardware restrictions. We thus propose leveraging the next best thing as real-world experience: internet videos of humans using their hands. Visual priors, such as visual features, are often learned from videos, but we believe that more information from videos can be utilized as a stronger prior. We build a learning algorithm, VideoDex, that leverages visual, action, and physical priors from human video datasets to guide robot behavior. These actions and physical priors in the neural network dictate the typical human behavior for a particular robot task. We test our approach on a robot arm and dexterous hand-based system and show strong results on various manipulation tasks, outperforming various state-of-the-art methods. Videos at https://video-dex.github.io",True,True,"Shaw, Kenneth and Bahl, Shikhar and Pathak, Deepak",2022,,,,
SPIDER: Scalable Physics-Informed Dexterous Retargeting,2511.09484v1,qinOneHandMultiple2023,\cite{qinOneHandMultiple2023},From One Hand to Multiple Hands: Imitation Learning for Dexterous Manipulation from Single-Camera Teleoperation,https://arxiv.org/abs/2204.12490v2,"We propose to perform imitation learning for dexterous manipulation with multi-finger robot hand from human demonstrations, and transfer the policy to the real robot hand. We introduce a novel single-camera teleoperation system to collect the 3D demonstrations efficiently with only an iPad and a computer. One key contribution of our system is that we construct a customized robot hand for each user in the physical simulator, which is a manipulator resembling the same kinematics structure and shape of the operator's hand. This provides an intuitive interface and avoid unstable human-robot hand retargeting for data collection, leading to large-scale and high quality data. Once the data is collected, the customized robot hand trajectories can be converted to different specified robot hands (models that are manufactured) to generate training demonstrations. With imitation learning using our data, we show large improvement over baselines with multiple complex manipulation tasks. Importantly, we show our learned policy is significantly more robust when transferring to the real robot. More videos can be found in the https://yzqin.github.io/dex-teleop-imitation .",True,True,"Qin, Yuzhe and Su, Hao and Wang, Xiaolong",2022,,,,
SPIDER: Scalable Physics-Informed Dexterous Retargeting,2511.09484v1,xuDexUMIUsingHuman2025,\cite{xuDexUMIUsingHuman2025},{{DexUMI}}: {{Using Human Hand}} as the {{Universal Manipulation Interface}} for {{Dexterous Manipulation}},,,True,False,"Xu, Mengda and Zhang, Han and Hou, Yifan and Xu, Zhenjia and Fan, Linxi and Veloso, Manuela and Song, Shuran",2025,,,,
SPIDER: Scalable Physics-Informed Dexterous Retargeting,2511.09484v1,wuOneShotTransferLongHorizon2024,\cite{wuOneShotTransferLongHorizon2024},One-Shot Transfer of Long-Horizon Extrinsic Manipulation Through Contact Retargeting,https://arxiv.org/abs/2404.07468v1,"Extrinsic manipulation, the use of environment contacts to achieve manipulation objectives, enables strategies that are otherwise impossible with a parallel jaw gripper. However, orchestrating a long-horizon sequence of contact interactions between the robot, object, and environment is notoriously challenging due to the scene diversity, large action space, and difficult contact dynamics. We observe that most extrinsic manipulation are combinations of short-horizon primitives, each of which depend strongly on initializing from a desirable contact configuration to succeed. Therefore, we propose to generalize one extrinsic manipulation trajectory to diverse objects and environments by retargeting contact requirements. We prepare a single library of robust short-horizon, goal-conditioned primitive policies, and design a framework to compose state constraints stemming from contacts specifications of each primitive. Given a test scene and a single demo prescribing the primitive sequence, our method enforces the state constraints on the test scene and find intermediate goal states using inverse kinematics. The goals are then tracked by the primitive policies. Using a 7+1 DoF robotic arm-gripper system, we achieved an overall success rate of 80.5% on hardware over 4 long-horizon extrinsic manipulation tasks, each with up to 4 primitives. Our experiments cover 10 objects and 6 environment configurations. We further show empirically that our method admits a wide range of demonstrations, and that contact retargeting is indeed the key to successfully combining primitives for long-horizon extrinsic manipulation. Code and additional details are available at stanford-tml.github.io/extrinsic-manipulation.",True,True,"Wu, Albert and Wang, Ruocheng and Chen, Sirui and Eppner, Clemens and Liu, C. Karen",2024,,,,arXiv:2404.07468
SPIDER: Scalable Physics-Informed Dexterous Retargeting,2511.09484v1,qinAnyTeleopGeneralVisionBased2024,\cite{qinAnyTeleopGeneralVisionBased2024},AnyTeleop: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System,https://arxiv.org/abs/2307.04577v3,"Vision-based teleoperation offers the possibility to endow robots with human-level intelligence to physically interact with the environment, while only requiring low-cost camera sensors. However, current vision-based teleoperation systems are designed and engineered towards a particular robot model and deploy environment, which scales poorly as the pool of the robot models expands and the variety of the operating environment increases. In this paper, we propose AnyTeleop, a unified and general teleoperation system to support multiple different arms, hands, realities, and camera configurations within a single system. Although being designed to provide great flexibility to the choice of simulators and real hardware, our system can still achieve great performance. For real-world experiments, AnyTeleop can outperform a previous system that was designed for a specific robot hardware with a higher success rate, using the same robot. For teleoperation in simulation, AnyTeleop leads to better imitation learning performance, compared with a previous system that is particularly designed for that simulator. Project page: https://yzqin.github.io/anyteleop/.",True,True,"Qin, Yuzhe and Yang, Wei and Huang, Binghao and Wyk, Karl Van and Su, Hao and Wang, Xiaolong and Chao, Yu-Wei and Fox, Dieter",2023,,,,
SPIDER: Scalable Physics-Informed Dexterous Retargeting,2511.09484v1,yinDexterityGenFoundationController2025,\cite{yinDexterityGenFoundationController2025},{{DexterityGen}}: {{Foundation Controller}} for {{Unprecedented Dexterity}},,,True,False,"Yin, Zhao-Heng and Wang, Changhao and Pineda, Luis and Hogan, Francois and Bodduluri, Krishna and Sharma, Akash and Lancaster, Patrick and Prasad, Ishita and Kalakrishnan, Mrinal and Malik, Jitendra and Lambeta, Mike and Wu, Tingfan and Abbeel, Pieter and Mukadam, Mustafa",2025,,,,
SPIDER: Scalable Physics-Informed Dexterous Retargeting,2511.09484v1,redaPhysicsbasedMotionRetargeting2023,\cite{redaPhysicsbasedMotionRetargeting2023},Physics-based Motion Retargeting from Sparse Inputs,https://arxiv.org/abs/2307.01938v1,"Avatars are important to create interactive and immersive experiences in virtual worlds. One challenge in animating these characters to mimic a user's motion is that commercial AR/VR products consist only of a headset and controllers, providing very limited sensor data of the user's pose. Another challenge is that an avatar might have a different skeleton structure than a human and the mapping between them is unclear. In this work we address both of these challenges. We introduce a method to retarget motions in real-time from sparse human sensor data to characters of various morphologies. Our method uses reinforcement learning to train a policy to control characters in a physics simulator. We only require human motion capture data for training, without relying on artist-generated animations for each avatar. This allows us to use large motion capture datasets to train general policies that can track unseen users from real and sparse data in real-time. We demonstrate the feasibility of our approach on three characters with different skeleton structure: a dinosaur, a mouse-like creature and a human. We show that the avatar poses often match the user surprisingly well, despite having no sensor information of the lower body available. We discuss and ablate the important components in our framework, specifically the kinematic retargeting step, the imitation, contact and action reward as well as our asymmetric actor-critic observations. We further explore the robustness of our method in a variety of settings including unbalancing, dancing and sports motions.",True,True,"Reda, Daniele and Won, Jungdam and Ye, Yuting and van de Panne, Michiel and Winkler, Alexander",2023,,,,arXiv:2307.01938
SPIDER: Scalable Physics-Informed Dexterous Retargeting,2511.09484v1,lakshmipathyKinematicMotionRetargeting2024,\cite{lakshmipathyKinematicMotionRetargeting2024},Kinematic {{Motion Retargeting}} for {{Contact-Rich Anthropomorphic Manipulations}},,,True,False,"Lakshmipathy, Arjun S. and Hodgins, Jessica K. and Pollard, Nancy S.",2025,,,,ACM Transactions on Graphics
SPIDER: Scalable Physics-Informed Dexterous Retargeting,2511.09484v1,yangPhysicsDrivenDataGeneration2025,\cite{yangPhysicsDrivenDataGeneration2025},Physics-{{Driven Data Generation}} for {{Contact-Rich Manipulation}} via {{Trajectory Optimization}},,,True,False,"Yang, Lujie and Suh, H. J. Terry and Zhao, Tong and Graesdal, Bernhard Paus and Kelestemur, Tarik and Wang, Jiuguang and Pang, Tao and Tedrake, Russ",2025,,,,
SPIDER: Scalable Physics-Informed Dexterous Retargeting,2511.09484v1,nakaokaTaskModelLower2005,\cite{nakaokaTaskModelLower2005},Task Model of Lower Body Motion for a Biped Humanoid Robot to Imitate Human Dances,,,True,False,"Nakaoka, Shinichiro and Nakazawa, Atsushi and Kanehiro, Fumio and Kaneko, Kenji and Morisawa, Mitsuharu and Ikeuchi, Katsushi",2005,,,,
SPIDER: Scalable Physics-Informed Dexterous Retargeting,2511.09484v1,lumCrossingHumanRobotEmbodiment2025,\cite{lumCrossingHumanRobotEmbodiment2025},Crossing the {{Human-Robot Embodiment Gap}} with {{Sim-to-Real RL}} Using {{One Human Demonstration}},,,True,False,"Lum, Tyler Ga Wei and Lee, Olivia Y. and Liu, C. Karen and Bohg, Jeannette",2025,,,,
SPIDER: Scalable Physics-Informed Dexterous Retargeting,2511.09484v1,liManipTransEfficientDexterous2025,\cite{liManipTransEfficientDexterous2025},ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via Residual Learning,https://arxiv.org/abs/2503.21860v1,"Human hands play a central role in interacting, motivating increasing research in dexterous robotic manipulation. Data-driven embodied AI algorithms demand precise, large-scale, human-like manipulation sequences, which are challenging to obtain with conventional reinforcement learning or real-world teleoperation. To address this, we introduce ManipTrans, a novel two-stage method for efficiently transferring human bimanual skills to dexterous robotic hands in simulation. ManipTrans first pre-trains a generalist trajectory imitator to mimic hand motion, then fine-tunes a specific residual module under interaction constraints, enabling efficient learning and accurate execution of complex bimanual tasks. Experiments show that ManipTrans surpasses state-of-the-art methods in success rate, fidelity, and efficiency. Leveraging ManipTrans, we transfer multiple hand-object datasets to robotic hands, creating DexManipNet, a large-scale dataset featuring previously unexplored tasks like pen capping and bottle unscrewing. DexManipNet comprises 3.3K episodes of robotic manipulation and is easily extensible, facilitating further policy training for dexterous hands and enabling real-world deployments.",True,True,"Li, Kailin and Li, Puhao and Liu, Tengyu and Li, Yuyang and Huang, Siyuan",2025,,,,
SPIDER: Scalable Physics-Informed Dexterous Retargeting,2511.09484v1,mandiDexMachinaFunctionalRetargeting2025,\cite{mandiDexMachinaFunctionalRetargeting2025},{{DexMachina}}: {{Functional Retargeting}} for {{Bimanual Dexterous Manipulation}},,,True,False,"Mandi, Zhao and Hou, Yifan and Fox, Dieter and Narang, Yashraj and Mandlekar, Ajay and Song, Shuran",2025,,,,arXiv:2505.24853
SPIDER: Scalable Physics-Informed Dexterous Retargeting,2511.09484v1,liuQuasiSimParameterizedQuasiPhysical2024,\cite{liuQuasiSimParameterizedQuasiPhysical2024},QuasiSim: Parameterized Quasi-Physical Simulators for Dexterous Manipulations Transfer,https://arxiv.org/abs/2404.07988v2,"We explore the dexterous manipulation transfer problem by designing simulators. The task wishes to transfer human manipulations to dexterous robot hand simulations and is inherently difficult due to its intricate, highly-constrained, and discontinuous dynamics and the need to control a dexterous hand with a DoF to accurately replicate human manipulations. Previous approaches that optimize in high-fidelity black-box simulators or a modified one with relaxed constraints only demonstrate limited capabilities or are restricted by insufficient simulation fidelity. We introduce parameterized quasi-physical simulators and a physics curriculum to overcome these limitations. The key ideas are 1) balancing between fidelity and optimizability of the simulation via a curriculum of parameterized simulators, and 2) solving the problem in each of the simulators from the curriculum, with properties ranging from high task optimizability to high fidelity. We successfully enable a dexterous hand to track complex and diverse manipulations in high-fidelity simulated environments, boosting the success rate by 11\%+ from the best-performed baseline. The project website is available at https://meowuu7.github.io/QuasiSim/.",True,True,"Liu, Xueyi and Lyu, Kangbo and Zhang, Jieqiong and Du, Tao and Yi, Li",2024,,,,
SPIDER: Scalable Physics-Informed Dexterous Retargeting,2511.09484v1,deboerTutorialCrossEntropyMethod2005,\cite{deboerTutorialCrossEntropyMethod2005},A {{Tutorial}} on the {{Cross-Entropy Method}},,,True,False,"De Boer, Pieter-Tjerk and Kroese, Dirk P. and Mannor, Shie and Rubinstein, Reuven Y.",2005,,,,Annals of Operations Research
SPIDER: Scalable Physics-Informed Dexterous Retargeting,2511.09484v1,salimansEvolutionStrategiesScalable2017,\cite{salimansEvolutionStrategiesScalable2017},Evolution Strategies as a Scalable Alternative to Reinforcement Learning,https://arxiv.org/abs/1703.03864v2,"We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.",True,True,"Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya",2017,,,,arXiv:1703.03864
SPIDER: Scalable Physics-Informed Dexterous Retargeting,2511.09484v1,frazierTutorialBayesianOptimization2018a,\cite{frazierTutorialBayesianOptimization2018a},A Tutorial on Bayesian Optimization,https://arxiv.org/abs/1807.02811v1,"Bayesian optimization is an approach to optimizing objective functions that take a long time (minutes or hours) to evaluate. It is best-suited for optimization over continuous domains of less than 20 dimensions, and tolerates stochastic noise in function evaluations. It builds a surrogate for the objective and quantifies the uncertainty in that surrogate using a Bayesian machine learning technique, Gaussian process regression, and then uses an acquisition function defined from this surrogate to decide where to sample. In this tutorial, we describe how Bayesian optimization works, including Gaussian process regression and three common acquisition functions: expected improvement, entropy search, and knowledge gradient. We then discuss more advanced techniques, including running multiple function evaluations in parallel, multi-fidelity and multi-information source optimization, expensive-to-evaluate constraints, random environmental conditions, multi-task Bayesian optimization, and the inclusion of derivative information. We conclude with a discussion of Bayesian optimization software and future research directions in the field. Within our tutorial material we provide a generalization of expected improvement to noisy evaluations, beyond the noise-free setting where it is more commonly applied. This generalization is justified by a formal decision-theoretic argument, standing in contrast to previous ad hoc modifications.",True,True,"Frazier, Peter I.",2018,,,,arXiv:1807.02811
SPIDER: Scalable Physics-Informed Dexterous Retargeting,2511.09484v1,williamsAggressiveDrivingModel2016,\cite{williamsAggressiveDrivingModel2016},Aggressive Driving with Model Predictive Path Integral Control,,,True,False,"Williams, Grady and Drews, Paul and Goldfain, Brian and Rehg, James M. and Theodorou, Evangelos A.",2016,,,,
SPIDER: Scalable Physics-Informed Dexterous Retargeting,2511.09484v1,xueFullOrderSamplingBasedMPC2024,\cite{xueFullOrderSamplingBasedMPC2024},Full-Order Sampling-Based MPC for Torque-Level Locomotion Control via Diffusion-Style Annealing,https://arxiv.org/abs/2409.15610v1,"Due to high dimensionality and non-convexity, real-time optimal control using full-order dynamics models for legged robots is challenging. Therefore, Nonlinear Model Predictive Control (NMPC) approaches are often limited to reduced-order models. Sampling-based MPC has shown potential in nonconvex even discontinuous problems, but often yields suboptimal solutions with high variance, which limits its applications in high-dimensional locomotion. This work introduces DIAL-MPC (Diffusion-Inspired Annealing for Legged MPC), a sampling-based MPC framework with a novel diffusion-style annealing process. Such an annealing process is supported by the theoretical landscape analysis of Model Predictive Path Integral Control (MPPI) and the connection between MPPI and single-step diffusion. Algorithmically, DIAL-MPC iteratively refines solutions online and achieves both global coverage and local convergence. In quadrupedal torque-level control tasks, DIAL-MPC reduces the tracking error of standard MPPI by $13.4$ times and outperforms reinforcement learning (RL) policies by $50\%$ in challenging climbing tasks without any training. In particular, DIAL-MPC enables precise real-world quadrupedal jumping with payload. To the best of our knowledge, DIAL-MPC is the first training-free method that optimizes over full-order quadruped dynamics in real-time.",True,True,"Xue, Haoru and Pan, Chaoyi and Yi, Zeji and Qu, Guannan and Shi, Guanya",2025,,,,
SPIDER: Scalable Physics-Informed Dexterous Retargeting,2511.09484v1,howellPredictiveSamplingRealtime2022,\cite{howellPredictiveSamplingRealtime2022},Predictive Sampling: Real-time Behaviour Synthesis with MuJoCo,https://arxiv.org/abs/2212.00541v2,"We introduce MuJoCo MPC (MJPC), an open-source, interactive application and software framework for real-time predictive control, based on MuJoCo physics. MJPC allows the user to easily author and solve complex robotics tasks, and currently supports three shooting-based planners: derivative-based iLQG and Gradient Descent, and a simple derivative-free method we call Predictive Sampling. Predictive Sampling was designed as an elementary baseline, mostly for its pedagogical value, but turned out to be surprisingly competitive with the more established algorithms. This work does not present algorithmic advances, and instead, prioritises performant algorithms, simple code, and accessibility of model-based methods via intuitive and interactive software. MJPC is available at: github.com/deepmind/mujoco_mpc, a video summary can be viewed at: dpmd.ai/mjpc.",True,True,"Howell, Taylor and Gileadi, Nimrod and Tunyasuvunakool, Saran and Zakka, Kevin and Erez, Tom and Tassa, Yuval",2022,,,,arXiv:2212.00541
SPIDER: Scalable Physics-Informed Dexterous Retargeting,2511.09484v1,liDROPDexterousReorientation2024,\cite{liDROPDexterousReorientation2024},DROP: Dexterous Reorientation via Online Planning,https://arxiv.org/abs/2409.14562v4,"Achieving human-like dexterity is a longstanding challenge in robotics, in part due to the complexity of planning and control for contact-rich systems. In reinforcement learning (RL), one popular approach has been to use massively-parallelized, domain-randomized simulations to learn a policy offline over a vast array of contact conditions, allowing robust sim-to-real transfer. Inspired by recent advances in real-time parallel simulation, this work considers instead the viability of online planning methods for contact-rich manipulation by studying the well-known in-hand cube reorientation task. We propose a simple architecture that employs a sampling-based predictive controller and vision-based pose estimator to search for contact-rich control actions online. We conduct thorough experiments to assess the real-world performance of our method, architectural design choices, and key factors for robustness, demonstrating that our simple sampling-based approach achieves performance comparable to prior RL-based works. Supplemental material: https://caltech-amber.github.io/drop.",True,True,"Li, Albert H. and Culbertson, Preston and Kurtz, Vince and Ames, Aaron D.",2025,,,,
SPIDER: Scalable Physics-Informed Dexterous Retargeting,2511.09484v1,kimSmoothModelPredictive2022,\cite{kimSmoothModelPredictive2022},Smooth Model Predictive Path Integral Control without Smoothing,https://arxiv.org/abs/2112.09988v8,"We present a sampling-based control approach that can generate smooth actions for general nonlinear systems without external smoothing algorithms. Model Predictive Path Integral (MPPI) control has been utilized in numerous robotic applications due to its appealing characteristics to solve non-convex optimization problems. However, the stochastic nature of sampling-based methods can cause significant chattering in the resulting commands. Chattering becomes more prominent in cases where the environment changes rapidly, possibly even causing the MPPI to diverge. To address this issue, we propose a method that seamlessly combines MPPI with an input-lifting strategy. In addition, we introduce a new action cost to smooth control sequence during trajectory rollouts while preserving the information theoretic interpretation of MPPI, which was derived from non-affine dynamics. We validate our method in two nonlinear control tasks with neural network dynamics: a pendulum swing-up task and a challenging autonomous driving task. The experimental results demonstrate that our method outperforms the MPPI baselines with additionally applied smoothing algorithms.",True,True,"Kim, Taekyung and Park, Gyuhyun and Kwak, Kiho and Bae, Jihwan and Lee, Wonsuk",2022,,,,RA-L
Expandable and Differentiable Dual Memories with Orthogonal Regularization for Exemplar-free Continual Learning,2511.09871v1,boschini2022transfer,\cite{boschini2022transfer},Transfer without Forgetting,https://arxiv.org/abs/2206.00388v2,"This work investigates the entanglement between Continual Learning (CL) and Transfer Learning (TL). In particular, we shed light on the widespread application of network pretraining, highlighting that it is itself subject to catastrophic forgetting. Unfortunately, this issue leads to the under-exploitation of knowledge transfer during later tasks. On this ground, we propose Transfer without Forgetting (TwF), a hybrid approach building upon a fixed pretrained sibling network, which continuously propagates the knowledge inherent in the source domain through a layer-wise loss term. Our experiments indicate that TwF steadily outperforms other CL methods across a variety of settings, averaging a 4.81% gain in Class-Incremental accuracy over a variety of datasets and different buffer sizes.",True,True,"Boschini, Matteo and Bonicelli, Lorenzo and Porrello, Angelo and Bellitto, Giovanni and Pennisi, Matteo and Palazzo, Simone and Spampinato, Concetto and Calderara, Simone",2022,,,,
Expandable and Differentiable Dual Memories with Orthogonal Regularization for Exemplar-free Continual Learning,2511.09871v1,benjamin2018measuring,\cite{benjamin2018measuring},Measuring and regularizing networks in function space,,,True,False,"Benjamin, Ari S and Rolnick, David and Kording, Konrad",2018,,,,arXiv preprint arXiv:1805.08289
Expandable and Differentiable Dual Memories with Orthogonal Regularization for Exemplar-free Continual Learning,2511.09871v1,rusu2016progressive,\cite{rusu2016progressive},Progressive Neural Networks,https://arxiv.org/abs/1606.04671v4,"Learning to solve complex sequences of tasks--while both leveraging transfer and avoiding catastrophic forgetting--remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.",True,True,"Rusu, Andrei A and Rabinowitz, Neil C and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia",2016,,,,arXiv preprint arXiv:1606.04671
Expandable and Differentiable Dual Memories with Orthogonal Regularization for Exemplar-free Continual Learning,2511.09871v1,yoon2017lifelong,\cite{yoon2017lifelong},Lifelong learning with dynamically expandable networks,,,True,False,"Yoon, Jaehong and Yang, Eunho and Lee, Jeongtae and Hwang, Sung Ju",2017,,,,arXiv preprint arXiv:1708.01547
Expandable and Differentiable Dual Memories with Orthogonal Regularization for Exemplar-free Continual Learning,2511.09871v1,pernici2021class,\cite{pernici2021class},Class-incremental learning with pre-allocated fixed classifiers,,,True,False,"Pernici, Federico and Bruni, Matteo and Baecchi, Claudio and Turchini, Francesco and Del Bimbo, Alberto",2021,,,,
Expandable and Differentiable Dual Memories with Orthogonal Regularization for Exemplar-free Continual Learning,2511.09871v1,pham2021dualnet,\cite{pham2021dualnet},"Dualnet: Continual learning, fast and slow",,,True,False,"Pham, Quang and Liu, Chenghao and Hoi, Steven",2021,,,,Advances in Neural Information Processing Systems
Expandable and Differentiable Dual Memories with Orthogonal Regularization for Exemplar-free Continual Learning,2511.09871v1,arani2022learning,\cite{arani2022learning},"Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System",https://arxiv.org/abs/2201.12604v2,"Humans excel at continually learning from an ever-changing environment whereas it remains a challenge for deep neural networks which exhibit catastrophic forgetting. The complementary learning system (CLS) theory suggests that the interplay between rapid instance-based learning and slow structured learning in the brain is crucial for accumulating and retaining knowledge. Here, we propose CLS-ER, a novel dual memory experience replay (ER) method which maintains short-term and long-term semantic memories that interact with the episodic memory. Our method employs an effective replay mechanism whereby new knowledge is acquired while aligning the decision boundaries with the semantic memories. CLS-ER does not utilize the task boundaries or make any assumption about the distribution of the data which makes it versatile and suited for ""general continual learning"". Our approach achieves state-of-the-art performance on standard benchmarks as well as more realistic general continual learning settings.",True,True,"Arani, Elahe and Sarfraz, Fahad and Zonooz, Bahram",2022,,,,arXiv preprint arXiv:2201.12604
Expandable and Differentiable Dual Memories with Orthogonal Regularization for Exemplar-free Continual Learning,2511.09871v1,qi2024interactive,\cite{qi2024interactive},Interactive continual learning: Fast and slow thinking,,,True,False,"Qi, Biqing and Chen, Xinquan and Gao, Junqi and Li, Dong and Liu, Jianxing and Wu, Ligang and Zhou, Bowen",2024,,,,
PriVi: Towards A General-Purpose Video Model For Primate Behavior In The Wild,2511.09675v1,rezende_stochastic_2014,\cite{rezende_stochastic_2014},Stochastic Backpropagation and Approximate Inference in Deep Generative Models,https://arxiv.org/abs/1401.4082v3,"We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.",True,True,,,,,,
PriVi: Towards A General-Purpose Video Model For Primate Behavior In The Wild,2511.09675v1,radford_improving_2018,\cite{radford_improving_2018},Improving GANs Using Optimal Transport,https://arxiv.org/abs/1803.05573v1,"We present Optimal Transport GAN (OT-GAN), a variant of generative adversarial nets minimizing a new metric measuring the distance between the generator distribution and the data distribution. This metric, which we call mini-batch energy distance, combines optimal transport in primal form with an energy distance defined in an adversarially learned feature space, resulting in a highly discriminative distance function with unbiased mini-batch gradients. Experimentally we show OT-GAN to be highly stable when trained with large mini-batches, and we present state-of-the-art results on several popular benchmark problems for image generation.",True,True,,,,,,
PriVi: Towards A General-Purpose Video Model For Primate Behavior In The Wild,2511.09675v1,caron_emerging_2021,\cite{caron_emerging_2021},Emerging Properties in Self-Supervised Vision Transformers,https://arxiv.org/abs/2104.14294v2,"In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base.",True,True,,,,,,
PriVi: Towards A General-Purpose Video Model For Primate Behavior In The Wild,2511.09675v1,bardes_revisiting_2024,\cite{bardes_revisiting_2024},Revisiting Feature Prediction for Learning Visual Representations from Video,https://arxiv.org/abs/2404.08471v1,"This paper explores feature prediction as a stand-alone objective for unsupervised learning from video and introduces V-JEPA, a collection of vision models trained solely using a feature prediction objective, without the use of pretrained image encoders, text, negative examples, reconstruction, or other sources of supervision. The models are trained on 2 million videos collected from public datasets and are evaluated on downstream image and video tasks. Our results show that learning by predicting video features leads to versatile visual representations that perform well on both motion and appearance-based tasks, without adaption of the model's parameters; e.g., using a frozen backbone. Our largest model, a ViT-H/16 trained only on videos, obtains 81.9% on Kinetics-400, 72.2% on Something-Something-v2, and 77.9% on ImageNet1K.",True,True,,,,,,
PriVi: Towards A General-Purpose Video Model For Primate Behavior In The Wild,2511.09675v1,assran_self-supervised_2023,\cite{assran_self-supervised_2023},Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture,https://arxiv.org/abs/2301.08243v3,"This paper demonstrates an approach for learning highly semantic image representations without relying on hand-crafted data-augmentations. We introduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), a non-generative approach for self-supervised learning from images. The idea behind I-JEPA is simple: from a single context block, predict the representations of various target blocks in the same image. A core design choice to guide I-JEPA towards producing semantic representations is the masking strategy; specifically, it is crucial to (a) sample target blocks with sufficiently large scale (semantic), and to (b) use a sufficiently informative (spatially distributed) context block. Empirically, when combined with Vision Transformers, we find I-JEPA to be highly scalable. For instance, we train a ViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strong downstream performance across a wide range of tasks, from linear classification to object counting and depth prediction.",True,True,,,,,,
PriVi: Towards A General-Purpose Video Model For Primate Behavior In The Wild,2511.09675v1,assran_v-jepa_2025,\cite{assran_v-jepa_2025},"V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning",https://arxiv.org/abs/2506.09985v1,"A major challenge for modern AI is to learn to understand the world and learn to act largely by observation. This paper explores a self-supervised approach that combines internet-scale video data with a small amount of interaction data (robot trajectories), to develop models capable of understanding, predicting, and planning in the physical world. We first pre-train an action-free joint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset comprising over 1 million hours of internet video. V-JEPA 2 achieves strong performance on motion understanding (77.3 top-1 accuracy on Something-Something v2) and state-of-the-art performance on human action anticipation (39.7 recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models. Additionally, after aligning V-JEPA 2 with a large language model, we demonstrate state-of-the-art performance on multiple video question-answering tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on TempCompass). Finally, we show how self-supervised learning can be applied to robotic planning tasks by post-training a latent action-conditioned world model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the Droid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different labs and enable picking and placing of objects using planning with image goals. Notably, this is achieved without collecting any data from the robots in these environments, and without any task-specific training or reward. This work demonstrates how self-supervised learning from web-scale data and a small amount of robot interaction data can yield a world model capable of planning in the physical world.",True,True,,,,,,
PriVi: Towards A General-Purpose Video Model For Primate Behavior In The Wild,2511.09675v1,oquab_dinov2_2024,\cite{oquab_dinov2_2024},DINOv2 Meets Text: A Unified Framework for Image- and Pixel-Level Vision-Language Alignment,https://arxiv.org/abs/2412.16334v1,"Self-supervised visual foundation models produce powerful embeddings that achieve remarkable performance on a wide range of downstream tasks. However, unlike vision-language models such as CLIP, self-supervised visual features are not readily aligned with language, hindering their adoption in open-vocabulary tasks. Our method, named dino.txt, unlocks this new ability for DINOv2, a widely used self-supervised visual encoder. We build upon the LiT training strategy, which trains a text encoder to align with a frozen vision model but leads to unsatisfactory results on dense tasks. We propose several key ingredients to improve performance on both global and dense tasks, such as concatenating the [CLS] token with the patch average to train the alignment and curating data using both text and image modalities. With these, we successfully train a CLIP-like model with only a fraction of the computational cost compared to CLIP while achieving state-of-the-art results in zero-shot classification and open-vocabulary semantic segmentation.",True,True,,,,,,
PriVi: Towards A General-Purpose Video Model For Primate Behavior In The Wild,2511.09675v1,schuhmann_laion-5b_2022,\cite{schuhmann_laion-5b_2022},LAION-5B: An open large-scale dataset for training next generation image-text models,https://arxiv.org/abs/2210.08402v1,"Groundbreaking language-vision architectures like CLIP and DALL-E proved the utility of training on large amounts of noisy image-text data, without relying on expensive accurate labels used in standard vision unimodal supervised learning. The resulting models showed capabilities of strong text-guided image generation and transfer to downstream tasks, while performing remarkably at zero-shot classification with noteworthy out-of-distribution robustness. Since then, large-scale language-vision models like ALIGN, BASIC, GLIDE, Flamingo and Imagen made further improvements. Studying the training and capabilities of such models requires datasets containing billions of image-text pairs. Until now, no datasets of this size have been made openly available for the broader research community. To address this problem and democratize research on large-scale multi-modal models, we present LAION-5B - a dataset consisting of 5.85 billion CLIP-filtered image-text pairs, of which 2.32B contain English language. We show successful replication and fine-tuning of foundational models like CLIP, GLIDE and Stable Diffusion using the dataset, and discuss further experiments enabled with an openly available dataset of this scale. Additionally we provide several nearest neighbor indices, an improved web-interface for dataset exploration and subset generation, and detection scores for watermark, NSFW, and toxic content detection. Announcement page https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/",True,True,,,,,,
PriVi: Towards A General-Purpose Video Model For Primate Behavior In The Wild,2511.09675v1,miech_howto100m_2019,\cite{miech_howto100m_2019},HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips,https://arxiv.org/abs/1906.03327v2,"Learning text-video embeddings usually requires a dataset of video clips with manually provided captions. However, such datasets are expensive and time consuming to create and therefore difficult to obtain on a large scale. In this work, we propose instead to learn such embeddings from video data with readily available natural language annotations in the form of automatically transcribed narrations. The contributions of this work are three-fold. First, we introduce HowTo100M: a large-scale dataset of 136 million video clips sourced from 1.22M narrated instructional web videos depicting humans performing and describing over 23k different visual tasks. Our data collection procedure is fast, scalable and does not require any additional manual annotation. Second, we demonstrate that a text-video embedding trained on this data leads to state-of-the-art results for text-to-video retrieval and action localization on instructional video datasets such as YouCook2 or CrossTask. Finally, we show that this embedding transfers well to other domains: fine-tuning on generic Youtube videos (MSR-VTT dataset) and movies (LSMDC dataset) outperforms models trained on these datasets alone. Our dataset, code and models will be publicly available at: www.di.ens.fr/willow/research/howto100m/.",True,True,,,,,,
PriVi: Towards A General-Purpose Video Model For Primate Behavior In The Wild,2511.09675v1,bolya_perception_2025,\cite{bolya_perception_2025},Perception Encoder: The best visual embeddings are not at the output of the network,https://arxiv.org/abs/2504.13181v2,"We introduce Perception Encoder (PE), a state-of-the-art vision encoder for image and video understanding trained via simple vision-language learning. Traditionally, vision encoders have relied on a variety of pretraining objectives, each tailored to specific downstream tasks such as classification, captioning, or localization. Surprisingly, after scaling our carefully tuned image pretraining recipe and refining with our robust video data engine, we find that contrastive vision-language training alone can produce strong, general embeddings for all of these downstream tasks. There is only one caveat: these embeddings are hidden within the intermediate layers of the network. To draw them out, we introduce two alignment methods: language alignment for multimodal language modeling, and spatial alignment for dense prediction. Together, our PE family of models achieves best-in-class results on a wide variety of tasks, including (1) zero-shot image and video classification and retrieval, simultaneously obtaining 86.6 average zero-shot ImageNet robustness and 76.9 zero-shot Kinetics-400 video classification; (2) document, image, and video Q&A, enabling 94.6 DocVQA, 80.9 InfographicVQA, and 82.7 PerceptionTest with an 8B LLM; and (3) spatial tasks such as detection, tracking, and depth estimation, setting a new COCO state-of-the-art of 66.0 box mAP. To foster further research, we release our models, code, and novel dataset of synthetically and human-annotated videos: https://github.com/facebookresearch/perception_models",True,True,,,,,,
PriVi: Towards A General-Purpose Video Model For Primate Behavior In The Wild,2511.09675v1,feichtenhofer_masked_2022,\cite{feichtenhofer_masked_2022},Masked Autoencoders As Spatiotemporal Learners,https://arxiv.org/abs/2205.09113v2,"This paper studies a conceptually simple extension of Masked Autoencoders (MAE) to spatiotemporal representation learning from videos. We randomly mask out spacetime patches in videos and learn an autoencoder to reconstruct them in pixels. Interestingly, we show that our MAE method can learn strong representations with almost no inductive bias on spacetime (only except for patch and positional embeddings), and spacetime-agnostic random masking performs the best. We observe that the optimal masking ratio is as high as 90% (vs. 75% on images), supporting the hypothesis that this ratio is related to information redundancy of the data. A high masking ratio leads to a large speedup, e.g., > 4x in wall-clock time or even more. We report competitive results on several challenging video datasets using vanilla Vision Transformers. We observe that MAE can outperform supervised pre-training by large margins. We further report encouraging results of training on real-world, uncurated Instagram data. Our study suggests that the general framework of masked autoencoding (BERT, MAE, etc.) can be a unified methodology for representation learning with minimal domain knowledge.",True,True,,,,,,
PriVi: Towards A General-Purpose Video Model For Primate Behavior In The Wild,2511.09675v1,gadre_datacomp_2023,\cite{gadre_datacomp_2023},DataComp: In search of the next generation of multimodal datasets,https://arxiv.org/abs/2304.14108v5,"Multimodal datasets are a critical component in recent breakthroughs such as Stable Diffusion and GPT-4, yet their design does not receive the same research attention as model architectures or training algorithms. To address this shortcoming in the ML ecosystem, we introduce DataComp, a testbed for dataset experiments centered around a new candidate pool of 12.8 billion image-text pairs from Common Crawl. Participants in our benchmark design new filtering techniques or curate new data sources and then evaluate their new dataset by running our standardized CLIP training code and testing the resulting model on 38 downstream test sets. Our benchmark consists of multiple compute scales spanning four orders of magnitude, which enables the study of scaling trends and makes the benchmark accessible to researchers with varying resources. Our baseline experiments show that the DataComp workflow leads to better training sets. In particular, our best baseline, DataComp-1B, enables training a CLIP ViT-L/14 from scratch to 79.2% zero-shot accuracy on ImageNet, outperforming OpenAI's CLIP ViT-L/14 by 3.7 percentage points while using the same training procedure and compute. We release DataComp and all accompanying code at www.datacomp.ai.",True,True,,,,,,
PriVi: Towards A General-Purpose Video Model For Primate Behavior In The Wild,2511.09675v1,ravi_sam_2025,\cite{ravi_sam_2025},Apple Intelligence Foundation Language Models: Tech Report 2025,https://arxiv.org/abs/2507.13575v3,"We introduce two multilingual, multimodal foundation language models that power Apple Intelligence features across Apple devices and services: i a 3B-parameter on-device model optimized for Apple silicon through architectural innovations such as KV-cache sharing and 2-bit quantization-aware training; and ii a scalable server model built on a novel Parallel-Track Mixture-of-Experts PT-MoE transformer that combines track parallelism, mixture-of-experts sparse computation, and interleaved global-local attention to deliver high quality with competitive cost on Apple's Private Cloud Compute platform. Both models are trained on large-scale multilingual and multimodal datasets sourced via responsible web crawling, licensed corpora, and high-quality synthetic data, then further refined with supervised fine-tuning and reinforcement learning on a new asynchronous platform. The resulting models support several additional languages while understanding images and executing tool calls. In public benchmarks and human evaluations, both the server model and the on-device model match or surpass comparably sized open baselines.
  A new Swift-centric Foundation Models framework exposes guided generation, constrained tool calling, and LoRA adapter fine-tuning, allowing developers to integrate these capabilities with a few lines of code. The latest advancements in Apple Intelligence models are grounded in our Responsible AI approach with safeguards like content filtering and locale-specific evaluation, as well as our commitment to protecting our users' privacy with innovations like Private Cloud Compute.",True,True,,,,,,
PriVi: Towards A General-Purpose Video Model For Primate Behavior In The Wild,2511.09675v1,kirillov_segment_2023,\cite{kirillov_segment_2023},Segment Anything,https://arxiv.org/abs/2304.02643v1,"We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.",True,True,,,,,,
PriVi: Towards A General-Purpose Video Model For Primate Behavior In The Wild,2511.09675v1,gabeff_mammalps_2025,\cite{gabeff_mammalps_2025},MammAlps: A multi-view video behavior monitoring dataset of wild mammals in the Swiss Alps,https://arxiv.org/abs/2503.18223v2,"Monitoring wildlife is essential for ecology and ethology, especially in light of the increasing human impact on ecosystems. Camera traps have emerged as habitat-centric sensors enabling the study of wildlife populations at scale with minimal disturbance. However, the lack of annotated video datasets limits the development of powerful video understanding models needed to process the vast amount of fieldwork data collected. To advance research in wild animal behavior monitoring we present MammAlps, a multimodal and multi-view dataset of wildlife behavior monitoring from 9 camera-traps in the Swiss National Park. MammAlps contains over 14 hours of video with audio, 2D segmentation maps and 8.5 hours of individual tracks densely labeled for species and behavior. Based on 6135 single animal clips, we propose the first hierarchical and multimodal animal behavior recognition benchmark using audio, video and reference scene segmentation maps as inputs. Furthermore, we also propose a second ecology-oriented benchmark aiming at identifying activities, species, number of individuals and meteorological conditions from 397 multi-view and long-term ecological events, including false positive triggers. We advocate that both tasks are complementary and contribute to bridging the gap between machine learning and ecology. Code and data are available at: https://github.com/eceo-epfl/MammAlps",True,True,,,,,,
PriVi: Towards A General-Purpose Video Model For Primate Behavior In The Wild,2511.09675v1,brookes_panaf20k_2024,\cite{brookes_panaf20k_2024},PanAf20K: A Large Video Dataset for Wild Ape Detection and Behaviour Recognition,https://arxiv.org/abs/2401.13554v2,"We present the PanAf20K dataset, the largest and most diverse open-access annotated video dataset of great apes in their natural environment. It comprises more than 7 million frames across ~20,000 camera trap videos of chimpanzees and gorillas collected at 14 field sites in tropical Africa as part of the Pan African Programme: The Cultured Chimpanzee. The footage is accompanied by a rich set of annotations and benchmarks making it suitable for training and testing a variety of challenging and ecologically important computer vision tasks including ape detection and behaviour recognition. Furthering AI analysis of camera trap information is critical given the International Union for Conservation of Nature now lists all species in the great ape family as either Endangered or Critically Endangered. We hope the dataset can form a solid basis for engagement of the AI community to improve performance, efficiency, and result interpretation in order to support assessments of great ape presence, abundance, distribution, and behaviour and thereby aid conservation efforts.",True,True,,,,,,
PriVi: Towards A General-Purpose Video Model For Primate Behavior In The Wild,2511.09675v1,brookes_panaf-fgbg_2025,\cite{brookes_panaf-fgbg_2025},The PanAf-FGBG Dataset: Understanding the Impact of Backgrounds in Wildlife Behaviour Recognition,https://arxiv.org/abs/2502.21201v3,"Computer vision analysis of camera trap video footage is essential for wildlife conservation, as captured behaviours offer some of the earliest indicators of changes in population health. Recently, several high-impact animal behaviour datasets and methods have been introduced to encourage their use; however, the role of behaviour-correlated background information and its significant effect on out-of-distribution generalisation remain unexplored. In response, we present the PanAf-FGBG dataset, featuring 20 hours of wild chimpanzee behaviours, recorded at over 350 individual camera locations. Uniquely, it pairs every video with a chimpanzee (referred to as a foreground video) with a corresponding background video (with no chimpanzee) from the same camera location. We present two views of the dataset: one with overlapping camera locations and one with disjoint locations. This setup enables, for the first time, direct evaluation of in-distribution and out-of-distribution conditions, and for the impact of backgrounds on behaviour recognition models to be quantified. All clips come with rich behavioural annotations and metadata including unique camera IDs and detailed textual scene descriptions. Additionally, we establish several baselines and present a highly effective latent-space normalisation technique that boosts out-of-distribution performance by +5.42% mAP for convolutional and +3.75% mAP for transformer-based models. Finally, we provide an in-depth analysis on the role of backgrounds in out-of-distribution behaviour recognition, including the so far unexplored impact of background durations (i.e., the count of background frames within foreground videos).",True,True,,,,,,
PriVi: Towards A General-Purpose Video Model For Primate Behavior In The Wild,2511.09675v1,brookes_chimpvlm_2024,\cite{brookes_chimpvlm_2024},ChimpVLM: Ethogram-Enhanced Chimpanzee Behaviour Recognition,https://arxiv.org/abs/2404.08937v1,"We show that chimpanzee behaviour understanding from camera traps can be enhanced by providing visual architectures with access to an embedding of text descriptions that detail species behaviours. In particular, we present a vision-language model which employs multi-modal decoding of visual features extracted directly from camera trap videos to process query tokens representing behaviours and output class predictions. Query tokens are initialised using a standardised ethogram of chimpanzee behaviour, rather than using random or name-based initialisations. In addition, the effect of initialising query tokens using a masked language model fine-tuned on a text corpus of known behavioural patterns is explored. We evaluate our system on the PanAf500 and PanAf20K datasets and demonstrate the performance benefits of our multi-modal decoding approach and query initialisation strategy on multi-class and multi-label recognition tasks, respectively. Results and ablations corroborate performance improvements. We achieve state-of-the-art performance over vision and vision-language models in top-1 accuracy (+6.34%) on PanAf500 and overall (+1.1%) and tail-class (+2.26%) mean average precision on PanAf20K. We share complete source code and network weights for full reproducibility of results and easy utilisation.",True,True,,,,,,
PriVi: Towards A General-Purpose Video Model For Primate Behavior In The Wild,2511.09675v1,brookes_triple-stream_2023,\cite{brookes_triple-stream_2023},Triple-stream Deep Metric Learning of Great Ape Behavioural Actions,https://arxiv.org/abs/2301.02642v1,"We propose the first metric learning system for the recognition of great ape behavioural actions. Our proposed triple stream embedding architecture works on camera trap videos taken directly in the wild and demonstrates that the utilisation of an explicit DensePose-C chimpanzee body part segmentation stream effectively complements traditional RGB appearance and optical flow streams. We evaluate system variants with different feature fusion techniques and long-tail recognition approaches. Results and ablations show performance improvements of ~12% in top-1 accuracy over previous results achieved on the PanAf-500 dataset containing 180,000 manually annotated frames across nine behavioural actions. Furthermore, we provide a qualitative analysis of our findings and augment the metric learning system with long-tail recognition techniques showing that average per class accuracy -- critical in the domain -- can be improved by ~23% compared to the literature on that dataset. Finally, since our embedding spaces are constructed as metric, we provide first data-driven visualisations of the great ape behavioural action spaces revealing emerging geometry and topology. We hope that the work sparks further interest in this vital application area of computer vision for the benefit of endangered great apes.",True,True,,,,,,
PriVi: Towards A General-Purpose Video Model For Primate Behavior In The Wild,2511.09675v1,iashin_self-supervised_2025,\cite{iashin_self-supervised_2025},Self-supervised Learning on Camera Trap Footage Yields a Strong Universal Face Embedder,https://arxiv.org/abs/2507.10552v1,"Camera traps are revolutionising wildlife monitoring by capturing vast amounts of visual data; however, the manual identification of individual animals remains a significant bottleneck. This study introduces a fully self-supervised approach to learning robust chimpanzee face embeddings from unlabeled camera-trap footage. Leveraging the DINOv2 framework, we train Vision Transformers on automatically mined face crops, eliminating the need for identity labels. Our method demonstrates strong open-set re-identification performance, surpassing supervised baselines on challenging benchmarks such as Bossou, despite utilising no labelled data during training. This work underscores the potential of self-supervised learning in biodiversity monitoring and paves the way for scalable, non-invasive population studies.",True,True,,,,,,
DT-NVS: Diffusion Transformers for Novel View Synthesis,2511.08823v1,dhariwal2021diffusion,\cite{dhariwal2021diffusion},Diffusion Models Beat GANs on Image Synthesis,https://arxiv.org/abs/2105.05233v4,"We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\times$128, 4.59 on ImageNet 256$\times$256, and 7.72 on ImageNet 512$\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\times$256 and 3.85 on ImageNet 512$\times$512. We release our code at https://github.com/openai/guided-diffusion",True,True,,,,,,
DT-NVS: Diffusion Transformers for Novel View Synthesis,2511.08823v1,salimans2022progressive,\cite{salimans2022progressive},Progressive Distillation for Fast Sampling of Diffusion Models,https://arxiv.org/abs/2202.00512v2,"Diffusion models have recently shown great promise for generative modeling, outperforming GANs on perceptual quality and autoregressive models at density estimation. A remaining downside is their slow sampling time: generating high quality samples takes many hundreds or thousands of model evaluations. Here we make two contributions to help eliminate this downside: First, we present new parameterizations of diffusion models that provide increased stability when using few sampling steps. Second, we present a method to distill a trained deterministic diffusion sampler, using many steps, into a new diffusion model that takes half as many sampling steps. We then keep progressively applying this distillation procedure to our model, halving the number of required sampling steps each time. On standard image generation benchmarks like CIFAR-10, ImageNet, and LSUN, we start out with state-of-the-art samplers taking as many as 8192 steps, and are able to distill down to models taking as few as 4 steps without losing much perceptual quality; achieving, for example, a FID of 3.0 on CIFAR-10 in 4 steps. Finally, we show that the full progressive distillation procedure does not take more time than it takes to train the original model, thus representing an efficient solution for generative modeling using diffusion at both train and test time.",True,True,,,,,,
DT-NVS: Diffusion Transformers for Novel View Synthesis,2511.08823v1,sitzmann2019scene,\cite{sitzmann2019scene},Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations,https://arxiv.org/abs/1906.01618v2,"Unsupervised learning with generative models has the potential of discovering rich representations of 3D scenes. While geometric deep learning has explored 3D-structure-aware representations of scene geometry, these models typically require explicit 3D supervision. Emerging neural scene representations can be trained only with posed 2D images, but existing methods ignore the three-dimensional structure of scenes. We propose Scene Representation Networks (SRNs), a continuous, 3D-structure-aware scene representation that encodes both geometry and appearance. SRNs represent scenes as continuous functions that map world coordinates to a feature representation of local scene properties. By formulating the image formation as a differentiable ray-marching algorithm, SRNs can be trained end-to-end from only 2D images and their camera poses, without access to depth or shape. This formulation naturally generalizes across scenes, learning powerful geometry and appearance priors in the process. We demonstrate the potential of SRNs by evaluating them for novel view synthesis, few-shot reconstruction, joint shape and appearance interpolation, and unsupervised discovery of a non-rigid face model.",True,True,,,,,,
DT-NVS: Diffusion Transformers for Novel View Synthesis,2511.08823v1,mildenhall2021nerf,\cite{mildenhall2021nerf},NeRF in the Dark: High Dynamic Range View Synthesis from Noisy Raw Images,https://arxiv.org/abs/2111.13679v1,"Neural Radiance Fields (NeRF) is a technique for high quality novel view synthesis from a collection of posed input images. Like most view synthesis methods, NeRF uses tonemapped low dynamic range (LDR) as input; these images have been processed by a lossy camera pipeline that smooths detail, clips highlights, and distorts the simple noise distribution of raw sensor data. We modify NeRF to instead train directly on linear raw images, preserving the scene's full dynamic range. By rendering raw output images from the resulting NeRF, we can perform novel high dynamic range (HDR) view synthesis tasks. In addition to changing the camera viewpoint, we can manipulate focus, exposure, and tonemapping after the fact. Although a single raw image appears significantly more noisy than a postprocessed one, we show that NeRF is highly robust to the zero-mean distribution of raw noise. When optimized over many noisy raw inputs (25-200), NeRF produces a scene representation so accurate that its rendered novel views outperform dedicated single and multi-image deep raw denoisers run on the same wide baseline input images. As a result, our method, which we call RawNeRF, can reconstruct scenes from extremely noisy images captured in near-darkness.",True,True,,,,,,
DT-NVS: Diffusion Transformers for Novel View Synthesis,2511.08823v1,athar2022rignerf,\cite{athar2022rignerf},RigNeRF: Fully Controllable Neural 3D Portraits,https://arxiv.org/abs/2206.06481v1,"Volumetric neural rendering methods, such as neural radiance fields (NeRFs), have enabled photo-realistic novel view synthesis. However, in their standard form, NeRFs do not support the editing of objects, such as a human head, within a scene. In this work, we propose RigNeRF, a system that goes beyond just novel view synthesis and enables full control of head pose and facial expressions learned from a single portrait video. We model changes in head pose and facial expressions using a deformation field that is guided by a 3D morphable face model (3DMM). The 3DMM effectively acts as a prior for RigNeRF that learns to predict only residuals to the 3DMM deformations and allows us to render novel (rigid) poses and (non-rigid) expressions that were not present in the input sequence. Using only a smartphone-captured short video of a subject for training, we demonstrate the effectiveness of our method on free view synthesis of a portrait scene with explicit head pose and expression controls. The project page can be found here: http://shahrukhathar.github.io/2022/06/06/RigNeRF.html",True,True,,,,,,
DT-NVS: Diffusion Transformers for Novel View Synthesis,2511.08823v1,reizenstein2021common,\cite{reizenstein2021common},Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction,https://arxiv.org/abs/2109.00512v1,"Traditional approaches for learning 3D object categories have been predominantly trained and evaluated on synthetic datasets due to the unavailability of real 3D-annotated category-centric data. Our main goal is to facilitate advances in this field by collecting real-world data in a magnitude similar to the existing synthetic counterparts. The principal contribution of this work is thus a large-scale dataset, called Common Objects in 3D, with real multi-view images of object categories annotated with camera poses and ground truth 3D point clouds. The dataset contains a total of 1.5 million frames from nearly 19,000 videos capturing objects from 50 MS-COCO categories and, as such, it is significantly larger than alternatives both in terms of the number of categories and objects. We exploit this new dataset to conduct one of the first large-scale ""in-the-wild"" evaluations of several new-view-synthesis and category-centric 3D reconstruction methods. Finally, we contribute NerFormer - a novel neural rendering method that leverages the powerful Transformer to reconstruct an object given a small number of its views. The CO3D dataset is available at https://github.com/facebookresearch/co3d .",True,True,,,,,,
DT-NVS: Diffusion Transformers for Novel View Synthesis,2511.08823v1,irshad2023neo360,\cite{irshad2023neo360},NeO 360: Neural Fields for Sparse View Synthesis of Outdoor Scenes,https://arxiv.org/abs/2308.12967v1,"Recent implicit neural representations have shown great results for novel view synthesis. However, existing methods require expensive per-scene optimization from many views hence limiting their application to real-world unbounded urban settings where the objects of interest or backgrounds are observed from very few views. To mitigate this challenge, we introduce a new approach called NeO 360, Neural fields for sparse view synthesis of outdoor scenes. NeO 360 is a generalizable method that reconstructs 360° scenes from a single or a few posed RGB images. The essence of our approach is in capturing the distribution of complex real-world outdoor 3D scenes and using a hybrid image-conditional triplanar representation that can be queried from any world point. Our representation combines the best of both voxel-based and bird's-eye-view (BEV) representations and is more effective and expressive than each. NeO 360's representation allows us to learn from a large collection of unbounded 3D scenes while offering generalizability to new views and novel scenes from as few as a single image during inference. We demonstrate our approach on the proposed challenging 360° unbounded dataset, called NeRDS 360, and show that NeO 360 outperforms state-of-the-art generalizable methods for novel view synthesis while also offering editing and composition capabilities. Project page: https://zubair-irshad.github.io/projects/neo360.html",True,True,,,,,,
DT-NVS: Diffusion Transformers for Novel View Synthesis,2511.08823v1,tang2023dreamgaussian,\cite{tang2023dreamgaussian},Universal entanglement signatures of interface conformal field theories,https://arxiv.org/abs/2308.03646v2,"An interface connecting two distinct conformal field theories hosts rich critical behaviors. In this work, we investigate the entanglement properties of such critical interface theories for probing the underlying universality. As inspired by holographic perspectives, we demonstrate vital features of various entanglement measures regarding such interfaces based on several paradigmatic lattice models. Crucially, for two subsystems adjacent at the interface, the mutual information and the reflected entropy exhibit identical leading logarithmic scaling, giving an effective interface central charge that takes the same value as the smaller central charge of the two conformal field theories. Our work demonstrates that the entanglement measure offers a powerful tool to explore the rich physics in critical interface theories.",True,True,,,,,,
DT-NVS: Diffusion Transformers for Novel View Synthesis,2511.08823v1,poole2022dreamfusion,\cite{poole2022dreamfusion},DreamFusion: Text-to-3D using 2D Diffusion,https://arxiv.org/abs/2209.14988v1,"Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors.",True,True,,,,,,
DT-NVS: Diffusion Transformers for Novel View Synthesis,2511.08823v1,karnewar2023holodiffusion,\cite{karnewar2023holodiffusion},HoloDiffusion: Training a 3D Diffusion Model using 2D Images,https://arxiv.org/abs/2303.16509v2,"Diffusion models have emerged as the best approach for generative modeling of 2D images. Part of their success is due to the possibility of training them on millions if not billions of images with a stable learning objective. However, extending these models to 3D remains difficult for two reasons. First, finding a large quantity of 3D training data is much more complex than for 2D images. Second, while it is conceptually trivial to extend the models to operate on 3D rather than 2D grids, the associated cubic growth in memory and compute complexity makes this infeasible. We address the first challenge by introducing a new diffusion setup that can be trained, end-to-end, with only posed 2D images for supervision; and the second challenge by proposing an image formation model that decouples model memory from spatial memory. We evaluate our method on real-world data, using the CO3D dataset which has not been used to train 3D generative models before. We show that our diffusion models are scalable, train robustly, and are competitive in terms of sample quality and fidelity to existing approaches for 3D generative modeling.",True,True,,,,,,
DT-NVS: Diffusion Transformers for Novel View Synthesis,2511.08823v1,anciukevicius2024denoising,\cite{anciukevicius2024denoising},Denoising Diffusion via Image-Based Rendering,https://arxiv.org/abs/2402.03445v2,"Generating 3D scenes is a challenging open problem, which requires synthesizing plausible content that is fully consistent in 3D space. While recent methods such as neural radiance fields excel at view synthesis and 3D reconstruction, they cannot synthesize plausible details in unobserved regions since they lack a generative capability. Conversely, existing generative methods are typically not capable of reconstructing detailed, large-scale scenes in the wild, as they use limited-capacity 3D scene representations, require aligned camera poses, or rely on additional regularizers. In this work, we introduce the first diffusion model able to perform fast, detailed reconstruction and generation of real-world 3D scenes. To achieve this, we make three contributions. First, we introduce a new neural scene representation, IB-planes, that can efficiently and accurately represent large 3D scenes, dynamically allocating more capacity as needed to capture details visible in each image. Second, we propose a denoising-diffusion framework to learn a prior over this novel 3D scene representation, using only 2D images without the need for any additional supervision signal such as masks or depths. This supports 3D reconstruction and generation in a unified architecture. Third, we develop a principled approach to avoid trivial 3D solutions when integrating the image-based rendering with the diffusion model, by dropping out representations of some images. We evaluate the model on several challenging datasets of real and synthetic images, and demonstrate superior results on generation, novel view synthesis and 3D reconstruction.",True,True,,,,,,
DT-NVS: Diffusion Transformers for Novel View Synthesis,2511.08823v1,szymanowicz2023viewset,\cite{szymanowicz2023viewset},Viewset Diffusion: (0-)Image-Conditioned 3D Generative Models from 2D Data,https://arxiv.org/abs/2306.07881v2,"We present Viewset Diffusion, a diffusion-based generator that outputs 3D objects while only using multi-view 2D data for supervision. We note that there exists a one-to-one mapping between viewsets, i.e., collections of several 2D views of an object, and 3D models. Hence, we train a diffusion model to generate viewsets, but design the neural network generator to reconstruct internally corresponding 3D models, thus generating those too. We fit a diffusion model to a large number of viewsets for a given category of objects. The resulting generator can be conditioned on zero, one or more input views. Conditioned on a single view, it performs 3D reconstruction accounting for the ambiguity of the task and allowing to sample multiple solutions compatible with the input. The model performs reconstruction efficiently, in a feed-forward manner, and is trained using only rendering losses using as few as three views per viewset. Project page: szymanowiczs.github.io/viewset-diffusion.",True,True,,,,,,
DT-NVS: Diffusion Transformers for Novel View Synthesis,2511.08823v1,dosovitskiy2020image,\cite{dosovitskiy2020image},An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,https://arxiv.org/abs/2010.11929v2,"While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",True,True,,,,,,
DT-NVS: Diffusion Transformers for Novel View Synthesis,2511.08823v1,kulhanek2022viewformer,\cite{kulhanek2022viewformer},ViewFormer: NeRF-free Neural Rendering from Few Images Using Transformers,https://arxiv.org/abs/2203.10157v2,"Novel view synthesis is a long-standing problem. In this work, we consider a variant of the problem where we are given only a few context views sparsely covering a scene or an object. The goal is to predict novel viewpoints in the scene, which requires learning priors. The current state of the art is based on Neural Radiance Field (NeRF), and while achieving impressive results, the methods suffer from long training times as they require evaluating millions of 3D point samples via a neural network for each image. We propose a 2D-only method that maps multiple context views and a query pose to a new image in a single pass of a neural network. Our model uses a two-stage architecture consisting of a codebook and a transformer model. The codebook is used to embed individual images into a smaller latent space, and the transformer solves the view synthesis task in this more compact space. To train our model efficiently, we introduce a novel branching attention mechanism that allows us to use the same model not only for neural rendering but also for camera pose estimation. Experimental results on real-world scenes show that our approach is competitive compared to NeRF-based methods while not reasoning explicitly in 3D, and it is faster to train.",True,True,,,,,,
DT-NVS: Diffusion Transformers for Novel View Synthesis,2511.08823v1,sajjadi2022scene,\cite{sajjadi2022scene},RUST: Latent Neural Scene Representations from Unposed Imagery,https://arxiv.org/abs/2211.14306v2,"Inferring the structure of 3D scenes from 2D observations is a fundamental challenge in computer vision. Recently popularized approaches based on neural scene representations have achieved tremendous impact and have been applied across a variety of applications. One of the major remaining challenges in this space is training a single model which can provide latent representations which effectively generalize beyond a single scene. Scene Representation Transformer (SRT) has shown promise in this direction, but scaling it to a larger set of diverse scenes is challenging and necessitates accurately posed ground truth data. To address this problem, we propose RUST (Really Unposed Scene representation Transformer), a pose-free approach to novel view synthesis trained on RGB images alone. Our main insight is that one can train a Pose Encoder that peeks at the target image and learns a latent pose embedding which is used by the decoder for view synthesis. We perform an empirical investigation into the learned latent pose structure and show that it allows meaningful test-time camera transformations and accurate explicit pose readouts. Perhaps surprisingly, RUST achieves similar quality as methods which have access to perfect camera pose, thereby unlocking the potential for large-scale training of amortized neural scene representations.",True,True,,,,,,
DT-NVS: Diffusion Transformers for Novel View Synthesis,2511.08823v1,jun2023shap,\cite{jun2023shap},JUNO sensitivity to the annihilation of MeV dark matter in the galactic halo,https://arxiv.org/abs/2306.09567v3,"We discuss JUNO sensitivity to the annihilation of MeV dark matter in the galactic halo via detecting inverse beta decay reactions of electron anti-neutrinos resulting from the annihilation. We study possible backgrounds to the signature, including the reactor neutrinos, diffuse supernova neutrino background, charged- and neutral-current interactions of atmospheric neutrinos, backgrounds from muon-induced fast neutrons and cosmogenic isotopes. A fiducial volume cut, as well as the pulse shape discrimination and the muon veto are applied to suppress the above backgrounds. It is shown that JUNO sensitivity to the thermally averaged dark matter annihilation rate in 10 years of exposure would be significantly better than the present-day best limit set by Super-Kamiokande and would be comparable to that expected by Hyper-Kamiokande.",True,True,,,,,,
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,pang2023mre,\cite{pang2023mre},MRE-Net: Multi-rate excitation network for deepfake video detection,,,True,False,"Pang, Guilin and Zhang, Baopeng and Teng, Zhu and Qi, Zige and Fan, Jianping",2023,,,,IEEE Transactions on Circuits and Systems for Video Technology
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,wang2023altfreezing,\cite{wang2023altfreezing},Altfreezing for more general video face forgery detection,,,True,False,"Wang, Zhendong and Bao, Jianmin and Zhou, Wengang and Wang, Weilun and Li, Houqiang",2023,,,,
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,zhang2024learning,\cite{zhang2024learning},Learning Natural Consistency Representation for Face Forgery Video Detection,,,True,False,"Zhang, Daichi and Xiao, Zihao and Li, Shikun and Lin, Fanzhao and Li, Jianmin and Ge, Shiming",2024,,,,arXiv preprint arXiv:2407.10550
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,yu2023augmented,\cite{yu2023augmented},Augmented Multi-Scale Spatiotemporal Inconsistency Magnifier for Generalized DeepFake Detection,,,True,False,"Yu, Yang and Zhao, Xiaohui and Ni, Rongrong and Yang, Siyuan and Zhao, Yao and Kot, Alex C.",2023,,,10.1109/TMM.2023.3237322,IEEE Transactions on Multimedia
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,choi2024exploiting,\cite{choi2024exploiting},Exploiting Style Latent Flows for Generalizing Deepfake Detection Video Detection,,,True,False,"Choi, Jongwook and Kim, Taehoon and Jeong, Yonghyun and Baek, Seungryul and Choi, Jongwon",2024,,,,arXiv preprint arXiv:2403.06592
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,zheng2021exploring,\cite{zheng2021exploring},Exploring temporal coherence for more general video face forgery detection,,,True,False,"Zheng, Yinglin and Bao, Jianmin and Chen, Dong and Zeng, Ming and Wen, Fang",2021,,,,
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,ge2022deepfake,\cite{ge2022deepfake},Deepfake video detection via predictive representation learning,,,True,False,"Ge, Shiming and Lin, Fanzhao and Li, Chenyu and Zhang, Daichi and Wang, Weiping and Zeng, Dan",2022,,,,"ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)"
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,li2020face,\cite{li2020face},Face x-ray for more general face forgery detection,,,True,False,"Li, Lingzhi and Bao, Jianmin and Zhang, Ting and Yang, Hao and Chen, Dong and Wen, Fang and Guo, Baining",2020,,,,
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,liu2020global,\cite{liu2020global},Global texture enhancement for fake face detection in the wild,,,True,False,"Liu, Zhengzhe and Qi, Xiaojuan and Torr, Philip HS",2020,,,,
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,zhao2021multi,\cite{zhao2021multi},Multi-attentional Deepfake Detection,https://arxiv.org/abs/2103.02406v3,"Face forgery by deepfake is widely spread over the internet and has raised severe societal concerns. Recently, how to detect such forgery contents has become a hot research topic and many deepfake detection methods have been proposed. Most of them model deepfake detection as a vanilla binary classification problem, i.e, first use a backbone network to extract a global feature and then feed it into a binary classifier (real/fake). But since the difference between the real and fake images in this task is often subtle and local, we argue this vanilla solution is not optimal. In this paper, we instead formulate deepfake detection as a fine-grained classification problem and propose a new multi-attentional deepfake detection network. Specifically, it consists of three key components: 1) multiple spatial attention heads to make the network attend to different local parts; 2) textural feature enhancement block to zoom in the subtle artifacts in shallow features; 3) aggregate the low-level textural feature and high-level semantic features guided by the attention maps. Moreover, to address the learning difficulty of this network, we further introduce a new regional independence loss and an attention guided data augmentation strategy. Through extensive experiments on different datasets, we demonstrate the superiority of our method over the vanilla binary classifier counterparts, and achieve state-of-the-art performance.",True,True,"Zhao, Hanqing and Zhou, Wenbo and Chen, Dongdong and Wei, Tianyi and Zhang, Weiming and Yu, Nenghai",2021,,,,
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,10286083,\cite{10286083},Constructing New Backbone Networks via Space-Frequency Interactive Convolution for Deepfake Detection,,,True,False,"Guo, Zhiqing and Jia, Zhenhong and Wang, Liejun and Wang, Dewang and Yang, Gaobo and Kasabov, Nikola",2024,,,10.1109/TIFS.2023.3324739,IEEE Transactions on Information Forensics and Security
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,gu2022exploiting,\cite{gu2022exploiting},Exploiting Fine-grained Face Forgery Clues via Progressive Enhancement Learning,https://arxiv.org/abs/2112.13977v1,"With the rapid development of facial forgery techniques, forgery detection has attracted more and more attention due to security concerns. Existing approaches attempt to use frequency information to mine subtle artifacts under high-quality forged faces. However, the exploitation of frequency information is coarse-grained, and more importantly, their vanilla learning process struggles to extract fine-grained forgery traces. To address this issue, we propose a progressive enhancement learning framework to exploit both the RGB and fine-grained frequency clues. Specifically, we perform a fine-grained decomposition of RGB images to completely decouple the real and fake traces in the frequency space. Subsequently, we propose a progressive enhancement learning framework based on a two-branch network, combined with self-enhancement and mutual-enhancement modules. The self-enhancement module captures the traces in different input spaces based on spatial noise enhancement and channel attention. The Mutual-enhancement module concurrently enhances RGB and frequency features by communicating in the shared spatial dimension. The progressive enhancement process facilitates the learning of discriminative features with fine-grained face forgery clues. Extensive experiments on several datasets show that our method outperforms the state-of-the-art face forgery detection methods.",True,True,"Gu, Qiqi and Chen, Shen and Yao, Taiping and Chen, Yang and Ding, Shouhong and Yi, Ran",2022,,,,
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,jeong2022frepgan,\cite{jeong2022frepgan},FrePGAN: Robust Deepfake Detection Using Frequency-level Perturbations,https://arxiv.org/abs/2202.03347v1,"Various deepfake detectors have been proposed, but challenges still exist to detect images of unknown categories or GAN models outside of the training settings. Such issues arise from the overfitting issue, which we discover from our own analysis and the previous studies to originate from the frequency-level artifacts in generated images. We find that ignoring the frequency-level artifacts can improve the detector's generalization across various GAN models, but it can reduce the model's performance for the trained GAN models. Thus, we design a framework to generalize the deepfake detector for both the known and unseen GAN models. Our framework generates the frequency-level perturbation maps to make the generated images indistinguishable from the real images. By updating the deepfake detector along with the training of the perturbation generator, our model is trained to detect the frequency-level artifacts at the initial iterations and consider the image-level irregularities at the last iterations. For experiments, we design new test scenarios varying from the training settings in GAN models, color manipulations, and object categories. Numerous experiments validate the state-of-the-art performance of our deepfake detector.",True,True,"Jeong, Yonghyun and Kim, Doyeon and Ro, Youngmin and Choi, Jongwon",2022,,,,
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,liu2024hierarchical,\cite{liu2024hierarchical},Hierarchical Forgery Classifier On Multi-modality Face Forgery Clues,https://arxiv.org/abs/2212.14629v2,"Face forgery detection plays an important role in personal privacy and social security. With the development of adversarial generative models, high-quality forgery images become more and more indistinguishable from real to humans. Existing methods always regard as forgery detection task as the common binary or multi-label classification, and ignore exploring diverse multi-modality forgery image types, e.g. visible light spectrum and near-infrared scenarios. In this paper, we propose a novel Hierarchical Forgery Classifier for Multi-modality Face Forgery Detection (HFC-MFFD), which could effectively learn robust patches-based hybrid domain representation to enhance forgery authentication in multiple-modality scenarios. The local spatial hybrid domain feature module is designed to explore strong discriminative forgery clues both in the image and frequency domain in local distinct face regions. Furthermore, the specific hierarchical face forgery classifier is proposed to alleviate the class imbalance problem and further boost detection performance. Experimental results on representative multi-modality face forgery datasets demonstrate the superior performance of the proposed HFC-MFFD compared with state-of-the-art algorithms. The source code and models are publicly available at https://github.com/EdWhites/HFC-MFFD.",True,True,"Liu, Decheng and Zheng, Zeyang and Peng, Chunlei and Wang, Yukai and Wang, Nannan and Gao, Xinbo",2024,,,10.1109/TMM.2023.3304913,IEEE Transactions on Multimedia
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,10107603,\cite{10107603},Interactive Two-Stream Network Across Modalities for Deepfake Detection,,,True,False,"Wu, Jianghao and Zhang, Baopeng and Li, Zhaoyang and Pang, Guilin and Teng, Zhu and Fan, Jianping",2023,,,10.1109/TCSVT.2023.3269841,IEEE Transactions on Circuits and Systems for Video Technology
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,chen2024compressed,\cite{chen2024compressed},Compressed Deepfake Video Detection Based on 3D Spatiotemporal Trajectories,https://arxiv.org/abs/2404.18149v1,"The misuse of deepfake technology by malicious actors poses a potential threat to nations, societies, and individuals. However, existing methods for detecting deepfakes primarily focus on uncompressed videos, such as noise characteristics, local textures, or frequency statistics. When applied to compressed videos, these methods experience a decrease in detection performance and are less suitable for real-world scenarios. In this paper, we propose a deepfake video detection method based on 3D spatiotemporal trajectories. Specifically, we utilize a robust 3D model to construct spatiotemporal motion features, integrating feature details from both 2D and 3D frames to mitigate the influence of large head rotation angles or insufficient lighting within frames. Furthermore, we separate facial expressions from head movements and design a sequential analysis method based on phase space motion trajectories to explore the feature differences between genuine and fake faces in deepfake videos. We conduct extensive experiments to validate the performance of our proposed method on several compressed deepfake benchmarks. The robustness of the well-designed features is verified by calculating the consistent distribution of facial landmarks before and after video compression.Our method yields satisfactory results and showcases its potential for practical applications.",True,True,"Chen, Zongmei and Liao, Xin and Wu, Xiaoshuai and Chen, Yanxiang",2024,,,,arXiv preprint arXiv:2404.18149
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,wang2023noise,\cite{wang2023noise},Noise based deepfake detection via multi-head relative-interaction,,,True,False,"Wang, Tianyi and Chow, Kam Pui",2023,,,,
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,qiao2024deepfake,\cite{qiao2024deepfake},Deepfake Detection Fighting Against Noisy Label Attack,,,True,False,"Qiao, Tong and Xie, Shichuang and Chen, Yanli and Retraint, Florent and Shi, Ran and Luo, Xiangyang",2024,,,10.1109/TMM.2024.3385286,IEEE Transactions on Multimedia
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,xu2023tall,\cite{xu2023tall},Tall: Thumbnail layout for deepfake video detection,,,True,False,"Xu, Yuting and Liang, Jian and Jia, Gengyun and Yang, Ziming and Zhang, Yanhao and He, Ran",2023,,,,
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,xu2024towards,\cite{xu2024towards},Towards Generalizable Deepfake Video Detection with Thumbnail Layout and Graph Reasoning,,,True,False,"Xu, Yuting and Liang, Jian and Sheng, Lijun and Zhang, Xiao-Yu",2024,,,,arXiv preprint arXiv:2403.10261
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,cozzolino2021id,\cite{cozzolino2021id},ID-Reveal: Identity-aware DeepFake Video Detection,https://arxiv.org/abs/2012.02512v3,"A major challenge in DeepFake forgery detection is that state-of-the-art algorithms are mostly trained to detect a specific fake method. As a result, these approaches show poor generalization across different types of facial manipulations, e.g., from face swapping to facial reenactment. To this end, we introduce ID-Reveal, a new approach that learns temporal facial features, specific of how a person moves while talking, by means of metric learning coupled with an adversarial training strategy. The advantage is that we do not need any training data of fakes, but only train on real videos. Moreover, we utilize high-level semantic features, which enables robustness to widespread and disruptive forms of post-processing. We perform a thorough experimental analysis on several publicly available benchmarks. Compared to state of the art, our method improves generalization and is more robust to low-quality videos, that are usually spread over social networks. In particular, we obtain an average improvement of more than 15% in terms of accuracy for facial reenactment on high compressed videos.",True,True,"Cozzolino, Davide and R{\""o}ssler, Andreas and Thies, Justus and Nie{\ss}ner, Matthias and Verdoliva, Luisa",2021,,,,
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,huang2023implicit,\cite{huang2023implicit},Implicit identity driven deepfake face swapping detection,,,True,False,"Huang, Baojin and Wang, Zhongyuan and Yang, Jifan and Ai, Jiaxin and Zou, Qin and Wang, Qian and Ye, Dengpan",2023,,,,
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,rossler2019faceforensics++,\cite{rossler2019faceforensics++},FaceForensics++: Learning to Detect Manipulated Facial Images,https://arxiv.org/abs/1901.08971v3,"The rapid progress in synthetic image generation and manipulation has now come to a point where it raises significant concerns for the implications towards society. At best, this leads to a loss of trust in digital content, but could potentially cause further harm by spreading false information or fake news. This paper examines the realism of state-of-the-art image manipulations, and how difficult it is to detect them, either automatically or by humans. To standardize the evaluation of detection methods, we propose an automated benchmark for facial manipulation detection. In particular, the benchmark is based on DeepFakes, Face2Face, FaceSwap and NeuralTextures as prominent representatives for facial manipulations at random compression level and size. The benchmark is publicly available and contains a hidden test set as well as a database of over 1.8 million manipulated images. This dataset is over an order of magnitude larger than comparable, publicly available, forgery datasets. Based on this data, we performed a thorough analysis of data-driven forgery detectors. We show that the use of additional domainspecific knowledge improves forgery detection to unprecedented accuracy, even in the presence of strong compression, and clearly outperforms human observers.",True,True,"Rossler, Andreas and Cozzolino, Davide and Verdoliva, Luisa and Riess, Christian and Thies, Justus and Nie{\ss}ner, Matthias",2019,,,,
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,yan2024transcending,\cite{yan2024transcending},Transcending forgery specificity with latent space augmentation for generalizable deepfake detection,,,True,False,"Yan, Zhiyuan and Luo, Yuhao and Lyu, Siwei and Liu, Qingshan and Wu, Baoyuan",2024,,,,
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,tan2024rethinking,\cite{tan2024rethinking},Rethinking the up-sampling operations in cnn-based generative network for generalizable deepfake detection,,,True,False,"Tan, Chuangchuang and Zhao, Yao and Wei, Shikui and Gu, Guanghua and Liu, Ping and Wei, Yunchao",2024,,,,
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,11045799,\cite{11045799},SAGNet: Decoupling Semantic-Agnostic Artifacts From Limited Training Data for Robust Generalization in Deepfake Detection,,,True,False,"Tao, Renshuai and Tan, Chuangchuang and Liu, Huan and Wang, Jiakai and Qin, Haotong and Chang, Yakun and Wang, Wei and Ni, Rongrong and Zhao, Yao",2025,,,10.1109/TIFS.2025.3581726,IEEE Transactions on Information Forensics and Security
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,lin2024preserving,\cite{lin2024preserving},Preserving Fairness Generalization in Deepfake Detection,https://arxiv.org/abs/2402.17229v1,"Although effective deepfake detection models have been developed in recent years, recent studies have revealed that these models can result in unfair performance disparities among demographic groups, such as race and gender. This can lead to particular groups facing unfair targeting or exclusion from detection, potentially allowing misclassified deepfakes to manipulate public opinion and undermine trust in the model. The existing method for addressing this problem is providing a fair loss function. It shows good fairness performance for intra-domain evaluation but does not maintain fairness for cross-domain testing. This highlights the significance of fairness generalization in the fight against deepfakes. In this work, we propose the first method to address the fairness generalization problem in deepfake detection by simultaneously considering features, loss, and optimization aspects. Our method employs disentanglement learning to extract demographic and domain-agnostic forgery features, fusing them to encourage fair learning across a flattened loss landscape. Extensive experiments on prominent deepfake datasets demonstrate our method's effectiveness, surpassing state-of-the-art approaches in preserving fairness during cross-domain deepfake detection. The code is available at https://github.com/Purdue-M2/Fairness-Generalization",True,True,"Lin, Li and He, Xinan and Ju, Yan and Wang, Xin and Ding, Feng and Hu, Shu",2024,,,,
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,10516609,\cite{10516609},Improving Generalization of Deepfake Detectors by Imposing Gradient Regularization,,,True,False,"Guan, Weinan and Wang, Wei and Dong, Jing and Peng, Bo",2024,,,10.1109/TIFS.2024.3396064,IEEE Transactions on Information Forensics and Security
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,nie2024dip,\cite{nie2024dip},DIP: Diffusion Learning of Inconsistency Pattern for General DeepFake Detection,https://arxiv.org/abs/2410.23663v1,"With the advancement of deepfake generation techniques, the importance of deepfake detection in protecting multimedia content integrity has become increasingly obvious. Recently, temporal inconsistency clues have been explored to improve the generalizability of deepfake video detection. According to our observation, the temporal artifacts of forged videos in terms of motion information usually exhibits quite distinct inconsistency patterns along horizontal and vertical directions, which could be leveraged to improve the generalizability of detectors. In this paper, a transformer-based framework for Diffusion Learning of Inconsistency Pattern (DIP) is proposed, which exploits directional inconsistencies for deepfake video detection. Specifically, DIP begins with a spatiotemporal encoder to represent spatiotemporal information. A directional inconsistency decoder is adopted accordingly, where direction-aware attention and inconsistency diffusion are incorporated to explore potential inconsistency patterns and jointly learn the inherent relationships. In addition, the SpatioTemporal Invariant Loss (STI Loss) is introduced to contrast spatiotemporally augmented sample pairs and prevent the model from overfitting nonessential forgery artifacts. Extensive experiments on several public datasets demonstrate that our method could effectively identify directional forgery clues and achieve state-of-the-art performance.",True,True,"Nie, Fan and Ni, Jiangqun and Zhang, Jian and Zhang, Bin and Zhang, Weizhe",2024,,,10.1109/TMM.2024.3521766,IEEE Transactions on Multimedia
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,11098842,\cite{11098842},IDCNet: Image Decomposition and Cross-View Distillation for Generalizable Deepfake Detection,,,True,False,"Wang, Zhiyuan and Chen, Yanxiang and Yao, Yuanzhi and Han, Meng and Xing, Wenpeng and Li, Meng",2025,,,10.1109/TIFS.2025.3593353,IEEE Transactions on Information Forensics and Security
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,10654318,\cite{10654318},Using Graph Neural Networks to Improve Generalization Capability of the Models for Deepfake Detection,,,True,False,"She, Huimin and Hu, Yongjian and Liu, Beibei and Li, Jicheng and Li, Chang-Tsun",2024,,,10.1109/TIFS.2024.3451356,IEEE Transactions on Information Forensics and Security
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,haliassos2021lips,\cite{haliassos2021lips},Lips don't lie: A generalisable and robust approach to face forgery detection,,,True,False,"Haliassos, Alexandros and Vougioukas, Konstantinos and Petridis, Stavros and Pantic, Maja",2021,,,,
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,haliassos2022leveraging,\cite{haliassos2022leveraging},Leveraging Real Talking Faces via Self-Supervision for Robust Forgery Detection,https://arxiv.org/abs/2201.07131v3,"One of the most pressing challenges for the detection of face-manipulated videos is generalising to forgery methods not seen during training while remaining effective under common corruptions such as compression. In this paper, we examine whether we can tackle this issue by harnessing videos of real talking faces, which contain rich information on natural facial appearance and behaviour and are readily available in large quantities online. Our method, termed RealForensics, consists of two stages. First, we exploit the natural correspondence between the visual and auditory modalities in real videos to learn, in a self-supervised cross-modal manner, temporally dense video representations that capture factors such as facial movements, expression, and identity. Second, we use these learned representations as targets to be predicted by our forgery detector along with the usual binary forgery classification task; this encourages it to base its real/fake decision on said factors. We show that our method achieves state-of-the-art performance on cross-manipulation generalisation and robustness experiments, and examine the factors that contribute to its performance. Our results suggest that leveraging natural and unlabelled videos is a promising direction for the development of more robust face forgery detectors.",True,True,"Haliassos, Alexandros and Mira, Rodrigo and Petridis, Stavros and Pantic, Maja",2022,,,,
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,wang2022deepfake,\cite{wang2022deepfake},Deepfake Forensics via An Adversarial Game,https://arxiv.org/abs/2103.13567v2,"With the progress in AI-based facial forgery (i.e., deepfake), people are increasingly concerned about its abuse. Albeit effort has been made for training classification (also known as deepfake detection) models to recognize such forgeries, existing models suffer from poor generalization to unseen forgery technologies and high sensitivity to changes in image/video quality. In this paper, we advocate adversarial training for improving the generalization ability to both unseen facial forgeries and unseen image/video qualities. We believe training with samples that are adversarially crafted to attack the classification models improves the generalization ability considerably. Considering that AI-based face manipulation often leads to high-frequency artifacts that can be easily spotted by models yet difficult to generalize, we further propose a new adversarial training method that attempts to blur out these specific artifacts, by introducing pixel-wise Gaussian blurring models. With adversarial training, the classification models are forced to learn more discriminative and generalizable features, and the effectiveness of our method can be verified by plenty of empirical evidence. Our code will be made publicly available.",True,True,"Wang, Zhi and Guo, Yiwen and Zuo, Wangmeng",2022,,,10.1109/TIP.2022.3172845,IEEE Transactions on Image Processing
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,prashnani2025generalizable,\cite{prashnani2025generalizable},Generalizable Deepfake Detection With Phase-Based Motion Analysis,,,True,False,"Prashnani, Ekta and Goebel, Michael and Manjunath, B. S.",2025,,,10.1109/TIP.2024.3441821,IEEE Transactions on Image Processing
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,chugh2020not,\cite{chugh2020not},Not made for each other-audio-visual dissonance-based deepfake detection and localization,,,True,False,"Chugh, Komal and Gupta, Parul and Dhall, Abhinav and Subramanian, Ramanathan",2020,,,,
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,liu2023mcl,\cite{liu2023mcl},Mcl: multimodal contrastive learning for deepfake detection,,,True,False,"Liu, Xiaolong and Yu, Yang and Li, Xiaolong and Zhao, Yao",2023,,,,IEEE Transactions on Circuits and Systems for Video Technology
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,yang2023avoid,\cite{yang2023avoid},Avoid-df: Audio-visual joint learning for detecting deepfake,,,True,False,"Yang, Wenyuan and Zhou, Xiaoyu and Chen, Zhikai and Guo, Bofei and Ba, Zhongjie and Xia, Zhihua and Cao, Xiaochun and Ren, Kui",2023,,,,IEEE Transactions on Information Forensics and Security
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,zou2024cross,\cite{zou2024cross},Cross-Modality and Within-Modality Regularization for Audio-Visual Deepfake Detection,,,True,False,"Zou, Heqing and Shen, Meng and Hu, Yuchen and Chen, Chen and Chng, Eng Siong and Rajan, Deepu",2024,,,,
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,shi2022learning,\cite{shi2022learning},Learning audio-visual speech representation by masked multimodal cluster prediction,,,True,False,"Shi, Bowen and Hsu, Wei-Ning and Lakhotia, Kushal and Mohamed, Abdelrahman",2022,,,,arXiv preprint arXiv:2201.02184
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,zhou2021joint,\cite{zhou2021joint},Joint audio-visual deepfake detection,,,True,False,"Zhou, Yipin and Lim, Ser-Nam",2021,,,,
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,yu2023pvass,\cite{yu2023pvass},PVASS-MDD: Predictive visual-audio alignment self-supervision for multimodal deepfake detection,,,True,False,"Yu, Yang and Liu, Xiaolong and Ni, Rongrong and Yang, Siyuan and Zhao, Yao and Kot, Alex C",2023,,,,IEEE Transactions on Circuits and Systems for Video Technology
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,cheng2023voice,\cite{cheng2023voice},Voice-Face Homogeneity Tells Deepfake,https://arxiv.org/abs/2203.02195v3,"Detecting forgery videos is highly desirable due to the abuse of deepfake. Existing detection approaches contribute to exploring the specific artifacts in deepfake videos and fit well on certain data. However, the growing technique on these artifacts keeps challenging the robustness of traditional deepfake detectors. As a result, the development of generalizability of these approaches has reached a blockage. To address this issue, given the empirical results that the identities behind voices and faces are often mismatched in deepfake videos, and the voices and faces have homogeneity to some extent, in this paper, we propose to perform the deepfake detection from an unexplored voice-face matching view. To this end, a voice-face matching method is devised to measure the matching degree of these two. Nevertheless, training on specific deepfake datasets makes the model overfit certain traits of deepfake algorithms. We instead, advocate a method that quickly adapts to untapped forgery, with a pre-training then fine-tuning paradigm. Specifically, we first pre-train the model on a generic audio-visual dataset, followed by the fine-tuning on downstream deepfake data. We conduct extensive experiments over three widely exploited deepfake datasets - DFDC, FakeAVCeleb, and DeepfakeTIMIT. Our method obtains significant performance gains as compared to other state-of-the-art competitors. It is also worth noting that our method already achieves competitive results when fine-tuned on limited deepfake data.",True,True,"Cheng, Harry and Guo, Yangyang and Wang, Tianyi and Li, Qi and Chang, Xiaojun and Nie, Liqiang",2023,,,,"ACM Transactions on Multimedia Computing, Communications and Applications"
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,feng2023self,\cite{feng2023self},Self-Supervised Video Forensics by Audio-Visual Anomaly Detection,https://arxiv.org/abs/2301.01767v2,"Manipulated videos often contain subtle inconsistencies between their visual and audio signals. We propose a video forensics method, based on anomaly detection, that can identify these inconsistencies, and that can be trained solely using real, unlabeled data. We train an autoregressive model to generate sequences of audio-visual features, using feature sets that capture the temporal synchronization between video frames and sound. At test time, we then flag videos that the model assigns low probability. Despite being trained entirely on real videos, our model obtains strong performance on the task of detecting manipulated speech videos. Project site: https://cfeng16.github.io/audio-visual-forensics",True,True,"Feng, Chao and Chen, Ziyang and Owens, Andrew",2023,,,,
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,chen2021audio,\cite{chen2021audio},Audio-visual synchronisation in the wild,,,True,False,"Chen, Honglie and Xie, Weidi and Afouras, Triantafyllos and Nagrani, Arsha and Vedaldi, Andrea and Zisserman, Andrew",2021,,,,arXiv preprint arXiv:2112.04432
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,oorloff2024avff,\cite{oorloff2024avff},AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection,https://arxiv.org/abs/2406.02951v1,"With the rapid growth in deepfake video content, we require improved and generalizable methods to detect them. Most existing detection methods either use uni-modal cues or rely on supervised training to capture the dissonance between the audio and visual modalities. While the former disregards the audio-visual correspondences entirely, the latter predominantly focuses on discerning audio-visual cues within the training corpus, thereby potentially overlooking correspondences that can help detect unseen deepfakes. We present Audio-Visual Feature Fusion (AVFF), a two-stage cross-modal learning method that explicitly captures the correspondence between the audio and visual modalities for improved deepfake detection. The first stage pursues representation learning via self-supervision on real videos to capture the intrinsic audio-visual correspondences. To extract rich cross-modal representations, we use contrastive learning and autoencoding objectives, and introduce a novel audio-visual complementary masking and feature fusion strategy. The learned representations are tuned in the second stage, where deepfake classification is pursued via supervised learning on both real and fake videos. Extensive experiments and analysis suggest that our novel representation learning paradigm is highly discriminative in nature. We report 98.6% accuracy and 99.1% AUC on the FakeAVCeleb dataset, outperforming the current audio-visual state-of-the-art by 14.9% and 9.9%, respectively.",True,True,"Oorloff, Trevine and Koppisetti, Surya and Bonettini, Nicol{\`o} and Solanki, Divyaraj and Colman, Ben and Yacoob, Yaser and Shahriyari, Ali and Bharaj, Gaurav",2024,,,,
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,gong2022contrastive,\cite{gong2022contrastive},Contrastive Audio-Visual Masked Autoencoder,https://arxiv.org/abs/2210.07839v4,"In this paper, we first extend the recent Masked Auto-Encoder (MAE) model from a single modality to audio-visual multi-modalities. Subsequently, we propose the Contrastive Audio-Visual Masked Auto-Encoder (CAV-MAE) by combining contrastive learning and masked data modeling, two major self-supervised learning frameworks, to learn a joint and coordinated audio-visual representation. Our experiments show that the contrastive audio-visual correspondence learning objective not only enables the model to perform audio-visual retrieval tasks, but also helps the model learn a better joint representation. As a result, our fully self-supervised pretrained CAV-MAE achieves a new SOTA accuracy of 65.9% on VGGSound, and is comparable with the previous best supervised pretrained model on AudioSet in the audio-visual event classification task. Code and pretrained models are at https://github.com/yuangongnd/cav-mae.",True,True,"Gong, Yuan and Rouditchenko, Andrew and Liu, Alexander H and Harwath, David and Karlinsky, Leonid and Kuehne, Hilde and Glass, James",2022,,,,arXiv preprint arXiv:2210.07839
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,lin2018bsn,\cite{lin2018bsn},Bsn: Boundary sensitive network for temporal action proposal generation,,,True,False,"Lin, Tianwei and Zhao, Xu and Su, Haisheng and Wang, Chongjing and Yang, Ming",2018,,,,
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,lin2019bmn,\cite{lin2019bmn},Bmn: Boundary-matching network for temporal action proposal generation,,,True,False,"Lin, Tianwei and Liu, Xiao and Li, Xin and Ding, Errui and Wen, Shilei",2019,,,,
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,su2021bsn++,\cite{su2021bsn++},Bsn++: Complementary boundary regressor with scale-balanced relation modeling for temporal action proposal generation,,,True,False,"Su, Haisheng and Gan, Weihao and Wu, Wei and Qiao, Yu and Yan, Junjie",2021,,,,
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,zhang2022actionformer,\cite{zhang2022actionformer},Actionformer: Localizing moments of actions with transformers,,,True,False,"Zhang, Chen-Lin and Wu, Jianxin and Li, Yin",2022,,,,
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,shi2023tridet,\cite{shi2023tridet},Tridet: Temporal action detection with relative boundary modeling,,,True,False,"Shi, Dingfeng and Zhong, Yujie and Cao, Qiong and Ma, Lin and Li, Jia and Tao, Dacheng",2023,,,,
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,cai2022you,\cite{cai2022you},Do you really mean that? content driven audio-visual deepfake dataset and multimodal method for temporal forgery localization,,,True,False,"Cai, Zhixi and Stefanov, Kalin and Dhall, Abhinav and Hayat, Munawar",2022,,,,
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,cai2023glitch,\cite{cai2023glitch},Glitch in the Matrix: A Large Scale Benchmark for Content Driven Audio-Visual Forgery Detection and Localization,,,True,False,"Cai, Zhixi and Ghosh, Shreya and Dhall, Abhinav and Gedeon, Tom and Stefanov, Kalin and Hayat, Munawar",2023,,,10.1016/j.cviu.2023.103818,Computer Vision and Image Understanding
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,cai2023av,\cite{cai2023av},AV-Deepfake1M: A Large-Scale LLM-Driven Audio-Visual Deepfake Dataset,https://arxiv.org/abs/2311.15308v2,"The detection and localization of highly realistic deepfake audio-visual content are challenging even for the most advanced state-of-the-art methods. While most of the research efforts in this domain are focused on detecting high-quality deepfake images and videos, only a few works address the problem of the localization of small segments of audio-visual manipulations embedded in real videos. In this research, we emulate the process of such content generation and propose the AV-Deepfake1M dataset. The dataset contains content-driven (i) video manipulations, (ii) audio manipulations, and (iii) audio-visual manipulations for more than 2K subjects resulting in a total of more than 1M videos. The paper provides a thorough description of the proposed data generation pipeline accompanied by a rigorous analysis of the quality of the generated data. The comprehensive benchmark of the proposed dataset utilizing state-of-the-art deepfake detection and localization methods indicates a significant drop in performance compared to previous datasets. The proposed dataset will play a vital role in building the next-generation deepfake localization methods. The dataset and associated code are available at https://github.com/ControlNet/AV-Deepfake1M .",True,True,"Cai, Zhixi and Ghosh, Shreya and Adatia, Aman Pankaj and Hayat, Munawar and Dhall, Abhinav and Stefanov, Kalin",2023,,,,arXiv preprint arXiv:2311.15308
Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization,2511.10212v1,zhang2023ummaformer,\cite{zhang2023ummaformer},UMMAFormer: A Universal Multimodal-adaptive Transformer Framework for Temporal Forgery Localization,https://arxiv.org/abs/2308.14395v1,"The emergence of artificial intelligence-generated content (AIGC) has raised concerns about the authenticity of multimedia content in various fields. However, existing research for forgery content detection has focused mainly on binary classification tasks of complete videos, which has limited applicability in industrial settings. To address this gap, we propose UMMAFormer, a novel universal transformer framework for temporal forgery localization (TFL) that predicts forgery segments with multimodal adaptation. Our approach introduces a Temporal Feature Abnormal Attention (TFAA) module based on temporal feature reconstruction to enhance the detection of temporal differences. We also design a Parallel Cross-Attention Feature Pyramid Network (PCA-FPN) to optimize the Feature Pyramid Network (FPN) for subtle feature enhancement. To evaluate the proposed method, we contribute a novel Temporal Video Inpainting Localization (TVIL) dataset specifically tailored for video inpainting scenes. Our experiments show that our approach achieves state-of-the-art performance on benchmark datasets, including Lav-DF, TVIL, and Psynd, significantly outperforming previous methods. The code and data are available at https://github.com/ymhzyj/UMMAFormer/.",True,True,"Zhang, Rui and Wang, Hongxia and Du, Mingshan and Liu, Hanqing and Zhou, Yang and Zeng, Qiang",2023,,,,
QLCoder: A Query Synthesizer For Static Analysis of Security Vulnerabilities,2511.08462v2,zhou2024largelanguagemodelvulnerability,\cite{zhou2024largelanguagemodelvulnerability},Large Language Model for Vulnerability Detection and Repair: Literature Review and the Road Ahead,,,True,False,Xin Zhou and Sicong Cao and Xiaobing Sun and David Lo,2024,,https://arxiv.org/abs/2404.02525,,
QLCoder: A Query Synthesizer For Static Analysis of Security Vulnerabilities,2511.08462v2,li2025iris,\cite{li2025iris},IRIS: LLM-Assisted Static Analysis for Detecting Security Vulnerabilities,https://arxiv.org/abs/2405.17238v3,"Software is prone to security vulnerabilities. Program analysis tools to detect them have limited effectiveness in practice due to their reliance on human labeled specifications. Large language models (or LLMs) have shown impressive code generation capabilities but they cannot do complex reasoning over code to detect such vulnerabilities especially since this task requires whole-repository analysis. We propose IRIS, a neuro-symbolic approach that systematically combines LLMs with static analysis to perform whole-repository reasoning for security vulnerability detection. Specifically, IRIS leverages LLMs to infer taint specifications and perform contextual analysis, alleviating needs for human specifications and inspection. For evaluation, we curate a new dataset, CWE-Bench-Java, comprising 120 manually validated security vulnerabilities in real-world Java projects. A state-of-the-art static analysis tool CodeQL detects only 27 of these vulnerabilities whereas IRIS with GPT-4 detects 55 (+28) and improves upon CodeQL's average false discovery rate by 5% points. Furthermore, IRIS identifies 4 previously unknown vulnerabilities which cannot be found by existing tools. IRIS is available publicly at https://github.com/iris-sast/iris.",True,True,"Li, Z. and Dutta, S. and Naik, M.",2025,Apr.,,10.48550/arXiv.2405.17238,
QLCoder: A Query Synthesizer For Static Analysis of Security Vulnerabilities,2511.08462v2,yang2025knighter,\cite{yang2025knighter},{KNighter: Transforming Static Analysis with LLM-Synthesized Checkers},,,True,False,"Yang, C. and Zhao, Z. and Xie, Z. and Li, H. and Zhang, L.",2025,Apr.,,10.48550/arXiv.2503.09002,
QLCoder: A Query Synthesizer For Static Analysis of Security Vulnerabilities,2511.08462v2,li2025automatedstaticvulnerabilitydetection,\cite{li2025automatedstaticvulnerabilitydetection},Automated Static Vulnerability Detection via a Holistic Neuro-symbolic Approach,https://arxiv.org/abs/2504.16057v3,"In this paper, we present MoCQ, a novel neuro-symbolic framework that combines the complementary strengths of Large Language Model (LLM) and classic vulnerability checkers to enable scalable, automated vulnerability detection. The key insight is to leverage an LLM to automatically generate vulnerability patterns and translate them into detection queries. Specifically, MoCQ incorporates an iterative loop in which an LLM refines queries based on carefully designed feedback information. The resulting queries are then executed to analyze large codebases and detect vulnerabilities. We evaluated MoCQ on 12 vulnerability types across four programming languages. MoCQ achieved comparable precision and recall compared to expert-developed queries, with significantly less expert time needed. MoCQ also uncovered 46 new vulnerability patterns that experts missed, each representing an overlooked vulnerability class. MoCQ further discovered seven previously unknown vulnerabilities in real-world applications.",True,True,Penghui Li and Songchen Yao and Josef Sarfati Korich and Changhua Luo and Jianjia Yu and Yinzhi Cao and Junfeng Yang,2025,,https://arxiv.org/abs/2504.16057,,
QLCoder: A Query Synthesizer For Static Analysis of Security Vulnerabilities,2511.08462v2,yang2024sweagentagentcomputerinterfacesenable,\cite{yang2024sweagentagentcomputerinterfacesenable},SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering,https://arxiv.org/abs/2405.15793v3,"Language model (LM) agents are increasingly being used to automate complicated tasks in digital environments. Just as humans benefit from powerful software applications, such as integrated development environments, for complex tasks like software engineering, we posit that LM agents represent a new category of end users with their own needs and abilities, and would benefit from specially-built interfaces to the software they use. We investigate how interface design affects the performance of language model agents. As a result of this exploration, we introduce SWE-agent: a system that facilitates LM agents to autonomously use computers to solve software engineering tasks. SWE-agent's custom agent-computer interface (ACI) significantly enhances an agent's ability to create and edit code files, navigate entire repositories, and execute tests and other programs. We evaluate SWE-agent on SWE-bench and HumanEvalFix, achieving state-of-the-art performance on both with a pass@1 rate of 12.5% and 87.7%, respectively, far exceeding the previous state-of-the-art achieved with non-interactive LMs. Finally, we provide insight on how the design of the ACI can impact agents' behavior and performance.",True,True,John Yang and Carlos E. Jimenez and Alexander Wettig and Kilian Lieret and Shunyu Yao and Karthik Narasimhan and Ofir Press,2024,,https://arxiv.org/abs/2405.15793,,
QLCoder: A Query Synthesizer For Static Analysis of Security Vulnerabilities,2511.08462v2,go_lspai_2025,\cite{go_lspai_2025},{LSPAI}: {An} {IDE} {Plugin} for {LLM}-{Powered} {Multi}-{Language} {Unit} {Test} {Generation} with {Language} {Server} {Protocol},,,True,False,"Go, Gwihwan and Zhou, Chijin and Zhang, Quan and Jiang, Yu and Wei, Zhao",2025,,https://dl.acm.org/doi/10.1145/3696630.3728540,10.1145/3696630.3728540,
QLCoder: A Query Synthesizer For Static Analysis of Security Vulnerabilities,2511.08462v2,blinn_statically_2024,\cite{blinn_statically_2024},Statically Contextualizing Large Language Models with Typed Holes,https://arxiv.org/abs/2409.00921v1,"Large language models (LLMs) have reshaped the landscape of program synthesis. However, contemporary LLM-based code completion systems often hallucinate broken code because they lack appropriate context, particularly when working with definitions not in the training data nor near the cursor. This paper demonstrates that tight integration with the type and binding structure of a language, as exposed by its language server, can address this contextualization problem in a token-efficient manner. In short, we contend that AIs need IDEs, too! In particular, we integrate LLM code generation into the Hazel live program sketching environment. The Hazel Language Server identifies the type and typing context of the hole being filled, even in the presence of errors, ensuring that a meaningful program sketch is always available. This allows prompting with codebase-wide contextual information not lexically local to the cursor, nor necessarily in the same file, but that is likely to be semantically local to the developer's goal. Completions synthesized by the LLM are then iteratively refined via further dialog with the language server. To evaluate these techniques, we introduce MVUBench, a dataset of model-view-update (MVU) web applications. These applications serve as challenge problems due to their reliance on application-specific data structures. We find that contextualization with type definitions is particularly impactful. After introducing our ideas in the context of Hazel we duplicate our techniques and port MVUBench to TypeScript in order to validate the applicability of these methods to higher-resource languages. Finally, we outline ChatLSP, a conservative extension to the Language Server Protocol (LSP) that language servers can implement to expose capabilities that AI code completion systems of various designs can use to incorporate static context when generating prompts for an LLM.",True,True,"Blinn, Andrew and Li, Xiang and Kim, June Hyung and Omar, Cyrus",2024,,https://dl.acm.org/doi/10.1145/3689728,10.1145/3689728,Artifact for Statically Contextualizing Large Language Models with Typed Holes
QLCoder: A Query Synthesizer For Static Analysis of Security Vulnerabilities,2511.08462v2,mora2024synthetic,\cite{mora2024synthetic},Synthetic programming elicitation for text-to-code in very low-resource programming and formal languages,,,True,False,"Mora, Federico and Wong, Justin and Lepe, Haley and Bhatia, Sahil and Elmaaroufi, Karim and Varghese, George and Gonzalez, Joseph E and Polgreen, Elizabeth and Seshia, Sanjit A",2024,,,,Advances in Neural Information Processing Systems
QLCoder: A Query Synthesizer For Static Analysis of Security Vulnerabilities,2511.08462v2,cassano2024knowledgetransferhighresourcelowresource,\cite{cassano2024knowledgetransferhighresourcelowresource},Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs,https://arxiv.org/abs/2308.09895v6,"Over the past few years, Large Language Models of Code (Code LLMs) have started to have a significant impact on programming practice. Code LLMs are also emerging as building blocks for research in programming languages and software engineering. However, Code LLMs produce impressive results on programming languages that are well represented in their training data (e.g., Java, Python, or JavaScript), but struggle with low-resource languages that have limited training data available. Low resource languages include OCaml, Racket, and several others.
  This paper presents an effective approach for boosting the performance of Code LLMs on low-resource languages using semi-synthetic data. Our approach, MultiPL-T, translates training data from high-resource languages into training data for low-resource languages in the following way. 1) We use a Code LLM to synthesize tests for commented code from a high-resource language, filtering out faulty tests and code with low test coverage. 2) We use a Code LLM to translate Python code to a target low-resource language, and use tests to validate the translation. We apply this approach to generate tens of thousands of validated training items for Julia, Lua, OCaml, R, and Racket. Furthermore, we use an open model (StarCoderBase) with open training data (The Stack), which allows us to decontaminate benchmarks, train models without violating licenses, and run experiments that could not otherwise be done.
  With MultiPL-T generated data, we present fine-tuned versions of StarCoderBase and Code Llama for Julia, Lua, OCaml, R, and Racket. On established benchmarks (MultiPL-E), these models outperform other open Code LLMs. The MultiPL-T approach is easy to apply to new languages, and is significantly more efficient and effective than alternatives such as training longer.",True,True,Federico Cassano and John Gouwar and Francesca Lucchetti and Claire Schlesinger and Anders Freeman and Carolyn Jane Anderson and Molly Q Feldman and Michael Greenberg and Abhinav Jangda and Arjun Guha,2024,,https://arxiv.org/abs/2308.09895,,
Anatomy-VLM: A Fine-grained Vision-Language Model for Medical Interpretation,2511.08402v1,radford2021learning,\cite{radford2021learning},Learning Transferable Visual Models From Natural Language Supervision,https://arxiv.org/abs/2103.00020v1,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",True,True,"Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",2021,,,,
Anatomy-VLM: A Fine-grained Vision-Language Model for Medical Interpretation,2511.08402v1,li2023blip,\cite{li2023blip},BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models,https://arxiv.org/abs/2301.12597v3,"The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.",True,True,"Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven",2023,,,,
Anatomy-VLM: A Fine-grained Vision-Language Model for Medical Interpretation,2511.08402v1,jia2021scaling,\cite{jia2021scaling},Scaling up visual and vision-language representation learning with noisy text supervision,,,True,False,"Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom",2021,,,,
Anatomy-VLM: A Fine-grained Vision-Language Model for Medical Interpretation,2511.08402v1,chen2025cost,\cite{chen2025cost},Cost-effective instruction learning for pathology vision and language analysis,,,True,False,"Chen, Kaitao and Liu, Mianxin and Yan, Fang and Ma, Lei and Shi, Xiaoming and Wang, Lilong and Wang, Xiaosong and Zhu, Lifeng and Wang, Zhe and Zhou, Mu and others",2025,,,,Nature Computational Science
Anatomy-VLM: A Fine-grained Vision-Language Model for Medical Interpretation,2511.08402v1,gao2025show,\cite{gao2025show},Show and Segment: Universal Medical Image Segmentation via In-Context Learning,,,True,False,"Gao, Yunhe and Liu, Di and Li, Zhuowei and Li, Yunsheng and Chen, Dongdong and Zhou, Mu and Metaxas, Dimitris N",2025,,,,
Anatomy-VLM: A Fine-grained Vision-Language Model for Medical Interpretation,2511.08402v1,chen2024survey,\cite{chen2024survey},A survey of medical vision-and-language applications and their techniques,,,True,False,"Chen, Qi and Zhao, Ruoshan and Wang, Sinuo and Phan, Vu Minh Hieu and Hengel, Anton van den and Verjans, Johan and Liao, Zhibin and To, Minh-Son and Xia, Yong and Chen, Jian and others",2024,,,,arXiv preprint arXiv:2411.12195
Anatomy-VLM: A Fine-grained Vision-Language Model for Medical Interpretation,2511.08402v1,liu2023visual,\cite{liu2023visual},Visual instruction tuning,,,True,False,"Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",2023,,,,Advances in neural information processing systems
Anatomy-VLM: A Fine-grained Vision-Language Model for Medical Interpretation,2511.08402v1,boecking2022making,\cite{boecking2022making},Making the most of text semantics to improve biomedical vision--language processing,,,True,False,"Boecking, Benedikt and Usuyama, Naoto and Bannur, Shruthi and Castro, Daniel C and Schwaighofer, Anton and Hyland, Stephanie and Wetscherek, Maria and Naumann, Tristan and Nori, Aditya and Alvarez-Valle, Javier and others",2022,,,,
Anatomy-VLM: A Fine-grained Vision-Language Model for Medical Interpretation,2511.08402v1,wang2022medclip,\cite{wang2022medclip},MedCLIP: Contrastive Learning from Unpaired Medical Images and Text,https://arxiv.org/abs/2210.10163v1,"Existing vision-text contrastive learning like CLIP aims to match the paired image and caption embeddings while pushing others apart, which improves representation transferability and supports zero-shot prediction. However, medical image-text datasets are orders of magnitude below the general images and captions from the internet. Moreover, previous methods encounter many false negatives, i.e., images and reports from separate patients probably carry the same semantics but are wrongly treated as negatives. In this paper, we decouple images and texts for multimodal contrastive learning thus scaling the usable training data in a combinatorial magnitude with low cost. We also propose to replace the InfoNCE loss with semantic matching loss based on medical knowledge to eliminate false negatives in contrastive learning. We prove that MedCLIP is a simple yet effective framework: it outperforms state-of-the-art methods on zero-shot prediction, supervised classification, and image-text retrieval. Surprisingly, we observe that with only 20K pre-training data, MedCLIP wins over the state-of-the-art method (using around 200K data). Our code is available at https://github.com/RyanWangZf/MedCLIP.",True,True,"Wang, Zifeng and Wu, Zhenbang and Agarwal, Dinesh and Sun, Jimeng",2022,,,,
Anatomy-VLM: A Fine-grained Vision-Language Model for Medical Interpretation,2511.08402v1,wu2023medklip,\cite{wu2023medklip},MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training in Radiology,https://arxiv.org/abs/2301.02228v3,"In this paper, we consider enhancing medical visual-language pre-training (VLP) with domain-specific knowledge, by exploiting the paired image-text reports from the radiological daily practice. In particular, we make the following contributions: First, unlike existing works that directly process the raw reports, we adopt a novel triplet extraction module to extract the medical-related information, avoiding unnecessary complexity from language grammar and enhancing the supervision signals; Second, we propose a novel triplet encoding module with entity translation by querying a knowledge base, to exploit the rich domain knowledge in medical field, and implicitly build relationships between medical entities in the language embedding space; Third, we propose to use a Transformer-based fusion model for spatially aligning the entity description with visual signals at the image patch level, enabling the ability for medical diagnosis; Fourth, we conduct thorough experiments to validate the effectiveness of our architecture, and benchmark on numerous public benchmarks, e.g., ChestX-ray14, RSNA Pneumonia, SIIM-ACR Pneumothorax, COVIDx CXR-2, COVID Rural, and EdemaSeverity. In both zero-shot and fine-tuning settings, our model has demonstrated strong performance compared with the former methods on disease classification and grounding.",True,True,"Wu, Chaoyi and Zhang, Xiaoman and Zhang, Ya and Wang, Yanfeng and Xie, Weidi",2023,,,,arXiv preprint arXiv:2301.02228
Anatomy-VLM: A Fine-grained Vision-Language Model for Medical Interpretation,2511.08402v1,datta2020understanding,\cite{datta2020understanding},"Understanding Spatial Language in Radiology: Representation Framework, Annotation, and Spatial Relation Extraction from Chest X-ray Reports using Deep Learning",https://arxiv.org/abs/1908.04485v1,"We define a representation framework for extracting spatial information from radiology reports (Rad-SpRL). We annotated a total of 2000 chest X-ray reports with 4 spatial roles corresponding to the common radiology entities. Our focus is on extracting detailed information of a radiologist's interpretation containing a radiographic finding, its anatomical location, corresponding probable diagnoses, as well as associated hedging terms. For this, we propose a deep learning-based natural language processing (NLP) method involving both word and character-level encodings. Specifically, we utilize a bidirectional long short-term memory (Bi-LSTM) conditional random field (CRF) model for extracting the spatial roles. The model achieved average F1 measures of 90.28 and 94.61 for extracting the Trajector and Landmark roles respectively whereas the performance was moderate for Diagnosis and Hedge roles with average F1 of 71.47 and 73.27 respectively. The corpus will soon be made available upon request.",True,True,"Datta, Surabhi and Si, Yuqi and Rodriguez, Laritza and Shooshan, Sonya E and Demner-Fushman, Dina and Roberts, Kirk",2020,,,,Journal of biomedical informatics
Anatomy-VLM: A Fine-grained Vision-Language Model for Medical Interpretation,2511.08402v1,waite2019analysis,\cite{waite2019analysis},Analysis of perceptual expertise in radiology--current knowledge and a new perspective,,,True,False,"Waite, Stephen and Grigorian, Arkadij and Alexander, Robert G and Macknik, Stephen L and Carrasco, Marisa and Heeger, David J and Martinez-Conde, Susana",2019,,,,Frontiers in human neuroscience
Anatomy-VLM: A Fine-grained Vision-Language Model for Medical Interpretation,2511.08402v1,dosovitskiy2020image,\cite{dosovitskiy2020image},An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,https://arxiv.org/abs/2010.11929v2,"While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",True,True,"Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others",2020,,,,arXiv preprint arXiv:2010.11929
Anatomy-VLM: A Fine-grained Vision-Language Model for Medical Interpretation,2511.08402v1,lu2025integrating,\cite{lu2025integrating},Integrating language into medical visual recognition and reasoning: A survey,,,True,False,"Lu, Yinbin and Wang, Alan",2025,,,,Medical Image Analysis
Anatomy-VLM: A Fine-grained Vision-Language Model for Medical Interpretation,2511.08402v1,shui2025large,\cite{shui2025large},Large-scale and fine-grained vision-language pre-training for enhanced ct image understanding,,,True,False,"Shui, Zhongyi and Zhang, Jianpeng and Cao, Weiwei and Wang, Sinuo and Guo, Ruizhe and Lu, Le and Yang, Lin and Ye, Xianghua and Liang, Tingbo and Zhang, Qi and others",2025,,,,arXiv preprint arXiv:2501.14548
Anatomy-VLM: A Fine-grained Vision-Language Model for Medical Interpretation,2511.08402v1,lai2024carzero,\cite{lai2024carzero},Carzero: Cross-attention alignment for radiology zero-shot classification,,,True,False,"Lai, Haoran and Yao, Qingsong and Jiang, Zihang and Wang, Rongsheng and He, Zhiyang and Tao, Xiaodong and Zhou, S Kevin",2024,,,,
Anatomy-VLM: A Fine-grained Vision-Language Model for Medical Interpretation,2511.08402v1,tanida2023interactive,\cite{tanida2023interactive},Interactive and explainable region-guided radiology report generation,,,True,False,"Tanida, Tim and M{\""u}ller, Philip and Kaissis, Georgios and Rueckert, Daniel",2023,,,,
Anatomy-VLM: A Fine-grained Vision-Language Model for Medical Interpretation,2511.08402v1,li2024anatomical,\cite{li2024anatomical},Anatomical Structure-Guided Medical Vision-Language Pre-training,https://arxiv.org/abs/2403.09294v1,"Learning medical visual representations through vision-language pre-training has reached remarkable progress. Despite the promising performance, it still faces challenges, i.e., local alignment lacks interpretability and clinical relevance, and the insufficient internal and external representation learning of image-report pairs. To address these issues, we propose an Anatomical Structure-Guided (ASG) framework. Specifically, we parse raw reports into triplets <anatomical region, finding, existence>, and fully utilize each element as supervision to enhance representation learning. For anatomical region, we design an automatic anatomical region-sentence alignment paradigm in collaboration with radiologists, considering them as the minimum semantic units to explore fine-grained local alignment. For finding and existence, we regard them as image tags, applying an image-tag recognition decoder to associate image features with their respective tags within each sample and constructing soft labels for contrastive learning to improve the semantic association of different image-report pairs. We evaluate the proposed ASG framework on two downstream tasks, including five public benchmarks. Experimental results demonstrate that our method outperforms the state-of-the-art methods.",True,True,"Li, Qingqiu and Yan, Xiaohan and Xu, Jilan and Yuan, Runtian and Zhang, Yuejie and Feng, Rui and Shen, Quanli and Zhang, Xiaobo and Wang, Shujun",2024,,,,
Social LSTM with Dynamic Occupancy Modeling for Realistic Pedestrian Trajectory Prediction,2511.09735v1,wang2024pedestrian,\cite{wang2024pedestrian},Pedestrian Trajectory Prediction Using Dynamics-based Deep Learning,https://arxiv.org/abs/2309.09021v2,"Pedestrian trajectory prediction plays an important role in autonomous driving systems and robotics. Recent work utilizing prominent deep learning models for pedestrian motion prediction makes limited a priori assumptions about human movements, resulting in a lack of explainability and explicit constraints enforced on predicted trajectories. We present a dynamics-based deep learning framework with a novel asymptotically stable dynamical system integrated into a Transformer-based model. We use an asymptotically stable dynamical system to model human goal-targeted motion by enforcing the human walking trajectory, which converges to a predicted goal position, and to provide the Transformer model with prior knowledge and explainability. Our framework features the Transformer model that works with a goal estimator and dynamical system to learn features from pedestrian motion history. The results show that our framework outperforms prominent models using five benchmark human motion datasets.",True,True,"Wang, Honghui and Zhi, Weiming and Batista, Gustavo and Chandra, Rohitash",2024,,,,
Social LSTM with Dynamic Occupancy Modeling for Realistic Pedestrian Trajectory Prediction,2511.09735v1,yang2025pedestrian,\cite{yang2025pedestrian},Pedestrian trajectory prediction model based on self-supervised spatiotemporal graph network,,,True,False,"Yang, Shiji and Xiao, Xuezhong",2025,,,,Intelligent Systems with Applications
Social LSTM with Dynamic Occupancy Modeling for Realistic Pedestrian Trajectory Prediction,2511.09735v1,chen2025trajectory,\cite{chen2025trajectory},Trajectory generation: a survey on methods and techniques,,,True,False,"Chen, Xin and Huang, Chengrui and Wang, Chenhao and Chen, Lisi",2025,,,,GeoInformatica
Social LSTM with Dynamic Occupancy Modeling for Realistic Pedestrian Trajectory Prediction,2511.09735v1,kothari2021human,\cite{kothari2021human},Human Trajectory Forecasting in Crowds: A Deep Learning Perspective,https://arxiv.org/abs/2007.03639v3,"Since the past few decades, human trajectory forecasting has been a field of active research owing to its numerous real-world applications: evacuation situation analysis, deployment of intelligent transport systems, traffic operations, to name a few. Early works handcrafted this representation based on domain knowledge. However, social interactions in crowded environments are not only diverse but often subtle. Recently, deep learning methods have outperformed their handcrafted counterparts, as they learned about human-human interactions in a more generic data-driven fashion. In this work, we present an in-depth analysis of existing deep learning-based methods for modelling social interactions. We propose two knowledge-based data-driven methods to effectively capture these social interactions. To objectively compare the performance of these interaction-based forecasting models, we develop a large scale interaction-centric benchmark TrajNet++, a significant yet missing component in the field of human trajectory forecasting. We propose novel performance metrics that evaluate the ability of a model to output socially acceptable trajectories. Experiments on TrajNet++ validate the need for our proposed metrics, and our method outperforms competitive baselines on both real-world and synthetic datasets.",True,True,"Kothari, Parth and Kreiss, Sven and Alahi, Alexandre",2021,,,,IEEE Transactions on Intelligent Transportation Systems
Social LSTM with Dynamic Occupancy Modeling for Realistic Pedestrian Trajectory Prediction,2511.09735v1,zhang2019sr,\cite{zhang2019sr},Sr-lstm: State refinement for lstm towards pedestrian trajectory prediction,,,True,False,"Zhang, Pu and Ouyang, Wanli and Zhang, Pengfei and Xue, Jianru and Zheng, Nanning",2019,,,,
Social LSTM with Dynamic Occupancy Modeling for Realistic Pedestrian Trajectory Prediction,2511.09735v1,huang2019stgat,\cite{huang2019stgat},Stgat: Modeling spatial-temporal interactions for human trajectory prediction,,,True,False,"Huang, Yingfan and Bi, Huikun and Li, Zhaoxin and Mao, Tianlu and Wang, Zhaoqi",2019,,,,
Social LSTM with Dynamic Occupancy Modeling for Realistic Pedestrian Trajectory Prediction,2511.09735v1,xue2019location,\cite{xue2019location},Location-velocity attention for pedestrian trajectory prediction,,,True,False,"Xue, Hao and Huynh, Du and Reynolds, Mark",2019,,,,
Social LSTM with Dynamic Occupancy Modeling for Realistic Pedestrian Trajectory Prediction,2511.09735v1,kosaraju2019social,\cite{kosaraju2019social},Social-BiGAT: Multimodal Trajectory Forecasting using Bicycle-GAN and Graph Attention Networks,https://arxiv.org/abs/1907.03395v2,"Predicting the future trajectories of multiple interacting agents in a scene has become an increasingly important problem for many different applications ranging from control of autonomous vehicles and social robots to security and surveillance. This problem is compounded by the presence of social interactions between humans and their physical interactions with the scene. While the existing literature has explored some of these cues, they mainly ignored the multimodal nature of each human's future trajectory. In this paper, we present Social-BiGAT, a graph-based generative adversarial network that generates realistic, multimodal trajectory predictions by better modelling the social interactions of pedestrians in a scene. Our method is based on a graph attention network (GAT) that learns reliable feature representations that encode the social interactions between humans in the scene, and a recurrent encoder-decoder architecture that is trained adversarially to predict, based on the features, the humans' paths. We explicitly account for the multimodal nature of the prediction problem by forming a reversible transformation between each scene and its latent noise vector, as in Bicycle-GAN. We show that our framework achieves state-of-the-art performance comparing it to several baselines on existing trajectory forecasting benchmarks.",True,True,"Kosaraju, Vineet and Sadeghian, Amir and Mart{\'\i}n-Mart{\'\i}n, Roberto and Reid, Ian and Rezatofighi, Hamid and Savarese, Silvio",2019,,,,Advances in neural information processing systems
Social LSTM with Dynamic Occupancy Modeling for Realistic Pedestrian Trajectory Prediction,2511.09735v1,yuan2021agentformer,\cite{yuan2021agentformer},Agentformer: Agent-aware transformers for socio-temporal multi-agent forecasting,,,True,False,"Yuan, Ye and Weng, Xinshuo and Ou, Yanglan and Kitani, Kris M",2021,,,,
Social LSTM with Dynamic Occupancy Modeling for Realistic Pedestrian Trajectory Prediction,2511.09735v1,li2024iaha,\cite{li2024iaha},IAHA: Intention-Aware Hybrid Attention Transformer for Trajectory Prediction,,,True,False,"Li, Meiyun and Li, Yuhang and Gao, Zhihao and Wang, Jiapei and Li, Yangyang",2024,,,,
Social LSTM with Dynamic Occupancy Modeling for Realistic Pedestrian Trajectory Prediction,2511.09735v1,zhu2024propagation,\cite{zhu2024propagation},Propagation Structure-Aware Graph Transformer for Robust and Interpretable Fake News Detection,,,True,False,"Zhu, Junyou and Gao, Chao and Yin, Ze and Li, Xianghua and Kurths, J{\""u}rgen",2024,,,,
Social LSTM with Dynamic Occupancy Modeling for Realistic Pedestrian Trajectory Prediction,2511.09735v1,gupta2018social,\cite{gupta2018social},Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks,https://arxiv.org/abs/1803.10892v1,"Understanding human motion behavior is critical for autonomous moving platforms (like self-driving cars and social robots) if they are to navigate human-centric environments. This is challenging because human motion is inherently multimodal: given a history of human motion paths, there are many socially plausible ways that people could move in the future. We tackle this problem by combining tools from sequence prediction and generative adversarial networks: a recurrent sequence-to-sequence model observes motion histories and predicts future behavior, using a novel pooling mechanism to aggregate information across people. We predict socially plausible futures by training adversarially against a recurrent discriminator, and encourage diverse predictions with a novel variety loss. Through experiments on several datasets we demonstrate that our approach outperforms prior work in terms of accuracy, variety, collision avoidance, and computational complexity.",True,True,"Gupta, Agrim and Johnson, Justin and Fei-Fei, Li and Savarese, Silvio and Alahi, Alexandre",2018,,,,
Social LSTM with Dynamic Occupancy Modeling for Realistic Pedestrian Trajectory Prediction,2511.09735v1,salzmann2020trajectron,\cite{salzmann2020trajectron},Trajectron++: Dynamically-Feasible Trajectory Forecasting With Heterogeneous Data,https://arxiv.org/abs/2001.03093v5,"Reasoning about human motion is an important prerequisite to safe and socially-aware robotic navigation. As a result, multi-agent behavior prediction has become a core component of modern human-robot interactive systems, such as self-driving cars. While there exist many methods for trajectory forecasting, most do not enforce dynamic constraints and do not account for environmental information (e.g., maps). Towards this end, we present Trajectron++, a modular, graph-structured recurrent model that forecasts the trajectories of a general number of diverse agents while incorporating agent dynamics and heterogeneous data (e.g., semantic maps). Trajectron++ is designed to be tightly integrated with robotic planning and control frameworks; for example, it can produce predictions that are optionally conditioned on ego-agent motion plans. We demonstrate its performance on several challenging real-world trajectory forecasting datasets, outperforming a wide array of state-of-the-art deterministic and generative methods.",True,True,"Salzmann, Tim and Ivanovic, Boris and Chakravarty, Punarjay and Pavone, Marco",2020,,,,Conference on Robot Learning (CoRL)
Social LSTM with Dynamic Occupancy Modeling for Realistic Pedestrian Trajectory Prediction,2511.09735v1,luo2023gsgformer,\cite{luo2023gsgformer},GSGFormer: A General Scene Graph Transformer for Trajectory Prediction,,,True,False,"Luo, Xingyu and Zhu, Junchi and Dai, Bo and Wang, Chao",2023,,,,
Social LSTM with Dynamic Occupancy Modeling for Realistic Pedestrian Trajectory Prediction,2511.09735v1,song2023tutr,\cite{song2023tutr},TUTR: Trajectory Unified Transformer for pedestrian trajectory prediction,,,True,False,"Song, Hao and Zhu, Yifan and Liu, Yuwei and Fan, Lihui and Shen, Chunhua",2023,,,,
Social LSTM with Dynamic Occupancy Modeling for Realistic Pedestrian Trajectory Prediction,2511.09735v1,chatagnon2025exploring,\cite{chatagnon2025exploring},Exploring Dense Crowd Dynamics: State of the Art and Emerging Paradigms,https://arxiv.org/abs/2505.05826v1,"Dense pedestrian crowds may pose significant safety risks, yet their underlying dynamics remain insufficiently understood to reliably prevent accidents. In these environments, physical interactions and contact forces fundamentally shape the dynamics of the crowd. However, accurately describing these interindividual interactions requires specific modeling and analytical approaches. This chapter reviews paradigms and models used to represent pedestrian dynamics in various contexts, highlighting the transition from classical approaches to models tailored for dense crowd conditions. We argue that further investigation is needed, featuring new experimental studies and new modeling paradigms, to better capture the complex dynamics that emerge in high-density situations.",True,True,"Chatagnon, Thomas and Tordeux, Antoine and Chraibi, Mohcine",2025,,,,arXiv preprint arXiv:2505.05826
Social LSTM with Dynamic Occupancy Modeling for Realistic Pedestrian Trajectory Prediction,2511.09735v1,song2019experiment,\cite{song2019experiment},Experiment calibrated simulation modeling of crowding forces in high density crowd,,,True,False,"Song, Jingni and Chen, Feng and Zhu, Yadi and Zhang, Na and Liu, Weiyu and Du, Kai",2019,,,,Ieee Access
Social LSTM with Dynamic Occupancy Modeling for Realistic Pedestrian Trajectory Prediction,2511.09735v1,korbmacher2024toward,\cite{korbmacher2024toward},Toward better pedestrian trajectory predictions: the role of density and time-to-collision in hybrid deep-learning algorithms,,,True,False,"Korbmacher, Raphael and Tordeux, Antoine",2024,,,,Sensors
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,kerbl20233d,\cite{kerbl20233d},3d gaussian splatting for real-time radiance field rendering.,,,True,False,"Kerbl, Bernhard and Kopanas, Georgios and Leimk{\""u}hler, Thomas and Drettakis, George",2023,,,,ACM Trans. Graph.
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,huang20242d,\cite{huang20242d},2d gaussian splatting for geometrically accurate radiance fields,,,True,False,"Huang, Binbin and Yu, Zehao and Chen, Anpei and Geiger, Andreas and Gao, Shenghua",2024,,,,
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,yu2024mip,\cite{yu2024mip},Mip-Splatting: Alias-free 3D Gaussian Splatting,https://arxiv.org/abs/2311.16493v1,"Recently, 3D Gaussian Splatting has demonstrated impressive novel view synthesis results, reaching high fidelity and efficiency. However, strong artifacts can be observed when changing the sampling rate, \eg, by changing focal length or camera distance. We find that the source for this phenomenon can be attributed to the lack of 3D frequency constraints and the usage of a 2D dilation filter. To address this problem, we introduce a 3D smoothing filter which constrains the size of the 3D Gaussian primitives based on the maximal sampling frequency induced by the input views, eliminating high-frequency artifacts when zooming in. Moreover, replacing 2D dilation with a 2D Mip filter, which simulates a 2D box filter, effectively mitigates aliasing and dilation issues. Our evaluation, including scenarios such a training on single-scale images and testing on multiple scales, validates the effectiveness of our approach.",True,True,"Yu, Zehao and Chen, Anpei and Huang, Binbin and Sattler, Torsten and Geiger, Andreas",2024,,,,
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,guedon2024sugar,\cite{guedon2024sugar},SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering,https://arxiv.org/abs/2311.12775v3,"We propose a method to allow precise and extremely fast mesh extraction from 3D Gaussian Splatting. Gaussian Splatting has recently become very popular as it yields realistic rendering while being significantly faster to train than NeRFs. It is however challenging to extract a mesh from the millions of tiny 3D gaussians as these gaussians tend to be unorganized after optimization and no method has been proposed so far. Our first key contribution is a regularization term that encourages the gaussians to align well with the surface of the scene. We then introduce a method that exploits this alignment to extract a mesh from the Gaussians using Poisson reconstruction, which is fast, scalable, and preserves details, in contrast to the Marching Cubes algorithm usually applied to extract meshes from Neural SDFs. Finally, we introduce an optional refinement strategy that binds gaussians to the surface of the mesh, and jointly optimizes these Gaussians and the mesh through Gaussian splatting rendering. This enables easy editing, sculpting, rigging, animating, compositing and relighting of the Gaussians using traditional softwares by manipulating the mesh instead of the gaussians themselves. Retrieving such an editable mesh for realistic rendering is done within minutes with our method, compared to hours with the state-of-the-art methods on neural SDFs, while providing a better rendering quality. Our project page is the following: https://anttwo.github.io/sugar/",True,True,"Gu{\'e}don, Antoine and Lepetit, Vincent",2024,,,,
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,hollein20243dgs,\cite{hollein20243dgs},3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt,https://arxiv.org/abs/2409.12892v2,"We present 3DGS-LM, a new method that accelerates the reconstruction of 3D Gaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored Levenberg-Marquardt (LM). Existing methods reduce the optimization time by decreasing the number of Gaussians or by improving the implementation of the differentiable rasterizer. However, they still rely on the ADAM optimizer to fit Gaussian parameters of a scene in thousands of iterations, which can take up to an hour. To this end, we change the optimizer to LM that runs in conjunction with the 3DGS differentiable rasterizer. For efficient GPU parallization, we propose a caching data structure for intermediate gradients that allows us to efficiently calculate Jacobian-vector products in custom CUDA kernels. In every LM iteration, we calculate update directions from multiple image subsets using these kernels and combine them in a weighted mean. Overall, our method is 20% faster than the original 3DGS while obtaining the same reconstruction quality. Our optimization is also agnostic to other methods that acclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS.",True,True,"H{\""o}llein, Lukas and Bo{\v{z}}i{\v{c}}, Alja{\v{z}} and Zollh{\""o}fer, Michael and Nie{\ss}ner, Matthias",2024,,,,arXiv preprint arXiv:2409.12892
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,matsuki2024gaussian,\cite{matsuki2024gaussian},Gaussian Splatting SLAM,https://arxiv.org/abs/2312.06741v2,"We present the first application of 3D Gaussian Splatting in monocular SLAM, the most fundamental but the hardest setup for Visual SLAM. Our method, which runs live at 3fps, utilises Gaussians as the only 3D representation, unifying the required representation for accurate, efficient tracking, mapping, and high-quality rendering. Designed for challenging monocular settings, our approach is seamlessly extendable to RGB-D SLAM when an external depth sensor is available. Several innovations are required to continuously reconstruct 3D scenes with high fidelity from a live camera. First, to move beyond the original 3DGS algorithm, which requires accurate poses from an offline Structure from Motion (SfM) system, we formulate camera tracking for 3DGS using direct optimisation against the 3D Gaussians, and show that this enables fast and robust tracking with a wide basin of convergence. Second, by utilising the explicit nature of the Gaussians, we introduce geometric verification and regularisation to handle the ambiguities occurring in incremental 3D dense reconstruction. Finally, we introduce a full SLAM system which not only achieves state-of-the-art results in novel view synthesis and trajectory estimation but also reconstruction of tiny and even transparent objects.",True,True,"Matsuki, Hidenobu and Murai, Riku and Kelly, Paul HJ and Davison, Andrew J",2024,,,,
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,jiang2025phystwin,\cite{jiang2025phystwin},Phystwin: Physics-informed reconstruction and simulation of deformable objects from videos,,,True,False,"Jiang, Hanxiao and Hsu, Hao-Yu and Zhang, Kaifeng and Yu, Hsin-Ni and Wang, Shenlong and Li, Yunzhu",2025,,,,arXiv preprint arXiv:2503.17973
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,qin2024langsplat,\cite{qin2024langsplat},LangSplat: 3D Language Gaussian Splatting,https://arxiv.org/abs/2312.16084v2,"Humans live in a 3D world and commonly use natural language to interact with a 3D scene. Modeling a 3D language field to support open-ended language queries in 3D has gained increasing attention recently. This paper introduces LangSplat, which constructs a 3D language field that enables precise and efficient open-vocabulary querying within 3D spaces. Unlike existing methods that ground CLIP language embeddings in a NeRF model, LangSplat advances the field by utilizing a collection of 3D Gaussians, each encoding language features distilled from CLIP, to represent the language field. By employing a tile-based splatting technique for rendering language features, we circumvent the costly rendering process inherent in NeRF. Instead of directly learning CLIP embeddings, LangSplat first trains a scene-wise language autoencoder and then learns language features on the scene-specific latent space, thereby alleviating substantial memory demands imposed by explicit modeling. Existing methods struggle with imprecise and vague 3D language fields, which fail to discern clear boundaries between objects. We delve into this issue and propose to learn hierarchical semantics using SAM, thereby eliminating the need for extensively querying the language field across various scales and the regularization of DINO features. Extensive experimental results show that LangSplat significantly outperforms the previous state-of-the-art method LERF by a large margin. Notably, LangSplat is extremely efficient, achieving a 199 $\times$ speedup compared to LERF at the resolution of 1440 $\times$ 1080. We strongly recommend readers to check out our video results at https://langsplat.github.io/",True,True,"Qin, Minghan and Li, Wanhua and Zhou, Jiawei and Wang, Haoqian and Pfister, Hanspeter",2024,,,,
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,barcellona2025dream,\cite{barcellona2025dream},Dream to Manipulate: Compositional World Models Empowering Robot Imitation Learning with Imagination,https://arxiv.org/abs/2412.14957v2,"A world model provides an agent with a representation of its environment, enabling it to predict the causal consequences of its actions. Current world models typically cannot directly and explicitly imitate the actual environment in front of a robot, often resulting in unrealistic behaviors and hallucinations that make them unsuitable for real-world robotics applications. To overcome those challenges, we propose to rethink robot world models as learnable digital twins. We introduce DreMa, a new approach for constructing digital twins automatically using learned explicit representations of the real world and its dynamics, bridging the gap between traditional digital twins and world models. DreMa replicates the observed world and its structure by integrating Gaussian Splatting and physics simulators, allowing robots to imagine novel configurations of objects and to predict the future consequences of robot actions thanks to its compositionality. We leverage this capability to generate new data for imitation learning by applying equivariant transformations to a small set of demonstrations. Our evaluations across various settings demonstrate significant improvements in accuracy and robustness by incrementing actions and object distributions, reducing the data needed to learn a policy and improving the generalization of the agents. As a highlight, we show that a real Franka Emika Panda robot, powered by DreMa's imagination, can successfully learn novel physical tasks from just a single example per task variation (one-shot policy learning). Our project page can be found in: https://dreamtomanipulate.github.io/.",True,True,"Barcellona, Leonardo and Zadaianchuk, Andrii and Allegro, Davide and Papa, Samuele and Ghidoni, Stefano and Gavves, Efstratios",2025,,,,
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,kocabas2024hugs,\cite{kocabas2024hugs},HUGS: Human Gaussian Splats,https://arxiv.org/abs/2311.17910v1,"Recent advances in neural rendering have improved both training and rendering times by orders of magnitude. While these methods demonstrate state-of-the-art quality and speed, they are designed for photogrammetry of static scenes and do not generalize well to freely moving humans in the environment. In this work, we introduce Human Gaussian Splats (HUGS) that represents an animatable human together with the scene using 3D Gaussian Splatting (3DGS). Our method takes only a monocular video with a small number of (50-100) frames, and it automatically learns to disentangle the static scene and a fully animatable human avatar within 30 minutes. We utilize the SMPL body model to initialize the human Gaussians. To capture details that are not modeled by SMPL (e.g. cloth, hairs), we allow the 3D Gaussians to deviate from the human body model. Utilizing 3D Gaussians for animated humans brings new challenges, including the artifacts created when articulating the Gaussians. We propose to jointly optimize the linear blend skinning weights to coordinate the movements of individual Gaussians during animation. Our approach enables novel-pose synthesis of human and novel view synthesis of both the human and the scene. We achieve state-of-the-art rendering quality with a rendering speed of 60 FPS while being ~100x faster to train over previous work. Our code will be announced here: https://github.com/apple/ml-hugs",True,True,"Kocabas, Muhammed and Chang, Jen-Hao Rick and Gabriel, James and Tuzel, Oncel and Ranjan, Anurag",2024,,,,
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,Hu_2024_CVPR,\cite{Hu_2024_CVPR},GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians,https://arxiv.org/abs/2312.02134v3,"We present GaussianAvatar, an efficient approach to creating realistic human avatars with dynamic 3D appearances from a single video. We start by introducing animatable 3D Gaussians to explicitly represent humans in various poses and clothing styles. Such an explicit and animatable representation can fuse 3D appearances more efficiently and consistently from 2D observations. Our representation is further augmented with dynamic properties to support pose-dependent appearance modeling, where a dynamic appearance network along with an optimizable feature tensor is designed to learn the motion-to-appearance mapping. Moreover, by leveraging the differentiable motion condition, our method enables a joint optimization of motions and appearances during avatar modeling, which helps to tackle the long-standing issue of inaccurate motion estimation in monocular settings. The efficacy of GaussianAvatar is validated on both the public dataset and our collected dataset, demonstrating its superior performances in terms of appearance quality and rendering efficiency.",True,True,"Hu, Liangxiao and Zhang, Hongwen and Zhang, Yuxiang and Zhou, Boyao and Liu, Boning and Zhang, Shengping and Nie, Liqiang",2024,June,,,
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,Xiao_2025_CVPR,\cite{Xiao_2025_CVPR},RoGSplat: Learning Robust Generalizable Human Gaussian Splatting from Sparse Multi-View Images,https://arxiv.org/abs/2503.14198v1,"This paper presents RoGSplat, a novel approach for synthesizing high-fidelity novel views of unseen human from sparse multi-view images, while requiring no cumbersome per-subject optimization. Unlike previous methods that typically struggle with sparse views with few overlappings and are less effective in reconstructing complex human geometry, the proposed method enables robust reconstruction in such challenging conditions. Our key idea is to lift SMPL vertices to dense and reliable 3D prior points representing accurate human body geometry, and then regress human Gaussian parameters based on the points. To account for possible misalignment between SMPL model and images, we propose to predict image-aligned 3D prior points by leveraging both pixel-level features and voxel-level features, from which we regress the coarse Gaussians. To enhance the ability to capture high-frequency details, we further render depth maps from the coarse 3D Gaussians to help regress fine-grained pixel-wise Gaussians. Experiments on several benchmark datasets demonstrate that our method outperforms state-of-the-art methods in novel view synthesis and cross-dataset generalization. Our code is available at https://github.com/iSEE-Laboratory/RoGSplat.",True,True,"Xiao, Junjin and Zhang, Qing and Nie, Yonewei and Zhu, Lei and Zheng, Wei-Shi",2025,June,,,
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,xiao2025rogsplat,\cite{xiao2025rogsplat},RoGSplat: Learning Robust Generalizable Human Gaussian Splatting from Sparse Multi-View Images,https://arxiv.org/abs/2503.14198v1,"This paper presents RoGSplat, a novel approach for synthesizing high-fidelity novel views of unseen human from sparse multi-view images, while requiring no cumbersome per-subject optimization. Unlike previous methods that typically struggle with sparse views with few overlappings and are less effective in reconstructing complex human geometry, the proposed method enables robust reconstruction in such challenging conditions. Our key idea is to lift SMPL vertices to dense and reliable 3D prior points representing accurate human body geometry, and then regress human Gaussian parameters based on the points. To account for possible misalignment between SMPL model and images, we propose to predict image-aligned 3D prior points by leveraging both pixel-level features and voxel-level features, from which we regress the coarse Gaussians. To enhance the ability to capture high-frequency details, we further render depth maps from the coarse 3D Gaussians to help regress fine-grained pixel-wise Gaussians. Experiments on several benchmark datasets demonstrate that our method outperforms state-of-the-art methods in novel view synthesis and cross-dataset generalization. Our code is available at https://github.com/iSEE-Laboratory/RoGSplat.",True,True,"Xiao, Junjin and Zhang, Qing and Nie, Yonewei and Zhu, Lei and Zheng, Wei-Shi",2025,,,,
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,svitov2024haha,\cite{svitov2024haha},HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior,https://arxiv.org/abs/2404.01053v2,"We present HAHA - a novel approach for animatable human avatar generation from monocular input videos. The proposed method relies on learning the trade-off between the use of Gaussian splatting and a textured mesh for efficient and high fidelity rendering. We demonstrate its efficiency to animate and render full-body human avatars controlled via the SMPL-X parametric model. Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh where it is necessary, like hair and out-of-mesh clothing. This results in a minimal number of Gaussians being used to represent the full avatar, and reduced rendering artifacts. This allows us to handle the animation of small body parts such as fingers that are traditionally disregarded. We demonstrate the effectiveness of our approach on two open datasets: SnapshotPeople and X-Humans. Our method demonstrates on par reconstruction quality to the state-of-the-art on SnapshotPeople, while using less than a third of Gaussians. HAHA outperforms previous state-of-the-art on novel poses from X-Humans both quantitatively and qualitatively.",True,True,"Svitov, David and Morerio, Pietro and Agapito, Lourdes and Del Bue, Alessio",2024,,,,
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,bogo2016keep,\cite{bogo2016keep},Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image,https://arxiv.org/abs/1607.08128v1,"We describe the first method to automatically estimate the 3D pose of the human body as well as its 3D shape from a single unconstrained image. We estimate a full 3D mesh and show that 2D joints alone carry a surprising amount of information about body shape. The problem is challenging because of the complexity of the human body, articulation, occlusion, clothing, lighting, and the inherent ambiguity in inferring 3D from 2D. To solve this, we first use a recently published CNN-based method, DeepCut, to predict (bottom-up) the 2D body joint locations. We then fit (top-down) a recently published statistical body shape model, called SMPL, to the 2D joints. We do so by minimizing an objective function that penalizes the error between the projected 3D model joints and detected 2D joints. Because SMPL captures correlations in human shape across the population, we are able to robustly fit it to very little data. We further leverage the 3D model to prevent solutions that cause interpenetration. We evaluate our method, SMPLify, on the Leeds Sports, HumanEva, and Human3.6M datasets, showing superior pose accuracy with respect to the state of the art.",True,True,"Bogo, Federica and Kanazawa, Angjoo and Lassner, Christoph and Gehler, Peter and Romero, Javier and Black, Michael J",2016,,,,
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,peng2023implicit,\cite{peng2023implicit},Implicit neural representations with structured latent codes for human body modeling,,,True,False,"Peng, Sida and Geng, Chen and Zhang, Yuanqing and Xu, Yinghao and Wang, Qianqian and Shuai, Qing and Zhou, Xiaowei and Bao, Hujun",2023,,,,IEEE Transactions on Pattern Analysis and Machine Intelligence
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,hu2024gauhuman,\cite{hu2024gauhuman},GauHuman: Articulated Gaussian Splatting from Monocular Human Videos,https://arxiv.org/abs/2312.02973v1,"We present, GauHuman, a 3D human model with Gaussian Splatting for both fast training (1 ~ 2 minutes) and real-time rendering (up to 189 FPS), compared with existing NeRF-based implicit representation modelling frameworks demanding hours of training and seconds of rendering per frame. Specifically, GauHuman encodes Gaussian Splatting in the canonical space and transforms 3D Gaussians from canonical space to posed space with linear blend skinning (LBS), in which effective pose and LBS refinement modules are designed to learn fine details of 3D humans under negligible computational cost. Moreover, to enable fast optimization of GauHuman, we initialize and prune 3D Gaussians with 3D human prior, while splitting/cloning via KL divergence guidance, along with a novel merge operation for further speeding up. Extensive experiments on ZJU_Mocap and MonoCap datasets demonstrate that GauHuman achieves state-of-the-art performance quantitatively and qualitatively with fast training and real-time rendering speed. Notably, without sacrificing rendering quality, GauHuman can fast model the 3D human performer with ~13k 3D Gaussians.",True,True,"Hu, Shoukang and Hu, Tao and Liu, Ziwei",2024,,,,
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,wen2024gomavatar,\cite{wen2024gomavatar},GoMAvatar: Efficient Animatable Human Modeling from Monocular Video Using Gaussians-on-Mesh,https://arxiv.org/abs/2404.07991v1,"We introduce GoMAvatar, a novel approach for real-time, memory-efficient, high-quality animatable human modeling. GoMAvatar takes as input a single monocular video to create a digital avatar capable of re-articulation in new poses and real-time rendering from novel viewpoints, while seamlessly integrating with rasterization-based graphics pipelines. Central to our method is the Gaussians-on-Mesh representation, a hybrid 3D model combining rendering quality and speed of Gaussian splatting with geometry modeling and compatibility of deformable meshes. We assess GoMAvatar on ZJU-MoCap data and various YouTube videos. GoMAvatar matches or surpasses current monocular human modeling algorithms in rendering quality and significantly outperforms them in computational efficiency (43 FPS) while being memory-efficient (3.63 MB per subject).",True,True,"Wen, Jing and Zhao, Xiaoming and Ren, Zhongzheng and Schwing, Alexander G and Wang, Shenlong",2024,,,,
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,sunoccfusion,\cite{sunoccfusion},OccFusion: Rendering Occluded Humans with Generative Diffusion Priors,,,True,False,"Sun, Adam and Xiang, Tiange and Delp, Scott and Fei-Fei, Li and Adeli, Ehsan",,,,,
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,dey2024hfgaussian,\cite{dey2024hfgaussian},HFGaussian: Learning Generalizable Gaussian Human with Integrated Human Features,https://arxiv.org/abs/2411.03086v1,"Recent advancements in radiance field rendering show promising results in 3D scene representation, where Gaussian splatting-based techniques emerge as state-of-the-art due to their quality and efficiency. Gaussian splatting is widely used for various applications, including 3D human representation. However, previous 3D Gaussian splatting methods either use parametric body models as additional information or fail to provide any underlying structure, like human biomechanical features, which are essential for different applications. In this paper, we present a novel approach called HFGaussian that can estimate novel views and human features, such as the 3D skeleton, 3D key points, and dense pose, from sparse input images in real time at 25 FPS. The proposed method leverages generalizable Gaussian splatting technique to represent the human subject and its associated features, enabling efficient and generalizable reconstruction. By incorporating a pose regression network and the feature splatting technique with Gaussian splatting, HFGaussian demonstrates improved capabilities over existing 3D human methods, showcasing the potential of 3D human representations with integrated biomechanics. We thoroughly evaluate our HFGaussian method against the latest state-of-the-art techniques in human Gaussian splatting and pose estimation, demonstrating its real-time, state-of-the-art performance.",True,True,"Dey, Arnab and Lu, Cheng-You and Comport, Andrew I and Sridhar, Srinath and Lin, Chin-Teng and Martinet, Jean",2024,,,,arXiv preprint arXiv:2411.03086
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,prospero2025gst,\cite{prospero2025gst},GST: Precise 3D Human Body from a Single Image with Gaussian Splatting Transformers,https://arxiv.org/abs/2409.04196v2,"Reconstructing posed 3D human models from monocular images has important applications in the sports industry, including performance tracking, injury prevention and virtual training. In this work, we combine 3D human pose and shape estimation with 3D Gaussian Splatting (3DGS), a representation of the scene composed of a mixture of Gaussians. This allows training or fine-tuning a human model predictor on multi-view images alone, without 3D ground truth. Predicting such mixtures for a human from a single input image is challenging due to self-occlusions and dependence on articulations, while also needing to retain enough flexibility to accommodate a variety of clothes and poses. Our key observation is that the vertices of standardized human meshes (such as SMPL) can provide an adequate spatial density and approximate initial position for the Gaussians. We can then train a transformer model to jointly predict comparatively small adjustments to these positions, as well as the other 3DGS attributes and the SMPL parameters. We show empirically that this combination (using only multi-view supervision) can achieve near real-time inference of 3D human models from a single image without expensive diffusion models or 3D points supervision, thus making it ideal for the sport industry at any level. More importantly, rendering is an effective auxiliary objective to refine 3D pose estimation by accounting for clothes and other geometric variations. The code is available at https://github.com/prosperolo/GST.",True,True,"Prospero, Lorenza and Hamdi, Abdullah and Henriques, Joao F and Rupprecht, Christian",2025,,,,
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,he2020epipolar,\cite{he2020epipolar},Epipolar Transformers,https://arxiv.org/abs/2005.04551v1,"A common approach to localize 3D human joints in a synchronized and calibrated multi-view setup consists of two-steps: (1) apply a 2D detector separately on each view to localize joints in 2D, and (2) perform robust triangulation on 2D detections from each view to acquire the 3D joint locations. However, in step 1, the 2D detector is limited to solving challenging cases which could potentially be better resolved in 3D, such as occlusions and oblique viewing angles, purely in 2D without leveraging any 3D information. Therefore, we propose the differentiable ""epipolar transformer"", which enables the 2D detector to leverage 3D-aware features to improve 2D pose estimation. The intuition is: given a 2D location p in the current view, we would like to first find its corresponding point p' in a neighboring view, and then combine the features at p' with the features at p, thus leading to a 3D-aware feature at p. Inspired by stereo matching, the epipolar transformer leverages epipolar constraints and feature matching to approximate the features at p'. Experiments on InterHand and Human3.6M show that our approach has consistent improvements over the baselines. Specifically, in the condition where no external data is used, our Human3.6M model trained with ResNet-50 backbone and image size 256 x 256 outperforms state-of-the-art by 4.23 mm and achieves MPJPE 26.9 mm.",True,True,"He, Yihui and Yan, Rui and Fragkiadaki, Katerina and Yu, Shoou-I",2020,,,,
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,ma2021transfusion,\cite{ma2021transfusion},Transfusion: Cross-view fusion with transformer for 3d human pose estimation,,,True,False,"Ma, Haoyu and Chen, Liangjian and Kong, Deying and Wang, Zhe and Liu, Xingwei and Tang, Hao and Yan, Xiangyi and Xie, Yusheng and Lin, Shih-Yao and Xie, Xiaohui",2021,,,,arXiv preprint arXiv:2110.09554
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,zhang2021adafuse,\cite{zhang2021adafuse},Adafuse: Adaptive multiview fusion for accurate human pose estimation in the wild,,,True,False,"Zhang, Zhe and Wang, Chunyu and Qiu, Weichao and Qin, Wenhu and Zeng, Wenjun",2021,,,,International Journal of Computer Vision
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,iskakov2019learnable,\cite{iskakov2019learnable},Learnable Triangulation of Human Pose,https://arxiv.org/abs/1905.05754v1,"We present two novel solutions for multi-view 3D human pose estimation based on new learnable triangulation methods that combine 3D information from multiple 2D views. The first (baseline) solution is a basic differentiable algebraic triangulation with an addition of confidence weights estimated from the input images. The second solution is based on a novel method of volumetric aggregation from intermediate 2D backbone feature maps. The aggregated volume is then refined via 3D convolutions that produce final 3D joint heatmaps and allow modelling a human pose prior. Crucially, both approaches are end-to-end differentiable, which allows us to directly optimize the target metric. We demonstrate transferability of the solutions across datasets and considerably improve the multi-view state of the art on the Human3.6M dataset. Video demonstration, annotations and additional materials will be posted on our project page (https://saic-violet.github.io/learnable-triangulation).",True,True,"Iskakov, Karim and Burkov, Egor and Lempitsky, Victor and Malkov, Yury",2019,,,,
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,ionescu2013human3,\cite{ionescu2013human3},Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments,,,True,False,"Ionescu, Catalin and Papava, Dragos and Olaru, Vlad and Sminchisescu, Cristian",2013,,,,IEEE transactions on pattern analysis and machine intelligence
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,joo2015panoptic,\cite{joo2015panoptic},Panoptic Studio: A Massively Multiview System for Social Interaction Capture,https://arxiv.org/abs/1612.03153v1,"We present an approach to capture the 3D motion of a group of people engaged in a social interaction. The core challenges in capturing social interactions are: (1) occlusion is functional and frequent; (2) subtle motion needs to be measured over a space large enough to host a social group; (3) human appearance and configuration variation is immense; and (4) attaching markers to the body may prime the nature of interactions. The Panoptic Studio is a system organized around the thesis that social interactions should be measured through the integration of perceptual analyses over a large variety of view points. We present a modularized system designed around this principle, consisting of integrated structural, hardware, and software innovations. The system takes, as input, 480 synchronized video streams of multiple people engaged in social activities, and produces, as output, the labeled time-varying 3D structure of anatomical landmarks on individuals in the space. Our algorithm is designed to fuse the ""weak"" perceptual processes in the large number of views by progressively generating skeletal proposals from low-level appearance cues, and a framework for temporal refinement is also presented by associating body parts to reconstructed dense 3D trajectory stream. Our system and method are the first in reconstructing full body motion of more than five people engaged in social interactions without using markers. We also empirically demonstrate the impact of the number of views in achieving this goal.",True,True,"Joo, Hanbyul and Liu, Hao and Tan, Lei and Gui, Lin and Nabbe, Bart and Matthews, Iain and Kanade, Takeo and Nobuhara, Shohei and Sheikh, Yaser",2015,,,,
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,davoodnia2024upose3d,\cite{davoodnia2024upose3d},UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues,https://arxiv.org/abs/2404.14634v3,"We introduce UPose3D, a novel approach for multi-view 3D human pose estimation, addressing challenges in accuracy and scalability. Our method advances existing pose estimation frameworks by improving robustness and flexibility without requiring direct 3D annotations. At the core of our method, a pose compiler module refines predictions from a 2D keypoints estimator that operates on a single image by leveraging temporal and cross-view information. Our novel cross-view fusion strategy is scalable to any number of cameras, while our synthetic data generation strategy ensures generalization across diverse actors, scenes, and viewpoints. Finally, UPose3D leverages the prediction uncertainty of both the 2D keypoint estimator and the pose compiler module. This provides robustness to outliers and noisy data, resulting in state-of-the-art performance in out-of-distribution settings. In addition, for in-distribution settings, UPose3D yields performance rivalling methods that rely on 3D annotated data while being the state-of-the-art among methods relying only on 2D supervision.",True,True,"Davoodnia, Vandad and Ghorbani, Saeed and Carbonneau, Marc-Andr{\'e} and Messier, Alexandre and Etemad, Ali",2024,,,,
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,moliner2024geometry,\cite{moliner2024geometry},Geometry-biased transformer for robust multi-view 3d human pose reconstruction,,,True,False,"Moliner, Olivier and Huang, Sangxia and {\AA}str{\""o}m, Kalle",2024,,,,
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,zhao2023triangulation,\cite{zhao2023triangulation},Triangulation residual loss for data-efficient 3D pose estimation,,,True,False,"Zhao, Jiachen and Yu, Tao and An, Liang and Huang, Yipeng and Deng, Fang and Dai, Qionghai",2023,,,,Advances in neural information processing systems
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,bragagnolo2025multi,\cite{bragagnolo2025multi},Multi-view Pose Fusion for Occlusion-Aware 3D Human Pose Estimation,,,True,False,"Bragagnolo, Laura and Terreran, Matteo and Allegro, Davide and Ghidoni, Stefano",2025,,,,
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,chen2024adaptivefusion,\cite{chen2024adaptivefusion},AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction,https://arxiv.org/abs/2409.04851v3,"Recent advancements in sensor technology and deep learning have led to significant progress in 3D human body reconstruction. However, most existing approaches rely on data from a specific sensor, which can be unreliable due to the inherent limitations of individual sensing modalities. Additionally, existing multi-modal fusion methods generally require customized designs based on the specific sensor combinations or setups, which limits the flexibility and generality of these methods. Furthermore, conventional point-image projection-based and Transformer-based fusion networks are susceptible to the influence of noisy modalities and sensor poses. To address these limitations and achieve robust 3D human body reconstruction in various conditions, we propose AdaptiveFusion, a generic adaptive multi-modal multi-view fusion framework that can effectively incorporate arbitrary combinations of uncalibrated sensor inputs. By treating different modalities from various viewpoints as equal tokens, and our handcrafted modality sampling module by leveraging the inherent flexibility of Transformer models, AdaptiveFusion is able to cope with arbitrary numbers of inputs and accommodate noisy modalities with only a single training network. Extensive experiments on large-scale human datasets demonstrate the effectiveness of AdaptiveFusion in achieving high-quality 3D human body reconstruction in various environments. In addition, our method achieves superior accuracy compared to state-of-the-art fusion methods.",True,True,"Chen, Anjun and Wang, Xiangyu and Xu, Zhi and Shi, Kun and Qin, Yan and Huo, Yuchi and Chen, Jiming and Ye, Qi",2024,,,,arXiv preprint arXiv:2409.04851
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,chen2022structural,\cite{chen2022structural},Structural triangulation: A closed-form solution to constrained 3d human pose estimation,,,True,False,"Chen, Zhuo and Zhao, Xu and Wan, Xiaoyue",2022,,,,
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,he2016deep,\cite{he2016deep},Deep residual learning for image recognition,,,True,False,"He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian",2016,,,,
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,dosovitskiy2020image,\cite{dosovitskiy2020image},An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,https://arxiv.org/abs/2010.11929v2,"While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",True,True,"Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others",2020,,,,arXiv preprint arXiv:2010.11929
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,chen2017deeplab,\cite{chen2017deeplab},"Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs",,,True,False,"Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L",2017,,,,IEEE transactions on pattern analysis and machine intelligence
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,Cheng_2022_CVPR,\cite{Cheng_2022_CVPR},Masked-Attention Mask Transformer for Universal Image Segmentation,,,True,False,"Cheng, Bowen and Misra, Ishan and Schwing, Alexander G. and Kirillov, Alexander and Girdhar, Rohit",2022,June,,,
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,martinez2017simple,\cite{martinez2017simple},A simple yet effective baseline for 3d human pose estimation,,,True,False,"Martinez, Julieta and Hossain, Rayat and Romero, Javier and Little, James J",2017,,,,
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,ye2023distilpose,\cite{ye2023distilpose},Distilpose: Tokenized pose regression with heatmap distillation,,,True,False,"Ye, Suhang and Zhang, Yingyi and Hu, Jie and Cao, Liujuan and Zhang, Shengchuan and Shen, Lei and Wang, Jun and Ding, Shouhong and Ji, Rongrong",2023,,,,
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,newell2016stacked,\cite{newell2016stacked},Stacked hourglass networks for human pose estimation,,,True,False,"Newell, Alejandro and Yang, Kaiyu and Deng, Jia",2016,,,,
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,xiao2018simple,\cite{xiao2018simple},Simple baselines for human pose estimation and tracking,,,True,False,"Xiao, Bin and Wu, Haiping and Wei, Yichen",2018,,,,
SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering,2511.08294v1,sun2019deep,\cite{sun2019deep},Deep high-resolution representation learning for human pose estimation,,,True,False,"Sun, Ke and Xiao, Bin and Liu, Dong and Wang, Jingdong",2019,,,,
ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking,2511.09833v1,tan2024large,\cite{tan2024large},Large language models for data annotation and synthesis: A survey,,,True,False,"Tan, Zhen and Li, Dawei and Wang, Song and Beigi, Alimohammad and Jiang, Bohan and Bhattacharjee, Amrita and Karami, Mansooreh and Li, Jundong and Cheng, Lu and Liu, Huan",2024,,,,
ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking,2511.09833v1,llmaaa2023zhang,\cite{llmaaa2023zhang},LLMaAA: Making Large Language Models as Active Annotators,https://arxiv.org/abs/2310.19596v2,"Prevalent supervised learning methods in natural language processing (NLP) are notoriously data-hungry, which demand large amounts of high-quality annotated data. In practice, acquiring such data is a costly endeavor. Recently, the superior few-shot performance of large language models (LLMs) has propelled the development of dataset generation, where the training data are solely synthesized from LLMs. However, such an approach usually suffers from low-quality issues, and requires orders of magnitude more labeled data to achieve satisfactory performance. To fully exploit the potential of LLMs and make use of massive unlabeled data, we propose LLMaAA, which takes LLMs as annotators and puts them into an active learning loop to determine what to annotate efficiently. To learn robustly with pseudo labels, we optimize both the annotation and training processes: (1) we draw k-NN examples from a small demonstration pool as in-context examples, and (2) we adopt the example reweighting technique to assign training samples with learnable weights. Compared with previous approaches, LLMaAA features both efficiency and reliability. We conduct experiments and analysis on two classic NLP tasks, named entity recognition and relation extraction. With LLMaAA, task-specific models trained from LLM-generated labels can outperform the teacher within only hundreds of annotated examples, which is much more cost-effective than other baselines.",True,True,"Ruoyu Zhang and
                  Yanzeng Li and
                  Yongliang Ma and
                  Ming Zhou and
                  Lei Zou",2023,,,,
ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking,2511.09833v1,pangakis2024knowledge,\cite{pangakis2024knowledge},Knowledge Distillation in Automated Annotation: Supervised Text Classification with LLM-Generated Training Labels,https://arxiv.org/abs/2406.17633v1,"Computational social science (CSS) practitioners often rely on human-labeled data to fine-tune supervised text classifiers. We assess the potential for researchers to augment or replace human-generated training data with surrogate training labels from generative large language models (LLMs). We introduce a recommended workflow and test this LLM application by replicating 14 classification tasks and measuring performance. We employ a novel corpus of English-language text classification data sets from recent CSS articles in high-impact journals. Because these data sets are stored in password-protected archives, our analyses are less prone to issues of contamination. For each task, we compare supervised classifiers fine-tuned using GPT-4 labels against classifiers fine-tuned with human annotations and against labels from GPT-4 and Mistral-7B with few-shot in-context learning. Our findings indicate that supervised classification models fine-tuned on LLM-generated labels perform comparably to models fine-tuned with labels from human annotators. Fine-tuning models using LLM-generated labels can be a fast, efficient and cost-effective method of building supervised text classifiers.",True,True,"Pangakis, Nicholas and Wolken, Samuel",2024,,,,Proceedings of the Sixth Workshop on Natural Language Processing and Computational Social Science
ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking,2511.09833v1,tseng2024expert,\cite{tseng2024expert},Are Expert-Level Language Models Expert-Level Annotators?,,,True,False,"Tseng, Yu-Min and Chen, Wei-Lin and Chen, Chung-Chi and Chen, Hsin-Hsi",2024,,,,
ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking,2511.09833v1,gligoric2024can,\cite{gligoric2024can},Can Unconfident LLM Annotations Be Used for Confident Conclusions?,https://arxiv.org/abs/2408.15204v2,"Large language models (LLMs) have shown high agreement with human raters across a variety of tasks, demonstrating potential to ease the challenges of human data collection. In computational social science (CSS), researchers are increasingly leveraging LLM annotations to complement slow and expensive human annotations. Still, guidelines for collecting and using LLM annotations, without compromising the validity of downstream conclusions, remain limited. We introduce Confidence-Driven Inference: a method that combines LLM annotations and LLM confidence indicators to strategically select which human annotations should be collected, with the goal of producing accurate statistical estimates and provably valid confidence intervals while reducing the number of human annotations needed. Our approach comes with safeguards against LLM annotations of poor quality, guaranteeing that the conclusions will be both valid and no less accurate than if we only relied on human annotations. We demonstrate the effectiveness of Confidence-Driven Inference over baselines in statistical estimation tasks across three CSS settings--text politeness, stance, and bias--reducing the needed number of human annotations by over 25% in each. Although we use CSS settings for demonstration, Confidence-Driven Inference can be used to estimate most standard quantities across a broad range of NLP problems.",True,True,"Gligori{\'c}, Kristina and Zrnic, Tijana and Lee, Cinoo and Cand{\`e}s, Emmanuel J and Jurafsky, Dan",2025,,,,Nations of the Americas Chapter of the Association for Computational Linguistics
ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking,2511.09833v1,chen2016xgboost,\cite{chen2016xgboost},XGBoost: A Scalable Tree Boosting System,https://arxiv.org/abs/1603.02754v3,"Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.",True,True,"Chen, Tianqi and Guestrin, Carlos",2016,,,,
ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking,2511.09833v1,kim2024meganno+,\cite{kim2024meganno+},MEGAnno+: A Human-LLM Collaborative Annotation System,https://arxiv.org/abs/2402.18050v1,"Large language models (LLMs) can label data faster and cheaper than humans for various NLP tasks. Despite their prowess, LLMs may fall short in understanding of complex, sociocultural, or domain-specific context, potentially leading to incorrect annotations. Therefore, we advocate a collaborative approach where humans and LLMs work together to produce reliable and high-quality labels. We present MEGAnno+, a human-LLM collaborative annotation system that offers effective LLM agent and annotation management, convenient and robust LLM annotation, and exploratory verification of LLM labels by humans.",True,True,"Kim, Hannah and Mitra, Kushan and Chen, Rafael Li and Rahman, Sajjadur and Zhang, Dan",2024,,,,arXiv preprint arXiv:2402.18050
ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking,2511.09833v1,wang2024human_llm,\cite{wang2024human_llm},Human-LLM Collaborative Annotation Through Effective Verification of LLM Labels,,,True,False,"Wang, Xinru and Kim, Hannah and Rahman, Sajjadur and Mitra, Kushan and Miao, Zhengjie",2024,,,10.1145/3613904.3641960,
ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking,2511.09833v1,mavromatis2023examples,\cite{mavromatis2023examples},Which examples to annotate for in-context learning? towards effective and efficient selection,,,True,False,"Mavromatis, Costas and Srinivasan, Balasubramaniam and Shen, Zhengyuan and Zhang, Jiani and Rangwala, Huzefa and Faloutsos, Christos and Karypis, George",2023,,,,arXiv preprint arXiv:2310.20046
ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking,2511.09833v1,wang2024model,\cite{wang2024model},Model-in-the-Loop (MILO): Accelerating Multimodal AI Data Annotation with LLMs,,,True,False,"Wang, Yifan and Stevens, David and Shah, Pranay and Jiang, Wenwen and Liu, Miao and Chen, Xu and Kuo, Robert and Li, Na and Gong, Boying and Lee, Daniel and others",2024,,,,arXiv preprint arXiv:2409.10702
ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking,2511.09833v1,gu2024survey,\cite{gu2024survey},A Survey on LLM-as-a-Judge,https://arxiv.org/abs/2411.15594v6,"Accurate and consistent evaluation is crucial for decision-making across numerous fields, yet it remains a challenging task due to inherent subjectivity, variability, and scale. Large Language Models (LLMs) have achieved remarkable success across diverse domains, leading to the emergence of ""LLM-as-a-Judge,"" where LLMs are employed as evaluators for complex tasks. With their ability to process diverse data types and provide scalable, cost-effective, and consistent assessments, LLMs present a compelling alternative to traditional expert-driven evaluations. However, ensuring the reliability of LLM-as-a-Judge systems remains a significant challenge that requires careful design and standardization. This paper provides a comprehensive survey of LLM-as-a-Judge, addressing the core question: How can reliable LLM-as-a-Judge systems be built? We explore strategies to enhance reliability, including improving consistency, mitigating biases, and adapting to diverse assessment scenarios. Additionally, we propose methodologies for evaluating the reliability of LLM-as-a-Judge systems, supported by a novel benchmark designed for this purpose. To advance the development and real-world deployment of LLM-as-a-Judge systems, we also discussed practical applications, challenges, and future directions. This survey serves as a foundational reference for researchers and practitioners in this rapidly evolving field.",True,True,"Gu, Jiawei and Jiang, Xuhui and Shi, Zhichao and Tan, Hexiang and Zhai, Xuehao and Xu, Chengjin and Li, Wei and Shen, Yinghan and Ma, Shengjie and Liu, Honghao and others",2024,,,,arXiv preprint arXiv:2411.15594
ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking,2511.09833v1,li2024llms,\cite{li2024llms},LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods,https://arxiv.org/abs/2412.05579v2,"The rapid advancement of Large Language Models (LLMs) has driven their expanding application across various fields. One of the most promising applications is their role as evaluators based on natural language responses, referred to as ''LLMs-as-judges''. This framework has attracted growing attention from both academia and industry due to their excellent effectiveness, ability to generalize across tasks, and interpretability in the form of natural language. This paper presents a comprehensive survey of the LLMs-as-judges paradigm from five key perspectives: Functionality, Methodology, Applications, Meta-evaluation, and Limitations. We begin by providing a systematic definition of LLMs-as-Judges and introduce their functionality (Why use LLM judges?). Then we address methodology to construct an evaluation system with LLMs (How to use LLM judges?). Additionally, we investigate the potential domains for their application (Where to use LLM judges?) and discuss methods for evaluating them in various contexts (How to evaluate LLM judges?). Finally, we provide a detailed analysis of the limitations of LLM judges and discuss potential future directions. Through a structured and comprehensive analysis, we aim aims to provide insights on the development and application of LLMs-as-judges in both research and practice. We will continue to maintain the relevant resource list at https://github.com/CSHaitao/Awesome-LLMs-as-Judges.",True,True,Haitao Li and Qian Dong and Junjie Chen and Huixue Su and Yujia Zhou and Qingyao Ai and Ziyi Ye and Yiqun Liu,2024,,https://arxiv.org/abs/2412.05579,,
ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking,2511.09833v1,chiang2023can,\cite{chiang2023can},Can Large Language Models Be an Alternative to Human Evaluations?,https://arxiv.org/abs/2305.01937v1,"Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans. However, human evaluation is very difficult to reproduce and its quality is notoriously unstable, hindering fair comparisons among different natural language processing (NLP) models and algorithms. Recently, large language models (LLMs) have demonstrated exceptional performance on unseen tasks when only the task instructions are provided. In this paper, we explore if such an ability of the LLMs can be used as an alternative to human evaluation. We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation. We use human evaluation and LLM evaluation to evaluate the texts in two NLP tasks: open-ended story generation and adversarial attacks. We show that the result of LLM evaluation is consistent with the results obtained by expert human evaluation: the texts rated higher by human experts are also rated higher by the LLMs. We also find that the results of LLM evaluation are stable over different formatting of the task instructions and the sampling algorithm used to generate the answer. We are the first to show the potential of using LLMs to assess the quality of texts and discuss the limitations and ethical considerations of LLM evaluation.",True,True,"Chiang, Cheng-Han and Lee, Hung-yi",2023,,,,The 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)
ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking,2511.09833v1,zheng2023judging,\cite{zheng2023judging},Judging llm-as-a-judge with mt-bench and chatbot arena,,,True,False,"Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others",2023,,,,Advances in Neural Information Processing Systems (NeurIPS)
ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking,2511.09833v1,huang2023large,\cite{huang2023large},Large Language Models Can Self-Improve,https://arxiv.org/abs/2210.11610v2,"Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate ""high-confidence"" rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and 63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.",True,True,"Huang, Jiaxin  and
      Gu, Shixiang  and
      Hou, Le  and
      Wu, Yuexin  and
      Wang, Xuezhi  and
      Yu, Hongkun  and
      Han, Jiawei",2023,,,10.18653/v1/2023.emnlp-main.67,
ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking,2511.09833v1,kamoi2024can,\cite{kamoi2024can},When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs,https://arxiv.org/abs/2406.01297v3,"Self-correction is an approach to improving responses from large language models (LLMs) by refining the responses using LLMs during inference. Prior work has proposed various self-correction frameworks using different sources of feedback, including self-evaluation and external feedback. However, there is still no consensus on the question of when LLMs can correct their own mistakes, as recent studies also report negative results. In this work, we critically survey broad papers and discuss the conditions required for successful self-correction. We first find that prior studies often do not define their research questions in detail and involve impractical frameworks or unfair evaluations that over-evaluate self-correction. To tackle these issues, we categorize research questions in self-correction research and provide a checklist for designing appropriate experiments. Our critical survey based on the newly categorized research questions shows that (1) no prior work demonstrates successful self-correction with feedback from prompted LLMs, except for studies in tasks that are exceptionally suited for self-correction, (2) self-correction works well in tasks that can use reliable external feedback, and (3) large-scale fine-tuning enables self-correction.",True,True,"Kamoi, Ryo and Zhang, Yusen and Zhang, Nan and Han, Jiawei and Zhang, Rui",2024,,,,Transactions of the Association for Computational Linguistics
ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking,2511.09833v1,song2025mind,\cite{song2025mind},Mind the Gap: Examining the Self-Improvement Capabilities of Large Language Models,,,True,False,Yuda Song and Hanlin Zhang and Carson Eisenach and Sham M. Kakade and Dean Foster and Udaya Ghai,2025,,,,
ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking,2511.09833v1,singh2023beyond,\cite{singh2023beyond},Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models,https://arxiv.org/abs/2312.06585v4,"Fine-tuning language models~(LMs) on human-generated data remains a prevalent practice. However, the performance of such models is often limited by the quantity and diversity of high-quality human data. In this paper, we explore whether we can go beyond human data on tasks where we have access to scalar feedback, for example, on math problems where one can verify correctness. To do so, we investigate a simple self-training method based on expectation-maximization, which we call ReST$^{EM}$, where we (1) generate samples from the model and filter them using binary feedback, (2) fine-tune the model on these samples, and (3) repeat this process a few times. Testing on advanced MATH reasoning and APPS coding benchmarks using PaLM-2 models, we find that ReST$^{EM}$ scales favorably with model size and significantly surpasses fine-tuning only on human data. Overall, our findings suggest self-training with feedback can substantially reduce dependence on human-generated data.",True,True,"Singh, Avi and Co-Reyes, John D and Agarwal, Rishabh and Anand, Ankesh and Patil, Piyush and Garcia, Xavier and Liu, Peter J and Harrison, James and Lee, Jaehoon and Xu, Kelvin and others",2023,,,,Transactions on Machine Learning Research (TMLR)
ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking,2511.09833v1,chen2023teaching,\cite{chen2023teaching},Teaching Large Language Models to Self-Debug,https://arxiv.org/abs/2304.05128v2,"Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, Self-Debugging with code explanation consistently improves the baseline by 2-3%, and improves the prediction accuracy on problems of the hardest level by 9%. On TransCoder and MBPP where unit tests are available, Self-Debugging improves the baseline accuracy by up to 12%. Meanwhile, by leveraging feedback messages and reusing failed predictions, Self-Debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10x candidate programs.",True,True,"Chen, Xinyun and Lin, Maxwell and Sch{\""a}rli, Nathanael and Zhou, Denny",2023,,,,International Conference on Machine Learning (ICML)
ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking,2511.09833v1,lee_volcano_2024,\cite{lee_volcano_2024},Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision,https://arxiv.org/abs/2311.07362v4,"Large multimodal models suffer from multimodal hallucination, where they provide incorrect responses misaligned with the given visual information. Recent works have conjectured that one of the reasons behind multimodal hallucination is due to the vision encoder failing to ground on the image properly. To mitigate this issue, we propose a novel approach that leverages self-feedback as visual cues. Building on this approach, we introduce Volcano, a multimodal self-feedback guided revision model. Volcano generates natural language feedback to its initial response based on the provided visual information and utilizes this feedback to self-revise its initial response. Volcano effectively reduces multimodal hallucination and achieves state-of-the-art on MMHal-Bench, POPE, and GAVIE. It also improves on general multimodal abilities and outperforms previous models on MM-Vet and MMBench. Through qualitative analysis, we show that Volcano's feedback is properly grounded on the image than the initial response. This indicates that Volcano can provide itself with richer visual information through feedback generation, leading to self-correct hallucinations. We publicly release our model, data, and code at https://github.com/kaistAI/Volcano}{github.com/kaistAI/Volcano",True,True,"Lee, Seongyun and Park, Sue Hyun and Jo, Yongrae and Seo, Minjoon",2024,,http://arxiv.org/abs/2311.07362,10.48550/arXiv.2311.07362,
ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking,2511.09833v1,saunders2022self,\cite{saunders2022self},Self-critiquing models for assisting human evaluators,,,True,False,"Saunders, William and Yeh, Catherine and Wu, Jeff and Bills, Steven and Ouyang, Long and Ward, Jonathan and Leike, Jan",2022,,,,arXiv preprint arXiv:2206.05802
ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking,2511.09833v1,ye2024justice,\cite{ye2024justice},Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge,https://arxiv.org/abs/2410.02736v2,"LLM-as-a-Judge has been widely utilized as an evaluation method in various benchmarks and served as supervised rewards in model training. However, despite their excellence in many domains, potential issues are under-explored, undermining their reliability and the scope of their utility. Therefore, we identify 12 key potential biases and propose a new automated bias quantification framework-CALM-which systematically quantifies and analyzes each type of bias in LLM-as-a-Judge by using automated and principle-guided modification. Our experiments cover multiple popular language models, and the results indicate that while advanced models have achieved commendable overall performance, significant biases persist in certain specific tasks. Empirical results suggest that there remains room for improvement in the reliability of LLM-as-a-Judge. Moreover, we also discuss the explicit and implicit influence of these biases and give some suggestions for the reliable application of LLM-as-a-Judge. Our work highlights the need for stakeholders to address these issues and remind users to exercise caution in LLM-as-a-Judge applications.",True,True,"Ye, Jiayi and Wang, Yanbo and Huang, Yue and Chen, Dongping and Zhang, Qihui and Moniz, Nuno and Gao, Tian and Geyer, Werner and Huang, Chao and Chen, Pin-Yu and others",2024,,,,The Thirteenth International Conference on Learning Representations (ICLR)
ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking,2511.09833v1,li2025preference,\cite{li2025preference},Preference Leakage: A Contamination Problem in LLM-as-a-judge,https://arxiv.org/abs/2502.01534v2,"Large Language Models (LLMs) as judges and LLM-based data synthesis have emerged as two fundamental LLM-driven data annotation methods in model development. While their combination significantly enhances the efficiency of model training and evaluation, little attention has been given to the potential contamination brought by this new model development paradigm. In this work, we expose preference leakage, a contamination problem in LLM-as-a-judge caused by the relatedness between the synthetic data generators and LLM-based evaluators. To study this issue, we first define three common relatednesses between the data generator LLM and the judge LLM: being the same model, having an inheritance relationship, and belonging to the same model family. Through extensive experiments, we empirically confirm the bias of judges towards their related student models caused by preference leakage across multiple LLM baselines and benchmarks. Further analysis suggests that preference leakage is a pervasive and real-world problem that is harder to detect compared to previously identified biases in LLM-as-a-judge scenarios. All of these findings imply that preference leakage is a widespread and challenging problem in the area of LLM-as-a-judge. We release all codes and data at: https://github.com/David-Li0406/Preference-Leakage.",True,True,Dawei Li and Renliang Sun and Yue Huang and Ming Zhong and Bohan Jiang and Jiawei Han and Xiangliang Zhang and Wei Wang and Huan Liu,2025,,https://arxiv.org/abs/2502.01534,,arXiv preprint arXiv:2502.01534
SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph,2511.08810v1,PGD,\cite{PGD},Towards Deep Learning Models Resistant to Adversarial Attacks,https://arxiv.org/abs/1706.06083v4,"Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist_challenge and https://github.com/MadryLab/cifar10_challenge.",True,True,Aleksander Madry and Aleksandar Makelov and Ludwig Schmidt and Dimitris Tsipras and Adrian Vladu,2018,,,,
SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph,2511.08810v1,adversarial,\cite{adversarial},On the Adversarial Robustness of Vision Transformers,,,True,False,Rulin Shao and Zhouxing Shi and Jinfeng Yi and Pin-Yu Chen and Cho-Jui Hsieh,2022,,,,Transactions on Machine Learning Research
SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph,2511.08810v1,Diffusiondefense,\cite{Diffusiondefense},Defending Against Frequency-Based Attacks with Diffusion Models,https://arxiv.org/abs/2504.11034v1,"Adversarial training is a common strategy for enhancing model robustness against adversarial attacks. However, it is typically tailored to the specific attack types it is trained on, limiting its ability to generalize to unseen threat models. Adversarial purification offers an alternative by leveraging a generative model to remove perturbations before classification. Since the purifier is trained independently of both the classifier and the threat models, it is better equipped to handle previously unseen attack scenarios. Diffusion models have proven highly effective for noise purification, not only in countering pixel-wise adversarial perturbations but also in addressing non-adversarial data shifts. In this study, we broaden the focus beyond pixel-wise robustness to explore the extent to which purification can mitigate both spectral and spatial adversarial attacks. Our findings highlight its effectiveness in handling diverse distortion patterns across low- to high-frequency regions.",True,True,Fatemeh Amerehi and Patrick Healy,2025,,,,
SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph,2511.08810v1,gradientobfu,\cite{gradientobfu},Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,,,True,False,Anish Athalye and Nicholas Carlini and David Wagner,2018,,,,
SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph,2511.08810v1,freqfusion,\cite{freqfusion},Frequency Domain Adversarial Training for Robust Volumetric Medical Image Segmentation,,,True,False,Asif Hanif and Muzammal Naseer and Salman Khan and Mubarak Shah and Fahad Shahbaz Khan,2023,,,,
SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph,2511.08810v1,FrequencyFusion2,\cite{FrequencyFusion2},Frequency Centric Defense Mechanisms against Adversarial Examples,https://arxiv.org/abs/2110.13935v1,"Adversarial example (AE) aims at fooling a Convolution Neural Network by introducing small perturbations in the input image.The proposed work uses the magnitude and phase of the Fourier Spectrum and the entropy of the image to defend against AE. We demonstrate the defense in two ways: by training an adversarial detector and denoising the adversarial effect. Experiments were conducted on the low-resolution CIFAR-10 and high-resolution ImageNet datasets. The adversarial detector has 99% accuracy for FGSM and PGD attacks on the CIFAR-10 dataset. However, the detection accuracy falls to 50% for sophisticated DeepFool and Carlini & Wagner attacks on ImageNet. We overcome the limitation by using autoencoder and show that 70% of AEs are correctly classified after denoising.",True,True,Sanket B. Shah and Param Raval and Harin Khakhi and Mehul S. Raval,2021,,,,
SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph,2511.08810v1,FrequencyFusion,\cite{FrequencyFusion},Adversarial Robustness of Deep Sensor Fusion Models,https://arxiv.org/abs/2006.13192v3,"We experimentally study the robustness of deep camera-LiDAR fusion architectures for 2D object detection in autonomous driving. First, we find that the fusion model is usually both more accurate, and more robust against single-source attacks than single-sensor deep neural networks. Furthermore, we show that without adversarial training, early fusion is more robust than late fusion, whereas the two perform similarly after adversarial training. However, we note that single-channel adversarial training of deep fusion is often detrimental even to robustness. Moreover, we observe cross-channel externalities, where single-channel adversarial training reduces robustness to attacks on the other channel. Additionally, we observe that the choice of adversarial model in adversarial training is critical: using attacks restricted to cars' bounding boxes is more effective in adversarial training and exhibits less significant cross-channel externalities. Finally, we find that joint-channel adversarial training helps mitigate many of the issues above, but does not significantly boost adversarial robustness.",True,True,Shaojie Wang and Tong Wu and Ayan Chakrabarti and Yevgeniy Vorobeychik,2022,,,,
SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph,2511.08810v1,edgemulti1,\cite{edgemulti1},"Adversarial Examples for Edge Detection: They Exist, and They Transfer",,,True,False,Christian Cosgrove and Alan Yuille,2020,,,,
SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph,2511.08810v1,multidefense1,\cite{multidefense1},MMCert: Provable Defense against Adversarial Attacks to Multi-modal Models,https://arxiv.org/abs/2403.19080v3,"Different from a unimodal model whose input is from a single modality, the input (called multi-modal input) of a multi-modal model is from multiple modalities such as image, 3D points, audio, text, etc. Similar to unimodal models, many existing studies show that a multi-modal model is also vulnerable to adversarial perturbation, where an attacker could add small perturbation to all modalities of a multi-modal input such that the multi-modal model makes incorrect predictions for it. Existing certified defenses are mostly designed for unimodal models, which achieve sub-optimal certified robustness guarantees when extended to multi-modal models as shown in our experimental results. In our work, we propose MMCert, the first certified defense against adversarial attacks to a multi-modal model. We derive a lower bound on the performance of our MMCert under arbitrary adversarial attacks with bounded perturbations to both modalities (e.g., in the context of auto-driving, we bound the number of changed pixels in both RGB image and depth image). We evaluate our MMCert using two benchmark datasets: one for the multi-modal road segmentation task and the other for the multi-modal emotion recognition task. Moreover, we compare our MMCert with a state-of-the-art certified defense extended from unimodal models. Our experimental results show that our MMCert outperforms the baseline.",True,True,Yanting Wang and Hongye Fu and Wei Zou and Jinyuan Jia,2024,,,,
SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph,2511.08810v1,multidefense2,\cite{multidefense2},Defending Multimodal Fusion Models against Single-Source Adversaries,https://arxiv.org/abs/2206.12714v1,"Beyond achieving high performance across many vision tasks, multimodal models are expected to be robust to single-source faults due to the availability of redundant information between modalities. In this paper, we investigate the robustness of multimodal neural networks against worst-case (i.e., adversarial) perturbations on a single modality. We first show that standard multimodal fusion models are vulnerable to single-source adversaries: an attack on any single modality can overcome the correct information from multiple unperturbed modalities and cause the model to fail. This surprising vulnerability holds across diverse multimodal tasks and necessitates a solution. Motivated by this finding, we propose an adversarially robust fusion strategy that trains the model to compare information coming from all the input sources, detect inconsistencies in the perturbed modality compared to the other modalities, and only allow information from the unperturbed modalities to pass through. Our approach significantly improves on state-of-the-art methods in single-source robustness, achieving gains of 7.8-25.2% on action recognition, 19.7-48.2% on object detection, and 1.6-6.7% on sentiment analysis, without degrading performance on unperturbed (i.e., clean) data.",True,True,Karren Yang and Wan-Yi Lin and Manash Barman and Filipe Condessa and Zico Kolter,2021,,,,
SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph,2511.08810v1,CLIP,\cite{CLIP},Learning Transferable Visual Models From Natural Language Supervision,https://arxiv.org/abs/2103.00020v1,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",True,True,Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever,2021,,,,
SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph,2511.08810v1,dino,\cite{dino},Grounding DINO: Marrying DINO with Grounded Pre‑Training for Open‑Set Object Detection,,,True,False,Shilong Liu and Zhaoyang Zeng and Tianhe Ren and Feng Li and Hao Zhang and Jie Yang and Chunyuan Li and Jianwei Yang and Hang Su and Jun Zhu and Lei Zhang,2024,,,,
SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph,2511.08810v1,ALBEF,\cite{ALBEF},Align Before Fuse: Vision and Language Representation Learning with Momentum Distillation,,,True,False,Junnan Li and Ramprasaath R. Selvaraju and Akhilesh Deepak Gotmare and Shafiq Joty and Caiming Xiong and Steven Hoi,2021,,,,
SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph,2511.08810v1,segmenteddefense,\cite{segmenteddefense},Defense against Adversarial Patch Attacks for Aerial Image Semantic Segmentation by Robust Feature Extraction,,,True,False,Yinkai Zan and Pingping Lu and Tingyu Meng,2023,,,,Remote Sensing
SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph,2511.08810v1,gnnsurvey,\cite{gnnsurvey},Graph Neural Networks in Vision-Language Image Understanding: A Survey,https://arxiv.org/abs/2303.03761v2,"2D image understanding is a complex problem within computer vision, but it holds the key to providing human-level scene comprehension. It goes further than identifying the objects in an image, and instead, it attempts to understand the scene. Solutions to this problem form the underpinning of a range of tasks, including image captioning, visual question answering (VQA), and image retrieval. Graphs provide a natural way to represent the relational arrangement between objects in an image, and thus, in recent years graph neural networks (GNNs) have become a standard component of many 2D image understanding pipelines, becoming a core architectural component, especially in the VQA group of tasks. In this survey, we review this rapidly evolving field and we provide a taxonomy of graph types used in 2D image understanding approaches, a comprehensive list of the GNN models used in this domain, and a roadmap of future potential developments. To the best of our knowledge, this is the first comprehensive survey that covers image captioning, visual question answering, and image retrieval techniques that focus on using GNNs as the main part of their architecture.",True,True,Henry Senior and Gregory Slabaugh and Shanxin Yuan and Luca Rossi,2024,,,,The Visual Computer
SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph,2511.08810v1,scene_graph,\cite{scene_graph},A Comprehensive Survey of Scene Graphs: Generation and Applications,,,True,False,Xiaojun Chang and Pengzhen Ren and Pengfei Xu and Zhihui Li and Xiaojiang Chen and Alex Hauptmann,2023,,,,IEEE Transactions on Pattern Analysis and Machine Intelligence
SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph,2511.08810v1,hoi_gnn,\cite{hoi_gnn},Learning Human-Object Interactions by Graph Parsing Neural Networks,https://arxiv.org/abs/1808.07962v1,"This paper addresses the task of detecting and recognizing human-object interactions (HOI) in images and videos. We introduce the Graph Parsing Neural Network (GPNN), a framework that incorporates structural knowledge while being differentiable end-to-end. For a given scene, GPNN infers a parse graph that includes i) the HOI graph structure represented by an adjacency matrix, and ii) the node labels. Within a message passing inference framework, GPNN iteratively computes the adjacency matrices and node labels. We extensively evaluate our model on three HOI detection benchmarks on images and videos: HICO-DET, V-COCO, and CAD-120 datasets. Our approach significantly outperforms state-of-art methods, verifying that GPNN is scalable to large datasets and applies to spatial-temporal settings. The code is available at https://github.com/SiyuanQi/gpnn.",True,True,Siyuan Qi and Wenguan Wang and Baoxiong Jia and Jianbing Shen and Song-Chun Zhu,2018,,,,
SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph,2511.08810v1,3d_gnn_review,\cite{3d_gnn_review},Graph Neural Networks in Point Clouds: A Survey,,,True,False,Dilong Li and Chenghui Lu and Ziyi Chen and Jianlong Guan and Jing Zhao and Jixiang Du,2024,,,,Remote Sensing
SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph,2511.08810v1,siftgraph,\cite{siftgraph},DRG‑NET: A graph neural network for computer‑aided grading of diabetic retinopathy,,,True,False,Amritha Abdul Salam and Manjunatha Mahadevappa and Asha Das and Madhu S. Nair,,,,,"Signal, Image and Video Processing"
SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph,2511.08810v1,splinecnn,\cite{splinecnn},SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels,https://arxiv.org/abs/1711.08920v2,"We present Spline-based Convolutional Neural Networks (SplineCNNs), a variant of deep neural networks for irregular structured and geometric input, e.g., graphs or meshes. Our main contribution is a novel convolution operator based on B-splines, that makes the computation time independent from the kernel size due to the local support property of the B-spline basis functions. As a result, we obtain a generalization of the traditional CNN convolution operator by using continuous kernel functions parametrized by a fixed number of trainable weights. In contrast to related approaches that filter in the spectral domain, the proposed method aggregates features purely in the spatial domain. In addition, SplineCNN allows entire end-to-end training of deep architectures, using only the geometric structure as input, instead of handcrafted feature descriptors. For validation, we apply our method on tasks from the fields of image graph classification, shape correspondence and graph node classification, and show that it outperforms or pars state-of-the-art approaches while being significantly faster and having favorable properties like domain-independence.",True,True,Matthias Fey and Jan Eric Lenssen and Frank Weichert and Heinrich Müller,2018,,,,
SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph,2511.08810v1,robust_gnn,\cite{robust_gnn},Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach,,,True,False,Kai Zhao and Qiyu Kang and Yang Song and Rui She and Sijie Wang and Wee Peng Tay,2023,,,,
SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph,2511.08810v1,gnn_defense,\cite{gnn_defense},Are Defenses for Graph Neural Networks Robust?,,,True,False,Felix Mujkanovic and Simon Geisler and Stephan Günnemann and Aleksandar Bojchevski,2022,,,,
VocalBench-zh: Decomposing and Benchmarking the Speech Conversational Abilities in Mandarin Context,2511.08230v1,chen2025minmo,\cite{chen2025minmo},MinMo: A Multimodal Large Language Model for Seamless Voice Interaction,https://arxiv.org/abs/2501.06282v1,"Recent advancements in large language models (LLMs) and multimodal speech-text models have laid the groundwork for seamless voice interactions, enabling real-time, natural, and human-like conversations. Previous models for voice interactions are categorized as native and aligned. Native models integrate speech and text processing in one framework but struggle with issues like differing sequence lengths and insufficient pre-training. Aligned models maintain text LLM capabilities but are often limited by small datasets and a narrow focus on speech tasks. In this work, we introduce MinMo, a Multimodal Large Language Model with approximately 8B parameters for seamless voice interaction. We address the main limitations of prior aligned multimodal models. We train MinMo through multiple stages of speech-to-text alignment, text-to-speech alignment, speech-to-speech alignment, and duplex interaction alignment, on 1.4 million hours of diverse speech data and a broad range of speech tasks. After the multi-stage training, MinMo achieves state-of-the-art performance across various benchmarks for voice comprehension and generation while maintaining the capabilities of text LLMs, and also facilitates full-duplex conversation, that is, simultaneous two-way communication between the user and the system. Moreover, we propose a novel and simple voice decoder that outperforms prior models in voice generation. The enhanced instruction-following capabilities of MinMo supports controlling speech generation based on user instructions, with various nuances including emotions, dialects, and speaking rates, and mimicking specific voices. For MinMo, the speech-to-text latency is approximately 100ms, full-duplex latency is approximately 600ms in theory and 800ms in practice. The MinMo project web page is https://funaudiollm.github.io/minmo, and the code and models will be released soon.",True,True,"Chen, Qian and Chen, Yafeng and Chen, Yanni and Chen, Mengzhe and Chen, Yingda and Deng, Chong and Du, Zhihao and Gao, Ruize and Gao, Changfeng and Gao, Zhifu and others",2025,,,,arXiv preprint arXiv:2501.06282
VocalBench-zh: Decomposing and Benchmarking the Speech Conversational Abilities in Mandarin Context,2511.08230v1,li2025baichuan,\cite{li2025baichuan},Baichuan-Omni Technical Report,https://arxiv.org/abs/2410.08565v4,"The salient multimodal capabilities and interactive experience of GPT-4o highlight its critical role in practical applications, yet it lacks a high-performing open-source counterpart. In this paper, we introduce Baichuan-omni, the first open-source 7B Multimodal Large Language Model (MLLM) adept at concurrently processing and analyzing modalities of image, video, audio, and text, while delivering an advanced multimodal interactive experience and strong performance. We propose an effective multimodal training schema starting with 7B model and proceeding through two stages of multimodal alignment and multitask fine-tuning across audio, image, video, and text modal. This approach equips the language model with the ability to handle visual and audio data effectively. Demonstrating strong performance across various omni-modal and multimodal benchmarks, we aim for this contribution to serve as a competitive baseline for the open-source community in advancing multimodal understanding and real-time interaction.",True,True,"Li, Yadong and Liu, Jun and Zhang, Tao and Chen, Song and Li, Tianpeng and Li, Zehuan and Liu, Lijun and Ming, Lingfeng and Dong, Guosheng and Pan, Da and others",2025,,,,arXiv preprint arXiv:2501.15368
VocalBench-zh: Decomposing and Benchmarking the Speech Conversational Abilities in Mandarin Context,2511.08230v1,zeng2024glm,\cite{zeng2024glm},Glm-4-voice: Towards intelligent and human-like end-to-end spoken chatbot,,,True,False,"Zeng, Aohan and Du, Zhengxiao and Liu, Mingdao and Wang, Kedong and Jiang, Shengmin and Zhao, Lei and Dong, Yuxiao and Tang, Jie",2024,,,,arXiv preprint arXiv:2412.02612
VocalBench-zh: Decomposing and Benchmarking the Speech Conversational Abilities in Mandarin Context,2511.08230v1,long2025vita,\cite{long2025vita},VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model,https://arxiv.org/abs/2505.03739v2,"With the growing requirement for natural human-computer interaction, speech-based systems receive increasing attention as speech is one of the most common forms of daily communication. However, the existing speech models still experience high latency when generating the first audio token during streaming, which poses a significant bottleneck for deployment. To address this issue, we propose VITA-Audio, an end-to-end large speech model with fast audio-text token generation. Specifically, we introduce a lightweight Multiple Cross-modal Token Prediction (MCTP) module that efficiently generates multiple audio tokens within a single model forward pass, which not only accelerates the inference but also significantly reduces the latency for generating the first audio in streaming scenarios. In addition, a four-stage progressive training strategy is explored to achieve model acceleration with minimal loss of speech quality. To our knowledge, VITA-Audio is the first multi-modal large language model capable of generating audio output during the first forward pass, enabling real-time conversational capabilities with minimal latency. VITA-Audio is fully reproducible and is trained on open-source data only. Experimental results demonstrate that our model achieves an inference speedup of 3~5x at the 7B parameter scale, but also significantly outperforms open-source models of similar model size on multiple benchmarks for automatic speech recognition (ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.",True,True,"Long, Zuwei and Shen, Yunhang and Fu, Chaoyou and Gao, Heting and Li, Lijiang and Chen, Peixian and Zhang, Mengdan and Shao, Hang and Li, Jian and Peng, Jinlong and others",2025,,,,arXiv preprint arXiv:2505.03739
VocalBench-zh: Decomposing and Benchmarking the Speech Conversational Abilities in Mandarin Context,2511.08230v1,wang2025vocalnet,\cite{wang2025vocalnet},VocalNet: Speech LLM with Multi-Token Prediction for Faster and High-Quality Generation,,,True,False,"Wang, Yuhao and Liu, Heyang and Cheng, Ziyang and Wu, Ronghua and Gu, Qunshan and Wang, Yanfeng and Wang, Yu",2025,,,,arXiv preprint arXiv:2504.04060
VocalBench-zh: Decomposing and Benchmarking the Speech Conversational Abilities in Mandarin Context,2511.08230v1,xu2025qwen2,\cite{xu2025qwen2},Qwen2. 5-omni technical report,,,True,False,"Xu, Jin and Guo, Zhifang and He, Jinzheng and Hu, Hangrui and He, Ting and Bai, Shuai and Chen, Keqin and Wang, Jialin and Fan, Yang and Dang, Kai and others",2025,,,,arXiv preprint arXiv:2503.20215
VocalBench-zh: Decomposing and Benchmarking the Speech Conversational Abilities in Mandarin Context,2511.08230v1,xu2025qwen3,\cite{xu2025qwen3},Qwen3-Omni Technical Report,https://arxiv.org/abs/2509.17765v1,"We present Qwen3-Omni, a single multimodal model that, for the first time, maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages, and speech generation in 10 languages. To reduce first-packet latency in streaming synthesis, Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms. To further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.",True,True,"Xu, Jin and Guo, Zhifang and Hu, Hangrui and Chu, Yunfei and Wang, Xiong and He, Jinzheng and Wang, Yuxuan and Shi, Xian and He, Ting and Zhu, Xinfa and others",2025,,,,arXiv preprint arXiv:2509.17765
VocalBench-zh: Decomposing and Benchmarking the Speech Conversational Abilities in Mandarin Context,2511.08230v1,coreteam2025mimoaudio,\cite{coreteam2025mimoaudio},MiMo-Audio: Audio Language Models are Few-Shot Learners,,,True,False,LLM-Core-Team Xiaomi,2025,,https://github.com/XiaomiMiMo/MiMo-Audio,,
VocalBench-zh: Decomposing and Benchmarking the Speech Conversational Abilities in Mandarin Context,2511.08230v1,chen2024voicebench,\cite{chen2024voicebench},VoiceBench: Benchmarking LLM-Based Voice Assistants,https://arxiv.org/abs/2410.17196v3,"Building on the success of large language models (LLMs), recent advancements such as GPT-4o have enabled real-time speech interactions through LLM-based voice assistants, offering a significantly improved user experience compared to traditional text-based interactions. However, the absence of benchmarks designed to evaluate these speech interaction capabilities has hindered progress of LLM-based voice assistants development. Current evaluations focus primarily on automatic speech recognition (ASR) or general knowledge evaluation with clean speeches, neglecting the more intricate, real-world scenarios that involve diverse speaker characteristics, environmental and content factors. To address this, we introduce VoiceBench, the first benchmark designed to provide a multi-faceted evaluation of LLM-based voice assistants. VoiceBench also includes both real and synthetic spoken instructions that incorporate the above three key real-world variations. Extensive experiments reveal the limitations of current LLM-based voice assistant models and offer valuable insights for future research and development in this field.",True,True,"Chen, Yiming and Yue, Xianghu and Zhang, Chen and Gao, Xiaoxue and Tan, Robby T and Li, Haizhou",2024,,,,arXiv preprint arXiv:2410.17196
VocalBench-zh: Decomposing and Benchmarking the Speech Conversational Abilities in Mandarin Context,2511.08230v1,zhang2025wildspeech,\cite{zhang2025wildspeech},WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation,,,True,False,"Zhang, Jian and Zhang, Linhao and Lei, Bokai and Wu, Chuhan and Jia, Wei and Zhou, Xiao",2025,,,,arXiv preprint arXiv:2506.21875
VocalBench-zh: Decomposing and Benchmarking the Speech Conversational Abilities in Mandarin Context,2511.08230v1,wang2025voiceassistant,\cite{wang2025voiceassistant},"VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing",https://arxiv.org/abs/2509.22651v1,"The growing capabilities of large language models and multimodal systems have spurred interest in voice-first AI assistants, yet existing benchmarks are inadequate for evaluating the full range of these systems' capabilities. We introduce VoiceAssistant-Eval, a comprehensive benchmark designed to assess AI assistants across listening, speaking, and viewing. VoiceAssistant-Eval comprises 10,497 curated examples spanning 13 task categories. These tasks include natural sounds, music, and spoken dialogue for listening; multi-turn dialogue, role-play imitation, and various scenarios for speaking; and highly heterogeneous images for viewing. To demonstrate its utility, we evaluate 21 open-source models and GPT-4o-Audio, measuring the quality of the response content and speech, as well as their consistency. The results reveal three key findings: (1) proprietary models do not universally outperform open-source models; (2) most models excel at speaking tasks but lag in audio understanding; and (3) well-designed smaller models can rival much larger ones. Notably, the mid-sized Step-Audio-2-mini (7B) achieves more than double the listening accuracy of LLaMA-Omni2-32B-Bilingual. However, challenges remain: multimodal (audio plus visual) input and role-play voice imitation tasks are difficult for current models, and significant gaps persist in robustness and safety alignment. VoiceAssistant-Eval identifies these gaps and establishes a rigorous framework for evaluating and guiding the development of next-generation AI assistants. Code and data will be released at https://mathllm.github.io/VoiceAssistantEval/ .",True,True,"Wang, Ke and Ren, Houxing and Lu, Zimu and Zhan, Mingjie and Li, Hongsheng",2025,,,,arXiv preprint arXiv:2509.22651
VocalBench-zh: Decomposing and Benchmarking the Speech Conversational Abilities in Mandarin Context,2511.08230v1,yan2025uro,\cite{yan2025uro},URO-Bench: A Comprehensive Benchmark for End-to-End Spoken Dialogue Models,,,True,False,"Yan, Ruiqi and Li, Xiquan and Chen, Wenxi and Niu, Zhikang and Yang, Chen and Ma, Ziyang and Yu, Kai and Chen, Xie",2025,,,,arXiv preprint arXiv:2502.17810
VocalBench-zh: Decomposing and Benchmarking the Speech Conversational Abilities in Mandarin Context,2511.08230v1,hu2025vcb,\cite{hu2025vcb},VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents,,,True,False,"Hu, Jiliang and Wang, Wenfu and Li, Zuchao and Li, Chenxing and Zhao, Yiyang and Li, Hanzhao and Zhang, Liqiang and Yu, Meng and Yu, Dong",2025,,,,arXiv preprint arXiv:2510.11098
VocalBench-zh: Decomposing and Benchmarking the Speech Conversational Abilities in Mandarin Context,2511.08230v1,li2025televal,\cite{li2025televal},TELEVAL: A dynamic benchmark designed for spoken language models in chinese interactive scenarios,,,True,False,"Li, Zehan and Chen, Hongjie and Zhang, Yuxin and Zhou, Jing and Wang, Xuening and Lv, Hang and Du, Mengjie and Song, Yaodong and Lian, Jie and Kang, Jian and others",2025,,,,arXiv preprint arXiv:2507.18061
Conformal Prediction for Multi-Source Detection on a Network,2511.08867v1,MalPaiLen:J22,\cite{MalPaiLen:J22},Well-calibrated confidence measures for multi-label text classification with a large number of labels,,,True,False,"Maltoudoglou, Lysimachos and Paisios, Andreas and Lenc, Ladislav and Mart{\'\i}nek, Ji{\v{r}}{\'\i} and Kr{\'a}l, Pavel and Papadopoulos, Harris",2022,,,,Pattern Recognition
Conformal Prediction for Multi-Source Detection on a Network,2511.08867v1,TyaGuo:C23,\cite{TyaGuo:C23},Multi-label Classification under Uncertainty: A Tree-based Conformal Prediction Approach,https://arxiv.org/abs/2404.19472v1,"Multi-label classification is a common challenge in various machine learning applications, where a single data instance can be associated with multiple classes simultaneously. The current paper proposes a novel tree-based method for multi-label classification using conformal prediction and multiple hypothesis testing. The proposed method employs hierarchical clustering with labelsets to develop a hierarchical tree, which is then formulated as a multiple-testing problem with a hierarchical structure. The split-conformal prediction method is used to obtain marginal conformal $p$-values for each tested hypothesis, and two \textit{hierarchical testing procedures} are developed based on marginal conformal $p$-values, including a hierarchical Bonferroni procedure and its modification for controlling the family-wise error rate. The prediction sets are thus formed based on the testing outcomes of these two procedures. We establish a theoretical guarantee of valid coverage for the prediction sets through proven family-wise error rate control of those two procedures. We demonstrate the effectiveness of our method in a simulation study and two real data analysis compared to other conformal methods for multi-label classification.",True,True,"Tyagi, Chhavi and Guo, Wenge",2023,13--15 Sep,,,
Conformal Prediction for Multi-Source Detection on a Network,2511.08867v1,KatPap:J24,\cite{KatPap:J24},Multi-label conformal prediction with a mahalanobis distance nonconformity measure,,,True,False,"Katsios, Kostas and Papadopoulos, Harris",2024,,,,Proceedings of Machine Learning Research
Conformal Prediction for Multi-Source Detection on a Network,2511.08867v1,CauGupDuc:J21,\cite{CauGupDuc:J21},Knowing what you know: valid and validated confidence sets in multiclass and multilabel prediction,https://arxiv.org/abs/2004.10181v3,"We develop conformal prediction methods for constructing valid predictive confidence sets in multiclass and multilabel problems without assumptions on the data generating distribution. A challenge here is that typical conformal prediction methods---which give marginal validity (coverage) guarantees---provide uneven coverage, in that they address easy examples at the expense of essentially ignoring difficult examples. By leveraging ideas from quantile regression, we build methods that always guarantee correct coverage but additionally provide (asymptotically optimal) conditional coverage for both multiclass and multilabel prediction problems. To address the potential challenge of exponentially large confidence sets in multilabel prediction, we build tree-structured classifiers that efficiently account for interactions between labels. Our methods can be bolted on top of any classification model---neural network, random forest, boosted tree---to guarantee its validity. We also provide an empirical evaluation, simultaneously providing new validation methods, that suggests the more robust coverage of our confidence sets.",True,True,Maxime Cauchois and Suyash Gupta and John C. Duchi,2021,,,,
Conformal Prediction for Multi-Source Detection on a Network,2511.08867v1,AnaSteAda:C24,\cite{AnaSteAda:C24},Conformal Risk Control,https://arxiv.org/abs/2208.02814v4,"We extend conformal prediction to control the expected value of any monotone loss function. The algorithm generalizes split conformal prediction together with its coverage guarantee. Like conformal prediction, the conformal risk control procedure is tight up to an $\mathcal{O}(1/n)$ factor. We also introduce extensions of the idea to distribution shift, quantile risk control, multiple and adversarial risk control, and expectations of U-statistics. Worked examples from computer vision and natural language processing demonstrate the usage of our algorithm to bound the false negative rate, graph distance, and token-level F1-score.",True,True,"Angelopoulos, Anastasios N and Bates, Stephen and Fisch, Adam and Lei, Lihua and Schuster, Tal",2024,,,,
Conformal Prediction for Multi-Source Detection on a Network,2511.08867v1,DawLiXu:C21,\cite{DawLiXu:C21},Diffusion source identification on networks with statistical confidence,,,True,False,"Dawkins, Quinlan E and Li, Tianxi and Xu, Haifeng",2021,,,,
Melodia: Training-Free Music Editing Guided by Attention Probing in Diffusion Models,2511.08252v1,agostinelli2023musiclm,\cite{agostinelli2023musiclm},MusicLM: Generating Music From Text,https://arxiv.org/abs/2301.11325v1,"We introduce MusicLM, a model generating high-fidelity music from text descriptions such as ""a calming violin melody backed by a distorted guitar riff"". MusicLM casts the process of conditional music generation as a hierarchical sequence-to-sequence modeling task, and it generates music at 24 kHz that remains consistent over several minutes. Our experiments show that MusicLM outperforms previous systems both in audio quality and adherence to the text description. Moreover, we demonstrate that MusicLM can be conditioned on both text and a melody in that it can transform whistled and hummed melodies according to the style described in a text caption. To support future research, we publicly release MusicCaps, a dataset composed of 5.5k music-text pairs, with rich text descriptions provided by human experts.",True,True,"Agostinelli, Andrea and Denk, Timo I and Borsos, Zal{\'a}n and Engel, Jesse and Verzetti, Mauro and Caillon, Antoine and Huang, Qingqing and Jansen, Aren and Roberts, Adam and Tagliasacchi, Marco and others",2023,,,,arXiv preprint arXiv:2301.11325
Melodia: Training-Free Music Editing Guided by Attention Probing in Diffusion Models,2511.08252v1,musicgen,\cite{musicgen},Simple and controllable music generation,,,True,False,"Copet, Jade and Kreuk, Felix and Gat, Itai and Remez, Tal and Kant, David and Synnaeve, Gabriel and Adi, Yossi and D{\'e}fossez, Alexandre",2023,,,,Advances in Neural Information Processing Systems
Melodia: Training-Free Music Editing Guided by Attention Probing in Diffusion Models,2511.08252v1,audioldm,\cite{audioldm},Audioldm: Text-to-audio generation with latent diffusion models,,,True,False,"Liu, Haohe and Chen, Zehua and Yuan, Yi and Mei, Xinhao and Liu, Xubo and Mandic, Danilo and Wang, Wenwu and Plumbley, Mark D",2023,,,,arXiv preprint arXiv:2301.12503
Melodia: Training-Free Music Editing Guided by Attention Probing in Diffusion Models,2511.08252v1,mousai,\cite{mousai},"{Mo\""{u}sai}: Text-to-music generation with long-context latent diffusion",,,True,False,"Schneider, Flavio and Kamal, Ojasv and Jin, Zhijing and Sch{\""o}lkopf, Bernhard",2023,,,,arXiv preprint arXiv:2301.11757
Melodia: Training-Free Music Editing Guided by Attention Probing in Diffusion Models,2511.08252v1,chen2024musicldm,\cite{chen2024musicldm},MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies,https://arxiv.org/abs/2308.01546v1,"Diffusion models have shown promising results in cross-modal generation tasks, including text-to-image and text-to-audio generation. However, generating music, as a special type of audio, presents unique challenges due to limited availability of music data and sensitive issues related to copyright and plagiarism. In this paper, to tackle these challenges, we first construct a state-of-the-art text-to-music model, MusicLDM, that adapts Stable Diffusion and AudioLDM architectures to the music domain. We achieve this by retraining the contrastive language-audio pretraining model (CLAP) and the Hifi-GAN vocoder, as components of MusicLDM, on a collection of music data samples. Then, to address the limitations of training data and to avoid plagiarism, we leverage a beat tracking model and propose two different mixup strategies for data augmentation: beat-synchronous audio mixup and beat-synchronous latent mixup, which recombine training audio directly or via a latent embeddings space, respectively. Such mixup strategies encourage the model to interpolate between musical training samples and generate new music within the convex hull of the training data, making the generated music more diverse while still staying faithful to the corresponding style. In addition to popular evaluation metrics, we design several new evaluation metrics based on CLAP score to demonstrate that our proposed MusicLDM and beat-synchronous mixup strategies improve both the quality and novelty of generated music, as well as the correspondence between input text and generated music.",True,True,"Chen, Ke and Wu, Yusong and Liu, Haohe and Nezhurina, Marianna and Berg-Kirkpatrick, Taylor and Dubnov, Shlomo",2024,,,,
Melodia: Training-Free Music Editing Guided by Attention Probing in Diffusion Models,2511.08252v1,audioldm2,\cite{audioldm2},AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining,https://arxiv.org/abs/2308.05734v3,"Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework introduces a general representation of audio, called ""language of audio"" (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate any modalities into LOA by using a GPT-2 model, and we perform self-supervised audio generation learning with a latent diffusion model conditioned on LOA. The proposed framework naturally brings advantages such as in-context learning abilities and reusable self-supervised pretrained AudioMAE and latent diffusion models. Experiments on the major benchmarks of text-to-audio, text-to-music, and text-to-speech demonstrate state-of-the-art or competitive performance against previous approaches. Our code, pretrained model, and demo are available at https://audioldm.github.io/audioldm2.",True,True,"Liu, Haohe and Yuan, Yi and Liu, Xubo and Mei, Xinhao and Kong, Qiuqiang and Tian, Qiao and Wang, Yuping and Wang, Wenwu and Wang, Yuxuan and Plumbley, Mark D",2024,,,,"IEEE/ACM Transactions on Audio, Speech, and Language Processing"
Melodia: Training-Free Music Editing Guided by Attention Probing in Diffusion Models,2511.08252v1,melodyflow,\cite{melodyflow},High Fidelity Text-Guided Music Editing via Single-Stage Flow Matching,https://arxiv.org/abs/2407.03648v2,"We introduce MelodyFlow, an efficient text-controllable high-fidelity music generation and editing model. It operates on continuous latent representations from a low frame rate 48 kHz stereo variational auto encoder codec. Based on a diffusion transformer architecture trained on a flow-matching objective the model can edit diverse high quality stereo samples of variable duration, with simple text descriptions. We adapt the ReNoise latent inversion method to flow matching and compare it with the original implementation and naive denoising diffusion implicit model (DDIM) inversion on a variety of music editing prompts. Our results indicate that our latent inversion outperforms both ReNoise and DDIM for zero-shot test-time text-guided editing on several objective metrics. Subjective evaluations exhibit a substantial improvement over previous state of the art for music editing. Code and model weights will be publicly made available. Samples are available at https://melodyflow.github.io.",True,True,"Lan, Gael Le and Shi, Bowen and Ni, Zhaoheng and Srinivasan, Sidd and Kumar, Anurag and Ellis, Brian and Kant, David and Nagaraja, Varun and Chang, Ernie and Hsu, Wei-Ning and others",2024,,,,arXiv preprint arXiv:2407.03648
Melodia: Training-Free Music Editing Guided by Attention Probing in Diffusion Models,2511.08252v1,evans2025stableaudioopen,\cite{evans2025stableaudioopen},Stable Audio Open,https://arxiv.org/abs/2407.14358v2,"Open generative models are vitally important for the community, allowing for fine-tunes and serving as baselines when presenting new models. However, most current text-to-audio models are private and not accessible for artists and researchers to build upon. Here we describe the architecture and training process of a new open-weights text-to-audio model trained with Creative Commons data. Our evaluation shows that the model's performance is competitive with the state-of-the-art across various metrics. Notably, the reported FDopenl3 results (measuring the realism of the generations) showcase its potential for high-quality stereo sound synthesis at 44.1kHz.",True,True,"Evans, Zach and Parker, Julian D and Carr, CJ and Zukowski, Zack and Taylor, Josiah and Pons, Jordi",2025,,,,
Melodia: Training-Free Music Editing Guided by Attention Probing in Diffusion Models,2511.08252v1,huang2022mulan,\cite{huang2022mulan},MuLan: A Joint Embedding of Music Audio and Natural Language,https://arxiv.org/abs/2208.12415v1,"Music tagging and content-based retrieval systems have traditionally been constructed using pre-defined ontologies covering a rigid set of music attributes or text queries. This paper presents MuLan: a first attempt at a new generation of acoustic models that link music audio directly to unconstrained natural language music descriptions. MuLan takes the form of a two-tower, joint audio-text embedding model trained using 44 million music recordings (370K hours) and weakly-associated, free-form text annotations. Through its compatibility with a wide range of music genres and text styles (including conventional music tags), the resulting audio-text representation subsumes existing ontologies while graduating to true zero-shot functionalities. We demonstrate the versatility of the MuLan embeddings with a range of experiments including transfer learning, zero-shot music tagging, language understanding in the music domain, and cross-modal retrieval applications.",True,True,"Huang, Qingqing and Jansen, Aren and Lee, Joonseok and Ganti, Ravi and Li, Judith Yue and Ellis, Daniel PW",2022,,,,arXiv preprint arXiv:2208.12415
Melodia: Training-Free Music Editing Guided by Attention Probing in Diffusion Models,2511.08252v1,wang2023audit,\cite{wang2023audit},AUDIT: Audio Editing by Following Instructions with Latent Diffusion Models,https://arxiv.org/abs/2304.00830v2,"Audio editing is applicable for various purposes, such as adding background sound effects, replacing a musical instrument, and repairing damaged audio. Recently, some diffusion-based methods achieved zero-shot audio editing by using a diffusion and denoising process conditioned on the text description of the output audio. However, these methods still have some problems: 1) they have not been trained on editing tasks and cannot ensure good editing effects; 2) they can erroneously modify audio segments that do not require editing; 3) they need a complete description of the output audio, which is not always available or necessary in practical scenarios. In this work, we propose AUDIT, an instruction-guided audio editing model based on latent diffusion models. Specifically, AUDIT has three main design features: 1) we construct triplet training data (instruction, input audio, output audio) for different audio editing tasks and train a diffusion model using instruction and input (to be edited) audio as conditions and generating output (edited) audio; 2) it can automatically learn to only modify segments that need to be edited by comparing the difference between the input and output audio; 3) it only needs edit instructions instead of full target audio descriptions as text input. AUDIT achieves state-of-the-art results in both objective and subjective metrics for several audio editing tasks (e.g., adding, dropping, replacement, inpainting, super-resolution). Demo samples are available at https://audit-demo.github.io/.",True,True,"Wang, Yuancheng and Ju, Zeqian and Tan, Xu and He, Lei and Wu, Zhizheng and Bian, Jiang and others",2023,,,,Advances in Neural Information Processing Systems
Melodia: Training-Free Music Editing Guided by Attention Probing in Diffusion Models,2511.08252v1,han2023instructme,\cite{han2023instructme},InstructME: An Instruction Guided Music Edit And Remix Framework with Latent Diffusion Models,https://arxiv.org/abs/2308.14360v3,"Music editing primarily entails the modification of instrument tracks or remixing in the whole, which offers a novel reinterpretation of the original piece through a series of operations. These music processing methods hold immense potential across various applications but demand substantial expertise. Prior methodologies, although effective for image and audio modifications, falter when directly applied to music. This is attributed to music's distinctive data nature, where such methods can inadvertently compromise the intrinsic harmony and coherence of music. In this paper, we develop InstructME, an Instruction guided Music Editing and remixing framework based on latent diffusion models. Our framework fortifies the U-Net with multi-scale aggregation in order to maintain consistency before and after editing. In addition, we introduce chord progression matrix as condition information and incorporate it in the semantic space to improve melodic harmony while editing. For accommodating extended musical pieces, InstructME employs a chunk transformer, enabling it to discern long-term temporal dependencies within music sequences. We tested InstructME in instrument-editing, remixing, and multi-round editing. Both subjective and objective evaluations indicate that our proposed method significantly surpasses preceding systems in music quality, text relevance and harmony. Demo samples are available at https://musicedit.github.io/",True,True,"Han, Bing and Dai, Junyu and Hao, Weituo and He, Xinyan and Guo, Dong and Chen, Jitong and Wang, Yuxuan and Qian, Yanmin and Song, Xuchen",2023,,,,arXiv preprint arXiv:2308.14360
Melodia: Training-Free Music Editing Guided by Attention Probing in Diffusion Models,2511.08252v1,plitsis2024investigating,\cite{plitsis2024investigating},Investigating Personalization Methods in Text to Music Generation,https://arxiv.org/abs/2309.11140v1,"In this work, we investigate the personalization of text-to-music diffusion models in a few-shot setting. Motivated by recent advances in the computer vision domain, we are the first to explore the combination of pre-trained text-to-audio diffusers with two established personalization methods. We experiment with the effect of audio-specific data augmentation on the overall system performance and assess different training strategies. For evaluation, we construct a novel dataset with prompts and music clips. We consider both embedding-based and music-specific metrics for quantitative evaluation, as well as a user study for qualitative evaluation. Our analysis shows that similarity metrics are in accordance with user preferences and that current personalization approaches tend to learn rhythmic music constructs more easily than melody. The code, dataset, and example material of this study are open to the research community.",True,True,"Plitsis, Manos and Kouzelis, Theodoros and Paraskevopoulos, Georgios and Katsouros, Vassilis and Panagakis, Yannis",2024,,,,
Melodia: Training-Free Music Editing Guided by Attention Probing in Diffusion Models,2511.08252v1,ruiz2023dreambooth,\cite{ruiz2023dreambooth},DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation,https://arxiv.org/abs/2208.12242v2,"Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for ""personalization"" of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions that do not appear in the reference images. We apply our technique to several previously-unassailable tasks, including subject recontextualization, text-guided view synthesis, and artistic rendering, all while preserving the subject's key features. We also provide a new dataset and evaluation protocol for this new task of subject-driven generation. Project page: https://dreambooth.github.io/",True,True,"Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir",2023,,,,
Melodia: Training-Free Music Editing Guided by Attention Probing in Diffusion Models,2511.08252v1,zhang2024instructmusicgen,\cite{zhang2024instructmusicgen},Instruct-MusicGen: Unlocking Text-to-Music Editing for Music Language Models via Instruction Tuning,https://arxiv.org/abs/2405.18386v3,"Recent advances in text-to-music editing, which employ text queries to modify music (e.g.\ by changing its style or adjusting instrumental components), present unique challenges and opportunities for AI-assisted music creation. Previous approaches in this domain have been constrained by the necessity to train specific editing models from scratch, which is both resource-intensive and inefficient; other research uses large language models to predict edited music, resulting in imprecise audio reconstruction. To Combine the strengths and address these limitations, we introduce Instruct-MusicGen, a novel approach that finetunes a pretrained MusicGen model to efficiently follow editing instructions such as adding, removing, or separating stems. Our approach involves a modification of the original MusicGen architecture by incorporating a text fusion module and an audio fusion module, which allow the model to process instruction texts and audio inputs concurrently and yield the desired edited music. Remarkably, Instruct-MusicGen only introduces 8% new parameters to the original MusicGen model and only trains for 5K steps, yet it achieves superior performance across all tasks compared to existing baselines, and demonstrates performance comparable to the models trained for specific tasks. This advancement not only enhances the efficiency of text-to-music editing but also broadens the applicability of music language models in dynamic music production environments.",True,True,"Zhang, Yixiao and Ikemiya, Yukara and Choi, Woosung and Murata, Naoki and Mart{\'\i}nez-Ram{\'\i}rez, Marco A and Lin, Liwei and Xia, Gus and Liao, Wei-Hsiang and Mitsufuji, Yuki and Dixon, Simon",2024,,,,arXiv preprint arXiv:2405.18386
Melodia: Training-Free Music Editing Guided by Attention Probing in Diffusion Models,2511.08252v1,magus,\cite{magus},MusicMagus: Zero-Shot Text-to-Music Editing via Diffusion Models,https://arxiv.org/abs/2402.06178v3,"Recent advances in text-to-music generation models have opened new avenues in musical creativity. However, music generation usually involves iterative refinements, and how to edit the generated music remains a significant challenge. This paper introduces a novel approach to the editing of music generated by such models, enabling the modification of specific attributes, such as genre, mood and instrument, while maintaining other aspects unchanged. Our method transforms text editing to \textit{latent space manipulation} while adding an extra constraint to enforce consistency. It seamlessly integrates with existing pretrained text-to-music diffusion models without requiring additional training. Experimental results demonstrate superior performance over both zero-shot and certain supervised baselines in style and timbre transfer evaluations. Additionally, we showcase the practical applicability of our approach in real-world music editing scenarios.",True,True,"Zhang, Yixiao and Ikemiya, Yukara and Xia, Gus and Murata, Naoki and Mart{\'\i}nez-Ram{\'\i}rez, Marco A and Liao, Wei-Hsiang and Mitsufuji, Yuki and Dixon, Simon",2024,,,,arXiv preprint arXiv:2402.06178
Melodia: Training-Free Music Editing Guided by Attention Probing in Diffusion Models,2511.08252v1,liu2024medic,\cite{liu2024medic},MEDIC: Zero-shot Music Editing with Disentangled Inversion Control,https://arxiv.org/abs/2407.13220v4,"Text-guided diffusion models revolutionize audio generation by adapting source audio to specific text prompts. However, existing zero-shot audio editing methods such as DDIM inversion accumulate errors across diffusion steps, reducing the effectiveness. Moreover, existing editing methods struggle with conducting complex non-rigid music edits while maintaining content integrity and high fidelity. To address these challenges, we propose MEDIC, a novel zero-shot music editing system based on innovative Disentangled Inversion Control (DIC) technique, which comprises Harmonized Attention Control and Disentangled Inversion. Disentangled Inversion disentangles the diffusion process into triple branches to rectify the deviated path of the source branch caused by DDIM inversion. Harmonized Attention Control unifies the mutual self-attention control and the cross-attention control with an intermediate Harmonic Branch to progressively generate the desired harmonic and melodic information in the target music. We also introduce ZoME-Bench, a comprehensive music editing benchmark with 1,100 samples covering ten distinct editing categories. ZoME-Bench facilitates both zero-shot and instruction-based music editing tasks. Our method outperforms state-of-the-art inversion techniques in editing fidelity and content preservation. The code and benchmark will be released. Audio samples are available at https://medic-edit.github.io/.",True,True,"Liu, Huadai and Wang, Jialei and Li, Xiangtai and Huang, Rongjie and Liu, Yang and Xu, Jiayang and Zhao, Zhou",2024,,,,arXiv preprint arXiv:2407.13220
Melodia: Training-Free Music Editing Guided by Attention Probing in Diffusion Models,2511.08252v1,manor2024ddpm,\cite{manor2024ddpm},Zero-shot unsupervised and text-based audio editing using DDPM inversion,,,True,False,"Manor, Hila and Michaeli, Tomer",2024,,,,arXiv preprint arXiv:2402.10009
Melodia: Training-Free Music Editing Guided by Attention Probing in Diffusion Models,2511.08252v1,meng2021sdedit,\cite{meng2021sdedit},Sdedit: Guided image synthesis and editing with stochastic differential equations,,,True,False,"Meng, Chenlin and He, Yutong and Song, Yang and Song, Jiaming and Wu, Jiajun and Zhu, Jun-Yan and Ermon, Stefano",2021,,,,arXiv preprint arXiv:2108.01073
Top2Ground: A Height-Aware Dual Conditioning Diffusion Model for Robust Aerial-to-Ground View Generation,2511.08258v1,ho2020denoising,\cite{ho2020denoising},Denoising Diffusion Probabilistic Models,https://arxiv.org/abs/2006.11239v2,"We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion",True,True,"Ho, Jonathan and Jain, Ajay and Abbeel, Pieter",2020,,,,
Top2Ground: A Height-Aware Dual Conditioning Diffusion Model for Robust Aerial-to-Ground View Generation,2511.08258v1,song2021scorebased,\cite{song2021scorebased},Score-Based Generative Modeling through Stochastic Differential Equations,https://arxiv.org/abs/2011.13456v2,"Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.",True,True,Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole,2021,,,,
Top2Ground: A Height-Aware Dual Conditioning Diffusion Model for Robust Aerial-to-Ground View Generation,2511.08258v1,dhariwal2021diffusion,\cite{dhariwal2021diffusion},Diffusion Models Beat GANs on Image Classification,https://arxiv.org/abs/2307.08702v1,"While many unsupervised learning models focus on one family of tasks, either generative or discriminative, we explore the possibility of a unified representation learner: a model which uses a single pre-training stage to address both families of tasks simultaneously. We identify diffusion models as a prime candidate. Diffusion models have risen to prominence as a state-of-the-art method for image generation, denoising, inpainting, super-resolution, manipulation, etc. Such models involve training a U-Net to iteratively predict and remove noise, and the resulting model can synthesize high fidelity, diverse, novel images. The U-Net architecture, as a convolution-based architecture, generates a diverse set of feature representations in the form of intermediate feature maps. We present our findings that these embeddings are useful beyond the noise prediction task, as they contain discriminative information and can also be leveraged for classification. We explore optimal methods for extracting and using these embeddings for classification tasks, demonstrating promising results on the ImageNet classification task. We find that with careful feature selection and pooling, diffusion models outperform comparable generative-discriminative methods such as BigBiGAN for classification tasks. We investigate diffusion models in the transfer learning regime, examining their performance on several fine-grained visual classification datasets. We compare these embeddings to those generated by competing architectures and pre-trainings for classification tasks.",True,True,"Dhariwal, Prafulla and Nichol, Alexander",2021,,,,
Top2Ground: A Height-Aware Dual Conditioning Diffusion Model for Robust Aerial-to-Ground View Generation,2511.08258v1,rombach2022high,\cite{rombach2022high},High-resolution image synthesis with latent diffusion models,,,True,False,"Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\""o}rn",2022,,,,
Top2Ground: A Height-Aware Dual Conditioning Diffusion Model for Robust Aerial-to-Ground View Generation,2511.08258v1,nichol2021glide,\cite{nichol2021glide},GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models,https://arxiv.org/abs/2112.10741v3,"Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im.",True,True,"Nichol, Alex and Dhariwal, Prafulla and Ramesh, Aditya and Shyam, Pranav and Mishkin, Pamela and McGrew, Bob and Sutskever, Ilya and Chen, Mark",2021,,,,arXiv preprint arXiv:2112.10741
Top2Ground: A Height-Aware Dual Conditioning Diffusion Model for Robust Aerial-to-Ground View Generation,2511.08258v1,saharia2022photorealistic,\cite{saharia2022photorealistic},Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding,https://arxiv.org/abs/2205.11487v1,"We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.",True,True,"Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily L and Ghasemipour, Kamyar and Gontijo Lopes, Raphael and Karagol Ayan, Burcu and Salimans, Tim and others",2022,,,,
Top2Ground: A Height-Aware Dual Conditioning Diffusion Model for Robust Aerial-to-Ground View Generation,2511.08258v1,kim2022diffusionclip,\cite{kim2022diffusionclip},DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation,https://arxiv.org/abs/2110.02711v6,"Recently, GAN inversion methods combined with Contrastive Language-Image Pretraining (CLIP) enables zero-shot image manipulation guided by text prompts. However, their applications to diverse real images are still difficult due to the limited GAN inversion capability. Specifically, these approaches often have difficulties in reconstructing images with novel poses, views, and highly variable contents compared to the training data, altering object identity, or producing unwanted image artifacts. To mitigate these problems and enable faithful manipulation of real images, we propose a novel method, dubbed DiffusionCLIP, that performs text-driven image manipulation using diffusion models. Based on full inversion capability and high-quality image generation power of recent diffusion models, our method performs zero-shot image manipulation successfully even between unseen domains and takes another step towards general application by manipulating images from a widely varying ImageNet dataset. Furthermore, we propose a novel noise combination method that allows straightforward multi-attribute manipulation. Extensive experiments and human evaluation confirmed robust and superior manipulation performance of our methods compared to the existing baselines. Code is available at https://github.com/gwang-kim/DiffusionCLIP.git.",True,True,"Kim, Gwanghyun and Kwon, Taesung and Ye, Jong Chul",2022,,,,
Top2Ground: A Height-Aware Dual Conditioning Diffusion Model for Robust Aerial-to-Ground View Generation,2511.08258v1,instructpix2pix,\cite{instructpix2pix},InstructPix2Pix: Learning to Follow Image Editing Instructions,https://arxiv.org/abs/2211.09800v2,"We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models -- a language model (GPT-3) and a text-to-image model (Stable Diffusion) -- to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions.",True,True,"Brooks, Tim and Holynski, Aleksander and Efros, Alexei A",2023,,,,
Top2Ground: A Height-Aware Dual Conditioning Diffusion Model for Robust Aerial-to-Ground View Generation,2511.08258v1,controlnet,\cite{controlnet},Adding Conditional Control to Text-to-Image Diffusion Models,https://arxiv.org/abs/2302.05543v3,"We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with ""zero convolutions"" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, eg, edges, depth, segmentation, human pose, etc, with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small (<50k) and large (>1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models.",True,True,"Zhang, Lvmin and Rao, Anyi and Agrawala, Maneesh",2023,,,,
Top2Ground: A Height-Aware Dual Conditioning Diffusion Model for Robust Aerial-to-Ground View Generation,2511.08258v1,treedfusion,\cite{treedfusion},Tree-D Fusion: Simulation-Ready Tree Dataset from Single Images with Diffusion Priors,https://arxiv.org/abs/2407.10330v1,"We introduce Tree D-fusion, featuring the first collection of 600,000 environmentally aware, 3D simulation-ready tree models generated through Diffusion priors. Each reconstructed 3D tree model corresponds to an image from Google's Auto Arborist Dataset, comprising street view images and associated genus labels of trees across North America. Our method distills the scores of two tree-adapted diffusion models by utilizing text prompts to specify a tree genus, thus facilitating shape reconstruction. This process involves reconstructing a 3D tree envelope filled with point markers, which are subsequently utilized to estimate the tree's branching structure using the space colonization algorithm conditioned on a specified genus.",True,True,"Lee, Jae Joong and Li, Bosheng and Beery, Sara and Huang, Jonathan and Fei, Songlin and Yeh, Raymond A and Benes, Bedrich",2025,,,,
Top2Ground: A Height-Aware Dual Conditioning Diffusion Model for Robust Aerial-to-Ground View Generation,2511.08258v1,zhai2017predicting,\cite{zhai2017predicting},Predicting Ground-Level Scene Layout from Aerial Imagery,https://arxiv.org/abs/1612.02709v1,"We introduce a novel strategy for learning to extract semantically meaningful features from aerial imagery. Instead of manually labeling the aerial imagery, we propose to predict (noisy) semantic features automatically extracted from co-located ground imagery. Our network architecture takes an aerial image as input, extracts features using a convolutional neural network, and then applies an adaptive transformation to map these features into the ground-level perspective. We use an end-to-end learning approach to minimize the difference between the semantic segmentation extracted directly from the ground image and the semantic segmentation predicted solely based on the aerial image. We show that a model learned using this strategy, with no additional training, is already capable of rough semantic labeling of aerial imagery. Furthermore, we demonstrate that by finetuning this model we can achieve more accurate semantic segmentation than two baseline initialization strategies. We use our network to address the task of estimating the geolocation and geoorientation of a ground image. Finally, we show how features extracted from an aerial image can be used to hallucinate a plausible ground-level panorama.",True,True,"Zhai, Menghua and Bessinger, Zachary and Workman, Scott and Jacobs, Nathan",2017,,,,
Top2Ground: A Height-Aware Dual Conditioning Diffusion Model for Robust Aerial-to-Ground View Generation,2511.08258v1,li2021sat2vid,\cite{li2021sat2vid},Sat2Vid: Street-view Panoramic Video Synthesis from a Single Satellite Image,https://arxiv.org/abs/2012.06628v3,"We present a novel method for synthesizing both temporally and geometrically consistent street-view panoramic video from a single satellite image and camera trajectory. Existing cross-view synthesis approaches focus on images, while video synthesis in such a case has not yet received enough attention. For geometrical and temporal consistency, our approach explicitly creates a 3D point cloud representation of the scene and maintains dense 3D-2D correspondences across frames that reflect the geometric scene configuration inferred from the satellite view. As for synthesis in the 3D space, we implement a cascaded network architecture with two hourglass modules to generate point-wise coarse and fine features from semantics and per-class latent vectors, followed by projection to frames and an upsampling module to obtain the final realistic video. By leveraging computed correspondences, the produced street-view video frames adhere to the 3D geometric scene structure and maintain temporal consistency. Qualitative and quantitative experiments demonstrate superior results compared to other state-of-the-art synthesis approaches that either lack temporal consistency or realistic appearance. To the best of our knowledge, our work is the first one to synthesize cross-view images to video.",True,True,"Li, Zuoyue and Li, Zhenqiang and Cui, Zhaopeng and Qin, Rongjun and Pollefeys, Marc and Oswald, Martin R",2021,,,,
Top2Ground: A Height-Aware Dual Conditioning Diffusion Model for Robust Aerial-to-Ground View Generation,2511.08258v1,regmi2018cross,\cite{regmi2018cross},Cross-View Image Synthesis using Conditional GANs,https://arxiv.org/abs/1803.03396v2,"Learning to generate natural scenes has always been a challenging task in computer vision. It is even more painstaking when the generation is conditioned on images with drastically different views. This is mainly because understanding, corresponding, and transforming appearance and semantic information across the views is not trivial. In this paper, we attempt to solve the novel problem of cross-view image synthesis, aerial to street-view and vice versa, using conditional generative adversarial networks (cGAN). Two new architectures called Crossview Fork (X-Fork) and Crossview Sequential (X-Seq) are proposed to generate scenes with resolutions of 64x64 and 256x256 pixels. X-Fork architecture has a single discriminator and a single generator. The generator hallucinates both the image and its semantic segmentation in the target view. X-Seq architecture utilizes two cGANs. The first one generates the target image which is subsequently fed to the second cGAN for generating its corresponding semantic segmentation map. The feedback from the second cGAN helps the first cGAN generate sharper images. Both of our proposed architectures learn to generate natural images as well as their semantic segmentation maps. The proposed methods show that they are able to capture and maintain the true semantics of objects in source and target views better than the traditional image-to-image translation method which considers only the visual appearance of the scene. Extensive qualitative and quantitative evaluations support the effectiveness of our frameworks, compared to two state of the art methods, for natural scene generation across drastically different views.",True,True,"Regmi, Krishna and Borji, Ali",2018,,,,
Top2Ground: A Height-Aware Dual Conditioning Diffusion Model for Robust Aerial-to-Ground View Generation,2511.08258v1,tang2019multi,\cite{tang2019multi},Multi-channel attention selection gan with cascaded semantic guidance for cross-view image translation,,,True,False,"Tang, Hao and Xu, Dan and Sebe, Nicu and Wang, Yanzhi and Corso, Jason J and Yan, Yan",2019,,,,
Top2Ground: A Height-Aware Dual Conditioning Diffusion Model for Robust Aerial-to-Ground View Generation,2511.08258v1,lu2020geometry,\cite{lu2020geometry},Geometry-aware satellite-to-ground image synthesis for urban areas,,,True,False,"Lu, Xiaohu and Li, Zuoyue and Cui, Zhaopeng and Oswald, Martin R and Pollefeys, Marc and Qin, Rongjun",2020,,,,
Top2Ground: A Height-Aware Dual Conditioning Diffusion Model for Robust Aerial-to-Ground View Generation,2511.08258v1,shi2022geometry,\cite{shi2022geometry},Geometry-Guided Street-View Panorama Synthesis from Satellite Imagery,https://arxiv.org/abs/2103.01623v4,"This paper presents a new approach for synthesizing a novel street-view panorama given an overhead satellite image. Taking a small satellite image patch as input, our method generates a Google's omnidirectional street-view type panorama, as if it is captured from the same geographical location as the center of the satellite patch. Existing works tackle this task as an image generation problem which adopts generative adversarial networks to implicitly learn the cross-view transformations, while ignoring the domain relevance. In this paper, we propose to explicitly establish the geometric correspondences between the two-view images so as to facilitate the cross-view transformation learning. Specifically, we observe that when a 3D point in the real world is visible in both views, there is a deterministic mapping between the projected points in the two-view images given the height information of this 3D point. Motivated by this, we develop a novel Satellite to Street-view image Projection (S2SP) module which explicitly establishes such geometric correspondences and projects the satellite images to the street viewpoint. With these projected satellite images as network input, we next employ a generator to synthesize realistic street-view panoramas that are geometrically consistent with the satellite images. Our S2SP module is differentiable and the whole framework is trained in an end-to-end manner. Extensive experimental results on two cross-view benchmark datasets demonstrate that our method generates images that better respect the scene geometry than existing approaches.",True,True,"Shi, Yujiao and Campbell, Dylan and Yu, Xin and Li, Hongdong",2022,,,,
Top2Ground: A Height-Aware Dual Conditioning Diffusion Model for Robust Aerial-to-Ground View Generation,2511.08258v1,sat2density,\cite{sat2density},Sat2Density: Faithful Density Learning from Satellite-Ground Image Pairs,https://arxiv.org/abs/2303.14672v2,"This paper aims to develop an accurate 3D geometry representation of satellite images using satellite-ground image pairs. Our focus is on the challenging problem of 3D-aware ground-views synthesis from a satellite image. We draw inspiration from the density field representation used in volumetric neural rendering and propose a new approach, called Sat2Density. Our method utilizes the properties of ground-view panoramas for the sky and non-sky regions to learn faithful density fields of 3D scenes in a geometric perspective. Unlike other methods that require extra depth information during training, our Sat2Density can automatically learn accurate and faithful 3D geometry via density representation without depth supervision. This advancement significantly improves the ground-view panorama synthesis task. Additionally, our study provides a new geometric perspective to understand the relationship between satellite and ground-view images in 3D space.",True,True,"Qian, Ming and Xiong, Jincheng and Xia, Gui-Song and Xue, Nan",2023,,,,
Top2Ground: A Height-Aware Dual Conditioning Diffusion Model for Robust Aerial-to-Ground View Generation,2511.08258v1,crossviewdiff,\cite{crossviewdiff},CrossViewDiff: A Cross-View Diffusion Model for Satellite-to-Street View Synthesis,https://arxiv.org/abs/2408.14765v1,"Satellite-to-street view synthesis aims at generating a realistic street-view image from its corresponding satellite-view image. Although stable diffusion models have exhibit remarkable performance in a variety of image generation applications, their reliance on similar-view inputs to control the generated structure or texture restricts their application to the challenging cross-view synthesis task. In this work, we propose CrossViewDiff, a cross-view diffusion model for satellite-to-street view synthesis. To address the challenges posed by the large discrepancy across views, we design the satellite scene structure estimation and cross-view texture mapping modules to construct the structural and textural controls for street-view image synthesis. We further design a cross-view control guided denoising process that incorporates the above controls via an enhanced cross-view attention module. To achieve a more comprehensive evaluation of the synthesis results, we additionally design a GPT-based scoring method as a supplement to standard evaluation metrics. We also explore the effect of different data sources (e.g., text, maps, building heights, and multi-temporal satellite imagery) on this task. Results on three public cross-view datasets show that CrossViewDiff outperforms current state-of-the-art on both standard and GPT-based evaluation metrics, generating high-quality street-view panoramas with more realistic structures and textures across rural, suburban, and urban scenes. The code and models of this work will be released at https://opendatalab.github.io/CrossViewDiff/.",True,True,"Chen, Yuankun and Rong, Dazhong and Li, Yi",2024,,,,
Revisiting Network Traffic Analysis: Compatible network flows for ML models,2511.08345v1,Thakkar2021,\cite{Thakkar2021},"A Review on Machine Learning and Deep Learning Perspectives of
              {IDS} for {IoT}: Recent Updates, Security Issues, and Challenges",,,True,False,"Thakkar, Ankit and Lohiya, Ritika",,,,10.1007/s11831-020-09496-0,Archives of Computational Methods in Engineering
Revisiting Network Traffic Analysis: Compatible network flows for ML models,2511.08345v1,kenyon_are_2020,\cite{kenyon_are_2020},Are public intrusion datasets fit for purpose characterising the state of the art in intrusion event datasets,,,True,False,"Kenyon, A. and Deka, L. and Elizondo, D.",2020,,,10.1016/j.cose.2020.102022,Computers and Security
Revisiting Network Traffic Analysis: Compatible network flows for ML models,2511.08345v1,RING2019,\cite{RING2019},A Survey of Network-based Intrusion Detection Data Sets,https://arxiv.org/abs/1903.02460v2,"Labeled data sets are necessary to train and evaluate anomaly-based network intrusion detection systems. This work provides a focused literature survey of data sets for network-based intrusion detection and describes the underlying packet- and flow-based network data in detail. The paper identifies 15 different properties to assess the suitability of individual data sets for specific evaluation scenarios. These properties cover a wide range of criteria and are grouped into five categories such as data volume or recording environment for offering a structured search. Based on these properties, a comprehensive overview of existing data sets is given. This overview also highlights the peculiarities of each data set. Furthermore, this work briefly touches upon other sources for network-based data such as traffic generators and traffic repositories. Finally, we discuss our observations and provide some recommendations for the use and creation of network-based data sets.",True,True,Markus Ring and Sarah Wunderlich and Deniz Scheuring and Dieter Landes and Andreas Hotho,2019,,,10.1016/j.cose.2019.06.005,Computers \& Security
Revisiting Network Traffic Analysis: Compatible network flows for ML models,2511.08345v1,Adeleke2022,\cite{Adeleke2022},Network Traffic Generation: A Survey and Methodology,,,True,False,"Adeleke, Oluwamayowa Ade and Bastin, Nicholas and Gurkan, Deniz",2022,,,10.1145/3488375,ACM Comput. Surv.
Revisiting Network Traffic Analysis: Compatible network flows for ML models,2511.08345v1,rodriguez_evaluation_2022,\cite{rodriguez_evaluation_2022},Evaluation of {Machine} {Learning} {Techniques} for {Traffic} {Flow}-{Based} {Intrusion} {Detection},,,True,False,"Rodríguez, María and Alesanco, Alvaro and Mehavilla, Lorena and García, Jose",2022,,,10.3390/s22239326,Sensors
Revisiting Network Traffic Analysis: Compatible network flows for ML models,2511.08345v1,GUERRA2022,\cite{GUERRA2022},Datasets are not enough: Challenges in labeling network traffic,,,True,False,Jorge Luis Guerra and Carlos Catania and Eduardo Veas,2022,,,10.1016/j.cose.2022.102810,Computers \& Security
Revisiting Network Traffic Analysis: Compatible network flows for ML models,2511.08345v1,sarhan_towards_2022,\cite{sarhan_towards_2022},Towards a Standard Feature Set for Network Intrusion Detection System Datasets,,,True,False,"Sarhan, M. and Layeghy, S. and Portmann, M.",2022,,,10.1007/s11036-021-01843-0,Mobile Networks and Applications
Revisiting Network Traffic Analysis: Compatible network flows for ML models,2511.08345v1,Silva2024,\cite{Silva2024},Efficient Network Traffic Feature Sets for IoT Intrusion Detection,https://arxiv.org/abs/2406.08042v1,"The use of Machine Learning (ML) models in cybersecurity solutions requires high-quality data that is stripped of redundant, missing, and noisy information. By selecting the most relevant features, data integrity and model efficiency can be significantly improved. This work evaluates the feature sets provided by a combination of different feature selection methods, namely Information Gain, Chi-Squared Test, Recursive Feature Elimination, Mean Absolute Deviation, and Dispersion Ratio, in multiple IoT network datasets. The influence of the smaller feature sets on both the classification performance and the training time of ML models is compared, with the aim of increasing the computational efficiency of IoT intrusion detection. Overall, the most impactful features of each dataset were identified, and the ML models obtained higher computational efficiency while preserving a good generalization, showing little to no difference between the sets.",True,True,"Silva, Miguel
and Vitorino, Jo{\~a}o
and Maia, Eva
and Pra{\c{c}}a, Isabel",2025,,,10.1007/978-3-031-76459-2_1,
Revisiting Network Traffic Analysis: Compatible network flows for ML models,2511.08345v1,Vitorino2024,\cite{Vitorino2024},Reliable feature selection for adversarially robust cyber-attack detection,,,True,False,"Vitorino, Jo{\~a}o and Silva, Miguel and Maia, Eva and Pra{\c c}a, Isabel",,,,10.1007/s12243-024-01047-z,Annals of Telecommunications
MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,2511.10376v1,Conceptgraphs,\cite{Conceptgraphs},ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning,https://arxiv.org/abs/2309.16650v1,"For robots to perform a wide variety of tasks, they require a 3D representation of the world that is semantically rich, yet compact and efficient for task-driven perception and planning. Recent approaches have attempted to leverage features from large vision-language models to encode semantics in 3D representations. However, these approaches tend to produce maps with per-point feature vectors, which do not scale well in larger environments, nor do they contain semantic spatial relationships between entities in the environment, which are useful for downstream planning. In this work, we propose ConceptGraphs, an open-vocabulary graph-structured representation for 3D scenes. ConceptGraphs is built by leveraging 2D foundation models and fusing their output to 3D by multi-view association. The resulting representations generalize to novel semantic classes, without the need to collect large 3D datasets or finetune models. We demonstrate the utility of this representation through a number of downstream planning tasks that are specified through abstract (language) prompts and require complex reasoning over spatial and semantic concepts. (Project page: https://concept-graphs.github.io/ Explainer video: https://youtu.be/mRhNkQwRYnc )",True,True,"Gu, Qiao and Kuwajerwala, Ali and Morin, Sacha and Jatavallabhula, Krishna Murthy and Sen, Bipasha and Agarwal, Aditya and Rivera, Corban and Paul, William and Ellis, Kirsty and Chellappa, Rama and others",2024,,,,
MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,2511.10376v1,related_14,\cite{related_14},3-D scene graph: A sparse and semantic representation of physical environments for intelligent agents,,,True,False,"Kim, Ue-Hwan and Park, Jin-Man and Song, Taek-Jin and Kim, Jong-Hwan",2019,,,,IEEE transactions on cybernetics
MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,2511.10376v1,related_33,\cite{related_33},"3D Dynamic Scene Graphs: Actionable Spatial Perception with Places, Objects, and Humans",https://arxiv.org/abs/2002.06289v2,"We present a unified representation for actionable spatial perception: 3D Dynamic Scene Graphs. Scene graphs are directed graphs where nodes represent entities in the scene (e.g. objects, walls, rooms), and edges represent relations (e.g. inclusion, adjacency) among nodes. Dynamic scene graphs (DSGs) extend this notion to represent dynamic scenes with moving agents (e.g. humans, robots), and to include actionable information that supports planning and decision-making (e.g. spatio-temporal relations, topology at different levels of abstraction). Our second contribution is to provide the first fully automatic Spatial PerceptIon eNgine(SPIN) to build a DSG from visual-inertial data. We integrate state-of-the-art techniques for object and human detection and pose estimation, and we describe how to robustly infer object, robot, and human nodes in crowded scenes. To the best of our knowledge, this is the first paper that reconciles visual-inertial SLAM and dense human mesh tracking. Moreover, we provide algorithms to obtain hierarchical representations of indoor environments (e.g. places, structures, rooms) and their relations. Our third contribution is to demonstrate the proposed spatial perception engine in a photo-realistic Unity-based simulator, where we assess its robustness and expressiveness. Finally, we discuss the implications of our proposal on modern robotics applications. 3D Dynamic Scene Graphs can have a profound impact on planning and decision-making, human-robot interaction, long-term autonomy, and scene prediction. A video abstract is available at https://youtu.be/SWbofjhyPzI",True,True,"Rosinol, Antoni and Gupta, Arjun and Abate, Marcus and Shi, Jingnan and Carlone, Luca",2020,,,,arXiv preprint arXiv:2002.06289
MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,2511.10376v1,related_32,\cite{related_32},SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning,https://arxiv.org/abs/2307.06135v2,"Large language models (LLMs) have demonstrated impressive results in developing generalist planning agents for diverse tasks. However, grounding these plans in expansive, multi-floor, and multi-room environments presents a significant challenge for robotics. We introduce SayPlan, a scalable approach to LLM-based, large-scale task planning for robotics using 3D scene graph (3DSG) representations. To ensure the scalability of our approach, we: (1) exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a 'semantic search' for task-relevant subgraphs from a smaller, collapsed representation of the full graph; (2) reduce the planning horizon for the LLM by integrating a classical path planner and (3) introduce an 'iterative replanning' pipeline that refines the initial plan using feedback from a scene graph simulator, correcting infeasible actions and avoiding planning failures. We evaluate our approach on two large-scale environments spanning up to 3 floors and 36 rooms with 140 assets and objects and show that our approach is capable of grounding large-scale, long-horizon task plans from abstract, and natural language instruction for a mobile manipulator robot to execute. We provide real robot video demonstrations on our project page https://sayplan.github.io.",True,True,"Rana, Krishan and Haviland, Jesse and Garg, Sourav and Abou-Chakra, Jad and Reid, Ian and Suenderhauf, Niko",2023,,,,arXiv preprint arXiv:2307.06135
MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,2511.10376v1,related_05,\cite{related_05},Context-aware entity grounding with open-vocabulary 3d scene graphs,,,True,False,"Chang, Haonan and Boyalakuntla, Kowndinya and Lu, Shiyang and Cai, Siwei and Jing, Eric and Keskar, Shreesh and Geng, Shijie and Abbas, Adeeb and Zhou, Lifeng and Bekris, Kostas and others",2023,,,,arXiv preprint arXiv:2309.15940
MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,2511.10376v1,hu2025imaginative,\cite{hu2025imaginative},Imaginative World Modeling with Scene Graphs for Embodied Agent Navigation,,,True,False,"Hu, Yue and Wu, Junzhe and Xu, Ruihan and Liu, Hang and Xi, Avery and Liu, Henry X and Vasudevan, Ram and Ghaffari, Maani",2025,,,,arXiv preprint arXiv:2508.06990
MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,2511.10376v1,luo2025learning,\cite{luo2025learning},Learning Bird’s Eye View scene graph and knowledge-inspired policy for embodied visual navigation,,,True,False,"Luo, Jian and Zhang, Jian and Yang, Jie and Huang, Siwei and Cai, Bo",2025,,,,Knowledge-Based Systems
MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,2511.10376v1,sg-nav,\cite{sg-nav},SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation,https://arxiv.org/abs/2410.08189v1,"In this paper, we propose a new framework for zero-shot object navigation. Existing zero-shot object navigation methods prompt LLM with the text of spatially closed objects, which lacks enough scene context for in-depth reasoning. To better preserve the information of environment and fully exploit the reasoning ability of LLM, we propose to represent the observed scene with 3D scene graph. The scene graph encodes the relationships between objects, groups and rooms with a LLM-friendly structure, for which we design a hierarchical chain-of-thought prompt to help LLM reason the goal location according to scene context by traversing the nodes and edges. Moreover, benefit from the scene graph representation, we further design a re-perception mechanism to empower the object navigation framework with the ability to correct perception error. We conduct extensive experiments on MP3D, HM3D and RoboTHOR environments, where SG-Nav surpasses previous state-of-the-art zero-shot methods by more than 10% SR on all benchmarks, while the decision process is explainable. To the best of our knowledge, SG-Nav is the first zero-shot method that achieves even higher performance than supervised object navigation methods on the challenging MP3D benchmark.",True,True,"Yin, Hang and Xu, Xiuwei and Wu, Zhenyu and Zhou, Jie and Lu, Jiwen",2024,,,,NeurIPS
MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,2511.10376v1,hong2025general,\cite{hong2025general},General Scene Adaptation for Vision-and-Language Navigation,,,True,False,"Hong, Haodong and Qiao, Yanyuan and Wang, Sen and Liu, Jiajun and Wu, Qi",2025,,,,arXiv preprint arXiv:2501.17403
MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,2511.10376v1,rlnav1,\cite{rlnav1},Scaling Open-Vocabulary Object Detection,https://arxiv.org/abs/2306.09683v3,"Open-vocabulary object detection has benefited greatly from pretrained vision-language models, but is still limited by the amount of available detection training data. While detection training data can be expanded by using Web image-text pairs as weak supervision, this has not been done at scales comparable to image-level pretraining. Here, we scale up detection data with self-training, which uses an existing detector to generate pseudo-box annotations on image-text pairs. Major challenges in scaling self-training are the choice of label space, pseudo-annotation filtering, and training efficiency. We present the OWLv2 model and OWL-ST self-training recipe, which address these challenges. OWLv2 surpasses the performance of previous state-of-the-art open-vocabulary detectors already at comparable training scales (~10M examples). However, with OWL-ST, we can scale to over 1B examples, yielding further large improvement: With an L/14 architecture, OWL-ST improves AP on LVIS rare classes, for which the model has seen no human box annotations, from 31.2% to 44.6% (43% relative improvement). OWL-ST unlocks Web-scale training for open-world localization, similar to what has been seen for image classification and language modelling.",True,True,"Minderer, Matthias and Gritsenko, Alexey and Houlsby, Neil",2023,,,,NeurIPS
MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,2511.10376v1,rlnav2,\cite{rlnav2},Pirlnav: Pretraining with imitation and rl finetuning for objectnav,,,True,False,"Ramrakhya, Ram and Batra, Dhruv and Wijmans, Erik and Das, Abhishek",2023,,,,
MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,2511.10376v1,related_11,\cite{related_11},Curious representation learning for embodied intelligence,,,True,False,"Du, Yilun and Gan, Chuang and Isola, Phillip",2021,,,,
MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,2511.10376v1,related_18,\cite{related_18},Renderable neural radiance map for visual navigation,,,True,False,"Kwon, Obin and Park, Jeongho and Oh, Songhwai",2023,,,,
MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,2511.10376v1,related_31,\cite{related_31},Poni: Potential functions for objectgoal navigation with interaction-free learning,,,True,False,"Ramakrishnan, Santhosh Kumar and Chaplot, Devendra Singh and Al-Halah, Ziad and Malik, Jitendra and Grauman, Kristen",2022,,,,
MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,2511.10376v1,related_41,\cite{related_41},DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames,https://arxiv.org/abs/1911.00357v2,"We present Decentralized Distributed Proximal Policy Optimization (DD-PPO), a method for distributed reinforcement learning in resource-intensive simulated environments. DD-PPO is distributed (uses multiple machines), decentralized (lacks a centralized server), and synchronous (no computation is ever stale), making it conceptually simple and easy to implement. In our experiments on training virtual robots to navigate in Habitat-Sim, DD-PPO exhibits near-linear scaling -- achieving a speedup of 107x on 128 GPUs over a serial implementation. We leverage this scaling to train an agent for 2.5 Billion steps of experience (the equivalent of 80 years of human experience) -- over 6 months of GPU-time training in under 3 days of wall-clock time with 64 GPUs.
  This massive-scale training not only sets the state of art on Habitat Autonomous Navigation Challenge 2019, but essentially solves the task --near-perfect autonomous navigation in an unseen environment without access to a map, directly from an RGB-D camera and a GPS+Compass sensor. Fortuitously, error vs computation exhibits a power-law-like distribution; thus, 90% of peak performance is obtained relatively early (at 100 million steps) and relatively cheaply (under 1 day with 8 GPUs). Finally, we show that the scene understanding and navigation policies learned can be transferred to other navigation tasks -- the analog of ImageNet pre-training + task-specific fine-tuning for embodied AI. Our model outperforms ImageNet pre-trained CNNs on these transfer tasks and can serve as a universal resource (all models and code are publicly available).",True,True,"Wijmans, Erik and Kadian, Abhishek and Morcos, Ari and Lee, Stefan and Essa, Irfan and Parikh, Devi and Savva, Manolis and Batra, Dhruv",2019,,,,arXiv preprint arXiv:1911.00357
MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,2511.10376v1,related_12,\cite{related_12},Cows on pasture: Baselines and benchmarks for language-driven zero-shot object navigation,,,True,False,"Gadre, Samir Yitzhak and Wortsman, Mitchell and Ilharco, Gabriel and Schmidt, Ludwig and Song, Shuran",2023,,,,
MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,2511.10376v1,related_28,\cite{related_28},Learning Transferable Visual Models From Natural Language Supervision,https://arxiv.org/abs/2103.00020v1,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",True,True,"Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",2021,,,,
MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,2511.10376v1,related_49,\cite{related_49},Esc: Exploration with soft commonsense constraints for zero-shot object navigation,,,True,False,"Zhou, Kaiwen and Zheng, Kaizhi and Pryor, Connor and Shen, Yilin and Jin, Hongxia and Getoor, Lise and Wang, Xin Eric",2023,,,,
MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,2511.10376v1,related_08,\cite{related_08},Open-vocabulary Queryable Scene Representations for Real World Planning,https://arxiv.org/abs/2209.09874v2,"Large language models (LLMs) have unlocked new capabilities of task planning from human instructions. However, prior attempts to apply LLMs to real-world robotic tasks are limited by the lack of grounding in the surrounding scene. In this paper, we develop NLMap, an open-vocabulary and queryable scene representation to address this problem. NLMap serves as a framework to gather and integrate contextual information into LLM planners, allowing them to see and query available objects in the scene before generating a context-conditioned plan. NLMap first establishes a natural language queryable scene representation with Visual Language models (VLMs). An LLM based object proposal module parses instructions and proposes involved objects to query the scene representation for object availability and location. An LLM planner then plans with such information about the scene. NLMap allows robots to operate without a fixed list of objects nor executable options, enabling real robot operation unachievable by previous methods. Project website: https://nlmap-saycan.github.io",True,True,"Chen, Boyuan and Xia, Fei and Ichter, Brian and Rao, Kanishka and Gopalakrishnan, Keerthana and Ryoo, Michael S and Stone, Austin and Kappler, Daniel",2022,,,,arXiv preprint arXiv:2209.09874
MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,2511.10376v1,vlfm,\cite{vlfm},VLFM: Vision-Language Frontier Maps for Zero-Shot Semantic Navigation,https://arxiv.org/abs/2312.03275v1,"Understanding how humans leverage semantic knowledge to navigate unfamiliar environments and decide where to explore next is pivotal for developing robots capable of human-like search behaviors. We introduce a zero-shot navigation approach, Vision-Language Frontier Maps (VLFM), which is inspired by human reasoning and designed to navigate towards unseen semantic objects in novel environments. VLFM builds occupancy maps from depth observations to identify frontiers, and leverages RGB observations and a pre-trained vision-language model to generate a language-grounded value map. VLFM then uses this map to identify the most promising frontier to explore for finding an instance of a given target object category. We evaluate VLFM in photo-realistic environments from the Gibson, Habitat-Matterport 3D (HM3D), and Matterport 3D (MP3D) datasets within the Habitat simulator. Remarkably, VLFM achieves state-of-the-art results on all three datasets as measured by success weighted by path length (SPL) for the Object Goal Navigation task. Furthermore, we show that VLFM's zero-shot nature enables it to be readily deployed on real-world robots such as the Boston Dynamics Spot mobile manipulation platform. We deploy VLFM on Spot and demonstrate its capability to efficiently navigate to target objects within an office building in the real world, without any prior knowledge of the environment. The accomplishments of VLFM underscore the promising potential of vision-language models in advancing the field of semantic navigation. Videos of real-world deployment can be viewed at naoki.io/vlfm.",True,True,"Yokoyama, Naoki and Ha, Sehoon and Batra, Dhruv and Wang, Jiuguang and Bucher, Bernadette",2024,,,,
MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,2511.10376v1,unigoal,\cite{unigoal},UniGoal: Towards Universal Zero-shot Goal-oriented Navigation,https://arxiv.org/abs/2503.10630v3,"In this paper, we propose a general framework for universal zero-shot goal-oriented navigation. Existing zero-shot methods build inference framework upon large language models (LLM) for specific tasks, which differs a lot in overall pipeline and fails to generalize across different types of goal. Towards the aim of universal zero-shot navigation, we propose a uniform graph representation to unify different goals, including object category, instance image and text description. We also convert the observation of agent into an online maintained scene graph. With this consistent scene and goal representation, we preserve most structural information compared with pure text and are able to leverage LLM for explicit graph-based reasoning. Specifically, we conduct graph matching between the scene graph and goal graph at each time instant and propose different strategies to generate long-term goal of exploration according to different matching states. The agent first iteratively searches subgraph of goal when zero-matched. With partial matching, the agent then utilizes coordinate projection and anchor pair alignment to infer the goal location. Finally scene graph correction and goal verification are applied for perfect matching. We also present a blacklist mechanism to enable robust switch between stages. Extensive experiments on several benchmarks show that our UniGoal achieves state-of-the-art zero-shot performance on three studied navigation tasks with a single model, even outperforming task-specific zero-shot methods and supervised universal methods.",True,True,"Yin, Hang and Xu, Xiuwei and Zhao, Linqing and Wang, Ziwei and Zhou, Jie and Lu, Jiwen",2025,,,,
MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,2511.10376v1,li2025rate,\cite{li2025rate},RATE-Nav: Region-Aware Termination Enhancement for Zero-shot Object Navigation with Vision-Language Models,https://arxiv.org/abs/2506.02354v1,"Object Navigation (ObjectNav) is a fundamental task in embodied artificial intelligence. Although significant progress has been made in semantic map construction and target direction prediction in current research, redundant exploration and exploration failures remain inevitable. A critical but underexplored direction is the timely termination of exploration to overcome these challenges. We observe a diminishing marginal effect between exploration steps and exploration rates and analyze the cost-benefit relationship of exploration. Inspired by this, we propose RATE-Nav, a Region-Aware Termination-Enhanced method. It includes a geometric predictive region segmentation algorithm and region-Based exploration estimation algorithm for exploration rate calculation. By leveraging the visual question answering capabilities of visual language models (VLMs) and exploration rates enables efficient termination.RATE-Nav achieves a success rate of 67.8% and an SPL of 31.3% on the HM3D dataset. And on the more challenging MP3D dataset, RATE-Nav shows approximately 10% improvement over previous zero-shot methods.",True,True,"Li, Junjie and Zhang, Nan and Qu, Xiaoyang and Lu, Kai and Li, Guokuan and Wan, Jiguang and Wang, Jianzong",2025,,,,arXiv preprint arXiv:2506.02354
MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,2511.10376v1,imggoal2,\cite{imggoal2},Navigating to Objects Specified by Images,https://arxiv.org/abs/2304.01192v1,"Images are a convenient way to specify which particular object instance an embodied agent should navigate to. Solving this task requires semantic visual reasoning and exploration of unknown environments. We present a system that can perform this task in both simulation and the real world. Our modular method solves sub-tasks of exploration, goal instance re-identification, goal localization, and local navigation. We re-identify the goal instance in egocentric vision using feature-matching and localize the goal instance by projecting matched features to a map. Each sub-task is solved using off-the-shelf components requiring zero fine-tuning. On the HM3D InstanceImageNav benchmark, this system outperforms a baseline end-to-end RL policy 7x and a state-of-the-art ImageNav model 2.3x (56% vs 25% success). We deploy this system to a mobile robot platform and demonstrate effective real-world performance, achieving an 88% success rate across a home and an office environment.",True,True,"Krantz, Jacob and Gervet, Theophile and Yadav, Karmesh and Wang, Austin and Paxton, Chris and Mottaghi, Roozbeh and Batra, Dhruv and Malik, Jitendra and Lee, Stefan and Chaplot, Devendra Singh",2023,,,,
MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,2511.10376v1,yan2025sign,\cite{yan2025sign},SIGN: Safety-Aware Image-Goal Navigation for Autonomous Drones via Reinforcement Learning,https://arxiv.org/abs/2508.12394v1,"Image-goal navigation (ImageNav) tasks a robot with autonomously exploring an unknown environment and reaching a location that visually matches a given target image. While prior works primarily study ImageNav for ground robots, enabling this capability for autonomous drones is substantially more challenging due to their need for high-frequency feedback control and global localization for stable flight. In this paper, we propose a novel sim-to-real framework that leverages visual reinforcement learning (RL) to achieve ImageNav for drones. To enhance visual representation ability, our approach trains the vision backbone with auxiliary tasks, including image perturbations and future transition prediction, which results in more effective policy training. The proposed algorithm enables end-to-end ImageNav with direct velocity control, eliminating the need for external localization. Furthermore, we integrate a depth-based safety module for real-time obstacle avoidance, allowing the drone to safely navigate in cluttered environments. Unlike most existing drone navigation methods that focus solely on reference tracking or obstacle avoidance, our framework supports comprehensive navigation behaviors--autonomous exploration, obstacle avoidance, and image-goal seeking--without requiring explicit global mapping. Code and model checkpoints will be released upon acceptance.",True,True,"Yan, Zichen and Huang, Rui and He, Lei and Guo, Shao and Zhao, Lin",2025,,,,ARXIV
MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,2511.10376v1,related_25,\cite{related_25},Instructnav: Zero-shot system for generic instruction navigation in unexplored environment,,,True,False,"Long, Yuxing and Cai, Wenzhe and Wang, Hongcheng and Zhan, Guanqi and Dong, Hao",2024,,,,arXiv preprint arXiv:2406.04882
MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,2511.10376v1,related_09,\cite{related_09},Mapgpt: Map-guided prompting with adaptive path planning for vision-and-language navigation,,,True,False,"Chen, Jiaqi and Lin, Bingqian and Xu, Ran and Chai, Zhenhua and Liang, Xiaodan and Wong, Kwan-Yee K",2024,,,,arXiv preprint arXiv:2401.07314
MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,2511.10376v1,related_20,\cite{related_20},"Tina: Think, interaction, and action framework for zero-shot vision language navigation",,,True,False,"Li, Dingbang and Chen, Wenzhou and Lin, Xin",2024,,,,
MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,2511.10376v1,related_26,\cite{related_26},Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions,https://arxiv.org/abs/2309.11382v1,"Visual language navigation (VLN) is an embodied task demanding a wide range of skills encompassing understanding, perception, and planning. For such a multifaceted challenge, previous VLN methods totally rely on one model's own thinking to make predictions within one round. However, existing models, even the most advanced large language model GPT4, still struggle with dealing with multiple tasks by single-round self-thinking. In this work, drawing inspiration from the expert consultation meeting, we introduce a novel zero-shot VLN framework. Within this framework, large models possessing distinct abilities are served as domain experts. Our proposed navigation agent, namely DiscussNav, can actively discuss with these experts to collect essential information before moving at every step. These discussions cover critical navigation subtasks like instruction understanding, environment perception, and completion estimation. Through comprehensive experiments, we demonstrate that discussions with domain experts can effectively facilitate navigation by perceiving instruction-relevant information, correcting inadvertent errors, and sifting through in-consistent movement decisions. The performances on the representative VLN task R2R show that our method surpasses the leading zero-shot VLN model by a large margin on all metrics. Additionally, real-robot experiments display the obvious advantages of our method over single-round self-thinking.",True,True,"Long, Yuxing and Li, Xiaoqi and Cai, Wenzhe and Dong, Hao",2024,,,,
MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,2511.10376v1,related_47,\cite{related_47},Mc-gpt: Empowering vision-and-language navigation with memory map and reasoning chains,,,True,False,"Zhan, Zhaohuan and Yu, Lisha and Yu, Sijie and Tan, Guang",2024,,,,arXiv preprint arXiv:2405.10620
MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,2511.10376v1,related_48,\cite{related_48},Navgpt: Explicit reasoning in vision-and-language navigation with large language models,,,True,False,"Zhou, Gengze and Hong, Yicong and Wu, Qi",2024,,,,
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,ICLR2021ViT,\cite{ICLR2021ViT},An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,https://arxiv.org/abs/2010.11929v2,"While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",True,True,"Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others",2020,,,,
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,ICCV2021pointTransformer,\cite{ICCV2021pointTransformer},Point Transformer,https://arxiv.org/abs/2012.09164v2,"Self-attention networks have revolutionized natural language processing and are making impressive strides in image analysis tasks such as image classification and object detection. Inspired by this success, we investigate the application of self-attention networks to 3D point cloud processing. We design self-attention layers for point clouds and use these to construct self-attention networks for tasks such as semantic scene segmentation, object part segmentation, and object classification. Our Point Transformer design improves upon prior work across domains and tasks. For example, on the challenging S3DIS dataset for large-scale semantic scene segmentation, the Point Transformer attains an mIoU of 70.4% on Area 5, outperforming the strongest prior model by 3.3 absolute percentage points and crossing the 70% mIoU threshold for the first time.",True,True,"Zhao, Hengshuang and Jiang, Li and Jia, Jiaya and Torr, Philip HS and Koltun, Vladlen",2021,,,,
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,ICML2021CLIP,\cite{ICML2021CLIP},Learning Transferable Visual Models From Natural Language Supervision,https://arxiv.org/abs/2103.00020v1,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",True,True,"Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",2021,,,,
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,iccv2021dino,\cite{iccv2021dino},Emerging Properties in Self-Supervised Vision Transformers,https://arxiv.org/abs/2104.14294v2,"In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base.",True,True,"Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand",2021,,,,
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,eccv2022pointmae,\cite{eccv2022pointmae},Masked autoencoders for point cloud self-supervised learning,,,True,False,"Pang, Yatian and Wang, Wenxiao and Tay, Francis EH and Liu, Wei and Tian, Yonghong and Yuan, Li",2022,,,,
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,cvpr2022pointbert,\cite{cvpr2022pointbert},Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling,https://arxiv.org/abs/2111.14819v2,"We present Point-BERT, a new paradigm for learning Transformers to generalize the concept of BERT to 3D point cloud. Inspired by BERT, we devise a Masked Point Modeling (MPM) task to pre-train point cloud Transformers. Specifically, we first divide a point cloud into several local point patches, and a point cloud Tokenizer with a discrete Variational AutoEncoder (dVAE) is designed to generate discrete point tokens containing meaningful local information. Then, we randomly mask out some patches of input point clouds and feed them into the backbone Transformers. The pre-training objective is to recover the original point tokens at the masked locations under the supervision of point tokens obtained by the Tokenizer. Extensive experiments demonstrate that the proposed BERT-style pre-training strategy significantly improves the performance of standard point cloud Transformers. Equipped with our pre-training strategy, we show that a pure Transformer architecture attains 93.8% accuracy on ModelNet40 and 83.1% accuracy on the hardest setting of ScanObjectNN, surpassing carefully designed point cloud models with much fewer hand-made designs. We also demonstrate that the representations learned by Point-BERT transfer well to new tasks and domains, where our models largely advance the state-of-the-art of few-shot point cloud classification task. The code and pre-trained models are available at https://github.com/lulutang0608/Point-BERT",True,True,"Yu, Xumin and Tang, Lulu and Rao, Yongming and Huang, Tiejun and Zhou, Jie and Lu, Jiwen",2022,,,,
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,iccv2021swin,\cite{iccv2021swin},Swin Transformer: Hierarchical Vision Transformer using Shifted Windows,https://arxiv.org/abs/2103.14030v2,"This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbf{S}hifted \textbf{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at~\url{https://github.com/microsoft/Swin-Transformer}.",True,True,"Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining",2021,,,,
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,pami2022vlt,\cite{pami2022vlt},VLT: Vision-language transformer and query generation for referring segmentation,,,True,False,"Ding, Henghui and Liu, Chang and Wang, Suchen and Jiang, Xudong",2022,,,,IEEE Trans. Pattern Anal. Mach. Intell.
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,cvpr2017pointnet,\cite{cvpr2017pointnet},PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation,https://arxiv.org/abs/1612.00593v2,"Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds and well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.",True,True,"Qi, Charles R and Su, Hao and Mo, Kaichun and Guibas, Leonidas J",2017,,,,
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,tpami2023Flatteningnet,\cite{tpami2023Flatteningnet},Flattening-Net: Deep Regular 2D Representation for 3D Point Cloud Analysis,https://arxiv.org/abs/2212.08892v2,"Point clouds are characterized by irregularity and unstructuredness, which pose challenges in efficient data exploitation and discriminative feature extraction. In this paper, we present an unsupervised deep neural architecture called Flattening-Net to represent irregular 3D point clouds of arbitrary geometry and topology as a completely regular 2D point geometry image (PGI) structure, in which coordinates of spatial points are captured in colors of image pixels. \mr{Intuitively, Flattening-Net implicitly approximates a locally smooth 3D-to-2D surface flattening process while effectively preserving neighborhood consistency.} \mr{As a generic representation modality, PGI inherently encodes the intrinsic property of the underlying manifold structure and facilitates surface-style point feature aggregation.} To demonstrate its potential, we construct a unified learning framework directly operating on PGIs to achieve \mr{diverse types of high-level and low-level} downstream applications driven by specific task networks, including classification, segmentation, reconstruction, and upsampling. Extensive experiments demonstrate that our methods perform favorably against the current state-of-the-art competitors. We will make the code and data publicly available at https://github.com/keeganhk/Flattening-Net.",True,True,"Zhang, Qijian and Hou, Junhui and Qian, Yue and Zeng, Yiming and Zhang, Juyong and He, Ying",2023,,,,IEEE Trans. Pattern Anal. Mach. Intell.
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,pamivote2cap,\cite{pamivote2cap},Vote2cap-detr++: Decoupling localization and describing for end-to-end 3d dense captioning,,,True,False,"Chen, Sijin and Zhu, Hongyuan and Li, Mingsheng and Chen, Xin and Guo, Peng and Lei, Yinjie and Yu, Gang and Li, Taihao and Chen, Tao",2024,,,,IEEE Trans. Pattern Anal. Mach. Intell.
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,cvm2021pct,\cite{cvm2021pct},PCT: Point cloud transformer,https://arxiv.org/abs/2012.09688v4,"The irregular domain and lack of ordering make it challenging to design deep neural networks for point cloud processing. This paper presents a novel framework named Point Cloud Transformer(PCT) for point cloud learning. PCT is based on Transformer, which achieves huge success in natural language processing and displays great potential in image processing. It is inherently permutation invariant for processing a sequence of points, making it well-suited for point cloud learning. To better capture local context within the point cloud, we enhance input embedding with the support of farthest point sampling and nearest neighbor search. Extensive experiments demonstrate that the PCT achieves the state-of-the-art performance on shape classification, part segmentation and normal estimation tasks.",True,True,"Guo, Meng-Hao and Cai, Jun-Xiong and Liu, Zheng-Ning and Mu, Tai-Jiang and Martin, Ralph R and Hu, Shi-Min",2021,,,,Comput. Vis. Media
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,pami2024genvcl,\cite{pami2024genvcl},Generative variational-contrastive learning for self-supervised point cloud representation,,,True,False,"Wang, Bohua and Tian, Zhiqiang and Ye, Aixue and Wen, Feng and Du, Shaoyi and Gao, Yue",2024,,,,IEEE Trans. Pattern Anal. Mach. Intell.
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,nips2022pointtranformerv2,\cite{nips2022pointtranformerv2},Point Transformer V2: Grouped Vector Attention and Partition-based Pooling,https://arxiv.org/abs/2210.05666v2,"As a pioneering work exploring transformer architecture for 3D point cloud understanding, Point Transformer achieves impressive results on multiple highly competitive benchmarks. In this work, we analyze the limitations of the Point Transformer and propose our powerful and efficient Point Transformer V2 model with novel designs that overcome the limitations of previous work. In particular, we first propose group vector attention, which is more effective than the previous version of vector attention. Inheriting the advantages of both learnable weight encoding and multi-head attention, we present a highly effective implementation of grouped vector attention with a novel grouped weight encoding layer. We also strengthen the position information for attention by an additional position encoding multiplier. Furthermore, we design novel and lightweight partition-based pooling methods which enable better spatial alignment and more efficient sampling. Extensive experiments show that our model achieves better performance than its predecessor and achieves state-of-the-art on several challenging 3D point cloud understanding benchmarks, including 3D point cloud segmentation on ScanNet v2 and S3DIS and 3D point cloud classification on ModelNet40. Our code will be available at https://github.com/Gofinge/PointTransformerV2.",True,True,"Wu, Xiaoyang and Lao, Yixing and Jiang, Li and Liu, Xihui and Zhao, Hengshuang",2022,,,,Adv. Neural Inform. Process. Syst.
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,cvpr2024pointtranformerv3,\cite{cvpr2024pointtranformerv3},"Point Transformer V3: Simpler, Faster, Stronger",https://arxiv.org/abs/2312.10035v2,"This paper is not motivated to seek innovation within the attention mechanism. Instead, it focuses on overcoming the existing trade-offs between accuracy and efficiency within the context of point cloud processing, leveraging the power of scale. Drawing inspiration from recent advances in 3D large-scale representation learning, we recognize that model performance is more influenced by scale than by intricate design. Therefore, we present Point Transformer V3 (PTv3), which prioritizes simplicity and efficiency over the accuracy of certain mechanisms that are minor to the overall performance after scaling, such as replacing the precise neighbor search by KNN with an efficient serialized neighbor mapping of point clouds organized with specific patterns. This principle enables significant scaling, expanding the receptive field from 16 to 1024 points while remaining efficient (a 3x increase in processing speed and a 10x improvement in memory efficiency compared with its predecessor, PTv2). PTv3 attains state-of-the-art results on over 20 downstream tasks that span both indoor and outdoor scenarios. Further enhanced with multi-dataset joint training, PTv3 pushes these results to a higher level.",True,True,"Wu, Xiaoyang and Jiang, Li and Wang, Peng-Shuai and Liu, Zhijian and Liu, Xihui and Qiao, Yu and Ouyang, Wanli and He, Tong and Zhao, Hengshuang",2024,,,,
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,CVPR2023winclip,\cite{CVPR2023winclip},WinCLIP: Zero-/Few-Shot Anomaly Classification and Segmentation,https://arxiv.org/abs/2303.14814v1,"Visual anomaly classification and segmentation are vital for automating industrial quality inspection. The focus of prior research in the field has been on training custom models for each quality inspection task, which requires task-specific images and annotation. In this paper we move away from this regime, addressing zero-shot and few-normal-shot anomaly classification and segmentation. Recently CLIP, a vision-language model, has shown revolutionary generality with competitive zero-/few-shot performance in comparison to full-supervision. But CLIP falls short on anomaly classification and segmentation tasks. Hence, we propose window-based CLIP (WinCLIP) with (1) a compositional ensemble on state words and prompt templates and (2) efficient extraction and aggregation of window/patch/image-level features aligned with text. We also propose its few-normal-shot extension WinCLIP+, which uses complementary information from normal images. In MVTec-AD (and VisA), without further tuning, WinCLIP achieves 91.8%/85.1% (78.1%/79.6%) AUROC in zero-shot anomaly classification and segmentation while WinCLIP+ does 93.1%/95.2% (83.8%/96.4%) in 1-normal-shot, surpassing state-of-the-art by large margins.",True,True,"Jeong, Jongheon and Zou, Yang and Kim, Taewan and Zhang, Dongqing and Ravichandran, Avinash and Dabeer, Onkar",2023,,,,
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,eccv2024adaclip,\cite{eccv2024adaclip},AdaCLIP: Adapting CLIP with Hybrid Learnable Prompts for Zero-Shot Anomaly Detection,,,True,False,"Cao, Yunkang and Zhang, Jiangning and Frittoli, Luca and Cheng, Yuqi and Shen, Weiming and Boracchi, Giacomo",2024,,,,
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,eccv2024vcpclip,\cite{eccv2024vcpclip},VCP-CLIP: A visual context prompting model for zero-shot anomaly segmentation,https://arxiv.org/abs/2407.12276v1,"Recently, large-scale vision-language models such as CLIP have demonstrated immense potential in zero-shot anomaly segmentation (ZSAS) task, utilizing a unified model to directly detect anomalies on any unseen product with painstakingly crafted text prompts. However, existing methods often assume that the product category to be inspected is known, thus setting product-specific text prompts, which is difficult to achieve in the data privacy scenarios. Moreover, even the same type of product exhibits significant differences due to specific components and variations in the production process, posing significant challenges to the design of text prompts. In this end, we propose a visual context prompting model (VCP-CLIP) for ZSAS task based on CLIP. The insight behind VCP-CLIP is to employ visual context prompting to activate CLIP's anomalous semantic perception ability. In specific, we first design a Pre-VCP module to embed global visual information into the text prompt, thus eliminating the necessity for product-specific prompts. Then, we propose a novel Post-VCP module, that adjusts the text embeddings utilizing the fine-grained features of the images. In extensive experiments conducted on 10 real-world industrial anomaly segmentation datasets, VCP-CLIP achieved state-of-the-art performance in ZSAS task. The code is available at https://github.com/xiaozhen228/VCP-CLIP.",True,True,"Qu, Zhen and Tao, Xian and Prasad, Mukesh and Shen, Fei and Zhang, Zhengtao and Gong, Xinyi and Ding, Guiguang",2024,,,,
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,wacv2024promptad,\cite{wacv2024promptad},Promptad: Zero-shot anomaly detection using text prompts,,,True,False,"Li, Yiting and Goodge, Adam and Liu, Fayao and Foo, Chuan-Sheng",2024,,,,
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,arxiv2023APRILGAN,\cite{arxiv2023APRILGAN},A Zero-/Few-Shot Anomaly Classification and Segmentation Method for CVPR 2023 VAND Workshop Challenge Tracks 1\&2: 1st Place on Zero-shot AD and 4th Place on Few-shot AD,,,True,False,"Chen, Xuhai and Han, Yue and Zhang, Jiangning",2023,,,,arXiv preprint arXiv:2305.17382
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,mm2024filo,\cite{mm2024filo},FiLo: Zero-Shot Anomaly Detection by Fine-Grained Description and High-Quality Localization,https://arxiv.org/abs/2404.13671v2,"Zero-shot anomaly detection (ZSAD) methods entail detecting anomalies directly without access to any known normal or abnormal samples within the target item categories. Existing approaches typically rely on the robust generalization capabilities of multimodal pretrained models, computing similarities between manually crafted textual features representing ""normal"" or ""abnormal"" semantics and image features to detect anomalies and localize anomalous patches. However, the generic descriptions of ""abnormal"" often fail to precisely match diverse types of anomalies across different object categories. Additionally, computing feature similarities for single patches struggles to pinpoint specific locations of anomalies with various sizes and scales. To address these issues, we propose a novel ZSAD method called FiLo, comprising two components: adaptively learned Fine-Grained Description (FG-Des) and position-enhanced High-Quality Localization (HQ-Loc). FG-Des introduces fine-grained anomaly descriptions for each category using Large Language Models (LLMs) and employs adaptively learned textual templates to enhance the accuracy and interpretability of anomaly detection. HQ-Loc, utilizing Grounding DINO for preliminary localization, position-enhanced text prompts, and Multi-scale Multi-shape Cross-modal Interaction (MMCI) module, facilitates more accurate localization of anomalies of different sizes and shapes. Experimental results on datasets like MVTec and VisA demonstrate that FiLo significantly improves the performance of ZSAD in both detection and localization, achieving state-of-the-art performance with an image-level AUC of 83.9% and a pixel-level AUC of 95.9% on the VisA dataset. Code is available at https://github.com/CASIA-IVA-Lab/FiLo.",True,True,"Gu, Zhaopeng and Zhu, Bingke and Zhu, Guibo and Chen, Yingying and Li, Hao and Tang, Ming and Wang, Jinqiao",2024,,,,
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,wacv2023utad,\cite{wacv2023utad},Zero-shot versus many-shot: Unsupervised texture anomaly detection,,,True,False,"Aota, Toshimichi and Tong, Lloyd Teh Tzer and Okatani, Takayuki",2023,,,,
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,nips2023ACR,\cite{nips2023ACR},Zero-Shot Anomaly Detection via Batch Normalization,https://arxiv.org/abs/2302.07849v4,"Anomaly detection (AD) plays a crucial role in many safety-critical application domains. The challenge of adapting an anomaly detector to drift in the normal data distribution, especially when no training data is available for the ""new normal,"" has led to the development of zero-shot AD techniques. In this paper, we propose a simple yet effective method called Adaptive Centered Representations (ACR) for zero-shot batch-level AD. Our approach trains off-the-shelf deep anomaly detectors (such as deep SVDD) to adapt to a set of inter-related training data distributions in combination with batch normalization, enabling automatic zero-shot generalization for unseen AD tasks. This simple recipe, batch normalization plus meta-training, is a highly effective and versatile tool. Our theoretical results guarantee the zero-shot generalization for unseen AD tasks; our empirical results demonstrate the first zero-shot AD results for tabular data and outperform existing methods in zero-shot anomaly detection and segmentation on image data from specialized domains. Code is at https://github.com/aodongli/zero-shot-ad-via-batch-norm",True,True,"Li, Aodong and Qiu, Chen and Kloft, Marius and Smyth, Padhraic and Rudolph, Maja and Mandt, Stephan",2023,,,,
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,iclr2024musc,\cite{iclr2024musc},MuSc: Zero-Shot Industrial Anomaly Classification and Segmentation with Mutual Scoring of the Unlabeled Images,https://arxiv.org/abs/2401.16753v1,"This paper studies zero-shot anomaly classification (AC) and segmentation (AS) in industrial vision. We reveal that the abundant normal and abnormal cues implicit in unlabeled test images can be exploited for anomaly determination, which is ignored by prior methods. Our key observation is that for the industrial product images, the normal image patches could find a relatively large number of similar patches in other unlabeled images, while the abnormal ones only have a few similar patches. We leverage such a discriminative characteristic to design a novel zero-shot AC/AS method by Mutual Scoring (MuSc) of the unlabeled images, which does not need any training or prompts. Specifically, we perform Local Neighborhood Aggregation with Multiple Degrees (LNAMD) to obtain the patch features that are capable of representing anomalies in varying sizes. Then we propose the Mutual Scoring Mechanism (MSM) to leverage the unlabeled test images to assign the anomaly score to each other. Furthermore, we present an optimization approach named Re-scoring with Constrained Image-level Neighborhood (RsCIN) for image-level anomaly classification to suppress the false positives caused by noises in normal images. The superior performance on the challenging MVTec AD and VisA datasets demonstrates the effectiveness of our approach. Compared with the state-of-the-art zero-shot approaches, MuSc achieves a $\textbf{21.1%}$ PRO absolute gain (from 72.7% to 93.8%) on MVTec AD, a $\textbf{19.4%}$ pixel-AP gain and a $\textbf{14.7%}$ pixel-AUROC gain on VisA. In addition, our zero-shot approach outperforms most of the few-shot approaches and is comparable to some one-class methods. Code is available at https://github.com/xrli-U/MuSc.",True,True,"Li, Xurui and Huang, Ziming and Xue, Feng and Zhou, Yu",2024,,,,
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,nips2024pointad,\cite{nips2024pointad},PointAD: Comprehending 3D Anomalies from Points and Pixels for Zero-shot 3D Anomaly Detection,https://arxiv.org/abs/2410.00320v4,"Zero-shot (ZS) 3D anomaly detection is a crucial yet unexplored field that addresses scenarios where target 3D training samples are unavailable due to practical concerns like privacy protection. This paper introduces PointAD, a novel approach that transfers the strong generalization capabilities of CLIP for recognizing 3D anomalies on unseen objects. PointAD provides a unified framework to comprehend 3D anomalies from both points and pixels. In this framework, PointAD renders 3D anomalies into multiple 2D renderings and projects them back into 3D space. To capture the generic anomaly semantics into PointAD, we propose hybrid representation learning that optimizes the learnable text prompts from 3D and 2D through auxiliary point clouds. The collaboration optimization between point and pixel representations jointly facilitates our model to grasp underlying 3D anomaly patterns, contributing to detecting and segmenting anomalies of unseen diverse 3D objects. Through the alignment of 3D and 2D space, our model can directly integrate RGB information, further enhancing the understanding of 3D anomalies in a plug-and-play manner. Extensive experiments show the superiority of PointAD in ZS 3D anomaly detection across diverse unseen objects.",True,True,"Zhou, Qihang and Yan, Jiangtao and He, Shibo and Meng, Wenchao and Chen, Jiming",2024,,,,Adv. Neural Inform. Process. Syst.
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,eccv2025r3d,\cite{eccv2025r3d},R3D-AD: Reconstruction via Diffusion for 3D Anomaly Detection,,,True,False,"Zhou, Zheyuan and Wang, Le and Fang, Naiyu and Wang, Zili and Qiu, Lemiao and Zhang, Shuyou",2025,,,,
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,cvpr2024anomaly-shapenet,\cite{cvpr2024anomaly-shapenet},Towards Scalable 3D Anomaly Detection and Localization: A Benchmark via 3D Anomaly Synthesis and A Self-Supervised Learning Network,,,True,False,"Li, Wenqiao and Xu, Xiaohao and Gu, Yao and Zheng, Bozhong and Gao, Shenghua and Wu, Yingna",2024,,,,
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,mm2023easynet,\cite{mm2023easynet},Easynet: An easy network for 3d industrial anomaly detection,,,True,False,"Chen, Ruitao and Xie, Guoyang and Liu, Jiaqi and Wang, Jinbao and Luo, Ziqi and Wang, Jinfan and Zheng, Feng",2023,,,,
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,cvpr2023btf,\cite{cvpr2023btf},Back to the feature: classical 3d features are (almost) all you need for 3d anomaly detection,,,True,False,"Horwitz, Eliahu and Hoshen, Yedid",2023,,,,
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,cvpr2023m3dm,\cite{cvpr2023m3dm},Multimodal Industrial Anomaly Detection via Hybrid Fusion,https://arxiv.org/abs/2303.00601v2,"2D-based Industrial Anomaly Detection has been widely discussed, however, multimodal industrial anomaly detection based on 3D point clouds and RGB images still has many untouched fields. Existing multimodal industrial anomaly detection methods directly concatenate the multimodal features, which leads to a strong disturbance between features and harms the detection performance. In this paper, we propose Multi-3D-Memory (M3DM), a novel multimodal anomaly detection method with hybrid fusion scheme: firstly, we design an unsupervised feature fusion with patch-wise contrastive learning to encourage the interaction of different modal features; secondly, we use a decision layer fusion with multiple memory banks to avoid loss of information and additional novelty classifiers to make the final decision. We further propose a point feature alignment operation to better align the point cloud and RGB features. Extensive experiments show that our multimodal industrial anomaly detection model outperforms the state-of-the-art (SOTA) methods on both detection and segmentation precision on MVTec-3D AD dataset. Code is available at https://github.com/nomewang/M3DM.",True,True,"Wang, Yue and Peng, Jinlong and Zhang, Jiangning and Yi, Ran and Wang, Yabiao and Wang, Chengjie",2023,,,,
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,icml2023shape-guided,\cite{icml2023shape-guided},Shape-guided dual-memory learning for 3D anomaly detection,,,True,False,"Chu, Yu-Min and Liu, Chieh and Hsieh, Ting-I and Chen, Hwann-Tzong and Liu, Tyng-Luh",2023,,,,
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,cvpr2024cfm,\cite{cvpr2024cfm},Multimodal Industrial Anomaly Detection by Crossmodal Feature Mapping,https://arxiv.org/abs/2312.04521v2,"The paper explores the industrial multimodal Anomaly Detection (AD) task, which exploits point clouds and RGB images to localize anomalies. We introduce a novel light and fast framework that learns to map features from one modality to the other on nominal samples. At test time, anomalies are detected by pinpointing inconsistencies between observed and mapped features. Extensive experiments show that our approach achieves state-of-the-art detection and segmentation performance in both the standard and few-shot settings on the MVTec 3D-AD dataset while achieving faster inference and occupying less memory than previous multimodal AD methods. Moreover, we propose a layer-pruning technique to improve memory and time efficiency with a marginal sacrifice in performance.",True,True,"Costanzino, Alex and Ramirez, Pierluigi Zama and Lisanti, Giuseppe and Di Stefano, Luigi",2024,,,,
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,nips2003RDM,\cite{nips2003RDM},Ranking on data manifolds,,,True,False,"Zhou, Dengyong and Weston, Jason and Gretton, Arthur and Bousquet, Olivier and Sch{\""o}lkopf, Bernhard",2003,,,,Adv. Neural Inform. Process. Syst.
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,pami2008riemannian,\cite{pami2008riemannian},Riemannian manifold learning,,,True,False,"Lin, Tong and Zha, Hongbin",2008,,,,IEEE Trans. Pattern Anal. Mach. Intell.
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,CVPR2012sd,\cite{CVPR2012sd},Affinity learning via self-diffusion for image segmentation and clustering,,,True,False,"Wang, Bo and Tu, Zhuowen",2012,,,,
MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples,2511.10047v1,pami2011adaptml,\cite{pami2011adaptml},Adaptive manifold learning,,,True,False,"Zhang, Zhenyue and Wang, Jing and Zha, Hongyuan",2011,,,,IEEE Trans. Pattern Anal. Mach. Intell.
Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy,2511.09737v1,kirk2023survey,\cite{kirk2023survey},A Survey of Zero-shot Generalisation in Deep Reinforcement Learning,https://arxiv.org/abs/2111.09794v6,"The study of zero-shot generalisation (ZSG) in deep Reinforcement Learning (RL) aims to produce RL algorithms whose policies generalise well to novel unseen situations at deployment time, avoiding overfitting to their training environments. Tackling this is vital if we are to deploy reinforcement learning algorithms in real world scenarios, where the environment will be diverse, dynamic and unpredictable. This survey is an overview of this nascent field. We rely on a unifying formalism and terminology for discussing different ZSG problems, building upon previous works. We go on to categorise existing benchmarks for ZSG, as well as current methods for tackling these problems. Finally, we provide a critical discussion of the current state of the field, including recommendations for future work. Among other conclusions, we argue that taking a purely procedural content generation approach to benchmark design is not conducive to progress in ZSG, we suggest fast online adaptation and tackling RL-specific problems as some areas for future work on methods for ZSG, and we recommend building benchmarks in underexplored problem settings such as offline RL ZSG and reward-function variation.",True,True,"Kirk, Robert and Zhang, Amy and Grefenstette, Edward and Rockt{\""a}schel, Tim",2023,,,,Journal of Artificial Intelligence Research
Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy,2511.09737v1,lyle2022learning,\cite{lyle2022learning},{Learning Dynamics and Generalization in Reinforcement Learning},,,True,False,"Lyle, Clare and Rowland, Mark and Dabney, Will and Kwiatkowska, Marta and Gal, Yarin",2022,,,,Int. Conf. on Machine Learning
Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy,2511.09737v1,langford2017,\cite{langford2017},Inverse Reinforcement Learning in Contextual MDPs,https://arxiv.org/abs/1905.09710v5,"We consider the task of Inverse Reinforcement Learning in Contextual Markov Decision Processes (MDPs). In this setting, contexts, which define the reward and transition kernel, are sampled from a distribution. In addition, although the reward is a function of the context, it is not provided to the agent. Instead, the agent observes demonstrations from an optimal policy. The goal is to learn the reward mapping, such that the agent will act optimally even when encountering previously unseen contexts, also known as zero-shot transfer. We formulate this problem as a non-differential convex optimization problem and propose a novel algorithm to compute its subgradients. Based on this scheme, we analyze several methods both theoretically, where we compare the sample complexity and scalability, and empirically. Most importantly, we show both theoretically and empirically that our algorithms perform zero-shot transfer (generalize to new and unseen contexts). Specifically, we present empirical experiments in a dynamic treatment regime, where the goal is to learn a reward function which explains the behavior of expert physicians based on recorded data of them treating patients diagnosed with sepsis.",True,True,"Langford, John",2017,,,10.1109/BigData.2017.8257902,
Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy,2511.09737v1,benjamins2021carl,\cite{benjamins2021carl},{CARL: A Benchmark for Contextual and Adaptive Reinforcement Learning},,,True,False,"Benjamins, Carolin and Eimer, Theresa and Schubert, Frederik and Biedenkapp, Andr{\'e} and Rosenhahn, Bodo and Hutter, Frank and Lindauer, Marius",2021,,,,"Eco. Theory RL, NeurIPS"
Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy,2511.09737v1,beukman2023dynamics,\cite{beukman2023dynamics},Dynamics Generalisation in Reinforcement Learning via Adaptive Context-Aware Policies,https://arxiv.org/abs/2310.16686v1,"While reinforcement learning has achieved remarkable successes in several domains, its real-world application is limited due to many methods failing to generalise to unfamiliar conditions. In this work, we consider the problem of generalising to new transition dynamics, corresponding to cases in which the environment's response to the agent's actions differs. For example, the gravitational force exerted on a robot depends on its mass and changes the robot's mobility. Consequently, in such cases, it is necessary to condition an agent's actions on extrinsic state information and pertinent contextual information reflecting how the environment responds. While the need for context-sensitive policies has been established, the manner in which context is incorporated architecturally has received less attention. Thus, in this work, we present an investigation into how context information should be incorporated into behaviour learning to improve generalisation. To this end, we introduce a neural network architecture, the Decision Adapter, which generates the weights of an adapter module and conditions the behaviour of an agent on the context information. We show that the Decision Adapter is a useful generalisation of a previously proposed architecture and empirically demonstrate that it results in superior generalisation performance compared to previous approaches in several environments. Beyond this, the Decision Adapter is more robust to irrelevant distractor variables than several alternative methods.",True,True,"Beukman, Michael and Jarvis, Devon and Klein, Richard and James, Steven and Rosman, Benjamin",2023,,,,Neural Information Processing Systems
Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy,2511.09737v1,chen2021context,\cite{chen2021context},Context-Aware Safe Reinforcement Learning for Non-Stationary Environments,https://arxiv.org/abs/2101.00531v1,"Safety is a critical concern when deploying reinforcement learning agents for realistic tasks. Recently, safe reinforcement learning algorithms have been developed to optimize the agent's performance while avoiding violations of safety constraints. However, few studies have addressed the non-stationary disturbances in the environments, which may cause catastrophic outcomes. In this paper, we propose the context-aware safe reinforcement learning (CASRL) method, a meta-learning framework to realize safe adaptation in non-stationary environments. We use a probabilistic latent variable model to achieve fast inference of the posterior environment transition distribution given the context data. Safety constraints are then evaluated with uncertainty-aware trajectory sampling. The high cost of safety violations leads to the rareness of unsafe records in the dataset. We address this issue by enabling prioritized sampling during model training and formulating prior safety constraints with domain knowledge during constrained planning. The algorithm is evaluated in realistic safety-critical environments with non-stationary disturbances. Results show that the proposed algorithm significantly outperforms existing baselines in terms of safety and robustness.",True,True,"Chen, Baiming and Liu, Zuxin and Zhu, Jiacheng and Xu, Mengdi and Ding, Wenhao and Li, Liang and Zhao, Ding",2021,,,,
Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy,2511.09737v1,lahmer2024fast,\cite{lahmer2024fast},Fast Context Adaptation in Cost-Aware Continual Learning,https://arxiv.org/abs/2306.03887v1,"In the past few years, DRL has become a valuable solution to automatically learn efficient resource management strategies in complex networks with time-varying statistics. However, the increased complexity of 5G and Beyond networks requires correspondingly more complex learning agents and the learning process itself might end up competing with users for communication and computational resources. This creates friction: on the one hand, the learning process needs resources to quickly convergence to an effective strategy; on the other hand, the learning process needs to be efficient, i.e., take as few resources as possible from the user's data plane, so as not to throttle users' QoS. In this paper, we investigate this trade-off and propose a dynamic strategy to balance the resources assigned to the data plane and those reserved for learning. With the proposed approach, a learning agent can quickly converge to an efficient resource allocation strategy and adapt to changes in the environment as for the CL paradigm, while minimizing the impact on the users' QoS. Simulation results show that the proposed method outperforms static allocation methods with minimal learning overhead, almost reaching the performance of an ideal out-of-band CL solution.",True,True,"Lahmer, Seyyidahmed and Mason, Federico and Chiariotti, Federico and Zanella, Andrea",2024,,,10.1109/TMLCN.2024.3386647,IEEE Transactions on Machine Learning in Communications and Networking
Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy,2511.09737v1,lee2020context,\cite{lee2020context},{Context-aware Dynamics Model for Generalization in Model-Based Reinforcement Learning},,,True,False,"Lee, Kimin and Seo, Younggyo and Lee, Seunghyun and Lee, Honglak and Shin, Jinwoo",2020,,,,
Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy,2511.09737v1,prasanna2024dreaming,\cite{prasanna2024dreaming},Dreaming of Many Worlds: Learning Contextual World Models Aids Zero-Shot Generalization,https://arxiv.org/abs/2403.10967v2,"Zero-shot generalization (ZSG) to unseen dynamics is a major challenge for creating generally capable embodied agents. To address the broader challenge, we start with the simpler setting of contextual reinforcement learning (cRL), assuming observability of the context values that parameterize the variation in the system's dynamics, such as the mass or dimensions of a robot, without making further simplifying assumptions about the observability of the Markovian state. Toward the goal of ZSG to unseen variation in context, we propose the contextual recurrent state-space model (cRSSM), which introduces changes to the world model of Dreamer (v3) (Hafner et al., 2023). This allows the world model to incorporate context for inferring latent Markovian states from the observations and modeling the latent dynamics. Our approach is evaluated on two tasks from the CARL benchmark suite, which is tailored to study contextual RL. Our experiments show that such systematic incorporation of the context improves the ZSG of the policies trained on the ""dreams"" of the world model. We further find qualitatively that our approach allows Dreamer to disentangle the latent state from context, allowing it to extrapolate its dreams to the many worlds of unseen contexts. The code for all our experiments is available at https://github.com/sai-prasanna/dreaming_of_many_worlds.",True,True,"Prasanna, Sai and Farid, Karim and Rajan, Raghu and Biedenkapp, Andr{\'e}",2024,,,,Reinforcement Learning Conference
Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy,2511.09737v1,leon2024duplex,\cite{leon2024duplex},{Discovering Creative Behaviors through DUPLEX: Diverse Universal Features for Policy Exploration},,,True,False,"Leon, Borja G and Riccio, Francesco and Subramanian, Kaushik and Wurman, Peter R and Stone, Peter",2024,,,,
Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy,2511.09737v1,lee2020science,\cite{lee2020science},Learning Quadrupedal Locomotion over Challenging Terrain,https://arxiv.org/abs/2010.11251v1,"Some of the most challenging environments on our planet are accessible to quadrupedal animals but remain out of reach for autonomous machines. Legged locomotion can dramatically expand the operational domains of robotics. However, conventional controllers for legged locomotion are based on elaborate state machines that explicitly trigger the execution of motion primitives and reflexes. These designs have escalated in complexity while falling short of the generality and robustness of animal locomotion. Here we present a radically robust controller for legged locomotion in challenging natural environments. We present a novel solution to incorporating proprioceptive feedback in locomotion control and demonstrate remarkable zero-shot generalization from simulation to natural environments. The controller is trained by reinforcement learning in simulation. It is based on a neural network that acts on a stream of proprioceptive signals. The trained controller has taken two generations of quadrupedal ANYmal robots to a variety of natural environments that are beyond the reach of prior published work in legged locomotion. The controller retains its robustness under conditions that have never been encountered during training: deformable terrain such as mud and snow, dynamic footholds such as rubble, and overground impediments such as thick vegetation and gushing water. The presented work opens new frontiers for robotics and indicates that radical robustness in natural environments can be achieved by training in much simpler domains.",True,True,Joonho Lee  and Jemin Hwangbo  and Lorenz Wellhausen  and Vladlen Koltun  and Marco Hutter,2020,,https://www.science.org/doi/abs/10.1126/scirobotics.abc5986,10.1126/scirobotics.abc5986,Science Robotics
Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy,2511.09737v1,kumar2021rma,\cite{kumar2021rma},{RMA: Rapid Motor Adaptation for Legged Robots},,,True,False,"Kumar, Ashish and Fu, Zipeng and Pathak, Deepak and Malik, Jitendra",2021,,,,Robotics: Science and Systems
Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy,2511.09737v1,tobin2017domain,\cite{tobin2017domain},{Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World},,,True,False,"Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter",2017,,,,
Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy,2511.09737v1,peng2018sim,\cite{peng2018sim},Sim-to-Real Transfer of Robotic Control with Dynamics Randomization,https://arxiv.org/abs/1710.06537v3,"Simulations are attractive environments for training agents as they provide an abundant source of data and alleviate certain safety concerns during the training process. But the behaviours developed by agents in simulation are often specific to the characteristics of the simulator. Due to modeling error, strategies that are successful in simulation may not transfer to their real world counterparts. In this paper, we demonstrate a simple method to bridge this ""reality gap"". By randomizing the dynamics of the simulator during training, we are able to develop policies that are capable of adapting to very different dynamics, including ones that differ significantly from the dynamics on which the policies were trained. This adaptivity enables the policies to generalize to the dynamics of the real world without any training on the physical system. Our approach is demonstrated on an object pushing task using a robotic arm. Despite being trained exclusively in simulation, our policies are able to maintain a similar level of performance when deployed on a real robot, reliably moving an object to a desired location from random initial configurations. We explore the impact of various design decisions and show that the resulting policies are robust to significant calibration error.",True,True,"Peng, Xue Bin and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter",2018,,,,
Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy,2511.09737v1,procgen,\cite{procgen},Leveraging Procedural Generation to Benchmark Reinforcement Learning,https://arxiv.org/abs/1912.01588v2,"We introduce Procgen Benchmark, a suite of 16 procedurally generated game-like environments designed to benchmark both sample efficiency and generalization in reinforcement learning. We believe that the community will benefit from increased access to high quality training environments, and we provide detailed experimental protocols for using this benchmark. We empirically demonstrate that diverse environment distributions are essential to adequately train and evaluate RL agents, thereby motivating the extensive use of procedural content generation. We then use this benchmark to investigate the effects of scaling model size, finding that larger models significantly improve both sample efficiency and generalization.",True,True,"Cobbe, Karl and Hesse, Chris and Hilton, Jacob and Schulman, John",2020,,,,
Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy,2511.09737v1,gisslen2021adversarial,\cite{gisslen2021adversarial},{Adversarial Reinforcement Learning for Procedural Content Generation},,,True,False,"Gissl{\'e}n, Linus and Eakins, Andy and Gordillo, Camilo and Bergdahl, Joakim and Tollmar, Konrad",2021,,,,
Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy,2511.09737v1,yu2017preparing,\cite{yu2017preparing},{Preparing for the Unknown: Learning a Universal Policy with Online System Identification},,,True,False,"Yu, Wenhao and Tan, Jie and Liu, C Karen and Turk, Greg",2017,,,,Robotics: Science and Systems
Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy,2511.09737v1,laskin2020reinforcement,\cite{laskin2020reinforcement},Reinforcement learning with augmented data,,,True,False,"Laskin, Misha and Lee, Kimin and Stooke, Adam and Pinto, Lerrel and Abbeel, Pieter and Srinivas, Aravind",2020,,,,Advances in neural information processing systems
Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy,2511.09737v1,hansen2021stabilizing,\cite{hansen2021stabilizing},{Stabilizing Deep Q-Learning with ConvNets and Vision Transformers under Data Augmentation},,,True,False,"Hansen, Nicklas and Su, Hao and Wang, Xiaolong",2021,,,,Neural Information Processing Systems
Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy,2511.09737v1,Wang_Wu_Hu_Wang_Lin_Lv_2024,\cite{Wang_Wu_Hu_Wang_Lin_Lv_2024},{What Effects the Generalization in Visual Reinforcement Learning: Policy Consistency with Truncated Return Prediction},,,True,False,"Wang, Shuo and Wu, Zhihao and Hu, Xiaobo and Wang, Jinwen and Lin, Youfang and Lv, Kai",2024,Mar.,https://ojs.aaai.org/index.php/AAAI/article/view/28369,10.1609/aaai.v38i6.28369,Proceedings of the AAAI Conference on Artificial Intelligence
Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy,2511.09737v1,grooten2024madi,\cite{grooten2024madi},{MaDi: Learning to Mask Distractions for Generalization in Visual Deep Reinforcement Learning},,,True,False,"Grooten, Bram and Tomilin, Tristan and Vasan, Gautham and Taylor, Matthew E and Mahmood, A Rupam and Fang, Meng and Pechenizkiy, Mykola and Mocanu, Decebal Constantin",2024,,,,Autonomous Agents and Multiagent Systems
Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy,2511.09737v1,huang2022spectrum,\cite{huang2022spectrum},{Spectrum Random Masking for Generalization in Image-based Reinforcement Learning},,,True,False,"Huang, Yangru and Peng, Peixi and Zhao, Yifan and Chen, Guangyao and Tian, Yonghong",2022,,,,Neural Information Processing Systems
Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy,2511.09737v1,wang2016learning,\cite{wang2016learning},Learning to reinforcement learn,https://arxiv.org/abs/1611.05763v3,"In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.",True,True,"Wang, Jane X and Kurth-Nelson, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt",2016,,,,arXiv preprint arXiv:1611.05763
Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy,2511.09737v1,rabinowitz2018machine,\cite{rabinowitz2018machine},Machine Theory of Mind,https://arxiv.org/abs/1802.07740v2,"Theory of mind (ToM; Premack & Woodruff, 1978) broadly refers to humans' ability to represent the mental states of others, including their desires, beliefs, and intentions. We propose to train a machine to build such models too. We design a Theory of Mind neural network -- a ToMnet -- which uses meta-learning to build models of the agents it encounters, from observations of their behaviour alone. Through this process, it acquires a strong prior model for agents' behaviour, as well as the ability to bootstrap to richer predictions about agents' characteristics and mental states using only a small number of behavioural observations. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep reinforcement learning agents from varied populations, and that it passes classic ToM tasks such as the ""Sally-Anne"" test (Wimmer & Perner, 1983; Baron-Cohen et al., 1985) of recognising that others can hold false beliefs about the world. We argue that this system -- which autonomously learns how to model other agents in its world -- is an important step forward for developing multi-agent AI systems, for building intermediating technology for machine-human interaction, and for advancing the progress on interpretable AI.",True,True,"Rabinowitz, Neil and Perbet, Frank and Song, Francis and Zhang, Chiyuan and Eslami, SM Ali and Botvinick, Matthew",2018,,,,
Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy,2511.09737v1,duan2016rl,\cite{duan2016rl},RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning,https://arxiv.org/abs/1611.02779v2,"Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a ""fast"" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL$^2$, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose (""slow"") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the ""fast"" RL algorithm on the current (previously unseen) MDP. We evaluate RL$^2$ experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL$^2$ is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL$^2$ on a vision-based navigation task and show that it scales up to high-dimensional problems.",True,True,"Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L and Sutskever, Ilya and Abbeel, Pieter",2016,,,,arXiv preprint arXiv:1611.02779
Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy,2511.09737v1,finn2017model,\cite{finn2017model},{Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},,,True,False,"Finn, Chelsea and Abbeel, Pieter and Levine, Sergey",2017,,,,
Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy,2511.09737v1,beck2023hypernetworks,\cite{beck2023hypernetworks},Hypernetworks in Meta-Reinforcement Learning,https://arxiv.org/abs/2210.11348v1,"Training a reinforcement learning (RL) agent on a real-world robotics task remains generally impractical due to sample inefficiency. Multi-task RL and meta-RL aim to improve sample efficiency by generalizing over a distribution of related tasks. However, doing so is difficult in practice: In multi-task RL, state of the art methods often fail to outperform a degenerate solution that simply learns each task separately. Hypernetworks are a promising path forward since they replicate the separate policies of the degenerate solution while also allowing for generalization across tasks, and are applicable to meta-RL. However, evidence from supervised learning suggests hypernetwork performance is highly sensitive to the initialization. In this paper, we 1) show that hypernetwork initialization is also a critical factor in meta-RL, and that naive initializations yield poor performance; 2) propose a novel hypernetwork initialization scheme that matches or exceeds the performance of a state-of-the-art approach proposed for supervised settings, as well as being simpler and more general; and 3) use this method to show that hypernetworks can improve performance in meta-RL by evaluating on multiple simulated robotics benchmarks.",True,True,"Beck, Jacob and Jackson, Matthew Thomas and Vuorio, Risto and Whiteson, Shimon",2023,,,,
Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy,2511.09737v1,rezaei2023hypernetworks,\cite{rezaei2023hypernetworks},{Hypernetworks for Zero-shot Transfer in Reinforcement Learning},,,True,False,"Rezaei-Shoshtari, Sahand and Morissette, Charlotte and Hogan, Francois R and Dudek, Gregory and Meger, David",2023,,,,
Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models,2511.08577v1,li2025implicit,\cite{li2025implicit},Implicit Reasoning in Large Language Models: A Comprehensive Survey,https://arxiv.org/abs/2509.02350v1,"Large Language Models (LLMs) have demonstrated strong generalization across a wide range of tasks. Reasoning with LLMs is central to solving multi-step problems and complex decision-making. To support efficient reasoning, recent studies have shifted attention from explicit chain-of-thought prompting toward implicit reasoning, where reasoning occurs silently via latent structures without emitting intermediate textual steps. Implicit reasoning brings advantages such as lower generation cost, faster inference, and better alignment with internal computation. Although prior surveys have discussed latent representations in the context of reasoning, a dedicated and mechanism-level examination of how reasoning unfolds internally within LLMs remains absent. This survey fills that gap by introducing a taxonomy centered on execution paradigms, shifting the focus from representational forms to computational strategies. We organize existing methods into three execution paradigms based on \textbf{\textit{how and where internal computation unfolds}}: latent optimization, signal-guided control, and layer-recurrent execution. We also review structural, behavioral and representation-based evidence that supports the presence of implicit reasoning in LLMs. We further provide a structured overview of the evaluation metrics and benchmarks used in existing works to assess the effectiveness and reliability of implicit reasoning. We maintain a continuously updated project at: https://github.com/digailab/awesome-llm-implicit-reasoning.",True,True,"Li, Jindong and Fu, Yali and Fan, Li and Liu, Jiahong and Shu, Yao and Qin, Chengwei and Yang, Menglin and King, Irwin and Ying, Rex",2025,,,,arXiv preprint arXiv:2509.02350
Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models,2511.08577v1,pfau2404let,\cite{pfau2404let},Let's Think Dot by Dot: Hidden Computation in Transformer Language Models,https://arxiv.org/abs/2404.15758v1,"Chain-of-thought responses from language models improve performance across most benchmarks. However, it remains unclear to what extent these performance gains can be attributed to human-like task decomposition or simply the greater computation that additional tokens allow. We show that transformers can use meaningless filler tokens (e.g., '......') in place of a chain of thought to solve two hard algorithmic tasks they could not solve when responding without intermediate tokens. However, we find empirically that learning to use filler tokens is difficult and requires specific, dense supervision to converge. We also provide a theoretical characterization of the class of problems where filler tokens are useful in terms of the quantifier depth of a first-order formula. For problems satisfying this characterization, chain-of-thought tokens need not provide information about the intermediate computational steps involved in multi-token computations. In summary, our results show that additional tokens can provide computational benefits independent of token choice. The fact that intermediate tokens can act as filler tokens raises concerns about large language models engaging in unauditable, hidden computations that are increasingly detached from the observed chain-of-thought tokens.",True,True,"Pfau, Jacob and Merrill, William and Bowman, Samuel R",2024,,,,URL https://arxiv. org/abs/2404.15758
Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models,2511.08577v1,goyal2310think,\cite{goyal2310think},Think before you speak: Training Language Models With Pause Tokens,https://arxiv.org/abs/2310.02226v3,"Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\%$ EM score on the QA task of SQuAD, $8\%$ on CommonSenseQA and $1\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",True,True,"Goyal, Sachin and Ji, Ziwei and Rawat, Ankit Singh and Menon, Aditya Krishna and Kumar, Sanjiv and Nagarajan, Vaishnavh",2024,,,,URL https://arxiv. org/abs/2310.02226
Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models,2511.08577v1,kim2025learning,\cite{kim2025learning},Learning to Insert [PAUSE] Tokens for Better Reasoning,,,True,False,"Kim, Eunki and Kim, Sangryul and Thorne, James",2025,,,,arXiv preprint arXiv:2506.03616
Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models,2511.08577v1,hao2024training,\cite{hao2024training},Training large language models to reason in a continuous latent space,,,True,False,"Hao, Shibo and Sukhbaatar, Sainbayar and Su, DiJia and Li, Xian and Hu, Zhiting and Weston, Jason and Tian, Yuandong",2024,,,,arXiv preprint arXiv:2412.06769
Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models,2511.08577v1,cheng2024compressed,\cite{cheng2024compressed},Compressed Chain of Thought: Efficient Reasoning Through Dense Representations,https://arxiv.org/abs/2412.13171v1,"Chain-of-thought (CoT) decoding enables language models to improve reasoning performance at the cost of high generation latency in decoding. Recent proposals have explored variants of contemplation tokens, a term we introduce that refers to special tokens used during inference to allow for extra computation. Prior work has considered fixed-length sequences drawn from a discrete set of embeddings as contemplation tokens. Here we propose Compressed Chain-of-Thought (CCoT), a framework to generate contentful and continuous contemplation tokens of variable sequence length. The generated contemplation tokens are compressed representations of explicit reasoning chains, and our method can be applied to off-the-shelf decoder language models. Through experiments, we illustrate how CCoT enables additional reasoning over dense contentful representations to achieve corresponding improvements in accuracy. Moreover, the reasoning improvements can be adaptively modified on demand by controlling the number of contemplation tokens generated.",True,True,"Cheng, Jeffrey and Van Durme, Benjamin",2024,,,,arXiv preprint arXiv:2412.13171
Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models,2511.08577v1,su2025token,\cite{su2025token},Token assorted: Mixing latent and text tokens for improved language model reasoning,,,True,False,"Su, DiJia and Zhu, Hanlin and Xu, Yingchen and Jiao, Jiantao and Tian, Yuandong and Zheng, Qinqing",2025,,,,arXiv preprint arXiv:2502.03275
Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models,2511.08577v1,liu2024expediting,\cite{liu2024expediting},Expediting and elevating large language model reasoning via hidden chain-of-thought decoding,,,True,False,"Liu, Tianqiao and Chen, Zui and Liu, Zitao and Tian, Mi and Luo, Weiqi",2024,,,,arXiv preprint arXiv:2409.08561
Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models,2511.08577v1,zhang2025soft,\cite{zhang2025soft},Soft thinking: Unlocking the reasoning potential of llms in continuous concept space,,,True,False,"Zhang, Zhen and He, Xuehai and Yan, Weixiang and Shen, Ao and Zhao, Chenyang and Wang, Shuohang and Shen, Yelong and Wang, Xin Eric",2025,,,,arXiv preprint arXiv:2505.15778
Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models,2511.08577v1,saunshi2025loopedTrans,\cite{saunshi2025loopedTrans},Reasoning with latent thoughts: On the power of looped transformers,,,True,False,"Saunshi, Nikunj and Dikkala, Nishanth and Li, Zhiyuan and Kumar, Sanjiv and Reddi, Sashank J",2025,,,,arXiv preprint arXiv:2502.17416
Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models,2511.08577v1,geiping2025scaling,\cite{geiping2025scaling},Scaling up test-time compute with latent reasoning: A recurrent depth approach,,,True,False,"Geiping, Jonas and McLeish, Sean and Jain, Neel and Kirchenbauer, John and Singh, Siddharth and Bartoldson, Brian R and Kailkhura, Bhavya and Bhatele, Abhinav and Goldstein, Tom",2025,,,,arXiv preprint arXiv:2502.05171
Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models,2511.08577v1,zeng2025pondering,\cite{zeng2025pondering},Pretraining Language Models to Ponder in Continuous Space,https://arxiv.org/abs/2505.20674v2,"Humans ponder before articulating complex sentence elements, enabling deeper cognitive processing through focused effort. In this work, we introduce this pondering process into language models by repeatedly invoking the forward process within a single token generation step. During pondering, instead of generating an actual token sampled from the prediction distribution, the model ponders by yielding a weighted sum of all token embeddings according to the predicted token distribution. The generated embedding is then fed back as input for another forward pass. We show that the model can learn to ponder in this way through self-supervised learning, without any human annotations. Experiments across three widely used open-source architectures-GPT-2, Pythia, and LLaMA-and extensive downstream task evaluations demonstrate the effectiveness and generality of our method. For language modeling tasks, pondering language models achieve performance comparable to vanilla models with twice the number of parameters. On 9 downstream benchmarks, our pondering-enhanced Pythia models significantly outperform the official Pythia models. Notably, PonderingPythia-2.8B surpasses Pythia-6.9B, and PonderingPythia-1B is comparable to TinyLlama-1.1B, which is trained on 10 times more data. The code is available at https://github.com/LUMIA-Group/PonderingLM.",True,True,"Zeng, Boyi and Song, Shixiang and Huang, Siyuan and Wang, Yixuan and Li, He and He, Ziwei and Wang, Xinbing and Li, Zhiyu and Lin, Zhouhan",2025,,,,arXiv preprint arXiv:2505.20674
Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models,2511.08577v1,bae2025mor,\cite{bae2025mor},Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation,https://arxiv.org/abs/2507.10524v3,"Scaling language models unlocks impressive capabilities, but the accompanying computational and memory demands make both training and deployment expensive. Existing efficiency efforts typically target either parameter sharing or adaptive computation, leaving open the question of how to attain both simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework that combines the two axes of efficiency inside a single Recursive Transformer. MoR reuses a shared stack of layers across recursion steps to achieve parameter efficiency, while lightweight routers enable adaptive token-level thinking by dynamically assigning different recursion depths to individual tokens. This allows MoR to focus quadratic attention computation only among tokens still active at a given recursion depth, further improving memory access efficiency by selectively caching only their key-value pairs. Beyond these core mechanisms, we also propose a KV sharing variant that reuses KV pairs from the first recursion, specifically designed to further decrease memory footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms a new Pareto frontier: at equal training FLOPs and smaller model sizes, it significantly lowers validation perplexity and improves few-shot accuracy, while delivering higher throughput compared with vanilla and existing recursive baselines. These gains demonstrate that MoR is an effective path towards large-model quality without incurring large-model cost.",True,True,"Bae, Sangmin and Kim, Yujin and Bayat, Reza and Kim, Sungnyun and Ha, Jiyoun and Schuster, Tal and Fisch, Adam and Harutyunyan, Hrayr and Ji, Ziwei and Courville, Aaron and others",2025,,,,arXiv preprint arXiv:2507.10524
Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models,2511.08577v1,zhu2025ouro,\cite{zhu2025ouro},Scaling Latent Reasoning via Looped Language Models,https://arxiv.org/abs/2510.25741v2,"Modern LLMs are trained to ""think"" primarily via explicit text generation, such as chain-of-thought (CoT), which defers reasoning to post-training and under-leverages pre-training data. We present and open-source Ouro, named after the recursive Ouroboros, a family of pre-trained Looped Language Models (LoopLM) that instead build reasoning into the pre-training phase through (i) iterative computation in latent space, (ii) an entropy-regularized objective for learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and 2.6B models enjoy superior performance that match the results of up to 12B SOTA LLMs across a wide range of benchmarks. Through controlled experiments, we show this advantage stems not from increased knowledge capacity, but from superior knowledge manipulation capabilities. We also show that LoopLM yields reasoning traces more aligned with final outputs than explicit CoT. We hope our results show the potential of LoopLM as a novel scaling direction in the reasoning era. Our model is available here: http://ouro-llm.github.io.",True,True,"Zhu, Rui-Jie and Wang, Zixuan and Hua, Kai and Zhang, Tianyu and Li, Ziniu and Que, Haoran and Wei, Boyi and Wen, Zixin and Yin, Fan and Xing, He and others",2025,,,,arXiv preprint arXiv:2510.25741
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,prajna2007framework,\cite{prajna2007framework},A Framework for Worst-Case and Stochastic Safety Verification Using Barrier Certificates,,,True,False,S. Prajna and A. Jadbabaie and G. J. Pappas,2007,,,,IEEE Transactions on Automatic Control
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,ames2019control,\cite{ames2019control},Control barrier functions: Theory and applications,,,True,False,"Ames, Aaron D and Coogan, Samuel and Egerstedt, Magnus and Notomista, Gennaro and Sreenath, Koushil and Tabuada, Paulo",2019,,,,
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,wooding2025protect,\cite{wooding2025protect},PRoTECT: Parallelized Construction of Safety Barrier Certificates for Nonlinear Polynomial Systems,https://arxiv.org/abs/2404.14804v2,"We develop an open-source software tool, called PRoTECT, for the parallelized construction of safety barrier certificates (BCs) for nonlinear polynomial systems. This tool employs sum-of-squares (SOS) optimization programs to systematically search for polynomial-type BCs, while aiming to verify safety properties over four classes of dynamical systems: (i) discrete-time stochastic systems, (ii) discrete-time deterministic systems, (iii) continuous-time stochastic systems, and (iv) continuous-time deterministic systems. PRoTECT is implemented in Python as an application programming interface (API), offering users the flexibility to interact either through its user-friendly graphic user interface (GUI) or via function calls from other Python programs. PRoTECT leverages parallelism across different barrier degrees to efficiently search for a feasible BC.",True,True,B. Wooding and V. Horbanov and A. Lavaei,2025,,,,arXiv preprint arXiv:2404.14804
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,kordabad2025sum,\cite{kordabad2025sum},Sum-of-Squares Certificates for Almost-Sure Reachability of Stochastic Polynomial Systems,,,True,False,"Kordabad, Arash Bahari and Majumdar, Rupak and Soudjani, Sadegh",2025,,,,arXiv preprint arXiv:2510.25513
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,abate2021fossil,\cite{abate2021fossil},FOSSIL: A Software Tool for the Formal Synthesis of {L}yapunov Functions and Barrier Certificates Using Neural Networks,,,True,False,A. Abate and D. Ahmed and A. Edwards and M. Giacobbe and A. Peruffo,2021,,,,
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,lindemann2018control,\cite{lindemann2018control},Control barrier functions for signal temporal logic tasks,,,True,False,"Lindemann, Lars and Dimarogonas, Dimos V",2018,,,,IEEE control systems letters
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,jagtap2020formal,\cite{jagtap2020formal},Formal Synthesis of Stochastic Systems via Control Barrier Certificates,https://arxiv.org/abs/1905.04585v2,"This paper focuses on synthesizing control policies for discrete-time stochastic control systems together with a lower bound on the probability that the systems satisfy the complex temporal properties. The desired properties of the system are expressed as linear temporal logic (LTL) specifications over finite traces. In particular, our approach decomposes the given specification into simpler reachability tasks based on its automata representation. We then propose the use of so-called \emph{control barrier certificate} to solve those simpler reachability tasks along with computing the corresponding controllers and probability bounds. Finally, we combine those controllers to obtain a hybrid control policy solving the considered problem. Under some assumptions, we also provide two systematic approaches for uncountable and finite input sets to search for control barrier certificates. We demonstrate the effectiveness of the proposed approach on a room temperature control and lane-keeping of a vehicle modeled as a four-dimensional single-track kinematic model. We compare our results with the discretization-based methods in the literature.",True,True,"Jagtap, Pushpak and Soudjani, Sadegh and Zamani, Majid",2020,,,,IEEE Transactions on Automatic Control
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,majumdar2024necessary,\cite{majumdar2024necessary},Necessary and sufficient certificates for almost sure reachability,,,True,False,"Majumdar, Rupak and Sathiyanarayana, VR and Soudjani, Sadegh",2024,,,,IEEE Control Systems Letters
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,nejati2023formal,\cite{nejati2023formal},Formal Verification of Unknown Discrete- and Continuous-Time Systems: A Data-Driven Approach,,,True,False,A. Nejati and A. Lavaei and P. Jagtap and S. Soudjani and M. Zamani,2023,,,,IEEE Transactions on Automatic Control
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,Jagtap2020CBCGP,\cite{Jagtap2020CBCGP},Control Barrier Functions for Unknown Nonlinear Systems using {G}aussian Processes,,,True,False,"Jagtap, Pushpak and Pappas, George J. and Zamani, Majid",2020,oct,,,
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,Cohen2022,\cite{Cohen2022},Robust Control Barrier Functions for Nonlinear Control Systems with Uncertainty: A Duality-based Approach,,,True,False,"Cohen, Max H. and Belta, Calin and Tron, Roberto",2022,aug,,,
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,Lopez2022uCBF,\cite{Lopez2022uCBF},Unmatched Control Barrier Functions: Certainty Equivalence Adaptive Safety,https://arxiv.org/abs/2207.13873v2,"This work applies universal adaptive control to control barrier functions to achieve forward invariance of a safe set despite the presence of unmatched parametric uncertainties. The approach combines two ideas. The first is to construct a family of control barrier functions that ensures the system is safe for all possible models. The second is to use online parameter adaptation to methodically select a control barrier function and corresponding safety controller from the allowable set. While such a combination does not necessarily yield forward invariance without additional requirements on the barrier function, we show that such invariance can be established by simply adjusting the adaptation gain online. It is also shown that the developed method is applicable to systems with safety constraints that have a relative degree greater than one. This work thus represents the first adaptive safety approach that successfully employs the certainty equivalence principle for general state constraints without sacrificing safety guarantees.",True,True,"Lopez, Brett T. and Slotine, Jean-Jacques E.",2023,,,,
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,Wang2018CBF,\cite{Wang2018CBF},Safe Learning of Quadrotor Dynamics Using Barrier Certificates,https://arxiv.org/abs/1710.05472v1,"To effectively control complex dynamical systems, accurate nonlinear models are typically needed. However, these models are not always known. In this paper, we present a data-driven approach based on Gaussian processes that learns models of quadrotors operating in partially unknown environments. What makes this challenging is that if the learning process is not carefully controlled, the system will go unstable, i.e., the quadcopter will crash. To this end, barrier certificates are employed for safe learning. The barrier certificates establish a non-conservative forward invariant safe region, in which high probability safety guarantees are provided based on the statistics of the Gaussian Process. A learning controller is designed to efficiently explore those uncertain states and expand the barrier certified safe region based on an adaptive sampling scheme. In addition, a recursive Gaussian Process prediction method is developed to learn the complex quadrotor dynamics in real-time. Simulation results are provided to demonstrate the effectiveness of the proposed approach.",True,True,"Wang, Li and Theodorou, Evangelos A and Egerstedt, Magnus",2018,,,,
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,Chekan2023UncertainConstraints,\cite{Chekan2023UncertainConstraints},Safety-Aware Learning-Based Control of Systems with Uncertainty Dependent Constraints,,,True,False,"Chekan, Jafar Abbaszadeh and Langbort, Cédric",2023,,,,
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,mathiesen2024data,\cite{mathiesen2024data},A data-driven approach for safety quantification of non-linear stochastic systems with unknown additive noise distribution,,,True,False,"Mathiesen, Frederik Baymler and Romao, Licio and Calvert, Simeon C and Laurenti, Luca and Abate, Alessandro",2024,,,,arXiv preprint arXiv:2410.06662
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,mazouz2024data,\cite{mazouz2024data},Data-Driven Permissible Safe Control with Barrier Certificates,https://arxiv.org/abs/2405.00136v2,"This paper introduces a method of identifying a maximal set of safe strategies from data for stochastic systems with unknown dynamics using barrier certificates. The first step is learning the dynamics of the system via Gaussian process (GP) regression and obtaining probabilistic errors for this estimate. Then, we develop an algorithm for constructing piecewise stochastic barrier functions to find a maximal permissible strategy set using the learned GP model, which is based on sequentially pruning the worst controls until a maximal set is identified. The permissible strategies are guaranteed to maintain probabilistic safety for the true system. This is especially important for learning-enabled systems, because a rich strategy space enables additional data collection and complex behaviors while remaining safe. Case studies on linear and nonlinear systems demonstrate that increasing the size of the dataset for learning the system grows the permissible strategy set.",True,True,"Mazouz, Rayan and Skovbekk, John and Mathiesen, Frederik Baymler and Frew, Eric and Laurenti, Luca and Lahijanian, Morteza",2024,,,,
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,Salamati2021DDCBC,\cite{Salamati2021DDCBC},Data-driven verification and synthesis of stochastic systems via barrier certificates,,,True,False,"Salamati, Ali and Lavaei, Abolfazl and Soudjani, Sadegh and Zamani, Majid",2024,,,,Automatica
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,wang2023stochastic,\cite{wang2023stochastic},Stochastic Control Barrier Functions with {Bayesian} Inference for Unknown Stochastic Differential Equations,,,True,False,"Wang, Chuanzheng and Meng, Yiming and Liu, Jun and Smith, Stephen",2023,,,,arXiv preprint arXiv:2312.12759
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,schon2024DRObarrier,\cite{schon2024DRObarrier},Data-Driven Distributionally Robust Safety Verification Using Barrier Certificates and Conditional Mean Embeddings,https://arxiv.org/abs/2403.10497v1,"Algorithmic verification of realistic systems to satisfy safety and other temporal requirements has suffered from poor scalability of the employed formal approaches. To design systems with rigorous guarantees, many approaches still rely on exact models of the underlying systems. Since this assumption can rarely be met in practice, models have to be inferred from measurement data or are bypassed completely. Whilst former usually requires the model structure to be known a-priori and immense amounts of data to be available, latter gives rise to a plethora of restrictive mathematical assumptions about the unknown dynamics. In a pursuit of developing scalable formal verification algorithms without shifting the problem to unrealistic assumptions, we employ the concept of barrier certificates, which can guarantee safety of the system, and learn the certificate directly from a compact set of system trajectories. We use conditional mean embeddings to embed data from the system into a reproducing kernel Hilbert space (RKHS) and construct an RKHS ambiguity set that can be inflated to robustify the result w.r.t. a set of plausible transition kernels. We show how to solve the resulting program efficiently using sum-of-squares optimization and a Gaussian process envelope. Our approach lifts the need for restrictive assumptions on the system dynamics and uncertainty, and suggests an improvement in the sample complexity of verifying the safety of a system on a tested case study compared to a state-of-the-art approach.",True,True,"Schön, Oliver and Zhong, Zhengang and Soudjani, Sadegh",2024,,,,
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,lucid,\cite{lucid},"{LUCID}: Learning-Enabled Uncertainty-Aware
Certification of Stochastic Dynamical Systems",,,True,False,"Casablanca, Ernesto and Sch\""on, Oliver and Zuliani, Paolo and Soudjani, Sadegh",2026,,,,
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,achiam2023gpt,\cite{achiam2023gpt},GPT-4 Technical Report,https://arxiv.org/abs/2303.08774v6,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",True,True,J. Achiam and S. Adler and S. Agarwal and L. Ahmad and I. Akkaya and F. L. Aleman and D. Almeida and J. Altenschmidt and S. Altman and S. Anadkat and R. Avila,2023,,,,arXiv preprint arXiv:2303.08774
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,zhao2023survey,\cite{zhao2023survey},Large Language Models: A Survey,https://arxiv.org/abs/2402.06196v3,"Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws \cite{kaplan2020scaling,hoffmann2022training}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we conclude the paper by discussing open challenges and future research directions.",True,True,W. X. Zhao and K. Zhou and J. Li and T. Tang and X. Wang and Y. Hou and Y. Min and B. Zhang and J. Zhang and Z. Dong and Y. Du,2023,,,,arXiv preprint arXiv:2303.18223
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,dale2021gpt3,\cite{dale2021gpt3},GPT-3: What's It Good For?,,,True,False,R. Dale,2021,,,,Natural Language Engineering
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,sanderson2023gpt4,\cite{sanderson2023gpt4},GPT-4 is Here: What Scientists Think,,,True,False,K. Sanderson,2023,,,,Nature
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,xu2024llm,\cite{xu2024llm},"{LLM}-Enabled Cyber-Physical Systems: Survey, Research Opportunities, and Challenges",,,True,False,W. Xu and M. Liu and O. Sokolsky and I. Lee and F. Kong,2024,,,,
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,jansen2020visually,\cite{jansen2020visually},Visually-Grounded Planning without Vision: Language Models Infer Detailed Plans from High-level Instructions,https://arxiv.org/abs/2009.14259v2,"The recently proposed ALFRED challenge task aims for a virtual robotic agent to complete complex multi-step everyday tasks in a virtual home environment from high-level natural language directives, such as ""put a hot piece of bread on a plate"". Currently, the best-performing models are able to complete less than 5% of these tasks successfully. In this work we focus on modeling the translation problem of converting natural language directives into detailed multi-step sequences of actions that accomplish those goals in the virtual environment. We empirically demonstrate that it is possible to generate gold multi-step plans from language directives alone without any visual input in 26% of unseen cases. When a small amount of visual information is incorporated, namely the starting location in the virtual environment, our best-performing GPT-2 model successfully generates gold command sequences in 58% of cases. Our results suggest that contextualized language models may provide strong visual semantic planning modules for grounded virtual agents.",True,True,P. A. Jansen,2020,,,,
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,lin2023text2motion,\cite{lin2023text2motion},Text2Motion: From Natural Language Instructions to Feasible Plans,https://arxiv.org/abs/2303.12153v5,"We propose Text2Motion, a language-based planning framework enabling robots to solve sequential manipulation tasks that require long-horizon reasoning. Given a natural language instruction, our framework constructs both a task- and motion-level plan that is verified to reach inferred symbolic goals. Text2Motion uses feasibility heuristics encoded in Q-functions of a library of skills to guide task planning with Large Language Models. Whereas previous language-based planners only consider the feasibility of individual skills, Text2Motion actively resolves geometric dependencies spanning skill sequences by performing geometric feasibility planning during its search. We evaluate our method on a suite of problems that require long-horizon reasoning, interpretation of abstract goals, and handling of partial affordance perception. Our experiments show that Text2Motion can solve these challenging problems with a success rate of 82%, while prior state-of-the-art language-based planning methods only achieve 13%. Text2Motion thus provides promising generalization characteristics to semantically diverse sequential manipulation tasks with geometric dependencies between skills.",True,True,K. Lin and C. Agia and T. Migimatsu and M. Pavone and J. Bohg,2023,,,,Autonomous Robots
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,rana2023sayplan,\cite{rana2023sayplan},SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning,https://arxiv.org/abs/2307.06135v2,"Large language models (LLMs) have demonstrated impressive results in developing generalist planning agents for diverse tasks. However, grounding these plans in expansive, multi-floor, and multi-room environments presents a significant challenge for robotics. We introduce SayPlan, a scalable approach to LLM-based, large-scale task planning for robotics using 3D scene graph (3DSG) representations. To ensure the scalability of our approach, we: (1) exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a 'semantic search' for task-relevant subgraphs from a smaller, collapsed representation of the full graph; (2) reduce the planning horizon for the LLM by integrating a classical path planner and (3) introduce an 'iterative replanning' pipeline that refines the initial plan using feedback from a scene graph simulator, correcting infeasible actions and avoiding planning failures. We evaluate our approach on two large-scale environments spanning up to 3 floors and 36 rooms with 140 assets and objects and show that our approach is capable of grounding large-scale, long-horizon task plans from abstract, and natural language instruction for a mobile manipulator robot to execute. We provide real robot video demonstrations on our project page https://sayplan.github.io.",True,True,K. Rana and J. Haviland and S. Garg and J. Abou-Chakra and I. Reid and N. Suenderhauf,2023,,,,
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,cui2023large,\cite{cui2023large},Large Language Models for Autonomous Driving: Real-World Experiments,,,True,False,C. Cui and Z. Yang and Y. Zhou and Y. Ma and J. Lu and Z. Wang,2023,,,,arXiv preprint arXiv:2312.09397
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,lewkowycz2022solving,\cite{lewkowycz2022solving},Solving Quantitative Reasoning Problems with Language Models,,,True,False,A. Lewkowycz and A. Andreassen and D. Dohan and E. Dyer and H. Michalewski and V. Ramasesh and A. Slone and C. Anil and I. Schlag and T. Gutman-Solo and others,2022,,,,
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,ahn2022saycan,\cite{ahn2022saycan},"Do as {I} Can, Not as {I} Say: Grounding Language in Robotic Affordances",,,True,False,Michael Ahn and Anthony Brohan and Noah Brown and Yevgen Chebotar and Omar Cortes and Byron David and Chelsea Finn and Chuyuan Fu and Keerthana Gopalakrishnan and Karol Hausman and Alex Herzog and Daniel Ho and Jasmine Hsu and Julian Ibarz and Brian Ichter and Alex Irpan and Eric Jang and Rosario Jauregui Ruano and Kyle Jeffrey and Sally Jesmonth and Nikhil J. Joshi and Ryan Julian and Dmitry Kalashnikov and Yuheng Kuang and Kuang-Huei Lee and Sergey Levine and Yao Lu and Linda Luu and Carolina Parada and Peter Pastor and Jornell Quiambao and Kanishka Rao and Jarek Rettinghouse and Diego Reyes and Pierre Sermanet and Nicolas Sievers and Clayton Tan and Alexander Toshev and Vincent Vanhoucke and Fei Xia and Ted Xiao and Peng Xu and Sichun Xu and Mengyuan Yan and Andy Zeng,2022,,,,
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,zitkovich2023rt2,\cite{zitkovich2023rt2},{RT-2}: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control,,,True,False,B. Zitkovich and T. Yu and S. Xu and others,2023,,,,Conference on Robot Learning
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,yang2023drivegpt4,\cite{yang2023drivegpt4},DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model,https://arxiv.org/abs/2310.01412v5,"Multimodal large language models (MLLMs) have emerged as a prominent area of interest within the research community, given their proficiency in handling and reasoning with non-textual data, including images and videos. This study seeks to extend the application of MLLMs to the realm of autonomous driving by introducing DriveGPT4, a novel interpretable end-to-end autonomous driving system based on LLMs. Capable of processing multi-frame video inputs and textual queries, DriveGPT4 facilitates the interpretation of vehicle actions, offers pertinent reasoning, and effectively addresses a diverse range of questions posed by users. Furthermore, DriveGPT4 predicts low-level vehicle control signals in an end-to-end fashion.These advanced capabilities are achieved through the utilization of a bespoke visual instruction tuning dataset, specifically tailored for autonomous driving applications, in conjunction with a mix-finetuning training strategy. DriveGPT4 represents the pioneering effort to leverage LLMs for the development of an interpretable end-to-end autonomous driving solution. Evaluations conducted on the BDD-X dataset showcase the superior qualitative and quantitative performance of DriveGPT4. Additionally, the fine-tuning of domain-specific data enables DriveGPT4 to yield close or even improved results in terms of autonomous driving grounding when contrasted with GPT4-V.",True,True,Z. Yang and S. S. Raman and A. Shah and S. Tellex,2023,,,,arXiv preprint arXiv:2310.01412
BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems,2511.09363v1,bayat2025llm,\cite{bayat2025llm},{LLM}-Enhanced Symbolic Control for Safety-Critical Applications,,,True,False,"Amir Bayat and Alessandro Abate and Necmiye Ozay and Rapha{\""e}l M. Jungers",2025,,,,
SSR: Socratic Self-Refine for Large Language Model Reasoning,2511.10621v1,kuhn2023semantic,\cite{kuhn2023semantic},Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation,,,True,False,"Kuhn, Lorenz and Gal, Yarin and Farquhar, Sebastian",2023,,,,
SSR: Socratic Self-Refine for Large Language Model Reasoning,2511.10621v1,manakul2023selfcheckgpt,\cite{manakul2023selfcheckgpt},SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models,https://arxiv.org/abs/2303.08896v3,"Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose ""SelfCheckGPT"", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.",True,True,"Manakul, Potsawee and Liusie, Adian and Gales, Mark JF",2023,,,,arXiv preprint arXiv:2303.08896
SSR: Socratic Self-Refine for Large Language Model Reasoning,2511.10621v1,kang2025scalable,\cite{kang2025scalable},Scalable best-of-n selection for large language models via self-certainty,,,True,False,"Kang, Zhewei and Zhao, Xuandong and Song, Dawn",2025,,,,arXiv preprint arXiv:2502.18581
SSR: Socratic Self-Refine for Large Language Model Reasoning,2511.10621v1,fu2025deep,\cite{fu2025deep},Deep think with confidence,,,True,False,"Fu, Yichao and Wang, Xuewei and Tian, Yuandong and Zhao, Jiawei",2025,,,,arXiv preprint arXiv:2508.15260
SSR: Socratic Self-Refine for Large Language Model Reasoning,2511.10621v1,zhang2025token,\cite{zhang2025token},Token-Level Uncertainty Estimation for Large Language Model Reasoning,,,True,False,"Zhang, Tunyu and Shi, Haizhou and Wang, Yibin and Wang, Hengyi and He, Xiaoxiao and Li, Zhuowei and Chen, Haoxian and Han, Ligong and Xu, Kai and Zhang, Huan and others",2025,,,,arXiv preprint arXiv:2505.11737
SSR: Socratic Self-Refine for Large Language Model Reasoning,2511.10621v1,zheng2023judging,\cite{zheng2023judging},Judging llm-as-a-judge with mt-bench and chatbot arena,,,True,False,"Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others",2023,,,,Advances in neural information processing systems
SSR: Socratic Self-Refine for Large Language Model Reasoning,2511.10621v1,gu2024survey,\cite{gu2024survey},A Survey on LLM-as-a-Judge,https://arxiv.org/abs/2411.15594v6,"Accurate and consistent evaluation is crucial for decision-making across numerous fields, yet it remains a challenging task due to inherent subjectivity, variability, and scale. Large Language Models (LLMs) have achieved remarkable success across diverse domains, leading to the emergence of ""LLM-as-a-Judge,"" where LLMs are employed as evaluators for complex tasks. With their ability to process diverse data types and provide scalable, cost-effective, and consistent assessments, LLMs present a compelling alternative to traditional expert-driven evaluations. However, ensuring the reliability of LLM-as-a-Judge systems remains a significant challenge that requires careful design and standardization. This paper provides a comprehensive survey of LLM-as-a-Judge, addressing the core question: How can reliable LLM-as-a-Judge systems be built? We explore strategies to enhance reliability, including improving consistency, mitigating biases, and adapting to diverse assessment scenarios. Additionally, we propose methodologies for evaluating the reliability of LLM-as-a-Judge systems, supported by a novel benchmark designed for this purpose. To advance the development and real-world deployment of LLM-as-a-Judge systems, we also discussed practical applications, challenges, and future directions. This survey serves as a foundational reference for researchers and practitioners in this rapidly evolving field.",True,True,"Gu, Jiawei and Jiang, Xuhui and Shi, Zhichao and Tan, Hexiang and Zhai, Xuehao and Xu, Chengjin and Li, Wei and Shen, Yinghan and Ma, Shengjie and Liu, Honghao and others",2024,,,,arXiv preprint arXiv:2411.15594
SSR: Socratic Self-Refine for Large Language Model Reasoning,2511.10621v1,zhou2025evaluating,\cite{zhou2025evaluating},Evaluating judges as evaluators: The jetts benchmark of llm-as-judges as test-time scaling evaluators,,,True,False,"Zhou, Yilun and Xu, Austin and Wang, Peifeng and Xiong, Caiming and Joty, Shafiq",2025,,,,arXiv preprint arXiv:2504.15253
SSR: Socratic Self-Refine for Large Language Model Reasoning,2511.10621v1,ren2023self,\cite{ren2023self},Self-Evaluation Improves Selective Generation in Large Language Models,https://arxiv.org/abs/2312.09300v1,"Safe deployment of large language models (LLMs) may benefit from a reliable method for assessing their generated content to determine when to abstain or to selectively generate. While likelihood-based metrics such as perplexity are widely employed, recent research has demonstrated the limitations of using sequence-level probability estimates given by LLMs as reliable indicators of generation quality. Conversely, LLMs have demonstrated strong calibration at the token level, particularly when it comes to choosing correct answers in multiple-choice questions or evaluating true/false statements. In this work, we reformulate open-ended generation tasks into token-level prediction tasks, and leverage LLMs' superior calibration at the token level. We instruct an LLM to self-evaluate its answers, employing either a multi-way comparison or a point-wise evaluation approach, with the option to include a ``None of the above'' option to express the model's uncertainty explicitly. We benchmark a range of scoring methods based on self-evaluation and evaluate their performance in selective generation using TruthfulQA and TL;DR. Through experiments with PaLM-2 and GPT-3, we demonstrate that self-evaluation based scores not only improve accuracy, but also correlate better with the overall quality of generated content.",True,True,"Ren, Jie and Zhao, Yao and Vu, Tu and Liu, Peter J and Lakshminarayanan, Balaji",2023,,,,
SSR: Socratic Self-Refine for Large Language Model Reasoning,2511.10621v1,chen2025sets,\cite{chen2025sets},Sets: Leveraging self-verification and self-correction for improved test-time scaling,,,True,False,"Chen, Jiefeng and Ren, Jie and Chen, Xinyun and Yang, Chengrun and Sun, Ruoxi and Yoon, Jinsung and Ar{\i}k, Sercan {\""O}",2025,,,,arXiv preprint arXiv:2501.19306
SSR: Socratic Self-Refine for Large Language Model Reasoning,2511.10621v1,huang2025efficient,\cite{huang2025efficient},Efficient Test-Time Scaling via Self-Calibration,https://arxiv.org/abs/2503.00031v1,"Increasing test-time computation is a straightforward approach to enhancing the quality of responses in Large Language Models (LLMs). While Best-of-N sampling and Self-Consistency with majority voting are simple and effective, they require a fixed number of sampling responses for each query, regardless of its complexity. This could result in wasted computation for simpler questions and insufficient exploration for more challenging ones. In this work, we argue that model confidence of responses can be used for improving the efficiency of test-time scaling. Unfortunately, LLMs are known to be overconfident and provide unreliable confidence estimation. To address this limitation, we introduce Self-Calibration by distilling Self-Consistency-derived confidence into the model itself. This enables reliable confidence estimation at test time with one forward pass. We then design confidence-based efficient test-time scaling methods to handle queries of various difficulty, such as Early-Stopping for Best-of-N and Self-Consistency with calibrated confidence. Experiments on three LLMs across six datasets demonstrate the effectiveness of our approach. Specifically, applying confidence-based Early Stopping to Best-of-N improves MathQA accuracy from 81.0 to 83.6 with a sample budget of 16 responses, indicating the efficacy of confidence-based sampling strategy at inference time.",True,True,"Huang, Chengsong and Huang, Langlin and Leng, Jixuan and Liu, Jiacheng and Huang, Jiaxin",2025,,,,arXiv preprint arXiv:2503.00031
SSR: Socratic Self-Refine for Large Language Model Reasoning,2511.10621v1,zhong2025solve,\cite{zhong2025solve},Solve-Detect-Verify: Inference-Time Scaling with Flexible Generative Verifier,https://arxiv.org/abs/2505.11966v1,"Large Language Model (LLM) reasoning for complex tasks inherently involves a trade-off between solution accuracy and computational efficiency. The subsequent step of verification, while intended to improve performance, further complicates this landscape by introducing its own challenging trade-off: sophisticated Generative Reward Models (GenRMs) can be computationally prohibitive if naively integrated with LLMs at test-time, while simpler, faster methods may lack reliability. To overcome these challenges, we introduce FlexiVe, a novel generative verifier that flexibly balances computational resources between rapid, reliable fast thinking and meticulous slow thinking using a Flexible Allocation of Verification Budget strategy. We further propose the Solve-Detect-Verify pipeline, an efficient inference-time scaling framework that intelligently integrates FlexiVe, proactively identifying solution completion points to trigger targeted verification and provide focused solver feedback. Experiments show FlexiVe achieves superior accuracy in pinpointing errors within reasoning traces on ProcessBench. Furthermore, on challenging mathematical reasoning benchmarks (AIME 2024, AIME 2025, and CNMO), our full approach outperforms baselines like self-consistency in reasoning accuracy and inference efficiency. Our system offers a scalable and effective solution to enhance LLM reasoning at test time.",True,True,"Zhong, Jianyuan and Li, Zeju and Xu, Zhijian and Wen, Xiangyu and Li, Kezhi and Xu, Qiang",2025,,,,arXiv preprint arXiv:2505.11966
SSR: Socratic Self-Refine for Large Language Model Reasoning,2511.10621v1,zhou2025variation,\cite{zhou2025variation},Variation in Verification: Understanding Verification Dynamics in Large Language Models,https://arxiv.org/abs/2509.17995v1,"Recent advances have shown that scaling test-time computation enables large language models (LLMs) to solve increasingly complex problems across diverse domains. One effective paradigm for test-time scaling (TTS) involves LLM generators producing multiple solution candidates, with LLM verifiers assessing the correctness of these candidates without reference answers. In this paper, we study generative verifiers, which perform verification by generating chain-of-thought (CoT) reasoning followed by a binary verdict. We systematically analyze verification dynamics across three dimensions - problem difficulty, generator capability, and verifier generation capability - with empirical studies on 12 benchmarks across mathematical reasoning, knowledge, and natural language reasoning tasks using 14 open-source models (2B to 72B parameter range) and GPT-4o. Our experiments reveal three key findings about verification effectiveness: (1) Easy problems allow verifiers to more reliably certify correct responses; (2) Weak generators produce errors that are easier to detect than strong generators; (3) Verification ability is generally correlated with the verifier's own problem-solving capability, but this relationship varies with problem difficulty. These findings reveal opportunities to optimize basic verification strategies in TTS applications. First, given the same verifier, some weak generators can nearly match stronger ones in post-verification TTS performance (e.g., the Gemma2-9B to Gemma2-27B performance gap shrinks by 75.5%). Second, we identify cases where strong verifiers offer limited advantage over weak ones, as both fail to provide meaningful verification gains, suggesting that verifier scaling alone cannot overcome fundamental verification challenges.",True,True,"Zhou, Yefan and Xu, Austin and Zhou, Yilun and Singh, Janvijay and Gui, Jiang and Joty, Shafiq",2025,,,,arXiv preprint arXiv:2509.17995
SSR: Socratic Self-Refine for Large Language Model Reasoning,2511.10621v1,zheng2023large,\cite{zheng2023large},"Large language models are not robust multiple choice selectors, 2024",,,True,False,"Zheng, Chujie and Zhou, Hao and Meng, Fandong and Zhou, Jie and Huang, Minlie",2023,,,,URL https://arxiv. org/abs/2309.03882
SSR: Socratic Self-Refine for Large Language Model Reasoning,2511.10621v1,shi2024judging,\cite{shi2024judging},Judging the judges: A systematic study of position bias in llm-as-a-judge,,,True,False,"Shi, Lin and Ma, Chiyu and Liang, Wenhua and Diao, Xingjian and Ma, Weicheng and Vosoughi, Soroush",2024,,,,arXiv preprint arXiv:2406.07791
SSR: Socratic Self-Refine for Large Language Model Reasoning,2511.10621v1,hu2024explaining,\cite{hu2024explaining},Explaining Length Bias in LLM-Based Preference Evaluations,https://arxiv.org/abs/2407.01085v5,"The use of large language models (LLMs) as judges, particularly in preference comparisons, has become widespread, but this reveals a notable bias towards longer responses, undermining the reliability of such evaluations. To better understand such bias, we propose to decompose the preference evaluation metric, specifically the win rate, into two key components: desirability and information mass, where the former is length-independent and related to trustworthiness such as correctness, toxicity, and consistency, and the latter is length-dependent and represents the amount of information in the response. We empirically demonstrated the decomposition through controlled experiments and found that response length impacts evaluations by influencing information mass. To derive a reliable evaluation metric that assesses content quality without being confounded by response length, we propose AdapAlpaca, a simple yet effective adjustment to win rate measurement. Specifically, AdapAlpaca ensures a fair comparison of response quality by aligning the lengths of reference and test model responses under equivalent length intervals.",True,True,"Hu, Zhengyu and Song, Linxin and Zhang, Jieyu and Xiao, Zheyuan and Wang, Tianfu and Chen, Zhengyu and Yuan, Nicholas Jing and Lian, Jianxun and Ding, Kaize and Xiong, Hui",2024,,,,arXiv preprint arXiv:2407.01085
SSR: Socratic Self-Refine for Large Language Model Reasoning,2511.10621v1,madaan2023self,\cite{madaan2023self},Self-refine: Iterative refinement with self-feedback,,,True,False,"Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others",2023,,,,Advances in Neural Information Processing Systems
SSR: Socratic Self-Refine for Large Language Model Reasoning,2511.10621v1,zhang2024accessing,\cite{zhang2024accessing},Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B,https://arxiv.org/abs/2406.07394v2,"This paper introduces the MCT Self-Refine (MCTSr) algorithm, an innovative integration of Large Language Models (LLMs) with Monte Carlo Tree Search (MCTS), designed to enhance performance in complex mathematical reasoning tasks. Addressing the challenges of accuracy and reliability in LLMs, particularly in strategic and mathematical reasoning, MCTSr leverages systematic exploration and heuristic self-refine mechanisms to improve decision-making frameworks within LLMs. The algorithm constructs a Monte Carlo search tree through iterative processes of Selection, self-refine, self-evaluation, and Backpropagation, utilizing an improved Upper Confidence Bound (UCB) formula to optimize the exploration-exploitation balance. Extensive experiments demonstrate MCTSr's efficacy in solving Olympiad-level mathematical problems, significantly improving success rates across multiple datasets, including GSM8K, GSM Hard, MATH, and Olympiad-level benchmarks, including Math Odyssey, AIME, and OlympiadBench. The study advances the application of LLMs in complex reasoning tasks and sets a foundation for future AI integration, enhancing decision-making accuracy and reliability in LLM-driven applications.",True,True,"Zhang, Di and Huang, Xiaoshui and Zhou, Dongzhan and Li, Yuqiang and Ouyang, Wanli",2024,,,,arXiv preprint arXiv:2406.07394
SSR: Socratic Self-Refine for Large Language Model Reasoning,2511.10621v1,bi2024forest,\cite{bi2024forest},Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning,https://arxiv.org/abs/2412.09078v5,"Large Language Models (LLMs) have demonstrated remarkable abilities across various language tasks, but solving complex reasoning problems remains a significant challenge. While existing methods, such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT), enhance reasoning by decomposing problems or structuring prompts, they typically perform a single pass of reasoning and may fail to revisit flawed paths, compromising accuracy. To address this limitation, we propose a novel reasoning framework called Forest-of-Thought (FoT), which integrates multiple reasoning trees to leverage collective decision-making for solving complex logical problems. FoT employs sparse activation strategies to select the most relevant reasoning paths, improving both efficiency and accuracy. Additionally, we introduce a dynamic self-correction strategy that enables real-time error correction, along with consensus-guided decision-making strategies to optimize both correctness and computational resources. Experimental results demonstrate that the FoT framework, combined with these strategies, significantly enhances the reasoning capabilities of LLMs, enabling them to solve complex tasks with greater precision and efficiency. Code will be available at https://github.com/iamhankai/Forest-of-Thought.",True,True,"Bi, Zhenni and Han, Kai and Liu, Chuanjian and Tang, Yehui and Wang, Yunhe",2024,,,,arXiv preprint arXiv:2412.09078
SSR: Socratic Self-Refine for Large Language Model Reasoning,2511.10621v1,teng2025atom,\cite{teng2025atom},Atom of thoughts for markov llm test-time scaling,,,True,False,"Teng, Fengwei and Yu, Zhaoyang and Shi, Quan and Zhang, Jiayi and Wu, Chenglin and Luo, Yuyu",2025,,,,arXiv preprint arXiv:2502.12018
SSR: Socratic Self-Refine for Large Language Model Reasoning,2511.10621v1,lightman2023let,\cite{lightman2023let},Let's Verify Step by Step,https://arxiv.org/abs/2305.20050v1,"In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78% of problems from a representative subset of the MATH test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model.",True,True,"Lightman, Hunter and Kosaraju, Vineet and Burda, Yuri and Edwards, Harrison and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl",2023,,,,
SSR: Socratic Self-Refine for Large Language Model Reasoning,2511.10621v1,skywork2024prm,\cite{skywork2024prm},Skywork-o1 Open Series,,,True,False,"He, Jujie and
                  Wei, Tianwen and
                  Yan, Rui and
                  Liu, Jiacai and
                  Wang, Chaojie and
                  Gan, Yimeng and
                  Tu, Shiwen and
                  Liu, Chris Yuhao and
                  Zeng, Liang and
                  Wang, Xiaokun and
                  Wang, Boyang and
                  Li, Yongcong and
                  Zhang, Fuxiang and
                  Xu, Jiacheng and
                  An, Bo and
                  Liu, Yang and
                  Zhou, Yahui",,,https://doi.org/10.5281/zenodo.16998085,10.5281/zenodo.16998085,
SSR: Socratic Self-Refine for Large Language Model Reasoning,2511.10621v1,zhang2025lessons,\cite{zhang2025lessons},The lessons of developing process reward models in mathematical reasoning,,,True,False,"Zhang, Zhenru and Zheng, Chujie and Wu, Yangzhen and Zhang, Beichen and Lin, Runji and Yu, Bowen and Liu, Dayiheng and Zhou, Jingren and Lin, Junyang",2025,,,,arXiv preprint arXiv:2501.07301
SSR: Socratic Self-Refine for Large Language Model Reasoning,2511.10621v1,ling2023deductive,\cite{ling2023deductive},Deductive Verification of Chain-of-Thought Reasoning,https://arxiv.org/abs/2306.03872v3,"Large Language Models (LLMs) significantly benefit from Chain-of-Thought (CoT) prompting in performing various reasoning tasks. While CoT allows models to produce more comprehensive reasoning processes, its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations and accumulated errors, thereby limiting models' ability to solve complex reasoning tasks. Inspired by how humans engage in careful and meticulous deductive logical reasoning processes to solve tasks, we seek to enable language models to perform explicit and rigorous deductive reasoning, and also ensure the trustworthiness of their reasoning process through self-verification. However, directly verifying the validity of an entire deductive reasoning process is challenging, even with advanced models like ChatGPT. In light of this, we propose to decompose a reasoning verification process into a series of step-by-step subprocesses, each only receiving their necessary context and premises. To facilitate this procedure, we propose Natural Program, a natural language-based deductive reasoning format. Our approach enables models to generate precise reasoning steps where subsequent steps are more rigorously grounded on prior steps. It also empowers language models to carry out reasoning self-verification in a step-by-step manner. By integrating this verification process into each deductive reasoning stage, we significantly enhance the rigor and trustfulness of generated reasoning steps. Along this process, we also improve the answer correctness on complex reasoning tasks. Code will be released at https://github.com/lz1oceani/verify_cot.",True,True,"Ling, Zhan and Fang, Yunhao and Li, Xuanlin and Huang, Zhiao and Lee, Mingu and Memisevic, Roland and Su, Hao",2023,,,,Advances in Neural Information Processing Systems
SSR: Socratic Self-Refine for Large Language Model Reasoning,2511.10621v1,zhao2025genprm,\cite{zhao2025genprm},GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning,https://arxiv.org/abs/2504.00891v2,"Recent advancements in Large Language Models (LLMs) have shown that it is promising to utilize Process Reward Models (PRMs) as verifiers to enhance the performance of LLMs. However, current PRMs face three key challenges: (1) limited process supervision and generalization capabilities, (2) dependence on scalar value prediction without leveraging the generative abilities of LLMs, and (3) inability to scale the test-time compute of PRMs. In this work, we introduce GenPRM, a generative process reward model that performs explicit Chain-of-Thought (CoT) reasoning with code verification before providing judgment for each reasoning step. To obtain high-quality process supervision labels and rationale data, we propose Relative Progress Estimation (RPE) and a rationale synthesis framework that incorporates code verification. Experimental results on ProcessBench and several mathematical reasoning tasks show that GenPRM significantly outperforms prior PRMs with only 23K training data from MATH dataset. Through test-time scaling, a 1.5B GenPRM outperforms GPT-4o, and a 7B GenPRM surpasses Qwen2.5-Math-PRM-72B on ProcessBench. Additionally, GenPRM demonstrates strong abilities to serve as a critic model for policy model refinement. This work establishes a new paradigm for process supervision that bridges the gap between PRMs and critic models in LLMs. Our code, model, and data will be available in https://ryanliu112.github.io/GenPRM.",True,True,"Zhao, Jian and Liu, Runze and Zhang, Kaiyan and Zhou, Zhimu and Gao, Junqi and Li, Dong and Lyu, Jiafei and Qian, Zhouyi and Qi, Biqing and Li, Xiu and others",2025,,,,arXiv preprint arXiv:2504.00891
SSR: Socratic Self-Refine for Large Language Model Reasoning,2511.10621v1,mukherjee2025premise,\cite{mukherjee2025premise},Premise-Augmented Reasoning Chains Improve Error Identification in Math reasoning with LLMs,https://arxiv.org/abs/2502.02362v6,"Chain-of-Thought (CoT) prompting enhances mathematical reasoning in large language models (LLMs) by enabling detailed step-by-step solutions. However, due to the verbosity of LLMs, the resulting reasoning chains can be long, making it harder to verify the reasoning steps and trace issues resulting from dependencies between the steps that may be farther away in the sequence of steps. Importantly, mathematical reasoning allows each step to be derived from a small set of premises, which are a subset of the preceding steps in the reasoning chain. In this paper, we present a framework that identifies the premises for each step, to improve the evaluation of reasoning. We restructure conventional linear reasoning chains into Premise Augmented Reasoning Chains (PARC) by introducing premise links, resulting in a directed acyclic graph where the nodes are the steps and the edges are the premise links. Through experiments with a PARC-based dataset that we built, namely PERL (Premises and ERrors identification in LLMs), we demonstrate that LLMs can reliably identify premises within complex reasoning chains. In particular, even open-source LLMs achieve 90% recall in premise identification. We also show that PARC helps to identify errors in reasoning chains more reliably. The accuracy of error identification improves by 6% to 16% absolute when step-by-step verification is carried out in PARC under the premises. Our findings highlight the utility of premise-centric representations in addressing complex problem-solving tasks and open new avenues for improving the reliability of LLM-based reasoning evaluations.",True,True,"Mukherjee, Sagnik and Chinta, Abhinav and Kim, Takyoung and Sharma, Tarun Anoop and Hakkani-T{\""u}r, Dilek",2025,,,,arXiv preprint arXiv:2502.02362
SSR: Socratic Self-Refine for Large Language Model Reasoning,2511.10621v1,fang2025graph,\cite{fang2025graph},Graph of Verification: Structured Verification of LLM Reasoning with Directed Acyclic Graphs,https://arxiv.org/abs/2506.12509v2,"Verifying the complex and multi-step reasoning of Large Language Models (LLMs) is a critical challenge, as holistic methods often overlook localized flaws. Step-by-step validation is a promising alternative, yet existing methods are often rigid. They struggle to adapt to diverse reasoning structures, from formal proofs to informal natural language narratives. To address this adaptability gap, we propose the Graph of Verification (GoV), a novel framework for adaptable and multi-granular verification. GoV's core innovation is its flexible ""node block"" architecture. This mechanism allows GoV to adaptively adjust its verification granularity--from atomic steps for formal tasks to entire paragraphs for natural language--to match the native structure of the reasoning process. This flexibility allows GoV to resolve the fundamental trade-off between verification precision and robustness. Experiments on both well-structured and loosely-structured benchmarks demonstrate GoV's versatility. The results show that GoV's adaptive approach significantly outperforms both holistic baselines and other state-of-the-art decomposition-based methods, establishing a new standard for training-free reasoning verification.",True,True,"Fang, Jiwei and Zhang, Bin and Wang, Changwei and Wan, Jin and Xu, Zhiwei",2025,,,,arXiv preprint arXiv:2506.12509
AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting,2511.09478v1,wang2024exploring,\cite{wang2024exploring},Exploring the reasoning abilities of multimodal large language models (mllms): A comprehensive survey on emerging trends in multimodal reasoning,,,True,False,"Wang, Yiqi and Chen, Wentao and Han, Xiaotian and Lin, Xudong and Zhao, Haiteng and Liu, Yongfei and Zhai, Bohan and Yuan, Jianbo and You, Quanzeng and Yang, Hongxia",2024,,,,arXiv preprint arXiv:2401.06805
AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting,2511.09478v1,saparov2022language,\cite{saparov2022language},Language models are greedy reasoners: A systematic formal analysis of chain-of-thought,,,True,False,"Saparov, Abulhair and He, He",2022,,,,arXiv preprint arXiv:2210.01240
AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting,2511.09478v1,xiong2025hs,\cite{xiong2025hs},HS-STAR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation,,,True,False,"Xiong, Feng and Xu, Hongling and Wang, Yifei and Cheng, Runxi and Wang, Yong and Chu, Xiangxiang",2025,,,,arXiv preprint arXiv:2505.19866
AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting,2511.09478v1,wang2025position,\cite{wang2025position},Position Bias Mitigates Position Bias:Mitigate Position Bias Through Inter-Position Knowledge Distillation,https://arxiv.org/abs/2508.15709v2,"Positional bias (PB), manifesting as non-uniform sensitivity across different contextual locations, significantly impairs long-context comprehension and processing capabilities. Previous studies have addressed PB either by modifying the underlying architectures or by employing extensive contextual awareness training. However, the former approach fails to effectively eliminate the substantial performance disparities, while the latter imposes significant data and computational overhead. To address PB effectively, we introduce \textbf{Pos2Distill}, a position to position knowledge distillation framework. Pos2Distill transfers the superior capabilities from advantageous positions to less favorable ones, thereby reducing the huge performance gaps. The conceptual principle is to leverage the inherent, position-induced disparity to counteract the PB itself. We identify distinct manifestations of PB under \textbf{\textsc{r}}etrieval and \textbf{\textsc{r}}easoning paradigms, thereby designing two specialized instantiations: \emph{Pos2Distill-R\textsuperscript{1}} and \emph{Pos2Distill-R\textsuperscript{2}} respectively, both grounded in this core principle. By employing the Pos2Distill approach, we achieve enhanced uniformity and significant performance gains across all contextual positions in long-context retrieval and reasoning tasks. Crucially, both specialized systems exhibit strong cross-task generalization mutually, while achieving superior performance on their respective tasks.",True,True,"Wang, Yifei and Xiong, Feng and Wang, Yong and Li, Linjing and Chu, Xiangxiang and Zeng, Daniel Dajun",2025,,,,
AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting,2511.09478v1,zhang2024improve,\cite{zhang2024improve},Improve Vision Language Model Chain-of-thought Reasoning,https://arxiv.org/abs/2410.16198v1,"Chain-of-thought (CoT) reasoning in vision language models (VLMs) is crucial for improving interpretability and trustworthiness. However, current training recipes lack robust CoT reasoning data, relying on datasets dominated by short annotations with minimal rationales. In this work, we show that training VLM on short answers does not generalize well to reasoning tasks that require more detailed responses. To address this, we propose a two-fold approach. First, we distill rationales from GPT-4o model to enrich the training data and fine-tune VLMs, boosting their CoT performance. Second, we apply reinforcement learning to further calibrate reasoning quality. Specifically, we construct positive (correct) and negative (incorrect) pairs of model-generated reasoning chains, by comparing their predictions with annotated short answers. Using this pairwise data, we apply the Direct Preference Optimization algorithm to refine the model's reasoning abilities. Our experiments demonstrate significant improvements in CoT reasoning on benchmark datasets and better generalization to direct answer prediction as well. This work emphasizes the importance of incorporating detailed rationales in training and leveraging reinforcement learning to strengthen the reasoning capabilities of VLMs.",True,True,"Zhang, Ruohong and Zhang, Bowen and Li, Yanghao and Zhang, Haotian and Sun, Zhiqing and Gan, Zhe and Yang, Yinfei and Pang, Ruoming and Yang, Yiming",2024,,,,arXiv preprint arXiv:2410.16198
AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting,2511.09478v1,yao2023tree,\cite{yao2023tree},Tree of Thoughts: Deliberate Problem Solving with Large Language Models,https://arxiv.org/abs/2305.10601v2,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",True,True,"Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik",2023,,,,Advances in neural information processing systems
AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting,2511.09478v1,dong2025insight,\cite{dong2025insight},Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models,https://arxiv.org/abs/2411.14432v2,"Large Language Models (LLMs) demonstrate enhanced capabilities and reliability by reasoning more, evolving from Chain-of-Thought prompting to product-level solutions like OpenAI o1. Despite various efforts to improve LLM reasoning, high-quality long-chain reasoning data and optimized training pipelines still remain inadequately explored in vision-language tasks. In this paper, we present Insight-V, an early effort to 1) scalably produce long and robust reasoning data for complex multi-modal tasks, and 2) an effective training pipeline to enhance the reasoning capabilities of multi-modal large language models (MLLMs). Specifically, to create long and structured reasoning data without human labor, we design a two-step pipeline with a progressive strategy to generate sufficiently long and diverse reasoning paths and a multi-granularity assessment method to ensure data quality. We observe that directly supervising MLLMs with such long and complex reasoning data will not yield ideal reasoning ability. To tackle this problem, we design a multi-agent system consisting of a reasoning agent dedicated to performing long-chain reasoning and a summary agent trained to judge and summarize reasoning results. We further incorporate an iterative DPO algorithm to enhance the reasoning agent's generation stability and quality. Based on the popular LLaVA-NeXT model and our stronger base MLLM, we demonstrate significant performance gains across challenging multi-modal benchmarks requiring visual reasoning. Benefiting from our multi-agent system, Insight-V can also easily maintain or improve performance on perception-focused multi-modal tasks.",True,True,"Dong, Yuhao and Liu, Zuyan and Sun, Hai-Long and Yang, Jingkang and Hu, Winston and Rao, Yongming and Liu, Ziwei",2025,,,,
AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting,2511.09478v1,xu2024llava,\cite{xu2024llava},LLaVA-CoT: Let Vision Language Models Reason Step-by-Step,https://arxiv.org/abs/2411.10440v6,"Large language models have demonstrated substantial advancements in reasoning capabilities. However, current Vision-Language Models (VLMs) often struggle to perform systematic and structured reasoning, especially when handling complex visual question-answering tasks. In this work, we introduce LLaVA-CoT, a large VLM designed to conduct autonomous multistage reasoning. Unlike chain-of-thought prompting, LLaVA-CoT independently engages in sequential stages of summarization, visual interpretation, logical reasoning, and conclusion generation. This structured approach enables LLaVA-CoT to achieve marked improvements on reasoning-intensive tasks. To accomplish this, we construct the LLaVA-CoT-100k dataset, integrating samples from various visual question answering sources and providing structured reasoning annotations. Besides, we propose a test-time stage-wise retracing search method (SWIRES), which enables effective and efficient test-time scaling. Remarkably, with only 100k training samples and test-time scaling, LLaVA-CoT not only outperforms its base model by 9.4% on a wide range of multimodal reasoning benchmarks, but also surpasses the performance of larger and even closed-source models, such as Gemini-1.5-pro, GPT-4o-mini, and Llama-3.2-90B-Vision-Instruct. The code, dataset, and pre-trained weights are publicly available at https://github.com/PKU-YuanGroup/LLaVA-CoT.",True,True,"Xu, Guowei and Jin, Peng and Hao, Li and Song, Yibing and Sun, Lichao and Yuan, Li",2024,,,,arXiv preprint arXiv:2411.10440
AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting,2511.09478v1,guo2025deepseek,\cite{guo2025deepseek},DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,https://arxiv.org/abs/2501.12948v1,"We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.",True,True,"Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others",2025,,,,arXiv preprint arXiv:2501.12948
AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting,2511.09478v1,yang2025r1,\cite{yang2025r1},R1-Onevision: Advancing Generalized Multimodal Reasoning through Cross-Modal Formalization,https://arxiv.org/abs/2503.10615v2,"Large Language Models have demonstrated remarkable reasoning capability in complex textual tasks. However, multimodal reasoning, which requires integrating visual and textual information, remains a significant challenge. Existing visual-language models often struggle to effectively analyze and reason visual content, resulting in suboptimal performance on complex reasoning tasks. Moreover, the absence of comprehensive benchmarks hinders the accurate assessment of multimodal reasoning capabilities. In this paper, we introduce R1-Onevision, a multimodal reasoning model designed to bridge the gap between visual perception and deep reasoning. To achieve this, we propose a cross-modal reasoning pipeline that transforms images into formal textural representations, enabling precise language-based reasoning. Leveraging this pipeline, we construct the R1-Onevision dataset which provides detailed, step-by-step multimodal reasoning annotations across diverse domains. We further develop the R1-Onevision model through supervised fine-tuning and reinforcement learning to cultivate advanced reasoning and robust generalization abilities. To comprehensively evaluate multimodal reasoning performance across different grades, we introduce R1-Onevision-Bench, a benchmark aligned with human educational stages, covering exams from junior high school to university and beyond. Experimental results show that R1-Onevision achieves state-of-the-art performance, outperforming models such as GPT-4o and Qwen2.5-VL on multiple challenging multimodal reasoning benchmarks.",True,True,"Yang, Yi and He, Xiaoxuan and Pan, Hongkun and Jiang, Xiyan and Deng, Yan and Yang, Xingtao and Lu, Haoyu and Yin, Dacheng and Rao, Fengyun and Zhu, Minfeng and others",2025,,,,arXiv preprint arXiv:2503.10615
AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting,2511.09478v1,huang2025vision,\cite{huang2025vision},Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models,https://arxiv.org/abs/2503.06749v2,"DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning capability of MLLMs. However, direct training with RL struggles to activate complex reasoning capabilities such as questioning and reflection in MLLMs, due to the absence of substantial high-quality multimodal reasoning data. To address this issue, we propose the reasoning MLLM, Vision-R1, to improve multimodal reasoning capability. Specifically, we first construct a high-quality multimodal CoT dataset without human annotations by leveraging an existing MLLM and DeepSeek-R1 through modality bridging and data filtering to obtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as cold-start initialization data for Vision-R1. To mitigate the optimization challenges caused by overthinking after cold start, we propose Progressive Thinking Suppression Training (PTST) strategy and employ Group Relative Policy Optimization (GRPO) with the hard formatting result reward function to gradually refine the model's ability to learn correct and complex reasoning processes on a 10K multimodal math dataset. Comprehensive experiments show our model achieves an average improvement of $\sim$6% across various multimodal math reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely used MathVista benchmark, which is only 0.4% lower than the leading reasoning model, OpenAI O1. The datasets and code will be released in: https://github.com/Osilly/Vision-R1 .",True,True,"Huang, Wenxuan and Jia, Bohan and Zhai, Zijie and Cao, Shaosheng and Ye, Zheyu and Zhao, Fei and Xu, Zhe and Hu, Yao and Lin, Shaohui",2025,,,,arXiv preprint arXiv:2503.06749
AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting,2511.09478v1,huang2025boosting,\cite{huang2025boosting},Boosting mllm reasoning with text-debiased hint-grpo,,,True,False,"Huang, Qihan and Chan, Long and Liu, Jinlong and He, Wanggui and Jiang, Hao and Song, Mingli and Chen, Jingyuan and Yao, Chang and Song, Jie",2025,,,,arXiv preprint arXiv:2503.23905
AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting,2511.09478v1,bengio2009curriculum,\cite{bengio2009curriculum},Curriculum Abductive Learning,https://arxiv.org/abs/2505.12275v2,"Abductive Learning (ABL) integrates machine learning with logical reasoning in a loop: a learning model predicts symbolic concept labels from raw inputs, which are revised through abduction using domain knowledge and then fed back for retraining. However, due to the nondeterminism of abduction, the training process often suffers from instability, especially when the knowledge base is large and complex, resulting in a prohibitively large abduction space. While prior works focus on improving candidate selection within this space, they typically treat the knowledge base as a static black box. In this work, we propose Curriculum Abductive Learning (C-ABL), a method that explicitly leverages the internal structure of the knowledge base to address the ABL training challenges. C-ABL partitions the knowledge base into a sequence of sub-bases, progressively introduced during training. This reduces the abduction space throughout training and enables the model to incorporate logic in a stepwise, smooth way. Experiments across multiple tasks show that C-ABL outperforms previous ABL implementations, significantly improves training stability, convergence speed, and final accuracy, especially under complex knowledge setting.",True,True,"Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason",2009,,,,
AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting,2511.09478v1,zhou2020uncertainty,\cite{zhou2020uncertainty},Uncertainty-aware curriculum learning for neural machine translation,,,True,False,"Zhou, Yikai and Yang, Baosong and Wong, Derek F and Wan, Yu and Chao, Lidia S",2020,,,,
AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting,2511.09478v1,wang2023efficienttrain,\cite{wang2023efficienttrain},EfficientTrain: Exploring Generalized Curriculum Learning for Training Visual Backbones,https://arxiv.org/abs/2211.09703v3,"The superior performance of modern deep networks usually comes with a costly training procedure. This paper presents a new curriculum learning approach for the efficient training of visual backbones (e.g., vision Transformers). Our work is inspired by the inherent learning dynamics of deep networks: we experimentally show that at an earlier training stage, the model mainly learns to recognize some 'easier-to-learn' discriminative patterns within each example, e.g., the lower-frequency components of images and the original information before data augmentation. Driven by this phenomenon, we propose a curriculum where the model always leverages all the training data at each epoch, while the curriculum starts with only exposing the 'easier-to-learn' patterns of each example, and introduces gradually more difficult patterns. To implement this idea, we 1) introduce a cropping operation in the Fourier spectrum of the inputs, which enables the model to learn from only the lower-frequency components efficiently, 2) demonstrate that exposing the features of original images amounts to adopting weaker data augmentation, and 3) integrate 1) and 2) and design a curriculum learning schedule with a greedy-search algorithm. The resulting approach, EfficientTrain, is simple, general, yet surprisingly effective. As an off-the-shelf method, it reduces the wall-time training cost of a wide variety of popular models (e.g., ResNet, ConvNeXt, DeiT, PVT, Swin, and CSWin) by >1.5x on ImageNet-1K/22K without sacrificing accuracy. It is also effective for self-supervised learning (e.g., MAE). Code is available at https://github.com/LeapLabTHU/EfficientTrain.",True,True,"Wang, Yulin and Yue, Yang and Lu, Rui and Liu, Tianjiao and Zhong, Zhao and Song, Shiji and Huang, Gao",2023,,,,
AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting,2511.09478v1,deng2025boosting,\cite{deng2025boosting},Boosting the generalization and reasoning of vision language models with curriculum reinforcement learning,,,True,False,"Deng, Huilin and Zou, Ding and Ma, Rui and Luo, Hongchen and Cao, Yang and Kang, Yu",2025,,,,arXiv preprint arXiv:2503.07065
AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting,2511.09478v1,team2025kimi,\cite{team2025kimi},Kimi k1. 5: Scaling reinforcement learning with llms,,,True,False,"Team, Kimi and Du, Angang and Gao, Bofei and Xing, Bowei and Jiang, Changjiu and Chen, Cheng and Li, Cheng and Xiao, Chenjun and Du, Chenzhuang and Liao, Chonghua and others",2025,,,,arXiv preprint arXiv:2501.12599
AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting,2511.09478v1,shi2025efficient,\cite{shi2025efficient},Efficient Reinforcement Finetuning via Adaptive Curriculum Learning,https://arxiv.org/abs/2504.05520v2,"Reinforcement finetuning (RFT) has shown great potential for enhancing the mathematical reasoning capabilities of large language models (LLMs), but it is often sample- and compute-inefficient, requiring extensive training. In this work, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuning), a method that significantly improves both the efficiency and final accuracy of RFT through adaptive curriculum learning. AdaRFT dynamically adjusts the difficulty of training problems based on the model's recent reward signals, ensuring that the model consistently trains on tasks that are challenging but solvable. This adaptive sampling strategy accelerates learning by maintaining an optimal difficulty range, avoiding wasted computation on problems that are too easy or too hard. AdaRFT requires only a lightweight extension to standard RFT algorithms like Proximal Policy Optimization (PPO), without modifying the reward function or model architecture. Experiments on competition-level math datasets-including AMC, AIME, and IMO-style problems-demonstrate that AdaRFT significantly improves both training efficiency and reasoning performance. We evaluate AdaRFT across multiple data distributions and model sizes, showing that it reduces training time by up to 2x and improves accuracy by a considerable margin, offering a more scalable and effective RFT framework.",True,True,"Shi, Taiwei and Wu, Yiyang and Song, Linxin and Zhou, Tianyi and Zhao, Jieyu",2025,,,,arXiv preprint arXiv:2504.05520
LOw-cOst yet High-Performant Sparse Matrix-Matrix Multiplication on Arm SME Architectures,2511.08158v2,rahman2021fusedmm,\cite{rahman2021fusedmm},FusedMM: A Unified SDDMM-SpMM Kernel for Graph Embedding and Graph Neural Networks,https://arxiv.org/abs/2011.06391v2,"We develop a fused matrix multiplication kernel that unifies sampled dense-dense matrix multiplication and sparse-dense matrix multiplication under a single operation called FusedMM. By using user-defined functions, FusedMM can capture almost all computational patterns needed by popular graph embedding and GNN approaches. FusedMM is an order of magnitude faster than its equivalent kernels in Deep Graph Library. The superior performance of FusedMM comes from the low-level vectorized kernels, a suitable load balancing scheme and an efficient utilization of the memory bandwidth. FusedMM can tune its performance using a code generator and perform equally well on Intel, AMD and ARM processors. FusedMM speeds up an end-to-end graph embedding algorithm by up to 28x on different processors.",True,True,"Rahman, Md Khaledur and Sujon, Majedul Haque and Azad, Ariful",2021,,,,
LOw-cOst yet High-Performant Sparse Matrix-Matrix Multiplication on Arm SME Architectures,2511.08158v2,hu2020featgraph,\cite{hu2020featgraph},Featgraph: A flexible and efficient backend for graph neural network systems,,,True,False,"Hu, Yuwei and Ye, Zihao and Wang, Minjie and Yu, Jiali and Zheng, Da and Li, Mu and Zhang, Zheng and Zhang, Zhiru and Wang, Yida",2020,,,,
LOw-cOst yet High-Performant Sparse Matrix-Matrix Multiplication on Arm SME Architectures,2511.08158v2,natesh2023rosko,\cite{natesh2023rosko},Rosko: Row Skipping Outer Products for Sparse Matrix Multiplication Kernels,https://arxiv.org/abs/2307.03930v1,"We propose Rosko -- row skipping outer products -- for deriving sparse matrix multiplication (SpMM) kernels in reducing computation and memory access requirements of deep neural networks (DNNs). Rosko allows skipping of entire row computations during program execution with low sparsity-management overheads. We analytically derive sparse CPU kernels that adapt to given hardware characteristics to effectively utilize processor cores and minimize data movement without the need for auto-tuning or search space exploration. Rosko can be integrated with other outer product scheduling methods, allowing them to leverage row skipping by using Rosko's packing format to skip unnecessary computation.
  Rosko kernels outperform existing auto-tuning and search-based solutions as well as state-of-the-art vendor-optimized libraries on real hardware across a variety of neural network workloads. For matrices with sparsities ranging from 65% to 99.8% typically found in machine learning, Rosko kernels achieve up to a 6.5x runtime reduction on Intel and ARM CPUs.",True,True,"Natesh, Vikas and Sabot, Andrew and Kung, HT and Ting, Mark",2023,,,,arXiv preprint arXiv:2307.03930
LOw-cOst yet High-Performant Sparse Matrix-Matrix Multiplication on Arm SME Architectures,2511.08158v2,wang2014intel,\cite{wang2014intel},Intel math kernel library,,,True,False,"Wang, Endong and Zhang, Qing and Shen, Bo and Zhang, Guangyong and Lu, Xiaowei and Wu, Qing and Wang, Yajuan",2014,,,,
LOw-cOst yet High-Performant Sparse Matrix-Matrix Multiplication on Arm SME Architectures,2511.08158v2,ArmPL2504,\cite{ArmPL2504},ARM Performance Libraries 25.04,,,True,False,{ARM},2025,,,,
LOw-cOst yet High-Performant Sparse Matrix-Matrix Multiplication on Arm SME Architectures,2511.08158v2,chen2018tvm,\cite{chen2018tvm},TVM: An Automated End-to-End Optimizing Compiler for Deep Learning,https://arxiv.org/abs/1802.04799v3,"There is an increasing need to bring machine learning to a wide diversity of hardware devices. Current frameworks rely on vendor-specific operator libraries and optimize for a narrow range of server-class GPUs. Deploying workloads to new platforms -- such as mobile phones, embedded devices, and accelerators (e.g., FPGAs, ASICs) -- requires significant manual effort. We propose TVM, a compiler that exposes graph-level and operator-level optimizations to provide performance portability to deep learning workloads across diverse hardware back-ends. TVM solves optimization challenges specific to deep learning, such as high-level operator fusion, mapping to arbitrary hardware primitives, and memory latency hiding. It also automates optimization of low-level programs to hardware characteristics by employing a novel, learning-based cost modeling method for rapid exploration of code optimizations. Experimental results show that TVM delivers performance across hardware back-ends that are competitive with state-of-the-art, hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPUs. We also demonstrate TVM's ability to target new accelerator back-ends, such as the FPGA-based generic deep learning accelerator. The system is open sourced and in production use inside several major companies.",True,True,"Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Shen, Haichen and Cowan, Meghan and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind",2018,,,,
LOw-cOst yet High-Performant Sparse Matrix-Matrix Multiplication on Arm SME Architectures,2511.08158v2,heinecke2016libxsmm,\cite{heinecke2016libxsmm},LIBXSMM: accelerating small matrix multiplications by runtime code generation,,,True,False,"Heinecke, Alexander and Henry, Greg and Hutchinson, Maxwell and Pabst, Hans",2016,,,,
LOw-cOst yet High-Performant Sparse Matrix-Matrix Multiplication on Arm SME Architectures,2511.08158v2,kjolstad2017tensor,\cite{kjolstad2017tensor},The tensor algebra compiler,,,True,False,"Kjolstad, Fredrik and Kamil, Shoaib and Chou, Stephen and Lugato, David and Amarasinghe, Saman",2017,,,,Proceedings of the ACM on Programming Languages
LOw-cOst yet High-Performant Sparse Matrix-Matrix Multiplication on Arm SME Architectures,2511.08158v2,gale2020sparse,\cite{gale2020sparse},Sparse gpu kernels for deep learning,,,True,False,"Gale, Trevor and Zaharia, Matei and Young, Cliff and Elsen, Erich",2020,,,,
LOw-cOst yet High-Performant Sparse Matrix-Matrix Multiplication on Arm SME Architectures,2511.08158v2,chen2021efficient,\cite{chen2021efficient},Efficient tensor core-based gpu kernels for structured sparsity under reduced precision,,,True,False,"Chen, Zhaodong and Qu, Zheng and Liu, Liu and Ding, Yufei and Xie, Yuan",2021,,,,
LOw-cOst yet High-Performant Sparse Matrix-Matrix Multiplication on Arm SME Architectures,2511.08158v2,huang2020ge,\cite{huang2020ge},Ge-spmm: General-purpose sparse matrix-matrix multiplication on gpus for graph neural networks,,,True,False,"Huang, Guyue and Dai, Guohao and Wang, Yu and Yang, Huazhong",2020,,,,
LOw-cOst yet High-Performant Sparse Matrix-Matrix Multiplication on Arm SME Architectures,2511.08158v2,pang2024row,\cite{pang2024row},A row decomposition-based approach for sparse matrix multiplication on GPUs,,,True,False,"Pang, Meng and Fei, Xiang and Qu, Peng and Zhang, Youhui and Li, Zhaolin",2024,,,,
LOw-cOst yet High-Performant Sparse Matrix-Matrix Multiplication on Arm SME Architectures,2511.08158v2,wang2023tc,\cite{wang2023tc},TC-GNN: Bridging Sparse GNN Computation and Dense Tensor Cores on GPUs,https://arxiv.org/abs/2112.02052v4,"Recently, graph neural networks (GNNs), as the backbone of graph-based machine learning, demonstrate great success in various domains (e.g., e-commerce). However, the performance of GNNs is usually unsatisfactory due to the highly sparse and irregular graph-based operations. To this end, we propose TC-GNN, the first GNN acceleration framework based on GPU Tensor Core Units (TCUs). The core idea is to reconcile the ""Sparse"" GNN computation with the high-performance ""Dense"" TCUs. Specifically, we conduct an in-depth analysis of the sparse operations in mainstream GNN computing frameworks. We introduce a novel sparse graph translation technique to facilitate TCU processing of the sparse GNN workload. We implement an effective CUDA core and TCU collaboration design to fully utilize GPU resources. We integrate TC-GNN with the PyTorch framework for high programmability. Rigorous experiments show an average of 1.70X speedup over the state-of-the-art DGL framework across various models and datasets.",True,True,"Wang, Yuke and Feng, Boyuan and Wang, Zheng and Huang, Guyue and Ding, Yufei",2023,,,,
LOw-cOst yet High-Performant Sparse Matrix-Matrix Multiplication on Arm SME Architectures,2511.08158v2,fan2024dtc,\cite{fan2024dtc},Dtc-spmm: Bridging the gap in accelerating general sparse matrix multiplication with tensor cores,,,True,False,"Fan, Ruibo and Wang, Wei and Chu, Xiaowen",2024,,,,
LOw-cOst yet High-Performant Sparse Matrix-Matrix Multiplication on Arm SME Architectures,2511.08158v2,li2022efficient,\cite{li2022efficient},Efficient Quantized Sparse Matrix Operations on Tensor Cores,https://arxiv.org/abs/2209.06979v4,"The exponentially growing model size drives the continued success of deep learning, but it brings prohibitive computation and memory cost. From the algorithm perspective, model sparsification and quantization have been studied to alleviate the problem. From the architecture perspective, hardware vendors provide Tensor cores for acceleration. However, it is very challenging to gain practical speedups from sparse, low-precision matrix operations on Tensor cores, because of the strict requirements for data layout and lack of support for efficiently manipulating the low-precision integers. We propose Magicube, a high-performance sparse-matrix library for low-precision integers on Tensor cores. Magicube supports SpMM and SDDMM, two major sparse operations in deep learning with mixed precision. Experimental results on an NVIDIA A100 GPU show that Magicube achieves on average 1.44x (up to 2.37x) speedup over the vendor-optimized library for sparse kernels, and 1.43x speedup over the state-of-the-art with a comparable accuracy for end-to-end sparse Transformer inference.",True,True,"Li, Shigang and Osawa, Kazuki and Hoefler, Torsten",2022,,,,
LOw-cOst yet High-Performant Sparse Matrix-Matrix Multiplication on Arm SME Architectures,2511.08158v2,hegde2019extensor,\cite{hegde2019extensor},Extensor: An accelerator for sparse tensor algebra,,,True,False,"Hegde, Kartik and Asghari-Moghaddam, Hadi and Pellauer, Michael and Crago, Neal and Jaleel, Aamer and Solomonik, Edgar and Emer, Joel and Fletcher, Christopher W",2019,,,,
LOw-cOst yet High-Performant Sparse Matrix-Matrix Multiplication on Arm SME Architectures,2511.08158v2,song2022sextans,\cite{song2022sextans},Sextans: A streaming accelerator for general-purpose sparse-matrix dense-matrix multiplication,,,True,False,"Song, Linghao and Chi, Yuze and Sohrabizadeh, Atefeh and Choi, Young-kyu and Lau, Jason and Cong, Jason",2022,,,,
LOw-cOst yet High-Performant Sparse Matrix-Matrix Multiplication on Arm SME Architectures,2511.08158v2,gerogiannis2023spade,\cite{gerogiannis2023spade},Spade: A flexible and scalable accelerator for spmm and sddmm,,,True,False,"Gerogiannis, Gerasimos and Yesil, Serif and Lenadora, Damitha and Cao, Dingyuan and Mendis, Charith and Torrellas, Josep",2023,,,,
LOw-cOst yet High-Performant Sparse Matrix-Matrix Multiplication on Arm SME Architectures,2511.08158v2,gerogiannis2024hottiles,\cite{gerogiannis2024hottiles},Hottiles: Accelerating spmm with heterogeneous accelerator architectures,,,True,False,"Gerogiannis, Gerasimos and Aananthakrishnan, Sriram and Torrellas, Josep and Hur, Ibrahim",2024,,,,
LOw-cOst yet High-Performant Sparse Matrix-Matrix Multiplication on Arm SME Architectures,2511.08158v2,li2025hyte,\cite{li2025hyte},HYTE: Flexible Tiling for Sparse Accelerators via Hybrid Static-Dynamic Approaches,,,True,False,"Li, Xintong and Li, Zhiyao and Gao, Mingyu",2025,,,,
QuCoWE Quantum Contrastive Word Embeddings with Variational Circuits for NearTerm Quantum Devices,2511.10179v1,harris1954distributional,\cite{harris1954distributional},Distributional structure,,,True,False,"Harris, Zellig S",1954,,,,Word
QuCoWE Quantum Contrastive Word Embeddings with Variational Circuits for NearTerm Quantum Devices,2511.10179v1,Mikolov2013NIPS,\cite{Mikolov2013NIPS},Distributed Representations of Words and Phrases and their Compositionality,,,True,False,"Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeff",2013,,,,
QuCoWE Quantum Contrastive Word Embeddings with Variational Circuits for NearTerm Quantum Devices,2511.10179v1,LevyGoldberg2014NIPS,\cite{LevyGoldberg2014NIPS},Neural Word Embedding as Implicit Matrix Factorization,,,True,False,"Levy, Omer and Goldberg, Yoav",2014,,,,
QuCoWE Quantum Contrastive Word Embeddings with Variational Circuits for NearTerm Quantum Devices,2511.10179v1,pennington2014glove,\cite{pennington2014glove},Glove: Global vectors for word representation,,,True,False,"Pennington, Jeffrey and Socher, Richard and Manning, Christopher D",2014,,,,
QuCoWE Quantum Contrastive Word Embeddings with Variational Circuits for NearTerm Quantum Devices,2511.10179v1,bojanowski2017enriching,\cite{bojanowski2017enriching},Enriching word vectors with subword information,,,True,False,"Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas",2017,,,,Transactions of the association for computational linguistics
QuCoWE Quantum Contrastive Word Embeddings with Variational Circuits for NearTerm Quantum Devices,2511.10179v1,coecke2020foundations,\cite{coecke2020foundations},Foundations for near-term quantum natural language processing,,,True,False,"Coecke, Bob and de Felice, Giovanni and Meichanetzidis, Konstantinos and Toumi, Alexis",2020,,,,arXiv preprint arXiv:2012.03755
QuCoWE Quantum Contrastive Word Embeddings with Variational Circuits for NearTerm Quantum Devices,2511.10179v1,heunen2013quantum,\cite{heunen2013quantum},"Quantum physics and linguistics: a compositional, diagrammatic discourse",,,True,False,"Heunen, Chris and Sadrzadeh, Mehrnoosh and Grefenstette, Edward",2013,,,,
QuCoWE Quantum Contrastive Word Embeddings with Variational Circuits for NearTerm Quantum Devices,2511.10179v1,Meichanetzidis2020,\cite{Meichanetzidis2020},Grammar-aware sentence classification on quantum computers,https://arxiv.org/abs/2012.03756v2,"Natural language processing (NLP) is at the forefront of great advances in contemporary AI, and it is arguably one of the most challenging areas of the field. At the same time, in the area of Quantum Computing (QC), with the steady growth of quantum hardware and notable improvements towards implementations of quantum algorithms, we are approaching an era when quantum computers perform tasks that cannot be done on classical computers with a reasonable amount of resources. This provides a new range of opportunities for AI, and for NLP specifically. In this work, we work with the Categorical Distributional Compositional (DisCoCat) model of natural language meaning, whose underlying mathematical underpinnings make it amenable to quantum instantiations. Earlier work on fault-tolerant quantum algorithms has already demonstrated potential quantum advantage for NLP, notably employing DisCoCat. In this work, we focus on the capabilities of noisy intermediate-scale quantum (NISQ) hardware and perform the first implementation of an NLP task on a NISQ processor, using the DisCoCat framework. Sentences are instantiated as parameterised quantum circuits; word-meanings are embedded in quantum states using parameterised quantum-circuits and the sentence's grammatical structure faithfully manifests as a pattern of entangling operations which compose the word-circuits into a sentence-circuit. The circuits' parameters are trained using a classical optimiser in a supervised NLP task of binary classification. Our novel QNLP model shows concrete promise for scalability as the quality of the quantum hardware improves in the near future and solidifies a novel branch of experimental research at the intersection of QC and AI.",True,True,"Meichanetzidis, K. and Toumi, A. and others",2020,,,,
QuCoWE Quantum Contrastive Word Embeddings with Variational Circuits for NearTerm Quantum Devices,2511.10179v1,chang2023variational,\cite{chang2023variational},Variational quantum classifiers for natural-language text,,,True,False,"Chang, Daniel T",2023,,,,arXiv preprint arXiv:2303.02469
QuCoWE Quantum Contrastive Word Embeddings with Variational Circuits for NearTerm Quantum Devices,2511.10179v1,Cerezo2021NatComm,\cite{Cerezo2021NatComm},Cost Function Dependent Barren Plateaus in Shallow Parametrized Quantum Circuits,https://arxiv.org/abs/2001.00550v3,"Variational quantum algorithms (VQAs) optimize the parameters $\vecθ$ of a parametrized quantum circuit $V(\vecθ)$ to minimize a cost function $C$. While VQAs may enable practical applications of noisy quantum computers, they are nevertheless heuristic methods with unproven scaling. Here, we rigorously prove two results, assuming $V(\vecθ)$ is an alternating layered ansatz composed of blocks forming local 2-designs. Our first result states that defining $C$ in terms of global observables leads to exponentially vanishing gradients (i.e., barren plateaus) even when $V(\vecθ)$ is shallow. Hence, several VQAs in the literature must revise their proposed costs. On the other hand, our second result states that defining $C$ with local observables leads to at worst a polynomially vanishing gradient, so long as the depth of $V(\vecθ)$ is $\mathcal{O}(\log n)$. Our results establish a connection between locality and trainability. We illustrate these ideas with large-scale simulations, up to 100 qubits, of a quantum autoencoder implementation.",True,True,"Cerezo, M. and Sone, A. and Volkoff, T. and Cincio, L. and Coles, P. J.",2021,,,,Nature Communications
QuCoWE Quantum Contrastive Word Embeddings with Variational Circuits for NearTerm Quantum Devices,2511.10179v1,kandala2017hardware,\cite{kandala2017hardware},Hardware-efficient Variational Quantum Eigensolver for Small Molecules and Quantum Magnets,https://arxiv.org/abs/1704.05018v2,"Quantum computers can be used to address molecular structure, materials science and condensed matter physics problems, which currently stretch the limits of existing high-performance computing resources. Finding exact numerical solutions to these interacting fermion problems has exponential cost, while Monte Carlo methods are plagued by the fermionic sign problem. These limitations of classical computational methods have made even few-atom molecular structures problems of practical interest for medium-sized quantum computers. Yet, thus far experimental implementations have been restricted to molecules involving only Period I elements. Here, we demonstrate the experimental optimization of up to six-qubit Hamiltonian problems with over a hundred Pauli terms, determining the ground state energy for molecules of increasing size, up to BeH2. This is enabled by a hardware-efficient variational quantum eigensolver with trial states specifically tailored to the available interactions in our quantum processor, combined with a compact encoding of fermionic Hamiltonians and a robust stochastic optimization routine. We further demonstrate the flexibility of our approach by applying the technique to a problem of quantum magnetism. Across all studied problems, we find agreement between experiment and numerical simulations with a noisy model of the device. These results help elucidate the requirements for scaling the method to larger systems, and aim at bridging the gap between problems at the forefront of high-performance computing and their implementation on quantum hardware.",True,True,"Kandala, Abhinav and Mezzacapo, Antonio and Temme, Kristan and Takita, Maika and Brink, Markus and Chow, Jerry M and Gambetta, Jay M",2017,,,,nature
QuCoWE Quantum Contrastive Word Embeddings with Variational Circuits for NearTerm Quantum Devices,2511.10179v1,PerezSalinas2020Quantum,\cite{PerezSalinas2020Quantum},Data re-uploading for a universal quantum classifier,,,True,False,"P{\'e}rez-Salinas, Adri{\'a}n and Cervera-Lierta, Alba and Gil-Fuster, Ernest and Latorre, Jos{\'e} I.",2020,,,,Quantum
QuCoWE Quantum Contrastive Word Embeddings with Variational Circuits for NearTerm Quantum Devices,2511.10179v1,skolik2021layerwise,\cite{skolik2021layerwise},Layerwise learning for quantum neural networks,,,True,False,"Skolik, Andrea and McClean, Jarrod R and Mohseni, Masoud and Van Der Smagt, Patrick and Leib, Martin",2021,,,,Quantum Machine Intelligence
QuCoWE Quantum Contrastive Word Embeddings with Variational Circuits for NearTerm Quantum Devices,2511.10179v1,temme2017error,\cite{temme2017error},Error mitigation for short-depth quantum circuits,,,True,False,"Temme, Kristan and Bravyi, Sergey and Gambetta, Jay M",2017,,,,Physical review letters
QuCoWE Quantum Contrastive Word Embeddings with Variational Circuits for NearTerm Quantum Devices,2511.10179v1,WallmanEmerson2016PRA,\cite{WallmanEmerson2016PRA},Noise tailoring for scalable quantum computation via randomized compiling,,,True,False,"Wallman, Joel J. and Emerson, Joseph",2016,,,,Physical Review A
PerspAct: Enhancing LLM Situated Collaboration Skills through Perspective Taking and Active Vision,2511.08098v1,ognibene2025scoopframeworkproactivecollaboration,\cite{ognibene2025scoopframeworkproactivecollaboration},SCOOP: A Framework for Proactive Collaboration and Social Continual Learning through Natural Language Interaction andCausal Reasoning,,,True,False,Dimitri Ognibene and Sabrina Patania and Luca Annese and Cansu Koyuturk and Franca Garzotto and Giuseppe Vizzari and Azzurra Ruggeri and Simone Colombani,2025,,https://arxiv.org/abs/2503.10241,,
PerspAct: Enhancing LLM Situated Collaboration Skills through Perspective Taking and Active Vision,2511.08098v1,brown2020language,\cite{brown2020language},Language models are few-shot learners,,,True,False,"Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",2020,,,,Advances in neural information processing systems
PerspAct: Enhancing LLM Situated Collaboration Skills through Perspective Taking and Active Vision,2511.08098v1,ahn2022can,\cite{ahn2022can},"Do as i can, not as i say: Grounding language in robotic affordances",,,True,False,"Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Fu, Chuyuan and Gopalakrishnan, Keerthana and Hausman, Karol and others",2022,,,,arXiv preprint arXiv:2204.01691
PerspAct: Enhancing LLM Situated Collaboration Skills through Perspective Taking and Active Vision,2511.08098v1,huang2022inner,\cite{huang2022inner},Inner Monologue: Embodied Reasoning through Planning with Language Models,https://arxiv.org/abs/2207.05608v1,"Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent's own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.",True,True,"Huang, Wenlong and Xia, Fei and Xiao, Ted and Chan, Harris and Liang, Jacky and Florence, Pete and Zeng, Andy and Tompson, Jonathan and Mordatch, Igor and Chebotar, Yevgen and others",2022,,,,arXiv preprint arXiv:2207.05608
PerspAct: Enhancing LLM Situated Collaboration Skills through Perspective Taking and Active Vision,2511.08098v1,grice1975logic,\cite{grice1975logic},Logic and conversation,,,True,False,"Grice, Herbert P",1975,,,,
PerspAct: Enhancing LLM Situated Collaboration Skills through Perspective Taking and Active Vision,2511.08098v1,flavell1981young,\cite{flavell1981young},Young children's knowledge about visual perception: Further evidence for the Level 1--Level 2 distinction.,,,True,False,"Flavell, John H and Everett, Barbara A and Croft, Karen and Flavell, Eleanor R",1981,,,,Developmental psychology
PerspAct: Enhancing LLM Situated Collaboration Skills through Perspective Taking and Active Vision,2511.08098v1,yao2023react,\cite{yao2023react},React: Synergizing reasoning and acting in language models,,,True,False,"Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan",2023,,,,
PerspAct: Enhancing LLM Situated Collaboration Skills through Perspective Taking and Active Vision,2511.08098v1,kosinski2024evaluating,\cite{kosinski2024evaluating},Re-evaluating Theory of Mind evaluation in large language models,https://arxiv.org/abs/2502.21098v1,"The question of whether large language models (LLMs) possess Theory of Mind (ToM) -- often defined as the ability to reason about others' mental states -- has sparked significant scientific and public interest. However, the evidence as to whether LLMs possess ToM is mixed, and the recent growth in evaluations has not resulted in a convergence. Here, we take inspiration from cognitive science to re-evaluate the state of ToM evaluation in LLMs. We argue that a major reason for the disagreement on whether LLMs have ToM is a lack of clarity on whether models should be expected to match human behaviors, or the computations underlying those behaviors. We also highlight ways in which current evaluations may be deviating from ""pure"" measurements of ToM abilities, which also contributes to the confusion. We conclude by discussing several directions for future research, including the relationship between ToM and pragmatic communication, which could advance our understanding of artificial systems as well as human cognition.",True,True,"Kosinski, Michal",2024,,,,Proceedings of the National Academy of Sciences
PerspAct: Enhancing LLM Situated Collaboration Skills through Perspective Taking and Active Vision,2511.08098v1,wilf2023think,\cite{wilf2023think},Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities,https://arxiv.org/abs/2311.10227v1,"Human interactions are deeply rooted in the interplay of thoughts, beliefs, and desires made possible by Theory of Mind (ToM): our cognitive ability to understand the mental states of ourselves and others. Although ToM may come naturally to us, emulating it presents a challenge to even the most advanced Large Language Models (LLMs). Recent improvements to LLMs' reasoning capabilities from simple yet effective prompting techniques such as Chain-of-Thought have seen limited applicability to ToM. In this paper, we turn to the prominent cognitive science theory ""Simulation Theory"" to bridge this gap. We introduce SimToM, a novel two-stage prompting framework inspired by Simulation Theory's notion of perspective-taking. To implement this idea on current ToM benchmarks, SimToM first filters context based on what the character in question knows before answering a question about their mental state. Our approach, which requires no additional training and minimal prompt-tuning, shows substantial improvement over existing methods, and our analysis reveals the importance of perspective-taking to Theory-of-Mind capabilities. Our findings suggest perspective-taking as a promising direction for future research into improving LLMs' ToM capabilities.",True,True,"Wilf, Alex and Lee, Sihyun Shawn and Liang, Paul Pu and Morency, Louis-Philippe",2023,,,,arXiv preprint arXiv:2311.10227
PerspAct: Enhancing LLM Situated Collaboration Skills through Perspective Taking and Active Vision,2511.08098v1,goral2024seeing,\cite{goral2024seeing},Seeing through their eyes: Evaluating visual perspective taking in vision language models,,,True,False,"G{\'o}ral, Gracjan and Ziarko, Alicja and Nauman, Michal and Wo{\l}czyk, Maciej",2024,,,,arXiv preprint arXiv:2409.12969
PerspAct: Enhancing LLM Situated Collaboration Skills through Perspective Taking and Active Vision,2511.08098v1,leonard2024failures,\cite{leonard2024failures},Failures in Perspective-taking of Multimodal AI Systems,https://arxiv.org/abs/2409.13929v1,"This study extends previous research on spatial representations in multimodal AI systems. Although current models demonstrate a rich understanding of spatial information from images, this information is rooted in propositional representations, which differ from the analog representations employed in human and animal spatial cognition. To further explore these limitations, we apply techniques from cognitive and developmental science to assess the perspective-taking abilities of GPT-4o. Our analysis enables a comparison between the cognitive development of the human brain and that of multimodal AI, offering guidance for future research and model development.",True,True,"Leonard, Bridget and Woodard, Kristin and Murray, Scott O",2024,,,,arXiv preprint arXiv:2409.13929
PerspAct: Enhancing LLM Situated Collaboration Skills through Perspective Taking and Active Vision,2511.08098v1,wang2024actiview,\cite{wang2024actiview},ActiView: Evaluating Active Perception Ability for Multimodal Large Language Models,https://arxiv.org/abs/2410.04659v2,"Active perception, a crucial human capability, involves setting a goal based on the current understanding of the environment and performing actions to achieve that goal. Despite significant efforts in evaluating Multimodal Large Language Models (MLLMs), active perception has been largely overlooked. To address this gap, we propose a novel benchmark named ActiView to evaluate active perception in MLLMs. We focus on a specialized form of Visual Question Answering (VQA) that eases and quantifies the evaluation yet challenging for existing MLLMs. Meanwhile, intermediate reasoning behaviors of models are also discussed. Given an image, we restrict the perceptual field of a model, requiring it to actively zoom or shift its perceptual field based on reasoning to answer the question successfully. We conduct extensive evaluation over 30 models, including proprietary and open-source models, and observe that restricted perceptual fields play a significant role in enabling active perception. Results reveal a significant gap in the active perception capability of MLLMs, indicating that this area deserves more attention. We hope that ActiView could help develop methods for MLLMs to understand multimodal inputs in more natural and holistic ways.",True,True,"Wang, Ziyue and Chen, Chi and Luo, Fuwen and Dong, Yurui and Zhang, Yuanchi and Xu, Yuzhuang and Wang, Xiaolong and Li, Peng and Liu, Yang",2024,,,,arXiv preprint arXiv:2410.04659
PerspAct: Enhancing LLM Situated Collaboration Skills through Perspective Taking and Active Vision,2511.08098v1,patania2024large,\cite{patania2024large},Large Language Models as an active Bayesian filter: information acquisition and integration,,,True,False,"Patania, Sabrina and Masiero, Emanuele and Brini, Luca and Piskovskyi, Valentyn and Ognibene, Dimitri and Donabauer, Gregor and Kruschwitz, Udo and others",2024,,,,
Difference Vector Equalization for Robust Fine-tuning of Vision-Language Models,2511.09973v1,Radford21,\cite{Radford21},Learning Transferable Visual Models From Natural Language Supervision,https://arxiv.org/abs/2103.00020v1,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",True,True,"Radford, A. and Kim, J. W. and Hallacy, C. and Ramesh, A. and Goh, G. and Agarwal, S. and Sastry, G. and Askell, A. and Mishkin, P. and Clark, J. and Krueger, G. and Sutskever, I.",2021,,,,
Difference Vector Equalization for Robust Fine-tuning of Vision-Language Models,2511.09973v1,Wortsman22,\cite{Wortsman22},Robust fine-tuning of zero-shot models,https://arxiv.org/abs/2109.01903v3,"Large pre-trained models such as CLIP or ALIGN offer consistent accuracy across a range of data distributions when performing zero-shot inference (i.e., without fine-tuning on a specific dataset). Although existing fine-tuning methods substantially improve accuracy on a given target distribution, they often reduce robustness to distribution shifts. We address this tension by introducing a simple and effective method for improving robustness while fine-tuning: ensembling the weights of the zero-shot and fine-tuned models (WiSE-FT). Compared to standard fine-tuning, WiSE-FT provides large accuracy improvements under distribution shift, while preserving high accuracy on the target distribution. On ImageNet and five derived distribution shifts, WiSE-FT improves accuracy under distribution shift by 4 to 6 percentage points (pp) over prior work while increasing ImageNet accuracy by 1.6 pp. WiSE-FT achieves similarly large robustness gains (2 to 23 pp) on a diverse set of six further distribution shifts, and accuracy gains of 0.8 to 3.3 pp compared to standard fine-tuning on seven commonly used transfer learning datasets. These improvements come at no additional computational cost during fine-tuning or inference.",True,True,"Wortsman, M. and Ilharco, G. and Kim, J. W. and Li, M. and Kornblith, S. and Roelofs, R. and Gontijo-Lopes, R. and Hajishirzi, H. and Farhadi, A. and Namkoong, H. and Schmidt, L.",2022,,,,
Difference Vector Equalization for Robust Fine-tuning of Vision-Language Models,2511.09973v1,Ilharco22,\cite{Ilharco22},Patching open-vocabulary models by interpolating weights,https://arxiv.org/abs/2208.05592v2,"Open-vocabulary models like CLIP achieve high accuracy across many image classification tasks. However, there are still settings where their zero-shot performance is far from optimal. We study model patching, where the goal is to improve accuracy on specific tasks without degrading accuracy on tasks where performance is already adequate. Towards this goal, we introduce PAINT, a patching method that uses interpolations between the weights of a model before fine-tuning and the weights after fine-tuning on a task to be patched. On nine tasks where zero-shot CLIP performs poorly, PAINT increases accuracy by 15 to 60 percentage points while preserving accuracy on ImageNet within one percentage point of the zero-shot model. PAINT also allows a single model to be patched on multiple tasks and improves with model scale. Furthermore, we identify cases of broad transfer, where patching on one task increases accuracy on other tasks even when the tasks have disjoint classes. Finally, we investigate applications beyond common benchmarks such as counting or reducing the impact of typographic attacks on CLIP. Our findings demonstrate that it is possible to expand the set of tasks on which open-vocabulary models achieve high accuracy without re-training them from scratch.",True,True,G. Ilharco and M. Wortsman and S. Y. Gadre and S. Song and H. Hajishirzi and S. Kornblith and A. Farhadi and L. Schmidt,2022,,,,
Difference Vector Equalization for Robust Fine-tuning of Vision-Language Models,2511.09973v1,Kumar22,\cite{Kumar22},Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution,https://arxiv.org/abs/2202.10054v1,"When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer -- the ""head""). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (Breeds-Living17, Breeds-Entity30, DomainNet, CIFAR $\to$ STL, CIFAR10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on average 2% higher accuracy ID but 7% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: fine-tuning overparameterized two-layer linear networks. We prove that the OOD error of fine-tuning is high when we initialize with a fixed or random head -- this is because while fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning heuristic, combines the benefits of both fine-tuning and linear probing. Empirically, LP-FT outperforms both fine-tuning and linear probing on the above datasets (1% better ID, 10% better OOD than full fine-tuning).",True,True,A. Kumar and A. Raghunathan and R. Jones and T. Ma and P. Liang,2022,,,,
Difference Vector Equalization for Robust Fine-tuning of Vision-Language Models,2511.09973v1,Goyal23,\cite{Goyal23},Finetune like you pretrain: Improved finetuning of zero-shot vision models,https://arxiv.org/abs/2212.00638v1,"Finetuning image-text models such as CLIP achieves state-of-the-art accuracies on a variety of benchmarks. However, recent works like WiseFT (Wortsman et al., 2021) and LP-FT (Kumar et al., 2022) have shown that even subtle differences in the finetuning process can lead to surprisingly large differences in the final performance, both for in-distribution (ID) and out-of-distribution (OOD) data. In this work, we show that a natural and simple approach of mimicking contrastive pretraining consistently outperforms alternative finetuning approaches. Specifically, we cast downstream class labels as text prompts and continue optimizing the contrastive loss between image embeddings and class-descriptive prompt embeddings (contrastive finetuning).
  Our method consistently outperforms baselines across 7 distribution shifts, 6 transfer learning, and 3 few-shot learning benchmarks. On WILDS-iWILDCam, our proposed approach FLYP outperforms the top of the leaderboard by $2.3\%$ ID and $2.7\%$ OOD, giving the highest reported accuracy. Averaged across 7 OOD datasets (2 WILDS and 5 ImageNet associated shifts), FLYP gives gains of $4.2\%$ OOD over standard finetuning and outperforms the current state of the art (LP-FT) by more than $1\%$ both ID and OOD. Similarly, on 3 few-shot learning benchmarks, our approach gives gains up to $4.6\%$ over standard finetuning and $4.4\%$ over the state of the art. In total, these benchmarks establish contrastive finetuning as a simple, intuitive, and state-of-the-art approach for supervised finetuning of image-text models like CLIP. Code is available at https://github.com/locuslab/FLYP.",True,True,"Goyal, S. and Kumar, A. and Garg, S. and Kolter, Z. and Raghunathan, A.",2023,,,,
Difference Vector Equalization for Robust Fine-tuning of Vision-Language Models,2511.09973v1,Han24,\cite{Han24},Anchor-based Robust Finetuning of Vision-Language Models,https://arxiv.org/abs/2404.06244v1,"We aim at finetuning a vision-language model without hurting its out-of-distribution (OOD) generalization. We address two types of OOD generalization, i.e., i) domain shift such as natural to sketch images, and ii) zero-shot capability to recognize the category that was not contained in the finetune data. Arguably, the diminished OOD generalization after finetuning stems from the excessively simplified finetuning target, which only provides the class information, such as ``a photo of a [CLASS]''. This is distinct from the process in that CLIP was pretrained, where there is abundant text supervision with rich semantic information. Therefore, we propose to compensate for the finetune process using auxiliary supervision with rich semantic information, which acts as anchors to preserve the OOD generalization. Specifically, two types of anchors are elaborated in our method, including i) text-compensated anchor which uses the images from the finetune set but enriches the text supervision from a pretrained captioner, ii) image-text-pair anchor which is retrieved from the dataset similar to pretraining data of CLIP according to the downstream task, associating with the original CLIP text with rich semantics. Those anchors are utilized as auxiliary semantic information to maintain the original feature space of CLIP, thereby preserving the OOD generalization capabilities. Comprehensive experiments demonstrate that our method achieves in-distribution performance akin to conventional finetuning while attaining new state-of-the-art results on domain shift and zero-shot learning benchmarks.",True,True,"Han, J. and Lin, Z. and Sun, Z. and Gao, Y. and Yan, K. and Ding, S. and Gao, Y. and Xia, G.",2024,,,,
Difference Vector Equalization for Robust Fine-tuning of Vision-Language Models,2511.09973v1,Rolnick19,\cite{Rolnick19},Experience Replay for Continual Learning,,,True,False,"Rolnick, D. and Ahuja, A. and Schwarz, J. and Lillicrap, T. and Wayne, G.",2019,,,,
Difference Vector Equalization for Robust Fine-tuning of Vision-Language Models,2511.09973v1,Sharma18_cc3m,\cite{Sharma18_cc3m},"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning",,,True,False,"Sharma, P. and Ding, N. and Goodman, S. and Soricut, R.",2018,,,,
Difference Vector Equalization for Robust Fine-tuning of Vision-Language Models,2511.09973v1,Chen20_simclr,\cite{Chen20_simclr},A Simple Framework for Contrastive Learning of Visual Representations,,,True,False,"Chen, T. and Kornblith, S. and Norouzi, M. and Hinton, G.",2020,,,,
Difference Vector Equalization for Robust Fine-tuning of Vision-Language Models,2511.09973v1,Gao21,\cite{Gao21},SimCSE: Simple Contrastive Learning of Sentence Embeddings,https://arxiv.org/abs/2104.08821v4,"This paper presents SimCSE, a simple contrastive learning framework that greatly advances state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation, and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework by using ""entailment"" pairs as positives and ""contradiction"" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman's correlation respectively, a 4.2% and 2.2% improvement compared to the previous best results. We also show -- both theoretically and empirically -- that the contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.",True,True,"Gao, T. and Yao, X. and Chen, D.",2021,,,,
Difference Vector Equalization for Robust Fine-tuning of Vision-Language Models,2511.09973v1,Xu18,\cite{Xu18},Spherical Latent Spaces for Stable Variational Autoencoders,,,True,False,"Xu, J. and Durrett, G.",2018,,,,
Difference Vector Equalization for Robust Fine-tuning of Vision-Language Models,2511.09973v1,Wang20_contrastive,\cite{Wang20_contrastive},Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere,https://arxiv.org/abs/2005.10242v10,"Contrastive representation learning has been outstandingly successful in practice. In this work, we identify two key properties related to the contrastive loss: (1) alignment (closeness) of features from positive pairs, and (2) uniformity of the induced distribution of the (normalized) features on the hypersphere. We prove that, asymptotically, the contrastive loss optimizes these properties, and analyze their positive effects on downstream tasks. Empirically, we introduce an optimizable metric to quantify each property. Extensive experiments on standard vision and language datasets confirm the strong agreement between both metrics and downstream task performance. Remarkably, directly optimizing for these two metrics leads to representations with comparable or better performance at downstream tasks than contrastive learning.
  Project Page: https://tongzhouwang.info/hypersphere
  Code: https://github.com/SsnL/align_uniform , https://github.com/SsnL/moco_align_uniform",True,True,"Wang, T. and Isola, P.",2020,,,,
Difference Vector Equalization for Robust Fine-tuning of Vision-Language Models,2511.09973v1,Goel22,\cite{Goel22},Cy{CLIP}: Cyclic Contrastive Language-Image Pretraining,,,True,False,S. Goel and H. Bansal and S. Bhatia and R. A. Rossi and V. Vinay and A. Grover,2022,,,,
Difference Vector Equalization for Robust Fine-tuning of Vision-Language Models,2511.09973v1,Oh23,\cite{Oh23},Geodesic Multi-Modal Mixup for Robust Fine-Tuning,,,True,False,C. Oh and J. So and H. Byun and Y. Lim and M. Shin and J. Jeon and K. Song,2023,,,,
Difference Vector Equalization for Robust Fine-tuning of Vision-Language Models,2511.09973v1,Yamaguchi25,\cite{Yamaguchi25},Post-pre-training for Modality Alignment in Vision-Language Foundation Models,,,True,False,"Yamaguchi, S. and Feng, D. and Kanai, S. and Adachi, K. and Chijiwa, D.",2025,,,,
Difference Vector Equalization for Robust Fine-tuning of Vision-Language Models,2511.09973v1,Zhou22_coop,\cite{Zhou22_coop},Learning to Prompt for Vision-Language Models,,,True,False,"Zhou, K. and Yang, J. and Loy, C. C. and Liu, Z.",2022,,,,International Journal of Computer Vision~(IJCV)
Difference Vector Equalization for Robust Fine-tuning of Vision-Language Models,2511.09973v1,Zhou22_cocoop,\cite{Zhou22_cocoop},Conditional Prompt Learning for Vision-Language Models,,,True,False,"Zhou, K. and Yang, J. and Loy, C. C. and Liu, Z.",2022,,,,
Difference Vector Equalization for Robust Fine-tuning of Vision-Language Models,2511.09973v1,Khattak23,\cite{Khattak23},MaPLe: Multi-modal Prompt Learning,https://arxiv.org/abs/2210.03117v3,"Pre-trained vision-language (V-L) models such as CLIP have shown excellent generalization ability to downstream tasks. However, they are sensitive to the choice of input text prompts and require careful selection of prompt templates to perform well. Inspired by the Natural Language Processing (NLP) literature, recent CLIP adaptation approaches learn prompts as the textual inputs to fine-tune CLIP for downstream tasks. We note that using prompting to adapt representations in a single branch of CLIP (language or vision) is sub-optimal since it does not allow the flexibility to dynamically adjust both representation spaces on a downstream task. In this work, we propose Multi-modal Prompt Learning (MaPLe) for both vision and language branches to improve alignment between the vision and language representations. Our design promotes strong coupling between the vision-language prompts to ensure mutual synergy and discourages learning independent uni-modal solutions. Further, we learn separate prompts across different early stages to progressively model the stage-wise feature relationships to allow rich context learning. We evaluate the effectiveness of our approach on three representative tasks of generalization to novel classes, new target datasets and unseen domain shifts. Compared with the state-of-the-art method Co-CoOp, MaPLe exhibits favorable performance and achieves an absolute gain of 3.45% on novel classes and 2.72% on overall harmonic-mean, averaged over 11 diverse image recognition datasets. Our code and pre-trained models are available at https://github.com/muzairkhattak/multimodal-prompt-learning.",True,True,"Khattak, M. U. and Rasheed, H. and Maaz, M. and Khan, S. and Khan, F. S.",2023,,,,
Difference Vector Equalization for Robust Fine-tuning of Vision-Language Models,2511.09973v1,Shu23,\cite{Shu23},CLIPood: Generalizing CLIP to Out-of-Distributions,https://arxiv.org/abs/2302.00864v2,"Out-of-distribution (OOD) generalization, where the model needs to handle distribution shifts from training, is a major challenge of machine learning. Contrastive language-image pre-training (CLIP) models have shown impressive zero-shot ability, but the further adaptation of CLIP on downstream tasks undesirably degrades OOD performances. This paper aims at generalizing CLIP to out-of-distribution test data on downstream tasks. We propose CLIPood, a fine-tuning method that can adapt CLIP models to OOD situations where both domain shifts and open classes may occur on the unseen test data. To exploit the semantic relations between classes from the text modality, CLIPood introduces a new training objective, margin metric softmax (MMS), with class adaptive margins for fine-tuning. To incorporate both pre-trained zero-shot model and fine-tuned task-adaptive model, CLIPood leverages a new optimization strategy, Beta moving average (BMA), to maintain a temporal ensemble weighted by Beta distribution. Experiments on diverse datasets with different OOD scenarios show that CLIPood consistently outperforms existing generalization techniques.",True,True,Y. Shu and X. Guo and J. Wu and X. Wang and J. Wang and M. Long,2023,,,,
Difference Vector Equalization for Robust Fine-tuning of Vision-Language Models,2511.09973v1,Wang24,\cite{Wang24},"A Comprehensive Survey of Continual Learning: Theory, Method and Application",https://arxiv.org/abs/2302.00487v3,"To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance degradation of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic significance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of continual learning as ensuring a proper stability-plasticity trade-off and an adequate intra/inter-task generalizability in the context of resource efficiency. Then we provide a state-of-the-art and elaborated taxonomy, extensively analyzing how representative methods address continual learning, and how they are adapted to particular challenges in realistic applications. Through an in-depth discussion of promising directions, we believe that such a holistic perspective can greatly facilitate subsequent exploration in this field and beyond.",True,True,"Wang, L. and Zhang, X. and Su, H. and Zhu, J.",2024,,,,IEEE Transactions on Pattern Analysis and Machine Intelligence
Difference Vector Equalization for Robust Fine-tuning of Vision-Language Models,2511.09973v1,Yu24,\cite{Yu24},Select and distill: Selective dual-teacher knowledge transfer for continual learning on vision-language models,,,True,False,"Yu, Y. and Huang, C. and Chen, J. and Chang, K. and Lai, Y. and Yang, F. and Wang, Y. F.",2024,,,,
MINDS: A Cross-cultural Dialogue Corpus for Social Norm Classification and Adherence Detection,2511.09918v1,ziems2023normbank,\cite{ziems2023normbank},NormBank: A Knowledge Bank of Situational Social Norms,https://arxiv.org/abs/2305.17008v2,"We present NormBank, a knowledge bank of 155k situational norms. This resource is designed to ground flexible normative reasoning for interactive, assistive, and collaborative AI systems. Unlike prior commonsense resources, NormBank grounds each inference within a multivalent sociocultural frame, which includes the setting (e.g., restaurant), the agents' contingent roles (waiter, customer), their attributes (age, gender), and other physical, social, and cultural constraints (e.g., the temperature or the country of operation). In total, NormBank contains 63k unique constraints from a taxonomy that we introduce and iteratively refine here. Constraints then apply in different combinations to frame social norms. Under these manipulations, norms are non-monotonic - one can cancel an inference by updating its frame even slightly. Still, we find evidence that neural models can help reliably extend the scope and coverage of NormBank. We further demonstrate the utility of this resource with a series of transfer experiments.",True,True,"Ziems, Caleb and Dwivedi-Yu, Jane and Wang, Yi-Chia and Halevy, Alon and Yang, Diyi",2023,,,,arXiv preprint arXiv:2305.17008
MINDS: A Cross-cultural Dialogue Corpus for Social Norm Classification and Adherence Detection,2511.09918v1,sap2019atomic,\cite{sap2019atomic},Atomic: An atlas of machine commonsense for if-then reasoning,,,True,False,"Sap, Maarten and Le Bras, Ronan and Allaway, Emily and Bhagavatula, Chandra and Lourie, Nicholas and Rashkin, Hannah and Roof, Brendan and Smith, Noah A and Choi, Yejin",2019,,,,
MINDS: A Cross-cultural Dialogue Corpus for Social Norm Classification and Adherence Detection,2511.09918v1,rashkin2018event2mind,\cite{rashkin2018event2mind},"Event2Mind: Commonsense Inference on Events, Intents, and Reactions",https://arxiv.org/abs/1805.06939v2,"We investigate a new commonsense inference task: given an event described in a short free-form text (""X drinks coffee in the morning""), a system reasons about the likely intents (""X wants to stay awake"") and reactions (""X feels alert"") of the event's participants. To support this study, we construct a new crowdsourced corpus of 25,000 event phrases covering a diverse range of everyday events and situations. We report baseline performance on this task, demonstrating that neural encoder-decoder models can successfully compose embedding representations of previously unseen events and reason about the likely intents and reactions of the event participants. In addition, we demonstrate how commonsense inference on people's intents and reactions can help unveil the implicit gender inequality prevalent in modern movie scripts.",True,True,"Rashkin, Hannah and Sap, Maarten and Allaway, Emily and Smith, Noah A and Choi, Yejin",2018,,,,arXiv preprint arXiv:1805.06939
MINDS: A Cross-cultural Dialogue Corpus for Social Norm Classification and Adherence Detection,2511.09918v1,emelin2020moral,\cite{emelin2020moral},"Moral Stories: Situated Reasoning about Norms, Intents, Actions, and their Consequences",https://arxiv.org/abs/2012.15738v1,"In social settings, much of human behavior is governed by unspoken rules of conduct. For artificial systems to be fully integrated into social environments, adherence to such norms is a central prerequisite. We investigate whether contemporary NLG models can function as behavioral priors for systems deployed in social settings by generating action hypotheses that achieve predefined goals under moral constraints. Moreover, we examine if models can anticipate likely consequences of (im)moral actions, or explain why certain actions are preferable by generating relevant norms. For this purpose, we introduce 'Moral Stories', a crowd-sourced dataset of structured, branching narratives for the study of grounded, goal-oriented social reasoning. Finally, we propose decoding strategies that effectively combine multiple expert models to significantly improve the quality of generated actions, consequences, and norms compared to strong baselines, e.g. though abductive reasoning.",True,True,"Emelin, Denis and Bras, Ronan Le and Hwang, Jena D and Forbes, Maxwell and Choi, Yejin",2020,,,,arXiv preprint arXiv:2012.15738
MINDS: A Cross-cultural Dialogue Corpus for Social Norm Classification and Adherence Detection,2511.09918v1,jiang2021can,\cite{jiang2021can},Can machines learn morality? the delphi experiment,,,True,False,"Jiang, Liwei and Hwang, Jena D and Bhagavatula, Chandra and Bras, Ronan Le and Liang, Jenny and Dodge, Jesse and Sakaguchi, Keisuke and Forbes, Maxwell and Borchardt, Jon and Gabriel, Saadia and others",2021,,,,arXiv preprint arXiv:2110.07574
MINDS: A Cross-cultural Dialogue Corpus for Social Norm Classification and Adherence Detection,2511.09918v1,kim2022prosocialdialog,\cite{kim2022prosocialdialog},Prosocialdialog: A prosocial backbone for conversational agents,,,True,False,"Kim, Hyunwoo and Yu, Youngjae and Jiang, Liwei and Lu, Ximing and Khashabi, Daniel and Kim, Gunhee and Choi, Yejin and Sap, Maarten",2022,,,,arXiv preprint arXiv:2205.12688
MINDS: A Cross-cultural Dialogue Corpus for Social Norm Classification and Adherence Detection,2511.09918v1,gu2021dream,\cite{gu2021dream},DREAM: Improving Situational QA by First Elaborating the Situation,https://arxiv.org/abs/2112.08656v3,"When people answer questions about a specific situation, e.g., ""I cheated on my mid-term exam last week. Was that wrong?"", cognitive science suggests that they form a mental picture of that situation before answering. While we do not know how language models (LMs) answer such questions, we conjecture that they may answer more accurately if they are also provided with additional details about the question situation, elaborating the ""scene"". To test this conjecture, we train a new model, DREAM, to answer questions that elaborate the scenes that situated questions are about, and then provide those elaborations as additional context to a question-answering (QA) model. We find that DREAM is able to create better scene elaborations (more accurate, useful, and consistent) than a representative state-of-the-art, zero-shot model (Macaw). We also find that using the scene elaborations as additional context improves the answer accuracy of a downstream QA system, including beyond that obtainable by simply further finetuning the QA system on DREAM's training data. These results suggest that adding focused elaborations about a situation can improve a system's reasoning about it, and may serve as an effective way of injecting new scenario based knowledge into QA models. Finally, our approach is dataset-neutral; we observe improved QA performance across different models, with even bigger gains on models with fewer parameters. We make our dataset and model publicly available at https://github.com/allenai/dream.",True,True,"Gu, Yuling and Mishra, Bhavana Dalvi and Clark, Peter",2021,,,,arXiv preprint arXiv:2112.08656
MINDS: A Cross-cultural Dialogue Corpus for Social Norm Classification and Adherence Detection,2511.09918v1,ziems2022moral,\cite{ziems2022moral},The moral integrity corpus: A benchmark for ethical dialogue systems,,,True,False,"Ziems, Caleb and Yu, Jane A and Wang, Yi-Chia and Halevy, Alon and Yang, Diyi",2022,,,,arXiv preprint arXiv:2204.03021
MINDS: A Cross-cultural Dialogue Corpus for Social Norm Classification and Adherence Detection,2511.09918v1,ch2023sociocultural,\cite{ch2023sociocultural},Sociocultural norm similarities and differences via situational alignment and explainable textual entailment,,,True,False,"CH-Wang, Sky and Saakyan, Arkadiy and Li, Oliver and Yu, Zhou and Muresan, Smaranda",2023,,,,arXiv preprint arXiv:2305.14492
MINDS: A Cross-cultural Dialogue Corpus for Social Norm Classification and Adherence Detection,2511.09918v1,forbes2020social,\cite{forbes2020social},Social Chemistry 101: Learning to Reason about Social and Moral Norms,https://arxiv.org/abs/2011.00620v3,"Social norms -- the unspoken commonsense rules about acceptable social behavior -- are crucial in understanding the underlying causes and intents of people's actions in narratives. For example, underlying an action such as ""wanting to call cops on my neighbors"" are social norms that inform our conduct, such as ""It is expected that you report crimes.""
  We present Social Chemistry, a new conceptual formalism to study people's everyday social norms and moral judgments over a rich spectrum of real life situations described in natural language. We introduce Social-Chem-101, a large-scale corpus that catalogs 292k rules-of-thumb such as ""it is rude to run a blender at 5am"" as the basic conceptual units. Each rule-of-thumb is further broken down with 12 different dimensions of people's judgments, including social judgments of good and bad, moral foundations, expected cultural pressure, and assumed legality, which together amount to over 4.5 million annotations of categorical labels and free-text descriptions.
  Comprehensive empirical results based on state-of-the-art neural models demonstrate that computational modeling of social norms is a promising research direction. Our model framework, Neural Norm Transformer, learns and generalizes Social-Chem-101 to successfully reason about previously unseen situations, generating relevant (and potentially novel) attribute-aware social rules-of-thumb.",True,True,"Forbes, Maxwell  and
      Hwang, Jena D.  and
      Shwartz, Vered  and
      Sap, Maarten  and
      Choi, Yejin",2020,,https://aclanthology.org/2020.emnlp-main.48/,10.18653/v1/2020.emnlp-main.48,
MINDS: A Cross-cultural Dialogue Corpus for Social Norm Classification and Adherence Detection,2511.09918v1,fung2022normsage,\cite{fung2022normsage},NormSAGE: Multi-Lingual Multi-Cultural Norm Discovery from Conversations On-the-Fly,https://arxiv.org/abs/2210.08604v2,"Norm discovery is important for understanding and reasoning about the acceptable behaviors and potential violations in human communication and interactions. We introduce NormSage, a framework for addressing the novel task of conversation-grounded multi-lingual, multi-cultural norm discovery, based on language model prompting and self-verification. NormSAGE leverages the expressiveness and implicit knowledge of the pretrained GPT-3 language model backbone, to elicit knowledge about norms through directed questions representing the norm discovery task and conversation context. It further addresses the risk of language model hallucination with a self-verification mechanism ensuring that the norms discovered are correct and are substantially grounded to their source conversations. Evaluation results show that our approach discovers significantly more relevant and insightful norms for conversations on-the-fly compared to baselines (>10+% in Likert scale rating). The norms discovered from Chinese conversation are also comparable to the norms discovered from English conversation in terms of insightfulness and correctness (<3% difference). In addition, the culture-specific norms are promising quality, allowing for 80% accuracy in culture pair human identification. Finally, our grounding process in norm discovery self-verification can be extended for instantiating the adherence and violation of any norm for a given conversation on-the-fly, with explainability and transparency. NormSAGE achieves an AUC of 95.4% in grounding, with natural language explanation matching human-written quality.",True,True,"Fung, Yi R and Chakraborty, Tuhin and Guo, Hao and Rambow, Owen and Muresan, Smaranda and Ji, Heng",2022,,,,arXiv preprint arXiv:2210.08604
MINDS: A Cross-cultural Dialogue Corpus for Social Norm Classification and Adherence Detection,2511.09918v1,li2023normdial,\cite{li2023normdial},NormDial: A Comparable Bilingual Synthetic Dialog Dataset for Modeling Social Norm Adherence and Violation,https://arxiv.org/abs/2310.14563v2,"Social norms fundamentally shape interpersonal communication. We present NormDial, a high-quality dyadic dialogue dataset with turn-by-turn annotations of social norm adherences and violations for Chinese and American cultures. Introducing the task of social norm observance detection, our dataset is synthetically generated in both Chinese and English using a human-in-the-loop pipeline by prompting large language models with a small collection of expert-annotated social norms. We show that our generated dialogues are of high quality through human evaluation and further evaluate the performance of existing large language models on this task. Our findings point towards new directions for understanding the nuances of social norms as they manifest in conversational contexts that span across languages and cultures.",True,True,"Li, Oliver and Subramanian, Mallika and Saakyan, Arkadiy and CH-Wang, Sky and Muresan, Smaranda",2023,,,,arXiv preprint arXiv:2310.14563
MINDS: A Cross-cultural Dialogue Corpus for Social Norm Classification and Adherence Detection,2511.09918v1,zhan2023socialdial,\cite{zhan2023socialdial},Socialdial: A benchmark for socially-aware dialogue systems,,,True,False,"Zhan, Haolan and Li, Zhuang and Wang, Yufei and Luo, Linhao and Feng, Tao and Kang, Xiaoxi and Hua, Yuncheng and Qu, Lizhen and Soon, Lay-Ki and Sharma, Suraj and others",2023,,,,
AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting,2511.09827v1,mildenhall2020nerf,\cite{mildenhall2020nerf},NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis,https://arxiv.org/abs/2003.08934v2,"We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(θ, φ)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",True,True,,,,,,
AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting,2511.09827v1,luiten2023dynamic,\cite{luiten2023dynamic},Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis,https://arxiv.org/abs/2308.09713v1,"We present a method that simultaneously addresses the tasks of dynamic scene novel-view synthesis and six degree-of-freedom (6-DOF) tracking of all dense scene elements. We follow an analysis-by-synthesis framework, inspired by recent work that models scenes as a collection of 3D Gaussians which are optimized to reconstruct input images via differentiable rendering. To model dynamic scenes, we allow Gaussians to move and rotate over time while enforcing that they have persistent color, opacity, and size. By regularizing Gaussians' motion and rotation with local-rigidity constraints, we show that our Dynamic 3D Gaussians correctly model the same area of physical space over time, including the rotation of that space. Dense 6-DOF tracking and dynamic reconstruction emerges naturally from persistent dynamic view synthesis, without requiring any correspondence or flow as input. We demonstrate a large number of downstream applications enabled by our representation, including first-person view synthesis, dynamic compositional scene synthesis, and 4D video editing.",True,True,,,,,,
AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting,2511.09827v1,guedon2023sugar,\cite{guedon2023sugar},SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering,https://arxiv.org/abs/2311.12775v3,"We propose a method to allow precise and extremely fast mesh extraction from 3D Gaussian Splatting. Gaussian Splatting has recently become very popular as it yields realistic rendering while being significantly faster to train than NeRFs. It is however challenging to extract a mesh from the millions of tiny 3D gaussians as these gaussians tend to be unorganized after optimization and no method has been proposed so far. Our first key contribution is a regularization term that encourages the gaussians to align well with the surface of the scene. We then introduce a method that exploits this alignment to extract a mesh from the Gaussians using Poisson reconstruction, which is fast, scalable, and preserves details, in contrast to the Marching Cubes algorithm usually applied to extract meshes from Neural SDFs. Finally, we introduce an optional refinement strategy that binds gaussians to the surface of the mesh, and jointly optimizes these Gaussians and the mesh through Gaussian splatting rendering. This enables easy editing, sculpting, rigging, animating, compositing and relighting of the Gaussians using traditional softwares by manipulating the mesh instead of the gaussians themselves. Retrieving such an editable mesh for realistic rendering is done within minutes with our method, compared to hours with the state-of-the-art methods on neural SDFs, while providing a better rendering quality. Our project page is the following: https://anttwo.github.io/sugar/",True,True,,,,,,
AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting,2511.09827v1,mihajlovic2024SplatFields,\cite{mihajlovic2024SplatFields},SplatFields: Neural Gaussian Splats for Sparse 3D and 4D Reconstruction,https://arxiv.org/abs/2409.11211v1,"Digitizing 3D static scenes and 4D dynamic events from multi-view images has long been a challenge in computer vision and graphics. Recently, 3D Gaussian Splatting (3DGS) has emerged as a practical and scalable reconstruction method, gaining popularity due to its impressive reconstruction quality, real-time rendering capabilities, and compatibility with widely used visualization tools. However, the method requires a substantial number of input views to achieve high-quality scene reconstruction, introducing a significant practical bottleneck. This challenge is especially severe in capturing dynamic scenes, where deploying an extensive camera array can be prohibitively costly. In this work, we identify the lack of spatial autocorrelation of splat features as one of the factors contributing to the suboptimal performance of the 3DGS technique in sparse reconstruction settings. To address the issue, we propose an optimization strategy that effectively regularizes splat features by modeling them as the outputs of a corresponding implicit neural field. This results in a consistent enhancement of reconstruction quality across various scenarios. Our approach effectively handles static and dynamic cases, as demonstrated by extensive testing across different setups and scene complexities.",True,True,,,,,,
AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting,2511.09827v1,alldieck2018video,\cite{alldieck2018video},Detailed Human Avatars from Monocular Video,https://arxiv.org/abs/1808.01338v1,"We present a novel method for high detail-preserving human avatar creation from monocular video. A parameterized body model is refined and optimized to maximally resemble subjects from a video showing them from all sides. Our avatars feature a natural face, hairstyle, clothes with garment wrinkles, and high-resolution texture. Our paper contributes facial landmark and shading-based human body shape refinement, a semantic texture prior, and a novel texture stitching strategy, resulting in the most sophisticated-looking human avatars obtained from a single video to date. Numerous results show the robustness and versatility of our method. A user study illustrates its superiority over the state-of-the-art in terms of identity preservation, level of detail, realism, and overall user preference.",True,True,,,,,,
AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting,2511.09827v1,alldieck2021imghum,\cite{alldieck2021imghum},imGHUM: Implicit Generative Models of 3D Human Shape and Articulated Pose,https://arxiv.org/abs/2108.10842v1,"We present imGHUM, the first holistic generative model of 3D human shape and articulated pose, represented as a signed distance function. In contrast to prior work, we model the full human body implicitly as a function zero-level-set and without the use of an explicit template mesh. We propose a novel network architecture and a learning paradigm, which make it possible to learn a detailed implicit generative model of human pose, shape, and semantics, on par with state-of-the-art mesh-based models. Our model features desired detail for human models, such as articulated pose including hand motion and facial expressions, a broad spectrum of shape variations, and can be queried at arbitrary resolutions and spatial locations. Additionally, our model has attached spatial semantics making it straightforward to establish correspondences between different shape instances, thus enabling applications that are difficult to tackle using classical implicit representations. In extensive experiments, we demonstrate the model accuracy and its applicability to current research problems.",True,True,,,,,,
AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting,2511.09827v1,kocabas2023hugs,\cite{kocabas2023hugs},HUGS: Human Gaussian Splats,https://arxiv.org/abs/2311.17910v1,"Recent advances in neural rendering have improved both training and rendering times by orders of magnitude. While these methods demonstrate state-of-the-art quality and speed, they are designed for photogrammetry of static scenes and do not generalize well to freely moving humans in the environment. In this work, we introduce Human Gaussian Splats (HUGS) that represents an animatable human together with the scene using 3D Gaussian Splatting (3DGS). Our method takes only a monocular video with a small number of (50-100) frames, and it automatically learns to disentangle the static scene and a fully animatable human avatar within 30 minutes. We utilize the SMPL body model to initialize the human Gaussians. To capture details that are not modeled by SMPL (e.g. cloth, hairs), we allow the 3D Gaussians to deviate from the human body model. Utilizing 3D Gaussians for animated humans brings new challenges, including the artifacts created when articulating the Gaussians. We propose to jointly optimize the linear blend skinning weights to coordinate the movements of individual Gaussians during animation. Our approach enables novel-pose synthesis of human and novel view synthesis of both the human and the scene. We achieve state-of-the-art rendering quality with a rendering speed of 60 FPS while being ~100x faster to train over previous work. Our code will be announced here: https://github.com/apple/ml-hugs",True,True,,,,,,
AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting,2511.09827v1,abdal2023gaussian,\cite{abdal2023gaussian},Gaussian Shell Maps for Efficient 3D Human Generation,https://arxiv.org/abs/2311.17857v1,"Efficient generation of 3D digital humans is important in several industries, including virtual reality, social media, and cinematic production. 3D generative adversarial networks (GANs) have demonstrated state-of-the-art (SOTA) quality and diversity for generated assets. Current 3D GAN architectures, however, typically rely on volume representations, which are slow to render, thereby hampering the GAN training and requiring multi-view-inconsistent 2D upsamplers. Here, we introduce Gaussian Shell Maps (GSMs) as a framework that connects SOTA generator network architectures with emerging 3D Gaussian rendering primitives using an articulable multi shell--based scaffold. In this setting, a CNN generates a 3D texture stack with features that are mapped to the shells. The latter represent inflated and deflated versions of a template surface of a digital human in a canonical body pose. Instead of rasterizing the shells directly, we sample 3D Gaussians on the shells whose attributes are encoded in the texture features. These Gaussians are efficiently and differentiably rendered. The ability to articulate the shells is important during GAN training and, at inference time, to deform a body into arbitrary user-defined poses. Our efficient rendering scheme bypasses the need for view-inconsistent upsamplers and achieves high-quality multi-view consistent renderings at a native resolution of $512 \times 512$ pixels. We demonstrate that GSMs successfully generate 3D humans when trained on single-view datasets, including SHHQ and DeepFashion.",True,True,,,,,,
AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting,2511.09827v1,zielonka2023drivable,\cite{zielonka2023drivable},Drivable 3D Gaussian Avatars,https://arxiv.org/abs/2311.08581v2,"We present Drivable 3D Gaussian Avatars (D3GA), a multi-layered 3D controllable model for human bodies that utilizes 3D Gaussian primitives embedded into tetrahedral cages. The advantage of using cages compared to commonly employed linear blend skinning (LBS) is that primitives like 3D Gaussians are naturally re-oriented and their kernels are stretched via the deformation gradients of the encapsulating tetrahedron. Additional offsets are modeled for the tetrahedron vertices, effectively decoupling the low-dimensional driving poses from the extensive set of primitives to be rendered. This separation is achieved through the localized influence of each tetrahedron on 3D Gaussians, resulting in improved optimization. Using the cage-based deformation model, we introduce a compositional pipeline that decomposes an avatar into layers, such as garments, hands, or faces, improving the modeling of phenomena like garment sliding. These parts can be conditioned on different driving signals, such as keypoints for facial expressions or joint-angle vectors for garments and the body. Our experiments on two multi-view datasets with varied body shapes, clothes, and motions show higher-quality results. They surpass PSNR and SSIM metrics of other SOTA methods using the same data while offering greater flexibility and compactness.",True,True,,,,,,
AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting,2511.09827v1,dhamo2023headgas,\cite{dhamo2023headgas},HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting,https://arxiv.org/abs/2312.02902v2,"3D head animation has seen major quality and runtime improvements over the last few years, particularly empowered by the advances in differentiable rendering and neural radiance fields. Real-time rendering is a highly desirable goal for real-world applications. We propose HeadGaS, a model that uses 3D Gaussian Splats (3DGS) for 3D head reconstruction and animation. In this paper we introduce a hybrid model that extends the explicit 3DGS representation with a base of learnable latent features, which can be linearly blended with low-dimensional parameters from parametric head models to obtain expression-dependent color and opacity values. We demonstrate that HeadGaS delivers state-of-the-art results in real-time inference frame rates, surpassing baselines by up to 2dB, while accelerating rendering speed by over x10.",True,True,,,,,,
AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting,2511.09827v1,junkawitsch2025eva,\cite{junkawitsch2025eva},EVA: Expressive Virtual Avatars from Multi-view Videos,https://arxiv.org/abs/2505.15385v1,"With recent advancements in neural rendering and motion capture algorithms, remarkable progress has been made in photorealistic human avatar modeling, unlocking immense potential for applications in virtual reality, augmented reality, remote communication, and industries such as gaming, film, and medicine. However, existing methods fail to provide complete, faithful, and expressive control over human avatars due to their entangled representation of facial expressions and body movements. In this work, we introduce Expressive Virtual Avatars (EVA), an actor-specific, fully controllable, and expressive human avatar framework that achieves high-fidelity, lifelike renderings in real time while enabling independent control of facial expressions, body movements, and hand gestures. Specifically, our approach designs the human avatar as a two-layer model: an expressive template geometry layer and a 3D Gaussian appearance layer. First, we present an expressive template tracking algorithm that leverages coarse-to-fine optimization to accurately recover body motions, facial expressions, and non-rigid deformation parameters from multi-view videos. Next, we propose a novel decoupled 3D Gaussian appearance model designed to effectively disentangle body and facial appearance. Unlike unified Gaussian estimation approaches, our method employs two specialized and independent modules to model the body and face separately. Experimental results demonstrate that EVA surpasses state-of-the-art methods in terms of rendering quality and expressiveness, validating its effectiveness in creating full-body avatars. This work represents a significant advancement towards fully drivable digital human models, enabling the creation of lifelike digital avatars that faithfully replicate human geometry and appearance.",True,True,,,,,,
AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting,2511.09827v1,mir2025gaspacho,\cite{mir2025gaspacho},GASPACHO: Gaussian Splatting for Controllable Humans and Objects,https://arxiv.org/abs/2503.09342v1,"We present GASPACHO: a method for generating photorealistic controllable renderings of human-object interactions. Given a set of multi-view RGB images of human-object interactions, our method reconstructs animatable templates of the human and object as separate sets of Gaussians simultaneously. Different from existing work, which focuses on human reconstruction and ignores objects as background, our method explicitly reconstructs both humans and objects, thereby allowing for controllable renderings of novel human object interactions in different poses from novel-camera viewpoints. During reconstruction, we constrain the Gaussians that generate rendered images to be a linear function of a set of canonical Gaussians. By simply changing the parameters of the linear deformation functions after training, our method can generate renderings of novel human-object interaction in novel poses from novel camera viewpoints. We learn the 3D Gaussian properties of the canonical Gaussians on the underlying 2D manifold of the canonical human and object templates. This in turns requires a canonical object template with a fixed UV unwrapping. To define such an object template, we use a feature based representation to track the object across the multi-view sequence. We further propose an occlusion aware photometric loss that allows for reconstructions under significant occlusions. Several experiments on two human-object datasets - BEHAVE and DNA-Rendering - demonstrate that our method allows for high-quality reconstruction of human and object templates under significant occlusion and the synthesis of controllable renderings of novel human-object interactions in novel human poses from novel camera views.",True,True,,,,,,
AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting,2511.09827v1,tevet2023human,\cite{tevet2023human},Human Motion Diffusion as a Generative Prior,https://arxiv.org/abs/2303.01418v3,"Recent work has demonstrated the significant potential of denoising diffusion models for generating human motion, including text-to-motion capabilities. However, these methods are restricted by the paucity of annotated motion data, a focus on single-person motions, and a lack of detailed control. In this paper, we introduce three forms of composition based on diffusion priors: sequential, parallel, and model composition. Using sequential composition, we tackle the challenge of long sequence generation. We introduce DoubleTake, an inference-time method with which we generate long animations consisting of sequences of prompted intervals and their transitions, using a prior trained only for short clips. Using parallel composition, we show promising steps toward two-person generation. Beginning with two fixed priors as well as a few two-person training examples, we learn a slim communication block, ComMDM, to coordinate interaction between the two resulting motions. Lastly, using model composition, we first train individual priors to complete motions that realize a prescribed motion for a given joint. We then introduce DiffusionBlending, an interpolation mechanism to effectively blend several such models to enable flexible and efficient fine-grained joint and trajectory-level control and editing. We evaluate the composition methods using an off-the-shelf motion diffusion model, and further compare the results to dedicated models trained for these specific tasks.",True,True,,,,,,
AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting,2511.09827v1,barquero2024seamless,\cite{barquero2024seamless},Seamless Human Motion Composition with Blended Positional Encodings,https://arxiv.org/abs/2402.15509v1,"Conditional human motion generation is an important topic with many applications in virtual reality, gaming, and robotics. While prior works have focused on generating motion guided by text, music, or scenes, these typically result in isolated motions confined to short durations. Instead, we address the generation of long, continuous sequences guided by a series of varying textual descriptions. In this context, we introduce FlowMDM, the first diffusion-based model that generates seamless Human Motion Compositions (HMC) without any postprocessing or redundant denoising steps. For this, we introduce the Blended Positional Encodings, a technique that leverages both absolute and relative positional encodings in the denoising chain. More specifically, global motion coherence is recovered at the absolute stage, whereas smooth and realistic transitions are built at the relative stage. As a result, we achieve state-of-the-art results in terms of accuracy, realism, and smoothness on the Babel and HumanML3D datasets. FlowMDM excels when trained with only a single description per motion sequence thanks to its Pose-Centric Cross-ATtention, which makes it robust against varying text descriptions at inference time. Finally, to address the limitations of existing HMC metrics, we propose two new metrics: the Peak Jerk and the Area Under the Jerk, to detect abrupt transitions.",True,True,,,,,,
Enhancing Explainability in Solar Energetic Particle Event Prediction: A Global Feature Mapping Approach,2511.09475v1,Whitman2023,\cite{Whitman2023},Review of Solar Energetic Particle Prediction Models,,,True,False,"Whitman, Kathryn and Egeland, et al.",2023,,,10.1016/j.asr.2022.08.006,Advances in Space Research
Enhancing Explainability in Solar Energetic Particle Event Prediction: A Global Feature Mapping Approach,2511.09475v1,Rotti2024,\cite{Rotti2024},Short-term Classification of Strong Solar Energetic Particle Events using Multivariate Time Series Classifiers,https://arxiv.org/abs/2403.17418v1,"Solar energetic particle (SEP) events are one of the most crucial aspects of space weather that require continuous monitoring and forecasting. Their prediction depends on various factors including source eruptions. In the present work, we use the Geostationary Solar Energetic Particle (GSEP) data set covering solar cycles 22, 23, and 24. We develop a framework using time series-based machine learning (ML) models with the aim of developing robust short-term forecasts by classifying SEP events. For this purpose, we introduce an ensemble learning approach that merges the results from univariate time series of three proton channels (10 MeV, 50 MeV, and 100 MeV) and the long band X-ray flux channel from the Geostationary Operational Environmental Satellite (GOES) missions and analyze their performance. We consider three models, namely, time series forest (TSF), supervised time series forest (STSF) and bag of SFA symbols (BOSS). Our study also focuses on understanding and developing confidence in the predictive capabilities of our models. Therefore, we utilize multiple evaluation techniques and metrics. Based on that, we find STSF to perform well in all scenarios. The summary of metrics for the STSF model is as follows: AUC = 0.981; F1-score = 0.960; TSS = 0.919; HSS = 0.920; GSS = 0.852; and MCC = 0.920. The Brier score loss of the STSF model is 0.077. This work lays the foundation for building near-real-time (NRT) short-term SEP event predictions using robust ML methods.",True,True,"Rotti,  Sumanth A. and Aydin,  Berkay and Martens,  Petrus C.",2024,,http://dx.doi.org/10.3847/1538-4357/ad374e,10.3847/1538-4357/ad374e,The Astrophysical Journal
Enhancing Explainability in Solar Energetic Particle Event Prediction: A Global Feature Mapping Approach,2511.09475v1,Boubrahimi2017,\cite{Boubrahimi2017},On the prediction of gt;100 MeV solar energetic particle events using GOES satellite data,,,True,False,"Boubrahimi,  Soukaina Filali and Aydin,  Berkay and Martens,  Petrus and Angryk,  Rafal",2017,,,10.1109/bigdata.2017.8258212,
Enhancing Explainability in Solar Energetic Particle Event Prediction: A Global Feature Mapping Approach,2511.09475v1,9750381,\cite{9750381},A Modular Approach to Building Solar Energetic Particle Event Forecasting Systems,,,True,False,"Ji, Anli and Arya, Akhil and Kempton, Dustin and Angryk, Rafal and Georgoulis, Manolis K. and Aydin, Berkay",2021,,,10.1109/CogMI52975.2021.00022,
Enhancing Explainability in Solar Energetic Particle Event Prediction: A Global Feature Mapping Approach,2511.09475v1,Kim2018,\cite{Kim2018},"A technique for prediction of SPEs from solar radio flux by statistical analysis,  ANN and GA",,,True,False,"Kim,  Kyong Nam and Sin,  Sun Ae and Song,  Kum Ae and Kong,  Jin Hyok",2018,,,10.1007/s10509-018-3263-8,Astrophysics and Space Science
Enhancing Explainability in Solar Energetic Particle Event Prediction: A Global Feature Mapping Approach,2511.09475v1,Huang2012,\cite{Huang2012},Ensemble prediction model of solar proton events associated with solar flares and coronal mass ejections,,,True,False,"Huang,  Xin and Wang,  Hua-Ning and Li,  Le-Ping",2012,,,10.1088/1674-4527/12/3/007,Research in Astronomy and Astrophysics
Enhancing Explainability in Solar Energetic Particle Event Prediction: A Global Feature Mapping Approach,2511.09475v1,9377906,\cite{9377906},All-Clear Flare Prediction Using Interval-based Time Series Classifiers,https://arxiv.org/abs/2105.01202v1,"An all-clear flare prediction is a type of solar flare forecasting that puts more emphasis on predicting non-flaring instances (often relatively small flares and flare quiet regions) with high precision while still maintaining valuable predictive results. While many flare prediction studies do not address this problem directly, all-clear predictions can be useful in operational context. However, in all-clear predictions, finding the right balance between avoiding false negatives (misses) and reducing the false positives (false alarms) is often challenging. Our study focuses on training and testing a set of interval-based time series classifiers named Time Series Forest (TSF). These classifiers will be used towards building an all-clear flare prediction system by utilizing multivariate time series data. Throughout this paper, we demonstrate our data collection, predictive model building and evaluation processes, and compare our time series classification models with baselines using our benchmark datasets. Our results show that time series classifiers provide better forecasting results in terms of skill scores, precision and recall metrics, and they can be further improved for more precise all-clear forecasts by tuning model hyperparameters.",True,True,"Ji, Anli and Aydin, Berkay and Georgoulis, Manolis K. and Angryk, Rafal",2020,,,10.1109/BigData50022.2020.9377906,
Enhancing Explainability in Solar Energetic Particle Event Prediction: A Global Feature Mapping Approach,2511.09475v1,aji2023,\cite{aji2023},Active Region-based Flare Forecasting with Sliding Window Multivariate Time Series Forest Classifiers,,,True,False,"Ji, Anli and Aydin, Berkay",2023,,,,
Enhancing Explainability in Solar Energetic Particle Event Prediction: A Global Feature Mapping Approach,2511.09475v1,10302639,\cite{10302639},Exploring Deep Learning for Full-disk Solar Flare Prediction with Empirical Insights from Guided Grad-CAM Explanations,,,True,False,"Pandey, Chetraj and Ji, Anli and Nandakumar, Trisha and Angryk, Rafal A. and Aydin, Berkay",2023,,,10.1109/DSAA60987.2023.10302639,
Enhancing Explainability in Solar Energetic Particle Event Prediction: A Global Feature Mapping Approach,2511.09475v1,Pandey2023,\cite{Pandey2023},Explaining Full-disk Deep Learning Model for Solar Flare Prediction using Attribution Methods,https://arxiv.org/abs/2307.15878v1,"This paper contributes to the growing body of research on deep learning methods for solar flare prediction, primarily focusing on highly overlooked near-limb flares and utilizing the attribution methods to provide a post hoc qualitative explanation of the model's predictions. We present a solar flare prediction model, which is trained using hourly full-disk line-of-sight magnetogram images and employs a binary prediction mode to forecast $\geq$M-class flares that may occur within the following 24-hour period. To address the class imbalance, we employ a fusion of data augmentation and class weighting techniques; and evaluate the overall performance of our model using the true skill statistic (TSS) and Heidke skill score (HSS). Moreover, we applied three attribution methods, namely Guided Gradient-weighted Class Activation Mapping, Integrated Gradients, and Deep Shapley Additive Explanations, to interpret and cross-validate our model's predictions with the explanations. Our analysis revealed that full-disk prediction of solar flares aligns with characteristics related to active regions (ARs). In particular, the key findings of this study are: (1) our deep learning models achieved an average TSS=0.51 and HSS=0.35, and the results further demonstrate a competent capability to predict near-limb solar flares and (2) the qualitative analysis of the model explanation indicates that our model identifies and uses features associated with ARs in central and near-limb locations from full-disk magnetograms to make corresponding predictions. In other words, our models learn the shape and texture-based characteristics of flaring ARs even at near-limb areas, which is a novel and critical capability with significant implications for operational forecasting.",True,True,"Pandey,  Chetraj and Angryk,  Rafal A. and Aydin,  Berkay",2023,,,10.1007/978-3-031-43430-3\_5,
Enhancing Explainability in Solar Energetic Particle Event Prediction: A Global Feature Mapping Approach,2511.09475v1,9671322,\cite{9671322},Solar Flare Forecasting with Deep Neural Networks using Compressed Full-disk HMI Magnetograms,,,True,False,"Pandey, Chetraj and Angryk, Rafal A. and Aydin, Berkay",2021,,,10.1109/BigData52589.2021.9671322,
Enhancing Explainability in Solar Energetic Particle Event Prediction: A Global Feature Mapping Approach,2511.09475v1,Pandey2022,\cite{Pandey2022},Towards coupling full-disk and active region-based flare prediction for operational space weather forecasting,,,True,False,Chetraj Pandey and Anli Ji and Rafal A. Angryk and Manolis K. Georgoulis and Berkay Aydin,2022,,,10.3389/fspas.2022.897301,Frontiers in Astronomy and Space Sciences
Enhancing Explainability in Solar Energetic Particle Event Prediction: A Global Feature Mapping Approach,2511.09475v1,10722839,\cite{10722839},Embedding Ordinality to Binary Loss Function for Improving Solar Flare Forecasting,https://arxiv.org/abs/2408.11768v1,"In this paper, we propose a novel loss function aimed at optimizing the binary flare prediction problem by embedding the intrinsic ordinal flare characteristics into the binary cross-entropy (BCE) loss function. This modification is intended to provide the model with better guidance based on the ordinal characteristics of the data and improve the overall performance of the models. For our experiments, we employ a ResNet34-based model with transfer learning to predict $\geq$M-class flares by utilizing the shape-based features of magnetograms of active region (AR) patches spanning from $-$90$^{\circ}$ to $+$90$^{\circ}$ of solar longitude as our input data. We use a composite skill score (CSS) as our evaluation metric, which is calculated as the geometric mean of the True Skill Score (TSS) and the Heidke Skill Score (HSS) to rank and compare our models' performance. The primary contributions of this work are as follows: (i) We introduce a novel approach to encode ordinality into a binary loss function showing an application to solar flare prediction, (ii) We enhance solar flare forecasting by enabling flare predictions for each AR across the entire solar disk, without any longitudinal restrictions, and evaluate and compare performance. (iii) Our candidate model, optimized with the proposed loss function, shows an improvement of $\sim$7%, $\sim$4%, and $\sim$3% for AR patches within $\pm$30$^\circ$, $\pm$60$^\circ$, and $\pm$90$^\circ$ of solar longitude, respectively in terms of CSS, when compared with standard BCE. Additionally, we demonstrate the ability to issue flare forecasts for ARs in near-limb regions (regions between $\pm$60$^{\circ}$ to $\pm$90$^{\circ}$) with a CSS=0.34 (TSS=0.50 and HSS=0.23), expanding the scope of AR-based models for solar flare prediction. This advances the reliability of solar flare forecasts, leading to more effective prediction capabilities.",True,True,"Pandey, Chetraj and Ji, Anli and Hong, Jinsu and Angryk, Rafal A. and Aydin, Berkay",2024,,,10.1109/DSAA61799.2024.10722839,
Enhancing Explainability in Solar Energetic Particle Event Prediction: A Global Feature Mapping Approach,2511.09475v1,9378006,\cite{9378006},On The Effectiveness of Imaging of Time Series for Flare Forecasting Problem,,,True,False,"Chen, Yang and Ji, Anli and Babajiyavar, Pavan Ajit and Ahmadzadeh, Azim and Angryk, Rafal A.",2020,,,,
Enhancing Explainability in Solar Energetic Particle Event Prediction: A Global Feature Mapping Approach,2511.09475v1,10460016,\cite{10460016},An Innovative Solar Flare Metadata Collection for Space Weather Analytics,,,True,False,"Hong, Jinsu and Pandey, Chetraj and Ji, Anli and Aydin, Berkay",2023,,,10.1109/ICMLA58977.2023.00063,
Enhancing Explainability in Solar Energetic Particle Event Prediction: A Global Feature Mapping Approach,2511.09475v1,Hong2023,\cite{Hong2023},Beyond Traditional Flare Forecasting: A Data-driven Labeling Approach for~High-fidelity Predictions,,,True,False,Jinsu Hong and Anli Ji and Chetraj Pandey and Berkay Aydin,2023,,https://doi.org/10.1007/978-3-031-39831-5\_34,10.1007/978-3-031-39831-5\_34,
Enhancing Explainability in Solar Energetic Particle Event Prediction: A Global Feature Mapping Approach,2511.09475v1,10431579,\cite{10431579},Enhancing Solar Flare Prediction with Innovative Data-Driven Labels,,,True,False,"Hong, Jinsu and Ji, Anli and Pandey, Chetraj and Aydin, Berkay",2023,,,10.1109/CogMI58952.2023.00035,
Enhancing Explainability in Solar Energetic Particle Event Prediction: A Global Feature Mapping Approach,2511.09475v1,pandey2023interpretable,\cite{pandey2023interpretable},Towards Interpretable Solar Flare Prediction with Attention-based Deep Neural Networks,https://arxiv.org/abs/2309.04558v1,"Solar flare prediction is a central problem in space weather forecasting and recent developments in machine learning and deep learning accelerated the adoption of complex models for data-driven solar flare forecasting. In this work, we developed an attention-based deep learning model as an improvement over the standard convolutional neural network (CNN) pipeline to perform full-disk binary flare predictions for the occurrence of $\geq$M1.0-class flares within the next 24 hours. For this task, we collected compressed images created from full-disk line-of-sight (LoS) magnetograms. We used data-augmented oversampling to address the class imbalance issue and used true skill statistic (TSS) and Heidke skill score (HSS) as the evaluation metrics. Furthermore, we interpreted our model by overlaying attention maps on input magnetograms and visualized the important regions focused on by the model that led to the eventual decision. The significant findings of this study are: (i) We successfully implemented an attention-based full-disk flare predictor ready for operational forecasting where the candidate model achieves an average TSS=0.54$\pm$0.03 and HSS=0.37$\pm$0.07. (ii) we demonstrated that our full-disk model can learn conspicuous features corresponding to active regions from full-disk magnetogram images, and (iii) our experimental evaluation suggests that our model can predict near-limb flares with adept skill and the predictions are based on relevant active regions (ARs) or AR characteristics from full-disk magnetograms.",True,True,Chetraj Pandey and Anli Ji and Rafal A. Angryk and Berkay Aydin,2023,,,,
OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large Language Models,2511.10287v1,sun2024trustllm,\cite{sun2024trustllm},TrustLLM: Trustworthiness in Large Language Models,https://arxiv.org/abs/2401.05561v6,"Large language models (LLMs), exemplified by ChatGPT, have gained considerable attention for their excellent natural language processing capabilities. Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs emerges as an important topic. This paper introduces TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. Specifically, we first propose a set of principles for trustworthy LLMs that span eight different dimensions. Based on these principles, we further establish a benchmark across six dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics. We then present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of over 30 datasets. Our findings firstly show that in general trustworthiness and utility (i.e., functional effectiveness) are positively related. Secondly, our observations reveal that proprietary LLMs generally outperform most open-source counterparts in terms of trustworthiness, raising concerns about the potential risks of widely accessible open-source LLMs. However, a few open-source LLMs come very close to proprietary ones. Thirdly, it is important to note that some LLMs may be overly calibrated towards exhibiting trustworthiness, to the extent that they compromise their utility by mistakenly treating benign prompts as harmful and consequently not responding. Finally, we emphasize the importance of ensuring transparency not only in the models themselves but also in the technologies that underpin trustworthiness. Knowing the specific trustworthy technologies that have been employed is crucial for analyzing their effectiveness.",True,True,"Sun, Lichao and Huang, Yue and Wang, Haoran and Wu, Siyuan and Zhang, Qihui and Gao, Chujie and Huang, Yixin and Lyu, Wenhan and Zhang, Yixuan and Li, Xiner and others",2024,,,,arXiv preprint arXiv:2401.05561
OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large Language Models,2511.10287v1,liu2023trustworthy,\cite{liu2023trustworthy},Trustworthy llms: a survey and guideline for evaluating large language models' alignment,,,True,False,"Liu, Yang and Yao, Yuanshun and Ton, Jean-Francois and Zhang, Xiaoying and Guo, Ruocheng and Cheng, Hao and Klochkov, Yegor and Taufiq, Muhammad Faaiz and Li, Hang",2023,,,,arXiv preprint arXiv:2308.05374
OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large Language Models,2511.10287v1,xu2024lvlm,\cite{xu2024lvlm},LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models,https://arxiv.org/abs/2306.09265v1,"Large Vision-Language Models (LVLMs) have recently played a dominant role in multimodal vision-language learning. Despite the great success, it lacks a holistic evaluation of their efficacy. This paper presents a comprehensive evaluation of publicly available large multimodal models by building a LVLM evaluation Hub (LVLM-eHub). Our LVLM-eHub consists of $8$ representative LVLMs such as InstructBLIP and MiniGPT-4, which are thoroughly evaluated by a quantitative capability evaluation and an online arena platform. The former evaluates $6$ categories of multimodal capabilities of LVLMs such as visual question answering and embodied artificial intelligence on $47$ standard text-related visual benchmarks, while the latter provides the user-level evaluation of LVLMs in an open-world question-answering scenario. The study reveals several innovative findings. First, instruction-tuned LVLM with massive in-domain data such as InstructBLIP heavily overfits many existing tasks, generalizing poorly in the open-world scenario. Second, instruction-tuned LVLM with moderate instruction-following data may result in object hallucination issues (i.e., generate objects that are inconsistent with target images in the descriptions). It either makes the current evaluation metric such as CIDEr for image captioning ineffective or generates wrong answers. Third, employing a multi-turn reasoning evaluation framework can mitigate the issue of object hallucination, shedding light on developing an effective pipeline for LVLM evaluation. The findings provide a foundational framework for the conception and assessment of innovative strategies aimed at enhancing zero-shot multimodal techniques. Our LVLM-eHub will be available at https://github.com/OpenGVLab/Multi-Modality-Arena",True,True,"Xu, Peng and Shao, Wenqi and Zhang, Kaipeng and Gao, Peng and Liu, Shuo and Lei, Meng and Meng, Fanqing and Huang, Siyuan and Qiao, Yu and Luo, Ping",2024,,,,IEEE Transactions on Pattern Analysis and Machine Intelligence
OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large Language Models,2511.10287v1,yin2023lamm,\cite{yin2023lamm},"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark",https://arxiv.org/abs/2306.06687v3,"Large language models have emerged as a promising approach towards achieving general-purpose AI agents. The thriving open-source LLM community has greatly accelerated the development of agents that support human-machine dialogue interaction through natural language processing. However, human interaction with the world extends beyond only text as a modality, and other modalities such as vision are also crucial. Recent works on multi-modal large language models, such as GPT-4V and Bard, have demonstrated their effectiveness in handling visual modalities. However, the transparency of these works is limited and insufficient to support academic research. To the best of our knowledge, we present one of the very first open-source endeavors in the field, LAMM, encompassing a Language-Assisted Multi-Modal instruction tuning dataset, framework, and benchmark. Our aim is to establish LAMM as a growing ecosystem for training and evaluating MLLMs, with a specific focus on facilitating AI agents capable of bridging the gap between ideas and execution, thereby enabling seamless human-AI interaction. Our main contribution is three-fold: 1) We present a comprehensive dataset and benchmark, which cover a wide range of vision tasks for 2D and 3D vision. Extensive experiments validate the effectiveness of our dataset and benchmark. 2) We outline the detailed methodology of constructing multi-modal instruction tuning datasets and benchmarks for MLLMs, enabling rapid scaling and extension of MLLM research to diverse domains, tasks, and modalities. 3) We provide a primary but potential MLLM training framework optimized for modality extension. We also provide baseline models, comprehensive experimental observations, and analysis to accelerate future research. Our baseline model is trained within 24 A100 GPU hours, framework supports training with V100 and RTX3090 is available thanks to the open-source society.",True,True,"Yin, Zhenfei and Wang, Jiong and Cao, Jianjian and Shi, Zhelun and Liu, Dingning and Li, Mukai and Huang, Xiaoshui and Wang, Zhiyong and Sheng, Lu and Bai, Lei and others",2023,,,,Advances in Neural Information Processing Systems
OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large Language Models,2511.10287v1,liu2024mm,\cite{liu2024mm},Mm-safetybench: A benchmark for safety evaluation of multimodal large language models,,,True,False,"Liu, Xin and Zhu, Yichen and Gu, Jindong and Lan, Yunshi and Yang, Chao and Qiao, Yu",2024,,,,
OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large Language Models,2511.10287v1,li2025priv,\cite{li2025priv},PRIV-QA: Privacy-Preserving Question Answering for Cloud Large Language Models,https://arxiv.org/abs/2502.13564v1,"The rapid development of large language models (LLMs) is redefining the landscape of human-computer interaction, and their integration into various user-service applications is becoming increasingly prevalent. However, transmitting user data to cloud-based LLMs presents significant risks of data breaches and unauthorized access to personal identification information. In this paper, we propose a privacy preservation pipeline for protecting privacy and sensitive information during interactions between users and LLMs in practical LLM usage scenarios. We construct SensitiveQA, the first privacy open-ended question-answering dataset. It comprises 57k interactions in Chinese and English, encompassing a diverse range of user-sensitive information within the conversations. Our proposed solution employs a multi-stage strategy aimed at preemptively securing user information while simultaneously preserving the response quality of cloud-based LLMs. Experimental validation underscores our method's efficacy in balancing privacy protection with maintaining robust interaction quality. The code and dataset are available at https://github.com/ligw1998/PRIV-QA.",True,True,"Li, Guangwei and Zhang, Yuansen and Wang, Yinggui and Yan, Shoumeng and Wang, Lei and Wei, Tao",2025,,,,arXiv preprint arXiv:2502.13564
OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large Language Models,2511.10287v1,tu2023many,\cite{tu2023many},How many unicorns are in this image? a safety evaluation benchmark for vision llms,,,True,False,"Tu, Haoqin and Cui, Chenhang and Wang, Zijun and Zhou, Yiyang and Zhao, Bingchen and Han, Junlin and Zhou, Wangchunshu and Yao, Huaxiu and Xie, Cihang",2023,,,,arXiv preprint arXiv:2311.16101
OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large Language Models,2511.10287v1,xu2022safebench,\cite{xu2022safebench},Safebench: A benchmarking platform for safety evaluation of autonomous vehicles,,,True,False,"Xu, Chejian and Ding, Wenhao and Lyu, Weijie and Liu, Zuxin and Wang, Shuai and He, Yihan and Hu, Hanjiang and Zhao, Ding and Li, Bo",2022,,,,Advances in Neural Information Processing Systems
OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large Language Models,2511.10287v1,zhang2024multitrust,\cite{zhang2024multitrust},MultiTrust: A Comprehensive Benchmark Towards Trustworthy Multimodal Large Language Models,https://arxiv.org/abs/2406.07057v2,"Despite the superior capabilities of Multimodal Large Language Models (MLLMs) across diverse tasks, they still face significant trustworthiness challenges. Yet, current literature on the assessment of trustworthy MLLMs remains limited, lacking a holistic evaluation to offer thorough insights into future improvements. In this work, we establish MultiTrust, the first comprehensive and unified benchmark on the trustworthiness of MLLMs across five primary aspects: truthfulness, safety, robustness, fairness, and privacy. Our benchmark employs a rigorous evaluation strategy that addresses both multimodal risks and cross-modal impacts, encompassing 32 diverse tasks with self-curated datasets. Extensive experiments with 21 modern MLLMs reveal some previously unexplored trustworthiness issues and risks, highlighting the complexities introduced by the multimodality and underscoring the necessity for advanced methodologies to enhance their reliability. For instance, typical proprietary models still struggle with the perception of visually confusing images and are vulnerable to multimodal jailbreaking and adversarial attacks; MLLMs are more inclined to disclose privacy in text and reveal ideological and cultural biases even when paired with irrelevant images in inference, indicating that the multimodality amplifies the internal risks from base LLMs. Additionally, we release a scalable toolbox for standardized trustworthiness research, aiming to facilitate future advancements in this important field. Code and resources are publicly available at: https://multi-trust.github.io/.",True,True,"Zhang, Yichi and Huang, Yao and Sun, Yitong and Liu, Chang and Zhao, Zhe and Fang, Zhengwei and Wang, Yifan and Chen, Huanran and Yang, Xiao and Wei, Xingxing and others",2024,,,,Advances in Neural Information Processing Systems
OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large Language Models,2511.10287v1,gu2024mllmguard,\cite{gu2024mllmguard},MLLMGuard: A Multi-dimensional Safety Evaluation Suite for Multimodal Large Language Models,https://arxiv.org/abs/2406.07594v2,"Powered by remarkable advancements in Large Language Models (LLMs), Multimodal Large Language Models (MLLMs) demonstrate impressive capabilities in manifold tasks. However, the practical application scenarios of MLLMs are intricate, exposing them to potential malicious instructions and thereby posing safety risks. While current benchmarks do incorporate certain safety considerations, they often lack comprehensive coverage and fail to exhibit the necessary rigor and robustness. For instance, the common practice of employing GPT-4V as both the evaluator and a model to be evaluated lacks credibility, as it tends to exhibit a bias toward its own responses. In this paper, we present MLLMGuard, a multidimensional safety evaluation suite for MLLMs, including a bilingual image-text evaluation dataset, inference utilities, and a lightweight evaluator. MLLMGuard's assessment comprehensively covers two languages (English and Chinese) and five important safety dimensions (Privacy, Bias, Toxicity, Truthfulness, and Legality), each with corresponding rich subtasks. Focusing on these dimensions, our evaluation dataset is primarily sourced from platforms such as social media, and it integrates text-based and image-based red teaming techniques with meticulous annotation by human experts. This can prevent inaccurate evaluation caused by data leakage when using open-source datasets and ensures the quality and challenging nature of our benchmark. Additionally, a fully automated lightweight evaluator termed GuardRank is developed, which achieves significantly higher evaluation accuracy than GPT-4. Our evaluation results across 13 advanced models indicate that MLLMs still have a substantial journey ahead before they can be considered safe and responsible.",True,True,"Gu, Tianle and Zhou, Zeyang and Huang, Kexin and Dandan, Liang and Wang, Yixu and Zhao, Haiquan and Yao, Yuanqi and Yang, Yujiu and Teng, Yan and Qiao, Yu and others",2024,,,,Advances in Neural Information Processing Systems
OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large Language Models,2511.10287v1,li2024rule,\cite{li2024rule},Rule-based data selection for large language models,,,True,False,"Li, Xiaomin and Gao, Mingye and Zhang, Zhiwei and Yue, Chang and Hu, Hong",2024,,,,arXiv preprint arXiv:2410.04715
OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large Language Models,2511.10287v1,zheng2023judging,\cite{zheng2023judging},Judging llm-as-a-judge with mt-bench and chatbot arena,,,True,False,"Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others",2023,,,,Advances in Neural Information Processing Systems
OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large Language Models,2511.10287v1,lin2023llm,\cite{lin2023llm},LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models,https://arxiv.org/abs/2305.13711v1,"We propose LLM-Eval, a unified multi-dimensional automatic evaluation method for open-domain conversations with large language models (LLMs). Existing evaluation methods often rely on human annotations, ground-truth responses, or multiple LLM prompts, which can be expensive and time-consuming. To address these issues, we design a single prompt-based evaluation method that leverages a unified evaluation schema to cover multiple dimensions of conversation quality in a single model call. We extensively evaluate the performance of LLM-Eval on various benchmark datasets, demonstrating its effectiveness, efficiency, and adaptability compared to state-of-the-art evaluation methods. Our analysis also highlights the importance of choosing suitable LLMs and decoding strategies for accurate evaluation results. LLM-Eval offers a versatile and robust solution for evaluating open-domain conversation systems, streamlining the evaluation process and providing consistent performance across diverse scenarios.",True,True,"Lin, Yen-Ting and Chen, Yun-Nung",2023,,,,arXiv preprint arXiv:2305.13711
OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large Language Models,2511.10287v1,liu2023g,\cite{liu2023g},G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment,https://arxiv.org/abs/2303.16634v3,"The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at https://github.com/nlpyang/geval",True,True,"Liu, Yang and Iter, Dan and Xu, Yichong and Wang, Shuohang and Xu, Ruochen and Zhu, Chenguang",2023,,,,arXiv preprint arXiv:2303.16634
OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large Language Models,2511.10287v1,li2023prd,\cite{li2023prd},Prd: Peer rank and discussion improve large language model based evaluations,,,True,False,"Li, Ruosen and Patel, Teerth and Du, Xinya",2023,,,,arXiv preprint arXiv:2307.02762
OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large Language Models,2511.10287v1,chan2023chateval,\cite{chan2023chateval},ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate,https://arxiv.org/abs/2308.07201v1,"Text evaluation has historically posed significant challenges, often demanding substantial labor and time cost. With the emergence of large language models (LLMs), researchers have explored LLMs' potential as alternatives for human evaluation. While these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality. Recognizing that best practices of human evaluation processes often involve multiple human annotators collaborating in the evaluation, we resort to a multi-agent debate framework, moving beyond single-agent prompting strategies. The multi-agent-based approach enables a group of LLMs to synergize with an array of intelligent counterparts, harnessing their distinct capabilities and expertise to enhance efficiency and effectiveness in handling intricate tasks. In this paper, we construct a multi-agent referee team called ChatEval to autonomously discuss and evaluate the quality of generated responses from different models on open-ended questions and traditional natural language generation (NLG) tasks. Our analysis shows that ChatEval transcends mere textual scoring, offering a human-mimicking evaluation process for reliable assessments. Our code is available at https://github.com/chanchimin/ChatEval.",True,True,"Chan, Chi-Min and Chen, Weize and Su, Yusheng and Yu, Jianxuan and Xue, Wei and Zhang, Shanghang and Fu, Jie and Liu, Zhiyuan",2023,,,,arXiv preprint arXiv:2308.07201
OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large Language Models,2511.10287v1,li2023collaborative,\cite{li2023collaborative},Collaborative evaluation: Exploring the synergy of large language models and humans for open-ended generation evaluation,,,True,False,"Li, Qintong and Cui, Leyang and Kong, Lingpeng and Bi, Wei",2023,,,,arXiv e-prints
OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large Language Models,2511.10287v1,shankar2024validates,\cite{shankar2024validates},Who validates the validators? aligning llm-assisted evaluation of llm outputs with human preferences,,,True,False,"Shankar, Shreya and Zamfirescu-Pereira, JD and Hartmann, Bj{\""o}rn and Parameswaran, Aditya and Arawjo, Ian",2024,,,,
How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation,2511.09748v1,papineni2002bleu,\cite{papineni2002bleu},BLEU: a Method for Automatic Evaluation of Machine Translation,,,True,False,"Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing",2002,,,,
How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation,2511.09748v1,banerjee2005meteor,\cite{banerjee2005meteor},METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments,,,True,False,"Banerjee, Satanjeev and Lavie, Alon",2005,,,,
How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation,2511.09748v1,rei2020comet,\cite{rei2020comet},COMET: A Neural Framework for MT Evaluation,,,True,False,"Rei, Ricardo and Farinha, Ana C. and Lavie, Alon and Martins, Andr{\'e} F. T.",2020,,,,
How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation,2511.09748v1,lommel2014multidimensional,\cite{lommel2014multidimensional},Multidimensional Quality Metrics (MQM): A Framework for Declaring and Describing Translation Quality Metrics,,,True,False,"Lommel, Arle Richard and Uszkoreit, Hans and Burchardt, Aljoscha",2014,,,,
How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation,2511.09748v1,specia2020findings,\cite{specia2020findings},Findings of the {WMT} 2020 Shared Task on Quality Estimation,,,True,False,"Specia, Lucia  and
      Blain, Fr{\'e}d{\'e}ric  and
      Fomicheva, Marina  and
      Fonseca, Erick  and
      Chaudhary, Vishrav  and
      Guzm{\'a}n, Francisco  and
      Martins, Andr{\'e} F. T.",2020,,https://aclanthology.org/2020.wmt-1.79/,,
How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation,2511.09748v1,federmann2021findings,\cite{federmann2021findings},Findings of the WMT 2021 Shared Tasks on Machine Translation,,,True,False,"Federmann, Christian and Freitag, Markus and Hoang, Hieu and others",2021,,,,
How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation,2511.09748v1,federmann2022findings,\cite{federmann2022findings},Findings of the WMT 2022 Shared Tasks on Machine Translation,,,True,False,"Federmann, Christian and Freitag, Markus and Hoang, Hieu and coauthors",2022,,,,
How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation,2511.09748v1,kocmi2023lm_eval,\cite{kocmi2023lm_eval},Large Language Models Are State-of-the-Art Evaluators of Translation Quality,,,True,False,"Kocmi, Tom  and
      Federmann, Christian",2023,,https://aclanthology.org/2023.eamt-1.19/,,
How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation,2511.09748v1,lu2023erroranalysis,\cite{lu2023erroranalysis},Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models,https://arxiv.org/abs/2303.13809v4,"Generative large language models (LLMs), e.g., ChatGPT, have demonstrated remarkable proficiency across several NLP tasks, such as machine translation, text summarization. Recent research (Kocmi and Federmann, 2023) has shown that utilizing LLMs for assessing the quality of machine translation (MT) achieves state-of-the-art performance at the system level but \textit{performs poorly at the segment level}. To further improve the performance of LLMs on MT quality assessment, we investigate several prompting designs, and propose a new prompting method called \textbf{\texttt{Error Analysis Prompting}} (EAPrompt) by combining Chain-of-Thoughts (Wei et al., 2022) and Error Analysis (Lu et al., 2023). This technique emulates the commonly accepted human evaluation framework - Multidimensional Quality Metrics (MQM, Freitag et al. (2021)) and \textit{produces explainable and reliable MT evaluations at both the system and segment level}. Experimental Results from the WMT22 metrics shared task validate the effectiveness of EAPrompt on various LLMs, with different structures. Further analysis confirms that EAPrompt effectively distinguishes major errors from minor ones, while also sharing a similar distribution of the number of errors with MQM. These findings highlight the potential of EAPrompt as a human-like evaluator prompting technique for MT evaluation.",True,True,"Lu, Qingyu  and
      Qiu, Baopu  and
      Ding, Liang  and
      Zhang, Kanjian  and
      Kocmi, Tom  and
      Tao, Dacheng",2024,,https://aclanthology.org/2024.findings-acl.520/,10.18653/v1/2024.findings-acl.520,
How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation,2511.09748v1,peng2023towards,\cite{peng2023towards},Towards Making the Most of {C}hat{GPT} for Machine Translation,,,True,False,"Peng, Keqin  and
      Ding, Liang  and
      Zhong, Qihuang  and
      Shen, Li  and
      Liu, Xuebo  and
      Zhang, Min  and
      Ouyang, Yuanxin  and
      Tao, Dacheng",2023,,https://aclanthology.org/2023.findings-emnlp.373/,10.18653/v1/2023.findings-emnlp.373,
How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation,2511.09748v1,9003090,\cite{9003090},Towards Contradiction Detection in German: a Translation-Driven Approach,,,True,False,"Sifa, Rafet and Pielka, Maren and Ramamurthy, Rajkumar and Ladi, Anna and Hillebrand, Lars and Bauckhage, Christian",2019,,,10.1109/SSCI44817.2019.9003090,
How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation,2511.09748v1,pielka2020contradiction,\cite{pielka2020contradiction},Tackling Contradiction Detection in German Using Machine Translation and End-to-End Recurrent Neural Networks,,,True,False,"Pielka, Maren and Sifa, Rafet and Hillebrand, Lars Patrick and Biesner, David and Ramamurthy, Rajkumar and Ladi, Anna and Bauckhage, Christian",2020,,,10.1109/ICPR48806.2021.9412171,
How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation,2511.09748v1,pucknat2022informed,\cite{pucknat2022informed},Towards Informed Pre-Training for Critical Error Detection in English-German,,,True,False,Lisa Pucknat and Maren Pielka and Rafet Sifa,2022,,https://api.semanticscholar.org/CorpusID:256873200,,
How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation,2511.09748v1,jung2024explainable,\cite{jung2024explainable},Explainable {CED}: A Dataset for Explainable Critical Error Detection in Machine Translation,,,True,False,"Jung, Dahyun  and
      Eo, Sugyeong  and
      Park, Chanjun  and
      Lim, Heuiseok",2024,,https://aclanthology.org/2024.naacl-srw.4/,10.18653/v1/2024.naacl-srw.4,
How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation,2511.09748v1,anonymous,\cite{anonymous},{SynCED-EnDe 2025: A Synthetic and Curated English - German Dataset for Critical Error Detection in Machine Translation},,,True,False,Muskaan Chopra and Lorenz Sparrenberg and Rafet Sifa,2025,,https://arxiv.org/abs/2510.05144,,
How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation,2511.09748v1,pielka2025translation,\cite{pielka2025translation},Automating Translation Checks of Financial Documents Using Large Language Models,,,True,False,"Pielka, Maren and Hahnb{\""u}ck, Max and Deu{\ss}er, Tobias and Uedelhoven, Daniel and Chatterjee, Moinam and Shah, Vijul and Soliman, Osama and von der Bank, Jannis and Das, Writwick and Talarico, Maria Chiara and Zhao, Cong and Held Celis, Carolina and Temath, Christian and Sifa, Rafet",2025,,,,Language Resources and Evaluation
How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation,2511.09748v1,unsloth2024,\cite{unsloth2024},Unsloth,,,True,False,"Huang, Eren and Contributors",2024,,https://github.com/unslothai/unsloth,,
How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation,2511.09748v1,modernbert,\cite{modernbert},"Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference",https://arxiv.org/abs/2412.13663v2,"Encoder-only transformer models such as BERT offer a great performance-size tradeoff for retrieval and classification tasks with respect to larger decoder-only models. Despite being the workhorse of numerous production pipelines, there have been limited Pareto improvements to BERT since its release. In this paper, we introduce ModernBERT, bringing modern model optimizations to encoder-only models and representing a major Pareto improvement over older encoders. Trained on 2 trillion tokens with a native 8192 sequence length, ModernBERT models exhibit state-of-the-art results on a large pool of evaluations encompassing diverse classification tasks and both single and multi-vector retrieval on different domains (including code). In addition to strong downstream performance, ModernBERT is also the most speed and memory efficient encoder and is designed for inference on common GPUs.",True,True,"Warner, Benjamin and Chaffin, Antoine and Clavi{\'e}, Benjamin and Weller, Orion and Hallstr{\""o}m, Oskar and Taghadouini, Said and Gallagher, Alexis and Biswas, Raja and Ladhak, Faisal and Aarsen, Tom and Adams, Griffin Thomas and Howard, Jeremy and Poli, Iacopo",2025,,,10.18653/v1/2025.acl-long.127,
How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation,2511.09748v1,mmbert,\cite{mmbert},mmBERT: A Modern Multilingual Encoder with Annealed Language Learning,,,True,False,"Marone, Marc and Weller, Orion and Fleshman, William and Yang, Eugene and Lawrie, Dawn and Durme, Benjamin",2025,09,,10.48550/arXiv.2509.06888,
How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation,2511.09748v1,meta2024llama3,\cite{meta2024llama3},The Llama 3 Herd of Models,,,True,False,Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey et. al,2024,,https://arxiv.org/abs/2407.21783,,
How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation,2511.09748v1,meta2025llama33,\cite{meta2025llama33},LLaMA 3.3,,,True,False,AI@Meta,2025,,https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/,,
How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation,2511.09748v1,gptoss2025report,\cite{gptoss2025report},GPT-OSS: Open Reproduction of GPT-3/4-Class Models for Research Transparency,,,True,False,OpenAI-OSS and Collaborators,2025,,https://huggingface.co/openai/gpt-oss-20b,,
How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation,2511.09748v1,gptoss2025lora,\cite{gptoss2025lora},LoRA-Enhanced GPT-OSS Models for Downstream Safety and Error Detection Tasks,,,True,False,OpenAI-OSS and Community Contributors,2025,,https://huggingface.co/openai/gpt-oss-120b,,
MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation,2511.09611v1,sd3,\cite{sd3},Scaling rectified flow transformers for high-resolution image synthesis,,,True,False,"Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and M{\""u}ller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and others",2024,,,,
MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation,2511.09611v1,wu2025qwen,\cite{wu2025qwen},Qwen-Image Technical Report,https://arxiv.org/abs/2508.02324v1,"We present Qwen-Image, an image generation foundation model in the Qwen series that achieves significant advances in complex text rendering and precise image editing. To address the challenges of complex text rendering, we design a comprehensive data pipeline that includes large-scale data collection, filtering, annotation, synthesis, and balancing. Moreover, we adopt a progressive training strategy that starts with non-text-to-text rendering, evolves from simple to complex textual inputs, and gradually scales up to paragraph-level descriptions. This curriculum learning approach substantially enhances the model's native text rendering capabilities. As a result, Qwen-Image not only performs exceptionally well in alphabetic languages such as English, but also achieves remarkable progress on more challenging logographic languages like Chinese. To enhance image editing consistency, we introduce an improved multi-task training paradigm that incorporates not only traditional text-to-image (T2I) and text-image-to-image (TI2I) tasks but also image-to-image (I2I) reconstruction, effectively aligning the latent representations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed the original image into Qwen2.5-VL and the VAE encoder to obtain semantic and reconstructive representations, respectively. This dual-encoding mechanism enables the editing module to strike a balance between preserving semantic consistency and maintaining visual fidelity. Qwen-Image achieves state-of-the-art performance, demonstrating its strong capabilities in both image generation and editing across multiple benchmarks.",True,True,"Wu, Chenfei and Li, Jiahao and Zhou, Jingren and Lin, Junyang and Gao, Kaiyuan and Yan, Kun and Yin, Sheng-ming and Bai, Shuai and Xu, Xiao and Chen, Yilei and others",2025,,,,arXiv preprint arXiv:2508.02324
MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation,2511.09611v1,labs2025flux1kontextflowmatching,\cite{labs2025flux1kontextflowmatching},FLUX.1 Kontext: Flow Matching for In-Context Image Generation and Editing in Latent Space,,,True,False,Black Forest Labs and Stephen Batifol and Andreas Blattmann and Frederic Boesel and Saksham Consul and Cyril Diagne and Tim Dockhorn and Jack English and Zion English and Patrick Esser and Sumith Kulal and Kyle Lacey and Yam Levi and Cheng Li and Dominik Lorenz and Jonas Müller and Dustin Podell and Robin Rombach and Harry Saini and Axel Sauer and Luke Smith,2025,,https://arxiv.org/abs/2506.15742,,
MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation,2511.09611v1,bai2025qwen2,\cite{bai2025qwen2},Qwen2 Technical Report,https://arxiv.org/abs/2407.10671v4,"This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of foundational and instruction-tuned language models, encompassing a parameter range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts model. Qwen2 surpasses most prior open-weight models, including its predecessor Qwen1.5, and exhibits competitive performance relative to proprietary models across diverse benchmarks on language understanding, generation, multilingual proficiency, coding, mathematics, and reasoning.
  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2 demonstrates robust multilingual capabilities, proficient in approximately 30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and global reach.
  To foster community innovation and accessibility, we have made the Qwen2 model weights openly available on Hugging Face and ModelScope, and the supplementary materials including example code on GitHub. These platforms also include resources for quantization, fine-tuning, and deployment, facilitating a wide range of applications and research endeavors.",True,True,"Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and others",2025,,,,arXiv preprint arXiv:2502.13923
MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation,2511.09611v1,team2024chameleon,\cite{team2024chameleon},Chameleon: Mixed-Modal Early-Fusion Foundation Models,https://arxiv.org/abs/2405.09818v2,"We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.",True,True,"Team, Chameleon",2024,,,,arXiv preprint arXiv:2405.09818
MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation,2511.09611v1,liao2025mogao,\cite{liao2025mogao},Mogao: An omni foundation model for interleaved multi-modal generation,,,True,False,"Liao, Chao and Liu, Liyang and Wang, Xun and Luo, Zhengxiong and Zhang, Xinyu and Zhao, Wenliang and Wu, Jie and Li, Liang and Tian, Zhi and Huang, Weilin",2025,,,,arXiv preprint arXiv:2505.05472
MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation,2511.09611v1,guo2025can,\cite{guo2025can},Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step,,,True,False,"Guo, Ziyu and Zhang, Renrui and Tong, Chengzhuo and Zhao, Zhizheng and Huang, Rui and Zhang, Haoquan and Zhang, Manyuan and Liu, Jiaming and Zhang, Shanghang and Gao, Peng and others",2025,,,,arXiv preprint arXiv:2501.13926
MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation,2511.09611v1,fang2025got,\cite{fang2025got},GoT: Unleashing Reasoning Capability of Multimodal Large Language Model for Visual Generation and Editing,https://arxiv.org/abs/2503.10639v1,"Current image generation and editing methods primarily process textual prompts as direct inputs without reasoning about visual composition and explicit operations. We present Generation Chain-of-Thought (GoT), a novel paradigm that enables generation and editing through an explicit language reasoning process before outputting images. This approach transforms conventional text-to-image generation and editing into a reasoning-guided framework that analyzes semantic relationships and spatial arrangements. We define the formulation of GoT and construct large-scale GoT datasets containing over 9M samples with detailed reasoning chains capturing semantic-spatial relationships. To leverage the advantages of GoT, we implement a unified framework that integrates Qwen2.5-VL for reasoning chain generation with an end-to-end diffusion model enhanced by our novel Semantic-Spatial Guidance Module. Experiments show our GoT framework achieves excellent performance on both generation and editing tasks, with significant improvements over baselines. Additionally, our approach enables interactive visual generation, allowing users to explicitly modify reasoning steps for precise image adjustments. GoT pioneers a new direction for reasoning-driven visual generation and editing, producing images that better align with human intent. To facilitate future research, we make our datasets, code, and pretrained models publicly available at https://github.com/rongyaofang/GoT.",True,True,"Fang, Rongyao and Duan, Chengqi and Wang, Kun and Huang, Linjiang and Li, Hao and Yan, Shilin and Tian, Hao and Zeng, Xingyu and Zhao, Rui and Dai, Jifeng and others",2025,,,,arXiv preprint arXiv:2503.10639
MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation,2511.09611v1,deng2025emerging,\cite{deng2025emerging},Emerging Properties in Unified Multimodal Pretraining,https://arxiv.org/abs/2505.14683v3,"Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems. In this work, we introduce BAGEL, an open-source foundational model that natively supports multimodal understanding and generation. BAGEL is a unified, decoder-only model pretrained on trillions of tokens curated from large-scale interleaved text, image, video, and web data. When scaled with such diverse multimodal interleaved data, BAGEL exhibits emerging capabilities in complex multimodal reasoning. As a result, it significantly outperforms open-source unified models in both multimodal generation and understanding across standard benchmarks, while exhibiting advanced multimodal reasoning abilities such as free-form image manipulation, future frame prediction, 3D manipulation, and world navigation. In the hope of facilitating further opportunities for multimodal research, we share the key findings, pretraining details, data creation protocal, and release our code and checkpoints to the community. The project page is at https://bagel-ai.org/",True,True,"Deng, Chaorui and Zhu, Deyao and Li, Kunchang and Gou, Chenhui and Li, Feng and Wang, Zeyu and Zhong, Shu and Yu, Weihao and Nie, Xiaonan and Song, Ziang and others",2025,,,,arXiv preprint arXiv:2505.14683
MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation,2511.09611v1,wu2025omnigen2,\cite{wu2025omnigen2},OmniGen2: Exploration to Advanced Multimodal Generation,https://arxiv.org/abs/2506.18871v3,"In this work, we introduce OmniGen2, a versatile and open-source generative model designed to provide a unified solution for diverse generation tasks, including text-to-image, image editing, and in-context generation. Unlike OmniGen v1, OmniGen2 features two distinct decoding pathways for text and image modalities, utilizing unshared parameters and a decoupled image tokenizer. This design enables OmniGen2 to build upon existing multimodal understanding models without the need to re-adapt VAE inputs, thereby preserving the original text generation capabilities. To facilitate the training of OmniGen2, we developed comprehensive data construction pipelines, encompassing image editing and in-context generation data. Additionally, we introduce a reflection mechanism tailored for image generation tasks and curate a dedicated reflection dataset based on OmniGen2. Despite its relatively modest parameter size, OmniGen2 achieves competitive results on multiple task benchmarks, including text-to-image and image editing. To further evaluate in-context generation, also referred to as subject-driven tasks, we introduce a new benchmark named OmniContext. OmniGen2 achieves state-of-the-art performance among open-source models in terms of consistency. We will release our models, training code, datasets, and data construction pipeline to support future research in this field. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link: https://github.com/VectorSpaceLab/OmniGen2",True,True,"Wu, Chenyuan and Zheng, Pengfei and Yan, Ruiran and Xiao, Shitao and Luo, Xin and Wang, Yueze and Li, Wanli and Jiang, Xiyan and Liu, Yexin and Zhou, Junjie and others",2025,,,,arXiv preprint arXiv:2506.18871
MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation,2511.09611v1,huang2025interleaving,\cite{huang2025interleaving},Interleaving Reasoning for Better Text-to-Image Generation,,,True,False,"Huang, Wenxuan and Chen, Shuang and Xie, Zheyong and Cao, Shaosheng and Tang, Shixiang and Shen, Yufan and Yin, Qingyu and Hu, Wenbo and Wang, Xiaoman and Tang, Yuntian and others",2025,,,,arXiv preprint arXiv:2509.06945
SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models,2511.08379v2,zou_23_universal,\cite{zou_23_universal},Universal and Transferable Adversarial Attacks on Aligned Language Models,,,True,False,Andy Zou and Zifan Wang and J. Zico Kolter and Matt Fredrikson,2023,,,,
SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models,2511.08379v2,andriushchenko_25_jailbreaking,\cite{andriushchenko_25_jailbreaking},Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks,https://arxiv.org/abs/2404.02151v4,"We show that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize a target logprob (e.g., of the token ""Sure""), potentially with multiple restarts. In this way, we achieve 100% attack success rate -- according to GPT-4 as a judge -- on Vicuna-13B, Mistral-7B, Phi-3-Mini, Nemotron-4-340B, Llama-2-Chat-7B/13B/70B, Llama-3-Instruct-8B, Gemma-7B, GPT-3.5, GPT-4o, and R2D2 from HarmBench that was adversarially trained against the GCG attack. We also show how to jailbreak all Claude models -- that do not expose logprobs -- via either a transfer or prefilling attack with a 100% success rate. In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models -- a task that shares many similarities with jailbreaking -- which is the algorithm that brought us the first place in the SaTML'24 Trojan Detection Competition. The common theme behind these attacks is that adaptivity is crucial: different models are vulnerable to different prompting templates (e.g., R2D2 is very sensitive to in-context learning prompts), some models have unique vulnerabilities based on their APIs (e.g., prefilling for Claude), and in some settings, it is crucial to restrict the token search space based on prior knowledge (e.g., for trojan detection). For reproducibility purposes, we provide the code, logs, and jailbreak artifacts in the JailbreakBench format at https://github.com/tml-epfl/llm-adaptive-attacks.",True,True,Maksym Andriushchenko and Francesco Croce and Nicolas Flammarion,2025,,https://openreview.net/forum?id=hXA8wqRdyV,,
SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models,2511.08379v2,mikolov_13_linguistic,\cite{mikolov_13_linguistic},Linguistic regularities in continuous space word representations,,,True,False,"Mikolov, Tom{\'a}{\v{s}} and Yih, Wen-tau and Zweig, Geoffrey",2013,,,,
SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models,2511.08379v2,elhage_22_toy,\cite{elhage_22_toy},Toy Models of Superposition,https://arxiv.org/abs/2209.10652v1,"Neural networks often pack many unrelated concepts into a single neuron - a puzzling phenomenon known as 'polysemanticity' which makes interpretability much more challenging. This paper provides a toy model where polysemanticity can be fully understood, arising as a result of models storing additional sparse features in ""superposition."" We demonstrate the existence of a phase change, a surprising connection to the geometry of uniform polytopes, and evidence of a link to adversarial examples. We also discuss potential implications for mechanistic interpretability.",True,True,"Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and others",2022,,,,arXiv preprint arXiv:2209.10652
SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models,2511.08379v2,nanda_23_emergent,\cite{nanda_23_emergent},Emergent Linear Representations in World Models of Self-Supervised Sequence Models,https://arxiv.org/abs/2309.00941v2,"How do sequence models represent their decision-making process? Prior work suggests that Othello-playing neural network learned nonlinear models of the board state (Li et al., 2023). In this work, we provide evidence of a closely related linear representation of the board. In particular, we show that probing for ""my colour"" vs. ""opponent's colour"" may be a simple yet powerful way to interpret the model's internal state. This precise understanding of the internal representations allows us to control the model's behaviour with simple vector arithmetic. Linear representations enable significant interpretability progress, which we demonstrate with further exploration of how the world model is computed.",True,True,"Nanda, Neel  and
      Lee, Andrew  and
      Wattenberg, Martin",2023,,https://aclanthology.org/2023.blackboxnlp-1.2/,10.18653/v1/2023.blackboxnlp-1.2,
SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models,2511.08379v2,engels_25_not,\cite{engels_25_not},Not All Language Model Features Are One-Dimensionally Linear,,,True,False,Joshua Engels and Eric J Michaud and Isaac Liao and Wes Gurnee and Max Tegmark,2025,,https://openreview.net/forum?id=d63a4AM4hb,,
SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models,2511.08379v2,kantamneni_25_language,\cite{kantamneni_25_language},Language Models Use Trigonometry to Do Addition,https://arxiv.org/abs/2502.00873v1,"Mathematical reasoning is an increasingly important indicator of large language model (LLM) capabilities, yet we lack understanding of how LLMs process even simple mathematical tasks. To address this, we reverse engineer how three mid-sized LLMs compute addition. We first discover that numbers are represented in these LLMs as a generalized helix, which is strongly causally implicated for the tasks of addition and subtraction, and is also causally relevant for integer division, multiplication, and modular arithmetic. We then propose that LLMs compute addition by manipulating this generalized helix using the ""Clock"" algorithm: to solve $a+b$, the helices for $a$ and $b$ are manipulated to produce the $a+b$ answer helix which is then read out to model logits. We model influential MLP outputs, attention head outputs, and even individual neuron preactivations with these helices and verify our understanding with causal interventions. By demonstrating that LLMs represent numbers on a helix and manipulate this helix to perform addition, we present the first representation-level explanation of an LLM's mathematical capability.",True,True,Subhash Kantamneni and Max Tegmark,2025,,https://openreview.net/forum?id=CqViN4dQJk,,
SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models,2511.08379v2,levy_25_language,\cite{levy_25_language},Language Models Encode Numbers Using Digit Representations in Base 10,https://arxiv.org/abs/2410.11781v2,"Large language models (LLMs) frequently make errors when handling even simple numerical problems, such as comparing two small numbers. A natural hypothesis is that these errors stem from how LLMs represent numbers, and specifically, whether their representations of numbers capture their numeric values. We tackle this question from the observation that LLM errors on numerical tasks are often distributed across the digits of the answer rather than normally around its numeric value. Through a series of probing experiments and causal interventions, we show that LLMs internally represent numbers with individual circular representations per-digit in base 10. This digit-wise representation, as opposed to a value representation, sheds light on the error patterns of models on tasks involving numerical reasoning and could serve as a basis for future studies on analyzing numerical mechanisms in LLMs.",True,True,"Levy, Amit Arnold  and
      Geva, Mor",2025,,https://aclanthology.org/2025.naacl-short.33/,10.18653/v1/2025.naacl-short.33,
SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models,2511.08379v2,modell_25_origins,\cite{modell_25_origins},The Origins of Representation Manifolds in Large Language Models,,,True,False,"Modell, Alexander and Rubin-Delanchy, Patrick and Whiteley, Nick",2025,,,,arXiv preprint arXiv:2505.18235
SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models,2511.08379v2,olah_jermyn_24_circuits,\cite{olah_jermyn_24_circuits},Circuits Updates -- July 2024: What is a Linear Representation? What is a Multidimensional Feature?,,,True,False,Chris Olah and Adam Jermyn,2024,July,https://transformer-circuits.pub/2024/july-update/index.html#linear-representations,,Transformer Circuits
SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models,2511.08379v2,arditi_24_refusal,\cite{arditi_24_refusal},Refusal in Language Models Is Mediated by a Single Direction,https://arxiv.org/abs/2406.11717v3,"Conversational large language models are fine-tuned for both instruction-following and safety, resulting in models that obey benign requests but refuse harmful ones. While this refusal behavior is widespread across chat models, its underlying mechanisms remain poorly understood. In this work, we show that refusal is mediated by a one-dimensional subspace, across 13 popular open-source chat models up to 72B parameters in size. Specifically, for each model, we find a single direction such that erasing this direction from the model's residual stream activations prevents it from refusing harmful instructions, while adding this direction elicits refusal on even harmless instructions. Leveraging this insight, we propose a novel white-box jailbreak method that surgically disables refusal with minimal effect on other capabilities. Finally, we mechanistically analyze how adversarial suffixes suppress propagation of the refusal-mediating direction. Our findings underscore the brittleness of current safety fine-tuning methods. More broadly, our work showcases how an understanding of model internals can be leveraged to develop practical methods for controlling model behavior.",True,True,Andy Arditi and Oscar Balcells Obeso and Aaquib Syed and Daniel Paleka and Nina Rimsky and Wes Gurnee and Neel Nanda,2024,,https://openreview.net/forum?id=pH3XAQME6c,,
SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models,2511.08379v2,pan_25_hidden,\cite{pan_25_hidden},The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Analysis of Orthogonal Safety Directions,,,True,False,Wenbo Pan and Zhichao Liu and Qiguang Chen and Xiangyang Zhou and Haining Yu and Xiaohua Jia,2025,,https://arxiv.org/abs/2502.09674,,
SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models,2511.08379v2,wollschläger_25_geometry,\cite{wollschläger_25_geometry},The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence,,,True,False,Tom Wollschläger and Jannes Elstner and Simon Geisler and Vincent Cohen-Addad and Stephan Günnemann and Johannes Gasteiger,2025,,https://arxiv.org/abs/2502.17420,,
An insight into the technical debt-fix trade off in software backporting,2511.09000v1,tian2012identifying,\cite{tian2012identifying},Identifying linux bug fixing patches,,,True,False,"Tian, Yuan and Lawall, Julia and Lo, David",2012,,,,
An insight into the technical debt-fix trade off in software backporting,2511.09000v1,thung2016recommending,\cite{thung2016recommending},Recommending code changes for automatic backporting of Linux device drivers,,,True,False,"Thung, Ferdian and Le, Xuan-Bach D and Lo, David and Lawall, Julia",2016,,,,
An insight into the technical debt-fix trade off in software backporting,2511.09000v1,b2,\cite{b2},"Backports: Change Types, Challenges and Strategies",,,True,False,"Chakroborti, Debasish and Schneider, Kevin A. and Roy, Chanchal K.",2022,,,10.1145/3524610.3527920,
An insight into the technical debt-fix trade off in software backporting,2511.09000v1,bogart2021and,\cite{bogart2021and},When and how to make breaking changes: Policies and practices in 18 open source software ecosystems,,,True,False,"Bogart, Chris and K{\""a}stner, Christian and Herbsleb, James and Thung, Ferdian",2021,,,,ACM Transactions on Software Engineering and Methodology (TOSEM)
An insight into the technical debt-fix trade off in software backporting,2511.09000v1,ss2,\cite{ss2},Back to the Past -- Analysing Backporting Practices in Package Dependency Networks,,,True,False,"Decan, Alexandre and Mens, Tom and Zerouali, Ahmed and De Roover, Coen",2021,,,10.1109/TSE.2021.3112204,IEEE Transactions on Software Engineering
An insight into the technical debt-fix trade off in software backporting,2511.09000v1,ray2013detecting,\cite{ray2013detecting},Detecting and characterizing semantic inconsistencies in ported code,,,True,False,"Ray, Baishakhi and Kim, Miryung and Person, Suzette and Rungta, Neha",2013,,,,
An insight into the technical debt-fix trade off in software backporting,2511.09000v1,shariffdeen2021automated,\cite{shariffdeen2021automated},Automated patch backporting in Linux (experience paper),,,True,False,"Shariffdeen, Ridwan and Gao, Xiang and Duck, Gregory J and Tan, Shin Hwei and Lawall, Julia and Roychoudhury, Abhik",2021,,,,
An insight into the technical debt-fix trade off in software backporting,2511.09000v1,shi2022backporting,\cite{shi2022backporting},Backporting security patches of web applications: A prototype design and implementation on injection vulnerability patches,,,True,False,"Shi, Youkun and Zhang, Yuan and Luo, Tianhan and Mao, Xiangyu and Cao, Yinzhi and Wang, Ziwen and Zhao, Yudi and Huang, Zongan and Yang, Min",2022,,,,
An insight into the technical debt-fix trade off in software backporting,2511.09000v1,yang2023enhancing,\cite{yang2023enhancing},Enhancing OSS Patch Backporting with Semantics,,,True,False,"Yang, Su and Xiao, Yang and Xu, Zhengzi and Sun, Chengyi and Ji, Chen and Zhang, Yuqing",2023,,,,
An insight into the technical debt-fix trade off in software backporting,2511.09000v1,10.1145/3639478.3643079,\cite{10.1145/3639478.3643079},A Study of Backporting Code in Open-Source Software for Characterizing Changesets,,,True,False,"Chakroborti, Debasish and Roy, Chanchal and Schneider, Kevin",2024,,https://doi.org/10.1145/3639478.3643079,10.1145/3639478.3643079,
An insight into the technical debt-fix trade off in software backporting,2511.09000v1,td_0_Cunningham,\cite{td_0_Cunningham},The WyCash portfolio management system,,,True,False,"Cunningham, Ward",1992,,,,ACM Sigplan Oops Messenger
An insight into the technical debt-fix trade off in software backporting,2511.09000v1,kruchten2012technical,\cite{kruchten2012technical},Technical debt: From metaphor to theory and practice,,,True,False,"Kruchten, Philippe and Nord, Robert L and Ozkaya, Ipek",2012,,,,Ieee software
An insight into the technical debt-fix trade off in software backporting,2511.09000v1,td_5,\cite{td_5},An exploratory study of the impact of code smells on software change-proneness,,,True,False,"Khomh, Foutse and Di Penta, Massimiliano and Gueheneuc, Yann-Gael",2009,,,,
An insight into the technical debt-fix trade off in software backporting,2511.09000v1,s7,\cite{s7},An exploratory study of the impact of antipatterns on class change-and fault-proneness,,,True,False,"Khomh, Foutse and Penta, Massimiliano Di and Gu{\'e}h{\'e}neuc, Yann-Ga{\""e}l and Antoniol, Giuliano",2012,,,,Empirical Software Engineering
An insight into the technical debt-fix trade off in software backporting,2511.09000v1,td_3,\cite{td_3},When and why your code starts to smell bad,,,True,False,"Tufano, Michele and Palomba, Fabio and Bavota, Gabriele and Oliveto, Rocco and Di Penta, Massimiliano and De Lucia, Andrea and Poshyvanyk, Denys",2015,,,,
An insight into the technical debt-fix trade off in software backporting,2511.09000v1,td_side_effects,\cite{td_side_effects},Examining the impact of self-admitted technical debt on software quality,,,True,False,"Wehaibi, Sultan and Shihab, Emad and Guerrouj, Latifa",2016,,,,
An insight into the technical debt-fix trade off in software backporting,2511.09000v1,td_comprehension,\cite{td_comprehension},"An empirical study of the impact of two antipatterns, blob and spaghetti code, on program comprehension",,,True,False,"Abbes, Marwen and Khomh, Foutse and Gueheneuc, Yann-Gael and Antoniol, Giuliano",2011,,,,
An insight into the technical debt-fix trade off in software backporting,2511.09000v1,yamashita2012code,\cite{yamashita2012code},Do code smells reflect important maintainability aspects?,,,True,False,"Yamashita, Aiko and Moonen, Leon",2012,,,,
An insight into the technical debt-fix trade off in software backporting,2511.09000v1,alfayez2018empirical,\cite{alfayez2018empirical},An empirical study of technical debt in open-source software systems,,,True,False,"Alfayez, Reem and Chen, Celia and Behnamghader, Pooyan and Srisopha, Kamonphop and Boehm, Barry",2018,,,,
An insight into the technical debt-fix trade off in software backporting,2511.09000v1,klinger2011enterprise,\cite{klinger2011enterprise},An enterprise perspective on technical debt,,,True,False,"Klinger, Tim and Tarr, Peri and Wagstrom, Patrick and Williams, Clay",2011,,,,
An insight into the technical debt-fix trade off in software backporting,2511.09000v1,arcoverde2011understanding,\cite{arcoverde2011understanding},Understanding the longevity of code smells: preliminary results of an explanatory survey,,,True,False,"Arcoverde, Roberta and Garcia, Alessandro and Figueiredo, Eduardo",2011,,,,
An insight into the technical debt-fix trade off in software backporting,2511.09000v1,molnar2020longitudinal,\cite{molnar2020longitudinal},Longitudinal Evaluation of Open-Source Software Maintainability,https://arxiv.org/abs/2003.00447v1,"We present a longitudinal study on the long-term evolution of maintainability in open-source software. Quality assessment remains at the forefront of both software research and practice, with many models and assessment methodologies proposed and used over time. Some of them helped create and shape standards such as ISO 9126 and 25010, which are well established today. Both describe software quality in terms of characteristics such as reliability, security or maintainability. An important body of research exists linking these characteristics with software metrics, and proposing ways to automate quality assessment by aggregating software metric values into higher-level quality models. We employ the Maintainability Index, technical debt ratio and a maintainability model based on the ARiSA Compendium. Our study covers the entire 18 year development history and all released versions for three complex, open-source applications. We determine the maintainability for each version using the proposed models, we compare obtained results and use manual source code examination to put them into context. We examine the common development patterns of the target applications and study the relation between refactoring and maintainability. Finally, we study the strengths and weaknesses of each maintainability model using manual source code examination as the baseline.",True,True,"Molnar, Arthur-Jozsef and Motogna, Simona",2020,,,,arXiv preprint arXiv:2003.00447
An insight into the technical debt-fix trade off in software backporting,2511.09000v1,nayebi2019longitudinal,\cite{nayebi2019longitudinal},A longitudinal study of identifying and paying down architecture debt,,,True,False,"Nayebi, Maleknaz and Cai, Yuanfang and Kazman, Rick and Ruhe, Guenther and Feng, Qiong and Carlson, Chris and Chew, Francis",2019,,,,
An insight into the technical debt-fix trade off in software backporting,2511.09000v1,td_7,\cite{td_7},Long-Term Evaluation of Technical Debt in Open-Source Software,https://arxiv.org/abs/2007.13422v1,"Existing software tools enable characterizing and measuring the amount of technical debt at selective granularity levels. In this paper we aim to study the evolution and characteristics of technical debt in open-source software. We carry out a longitudinal study that covers the entire development history of several complex applications. We study how technical debt is introduced in software, as well as identify how developers handle its accumulation over the long term. We carried out our evaluation using three complex, open-source Java applications. All 110 released versions, covering more than 10 years of development history for each application were analyzed using SonarQube. We studied how the amount, composition and history of technical debt changed during development, compared our results across the studied applications and present our most important findings. For each application, we identified key versions during which large amounts of technical debt were added, removed or both. This had significantly more impact when compared to the lines of code or class count increases that generally occurred during development. Within each version, we found high correlation between file lines of code and technical debt. We observed that the Pareto principle was satisfied for the studied applications, as 20% of issue types generated around 80% of total technical debt. Early application versions showed greater fluctuation in the amount of existing technical debt. Application size appeared to be an unreliable predictor for the quantity of technical debt. Most debt was introduced in applications as part of milestone releases that expanded their feature set. We also discovered that technical debt issues persist for a long time in source code, and their removal did not appear to be prioritized according to type or severity.",True,True,"Molnar, AJ and Motogna, S",,,,,
Agentic Economic Modeling,2510.25743v1,charness2025next,\cite{charness2025next},The next generation of experimental research with LLMs,,,True,False,"Charness, Gary and Jabarian, Brian and List, John A",2025.0,,,,Nature Human Behaviour
Agentic Economic Modeling,2510.25743v1,davidson2025integrating,\cite{davidson2025integrating},"Integrating Generative Artificial Intelligence into Social Science Research: Measurement, Prompting, and Simulation",,,True,False,"Davidson, Thomas and Karell, Daniel",2025.0,,,,Sociological Methods \& Research
Agentic Economic Modeling,2510.25743v1,broska2025mixed,\cite{broska2025mixed},The Mixed Subjects Design: Treating Large Language Models as Potentially Informative Observations,,,True,False,"Broska, David and Howes, Michael and van Loon, Austin",2025.0,,,,Sociological Methods \& Research
Agentic Economic Modeling,2510.25743v1,arora2025ai,\cite{arora2025ai},AI--human hybrids for marketing research: Leveraging large language models (LLMs) as collaborators,,,True,False,"Arora, Neeraj and Chakraborty, Ishita and Nishimura, Yohei",2025.0,,,,Journal of Marketing
Agentic Economic Modeling,2510.25743v1,boelaert2025machine,\cite{boelaert2025machine},Machine Bias. How Do Generative Language Models Answer Opinion Polls?,,,True,False,"Boelaert, Julien and Coavoux, Samuel and Ollion, Etienne and Petev, Ivaylo and Pr{\""a}g, Patrick",2025.0,,,,Sociological Methods \& Research
Agentic Economic Modeling,2510.25743v1,lyman2025balancing,\cite{lyman2025balancing},Balancing large language model alignment and algorithmic fidelity in social science research,,,True,False,"Lyman, Alex and Hepner, Bryce and Argyle, Lisa P and Busby, Ethan C and Gubler, Joshua R and Wingate, David",2025.0,,,,Sociological Methods \& Research
Agentic Economic Modeling,2510.25743v1,kozlowski2025simulating,\cite{kozlowski2025simulating},Simulating Subjects: The Promise and Peril of Artificial Intelligence Stand-Ins for Social Agents and Interactions,,,True,False,"Kozlowski, Austin C and Evans, James",2025.0,,,,Sociological Methods \& Research
Agentic Economic Modeling,2510.25743v1,wang2024large,\cite{wang2024large},Large language models for market research: A data-augmentation approach,,,True,False,"Wang, Mengxin and Zhang, Dennis J and Zhang, Heng",2024.0,,,,arXiv preprint arXiv:2412.19363
Agentic Economic Modeling,2510.25743v1,bui2025mixture,\cite{bui2025mixture},Mixture-of-personas language models for population simulation,,,True,False,"Bui, Ngoc and Nguyen, Hieu Trung and Kumar, Shantanu and Theodore, Julian and Qiu, Weikang and Nguyen, Viet Anh and Ying, Rex",2025.0,,,,arXiv preprint arXiv:2504.05019
Agentic Economic Modeling,2510.25743v1,leng2024can,\cite{leng2024can},Can LLMs mimic human-like mental accounting and behavioral biases?,,,True,False,"Leng, Yan",2024.0,,,,Available at SSRN 4705130
Agentic Economic Modeling,2510.25743v1,leng2023llm,\cite{leng2023llm},Do LLM Agents Exhibit Social Behavior?,https://arxiv.org/abs/2312.15198v3,"As LLMs increasingly take on roles in human-AI interactions and autonomous AI systems, understanding their social behavior becomes important for informed use and continuous improvement. However, their behaviors in social interactions with humans and other agents, as well as the mechanisms shaping their responses, remain underexplored. To address this gap, we introduce a novel probabilistic framework, State-Understanding-Value-Action (SUVA), to systematically analyze LLM responses in social contexts based on their textual outputs (i.e., utterances). Using canonical behavioral economics games and social preference concepts relatable to LLM users, SUVA assesses LLMs' social behavior through both their final decisions and the response generation processes leading to those decisions. Our analysis of eight LLMs -- including two GPT, four LLaMA, and two Mistral models -- suggests that most models do not generate decisions aligned solely with self-interest; instead, they often produce responses that reflect social welfare considerations and display patterns consistent with direct and indirect reciprocity. Additionally, higher-capacity models more frequently display group identity effects. The SUVA framework also provides explainable tools -- including tree-based visualizations and probabilistic dependency analysis -- to elucidate how factors in LLMs' utterance-based reasoning influence their decisions. We demonstrate that utterance-based reasoning reliably predicts LLMs' final actions; references to altruism, fairness, and cooperation in the reasoning increase the likelihood of prosocial actions, while mentions of self-interest and competition reduce them. Overall, our framework enables practitioners to assess LLMs for applications involving social interactions, and provides researchers with a structured method to interpret how LLM behavior arises from utterance-based reasoning.",True,True,"Leng, Yan and Yuan, Yuan",2023.0,,,,arXiv preprint arXiv:2312.15198
Agentic Economic Modeling,2510.25743v1,li2025llm,\cite{li2025llm},LLM Generated Persona is a Promise with a Catch,,,True,False,"Li, Ang and Chen, Haozhe and Namkoong, Hongseok and Peng, Tianyi",2025.0,,,,arXiv preprint arXiv:2503.16527
Algorithmic Predation: Equilibrium Analysis in Dynamic Oligopolies with Smooth Market Sharing,2510.27008v1,fudenberg2013dynamic,\cite{fudenberg2013dynamic},Dynamic models of oligopoly,,,True,False,"Fudenberg, Drew and Tirole, Jean",2013.0,,,,
Algorithmic Predation: Equilibrium Analysis in Dynamic Oligopolies with Smooth Market Sharing,2510.27008v1,gerpottCompetitivePricingOnline2022,\cite{gerpottCompetitivePricingOnline2022},Competitive Pricing on Online Markets: A Literature Review,,,True,False,"Gerpott, Torsten J. and Berends, Jan",2022.0,,,,Journal of Revenue and Pricing Management
Algorithmic Predation: Equilibrium Analysis in Dynamic Oligopolies with Smooth Market Sharing,2510.27008v1,phlips1989dynamic,\cite{phlips1989dynamic},A dynamic oligopoly model with demand inertia and inventories,,,True,False,"Phlips, Louis and Richard, Jean-Francois",1989.0,,,,Mathematical Social Sciences
Algorithmic Predation: Equilibrium Analysis in Dynamic Oligopolies with Smooth Market Sharing,2510.27008v1,farrell1988dynamic,\cite{farrell1988dynamic},Dynamic competition with switching costs,,,True,False,"Farrell, Joseph and Shapiro, Carl",1988.0,,,,The RAND Journal of Economics
Algorithmic Predation: Equilibrium Analysis in Dynamic Oligopolies with Smooth Market Sharing,2510.27008v1,bayer2007network,\cite{bayer2007network},"Network externalities, demand inertia and dynamic pricing in an experimental oligopoly market",,,True,False,"Bayer, Ralph-C and Chan, Mickey",2007.0,,,,Economic Record
Algorithmic Predation: Equilibrium Analysis in Dynamic Oligopolies with Smooth Market Sharing,2510.27008v1,maskin1988theory,\cite{maskin1988theory},"A theory of dynamic oligopoly, II: Price competition, kinked demand curves, and Edgeworth cycles",,,True,False,"Maskin, Eric and Tirole, Jean",1988.0,,,,Econometrica: Journal of the Econometric Society
Algorithmic Predation: Equilibrium Analysis in Dynamic Oligopolies with Smooth Market Sharing,2510.27008v1,cabralLearningCurveMarket1994,\cite{cabralLearningCurveMarket1994},"The {{Learning Curve}}, {{Market Dominance}}, and {{Predatory Pricing}}",,,True,False,"Cabral, Luis and Riordan, Michael",1994.0,,,,Econometrica
Algorithmic Predation: Equilibrium Analysis in Dynamic Oligopolies with Smooth Market Sharing,2510.27008v1,besanko2014economics,\cite{besanko2014economics},The economics of predation: What drives pricing when there is learning-by-doing?,,,True,False,"Besanko, David and Doraszelski, Ulrich and Kryukov, Yaroslav",2014.0,,,,American Economic Review
Algorithmic Predation: Equilibrium Analysis in Dynamic Oligopolies with Smooth Market Sharing,2510.27008v1,reyDynamicModelPredation2022,\cite{reyDynamicModelPredation2022},A {{Dynamic Model}} of {{Predation}},,,True,False,"Rey, Patrick and Spiegel, Yossi and Stahl, Konrad O.",2022.0,,,,
Algorithmic Predation: Equilibrium Analysis in Dynamic Oligopolies with Smooth Market Sharing,2510.27008v1,pakes1992computing,\cite{pakes1992computing},Computing markov perfect nash equilibria: Numerical implications of a dynamic differentiated product model,,,True,False,"Pakes, Ariel and McGuire, Paul",1992.0,,,,
Algorithmic Predation: Equilibrium Analysis in Dynamic Oligopolies with Smooth Market Sharing,2510.27008v1,bylkaDiscreteTimeDynamic2000,\cite{bylkaDiscreteTimeDynamic2000},Discrete Time Dynamic Game Model for Price Competition in an Oligopoly,,,True,False,"Bylka, Stanis{\l}aw and Ambroszkiewicz, Stanis{\l}aw and Komar, Jan",2000.0,,,,Annals of Operations Research
Algorithmic Predation: Equilibrium Analysis in Dynamic Oligopolies with Smooth Market Sharing,2510.27008v1,fudenbergTheoryLearningGames1999,\cite{fudenbergTheoryLearningGames1999},The Theory of Learning in Games,,,True,False,"Fudenberg, Drew and Levine, David K.",1999.0,,,,
Algorithmic Predation: Equilibrium Analysis in Dynamic Oligopolies with Smooth Market Sharing,2510.27008v1,milionisImpossibilityTheoremGame2023,\cite{milionisImpossibilityTheoremGame2023},An Impossibility Theorem in Game Dynamics,,,True,False,"Milionis, Jason and Papadimitriou, Christos and Piliouras, Georgios and Spendlove, Kelly",2023.0,,,,Proceedings of the National Academy of Sciences
Algorithmic Predation: Equilibrium Analysis in Dynamic Oligopolies with Smooth Market Sharing,2510.27008v1,mazumdarPolicyGradientAlgorithmsHave2020,\cite{mazumdarPolicyGradientAlgorithmsHave2020},Policy-Gradient Algorithms Have No Guarantees of Convergence in Linear Quadratic Games,https://arxiv.org/abs/1907.03712v2,"We show by counterexample that policy-gradient algorithms have no guarantees of even local convergence to Nash equilibria in continuous action and state space multi-agent settings. To do so, we analyze gradient-play in N-player general-sum linear quadratic games, a classic game setting which is recently emerging as a benchmark in the field of multi-agent learning. In such games the state and action spaces are continuous and global Nash equilibria can be found be solving coupled Ricatti equations. Further, gradient-play in LQ games is equivalent to multi agent policy-gradient. We first show that these games are surprisingly not convex games. Despite this, we are still able to show that the only critical points of the gradient dynamics are global Nash equilibria. We then give sufficient conditions under which policy-gradient will avoid the Nash equilibria, and generate a large number of general-sum linear quadratic games that satisfy these conditions. In such games we empirically observe the players converging to limit cycles for which the time average does not coincide with a Nash equilibrium. The existence of such games indicates that one of the most popular approaches to solving reinforcement learning problems in the classic reinforcement learning setting has no local guarantee of convergence in multi-agent settings. Further, the ease with which we can generate these counterexamples suggests that such situations are not mere edge cases and are in fact quite common.",True,True,"Mazumdar, Eric and Ratliff, Lillian J. and Jordan, Michael I. and Sastry, S. Shankar",2020.0,,,,
Algorithmic Predation: Equilibrium Analysis in Dynamic Oligopolies with Smooth Market Sharing,2510.27008v1,daskalakisLearningAlgorithmsNash2010,\cite{daskalakisLearningAlgorithmsNash2010},On Learning Algorithms for {{Nash}} Equilibria,,,True,False,"Daskalakis, Constantinos and Frongillo, Rafael and Papadimitriou, Christos H and Pierrakos, George and Valiant, Gregory",2010.0,,,,
Algorithmic Predation: Equilibrium Analysis in Dynamic Oligopolies with Smooth Market Sharing,2510.27008v1,bichlerConvergenceLearningAlgorithms2023,\cite{bichlerConvergenceLearningAlgorithms2023},On the {{Convergence}} of {{Learning Algorithms}} in {{Bayesian Auction Games}},,,True,False,"Bichler, Martin and Lunowa, Stephan B. and Oberlechner, Matthias and Pieroth, Fabian R. and Wohlmuth, Barbara",2023.0,,,,arXiv e-prints
Algorithmic Predation: Equilibrium Analysis in Dynamic Oligopolies with Smooth Market Sharing,2510.27008v1,serefahunbayUniquenessBayesianCoarse2024,\cite{serefahunbayUniquenessBayesianCoarse2024},On the {{Uniqueness}} of {{Bayesian Coarse Correlated Equilibria}} in {{Standard First-Price}} and {{All-Pay Auctions}},,,True,False,"{\c S}eref Ahunbay, Mete and Bichler, Martin",2024.0,,,,arXiv e-prints
Algorithmic Predation: Equilibrium Analysis in Dynamic Oligopolies with Smooth Market Sharing,2510.27008v1,pieroth2025,\cite{pieroth2025},Deep reinforcement learning for equilibrium computation in multi-stage auctions and contests,,,True,False,"Pieroth, Fabian Raoul and Kohring, Nils and Bichler, Martin",2025.0,,,,Management Science
Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs,2511.05766v1,bereska2024mechanisticinterpretabilityaisafety,\cite{bereska2024mechanisticinterpretabilityaisafety},{Mechanistic Interpretability for {AI} Safety -- A Review},,,True,False,Leonard Bereska and Efstratios Gavves,2024.0,,https://arxiv.org/abs/2404.14082,,
Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs,2511.05766v1,TVERSKY1973207,\cite{TVERSKY1973207},{Availability: A Heuristic for Judging Frequency and Probability},,,True,False,Amos Tversky and Daniel Kahneman,1973.0,,https://www.sciencedirect.com/science/article/pii/0010028573900339,10.1016/0010-0285(73)90033-9,Cognitive Psychology
Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs,2511.05766v1,doi:10.1126/science.185.4157.1124,\cite{doi:10.1126/science.185.4157.1124},{Judgment under Uncertainty: Heuristics and Biases},,,True,False,Amos Tversky and Daniel Kahneman,1974.0,,https://www.science.org/doi/abs/10.1126/science.185.4157.1124,10.1126/science.185.4157.1124,Science
Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs,2511.05766v1,tversky1981framing,\cite{tversky1981framing},{The Framing of Decisions and the Psychology of Choice},,,True,False,Amos Tversky and Daniel Kahneman,1981.0,,http://www.jstor.org/stable/1685855,,Science
Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs,2511.05766v1,cui2025large,\cite{cui2025large},Can Large Language Models Replace Human Subjects? A Large-Scale Replication of Scenario-Based Experiments in Psychology and Management,https://arxiv.org/abs/2409.00128v3,"Artificial Intelligence (AI) is increasingly being integrated into scientific research, particularly in the social sciences, where understanding human behavior is critical. Large Language Models (LLMs) have shown promise in replicating human-like responses in various psychological experiments. We conducted a large-scale study replicating 156 psychological experiments from top social science journals using three state-of-the-art LLMs (GPT-4, Claude 3.5 Sonnet, and DeepSeek v3). Our results reveal that while LLMs demonstrate high replication rates for main effects (73-81%) and moderate to strong success with interaction effects (46-63%), They consistently produce larger effect sizes than human studies, with Fisher Z values approximately 2-3 times higher than human studies. Notably, LLMs show significantly lower replication rates for studies involving socially sensitive topics such as race, gender and ethics. When original studies reported null findings, LLMs produced significant results at remarkably high rates (68-83%) - while this could reflect cleaner data with less noise, as evidenced by narrower confidence intervals, it also suggests potential risks of effect size overestimation. Our results demonstrate both the promise and challenges of LLMs in psychological research, offering efficient tools for pilot testing and rapid hypothesis validation while enriching rather than replacing traditional human subject studies, yet requiring more nuanced interpretation and human validation for complex social phenomena and culturally sensitive research questions.",True,True,Ziyan Cui and Ning Li and Huaikang Zhou,2025.0,,https://www.nature.com/articles/s43588-025-00829-y,10.1038/s43588-025-00829-y,Nature Computational Science
Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs,2511.05766v1,binz2023cognitive,\cite{binz2023cognitive},Using cognitive psychology to understand GPT-3,https://arxiv.org/abs/2206.14576v1,"We study GPT-3, a recent large language model, using tools from cognitive psychology. More specifically, we assess GPT-3's decision-making, information search, deliberation, and causal reasoning abilities on a battery of canonical experiments from the literature. We find that much of GPT-3's behavior is impressive: it solves vignette-based tasks similarly or better than human subjects, is able to make decent decisions from descriptions, outperforms humans in a multi-armed bandit task, and shows signatures of model-based reinforcement learning. Yet we also find that small perturbations to vignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures of directed exploration, and that it fails miserably in a causal reasoning task. These results enrich our understanding of current large language models and pave the way for future investigations using tools from cognitive psychology to study increasingly capable and opaque artificial agents.",True,True,Marcel Binz and Eric Schulz,2023.0,,https://www.pnas.org/doi/abs/10.1073/pnas.2218523120,10.1073/pnas.2218523120,Proceedings of the National Academy of Sciences
Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs,2511.05766v1,suri2023largelanguagemodelsdecision,\cite{suri2023largelanguagemodelsdecision},Do Large Language Models Show Decision Heuristics Similar to Humans? A Case Study Using GPT-3.5,https://arxiv.org/abs/2305.04400v1,"A Large Language Model (LLM) is an artificial intelligence system that has been trained on vast amounts of natural language data, enabling it to generate human-like responses to written or spoken language input. GPT-3.5 is an example of an LLM that supports a conversational agent called ChatGPT. In this work, we used a series of novel prompts to determine whether ChatGPT shows heuristics, biases, and other decision effects. We also tested the same prompts on human participants. Across four studies, we found that ChatGPT was influenced by random anchors in making estimates (Anchoring Heuristic, Study 1); it judged the likelihood of two events occurring together to be higher than the likelihood of either event occurring alone, and it was erroneously influenced by salient anecdotal information (Representativeness and Availability Heuristic, Study 2); it found an item to be more efficacious when its features were presented positively rather than negatively - even though both presentations contained identical information (Framing Effect, Study 3); and it valued an owned item more than a newly found item even though the two items were identical (Endowment Effect, Study 4). In each study, human participants showed similar effects. Heuristics and related decision effects in humans are thought to be driven by cognitive and affective processes such as loss aversion and effort reduction. The fact that an LLM - which lacks these processes - also shows such effects invites consideration of the possibility that language may play a role in generating these effects in humans.",True,True,Gaurav Suri and Lily R. Slater and Ali Ziaee and Morgan Nguyen,2023.0,,https://arxiv.org/abs/2305.04400,,
Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs,2511.05766v1,lou2024anchoringbiaslargelanguage,\cite{lou2024anchoringbiaslargelanguage},Anchoring Bias in Large Language Models: An Experimental Study,https://arxiv.org/abs/2412.06593v2,"Large Language Models (LLMs) like GPT-4 and Gemini have significantly advanced artificial intelligence by enabling machines to generate and comprehend human-like text. Despite their impressive capabilities, LLMs are not immune to limitations, including various biases. While much research has explored demographic biases, the cognitive biases in LLMs have not been equally scrutinized. This study delves into anchoring bias, a cognitive bias where initial information disproportionately influences judgment. Utilizing an experimental dataset, we examine how anchoring bias manifests in LLMs and verify the effectiveness of various mitigation strategies. Our findings highlight the sensitivity of LLM responses to biased hints. At the same time, our experiments show that, to mitigate anchoring bias, one needs to collect hints from comprehensive angles to prevent the LLMs from being anchored to individual pieces of information, while simple algorithms such as Chain-of-Thought, Thoughts of Principles, Ignoring Anchor Hints, and Reflection are not sufficient.",True,True,Jiaxu Lou and Yifan Sun,2024.0,,https://arxiv.org/abs/2412.06593,,
Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs,2511.05766v1,stureborg2024largelanguagemodelsinconsistent,\cite{stureborg2024largelanguagemodelsinconsistent},{Large Language Models are Inconsistent and Biased Evaluators},,,True,False,Rickard Stureborg and Dimitris Alikaniotis and Yoshi Suhara,2024.0,,https://arxiv.org/abs/2405.01724,,
Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs,2511.05766v1,wang2023largelanguagemodelsfair,\cite{wang2023largelanguagemodelsfair},{Large Language Models are not Fair Evaluators},,,True,False,Peiyi Wang and Lei Li and Liang Chen and Zefan Cai and Dawei Zhu and Binghuai Lin and Yunbo Cao and Qi Liu and Tianyu Liu and Zhifang Sui,2023.0,,https://arxiv.org/abs/2305.17926,,
Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs,2511.05766v1,chen2024aicognitivelybiasedexploratory,\cite{chen2024aicognitivelybiasedexploratory},AI Can Be Cognitively Biased: An Exploratory Study on Threshold Priming in LLM-Based Batch Relevance Assessment,https://arxiv.org/abs/2409.16022v2,"Cognitive biases are systematic deviations in thinking that lead to irrational judgments and problematic decision-making, extensively studied across various fields. Recently, large language models (LLMs) have shown advanced understanding capabilities but may inherit human biases from their training data. While social biases in LLMs have been well-studied, cognitive biases have received less attention, with existing research focusing on specific scenarios. The broader impact of cognitive biases on LLMs in various decision-making contexts remains underexplored. We investigated whether LLMs are influenced by the threshold priming effect in relevance judgments, a core task and widely-discussed research topic in the Information Retrieval (IR) coummunity. The priming effect occurs when exposure to certain stimuli unconsciously affects subsequent behavior and decisions. Our experiment employed 10 topics from the TREC 2019 Deep Learning passage track collection, and tested AI judgments under different document relevance scores, batch lengths, and LLM models, including GPT-3.5, GPT-4, LLaMa2-13B and LLaMa2-70B. Results showed that LLMs tend to give lower scores to later documents if earlier ones have high relevance, and vice versa, regardless of the combination and model used. Our finding demonstrates that LLM%u2019s judgments, similar to human judgments, are also influenced by threshold priming biases, and suggests that researchers and system engineers should take into account potential human-like cognitive biases in designing, evaluating, and auditing LLMs in IR tasks and beyond.",True,True,Nuo Chen and Jiqun Liu and Xiaoyu Dong and Qijiong Liu and Tetsuya Sakai and Xiao-Ming Wu,2024.0,,https://arxiv.org/abs/2409.16022,,
Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs,2511.05766v1,sumita2024cognitivebiaseslargelanguage,\cite{sumita2024cognitivebiaseslargelanguage},{Cognitive Biases in Large Language Models: A Survey and Mitigation Experiments},,,True,False,Yasuaki Sumita and Koh Takeuchi and Hisashi Kashima,2024.0,,https://arxiv.org/abs/2412.00323,,
Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs,2511.05766v1,li2025anchoredanswersunravellingpositional,\cite{li2025anchoredanswersunravellingpositional},Anchored Answers: Unravelling Positional Bias in GPT-2's Multiple-Choice Questions,https://arxiv.org/abs/2405.03205v3,"Large Language Models (LLMs), such as the GPT-4 and LLaMA families, have demonstrated considerable success across diverse tasks, including multiple-choice questions (MCQs). However, these models exhibit a positional bias, particularly an even worse anchored bias in the GPT-2 family, where they consistently favour the first choice 'A' in MCQs during inference. This anchored bias challenges the integrity of GPT-2's decision-making process, as it skews performance based on the position rather than the content of the choices in MCQs. In this study, we utilise the mechanistic interpretability approach to identify the internal modules within GPT-2 models responsible for this bias. We focus on the Multi-Layer Perceptron (MLP) layers and attention heads, using the ""logit lens"" method to trace and modify the specific value vectors that contribute to the bias. By updating these vectors within MLP and recalibrating attention patterns to neutralise the preference for the first choice 'A', we effectively mitigate the anchored bias. Our interventions not only mitigate the bias but also improve the overall MCQ prediction accuracy for the GPT-2 family across various datasets. This work represents the first comprehensive mechanistic analysis of anchored bias from the failing cases in MCQs within the GPT-2 models, introducing targeted, minimal-intervention strategies that significantly enhance GPT2 model robustness and accuracy in MCQs. Our code is available at https://github.com/ruizheliUOA/Anchored_Bias_GPT2.",True,True,Ruizhe Li and Yanjun Gao,2025.0,,https://arxiv.org/abs/2405.03205,,
Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs,2511.05766v1,shapley1953value,\cite{shapley1953value},{A Value for n-Person Games},,,True,False,Lloyd S. Shapley,1953.0,,,,
Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs,2511.05766v1,mohammadi2024explaininglargelanguagemodels,\cite{mohammadi2024explaininglargelanguagemodels},{Explaining Large Language Models Decisions Using Shapley Values},,,True,False,Behnam Mohammadi,2024.0,,https://arxiv.org/abs/2404.01332,,
Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs,2511.05766v1,goldshmidt2024tokenshapinterpretinglargelanguage,\cite{goldshmidt2024tokenshapinterpretinglargelanguage},TokenSHAP: Interpreting Large Language Models with Monte Carlo Shapley Value Estimation,https://arxiv.org/abs/2407.10114v2,"As large language models (LLMs) become increasingly prevalent in critical applications, the need for interpretable AI has grown. We introduce TokenSHAP, a novel method for interpreting LLMs by attributing importance to individual tokens or substrings within input prompts. This approach adapts Shapley values from cooperative game theory to natural language processing, offering a rigorous framework for understanding how different parts of an input contribute to a model's response. TokenSHAP leverages Monte Carlo sampling for computational efficiency, providing interpretable, quantitative measures of token importance. We demonstrate its efficacy across diverse prompts and LLM architectures, showing consistent improvements over existing baselines in alignment with human judgments, faithfulness to model behavior, and consistency.
  Our method's ability to capture nuanced interactions between tokens provides valuable insights into LLM behavior, enhancing model transparency, improving prompt engineering, and aiding in the development of more reliable AI systems. TokenSHAP represents a significant step towards the necessary interpretability for responsible AI deployment, contributing to the broader goal of creating more transparent, accountable, and trustworthy AI systems.",True,True,Roni Goldshmidt and Miriam Horovicz,2024.0,,https://arxiv.org/abs/2407.10114,,
Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs,2511.05766v1,NIPS2017_7062,\cite{NIPS2017_7062},A Unified Approach to Interpreting Model Predictions,https://arxiv.org/abs/1705.07874v2,"Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.",True,True,"Lundberg, Scott M and Lee, Su-In",2017.0,,http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf,,
A Multi-Layer Machine Learning and Econometric Pipeline for Forecasting Market Risk: Evidence from Cryptoasset Liquidity Spillovers,2510.20066v1,granger1969,\cite{granger1969},Investigating Causal Relations by Econometric Models and Cross-spectral Methods,,,True,False,"Granger, C. W. J.",1969.0,,,10.2307/1912791,Econometrica
A Multi-Layer Machine Learning and Econometric Pipeline for Forecasting Market Risk: Evidence from Cryptoasset Liquidity Spillovers,2510.20066v1,sims1980,\cite{sims1980},Macroeconomics and Reality,,,True,False,"Sims, Christopher A.",1980.0,,,10.2307/1912017,Econometrica
A Multi-Layer Machine Learning and Econometric Pipeline for Forecasting Market Risk: Evidence from Cryptoasset Liquidity Spillovers,2510.20066v1,amihud2002,\cite{amihud2002},Illiquidity and Stock Returns: Cross-section and Time-series Effects,,,True,False,"Amihud, Yakov",2002.0,,,10.1016/S1386-4181(01)00024-6,Journal of Financial Markets
A Multi-Layer Machine Learning and Econometric Pipeline for Forecasting Market Risk: Evidence from Cryptoasset Liquidity Spillovers,2510.20066v1,parkinson1980,\cite{parkinson1980},The Extreme Value Method for Estimating the Variance of the Rate of Return,,,True,False,"Parkinson, Michael",1980.0,,,10.1086/296071,Journal of Business
A Multi-Layer Machine Learning and Econometric Pipeline for Forecasting Market Risk: Evidence from Cryptoasset Liquidity Spillovers,2510.20066v1,bollerslev1986,\cite{bollerslev1986},Functional generalized autoregressive conditional heteroskedasticity,https://arxiv.org/abs/1509.03813v2,"Heteroskedasticity is a common feature of financial time series and is commonly addressed in the model building process through the use of ARCH and GARCH processes. More recently multivariate variants of these processes have been in the focus of research with attention given to methods seeking an efficient and economic estimation of a large number of model parameters. Due to the need for estimation of many parameters, however, these models may not be suitable for modeling now prevalent high-frequency volatility data. One potentially useful way to bypass these issues is to take a functional approach. In this paper, theory is developed for a new functional version of the generalized autoregressive conditionally heteroskedastic process, termed fGARCH. The main results are concerned with the structure of the fGARCH(1,1) process, providing criteria for the existence of a strictly stationary solutions both in the space of square-integrable and continuous functions. An estimation procedure is introduced and its consistency verified. A small empirical study highlights potential applications to intraday volatility estimation.",True,True,"Bollerslev, Tim",1986.0,,,10.1016/0304-4076(86)90063-1,Journal of Econometrics
A Multi-Layer Machine Learning and Econometric Pipeline for Forecasting Market Risk: Evidence from Cryptoasset Liquidity Spillovers,2510.20066v1,corsi2009simple,\cite{corsi2009simple},A Simple Approximate Long-Memory Model of Realized Volatility,,,True,False,"Corsi, Fulvio",2009.0,,,10.1093/jjfinec/nbp001,Journal of Financial Econometrics
A Multi-Layer Machine Learning and Econometric Pipeline for Forecasting Market Risk: Evidence from Cryptoasset Liquidity Spillovers,2510.20066v1,newey1987,\cite{newey1987},"A Simple, Positive Semi-definite, Heteroskedasticity and Autocorrelation Consistent Covariance Matrix",,,True,False,"Newey, Whitney K. and West, Kenneth D.",1987.0,,,10.2307/1913610,Econometrica
A Multi-Layer Machine Learning and Econometric Pipeline for Forecasting Market Risk: Evidence from Cryptoasset Liquidity Spillovers,2510.20066v1,chen2016xgboost,\cite{chen2016xgboost},XGBoost: A Scalable Tree Boosting System,https://arxiv.org/abs/1603.02754v3,"Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.",True,True,"Chen, Tianqi and Guestrin, Carlos",2016.0,,,10.1145/2939672.2939785,
A Multi-Layer Machine Learning and Econometric Pipeline for Forecasting Market Risk: Evidence from Cryptoasset Liquidity Spillovers,2510.20066v1,lundberg2017unified,\cite{lundberg2017unified},A Unified Approach to Interpreting Model Predictions,https://arxiv.org/abs/1705.07874v2,"Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.",True,True,"Lundberg, Scott M. and Lee, Su-In",2017.0,,https://doi.org/10.48550/arXiv.1705.07874,10.48550/arXiv.1705.07874,
Diffolio: A Diffusion Model for Multivariate Probabilistic Financial Time-Series Forecasting and Portfolio Construction,2511.07014v1,yoon2019time,\cite{yoon2019time},Time-series Transformer Generative Adversarial Networks,https://arxiv.org/abs/2205.11164v1,"Many real-world tasks are plagued by limitations on data: in some instances very little data is available and in others, data is protected by privacy enforcing regulations (e.g. GDPR). We consider limitations posed specifically on time-series data and present a model that can generate synthetic time-series which can be used in place of real data. A model that generates synthetic time-series data has two objectives: 1) to capture the stepwise conditional distribution of real sequences, and 2) to faithfully model the joint distribution of entire real sequences. Autoregressive models trained via maximum likelihood estimation can be used in a system where previous predictions are fed back in and used to predict future ones; in such models, errors can accrue over time. Furthermore, a plausible initial value is required making MLE based models not really generative. Many downstream tasks learn to model conditional distributions of the time-series, hence, synthetic data drawn from a generative model must satisfy 1) in addition to performing 2). We present TsT-GAN, a framework that capitalises on the Transformer architecture to satisfy the desiderata and compare its performance against five state-of-the-art models on five datasets and show that TsT-GAN achieves higher predictive performance on all datasets.",True,True,"Yoon, Jinsung and Jarrett, Daniel and Van der Schaar, Mihaela",2019.0,,,,Advances in neural information processing systems
Diffolio: A Diffusion Model for Multivariate Probabilistic Financial Time-Series Forecasting and Portfolio Construction,2511.07014v1,takahashi2019modeling,\cite{takahashi2019modeling},Modeling financial time-series with generative adversarial networks,,,True,False,"Takahashi, Shuntaro and Chen, Yu and Tanaka-Ishii, Kumiko",2019.0,,,,Physica A: Statistical Mechanics and its Applications
Diffolio: A Diffusion Model for Multivariate Probabilistic Financial Time-Series Forecasting and Portfolio Construction,2511.07014v1,wiese2020quant,\cite{wiese2020quant},Quant GANs: Deep Generation of Financial Time Series,https://arxiv.org/abs/1907.06673v2,"Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity.",True,True,"Wiese, Magnus and Knobloch, Robert and Korn, Ralf and Kretschmer, Peter",2020.0,,,,Quantitative Finance
Diffolio: A Diffusion Model for Multivariate Probabilistic Financial Time-Series Forecasting and Portfolio Construction,2511.07014v1,vuletic2024fin,\cite{vuletic2024fin},{Fin-gan: Forecasting and classifying financial time series via generative adversarial networks},,,True,False,"Vuleti{\'c}, Milena and Prenzel, Felix and Cucuringu, Mihai",2024.0,,,,Quantitative Finance
Diffolio: A Diffusion Model for Multivariate Probabilistic Financial Time-Series Forecasting and Portfolio Construction,2511.07014v1,liao2024sig,\cite{liao2024sig},{Sig-Wasserstein GANs for conditional time series generation},,,True,False,"Liao, Shujian and Ni, Hao and Sabate-Vidales, Marc and Szpruch, Lukasz and Wiese, Magnus and Xiao, Baoren",2024.0,,,,Mathematical Finance
Diffolio: A Diffusion Model for Multivariate Probabilistic Financial Time-Series Forecasting and Portfolio Construction,2511.07014v1,duan2022factorvae,\cite{duan2022factorvae},{Factorvae: A probabilistic dynamic factor model based on variational autoencoder for predicting cross-sectional stock returns},,,True,False,"Duan, Yitong and Wang, Lei and Zhang, Qizhong and Li, Jian",2022.0,,,,
Diffolio: A Diffusion Model for Multivariate Probabilistic Financial Time-Series Forecasting and Portfolio Construction,2511.07014v1,sohl2015deep,\cite{sohl2015deep},Deep Unsupervised Learning using Nonequilibrium Thermodynamics,https://arxiv.org/abs/1503.03585v8,"A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.",True,True,"Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya",2015.0,,,,
Diffolio: A Diffusion Model for Multivariate Probabilistic Financial Time-Series Forecasting and Portfolio Construction,2511.07014v1,ho2020denoising,\cite{ho2020denoising},Denoising Diffusion Probabilistic Models,https://arxiv.org/abs/2006.11239v2,"We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion",True,True,"Ho, Jonathan and Jain, Ajay and Abbeel, Pieter",2020.0,,,,Advances in neural information processing systems
Diffolio: A Diffusion Model for Multivariate Probabilistic Financial Time-Series Forecasting and Portfolio Construction,2511.07014v1,kong2020diffwave,\cite{kong2020diffwave},{Diffwave: A versatile diffusion model for audio synthesis},,,True,False,"Kong, Zhifeng and Ping, Wei and Huang, Jiaji and Zhao, Kexin and Catanzaro, Bryan",2020.0,,,,arXiv preprint arXiv:2009.09761
Diffolio: A Diffusion Model for Multivariate Probabilistic Financial Time-Series Forecasting and Portfolio Construction,2511.07014v1,kollovieh2023predict,\cite{kollovieh2023predict},"Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting",https://arxiv.org/abs/2307.11494v3,"Diffusion models have achieved state-of-the-art performance in generative modeling tasks across various domains. Prior works on time series diffusion models have primarily focused on developing conditional models tailored to specific forecasting or imputation tasks. In this work, we explore the potential of task-agnostic, unconditional diffusion models for several time series applications. We propose TSDiff, an unconditionally-trained diffusion model for time series. Our proposed self-guidance mechanism enables conditioning TSDiff for downstream tasks during inference, without requiring auxiliary networks or altering the training procedure. We demonstrate the effectiveness of our method on three different time series tasks: forecasting, refinement, and synthetic data generation. First, we show that TSDiff is competitive with several task-specific conditional forecasting methods (predict). Second, we leverage the learned implicit probability density of TSDiff to iteratively refine the predictions of base forecasters with reduced computational overhead over reverse diffusion (refine). Notably, the generative performance of the model remains intact -- downstream forecasters trained on synthetic samples from TSDiff outperform forecasters that are trained on samples from other state-of-the-art generative time series models, occasionally even outperforming models trained on real data (synthesize).",True,True,"Kollovieh, Marcel and Ansari, Abdul Fatir and Bohlke-Schneider, Michael and Zschiegner, Jasper and Wang, Hao and Wang, Yuyang Bernie",2023.0,,,,Advances in Neural Information Processing Systems
Diffolio: A Diffusion Model for Multivariate Probabilistic Financial Time-Series Forecasting and Portfolio Construction,2511.07014v1,rasul2021autoregressive,\cite{rasul2021autoregressive},Autoregressive denoising diffusion models for multivariate probabilistic time series forecasting,,,True,False,"Rasul, Kashif and Seward, Calvin and Schuster, Ingmar and Vollgraf, Roland",2021.0,,,,
Diffolio: A Diffusion Model for Multivariate Probabilistic Financial Time-Series Forecasting and Portfolio Construction,2511.07014v1,tashiro2021csdi,\cite{tashiro2021csdi},CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation,https://arxiv.org/abs/2107.03502v2,"The imputation of missing values in time series has many applications in healthcare and finance. While autoregressive models are natural candidates for time series imputation, score-based diffusion models have recently outperformed existing counterparts including autoregressive models in many tasks such as image generation and audio synthesis, and would be promising for time series imputation. In this paper, we propose Conditional Score-based Diffusion models for Imputation (CSDI), a novel time series imputation method that utilizes score-based diffusion models conditioned on observed data. Unlike existing score-based approaches, the conditional diffusion model is explicitly trained for imputation and can exploit correlations between observed values. On healthcare and environmental data, CSDI improves by 40-65% over existing probabilistic imputation methods on popular performance metrics. In addition, deterministic imputation by CSDI reduces the error by 5-20% compared to the state-of-the-art deterministic imputation methods. Furthermore, CSDI can also be applied to time series interpolation and probabilistic forecasting, and is competitive with existing baselines. The code is available at https://github.com/ermongroup/CSDI.",True,True,"Tashiro, Yusuke and Song, Jiaming and Song, Yang and Ermon, Stefano",2021.0,,,,Advances in neural information processing systems
Diffolio: A Diffusion Model for Multivariate Probabilistic Financial Time-Series Forecasting and Portfolio Construction,2511.07014v1,alcaraz2022diffusion,\cite{alcaraz2022diffusion},Diffusion-based time series imputation and forecasting with structured state space models,,,True,False,"Alcaraz, Juan Miguel Lopez and Strodthoff, Nils",2022.0,,,,arXiv preprint arXiv:2208.09399
Diffolio: A Diffusion Model for Multivariate Probabilistic Financial Time-Series Forecasting and Portfolio Construction,2511.07014v1,fan2024mg,\cite{fan2024mg},MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process,https://arxiv.org/abs/2403.05751v2,"Recently, diffusion probabilistic models have attracted attention in generative time series forecasting due to their remarkable capacity to generate high-fidelity samples. However, the effective utilization of their strong modeling ability in the probabilistic time series forecasting task remains an open question, partially due to the challenge of instability arising from their stochastic nature. To address this challenge, we introduce a novel Multi-Granularity Time Series Diffusion (MG-TSD) model, which achieves state-of-the-art predictive performance by leveraging the inherent granularity levels within the data as given targets at intermediate diffusion steps to guide the learning process of diffusion models. The way to construct the targets is motivated by the observation that the forward process of the diffusion model, which sequentially corrupts the data distribution to a standard normal distribution, intuitively aligns with the process of smoothing fine-grained data into a coarse-grained representation, both of which result in a gradual loss of fine distribution features. In the study, we derive a novel multi-granularity guidance diffusion loss function and propose a concise implementation method to effectively utilize coarse-grained data across various granularity levels. More importantly, our approach does not rely on additional external data, making it versatile and applicable across various domains. Extensive experiments conducted on real-world datasets demonstrate that our MG-TSD model outperforms existing time series prediction methods.",True,True,"Fan, Xinyao and Wu, Yueying and Xu, Chang and Huang, Yuhao and Liu, Weiqing and Bian, Jiang",2024.0,,,,arXiv preprint arXiv:2403.05751
Diffolio: A Diffusion Model for Multivariate Probabilistic Financial Time-Series Forecasting and Portfolio Construction,2511.07014v1,yuan2024diffusion,\cite{yuan2024diffusion},{Diffusion-ts: Interpretable diffusion for general time series generation},,,True,False,"Yuan, Xinyu and Qiao, Yan",2024.0,,,,arXiv preprint arXiv:2403.01742
Diffolio: A Diffusion Model for Multivariate Probabilistic Financial Time-Series Forecasting and Portfolio Construction,2511.07014v1,huang2024generative,\cite{huang2024generative},Generative learning for financial time series with irregular and scale-invariant patterns,,,True,False,"Huang, Hongbin and Chen, Minghua and Qiao, Xiao",2024.0,,,,
Diffolio: A Diffusion Model for Multivariate Probabilistic Financial Time-Series Forecasting and Portfolio Construction,2511.07014v1,takahashi2025generation,\cite{takahashi2025generation},Generation of synthetic financial time series by diffusion models,https://arxiv.org/abs/2410.18897v1,"Despite its practical significance, generating realistic synthetic financial time series is challenging due to statistical properties known as stylized facts, such as fat tails, volatility clustering, and seasonality patterns. Various generative models, including generative adversarial networks (GANs) and variational autoencoders (VAEs), have been employed to address this challenge, although no model yet satisfies all the stylized facts. We alternatively propose utilizing diffusion models, specifically denoising diffusion probabilistic models (DDPMs), to generate synthetic financial time series. This approach employs wavelet transformation to convert multiple time series (into images), such as stock prices, trading volumes, and spreads. Given these converted images, the model gains the ability to generate images that can be transformed back into realistic time series by inverse wavelet transformation. We demonstrate that our proposed approach satisfies stylized facts.",True,True,"Takahashi, Tomonori and Mizuno, Takayuki",2025.0,,,,Quantitative Finance
Diffolio: A Diffusion Model for Multivariate Probabilistic Financial Time-Series Forecasting and Portfolio Construction,2511.07014v1,welch2008comprehensive,\cite{welch2008comprehensive},A comprehensive look at the empirical performance of equity premium prediction,,,True,False,"Welch, Ivo and Goyal, Amit",2008.0,,,,The Review of Financial Studies
Diffolio: A Diffusion Model for Multivariate Probabilistic Financial Time-Series Forecasting and Portfolio Construction,2511.07014v1,gu2020empirical,\cite{gu2020empirical},Empirical asset pricing via machine learning,,,True,False,"Gu, Shihao and Kelly, Bryan and Xiu, Dacheng",2020.0,,,,The Review of Financial Studies
Diffolio: A Diffusion Model for Multivariate Probabilistic Financial Time-Series Forecasting and Portfolio Construction,2511.07014v1,cong2021alphaportfolio,\cite{cong2021alphaportfolio},AlphaPortfolio: Direct construction through deep reinforcement learning and interpretable AI,,,True,False,"Cong, Lin William and Tang, Ke and Wang, Jingyuan and Zhang, Yang",2021.0,,,,Available at SSRN 3554486
Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas,2511.02458v1,carriero_macroeconomic_2024,\cite{carriero_macroeconomic_2024},Macroeconomic {Forecasting} with {Large} {Language} {Models},,,True,False,"Carriero, Andrea and Pettenuzzo, Davide and Shekhar, Shubhranshu",2024.0,,https://openreview.net/forum?id=hNU5kFeo9r,,CoRR
Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas,2511.02458v1,bybee_surveying_2023,\cite{bybee_surveying_2023},Surveying Generative AI's Economic Expectations,https://arxiv.org/abs/2305.02823v2,"I introduce a survey of economic expectations formed by querying a large language model (LLM)'s expectations of various financial and macroeconomic variables based on a sample of news articles from the Wall Street Journal between 1984 and 2021. I find the resulting expectations closely match existing surveys including the Survey of Professional Forecasters (SPF), the American Association of Individual Investors, and the Duke CFO Survey. Importantly, I document that LLM based expectations match many of the deviations from full-information rational expectations exhibited in these existing survey series. The LLM's macroeconomic expectations exhibit under-reaction commonly found in consensus SPF forecasts. Additionally, its return expectations are extrapolative, disconnected from objective measures of expected returns, and negatively correlated with future realized returns. Finally, using a sample of articles outside of the LLM's training period I find that the correlation with existing survey measures persists -- indicating these results do not reflect memorization but generalization on the part of the LLM. My results provide evidence for the potential of LLMs to help us better understand human beliefs and navigate possible models of nonrational expectations.",True,True,"Bybee, Leland",2023.0,,http://arxiv.org/abs/2305.02823,10.48550/arXiv.2305.02823,
Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas,2511.02458v1,faria-e-castro_artificial_2024,\cite{faria-e-castro_artificial_2024},Artificial {Intelligence} and {Inflation} {Forecasts},,,True,False,"Faria-e-Castro, Miguel and Leibovici, Fernando",2024.0,,https://www.stlouisfed.org/publications/review/2024/nov/artificial-intelligence-and-inflation-forecasts,10.20955/r.2024.12,Review
Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas,2511.02458v1,lopez-lira_memorization_2025,\cite{lopez-lira_memorization_2025},The {Memorization} {Problem}: {Can} {We} {Trust} {LLMs}' {Economic} {Forecasts}?,,,True,False,"Lopez-Lira, Alejandro and Tang, Yuehua and Zhu, Mingyin",2025.0,,http://arxiv.org/abs/2504.14765,10.48550/arXiv.2504.14765,
Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas,2511.02458v1,tan2024language,\cite{tan2024language},Are language models actually useful for time series forecasting?,,,True,False,"Tan, Mingtian and Merrill, Mike and Gupta, Vinayak and Althoff, Tim and Hartvigsen, Tom",2024.0,,,,Advances in Neural Information Processing Systems
Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas,2511.02458v1,llm-econometric,\cite{llm-econometric},Large Language Models: An Applied Econometric Framework,https://arxiv.org/abs/2412.07031v2,"How can we use the novel capacities of large language models (LLMs) in empirical research? And how can we do so while accounting for their limitations, which are themselves only poorly understood? We develop an econometric framework to answer this question that distinguishes between two types of empirical tasks. Using LLMs for prediction problems (including hypothesis generation) is valid under one condition: no ``leakage'' between the LLM's training dataset and the researcher's sample. No leakage can be ensured by using open-source LLMs with documented training data and published weights. Using LLM outputs for estimation problems to automate the measurement of some economic concept (expressed either by some text or from human subjects) requires the researcher to collect at least some validation data: without such data, the errors of the LLM's automation cannot be assessed and accounted for. As long as these steps are taken, LLM outputs can be used in empirical research with the familiar econometric guarantees we desire. Using two illustrative applications to finance and political economy, we find that these requirements are stringent; when they are violated, the limitations of LLMs now result in unreliable empirical estimates. Our results suggest the excitement around the empirical uses of LLMs is warranted -- they allow researchers to effectively use even small amounts of language data for both prediction and estimation -- but only with these safeguards in place.",True,True,Jens Ludwig and Sendhil Mullainathan and Ashesh Rambachan,,,https://ideas.repec.org/p/nbr/nberwo/33344.html,,
Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas,2511.02458v1,korinek_language_2023,\cite{korinek_language_2023},Language {Models} and {Cognitive} {Automation} for {Economic} {Research},,,True,False,"Korinek, Anton",2023.0,,https://www.nber.org/papers/w30957,10.3386/w30957,
Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas,2511.02458v1,zarifhonarvar_evidence_2024,\cite{zarifhonarvar_evidence_2024},Evidence on {Inflation} {Expectations} {Formation} {Using} {Large} {Language} {Models},,,True,False,"Zarifhonarvar, Ali",2024.0,,https://papers.ssrn.com/abstract=4825076,10.2139/ssrn.4825076,
Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas,2511.02458v1,horton_large_2023,\cite{horton_large_2023},Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus?,https://arxiv.org/abs/2301.07543v1,"Newly-developed large language models (LLM) -- because of how they are trained and designed -- are implicit computational models of humans -- a homo silicus. These models can be used the same way economists use homo economicus: they can be given endowments, information, preferences, and so on and then their behavior can be explored in scenarios via simulation. I demonstrate this approach using OpenAI's GPT3 with experiments derived from Charness and Rabin (2002), Kahneman, Knetsch and Thaler (1986) and Samuelson and Zeckhauser (1988). The findings are qualitatively similar to the original results, but it is also trivially easy to try variations that offer fresh insights. Departing from the traditional laboratory paradigm, I also create a hiring scenario where an employer faces applicants that differ in experience and wage ask and then analyze how a minimum wage affects realized wages and the extent of labor-labor substitution.",True,True,"Horton, John J.",2023.0,,https://www.nber.org/papers/w31122,10.3386/w31122,
Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas,2511.02458v1,argyle2023out,\cite{argyle2023out},"Out of One, Many: Using Language Models to Simulate Human Samples",https://arxiv.org/abs/2209.06899v1,"We propose and explore the possibility that language models can be studied as effective proxies for specific human sub-populations in social science research. Practical and research applications of artificial intelligence tools have sometimes been limited by problematic biases (such as racism or sexism), which are often treated as uniform properties of the models. We show that the ""algorithmic bias"" within one such tool -- the GPT-3 language model -- is instead both fine-grained and demographically correlated, meaning that proper conditioning will cause it to accurately emulate response distributions from a wide variety of human subgroups. We term this property ""algorithmic fidelity"" and explore its extent in GPT-3. We create ""silicon samples"" by conditioning the model on thousands of socio-demographic backstories from real human participants in multiple large surveys conducted in the United States. We then compare the silicon and human samples to demonstrate that the information contained in GPT-3 goes far beyond surface similarity. It is nuanced, multifaceted, and reflects the complex interplay between ideas, attitudes, and socio-cultural context that characterize human attitudes. We suggest that language models with sufficient algorithmic fidelity thus constitute a novel and powerful tool to advance understanding of humans and society across a variety of disciplines.",True,True,"Argyle, Lisa P and Busby, Ethan C and Fulda, Nancy and Gubler, Joshua R and Rytting, Christopher and Wingate, David",2023.0,,,,Political Analysis
Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas,2511.02458v1,geng2024large,\cite{geng2024large},Are large language models chameleons? An attempt to simulate social surveys,,,True,False,"Geng, Mingmeng and He, Sihong and Trotta, Roberto",2024.0,,,,arXiv preprint arXiv:2405.19323
Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas,2511.02458v1,fell2024energy,\cite{fell2024energy},Energy social surveys replicated with Large Language Model agents,,,True,False,"Fell, Michael J",2024.0,,,,Available at SSRN 4686345
Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas,2511.02458v1,dominguez-olmedo_questioning_2024,\cite{dominguez-olmedo_questioning_2024},Questioning the {Survey} {Responses} of {Large} {Language} {Models},,,True,False,"Dominguez-Olmedo, Ricardo and Hardt, Moritz and Mendler-Dünner, Celestine",2024.0,,https://openreview.net/forum?id=Oo7dlLgqQX,,
Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas,2511.02458v1,hansen_simulating_2025,\cite{hansen_simulating_2025},Simulating the {Survey} of {Professional} {Forecasters},,,True,False,"Hansen, Anne Lundgaard and Horton, John J. and Kazinnik, Sophia and Puzzello, Daniela and Zarifhonarvar, Ali",2025.0,,https://www.ssrn.com/abstract=5066286,10.2139/ssrn.5066286,
Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas,2511.02458v1,sclarquantifying,\cite{sclarquantifying},Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting,https://arxiv.org/abs/2310.11324v2,"As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model. In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting. We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format. We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format. To facilitate systematic analysis we propose FormatSpread, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights. Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.",True,True,Melanie Sclar and Yejin Choi and Yulia Tsvetkov and Alane Suhr,2024.0,,https://openreview.net/forum?id=RIu5lyNXjT,,
Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas,2511.02458v1,kong-etal-2024-better,\cite{kong-etal-2024-better},Better Zero-Shot Reasoning with Role-Play Prompting,,,True,False,"Kong, Aobo  and
      Zhao, Shiwan  and
      Chen, Hao  and
      Li, Qicheng  and
      Qin, Yong  and
      Sun, Ruiqi  and
      Zhou, Xin  and
      Wang, Enzhi  and
      Dong, Xiaohang",2024.0,,https://aclanthology.org/2024.naacl-long.228/,10.18653/v1/2024.naacl-long.228,
Reimagining Agent-based Modeling with Large Language Model Agents via Shachi,2509.21862v2,gilbert2019agent,\cite{gilbert2019agent},Agent-based models,,,True,False,"Gilbert, Nigel",2019.0,,,,
Reimagining Agent-based Modeling with Large Language Model Agents via Shachi,2509.21862v2,davidsson2002agent,\cite{davidsson2002agent},Agent based social simulation: A computer science view,,,True,False,"Davidsson, Paul",2002.0,,,,Journal of artificial societies and social simulation
Reimagining Agent-based Modeling with Large Language Model Agents via Shachi,2509.21862v2,gilbert2000build,\cite{gilbert2000build},How to build and use agent-based models in social science,,,True,False,"Gilbert, Nigel and Terna, Pietro",2000.0,,,,Mind \& Society
Reimagining Agent-based Modeling with Large Language Model Agents via Shachi,2509.21862v2,tesfatsion2006agent,\cite{tesfatsion2006agent},Agent-based computational economics: A constructive approach to economic theory,,,True,False,"Tesfatsion, Leigh",2006.0,,,,Handbook of computational economics
Reimagining Agent-based Modeling with Large Language Model Agents via Shachi,2509.21862v2,tesfatsion2006handbook,\cite{tesfatsion2006handbook},Handbook of computational economics: agent-based computational economics,,,True,False,"Tesfatsion, Leigh and Judd, Kenneth L",2006.0,,,,
Reimagining Agent-based Modeling with Large Language Model Agents via Shachi,2509.21862v2,steinbacher2021advances,\cite{steinbacher2021advances},Advances in the agent-based modeling of economic and social behavior,,,True,False,"Steinbacher, Mitja and Raddant, Matthias and Karimi, Fariba and Camacho Cuena, Eva and Alfarano, Simone and Iori, Giulia and Lux, Thomas",2021.0,,,,SN Business \& Economics
Reimagining Agent-based Modeling with Large Language Model Agents via Shachi,2509.21862v2,gao2024large,\cite{gao2024large},Large language models empowered agent-based modeling and simulation: A survey and perspectives,,,True,False,"Gao, Chen and Lan, Xiaochong and Li, Nian and Yuan, Yuan and Ding, Jingtao and Zhou, Zhilun and Xu, Fengli and Li, Yong",2024.0,,,,Humanities and Social Sciences Communications
Reimagining Agent-based Modeling with Large Language Model Agents via Shachi,2509.21862v2,nisioti2024text,\cite{nisioti2024text},From text to life: On the reciprocal relationship between artificial life and large language models,,,True,False,"Nisioti, Eleni and Glanois, Claire and Najarro, Elias and Dai, Andrew and Meyerson, Elliot and Pedersen, Joachim Winther and Teodorescu, Laetitia and Hayes, Conor F and Sudhakaran, Shyam and Risi, Sebastian",2024.0,,,,
Reimagining Agent-based Modeling with Large Language Model Agents via Shachi,2509.21862v2,huang2023humanity,\cite{huang2023humanity},On the humanity of conversational ai: Evaluating the psychological portrayal of llms,,,True,False,"Huang, Jen-tse and Wang, Wenxuan and Li, Eric John and Lam, Man Ho and Ren, Shujie and Yuan, Youliang and Jiao, Wenxiang and Tu, Zhaopeng and Lyu, Michael",2023.0,,,,
Reimagining Agent-based Modeling with Large Language Model Agents via Shachi,2509.21862v2,park2023generative,\cite{park2023generative},Generative Agents: Interactive Simulacra of Human Behavior,https://arxiv.org/abs/2304.03442v2,"Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.",True,True,"Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S",2023.0,,,,
Reimagining Agent-based Modeling with Large Language Model Agents via Shachi,2509.21862v2,yang2024oasis,\cite{yang2024oasis},OASIS: Open Agent Social Interaction Simulations with One Million Agents,https://arxiv.org/abs/2411.11581v5,"There has been a growing interest in enhancing rule-based agent-based models (ABMs) for social media platforms (i.e., X, Reddit) with more realistic large language model (LLM) agents, thereby allowing for a more nuanced study of complex systems. As a result, several LLM-based ABMs have been proposed in the past year. While they hold promise, each simulator is specifically designed to study a particular scenario, making it time-consuming and resource-intensive to explore other phenomena using the same ABM. Additionally, these models simulate only a limited number of agents, whereas real-world social media platforms involve millions of users. To this end, we propose OASIS, a generalizable and scalable social media simulator. OASIS is designed based on real-world social media platforms, incorporating dynamically updated environments (i.e., dynamic social networks and post information), diverse action spaces (i.e., following, commenting), and recommendation systems (i.e., interest-based and hot-score-based). Additionally, OASIS supports large-scale user simulations, capable of modeling up to one million users. With these features, OASIS can be easily extended to different social media platforms to study large-scale group phenomena and behaviors. We replicate various social phenomena, including information spreading, group polarization, and herd effects across X and Reddit platforms. Moreover, we provide observations of social phenomena at different agent group scales. We observe that the larger agent group scale leads to more enhanced group dynamics and more diverse and helpful agents' opinions. These findings demonstrate OASIS's potential as a powerful tool for studying complex systems in digital environments.",True,True,"Yang, Ziyi and Zhang, Zaibin and Zheng, Zirui and Jiang, Yuxian and Gan, Ziyue and Wang, Zhiyu and Ling, Zijian and Chen, Jinsong and Ma, Martz and Dong, Bowen and others",2024.0,,,,arXiv preprint arXiv:2411.11581
Reimagining Agent-based Modeling with Large Language Model Agents via Shachi,2509.21862v2,zhou2023sotopia,\cite{zhou2023sotopia},Sotopia: Interactive evaluation for social intelligence in language agents,,,True,False,"Zhou, Xuhui and Zhu, Hao and Mathur, Leena and Zhang, Ruohong and Yu, Haofei and Qi, Zhengyang and Morency, Louis-Philippe and Bisk, Yonatan and Fried, Daniel and Neubig, Graham and others",2024.0,,,,International Conference on Learning Representations
Reimagining Agent-based Modeling with Large Language Model Agents via Shachi,2509.21862v2,li2023econagent,\cite{li2023econagent},EconAgent: Large Language Model-Empowered Agents for Simulating Macroeconomic Activities,https://arxiv.org/abs/2310.10436v4,"The advent of artificial intelligence has led to a growing emphasis on data-driven modeling in macroeconomics, with agent-based modeling (ABM) emerging as a prominent bottom-up simulation paradigm. In ABM, agents (e.g., households, firms) interact within a macroeconomic environment, collectively generating market dynamics. Existing agent modeling typically employs predetermined rules or learning-based neural networks for decision-making. However, customizing each agent presents significant challenges, complicating the modeling of agent heterogeneity. Additionally, the influence of multi-period market dynamics and multifaceted macroeconomic factors are often overlooked in decision-making processes. In this work, we introduce EconAgent, a large language model-empowered agent with human-like characteristics for macroeconomic simulation. We first construct a simulation environment that incorporates various market dynamics driven by agents' decisions regarding work and consumption. Through the perception module, we create heterogeneous agents with distinct decision-making mechanisms. Furthermore, we model the impact of macroeconomic trends using a memory module, which allows agents to reflect on past individual experiences and market dynamics. Simulation experiments show that EconAgent can make realistic decisions, leading to more reasonable macroeconomic phenomena compared to existing rule-based or learning-based agents. Our codes are released at https://github.com/tsinghua-fib-lab/ACL24-EconAgent.",True,True,"Li, Nian and Gao, Chen and Li, Mingyu and Li, Yong and Liao, Qingmin",2024.0,,,,Annual Meeting of the Association for Computational Linguistics
Reimagining Agent-based Modeling with Large Language Model Agents via Shachi,2509.21862v2,zhang2024ai,\cite{zhang2024ai},When AI Meets Finance (StockAgent): Large Language Model-based Stock Trading in Simulated Real-world Environments,https://arxiv.org/abs/2407.18957v4,"Can AI Agents simulate real-world trading environments to investigate the impact of external factors on stock trading activities (e.g., macroeconomics, policy changes, company fundamentals, and global events)? These factors, which frequently influence trading behaviors, are critical elements in the quest for maximizing investors' profits. Our work attempts to solve this problem through large language model based agents. We have developed a multi-agent AI system called StockAgent, driven by LLMs, designed to simulate investors' trading behaviors in response to the real stock market. The StockAgent allows users to evaluate the impact of different external factors on investor trading and to analyze trading behavior and profitability effects. Additionally, StockAgent avoids the test set leakage issue present in existing trading simulation systems based on AI Agents. Specifically, it prevents the model from leveraging prior knowledge it may have acquired related to the test data. We evaluate different LLMs under the framework of StockAgent in a stock trading environment that closely resembles real-world conditions. The experimental results demonstrate the impact of key external factors on stock market trading, including trading behavior and stock price fluctuation rules. This research explores the study of agents' free trading gaps in the context of no prior knowledge related to market data. The patterns identified through StockAgent simulations provide valuable insights for LLM-based investment advice and stock recommendation. The code is available at https://github.com/MingyuJ666/Stockagent.",True,True,"Zhang, Chong and Liu, Xinyi and Zhang, Zhongmou and Jin, Mingyu and Li, Lingyao and Wang, Zhenting and Hua, Wenyue and Shu, Dong and Zhu, Suiyuan and Jin, Xiaobo and others",2024.0,,,,arXiv preprint arXiv:2407.18957
Reimagining Agent-based Modeling with Large Language Model Agents via Shachi,2509.21862v2,chen2023put,\cite{chen2023put},Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena,https://arxiv.org/abs/2310.05746v4,"Recent advancements in Large Language Models (LLMs) showcase advanced reasoning, yet NLP evaluations often depend on static benchmarks. Evaluating this necessitates environments that test strategic reasoning in dynamic, competitive scenarios requiring long-term planning. We introduce AucArena, a novel evaluation suite that simulates auctions, a setting chosen for being highly unpredictable and involving many skills related to resource and risk management, while also being easy to evaluate. We conduct controlled experiments using state-of-the-art LLMs to power bidding agents to benchmark their planning and execution skills. Our research demonstrates that LLMs, such as GPT-4, possess key skills for auction participation, such as budget management and goal adherence, which improve with adaptive strategies. This highlights LLMs' potential in modeling complex social interactions in competitive contexts. However, variability in LLM performance and occasional outperformance by simpler methods indicate opportunities for further advancements in LLM design and the value of our simulation environment for ongoing testing and refinement.",True,True,"Chen, Jiangjie and Yuan, Siyu and Ye, Rong and Majumder, Bodhisattwa Prasad and Richardson, Kyle",2023.0,,,,arXiv preprint arXiv:2310.05746
Reimagining Agent-based Modeling with Large Language Model Agents via Shachi,2509.21862v2,wu2024autogen,\cite{wu2024autogen},AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation,https://arxiv.org/abs/2308.08155v2,"AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.",True,True,"Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Li, Beibin and Zhu, Erkang and Jiang, Li and Zhang, Xiaoyun and Zhang, Shaokun and Liu, Jiale and others",2024.0,,,,
Reimagining Agent-based Modeling with Large Language Model Agents via Shachi,2509.21862v2,vezhnevets2023generative,\cite{vezhnevets2023generative},"Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia",,,True,False,"Vezhnevets, Alexander Sasha and Agapiou, John P and Aharon, Avia and Ziv, Ron and Matyas, Jayd and Du{\'e}{\~n}ez-Guzm{\'a}n, Edgar A and Cunningham, William A and Osindero, Simon and Karmon, Danny and Leibo, Joel Z",2023.0,,,,arXiv preprint arXiv:2312.03664
Reimagining Agent-based Modeling with Large Language Model Agents via Shachi,2509.21862v2,expectedparrot_edsl_2023,\cite{expectedparrot_edsl_2023},{EDSL: The Expected Document-Symbol Language},,,True,False,{Expected Parrot},2023.0,,,,
Reimagining Agent-based Modeling with Large Language Model Agents via Shachi,2509.21862v2,qiang2025mledojointeractiveenvironmentsempowering,\cite{qiang2025mledojointeractiveenvironmentsempowering},MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine Learning Engineering,,,True,False,Rushi Qiang and Yuchen Zhuang and Yinghao Li and Dingu Sagar V K and Rongzhi Zhang and Changhao Li and Ian Shu-Hei Wong and Sherry Yang and Percy Liang and Chao Zhang and Bo Dai,2025.0,,https://arxiv.org/abs/2505.07782,,
Cross-Validated Causal Inference: a Modern Method to Combine Experimental and Observational Data,2511.00727v1,wolpert1992stacked,\cite{wolpert1992stacked},Issues in Stacked Generalization,https://arxiv.org/abs/1105.5466v1,"Stacked generalization is a general method of using a    high-level model to combine lower-level models to achieve greater    predictive accuracy.  In this paper we address two crucial issues    which have been considered to be a `black art' in classification tasks    ever since the introduction of stacked generalization in 1992 by    Wolpert: the type of generalizer that is suitable to derive the    higher-level model, and the kind of attributes that should be used as    its input.  We find that best results are obtained when the    higher-level model combines the confidence (and not just the    predictions) of the lower-level ones.   We demonstrate the effectiveness of stacked generalization for combining     three different types of learning algorithms for classification tasks.    We also compare the performance of stacked generalization with    majority vote and published results of arcing and bagging.",True,True,"Wolpert, David H",1992.0,,,,Neural Networks
Cross-Validated Causal Inference: a Modern Method to Combine Experimental and Observational Data,2511.00727v1,breiman1996stacked,\cite{breiman1996stacked},Stacked regressions,,,True,False,"Breiman, Leo",1996.0,,,,Machine Learning
Cross-Validated Causal Inference: a Modern Method to Combine Experimental and Observational Data,2511.00727v1,tsybakov2003optimal,\cite{tsybakov2003optimal},Optimal rates of aggregation,,,True,False,"Tsybakov, Alexandre B",2003.0,,,,
Cross-Validated Causal Inference: a Modern Method to Combine Experimental and Observational Data,2511.00727v1,tsybakov2004optimal,\cite{tsybakov2004optimal},Optimal aggregation of classifiers in statistical learning,,,True,False,"Tsybakov, Alexander B",2004.0,,,,The Annals of Statistics
Cross-Validated Causal Inference: a Modern Method to Combine Experimental and Observational Data,2511.00727v1,van2007super,\cite{van2007super},Super learner,,,True,False,"Van der Laan, Mark J and Polley, Eric C and Hubbard, Alan E",2007.0,,,,Statistical Applications in Genetics and Molecular Biology
Cross-Validated Causal Inference: a Modern Method to Combine Experimental and Observational Data,2511.00727v1,kallus2018removing,\cite{kallus2018removing},Removing Hidden Confounding by Experimental Grounding,https://arxiv.org/abs/1810.11646v1,"Observational data is increasingly used as a means for making individual-level causal predictions and intervention recommendations. The foremost challenge of causal inference from observational data is hidden confounding, whose presence cannot be tested in data and can invalidate any causal conclusion. Experimental data does not suffer from confounding but is usually limited in both scope and scale. We introduce a novel method of using limited experimental data to correct the hidden confounding in causal effect models trained on larger observational data, even if the observational data does not fully overlap with the experimental data. Our method makes strictly weaker assumptions than existing approaches, and we prove conditions under which it yields a consistent estimator. We demonstrate our method's efficacy using real-world data from a large educational experiment.",True,True,"Kallus, Nathan and Puli, Aahlad Manas and Shalit, Uri",2018.0,,,,Advances in Neural Information Processing Systems
Cross-Validated Causal Inference: a Modern Method to Combine Experimental and Observational Data,2511.00727v1,yang2020combining,\cite{yang2020combining},Combining multiple observational data sources to estimate causal effects,https://arxiv.org/abs/1801.00802v4,"The era of big data has witnessed an increasing availability of multiple data sources for statistical analyses. We consider estimation of causal effects combining big main data with unmeasured confounders and smaller validation data with supplementary information on these confounders. Under the unconfoundedness assumption with completely observed confounders, the smaller validation data allow for constructing consistent estimators for causal effects, but the big main data can only give error-prone estimators in general. However, by leveraging the information in the big main data in a principled way, we can improve the estimation efficiencies yet preserve the consistencies of the initial estimators based solely on the validation data. Our framework applies to asymptotically normal estimators, including the commonly-used regression imputation, weighting, and matching estimators, and does not require a correct specification of the model relating the unmeasured confounders to the observed variables. We also propose appropriate bootstrap procedures, which makes our method straightforward to implement using software routines for existing estimators.",True,True,"Yang, Shu and Ding, Peng",2020.0,,,,Journal of the American Statistical Association
Cross-Validated Causal Inference: a Modern Method to Combine Experimental and Observational Data,2511.00727v1,rosenman2023combining,\cite{rosenman2023combining},Combining observational and experimental datasets using shrinkage estimators,,,True,False,"Rosenman, Evan TR and Basse, Guillaume and Owen, Art B and Baiocchi, Mike",2023.0,,,,Biometrics
Cross-Validated Causal Inference: a Modern Method to Combine Experimental and Observational Data,2511.00727v1,gui2024combining,\cite{gui2024combining},Combining observational and experimental data to improve efficiency using imperfect instruments,,,True,False,"Gui, George Z",2024.0,,,,Marketing Science
Cross-Validated Causal Inference: a Modern Method to Combine Experimental and Observational Data,2511.00727v1,yang2023elastic,\cite{yang2023elastic},Elastic integrative analysis of randomised trial and real-world data for treatment heterogeneity estimation,,,True,False,"Yang, Shu and Gao, Chenyin and Zeng, Donglin and Wang, Xiaofei",2023.0,,,,Journal of the Royal Statistical Society Series B: Statistical Methodology
Cross-Validated Causal Inference: a Modern Method to Combine Experimental and Observational Data,2511.00727v1,robins1994estimation,\cite{robins1994estimation},Estimation of regression coefficients when some regressors are not always observed,,,True,False,"Robins, James M and Rotnitzky, Andrea and Zhao, Lue Ping",1994.0,,,,Journal of the American Statistical Association
Cross-Validated Causal Inference: a Modern Method to Combine Experimental and Observational Data,2511.00727v1,ross2009pooled,\cite{ross2009pooled},Pooled analysis of rofecoxib placebo-controlled clinical trial data: Lessons for postmarket pharmaceutical safety surveillance,,,True,False,"Ross, Joseph S and Madigan, David and Hill, Kevin P and Egilman, David S and Wang, Yongfei and Krumholz, Harlan M",2009.0,,,,Archives of Internal Medicine
Cross-Validated Causal Inference: a Modern Method to Combine Experimental and Observational Data,2511.00727v1,gao2023pretest,\cite{gao2023pretest},Pretest estimation in combining probability and non-probability samples,,,True,False,"Gao, Chenyin and Yang, Shu",2023.0,,,,Electronic Journal of Statistics
Cross-Validated Causal Inference: a Modern Method to Combine Experimental and Observational Data,2511.00727v1,xiong2023federated,\cite{xiong2023federated},Federated Causal Inference in Heterogeneous Observational Data,https://arxiv.org/abs/2107.11732v5,"We are interested in estimating the effect of a treatment applied to individuals at multiple sites, where data is stored locally for each site. Due to privacy constraints, individual-level data cannot be shared across sites; the sites may also have heterogeneous populations and treatment assignment mechanisms. Motivated by these considerations, we develop federated methods to draw inference on the average treatment effects of combined data across sites. Our methods first compute summary statistics locally using propensity scores and then aggregate these statistics across sites to obtain point and variance estimators of average treatment effects. We show that these estimators are consistent and asymptotically normal. To achieve these asymptotic properties, we find that the aggregation schemes need to account for the heterogeneity in treatment assignments and in outcomes across sites. We demonstrate the validity of our federated methods through a comparative study of two large medical claims databases.",True,True,"Xiong, Ruoxuan and Koenecke, Allison and Powell, Michael and Shen, Zhu and Vogelstein, Joshua T and Athey, Susan",2023.0,,,,Statistics in Medicine
Cross-Validated Causal Inference: a Modern Method to Combine Experimental and Observational Data,2511.00727v1,stein1956inadmissibility,\cite{stein1956inadmissibility},Inadmissibility of the usual estimator for the mean of a multivariate normal distribution,,,True,False,"Stein, Charles and others",1956.0,,,,
Cross-Validated Causal Inference: a Modern Method to Combine Experimental and Observational Data,2511.00727v1,green1991james,\cite{green1991james},A James-Stein type estimator for combining unbiased and possibly biased estimators,,,True,False,"Green, Edwin J and Strawderman, William E",1991.0,,,,Journal of the American Statistical Association
Cross-Validated Causal Inference: a Modern Method to Combine Experimental and Observational Data,2511.00727v1,green2005improved,\cite{green2005improved},Improved estimation for multiple means with heterogeneous variances,,,True,False,"Green, Edwin J and Strawderman, William E and Amateis, Ralph L and Reams, Gregory A",2005.0,,,,Forest Science
Efficiently Learning Synthetic Control Models for High-dimensional Disaggregated Data,2510.22828v1,dube2015pooling,\cite{dube2015pooling},Pooling multiple case studies using synthetic controls: An application to minimum wage policies,,,True,False,"Dube, Arindrajit and Zipperer, Ben",2015.0,,,,
Efficiently Learning Synthetic Control Models for High-dimensional Disaggregated Data,2510.22828v1,robbins2017framework,\cite{robbins2017framework},"A framework for synthetic control methods with high-dimensional, micro-level data: evaluating a neighborhood-specific crime intervention",,,True,False,"Robbins, Michael W and Saunders, Jessica and Kilmer, Beau",2017.0,,,,Journal of the American Statistical Association
Efficiently Learning Synthetic Control Models for High-dimensional Disaggregated Data,2510.22828v1,hazlett2018trajectory,\cite{hazlett2018trajectory},Trajectory balancing: A general reweighting approach to causal inference with time-series cross-sectional data,,,True,False,"Hazlett, Chad and Xu, Yiqing",2018.0,,,,Available at SSRN 3214231
Efficiently Learning Synthetic Control Models for High-dimensional Disaggregated Data,2510.22828v1,abadie2021synthetic,\cite{abadie2021synthetic},Synthetic controls for experimental design,,,True,False,"Abadie, Alberto and Zhao, Jinglong",2021.0,,,,arXiv preprint arXiv:2108.02196
Efficiently Learning Synthetic Control Models for High-dimensional Disaggregated Data,2510.22828v1,hollingsworth2020tactics,\cite{hollingsworth2020tactics},Tactics for design and inference in synthetic control studies: An applied example using high-dimensional data,,,True,False,"Hollingsworth, Alex and Wing, Coady",2020.0,,,,Available at SSRN 3592088
Efficiently Learning Synthetic Control Models for High-dimensional Disaggregated Data,2510.22828v1,abadie2021penalized,\cite{abadie2021penalized},A penalized synthetic control estimator for disaggregated data,,,True,False,"Abadie, Alberto and L’Hour, J{\'e}r{\'e}my",2021.0,,,,Journal of the American Statistical Association
Efficiently Learning Synthetic Control Models for High-dimensional Disaggregated Data,2510.22828v1,turlach2005simultaneous,\cite{turlach2005simultaneous},Simultaneous variable selection,,,True,False,"Turlach, Berwin A and Venables, William N and Wright, Stephen J",2005.0,,,,Technometrics
Efficiently Learning Synthetic Control Models for High-dimensional Disaggregated Data,2510.22828v1,yuan2007dimension,\cite{yuan2007dimension},Dimension reduction and coefficient estimation in multivariate linear regression,,,True,False,"Yuan, Ming and Ekici, Ali and Lu, Zhaosong and Monteiro, Renato",2007.0,,,,Journal of the Royal Statistical Society: Series B (Statistical Methodology)
Efficiently Learning Synthetic Control Models for High-dimensional Disaggregated Data,2510.22828v1,obozinski2011support,\cite{obozinski2011support},Support union recovery in high-dimensional multivariate regression,https://arxiv.org/abs/0808.0711v2,"In multivariate regression, a $K$-dimensional response vector is regressed upon a common set of $p$ covariates, with a matrix $B^*\in\mathbb{R}^{p\times K}$ of regression coefficients. We study the behavior of the multivariate group Lasso, in which block regularization based on the $\ell_1/\ell_2$ norm is used for support union recovery, or recovery of the set of $s$ rows for which $B^*$ is nonzero. Under high-dimensional scaling, we show that the multivariate group Lasso exhibits a threshold for the recovery of the exact row pattern with high probability over the random design and noise that is specified by the sample complexity parameter $θ(n,p,s):=n/[2ψ(B^*)\log(p-s)]$. Here $n$ is the sample size, and $ψ(B^*)$ is a sparsity-overlap function measuring a combination of the sparsities and overlaps of the $K$-regression coefficient vectors that constitute the model. We prove that the multivariate group Lasso succeeds for problem sequences $(n,p,s)$ such that $θ(n,p,s)$ exceeds a critical level $θ_u$, and fails for sequences such that $θ(n,p,s)$ lies below a critical level $θ_{\ell}$. For the special case of the standard Gaussian ensemble, we show that $θ_{\ell}=θ_u$ so that the characterization is sharp. The sparsity-overlap function $ψ(B^*)$ reveals that, if the design is uncorrelated on the active rows, $\ell_1/\ell_2$ regularization for multivariate regression never harms performance relative to an ordinary Lasso approach and can yield substantial improvements in sample complexity (up to a factor of $K$) when the coefficient vectors are suitably orthogonal. For more general designs, it is possible for the ordinary Lasso to outperform the multivariate group Lasso. We complement our analysis with simulations that demonstrate the sharpness of our theoretical results, even for relatively small problems.",True,True,"Obozinski, Guillaume and Wainwright, Martin J and Jordan, Michael I",2011.0,,,,The Annals of Statistics
Efficiently Learning Synthetic Control Models for High-dimensional Disaggregated Data,2510.22828v1,negahban2011estimation,\cite{negahban2011estimation},Estimation of (near) low-rank matrices with noise and high-dimensional scaling,https://arxiv.org/abs/0912.5100v1,"High-dimensional inference refers to problems of statistical estimation in which the ambient dimension of the data may be comparable to or possibly even larger than the sample size. We study an instance of high-dimensional inference in which the goal is to estimate a matrix $Θ^* \in \real^{k \times p}$ on the basis of $N$ noisy observations, and the unknown matrix $Θ^*$ is assumed to be either exactly low rank, or ``near'' low-rank, meaning that it can be well-approximated by a matrix with low rank. We consider an $M$-estimator based on regularization by the trace or nuclear norm over matrices, and analyze its performance under high-dimensional scaling. We provide non-asymptotic bounds on the Frobenius norm error that hold for a general class of noisy observation models, and then illustrate their consequences for a number of specific matrix models, including low-rank multivariate or multi-task regression, system identification in vector autoregressive processes, and recovery of low-rank matrices from random projections. Simulation results show excellent agreement with the high-dimensional scaling of the error predicted by our theory.",True,True,"Negahban, Sahand and Wainwright, Martin J",2011.0,,,,The Annals of Statistics
Efficiently Learning Synthetic Control Models for High-dimensional Disaggregated Data,2510.22828v1,rothman2010sparse,\cite{rothman2010sparse},Sparse multivariate regression with covariance estimation,,,True,False,"Rothman, Adam J and Levina, Elizaveta and Zhu, Ji",2010.0,,,,Journal of Computational and Graphical Statistics
Efficiently Learning Synthetic Control Models for High-dimensional Disaggregated Data,2510.22828v1,niu2019simultaneous,\cite{niu2019simultaneous},Simultaneous estimation and inference for multiple response variables,,,True,False,"Niu, Xiaomeng and Cho, Hyunkeun Ryan",2019.0,,,,Communications in Statistics-Theory and Methods
Efficiently Learning Synthetic Control Models for High-dimensional Disaggregated Data,2510.22828v1,chang2022robust,\cite{chang2022robust},Robust multivariate lasso regression with covariance estimation,,,True,False,"Chang, Le and Welsh, AH",2022.0,,,,Journal of Computational and Graphical Statistics
Efficiently Learning Synthetic Control Models for High-dimensional Disaggregated Data,2510.22828v1,molstad2021explicit,\cite{molstad2021explicit},An explicit mean-covariance parameterization for multivariate response linear regression,,,True,False,"Molstad, Aaron J and Weng, Guangwei and Doss, Charles R and Rothman, Adam J",2021.0,,,,Journal of Computational and Graphical Statistics
Efficiently Learning Synthetic Control Models for High-dimensional Disaggregated Data,2510.22828v1,molstad2021new,\cite{molstad2021new},New insights for the multivariate square-root lasso,,,True,False,"Molstad, Aaron J",2021.0,,,,
The AI Productivity Index (APEX),2509.25721v2,mialon2023gaiabenchmarkgeneralai,\cite{mialon2023gaiabenchmarkgeneralai},GAIA: a benchmark for General AI Assistants,,,True,False,Grégoire Mialon and Clémentine Fourrier and Craig Swift and Thomas Wolf and Yann LeCun and Thomas Scialom,2023.0,,https://arxiv.org/abs/2311.12983,,
The AI Productivity Index (APEX),2509.25721v2,hendrycks2021measuringmassivemultitasklanguage,\cite{hendrycks2021measuringmassivemultitasklanguage},Measuring Massive Multitask Language Understanding,https://arxiv.org/abs/2009.03300v3,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",True,True,Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt,2021.0,,https://arxiv.org/abs/2009.03300,,
The AI Productivity Index (APEX),2509.25721v2,gema2025mmlu,\cite{gema2025mmlu},Are We Done with MMLU?,,,True,False,Aryo Pradipta Gema and Joshua Ong Jun Leang and Giwon Hong and Alessio Devoto and Alberto Carlo Maria Mancino and Rohit Saxena and Xuanli He and Yu Zhao and Xiaotang Du and Mohammad Reza Ghasemi Madani and Claire Barale and Robert McHardy and Joshua Harris and Jean Kaddour and Emile van Krieken and Pasquale Minervini,2025.0,,https://arxiv.org/abs/2406.04127,,
The AI Productivity Index (APEX),2509.25721v2,wang2024mmluprorobustchallengingmultitask,\cite{wang2024mmluprorobustchallengingmultitask},MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark,,,True,False,Yubo Wang and Xueguang Ma and Ge Zhang and Yuansheng Ni and Abhranil Chandra and Shiguang Guo and Weiming Ren and Aaran Arulraj and Xuan He and Ziyan Jiang and Tianle Li and Max Ku and Kai Wang and Alex Zhuang and Rongqi Fan and Xiang Yue and Wenhu Chen,2024.0,,https://arxiv.org/abs/2406.01574,,
The AI Productivity Index (APEX),2509.25721v2,rein2023gpqagraduatelevelgoogleproofqa,\cite{rein2023gpqagraduatelevelgoogleproofqa},GPQA: A Graduate-Level Google-Proof Q&A Benchmark,https://arxiv.org/abs/2311.12022v1,"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.",True,True,David Rein and Betty Li Hou and Asa Cooper Stickland and Jackson Petty and Richard Yuanzhe Pang and Julien Dirani and Julian Michael and Samuel R. Bowman,2023.0,,https://arxiv.org/abs/2311.12022,,
The AI Productivity Index (APEX),2509.25721v2,phan2025humanitysexam,\cite{phan2025humanitysexam},Humanity's Last Exam,https://arxiv.org/abs/2501.14249v9,"Benchmarks are important tools for tracking the rapid advancements in large language model (LLM) capabilities. However, benchmarks are not keeping pace in difficulty: LLMs now achieve over 90\% accuracy on popular benchmarks like MMLU, limiting informed measurement of state-of-the-art LLM capabilities. In response, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at the frontier of human knowledge, designed to be the final closed-ended academic benchmark of its kind with broad subject coverage. HLE consists of 2,500 questions across dozens of subjects, including mathematics, humanities, and the natural sciences. HLE is developed globally by subject-matter experts and consists of multiple-choice and short-answer questions suitable for automated grading. Each question has a known solution that is unambiguous and easily verifiable, but cannot be quickly answered via internet retrieval. State-of-the-art LLMs demonstrate low accuracy and calibration on HLE, highlighting a significant gap between current LLM capabilities and the expert human frontier on closed-ended academic questions. To inform research and policymaking upon a clear understanding of model capabilities, we publicly release HLE at https://lastexam.ai.",True,True,Long Phan and Alice Gatti and Ziwen Han and Nathaniel Li and Josephina Hu and Hugh Zhang and Chen Bo Calvin Zhang and Mohamed Shaaban and John Ling and Sean Shi and Michael Choi and Anish Agrawal and Arnav Chopra and Adam Khoja and Ryan Kim and Richard Ren and Jason Hausenloy and Oliver Zhang and Mantas Mazeika and Dmitry Dodonov and Tung Nguyen and Jaeho Lee and Daron Anderson and Mikhail Doroshenko and Alun Cennyth Stokes and Mobeen Mahmood and Oleksandr Pokutnyi and Oleg Iskra and Jessica P. Wang and John-Clark Levin and Mstyslav Kazakov and Fiona Feng and Steven Y. Feng and Haoran Zhao and Michael Yu and Varun Gangal and Chelsea Zou and Zihan Wang and Serguei Popov and Robert Gerbicz and Geoff Galgon and Johannes Schmitt and Will Yeadon and Yongki Lee and Scott Sauers and Alvaro Sanchez and Fabian Giska and Marc Roth and Søren Riis and Saiteja Utpala and Noah Burns and Gashaw M. Goshu and Mohinder Maheshbhai Naiya and Chidozie Agu and Zachary Giboney and Antrell Cheatom and Francesco Fournier-Facio and Sarah-Jane Crowson and Lennart Finke and Zerui Cheng and Jennifer Zampese and Ryan G. Hoerr and Mark Nandor and Hyunwoo Park and Tim Gehrunger and Jiaqi Cai and Ben McCarty and Alexis C Garretson and Edwin Taylor and Damien Sileo and Qiuyu Ren and Usman Qazi and Lianghui Li and Jungbae Nam and John B. Wydallis and Pavel Arkhipov and Jack Wei Lun Shi and Aras Bacho and Chris G. Willcocks and Hangrui Cao and Sumeet Motwani and Emily de Oliveira Santos and Johannes Veith and Edward Vendrow and Doru Cojoc and Kengo Zenitani and Joshua Robinson and Longke Tang and Yuqi Li and Joshua Vendrow and Natanael Wildner Fraga and Vladyslav Kuchkin and Andrey Pupasov Maksimov and Pierre Marion and Denis Efremov and Jayson Lynch and Kaiqu Liang and Aleksandar Mikov and Andrew Gritsevskiy and Julien Guillod and Gözdenur Demir and Dakotah Martinez and Ben Pageler and Kevin Zhou and Saeed Soori and Ori Press and Henry Tang and Paolo Rissone and Sean R. Green and Lina Brüssel and Moon Twayana and Aymeric Dieuleveut and Joseph Marvin Imperial and Ameya Prabhu and Jinzhou Yang and Nick Crispino and Arun Rao and Dimitri Zvonkine and Gabriel Loiseau and Mikhail Kalinin and Marco Lukas and Ciprian Manolescu and Nate Stambaugh and Subrata Mishra and Tad Hogg and Carlo Bosio and Brian P Coppola and Julian Salazar and Jaehyeok Jin and Rafael Sayous and Stefan Ivanov and Philippe Schwaller and Shaipranesh Senthilkuma and Andres M Bran and Andres Algaba and Kelsey Van den Houte and Lynn Van Der Sypt and Brecht Verbeken and David Noever and Alexei Kopylov and Benjamin Myklebust and Bikun Li and Lisa Schut and Evgenii Zheltonozhskii and Qiaochu Yuan and Derek Lim and Richard Stanley and Tong Yang and John Maar and Julian Wykowski and Martí Oller and Anmol Sahu and Cesare Giulio Ardito and Yuzheng Hu and Ariel Ghislain Kemogne Kamdoum and Alvin Jin and Tobias Garcia Vilchis and Yuexuan Zu and Martin Lackner and James Koppel and Gongbo Sun and Daniil S. Antonenko and Steffi Chern and Bingchen Zhao and Pierrot Arsene and Joseph M Cavanagh and Daofeng Li and Jiawei Shen and Donato Crisostomi and Wenjin Zhang and Ali Dehghan and Sergey Ivanov and David Perrella and Nurdin Kaparov and Allen Zang and Ilia Sucholutsky and Arina Kharlamova and Daniil Orel and Vladislav Poritski and Shalev Ben-David and Zachary Berger and Parker Whitfill and Michael Foster and Daniel Munro and Linh Ho and Shankar Sivarajan and Dan Bar Hava and Aleksey Kuchkin and David Holmes and Alexandra Rodriguez-Romero and Frank Sommerhage and Anji Zhang and Richard Moat and Keith Schneider and Zakayo Kazibwe and Don Clarke and Dae Hyun Kim and Felipe Meneguitti Dias and Sara Fish and Veit Elser and Tobias Kreiman and Victor Efren Guadarrama Vilchis and Immo Klose and Ujjwala Anantheswaran and Adam Zweiger and Kaivalya Rawal and Jeffery Li and Jeremy Nguyen and Nicolas Daans and Haline Heidinger and Maksim Radionov and Václav Rozhoň and Vincent Ginis and Christian Stump and Niv Cohen and Rafał Poświata and Josef Tkadlec and Alan Goldfarb and Chenguang Wang and Piotr Padlewski and Stanislaw Barzowski and Kyle Montgomery and Ryan Stendall and Jamie Tucker-Foltz and Jack Stade and T. Ryan Rogers and Tom Goertzen and Declan Grabb and Abhishek Shukla and Alan Givré and John Arnold Ambay and Archan Sen and Muhammad Fayez Aziz and Mark H Inlow and Hao He and Ling Zhang and Younesse Kaddar and Ivar Ängquist and Yanxu Chen and Harrison K Wang and Kalyan Ramakrishnan and Elliott Thornley and Antonio Terpin and Hailey Schoelkopf and Eric Zheng and Avishy Carmi and Ethan D. L. Brown and Kelin Zhu and Max Bartolo and Richard Wheeler and Martin Stehberger and Peter Bradshaw and JP Heimonen and Kaustubh Sridhar and Ido Akov and Jennifer Sandlin and Yury Makarychev and Joanna Tam and Hieu Hoang and David M. Cunningham and Vladimir Goryachev and Demosthenes Patramanis and Michael Krause and Andrew Redenti and David Aldous and Jesyin Lai and Shannon Coleman and Jiangnan Xu and Sangwon Lee and Ilias Magoulas and Sandy Zhao and Ning Tang and Michael K. Cohen and Orr Paradise and Jan Hendrik Kirchner and Maksym Ovchynnikov and Jason O. Matos and Adithya Shenoy and Michael Wang and Yuzhou Nie and Anna Sztyber-Betley and Paolo Faraboschi and Robin Riblet and Jonathan Crozier and Shiv Halasyamani and Shreyas Verma and Prashant Joshi and Eli Meril and Ziqiao Ma and Jérémy Andréoletti and Raghav Singhal and Jacob Platnick and Volodymyr Nevirkovets and Luke Basler and Alexander Ivanov and Seri Khoury and Nils Gustafsson and Marco Piccardo and Hamid Mostaghimi and Qijia Chen and Virendra Singh and Tran Quoc Khánh and Paul Rosu and Hannah Szlyk and Zachary Brown and Himanshu Narayan and Aline Menezes and Jonathan Roberts and William Alley and Kunyang Sun and Arkil Patel and Max Lamparth and Anka Reuel and Linwei Xin and Hanmeng Xu and Jacob Loader and Freddie Martin and Zixuan Wang and Andrea Achilleos and Thomas Preu and Tomek Korbak and Ida Bosio and Fereshteh Kazemi and Ziye Chen and Biró Bálint and Eve J. Y. Lo and Jiaqi Wang and Maria Inês S. Nunes and Jeremiah Milbauer and M Saiful Bari and Zihao Wang and Behzad Ansarinejad and Yewen Sun and Stephane Durand and Hossam Elgnainy and Guillaume Douville and Daniel Tordera and George Balabanian and Hew Wolff and Lynna Kvistad and Hsiaoyun Milliron and Ahmad Sakor and Murat Eron and Andrew Favre D. O. and Shailesh Shah and Xiaoxiang Zhou and Firuz Kamalov and Sherwin Abdoli and Tim Santens and Shaul Barkan and Allison Tee and Robin Zhang and Alessandro Tomasiello and G. Bruno De Luca and Shi-Zhuo Looi and Vinh-Kha Le and Noam Kolt and Jiayi Pan and Emma Rodman and Jacob Drori and Carl J Fossum and Niklas Muennighoff and Milind Jagota and Ronak Pradeep and Honglu Fan and Jonathan Eicher and Michael Chen and Kushal Thaman and William Merrill and Moritz Firsching and Carter Harris and Stefan Ciobâcă and Jason Gross and Rohan Pandey and Ilya Gusev and Adam Jones and Shashank Agnihotri and Pavel Zhelnov and Mohammadreza Mofayezi and Alexander Piperski and David K. Zhang and Kostiantyn Dobarskyi and Roman Leventov and Ignat Soroko and Joshua Duersch and Vage Taamazyan and Andrew Ho and Wenjie Ma and William Held and Ruicheng Xian and Armel Randy Zebaze and Mohanad Mohamed and Julian Noah Leser and Michelle X Yuan and Laila Yacar and Johannes Lengler and Katarzyna Olszewska and Claudio Di Fratta and Edson Oliveira and Joseph W. Jackson and Andy Zou and Muthu Chidambaram and Timothy Manik and Hector Haffenden and Dashiell Stander and Ali Dasouqi and Alexander Shen and Bita Golshani and David Stap and Egor Kretov and Mikalai Uzhou and Alina Borisovna Zhidkovskaya and Nick Winter and Miguel Orbegozo Rodriguez and Robert Lauff and Dustin Wehr and Colin Tang and Zaki Hossain and Shaun Phillips and Fortuna Samuele and Fredrik Ekström and Angela Hammon and Oam Patel and Faraz Farhidi and George Medley and Forough Mohammadzadeh and Madellene Peñaflor and Haile Kassahun and Alena Friedrich and Rayner Hernandez Perez and Daniel Pyda and Taom Sakal and Omkar Dhamane and Ali Khajegili Mirabadi and Eric Hallman and Kenchi Okutsu and Mike Battaglia and Mohammad Maghsoudimehrabani and Alon Amit and Dave Hulbert and Roberto Pereira and Simon Weber and Handoko and Anton Peristyy and Stephen Malina and Mustafa Mehkary and Rami Aly and Frank Reidegeld and Anna-Katharina Dick and Cary Friday and Mukhwinder Singh and Hassan Shapourian and Wanyoung Kim and Mariana Costa and Hubeyb Gurdogan and Harsh Kumar and Chiara Ceconello and Chao Zhuang and Haon Park and Micah Carroll and Andrew R. Tawfeek and Stefan Steinerberger and Daattavya Aggarwal and Michael Kirchhof and Linjie Dai and Evan Kim and Johan Ferret and Jainam Shah and Yuzhou Wang and Minghao Yan and Krzysztof Burdzy and Lixin Zhang and Antonio Franca and Diana T. Pham and Kang Yong Loh and Joshua Robinson and Abram Jackson and Paolo Giordano and Philipp Petersen and Adrian Cosma and Jesus Colino and Colin White and Jacob Votava and Vladimir Vinnikov and Ethan Delaney and Petr Spelda and Vit Stritecky and Syed M. Shahid and Jean-Christophe Mourrat and Lavr Vetoshkin and Koen Sponselee and Renas Bacho and Zheng-Xin Yong and Florencia de la Rosa and Nathan Cho and Xiuyu Li and Guillaume Malod and Orion Weller and Guglielmo Albani and Leon Lang and Julien Laurendeau and Dmitry Kazakov and Fatimah Adesanya and Julien Portier and Lawrence Hollom and Victor Souza and Yuchen Anna Zhou and Julien Degorre and Yiğit Yalın and Gbenga Daniel Obikoya and Rai and Filippo Bigi and M. C. Boscá and Oleg Shumar and Kaniuar Bacho and Gabriel Recchia and Mara Popescu and Nikita Shulga and Ngefor Mildred Tanwie and Thomas C. H. Lux and Ben Rank and Colin Ni and Matthew Brooks and Alesia Yakimchyk and Huanxu and Liu and Stefano Cavalleri and Olle Häggström and Emil Verkama and Joshua Newbould and Hans Gundlach and Leonor Brito-Santana and Brian Amaro and Vivek Vajipey and Rynaa Grover and Ting Wang and Yosi Kratish and Wen-Ding Li and Sivakanth Gopi and Andrea Caciolai and Christian Schroeder de Witt and Pablo Hernández-Cámara and Emanuele Rodolà and Jules Robins and Dominic Williamson and Vincent Cheng and Brad Raynor and Hao Qi and Ben Segev and Jingxuan Fan and Sarah Martinson and Erik Y. Wang and Kaylie Hausknecht and Michael P. Brenner and Mao Mao and Christoph Demian and Peyman Kassani and Xinyu Zhang and David Avagian and Eshawn Jessica Scipio and Alon Ragoler and Justin Tan and Blake Sims and Rebeka Plecnik and Aaron Kirtland and Omer Faruk Bodur and D. P. Shinde and Yan Carlos Leyva Labrador and Zahra Adoul and Mohamed Zekry and Ali Karakoc and Tania C. B. Santos and Samir Shamseldeen and Loukmane Karim and Anna Liakhovitskaia and Nate Resman and Nicholas Farina and Juan Carlos Gonzalez and Gabe Maayan and Earth Anderson and Rodrigo De Oliveira Pena and Elizabeth Kelley and Hodjat Mariji and Rasoul Pouriamanesh and Wentao Wu and Ross Finocchio and Ismail Alarab and Joshua Cole and Danyelle Ferreira and Bryan Johnson and Mohammad Safdari and Liangti Dai and Siriphan Arthornthurasuk and Isaac C. McAlister and Alejandro José Moyano and Alexey Pronin and Jing Fan and Angel Ramirez-Trinidad and Yana Malysheva and Daphiny Pottmaier and Omid Taheri and Stanley Stepanic and Samuel Perry and Luke Askew and Raúl Adrián Huerta Rodríguez and Ali M. R. Minissi and Ricardo Lorena and Krishnamurthy Iyer and Arshad Anil Fasiludeen and Ronald Clark and Josh Ducey and Matheus Piza and Maja Somrak and Eric Vergo and Juehang Qin and Benjámin Borbás and Eric Chu and Jack Lindsey and Antoine Jallon and I. M. J. McInnis and Evan Chen and Avi Semler and Luk Gloor and Tej Shah and Marc Carauleanu and Pascal Lauer and Tran Đuc Huy and Hossein Shahrtash and Emilien Duc and Lukas Lewark and Assaf Brown and Samuel Albanie and Brian Weber and Warren S. Vaz and Pierre Clavier and Yiyang Fan and Gabriel Poesia Reis e Silva and Long and Lian and Marcus Abramovitch and Xi Jiang and Sandra Mendoza and Murat Islam and Juan Gonzalez and Vasilios Mavroudis and Justin Xu and Pawan Kumar and Laxman Prasad Goswami and Daniel Bugas and Nasser Heydari and Ferenc Jeanplong and Thorben Jansen and Antonella Pinto and Archimedes Apronti and Abdallah Galal and Ng Ze-An and Ankit Singh and Tong Jiang and Joan of Arc Xavier and Kanu Priya Agarwal and Mohammed Berkani and Gang Zhang and Zhehang Du and Benedito Alves de Oliveira Junior and Dmitry Malishev and Nicolas Remy and Taylor D. Hartman and Tim Tarver and Stephen Mensah and Gautier Abou Loume and Wiktor Morak and Farzad Habibi and Sarah Hoback and Will Cai and Javier Gimenez and Roselynn Grace Montecillo and Jakub Łucki and Russell Campbell and Asankhaya Sharma and Khalida Meer and Shreen Gul and Daniel Espinosa Gonzalez and Xavier Alapont and Alex Hoover and Gunjan Chhablani and Freddie Vargus and Arunim Agarwal and Yibo Jiang and Deepakkumar Patil and David Outevsky and Kevin Joseph Scaria and Rajat Maheshwari and Abdelkader Dendane and Priti Shukla and Ashley Cartwright and Sergei Bogdanov and Niels Mündler and Sören Möller and Luca Arnaboldi and Kunvar Thaman and Muhammad Rehan Siddiqi and Prajvi Saxena and Himanshu Gupta and Tony Fruhauff and Glen Sherman and Mátyás Vincze and Siranut Usawasutsakorn and Dylan Ler and Anil Radhakrishnan and Innocent Enyekwe and Sk Md Salauddin and Jiang Muzhen and Aleksandr Maksapetyan and Vivien Rossbach and Chris Harjadi and Mohsen Bahaloohoreh and Claire Sparrow and Jasdeep Sidhu and Sam Ali and Song Bian and John Lai and Eric Singer and Justine Leon Uro and Greg Bateman and Mohamed Sayed and Ahmed Menshawy and Darling Duclosel and Dario Bezzi and Yashaswini Jain and Ashley Aaron and Murat Tiryakioglu and Sheeshram Siddh and Keith Krenek and Imad Ali Shah and Jun Jin and Scott Creighton and Denis Peskoff and Zienab EL-Wasif and Ragavendran P V and Michael Richmond and Joseph McGowan and Tejal Patwardhan and Hao-Yu Sun and Ting Sun and Nikola Zubić and Samuele Sala and Stephen Ebert and Jean Kaddour and Manuel Schottdorf and Dianzhuo Wang and Gerol Petruzella and Alex Meiburg and Tilen Medved and Ali ElSheikh and S Ashwin Hebbar and Lorenzo Vaquero and Xianjun Yang and Jason Poulos and Vilém Zouhar and Sergey Bogdanik and Mingfang Zhang and Jorge Sanz-Ros and David Anugraha and Yinwei Dai and Anh N. Nhu and Xue Wang and Ali Anil Demircali and Zhibai Jia and Yuyin Zhou and Juncheng Wu and Mike He and Nitin Chandok and Aarush Sinha and Gaoxiang Luo and Long Le and Mickaël Noyé and Michał Perełkiewicz and Ioannis Pantidis and Tianbo Qi and Soham Sachin Purohit and Letitia Parcalabescu and Thai-Hoa Nguyen and Genta Indra Winata and Edoardo M. Ponti and Hanchen Li and Kaustubh Dhole and Jongee Park and Dario Abbondanza and Yuanli Wang and Anupam Nayak and Diogo M. Caetano and Antonio A. W. L. Wong and Maria del Rio-Chanona and Dániel Kondor and Pieter Francois and Ed Chalstrey and Jakob Zsambok and Dan Hoyer and Jenny Reddish and Jakob Hauser and Francisco-Javier Rodrigo-Ginés and Suchandra Datta and Maxwell Shepherd and Thom Kamphuis and Qizheng Zhang and Hyunjun Kim and Ruiji Sun and Jianzhu Yao and Franck Dernoncourt and Satyapriya Krishna and Sina Rismanchian and Bonan Pu and Francesco Pinto and Yingheng Wang and Kumar Shridhar and Kalon J. Overholt and Glib Briia and Hieu Nguyen and David and Soler Bartomeu and Tony CY Pang and Adam Wecker and Yifan Xiong and Fanfei Li and Lukas S. Huber and Joshua Jaeger and Romano De Maddalena and Xing Han Lù and Yuhui Zhang and Claas Beger and Patrick Tser Jern Kon and Sean Li and Vivek Sanker and Ming Yin and Yihao Liang and Xinlu Zhang and Ankit Agrawal and Li S. Yifei and Zechen Zhang and Mu Cai and Yasin Sonmez and Costin Cozianu and Changhao Li and Alex Slen and Shoubin Yu and Hyun Kyu Park and Gabriele Sarti and Marcin Briański and Alessandro Stolfo and Truong An Nguyen and Mike Zhang and Yotam Perlitz and Jose Hernandez-Orallo and Runjia Li and Amin Shabani and Felix Juefei-Xu and Shikhar Dhingra and Orr Zohar and My Chiffon Nguyen and Alexander Pondaven and Abdurrahim Yilmaz and Xuandong Zhao and Chuanyang Jin and Muyan Jiang and Stefan Todoran and Xinyao Han and Jules Kreuer and Brian Rabern and Anna Plassart and Martino Maggetti and Luther Yap and Robert Geirhos and Jonathon Kean and Dingsu Wang and Sina Mollaei and Chenkai Sun and Yifan Yin and Shiqi Wang and Rui Li and Yaowen Chang and Anjiang Wei and Alice Bizeul and Xiaohan Wang and Alexandre Oliveira Arrais and Kushin Mukherjee and Jorge Chamorro-Padial and Jiachen Liu and Xingyu Qu and Junyi Guan and Adam Bouyamourn and Shuyu Wu and Martyna Plomecka and Junda Chen and Mengze Tang and Jiaqi Deng and Shreyas Subramanian and Haocheng Xi and Haoxuan Chen and Weizhi Zhang and Yinuo Ren and Haoqin Tu and Sejong Kim and Yushun Chen and Sara Vera Marjanović and Junwoo Ha and Grzegorz Luczyna and Jeff J. Ma and Zewen Shen and Dawn Song and Cedegao E. Zhang and Zhun Wang and Gaël Gendron and Yunze Xiao and Leo Smucker and Erica Weng and Kwok Hao Lee and Zhe Ye and Stefano Ermon and Ignacio D. Lopez-Miguel and Theo Knights and Anthony Gitter and Namkyu Park and Boyi Wei and Hongzheng Chen and Kunal Pai and Ahmed Elkhanany and Han Lin and Philipp D. Siedler and Jichao Fang and Ritwik Mishra and Károly Zsolnai-Fehér and Xilin Jiang and Shadab Khan and Jun Yuan and Rishab Kumar Jain and Xi Lin and Mike Peterson and Zhe Wang and Aditya Malusare and Maosen Tang and Isha Gupta and Ivan Fosin and Timothy Kang and Barbara Dworakowska and Kazuki Matsumoto and Guangyao Zheng and Gerben Sewuster and Jorge Pretel Villanueva and Ivan Rannev and Igor Chernyavsky and Jiale Chen and Deepayan Banik and Ben Racz and Wenchao Dong and Jianxin Wang and Laila Bashmal and Duarte V. Gonçalves and Wei Hu and Kaushik Bar and Ondrej Bohdal and Atharv Singh Patlan and Shehzaad Dhuliawala and Caroline Geirhos and Julien Wist and Yuval Kansal and Bingsen Chen and Kutay Tire and Atak Talay Yücel and Brandon Christof and Veerupaksh Singla and Zijian Song and Sanxing Chen and Jiaxin Ge and Kaustubh Ponkshe and Isaac Park and Tianneng Shi and Martin Q. Ma and Joshua Mak and Sherwin Lai and Antoine Moulin and Zhuo Cheng and Zhanda Zhu and Ziyi Zhang and Vaidehi Patil and Ketan Jha and Qiutong Men and Jiaxuan Wu and Tianchi Zhang and Bruno Hebling Vieira and Alham Fikri Aji and Jae-Won Chung and Mohammed Mahfoud and Ha Thi Hoang and Marc Sperzel and Wei Hao and Kristof Meding and Sihan Xu and Vassilis Kostakos and Davide Manini and Yueying Liu and Christopher Toukmaji and Jay Paek and Eunmi Yu and Arif Engin Demircali and Zhiyi Sun and Ivan Dewerpe and Hongsen Qin and Roman Pflugfelder and James Bailey and Johnathan Morris and Ville Heilala and Sybille Rosset and Zishun Yu and Peter E. Chen and Woongyeong Yeo and Eeshaan Jain and Ryan Yang and Sreekar Chigurupati and Julia Chernyavsky and Sai Prajwal Reddy and Subhashini Venugopalan and Hunar Batra and Core Francisco Park and Hieu Tran and Guilherme Maximiano and Genghan Zhang and Yizhuo Liang and Hu Shiyu and Rongwu Xu and Rui Pan and Siddharth Suresh and Ziqi Liu and Samaksh Gulati and Songyang Zhang and Peter Turchin and Christopher W. Bartlett and Christopher R. Scotese and Phuong M. Cao and Aakaash Nattanmai and Gordon McKellips and Anish Cheraku and Asim Suhail and Ethan Luo and Marvin Deng and Jason Luo and Ashley Zhang and Kavin Jindel and Jay Paek and Kasper Halevy and Allen Baranov and Michael Liu and Advaith Avadhanam and David Zhang and Vincent Cheng and Brad Ma and Evan Fu and Liam Do and Joshua Lass and Hubert Yang and Surya Sunkari and Vishruth Bharath and Violet Ai and James Leung and Rishit Agrawal and Alan Zhou and Kevin Chen and Tejas Kalpathi and Ziqi Xu and Gavin Wang and Tyler Xiao and Erik Maung and Sam Lee and Ryan Yang and Roy Yue and Ben Zhao and Julia Yoon and Sunny Sun and Aryan Singh and Ethan Luo and Clark Peng and Tyler Osbey and Taozhi Wang and Daryl Echeazu and Hubert Yang and Timothy Wu and Spandan Patel and Vidhi Kulkarni and Vijaykaarti Sundarapandiyan and Ashley Zhang and Andrew Le and Zafir Nasim and Srikar Yalam and Ritesh Kasamsetty and Soham Samal and Hubert Yang and David Sun and Nihar Shah and Abhijeet Saha and Alex Zhang and Leon Nguyen and Laasya Nagumalli and Kaixin Wang and Alan Zhou and Aidan Wu and Jason Luo and Anwith Telluri and Summer Yue and Alexandr Wang and Dan Hendrycks,2025.0,,https://arxiv.org/abs/2501.14249,,
The AI Productivity Index (APEX),2509.25721v2,cobbe2021trainingverifierssolvemath,\cite{cobbe2021trainingverifierssolvemath},Training Verifiers to Solve Math Word Problems,https://arxiv.org/abs/2110.14168v2,"State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",True,True,Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman,2021.0,,https://arxiv.org/abs/2110.14168,,
The AI Productivity Index (APEX),2509.25721v2,islam2023financebenchnewbenchmarkfinancial,\cite{islam2023financebenchnewbenchmarkfinancial},FinanceBench: A New Benchmark for Financial Question Answering,,,True,False,Pranab Islam and Anand Kannappan and Douwe Kiela and Rebecca Qian and Nino Scherrer and Bertie Vidgen,2023.0,,https://arxiv.org/abs/2311.11944,,
The AI Productivity Index (APEX),2509.25721v2,jin2019pubmedqadatasetbiomedicalresearch,\cite{jin2019pubmedqadatasetbiomedicalresearch},PubMedQA: A Dataset for Biomedical Research Question Answering,,,True,False,Qiao Jin and Bhuwan Dhingra and Zhengping Liu and William W. Cohen and Xinghua Lu,2019.0,,https://arxiv.org/abs/1909.06146,,
The AI Productivity Index (APEX),2509.25721v2,lu2022learnexplainmultimodalreasoning,\cite{lu2022learnexplainmultimodalreasoning},Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering,https://arxiv.org/abs/2209.09513v2,"When answering a question, humans utilize the information available across different modalities to synthesize a consistent and complete chain of thought (CoT). This process is normally a black box in the case of deep learning models like large-scale language models. Recently, science question benchmarks have been used to diagnose the multi-hop reasoning ability and interpretability of an AI system. However, existing datasets fail to provide annotations for the answers, or are restricted to the textual-only modality, small scales, and limited domain diversity. To this end, we present Science Question Answering (ScienceQA), a new benchmark that consists of ~21k multimodal multiple choice questions with a diverse set of science topics and annotations of their answers with corresponding lectures and explanations. We further design language models to learn to generate lectures and explanations as the chain of thought (CoT) to mimic the multi-hop reasoning process when answering ScienceQA questions. ScienceQA demonstrates the utility of CoT in language models, as CoT improves the question answering performance by 1.20% in few-shot GPT-3 and 3.99% in fine-tuned UnifiedQA. We also explore the upper bound for models to leverage explanations by feeding those in the input; we observe that it improves the few-shot performance of GPT-3 by 18.96%. Our analysis further shows that language models, similar to humans, benefit from explanations to learn from fewer data and achieve the same performance with just 40% of the data. The data and code are available at https://scienceqa.github.io.",True,True,Pan Lu and Swaroop Mishra and Tony Xia and Liang Qiu and Kai-Wei Chang and Song-Chun Zhu and Oyvind Tafjord and Peter Clark and Ashwin Kalyan,2022.0,,https://arxiv.org/abs/2209.09513,,
The AI Productivity Index (APEX),2509.25721v2,clark2018thinksolvedquestionanswering,\cite{clark2018thinksolvedquestionanswering},"Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge",https://arxiv.org/abs/1803.05457v1,"We present a new question set, text corpus, and baselines assembled to encourage AI research in advanced question answering. Together, these constitute the AI2 Reasoning Challenge (ARC), which requires far more powerful knowledge and reasoning than previous challenges such as SQuAD or SNLI. The ARC question set is partitioned into a Challenge Set and an Easy Set, where the Challenge Set contains only questions answered incorrectly by both a retrieval-based algorithm and a word co-occurence algorithm. The dataset contains only natural, grade-school science questions (authored for human tests), and is the largest public-domain set of this kind (7,787 questions). We test several baselines on the Challenge Set, including leading neural models from the SQuAD and SNLI tasks, and find that none are able to significantly outperform a random baseline, reflecting the difficult nature of this task. We are also releasing the ARC Corpus, a corpus of 14M science sentences relevant to the task, and implementations of the three neural baseline models tested. Can your model perform better? We pose ARC as a challenge to the community.",True,True,Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord,2018.0,,https://arxiv.org/abs/1803.05457,,
The AI Productivity Index (APEX),2509.25721v2,chollet2025arcagi2newchallengefrontier,\cite{chollet2025arcagi2newchallengefrontier},ARC-AGI-2: A New Challenge for Frontier AI Reasoning Systems,,,True,False,Francois Chollet and Mike Knoop and Gregory Kamradt and Bryan Landers and Henry Pinkard,2025.0,,https://arxiv.org/abs/2505.11831,,
The AI Productivity Index (APEX),2509.25721v2,openai_gdpval,\cite{openai_gdpval},GDPval: Evaluating AI’s Economic Value,,,True,False,OpenAI,2025.0,,https://cdn.openai.com/pdf/d5eb7428-c4e9-4a33-bd86-86dd4bcf12ce/GDPval.pdf,,
The AI Productivity Index (APEX),2509.25721v2,handa2025economictasksperformedai,\cite{handa2025economictasksperformedai},Which Economic Tasks are Performed with AI? Evidence from Millions of Claude Conversations,,,True,False,Kunal Handa and Alex Tamkin and Miles McCain and Saffron Huang and Esin Durmus and Sarah Heck and Jared Mueller and Jerry Hong and Stuart Ritchie and Tim Belonax and Kevin K. Troy and Dario Amodei and Jared Kaplan and Jack Clark and Deep Ganguli,2025.0,,https://arxiv.org/abs/2503.04761,,
The AI Productivity Index (APEX),2509.25721v2,backlund2025vendingbenchbenchmarklongtermcoherence,\cite{backlund2025vendingbenchbenchmarklongtermcoherence},Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents,,,True,False,Axel Backlund and Lukas Petersson,2025.0,,https://arxiv.org/abs/2502.15840,,
The AI Productivity Index (APEX),2509.25721v2,xu2025theagentcompanybenchmarkingllmagents,\cite{xu2025theagentcompanybenchmarkingllmagents},TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks,https://arxiv.org/abs/2412.14161v3,"We interact with computers on an everyday basis, be it in everyday life or work, and many aspects of work can be done entirely with access to a computer and the Internet. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments. But how performant are AI agents at accelerating or even autonomously performing work-related tasks? The answer to this question has important implications both for industry looking to adopt AI into their workflows and for economic policy to understand the effects that adoption of AI may have on the labor market. To measure the progress of these LLM agents' performance on performing real-world professional tasks, in this paper we introduce TheAgentCompany, an extensible benchmark for evaluating AI agents that interact with the world in similar ways to those of a digital worker: by browsing the Web, writing code, running programs, and communicating with other coworkers. We build a self-contained environment with internal web sites and data that mimics a small software company environment, and create a variety of tasks that may be performed by workers in such a company. We test baseline agents powered by both closed API-based and open-weights language models (LMs), and find that the most competitive agent can complete 30% of tasks autonomously. This paints a nuanced picture on task automation with LM agents--in a setting simulating a real workplace, a good portion of simpler tasks could be solved autonomously, but more difficult long-horizon tasks are still beyond the reach of current systems. We release code, data, environment, and experiments on https://the-agent-company.com.",True,True,Frank F. Xu and Yufan Song and Boxuan Li and Yuxuan Tang and Kritanjali Jain and Mengxue Bao and Zora Z. Wang and Xuhui Zhou and Zhitong Guo and Murong Cao and Mingyang Yang and Hao Yang Lu and Amaad Martin and Zhe Su and Leander Maben and Raj Mehta and Wayne Chi and Lawrence Jang and Yiqing Xie and Shuyan Zhou and Graham Neubig,2025.0,,https://arxiv.org/abs/2412.14161,,
The AI Productivity Index (APEX),2509.25721v2,HENDRIX2022331,\cite{HENDRIX2022331},Assessing the Economic Value of Clinical Artificial Intelligence: Challenges and Opportunities,,,True,False,Nathaniel Hendrix and David L. Veenstra and Mindy Cheng and Nicholas C. Anderson and Stéphane Verguet,2022.0,,https://www.sciencedirect.com/science/article/pii/S1098301521017435,https://doi.org/10.1016/j.jval.2021.08.015,Value in Health
The AI Productivity Index (APEX),2509.25721v2,becker2025measuringimpactearly2025ai,\cite{becker2025measuringimpactearly2025ai},Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity,,,True,False,Joel Becker and Nate Rush and Elizabeth Barnes and David Rein,2025.0,,https://arxiv.org/abs/2507.09089,,
The AI Productivity Index (APEX),2509.25721v2,peng2023impactaideveloperproductivity,\cite{peng2023impactaideveloperproductivity},The Impact of AI on Developer Productivity: Evidence from GitHub Copilot,,,True,False,Sida Peng and Eirini Kalliamvakou and Peter Cihon and Mert Demirer,2023.0,,https://arxiv.org/abs/2302.06590,,
The AI Productivity Index (APEX),2509.25721v2,paradis2024doesaiimpactdevelopment,\cite{paradis2024doesaiimpactdevelopment},How much does AI impact development speed? An enterprise-based randomized controlled trial,https://arxiv.org/abs/2410.12944v3,"How much does AI assistance impact developer productivity? To date, the software engineering literature has provided a range of answers, targeting a diversity of outcomes: from perceived productivity to speed on task and developer throughput. Our randomized controlled trial with 96 full-time Google software engineers contributes to this literature by sharing an estimate of the impact of three AI features on the time developers spent on a complex, enterprise-grade task. We found that AI significantly shortened the time developers spent on task. Our best estimate of the size of this effect, controlling for factors known to influence developer time on task, stands at about 21\%, although our confidence interval is large. We also found an interesting effect whereby developers who spend more hours on code-related activities per day were faster with AI. Product and future research considerations are discussed. In particular, we invite further research that explores the impact of AI at the ecosystem level and across multiple suites of AI-enhanced tools, since we cannot assume that the effect size obtained in our lab study will necessarily apply more broadly, or that the effect of AI found using internal Google tooling in the summer of 2024 will translate across tools and over time.",True,True,Elise Paradis and Kate Grey and Quinn Madison and Daye Nam and Andrew Macvean and Vahid Meimand and Nan Zhang and Ben Ferrari-Church and Satish Chandra,2024.0,,https://arxiv.org/abs/2410.12944,,
The AI Productivity Index (APEX),2509.25721v2,miserendino2025swelancerfrontierllmsearn,\cite{miserendino2025swelancerfrontierllmsearn},SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?,https://arxiv.org/abs/2502.12115v4,"We introduce SWE-Lancer, a benchmark of over 1,400 freelance software engineering tasks from Upwork, valued at \$1 million USD total in real-world payouts. SWE-Lancer encompasses both independent engineering tasks--ranging from \$50 bug fixes to \$32,000 feature implementations--and managerial tasks, where models choose between technical implementation proposals. Independent tasks are graded with end-to-end tests triple-verified by experienced software engineers, while managerial decisions are assessed against the choices of the original hired engineering managers. We evaluate model performance and find that frontier models are still unable to solve the majority of tasks. To facilitate future research, we open-source a unified Docker image and a public evaluation split, SWE-Lancer Diamond (https://github.com/openai/SWELancer-Benchmark). By mapping model performance to monetary value, we hope SWE-Lancer enables greater research into the economic impact of AI model development.",True,True,Samuel Miserendino and Michele Wang and Tejal Patwardhan and Johannes Heidecke,2025.0,,https://arxiv.org/abs/2502.12115,,
Fine-Tuning Vision-Language Models for Multimodal Polymer Property Prediction,2511.05577v1,radford2021learning,\cite{radford2021learning},Learning Transferable Visual Models From Natural Language Supervision,https://arxiv.org/abs/2103.00020v1,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",True,True,"Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",2021.0,,,,
Fine-Tuning Vision-Language Models for Multimodal Polymer Property Prediction,2511.05577v1,OpenAI2023GPT4TR,\cite{OpenAI2023GPT4TR},GPT-4 Technical Report,https://arxiv.org/abs/2303.08774v6,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",True,True,OpenAI,2023.0,,https://api.semanticscholar.org/CorpusID:257532815,,ArXiv
Fine-Tuning Vision-Language Models for Multimodal Polymer Property Prediction,2511.05577v1,team2023gemini,\cite{team2023gemini},Gemini: A Family of Highly Capable Multimodal Models,https://arxiv.org/abs/2312.11805v5,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI.",True,True,"Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others",2023.0,,,,arXiv preprint arXiv:2312.11805
Fine-Tuning Vision-Language Models for Multimodal Polymer Property Prediction,2511.05577v1,alayrac2022flamingo,\cite{alayrac2022flamingo},Flamingo: a visual language model for few-shot learning,,,True,False,"Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others",2022.0,,,,Advances in Neural Information Processing Systems
Fine-Tuning Vision-Language Models for Multimodal Polymer Property Prediction,2511.05577v1,awadalla2023openflamingo,\cite{awadalla2023openflamingo},Openflamingo: An open-source framework for training large autoregressive vision-language models,,,True,False,"Awadalla, Anas and Gao, Irena and Gardner, Josh and Hessel, Jack and Hanafy, Yusuf and Zhu, Wanrong and Marathe, Kalyani and Bitton, Yonatan and Gadre, Samir and Sagawa, Shiori and others",2023.0,,,,arXiv preprint arXiv:2308.01390
Fine-Tuning Vision-Language Models for Multimodal Polymer Property Prediction,2511.05577v1,meta2024llama,\cite{meta2024llama},"Llama 3.2: Revolutionizing edge ai and vision with open, customizable models",,,True,False,"Meta, AI",2024.0,,,,Meta AI Blog. Retrieved December
Fine-Tuning Vision-Language Models for Multimodal Polymer Property Prediction,2511.05577v1,liu2023visual,\cite{liu2023visual},Visual instruction tuning,,,True,False,"Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",2023.0,,,,arXiv preprint arXiv:2304.08485
Fine-Tuning Vision-Language Models for Multimodal Polymer Property Prediction,2511.05577v1,li2023llava,\cite{li2023llava},LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day,https://arxiv.org/abs/2306.00890v1,"Conversational generative AI has demonstrated remarkable promise for empowering biomedical practitioners, but current investigations focus on unimodal text. Multimodal conversational AI has seen rapid progress by leveraging billions of image-text pairs from the public web, but such general-domain vision-language models still lack sophistication in understanding and conversing about biomedical images. In this paper, we propose a cost-efficient approach for training a vision-language conversational assistant that can answer open-ended research questions of biomedical images. The key idea is to leverage a large-scale, broad-coverage biomedical figure-caption dataset extracted from PubMed Central, use GPT-4 to self-instruct open-ended instruction-following data from the captions, and then fine-tune a large general-domain vision-language model using a novel curriculum learning method. Specifically, the model first learns to align biomedical vocabulary using the figure-caption pairs as is, then learns to master open-ended conversational semantics using GPT-4 generated instruction-following data, broadly mimicking how a layperson gradually acquires biomedical knowledge. This enables us to train a Large Language and Vision Assistant for BioMedicine (LLaVA-Med) in less than 15 hours (with eight A100s). LLaVA-Med exhibits excellent multimodal conversational capability and can follow open-ended instruction to assist with inquiries about a biomedical image. On three standard biomedical visual question answering datasets, LLaVA-Med outperforms previous supervised state-of-the-art on certain metrics. To facilitate biomedical multimodal research, we will release our instruction-following data and the LLaVA-Med model.",True,True,"Li, Chunyuan and Wong, Cliff and Zhang, Sheng and Usuyama, Naoto and Liu, Haotian and Yang, Jianwei and Naumann, Tristan and Poon, Hoifung and Gao, Jianfeng",2023.0,,,,arXiv preprint arXiv:2306.00890
Fine-Tuning Vision-Language Models for Multimodal Polymer Property Prediction,2511.05577v1,horawalavithana2023scitune,\cite{horawalavithana2023scitune},SCITUNE: Aligning Large Language Models with Scientific Multimodal Instructions,https://arxiv.org/abs/2307.01139v1,"Instruction finetuning is a popular paradigm to align large language models (LLM) with human intent. Despite its popularity, this idea is less explored in improving the LLMs to align existing foundation models with scientific disciplines, concepts and goals. In this work, we present SciTune as a tuning framework to improve the ability of LLMs to follow scientific multimodal instructions. To test our methodology, we use a human-generated scientific instruction tuning dataset and train a large multimodal model LLaMA-SciTune that connects a vision encoder and LLM for science-focused visual and language understanding. In comparison to the models that are finetuned with machine generated data only, LLaMA-SciTune surpasses human performance on average and in many sub-categories on the ScienceQA benchmark.",True,True,"Horawalavithana, Sameera and Munikoti, Sai and Stewart, Ian and Kvinge, Henry",2023.0,,,,arXiv preprint arXiv:2307.01139
Fine-Tuning Vision-Language Models for Multimodal Polymer Property Prediction,2511.05577v1,alshibli2025vision,\cite{alshibli2025vision},Vision-BioLLM: Large vision language model for visual dialogue in biomedical imagery,,,True,False,"Alshibli, Ahmad and Bazi, Yakoub and Al Rahhal, Mohamad Mahmoud and Zuair, Mansour",2025.0,,,,Biomedical Signal Processing and Control
Fine-Tuning Vision-Language Models for Multimodal Polymer Property Prediction,2511.05577v1,huan2016polymer,\cite{huan2016polymer},A polymer dataset for accelerated property prediction and design,,,True,False,"Huan, Tran Doan and Mannodi-Kanakkithodi, Arun and Kim, Chiho and Sharma, Vinit and Pilania, Ghanshyam and Ramprasad, Rampi",2016.0,,,,Scientific data
Fine-Tuning Vision-Language Models for Multimodal Polymer Property Prediction,2511.05577v1,kim2018polymer,\cite{kim2018polymer},Polymer genome: a data-powered polymer informatics platform for property predictions,,,True,False,"Kim, Chiho and Chandrasekaran, Anand and Huan, Tran Doan and Das, Deya and Ramprasad, Rampi",2018.0,,,,The Journal of Physical Chemistry C
Fine-Tuning Vision-Language Models for Multimodal Polymer Property Prediction,2511.05577v1,doan2020machine,\cite{doan2020machine},Machine-learning predictions of polymer properties with Polymer Genome,,,True,False,"Doan Tran, Huan and Kim, Chiho and Chen, Lihua and Chandrasekaran, Anand and Batra, Rohit and Venkatram, Shruti and Kamal, Deepak and Lightstone, Jordan P and Gurnani, Rishi and Shetty, Pranav and others",2020.0,,,,Journal of Applied Physics
Fine-Tuning Vision-Language Models for Multimodal Polymer Property Prediction,2511.05577v1,devlin2019bert,\cite{devlin2019bert},BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,https://arxiv.org/abs/1810.04805v2,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.
  BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",True,True,"Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina",2019.0,,,,
Fine-Tuning Vision-Language Models for Multimodal Polymer Property Prediction,2511.05577v1,kuenneth2023polybert,\cite{kuenneth2023polybert},polyBERT: A chemical language model to enable fully machine-driven ultrafast polymer informatics,https://arxiv.org/abs/2209.14803v1,"Polymers are a vital part of everyday life. Their chemical universe is so large that it presents unprecedented opportunities as well as significant challenges to identify suitable application-specific candidates. We present a complete end-to-end machine-driven polymer informatics pipeline that can search this space for suitable candidates at unprecedented speed and accuracy. This pipeline includes a polymer chemical fingerprinting capability called polyBERT (inspired by Natural Language Processing concepts), and a multitask learning approach that maps the polyBERT fingerprints to a host of properties. polyBERT is a chemical linguist that treats the chemical structure of polymers as a chemical language. The present approach outstrips the best presently available concepts for polymer property prediction based on handcrafted fingerprint schemes in speed by two orders of magnitude while preserving accuracy, thus making it a strong candidate for deployment in scalable architectures including cloud infrastructures.",True,True,"Kuenneth, Christopher and Ramprasad, Rampi",2023.0,,,,Nature communications
Fine-Tuning Vision-Language Models for Multimodal Polymer Property Prediction,2511.05577v1,wang2024predicting,\cite{wang2024predicting},Predicting Polymer Properties Based on Multimodal Multitask Pretraining,,,True,False,"Wang, Fanmeng and Guo, Wentao and Cheng, Minjie and Yuan, Shen and Xu, Hongteng and Gao, Zhifeng",2024.0,,,,arXiv e-prints
Fine-Tuning Vision-Language Models for Multimodal Polymer Property Prediction,2511.05577v1,gupta2025benchmarking,\cite{gupta2025benchmarking},Benchmarking Large Language Models for Polymer Property Predictions,,,True,False,"Gupta, Sonakshi and Mahmood, Akhlak and Shukla, Shivank and Ramprasad, Rampi",2025.0,,,,arXiv preprint arXiv:2506.02129
Fine-Tuning Vision-Language Models for Multimodal Polymer Property Prediction,2511.05577v1,zhang2025multimodal,\cite{zhang2025multimodal},Multimodal machine learning with large language embedding model for polymer property prediction,,,True,False,"Zhang, Tianren and Yang, Dai-Bei",2025.0,,,,arXiv preprint arXiv:2503.22962
Fine-Tuning Vision-Language Models for Multimodal Polymer Property Prediction,2511.05577v1,zhou2023uni,\cite{zhou2023uni},Uni-mol: A universal 3d molecular representation learning framework,,,True,False,"Zhou, Gengmo and Gao, Zhifeng and Ding, Qiankun and Zheng, Hang and Xu, Hongteng and Wei, Zhewei and Zhang, Linfeng and Ke, Guolin",2023.0,,,,
Quantum Tensor Representation via Circuit Partitioning and Reintegration,2511.05492v1,bravyi2005universal,\cite{bravyi2005universal},Universal quantum computation with ideal Clifford gates and noisy ancillas,,,True,False,"Bravyi, Sergey and Kitaev, Alexei",2005.0,,,,"Physical Review A—Atomic, Molecular, and Optical Physics"
Quantum Tensor Representation via Circuit Partitioning and Reintegration,2511.05492v1,bravyi2016trading,\cite{bravyi2016trading},Trading classical and quantum computational resources,,,True,False,"Bravyi, Sergey and Smith, Graeme and Smolin, John A",2016.0,,,,Physical Review X
Quantum Tensor Representation via Circuit Partitioning and Reintegration,2511.05492v1,peng2020simulating,\cite{peng2020simulating},Simulating Large Quantum Circuits on a Small Quantum Computer,https://arxiv.org/abs/1904.00102v2,"Limited quantum memory is one of the most important constraints for near-term quantum devices. Understanding whether a small quantum computer can simulate a larger quantum system, or execute an algorithm requiring more qubits than available, is both of theoretical and practical importance. In this Letter, we introduce cluster parameters $K$ and $d$ of a quantum circuit. The tensor network of such a circuit can be decomposed into clusters of size at most $d$ with at most $K$ qubits of inter-cluster quantum communication. We propose a cluster simulation scheme that can simulate any $(K,d)$-clustered quantum circuit on a $d$-qubit machine in time roughly $2^{O(K)}$, with further speedups possible when taking more fine-grained circuit structure into account. We show how our scheme can be used to simulate clustered quantum systems -- such as large molecules -- that can be partitioned into multiple significantly smaller clusters with weak interactions among them. By using a suitable clustered ansatz, we also experimentally demonstrate that a quantum variational eigensolver can still achieve the desired performance for estimating the energy of the BeH$_2$ molecule while running on a physical quantum device with half the number of required qubits.",True,True,"Peng, Tianyi and Harrow, Aram W and Ozols, Maris and Wu, Xiaodi",2020.0,,,,Physical review letters
Quantum Tensor Representation via Circuit Partitioning and Reintegration,2511.05492v1,mitarai2019methodology,\cite{mitarai2019methodology},Methodology for replacing indirect measurements with direct measurements,,,True,False,"Mitarai, Kosuke and Fujii, Keisuke",2019.0,,,,Physical Review Research
Quantum Tensor Representation via Circuit Partitioning and Reintegration,2511.05492v1,perlin2021quantum,\cite{perlin2021quantum},Quantum circuit cutting with maximum-likelihood tomography,,,True,False,"Perlin, Michael A and Saleem, Zain H and Suchara, Martin and Osborn, James C",2021.0,,,,npj Quantum Information
Quantum Tensor Representation via Circuit Partitioning and Reintegration,2511.05492v1,piveteau2023circuit,\cite{piveteau2023circuit},Circuit knitting with classical communication,,,True,False,"Piveteau, Christophe and Sutter, David",2023.0,,,,IEEE Transactions on Information Theory
Quantum Tensor Representation via Circuit Partitioning and Reintegration,2511.05492v1,yang2024understanding,\cite{yang2024understanding},Understanding the Scalability of Circuit Cutting Techniques for Practical Quantum Applications,,,True,False,"Yang, Songqinghao and Murali, Prakash",2024.0,,,,arXiv preprint arXiv:2411.17756
Quantum Tensor Representation via Circuit Partitioning and Reintegration,2511.05492v1,gentinetta2024overhead,\cite{gentinetta2024overhead},Overhead-constrained circuit knitting for variational quantum dynamics,,,True,False,"Gentinetta, Gian and Metz, Friederike and Carleo, Giuseppe",2024.0,,,,Quantum
Quantum Tensor Representation via Circuit Partitioning and Reintegration,2511.05492v1,jing2025circuit,\cite{jing2025circuit},Circuit Knitting Faces Exponential Sampling Overhead Scaling Bounded by Entanglement Cost,https://arxiv.org/abs/2404.03619v2,"Circuit knitting, a method for connecting quantum circuits across multiple processors to simulate nonlocal quantum operations, is a promising approach for distributed quantum computing. While various techniques have been developed for circuit knitting, we uncover fundamental limitations to the scalability of this technology. We prove that the sampling overhead of circuit knitting is exponentially lower bounded by the exact entanglement cost of the target bipartite dynamic, even for asymptotic overhead in the parallel cut regime. Specifically, we prove that the regularized sampling overhead assisted with local operations and classical communication (LOCC), of any bipartite quantum channel is lower bounded by the exponential of its exact entanglement cost under separable preserving operations. Furthermore, we show that the regularized sampling overhead for simulating a general bipartite channel via LOCC is lower bounded by $κ$-entanglement and max-Rains information, providing efficiently computable benchmarks. Our work reveals a profound connection between virtual quantum information processing via quasi-probability decomposition and quantum Shannon theory, highlighting the critical role of entanglement in distributed quantum computing.",True,True,"Jing, Mingrui and Zhu, Chengkai and Wang, Xin",2025.0,,,,Physical Review A
Quantum Tensor Representation via Circuit Partitioning and Reintegration,2511.05492v1,temme2017error,\cite{temme2017error},Error mitigation for short-depth quantum circuits,,,True,False,"Temme, Kristan and Bravyi, Sergey and Gambetta, Jay M",2017.0,,,,Physical review letters
Quantum Tensor Representation via Circuit Partitioning and Reintegration,2511.05492v1,tang2022cutting,\cite{tang2022cutting},Cutting Quantum Circuits to Run on Quantum and Classical Platforms,https://arxiv.org/abs/2205.05836v1,"Quantum computing (QC) offers a new computing paradigm that has the potential to provide significant speedups over classical computing. Each additional qubit doubles the size of the computational state space available to a quantum algorithm. Such exponentially expanding reach underlies QC's power, but at the same time puts demanding requirements on the quantum processing units (QPU) hardware. On the other hand, purely classical simulations of quantum circuits on either central processing unit (CPU) or graphics processing unit (GPU) scale poorly as they quickly become bottlenecked by runtime and memory. This paper introduces CutQC, a scalable hybrid computing approach that distributes a large quantum circuit onto quantum (QPU) and classical platforms (CPU or GPU) for co-processing. CutQC demonstrates evaluation of quantum circuits that are larger than the limit of QPU or classical simulation, and achieves much higher quantum circuit evaluation fidelity than the large NISQ devices achieve in real-system runs.",True,True,"Tang, Wei and Martonosi, Margaret",2022.0,,,,arXiv preprint arXiv:2205.05836
Quantum Tensor Representation via Circuit Partitioning and Reintegration,2511.05492v1,shehzad2024automated,\cite{shehzad2024automated},Automated cut finding and circuit knitting on large quantum circuits,,,True,False,"Shehzad, Ibrahim and Pednault, Edwin and Garrison, James R and Johnson, Caleb and Fuller, Bryce and Glick, Jennifer R",2024.0,,,,
Quantum Tensor Representation via Circuit Partitioning and Reintegration,2511.05492v1,lowe2023fast,\cite{lowe2023fast},Fast quantum circuit cutting with randomized measurements,,,True,False,"Lowe, Angus and Medvidovi{\'c}, Matija and Hayes, Anthony and O'Riordan, Lee J and Bromley, Thomas R and Arrazola, Juan Miguel and Killoran, Nathan",2023.0,,,,Quantum
Quantum Tensor Representation via Circuit Partitioning and Reintegration,2511.05492v1,hart2024reconstructing,\cite{hart2024reconstructing},Reconstructing Cut Quantum Circuits Maximising Fidelity between Quantum States,,,True,False,"Hart, Michael and McAllister, John",2024.0,,,,
Quantum Tensor Representation via Circuit Partitioning and Reintegration,2511.05492v1,tejedor2025distributed,\cite{tejedor2025distributed},Distributed Quantum Circuit Cutting for Hybrid Quantum-Classical High-Performance Computing,,,True,False,"Tejedor, Mar and Casas, Berta and Conejero, Javier and Cervera-Lierta, Alba and Badia, Rosa M",2025.0,,,,arXiv preprint arXiv:2505.01184
Quantum Tensor Representation via Circuit Partitioning and Reintegration,2511.05492v1,carrera2024combining,\cite{carrera2024combining},Combining quantum processors with real-time classical communication,,,True,False,"Carrera Vazquez, Almudena and Tornow, Caroline and Riste, Diego and Woerner, Stefan and Takita, Maika and Egger, Daniel J",2024.0,,,,Nature
Neural Network for Subgrid Turbulence Modeling for Large Eddy Simulations,2511.05103v2,sanderse2024scientific,\cite{sanderse2024scientific},Scientific machine learning for closure models in multiscale problems: A review,,,True,False,"Sanderse, Benjamin and Stinis, Panos and Maulik, Romit and Ahmed, Shady E",2024.0,,,,arXiv preprint arXiv:2403.02913
Neural Network for Subgrid Turbulence Modeling for Large Eddy Simulations,2511.05103v2,dai2023development,\cite{dai2023development},Development of a new dynamic Smagorinsky model by an artificial neural network for prediction of outdoor airflow and pollutant dispersion,,,True,False,"Dai, Ting and Liu, Sumei and Liu, Junjie and Jiang, Nan and Chen, Qingyan",2023.0,,,,Building and Environment
Neural Network for Subgrid Turbulence Modeling for Large Eddy Simulations,2511.05103v2,yu2022kinetic,\cite{yu2022kinetic},Kinetic-energy-flux-constrained model using an artificial neural network for large-eddy simulation of compressible wall-bounded turbulence,,,True,False,"Yu, Changping and Yuan, Zelong and Qi, Han and Wang, Jianchun and Li, Xinliang and Chen, Shiyi",2022.0,,,,Journal of Fluid Mechanics
Neural Network for Subgrid Turbulence Modeling for Large Eddy Simulations,2511.05103v2,Kurz_2022,\cite{Kurz_2022},A machine learning framework for LES closure terms,,,True,False,"Kurz, Marius and Beck, Andrea",2022.0,,,10.1553/etna_vol56s117,ETNA - Electronic Transactions on Numerical Analysis
Neural Network for Subgrid Turbulence Modeling for Large Eddy Simulations,2511.05103v2,Kim_Park_Choi_2024,\cite{Kim_Park_Choi_2024},Large eddy simulation of flow over a circular cylinder with a neural-network-based subgrid-scale model,,,True,False,"Kim, Myunghwa and Park, Jonghwan and Choi, Haecheon",2024.0,,,10.1017/jfm.2024.154,Journal of Fluid Mechanics
Neural Network for Subgrid Turbulence Modeling for Large Eddy Simulations,2511.05103v2,Park_Choi_2021,\cite{Park_Choi_2021},Toward neural-network-based large eddy simulation: application to turbulent channel flow,,,True,False,"Park, Jonghwan and Choi, Haecheon",2021.0,,,10.1017/jfm.2020.931,Journal of Fluid Mechanics
Neural Network for Subgrid Turbulence Modeling for Large Eddy Simulations,2511.05103v2,BECK2019108910,\cite{BECK2019108910},Deep neural networks for data-driven LES closure models,,,True,False,Andrea Beck and David Flad and Claus-Dieter Munz,2019.0,,,https://doi.org/10.1016/j.jcp.2019.108910,Journal of Computational Physics
Neural Network for Subgrid Turbulence Modeling for Large Eddy Simulations,2511.05103v2,kim2024generalizabledatadriventurbulenceclosure,\cite{kim2024generalizabledatadriventurbulenceclosure},Generalizable data-driven turbulence closure modeling on unstructured grids with differentiable physics,https://arxiv.org/abs/2307.13533v3,"Differentiable physical simulators are proving to be valuable tools for developing data-driven models for computational fluid dynamics (CFD). In particular, these simulators enable end-to-end training of machine learning (ML) models embedded within CFD solvers. This paradigm enables novel algorithms which combine the generalization power and low cost of physics-based simulations with the flexibility and automation of deep learning methods. In this study, we introduce a framework for embedding deep learning models within a finite element solver for incompressible Navier-Stokes equations, specifically applying this approach to learn a subgrid-scale (SGS) closure with a graph neural network (GNN). We first demonstrate the feasibility of the approach on flow over a two-dimensional backward-facing step, using it as a proof of concept to show that solver-consistent training produces stable and physically meaningful closures. Then, we extend this to a turbulent flow over a three-dimensional backward-facing step. In this setting, the GNN-based closure not only attains low prediction errors, but also recovers key turbulence statistics and preserves multiscale turbulent structures. We further demonstrate that the closure can be identified in data-limited learning scenarios as well. Overall, the proposed end-to-end learning paradigm offers a viable pathway toward physically consistent and generalizable data-driven SGS modeling on complex and unstructured domains.",True,True,Hojin Kim and Varun Shankar and Venkatasubramanian Viswanathan and Romit Maulik,2024.0,,,,
Neural Network for Subgrid Turbulence Modeling for Large Eddy Simulations,2511.05103v2,List_2022,\cite{List_2022},Learned turbulence modelling with differentiable fluid solvers: physics-based loss functions and optimisation horizons,,,True,False,"List, Björn and Chen, Li-Wei and Thuerey, Nils",2022.0,,,10.1017/jfm.2022.738,Journal of Fluid Mechanics
Neural Network for Subgrid Turbulence Modeling for Large Eddy Simulations,2511.05103v2,Beck_2021,\cite{Beck_2021},A Perspective on Machine Learning Methods in Turbulence Modelling,https://arxiv.org/abs/2010.12226v1,"This work presents a review of the current state of research in data-driven turbulence closure modeling. It offers a perspective on the challenges and open issues, but also on the advantages and promises of machine learning methods applied to parameter estimation, model identification, closure term reconstruction and beyond, mostly from the perspective of Large Eddy Simulation and related techniques. We stress that consistency of the training data, the model, the underlying physics and the discretization is a key issue that needs to be considered for a successful ML-augmented modeling strategy. In order to make the discussion useful for non-experts in either field, we introduce both the modeling problem in turbulence as well as the prominent ML paradigms and methods in a concise and self-consistent manner. Following, we present a survey of the current data-driven model concepts and methods, highlight important developments and put them into the context of the discussed challenges.",True,True,"Beck, Andrea and Kurz, Marius",2021.0,,,10.1002/gamm.202100002,GAMM-Mitteilungen
Neural Network for Subgrid Turbulence Modeling for Large Eddy Simulations,2511.05103v2,NISTA2025106498,\cite{NISTA2025106498},Parallel implementation and performance of super-resolution generative adversarial network turbulence models for large-eddy simulation,,,True,False,Ludovico Nista and Christoph D.K. Schumann and Peicho Petkov and Valentin Pavlov and Temistocle Grenga and Jonathan F. MacArt and Antonio Attili and Stoyan Markov and Heinz Pitsch,2025.0,,,https://doi.org/10.1016/j.compfluid.2024.106498,Computers \& Fluids
Neural Network for Subgrid Turbulence Modeling for Large Eddy Simulations,2511.05103v2,Fukami_2020,\cite{Fukami_2020},Machine learning based spatio-temporal super resolution reconstruction of turbulent flows,https://arxiv.org/abs/2004.11566v2,"We present a new turbulent data reconstruction method with supervised machine learning techniques inspired by super resolution and inbetweening, which can recover high-resolution turbulent flows from grossly coarse flow data in space and time. For the present machine learning based data reconstruction, we use the downsampled skip-connection/multi-scale model based on a convolutional neural network to incorporate the multi-scale nature of fluid flows into its network structure. As an initial example, the model is applied to a two-dimensional cylinder wake at $Re_D$ = 100. The reconstructed flow fields by the proposed method show great agreement with the reference data obtained by direct numerical simulation. Next, we examine the capability of the proposed model for a two-dimensional decaying homogeneous isotropic turbulence. The machine-learned models can follow the decaying evolution from coarse input data in space and time, according to the assessment with the turbulence statistics. The proposed concept is further investigated for a complex turbulent channel flow over a three-dimensional domain at $Re_τ$ =180. The present model can reconstruct high-resolved turbulent flows from very coarse input data in space, and it can also reproduce the temporal evolution when the time interval is appropriately chosen. The dependence on the amount of training snapshots and duration between the first and last frames based on a temporal two-point correlation coefficient are also assessed to reveal the capability and robustness of spatio-temporal super resolution reconstruction. These results suggest that the present method can meet a range of flow reconstructions for supporting computational and experimental efforts.",True,True,"Fukami, Kai and Fukagata, Koji and Taira, Kunihiko",2020.0,,,10.1017/jfm.2020.948,Journal of Fluid Mechanics
Neural Network for Subgrid Turbulence Modeling for Large Eddy Simulations,2511.05103v2,Yuan_2020,\cite{Yuan_2020},Deconvolutional artificial neural network models for large eddy simulation of turbulence,https://arxiv.org/abs/2007.14212v2,"Deconvolutional artificial neural network (DANN) models are developed for subgrid-scale (SGS) stress in large eddy simulation (LES) of turbulence. The filtered velocities at different spatial points are used as input features of the DANN models to reconstruct the unfiltered velocity. The grid width of the DANN models is chosen to be smaller than the filter width, in order to accurately model the effects of SGS dynamics. The DANN models can predict the SGS stress more accurately than the conventional approximate deconvolution method (ADM) and velocity gradient model (VGM) in a prior study: the correlation coefficients can be made larger than 99\% and the relative errors can be made less than 15\% for the DANN model. In an a posteriori study, a comprehensive comparison of the DANN model, the implicit large eddy simulation (ILES), the dynamic Smagorinsky model (DSM), and the dynamic mixed model (DMM) shows that: the DANN model is superior to the ILES, DSM, and DMM models in the prediction of the velocity spectrum, various statistics of velocity and the instantaneous coherent structures without increasing the considerable computational cost. Besides, the trained DANN models without any fine-tuning can predict the velocity statistics well for different filter widths. These results indicate that the DANN framework with consideration of SGS spatial features is a promising approach to develop advanced SGS models in the LES of turbulence.",True,True,"Yuan, Zelong and Xie, Chenyue and Wang, Jianchun",2020.0,,,10.1063/5.0027146,Physics of Fluids
Neural Network for Subgrid Turbulence Modeling for Large Eddy Simulations,2511.05103v2,cuomo2022scientificmachinelearningphysicsinformed,\cite{cuomo2022scientificmachinelearningphysicsinformed},Scientific Machine Learning through Physics-Informed Neural Networks: Where we are and What's next,https://arxiv.org/abs/2201.05624v4,"Physics-Informed Neural Networks (PINN) are neural networks (NNs) that encode model equations, like Partial Differential Equations (PDE), as a component of the neural network itself. PINNs are nowadays used to solve PDEs, fractional equations, integral-differential equations, and stochastic PDEs. This novel methodology has arisen as a multi-task learning framework in which a NN must fit observed data while reducing a PDE residual. This article provides a comprehensive review of the literature on PINNs: while the primary goal of the study was to characterize these networks and their related advantages and disadvantages. The review also attempts to incorporate publications on a broader range of collocation-based physics informed neural networks, which stars form the vanilla PINN, as well as many other variants, such as physics-constrained neural networks (PCNN), variational hp-VPINN, and conservative PINN (CPINN). The study indicates that most research has focused on customizing the PINN through different activation functions, gradient optimization techniques, neural network structures, and loss function structures. Despite the wide range of applications for which PINNs have been used, by demonstrating their ability to be more feasible in some contexts than classical numerical techniques like Finite Element Method (FEM), advancements are still possible, most notably theoretical issues that remain unresolved.",True,True,Salvatore Cuomo and Vincenzo Schiano di Cola and Fabio Giampaolo and Gianluigi Rozza and Maziar Raissi and Francesco Piccialli,2022.0,,,,
Neural Network for Subgrid Turbulence Modeling for Large Eddy Simulations,2511.05103v2,raissi2017physicsinformeddeeplearning,\cite{raissi2017physicsinformeddeeplearning},Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations,https://arxiv.org/abs/1711.10561v1,"We introduce physics informed neural networks -- neural networks that are trained to solve supervised learning tasks while respecting any given law of physics described by general nonlinear partial differential equations. In this two part treatise, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct classes of algorithms, namely continuous time and discrete time models. The resulting neural networks form a new class of data-efficient universal function approximators that naturally encode any underlying physical laws as prior information. In this first part, we demonstrate how these networks can be used to infer solutions to partial differential equations, and obtain physics-informed surrogate models that are fully differentiable with respect to all input coordinates and free parameters.",True,True,Maziar Raissi and Paris Perdikaris and George Em Karniadakis,2017.0,,,,
Neural Network for Subgrid Turbulence Modeling for Large Eddy Simulations,2511.05103v2,LING201622,\cite{LING201622},Machine learning strategies for systems with invariance properties,,,True,False,Julia Ling and Reese Jones and Jeremy Templeton,2016.0,,,https://doi.org/10.1016/j.jcp.2016.05.003,Journal of Computational Physics
Neural Network for Subgrid Turbulence Modeling for Large Eddy Simulations,2511.05103v2,Wu_2018,\cite{Wu_2018},Physics-informed machine learning approach for augmenting turbulence models: A comprehensive framework,,,True,False,"Wu, Jin-Long and Xiao, Heng and Paterson, Eric",2018.0,,,10.1103/physrevfluids.3.074602,Physical Review Fluids
Neural Network for Subgrid Turbulence Modeling for Large Eddy Simulations,2511.05103v2,BOSE2024107483,\cite{BOSE2024107483},Invariance embedded physics-infused deep neural network-based sub-grid scale models for turbulent flows,,,True,False,Rikhi Bose and Arunabha M. Roy,2024.0,,,https://doi.org/10.1016/j.engappai.2023.107483,Engineering Applications of Artificial Intelligence
Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning,2511.04856v1,meyer2022survey,\cite{meyer2022survey},A Survey on Quantum Reinforcement Learning,https://arxiv.org/abs/2211.03464v2,"Quantum reinforcement learning is an emerging field at the intersection of quantum computing and machine learning. While we intend to provide a broad overview of the literature on quantum reinforcement learning - our interpretation of this term will be clarified below - we put particular emphasis on recent developments. With a focus on already available noisy intermediate-scale quantum devices, these include variational quantum circuits acting as function approximators in an otherwise classical reinforcement learning setting. In addition, we survey quantum reinforcement learning algorithms based on future fault-tolerant hardware, some of which come with a provable quantum advantage. We provide both a birds-eye-view of the field, as well as summaries and reviews for selected parts of the literature.",True,True,"Meyer, Nico and Ufrecht, Christian and Periyasamy, Maniraman and Scherer, Daniel D and Plinge, Axel and Mutschler, Christopher",2022.0,,,,arXiv preprint arXiv:2211.03464
Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning,2511.04856v1,Brassard2000QuantumAA,\cite{Brassard2000QuantumAA},Quantum amplitude amplification and estimation,,,True,False,"Brassard, Gilles and Høyer, Peter and Mosca, Michele and Tapp, Alain",2002.0,,,,Contemporary Mathematics
Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning,2511.04856v1,jerbi2021parametrized,\cite{jerbi2021parametrized},Parametrized quantum policies for reinforcement learning,,,True,False,"Jerbi, Sofiene and Gyurik, Casper and Marshall, Simon and Briegel, Hans and Dunjko, Vedran",2021.0,,,,Advances in Neural Information Processing Systems
Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning,2511.04856v1,kent2024using,\cite{kent2024using},Using Quantum Solved Deep Boltzmann Machines to Increase the Data Efficiency of RL Agents,https://arxiv.org/abs/2408.17240v1,"Deep Learning algorithms, such as those used in Reinforcement Learning, often require large quantities of data to train effectively. In most cases, the availability of data is not a significant issue. However, for some contexts, such as in autonomous cyber defence, we require data efficient methods. Recently, Quantum Machine Learning and Boltzmann Machines have been proposed as solutions to this challenge. In this work we build upon the pre-existing work to extend the use of Deep Boltzmann Machines to the cutting edge algorithm Proximal Policy Optimisation in a Reinforcement Learning cyber defence environment. We show that this approach, when solved using a D-WAVE quantum annealer, can lead to a two-fold increase in data efficiency. We therefore expect it to be used by the machine learning and quantum communities who are hoping to capitalise on data-efficient Reinforcement Learning methods.",True,True,"Kent, Daniel and O'Rourke, Clement and Southall, Jake and Duncan, Kirsty and Bedford, Adrian",2024.0,,,,
Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning,2511.04856v1,crawford2018reinforcement,\cite{crawford2018reinforcement},Reinforcement Learning Using Quantum Boltzmann Machines,https://arxiv.org/abs/1612.05695v3,"We investigate whether quantum annealers with select chip layouts can outperform classical computers in reinforcement learning tasks. We associate a transverse field Ising spin Hamiltonian with a layout of qubits similar to that of a deep Boltzmann machine (DBM) and use simulated quantum annealing (SQA) to numerically simulate quantum sampling from this system. We design a reinforcement learning algorithm in which the set of visible nodes representing the states and actions of an optimal policy are the first and last layers of the deep network. In absence of a transverse field, our simulations show that DBMs are trained more effectively than restricted Boltzmann machines (RBM) with the same number of nodes. We then develop a framework for training the network as a quantum Boltzmann machine (QBM) in the presence of a significant transverse field for reinforcement learning. This method also outperforms the reinforcement learning method that uses RBMs.",True,True,"Crawford, Daniel and Levit, Anna and Ghadermarzy, Navid and Oberoi, Jaspreet S and Ronagh, Pooya",2018.0,,,,Quantum Information and Computation
Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning,2511.04856v1,levit2017free,\cite{levit2017free},Free energy-based reinforcement learning using a quantum processor,https://arxiv.org/abs/1706.00074v1,"Recent theoretical and experimental results suggest the possibility of using current and near-future quantum hardware in challenging sampling tasks. In this paper, we introduce free energy-based reinforcement learning (FERL) as an application of quantum hardware. We propose a method for processing a quantum annealer's measured qubit spin configurations in approximating the free energy of a quantum Boltzmann machine (QBM). We then apply this method to perform reinforcement learning on the grid-world problem using the D-Wave 2000Q quantum annealer. The experimental results show that our technique is a promising method for harnessing the power of quantum sampling in reinforcement learning tasks.",True,True,"Levit, Anna and Crawford, Daniel and Ghadermarzy, Navid and Oberoi, Jaspreet S and Zahedinejad, Ehsan and Ronagh, Pooya",2017.0,,,,arXiv preprint arXiv:1706.00074
Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning,2511.04856v1,neumann2020multi,\cite{neumann2020multi},Multi-agent reinforcement learning using simulated quantum annealing,,,True,False,"Neumann, Niels MP and de Heer, Paolo BUL and Chiscop, Irina and Phillipson, Frank",2020.0,,,,
Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning,2511.04856v1,schenk2024hybrid,\cite{schenk2024hybrid},Hybrid actor-critic algorithm for quantum reinforcement learning at cern beam lines,,,True,False,"Schenk, Michael and Combarro, El{\'\i}as F and Grossi, Michele and Kain, Verena and Li, Kevin Shing Bruce and Popa, Mircea-Marian and Vallecorsa, Sofia",2024.0,,,,Quantum Science and Technology
Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning,2511.04856v1,cho2011improved,\cite{cho2011improved},Improved learning of Gaussian-Bernoulli restricted Boltzmann machines,,,True,False,"Cho, KyungHyun and Ilin, Alexander and Raiko, Tapani",2011.0,,,,
Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning,2511.04856v1,melchior2017gaussian,\cite{melchior2017gaussian},Gaussian-binary Restricted Boltzmann Machines on Modeling Natural Image Statistics,https://arxiv.org/abs/1401.5900v1,"We present a theoretical analysis of Gaussian-binary restricted Boltzmann machines (GRBMs) from the perspective of density models. The key aspect of this analysis is to show that GRBMs can be formulated as a constrained mixture of Gaussians, which gives a much better insight into the model's capabilities and limitations. We show that GRBMs are capable of learning meaningful features both in a two-dimensional blind source separation task and in modeling natural images. Further, we show that reported difficulties in training GRBMs are due to the failure of the training algorithm rather than the model itself. Based on our analysis we are able to propose several training recipes, which allowed successful and fast training in our experiments. Finally, we discuss the relationship of GRBMs to several modifications that have been proposed to improve the model.",True,True,"Melchior, Jan and Wang, Nan and Wiskott, Laurenz",2017.0,,,,PLoS One
Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning,2511.04856v1,welling2004exponential,\cite{welling2004exponential},Exponential family harmoniums with an application to information retrieval,,,True,False,"Welling, Max and Rosen-Zvi, Michal and Hinton, Geoffrey E",2004.0,,,,Advances in neural information processing systems
Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning,2511.04856v1,li2018exponential,\cite{li2018exponential},Exponential family restricted Boltzmann machines and annealed importance sampling,,,True,False,"Li, Yifeng and Zhu, Xiaodan",2018.0,,,,
Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning,2511.04856v1,bangar2025continuous,\cite{bangar2025continuous},Continuous-variable Quantum Boltzmann Machine,https://arxiv.org/abs/2405.06580v1,"We propose a continuous-variable quantum Boltzmann machine (CVQBM) using a powerful energy-based neural network. It can be realized experimentally on a continuous-variable (CV) photonic quantum computer. We used a CV quantum imaginary time evolution (QITE) algorithm to prepare the essential thermal state and then designed the CVQBM to proficiently generate continuous probability distributions. We applied our method to both classical and quantum data. Using real-world classical data, such as synthetic aperture radar (SAR) images, we generated probability distributions. For quantum data, we used the output of CV quantum circuits. We obtained high fidelity and low Kuller-Leibler (KL) divergence showing that our CVQBM learns distributions from given data well and generates data sampling from that distribution efficiently. We also discussed the experimental feasibility of our proposed CVQBM. Our method can be applied to a wide range of real-world problems by choosing an appropriate target distribution (corresponding to, e.g., SAR images, medical images, and risk management in finance). Moreover, our CVQBM is versatile and could be programmed to perform tasks beyond generation, such as anomaly detection.",True,True,"Bangar, Shikha and Sunny, Leanto and Yeter-Aydeniz, K{\""u}bra and Siopsis, George",2025.0,,,,Quantum Machine Intelligence
Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning,2511.04856v1,watkins1992q,\cite{watkins1992q},Q-learning,,,True,False,"Watkins, Christopher JCH and Dayan, Peter",1992.0,,,,Machine learning
Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning,2511.04856v1,katz2017reluplex,\cite{katz2017reluplex},Reluplex: An efficient SMT solver for verifying deep neural networks,,,True,False,"Katz, Guy and Barrett, Clark and Dill, David L and Julian, Kyle and Kochenderfer, Mykel J",2017.0,,,,
Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning,2511.04856v1,ryucaql,\cite{ryucaql},CAQL: Continuous Action Q-Learning,https://arxiv.org/abs/1909.12397v3,"Value-based reinforcement learning (RL) methods like Q-learning have shown success in a variety of domains. One challenge in applying Q-learning to continuous-action RL problems, however, is the continuous action maximization (max-Q) required for optimal Bellman backup. In this work, we develop CAQL, a (class of) algorithm(s) for continuous-action Q-learning that can use several plug-and-play optimizers for the max-Q problem. Leveraging recent optimization results for deep neural networks, we show that max-Q can be solved optimally using mixed-integer programming (MIP). When the Q-function representation has sufficient power, MIP-based optimization gives rise to better policies and is more robust than approximate methods (e.g., gradient ascent, cross-entropy search). We further develop several techniques to accelerate inference in CAQL, which despite their approximate nature, perform well. We compare CAQL with state-of-the-art RL algorithms on benchmark continuous-control problems that have different degrees of action constraints and show that CAQL outperforms policy-based methods in heavily constrained environments, often dramatically.",True,True,"Ryu, Moonkyung and Chow, Yinlam and Anderson, Ross and Tjandraatmadja, Christian and Boutilier, Craig",2020.0,,,,
Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning,2511.04856v1,burtea2024constrained,\cite{burtea2024constrained},Constrained continuous-action reinforcement learning for supply chain inventory management,,,True,False,"Burtea, Radu and Tsay, Calvin",2024.0,,,,Computers \& Chemical Engineering
Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning,2511.04856v1,kalashnikov2018scalable,\cite{kalashnikov2018scalable},Scalable deep reinforcement learning for vision-based robotic manipulation,,,True,False,"Kalashnikov, Dmitry and Irpan, Alex and Pastor, Peter and Ibarz, Julian and Herzog, Alexander and Jang, Eric and Quillen, Deirdre and Holly, Ethan and Kalakrishnan, Mrinal and Vanhoucke, Vincent and others",2018.0,,,,
Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning,2511.04856v1,simmons2019q,\cite{simmons2019q},Q-learning for continuous actions with cross-entropy guided policies,,,True,False,"Simmons-Edler, Riley and Eisner, Ben and Mitchell, Eric and Seung, Sebastian and Lee, Daniel",2019.0,,,,arXiv preprint arXiv:1903.10605
Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning,2511.04856v1,perakis2022optimizing,\cite{perakis2022optimizing},Optimizing Objective Functions from Trained ReLU Neural Networks via Sampling,https://arxiv.org/abs/2205.14189v2,"This paper introduces scalable, sampling-based algorithms that optimize trained neural networks with ReLU activations. We first propose an iterative algorithm that takes advantage of the piecewise linear structure of ReLU neural networks and reduces the initial mixed-integer optimization problem (MIP) into multiple easy-to-solve linear optimization problems (LPs) through sampling. Subsequently, we extend this approach by searching around the neighborhood of the LP solution computed at each iteration. This scheme allows us to devise a second, enhanced algorithm that reduces the initial MIP problem into smaller, easier-to-solve MIPs. We analytically show the convergence of the methods and we provide a sample complexity guarantee. We also validate the performance of our algorithms by comparing them against state-of-the-art MIP-based methods. Finally, we show computationally how the sampling algorithms can be used effectively to warm-start MIP-based methods.",True,True,"Perakis, Georgia and Tsiourvas, Asterios",2022.0,,,,arXiv preprint arXiv:2205.14189
Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning,2511.04856v1,gu2016continuous,\cite{gu2016continuous},Continuous deep q-learning with model-based acceleration,,,True,False,"Gu, Shixiang and Lillicrap, Timothy and Sutskever, Ilya and Levine, Sergey",2016.0,,,,
Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning,2511.04856v1,plaksin2022continuous,\cite{plaksin2022continuous},Continuous deep Q-learning in optimal control problems: Normalized advantage functions analysis,,,True,False,"Plaksin, Anton and Martyanov, Stepan",2022.0,,,,Advances in Neural Information Processing Systems
Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning,2511.04856v1,amos2017input,\cite{amos2017input},Input Convex Neural Networks,https://arxiv.org/abs/1609.07152v3,"This paper presents the input convex neural network architecture. These are scalar-valued (potentially deep) neural networks with constraints on the network parameters such that the output of the network is a convex function of (some of) the inputs. The networks allow for efficient inference via optimization over some inputs to the network given others, and can be applied to settings including structured prediction, data imputation, reinforcement learning, and others. In this paper we lay the basic groundwork for these models, proposing methods for inference, optimization and learning, and analyze their representational power. We show that many existing neural network architectures can be made input-convex with a minor modification, and develop specialized optimization algorithms tailored to this setting. Finally, we highlight the performance of the methods on multi-label prediction, image completion, and reinforcement learning problems, where we show improvement over the existing state of the art in many cases.",True,True,"Amos, Brandon and Xu, Lei and Kolter, J Zico",2017.0,,,,
JW-Flare: Accurate Solar Flare Forecasting Method Based on Multimodal Large Language Models,2511.08970v1,giovanelli1939relations,\cite{giovanelli1939relations},The Relations Between Eruptions and Sunspots.,,,True,False,"Giovanelli, Ronald Gordon",1939.0,,,,"Astrophysical Journal, vol. 89, p. 555"
JW-Flare: Accurate Solar Flare Forecasting Method Based on Multimodal Large Language Models,2511.08970v1,huang2024short,\cite{huang2024short},Short-term solar eruptive activity prediction models based on machine learning approaches: A review,,,True,False,"Huang, Xin and Zhao, Zhongrui and Zhong, Yufeng and Xu, Long and Kors{\'o}s, Marianna B and Erd{\'e}lyi, R",2024.0,,,,Science China Earth Sciences
JW-Flare: Accurate Solar Flare Forecasting Method Based on Multimodal Large Language Models,2511.08970v1,karakatsanis2008soc,\cite{karakatsanis2008soc},SOC and Chaos into the Solar Activity,,,True,False,"Karakatsanis, LP and Pavlos, GP",2008.0,,,,Nonlinear Phenomena in Complex Systems
JW-Flare: Accurate Solar Flare Forecasting Method Based on Multimodal Large Language Models,2511.08970v1,korsos2018applying,\cite{korsos2018applying},Applying the weighted horizontal magnetic gradient method to a simulated flaring active region,,,True,False,"Kors{\'o}s, Marianna Brigitta and Chatterjee, Piyali and Erd{\'e}lyi, R{\'o}bert",2018.0,,,,The Astrophysical Journal
JW-Flare: Accurate Solar Flare Forecasting Method Based on Multimodal Large Language Models,2511.08970v1,lin2009studies,\cite{lin2009studies},Studies of solar flares and CMEs related to the space solar missions in the future,,,True,False,"Lin, Jun",2009.0,,,,"Science in China Series G: Physics, Mechanics and Astronomy"
JW-Flare: Accurate Solar Flare Forecasting Method Based on Multimodal Large Language Models,2511.08970v1,ning2009investigation,\cite{ning2009investigation},The investigation of the Neupert effect in two solar flares,,,True,False,"Ning, ZongJun",2009.0,,,,"Science in China Series G: Physics, Mechanics and Astronomy"
JW-Flare: Accurate Solar Flare Forecasting Method Based on Multimodal Large Language Models,2511.08970v1,ning2012power,\cite{ning2012power},Power conversion factor in solar flares,,,True,False,"Ning, ZongJun",2012.0,,,,Chinese Science Bulletin
JW-Flare: Accurate Solar Flare Forecasting Method Based on Multimodal Large Language Models,2511.08970v1,wang2012solar,\cite{wang2012solar},Solar activity studies: From a magnetohydrodynamics description to a plasma perspective,,,True,False,"Wang, JingXiu",2012.0,,,,Chinese Science Bulletin
JW-Flare: Accurate Solar Flare Forecasting Method Based on Multimodal Large Language Models,2511.08970v1,bloomfield2012toward,\cite{bloomfield2012toward},Toward Reliable Benchmarking of Solar Flare Forecasting Methods,https://arxiv.org/abs/1202.5995v1,"Solar flares occur in complex sunspot groups, but it remains unclear how the probability of producing a flare of a given magnitude relates to the characteristics of the sunspot group. Here, we use Geostationary Operational Environmental Satellite X-ray flares and McIntosh group classifications from solar cycles 21 and 22 to calculate average flare rates for each McIntosh class and use these to determine Poisson probabilities for different flare magnitudes. Forecast verification measures are studied to find optimum thresholds to convert Poisson flare probabilities into yes/no predictions of cycle 23 flares. A case is presented to adopt the true skill statistic (TSS) as a standard for forecast comparison over the commonly used Heidke skill score (HSS). In predicting flares over 24 hr, the maximum values of TSS achieved are 0.44 (C-class), 0.53 (M-class), 0.74 (X-class), 0.54 (>=M1.0), and 0.46 (>=C1.0). The maximum values of HSS are 0.38 (C-class), 0.27 (M-class), 0.14 (X-class), 0.28 (>=M1.0), and 0.41 (>=C1.0). These show that Poisson probabilities perform comparably to some more complex prediction systems, but the overall inaccuracy highlights the problem with using average values to represent flaring rate distributions.",True,True,"Bloomfield, D Shaun and Higgins, Paul A and McAteer, RT James and Gallagher, Peter T",2012.0,,,,The Astrophysical Journal Letters
JW-Flare: Accurate Solar Flare Forecasting Method Based on Multimodal Large Language Models,2511.08970v1,yuan2020solar,\cite{yuan2020solar},Solar flare forecasting model of combining principal component analysis with support vector machine,,,True,False,"Yuan, F and Lin, JB and Deng, YY and Guo, JJ and Wang, G",2020.0,,,,Chin. Sci Bull
JW-Flare: Accurate Solar Flare Forecasting Method Based on Multimodal Large Language Models,2511.08970v1,abduallah2021deepsun,\cite{abduallah2021deepsun},DeepSun: Machine-learning-as-a-service for solar flare prediction,,,True,False,"Abduallah, Yasser and Wang, Jason TL and Nie, Yang and Liu, Chang and Wang, Haimin",2021.0,,,,Research in Astronomy and Astrophysics
JW-Flare: Accurate Solar Flare Forecasting Method Based on Multimodal Large Language Models,2511.08970v1,huang2018deep,\cite{huang2018deep},Deep learning based solar flare forecasting model. I. Results for line-of-sight magnetograms,,,True,False,"Huang, Xin and Wang, Huaning and Xu, Long and Liu, Jinfu and Li, Rong and Dai, Xinghua",2018.0,,,,The Astrophysical Journal
JW-Flare: Accurate Solar Flare Forecasting Method Based on Multimodal Large Language Models,2511.08970v1,nishizuka2018deep,\cite{nishizuka2018deep},Deep Flare Net (DeFN) model for solar flare prediction,https://arxiv.org/abs/1805.03421v1,"We developed a solar flare prediction model using a deep neural network (DNN), named Deep Flare Net (DeFN). The model can calculate the probability of flares occurring in the following 24 h in each active region, which is used to determine the most likely maximum classes of flares via a binary classification (e.g., >=M class versus <M class or >=C class versus <C class). From 3x10^5 observation images taken during 2010-2015 by Solar Dynamic Observatory, we automatically detected sunspots and calculated 79 features for each region, to which flare occurrence labels of X-, M-, and C-class were attached. We adopted the features used in Nishizuka et al. (2017) and added some features for operational prediction: coronal hot brightening at 131 A (T>=10^7 K) and the X-ray and 131 A intensity data 1 and 2 h before an image. For operational evaluation, we divided the database into two for training and testing: the dataset in 2010-2014 for training and the one in 2015 for testing. The DeFN model consists of deep multilayer neural networks, formed by adapting skip connections and batch normalizations. To statistically predict flares, the DeFN model was trained to optimize the skill score, i.e., the true skill statistic (TSS). As a result, we succeeded in predicting flares with TSS=0.80 for >=M-class flares and TSS=0.63 for >=C-class flares. Note that in usual DNN models, the prediction process is a black box. However, in the DeFN model, the features are manually selected, and it is possible to analyze which features are effective for prediction after evaluation.",True,True,"Nishizuka, Naoto and Sugiura, Komei and Kubo, Yuki and Den, Mitsue and Ishii, Mamoru",2018.0,,,,The Astrophysical Journal
JW-Flare: Accurate Solar Flare Forecasting Method Based on Multimodal Large Language Models,2511.08970v1,nishizuka2020reliable,\cite{nishizuka2020reliable},Reliable Probability Forecast of Solar Flares: Deep Flare Net-Reliable (DeFN-R),https://arxiv.org/abs/2007.02564v1,"We developed a reliable probabilistic solar flare forecasting model using a deep neural network, named Deep Flare Net-Reliable (DeFN-R). The model can predict the maximum classes of flares that occur in the following 24 h after observing images, along with the event occurrence probability. We detected active regions from 3x10^5 solar images taken during 2010-2015 by Solar Dynamic Observatory and extracted 79 features for each region, which we annotated with flare occurrence labels of X-, M-, and C-classes. The extracted features are the same as used by Nishizuka et al. (2018); for example, line-of-sight/vector magnetograms in the photosphere, brightening in the corona, and the X-ray emissivity 1 and 2 h before an image. We adopted a chronological split of the database into two for training and testing in an operational setting: the dataset in 2010-2014 for training and the one in 2015 for testing. DeFN-R is composed of multilayer perceptrons formed by batch normalizations and skip connections. By tuning optimization methods, DeFN-R was trained to optimize the Brier skill score (BSS). As a result, we achieved BSS = 0.41 for >=C-class flare predictions and 0.30 for >=M-class flare predictions by improving the reliability diagram while keeping the relative operating characteristic curve almost the same. Note that DeFN is optimized for deterministic prediction, which is determined with a normalized threshold of 50%. On the other hand, DeFN-R is optimized for a probability forecast based on the observation event rate, whose probability threshold can be selected according to users' purposes.",True,True,"Nishizuka, Naoto and Kubo, Yuki and Sugiura, Komei and Den, Mitsue and Ishii, Mamoru",2020.0,,,,The Astrophysical Journal
JW-Flare: Accurate Solar Flare Forecasting Method Based on Multimodal Large Language Models,2511.08970v1,nishizuka2021operational,\cite{nishizuka2021operational},Operational solar flare prediction model using Deep Flare Net,https://arxiv.org/abs/2112.00977v1,"We developed an operational solar flare prediction model using deep neural networks, named Deep Flare Net (DeFN). DeFN can issue probabilistic forecasts of solar flares in two categories, such as >=M-class and <M-class events or >=C-class and <C-class events, occurring in the next 24 h after observations and the maximum class of flares occurring in the next 24 h. DeFN is set to run every 6 h and has been operated since January 2019. The input database of solar observation images taken by the Solar Dynamic Observatory (SDO) is downloaded from the data archive operated by the Joint Science Operations Center (JSOC) of Stanford University. Active regions are automatically detected from magnetograms, and 79 features are extracted from each region nearly in real time using multiwavelength observation data. Flare labels are attached to the feature database, and then, the database is standardized and input into DeFN for prediction. DeFN was pretrained using the datasets obtained from 2010 to 2015. The model was evaluated with the skill score of the true skill statistics (TSS) and achieved predictions with TSS = 0.80 for >=M-class flares and TSS = 0.63 for >=C-class flares. For comparison, we evaluated the operationally forecast results from January 2019 to June 2020. We found that operational DeFN forecasts achieved TSS = 0.70 (0.84) for >=C-class flares with the probability threshold of 50 (40)%, although there were very few M-class flares during this period and we should continue monitoring the results for a longer time. Here, we adopted a chronological split to divide the database into two for training and testing. The chronological split appears suitable for evaluating operational models. Furthermore, we proposed the use of time-series cross-validation. The procedure achieved TSS = 0.70 for >=M-class flares and 0.59 for >=C-class flares using the datasets obtained from 2010 to 2017.",True,True,"Nishizuka, Naoto and Kubo, Y{\^u}ki and Sugiura, Komei and Den, Mitsue and Ishii, Mamoru",2021.0,,,,"Earth, Planets and Space"
JW-Flare: Accurate Solar Flare Forecasting Method Based on Multimodal Large Language Models,2511.08970v1,zheng2023multiclass,\cite{zheng2023multiclass},Multiclass solar flare forecasting models with different deep learning algorithms,,,True,False,"Zheng, Yanfang and Li, Xuebao and Yan, Shuainan and Huang, Xusheng and Lou, Hengrui and Li, Zhe",2023.0,,,,Monthly Notices of the Royal Astronomical Society
JW-Flare: Accurate Solar Flare Forecasting Method Based on Multimodal Large Language Models,2511.08970v1,abduallah2023operational,\cite{abduallah2023operational},Operational prediction of solar flares using a transformer-based framework,,,True,False,"Abduallah, Yasser and Wang, Jason TL and Wang, Haimin and Xu, Yan",2023.0,,,,Scientific reports
Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions,2511.01464v1,Peter2009,\cite{Peter2009},Multiscale simulation of soft matter systems - From the atomistic to the coarse-grained level and back,,,True,False,Christine Peter and Kurt Kremer,2009.0,,,10.1039/b912027k,Soft Matter
Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions,2511.01464v1,Rzepiela2010,\cite{Rzepiela2010},Software news and update reconstruction of atomistic details from coarse-grained structures,,,True,False,Andrzej J. Rzepiela and Lars V. Schäfer and Nicolae Goga and H. Jelger Risselada and Alex H. De Vries and Siewert J. Marrink,2010.0,,,10.1002/jcc.21415,Journal of Computational Chemistry
Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions,2511.01464v1,Wassenaar2014,\cite{Wassenaar2014},Going backward: A flexible geometric approach to reverse transformation from coarse grained to atomistic models,,,True,False,Tsjerk A. Wassenaar and Kristyna Pluhackova and Rainer A. Böckmann and Siewert J. Marrink and D. Peter Tieleman,2014.0,,,10.1021/ct400617g,Journal of Chemical Theory and Computation
Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions,2511.01464v1,Stieffenhofer2020,\cite{Stieffenhofer2020},Adversarial Reverse Mapping of Equilibrated Condensed-Phase Molecular Structures,https://arxiv.org/abs/2003.07753v1,A tight and consistent link between resolutions is crucial to further expand the impact of multiscale modeling for complex materials. We herein tackle the generation of condensed molecular structures as a refinement -- backmapping -- of a coarse-grained structure. Traditional schemes start from a rough coarse-to-fine mapping and perform further energy minimization and molecular dynamics simulations to equilibrate the system. In this study we introduce DeepBackmap: A deep neural network based approach to directly predict equilibrated molecular structures for condensed-phase systems. We use generative adversarial networks to learn the Boltzmann distribution from training data and realize reverse mapping by using the coarse-grained structure as a conditional input. We apply our method to a challenging condensed-phase polymeric system. We observe that the model trained in a melt has remarkable transferability to the crystalline phase. The combination of data-driven and physics-based aspects of our architecture help reach temperature transferability with only limited training data.,True,True,Marc Stieffenhofer and Michael Wand and Tristan Bereau,2020.0,12,,10.1088/2632-2153/abb6d4,Machine Learning: Science and Technology
Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions,2511.01464v1,Li2020,\cite{Li2020},Backmapping coarse-grained macromolecules: An efficient and versatile machine learning approach,,,True,False,Wei Li and Craig Burkhart and Patrycja Polińska and Vagelis Harmandaris and Manolis Doxastakis,2020.0,7,,10.1063/5.0012320,The Journal of Chemical Physics
Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions,2511.01464v1,Wang2022,\cite{Wang2022},Generative Coarse-Graining of Molecular Conformations,https://arxiv.org/abs/2201.12176v2,"Coarse-graining (CG) of molecular simulations simplifies the particle representation by grouping selected atoms into pseudo-beads and drastically accelerates simulation. However, such CG procedure induces information losses, which makes accurate backmapping, i.e., restoring fine-grained (FG) coordinates from CG coordinates, a long-standing challenge. Inspired by the recent progress in generative models and equivariant networks, we propose a novel model that rigorously embeds the vital probabilistic nature and geometric consistency requirements of the backmapping transformation. Our model encodes the FG uncertainties into an invariant latent space and decodes them back to FG geometries via equivariant convolutions. To standardize the evaluation of this domain, we provide three comprehensive benchmarks based on molecular dynamics trajectories. Experiments show that our approach always recovers more realistic structures and outperforms existing data-driven methods with a significant margin.",True,True,"Wang, Wujie and Xu, Minkai and Cai, Chen and Miller, Benjamin K and Smidt, Tess and Wang, Yusu and Tang, Jian and Gomez-Bombarelli, Rafael",2022.0,17--23 Jul,https://proceedings.mlr.press/v162/wang22ag.html,,
Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions,2511.01464v1,Shmilovich2022,\cite{Shmilovich2022},Temporally coherent backmapping of molecular trajectories from coarse-grained to atomistic resolution,https://arxiv.org/abs/2205.05213v2,"Coarse-graining offers a means to extend the achievable time and length scales of molecular dynamics simulations beyond what is practically possible in the atomistic regime. Sampling molecular configurations of interest can be done efficiently using coarse-grained simulations, from which meaningful physicochemical information can be inferred if the corresponding all-atom configurations are reconstructed. However, this procedure of backmapping to reintroduce the lost atomistic detail into coarse-grain structures has proven a challenging task due to the many feasible atomistic configurations that can be associated with one coarse-grain structure. Existing backmapping methods are strictly frame-based, relying on either heuristics to replace coarse-grain particles with atomic fragments and subsequent relaxation, or parameterized models to propose atomic coordinates separately and independently for each coarse-grain structure. These approaches neglect information from previous trajectory frames that is critical to ensuring temporal coherence of the backmapped trajectory, while also offering information potentially helpful to produce higher-fidelity atomic reconstructions. In this work we present a deep learning-enabled data-driven approach for temporally coherent backmapping that explicitly incorporates information from preceding trajectory structures. Our method trains a conditional variational autoencoder to non-deterministically reconstruct atomistic detail conditioned on both the target coarse-grain configuration and the previously reconstructed atomistic configuration. We demonstrate our backmapping approach on two exemplar biomolecular systems: alanine dipeptide and the miniprotein chignolin. We show that our backmapped trajectories accurately recover the structural, thermodynamic, and kinetic properties of the atomistic trajectory data.",True,True,Kirill Shmilovich and Marc Stieffenhofer and Nicholas E. Charron and Moritz Hoffmann,2022.0,12,,10.1021/acs.jpca.2c07716,The Journal of Physical Chemistry A
Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions,2511.01464v1,Jones2023,\cite{Jones2023},DiAMoNDBack: Diffusion-denoising Autoregressive Model for Non-Deterministic Backmapping of Cα Protein Traces,https://arxiv.org/abs/2307.12451v1,"Coarse-grained molecular models of proteins permit access to length and time scales unattainable by all-atom models and the simulation of processes that occur on long-time scales such as aggregation and folding. The reduced resolution realizes computational accelerations but an atomistic representation can be vital for a complete understanding of mechanistic details. Backmapping is the process of restoring all-atom resolution to coarse-grained molecular models. In this work, we report DiAMoNDBack (Diffusion-denoising Autoregressive Model for Non-Deterministic Backmapping) as an autoregressive denoising diffusion probability model to restore all-atom details to coarse-grained protein representations retaining only Cα coordinates. The autoregressive generation process proceeds from the protein N-terminus to C-terminus in a residue-by-residue fashion conditioned on the Cα trace and previously backmapped backbone and side chain atoms within the local neighborhood. The local and autoregressive nature of our model makes it transferable between proteins. The stochastic nature of the denoising diffusion process means that the model generates a realistic ensemble of backbone and side chain all-atom configurations consistent with the coarse-grained Cα trace. We train DiAMoNDBack over 65k+ structures from Protein Data Bank (PDB) and validate it in applications to a hold-out PDB test set, intrinsically-disordered protein structures from the Protein Ensemble Database (PED), molecular dynamics simulations of fast-folding mini-proteins from DE Shaw Research, and coarse-grained simulation data. We achieve state-of-the-art reconstruction performance in terms of correct bond formation, avoidance of side chain clashes, and diversity of the generated side chain configurational states. We make DiAMoNDBack model publicly available as a free and open source Python package.",True,True,Michael S. Jones and Kirill Shmilovich and Andrew L. Ferguson,2023.0,,,10.1021/acs.jctc.3c00840,Journal of Chemical Theory and Computation
Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions,2511.01464v1,ferguson2025flowback,\cite{ferguson2025flowback},FlowBack: A Generalized Flow-Matching Approach for Biomolecular Backmapping,,,True,False,Michael S. Jones and Smayan Khanna and Andrew L. Ferguson,2025.0,1,,10.1021/acs.jcim.4c02046,Journal of Chemical Information and Modeling
Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions,2511.01464v1,Berlaga2025,\cite{Berlaga2025},FlowBack-Adjoint: Physics-Aware and Energy-Guided Conditional Flow-Matching for All-Atom Protein Backmapping,,,True,False,Alex Berlaga and Michael S. Jones and Andrew L. Ferguson,2025.0,8,,,arXiv preprint arXiv:2508.03619
Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions,2511.01464v1,UgarteLaTorre2025,\cite{UgarteLaTorre2025},CGBack: Diffusion Model for Backmapping Large-Scale and Complex Coarse-Grained Molecular Systems,,,True,False,Diego Ugarte La Torre and Yuji Sugita,2025.0,9,,10.1021/acs.jcim.5c01281,Journal of Chemical Information and Modeling
Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions,2511.01464v1,rezende2015flows,\cite{rezende2015flows},Variational inference with normalizing flows,,,True,False,Danilo Jimenez Rezende and Shakir Mohamed,2015.0,,,,
Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions,2511.01464v1,Chen2018,\cite{Chen2018},Neural Ordinary Differential Equations,https://arxiv.org/abs/1806.07366v5,"We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.",True,True,"Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K",2018.0,,https://proceedings.neurips.cc/paper_files/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf,,
Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions,2511.01464v1,Lipman2023,\cite{Lipman2023},Flow Matching for Generative Modeling,,,True,False,Yaron Lipman and Ricky T. Q. Chen and Heli Ben-Hamu and Maximilian Nickel and Matthew Le,2023.0,,https://openreview.net/forum?id=PqvMRDCJT9t,,
Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions,2511.01464v1,Albergo2023,\cite{Albergo2023},Stochastic Interpolants: A Unifying Framework for Flows and Diffusions,,,True,False,Michael S. Albergo and Nicholas M. Boffi and Eric Vanden-Eijnden,2023.0,3,https://arxiv.org/abs/2303.08797v3,,arXiv preprint arXiv:2303.08797
Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions,2511.01464v1,Tong2024,\cite{Tong2024},Simulation-Free {S}chrödinger Bridges via Score and Flow Matching,,,True,False,"Tong, Alexander Y. and Malkin, Nikolay and Fatras, Kilian and Atanackovic, Lazar and Zhang, Yanlei and Huguet, Guillaume and Wolf, Guy and Bengio, Yoshua",2024.0,02--04 May,https://proceedings.mlr.press/v238/tong24a.html,,
Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions,2511.01464v1,Albergo2023data,\cite{Albergo2023data},Stochastic Interpolants with Data-Dependent Couplings,,,True,False,"Albergo, Michael Samuel and Goldstein, Mark and Boffi, Nicholas Matthew and Ranganath, Rajesh and Vanden-Eijnden, Eric",2024.0,21--27 Jul,https://proceedings.mlr.press/v235/albergo24a.html,,
Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions,2511.01464v1,Brehmer2020,\cite{Brehmer2020},Flows for simultaneous manifold learning and density estimation,,,True,False,"Brehmer, Johann and Cranmer, Kyle",2020.0,,https://proceedings.neurips.cc/paper_files/paper/2020/file/051928341be67dcba03f0e04104d9047-Paper.pdf,,
Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions,2511.01464v1,Foley2015,\cite{Foley2015},The impact of resolution upon entropy and information in coarse-grained models,,,True,False,Thomas T. Foley and M. Scott Shell and W. G. Noid,2015.0,12,,10.1063/1.4929836,The Journal of Chemical Physics
Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions,2511.01464v1,Kidder2024,\cite{Kidder2024},Analysis of mapping atomic models to coarse-grained resolution,,,True,False,Katherine M. Kidder and W. G. Noid,2024.0,10,,10.1063/5.0220989,The Journal of Chemical Physics
Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions,2511.01464v1,Giulini2020,\cite{Giulini2020},An Information-Theory-Based Approach for Optimal Model Reduction of Biomolecules,,,True,False,Marco Giulini and Roberto Menichetti and M. Scott Shell and Raffaello Potestio,2020.0,,,10.1021/acs.jctc.0c00676,Journal of Chemical Theory and Computation
Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions,2511.01464v1,Giulini2024,\cite{Giulini2024},"EXCOGITO, an extensible coarse-graining toolbox for the investigation of biomolecules by means of low-resolution representation",https://arxiv.org/abs/2403.08097v2,"Bottom-up coarse-grained (CG) models proved to be essential to complement and sometimes even replace all-atom representations of soft matter systems and biological macromolecules. The development of low-resolution models takes the moves from the reduction of the degrees of freedom employed, that is, the definition of a mapping between a system's high-resolution description and its simplified counterpart. Even in the absence of an explicit parametrisation and simulation of a CG model, the observation of the atomistic system in simpler terms can be informative: this idea is leveraged by the mapping entropy, a measure of the information loss inherent to the process of coarsening. Mapping entropy lies at the heart of the extensible coarse-graining toolbox, or EXCOGITO, developed to perform a number of operations and analyses on molecular systems pivoting around the properties of mappings. EXCOGITO can process an all-atom trajectory to compute the mapping entropy, identify the mapping that minimizes it, and establish quantitative relations between a low-resolution representation and the geometrical, structural, and energetic features of the system. Here, the software, which is available free of charge under an open-source licence, is presented and showcased to introduce potential users to its capabilities and usage.
  Published on the J. Chem. Inf. Model. on June 11, 2024. DOI: https://doi.org/10.1021/acs.jcim.4c00490",True,True,Marco Giulini and Raffaele Fiorentini and Luca Tubiana and Raffaello Potestio and Roberto Menichetti,2024.0,6,https://pubs.acs.org/doi/full/10.1021/acs.jcim.4c00490,10.1021/ACS.JCIM.4C00490/ASSET/IMAGES/LARGE/CI4C00490_0007.JPEG,Journal of Chemical Information and Modeling
Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions,2511.01464v1,Armstrong2012,\cite{Armstrong2012},Statistical mechanics of coarse graining: Estimating dynamical speedups from excess entropies,,,True,False,J. A. Armstrong and C. Chakravarty and P. Ballone,2012.0,,,10.1063/1.3697383,Journal of Chemical Physics
Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions,2511.01464v1,Jin2023,\cite{Jin2023},Understanding dynamics in coarse-grained models. I. Universal excess entropy scaling relationship,,,True,False,Jaehyeok Jin and Kenneth S. Schweizer and Gregory A. Voth,2023.0,,,10.1063/5.0116299,Journal of Chemical Physics
Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions,2511.01464v1,Mussi2025,\cite{Mussi2025},Predicting energetic and entropic driving forces with coarse-grained models,,,True,False,Lucus M. Mussi and W. G. Noid,2025.0,8,,10.1063/5.0281108,The Journal of Chemical Physics
Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions,2511.01464v1,Holtzman2022,\cite{Holtzman2022},"Making sense of complex systems through resolution, relevance, and mapping entropy",https://arxiv.org/abs/2203.00100v2,"Complex systems are characterised by a tight, nontrivial interplay of their constituents, which gives rise to a multi-scale spectrum of emergent properties. In this scenario, it is practically and conceptually difficult to identify those degrees of freedom that mostly determine the behaviour of the system and separate them from less prominent players. Here, we tackle this problem making use of three measures of statistical information: resolution, relevance, and mapping entropy. We address the links existing among them, taking the moves from the established relation between resolution and relevance and further developing novel connections between resolution and mapping entropy; by these means we can identify, in a quantitative manner, the number and selection of degrees of freedom of the system that preserve the largest information content about the generative process that underlies an empirical dataset. The method, which is implemented in a freely available software, is fully general, as it is shown through the application to three very diverse systems, namely a toy model of independent binary spins, a coarse-grained representation of the financial stock market, and a fully atomistic simulation of a protein.",True,True,Roi Holtzman and Marco Giulini and Raffaello Potestio,2022.0,10,,10.1103/PhysRevE.106.044101,Physical Review E
MiniFool -- Physics-Constraint-Aware Minimizer-Based Adversarial Attacks in Deep Neural Networks,2511.01352v1,Flek:2025ecg,\cite{Flek:2025ecg},Enforcing Fundamental Relations via Adversarial Attacks on Input Parameter Correlations,https://arxiv.org/abs/2501.05588v1,"Correlations between input parameters play a crucial role in many scientific classification tasks, since these are often related to fundamental laws of nature. For example, in high energy physics, one of the common deep learning use-cases is the classification of signal and background processes in particle collisions. In many such cases, the fundamental principles of the correlations between observables are often better understood than the actual distributions of the observables themselves. In this work, we present a new adversarial attack algorithm called Random Distribution Shuffle Attack (RDSA), emphasizing the correlations between observables in the network rather than individual feature characteristics. Correct application of the proposed novel attack can result in a significant improvement in classification performance - particularly in the context of data augmentation - when using the generated adversaries within adversarial training. Given that correlations between input features are also crucial in many other disciplines. We demonstrate the RDSA effectiveness on six classification tasks, including two particle collision challenges (using CERN Open Data), hand-written digit recognition (MNIST784), human activity recognition (HAR), weather forecasting (Rain in Australia), and ICU patient mortality (MIMIC-IV), demonstrating a general use case beyond fundamental physics for this new type of adversarial attack algorithms.",True,True,Timo Saala and Lucie Flek and Alexander Jung and Akbar Karimi and Alexander Schmidt and Matthias Schott and Philipp Soldin and Christopher Wiebusch,2025.0,,,10.48550/arXiv.2501.05588,
MiniFool -- Physics-Constraint-Aware Minimizer-Based Adversarial Attacks in Deep Neural Networks,2511.01352v1,Stein:991721,\cite{Stein:991721},{Novel jet flavour tagging algorithms exploiting adversarial deep learning techniques with efficient computing methods and preparation of open data for robustness studies},,,True,False,"Stein, Annika",2024.0,,,10.18154/RWTH-2024-07840,
MiniFool -- Physics-Constraint-Aware Minimizer-Based Adversarial Attacks in Deep Neural Networks,2511.01352v1,Stein:2022nvf,\cite{Stein:2022nvf},Improving Robustness of Jet Tagging Algorithms with Adversarial Training,https://arxiv.org/abs/2203.13890v2,"Deep learning is a standard tool in the field of high-energy physics, facilitating considerable sensitivity enhancements for numerous analysis strategies. In particular, in identification of physics objects, such as jet flavor tagging, complex neural network architectures play a major role. However, these methods are reliant on accurate simulations. Mismodeling can lead to non-negligible differences in performance in data that need to be measured and calibrated against. We investigate the classifier response to input data with injected mismodelings and probe the vulnerability of flavor tagging algorithms via application of adversarial attacks. Subsequently, we present an adversarial training strategy that mitigates the impact of such simulated attacks and improves the classifier robustness. We examine the relationship between performance and vulnerability and show that this method constitutes a promising approach to reduce the vulnerability to poor modeling.",True,True,"Stein, Annika and Coubez, Xavier and Mondal, Spandan and Novak, Andrzej and Schmidt, Alexander",2022.0,,,10.1007/s41781-022-00087-1,Comput. Softw. Big Sci.
MiniFool -- Physics-Constraint-Aware Minimizer-Based Adversarial Attacks in Deep Neural Networks,2511.01352v1,Goodfellow:2014rpb,\cite{Goodfellow:2014rpb},{Explaining and Harnessing Adversarial Examples},,,True,False,Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy,2015.0,,,10.48550/arXiv.1412.6572,
MiniFool -- Physics-Constraint-Aware Minimizer-Based Adversarial Attacks in Deep Neural Networks,2511.01352v1,Madry:2017tvh,\cite{Madry:2017tvh},Towards Deep Learning Models Resistant to Adversarial Attacks,https://arxiv.org/abs/1706.06083v4,"Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist_challenge and https://github.com/MadryLab/cifar10_challenge.",True,True,Aleksander Madry and Aleksandar Makelov and Ludwig Schmidt and Dimitris Tsipras and Adrian Vladu,2019.0,,,10.48550/arXiv.1706.06083,
MiniFool -- Physics-Constraint-Aware Minimizer-Based Adversarial Attacks in Deep Neural Networks,2511.01352v1,Deepfoolpaper,\cite{Deepfoolpaper},{DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks},,,True,False,"Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal",2016.0,,,10.1109/CVPR.2016.282,
MiniFool -- Physics-Constraint-Aware Minimizer-Based Adversarial Attacks in Deep Neural Networks,2511.01352v1,masterjanik,\cite{masterjanik},{Using Adversarial Attacks to Fool IceCube’s Deep Neural Networks},,,True,False,Oliver Janik,2023.0,,https://www.institut3b.physik.rwth-aachen.de/global/show_document.asp?id=aaaaaaaacgjxigu,,
MiniFool -- Physics-Constraint-Aware Minimizer-Based Adversarial Attacks in Deep Neural Networks,2511.01352v1,szegedy2013intriguing,\cite{szegedy2013intriguing},Intriguing properties of neural networks,https://arxiv.org/abs/1312.6199v4,"Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.
  First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks.
  Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",True,True,Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian Goodfellow and Rob Fergus,2014.0,,,10.48550/arXiv.1312.6199,
MiniFool -- Physics-Constraint-Aware Minimizer-Based Adversarial Attacks in Deep Neural Networks,2511.01352v1,carlini2017,\cite{carlini2017},{Towards Evaluating the Robustness of Neural Networks},,,True,False,"Carlini, Nicholas and Wagner, David",2017.0,,,10.1109/SP.2017.49,
Qubit Mapping and Routing tailored to Advanced Quantum ISAs: Not as Costly as You Think,2511.04608v2,codina2001unified,\cite{codina2001unified},A unified modulo scheduling and register allocation technique for clustered processors,,,True,False,"Codina, Josep M and S{\'a}nchez, Jes{\'u}s and Gonz{\'a}lez, Antonio",2001.0,,,,
Qubit Mapping and Routing tailored to Advanced Quantum ISAs: Not as Costly as You Think,2511.04608v2,hennessy1983postpass,\cite{hennessy1983postpass},Postpass code optimization of pipeline constraints,,,True,False,"Hennessy, John L and Gross, Thomas",1983.0,,,,ACM Transactions on Programming Languages and Systems (TOPLAS)
Qubit Mapping and Routing tailored to Advanced Quantum ISAs: Not as Costly as You Think,2511.04608v2,chaitin1982register,\cite{chaitin1982register},Register allocation \& spilling via graph coloring,,,True,False,"Chaitin, Gregory J",1982.0,,,,ACM Sigplan Notices
Qubit Mapping and Routing tailored to Advanced Quantum ISAs: Not as Costly as You Think,2511.04608v2,poletto1999linear,\cite{poletto1999linear},Linear scan register allocation,,,True,False,"Poletto, Massimiliano and Sarkar, Vivek",1999.0,,,,ACM Transactions on Programming Languages and Systems (TOPLAS)
Qubit Mapping and Routing tailored to Advanced Quantum ISAs: Not as Costly as You Think,2511.04608v2,zulehner2018efficient,\cite{zulehner2018efficient},An efficient methodology for mapping quantum circuits to the IBM QX architectures,,,True,False,"Zulehner, Alwin and Paler, Alexandru and Wille, Robert",2018.0,,,,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems
Qubit Mapping and Routing tailored to Advanced Quantum ISAs: Not as Costly as You Think,2511.04608v2,li2019tackling,\cite{li2019tackling},Tackling the qubit mapping problem for NISQ-era quantum devices,,,True,False,"Li, Gushu and Ding, Yufei and Xie, Yuan",2019.0,,,,
Qubit Mapping and Routing tailored to Advanced Quantum ISAs: Not as Costly as You Think,2511.04608v2,lao2021timing,\cite{lao2021timing},Timing and resource-aware mapping of quantum circuits to superconducting processors,,,True,False,"Lao, Lingling and Van Someren, Hans and Ashraf, Imran and Almudever, Carmen G",2021.0,,,,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems
Qubit Mapping and Routing tailored to Advanced Quantum ISAs: Not as Costly as You Think,2511.04608v2,ddroute2025,\cite{ddroute2025},DDRoute: a Novel Depth-Driven Approach to the Qubit Routing Problem,,,True,False,"Annechini, Alessandro and Venere, Marco and Sciuto, Donatella and Santambrogio, Marco",2025.0,,,,
Qubit Mapping and Routing tailored to Advanced Quantum ISAs: Not as Costly as You Think,2511.04608v2,zou2024lightsabre,\cite{zou2024lightsabre},Lightsabre: A lightweight and enhanced sabre algorithm,,,True,False,"Zou, Henry and Treinish, Matthew and Hartman, Kevin and Ivrii, Alexander and Lishman, Jake",2024.0,,,,arXiv preprint arXiv:2409.08368
Qubit Mapping and Routing tailored to Advanced Quantum ISAs: Not as Costly as You Think,2511.04608v2,childs2019circuit,\cite{childs2019circuit},Circuit transformations for quantum architectures,,,True,False,"Childs, Andrew M and Schoute, Eddie and Unsal, Cem M",2019.0,,,,arXiv preprint arXiv:1902.09102
Qubit Mapping and Routing tailored to Advanced Quantum ISAs: Not as Costly as You Think,2511.04608v2,zhang2021time,\cite{zhang2021time},Time-optimal qubit mapping,,,True,False,"Zhang, Chi and Hayes, Ari B and Qiu, Longfei and Jin, Yuwei and Chen, Yanhao and Zhang, Eddy Z",2021.0,,,,
Qubit Mapping and Routing tailored to Advanced Quantum ISAs: Not as Costly as You Think,2511.04608v2,tan2020optimal,\cite{tan2020optimal},Optimal layout synthesis for quantum computing,,,True,False,"Tan, Bochen and Cong, Jason",2020.0,,,,
Qubit Mapping and Routing tailored to Advanced Quantum ISAs: Not as Costly as You Think,2511.04608v2,ibmFractionalGates,\cite{ibmFractionalGates},New fractional gates reduce circuit depth for utility-scale workloads,,,True,False,{IBM Quantum},2024.0,,,,
Qubit Mapping and Routing tailored to Advanced Quantum ISAs: Not as Costly as You Think,2511.04608v2,ionqPartialGates,\cite{ionqPartialGates},Getting started with IonQ's hardware-native gateset,,,True,False,{IonQ},2023.0,,,,
Qubit Mapping and Routing tailored to Advanced Quantum ISAs: Not as Costly as You Think,2511.04608v2,yale2025realization,\cite{yale2025realization},Realization and calibration of continuously parameterized two-qubit gates on a trapped-ion quantum processor,,,True,False,"Yale, Christopher G and Burch, Ashlyn D and Chow, Matthew NH and Ruzic, Brandon P and Lobser, Daniel S and McFarland, Brian K and Revelle, Melissa C and Clark, Susan M",2025.0,,,,arXiv preprint arXiv:2504.06259
Qubit Mapping and Routing tailored to Advanced Quantum ISAs: Not as Costly as You Think,2511.04608v2,chen2024one,\cite{chen2024one},One Gate Scheme to Rule Them All: Introducing a Complex Yet Reduced Instruction Set for Quantum Computing,https://arxiv.org/abs/2312.05652v3,"The design and architecture of a quantum instruction set are paramount to the performance of a quantum computer. This work introduces a gate scheme for qubits with $XX+YY$ coupling that directly and efficiently realizes any two-qubit gate up to single-qubit gates. First, this scheme enables high-fidelity execution of quantum operations and achieves minimum possible gate times. Second, since the scheme spans the entire $\textbf{SU}(4)$ group of two-qubit gates, we can use it to attain the optimal two-qubit gate count for algorithm implementation. These two advantages in synergy give rise to a quantum Complex yet Reduced Instruction Set Computer (CRISC). Though the gate scheme is compact, it supports a comprehensive array of quantum operations. This may seem paradoxical but is realizable due to the fundamental differences between quantum and classical computer architectures.
  Using our gate scheme, we observe marked improvements across various applications, including generic $n$-qubit gate synthesis, quantum volume, and qubit routing. Furthermore, the proposed scheme also realizes a gate locally equivalent to the commonly used CNOT gate with a gate time of $\fracπ{2g}$, where $g$ is the two-qubit coupling. The AshN scheme is also completely impervious to $ZZ$ error, the main coherent error in transversely coupled systems, as the control parameters implementing the gates can be easily adjusted to take the $ZZ$ term into account.",True,True,"Chen, Jianxin and Ding, Dawei and Gong, Weiyuan and Huang, Cupjin and Ye, Qi",2024.0,,,,
Qubit Mapping and Routing tailored to Advanced Quantum ISAs: Not as Costly as You Think,2511.04608v2,chen2025efficient,\cite{chen2025efficient},Efficient Implementation of Arbitrary Two-Qubit Gates via Unified Control,https://arxiv.org/abs/2502.03612v1,"The native gate set is fundamental to the performance of quantum devices, as it governs the accuracy of basic quantum operations and dictates the complexity of implementing quantum algorithms. Traditional approaches to extending gate sets often require accessing multiple transitions within an extended Hilbert space, leading to increased control complexity while offering only a limited set of gates. Here, we experimentally demonstrate a unified and highly versatile gate scheme capable of natively generating arbitrary two-qubit gates using only exchange interaction and qubit driving on a superconducting quantum processor, achieving maximum expressivity. Using a state-of-the-art transmon-coupler-transmon architecture, we achieve high fidelities averaging $99.37 \pm 0.07\%$ across a wide range of commonly used two-qubit unitaries. This outstanding performance, combined with reduced complexity, enables precise multipartite entangled state preparation, as demonstrated. To further enhance its applicability, we also show the high-fidelity realization of the unique B gate, which efficiently synthesizes the entire family of two-qubit gates. Our results highlight that fully exploiting the capabilities of a single interaction can yield a comprehensive and highly accurate gate set. With maximum expressivity, gate-time optimality, demonstrated high fidelity, and easy adaptability to other quantum platforms, our unified control scheme paves the way for optimal performance in quantum devices, offering exciting prospects for advancing quantum hardware and algorithm development.",True,True,"Chen, Zhen
and Liu, Weiyang
and Ma, Yanjun
and Sun, Weijie
and Wang, Ruixia
and Wang, He
and Xu, Huikai
and Xue, Guangming
and Yan, Haisheng
and Yang, Zhen
and Ding, Jiayu
and Gao, Yang
and Li, Feiyu
and Zhang, Yujia
and Zhang, Zikang
and Jin, Yirong
and Yu, Haifeng
and Chen, Jianxin
and Yan, Fei",2025.0,Aug,https://doi.org/10.1038/s41567-025-02990-x,10.1038/s41567-025-02990-x,Nature Physics
Qubit Mapping and Routing tailored to Advanced Quantum ISAs: Not as Costly as You Think,2511.04608v2,mckinney2024mirage,\cite{mckinney2024mirage},Mirage: Quantum circuit decomposition and routing collaborative design using mirror gates,,,True,False,"McKinney, Evan and Hatridge, Michael and Jones, Alex K",2024.0,,,,
Qubit Mapping and Routing tailored to Advanced Quantum ISAs: Not as Costly as You Think,2511.04608v2,huang2023quantum,\cite{huang2023quantum},Quantum Instruction Set Design for Performance,,,True,False,"Huang, Cupjin and Wang, Tenghui and Wu, Feng and Ding, Dawei and Ye, Qi and Kong, Linghang and Zhang, Fang and Ni, Xiaotong and Song, Zhijun and Shi, Yaoyun and Zhao, Hui-Hai and Deng, Chunqing and Chen, Jianxin",2023.0,Feb,https://link.aps.org/doi/10.1103/PhysRevLett.130.070601,10.1103/PhysRevLett.130.070601,Physical Review Letters
Qubit Mapping and Routing tailored to Advanced Quantum ISAs: Not as Costly as You Think,2511.04608v2,bqskit,\cite{bqskit},{Berkeley Quantum Synthesis Toolkit (BQSKit)},,,True,False,"Younis, Ed and Iancu, Costin C and Lavrijsen, Wim and Davis, Marc and Smith, Ethan",2021.0,4,,10.11578/dc.20210603.2,
Qubit Mapping and Routing tailored to Advanced Quantum ISAs: Not as Costly as You Think,2511.04608v2,davis2019heuristics,\cite{davis2019heuristics},Heuristics for quantum compiling with a continuous gate set,,,True,False,"Davis, Marc Grau and Smith, Ethan and Tudor, Ana and Sen, Koushik and Siddiqi, Irfan and Iancu, Costin",2019.0,,,,
Qubit Mapping and Routing tailored to Advanced Quantum ISAs: Not as Costly as You Think,2511.04608v2,wu2020qgo,\cite{wu2020qgo},QGo: Scalable Quantum Circuit Optimization Using Automated Synthesis,https://arxiv.org/abs/2012.09835v5,"The current phase of quantum computing is in the Noisy Intermediate-Scale Quantum (NISQ) era. On NISQ devices, two-qubit gates such as CNOTs are much noisier than single-qubit gates, so it is essential to minimize their count. Quantum circuit synthesis is a process of decomposing an arbitrary unitary into a sequence of quantum gates, and can be used as an optimization tool to produce shorter circuits to improve overall circuit fidelity. However, the time-to-solution of synthesis grows exponentially with the number of qubits. As a result, synthesis is intractable for circuits on a large qubit scale.
  In this paper, we propose a hierarchical, block-by-block optimization framework, QGo, for quantum circuit optimization. Our approach allows an exponential cost optimization to scale to large circuits. QGo uses a combination of partitioning and synthesis: 1) partition the circuit into a sequence of independent circuit blocks; 2) re-generate and optimize each block using quantum synthesis; and 3) re-compose the final circuit by stitching all the blocks together. We perform our analysis and show the fidelity improvements in three different regimes: small-size circuits on real devices, medium-size circuits on noise simulations, and large-size circuits on analytical models. Using a set of NISQ benchmarks, we show that QGo can reduce the number of CNOT gates by 29.9% on average and up to 50% when compared with industrial compilers such as t|ket>. When executed on the IBM Athens system, shorter depth leads to higher circuit fidelity. We also demonstrate the scalability of our QGo technique to optimize circuits of 60+ qubits. Our technique is the first demonstration of successfully employing and scaling synthesis in the compilation toolchain for large circuits. Overall, our approach is robust for direct incorporation in production compiler toolchains.",True,True,"Wu, Xin-Chuan and Davis, Marc Grau and Chong, Frederic T and Iancu, Costin",2020.0,,,,arXiv preprint arXiv:2012.09835
Qubit Mapping and Routing tailored to Advanced Quantum ISAs: Not as Costly as You Think,2511.04608v2,kukliansky2023qfactor,\cite{kukliansky2023qfactor},QFactor: A Domain-Specific Optimizer for Quantum Circuit Instantiation,,,True,False,"Kukliansky, Alon and Younis, Ed and Cincio, Lukasz and Iancu, Costin",2023.0,,,,
Qubit Mapping and Routing tailored to Advanced Quantum ISAs: Not as Costly as You Think,2511.04608v2,younis2021qfast,\cite{younis2021qfast},Qfast: Conflating search and numerical optimization for scalable quantum circuit synthesis,,,True,False,"Younis, Ed and Sen, Koushik and Yelick, Katherine and Iancu, Costin",2021.0,,,,
Qubit Mapping and Routing tailored to Advanced Quantum ISAs: Not as Costly as You Think,2511.04608v2,kalloor2024quantum,\cite{kalloor2024quantum},Quantum hardware roofline: Evaluating the impact of gate expressivity on quantum processor design,,,True,False,"Kalloor, Justin and Weiden, Mathias and Younis, Ed and Kubiatowicz, John and De Jong, Bert and Iancu, Costin",2024.0,,,,
Qubit Mapping and Routing tailored to Advanced Quantum ISAs: Not as Costly as You Think,2511.04608v2,mcewen2023relaxing,\cite{mcewen2023relaxing},Relaxing hardware requirements for surface code circuits using time-dynamics,,,True,False,"McEwen, Matt and Bacon, Dave and Gidney, Craig",2023.0,,,,Quantum
Qubit Mapping and Routing tailored to Advanced Quantum ISAs: Not as Costly as You Think,2511.04608v2,eickbusch2024demonstrating,\cite{eickbusch2024demonstrating},Demonstrating dynamic surface codes,https://arxiv.org/abs/2412.14360v2,"A remarkable characteristic of quantum computing is the potential for reliable computation despite faulty qubits. This can be achieved through quantum error correction, which is typically implemented by repeatedly applying static syndrome checks, permitting correction of logical information. Recently, the development of time-dynamic approaches to error correction has uncovered new codes and new code implementations. In this work, we experimentally demonstrate three time-dynamic implementations of the surface code, each offering a unique solution to hardware design challenges and introducing flexibility in surface code realization. First, we embed the surface code on a hexagonal lattice, reducing the necessary couplings per qubit from four to three. Second, we walk a surface code, swapping the role of data and measure qubits each round, achieving error correction with built-in removal of accumulated non-computational errors. Finally, we realize the surface code using iSWAP gates instead of the traditional CNOT, extending the set of viable gates for error correction without additional overhead. We measure the error suppression factor when scaling from distance-3 to distance-5 codes of $Λ_{35,\text{hex}} = 2.15(2)$, $Λ_{35,\text{walk}} = 1.69(6)$, and $Λ_{35,\text{iSWAP}} = 1.56(2)$, achieving state-of-the-art error suppression for each. With detailed error budgeting, we explore their performance trade-offs and implications for hardware design. This work demonstrates that dynamic circuit approaches satisfy the demands for fault-tolerance and opens new alternative avenues for scalable hardware design.",True,True,"Eickbusch, Alec and McEwen, Matt and Sivak, Volodymyr and Bourassa, Alexandre and Atalaya, Juan and Claes, Jahan and Kafri, Dvir and Gidney, Craig and Warren, Christopher W. and Gross, Jonathan and Opremcak, Alex and Zobrist, Nicholas and Miao, Kevin C. and Roberts, Gabrielle and Satzinger, Kevin J. and Bengtsson, Andreas and Neeley, Matthew and Livingston, William P. and Greene, Alex and Acharya, Rajeev and Beni, Laleh Aghababaie and Aigeldinger, Georg and Alcaraz, Ross and Andersen, Trond I. and Ansmann, Markus and Arute, Frank and Arya, Kunal and Asfaw, Abraham and Babbush, Ryan and Ballard, Brian and Bardin, Joseph C. and Bilmes, Alexander and Bovaird, Jenna and Bowers, Dylan and Brill, Leon and Broughton, Michael and Browne, David A. and Buchea, Brett and Buckley, Bob B. and Burger, Tim and Burkett, Brian and Bushnell, Nicholas and Cabrera, Anthony and Campero, Juan and Chang, Hung-Shen and Chiaro, Ben and Chih, Liang-Ying and Cleland, Agnetta Y. and Cogan, Josh and Collins, Roberto and Conner, Paul and Courtney, William and Crook, Alexander L. and Curtin, Ben and Das, Sayan and Del Toro Barba, Alexander and Demura, Sean and De Lorenzo, Laura and Di Paolo, Agustin and Donohoe, Paul and Drozdov, Ilya K. and Dunsworth, Andrew and Elbag, Aviv Moshe and Elzouka, Mahmoud and Erickson, Catherine and Ferreira, Vinicius S. and Flores Burgos, Leslie and Forati, Ebrahim and Fowler, Austin G. and Foxen, Brooks and Ganjam, Suhas and Garcia, Gonzalo and Gasca, Robert and Genois, Élie and Giang, William and Gilboa, Dar and Gosula, Raja and Grajales Dau, Alejandro and Graumann, Dietrich and Ha, Tan and Habegger, Steve and Hansen, Monica and Harrigan, Matthew P. and Harrington, Sean D. and Heslin, Stephen and Heu, Paula and Higgott, Oscar and Hiltermann, Reno and Hilton, Jeremy and Huang, Hsin-Yuan and Huff, Ashley and Huggins, William J. and Jeffrey, Evan and Jiang, Zhang and Jin, Xiaoxuan and Jones, Cody and Joshi, Chaitali and Juhas, Pavol and Kabel, Andreas and Kang, Hui and Karamlou, Amir H. and Kechedzhi, Kostyantyn and Khaire, Trupti and Khattar, Tanuj and Khezri, Mostafa and Kim, Seon and Kobrin, Bryce and Korotkov, Alexander N. and Kostritsa, Fedor and Kreikebaum, John Mark and Kurilovich, Vladislav D. and Landhuis, David and Lange-Dei, Tiano and Langley, Brandon W. and Lau, Kim-Ming and Ledford, Justin and Lee, Kenny and Lester, Brian J. and Le Guevel, Loïck and Li, Wing Yan and Lill, Alexander T. and Locharla, Aditya and Lucero, Erik and Lundahl, Daniel and Lunt, Aaron and Madhuk, Sid and Maloney, Ashley and Mandrà, Salvatore and Martin, Leigh S. and Martin, Orion and Maxfield, Cameron and McClean, Jarrod R. and Meeks, Seneca and Megrant, Anthony and Molavi, Reza and Molina, Sebastian and Montazeri, Shirin and Movassagh, Ramis and Newman, Michael and Nguyen, Anthony and Nguyen, Murray and Ni, Chia-Hung and Oas, Logan and Orosco, Raymond and Ottosson, Kristoffer and Pizzuto, Alex and Potter, Rebecca and Pritchard, Orion and Quintana, Chris and Ramachandran, Ganesh and Reagor, Matthew J. and Rhodes, David M. and Rosenberg, Eliott and Rossi, Elizabeth and Sankaragomathi, Kannan and Schurkus, Henry F. and Shearn, Michael J. and Shorter, Aaron and Shutty, Noah and Shvarts, Vladimir and Small, Spencer and Smith, W. Clarke and Springer, Sofia and Sterling, George and Suchard, Jordan and Szasz, Aaron and Sztein, Alex and Thor, Douglas and Tomita, Eifu and Torres, Alfredo and Torunbalci, M. Mert and Vaishnav, Abeer and Vargas, Justin and Vdovichev, Sergey and Vidal, Guifre and Vollgraff Heidweiller, Catherine and Waltman, Steven and Waltz, Jonathan and Wang, Shannon X. and Ware, Brayden and Weidel, Travis and White, Theodore and Wong, Kristi and Woo, Bryan W. K. and Woodson, Maddy and Xing, Cheng and Yao, Z. Jamie and Yeh, Ping and Ying, Bicheng and Yoo, Juhwan and Yosri, Noureldin and Young, Grayson and Zalcman, Adam and Zhang, Yaxing and Zhu, Ningfeng and Boixo, Sergio and Kelly, Julian and Smelyanskiy, Vadim and Neven, Hartmut and Bacon, Dave and Chen, Zijun and Klimov, Paul V. and Roushan, Pedram and Neill, Charles and Chen, Yu and Morvan, Alexis",2024.0,,,,arXiv preprint arXiv:2412.14360
Qubit Mapping and Routing tailored to Advanced Quantum ISAs: Not as Costly as You Think,2511.04608v2,zhou2024halma,\cite{zhou2024halma},Halma: a routing-based technique for defect mitigation in quantum error correction,,,True,False,"Zhou, Runshi and Zhang, Fang and Kong, Linghang and Chen, Jianxin",2024.0,,,,arXiv preprint arXiv:2412.21000
Uncertainty Guided Online Ensemble for Non-stationary Data Streams in Fusion Science,2511.02092v1,Kates-Harbeck2019,\cite{Kates-Harbeck2019},Predicting disruptive instabilities in controlled fusion plasmas through deep learning,,,True,False,"Kates-Harbeck, Julian
and Svyatkovskiy, Alexey
and Tang, William",2019.0,Apr,https://doi.org/10.1038/s41586-019-1116-4,10.1038/s41586-019-1116-4,Nature
Uncertainty Guided Online Ensemble for Non-stationary Data Streams in Fusion Science,2511.02092v1,https://doi.org/10.1002/ctpp.202200095,\cite{https://doi.org/10.1002/ctpp.202200095},Implementation of AI/Deep Learning Disruption Predictor into a Plasma Control System,https://arxiv.org/abs/2204.01289v1,"This paper reports on advances to the state-of-the-art deep-learning disruption prediction models based on the Fusion Recurrent Neural Network (FRNN) originally introduced a 2019 Nature publication. In particular, the predictor now features not only the disruption score, as an indicator of the probability of an imminent disruption, but also a sensitivity score in real-time to indicate the underlying reasons for the imminent disruption. This adds valuable physics-interpretability for the deep-learning model and can provide helpful guidance for control actuators now that it is fully implemented into a modern Plasma Control System (PCS). The advance is a significant step forward in moving from modern deep-learning disruption prediction to real-time control and brings novel AI-enabled capabilities relevant for application to the future burning plasma ITER system. Our analyses use large amounts of data from JET and DIII-D vetted in the earlier NATURE publication. In addition to when a shot is predicted to disrupt, this paper addresses reasons why by carrying out sensitivity studies. FRNN is accordingly extended to use many more channels of information, including measured DIII-D signals such as (i) the n1rms signal that is correlated with the n =1 modes with finite frequency, including neoclassical tearing mode and sawtooth dynamics, (ii) the bolometer data indicative of plasma impurity content, and (iii) q-min, the minimum value of the safety factor relevant to the key physics of kink modes. The additional channels and interpretability features expand the ability of the deep learning FRNN software to provide information about disruption subcategories as well as more precise and direct guidance for the actuators in a plasma control system.",True,True,"Tang, William and Dong, Ge and Barr, Jayson and Erickson, Keith and Conlin, Rory and Boyer, Dan and Kates-Harbeck, Julian and Felker, Kyle and Rea, Cristina and Logan, Nikolas and Svyatkovskiy, Alexey and Feibush, Eliot and Abbatte, Joseph and Clement, Mitchell and Grierson, Brian and Nazikian, Raffi and Lin, Zhihong and Eldon, David and Moser, Auna and Maslov, Mikhail",2023.0,,https://onlinelibrary.wiley.com/doi/abs/10.1002/ctpp.202200095,https://doi.org/10.1002/ctpp.202200095,Contributions to Plasma Physics
Uncertainty Guided Online Ensemble for Non-stationary Data Streams in Fusion Science,2511.02092v1,Tinguely_2019,\cite{Tinguely_2019},An application of survival analysis to disruption prediction via Random Forests,https://arxiv.org/abs/1907.04291v1,"One of the most pressing challenges facing the fusion community is adequately mitigating or, even better, avoiding disruptions of tokamak plasmas. However, before this can be done, disruptions must first be predicted with sufficient warning time to actuate a response. The established field of survival analysis provides a convenient statistical framework for time-to-event (i.e. time-to-disruption) studies. This paper demonstrates the integration of an existing disruption prediction machine learning algorithm with the Kaplan-Meier estimator of survival probability. Specifically discussed are the implied warning times from binary classification of disruption databases and the interpretation of output signals from Random Forest algorithms trained and tested on these databases. This survival analysis approach is applied to both smooth and noisy test data to highlight important features of the survival and hazard functions. In addition, this method is applied to three Alcator C-Mod plasma discharges and compared to a threshold-based scheme for triggering alarms. In one case, both techniques successfully predict the disruption; although, in another, neither warns of the impending disruption with enough time to mitigate. For the final discharge, the survival analysis approach could avoid the false alarm triggered by the threshold method. Limitations of this analysis and opportunities for future work are also presented.",True,True,"Tinguely, R A and Montes, K J and Rea, C and Sweeney, R and Granetz, R S",2019.0,,http://dx.doi.org/10.1088/1361-6587/ab32fc,10.1088/1361-6587/ab32fc,Plasma Physics and Controlled Fusion
Uncertainty Guided Online Ensemble for Non-stationary Data Streams in Fusion Science,2511.02092v1,Zhu_2021,\cite{Zhu_2021},Scenario adaptive disruption prediction study for next generation burning-plasma tokamaks,https://arxiv.org/abs/2109.08956v1,"Next generation high performance (HP) tokamaks risk damage from unmitigated disruptions at high current and power. Achieving reliable disruption prediction for a device's HP operation based on its low performance (LP) data is key to success. In this letter, through explorative data analysis and dedicated numerical experiments on multiple existing tokamaks, we demonstrate how the operational regimes of tokamaks can affect the power of a trained disruption predictor. First, our results suggest data-driven disruption predictors trained on abundant LP discharges work poorly on the HP regime of the same tokamak, which is a consequence of the distinct distributions of the tightly correlated signals related to disruptions in these two regimes. Second, we find that matching operational parameters among tokamaks strongly improves cross-machine accuracy which implies our model learns from the underlying scalings of dimensionless physics parameters like q_{95}, β_{p} and confirms the importance of these parameters in disruption physics and cross machine domain matching from the data-driven perspective. Finally, our results show how in the absence of HP data from the target devices, the best predictivity of the HP regime for the target machine can be achieved by combining LP data from the target with HP data from other machines. These results provide a possible disruption predictor development strategy for next generation tokamaks, such as ITER and SPARC, and highlight the importance of developing on existing machines baseline scenario discharges of future tokamaks to collect more relevant disruptive data.",True,True,"Zhu, J. and Rea, C. and Granetz, R.S. and Marmar, E.S. and Montes, K.J. and Sweeney, R. and Tinguely, R.A. and Chen, D.L. and Shen, B. and Xiao, B.J. and Humphreys, D. and Barr, J. and Meneghini, O.",2021.0,,http://dx.doi.org/10.1088/1741-4326/ac28ae,10.1088/1741-4326/ac28ae,Nuclear Fusion
Uncertainty Guided Online Ensemble for Non-stationary Data Streams in Fusion Science,2511.02092v1,Nemani2023uqml,\cite{Nemani2023uqml},Uncertainty quantification in machine learning for engineering design and health prognostics: A tutorial,,,True,False,Venkat Nemani and Luca Biggio and Xun Huan and Zhen Hu and Olga Fink and Anh Tran and Yan Wang and Xiaoge Zhang and Chao Hu,2023.0,,https://www.sciencedirect.com/science/article/pii/S0888327023007045,10.1016/j.ymssp.2023.110796,Mechanical Systems and Signal Processing
Uncertainty Guided Online Ensemble for Non-stationary Data Streams in Fusion Science,2511.02092v1,blundell2015weight,\cite{blundell2015weight},Weight Uncertainty in Neural Networks,https://arxiv.org/abs/1505.05424v2,"We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.",True,True,"Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan",2015.0,,,,
Uncertainty Guided Online Ensemble for Non-stationary Data Streams in Fusion Science,2511.02092v1,gal2016dropout,\cite{gal2016dropout},Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning,https://arxiv.org/abs/1506.02142v6,"Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.",True,True,"Gal, Yarin and Ghahramani, Zoubin",2016.0,20--22 Jun,https://proceedings.mlr.press/v48/gal16.html,,
Uncertainty Guided Online Ensemble for Non-stationary Data Streams in Fusion Science,2511.02092v1,lakshminarayanan2017simple,\cite{lakshminarayanan2017simple},Simple and scalable distributed Bayesian neural networks,,,True,False,"Lakshminarayanan, B. and Pritzel, A. and Blundell, C.",2017.0,,,,arXiv preprint arXiv:1703.01957
Uncertainty Guided Online Ensemble for Non-stationary Data Streams in Fusion Science,2511.02092v1,nam2025machine,\cite{nam2025machine},Machine learning based energy confinement time extrapolation via multi-tokamak database,,,True,False,Hyungkeun Nam and Jaemin Seo,,,,,"The developed model shows improved predictive capability compared to existing approaches and offers a powerful tool for exploring and optimizing future reactor designs and operational scenarios, including the crucial impact of factors like wall materials"
Uncertainty Guided Online Ensemble for Non-stationary Data Streams in Fusion Science,2511.02092v1,koenker_2005,\cite{koenker_2005},Asymptotic Theory of Quantile Regression,,,True,False,"Koenker, Roger",2005.0,,,10.1017/CBO9780511754098.005,
Uncertainty Guided Online Ensemble for Non-stationary Data Streams in Fusion Science,2511.02092v1,rajput2020uncertainty,\cite{rajput2020uncertainty},Uncertainty Aware Deep Learning for Particle Accelerators,,,True,False,Kishansingh Rajput and Malachi Schram and Karthik Somayaji,2023.0,,https://arxiv.org/abs/2309.14502,,
Uncertainty Guided Online Ensemble for Non-stationary Data Streams in Fusion Science,2511.02092v1,PhysRevAccelBeams.26.044602,\cite{PhysRevAccelBeams.26.044602},Uncertainty aware machine-learning-based surrogate models for particle accelerators: Study at the Fermilab Booster Accelerator Complex,,,True,False,"Schram, Malachi and Rajput, Kishansingh and NS, Karthik Somayaji and Li, Peng and St. John, Jason and Sharma, Himanshu",2023.0,Apr,https://link.aps.org/doi/10.1103/PhysRevAccelBeams.26.044602,10.1103/PhysRevAccelBeams.26.044602,Phys. Rev. Accel. Beams
Uncertainty Guided Online Ensemble for Non-stationary Data Streams in Fusion Science,2511.02092v1,IJCAI2022,\cite{IJCAI2022},Recent Advances in Concept Drift Adaptation Methods for Deep Learning,,,True,False,Yuanwei Jin and et al.,2022.0,,https://www.ijcai.org/proceedings/2022/0788.pdf,,
Uncertainty Guided Online Ensemble for Non-stationary Data Streams in Fusion Science,2511.02092v1,Zhao2024,\cite{Zhao2024},Proactive Model Adaptation Against Concept Drift for Online Time Series Forecasting,https://arxiv.org/abs/2412.08435v4,"Time series forecasting always faces the challenge of concept drift, where data distributions evolve over time, leading to a decline in forecast model performance. Existing solutions are based on online learning, which continually organize recent time series observations as new training samples and update model parameters according to the forecasting feedback on recent data. However, they overlook a critical issue: obtaining ground-truth future values of each sample should be delayed until after the forecast horizon. This delay creates a temporal gap between the training samples and the test sample. Our empirical analysis reveals that the gap can introduce concept drift, causing forecast models to adapt to outdated concepts. In this paper, we present Proceed, a novel proactive model adaptation framework for online time series forecasting. Proceed first estimates the concept drift between the recently used training samples and the current test sample. It then employs an adaptation generator to efficiently translate the estimated drift into parameter adjustments, proactively adapting the model to the test sample. To enhance the generalization capability of the framework, Proceed is trained on synthetic diverse concept drifts. Extensive experiments on five real-world datasets across various forecast models demonstrate that Proceed brings more performance improvements than the state-of-the-art online learning methods, significantly facilitating forecast models' resilience against concept drifts. Code is available at https://github.com/SJTU-DMTai/OnlineTSF.",True,True,Lifan Zhao and Yanyan Shen,2024.0,,https://arxiv.org/abs/2412.08435,,arXiv preprint arXiv:2412.08435
Uncertainty Guided Online Ensemble for Non-stationary Data Streams in Fusion Science,2511.02092v1,YangShami2021,\cite{YangShami2021},A Lightweight Concept Drift Detection and Adaptation Framework for IoT Data Streams,,,True,False,Li Yang and Abdallah Shami,2021.0,,,,
Uncertainties in Physics-informed Inverse Problems: The Hidden Risk in Scientific AI,2511.04564v1,raissi2019physics,\cite{raissi2019physics},Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations,,,True,False,"Raissi, Maziar and Perdikaris, Paris and Karniadakis, George E",2019.0,,,,Journal of Computational Physics
Uncertainties in Physics-informed Inverse Problems: The Hidden Risk in Scientific AI,2511.04564v1,greydanus2019hamiltonian,\cite{greydanus2019hamiltonian},Hamiltonian Neural Networks,https://arxiv.org/abs/1906.01563v3,"Even though neural networks enjoy widespread use, they still struggle to learn the basic laws of physics. How might we endow them with better inductive biases? In this paper, we draw inspiration from Hamiltonian mechanics to train models that learn and respect exact conservation laws in an unsupervised manner. We evaluate our models on problems where conservation of energy is important, including the two-body problem and pixel observations of a pendulum. Our model trains faster and generalizes better than a regular neural network. An interesting side effect is that our model is perfectly reversible in time.",True,True,"Greydanus, Samuel and Dzamba, Misko and Yosinski, Jason",2019.0,,,,
Uncertainties in Physics-informed Inverse Problems: The Hidden Risk in Scientific AI,2511.04564v1,karniadakis2021physics,\cite{karniadakis2021physics},Physics-informed machine learning,,,True,False,"Karniadakis, George Em and Kevrekidis, Ioannis G and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu",2021.0,,,,Nature Reviews Physics
Uncertainties in Physics-informed Inverse Problems: The Hidden Risk in Scientific AI,2511.04564v1,raissi2018hidden,\cite{raissi2018hidden},Hidden Physics Models: Machine Learning of Nonlinear Partial Differential Equations,https://arxiv.org/abs/1708.00588v2,"While there is currently a lot of enthusiasm about ""big data"", useful data is usually ""small"" and expensive to acquire. In this paper, we present a new paradigm of learning partial differential equations from {\em small} data. In particular, we introduce \emph{hidden physics models}, which are essentially data-efficient learning machines capable of leveraging the underlying laws of physics, expressed by time dependent and nonlinear partial differential equations, to extract patterns from high-dimensional data generated from experiments. The proposed methodology may be applied to the problem of learning, system identification, or data-driven discovery of partial differential equations. Our framework relies on Gaussian processes, a powerful tool for probabilistic inference over functions, that enables us to strike a balance between model complexity and data fitting. The effectiveness of the proposed approach is demonstrated through a variety of canonical problems, spanning a number of scientific domains, including the Navier-Stokes, Schrödinger, Kuramoto-Sivashinsky, and time dependent linear fractional equations. The methodology provides a promising new direction for harnessing the long-standing developments of classical methods in applied mathematics and mathematical physics to design learning machines with the ability to operate in complex domains without requiring large quantities of data.",True,True,"Raissi, Maziar and Perdikaris, Paris and Karniadakis, George E",2018.0,,,,Journal of Computational Physics
Uncertainties in Physics-informed Inverse Problems: The Hidden Risk in Scientific AI,2511.04564v1,yang2021b,\cite{yang2021b},{B-PINNs}: Bayesian physics-informed neural networks for forward and inverse {PDE} problems with noisy data,,,True,False,"Yang, Liu and Meng, Xuhui and Karniadakis, George Em",2021.0,,,,Journal of Computational Physics
Uncertainties in Physics-informed Inverse Problems: The Hidden Risk in Scientific AI,2511.04564v1,calderon1980inverse,\cite{calderon1980inverse},On an inverse boundary value problem,,,True,False,"Calder{\'o}n, Alberto P",1980.0,,,,Seminar on Numerical Analysis and its Applications to Continuum Physics
Uncertainties in Physics-informed Inverse Problems: The Hidden Risk in Scientific AI,2511.04564v1,sylvester1987global,\cite{sylvester1987global},A global uniqueness theorem for an inverse boundary value problem,,,True,False,"Sylvester, John and Uhlmann, Gunther",1987.0,,,,Annals of Mathematics
Uncertainties in Physics-informed Inverse Problems: The Hidden Risk in Scientific AI,2511.04564v1,yamamoto2009carleman,\cite{yamamoto2009carleman},Carleman estimates for parabolic equations and applications,,,True,False,"Yamamoto, Masahiro",2009.0,,,,Inverse Problems
Uncertainties in Physics-informed Inverse Problems: The Hidden Risk in Scientific AI,2511.04564v1,bellassoued2017carleman,\cite{bellassoued2017carleman},{Carleman Estimates and Inverse Problems for Hyperbolic Equations},,,True,False,Mourad Bellassoued and Masahiro Yamamoto,2017.0,,,10.1007/978-4-431-56600-7,
Uncertainties in Physics-informed Inverse Problems: The Hidden Risk in Scientific AI,2511.04564v1,krishnapriyan2021characterizing,\cite{krishnapriyan2021characterizing},Characterizing possible failure modes in physics-informed neural networks,https://arxiv.org/abs/2109.01050v2,"Recent work in scientific machine learning has developed so-called physics-informed neural network (PINN) models. The typical approach is to incorporate physical domain knowledge as soft constraints on an empirical loss function and use existing machine learning methodologies to train the model. We demonstrate that, while existing PINN methodologies can learn good models for relatively trivial problems, they can easily fail to learn relevant physical phenomena for even slightly more complex problems. In particular, we analyze several distinct situations of widespread physical interest, including learning differential equations with convection, reaction, and diffusion operators. We provide evidence that the soft regularization in PINNs, which involves PDE-based differential operators, can introduce a number of subtle problems, including making the problem more ill-conditioned. Importantly, we show that these possible failure modes are not due to the lack of expressivity in the NN architecture, but that the PINN's setup makes the loss landscape very hard to optimize. We then describe two promising solutions to address these failure modes. The first approach is to use curriculum regularization, where the PINN's loss term starts from a simple PDE regularization, and becomes progressively more complex as the NN gets trained. The second approach is to pose the problem as a sequence-to-sequence learning task, rather than learning to predict the entire space-time at once. Extensive testing shows that we can achieve up to 1-2 orders of magnitude lower error with these methods as compared to regular PINN training.",True,True,"Krishnapriyan, Aditi and Gholami, Amir and Zhe, Sujeeth and Kirby, Robert M and Mahoney, Michael W",2021.0,,,,Advances in Neural Information Processing Systems (NeurIPS)
Uncertainties in Physics-informed Inverse Problems: The Hidden Risk in Scientific AI,2511.04564v1,mishra2022estimates,\cite{mishra2022estimates},Estimates of uncertainty in physics-informed learning of {PDEs},,,True,False,"Mishra, Siddhartha and Molinaro, Raffaele and Schwab, Christoph",2022.0,,,,Mathematics of Computation
"Learning Pseudorandom Numbers with Transformers: Permuted Congruential Generators, Curricula, and Interpretability",2510.26792v1,Bouillaguet_Martinez_Sauvage_2020,\cite{Bouillaguet_Martinez_Sauvage_2020},Practical seed-recovery for the PCG Pseudo-Random Number Generator,,,True,False,"Bouillaguet, Charles and Martinez, Florette and Sauvage, Julia",2020.0,Sep.,https://tosc.iacr.org/index.php/ToSC/article/view/8700,10.13154/tosc.v2020.i3.175-196,IACR Transactions on Symmetric Cryptology
"Learning Pseudorandom Numbers with Transformers: Permuted Congruential Generators, Curricula, and Interpretability",2510.26792v1,10.1145/1553374.1553380,\cite{10.1145/1553374.1553380},Curriculum Abductive Learning,https://arxiv.org/abs/2505.12275v2,"Abductive Learning (ABL) integrates machine learning with logical reasoning in a loop: a learning model predicts symbolic concept labels from raw inputs, which are revised through abduction using domain knowledge and then fed back for retraining. However, due to the nondeterminism of abduction, the training process often suffers from instability, especially when the knowledge base is large and complex, resulting in a prohibitively large abduction space. While prior works focus on improving candidate selection within this space, they typically treat the knowledge base as a static black box. In this work, we propose Curriculum Abductive Learning (C-ABL), a method that explicitly leverages the internal structure of the knowledge base to address the ABL training challenges. C-ABL partitions the knowledge base into a sequence of sub-bases, progressively introduced during training. This reduces the abduction space throughout training and enables the model to incorporate logic in a stepwise, smooth way. Experiments across multiple tasks show that C-ABL outperforms previous ABL implementations, significantly improves training stability, convergence speed, and final accuracy, especially under complex knowledge setting.",True,True,"Bengio, Yoshua and Louradour, J\'{e}r\^{o}me and Collobert, Ronan and Weston, Jason",2009.0,,https://doi.org/10.1145/1553374.1553380,10.1145/1553374.1553380,
"Learning Pseudorandom Numbers with Transformers: Permuted Congruential Generators, Curricula, and Interpretability",2510.26792v1,wu2021curriculawork,\cite{wu2021curriculawork},When Do Curricula Work?,https://arxiv.org/abs/2012.03107v3,"Inspired by human learning, researchers have proposed ordering examples during training based on their difficulty. Both curriculum learning, exposing a network to easier examples early in training, and anti-curriculum learning, showing the most difficult examples first, have been suggested as improvements to the standard i.i.d. training. In this work, we set out to investigate the relative benefits of ordered learning. We first investigate the \emph{implicit curricula} resulting from architectural and optimization bias and find that samples are learned in a highly consistent order. Next, to quantify the benefit of \emph{explicit curricula}, we conduct extensive experiments over thousands of orderings spanning three kinds of learning: curriculum, anti-curriculum, and random-curriculum -- in which the size of the training dataset is dynamically increased over time, but the examples are randomly ordered. We find that for standard benchmark datasets, curricula have only marginal benefits, and that randomly ordered samples perform as well or better than curricula and anti-curricula, suggesting that any benefit is entirely due to the dynamic training set size. Inspired by common use cases of curriculum learning in practice, we investigate the role of limited training time budget and noisy data in the success of curriculum learning. Our experiments demonstrate that curriculum, but not anti-curriculum can indeed improve the performance either with limited training time budget or in existence of noisy data.",True,True,Xiaoxia Wu and Ethan Dyer and Behnam Neyshabur,2021.0,,https://arxiv.org/abs/2012.03107,,
"Learning Pseudorandom Numbers with Transformers: Permuted Congruential Generators, Curricula, and Interpretability",2510.26792v1,garg2023transformerslearnincontextcase,\cite{garg2023transformerslearnincontextcase},What Can Transformers Learn In-Context? A Case Study of Simple Function Classes,https://arxiv.org/abs/2208.01066v3,"In-context learning refers to the ability of a model to condition on a prompt sequence consisting of in-context examples (input-output pairs corresponding to some task) along with a new query input, and generate the corresponding output. Crucially, in-context learning happens only at inference time without any parameter updates to the model. While large language models such as GPT-3 exhibit some ability to perform in-context learning, it is unclear what the relationship is between tasks on which this succeeds and what is present in the training data. To make progress towards understanding in-context learning, we consider the well-defined problem of training a model to in-context learn a function class (e.g., linear functions): that is, given data derived from some functions in the class, can we train a model to in-context learn ""most"" functions from this class? We show empirically that standard Transformers can be trained from scratch to perform in-context learning of linear functions -- that is, the trained model is able to learn unseen linear functions from in-context examples with performance comparable to the optimal least squares estimator. In fact, in-context learning is possible even under two forms of distribution shift: (i) between the training data of the model and inference-time prompts, and (ii) between the in-context examples and the query input during inference. We also show that we can train Transformers to in-context learn more complex function classes -- namely sparse linear functions, two-layer neural networks, and decision trees -- with performance that matches or exceeds task-specific learning algorithms. Our code and models are available at https://github.com/dtsip/in-context-learning .",True,True,Shivam Garg and Dimitris Tsipras and Percy Liang and Gregory Valiant,2023.0,,https://arxiv.org/abs/2208.01066,,
"Learning Pseudorandom Numbers with Transformers: Permuted Congruential Generators, Curricula, and Interpretability",2510.26792v1,saxena2024making,\cite{saxena2024making},Making Hard Problems Easier with Custom Data Distributions and Loss Regularization: A Case Study in Modular Arithmetic,,,True,False,"Saxena, Eshika and Alfarano, Alberto and Charton, Fran{\c{c}}ois and Allen-Zhu, Zeyuan and Wenger, Emily and Lauter, Kristin",2024.0,,,,arXiv preprint arXiv:2410.03569
"Learning Pseudorandom Numbers with Transformers: Permuted Congruential Generators, Curricula, and Interpretability",2510.26792v1,rivest1991cryptography,\cite{rivest1991cryptography},Cryptography and machine learning,,,True,False,"Rivest, Ronald L",1991.0,,,,
"Learning Pseudorandom Numbers with Transformers: Permuted Congruential Generators, Curricula, and Interpretability",2510.26792v1,wenger2022salsa,\cite{wenger2022salsa},Salsa: Attacking lattice cryptography with transformers,,,True,False,"Wenger, Emily and Chen, Mingjie and Charton, Francois and Lauter, Kristin E",2022.0,,,,Advances in Neural Information Processing Systems
"Learning Pseudorandom Numbers with Transformers: Permuted Congruential Generators, Curricula, and Interpretability",2510.26792v1,tao2025howtransformerspredictpseudorandom,\cite{tao2025howtransformerspredictpseudorandom},(How) Can Transformers Predict Pseudo-Random Numbers?,https://arxiv.org/abs/2502.10390v2,"Transformers excel at discovering patterns in sequential data, yet their fundamental limitations and learning mechanisms remain crucial topics of investigation. In this paper, we study the ability of Transformers to learn pseudo-random number sequences from linear congruential generators (LCGs), defined by the recurrence relation $x_{t+1} = a x_t + c \;\mathrm{mod}\; m$. We find that with sufficient architectural capacity and training data variety, Transformers can perform in-context prediction of LCG sequences with unseen moduli ($m$) and parameters ($a,c$). By analyzing the embedding layers and attention patterns, we uncover how Transformers develop algorithmic structures to learn these sequences in two scenarios of increasing complexity. First, we investigate how Transformers learn LCG sequences with unseen ($a, c$) but fixed modulus; and demonstrate successful learning up to $m = 2^{32}$. We find that models learn to factorize $m$ and utilize digit-wise number representations to make sequential predictions. In the second, more challenging scenario of unseen moduli, we show that Transformers can generalize to unseen moduli up to $m_{\text{test}} = 2^{16}$. In this case, the model employs a two-step strategy: first estimating the unknown modulus from the context, then utilizing prime factorizations to generate predictions. For this task, we observe a sharp transition in the accuracy at a critical depth $d= 3$. We also find that the number of in-context sequence elements needed to reach high accuracy scales sublinearly with the modulus.",True,True,Tao Tao and Darshil Doshi and Dayal Singh Kalra and Tianyu He and Maissam Barkeshli,2025.0,,https://arxiv.org/abs/2502.10390,,
"Learning Pseudorandom Numbers with Transformers: Permuted Congruential Generators, Curricula, and Interpretability",2510.26792v1,power2022grokkinggeneralizationoverfittingsmall,\cite{power2022grokkinggeneralizationoverfittingsmall},Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets,https://arxiv.org/abs/2201.02177v1,"In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of ""grokking"" a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.",True,True,Alethea Power and Yuri Burda and Harri Edwards and Igor Babuschkin and Vedant Misra,2022.0,,https://arxiv.org/abs/2201.02177,,
"Learning Pseudorandom Numbers with Transformers: Permuted Congruential Generators, Curricula, and Interpretability",2510.26792v1,gromov2023grokkingmodulararithmetic,\cite{gromov2023grokkingmodulararithmetic},Grokking modular arithmetic,https://arxiv.org/abs/2301.02679v1,"We present a simple neural network that can learn modular arithmetic tasks and exhibits a sudden jump in generalization known as ``grokking''. Concretely, we present (i) fully-connected two-layer networks that exhibit grokking on various modular arithmetic tasks under vanilla gradient descent with the MSE loss function in the absence of any regularization; (ii) evidence that grokking modular arithmetic corresponds to learning specific feature maps whose structure is determined by the task; (iii) analytic expressions for the weights -- and thus for the feature maps -- that solve a large class of modular arithmetic tasks; and (iv) evidence that these feature maps are also found by vanilla gradient descent as well as AdamW, thereby establishing complete interpretability of the representations learnt by the network.",True,True,Andrey Gromov,2023.0,,https://arxiv.org/abs/2301.02679,,
"Learning Pseudorandom Numbers with Transformers: Permuted Congruential Generators, Curricula, and Interpretability",2510.26792v1,zhong2023clockpizzastoriesmechanistic,\cite{zhong2023clockpizzastoriesmechanistic},The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks,,,True,False,Ziqian Zhong and Ziming Liu and Max Tegmark and Jacob Andreas,2023.0,,https://arxiv.org/abs/2306.17844,,
"Learning Pseudorandom Numbers with Transformers: Permuted Congruential Generators, Curricula, and Interpretability",2510.26792v1,nanda2023progressmeasuresgrokkingmechanistic,\cite{nanda2023progressmeasuresgrokkingmechanistic},Progress measures for grokking via mechanistic interpretability,,,True,False,Neel Nanda and Lawrence Chan and Tom Lieberum and Jess Smith and Jacob Steinhardt,2023.0,,https://arxiv.org/abs/2301.05217,,
"Learning Pseudorandom Numbers with Transformers: Permuted Congruential Generators, Curricula, and Interpretability",2510.26792v1,doshi2024grokgrokdisentanglinggeneralization,\cite{doshi2024grokgrokdisentanglinggeneralization},To grok or not to grok: Disentangling generalization and memorization on corrupted algorithmic datasets,,,True,False,Darshil Doshi and Aritra Das and Tianyu He and Andrey Gromov,2024.0,,https://arxiv.org/abs/2310.13061,,
"Learning Pseudorandom Numbers with Transformers: Permuted Congruential Generators, Curricula, and Interpretability",2510.26792v1,charton2024emergentpropertiesrepeatedexamples,\cite{charton2024emergentpropertiesrepeatedexamples},Emergent properties with repeated examples,,,True,False,François Charton and Julia Kempe,2024.0,,https://arxiv.org/abs/2410.07041,,
"Learning Pseudorandom Numbers with Transformers: Permuted Congruential Generators, Curricula, and Interpretability",2510.26792v1,liu2022understandinggrokkingeffectivetheory,\cite{liu2022understandinggrokkingeffectivetheory},Towards Understanding Grokking: An Effective Theory of Representation Learning,https://arxiv.org/abs/2205.10343v2,"We aim to understand grokking, a phenomenon where models generalize long after overfitting their training set. We present both a microscopic analysis anchored by an effective theory and a macroscopic analysis of phase diagrams describing learning performance across hyperparameters. We find that generalization originates from structured representations whose training dynamics and dependence on training set size can be predicted by our effective theory in a toy setting. We observe empirically the presence of four learning phases: comprehension, grokking, memorization, and confusion. We find representation learning to occur only in a ""Goldilocks zone"" (including comprehension and grokking) between memorization and confusion. We find on transformers the grokking phase stays closer to the memorization phase (compared to the comprehension phase), leading to delayed generalization. The Goldilocks phase is reminiscent of ""intelligence from starvation"" in Darwinian evolution, where resource limitations drive discovery of more efficient solutions. This study not only provides intuitive explanations of the origin of grokking, but also highlights the usefulness of physics-inspired tools, e.g., effective theories and phase diagrams, for understanding deep learning.",True,True,Ziming Liu and Ouail Kitouni and Niklas Nolte and Eric J. Michaud and Max Tegmark and Mike Williams,2022.0,,https://arxiv.org/abs/2205.10343,,
"Learning Pseudorandom Numbers with Transformers: Permuted Congruential Generators, Curricula, and Interpretability",2510.26792v1,LearnToGrok,\cite{LearnToGrok},Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks,https://arxiv.org/abs/2406.02550v2,"Large language models can solve tasks that were not present in the training set. This capability is believed to be due to in-context learning and skill composition. In this work, we study the emergence of in-context learning and skill composition in a collection of modular arithmetic tasks. Specifically, we consider a finite collection of linear modular functions $z = a \, x + b \, y \;\mathrm{mod}\; p$ labeled by the vector $(a, b) \in \mathbb{Z}_p^2$. We use some of these tasks for pre-training and the rest for out-of-distribution testing. We empirically show that a GPT-style transformer exhibits a transition from in-distribution to out-of-distribution generalization as the number of pre-training tasks increases. We find that the smallest model capable of out-of-distribution generalization requires two transformer blocks, while for deeper models, the out-of-distribution generalization phase is \emph{transient}, necessitating early stopping. Finally, we perform an interpretability study of the pre-trained models, revealing highly structured representations in both attention heads and MLPs; and discuss the learned algorithms. Notably, we find an algorithmic shift in deeper models, as we go from few to many in-context examples.",True,True,"He, Tianyu and Doshi, Darshil and Das, Aritra and Gromov, Andrey",2024.0,,https://proceedings.neurips.cc/paper_files/paper/2024/file/17d60fef592086d1a5cb136f1946df59-Paper-Conference.pdf,,
Towards Continuous-variable Quantum Neural Networks for Biomedical Imaging,2511.02051v1,Schuld2018,\cite{Schuld2018},Supervised Learning with Quantum Computers,,,True,False,Maria Schuld and Francesco Petruccione,2018.0,,https://link.springer.com/10.1007/978-3-319-96424-9,10.1007/978-3-319-96424-9,
Towards Continuous-variable Quantum Neural Networks for Biomedical Imaging,2511.02051v1,Schuld2019,\cite{Schuld2019},Quantum machine learning in feature Hilbert spaces,https://arxiv.org/abs/1803.07128v1,"The basic idea of quantum computing is surprisingly similar to that of kernel methods in machine learning, namely to efficiently perform computations in an intractably large Hilbert space. In this paper we explore some theoretical foundations of this link and show how it opens up a new avenue for the design of quantum machine learning algorithms. We interpret the process of encoding inputs in a quantum state as a nonlinear feature map that maps data to quantum Hilbert space. A quantum computer can now analyse the input data in this feature space. Based on this link, we discuss two approaches for building a quantum model for classification. In the first approach, the quantum device estimates inner products of quantum states to compute a classically intractable kernel. This kernel can be fed into any classical kernel method such as a support vector machine. In the second approach, we can use a variational quantum circuit as a linear model that classifies data explicitly in Hilbert space. We illustrate these ideas with a feature map based on squeezing in a continuous-variable system, and visualise the working principle with $2$-dimensional mini-benchmark datasets.",True,True,Maria Schuld and Nathan Killoran,2019.0,2,https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.122.040504,10.1103/PhysRevLett.122.040504,Physical Review Letters
Towards Continuous-variable Quantum Neural Networks for Biomedical Imaging,2511.02051v1,Sengupta2021,\cite{Sengupta2021},Quantum algorithm for quicker clinical prognostic analysis: an application and experimental study using CT scan images of COVID-19 patients,,,True,False,Kinshuk Sengupta and Praveen Ranjan Srivastava,2021.0,12,https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-021-01588-6,10.1186/S12911-021-01588-6/FIGURES/9,BMC Medical Informatics and Decision Making
Towards Continuous-variable Quantum Neural Networks for Biomedical Imaging,2511.02051v1,Abdulsalam2022,\cite{Abdulsalam2022},Explainable Heart Disease Prediction Using Ensemble-Quantum Machine Learning Approach,,,True,False,Ghada Abdulsalam and Souham Meshoul and Hadil Shaiba,2022.0,9,https://www.techscience.com/iasc/v36n1/50016.html https://www.techscience.com/iasc/v36n1/50016,10.32604/IASC.2023.032262,Intelligent Automation \& Soft Computing
Towards Continuous-variable Quantum Neural Networks for Biomedical Imaging,2511.02051v1,Sagingalieva2023,\cite{Sagingalieva2023},Hybrid Quantum Neural Network for Drug Response Prediction,,,True,False,Asel Sagingalieva and Mohammad Kordzanganeh and Nurbolat Kenbayev and Daria Kosichkina and Tatiana Tomashuk and Alexey Melnikov,2023.0,5,https://www.mdpi.com/2072-6694/15/10/2705/htm https://www.mdpi.com/2072-6694/15/10/2705,10.3390/CANCERS15102705,"Cancers 2023, Vol. 15, Page 2705"
Towards Continuous-variable Quantum Neural Networks for Biomedical Imaging,2511.02051v1,Reka2024,\cite{Reka2024},Exploring Quantum Machine Learning for Enhanced Skin Lesion Classification: A Comparative Study of Implementation Methods,,,True,False,S. Sofana Reka and H. Leela Karthikeyan and A. Jack Shakil and Prakash Venugopal and Manigandan Muniraj,2024.0,,,10.1109/ACCESS.2024.3434681,IEEE Access
Towards Continuous-variable Quantum Neural Networks for Biomedical Imaging,2511.02051v1,Ara2025,\cite{Ara2025},Hybrid quantum-classical deep learning framework for balanced multiclass diabetic retinopathy classification,,,True,False,Tabassum Ara and Ved Prakash Mishra and Manish Bali and Anuradha Yenkikar,2025.0,12,,10.1016/J.MEX.2025.103605,MethodsX
Towards Continuous-variable Quantum Neural Networks for Biomedical Imaging,2511.02051v1,Marzoug2025,\cite{Marzoug2025},Quantum-Enhanced Dual-Backbone Architecture for Accurate Gastrointestinal Disease Detection Using Endoscopic Imaging,,,True,False,Nabil Marzoug and Khidhr Halab and Othmane El Meslouhi and Zouhair Elamrani Abou Elassad and Moulay A. Akhloufi,2025.0,9,https://www.mdpi.com/2673-7426/5/3/51/htm https://www.mdpi.com/2673-7426/5/3/51,10.3390/BIOMEDINFORMATICS5030051,"BioMedInformatics 2025, Vol. 5, Page 51"
Towards Continuous-variable Quantum Neural Networks for Biomedical Imaging,2511.02051v1,Lau2017,\cite{Lau2017},Quantum machine learning over infinite dimensions,https://arxiv.org/abs/1603.06222v2,"Machine learning is a fascinating and exciting field within computer science. Recently, this excitement has been transferred to the quantum information realm. Currently, all proposals for the quantum version of machine learning utilize the finite-dimensional substrate of discrete variables. Here we generalize quantum machine learning to the more complex, but still remarkably practical, infinite-dimensional systems. We present the critical subroutines of quantum machine learning algorithms for an all-photonic continuous-variable quantum computer that achieve an exponential speedup compared to their equivalent classical counterparts. Finally, we also map out an experimental implementation which can be used as a blueprint for future photonic demonstrations.",True,True,Hoi Kwan Lau and Raphael Pooser and George Siopsis and Christian Weedbrook,2017.0,2,https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.118.080501,10.1103/PhysRevLett.118.080501,Physical Review Letters
Towards Continuous-variable Quantum Neural Networks for Biomedical Imaging,2511.02051v1,Killoran2018,\cite{Killoran2018},Continuous-variable quantum neural networks,https://arxiv.org/abs/1806.06871v1,"We introduce a general method for building neural networks on quantum computers. The quantum neural network is a variational quantum circuit built in the continuous-variable (CV) architecture, which encodes quantum information in continuous degrees of freedom such as the amplitudes of the electromagnetic field. This circuit contains a layered structure of continuously parameterized gates which is universal for CV quantum computation. Affine transformations and nonlinear activation functions, two key elements in neural networks, are enacted in the quantum network using Gaussian and non-Gaussian gates, respectively. The non-Gaussian gates provide both the nonlinearity and the universality of the model. Due to the structure of the CV model, the CV quantum neural network can encode highly nonlinear transformations while remaining completely unitary. We show how a classical network can be embedded into the quantum formalism and propose quantum versions of various specialized model such as convolutional, recurrent, and residual networks. Finally, we present numerous modeling experiments built with the Strawberry Fields software library. These experiments, including a classifier for fraud detection, a network which generates Tetris images, and a hybrid classical-quantum autoencoder, demonstrate the capability and adaptability of CV quantum neural networks.",True,True,Nathan Killoran and Thomas R. Bromley and Juan Miguel Arrazola and Maria Schuld and Nicolás Quesada and Seth Lloyd,2018.0,6,http://arxiv.org/abs/1806.06871 http://dx.doi.org/10.1103/PhysRevResearch.1.033063,10.1103/PhysRevResearch.1.033063,Physical Review Research
Towards Continuous-variable Quantum Neural Networks for Biomedical Imaging,2511.02051v1,Moody2019,\cite{Moody2019},Machine learning method for state preparation and gate synthesis on photonic quantum computers,,,True,False,Galan Moody and Volker J Sorger and Daniel J Blumenthal and al - and Chen Ding and Xiao-Yue Xu and Yun-Fei Niu and Juan Miguel Arrazola and Thomas R Bromley and Josh Izaac and Casey R Myers and Kamil Brádler and Nathan Killoran,2019.0,1,https://iopscience.iop.org/article/10.1088/2058-9565/aaf59e https://iopscience.iop.org/article/10.1088/2058-9565/aaf59e/meta,10.1088/2058-9565/AAF59E,Quantum Science and Technology
Towards Continuous-variable Quantum Neural Networks for Biomedical Imaging,2511.02051v1,Li2022,\cite{Li2022},Quantum kernels with Gaussian state encoding for machine learning,,,True,False,Long Hin Li and Dan Bo Zhang and Z. D. Wang,2022.0,6,,10.1016/J.PHYSLETA.2022.128088,Physics Letters A
Towards Continuous-variable Quantum Neural Networks for Biomedical Imaging,2511.02051v1,Ghasemian2023,\cite{Ghasemian2023},Quantum machine learning based on continuous variable single-photon states: an elementary foundation for quantum neural networks,,,True,False,Ebrahim Ghasemian and Abolhassan Razminia and Habib Rostami,2023.0,10,https://link.springer.com/article/10.1007/s11128-023-04137-4,10.1007/S11128-023-04137-4/FIGURES/9,Quantum Information Processing
Towards Continuous-variable Quantum Neural Networks for Biomedical Imaging,2511.02051v1,Kairon2021,\cite{Kairon2021},COVID-19 Outbreak Prediction Using Quantum Neural Networks,,,True,False,Pranav Kairon and Siddhartha Bhattacharyya,2021.0,,https://link.springer.com/chapter/10.1007/978-981-15-9290-4_12,10.1007/978-981-15-9290-4_12,Advances in Intelligent Systems and Computing
Towards Continuous-variable Quantum Neural Networks for Biomedical Imaging,2511.02051v1,Choe2022,\cite{Choe2022},Continuous Variable Quantum MNIST Classifiers,https://arxiv.org/abs/2204.01194v1,"In this paper, classical and continuous variable (CV) quantum neural network hybrid multiclassifiers are presented using the MNIST dataset. The combination of cutoff dimension and probability measurement method in the CV model allows a quantum circuit to produce output vectors of size equal to n raised to the power of n where n represents cutoff dimension and m, the number of qumodes. They are then translated as one-hot encoded labels, padded with an appropriate number of zeros. The total of eight different classifiers are built using 2,3,...,8 qumodes, based on the binary classifier architecture proposed in Continuous variable quantum neural networks. The displacement gate and the Kerr gate in the CV model allow for the bias addition and nonlinear activation components of classical neural networks to quantum. The classifiers are composed of a classical feedforward neural network, a quantum data encoding circuit, and a CV quantum neural network circuit. On a truncated MNIST dataset of 600 samples, a 4 qumode hybrid classifier achieves 100% training accuracy.",True,True,Sophie Choe,2022.0,4,https://arxiv.org/abs/2204.01194v1,,
Towards Continuous-variable Quantum Neural Networks for Biomedical Imaging,2511.02051v1,Anand2024,\cite{Anand2024},Time-Series Forecasting Using Continuous Variables-Based Quantum Neural Networks,,,True,False,Prabhat Anand and M. Girish Chandra and Ankit Khandelwal,2024.0,,,10.1109/COMSNETS59351.2024.10427192,"2024 16th International Conference on COMmunication Systems and NETworkS, COMSNETS 2024"
Towards Continuous-variable Quantum Neural Networks for Biomedical Imaging,2511.02051v1,medmnistv2,\cite{medmnistv2},MedMNIST v2 -- A large-scale lightweight benchmark for 2D and 3D biomedical image classification,https://arxiv.org/abs/2110.14795v2,"We introduce MedMNIST v2, a large-scale MNIST-like dataset collection of standardized biomedical images, including 12 datasets for 2D and 6 datasets for 3D. All images are pre-processed into a small size of 28x28 (2D) or 28x28x28 (3D) with the corresponding classification labels so that no background knowledge is required for users. Covering primary data modalities in biomedical images, MedMNIST v2 is designed to perform classification on lightweight 2D and 3D images with various dataset scales (from 100 to 100,000) and diverse tasks (binary/multi-class, ordinal regression, and multi-label). The resulting dataset, consisting of 708,069 2D images and 10,214 3D images in total, could support numerous research / educational purposes in biomedical image analysis, computer vision, and machine learning. We benchmark several baseline methods on MedMNIST v2, including 2D / 3D neural networks and open-source / commercial AutoML tools. The data and code are publicly available at https://medmnist.com/.",True,True,"Yang, Jiancheng and Shi, Rui and Wei, Donglai and Liu, Zequan and Zhao, Lin and Ke, Bilian and Pfister, Hanspeter and Ni, Bingbing",2023.0,,,,Scientific Data
MicroEvoEval: A Systematic Evaluation Framework for Image-Based Microstructure Evolution Prediction,2511.08955v1,chen2002phase,\cite{chen2002phase},Phase-field models for microstructure evolution,,,True,False,"Chen, Long-Qing",2002.0,,,,Annual Review of Materials Research
MicroEvoEval: A Systematic Evaluation Framework for Image-Based Microstructure Evolution Prediction,2511.08955v1,steinbach1996phase,\cite{steinbach1996phase},A phase field concept for multiphase systems,,,True,False,"Steinbach, Ingo and Pezzolla, Franco and Nestler, Britta and See{\ss}elberg, Markus and Prieler, Robert and Schmitz, Georg J and Rezende, Joao LL",1996.0,,,,Physica D: Nonlinear Phenomena
MicroEvoEval: A Systematic Evaluation Framework for Image-Based Microstructure Evolution Prediction,2511.08955v1,greenwood2018quantitative,\cite{greenwood2018quantitative},Quantitative 3D phase field modelling of solidification using next-generation adaptive mesh refinement,,,True,False,"Greenwood, Michael and Shampur, KN and Ofori-Opoku, Nana and Pinomaa, Tatu and Wang, Lei and Gurevich, Sebastian and Provatas, Nikolas",2018.0,,,,Computational Materials Science
MicroEvoEval: A Systematic Evaluation Framework for Image-Based Microstructure Evolution Prediction,2511.08955v1,guo2015solving,\cite{guo2015solving},On solving the 3-D phase field equations by employing a parallel-adaptive mesh refinement (Para-AMR) algorithm,,,True,False,"Guo, Zhipeng and Xiong, SM",2015.0,,,,Computer Physics Communications
MicroEvoEval: A Systematic Evaluation Framework for Image-Based Microstructure Evolution Prediction,2511.08955v1,acar2016markov,\cite{acar2016markov},A Markov random field approach for modeling spatio-temporal evolution of microstructures,,,True,False,"Acar, Pinar and Sundararaghavan, Veera",2016.0,,,,Modelling and Simulation in Materials Science and Engineering
MicroEvoEval: A Systematic Evaluation Framework for Image-Based Microstructure Evolution Prediction,2511.08955v1,mao2024spatiotemporal,\cite{mao2024spatiotemporal},Spatiotemporal prediction of solidified dendrites based on convolutional long-short-term neural network,,,True,False,"Mao, Hong and Xie, Chenyang and Pan, Jingwen and Cao, Qingzheng and Zhang, Xiaohong and Luo, Yun and Du, Yong and Ning, Han",2024.0,,,,Materials Today Communications
MicroEvoEval: A Systematic Evaluation Framework for Image-Based Microstructure Evolution Prediction,2511.08955v1,lanzoni2022morphological,\cite{lanzoni2022morphological},"Morphological evolution via surface diffusion learned by convolutional, recurrent neural networks: Extrapolation and prediction uncertainty",,,True,False,"Lanzoni, Daniele and Albani, Marco and Bergamaschini, Roberto and Montalenti, Francesco",2022.0,,,,Physical Review Materials
MicroEvoEval: A Systematic Evaluation Framework for Image-Based Microstructure Evolution Prediction,2511.08955v1,farizhandi2023spatiotemporal,\cite{farizhandi2023spatiotemporal},Spatiotemporal prediction of microstructure evolution with predictive recurrent neural network,,,True,False,"Farizhandi, Amir Abbas Kazemzadeh and Mamivand, Mahmood",2023.0,,,,Computational Materials Science
MicroEvoEval: A Systematic Evaluation Framework for Image-Based Microstructure Evolution Prediction,2511.08955v1,yang2021self,\cite{yang2021self},Self-supervised learning and prediction of microstructure evolution with convolutional recurrent neural networks,,,True,False,"Yang, Kaiqi and Cao, Yifan and Zhang, Youtian and Fan, Shaoxun and Tang, Ming and Aberg, Daniel and Sadigh, Babak and Zhou, Fei",2021.0,,,,Patterns
MicroEvoEval: A Systematic Evaluation Framework for Image-Based Microstructure Evolution Prediction,2511.08955v1,jing2025research,\cite{jing2025research},Research on spatiotemporal prediction model of grain microstructure evolution based on VMamba network,,,True,False,"Jing-jie, Li and Zhu, Chang-sheng and Tian-yu, Li and Zi-hao, Gao and Shuo, Liu and Hang, Cao and Jin-tao, Miao",2025.0,,,,Computational Materials Science
MicroEvoEval: A Systematic Evaluation Framework for Image-Based Microstructure Evolution Prediction,2511.08955v1,Wang2018predrnn++,\cite{Wang2018predrnn++},Predrnn++: Towards a resolution of the deep-in-time dilemma in spatiotemporal predictive learning,,,True,False,"Wang, Yunbo and Gao, Zhifeng and Long, Mingsheng and Wang, Jianmin and Yu, Philip S",2018.0,,,,
MicroEvoEval: A Systematic Evaluation Framework for Image-Based Microstructure Evolution Prediction,2511.08955v1,chang2021mau,\cite{chang2021mau},Mau: A motion-aware unit for video prediction and beyond,,,True,False,"Chang, Zheng and Zhang, Xinfeng and Wang, Shanshe and Ma, Siwei and Ye, Yan and Xinguang, Xiang and Gao, Wen",2021.0,,,,Advances in Neural Information Processing Systems
MicroEvoEval: A Systematic Evaluation Framework for Image-Based Microstructure Evolution Prediction,2511.08955v1,gao2022simvp,\cite{gao2022simvp},SimVP: Simpler yet Better Video Prediction,https://arxiv.org/abs/2206.05099v1,"From CNN, RNN, to ViT, we have witnessed remarkable advancements in video prediction, incorporating auxiliary inputs, elaborate neural architectures, and sophisticated training strategies. We admire these progresses but are confused about the necessity: is there a simple method that can perform comparably well? This paper proposes SimVP, a simple video prediction model that is completely built upon CNN and trained by MSE loss in an end-to-end fashion. Without introducing any additional tricks and complicated strategies, we can achieve state-of-the-art performance on five benchmark datasets. Through extended experiments, we demonstrate that SimVP has strong generalization and extensibility on real-world datasets. The significant reduction of training cost makes it easier to scale to complex scenarios. We believe SimVP can serve as a solid baseline to stimulate the further development of video prediction. The code is available at \href{https://github.com/gaozhangyang/SimVP-Simpler-yet-Better-Video-Prediction}{Github}.",True,True,"Gao, Zhangyang and Tan, Cheng and Wu, Lirong and Li, Stan Z",2022.0,,,,
MicroEvoEval: A Systematic Evaluation Framework for Image-Based Microstructure Evolution Prediction,2511.08955v1,tan2025simvpv2,\cite{tan2025simvpv2},SimVPv2: Towards Simple yet Powerful Spatiotemporal Predictive Learning,https://arxiv.org/abs/2211.12509v4,"Recent years have witnessed remarkable advances in spatiotemporal predictive learning, with methods incorporating auxiliary inputs, complex neural architectures, and sophisticated training strategies. While SimVP has introduced a simpler, CNN-based baseline for this task, it still relies on heavy Unet-like architectures for spatial and temporal modeling, which still suffers from high complexity and computational overhead. In this paper, we propose SimVPv2, a streamlined model that eliminates the need for Unet architectures and demonstrates that plain stacks of convolutional layers, enhanced with an efficient Gated Spatiotemporal Attention mechanism, can deliver state-of-the-art performance. SimVPv2 not only simplifies the model architecture but also improves both performance and computational efficiency. On the standard Moving MNIST benchmark, SimVPv2 achieves superior performance compared to SimVP, with fewer FLOPs, about half the training time, and 60% faster inference efficiency. Extensive experiments across eight diverse datasets, including real-world tasks such as traffic forecasting and climate prediction, further demonstrate that SimVPv2 offers a powerful yet straightforward solution, achieving robust generalization across various spatiotemporal learning scenarios. We believe the proposed SimVPv2 can serve as a solid baseline to benefit the spatiotemporal predictive learning community.",True,True,"Tan, Cheng and Gao, Zhangyang and Li, Siyuan and Li, Stan Z",2025.0,,,,IEEE Transactions on Multimedia
MicroEvoEval: A Systematic Evaluation Framework for Image-Based Microstructure Evolution Prediction,2511.08955v1,tang2024predformer,\cite{tang2024predformer},Video Prediction Transformers without Recurrence or Convolution,https://arxiv.org/abs/2410.04733v3,"Video prediction has witnessed the emergence of RNN-based models led by ConvLSTM, and CNN-based models led by SimVP. Following the significant success of ViT, recent works have integrated ViT into both RNN and CNN frameworks, achieving improved performance. While we appreciate these prior approaches, we raise a fundamental question: Is there a simpler yet more effective solution that can eliminate the high computational cost of RNNs while addressing the limited receptive fields and poor generalization of CNNs? How far can it go with a simple pure transformer model for video prediction? In this paper, we propose PredFormer, a framework entirely based on Gated Transformers. We provide a comprehensive analysis of 3D Attention in the context of video prediction. Extensive experiments demonstrate that PredFormer delivers state-of-the-art performance across four standard benchmarks. The significant improvements in both accuracy and efficiency highlight the potential of PredFormer as a strong baseline for real-world video prediction applications. The source code and trained models will be released at https://github.com/yyyujintang/PredFormer.",True,True,Yujin Tang and Lu Qi and Fei Xie and Xiangtai Li and Chao Ma and Ming-Hsuan Yang,2025.0,,https://arxiv.org/abs/2410.04733,,
MicroEvoEval: A Systematic Evaluation Framework for Image-Based Microstructure Evolution Prediction,2511.08955v1,tang2023swinlstm,\cite{tang2023swinlstm},Swinlstm: Improving spatiotemporal prediction accuracy using swin transformer and lstm,,,True,False,"Tang, Song and Li, Chuang and Zhang, Pu and Tang, RongNian",2023.0,,,,
MicroEvoEval: A Systematic Evaluation Framework for Image-Based Microstructure Evolution Prediction,2511.08955v1,tan2023temporal,\cite{tan2023temporal},Temporal Attention Unit: Towards Efficient Spatiotemporal Predictive Learning,https://arxiv.org/abs/2206.12126v3,"Spatiotemporal predictive learning aims to generate future frames by learning from historical frames. In this paper, we investigate existing methods and present a general framework of spatiotemporal predictive learning, in which the spatial encoder and decoder capture intra-frame features and the middle temporal module catches inter-frame correlations. While the mainstream methods employ recurrent units to capture long-term temporal dependencies, they suffer from low computational efficiency due to their unparallelizable architectures. To parallelize the temporal module, we propose the Temporal Attention Unit (TAU), which decomposes the temporal attention into intra-frame statical attention and inter-frame dynamical attention. Moreover, while the mean squared error loss focuses on intra-frame errors, we introduce a novel differential divergence regularization to take inter-frame variations into account. Extensive experiments demonstrate that the proposed method enables the derived model to achieve competitive performance on various spatiotemporal prediction benchmarks.",True,True,"Tan, Cheng and Gao, Zhangyang and Wu, Lirong and Xu, Yongjie and Xia, Jun and Li, Siyuan and Li, Stan Z",2023.0,,,,
MicroEvoEval: A Systematic Evaluation Framework for Image-Based Microstructure Evolution Prediction,2511.08955v1,tang2024vmrnn,\cite{tang2024vmrnn},Vmrnn: Integrating vision mamba and lstm for efficient and accurate spatiotemporal forecasting,,,True,False,"Tang, Yujin and Dong, Peijie and Tang, Zhenheng and Chu, Xiaowen and Liang, Junwei",2024.0,,,,
Rediscovering the Lunar Equation of the Centre with AI Feynman via Embedded Physical Biases,2511.09979v1,karniadakisPhysicsinformedMachineLearning2021,\cite{karniadakisPhysicsinformedMachineLearning2021},Physics-Informed Machine Learning,,,True,False,"Karniadakis, George Em and Kevrekidis, Ioannis G. and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu",,,https://doi.org/10.1038/s42254-021-00314-5,10.1038/s42254-021-00314-5,Nature Reviews Physics
Rediscovering the Lunar Equation of the Centre with AI Feynman via Embedded Physical Biases,2511.09979v1,makke2023interpretablescientificdiscoverysymbolic,\cite{makke2023interpretablescientificdiscoverysymbolic},Interpretable Scientific Discovery with Symbolic Regression: A Review,,,True,False,Nour Makke and Sanjay Chawla,2023.0,,https://arxiv.org/abs/2211.10873,,
Rediscovering the Lunar Equation of the Centre with AI Feynman via Embedded Physical Biases,2511.09979v1,oh2023geneticprogrammingbasedsymbolic,\cite{oh2023geneticprogrammingbasedsymbolic},Genetic Programming Based Symbolic Regression for Analytical Solutions to Differential Equations,https://arxiv.org/abs/2302.03175v1,"In this paper, we present a machine learning method for the discovery of analytic solutions to differential equations. The method utilizes an inherently interpretable algorithm, genetic programming based symbolic regression. Unlike conventional accuracy measures in machine learning we demonstrate the ability to recover true analytic solutions, as opposed to a numerical approximation. The method is verified by assessing its ability to recover known analytic solutions for two separate differential equations. The developed method is compared to a conventional, purely data-driven genetic programming based symbolic regression algorithm. The reliability of successful evolution of the true solution, or an algebraic equivalent, is demonstrated.",True,True,Hongsup Oh and Roman Amici and Geoffrey Bomarito and Shandian Zhe and Robert Kirby and Jacob Hochhalter,2023.0,,https://arxiv.org/abs/2302.03175,,
Rediscovering the Lunar Equation of the Centre with AI Feynman via Embedded Physical Biases,2511.09979v1,Khoo2023.1,\cite{Khoo2023.1},"Celestial Machine Learning: Discovering the Planarity, Heliocentricity, and Orbital Equation of Mars with AI Feynman",,,True,False,"Khoo, Zi-Yu and Rajiv, Gokul and Yang, Abel and Low, Jonathan Sze Choong and Bressan, Stéphane",2023.0,,http://dx.doi.org/10.1007/978-3-031-48316-5_21,10.1007/978-3-031-48316-5_21,
Rediscovering the Lunar Equation of the Centre with AI Feynman via Embedded Physical Biases,2511.09979v1,Brunton2016,\cite{Brunton2016},Discovering governing equations from data: Sparse identification of nonlinear dynamical systems,https://arxiv.org/abs/1509.03580v1,"The ability to discover physical laws and governing equations from data is one of humankind's greatest intellectual achievements. A quantitative understanding of dynamic constraints and balances in nature has facilitated rapid development of knowledge and enabled advanced technological achievements, including aircraft, combustion engines, satellites, and electrical power. In this work, we combine sparsity-promoting techniques and machine learning with nonlinear dynamical systems to discover governing physical equations from measurement data. The only assumption about the structure of the model is that there are only a few important terms that govern the dynamics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems. In particular, we use sparse regression to determine the fewest terms in the dynamic governing equations required to accurately represent the data. The resulting models are parsimonious, balancing model complexity with descriptive ability while avoiding overfitting. We demonstrate the algorithm on a wide range of problems, from simple canonical systems, including linear and nonlinear oscillators and the chaotic Lorenz system, to the fluid vortex shedding behind an obstacle. The fluid example illustrates the ability of this method to discover the underlying dynamics of a system that took experts in the community nearly 30 years to resolve. We also show that this method generalizes to parameterized, time-varying, or externally forced systems.",True,True,"Brunton, Steven L. and Proctor, Joshua L. and Kutz, J. Nathan",2016.0,,http://dx.doi.org/10.1073/pnas.1517384113,10.1073/pnas.1517384113,Proceedings of the National Academy of Sciences
Rediscovering the Lunar Equation of the Centre with AI Feynman via Embedded Physical Biases,2511.09979v1,Lemos2023,\cite{Lemos2023},{Rediscovering orbital mechanics with machine learning},,,True,False,"Lemos, Pablo and Jeffrey, Niall and Cranmer, Miles and Ho, Shirley and Battaglia, Peter",2023.0,,,10.1088/2632-2153/acfa63,Machine Learning: Science and Technology
Rediscovering the Lunar Equation of the Centre with AI Feynman via Embedded Physical Biases,2511.09979v1,cranmer2023interpretablemachinelearningscience,\cite{cranmer2023interpretablemachinelearningscience},Interpretable Machine Learning for Science with PySR and SymbolicRegression.jl,,,True,False,Miles Cranmer,2023.0,,https://arxiv.org/abs/2305.01582,,
Rediscovering the Lunar Equation of the Centre with AI Feynman via Embedded Physical Biases,2511.09979v1,udrescu2020aifeynmanphysicsinspiredmethod,\cite{udrescu2020aifeynmanphysicsinspiredmethod},AI Feynman: a Physics-Inspired Method for Symbolic Regression,,,True,False,Silviu-Marian Udrescu and Max Tegmark,2020.0,,https://arxiv.org/abs/1905.11481,,
Space-Time Optimisations for Early Fault-Tolerant Quantum Computation,2511.08848v1,Herr2017,\cite{Herr2017},Optimization of Lattice Surgery is NP-Hard,https://arxiv.org/abs/1702.00591v3,"The traditional method for computation in either the surface code or in the Raussendorf model is the creation of holes or ""defects"" within the encoded lattice of qubits that are manipulated via topological braiding to enact logic gates. However, this is not the only way to achieve universal, fault-tolerant computation. In this work, we focus on the Lattice Surgery representation, which realizes transversal logic operations without destroying the intrinsic 2D nearest-neighbor properties of the braid-based surface code and achieves universality without defects and braid based logic. For both techniques there are open questions regarding the compilation and resource optimization of quantum circuits. Optimization in braid-based logic is proving to be difficult and the classical complexity associated with this problem has yet to be determined. In the context of lattice-surgery-based logic, we can introduce an optimality condition, which corresponds to a circuit with the lowest resource requirements in terms of physical qubits and computational time, and prove that the complexity of optimizing a quantum circuit in the lattice surgery model is NP-hard.",True,True,"Herr, Daniel
and Nori, Franco
and Devitt, Simon J.",2017.0,Sep,https://doi.org/10.1038/s41534-017-0035-1,10.1038/s41534-017-0035-1,npj Quantum Information
Space-Time Optimisations for Early Fault-Tolerant Quantum Computation,2511.08848v1,Litinski2019gameofsurfacecodes,\cite{Litinski2019gameofsurfacecodes},A {G}ame of {S}urface {C}odes: {L}arge-{S}cale {Q}uantum {C}omputing with {L}attice {S}urgery,,,True,False,"Litinski, Daniel",2019.0,,https://doi.org/10.22331/q-2019-03-05-128,10.22331/q-2019-03-05-128,{Quantum}
Space-Time Optimisations for Early Fault-Tolerant Quantum Computation,2511.08848v1,10.1145/3720416,\cite{10.1145/3720416},Dependency-Aware Compilation for Surface Code Quantum Architectures,,,True,False,"Molavi, Abtin and Xu, Amanda and Tannu, Swamit and Albarghouthi, Aws",2025.0,,https://doi.org/10.1145/3720416,10.1145/3720416,Proc. ACM Program. Lang.
Space-Time Optimisations for Early Fault-Tolerant Quantum Computation,2511.08848v1,10.1109/MICRO.2018.00072,\cite{10.1109/MICRO.2018.00072},Magic-State Functional Units: Mapping and Scheduling Multi-Level Distillation Circuits for Fault-Tolerant Quantum Architectures,https://arxiv.org/abs/1809.01302v1,"Quantum computers have recently made great strides and are on a long-term path towards useful fault-tolerant computation. A dominant overhead in fault-tolerant quantum computation is the production of high-fidelity encoded qubits, called magic states, which enable reliable error-corrected computation. We present the first detailed designs of hardware functional units that implement space-time optimized magic-state factories for surface code error-corrected machines. Interactions among distant qubits require surface code braids (physical pathways on chip) which must be routed. Magic-state factories are circuits comprised of a complex set of braids that is more difficult to route than quantum circuits considered in previous work [1]. This paper explores the impact of scheduling techniques, such as gate reordering and qubit renaming, and we propose two novel mapping techniques: braid repulsion and dipole moment braid rotation. We combine these techniques with graph partitioning and community detection algorithms, and further introduce a stitching algorithm for mapping subgraphs onto a physical machine. Our results show a factor of 5.64 reduction in space-time volume compared to the best-known previous designs for magic-state factories.",True,True,"Ding, Yongshan and Holmes, Adam and Javadi-Abhari, Ali and Franklin, Diana and Martonosi, Margaret and Chong, Frederic T.",2018.0,,https://doi.org/10.1109/MICRO.2018.00072,10.1109/MICRO.2018.00072,
Space-Time Optimisations for Early Fault-Tolerant Quantum Computation,2511.08848v1,10.1145/3466752.3480072,\cite{10.1145/3466752.3480072},AutoBraid: A Framework for Enabling Efficient Surface Code Communication in Quantum Computing,,,True,False,"Hua, Fei and Chen, Yanhao and Jin, Yuwei and Zhang, Chi and Hayes, Ari and Zhang, Youtao and Zhang, Eddy Z.",2021.0,,https://doi.org/10.1145/3466752.3480072,10.1145/3466752.3480072,
Space-Time Optimisations for Early Fault-Tolerant Quantum Computation,2511.08848v1,Watkins2024highperformance,\cite{Watkins2024highperformance},A {H}igh {P}erformance {C}ompiler for {V}ery {L}arge {S}cale {S}urface {C}ode {C}omputations,,,True,False,"Watkins, George and Nguyen, Hoang Minh and Watkins, Keelan and Pearce, Steven and Lau, Hoi-Kwan and Paler, Alexandru",2024.0,,https://doi.org/10.22331/q-2024-05-22-1354,10.22331/q-2024-05-22-1354,{Quantum}
Space-Time Optimisations for Early Fault-Tolerant Quantum Computation,2511.08848v1,Moflic:2024xhe,\cite{Moflic:2024xhe},{On the Constant Depth Implementation of Pauli Exponentials},,,True,False,"Moflic, Ioana and Paler, Alexandru",2024.0,8,,,
Space-Time Optimisations for Early Fault-Tolerant Quantum Computation,2511.08848v1,PRXQuantum.3.020342,\cite{PRXQuantum.3.020342},Surface code compilation via edge-disjoint paths,https://arxiv.org/abs/2110.11493v2,"We provide an efficient algorithm to compile quantum circuits for fault-tolerant execution. We target surface codes, which form a 2D grid of logical qubits with nearest-neighbor logical operations. Embedding an input circuit's qubits in surface codes can result in long-range two-qubit operations across the grid. We show how to prepare many long-range Bell pairs on qubits connected by edge-disjoint paths of ancillas in constant depth that can be used to perform these long-range operations. This forms one core part of our Edge-Disjoint Paths Compilation (EDPC) algorithm, by easily performing many parallel long-range Clifford operations in constant depth. It also allows us to establish a connection between surface code compilation and several well-studied edge-disjoint paths problems. Similar techniques allow us to perform non-Clifford single-qubit rotations far from magic state distillation factories. In this case, we can easily find the maximum set of paths by a max-flow reduction, which forms the other major part of EDPC. EDPC has the best asymptotic worst-case performance guarantees on the circuit depth for compiling parallel operations when compared to related compilation methods based on swaps and network coding. EDPC also shows a quadratic depth improvement over sequential Pauli-based compilation for parallel rotations requiring magic resources. We implement EDPC and find significantly improved performance for circuits built from parallel cnots, and for circuits which implement the multi-controlled $X$ gate.",True,True,"Beverland, Michael and Kliuchnikov, Vadym and Schoute, Eddie",2022.0,May,https://link.aps.org/doi/10.1103/PRXQuantum.3.020342,10.1103/PRXQuantum.3.020342,PRX Quantum
Space-Time Optimisations for Early Fault-Tolerant Quantum Computation,2511.08848v1,10.1145/3470496.3527394,\cite{10.1145/3470496.3527394},2QAN: a quantum compiler for 2-local qubit hamiltonian simulation algorithms,,,True,False,"Lao, Lingling and Browne, Dan E.",2022.0,,https://doi.org/10.1145/3470496.3527394,10.1145/3470496.3527394,
Space-Time Optimisations for Early Fault-Tolerant Quantum Computation,2511.08848v1,10.1145/3297858.3304023,\cite{10.1145/3297858.3304023},Tackling the Qubit Mapping Problem for NISQ-Era Quantum Devices,,,True,False,"Li, Gushu and Ding, Yufei and Xie, Yuan",2019.0,,https://doi.org/10.1145/3297858.3304023,10.1145/3297858.3304023,
SSTODE: Ocean-Atmosphere Physics-Informed Neural ODEs for Sea Surface Temperature Prediction,2511.05629v1,veeresha2021numerical,\cite{veeresha2021numerical},A Numerical Approach to the Coupled Atmospheric Ocean Model using a Fractional Operator,,,True,False,"Veeresha, Pundikala",2021.0,,,,Mathematical Modelling and Numerical Simulation with Applications
SSTODE: Ocean-Atmosphere Physics-Informed Neural ODEs for Sea Surface Temperature Prediction,2511.05629v1,lin2020self,\cite{lin2020self},Self-Attention ConvLSTM for Spatiotemporal Prediction,,,True,False,"Lin, Zhihui and Li, Maomao and Zheng, Zhuobin and Cheng, Yangyang and Yuan, Chun",2020.0,,,,
SSTODE: Ocean-Atmosphere Physics-Informed Neural ODEs for Sea Surface Temperature Prediction,2511.05629v1,tang2024vmrnn,\cite{tang2024vmrnn},Vmrnn: Integrating Vision Mamba and Lstm for Efficient and Accurate Spatiotemporal Forecasting,,,True,False,"Tang, Yujin and Dong, Peijie and Tang, Zhenheng and Chu, Xiaowen and Liang, Junwei",2024.0,,,,
SSTODE: Ocean-Atmosphere Physics-Informed Neural ODEs for Sea Surface Temperature Prediction,2511.05629v1,tan2025simvpv2,\cite{tan2025simvpv2},SimVPv2: Towards Simple yet Powerful Spatiotemporal Predictive Learning,https://arxiv.org/abs/2211.12509v4,"Recent years have witnessed remarkable advances in spatiotemporal predictive learning, with methods incorporating auxiliary inputs, complex neural architectures, and sophisticated training strategies. While SimVP has introduced a simpler, CNN-based baseline for this task, it still relies on heavy Unet-like architectures for spatial and temporal modeling, which still suffers from high complexity and computational overhead. In this paper, we propose SimVPv2, a streamlined model that eliminates the need for Unet architectures and demonstrates that plain stacks of convolutional layers, enhanced with an efficient Gated Spatiotemporal Attention mechanism, can deliver state-of-the-art performance. SimVPv2 not only simplifies the model architecture but also improves both performance and computational efficiency. On the standard Moving MNIST benchmark, SimVPv2 achieves superior performance compared to SimVP, with fewer FLOPs, about half the training time, and 60% faster inference efficiency. Extensive experiments across eight diverse datasets, including real-world tasks such as traffic forecasting and climate prediction, further demonstrate that SimVPv2 offers a powerful yet straightforward solution, achieving robust generalization across various spatiotemporal learning scenarios. We believe the proposed SimVPv2 can serve as a solid baseline to benefit the spatiotemporal predictive learning community.",True,True,"Tan, Cheng and Gao, Zhangyang and Li, Siyuan and Li, Stan Z.",2025.0,,,10.1109/TMM.2025.3543051,IEEE Transactions on Multimedia
SSTODE: Ocean-Atmosphere Physics-Informed Neural ODEs for Sea Surface Temperature Prediction,2511.05629v1,shi2024oceanvp,\cite{shi2024oceanvp},OceanVP: A HYCOM Cased Benchmark Dataset and a Relational Spatiotemporal Predictive Network for Oceanic Variable Prediction,,,True,False,"Shi, Zhensheng and Zheng, Haiyong and Dong, Junyu",2024.0,,,,Ocean Engineering
SSTODE: Ocean-Atmosphere Physics-Informed Neural ODEs for Sea Surface Temperature Prediction,2511.05629v1,xiong2023ai,\cite{xiong2023ai},AI-GOMS: Large AI-Driven Global Ocean Modeling System,https://arxiv.org/abs/2308.03152v2,"Ocean modeling is a powerful tool for simulating the physical, chemical, and biological processes of the ocean, which is the foundation for marine science research and operational oceanography. Modern numerical ocean modeling mainly consists of governing equations and numerical algorithms. Nonlinear instability, computational expense, low reusability efficiency and high coupling costs have gradually become the main bottlenecks for the further development of numerical ocean modeling. Recently, artificial intelligence-based modeling in scientific computing has shown revolutionary potential for digital twins and scientific simulations, but the bottlenecks of numerical ocean modeling have not been further solved. Here, we present AI-GOMS, a large AI-driven global ocean modeling system, for accurate and efficient global ocean daily prediction. AI-GOMS consists of a backbone model with the Fourier-based Masked Autoencoder structure for basic ocean variable prediction and lightweight fine-tuning models incorporating regional downscaling, wave decoding, and biochemistry coupling modules. AI-GOMS has achieved the best performance in 30 days of prediction for the global ocean basic variables with 15 depth layers at 1/4° spatial resolution. Beyond the good performance in statistical metrics, AI-GOMS realizes the simulation of mesoscale eddies in the Kuroshio region at 1/12° spatial resolution and ocean stratification in the tropical Pacific Ocean. AI-GOMS provides a new backbone-downstream paradigm for Earth system modeling, which makes the system transferable, scalable and reusable.",True,True,"Xiong, Wei and Xiang, Yanfei and Wu, Hao and Zhou, Shuyi and Sun, Yuze and Ma, Muyuan and Huang, Xiaomeng",2023.0,,,,arXiv preprint arXiv:2308.03152
SSTODE: Ocean-Atmosphere Physics-Informed Neural ODEs for Sea Surface Temperature Prediction,2511.05629v1,wang2024xihe,\cite{wang2024xihe},Xihe: A Data-driven Model for Global Ocean Eddy-Resolving Forecasting,,,True,False,"Wang, Xiang and Wang, Renzhi and Hu, Ningzi and Wang, Pinqiang and Huo, Peng and Wang, Guihua and Wang, Huizan and Wang, Senzhang and Zhu, Junxing and Xu, Jianbo and others",2024.0,,,,arXiv preprint arXiv:2402.02995
SSTODE: Ocean-Atmosphere Physics-Informed Neural ODEs for Sea Surface Temperature Prediction,2511.05629v1,cui2025forecasting,\cite{cui2025forecasting},Forecasting the Eddying Ocean with a Deep Neural Network,,,True,False,"Cui, Yingzhe and Wu, Ruohan and Zhang, Xiang and Zhu, Ziqi and Liu, Bo and Shi, Jun and Chen, Junshi and Liu, Hailong and Zhou, Shenghui and Su, Liang and others",2025.0,,,,Nature Communications
SSTODE: Ocean-Atmosphere Physics-Informed Neural ODEs for Sea Surface Temperature Prediction,2511.05629v1,xu2024generalizing,\cite{xu2024generalizing},Generalizing Weather Forecast to Fine-grained Temporal Scales via Physics-AI Hybrid Modeling,https://arxiv.org/abs/2405.13796v5,"Data-driven artificial intelligence (AI) models have made significant advancements in weather forecasting, particularly in medium-range and nowcasting. However, most data-driven weather forecasting models are black-box systems that focus on learning data mapping rather than fine-grained physical evolution in the time dimension. Consequently, the limitations in the temporal scale of datasets prevent these models from forecasting at finer time scales. This paper proposes a physics-AI hybrid model (i.e., WeatherGFT) which generalizes weather forecasts to finer-grained temporal scales beyond training dataset. Specifically, we employ a carefully designed PDE kernel to simulate physical evolution on a small time scale (e.g., 300 seconds) and use a parallel neural networks with a learnable router for bias correction. Furthermore, we introduce a lead time-aware training framework to promote the generalization of the model at different lead times. The weight analysis of physics-AI modules indicates that physics conducts major evolution while AI performs corrections adaptively. Extensive experiments show that WeatherGFT trained on an hourly dataset, effectively generalizes forecasts across multiple time scales, including 30-minute, which is even smaller than the dataset's temporal resolution.",True,True,"Xu, Wanghan and Ling, Fenghua and Han, Tao and Chen, Hao and Ouyang, Wanli and BAI, LEI",2024.0,,,,Advances in Neural Information Processing Systems
SSTODE: Ocean-Atmosphere Physics-Informed Neural ODEs for Sea Surface Temperature Prediction,2511.05629v1,li2024deepphysinet,\cite{li2024deepphysinet},Deepphysinet: Bridging Deep Learning and Atmospheric Physics for Accurate and Continuous Weather Modeling,,,True,False,"Li, Wenyuan and Liu, Zili and Chen, Keyan and Chen, Hao and Liang, Shunlin and Zou, Zhengxia and Shi, Zhenwei",2024.0,,,,arXiv preprint arXiv:2401.04125
SSTODE: Ocean-Atmosphere Physics-Informed Neural ODEs for Sea Surface Temperature Prediction,2511.05629v1,ghosh2023rans,\cite{ghosh2023rans},RANS-PINN based Simulation Surrogates for Predicting Turbulent Flows,https://arxiv.org/abs/2306.06034v3,"Physics-informed neural networks (PINNs) provide a framework to build surrogate models for dynamical systems governed by differential equations. During the learning process, PINNs incorporate a physics-based regularization term within the loss function to enhance generalization performance. Since simulating dynamics controlled by partial differential equations (PDEs) can be computationally expensive, PINNs have gained popularity in learning parametric surrogates for fluid flow problems governed by Navier-Stokes equations. In this work, we introduce RANS-PINN, a modified PINN framework, to predict flow fields (i.e., velocity and pressure) in high Reynolds number turbulent flow regimes. To account for the additional complexity introduced by turbulence, RANS-PINN employs a 2-equation eddy viscosity model based on a Reynolds-averaged Navier-Stokes (RANS) formulation. Furthermore, we adopt a novel training approach that ensures effective initialization and balance among the various components of the loss function. The effectiveness of the RANS-PINN framework is then demonstrated using a parametric PINN.",True,True,"Ghosh, Shinjan and Chakraborty, Amit and Brikis, Georgia Olympia and Dey, Biswadip",2023.0,,,,arXiv preprint arXiv:2306.06034
SSTODE: Ocean-Atmosphere Physics-Informed Neural ODEs for Sea Surface Temperature Prediction,2511.05629v1,sun2020physics,\cite{sun2020physics},Physics-Constrained Bayesian Neural Network for Fluid Flow Reconstruction with Sparse and Noisy Data,https://arxiv.org/abs/2001.05542v1,"In many applications, flow measurements are usually sparse and possibly noisy. The reconstruction of a high-resolution flow field from limited and imperfect flow information is significant yet challenging. In this work, we propose an innovative physics-constrained Bayesian deep learning approach to reconstruct flow fields from sparse, noisy velocity data, where equation-based constraints are imposed through the likelihood function and uncertainty of the reconstructed flow can be estimated. Specifically, a Bayesian deep neural network is trained on sparse measurement data to capture the flow field. In the meantime, the violation of physical laws will be penalized on a large number of spatiotemporal points where measurements are not available. A non-parametric variational inference approach is applied to enable efficient physics-constrained Bayesian learning. Several test cases on idealized vascular flows with synthetic measurement data are studied to demonstrate the merit of the proposed method.",True,True,"Sun, Luning and Wang, Jian-Xun",2020.0,,,,Theoretical and Applied Mechanics Letters
SSTODE: Ocean-Atmosphere Physics-Informed Neural ODEs for Sea Surface Temperature Prediction,2511.05629v1,chen2018neural,\cite{chen2018neural},Neural Ordinary Differential Equations,https://arxiv.org/abs/1806.07366v5,"We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.",True,True,"Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K",2018.0,,https://proceedings.neurips.cc/paper_files/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf,,
SSTODE: Ocean-Atmosphere Physics-Informed Neural ODEs for Sea Surface Temperature Prediction,2511.05629v1,chu2024adaptive,\cite{chu2024adaptive},Adaptive Decision Spatio-temporal Neural ODE for Traffic Flow Forecasting with Multi-Kernel Temporal Dynamic Dilation Convolution,,,True,False,"Chu, Zihao and Ma, Wenming and Li, Mingqi and Chen, Hao",2024.0,,,,Neural Networks
SSTODE: Ocean-Atmosphere Physics-Informed Neural ODEs for Sea Surface Temperature Prediction,2511.05629v1,rojas2021reduced,\cite{rojas2021reduced},Reduced-order Model for Fluid Flows via Neural Ordinary Differential Equations,,,True,False,"Rojas, Carlos JG and Dengel, Andreas and Ribeiro, Mateus Dias",2021.0,,,,arXiv preprint arXiv:2102.02248
SSTODE: Ocean-Atmosphere Physics-Informed Neural ODEs for Sea Surface Temperature Prediction,2511.05629v1,tian2025air,\cite{tian2025air},Air Quality Prediction with Physics-Guided Dual Neural ODEs in Open Systems,,,True,False,"Tian, Jindong and Liang, Yuxuan and Xu, Ronghui and Chen, Peng and Guo, Chenjuan and Zhou, Aoying and Pan, Lujia and Rao, Zhongwen and Yang, Bin",2024.0,,,,
SSTODE: Ocean-Atmosphere Physics-Informed Neural ODEs for Sea Surface Temperature Prediction,2511.05629v1,hwang2021climate,\cite{hwang2021climate},Climate Modeling with Neural Diffusion Equations,,,True,False,"Hwang, Jeehyun and Choi, Jeongwhan and Choi, Hwangyong and Lee, Kookjin and Lee, Dongeun and Park, Noseong",2021.0,,,,
SSTODE: Ocean-Atmosphere Physics-Informed Neural ODEs for Sea Surface Temperature Prediction,2511.05629v1,verma2024climode,\cite{verma2024climode},Clim{ODE}: Climate Forecasting With Physics-informed Neural {ODE}s,,,True,False,Yogesh Verma and Markus Heinonen and Vikas Garg,2024.0,,https://openreview.net/forum?id=xuY33XhEGR,,
"OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights",2511.01019v2,ji2023survey,\cite{ji2023survey},Survey of Hallucination in Natural Language Generation,https://arxiv.org/abs/2202.03629v7,"Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before. In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions; (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, machine translation, and visual-language generation; and (3) hallucinations in large language models (LLMs). This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.",True,True,"Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale",2023.0,,http://dx.doi.org/10.1145/3571730,10.1145/3571730,ACM Computing Surveys
"OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights",2511.01019v2,singhal2023large,\cite{singhal2023large},Large Language Models Encode Clinical Knowledge,https://arxiv.org/abs/2212.13138v1,"Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, but the quality bar for medical and clinical applications is high. Today, attempts to assess models' clinical knowledge typically rely on automated evaluations on limited benchmarks. There is no standard to evaluate model predictions and reasoning across a breadth of tasks. To address this, we present MultiMedQA, a benchmark combining six existing open question answering datasets spanning professional medical exams, research, and consumer queries; and HealthSearchQA, a new free-response dataset of medical questions searched online. We propose a framework for human evaluation of model answers along multiple axes including factuality, precision, possible harm, and bias. In addition, we evaluate PaLM (a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM, on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA, MedMCQA, PubMedQA, MMLU clinical topics), including 67.6% accuracy on MedQA (US Medical License Exam questions), surpassing prior state-of-the-art by over 17%. However, human evaluation reveals key gaps in Flan-PaLM responses. To resolve this we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, recall of knowledge, and medical reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal important limitations of today's models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLM models for clinical applications.",True,True,"Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S. and Wei, Jason and Chung, Hyung and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and Payne, Perry and Gamble, Paul and Kelly, Christopher and Babiker, Abubakr and Schärli, Nathanael and Chowdhery, Aakanksha and Mansfield, Philip and Demner-Fushman, Dina and Natarajan, Vivek",2023.0,07,,10.1038/s41586-023-06291-2,Nature
"OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights",2511.01019v2,mostafa2024grag,\cite{mostafa2024grag},G-RAG: Knowledge Expansion in Material Science,https://arxiv.org/abs/2411.14592v2,"In the field of Material Science, effective information retrieval systems are essential for facilitating research. Traditional Retrieval-Augmented Generation (RAG) approaches in Large Language Models (LLMs) often encounter challenges such as outdated information, hallucinations, limited interpretability due to context constraints, and inaccurate retrieval. To address these issues, Graph RAG integrates graph databases to enhance the retrieval process. Our proposed method processes Material Science documents by extracting key entities (referred to as MatIDs) from sentences, which are then utilized to query external Wikipedia knowledge bases (KBs) for additional relevant information. We implement an agent-based parsing technique to achieve a more detailed representation of the documents. Our improved version of Graph RAG called G-RAG further leverages a graph database to capture relationships between these entities, improving both retrieval accuracy and contextual understanding. This enhanced approach demonstrates significant improvements in performance for domains that require precise information retrieval, such as Material Science.",True,True,Radeen Mostafa and Mirza Nihal Baig and Mashaekh Tausif Ehsan and Jakir Hasan,2024.0,,https://arxiv.org/abs/2411.14592,,
"OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights",2511.01019v2,schick2023toolformer,\cite{schick2023toolformer},Toolformer: Language Models Can Teach Themselves to Use Tools,https://arxiv.org/abs/2302.04761v1,"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",True,True,"Schick, Timo and Dwivedi-Yu, Shanya and Hou, Yujia and others",2023.0,,,,arXiv preprint arXiv:2302.04761
"OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights",2511.01019v2,shen2023hugginggpt,\cite{shen2023hugginggpt},HuggingGPT: Solving AI Tasks with ChatGPT and Its Friends in HuggingFace,,,True,False,"Shen, Yujia and Liu, Kaitao and Dou, Deqing and others",2023.0,,,,arXiv preprint arXiv:2303.17580
"OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights",2511.01019v2,nakano2022webgpt,\cite{nakano2022webgpt},WebGPT: Browser-assisted question-answering with human feedback,https://arxiv.org/abs/2112.09332v3,"We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56% of the time to those of our human demonstrators, and 69% of the time to the highest-voted answer from Reddit.",True,True,Reiichiro Nakano and Jacob Hilton and Suchir Balaji and Jeff Wu and Long Ouyang and Christina Kim and Christopher Hesse and Shantanu Jain and Vineet Kosaraju and William Saunders and Xu Jiang and Karl Cobbe and Tyna Eloundou and Gretchen Krueger and Kevin Button and Matthew Knight and Benjamin Chess and John Schulman,2022.0,,https://arxiv.org/abs/2112.09332,,
"OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights",2511.01019v2,thulke2024climategpt,\cite{thulke2024climategpt},ClimateGPT: Towards AI Synthesizing Interdisciplinary Research on Climate Change,https://arxiv.org/abs/2401.09646v1,"This paper introduces ClimateGPT, a model family of domain-specific large language models that synthesize interdisciplinary research on climate change. We trained two 7B models from scratch on a science-oriented dataset of 300B tokens. For the first model, the 4.2B domain-specific tokens were included during pre-training and the second was adapted to the climate domain after pre-training. Additionally, ClimateGPT-7B, 13B and 70B are continuously pre-trained from Llama~2 on a domain-specific dataset of 4.2B tokens. Each model is instruction fine-tuned on a high-quality and human-generated domain-specific dataset that has been created in close cooperation with climate scientists. To reduce the number of hallucinations, we optimize the model for retrieval augmentation and propose a hierarchical retrieval strategy. To increase the accessibility of our model to non-English speakers, we propose to make use of cascaded machine translation and show that this approach can perform comparably to natively multilingual models while being easier to scale to a large number of languages. Further, to address the intrinsic interdisciplinary aspect of climate change we consider different research perspectives. Therefore, the model can produce in-depth answers focusing on different perspectives in addition to an overall answer. We propose a suite of automatic climate-specific benchmarks to evaluate LLMs. On these benchmarks, ClimateGPT-7B performs on par with the ten times larger Llama-2-70B Chat model while not degrading results on general domain benchmarks. Our human evaluation confirms the trends we saw in our benchmarks. All models were trained and evaluated using renewable energy and are released publicly.",True,True,David Thulke and Yingbo Gao and Petrus Pelser and Rein Brune and Rricha Jalota and Floris Fok and Michael Ramos and Ian van Wyk and Abdallah Nasir and Hayden Goldstein and Taylor Tragemann and Katie Nguyen and Ariana Fowler and Andrew Stanco and Jon Gabriel and Jordan Taylor and Dean Moro and Evgenii Tsymbalov and Juliette de Waal and Evgeny Matusov and Mudar Yaghi and Mohammad Shihadah and Hermann Ney and Christian Dugast and Jonathan Dotan and Daniel Erasmus,2024.0,,https://arxiv.org/abs/2401.09646,,
"OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights",2511.01019v2,lin2024geoga,\cite{lin2024geoga},GeoGalactica: A Scientific Large Language Model in Geoscience,https://arxiv.org/abs/2401.00434v2,"Large language models (LLMs) have achieved huge success for their general knowledge and ability to solve a wide spectrum of tasks in natural language processing (NLP). Due to their impressive abilities, LLMs have shed light on potential inter-discipline applications to foster scientific discoveries of a specific domain by using artificial intelligence (AI for science, AI4S). In the meantime, utilizing NLP techniques in geoscience research and practice is wide and convoluted, contributing from knowledge extraction and document classification to question answering and knowledge discovery. In this work, we take the initial step to leverage LLM for science, through a rather straightforward approach. We try to specialize an LLM into geoscience, by further pre-training the model with a vast amount of texts in geoscience, as well as supervised fine-tuning (SFT) the resulting model with our custom collected instruction tuning dataset. These efforts result in a model GeoGalactica consisting of 30 billion parameters. To our best knowledge, it is the largest language model for the geoscience domain. More specifically, GeoGalactica is from further pre-training of Galactica. We train GeoGalactica over a geoscience-related text corpus containing 65 billion tokens, preserving as the largest geoscience-specific text corpus. Then we fine-tune the model with 1 million pairs of instruction-tuning data consisting of questions that demand professional geoscience knowledge to answer. In this technical report, we will illustrate in detail all aspects of GeoGalactica, including data collection, data cleaning, base model selection, pre-training, SFT, and evaluation. We open-source our data curation tools and the checkpoints of GeoGalactica during the first 3/4 of pre-training.",True,True,Zhouhan Lin and Cheng Deng and Le Zhou and Tianhang Zhang and Yi Xu and Yutong Xu and Zhongmou He and Yuanyuan Shi and Beiya Dai and Yunchong Song and Boyi Zeng and Qiyuan Chen and Yuxun Miao and Bo Xue and Shu Wang and Luoyi Fu and Weinan Zhang and Junxian He and Yunqiang Zhu and Xinbing Wang and Chenghu Zhou,2024.0,,https://arxiv.org/abs/2401.00434,,
"OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights",2511.01019v2,bi2024oceangpt,\cite{bi2024oceangpt},OceanGPT: A Large Language Model for Ocean Science Tasks,,,True,False,Zhen Bi and Ningyu Zhang and Yida Xue and Yixin Ou and Daxiong Ji and Guozhou Zheng and Huajun Chen,2024.0,,https://arxiv.org/abs/2310.02031,,
"OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights",2511.01019v2,Widlansky2025,\cite{Widlansky2025},Building an Intelligent Data Exploring Assistant for Geoscientists,,,True,False,"Widlansky, Matthew J. and Komar, Nemanja",2025.0,,https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2025JH000649,https://doi.org/10.1029/2025JH000649,Journal of Geophysical Research: Machine Learning and Computation
Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports,2511.01052v1,Odisho2020-vy,\cite{Odisho2020-vy},Natural language processing systems for pathology parsing in limited data environments with uncertainty estimation,,,True,False,"Odisho, Anobel Y and Park, Briton and Altieri, Nicholas and DeNero, John and Cooperberg, Matthew R and Carroll, Peter R and Yu, Bin",2020.0,10,https://doi.org/10.1093/jamiaopen/ooaa029,10.1093/jamiaopen/ooaa029,JAMIA Open
Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports,2511.01052v1,De_Angeli2021-ro,\cite{De_Angeli2021-ro},Deep active learning for classifying cancer pathology reports,,,True,False,"De Angeli, Kevin and Gao, Shang and Alawad, Mohammed and Yoon, Hong-Jun and Schaefferkoetter, Noah and Wu, Xiao-Cheng and Durbin, Eric B. and Doherty, Jennifer and Stroup, Antoinette and Coyle, Linda and Penberthy, Lynne and Tourassi, Georgia",2021.0,,https://doi.org/10.1186/s12859-021-04047-1,10.1186/s12859-021-04047-1,BMC Bioinformatics
Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports,2511.01052v1,Gao2018-ex,\cite{Gao2018-ex},Hierarchical attention networks for information extraction from cancer pathology reports,,,True,False,"Gao, Shang and Young, Michael T and Qiu, John X and Yoon, Hong-Jun and Christian, James B and Fearn, Paul A and Tourassi, Georgia D and Ramanthan, Arvind",2017.0,11,https://doi.org/10.1093/jamia/ocx131,10.1093/jamia/ocx131,Journal of the American Medical Informatics Association
Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports,2511.01052v1,Gao2019-qc,\cite{Gao2019-qc},Classifying cancer pathology reports with hierarchical self-attention networks,,,True,False,Shang Gao and John X. Qiu and Mohammed Alawad and Jacob D. Hinkle and Noah Schaefferkoetter and Hong-Jun Yoon and Blair Christian and Paul A. Fearn and Lynne Penberthy and Xiao-Cheng Wu and Linda Coyle and Georgia Tourassi and Arvind Ramanathan,2019.0,,https://www.sciencedirect.com/science/article/pii/S0933365719303562,https://doi.org/10.1016/j.artmed.2019.101726,Artificial Intelligence in Medicine
Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports,2511.01052v1,Wu2020-du,\cite{Wu2020-du},Structured Information Extraction of Pathology Reports with Attention-based Graph Convolutional Network,,,True,False,"Wu, Jialun and Tang, Kaiwen and Zhang, Haichuan and Wang, Chunbao and Li, Chen",2020.0,,,10.1109/BIBM49941.2020.9313347,
Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports,2511.01052v1,Vaswani2017-gk,\cite{Vaswani2017-gk},Not All Attention Is All You Need,https://arxiv.org/abs/2104.04692v3,"Beyond the success story of pre-trained language models (PrLMs) in recent natural language processing, they are susceptible to over-fitting due to unusual large model size. To this end, dropout serves as a therapy. However, existing methods like random-based, knowledge-based and search-based dropout are more general but less effective onto self-attention based models, which are broadly chosen as the fundamental architecture of PrLMs. In this paper, we propose a novel dropout method named AttendOut to let self-attention empowered PrLMs capable of more robust task-specific tuning. We demonstrate that state-of-the-art models with elaborate training design may achieve much stronger results. We verify the universality of our approach on extensive natural language processing tasks.",True,True,"Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \L{}ukasz and Polosukhin, Illia",2017.0,,,,
Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports,2511.01052v1,Carlini2023-ys,\cite{Carlini2023-ys},Quantifying Memorization Across Neural Language Models,https://arxiv.org/abs/2202.07646v3,"Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others).
  We describe three log-linear relationships that quantify the degree to which LMs emit memorized training data. Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes more complicated when generalizing these results across model families. On the whole, we find that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations.",True,True,"Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew
               and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan",,,,,
Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports,2511.01052v1,Kefeli2023-ec,\cite{Kefeli2023-ec},Generalizable and automated classification of {TNM} stage from pathology reports with external validation,,,True,False,"Kefeli, Jenna and Berkowitz, Jacob and Acitores Cortina, Jose M. and Tsang, Kevin K. and Tatonetti, Nicholas P.",2024.0,,https://doi.org/10.1038/s41467-024-53190-9,10.1038/s41467-024-53190-9,Nature Communications
Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports,2511.01052v1,Li2022-lw,\cite{Li2022-lw},Clinical-Longformer and Clinical-BigBird: Transformers for long clinical sequences,,,True,False,Yikuan Li and Ramsey M. Wehbe and Faraz S. Ahmad and Hanyin Wang and Yuan Luo,2022.0,,https://arxiv.org/abs/2201.11838,,
Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports,2511.01052v1,Chang2024-qd,\cite{Chang2024-qd},Classifying Cancer Stage with Open-Source Clinical Large Language Models,,,True,False,"Chang, Chia-Hsuan and Lucas, Mary M. and Lu-Yao, Grace and Yang, Christopher C.",2024.0,,https://doi.ieeecomputersociety.org/10.1109/ICHI61247.2024.00018,10.1109/ICHI61247.2024.00018,
Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports,2511.01052v1,Christophe2024-wz,\cite{Christophe2024-wz},Med42 -- Evaluating Fine-Tuning Strategies for Medical LLMs: Full-Parameter vs. Parameter-Efficient Approaches,https://arxiv.org/abs/2404.14779v1,"This study presents a comprehensive analysis and comparison of two predominant fine-tuning methodologies - full-parameter fine-tuning and parameter-efficient tuning - within the context of medical Large Language Models (LLMs). We developed and refined a series of LLMs, based on the Llama-2 architecture, specifically designed to enhance medical knowledge retrieval, reasoning, and question-answering capabilities. Our experiments systematically evaluate the effectiveness of these tuning strategies across various well-known medical benchmarks. Notably, our medical LLM Med42 showed an accuracy level of 72% on the US Medical Licensing Examination (USMLE) datasets, setting a new standard in performance for openly available medical LLMs. Through this comparative analysis, we aim to identify the most effective and efficient method for fine-tuning LLMs in the medical domain, thereby contributing significantly to the advancement of AI-driven healthcare applications.",True,True,Clément Christophe and Praveen K Kanithi and Prateek Munjal and Tathagata Raha and Nasir Hayat and Ronnie Rajan and Ahmed Al-Mahrooqi and Avani Gupta and Muhammad Umar Salman and Gurpreet Gosal and Bhargav Kanakiya and Charles Chen and Natalia Vassilieva and Boulbaba Ben Amor and Marco AF Pimentel and Shadab Khan,2024.0,,https://arxiv.org/abs/2404.14779,,
Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports,2511.01052v1,Chang2024-qu,\cite{Chang2024-qu},Beyond Self-Consistency: Ensemble Reasoning Boosts Consistency and Accuracy of LLMs in Cancer Staging,https://arxiv.org/abs/2404.13149v1,"Advances in large language models (LLMs) have encouraged their adoption in the healthcare domain where vital clinical information is often contained in unstructured notes. Cancer staging status is available in clinical reports, but it requires natural language processing to extract the status from the unstructured text. With the advance in clinical-oriented LLMs, it is promising to extract such status without extensive efforts in training the algorithms. Prompting approaches of the pre-trained LLMs that elicit a model's reasoning process, such as chain-of-thought, may help to improve the trustworthiness of the generated responses. Using self-consistency further improves model performance, but often results in inconsistent generations across the multiple reasoning paths. In this study, we propose an ensemble reasoning approach with the aim of improving the consistency of the model generations. Using an open access clinical large language model to determine the pathologic cancer stage from real-world pathology reports, we show that the ensemble reasoning approach is able to improve both the consistency and performance of the LLM in determining cancer stage, thereby demonstrating the potential to use these models in clinical or other domains where reliability and trustworthiness are critical.",True,True,"Chang, Chia-Hsuan
and Lucas, Mary M.
and Lee, Yeawon
and Yang, Christopher C.
and Lu-Yao, Grace",2024.0,,,,
Hybrid action Reinforcement Learning for quantum architecture search,2511.04967v2,mcclean2018barren,\cite{mcclean2018barren},Barren plateaus in quantum neural network training landscapes,,,True,False,"McClean, Jarrod R and Boixo, Sergio and Smelyanskiy, Vadim N and Babbush, Ryan and Neven, Hartmut",2018.0,,,,Nature communications
Hybrid action Reinforcement Learning for quantum architecture search,2511.04967v2,van2016deep,\cite{van2016deep},Deep reinforcement learning with double q-learning,,,True,False,"Van Hasselt, Hado and Guez, Arthur and Silver, David",2016.0,,,,
Hybrid action Reinforcement Learning for quantum architecture search,2511.04967v2,khairy2020learning,\cite{khairy2020learning},Learning to optimize variational quantum circuits to solve combinatorial problems,,,True,False,"Khairy, Sami and Shaydulin, Ruslan and Cincio, Lukasz and Alexeev, Yuri and Balaprakash, Prasanna",2020.0,,,,
Hybrid action Reinforcement Learning for quantum architecture search,2511.04967v2,ostaszewski2021reinforcement,\cite{ostaszewski2021reinforcement},Reinforcement learning for optimization of variational quantum circuit architectures,,,True,False,"Ostaszewski, Mateusz and Trenkwalder, Lea M and Masarczyk, Wojciech and Scerri, Eleanor and Dunjko, Vedran",2021.0,,,,Advances in neural information processing systems
Hybrid action Reinforcement Learning for quantum architecture search,2511.04967v2,patel2024curriculum,\cite{patel2024curriculum},Curriculum reinforcement learning for quantum architecture search under hardware errors,,,True,False,"Patel, Yash J and Kundu, Akash and Ostaszewski, Mateusz and Bonet-Monroig, Xavier and Dunjko, Vedran and Danaci, Onur",2024.0,,,,
Hybrid action Reinforcement Learning for quantum architecture search,2511.04967v2,dutta2025qas,\cite{dutta2025qas},QAS-QTNs: Curriculum Reinforcement Learning-Driven Quantum Architecture Search for Quantum Tensor Networks,https://arxiv.org/abs/2507.12013v1,"Quantum Architecture Search (QAS) is an emerging field aimed at automating the design of quantum circuits for optimal performance. This paper introduces a novel QAS framework employing hybrid quantum reinforcement learning with quantum curriculum learning strategies, enabling learning agents to tackle increasingly complex quantum circuit design tasks. We benchmark four state-of-the-art classical reinforcement learning algorithms (A2C, PPO, DDQN, TD3) against their quantum-enhanced counterparts (QA2C, QPPO, QDDQN, QTD3) for optimizing variational quantum circuits (VQCs). Our approach progressively increases circuit depth and gate complexity during training, leveraging parameterized quantum circuits as function approximations. To improve learning efficiency and stability, all algorithms, both classical and quantum, are augmented with Prioritized Experience Replay (PER). Experimental results show that quantum-enhanced RL significantly outperforms classical methods. In a 2-qubit environment, PERQDDQN achieves a success probability of 0.46 with ~3,000 optimal successes, surpassing classical PERDDQN (0.42, ~2,400). In the more complex 3-qubit setting, PERQDDQN and PERQTD3 reach success probabilities of ~0.47, with optimal success counts of ~3,800 and ~3,600, respectively, outperforming their classical counterparts. Additionally, we apply our QAS-QTN approach to a classification problem, where the optimized quantum circuit achieves an accuracy of 90.33\%, outperforming quantum models consisting of random ansatz. This hybrid classical-quantum approach leads to faster convergence and more efficient quantum circuit designs, demonstrating its potential for advancing automated quantum architecture search.",True,True,"Dutta, Siddhant and Innan, Nouhaila and Yahia, Sadok Ben and Shafique, Muhammad",2025.0,,,,arXiv preprint arXiv:2507.12013
Hybrid action Reinforcement Learning for quantum architecture search,2511.04967v2,ikhtiarudin2025benchrl,\cite{ikhtiarudin2025benchrl},BenchRL-QAS: Benchmarking reinforcement learning algorithms for quantum architecture search,https://arxiv.org/abs/2507.12189v2,"We present BenchRL-QAS, a unified benchmarking framework for reinforcement learning (RL) in quantum architecture search (QAS) across a spectrum of variational quantum algorithm tasks on 2- to 8-qubit systems. Our study systematically evaluates 9 different RL agents, including both value-based and policy-gradient methods, on quantum problems such as variational eigensolver, quantum state diagonalization, variational quantum classification (VQC), and state preparation, under both noiseless and noisy execution settings. To ensure fair comparison, we propose a weighted ranking metric that integrates accuracy, circuit depth, gate count, and training time. Results demonstrate that no single RL method dominates universally, the performance dependents on task type, qubit count, and noise conditions providing strong evidence of no free lunch principle in RL-QAS. As a byproduct we observe that a carefully chosen RL algorithm in RL-based VQC outperforms baseline VQCs. BenchRL-QAS establishes the most extensive benchmark for RL-based QAS to date, codes and experimental made publicly available for reproducibility and future advances.",True,True,"Ikhtiarudin, Azhar and Das, Aditi and Thakkar, Param and Kundu, Akash",2025.0,,,,arXiv preprint arXiv:2507.12189
Hybrid action Reinforcement Learning for quantum architecture search,2511.04967v2,kundu2025tensorrl,\cite{kundu2025tensorrl},TensorRL-QAS: Reinforcement learning with tensor networks for scalable quantum architecture search,,,True,False,"Kundu, Akash and Mangini, Stefano",2025.0,,,,arXiv preprint arXiv:2505.09371
Hybrid action Reinforcement Learning for quantum architecture search,2511.04967v2,turati2025automated,\cite{turati2025automated},Automated Design of Structured Variational Quantum Circuits with Reinforcement Learning,https://arxiv.org/abs/2507.16001v1,"Variational Quantum Algorithms (VQAs) are among the most promising approaches for leveraging near-term quantum hardware, yet their effectiveness strongly depends on the design of the underlying circuit ansatz, which is typically constructed with heuristic methods. In this work, we represent the synthesis of variational quantum circuits as a sequential decision-making problem, where gates are added iteratively in order to optimize an objective function, and we introduce two reinforcement learning-based methods, RLVQC Global and RLVQC Block, tailored to combinatorial optimization problems. RLVQC Block creates ansatzes that generalize the Quantum Approximate Optimization Algorithm (QAOA), by discovering a two-qubits block that is applied to all the interacting qubit pairs. While RLVQC Global further generalizes the ansatz and adds gates unconstrained by the structure of the interacting qubits. Both methods adopt the Proximal Policy Optimization (PPO) algorithm and use empirical measurement outcomes as state observations to guide the agent. We evaluate the proposed methods on a broad set of QUBO instances derived from classical graph-based optimization problems. Our results show that both RLVQC methods exhibit strong results with RLVQC Block consistently outperforming QAOA and generally surpassing RLVQC Global. While RLVQC Block produces circuits with depth comparable to QAOA, the Global variant is instead able to find significantly shorter ones. These findings suggest that reinforcement learning methods can be an effective tool to discover new ansatz structures tailored for specific problems and that the most effective circuit design strategy lies between rigid predefined architectures and completely unconstrained ones, offering a favourable trade-off between structure and adaptability.",True,True,"Turati, Gloria and Foder{\`a}, Simone and Nembrini, Riccardo and Dacrema, Maurizio Ferrari and Cremonesi, Paolo",2025.0,,,,arXiv preprint arXiv:2507.16001
Hybrid action Reinforcement Learning for quantum architecture search,2511.04967v2,kundu2024reinforcement,\cite{kundu2024reinforcement},Reinforcement learning with learned gadgets to tackle hard quantum problems on real hardware,,,True,False,"Kundu, Akash and Sarra, Leopoldo",2024.0,,,,arXiv preprint arXiv:2411.00230
Hybrid action Reinforcement Learning for quantum architecture search,2511.04967v2,olle2025scaling,\cite{olle2025scaling},Scaling the Automated Discovery of Quantum Circuits via Reinforcement Learning with Gadgets,,,True,False,"Olle, Jan and Yevtushenko, Oleg M and Marquardt, Florian",2025.0,,,,arXiv preprint arXiv:2503.11638
Hybrid action Reinforcement Learning for quantum architecture search,2511.04967v2,li2024quarl,\cite{li2024quarl},Quarl: A Learning-Based Quantum Circuit Optimizer,https://arxiv.org/abs/2307.10120v1,"Optimizing quantum circuits is challenging due to the very large search space of functionally equivalent circuits and the necessity of applying transformations that temporarily decrease performance to achieve a final performance improvement. This paper presents Quarl, a learning-based quantum circuit optimizer. Applying reinforcement learning (RL) to quantum circuit optimization raises two main challenges: the large and varying action space and the non-uniform state representation. Quarl addresses these issues with a novel neural architecture and RL-training procedure. Our neural architecture decomposes the action space into two parts and leverages graph neural networks in its state representation, both of which are guided by the intuition that optimization decisions can be mostly guided by local reasoning while allowing global circuit-wide reasoning. Our evaluation shows that Quarl significantly outperforms existing circuit optimizers on almost all benchmark circuits. Surprisingly, Quarl can learn to perform rotation merging, a complex, non-local circuit optimization implemented as a separate pass in existing optimizers.",True,True,"Li, Zikun and Peng, Jinjun and Mei, Yixuan and Lin, Sina and Wu, Yi and Padon, Oded and Jia, Zhihao",2024.0,,,,Proceedings of the ACM on Programming Languages
Hybrid action Reinforcement Learning for quantum architecture search,2511.04967v2,du2022quantum,\cite{du2022quantum},Quantum circuit architecture search for variational quantum algorithms,,,True,False,"Du, Yuxuan and Huang, Tao and You, Shan and Hsieh, Min-Hsiu and Tao, Dacheng",2022.0,,,,npj Quantum Information
Hybrid action Reinforcement Learning for quantum architecture search,2511.04967v2,he2024training,\cite{he2024training},Training-free quantum architecture search,,,True,False,"He, Zhimin and Deng, Maijie and Zheng, Shenggen and Li, Lvzhou and Situ, Haozhen",2024.0,,,,
Hybrid action Reinforcement Learning for quantum architecture search,2511.04967v2,hausknecht2015deep,\cite{hausknecht2015deep},Deep Reinforcement Learning in Parameterized Action Space,https://arxiv.org/abs/1511.04143v5,"Recent work has shown that deep neural networks are capable of approximating both value functions and policies in reinforcement learning domains featuring continuous state and action spaces. However, to the best of our knowledge no previous work has succeeded at using deep neural networks in structured (parameterized) continuous action spaces. To fill this gap, this paper focuses on learning within the domain of simulated RoboCup soccer, which features a small set of discrete action types, each of which is parameterized with continuous variables. The best learned agent can score goals more reliably than the 2012 RoboCup champion agent. As such, this paper represents a successful extension of deep reinforcement learning to the class of parameterized action space MDPs.",True,True,"Hausknecht, Matthew and Stone, Peter",2015.0,,,,arXiv preprint arXiv:1511.04143
Hybrid action Reinforcement Learning for quantum architecture search,2511.04967v2,fan2019hybrid,\cite{fan2019hybrid},Hybrid Actor-Critic Reinforcement Learning in Parameterized Action Space,https://arxiv.org/abs/1903.01344v3,"In this paper we propose a hybrid architecture of actor-critic algorithms for reinforcement learning in parameterized action space, which consists of multiple parallel sub-actor networks to decompose the structured action space into simpler action spaces along with a critic network to guide the training of all sub-actor networks. While this paper is mainly focused on parameterized action space, the proposed architecture, which we call hybrid actor-critic, can be extended for more general action spaces which has a hierarchical structure. We present an instance of the hybrid actor-critic architecture based on proximal policy optimization (PPO), which we refer to as hybrid proximal policy optimization (H-PPO). Our experiments test H-PPO on a collection of tasks with parameterized action space, where H-PPO demonstrates superior performance over previous methods of parameterized action reinforcement learning.",True,True,"Fan, Zhou and Su, Rui and Zhang, Weinan and Yu, Yong",2019.0,,,,arXiv preprint arXiv:1903.01344
Hybrid action Reinforcement Learning for quantum architecture search,2511.04967v2,lihyar,\cite{lihyar},HyAR: Addressing Discrete-Continuous Action Reinforcement Learning via Hybrid Action Representation,https://arxiv.org/abs/2109.05490v3,"Discrete-continuous hybrid action space is a natural setting in many practical problems, such as robot control and game AI. However, most previous Reinforcement Learning (RL) works only demonstrate the success in controlling with either discrete or continuous action space, while seldom take into account the hybrid action space. One naive way to address hybrid action RL is to convert the hybrid action space into a unified homogeneous action space by discretization or continualization, so that conventional RL algorithms can be applied. However, this ignores the underlying structure of hybrid action space and also induces the scalability issue and additional approximation difficulties, thus leading to degenerated results. In this paper, we propose Hybrid Action Representation (HyAR) to learn a compact and decodable latent representation space for the original hybrid action space. HyAR constructs the latent space and embeds the dependence between discrete action and continuous parameter via an embedding table and conditional Variantional Auto-Encoder (VAE). To further improve the effectiveness, the action representation is trained to be semantically smooth through unsupervised environmental dynamics prediction. Finally, the agent then learns its policy with conventional DRL algorithms in the learned representation space and interacts with the environment by decoding the hybrid action embeddings to the original action space. We evaluate HyAR in a variety of environments with discrete-continuous action space. The results demonstrate the superiority of HyAR when compared with previous baselines, especially for high-dimensional action spaces.",True,True,"Li, Boyan and Tang, Hongyao and ZHENG, YAN and HAO, Jianye and Li, Pengyi and Wang, Zhen and Meng, Zhaopeng and Wang, LI",2022.0,,,,
CUNQA: a Distributed Quantum Computing emulator for HPC,2511.05209v1,chundury2024qfw,\cite{chundury2024qfw},QFw: A Quantum Framework for Large-scale HPC Ecosystems,,,True,False,"Chundury, Srikar and Shehata, Amir and Naughton III, Thomas and Kim, Seongmin and Mueller, Frank and Suh, In-Saeng",2024.0,,,,
CUNQA: a Distributed Quantum Computing emulator for HPC,2511.05209v1,beck2024,\cite{beck2024},Integrating quantum computing resources into scientific HPC ecosystems,,,True,False,Thomas Beck and Alessandro Baroni and Ryan Bennink and Gilles Buchs and Eduardo Antonio Coello Pérez and Rafael Ferreira {da Silva} and Muralikrishnan Gopalakrishnan Meena and Kalyan Gottiparthi and Peter Groszkowski and Travis S. Humble and Ryan Landfield and Ketan Maheshwari and Sarp Oral and Michael A. Sandoval and Amir Shehata and In-Saeng Suh and Christopher Zimmer,2024.0,,https://www.sciencedirect.com/science/article/pii/S0167739X24003583,https://doi.org/10.1016/j.future.2024.06.058,Future Generation Computer Systems
CUNQA: a Distributed Quantum Computing emulator for HPC,2511.05209v1,shehata2024frameworkintegratingquantumsimulation,\cite{shehata2024frameworkintegratingquantumsimulation},A Framework for Integrating Quantum Simulation and High Performance Computing,,,True,False,Amir Shehata and Thomas Naughton and In-Saeng Suh,2024.0,,https://arxiv.org/abs/2408.08098,,
CUNQA: a Distributed Quantum Computing emulator for HPC,2511.05209v1,shehata2026,\cite{shehata2026},Bridging paradigms: Designing for HPC-Quantum convergence,,,True,False,Amir Shehata and Peter Groszkowski and Thomas Naughton and Muralikrishnan {Gopalakrishnan Meena} and Elaine Wong and Daniel Claudino and Rafael {Ferreira da Silva} and Thomas Beck,2026.0,,https://www.sciencedirect.com/science/article/pii/S0167739X25002754,https://doi.org/10.1016/j.future.2025.107980,Future Generation Computer Systems
CUNQA: a Distributed Quantum Computing emulator for HPC,2511.05209v1,kaya2024,\cite{kaya2024},A Software Platform to Support Disaggregated Quantum Accelerators,,,True,False,"Kaya, Ercüment and Echavarria, Jorge and Farooqi, Muhammad Nufail and Swierkowska, Aleksandra and Hopf, Patrick and Mete, Burak and Burgholzer, Lukas and Wille, Robert and Schulz, Laura and Schulz, Martin",2024.0,,,10.1109/SCW63240.2024.00205,
CUNQA: a Distributed Quantum Computing emulator for HPC,2511.05209v1,burgholzer2025mqss,\cite{burgholzer2025mqss},"The Munich Quantum Software Stack: Connecting End Users, Integrating Diverse Quantum Technologies, Accelerating HPC",,,True,False,Lukas Burgholzer and Jorge Echavarria and Patrick Hopf and Yannick Stade and Damian Rovara and Ludwig Schmid and Ercüment Kaya and Burak Mete and Muhammad Nufail Farooqi and Minh Chung and Marco De Pascale and Laura Schulz and Martin Schulz and Robert Wille,2025.0,,https://arxiv.org/abs/2509.02674,,
CUNQA: a Distributed Quantum Computing emulator for HPC,2511.05209v1,kaya2024qpi,\cite{kaya2024qpi},QPI: A Programming Interface for Quantum Computers,,,True,False,"Kaya, Ercüment and Mete, Burak and Schulz, Laura and Farooqi, Muhammad Nufail and Echavarria, Jorge and Schulz, Martin",2024.0,,,10.1109/QCE60285.2024.10293,
CUNQA: a Distributed Quantum Computing emulator for HPC,2511.05209v1,mahesh2025conqure,\cite{mahesh2025conqure},CONQURE: A Co-Execution Environment for Quantum and Classical Resources,,,True,False,Atulya Mahesh and Swastik Mittal and Frank Mueller,2025.0,,https://arxiv.org/abs/2505.02241,,
CUNQA: a Distributed Quantum Computing emulator for HPC,2511.05209v1,Du2025,\cite{Du2025},QCCP: a taskflow programming model for emerging computing scenario,,,True,False,"Du, Qiming and Xu, Jinchen and Zhu, Yu and Lian, Hang and Xiong, Qibing and Zheng, Danyang and Liu, Yi and Tu, Zheng and Shan, Zheng",2025.0,Feb,https://doi.org/10.1140/epjqt/s40507-025-00318-5,10.1140/epjqt/s40507-025-00318-5,EPJ Quantum Technology
CUNQA: a Distributed Quantum Computing emulator for HPC,2511.05209v1,claudino2024,\cite{claudino2024},Parallel Quantum Computing Simulations via Quantum Accelerator Platform Virtualization,https://arxiv.org/abs/2406.03466v1,"Quantum circuit execution is the central task in quantum computation. Due to inherent quantum-mechanical constraints, quantum computing workflows often involve a considerable number of independent measurements over a large set of slightly different quantum circuits. Here we discuss a simple model for parallelizing simulation of such quantum circuit executions that is based on introducing a large array of virtual quantum processing units, mapped to classical HPC nodes, as a parallel quantum computing platform. Implemented within the XACC framework, the model can readily take advantage of its backend-agnostic features, enabling parallel quantum circuit execution over any target backend supported by XACC. We illustrate the performance of this approach by demonstrating strong scaling in two pertinent domain science problems, namely in computing the gradients for the multi-contracted variational quantum eigensolver and in data-driven quantum circuit learning, where we vary the number of qubits and the number of circuit layers. The latter (classical) simulation leverages the cuQuantum SDK library to run efficiently on GPU-accelerated HPC platforms.",True,True,"Claudino, Daniel and Lyakh, Dmitry I. and McCaskey, Alexander J.",2024.0,nov,https://doi.org/10.1016/j.future.2024.06.007,10.1016/j.future.2024.06.007,Future Gener. Comput. Syst.
CUNQA: a Distributed Quantum Computing emulator for HPC,2511.05209v1,mccaskey2020xacc,\cite{mccaskey2020xacc},XACC: A System-Level Software Infrastructure for Heterogeneous Quantum-Classical Computing,https://arxiv.org/abs/1911.02452v1,"Quantum programming techniques and software have advanced significantly over the past five years, with a majority focusing on high-level language frameworks targeting remote REST library APIs. As quantum computing architectures advance and become more widely available, lower-level, system software infrastructures will be needed to enable tighter, co-processor programming and access models. Here we present XACC, a system-level software infrastructure for quantum-classical computing that promotes a service-oriented architecture to expose interfaces for core quantum programming, compilation, and execution tasks. We detail XACC's interfaces, their interactions, and its implementation as a hardware-agnostic framework for both near-term and future quantum-classical architectures. We provide concrete examples demonstrating the utility of this framework with paradigmatic tasks. Our approach lays the foundation for the development of compilers, associated runtimes, and low-level system tools tightly integrating quantum and classical workflows.",True,True,Alexander J McCaskey and Dmitry I Lyakh and Eugene F Dumitrescu and Sarah S Powers and Travis S Humble,2020.0,feb,https://doi.org/10.1088,10.1088/2058-9565/ab6bf6,Quantum Science and Technology
Energy Loss Functions for Physical Systems,2511.02087v1,lecun2006tutorial,\cite{lecun2006tutorial},A tutorial on energy-based learning,,,True,False,"LeCun, Yann and Chopra, Sumit and Hadsell, Raia and Ranzato, M and Huang, Fujie and others",2006.0,,,,Predicting structured data
Energy Loss Functions for Physical Systems,2511.02087v1,du2019implicit,\cite{du2019implicit},Implicit generation and modeling with energy based models,,,True,False,"Du, Yilun and Mordatch, Igor",2019.0,,,,Advances in neural information processing systems
Energy Loss Functions for Physical Systems,2511.02087v1,grathwohl2019your,\cite{grathwohl2019your},Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One,https://arxiv.org/abs/1912.03263v3,"We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x,y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may beused and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, andout-of-distribution detection while also enabling our models to generate samplesrivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and presentan approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-artin both generative and discriminative learning within one hybrid model.",True,True,"Grathwohl, Will and Wang, Kuan-Chieh and Jacobsen, J{\""o}rn-Henrik and Duvenaud, David and Norouzi, Mohammad and Swersky, Kevin",2019.0,,,,arXiv preprint arXiv:1912.03263
Energy Loss Functions for Physical Systems,2511.02087v1,raissi2019physics,\cite{raissi2019physics},Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations,,,True,False,"Raissi, Maziar and Perdikaris, Paris and Karniadakis, George E",2019.0,,,,Journal of Computational physics
Energy Loss Functions for Physical Systems,2511.02087v1,bastek2024physics,\cite{bastek2024physics},Physics-Informed Diffusion Models,https://arxiv.org/abs/2403.14404v4,"Generative models such as denoising diffusion models are quickly advancing their ability to approximate highly complex data distributions. They are also increasingly leveraged in scientific machine learning, where samples from the implied data distribution are expected to adhere to specific governing equations. We present a framework that unifies generative modeling and partial differential equation fulfillment by introducing a first-principle-based loss term that enforces generated samples to fulfill the underlying physical constraints. Our approach reduces the residual error by up to two orders of magnitude compared to previous work in a fluid flow case study and outperforms task-specific frameworks in relevant metrics for structural topology optimization. We also present numerical evidence that our extended training objective acts as a natural regularization mechanism against overfitting. Our framework is simple to implement and versatile in its applicability for imposing equality and inequality constraints as well as auxiliary optimization objectives.",True,True,"Bastek, Jan-Hendrik and Sun, WaiChing and Kochmann, Dennis M",2024.0,,,,arXiv preprint arXiv:2403.14404
Energy Loss Functions for Physical Systems,2511.02087v1,greydanus2019hamiltonian,\cite{greydanus2019hamiltonian},Hamiltonian Neural Networks,https://arxiv.org/abs/1906.01563v3,"Even though neural networks enjoy widespread use, they still struggle to learn the basic laws of physics. How might we endow them with better inductive biases? In this paper, we draw inspiration from Hamiltonian mechanics to train models that learn and respect exact conservation laws in an unsupervised manner. We evaluate our models on problems where conservation of energy is important, including the two-body problem and pixel observations of a pendulum. Our model trains faster and generalizes better than a regular neural network. An interesting side effect is that our model is perfectly reversible in time.",True,True,"Greydanus, Samuel and Dzamba, Misko and Yosinski, Jason",2019.0,,,,Advances in neural information processing systems
Energy Loss Functions for Physical Systems,2511.02087v1,cranmer2020lagrangian,\cite{cranmer2020lagrangian},Lagrangian Neural Networks,https://arxiv.org/abs/2003.04630v2,"Accurate models of the world are built upon notions of its underlying symmetries. In physics, these symmetries correspond to conservation laws, such as for energy and momentum. Yet even though neural network models see increasing use in the physical sciences, they struggle to learn these symmetries. In this paper, we propose Lagrangian Neural Networks (LNNs), which can parameterize arbitrary Lagrangians using neural networks. In contrast to models that learn Hamiltonians, LNNs do not require canonical coordinates, and thus perform well in situations where canonical momenta are unknown or difficult to compute. Unlike previous approaches, our method does not restrict the functional form of learned energies and will produce energy-conserving models for a variety of tasks. We test our approach on a double pendulum and a relativistic particle, demonstrating energy conservation where a baseline approach incurs dissipation and modeling relativity without canonical coordinates where a Hamiltonian approach fails. Finally, we show how this model can be applied to graphs and continuous systems using a Lagrangian Graph Network, and demonstrate it on the 1D wave equation.",True,True,"Cranmer, Miles and Greydanus, Sam and Hoyer, Stephan and Battaglia, Peter and Spergel, David and Ho, Shirley",2020.0,,,,arXiv preprint arXiv:2003.04630
Energy Loss Functions for Physical Systems,2511.02087v1,noe2019boltzmann,\cite{noe2019boltzmann},Boltzmann Generators -- Sampling Equilibrium States of Many-Body Systems with Deep Learning,https://arxiv.org/abs/1812.01729v2,"Computing equilibrium states in condensed-matter many-body systems, such as solvated proteins, is a long-standing challenge. Lacking methods for generating statistically independent equilibrium samples in ""one shot"", vast computational effort is invested for simulating these system in small steps, e.g., using Molecular Dynamics. Combining deep learning and statistical mechanics, we here develop Boltzmann Generators, that are shown to generate unbiased one-shot equilibrium samples of representative condensed matter systems and proteins. Boltzmann Generators use neural networks to learn a coordinate transformation of the complex configurational equilibrium distribution to a distribution that can be easily sampled. Accurate computation of free energy differences and discovery of new configurations are demonstrated, providing a statistical mechanics tool that can avoid rare events during sampling without prior knowledge of reaction coordinates.",True,True,"No{\'e}, Frank and Olsson, Simon and K{\""o}hler, Jonas and Wu, Hao",2019.0,,,,Science
Energy Loss Functions for Physical Systems,2511.02087v1,kohler2020equivariant,\cite{kohler2020equivariant},Equivariant Flows: Exact Likelihood Generative Learning for Symmetric Densities,https://arxiv.org/abs/2006.02425v2,"Normalizing flows are exact-likelihood generative neural networks which approximately transform samples from a simple prior distribution to samples of the probability distribution of interest. Recent work showed that such generative models can be utilized in statistical mechanics to sample equilibrium states of many-body systems in physics and chemistry. To scale and generalize these results, it is essential that the natural symmetries in the probability density -- in physics defined by the invariances of the target potential -- are built into the flow. We provide a theoretical sufficient criterion showing that the distribution generated by \textit{equivariant} normalizing flows is invariant with respect to these symmetries by design. Furthermore, we propose building blocks for flows which preserve symmetries which are usually found in physical/chemical many-body particle systems. Using benchmark systems motivated from molecular physics, we demonstrate that those symmetry preserving flows can provide better generalization capabilities and sampling efficiency.",True,True,"K{\""o}hler, Jonas and Klein, Leon and No{\'e}, Frank",2020.0,,,,
Energy Loss Functions for Physical Systems,2511.02087v1,klein2025transferableboltzmanngenerators,\cite{klein2025transferableboltzmanngenerators},Transferable Boltzmann Generators,https://arxiv.org/abs/2406.14426v2,"The generation of equilibrium samples of molecular systems has been a long-standing problem in statistical physics. Boltzmann Generators are a generative machine learning method that addresses this issue by learning a transformation via a normalizing flow from a simple prior distribution to the target Boltzmann distribution of interest. Recently, flow matching has been employed to train Boltzmann Generators for small molecular systems in Cartesian coordinates. We extend this work and propose a first framework for Boltzmann Generators that are transferable across chemical space, such that they predict zero-shot Boltzmann distributions for test molecules without being retrained for these systems. These transferable Boltzmann Generators allow approximate sampling from the target distribution of unseen systems, as well as efficient reweighting to the target Boltzmann distribution. The transferability of the proposed framework is evaluated on dipeptides, where we show that it generalizes efficiently to unseen systems. Furthermore, we demonstrate that our proposed architecture enhances the efficiency of Boltzmann Generators trained on single molecular systems.",True,True,Leon Klein and Frank Noé,2025.0,,https://arxiv.org/abs/2406.14426,,
Walsh-Hadamard Neural Operators for Solving PDEs with Discontinuous Coefficients,2511.07347v1,kovachki2021neural,\cite{kovachki2021neural},Neural Operator: Learning Maps Between Function Spaces,https://arxiv.org/abs/2108.08481v6,"The classical development of neural networks has primarily focused on learning mappings between finite dimensional Euclidean spaces or finite sets. We propose a generalization of neural networks to learn operators, termed neural operators, that map between infinite dimensional function spaces. We formulate the neural operator as a composition of linear integral operators and nonlinear activation functions. We prove a universal approximation theorem for our proposed neural operator, showing that it can approximate any given nonlinear continuous operator. The proposed neural operators are also discretization-invariant, i.e., they share the same model parameters among different discretization of the underlying function spaces. Furthermore, we introduce four classes of efficient parameterization, viz., graph neural operators, multi-pole graph neural operators, low-rank neural operators, and Fourier neural operators. An important application for neural operators is learning surrogate maps for the solution operators of partial differential equations (PDEs). We consider standard PDEs such as the Burgers, Darcy subsurface flow, and the Navier-Stokes equations, and show that the proposed neural operators have superior performance compared to existing machine learning based methodologies, while being several orders of magnitude faster than conventional PDE solvers.",True,True,"Kovachki, Nikola and Li, Zongyi and Liu, Burigede and Azizzadenesheli, Kamyar and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima",2023.0,,,,Journal of Machine Learning Research
Walsh-Hadamard Neural Operators for Solving PDEs with Discontinuous Coefficients,2511.07347v1,lu2021learning,\cite{lu2021learning},Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators,,,True,False,"Lu, Lu and Jin, Pengzhan and Pang, Guofei and Zhang, Zhongqiang and Karniadakis, George Em",2021.0,,,,Nature Machine Intelligence
Walsh-Hadamard Neural Operators for Solving PDEs with Discontinuous Coefficients,2511.07347v1,li2020fourier,\cite{li2020fourier},Fourier neural operator for parametric partial differential equations,,,True,False,"Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima",2020.0,,,,arXiv preprint arXiv:2010.08895
Walsh-Hadamard Neural Operators for Solving PDEs with Discontinuous Coefficients,2511.07347v1,li2021fourier,\cite{li2021fourier},{Fourier neural operator with learned deformations for pdes on general geometries},,,True,False,"Li, Zongyi and Huang, Daniel Z. and Liu, Burigede and Anandkumar, Anima",2023.0,,,,{Journal of Machine Learning Research}
Walsh-Hadamard Neural Operators for Solving PDEs with Discontinuous Coefficients,2511.07347v1,tran2021factorized,\cite{tran2021factorized},Factorized Fourier Neural Operators,https://arxiv.org/abs/2111.13802v4,"We propose the Factorized Fourier Neural Operator (F-FNO), a learning-based approach for simulating partial differential equations (PDEs). Starting from a recently proposed Fourier representation of flow fields, the F-FNO bridges the performance gap between pure machine learning approaches to that of the best numerical or hybrid solvers. This is achieved with new representations - separable spectral layers and improved residual connections - and a combination of training strategies such as the Markov assumption, Gaussian noise, and cosine learning rate decay. On several challenging benchmark PDEs on regular grids, structured meshes, and point clouds, the F-FNO can scale to deeper networks and outperform both the FNO and the geo-FNO, reducing the error by 83% on the Navier-Stokes problem, 31% on the elasticity problem, 57% on the airfoil flow problem, and 60% on the plastic forging problem. Compared to the state-of-the-art pseudo-spectral method, the F-FNO can take a step size that is an order of magnitude larger in time and achieve an order of magnitude speedup to produce the same solution quality.",True,True,"Tran, Alasdair and Mathews, Alexander and Xie, Lexing and Ong, Cheng Soon",2021.0,,,,arXiv preprint arXiv:2111.13802
Walsh-Hadamard Neural Operators for Solving PDEs with Discontinuous Coefficients,2511.07347v1,cao2021choosing,\cite{cao2021choosing},Choose a Transformer: Fourier or Galerkin,https://arxiv.org/abs/2105.14995v4,"In this paper, we apply the self-attention from the state-of-the-art Transformer in Attention Is All You Need for the first time to a data-driven operator learning problem related to partial differential equations. An effort is put together to explain the heuristics of, and to improve the efficacy of the attention mechanism. By employing the operator approximation theory in Hilbert spaces, it is demonstrated for the first time that the softmax normalization in the scaled dot-product attention is sufficient but not necessary. Without softmax, the approximation capacity of a linearized Transformer variant can be proved to be comparable to a Petrov-Galerkin projection layer-wise, and the estimate is independent with respect to the sequence length. A new layer normalization scheme mimicking the Petrov-Galerkin projection is proposed to allow a scaling to propagate through attention layers, which helps the model achieve remarkable accuracy in operator learning tasks with unnormalized data. Finally, we present three operator learning experiments, including the viscid Burgers' equation, an interface Darcy flow, and an inverse interface coefficient identification problem. The newly proposed simple attention-based operator learner, Galerkin Transformer, shows significant improvements in both training cost and evaluation accuracy over its softmax-normalized counterparts.",True,True,"Cao, Shuhao",2021.0,,,,Advances in Neural Information Processing Systems
Walsh-Hadamard Neural Operators for Solving PDEs with Discontinuous Coefficients,2511.07347v1,li2022fourier,\cite{li2022fourier},Fourier neural operator with learned deformations for PDEs on general geometries,,,True,False,"Li, Zongyi and Huang, Daniel Zhengyu and Liu, Burigede and Anandkumar, Anima",2022.0,,,,Journal of Computational Physics
Walsh-Hadamard Neural Operators for Solving PDEs with Discontinuous Coefficients,2511.07347v1,pathak2022fourcastnet,\cite{pathak2022fourcastnet},FourCastNet: A Global Data-driven High-resolution Weather Model using Adaptive Fourier Neural Operators,https://arxiv.org/abs/2202.11214v1,"FourCastNet, short for Fourier Forecasting Neural Network, is a global data-driven weather forecasting model that provides accurate short to medium-range global predictions at $0.25^{\circ}$ resolution. FourCastNet accurately forecasts high-resolution, fast-timescale variables such as the surface wind speed, precipitation, and atmospheric water vapor. It has important implications for planning wind energy resources, predicting extreme weather events such as tropical cyclones, extra-tropical cyclones, and atmospheric rivers. FourCastNet matches the forecasting accuracy of the ECMWF Integrated Forecasting System (IFS), a state-of-the-art Numerical Weather Prediction (NWP) model, at short lead times for large-scale variables, while outperforming IFS for variables with complex fine-scale structure, including precipitation. FourCastNet generates a week-long forecast in less than 2 seconds, orders of magnitude faster than IFS. The speed of FourCastNet enables the creation of rapid and inexpensive large-ensemble forecasts with thousands of ensemble-members for improving probabilistic forecasting. We discuss how data-driven deep learning models such as FourCastNet are a valuable addition to the meteorology toolkit to aid and augment NWP models.",True,True,"Pathak, Jaideep and Subramanian, Shashank and Harrington, Peter and Raja, Sanjeev and Chattopadhyay, Ashesh and Mardani, Morteza and others",2022.0,,,,arXiv preprint arXiv:2202.11214
Walsh-Hadamard Neural Operators for Solving PDEs with Discontinuous Coefficients,2511.07347v1,carslaw1906theory,\cite{carslaw1906theory},{The Theory of Fourier’s Series and Integrals},,,True,False,"{Carslaw, Horatio S}",1906.0,,,,{Nature}
Walsh-Hadamard Neural Operators for Solving PDEs with Discontinuous Coefficients,2511.07347v1,raissi2019physics,\cite{raissi2019physics},Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations,,,True,False,"Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em",2019.0,,,,Journal of Computational Physics
Walsh-Hadamard Neural Operators for Solving PDEs with Discontinuous Coefficients,2511.07347v1,karniadakis2021physics,\cite{karniadakis2021physics},Physics-informed machine learning,,,True,False,"Karniadakis, George Em and Kevrekidis, Ioannis G and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu",2021.0,,,,Nature Reviews Physics
Walsh-Hadamard Neural Operators for Solving PDEs with Discontinuous Coefficients,2511.07347v1,mao2020physics,\cite{mao2020physics},Physics-informed neural networks for high-speed flows,,,True,False,"Mao, Zhiping and Jagtap, Ameya D and Karniadakis, George Em",2020.0,,,,Computer Methods in Applied Mechanics and Engineering
Walsh-Hadamard Neural Operators for Solving PDEs with Discontinuous Coefficients,2511.07347v1,walsh1923closed,\cite{walsh1923closed},A closed set of normal orthogonal functions,,,True,False,"Walsh, Joseph L",1923.0,,,,American Journal of Mathematics
Walsh-Hadamard Neural Operators for Solving PDEs with Discontinuous Coefficients,2511.07347v1,beauchamp1975walsh,\cite{beauchamp1975walsh},Walsh functions and their applications,,,True,False,"Beauchamp, Kenneth G",1975.0,,,,
Walsh-Hadamard Neural Operators for Solving PDEs with Discontinuous Coefficients,2511.07347v1,fine1949walsh,\cite{fine1949walsh},On the Walsh functions,,,True,False,"Fine, Nathan J",1949.0,,,,Transactions of the American Mathematical Society
Walsh-Hadamard Neural Operators for Solving PDEs with Discontinuous Coefficients,2511.07347v1,fino1976unified,\cite{fino1976unified},Unified matrix treatment of the fast Walsh-Hadamard transform,,,True,False,"Fino, Benjamin J and Algazi, V Ralph",1976.0,,,,IEEE Transactions on Computers
Walsh-Hadamard Neural Operators for Solving PDEs with Discontinuous Coefficients,2511.07347v1,shanmugam1979walsh,\cite{shanmugam1979walsh},Walsh-Hadamard transform for image coding,,,True,False,"Shanmugam, K Sam and Breipohl, Arthur M",1979.0,,,,Proceedings of the IEEE
Walsh-Hadamard Neural Operators for Solving PDEs with Discontinuous Coefficients,2511.07347v1,li2020neural,\cite{li2020neural},Neural Operator: Graph Kernel Network for Partial Differential Equations,https://arxiv.org/abs/2003.03485v1,"The classical development of neural networks has been primarily for mappings between a finite-dimensional Euclidean space and a set of classes, or between two finite-dimensional Euclidean spaces. The purpose of this work is to generalize neural networks so that they can learn mappings between infinite-dimensional spaces (operators). The key innovation in our work is that a single set of network parameters, within a carefully designed network architecture, may be used to describe mappings between infinite-dimensional spaces and between different finite-dimensional approximations of those spaces. We formulate approximation of the infinite-dimensional mapping by composing nonlinear activation functions and a class of integral operators. The kernel integration is computed by message passing on graph networks. This approach has substantial practical consequences which we will illustrate in the context of mappings between input data to partial differential equations (PDEs) and their solutions. In this context, such learned networks can generalize among different approximation methods for the PDE (such as finite difference or finite element methods) and among approximations corresponding to different underlying levels of resolution and discretization. Experiments confirm that the proposed graph kernel network does have the desired properties and show competitive performance compared to the state of the art solvers.",True,True,"Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima",2020.0,,,,arXiv preprint arXiv:2003.03485
Walsh-Hadamard Neural Operators for Solving PDEs with Discontinuous Coefficients,2511.07347v1,li2020multipole,\cite{li2020multipole},Multipole graph neural operator for parametric partial differential equations,,,True,False,"Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Stuart, Andrew and Bhattacharya, Kaushik and Anandkumar, Anima",2020.0,,,,Advances in Neural Information Processing Systems
Walsh-Hadamard Neural Operators for Solving PDEs with Discontinuous Coefficients,2511.07347v1,mo2019deep,\cite{mo2019deep},Deep convolutional encoder-decoder networks for uncertainty quantification of dynamic multiphase flow in heterogeneous media,,,True,False,"Mo, Shaoxing and Zhu, Yinhao and Zabaras, Nicholas and Shi, Xiaoqing and Wu, Jichun",2019.0,,,,Water Resources Research
Walsh-Hadamard Neural Operators for Solving PDEs with Discontinuous Coefficients,2511.07347v1,zhu2019physics,\cite{zhu2019physics},Bayesian Deep Convolutional Encoder-Decoder Networks for Surrogate Modeling and Uncertainty Quantification,https://arxiv.org/abs/1801.06879v1,"We are interested in the development of surrogate models for uncertainty quantification and propagation in problems governed by stochastic PDEs using a deep convolutional encoder-decoder network in a similar fashion to approaches considered in deep learning for image-to-image regression tasks. Since normal neural networks are data intensive and cannot provide predictive uncertainty, we propose a Bayesian approach to convolutional neural nets. A recently introduced variational gradient descent algorithm based on Stein's method is scaled to deep convolutional networks to perform approximate Bayesian inference on millions of uncertain network parameters. This approach achieves state of the art performance in terms of predictive accuracy and uncertainty quantification in comparison to other approaches in Bayesian neural networks as well as techniques that include Gaussian processes and ensemble methods even when the training data size is relatively small. To evaluate the performance of this approach, we consider standard uncertainty quantification benchmark problems including flow in heterogeneous media defined in terms of limited data-driven permeability realizations. The performance of the surrogate model developed is very good even though there is no underlying structure shared between the input (permeability) and output (flow/pressure) fields as is often the case in the image-to-image regression models used in computer vision problems. Studies are performed with an underlying stochastic input dimensionality up to $4,225$ where most other uncertainty quantification methods fail. Uncertainty propagation tasks are considered and the predictive output Bayesian statistics are compared to those obtained with Monte Carlo estimates.",True,True,"Zhu, Yinhao and Zabaras, Nicholas",2018.0,,,,Journal of Computational Physics
Walsh-Hadamard Neural Operators for Solving PDEs with Discontinuous Coefficients,2511.07347v1,raissi2018numerical,\cite{raissi2018numerical},Numerical Gaussian processes for time-dependent and nonlinear partial differential equations,,,True,False,"Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em",2018.0,,,,SIAM Journal on Scientific Computing
Walsh-Hadamard Neural Operators for Solving PDEs with Discontinuous Coefficients,2511.07347v1,cai2021physics,\cite{cai2021physics},Physics-informed neural networks (PINNs) for heat transfer problems,,,True,False,"Cai, Shengze and Mao, Zhiping and Wang, Zhicheng and Yin, Minglang and Karniadakis, George Em",2021.0,,,,Journal of Heat Transfer
Walsh-Hadamard Neural Operators for Solving PDEs with Discontinuous Coefficients,2511.07347v1,kashinath2021physics,\cite{kashinath2021physics},Physics-informed machine learning: Case studies for weather and climate modelling,,,True,False,"Kashinath, Karthik and Mustafa, Mohamad and Albert, Adrian and Wu, Jiali and Jiang, Chengchao and Esmaeilzadeh, Soheil and others",2021.0,,,,Philosophical Transactions of the Royal Society A
VEDA: 3D Molecular Generation via Variance-Exploding Diffusion with Annealing,2511.09568v1,ho2020denoising,\cite{ho2020denoising},Denoising Diffusion Probabilistic Models,https://arxiv.org/abs/2006.11239v2,"We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion",True,True,"Ho, Jonathan and Jain, Ajay and Abbeel, Pieter",2020.0,,,,Advances in neural information processing systems
VEDA: 3D Molecular Generation via Variance-Exploding Diffusion with Annealing,2511.09568v1,songscore,\cite{songscore},Score-Based Generative Modeling through Stochastic Differential Equations,https://arxiv.org/abs/2011.13456v2,"Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.",True,True,"Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben",,,,,
VEDA: 3D Molecular Generation via Variance-Exploding Diffusion with Annealing,2511.09568v1,karras2022elucidating,\cite{karras2022elucidating},Elucidating the design space of diffusion-based generative models,,,True,False,"Karras, Tero and Aittala, Miika and Aila, Timo and Laine, Samuli",2022.0,,,,Advances in neural information processing systems
VEDA: 3D Molecular Generation via Variance-Exploding Diffusion with Annealing,2511.09568v1,lipman2023flow,\cite{lipman2023flow},Flow Matching for Generative Modeling,,,True,False,"Lipman, Yaron and Chen, Ricky TQ and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt",2023.0,,,,
VEDA: 3D Molecular Generation via Variance-Exploding Diffusion with Annealing,2511.09568v1,gao2024diffusion,\cite{gao2024diffusion},Diffusion meets flow matching: Two sides of the same coin,,,True,False,"Gao, Ruiqi and Hoogeboom, Emiel and Heek, Jonathan and Bortoli, VD and Murphy, Kevin P and Salimans, Tim",2024.0,,,,The Internet
VEDA: 3D Molecular Generation via Variance-Exploding Diffusion with Annealing,2511.09568v1,devlin2019bert,\cite{devlin2019bert},BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,https://arxiv.org/abs/1810.04805v2,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.
  BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",True,True,"Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina",2019.0,,,,
VEDA: 3D Molecular Generation via Variance-Exploding Diffusion with Annealing,2511.09568v1,austin2021structured,\cite{austin2021structured},Structured Denoising Diffusion Models in Discrete State-Spaces,https://arxiv.org/abs/2107.03006v3,"Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusion-like generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. 2021, by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss. For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on LM1B. On the image dataset CIFAR-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space DDPM model.",True,True,"Austin, Jacob and Johnson, Daniel D and Ho, Jonathan and Tarlow, Daniel and Van Den Berg, Rianne",2021.0,,,,Advances in neural information processing systems
VEDA: 3D Molecular Generation via Variance-Exploding Diffusion with Annealing,2511.09568v1,lou2024discrete,\cite{lou2024discrete},Discrete diffusion modeling by estimating the ratios of the data distribution,,,True,False,"Lou, Aaron and Meng, Chenlin and Ermon, Stefano",2024.0,,,,
VEDA: 3D Molecular Generation via Variance-Exploding Diffusion with Annealing,2511.09568v1,campbell2024generative,\cite{campbell2024generative},Generative Flows on Discrete State-Spaces: Enabling Multimodal Flows with Applications to Protein Co-Design,https://arxiv.org/abs/2402.04997v2,"Combining discrete and continuous data is an important capability for generative models. We present Discrete Flow Models (DFMs), a new flow-based model of discrete data that provides the missing link in enabling flow-based generative models to be applied to multimodal continuous and discrete data problems. Our key insight is that the discrete equivalent of continuous space flow matching can be realized using Continuous Time Markov Chains. DFMs benefit from a simple derivation that includes discrete diffusion models as a specific instance while allowing improved performance over existing diffusion-based approaches. We utilize our DFMs method to build a multimodal flow-based modeling framework. We apply this capability to the task of protein co-design, wherein we learn a model for jointly generating protein structure and sequence. Our approach achieves state-of-the-art co-design performance while allowing the same multimodal model to be used for flexible generation of the sequence or structure.",True,True,"Campbell, Andrew and Yim, Jason and Barzilay, Regina and Rainforth, Tom and Jaakkola, Tommi",2024.0,,,,
VEDA: 3D Molecular Generation via Variance-Exploding Diffusion with Annealing,2511.09568v1,gat2024discrete,\cite{gat2024discrete},Discrete Flow Matching,https://arxiv.org/abs/2407.15595v2,"Despite Flow Matching and diffusion models having emerged as powerful generative paradigms for continuous variables such as images and videos, their application to high-dimensional discrete data, such as language, is still limited. In this work, we present Discrete Flow Matching, a novel discrete flow paradigm designed specifically for generating discrete data. Discrete Flow Matching offers several key contributions:(i) it works with a general family of probability paths interpolating between source and target distributions; (ii) it allows for a generic formula for sampling from these probability paths using learned posteriors such as the probability denoiser ($x$-prediction) and noise-prediction ($ε$-prediction); (iii) practically, focusing on specific probability paths defined with different schedulers improves generative perplexity compared to previous discrete diffusion and flow models; and (iv) by scaling Discrete Flow Matching models up to 1.7B parameters, we reach 6.7% Pass@1 and 13.4% Pass@10 on HumanEval and 6.7% Pass@1 and 20.6% Pass@10 on 1-shot MBPP coding benchmarks. Our approach is capable of generating high-quality discrete data in a non-autoregressive fashion, significantly closing the gap between autoregressive models and discrete flow models.",True,True,"Gat, Itai and Remez, Tal and Shaul, Neta and Kreuk, Felix and Chen, Ricky TQ and Synnaeve, Gabriel and Adi, Yossi and Lipman, Yaron",2024.0,,,,Advances in Neural Information Processing Systems
VEDA: 3D Molecular Generation via Variance-Exploding Diffusion with Annealing,2511.09568v1,satorras2021n_EGNN,\cite{satorras2021n_EGNN},Equivariant Polynomials for Graph Neural Networks,https://arxiv.org/abs/2302.11556v2,"Graph Neural Networks (GNN) are inherently limited in their expressive power. Recent seminal works (Xu et al., 2019; Morris et al., 2019b) introduced the Weisfeiler-Lehman (WL) hierarchy as a measure of expressive power. Although this hierarchy has propelled significant advances in GNN analysis and architecture developments, it suffers from several significant limitations. These include a complex definition that lacks direct guidance for model improvement and a WL hierarchy that is too coarse to study current GNNs. This paper introduces an alternative expressive power hierarchy based on the ability of GNNs to calculate equivariant polynomials of a certain degree. As a first step, we provide a full characterization of all equivariant graph polynomials by introducing a concrete basis, significantly generalizing previous results. Each basis element corresponds to a specific multi-graph, and its computation over some graph data input corresponds to a tensor contraction problem. Second, we propose algorithmic tools for evaluating the expressiveness of GNNs using tensor contraction sequences, and calculate the expressive power of popular GNNs. Finally, we enhance the expressivity of common GNN architectures by adding polynomial features or additional operations / aggregations inspired by our theory. These enhanced GNNs demonstrate state-of-the-art results in experiments across multiple graph learning benchmarks.",True,True,"Satorras, V{\i}ctor Garcia and Hoogeboom, Emiel and Welling, Max",2021.0,,,,
VEDA: 3D Molecular Generation via Variance-Exploding Diffusion with Annealing,2511.09568v1,zhang2025d3mes,\cite{zhang2025d3mes},D3MES: Diffusion Transformer with multihead equivariant self-attention for 3D molecule generation,,,True,False,"Zhang, Zhejun and Chen, Yuanping and Chu, Shibing",2025.0,,,,arXiv preprint arXiv:2501.07077
VEDA: 3D Molecular Generation via Variance-Exploding Diffusion with Annealing,2511.09568v1,hoogeboom2022equivariant,\cite{hoogeboom2022equivariant},Equivariant diffusion for molecule generation in 3d,,,True,False,"Hoogeboom, Emiel and Satorras, V{\i}ctor Garcia and Vignac, Cl{\'e}ment and Welling, Max",2022.0,,,,
VEDA: 3D Molecular Generation via Variance-Exploding Diffusion with Annealing,2511.09568v1,xu2023geometric,\cite{xu2023geometric},Geometric latent diffusion models for 3d molecule generation,,,True,False,"Xu, Minkai and Powers, Alexander S and Dror, Ron O and Ermon, Stefano and Leskovec, Jure",2023.0,,,,
VEDA: 3D Molecular Generation via Variance-Exploding Diffusion with Annealing,2511.09568v1,song2023equivariantFM,\cite{song2023equivariantFM},Equivariant flow matching with hybrid probability transport for 3d molecule generation,,,True,False,"Song, Yuxuan and Gong, Jingjing and Xu, Minkai and Cao, Ziyao and Lan, Yanyan and Ermon, Stefano and Zhou, Hao and Ma, Wei-Ying",2023.0,,,,Advances in Neural Information Processing Systems
VEDA: 3D Molecular Generation via Variance-Exploding Diffusion with Annealing,2511.09568v1,song2023unified,\cite{song2023unified},Unified generative modeling of 3d molecules with bayesian flow networks,,,True,False,"Song, Yuxuan and Gong, Jingjing and Zhou, Hao and Zheng, Mingyue and Liu, Jingjing and Ma, Wei-Ying",2023.0,,,,
VEDA: 3D Molecular Generation via Variance-Exploding Diffusion with Annealing,2511.09568v1,irwin2025semlaflow,\cite{irwin2025semlaflow},SemlaFlow -- Efficient 3D Molecular Generation with Latent Attention and Equivariant Flow Matching,,,True,False,Ross Irwin and Alessandro Tibo and Jon Paul Janet and Simon Olsson,2025.0,,https://openreview.net/forum?id=bee2G6pEh0,,
VEDA: 3D Molecular Generation via Variance-Exploding Diffusion with Annealing,2511.09568v1,nikitin2025geom,\cite{nikitin2025geom},GEOM-Drugs Revisited: Toward More Chemically Accurate Benchmarks for 3D Molecule Generation,https://arxiv.org/abs/2505.00169v2,"Deep generative models have shown significant promise in generating valid 3D molecular structures, with the GEOM-Drugs dataset serving as a key benchmark. However, current evaluation protocols suffer from critical flaws, including incorrect valency definitions, bugs in bond order calculations, and reliance on force fields inconsistent with the reference data. In this work, we revisit GEOM-Drugs and propose a corrected evaluation framework: we identify and fix issues in data preprocessing, construct chemically accurate valency tables, and introduce a GFN2-xTB-based geometry and energy benchmark. We retrain and re-evaluate several leading models under this framework, providing updated performance metrics and practical recommendations for future benchmarking. Our results underscore the need for chemically rigorous evaluation practices in 3D molecular generation. Our recommended evaluation methods and GEOM-Drugs processing scripts are available at https://github.com/isayevlab/geom-drugs-3dgen-evaluation.",True,True,"Nikitin, Filipp and Dunn, Ian and Koes, David Ryan and Isayev, Olexandr",2025.0,,,,arXiv preprint arXiv:2505.00169
QEF: Reproducible and Exploratory Quantum Software Experiments,2511.04563v1,bergholm:2022:pennylane,\cite{bergholm:2022:pennylane},PennyLane: Automatic differentiation of hybrid quantum-classical computations,https://arxiv.org/abs/1811.04968v4,"PennyLane is a Python 3 software framework for differentiable programming of quantum computers. The library provides a unified architecture for near-term quantum computing devices, supporting both qubit and continuous-variable paradigms. PennyLane's core feature is the ability to compute gradients of variational quantum circuits in a way that is compatible with classical techniques such as backpropagation. PennyLane thus extends the automatic differentiation algorithms common in optimization and machine learning to include quantum and hybrid computations. A plugin system makes the framework compatible with any gate-based quantum simulator or hardware. We provide plugins for hardware providers including the Xanadu Cloud, Amazon Braket, and IBM Quantum, allowing PennyLane optimizations to be run on publicly accessible quantum devices. On the classical front, PennyLane interfaces with accelerated machine learning libraries such as TensorFlow, PyTorch, JAX, and Autograd. PennyLane can be used for the optimization of variational quantum eigensolvers, quantum approximate optimization, quantum machine learning models, and many other applications.",True,True,Ville Bergholm and Josh Izaac and Maria Schuld and Christian Gogolin and Shahnawaz Ahmed and Vishnu Ajith and M. Sohaib Alam and Guillermo Alonso-Linaje and B. AkashNarayanan and Ali Asadi and Juan Miguel Arrazola and Utkarsh Azad and Sam Banning and Carsten Blank and Thomas R Bromley and Benjamin A. Cordier and Jack Ceroni and Alain Delgado and Olivia Di Matteo and Amintor Dusko and Tanya Garg and Diego Guala and Anthony Hayes and Ryan Hill and Aroosa Ijaz and Theodor Isacsson and David Ittah and Soran Jahangiri and Prateek Jain and Edward Jiang and Ankit Khandelwal and Korbinian Kottmann and Robert A. Lang and Christina Lee and Thomas Loke and Angus Lowe and Keri McKiernan and Johannes Jakob Meyer and J. A. Montañez-Barrera and Romain Moyard and Zeyue Niu and Lee James O'Riordan and Steven Oud and Ashish Panigrahi and Chae-Yeun Park and Daniel Polatajko and Nicolás Quesada and Chase Roberts and Nahum Sá and Isidor Schoch and Borun Shi and Shuli Shu and Sukin Sim and Arshpreet Singh and Ingrid Strandberg and Jay Soni and Antal Száva and Slimane Thabet and Rodrigo A. Vargas-Hernández and Trevor Vincent and Nicola Vitucci and Maurice Weber and David Wierichs and Roeland Wiersema and Moritz Willmann and Vincent Wong and Shaoming Zhang and Nathan Killoran,2022.0,,,10.48550/arXiv.1811.04968,
QEF: Reproducible and Exploratory Quantum Software Experiments,2511.04563v1,broughton:2021:tensorflow,\cite{broughton:2021:tensorflow},TensorFlow Quantum: A Software Framework for Quantum Machine Learning,,,True,False,Michael Broughton and Guillaume Verdon and Trevor McCourt and Antonio J. Martinez and Jae Hyeon Yoo and Sergei V. Isakov and Philip Massey and Ramin Halavati and Murphy Yuezhen Niu and Alexander Zlokapa and Evan Peters and Owen Lockwood and Andrea Skolik and Sofiene Jerbi and Vedran Dunjko and Martin Leib and Michael Streif and David Von Dollen and Hongxiang Chen and Shuxiang Cao and Roeland Wiersema and Hsin-Yuan Huang and Jarrod R. McClean and Ryan Babbush and Sergio Boixo and Dave Bacon and Alan K. Ho and Hartmut Neven and Masoud Mohseni,2021.0,,,10.48550/arXiv.2003.02989,
QEF: Reproducible and Exploratory Quantum Software Experiments,2511.04563v1,sharma:2022:openqaoa,\cite{sharma:2022:openqaoa},{OpenQAOA-An SDK for QAOA},,,True,False,Vishal Sharma and Nur Shahidee Bin Saharan and Shao-Hen Chiew and Ezequiel Ignacio Rodríguez Chiacchio and Leonardo Disilvestro and Tommaso Federico Demarie and Ewan Munro,2022.0,,,10.48550/arXiv.2210.08695,
QEF: Reproducible and Exploratory Quantum Software Experiments,2511.04563v1,johnson:2022:qiskitruntime,\cite{johnson:2022:qiskitruntime},"Qiskit runtime, a quantum-classical execution platform for cloud-accessible quantum computers",,,True,False,"Johnson, Blake",2022.0,,,,
QEF: Reproducible and Exploratory Quantum Software Experiments,2511.04563v1,hooyberghs:2021:azure,\cite{hooyberghs:2021:azure},Introducing {{Microsoft Quantum Computing}} for {{Developers}}: {{Using}} the {{Quantum Development Kit}} and {{Q}}\#,,,True,False,"Hooyberghs, Johnny",,,,10.1007/978-1-4842-7246-6,
QEF: Reproducible and Exploratory Quantum Software Experiments,2511.04563v1,mauerer:2005:cqpl,\cite{mauerer:2005:cqpl},Semantics and Simulation of Communication in Quantum Programming,,,True,False,Wolfgang Mauerer,2005.0,,https://arxiv.org/abs/quant-ph/0511145,10.48550/arXiv.quant-ph/0511145,
QEF: Reproducible and Exploratory Quantum Software Experiments,2511.04563v1,carbonelli:24,\cite{carbonelli:24},Challenges for Quantum Software Engineering: An Industrial Use Case Perspective,,,True,False,"Carbonelli, Cecilia and Felderer, Michael and Jung, Matthias and Lobe, Elisabeth and Lochau, Malte and Luber, Sebastian and Mauerer, Wolfgang and Ramler, Rudolf and Schäfer, Ina and Schroth, Christoph",2024.0,4,,10.1007/978-3-031-64136-7_12,
QEF: Reproducible and Exploratory Quantum Software Experiments,2511.04563v1,sivarajah:2022:tierkreis,\cite{sivarajah:2022:tierkreis},Tierkreis: A dataflow framework for hybrid quantum-classical computing,,,True,False,"Sivarajah, Seyon and Heidemann, Lukas and Lawrence, Alan and Duncan, Ross",2022.0,,,10.1109/QCS56647.2022.00007,
QEF: Reproducible and Exploratory Quantum Software Experiments,2511.04563v1,beisel:2022:quokka,\cite{beisel:2022:quokka},Quokka: a service ecosystem for workflow-based execution of variational quantum algorithms,,,True,False,"Beisel, Martin and Barzen, Johanna and Garhofer, Simon and Leymann, Frank and Truger, Felix and Weder, Benjamin and Yussupov, Vladimir",2022.0,,,10.1007/978-3-031-26507-5_35,
QEF: Reproducible and Exploratory Quantum Software Experiments,2511.04563v1,lorenz:2025:benchmarkingquantum,\cite{lorenz:2025:benchmarkingquantum},Systematic benchmarking of quantum computers: status and recommendations,https://arxiv.org/abs/2503.04905v1,"Architectures for quantum computing can only be scaled up when they are accompanied by suitable benchmarking techniques. The document provides a comprehensive overview of the state and recommendations for systematic benchmarking of quantum computers. Benchmarking is crucial for assessing the performance of quantum computers, including the hardware, software, as well as algorithms and applications. The document highlights key aspects such as component-level, system-level, software-level, HPC-level, and application-level benchmarks. Component-level benchmarks focus on the performance of individual qubits and gates, while system-level benchmarks evaluate the entire quantum processor. Software-level benchmarks consider the compiler's efficiency and error mitigation techniques. HPC-level and cloud benchmarks address integration with classical systems and cloud platforms, respectively. Application-level benchmarks measure performance in real-world use cases. The document also discusses the importance of standardization to ensure reproducibility and comparability of benchmarks, and highlights ongoing efforts in the quantum computing community towards establishing these benchmarks. Recommendations for future steps emphasize the need for developing standardized evaluation routines and integrating benchmarks with broader quantum technology activities.",True,True,Jeanette Miriam Lorenz and Thomas Monz and Jens Eisert and Daniel Reitzner and Félicien Schopfer and Frédéric Barbaresco and Krzysztof Kurowski and Ward van der Schoot and Thomas Strohm and Jean Senellart and Cécile M. Perrault and Martin Knufinke and Ziyad Amodjee and Mattia Giardini,2025.0,,,10.48550/arXiv.2503.04905,
QEF: Reproducible and Exploratory Quantum Software Experiments,2511.04563v1,finvzgar:2022:quark,\cite{finvzgar:2022:quark},Quark: A framework for quantum computing application benchmarking,,,True,False,"Fin\v{z}gar, Jernej Rudi and Ross, Philipp and Hölscher, Leonhard and Klepsch, Johannes and Luckow, Andre",2022.0,,,10.1109/QCE53715.2022.00042,
QEF: Reproducible and Exploratory Quantum Software Experiments,2511.04563v1,tomesh:2022:supermarq,\cite{tomesh:2022:supermarq},SupermarQ: A Scalable Quantum Benchmark Suite,https://arxiv.org/abs/2202.11045v3,"The emergence of quantum computers as a new computational paradigm has been accompanied by speculation concerning the scope and timeline of their anticipated revolutionary changes. While quantum computing is still in its infancy, the variety of different architectures used to implement quantum computations make it difficult to reliably measure and compare performance. This problem motivates our introduction of SupermarQ, a scalable, hardware-agnostic quantum benchmark suite which uses application-level metrics to measure performance. SupermarQ is the first attempt to systematically apply techniques from classical benchmarking methodology to the quantum domain. We define a set of feature vectors to quantify coverage, select applications from a variety of domains to ensure the suite is representative of real workloads, and collect benchmark results from the IBM, IonQ, and AQT@LBNL platforms. Looking forward, we envision that quantum benchmarking will encompass a large cross-community effort built on open source, constantly evolving benchmark suites. We introduce SupermarQ as an important step in this direction.",True,True,Teague Tomesh and Pranav Gokhale and Victory Omole and Gokul Subramanian Ravi and Kaitlin N. Smith and Joshua Viszlai and Xin-Chuan Wu and Nikos Hardavellas and Margaret R. Martonosi and Frederic T. Chong,2022.0,,,10.48550/arXiv.2202.11045,
ForeSWE: Forecasting Snow-Water Equivalent with an Uncertainty-Aware Attention Model,2511.08856v1,subseasonalForecast,\cite{subseasonalForecast},{Subseasonal Forecast Skill of Snow Water Equivalent and Its Link with Temperature in Selected SubX Models},,,True,False,G. T. Diro and H. Lin,2020.0,,https://journals.ametsoc.org/view/journals/wefo/35/1/waf-d-19-0074.1.xml,10.1175/WAF-D-19-0074.1,Weather and Forecasting
ForeSWE: Forecasting Snow-Water Equivalent with an Uncertainty-Aware Attention Model,2511.08856v1,brodzik2016measures,\cite{brodzik2016measures},"{MEaSUREs calibrated enhanced-resolution passive microwave daily EASE-grid 2.0 brightness temperature ESDR, version 1}",,,True,False,"Brodzik, MJ and Long, DG and Hardman, MA and Paget, A and Armstrong, R",2016.0,,,,Digital Media
ForeSWE: Forecasting Snow-Water Equivalent with an Uncertainty-Aware Attention Model,2511.08856v1,sarhadi2014snow,\cite{sarhadi2014snow},"{Snow water equivalent time-series forecasting in Ontario, Canada, in link to large atmospheric circulations}",,,True,False,"Sarhadi, Ali and Kelly, Richard and Modarres, Reza",2014.0,,,,Hydrological Processes
ForeSWE: Forecasting Snow-Water Equivalent with an Uncertainty-Aware Attention Model,2511.08856v1,arima,\cite{arima},{Time Series Analysis: Forecasting and Control},,,True,False,"Box, George E. P. and Jenkins, Gwilym M.",1970.0,,,,
ForeSWE: Forecasting Snow-Water Equivalent with an Uncertainty-Aware Attention Model,2511.08856v1,sarima,\cite{sarima},{Time Series Analysis: Forecasting and Control (2nd ed.)},,,True,False,"Box, George E. P. and Jenkins, Gwilym M.",1976.0,,,,
ForeSWE: Forecasting Snow-Water Equivalent with an Uncertainty-Aware Attention Model,2511.08856v1,FRANZ2010820,\cite{FRANZ2010820},Addressing snow model uncertainty for hydrologic prediction,,,True,False,Kristie J. Franz and Phil Butcher and Newsha K. Ajami,2010.0,,https://www.sciencedirect.com/science/article/pii/S0309170810001028,https://doi.org/10.1016/j.advwatres.2010.05.004,Advances in Water Resources
ForeSWE: Forecasting Snow-Water Equivalent with an Uncertainty-Aware Attention Model,2511.08856v1,CUI2023128835,\cite{CUI2023128835},{Mapping of snow water equivalent by a deep-learning model assimilating snow observations},,,True,False,Guotao Cui and Michael Anderson and Roger Bales,2023.0,,https://www.sciencedirect.com/science/article/pii/S0022169422014056,https://doi.org/10.1016/j.jhydrol.2022.128835,Journal of Hydrology
ForeSWE: Forecasting Snow-Water Equivalent with an Uncertainty-Aware Attention Model,2511.08856v1,hochreiter1997long,\cite{hochreiter1997long},Associative Long Short-Term Memory,https://arxiv.org/abs/1602.03032v2,"We investigate a new method to augment recurrent neural networks with extra memory without increasing the number of network parameters. The system has an associative memory based on complex-valued vectors and is closely related to Holographic Reduced Representations and Long Short-Term Memory networks. Holographic Reduced Representations have limited capacity: as they store more information, each retrieval becomes noisier due to interference. Our system in contrast creates redundant copies of stored information, which enables retrieval with reduced noise. Experiments demonstrate faster learning on multiple memorization tasks.",True,True,"Hochreiter, Sepp and Schmidhuber, J{\""u}rgen",1997.0,,,,Neural Computation
ForeSWE: Forecasting Snow-Water Equivalent with an Uncertainty-Aware Attention Model,2511.08856v1,thapa2024attention,\cite{thapa2024attention},{Attention-Based Models for Snow-Water Equivalent Prediction},,,True,False,"Thapa, Krishu K and Singh, Bhupinderjeet and Savalkar, Supriya and Fern, Alan and Rajagopalan, Kirti and Kalyanaraman, Ananth",2024.0,,,,
ForeSWE: Forecasting Snow-Water Equivalent with an Uncertainty-Aware Attention Model,2511.08856v1,nguyen2023climax,\cite{nguyen2023climax},{ClimaX: A foundation model for weather and climate},,,True,False,"Nguyen, Tung and Brandstetter, Johannes and Kapoor, Ashish and Gupta, Jayesh K and Grover, Aditya",2023.0,,,,arXiv preprint arXiv:2301.10343
ForeSWE: Forecasting Snow-Water Equivalent with an Uncertainty-Aware Attention Model,2511.08856v1,nguyen2023scaling,\cite{nguyen2023scaling},{Scaling transformer neural networks for skillful and reliable medium-range weather forecasting},,,True,False,"Nguyen, Tung and Shah, Rohan and Bansal, Hritik and Arcomano, Troy and Maulik, Romit and Kotamarthi, Veerabhadra and Foster, Ian and Madireddy, Sandeep and Grover, Aditya",2023.0,,,,arXiv preprint arXiv:2312.03876
Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning,2511.06854v1,che2018recurrent,\cite{che2018recurrent},Recurrent neural networks for multivariate time series with missing values,,,True,False,"Che, Zhengping and Purushotham, Sanjay and Cho, Kyunghyun and Sontag, David and Liu, Yan",2018.0,,,,Scientific reports
Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning,2511.06854v1,chen2022nonstationary,\cite{chen2022nonstationary},Nonstationary temporal matrix factorization for multivariate time series forecasting,,,True,False,"Chen, Xinyu and Zhang, Chengyuan and Zhao, Xi-Le and Saunier, Nicolas and Sun, Lijun",2022.0,,,,arXiv preprint arXiv:2203.10651
Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning,2511.06854v1,fan2022dynamic,\cite{fan2022dynamic},Dynamic Nonlinear Matrix Completion for Time-Varying Data Imputation,,,True,False,Jicong Fan,2022.0,,,,
Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning,2511.06854v1,du2023saits,\cite{du2023saits},SAITS: Self-Attention-based Imputation for Time Series,https://arxiv.org/abs/2202.08516v5,"Missing data in time series is a pervasive problem that puts obstacles in the way of advanced analysis. A popular solution is imputation, where the fundamental challenge is to determine what values should be filled in. This paper proposes SAITS, a novel method based on the self-attention mechanism for missing value imputation in multivariate time series. Trained by a joint-optimization approach, SAITS learns missing values from a weighted combination of two diagonally-masked self-attention (DMSA) blocks. DMSA explicitly captures both the temporal dependencies and feature correlations between time steps, which improves imputation accuracy and training speed. Meanwhile, the weighted-combination design enables SAITS to dynamically assign weights to the learned representations from two DMSA blocks according to the attention map and the missingness information. Extensive experiments quantitatively and qualitatively demonstrate that SAITS outperforms the state-of-the-art methods on the time-series imputation task efficiently and reveal SAITS' potential to improve the learning performance of pattern recognition models on incomplete time-series data from the real world. The code is open source on GitHub at https://github.com/WenjieDu/SAITS.",True,True,"Du, Wenjie and C{\^o}t{\'e}, David and Liu, Yan",2023.0,,,,Expert Systems with Applications
Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning,2511.06854v1,zhang2021graph,\cite{zhang2021graph},Graph-Guided Network for Irregularly Sampled Multivariate Time Series,,,True,False,"Zhang, Xiang and Zeman, Marko and Tsiligkaridis, Theodoros and Zitnik, Marinka",2021.0,,,,
Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning,2511.06854v1,agarwal2023modelling,\cite{agarwal2023modelling},Modelling Irregularly Sampled Time Series Without Imputation,,,True,False,"Agarwal, Rohit and Sinha, Aman and Prasad, Dilip K and Clausel, Marianne and Horsch, Alexander and Constant, Mathieu and Coubez, Xavier",2023.0,,,,arXiv preprint arXiv:2309.08698
Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning,2511.06854v1,shukla2021multitime,\cite{shukla2021multitime},Multi-Time Attention Networks for Irregularly Sampled Time Series,,,True,False,Satya Narayan Shukla and Benjamin Marlin,2021.0,,,,
Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning,2511.06854v1,zhang2023warpformer,\cite{zhang2023warpformer},Warpformer: A Multi-scale Modeling Approach for Irregular Clinical Time Series,https://arxiv.org/abs/2306.09368v1,"Irregularly sampled multivariate time series are ubiquitous in various fields, particularly in healthcare, and exhibit two key characteristics: intra-series irregularity and inter-series discrepancy. Intra-series irregularity refers to the fact that time-series signals are often recorded at irregular intervals, while inter-series discrepancy refers to the significant variability in sampling rates among diverse series. However, recent advances in irregular time series have primarily focused on addressing intra-series irregularity, overlooking the issue of inter-series discrepancy. To bridge this gap, we present Warpformer, a novel approach that fully considers these two characteristics. In a nutshell, Warpformer has several crucial designs, including a specific input representation that explicitly characterizes both intra-series irregularity and inter-series discrepancy, a warping module that adaptively unifies irregular time series in a given scale, and a customized attention module for representation learning. Additionally, we stack multiple warping and attention modules to learn at different scales, producing multi-scale representations that balance coarse-grained and fine-grained signals for downstream tasks. We conduct extensive experiments on widely used datasets and a new large-scale benchmark built from clinical databases. The results demonstrate the superiority of Warpformer over existing state-of-the-art approaches.",True,True,"Zhang, Jiawen and Zheng, Shun and Cao, Wei and Bian, Jiang and Li, Jia",2023.0,,,,
Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning,2511.06854v1,li2023time,\cite{li2023time},Time Series as Images: Vision Transformer for Irregularly Sampled Time Series,https://arxiv.org/abs/2303.12799v2,"Irregularly sampled time series are increasingly prevalent, particularly in medical domains. While various specialized methods have been developed to handle these irregularities, effectively modeling their complex dynamics and pronounced sparsity remains a challenge. This paper introduces a novel perspective by converting irregularly sampled time series into line graph images, then utilizing powerful pre-trained vision transformers for time series classification in the same way as image classification. This method not only largely simplifies specialized algorithm designs but also presents the potential to serve as a universal framework for time series modeling. Remarkably, despite its simplicity, our approach outperforms state-of-the-art specialized algorithms on several popular healthcare and human activity datasets. Especially in the rigorous leave-sensors-out setting where a portion of variables is omitted during testing, our method exhibits strong robustness against varying degrees of missing observations, achieving an impressive improvement of 42.8% in absolute F1 score points over leading specialized baselines even with half the variables masked. Code and data are available at https://github.com/Leezekun/ViTST",True,True,Zekun Li and Shiyang Li and Xifeng Yan,2023.0,,,,
Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning,2511.06854v1,yalavarthi2024grafiti,\cite{yalavarthi2024grafiti},GraFITi: Graphs for Forecasting Irregularly Sampled Time Series,,,True,False,"Yalavarthi, Vijaya Krishna and Madhusudhanan, Kiran and Scholz, Randolf and Ahmed, Nourhan and Burchert, Johannes and Jawed, Shayan and Born, Stefan and Schmidt-Thieme, Lars",2024.0,,,,
Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning,2511.06854v1,zhang2024tpatchgnn,\cite{zhang2024tpatchgnn},Irregular Multivariate Time Series Forecasting: A Transformable Patching Graph Neural Networks Approach,,,True,False,"Zhang, Weijia and Yin, Chenlong and Liu, Hao and Zhou, Xiaofang and Xiong, Hui",2024.0,,,,
Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning,2511.06854v1,de2019gru,\cite{de2019gru},GRU-ODE-Bayes: Continuous modeling of sporadically-observed time series,https://arxiv.org/abs/1905.12374v2,"Modeling real-world multidimensional time series can be particularly challenging when these are sporadically observed (i.e., sampling is irregular both in time and across dimensions)-such as in the case of clinical patient data. To address these challenges, we propose (1) a continuous-time version of the Gated Recurrent Unit, building upon the recent Neural Ordinary Differential Equations (Chen et al., 2018), and (2) a Bayesian update network that processes the sporadic observations. We bring these two ideas together in our GRU-ODE-Bayes method. We then demonstrate that the proposed method encodes a continuity prior for the latent process and that it can exactly represent the Fokker-Planck dynamics of complex processes driven by a multidimensional stochastic differential equation. Additionally, empirical evaluation shows that our method outperforms the state of the art on both synthetic data and real-world data with applications in healthcare and climate forecast. What is more, the continuity prior is shown to be well suited for low number of samples settings.",True,True,"De Brouwer, Edward and Simm, Jaak and Arany, Adam and Moreau, Yves",2019.0,,,,NeurIPS
Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning,2511.06854v1,schirmer2022modeling,\cite{schirmer2022modeling},Modeling irregular time series with continuous recurrent units,,,True,False,"Schirmer, Mona and Eltayeb, Mazin and Lessmann, Stefan and Rudolph, Maja",2022.0,,,,
Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning,2511.06854v1,kidger2020neural,\cite{kidger2020neural},Neural controlled differential equations for irregular time series,,,True,False,"Kidger, Patrick and Morrill, James and Foster, James and Lyons, Terry",2020.0,,,,NeurIPS
Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning,2511.06854v1,rubanova2019latent,\cite{rubanova2019latent},Latent ordinary differential equations for irregularly-sampled time series,,,True,False,"Rubanova, Yulia and Chen, Ricky TQ and Duvenaud, David K",2019.0,,,,NeurIPS
Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning,2511.06854v1,jhin2022exit,\cite{jhin2022exit},Exit: Extrapolation and interpolation-based neural controlled differential equations for time-series classification and forecasting,,,True,False,"Jhin, Sheo Yon and Lee, Jaehoon and Jo, Minju and Kook, Seungji and Jeon, Jinsung and Hyeong, Jihyeon and Kim, Jayoung and Park, Noseong",2022.0,,,,
Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning,2511.06854v1,oh2025comprehensive,\cite{oh2025comprehensive},Comprehensive Review of Neural Differential Equations for Time Series Analysis,https://arxiv.org/abs/2502.09885v4,"Time series modeling and analysis have become critical in various domains. Conventional methods such as RNNs and Transformers, while effective for discrete-time and regularly sampled data, face significant challenges in capturing the continuous dynamics and irregular sampling patterns inherent in real-world scenarios. Neural Differential Equations (NDEs) represent a paradigm shift by combining the flexibility of neural networks with the mathematical rigor of differential equations. This paper presents a comprehensive review of NDE-based methods for time series analysis, including neural ordinary differential equations, neural controlled differential equations, and neural stochastic differential equations. We provide a detailed discussion of their mathematical formulations, numerical methods, and applications, highlighting their ability to model continuous-time dynamics. Furthermore, we address key challenges and future research directions. This survey serves as a foundation for researchers and practitioners seeking to leverage NDEs for advanced time series analysis.",True,True,"Oh, YongKyung and Kam, Seungsu and Lee, Jonghun and Lim, Dong-Young and Kim, Sungil and Bui, Alex",2025.0,,,,arXiv preprint arXiv:2502.09885
Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning,2511.06854v1,zhang2025diffode,\cite{zhang2025diffode},DiffODE: Neural ODE with Differentiable Hidden State for Irregular Time Series Analysis,,,True,False,"Zhang, Yudong and Wang, Xu and Yu, Xuan and Zhou, Zhengyang and Xu, Xing and Bai, Lei and Wang, Yang",2025.0,,,,
Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning,2511.06854v1,chowdhury2023primenet,\cite{chowdhury2023primenet},Primenet: Pre-training for irregular multivariate time series,,,True,False,"Chowdhury, Ranak Roy and Li, Jiacheng and Zhang, Xiyuan and Hong, Dezhi and Gupta, Rajesh K and Shang, Jingbo",2023.0,,,,
Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning,2511.06854v1,beebe2023paits,\cite{beebe2023paits},PAITS: Pretraining and Augmentation for Irregularly-Sampled Time Series,,,True,False,"Beebe-Wang, Nicasia and Ebrahimi, Sayna and Yoon, Jinsung and Arik, Sercan O and Pfister, Tomas",2023.0,,,,arXiv preprint arXiv:2308.13703
Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning,2511.06854v1,tashiro2021csdi,\cite{tashiro2021csdi},CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation,https://arxiv.org/abs/2107.03502v2,"The imputation of missing values in time series has many applications in healthcare and finance. While autoregressive models are natural candidates for time series imputation, score-based diffusion models have recently outperformed existing counterparts including autoregressive models in many tasks such as image generation and audio synthesis, and would be promising for time series imputation. In this paper, we propose Conditional Score-based Diffusion models for Imputation (CSDI), a novel time series imputation method that utilizes score-based diffusion models conditioned on observed data. Unlike existing score-based approaches, the conditional diffusion model is explicitly trained for imputation and can exploit correlations between observed values. On healthcare and environmental data, CSDI improves by 40-65% over existing probabilistic imputation methods on popular performance metrics. In addition, deterministic imputation by CSDI reduces the error by 5-20% compared to the state-of-the-art deterministic imputation methods. Furthermore, CSDI can also be applied to time series interpolation and probabilistic forecasting, and is competitive with existing baselines. The code is available at https://github.com/ermongroup/CSDI.",True,True,"Tashiro, Yusuke and Song, Jiaming and Song, Yang and Ermon, Stefano",2021.0,,,,NeurIPS
Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning,2511.06854v1,islam2025self,\cite{islam2025self},Self-attention-based Diffusion Model for Time-series Imputation in Partial Blackout Scenarios,https://arxiv.org/abs/2503.01737v1,"Missing values in multivariate time series data can harm machine learning performance and introduce bias. These gaps arise from sensor malfunctions, blackouts, and human error and are typically addressed by data imputation. Previous work has tackled the imputation of missing data in random, complete blackouts and forecasting scenarios. The current paper addresses a more general missing pattern, which we call ""partial blackout,"" where a subset of features is missing for consecutive time steps. We introduce a two-stage imputation process using self-attention and diffusion processes to model feature and temporal correlations. Notably, our model effectively handles missing data during training, enhancing adaptability and ensuring reliable imputation and performance, even with incomplete datasets. Our experiments on benchmark and two real-world time series datasets demonstrate that our model outperforms the state-of-the-art in partial blackout scenarios and shows better scalability.",True,True,"Islam, Mohammad Rafid Ul and Tadepalli, Prasad and Fern, Alan",2025.0,,,,
Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning,2511.06854v1,yu2018generative,\cite{yu2018generative},Generative image inpainting with contextual attention,,,True,False,"Yu, Jiahui and Lin, Zhe and Yang, Jimei and Shen, Xiaohui and Lu, Xin and Huang, Thomas S",2018.0,,,,
Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning,2511.06854v1,meng2013robust,\cite{meng2013robust},Robust matrix factorization with unknown noise,,,True,False,"Meng, Deyu and De La Torre, Fernando",2013.0,,,,
Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning,2511.06854v1,hasan2016learning,\cite{hasan2016learning},Learning Temporal Regularity in Video Sequences,https://arxiv.org/abs/1604.04574v1,"Perceiving meaningful activities in a long video sequence is a challenging problem due to ambiguous definition of 'meaningfulness' as well as clutters in the scene. We approach this problem by learning a generative model for regular motion patterns, termed as regularity, using multiple sources with very limited supervision. Specifically, we propose two methods that are built upon the autoencoders for their ability to work with little to no supervision. We first leverage the conventional handcrafted spatio-temporal local features and learn a fully connected autoencoder on them. Second, we build a fully convolutional feed-forward autoencoder to learn both the local features and the classifiers as an end-to-end learning framework. Our model can capture the regularities from multiple datasets. We evaluate our methods in both qualitative and quantitative ways - showing the learned regularity of videos in various aspects and demonstrating competitive performance on anomaly detection datasets as an application.",True,True,"Hasan, Mahmudul and Choi, Jonghyun and Neumann, Jan and Roy-Chowdhury, Amit K and Davis, Larry S",2016.0,,,,
Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning,2511.06854v1,liu2018future,\cite{liu2018future},Future frame prediction for anomaly detection--a new baseline,,,True,False,"Liu, Wen and Luo, Weixin and Lian, Dongze and Gao, Shenghua",2018.0,,,,
Online Bayesian Experimental Design for Partially Observed Dynamical Systems,2511.04403v1,Rainforth24,\cite{Rainforth24},Modern {B}ayesian experimental design,,,True,False,"Rainforth, Tom and Foster, Adam and Ivanova, Desi R and Bickford Smith, Freddie",2024.0,,,,Statistical Science
Online Bayesian Experimental Design for Partially Observed Dynamical Systems,2511.04403v1,huan2024optimal,\cite{huan2024optimal},Optimal experimental design: {Formulations} and computations,,,True,False,"Huan, Xun and Jagalur, Jayanth and Marzouk, Youssef",2024.0,,,,Acta Numerica
Online Bayesian Experimental Design for Partially Observed Dynamical Systems,2511.04403v1,Rainforth18,\cite{Rainforth18},On nesting {M}onte {C}arlo estimators,,,True,False,"Rainforth, Tom and Cornish, Rob and Yang, Hongseok and Warrington, Andrew and Wood, Frank",2018.0,,,,
Online Bayesian Experimental Design for Partially Observed Dynamical Systems,2511.04403v1,Foster19,\cite{Foster19},Variational {B}ayesian optimal experimental design,,,True,False,"Foster, Adam and Jankowiak, Martin and Bingham, Elias and Horsfall, Paul and Teh, Yee Whye and Rainforth, Thomas and Goodman, Noah",2019.0,,,,Advances in Neural Information Processing Systems
Online Bayesian Experimental Design for Partially Observed Dynamical Systems,2511.04403v1,Foster20,\cite{Foster20},A Unified Stochastic Gradient Approach to Designing Bayesian-Optimal Experiments,https://arxiv.org/abs/1911.00294v2,"We introduce a fully stochastic gradient based approach to Bayesian optimal experimental design (BOED). Our approach utilizes variational lower bounds on the expected information gain (EIG) of an experiment that can be simultaneously optimized with respect to both the variational and design parameters. This allows the design process to be carried out through a single unified stochastic gradient ascent procedure, in contrast to existing approaches that typically construct a pointwise EIG estimator, before passing this estimator to a separate optimizer. We provide a number of different variational objectives including the novel adaptive contrastive estimation (ACE) bound. Finally, we show that our gradient-based approaches are able to provide effective design optimization in substantially higher dimensional settings than existing approaches.",True,True,"Foster, Adam and Jankowiak, Martin and O’Meara, Matthew and Teh, Yee Whye and Rainforth, Tom",2020.0,,,,
Online Bayesian Experimental Design for Partially Observed Dynamical Systems,2511.04403v1,Foster21,\cite{Foster21},Deep Adaptive Design: Amortizing Sequential Bayesian Experimental Design,https://arxiv.org/abs/2103.02438v2,"We introduce Deep Adaptive Design (DAD), a method for amortizing the cost of adaptive Bayesian experimental design that allows experiments to be run in real-time. Traditional sequential Bayesian optimal experimental design approaches require substantial computation at each stage of the experiment. This makes them unsuitable for most real-world applications, where decisions must typically be made quickly. DAD addresses this restriction by learning an amortized design network upfront and then using this to rapidly run (multiple) adaptive experiments at deployment time. This network represents a design policy which takes as input the data from previous steps, and outputs the next design using a single forward pass; these design decisions can be made in milliseconds during the live experiment. To train the network, we introduce contrastive information bounds that are suitable objectives for the sequential setting, and propose a customized network architecture that exploits key symmetries. We demonstrate that DAD successfully amortizes the process of experimental design, outperforming alternative strategies on a number of problems.",True,True,"Foster, Adam and Ivanova, Desi R and Malik, Ilyas and Rainforth, Tom",2021.0,,,,
Online Bayesian Experimental Design for Partially Observed Dynamical Systems,2511.04403v1,Kleinegesse20,\cite{Kleinegesse20},{B}ayesian experimental design for implicit models by mutual information neural estimation,,,True,False,"Kleinegesse, Steven and Gutmann, Michael U",2020.0,,,,
Online Bayesian Experimental Design for Partially Observed Dynamical Systems,2511.04403v1,Kleinegesse21,\cite{Kleinegesse21},Gradient-based {B}ayesian experimental design for implicit models using mutual information lower bounds,,,True,False,"Kleinegesse, Steven and Gutmann, Michael U",2021.0,,,,arXiv preprint arXiv:2105.04379
Online Bayesian Experimental Design for Partially Observed Dynamical Systems,2511.04403v1,Ivanova21,\cite{Ivanova21},Implicit Deep Adaptive Design: Policy-Based Experimental Design without Likelihoods,https://arxiv.org/abs/2111.02329v1,"We introduce implicit Deep Adaptive Design (iDAD), a new method for performing adaptive experiments in real-time with implicit models. iDAD amortizes the cost of Bayesian optimal experimental design (BOED) by learning a design policy network upfront, which can then be deployed quickly at the time of the experiment. The iDAD network can be trained on any model which simulates differentiable samples, unlike previous design policy work that requires a closed form likelihood and conditionally independent experiments. At deployment, iDAD allows design decisions to be made in milliseconds, in contrast to traditional BOED approaches that require heavy computation during the experiment itself. We illustrate the applicability of iDAD on a number of experiments, and show that it provides a fast and effective mechanism for performing adaptive design with implicit models.",True,True,"Ivanova, Desi R and Foster, Adam and Kleinegesse, Steven and Gutmann, Michael U and Rainforth, Thomas",2021.0,,,,Advances in neural information processing systems
Online Bayesian Experimental Design for Partially Observed Dynamical Systems,2511.04403v1,Kleinegesse19,\cite{Kleinegesse19},Efficient {B}ayesian experimental design for implicit models,,,True,False,"Kleinegesse, Steven and Gutmann, Michael U",2019.0,,,,
Online Bayesian Experimental Design for Partially Observed Dynamical Systems,2511.04403v1,Dehideniya18,\cite{Dehideniya18},Optimal {B}ayesian design for discriminating between models with intractable likelihoods in epidemiology,,,True,False,"Dehideniya, Mahasen B and Drovandi, Christopher C and McGree, James M",2018.0,,,,Computational Statistics \& Data Analysis
Online Bayesian Experimental Design for Partially Observed Dynamical Systems,2511.04403v1,Drovandi13,\cite{Drovandi13},{B}ayesian experimental design for models with intractable likelihoods,,,True,False,"Drovandi, Christopher C and Pettitt, Anthony N",2013.0,,,,Biometrics
Online Bayesian Experimental Design for Partially Observed Dynamical Systems,2511.04403v1,Hainy16,\cite{Hainy16},Likelihood-free extensions for {B}ayesian sequentially designed experiments,,,True,False,"Hainy, Markus and Drovandi, Christopher C and McGree, James M",2016.0,,,,
Online Bayesian Experimental Design for Partially Observed Dynamical Systems,2511.04403v1,Price16,\cite{Price16},On the efficient determination of optimal {B}ayesian experimental designs using {ABC}: {A} case study in optimal observation of epidemics,,,True,False,"Price, David J and Bean, Nigel G and Ross, Joshua V and Tuke, Jonathan",2016.0,,,,Journal of Statistical Planning and Inference
Online Bayesian Experimental Design for Partially Observed Dynamical Systems,2511.04403v1,Iqbal24,\cite{Iqbal24},Nesting particle filters for experimental design in dynamical systems,,,True,False,"Iqbal, Sahel and Corenflos, Adrien and S{\""a}rkk{\""a}, Simo and Abdulsamad, Hany",2024.0,,,,
Online Bayesian Experimental Design for Partially Observed Dynamical Systems,2511.04403v1,iqbal2024recursive,\cite{iqbal2024recursive},Recursive nested filtering for efficient amortized {B}ayesian experimental design,,,True,False,"Iqbal, Sahel and Abdulsamad, Hany and P{\'e}rez-Vieites, Sara and S{\""a}rkk{\""a}, Simo and Corenflos, Adrien",2024.0,,,,arXiv preprint arXiv:2409.05354
Turbo-DDCM: Fast and Flexible Zero-Shot Diffusion-Based Image Compression,2511.06424v1,NEURIPS2023_ccf6d8b4,\cite{NEURIPS2023_ccf6d8b4},Lossy Image Compression with Conditional Diffusion Models,,,True,False,"Yang, Ruihan and Mandt, Stephan",2023.0,,,,
Turbo-DDCM: Fast and Flexible Zero-Shot Diffusion-Based Image Compression,2511.06424v1,ghouse2023residual,\cite{ghouse2023residual},A residual diffusion model for high perceptual quality codec augmentation,,,True,False,"Ghouse, Noor Fathima and Petersen, Jens and Wiggers, Auke and Xu, Tianlin and Sautiere, Guillaume",2023.0,,,,arXiv preprint arXiv:2301.05489
Turbo-DDCM: Fast and Flexible Zero-Shot Diffusion-Based Image Compression,2511.06424v1,iwai2024controlling,\cite{iwai2024controlling},"Controlling rate, distortion, and realism: Towards a single comprehensive neural image compression model",,,True,False,"Iwai, Shoma and Miyazaki, Tomo and Omachi, Shinichiro",2024.0,,,,
Turbo-DDCM: Fast and Flexible Zero-Shot Diffusion-Based Image Compression,2511.06424v1,korber2024perco,\cite{korber2024perco},PerCo (SD): Open Perceptual Compression,https://arxiv.org/abs/2409.20255v1,"We introduce PerCo (SD), a perceptual image compression method based on Stable Diffusion v2.1, targeting the ultra-low bit range. PerCo (SD) serves as an open and competitive alternative to the state-of-the-art method PerCo, which relies on a proprietary variant of GLIDE and remains closed to the public. In this work, we review the theoretical foundations, discuss key engineering decisions in adapting PerCo to the Stable Diffusion ecosystem, and provide a comprehensive comparison, both quantitatively and qualitatively. On the MSCOCO-30k dataset, PerCo (SD) demonstrates improved perceptual characteristics at the cost of higher distortion. We partly attribute this gap to the different model capacities being used (866M vs. 1.4B). We hope our work contributes to a deeper understanding of the underlying mechanisms and paves the way for future advancements in the field. Code and trained models will be released at https://github.com/Nikolai10/PerCo.",True,True,"K{\""o}rber, Nikolai and Kromer, Eduard and Siebert, Andreas and Hauke, Sascha and Mueller-Gritschneder, Daniel and Schuller, Bj{\""o}rn",2024.0,,,,
Turbo-DDCM: Fast and Flexible Zero-Shot Diffusion-Based Image Compression,2511.06424v1,careil2024towards,\cite{careil2024towards},Towards image compression with perfect realism at ultra-low bitrates,,,True,False,Marlene Careil and Matthew J. Muckley and Jakob Verbeek and St{\'e}phane Lathuili{\`e}re,2024.0,,https://openreview.net/forum?id=ktdETU9JBg,,
Turbo-DDCM: Fast and Flexible Zero-Shot Diffusion-Based Image Compression,2511.06424v1,park2025diffosinglestepdiffusionimage,\cite{park2025diffosinglestepdiffusionimage},{DiffO}: Single-step Diffusion for Image Compression at Ultra-Low Bitrates,,,True,False,Chanung Park and Joo Chan Lee and Jong Hwan Ko,2025.0,,https://arxiv.org/abs/2506.16572,,
Turbo-DDCM: Fast and Flexible Zero-Shot Diffusion-Based Image Compression,2511.06424v1,xue2025onestepdiffusionbasedimagecompression,\cite{xue2025onestepdiffusionbasedimagecompression},One-Step Diffusion-Based Image Compression with Semantic Distillation,https://arxiv.org/abs/2505.16687v1,"While recent diffusion-based generative image codecs have shown impressive performance, their iterative sampling process introduces unpleasing latency. In this work, we revisit the design of a diffusion-based codec and argue that multi-step sampling is not necessary for generative compression. Based on this insight, we propose OneDC, a One-step Diffusion-based generative image Codec -- that integrates a latent compression module with a one-step diffusion generator. Recognizing the critical role of semantic guidance in one-step diffusion, we propose using the hyperprior as a semantic signal, overcoming the limitations of text prompts in representing complex visual content. To further enhance the semantic capability of the hyperprior, we introduce a semantic distillation mechanism that transfers knowledge from a pretrained generative tokenizer to the hyperprior codec. Additionally, we adopt a hybrid pixel- and latent-domain optimization to jointly enhance both reconstruction fidelity and perceptual realism. Extensive experiments demonstrate that OneDC achieves SOTA perceptual quality even with one-step generation, offering over 40% bitrate reduction and 20x faster decoding compared to prior multi-step diffusion-based codecs. Code will be released later.",True,True,Naifu Xue and Zhaoyang Jia and Jiahao Li and Bin Li and Yuan Zhang and Yan Lu,2025.0,,https://arxiv.org/abs/2505.16687,,
Turbo-DDCM: Fast and Flexible Zero-Shot Diffusion-Based Image Compression,2511.06424v1,ho2020denoising,\cite{ho2020denoising},Denoising Diffusion Probabilistic Models,https://arxiv.org/abs/2006.11239v2,"We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion",True,True,"Ho, Jonathan and Jain, Ajay and Abbeel, Pieter",2020.0,,,,
Turbo-DDCM: Fast and Flexible Zero-Shot Diffusion-Based Image Compression,2511.06424v1,kawar2022denoising,\cite{kawar2022denoising},Denoising Diffusion Restoration Models,https://arxiv.org/abs/2201.11793v3,"Many interesting tasks in image restoration can be cast as linear inverse problems. A recent family of approaches for solving these problems uses stochastic algorithms that sample from the posterior distribution of natural images given the measurements. However, efficient solutions often require problem-specific supervised training to model the posterior, whereas unsupervised methods that are not problem-specific typically rely on inefficient iterative methods. This work addresses these issues by introducing Denoising Diffusion Restoration Models (DDRM), an efficient, unsupervised posterior sampling method. Motivated by variational inference, DDRM takes advantage of a pre-trained denoising diffusion generative model for solving any linear inverse problem. We demonstrate DDRM's versatility on several image datasets for super-resolution, deblurring, inpainting, and colorization under various amounts of measurement noise. DDRM outperforms the current leading unsupervised methods on the diverse ImageNet dataset in reconstruction quality, perceptual quality, and runtime, being 5x faster than the nearest competitor. DDRM also generalizes well for natural images out of the distribution of the observed ImageNet training set.",True,True,"Kawar, Bahjat and Elad, Michael and Ermon, Stefano and Song, Jiaming",2022.0,,,,
Turbo-DDCM: Fast and Flexible Zero-Shot Diffusion-Based Image Compression,2511.06424v1,raphaeli2025silosolvinginverseproblems,\cite{raphaeli2025silosolvinginverseproblems},SILO: Solving Inverse Problems with Latent Operators,,,True,False,Ron Raphaeli and Sean Man and Michael Elad,2025.0,,https://arxiv.org/abs/2501.11746,,
Turbo-DDCM: Fast and Flexible Zero-Shot Diffusion-Based Image Compression,2511.06424v1,man2025proxiesdistortionconsistencyapplications,\cite{man2025proxiesdistortionconsistencyapplications},Proxies for Distortion and Consistency with Applications for Real-World Image Restoration,,,True,False,Sean Man and Guy Ohayon and Ron Raphaeli and Michael Elad,2025.0,,https://arxiv.org/abs/2501.12102,,
Turbo-DDCM: Fast and Flexible Zero-Shot Diffusion-Based Image Compression,2511.06424v1,manor2024zeroshot,\cite{manor2024zeroshot},Zero-Shot Unsupervised and Text-Based Audio Editing Using {DDPM} Inversion,,,True,False,"Manor, Hila and Michaeli, Tomer",2024.0,21--27 Jul,https://proceedings.mlr.press/v235/manor24a.html,,
Turbo-DDCM: Fast and Flexible Zero-Shot Diffusion-Based Image Compression,2511.06424v1,cohen2024slicedit,\cite{cohen2024slicedit},Slicedit: Zero-Shot Video Editing With Text-to-Image Diffusion Models Using Spatio-Temporal Slices,https://arxiv.org/abs/2405.12211v1,"Text-to-image (T2I) diffusion models achieve state-of-the-art results in image synthesis and editing. However, leveraging such pretrained models for video editing is considered a major challenge. Many existing works attempt to enforce temporal consistency in the edited video through explicit correspondence mechanisms, either in pixel space or between deep features. These methods, however, struggle with strong nonrigid motion. In this paper, we introduce a fundamentally different approach, which is based on the observation that spatiotemporal slices of natural videos exhibit similar characteristics to natural images. Thus, the same T2I diffusion model that is normally used only as a prior on video frames, can also serve as a strong prior for enhancing temporal consistency by applying it on spatiotemporal slices. Based on this observation, we present Slicedit, a method for text-based video editing that utilizes a pretrained T2I diffusion model to process both spatial and spatiotemporal slices. Our method generates videos that retain the structure and motion of the original video while adhering to the target text. Through extensive experiments, we demonstrate Slicedit's ability to edit a wide range of real-world videos, confirming its clear advantages compared to existing competing methods. Webpage: https://matankleiner.github.io/slicedit/",True,True,"Cohen, Nathaniel and Kulikov, Vladimir and Kleiner, Matan and Huberman-Spiegelglas, Inbar and Michaeli, Tomer",2024.0,21--27 Jul,https://proceedings.mlr.press/v235/cohen24a.html,,
Turbo-DDCM: Fast and Flexible Zero-Shot Diffusion-Based Image Compression,2511.06424v1,elata2024zero,\cite{elata2024zero},Zero-Shot Image Compression with Diffusion-Based Posterior Sampling,,,True,False,"Elata, Noam and Michaeli, Tomer and Elad, Michael",2024.0,,,,arXiv preprint arXiv:2407.09896
Turbo-DDCM: Fast and Flexible Zero-Shot Diffusion-Based Image Compression,2511.06424v1,theis2022lossy,\cite{theis2022lossy},Lossy compression with gaussian diffusion,,,True,False,"Theis, Lucas and Salimans, Tim and Hoffman, Matthew D and Mentzer, Fabian",2022.0,,,,arXiv preprint arXiv:2206.08889
Turbo-DDCM: Fast and Flexible Zero-Shot Diffusion-Based Image Compression,2511.06424v1,DonohoCompressedSensing,\cite{DonohoCompressedSensing},"""Compressed"" Compressed Sensing",https://arxiv.org/abs/1001.4295v1,"The field of compressed sensing has shown that a sparse but otherwise arbitrary vector can be recovered exactly from a small number of randomly constructed linear projections (or samples). The question addressed in this paper is whether an even smaller number of samples is sufficient when there exists prior knowledge about the distribution of the unknown vector, or when only partial recovery is needed. An information-theoretic lower bound with connections to free probability theory and an upper bound corresponding to a computationally simple thresholding estimator are derived. It is shown that in certain cases (e.g. discrete valued vectors or large distortions) the number of samples can be decreased. Interestingly though, it is also shown that in many cases no reduction is possible.",True,True,"Donoho, D.L.",2006.0,,,10.1109/TIT.2006.871582,IEEE Transactions on Information Theory
Turbo-DDCM: Fast and Flexible Zero-Shot Diffusion-Based Image Compression,2511.06424v1,theis2022algorithmscommunicationsamples,\cite{theis2022algorithmscommunicationsamples},Algorithms for the Communication of Samples,,,True,False,Lucas Theis and Noureldin Yosri,2022.0,,https://arxiv.org/abs/2110.12805,,
Turbo-DDCM: Fast and Flexible Zero-Shot Diffusion-Based Image Compression,2511.06424v1,ohayon2025compressed,\cite{ohayon2025compressed},Compressed Image Generation with Denoising Diffusion Codebook Models,,,True,False,Guy Ohayon and Hila Manor and Tomer Michaeli and Michael Elad,2025.0,,https://openreview.net/forum?id=cQHwUckohW,,
Turbo-DDCM: Fast and Flexible Zero-Shot Diffusion-Based Image Compression,2511.06424v1,vonderfecht2025lossy,\cite{vonderfecht2025lossy},Lossy Compression with Pretrained Diffusion Models,,,True,False,Jeremy Vonderfecht and Feng Liu,2025.0,,https://openreview.net/forum?id=raUnLe0Z04,,
Turbo-DDCM: Fast and Flexible Zero-Shot Diffusion-Based Image Compression,2511.06424v1,li2023roibaseddeepimagecompression,\cite{li2023roibaseddeepimagecompression},{ROI}-based Deep Image Compression with Swin Transformers,,,True,False,Binglin Li and Jie Liang and Haisheng Fu and Jingning Han,2023.0,,https://arxiv.org/abs/2305.07783,,
Turbo-DDCM: Fast and Flexible Zero-Shot Diffusion-Based Image Compression,2511.06424v1,jin2025customizableroibaseddeepimage,\cite{jin2025customizableroibaseddeepimage},Customizable ROI-Based Deep Image Compression,https://arxiv.org/abs/2507.00373v3,"Region of Interest (ROI)-based image compression optimizes bit allocation by prioritizing ROI for higher-quality reconstruction. However, as the users (including human clients and downstream machine tasks) become more diverse, ROI-based image compression needs to be customizable to support various preferences. For example, different users may define distinct ROI or require different quality trade-offs between ROI and non-ROI. Existing ROI-based image compression schemes predefine the ROI, making it unchangeable, and lack effective mechanisms to balance reconstruction quality between ROI and non-ROI. This work proposes a paradigm for customizable ROI-based deep image compression. First, we develop a Text-controlled Mask Acquisition (TMA) module, which allows users to easily customize their ROI for compression by just inputting the corresponding semantic \emph{text}. It makes the encoder controlled by text. Second, we design a Customizable Value Assign (CVA) mechanism, which masks the non-ROI with a changeable extent decided by users instead of a constant one to manage the reconstruction quality trade-off between ROI and non-ROI. Finally, we present a Latent Mask Attention (LMA) module, where the latent spatial prior of the mask and the latent Rate-Distortion Optimization (RDO) prior of the image are extracted and fused in the latent space, and further used to optimize the latent representation of the source image. Experimental results demonstrate that our proposed customizable ROI-based deep image compression paradigm effectively addresses the needs of customization for ROI definition and mask acquisition as well as the reconstruction quality trade-off management between the ROI and non-ROI.",True,True,Jian Jin and Fanxin Xia and Feng Ding and Xinfeng Zhang and Meiqin Liu and Yao Zhao and Weisi Lin and Lili Meng,2025.0,,https://arxiv.org/abs/2507.00373,,
Turbo-DDCM: Fast and Flexible Zero-Shot Diffusion-Based Image Compression,2511.06424v1,srivastava2025regionbasedmedicalimage,\cite{srivastava2025regionbasedmedicalimage},Region of Interest based Medical Image Compression,https://arxiv.org/abs/2501.02895v1,"The vast volume of medical image data necessitates efficient compression techniques to support remote healthcare services. This paper explores Region of Interest (ROI) coding to address the balance between compression rate and image quality. By leveraging UNET segmentation on the Brats 2020 dataset, we accurately identify tumor regions, which are critical for diagnosis. These regions are then subjected to High Efficiency Video Coding (HEVC) for compression, enhancing compression rates while preserving essential diagnostic information. This approach ensures that critical image regions maintain their quality, while non-essential areas are compressed more. Our method optimizes storage space and transmission bandwidth, meeting the demands of telemedicine and large-scale medical imaging. Through this technique, we provide a robust solution that maintains the integrity of vital data and improves the efficiency of medical image handling.",True,True,Utkarsh Prakash Srivastava and Toshiaki Fujii,2025.0,,https://arxiv.org/abs/2501.02895,,
Turbo-DDCM: Fast and Flexible Zero-Shot Diffusion-Based Image Compression,2511.06424v1,Xu_2025_CVPR,\cite{Xu_2025_CVPR},Decouple Distortion from Perception: Region Adaptive Diffusion for Extreme-low Bitrate Perception Image Compression,,,True,False,"Xu, Jinchang and Wang, Shaokang and Chen, Jintao and Li, Zhe and Jia, Peidong and Zhao, Fei and Xiang, Guoqing and Hao, Zhijian and Zhang, Shanghang and Xie, Xiaodong",2025.0,June,,,
Linear Gradient Prediction with Control Variates,2511.05187v1,jaderberg2017decoupled,\cite{jaderberg2017decoupled},Decoupled Neural Interfaces using Synthetic Gradients,https://arxiv.org/abs/1608.05343v2,"Training directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates. All layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modelled subgraph will produce using only local information. In particular we focus on modelling error gradients: by using the modelled synthetic gradient in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously i.e. we realise decoupled neural interfaces. We show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one's future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass -- amounting to independent networks which co-learn such that they can be composed into a single functioning corporation.",True,True,"Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Silver, David and Kavukcuoglu, Koray",2017.0,,,,
Linear Gradient Prediction with Control Variates,2511.05187v1,czarnecki2017understanding,\cite{czarnecki2017understanding},Understanding synthetic gradients and decoupled neural interfaces,,,True,False,"Czarnecki, Wojciech Marian and {\'S}wirszcz, Grzegorz and Jaderberg, Max and Osindero, Simon and Vinyals, Oriol and Kavukcuoglu, Koray",2017.0,,,,
Linear Gradient Prediction with Control Variates,2511.05187v1,sun2017meprop,\cite{sun2017meprop},meprop: Sparsified back propagation for accelerated deep learning with reduced overfitting,,,True,False,"Sun, Xu and Ren, Xuancheng and Ma, Shuming and Wang, Houfeng",2017.0,,,,
Linear Gradient Prediction with Control Variates,2511.05187v1,kleijnen1975statistical,\cite{kleijnen1975statistical},Statistical techniques in simulation: in two parts,,,True,False,"Kleijnen, Jack PC",1975.0,,,,
Linear Gradient Prediction with Control Variates,2511.05187v1,wang2013variance,\cite{wang2013variance},Variance reduction for stochastic gradient optimization,,,True,False,"Wang, Chong and Chen, Xi and Smola, Alexander J and Xing, Eric P",2013.0,,,,Advances in neural information processing systems
Linear Gradient Prediction with Control Variates,2511.05187v1,jacot2018neural,\cite{jacot2018neural},Neural tangent kernel: Convergence and generalization in neural networks,,,True,False,"Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment",2018.0,,,,Advances in neural information processing systems
Linear Gradient Prediction with Control Variates,2511.05187v1,yang2019wide,\cite{yang2019wide},Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes,https://arxiv.org/abs/1910.12478v3,"Wide neural networks with random weights and biases are Gaussian processes, as originally observed by Neal (1995) and more recently by Lee et al. (2018) and Matthews et al. (2018) for deep fully-connected networks, as well as by Novak et al. (2019) and Garriga-Alonso et al. (2019) for deep convolutional networks. We show that this Neural Network-Gaussian Process correspondence surprisingly extends to all modern feedforward or recurrent neural networks composed of multilayer perceptron, RNNs (e.g. LSTMs, GRUs), (nD or graph) convolution, pooling, skip connection, attention, batch normalization, and/or layer normalization. More generally, we introduce a language for expressing neural network computations, and our result encompasses all such expressible neural networks. This work serves as a tutorial on the *tensor programs* technique formulated in Yang (2019) and elucidates the Gaussian Process results obtained there. We provide open-source implementations of the Gaussian Process kernels of simple RNN, GRU, transformer, and batchnorm+ReLU network at github.com/thegregyang/GP4A.",True,True,"Yang, Greg",2019.0,,,,Advances in Neural Information Processing Systems
Linear Gradient Prediction with Control Variates,2511.05187v1,liu2020linearity,\cite{liu2020linearity},On the linearity of large non-linear models: when and why the tangent kernel is constant,,,True,False,"Liu, Chaoyue and Zhu, Libin and Belkin, Misha",2020.0,,,,Advances in Neural Information Processing Systems
Linear Gradient Prediction with Control Variates,2511.05187v1,bietti2020deep,\cite{bietti2020deep},Deep equals shallow for ReLU networks in kernel regimes,,,True,False,"Bietti, Alberto and Bach, Francis",2020.0,,,,arXiv preprint arXiv:2009.14397
Linear Gradient Prediction with Control Variates,2511.05187v1,geifman2020similarity,\cite{geifman2020similarity},On the similarity between the laplace and neural tangent kernels,,,True,False,"Geifman, Amnon and Yadav, Abhay and Kasten, Yoni and Galun, Meirav and Jacobs, David and Ronen, Basri",2020.0,,,,Advances in Neural Information Processing Systems
Linear Gradient Prediction with Control Variates,2511.05187v1,murray2022characterizing,\cite{murray2022characterizing},Characterizing the spectrum of the NTK via a power series expansion,,,True,False,"Murray, Michael and Jin, Hui and Bowman, Benjamin and Montufar, Guido",2022.0,,,,arXiv preprint arXiv:2211.07844
Linear Gradient Prediction with Control Variates,2511.05187v1,sagun2016eigenvalues,\cite{sagun2016eigenvalues},Eigenvalues of the hessian in deep learning: Singularity and beyond,,,True,False,"Sagun, Levent and Bottou, Leon and LeCun, Yann",2016.0,,,,arXiv preprint arXiv:1611.07476
Linear Gradient Prediction with Control Variates,2511.05187v1,gur2018gradient,\cite{gur2018gradient},Gradient Descent Happens in a Tiny Subspace,https://arxiv.org/abs/1812.04754v1,"We show that in a variety of large-scale deep learning scenarios the gradient dynamically converges to a very small subspace after a short period of training. The subspace is spanned by a few top eigenvectors of the Hessian (equal to the number of classes in the dataset), and is mostly preserved over long periods of training. A simple argument then suggests that gradient descent may happen mostly in this subspace. We give an example of this effect in a solvable model of classification, and we comment on possible implications for optimization and learning.",True,True,"Gur-Ari, Guy and Roberts, Daniel A and Dyer, Ethan",2018.0,,,,arXiv preprint arXiv:1812.04754
Linear Gradient Prediction with Control Variates,2511.05187v1,sonthalia2025low,\cite{sonthalia2025low},Low rank gradients and where to find them,,,True,False,"Sonthalia, Rishi and Murray, Michael and Mont{\'u}far, Guido",2025.0,,,,arXiv preprint arXiv:2510.01303
Linear Gradient Prediction with Control Variates,2511.05187v1,gomez2017reversible,\cite{gomez2017reversible},The reversible residual network: Backpropagation without storing activations,,,True,False,"Gomez, Aidan N and Ren, Mengye and Urtasun, Raquel and Grosse, Roger B",2017.0,,,,Advances in neural information processing systems
Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing,2510.22044v1,zhang2022sampling,\cite{zhang2022sampling},Sampling in constrained domains with orthogonal-space variational gradient descent,,,True,False,"Zhang, Ruqi and Liu, Qiang and Tong, Xin",2022.0,,,,
Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing,2510.22044v1,power2024constrained,\cite{power2024constrained},Constrained Stein Variational Trajectory Optimization,https://arxiv.org/abs/2308.12110v3,"We present Constrained Stein Variational Trajectory Optimization (CSVTO), an algorithm for performing trajectory optimization with constraints on a set of trajectories in parallel. We frame constrained trajectory optimization as a novel form of constrained functional minimization over trajectory distributions, which avoids treating the constraints as a penalty in the objective and allows us to generate diverse sets of constraint-satisfying trajectories. Our method uses Stein Variational Gradient Descent (SVGD) to find a set of particles that approximates a distribution over low-cost trajectories while obeying constraints. CSVTO is applicable to problems with differentiable equality and inequality constraints and includes a novel particle re-sampling step to escape local minima. By explicitly generating diverse sets of trajectories, CSVTO is better able to avoid poor local minima and is more robust to initialization. We demonstrate that CSVTO outperforms baselines in challenging highly-constrained tasks, such as a 7DoF wrench manipulation task, where CSVTO outperforms all baselines both in success and constraint satisfaction.",True,True,"Power, Thomas and Berenson, Dmitry",2024.0,,,,IEEE Transactions on Robotics
Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing,2510.22044v1,zhang2024functional,\cite{zhang2024functional},Functional gradient flows for constrained sampling,,,True,False,"Zhang, Shiyue and Yu, Longlin and Cheng, Ziheng and Zhang, Cheng",2024.0,,,,
Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing,2510.22044v1,muehlebach2025accelerated,\cite{muehlebach2025accelerated},Accelerated First-Order Optimization under Nonlinear Constraints,https://arxiv.org/abs/2302.00316v3,"We exploit analogies between first-order algorithms for constrained optimization and non-smooth dynamical systems to design a new class of accelerated first-order algorithms for constrained optimization. Unlike Frank-Wolfe or projected gradients, these algorithms avoid optimization over the entire feasible set at each iteration. We prove convergence to stationary points even in a nonconvex setting and we derive accelerated rates for the convex setting both in continuous time, as well as in discrete time. An important property of these algorithms is that constraints are expressed in terms of velocities instead of positions, which naturally leads to sparse, local and convex approximations of the feasible set (even if the feasible set is nonconvex). Thus, the complexity tends to grow mildly in the number of decision variables and in the number of constraints, which makes the algorithms suitable for machine learning applications. We apply our algorithms to a compressed sensing and a sparse regression problem, showing that we can treat nonconvex $\ell^p$ constraints ($p<1$) efficiently, while recovering state-of-the-art performance for $p=1$.",True,True,"Muehlebach, Michael and Jordan, Michael I",2025.0,,,,Mathematical Programming
Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing,2510.22044v1,JMLR:v23:21-0798,\cite{JMLR:v23:21-0798},On Constraints in First-Order Optimization: A View from Non-Smooth Dynamical Systems,https://arxiv.org/abs/2107.08225v3,"We introduce a class of first-order methods for smooth constrained optimization that are based on an analogy to non-smooth dynamical systems. Two distinctive features of our approach are that (i) projections or optimizations over the entire feasible set are avoided, in stark contrast to projected gradient methods or the Frank-Wolfe method, and (ii) iterates are allowed to become infeasible, which differs from active set or feasible direction methods, where the descent motion stops as soon as a new constraint is encountered. The resulting algorithmic procedure is simple to implement even when constraints are nonlinear, and is suitable for large-scale constrained optimization problems in which the feasible set fails to have a simple structure. The key underlying idea is that constraints are expressed in terms of velocities instead of positions, which has the algorithmic consequence that optimizations over feasible sets at each iteration are replaced with optimizations over local, sparse convex approximations. In particular, this means that at each iteration only constraints that are violated are taken into account. The result is a simplified suite of algorithms and an expanded range of possible applications in machine learning.",True,True,Michael Muehlebach and Michael I. Jordan,2022.0,,,,Journal of Machine Learning Research
Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing,2510.22044v1,schechtman2023orthogonal,\cite{schechtman2023orthogonal},Orthogonal Directions Constrained Gradient Method: from non-linear equality constraints to Stiefel manifold,https://arxiv.org/abs/2303.09261v1,"We consider the problem of minimizing a non-convex function over a smooth manifold $\mathcal{M}$. We propose a novel algorithm, the Orthogonal Directions Constrained Gradient Method (ODCGM) which only requires computing a projection onto a vector space. ODCGM is infeasible but the iterates are constantly pulled towards the manifold, ensuring the convergence of ODCGM towards $\mathcal{M}$. ODCGM is much simpler to implement than the classical methods which require the computation of a retraction. Moreover, we show that ODCGM exhibits the near-optimal oracle complexities $\mathcal{O}(1/\varepsilon^2)$ and $\mathcal{O}(1/\varepsilon^4)$ in the deterministic and stochastic cases, respectively. Furthermore, we establish that, under an appropriate choice of the projection metric, our method recovers the landing algorithm of Ablin and Peyré (2022), a recently introduced algorithm for optimization over the Stiefel manifold. As a result, we significantly extend the analysis of Ablin and Peyré (2022), establishing near-optimal rates both in deterministic and stochastic frameworks. Finally, we perform numerical experiments which shows the efficiency of ODCGM in a high-dimensional setting.",True,True,"Schechtman, Sholom and Tiapkin, Daniil and Muehlebach, Michael and Moulines, Eric",2023.0,,,,
Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing,2510.22044v1,rousset2010free,\cite{rousset2010free},Free energy computations: a mathematical perspective,,,True,False,"Rousset, Mathias and Stoltz, Gabriel and Lelievre, Tony",2010.0,,,,
Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing,2510.22044v1,lelievre2012langevin,\cite{lelievre2012langevin},{L}angevin dynamics with constraints and computation of free energy differences,,,True,False,"Lelievre, Tony and Rousset, Mathias and Stoltz, Gabriel",2012.0,,,,Mathematics of Computation
Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing,2510.22044v1,lelievre2019hybrid,\cite{lelievre2019hybrid},Hybrid {M}onte {C}arlo methods for sampling probability measures on submanifolds,,,True,False,"Leli{\`e}vre, Tony and Rousset, Mathias and Stoltz, Gabriel",2019.0,,,,Numerische Mathematik
Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing,2510.22044v1,ciccotti1986molecular,\cite{ciccotti1986molecular},Molecular dynamics simulation of rigid molecules,,,True,False,"Ciccotti, Giovanni and Ryckaert, Jean-Paul",1986.0,,,,Computer Physics Reports
Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing,2510.22044v1,ryckaert1977numerical,\cite{ryckaert1977numerical},Numerical integration of the cartesian equations of motion of a system with constraints: molecular dynamics of n-alkanes,,,True,False,"Ryckaert, Jean-Paul and Ciccotti, Giovanni and Berendsen, Herman JC",1977.0,,,,Journal of Computational Physics
Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing,2510.22044v1,andersen1983rattle,\cite{andersen1983rattle},Rattle: A “velocity” version of the shake algorithm for molecular dynamics calculations,,,True,False,"Andersen, Hans C",1983.0,,,,Journal of computational Physics
Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing,2510.22044v1,bubeck2018sampling,\cite{bubeck2018sampling},Sampling from a log-concave distribution with projected {L}angevin {M}onte {C}arlo,,,True,False,"Bubeck, S{\'e}bastien and Eldan, Ronen and Lehec, Joseph",2018.0,,,,Discrete \& Computational Geometry
Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing,2510.22044v1,bubeck2015finite,\cite{bubeck2015finite},{Finite-time analysis of projected Langevin Monte Carlo},,,True,False,"Bubeck, Sebastien and Eldan, Ronen and Lehec, Joseph",2015.0,,,,
Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing,2510.22044v1,kook2022sampling,\cite{kook2022sampling},Sampling with {R}iemannian {H}amiltonian {M}onte {C}arlo in a constrained space,,,True,False,"Kook, Yunbum and Lee, Yin Tat and Shen, Ruoqi and Vempala, Santosh S",2022.0,,,,
Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing,2510.22044v1,kook2024gaussian,\cite{kook2024gaussian},Gaussian cooling and {Dikin} walks: The interior-point method for logconcave sampling,,,True,False,"Kook, Yunbum and Vempala, Santosh S",2024.0,,,,
Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing,2510.22044v1,sato2025convergence,\cite{sato2025convergence},Convergence Error Analysis of Reflected Gradient Langevin Dynamics for Globally Optimizing Non-Convex Constrained Problems,https://arxiv.org/abs/2203.10215v3,"Gradient Langevin dynamics and a variety of its variants have attracted increasing attention owing to their convergence towards the global optimal solution, initially in the unconstrained convex framework while recently even in convex constrained non-convex problems. In the present work, we extend those frameworks to non-convex problems on a non-convex feasible region with a global optimization algorithm built upon reflected gradient Langevin dynamics and derive its convergence rates. By effectively making use of its reflection at the boundary in combination with the probabilistic representation for the Poisson equation with the Neumann boundary condition, we present promising convergence rates, particularly faster than the existing one for convex constrained non-convex problems.",True,True,"Sato, Kanji and Takeda, Akiko and Kawai, Reiichiro and Suzuki, Taiji",2025.0,,,,Japan Journal of Industrial and Applied Mathematics
Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing,2510.22044v1,zhang2020wasserstein,\cite{zhang2020wasserstein},Wasserstein control of mirror {L}angevin {M}onte {C}arlo,,,True,False,"Zhang, Kelvin Shuangjian and Peyr{\'e}, Gabriel and Fadili, Jalal and Pereyra, Marcelo",2020.0,,,,
Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing,2510.22044v1,ahn2021efficient,\cite{ahn2021efficient},Efficient constrained sampling via the mirror-{L}angevin algorithm,,,True,False,"Ahn, Kwangjun and Chewi, Sinho",2021.0,,,,
Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing,2510.22044v1,li2022mirror,\cite{li2022mirror},The mirror {L}angevin algorithm converges with vanishing bias,,,True,False,"Li, Ruilin and Tao, Molei and Vempala, Santosh S and Wibisono, Andre",2022.0,,,,
Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing,2510.22044v1,karagulyan2020penalized,\cite{karagulyan2020penalized},Penalized {L}angevin dynamics with vanishing penalty for smooth and log-concave targets,,,True,False,"Karagulyan, Avetik and Dalalyan, Arnak",2020.0,,,,
Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing,2510.22044v1,gurbuzbalaban2024penalized,\cite{gurbuzbalaban2024penalized},Penalized Overdamped and Underdamped {L}angevin {M}onte {C}arlo Algorithms for Constrained Sampling,,,True,False,"Gurbuzbalaban, Mert and Hu, Yuanhan and Zhu, Lingjiong",2024.0,,,,Journal of Machine Learning Research
Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing,2510.22044v1,chamon2024constrained,\cite{chamon2024constrained},Constrained Sampling with Primal-Dual {L}angevin {M}onte {C}arlo,,,True,False,"Chamon, Luiz F and Karimi, Mohammad R and Korba, Anna",2024.0,,,,
Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing,2510.22044v1,girolami2011riemann,\cite{girolami2011riemann},{R}iemann manifold {L}angevin and {H}amiltonian {M}onte {C}arlo methods,,,True,False,"Girolami, Mark and Calderhead, Ben",2011.0,,,,Journal of the Royal Statistical Society Series B: Statistical Methodology
Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing,2510.22044v1,cheng2022efficient,\cite{cheng2022efficient},Efficient sampling on {R}iemannian manifolds via {L}angevin {MCMC},,,True,False,"Cheng, Xiang and Zhang, Jingzhao and Sra, Suvrit",2022.0,,,,
Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing,2510.22044v1,gatmiry2022convergence,\cite{gatmiry2022convergence},Convergence of the {R}iemannian {L}angevin algorithm,,,True,False,"Gatmiry, Khashayar and Vempala, Santosh S",2022.0,,,,arXiv preprint arXiv:2204.10818
Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing,2510.22044v1,kong2024convergence,\cite{kong2024convergence},Convergence of Kinetic Langevin Monte Carlo on Lie groups,https://arxiv.org/abs/2403.12012v2,"Explicit, momentum-based dynamics for optimizing functions defined on Lie groups was recently constructed, based on techniques such as variational optimization and left trivialization. We appropriately add tractable noise to the optimization dynamics to turn it into a sampling dynamics, leveraging the advantageous feature that the trivialized momentum variable is Euclidean despite that the potential function lives on a manifold. We then propose a Lie-group MCMC sampler, by delicately discretizing the resulting kinetic-Langevin-type sampling dynamics. The Lie group structure is exactly preserved by this discretization. Exponential convergence with explicit convergence rate for both the continuous dynamics and the discrete sampler are then proved under $W_2$ distance. Only compactness of the Lie group and geodesically $L$-smoothness of the potential function are needed. To the best of our knowledge, this is the first convergence result for kinetic Langevin on curved spaces, and also the first quantitative result that requires no convexity or, at least not explicitly, any common relaxation such as isoperimetry.",True,True,"Kong, Lingkai and Tao, Molei",2024.0,,,,
Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction,2511.05396v1,mannor2016robust,\cite{mannor2016robust},Clustering Time Series and the Surprising Robustness of HMMs,https://arxiv.org/abs/1605.02531v2,"Suppose that we are given a time series where consecutive samples are believed to come from a probabilistic source, that the source changes from time to time and that the total number of sources is fixed. Our objective is to estimate the distributions of the sources. A standard approach to this problem is to model the data as a hidden Markov model (HMM). However, since the data often lacks the Markov or the stationarity properties of an HMM, one can ask whether this approach is still suitable or perhaps another approach is required. In this paper we show that a maximum likelihood HMM estimator can be used to approximate the source distributions in a much larger class of models than HMMs. Specifically, we propose a natural and fairly general non-stationary model of the data, where the only restriction is that the sources do not change too often. Our main result shows that for this model, a maximum-likelihood HMM estimator produces the correct second moment of the data, and the results can be extended to higher moments.",True,True,,,,,,
Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction,2511.05396v1,panaganti2022robust,\cite{panaganti2022robust},Robust Reinforcement Learning using Offline Data,https://arxiv.org/abs/2208.05129v2,"The goal of robust reinforcement learning (RL) is to learn a policy that is robust against the uncertainty in model parameters. Parameter uncertainty commonly occurs in many real-world RL applications due to simulator modeling errors, changes in the real-world system dynamics over time, and adversarial disturbances. Robust RL is typically formulated as a max-min problem, where the objective is to learn the policy that maximizes the value against the worst possible models that lie in an uncertainty set. In this work, we propose a robust RL algorithm called Robust Fitted Q-Iteration (RFQI), which uses only an offline dataset to learn the optimal robust policy. Robust RL with offline data is significantly more challenging than its non-robust counterpart because of the minimization over all models present in the robust Bellman operator. This poses challenges in offline data collection, optimization over the models, and unbiased estimation. In this work, we propose a systematic approach to overcome these challenges, resulting in our RFQI algorithm. We prove that RFQI learns a near-optimal robust policy under standard assumptions and demonstrate its superior performance on standard benchmark problems.",True,True,,,,,,
Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction,2511.05396v1,wang2024sample,\cite{wang2024sample},Asteroid (101955) Bennu in the Laboratory: Properties of the Sample Collected by OSIRIS-REx,https://arxiv.org/abs/2404.12536v1,"On 24 September 2023, the NASA OSIRIS-REx mission dropped a capsule to Earth containing approximately 120 g of pristine carbonaceous regolith from Bennu. We describe the delivery and initial allocation of this asteroid sample and introduce its bulk physical, chemical, and mineralogical properties from early analyses. The regolith is very dark overall, with higher-reflectance inclusions and particles interspersed. Particle sizes range from sub-micron dust to a stone about 3.5 cm long. Millimeter-scale and larger stones typically have hummocky or angular morphologies. A subset of the stones appears mottled by brighter material that occurs as veins and crusts. Hummocky stones have the lowest densities and mottled stones have the highest. Remote sensing of the surface of Bennu detected hydrated phyllosilicates, magnetite, organic compounds, carbonates, and scarce anhydrous silicates, all of which the sample confirms. We also find sulfides, presolar grains, and, less expectedly, Na-rich phosphates, as well as other trace phases. The sample composition and mineralogy indicate substantial aqueous alteration and resemble those of Ryugu and the most chemically primitive, low-petrologic-type carbonaceous chondrites. Nevertheless, we find distinct hydrogen, nitrogen, and oxygen isotopic compositions, and some of the material we analyzed is enriched in fluid-mobile elements. Our findings underscore the value of sample return, especially for low-density material that may not readily survive atmospheric entry, and lay the groundwork for more comprehensive analyses.",True,True,,,,,,
Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction,2511.05396v1,liu2024minimax,\cite{liu2024minimax},MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention,https://arxiv.org/abs/2506.13585v1,"We introduce MiniMax-M1, the world's first open-weight, large-scale hybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid Mixture-of-Experts (MoE) architecture combined with a lightning attention mechanism. The model is developed based on our previous MiniMax-Text-01 model, which contains a total of 456 billion parameters with 45.9 billion parameters activated per token. The M1 model natively supports a context length of 1 million tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning attention mechanism in MiniMax-M1 enables efficient scaling of test-time compute. These properties make M1 particularly suitable for complex tasks that require processing long inputs and thinking extensively. MiniMax-M1 is trained using large-scale reinforcement learning (RL) on diverse problems including sandbox-based, real-world software engineering environments. In addition to M1's inherent efficiency advantage for RL training, we propose CISPO, a novel RL algorithm to further enhance RL efficiency. CISPO clips importance sampling weights rather than token updates, outperforming other competitive RL variants. Combining hybrid-attention and CISPO enables MiniMax-M1's full RL training on 512 H800 GPUs to complete in only three weeks, with a rental cost of just $534,700. We release two versions of MiniMax-M1 models with 40K and 80K thinking budgets respectively, where the 40K model represents an intermediate phase of the 80K training. Experiments on standard benchmarks show that our models are comparable or superior to strong open-weight models such as the original DeepSeek-R1 and Qwen3-235B, with particular strengths in complex software engineering, tool utilization, and long-context tasks. We publicly release MiniMax-M1 at https://github.com/MiniMax-AI/MiniMax-M1.",True,True,,,,,,
Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction,2511.05396v1,panaganti2024model,\cite{panaganti2024model},Model-Free Robust $φ$-Divergence Reinforcement Learning Using Both Offline and Online Data,https://arxiv.org/abs/2405.05468v1,"The robust $φ$-regularized Markov Decision Process (RRMDP) framework focuses on designing control policies that are robust against parameter uncertainties due to mismatches between the simulator (nominal) model and real-world settings. This work makes two important contributions. First, we propose a model-free algorithm called Robust $φ$-regularized fitted Q-iteration (RPQ) for learning an $ε$-optimal robust policy that uses only the historical data collected by rolling out a behavior policy (with robust exploratory requirement) on the nominal model. To the best of our knowledge, we provide the first unified analysis for a class of $φ$-divergences achieving robust optimal policies in high-dimensional systems with general function approximation. Second, we introduce the hybrid robust $φ$-regularized reinforcement learning framework to learn an optimal robust policy using both historical data and online sampling. Towards this framework, we propose a model-free algorithm called Hybrid robust Total-variation-regularized Q-iteration (HyTQ: pronounced height-Q). To the best of our knowledge, we provide the first improved out-of-data-distribution assumption in large-scale problems with general function approximation under the hybrid robust $φ$-regularized reinforcement learning framework. Finally, we provide theoretical guarantees on the performance of the learned policies of our algorithms on systems with arbitrary large state space.",True,True,,,,,,
Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction,2511.05396v1,badrinath2021robust,\cite{badrinath2021robust},Sample Complexity of Robust Reinforcement Learning with a Generative Model,https://arxiv.org/abs/2112.01506v3,"The Robust Markov Decision Process (RMDP) framework focuses on designing control policies that are robust against the parameter uncertainties due to the mismatches between the simulator model and real-world settings. An RMDP problem is typically formulated as a max-min problem, where the objective is to find the policy that maximizes the value function for the worst possible model that lies in an uncertainty set around a nominal model. The standard robust dynamic programming approach requires the knowledge of the nominal model for computing the optimal robust policy. In this work, we propose a model-based reinforcement learning (RL) algorithm for learning an $ε$-optimal robust policy when the nominal model is unknown. We consider three different forms of uncertainty sets, characterized by the total variation distance, chi-square divergence, and KL divergence. For each of these uncertainty sets, we give a precise characterization of the sample complexity of our proposed algorithm. In addition to the sample complexity results, we also present a formal analytical argument on the benefit of using robust policies. Finally, we demonstrate the performance of our algorithm on two benchmark problems.",True,True,,,,,,
Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction,2511.05396v1,eysenbach2020off,\cite{eysenbach2020off},Off-Dynamics Reinforcement Learning: Training for Transfer with Domain Classifiers,https://arxiv.org/abs/2006.13916v2,"We propose a simple, practical, and intuitive approach for domain adaptation in reinforcement learning. Our approach stems from the idea that the agent's experience in the source domain should look similar to its experience in the target domain. Building off of a probabilistic view of RL, we formally show that we can achieve this goal by compensating for the difference in dynamics by modifying the reward function. This modified reward function is simple to estimate by learning auxiliary classifiers that distinguish source-domain transitions from target-domain transitions. Intuitively, the modified reward function penalizes the agent for visiting states and taking actions in the source domain which are not possible in the target domain. Said another way, the agent is penalized for transitions that would indicate that the agent is interacting with the source domain, rather than the target domain. Our approach is applicable to domains with continuous states and actions and does not require learning an explicit model of the dynamics. On discrete and continuous control tasks, we illustrate the mechanics of our approach and demonstrate its scalability to high-dimensional tasks.",True,True,,,,,,
Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction,2511.05396v1,guo2024off,\cite{guo2024off},The Ninth NTIRE 2024 Efficient Super-Resolution Challenge Report,https://arxiv.org/abs/2404.10343v2,"This paper provides a comprehensive review of the NTIRE 2024 challenge, focusing on efficient single-image super-resolution (ESR) solutions and their outcomes. The task of this challenge is to super-resolve an input image with a magnification factor of x4 based on pairs of low and corresponding high-resolution images. The primary objective is to develop networks that optimize various aspects such as runtime, parameters, and FLOPs, while still maintaining a peak signal-to-noise ratio (PSNR) of approximately 26.90 dB on the DIV2K_LSDIR_valid dataset and 26.99 dB on the DIV2K_LSDIR_test dataset. In addition, this challenge has 4 tracks including the main track (overall performance), sub-track 1 (runtime), sub-track 2 (FLOPs), and sub-track 3 (parameters). In the main track, all three metrics (ie runtime, FLOPs, and parameter count) were considered. The ranking of the main track is calculated based on a weighted sum-up of the scores of all other sub-tracks. In sub-track 1, the practical runtime performance of the submissions was evaluated, and the corresponding score was used to determine the ranking. In sub-track 2, the number of FLOPs was considered. The score calculated based on the corresponding FLOPs was used to determine the ranking. In sub-track 3, the number of parameters was considered. The score calculated based on the corresponding parameters was used to determine the ranking. RLFN is set as the baseline for efficiency measurement. The challenge had 262 registered participants, and 34 teams made valid submissions. They gauge the state-of-the-art in efficient single-image super-resolution. To facilitate the reproducibility of the challenge and enable other researchers to build upon these findings, the code and the pre-trained model of validated solutions are made publicly available at https://github.com/Amazingren/NTIRE2024_ESR/.",True,True,,,,,,
Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction,2511.05396v1,da2025survey,\cite{da2025survey},Roman Galactic Plane Survey Definition Committee Report,https://arxiv.org/abs/2511.07494v1,"The Roman Galactic Plane Survey (RGPS) is a 700-hour program approved for early definition as a community-designed General Astrophysics Survey. It was selected following a proposal call for science programs that would benefit from an early community-based definition (Sanderson et al 2024). The community was invited to submit white papers and science pitches with a deadline of May 20, 2024; the Roman Galactic Plane Survey Definition Committee (RGPS-DC) first met on Sep 11, 2024. Based on the input provided, the RGPS-DC recommends a survey consisting of three elements: (1) a wide-field science element (691 sq deg, 541 hrs) covering the Galactic plane, Galactic latitude |b|<2 deg and Galactic longitude l=+50.1 deg to -79 deg (281 deg), in four filters (F129, F159, F184, and F213) with higher latitude extensions for the bulge, the Serpens South/W40 star formation region, and Carina, (2) a time-domain science element (19 sq deg , 130 hrs) of six fields, including the full Nuclear Stellar Disk (NSD) and Central Molecular Zone (CMZ), with coverage in seven filters and repeat observations in one or more filters with cadences from 11 minutes to weeks, and (3) a deep-field/spectroscopic science element (4 sq deg , 30 hrs) consisting of fifteen Roman pointings (with a wide range of extinction, diffuse emission, stellar density and population) using longer exposure times in seven filters in addition to grism and prism observations. This document summarizes the science that can be done with this survey, the process of survey definition, and details on all of the program elements.",True,True,,,,,,
Covariance Scattering Transforms,2511.08878v1,Jolliffe2002pca,\cite{Jolliffe2002pca},Principal Component Projection Without Principal Component Analysis,https://arxiv.org/abs/1602.06872v2,"We show how to efficiently project a vector onto the top principal components of a matrix, without explicitly computing these components. Specifically, we introduce an iterative algorithm that provably computes the projection using few calls to any black-box routine for ridge regression.
  By avoiding explicit principal component analysis (PCA), our algorithm is the first with no runtime dependence on the number of top principal components. We show that it can be used to give a fast iterative method for the popular principal component regression problem, giving the first major runtime improvement over the naive method of combining PCA with regression.
  To achieve our results, we first observe that ridge regression can be used to obtain a ""smooth projection"" onto the top principal components. We then sharpen this approximation to true projection using a low-degree polynomial approximation to the matrix step function. Step function approximation is a topic of long-term interest in scientific computing. We extend prior theory by constructing polynomials with simple iterative structure and rigorously analyzing their behavior under limited precision.",True,True,"{Jolliffe}, Ian",,,,,
Covariance Scattering Transforms,2511.08878v1,child2006essentials,\cite{child2006essentials},The essentials of factor analysis,,,True,False,"Child, Dennis",2006.0,,,,
Covariance Scattering Transforms,2511.08878v1,sihag2022covariance,\cite{sihag2022covariance},coVariance Neural Networks,https://arxiv.org/abs/2205.15856v4,"Graph neural networks (GNN) are an effective framework that exploit inter-relationships within graph-structured data for learning. Principal component analysis (PCA) involves the projection of data on the eigenspace of the covariance matrix and draws similarities with the graph convolutional filters in GNNs. Motivated by this observation, we study a GNN architecture, called coVariance neural network (VNN), that operates on sample covariance matrices as graphs. We theoretically establish the stability of VNNs to perturbations in the covariance matrix, thus, implying an advantage over standard PCA-based data analysis approaches that are prone to instability due to principal components associated with close eigenvalues. Our experiments on real-world datasets validate our theoretical results and show that VNN performance is indeed more stable than PCA-based statistical approaches. Moreover, our experiments on multi-resolution datasets also demonstrate that VNNs are amenable to transferability of performance over covariance matrices of different dimensions; a feature that is infeasible for PCA-based approaches.",True,True,"Sihag, Saurabh and Mateos, Gonzalo and McMillan, Corey and Ribeiro, Alejandro",2022.0,,,,Advances in neural information processing systems
Covariance Scattering Transforms,2511.08878v1,sihag2024explainable,\cite{sihag2024explainable},Explainable Brain Age Prediction using coVariance Neural Networks,https://arxiv.org/abs/2305.18370v3,"In computational neuroscience, there has been an increased interest in developing machine learning algorithms that leverage brain imaging data to provide estimates of ""brain age"" for an individual. Importantly, the discordance between brain age and chronological age (referred to as ""brain age gap"") can capture accelerated aging due to adverse health conditions and therefore, can reflect increased vulnerability towards neurological disease or cognitive impairments. However, widespread adoption of brain age for clinical decision support has been hindered due to lack of transparency and methodological justifications in most existing brain age prediction algorithms. In this paper, we leverage coVariance neural networks (VNN) to propose an explanation-driven and anatomically interpretable framework for brain age prediction using cortical thickness features. Specifically, our brain age prediction framework extends beyond the coarse metric of brain age gap in Alzheimer's disease (AD) and we make two important observations: (i) VNNs can assign anatomical interpretability to elevated brain age gap in AD by identifying contributing brain regions, (ii) the interpretability offered by VNNs is contingent on their ability to exploit specific eigenvectors of the anatomical covariance matrix. Together, these observations facilitate an explainable and anatomically interpretable perspective to the task of brain age prediction.",True,True,"Sihag, Saurabh and Mateos, Gonzalo and McMillan, Corey and Ribeiro, Alejandro",2024.0,,,,Advances in Neural Information Processing Systems
Covariance Scattering Transforms,2511.08878v1,cavallo2024stvnn,\cite{cavallo2024stvnn},Spatiotemporal Covariance Neural Networks,https://arxiv.org/abs/2409.10068v1,"Modeling spatiotemporal interactions in multivariate time series is key to their effective processing, but challenging because of their irregular and often unknown structure. Statistical properties of the data provide useful biases to model interdependencies and are leveraged by correlation and covariance-based networks as well as by processing pipelines relying on principal component analysis (PCA). However, PCA and its temporal extensions suffer instabilities in the covariance eigenvectors when the corresponding eigenvalues are close to each other, making their application to dynamic and streaming data settings challenging. To address these issues, we exploit the analogy between PCA and graph convolutional filters to introduce the SpatioTemporal coVariance Neural Network (STVNN), a relational learning model that operates on the sample covariance matrix of the time series and leverages joint spatiotemporal convolutions to model the data. To account for the streaming and non-stationary setting, we consider an online update of the parameters and sample covariance matrix. We prove the STVNN is stable to the uncertainties introduced by these online estimations, thus improving over temporal PCA-based methods. Experimental results corroborate our theoretical findings and show that STVNN is competitive for multivariate time series processing, it adapts to changes in the data distribution, and it is orders of magnitude more stable than online temporal PCA.",True,True,"Cavallo, Andrea
and Sabbaqi, Mohammad
and Isufi, Elvin",2024.0,,,,
Covariance Scattering Transforms,2511.08878v1,cavallo2024sparsecovarianceneuralnetworks,\cite{cavallo2024sparsecovarianceneuralnetworks},Sparse Covariance Neural Networks,https://arxiv.org/abs/2410.01669v2,"Covariance Neural Networks (VNNs) perform graph convolutions on the covariance matrix of input data to leverage correlation information as pairwise connections. They have achieved success in a multitude of applications such as neuroscience, financial forecasting, and sensor networks. However, the empirical covariance matrix on which VNNs operate typically contains spurious correlations, creating a mismatch with the actual covariance matrix that degrades VNNs' performance and computational efficiency. To tackle this issue, we put forth Sparse coVariance Neural Networks (S-VNNs), a framework that applies sparsification techniques on the sample covariance matrix and incorporates the latter into the VNN architecture. We investigate the S-VNN when the underlying data covariance matrix is both sparse and dense. When the true covariance matrix is sparse, we propose hard and soft thresholding to improve the covariance estimation and reduce the computational cost. Instead, when the true covariance is dense, we propose a stochastic sparsification where data correlations are dropped in probability according to principled strategies. Besides performance and computation improvements, we show that S-VNNs are more stable to finite-sample covariance estimations than nominal VNNs and the analogous sparse principal component analysis. By analyzing the impact of sparsification on their behavior, we tie the S-VNN stability to the data distribution and sparsification approach. We support our theoretical findings with experimental results on a variety of application scenarios, ranging from brain data to human action recognition, and show an improved task performance, improved stability, and reduced computational time compared to alternatives.",True,True,Andrea Cavallo and Zhan Gao and Elvin Isufi,2024.0,,https://arxiv.org/abs/2410.01669,,
Covariance Scattering Transforms,2511.08878v1,cavallo2025precision,\cite{cavallo2025precision},Precision Neural Networks: Joint Graph And Relational Learning,https://arxiv.org/abs/2509.14821v1,"CoVariance Neural Networks (VNNs) perform convolutions on the graph determined by the covariance matrix of the data, which enables expressive and stable covariance-based learning. However, covariance matrices are typically dense, fail to encode conditional independence, and are often precomputed in a task-agnostic way, which may hinder performance. To overcome these limitations, we study Precision Neural Networks (PNNs), i.e., VNNs on the precision matrix -- the inverse covariance. The precision matrix naturally encodes statistical independence, often exhibits sparsity, and preserves the covariance spectral structure. To make precision estimation task-aware, we formulate an optimization problem that jointly learns the network parameters and the precision matrix, and solve it via alternating optimization, by sequentially updating the network weights and the precision estimate. We theoretically bound the distance between the estimated and true precision matrices at each iteration, and demonstrate the effectiveness of joint estimation compared to two-step approaches on synthetic and real-world data.",True,True,"Cavallo, Andrea and Rey, Samuel and Marques, Antonio G and Isufi, Elvin",2025.0,,,,arXiv preprint arXiv:2509.14821
Covariance Scattering Transforms,2511.08878v1,cavallo2025fair,\cite{cavallo2025fair},Fair CoVariance Neural Networks,https://arxiv.org/abs/2409.08558v2,"Covariance-based data processing is widespread across signal processing and machine learning applications due to its ability to model data interconnectivities and dependencies. However, harmful biases in the data may become encoded in the sample covariance matrix and cause data-driven methods to treat different subpopulations unfairly. Existing works such as fair principal component analysis (PCA) mitigate these effects, but remain unstable in low sample regimes, which in turn may jeopardize the fairness goal. To address both biases and instability, we propose Fair coVariance Neural Networks (FVNNs), which perform graph convolutions on the covariance matrix for both fair and accurate predictions. Our FVNNs provide a flexible model compatible with several existing bias mitigation techniques. In particular, FVNNs allow for mitigating the bias in two ways: first, they operate on fair covariance estimates that remove biases from their principal components; second, they are trained in an end-to-end fashion via a fairness regularizer in the loss function so that the model parameters are tailored to solve the task directly in a fair manner. We prove that FVNNs are intrinsically fairer than analogous PCA approaches thanks to their stability in low sample regimes. We validate the robustness and fairness of our model on synthetic and real-world data, showcasing the flexibility of FVNNs along with the tradeoff between fair and accurate performance.",True,True,"Cavallo, Andrea and Navarro, Madeline and Segarra, Santiago and Isufi, Elvin",2025.0,,,,
Covariance Scattering Transforms,2511.08878v1,mallat1999wavelet,\cite{mallat1999wavelet},A wavelet tour of signal processing,,,True,False,"Mallat, St{\'e}phane",1999.0,,,,
Covariance Scattering Transforms,2511.08878v1,hammond2011wavelets,\cite{hammond2011wavelets},Wavelets on Graphs via Spectral Graph Theory,https://arxiv.org/abs/0912.3848v1,"We propose a novel method for constructing wavelet transforms of functions defined on the vertices of an arbitrary finite weighted graph. Our approach is based on defining scaling using the the graph analogue of the Fourier domain, namely the spectral decomposition of the discrete graph Laplacian $Ł$. Given a wavelet generating kernel $g$ and a scale parameter $t$, we define the scaled wavelet operator $T_g^t = g(tŁ)$. The spectral graph wavelets are then formed by localizing this operator by applying it to an indicator function. Subject to an admissibility condition on $g$, this procedure defines an invertible transform. We explore the localization properties of the wavelets in the limit of fine scales. Additionally, we present a fast Chebyshev polynomial approximation algorithm for computing the transform that avoids the need for diagonalizing $Ł$. We highlight potential applications of the transform through examples of wavelets on graphs corresponding to a variety of different problem domains.",True,True,"Hammond, David K and Vandergheynst, Pierre and Gribonval, R{\'e}mi",2011.0,,,,Applied and Computational Harmonic Analysis
Covariance Scattering Transforms,2511.08878v1,shuman2015spectrum,\cite{shuman2015spectrum},Spectrum-Adapted Tight Graph Wavelet and Vertex-Frequency Frames,https://arxiv.org/abs/1311.0897v1,"We consider the problem of designing spectral graph filters for the construction of dictionaries of atoms that can be used to efficiently represent signals residing on weighted graphs. While the filters used in previous spectral graph wavelet constructions are only adapted to the length of the spectrum, the filters proposed in this paper are adapted to the distribution of graph Laplacian eigenvalues, and therefore lead to atoms with better discriminatory power. Our approach is to first characterize a family of systems of uniformly translated kernels in the graph spectral domain that give rise to tight frames of atoms generated via generalized translation on the graph. We then warp the uniform translates with a function that approximates the cumulative spectral density function of the graph Laplacian eigenvalues. We use this approach to construct computationally efficient, spectrum-adapted, tight vertex-frequency and graph wavelet frames. We give numerous examples of the resulting spectrum-adapted graph filters, and also present an illustrative example of vertex-frequency analysis using the proposed construction.",True,True,"Shuman, David I and Wiesmeyr, Christoph and Holighaus, Nicki and Vandergheynst, Pierre",2015.0,,,,IEEE Transactions on Signal Processing
Covariance Scattering Transforms,2511.08878v1,bruna2013invariant,\cite{bruna2013invariant},Invariant Scattering Convolution Networks,https://arxiv.org/abs/1203.1513v2,"A wavelet scattering network computes a translation invariant image representation, which is stable to deformations and preserves high frequency information for classification. It cascades wavelet transform convolutions with non-linear modulus and averaging operators. The first network layer outputs SIFT-type descriptors whereas the next layers provide complementary invariant information which improves classification. The mathematical analysis of wavelet scattering networks explains important properties of deep convolution networks for classification.
  A scattering representation of stationary processes incorporates higher order moments and can thus discriminate textures having the same Fourier power spectrum. State of the art classification results are obtained for handwritten digits and texture discrimination, using a Gaussian kernel SVM and a generative PCA classifier.",True,True,"Bruna, Joan and Mallat, St{\'e}phane",2013.0,,,,IEEE transactions on pattern analysis and machine intelligence
Covariance Scattering Transforms,2511.08878v1,gama2019diffusion,\cite{gama2019diffusion},Diffusion Scattering Transforms on Graphs,https://arxiv.org/abs/1806.08829v2,"Stability is a key aspect of data analysis. In many applications, the natural notion of stability is geometric, as illustrated for example in computer vision. Scattering transforms construct deep convolutional representations which are certified stable to input deformations. This stability to deformations can be interpreted as stability with respect to changes in the metric structure of the domain. In this work, we show that scattering transforms can be generalized to non-Euclidean domains using diffusion wavelets, while preserving a notion of stability with respect to metric changes in the domain, measured with diffusion maps. The resulting representation is stable to metric perturbations of the domain while being able to capture ""high-frequency"" information, akin to the Euclidean Scattering.",True,True,"Gama, Fernando and Bruna, Joan and Ribeiro, Alejandro",2019.0,,,,
Covariance Scattering Transforms,2511.08878v1,gama2020stability,\cite{gama2020stability},Stability Properties of Graph Neural Networks,https://arxiv.org/abs/1905.04497v5,"Graph neural networks (GNNs) have emerged as a powerful tool for nonlinear processing of graph signals, exhibiting success in recommender systems, power outage prediction, and motion planning, among others. GNNs consists of a cascade of layers, each of which applies a graph convolution, followed by a pointwise nonlinearity. In this work, we study the impact that changes in the underlying topology have on the output of the GNN. First, we show that GNNs are permutation equivariant, which implies that they effectively exploit internal symmetries of the underlying topology. Then, we prove that graph convolutions with integral Lipschitz filters, in combination with the frequency mixing effect of the corresponding nonlinearities, yields an architecture that is both stable to small changes in the underlying topology and discriminative of information located at high frequencies. These are two properties that cannot simultaneously hold when using only linear graph filters, which are either discriminative or stable, thus explaining the superior performance of GNNs.",True,True,"Gama, Fernando and Bruna, Joan and Ribeiro, Alejandro",2020.0,,,,IEEE Transactions on Signal Processing
Covariance Scattering Transforms,2511.08878v1,koke2022graph,\cite{koke2022graph},Graph Scattering beyond Wavelet Shackles,https://arxiv.org/abs/2301.11456v1,"This work develops a flexible and mathematically sound framework for the design and analysis of graph scattering networks with variable branching ratios and generic functional calculus filters. Spectrally-agnostic stability guarantees for node- and graph-level perturbations are derived; the vertex-set non-preserving case is treated by utilizing recently developed mathematical-physics based tools. Energy propagation through the network layers is investigated and related to truncation stability. New methods of graph-level feature aggregation are introduced and stability of the resulting composite scattering architectures is established. Finally, scattering transforms are extended to edge- and higher order tensorial input. Theoretical results are complemented by numerical investigations: Suitably chosen cattering networks conforming to the developed theory perform better than traditional graph-wavelet based scattering approaches in social network graph classification tasks and significantly outperform other graph-based learning approaches to regression of quantum-chemical energies on QM7.",True,True,"Koke, Christian and Kutyniok, Gitta",2022.0,,,,Advances in Neural Information Processing Systems
Covariance Scattering Transforms,2511.08878v1,anden2011multiscale,\cite{anden2011multiscale},Multiscale Scattering for Audio Classification.,,,True,False,"And{\'e}n, Joakim and Mallat, St{\'e}phane",2011.0,,,,
Covariance Scattering Transforms,2511.08878v1,madhu2024unsupervised,\cite{madhu2024unsupervised},Unsupervised Parameter-free Simplicial Representation Learning with Scattering Transforms,,,True,False,Hiren Madhu and Sravanthi Gurugubelli and Sundeep Prabhakar Chepuri,2024.0,,https://openreview.net/forum?id=wmljUnbjy6,,
Adaptive Sample Sharing for Linear Regression,2510.16986v1,PanYang2010,\cite{PanYang2010},A Survey on Transfer Learning,,,True,False,"Pan, Sinno Jialin and Yang, Qiang",2010.0,,,,IEEE Transactions on Knowledge and Data Engineering
Adaptive Sample Sharing for Linear Regression,2510.16986v1,WeissKhoshgoftaarWang2016,\cite{WeissKhoshgoftaarWang2016},A Survey of Transfer Learning,,,True,False,"Weiss, Karl and Khoshgoftaar, Taghi M. and Wang, DingDing",2016.0,,,,Journal of Big Data
Adaptive Sample Sharing for Linear Regression,2510.16986v1,zhuang2020comprehensive,\cite{zhuang2020comprehensive},A Comprehensive Survey on Transfer Learning,https://arxiv.org/abs/1911.02685v3,"Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning researches, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey paper reviews more than forty representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over twenty representative transfer learning models are used for experiments. The models are performed on three different datasets, i.e., Amazon Reviews, Reuters-21578, and Office-31. And the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.",True,True,"Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing",2020.0,,,,Proceedings of the IEEE
Adaptive Sample Sharing for Linear Regression,2510.16986v1,EvgeniouPontil2004,\cite{EvgeniouPontil2004},Sign-regularized Multi-task Learning,https://arxiv.org/abs/2102.11191v1,"Multi-task learning is a framework that enforces different learning tasks to share their knowledge to improve their generalization performance. It is a hot and active domain that strives to handle several core issues; particularly, which tasks are correlated and similar, and how to share the knowledge among correlated tasks. Existing works usually do not distinguish the polarity and magnitude of feature weights and commonly rely on linear correlation, due to three major technical challenges in: 1) optimizing the models that regularize feature weight polarity, 2) deciding whether to regularize sign or magnitude, 3) identifying which tasks should share their sign and/or magnitude patterns. To address them, this paper proposes a new multi-task learning framework that can regularize feature weight signs across tasks. We innovatively formulate it as a biconvex inequality constrained optimization with slacks and propose a new efficient algorithm for the optimization with theoretical guarantees on generalization performance and convergence. Extensive experiments on multiple datasets demonstrate the proposed methods' effectiveness, efficiency, and reasonableness of the regularized feature weighted patterns.",True,True,"Evgeniou, Theodoros and Pontil, Massimiliano",2004.0,,,,
Adaptive Sample Sharing for Linear Regression,2510.16986v1,tan2018survey,\cite{tan2018survey},Transferability in Deep Learning: A Survey,https://arxiv.org/abs/2201.05867v1,"The success of deep learning algorithms generally depends on large-scale data, while humans appear to have inherent ability of knowledge transfer, by recognizing and applying relevant knowledge from previous learning experiences when encountering and solving unseen tasks. Such an ability to acquire and reuse knowledge is known as transferability in deep learning. It has formed the long-term quest towards making deep learning as data-efficient as human learning, and has been motivating fruitful design of more powerful deep learning algorithms. We present this survey to connect different isolated areas in deep learning with their relation to transferability, and to provide a unified and complete view to investigating transferability through the whole lifecycle of deep learning. The survey elaborates the fundamental goals and challenges in parallel with the core principles and methods, covering recent cornerstones in deep architectures, pre-training, task adaptation and domain adaptation. This highlights unanswered questions on the appropriate objectives for learning transferable knowledge and for adapting the knowledge to new tasks and domains, avoiding catastrophic forgetting and negative transfer. Finally, we implement a benchmark and an open-source library, enabling a fair evaluation of deep learning methods in terms of transferability.",True,True,"Tan, Chuanqi and Sun, Fuchun and Kong, Tao and Zhang, Wenchang and Yang, Chao and Liu, Chunfang",2018.0,,,,
Adaptive Sample Sharing for Linear Regression,2510.16986v1,BenDavid2010,\cite{BenDavid2010},A Theory of Learning from Different Domains,,,True,False,"Ben-David, Shai and Blitzer, John and Crammer, Koby and Pereira, Fernando",2010.0,,,,Machine Learning
Adaptive Sample Sharing for Linear Regression,2510.16986v1,Csurka2017,\cite{Csurka2017},Domain Adaptation for Visual Applications: A Comprehensive Survey,,,True,False,"Csurka, Gabriela",2017.0,,,,arXiv preprint arXiv:1702.05374
Adaptive Sample Sharing for Linear Regression,2510.16986v1,yosinski2014transferable,\cite{yosinski2014transferable},How Transferable Are Features in Deep Neural Networks?,,,True,False,"Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod",2014.0,,,,
Adaptive Sample Sharing for Linear Regression,2510.16986v1,Shimodaira2000,\cite{Shimodaira2000},Improving Predictive Inference under Covariate Shift by Weighting the Log-Likelihood Function,,,True,False,"Shimodaira, Hidetoshi",2000.0,,,,Journal of Statistical Planning and Inference
Adaptive Sample Sharing for Linear Regression,2510.16986v1,SugiyamaKrauledatMueller2007,\cite{SugiyamaKrauledatMueller2007},Covariate Shift Adaptation by Importance Weighted Cross Validation,,,True,False,"Sugiyama, Masashi and Krauledat, Matthias and M{\""u}ller, Klaus-Robert",2007.0,,,,Journal of Machine Learning Research
Adaptive Sample Sharing for Linear Regression,2510.16986v1,CourtyFlamaryTuiaRakotomamonjy2017,\cite{CourtyFlamaryTuiaRakotomamonjy2017},Optimal Transport for Domain Adaptation,,,True,False,"Courty, Nicolas and Flamary, R{\'e}mi and Tuia, Devis and Rakotomamonjy, Alain",2017.0,,,10.1109/TPAMI.2016.2592677,IEEE Transactions on Pattern Analysis and Machine Intelligence
Adaptive Sample Sharing for Linear Regression,2510.16986v1,ArgyriouEvgeniouPontil2007,\cite{ArgyriouEvgeniouPontil2007},Multi-Stage Multi-Task Feature Learning,https://arxiv.org/abs/1210.5806v1,"Multi-task sparse feature learning aims to improve the generalization performance by exploiting the shared features among tasks. It has been successfully applied to many applications including computer vision and biomedical informatics. Most of the existing multi-task sparse feature learning algorithms are formulated as a convex sparse regularization problem, which is usually suboptimal, due to its looseness for approximating an $\ell_0$-type regularizer. In this paper, we propose a non-convex formulation for multi-task sparse feature learning based on a novel non-convex regularizer. To solve the non-convex optimization problem, we propose a Multi-Stage Multi-Task Feature Learning (MSMTFL) algorithm; we also provide intuitive interpretations, detailed convergence and reproducibility analysis for the proposed algorithm. Moreover, we present a detailed theoretical analysis showing that MSMTFL achieves a better parameter estimation error bound than the convex formulation. Empirical studies on both synthetic and real-world data sets demonstrate the effectiveness of MSMTFL in comparison with the state of the art multi-task sparse feature learning algorithms.",True,True,"Argyriou, Andreas and Evgeniou, Theodoros and Pontil, Massimiliano",2007.0,,,,
Adaptive Sample Sharing for Linear Regression,2510.16986v1,MansourMohriRostamizadeh2009,\cite{MansourMohriRostamizadeh2009},Domain Adaptation: Learning Bounds and Algorithms,,,True,False,"Mansour, Yishay and Mohri, Mehryar and Rostamizadeh, Afshin",2009.0,,,,
Adaptive Sample Sharing for Linear Regression,2510.16986v1,Rosenstein2005Transfer,\cite{Rosenstein2005Transfer},To Transfer or Not To Transfer,,,True,False,"Rosenstein, Michael T. and Marx, Zvika and Kaelbling, Leslie Pack and Dietterich, Thomas G.",2005.0,,,,
Adaptive Sample Sharing for Linear Regression,2510.16986v1,zhang2022survey,\cite{zhang2022survey},A Survey on Negative Transfer,https://arxiv.org/abs/2009.00909v4,"Transfer learning (TL) utilizes data or knowledge from one or more source domains to facilitate the learning in a target domain. It is particularly useful when the target domain has very few or no labeled data, due to annotation expense, privacy concerns, etc. Unfortunately, the effectiveness of TL is not always guaranteed. Negative transfer (NT), i.e., leveraging source domain data/knowledge undesirably reduces the learning performance in the target domain, has been a long-standing and challenging problem in TL. Various approaches have been proposed in the literature to handle it. However, there does not exist a systematic survey on the formulation of NT, the factors leading to NT, and the algorithms that mitigate NT. This paper fills this gap, by first introducing the definition of NT and its factors, then reviewing about fifty representative approaches for overcoming NT, according to four categories: secure transfer, domain similarity estimation, distant transfer, and NT mitigation. NT in related fields, e.g., multi-task learning, lifelong learning, and adversarial attacks, are also discussed.",True,True,"Zhang, Wen and Deng, Lingfei and Zhang, Lei and Wu, Dongrui",2022.0,,,,IEEE/CAA Journal of Automatica Sinica
Adaptive Sample Sharing for Linear Regression,2510.16986v1,wang2019characterizing,\cite{wang2019characterizing},Characterizing and Avoiding Negative Transfer,,,True,False,"Wang, Zirui and Dai, Zihang and P{\'o}czos, Barnab{\'a}s and Carbonell, Jaime",2019.0,,,,
Adaptive Sample Sharing for Linear Regression,2510.16986v1,obst2021transfer,\cite{obst2021transfer},Transfer Learning for Linear Regression: A Statistical Test of Gain,,,True,False,"Obst, David and Ghattas, Badih and Cugliari, Jairo and Oppenheim, Georges and Claudel, Sandra and Goude, Yannig",2021.0,,,,arXiv preprint arXiv:2102.09504
Adaptive Sample Sharing for Linear Regression,2510.16986v1,cao2023risk,\cite{cao2023risk},Risk of Transfer Learning and Its Applications in Finance,,,True,False,"Cao, Haoyang and Gu, Haotian and Guo, Xin and Rosenbaum, Mathieu",2023.0,,,,arXiv preprint arXiv:2311.03283
Adaptive Sample Sharing for Linear Regression,2510.16986v1,ChenOwenShi2014,\cite{ChenOwenShi2014},Data enriched linear regression,https://arxiv.org/abs/1304.1837v3,"We present a linear regression method for predictions on a small data set making use of a second possibly biased data set that may be much larger. Our method fits linear regressions to the two data sets while penalizing the difference between predictions made by those two models. The resulting algorithm is a shrinkage method similar to those used in small area estimation. We find a Stein-type finding for Gaussian responses: when the model has 5 or more coefficients and 10 or more error degrees of freedom, it becomes inadmissible to use only the small data set, no matter how large the bias is. We also present both plug-in and AICc-based methods to tune our penalty parameter. Most of our results use an $L_2$ penalty, but we obtain formulas for $L_1$ penalized estimates when the model is specialized to the location setting. Ordinary Stein shrinkage provides an inadmissibility result for only 3 or more coefficients, but we find that our shrinkage method typically produces much lower squared errors in as few as 5 or 10 dimensions when the bias is small and essentially equivalent squared errors when the bias is large.",True,True,"Chen, Aiyou and Owen, Art B. and Shi, Minghui",2014.0,,,,arXiv preprint arXiv:1304.1837
Adaptive Sample Sharing for Linear Regression,2510.16986v1,ObozinskiWainwrightJordan2010,\cite{ObozinskiWainwrightJordan2010},Support union recovery in high-dimensional multivariate regression,https://arxiv.org/abs/0808.0711v2,"In multivariate regression, a $K$-dimensional response vector is regressed upon a common set of $p$ covariates, with a matrix $B^*\in\mathbb{R}^{p\times K}$ of regression coefficients. We study the behavior of the multivariate group Lasso, in which block regularization based on the $\ell_1/\ell_2$ norm is used for support union recovery, or recovery of the set of $s$ rows for which $B^*$ is nonzero. Under high-dimensional scaling, we show that the multivariate group Lasso exhibits a threshold for the recovery of the exact row pattern with high probability over the random design and noise that is specified by the sample complexity parameter $θ(n,p,s):=n/[2ψ(B^*)\log(p-s)]$. Here $n$ is the sample size, and $ψ(B^*)$ is a sparsity-overlap function measuring a combination of the sparsities and overlaps of the $K$-regression coefficient vectors that constitute the model. We prove that the multivariate group Lasso succeeds for problem sequences $(n,p,s)$ such that $θ(n,p,s)$ exceeds a critical level $θ_u$, and fails for sequences such that $θ(n,p,s)$ lies below a critical level $θ_{\ell}$. For the special case of the standard Gaussian ensemble, we show that $θ_{\ell}=θ_u$ so that the characterization is sharp. The sparsity-overlap function $ψ(B^*)$ reveals that, if the design is uncorrelated on the active rows, $\ell_1/\ell_2$ regularization for multivariate regression never harms performance relative to an ordinary Lasso approach and can yield substantial improvements in sample complexity (up to a factor of $K$) when the coefficient vectors are suitably orthogonal. For more general designs, it is possible for the ordinary Lasso to outperform the multivariate group Lasso. We complement our analysis with simulations that demonstrate the sharpness of our theoretical results, even for relatively small problems.",True,True,"Obozinski, Guillaume and Wainwright, Martin J. and Jordan, Michael I.",2011.0,,,,The Annals of Statistics
Adaptive Sample Sharing for Linear Regression,2510.16986v1,Daume2007,\cite{Daume2007},Frustratingly Easy Domain Adaptation,https://arxiv.org/abs/0907.1815v1,"We describe an approach to domain adaptation that is appropriate exactly in the case when one has enough ``target'' data to do slightly better than just using only ``source'' data. Our approach is incredibly simple, easy to implement as a preprocessing step (10 lines of Perl!) and outperforms state-of-the-art approaches on a range of datasets. Moreover, it is trivially extended to a multi-domain adaptation problem, where one has data from a variety of different domains.",True,True,"Daum{\'e} III, Hal",2007.0,,,,
Adaptive Sample Sharing for Linear Regression,2510.16986v1,kuzborskij2013stability,\cite{kuzborskij2013stability},Stability and Hypothesis Transfer Learning,,,True,False,"Kuzborskij, Ilja and Orabona, Francesco",2013.0,,,,
Adaptive Sample Sharing for Linear Regression,2510.16986v1,sorocky2020share,\cite{sorocky2020share},To Share or Not to Share? Performance Guarantees and the Asymmetric Nature of Cross-Robot Experience Transfer,,,True,False,"Sorocky, Michael J. and Zhou, Siqi and Schoellig, Angela P.",2020.0,,,,IEEE Control Systems Letters
Adaptive Sample Sharing for Linear Regression,2510.16986v1,CherkaouiICML2025,\cite{CherkaouiICML2025},Adaptive Sample Sharing for Multi-Agent Linear Bandits,,,True,False,"Cherkaoui, Hamza and Barlier, Merwan and Colin, Igor",2025.0,,,,
Adaptive Sample Sharing for Linear Regression,2510.16986v1,deSouzaAcerbiAISTATS2022,\cite{deSouzaAcerbiAISTATS2022},Sample Sharing in Parallel Bayesian Inference,,,True,False,"de Souza, Firstname and Acerbi, Firstname",2022.0,,,,
Functional Adjoint Sampler: Scalable Sampling on Infinite Dimensional Spaces,2511.06239v1,song2021scorebased,\cite{song2021scorebased},Score-Based Generative Modeling through Stochastic Differential Equations,https://arxiv.org/abs/2011.13456v2,"Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.",True,True,Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole,2021.0,,https://openreview.net/forum?id=PxTIG12RRHS,,
Functional Adjoint Sampler: Scalable Sampling on Infinite Dimensional Spaces,2511.06239v1,lim2023score,\cite{lim2023score},Score-based Diffusion Models in Function Space,https://arxiv.org/abs/2302.07400v3,"Diffusion models have recently emerged as a powerful framework for generative modeling. They consist of a forward process that perturbs input data with Gaussian white noise and a reverse process that learns a score function to generate samples by denoising. Despite their tremendous success, they are mostly formulated on finite-dimensional spaces, e.g., Euclidean, limiting their applications to many domains where the data has a functional form, such as in scientific computing and 3D geometric data analysis. This work introduces a mathematically rigorous framework called Denoising Diffusion Operators (DDOs) for training diffusion models in function space. In DDOs, the forward process perturbs input functions gradually using a Gaussian process. The generative process is formulated by a function-valued annealed Langevin dynamic. Our approach requires an appropriate notion of the score for the perturbed data distribution, which we obtain by generalizing denoising score matching to function spaces that can be infinite-dimensional. We show that the corresponding discretized algorithm generates accurate samples at a fixed cost independent of the data resolution. We theoretically and numerically verify the applicability of our approach on a set of function-valued problems, including generating solutions to the Navier-Stokes equation viewed as the push-forward distribution of forcings from a Gaussian Random Field (GRF), as well as volcano InSAR and MNIST-SDF.",True,True,"Lim, Jae Hyun and Kovachki, Nikola B and Baptista, Ricardo and Beckham, Christopher and Azizzadenesheli, Kamyar and Kossaifi, Jean and Voleti, Vikram and Song, Jiaming and Kreis, Karsten and Kautz, Jan and others",2023.0,,,,arXiv preprint arXiv:2302.07400
Functional Adjoint Sampler: Scalable Sampling on Infinite Dimensional Spaces,2511.06239v1,lim2023scorebased,\cite{lim2023scorebased},Score-Based Generative Modeling through Stochastic Differential Equations,https://arxiv.org/abs/2011.13456v2,"Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.",True,True,Sungbin Lim and Eunbi Yoon and Taehyun Byun and Taewon Kang and Seungwoo Kim and Kyungjae Lee and Sungjoon Choi,2023.0,,https://openreview.net/forum?id=GrElRvXnEj,,
Functional Adjoint Sampler: Scalable Sampling on Infinite Dimensional Spaces,2511.06239v1,pidstrigach2023infinite,\cite{pidstrigach2023infinite},Infinite-dimensional diffusion models for function spaces,,,True,False,"Pidstrigach, Jakiw and Marzouk, Youssef and Reich, Sebastian and Wang, Sven",2023.0,,,,arXiv e-prints
Functional Adjoint Sampler: Scalable Sampling on Infinite Dimensional Spaces,2511.06239v1,franzese2024continuous,\cite{franzese2024continuous},Continuous-Time Functional Diffusion Processes,https://arxiv.org/abs/2303.00800v3,"We introduce Functional Diffusion Processes (FDPs), which generalize score-based diffusion models to infinite-dimensional function spaces. FDPs require a new mathematical framework to describe the forward and backward dynamics, and several extensions to derive practical training objectives. These include infinite-dimensional versions of Girsanov theorem, in order to be able to compute an ELBO, and of the sampling theorem, in order to guarantee that functional evaluations in a countable set of points are equivalent to infinite-dimensional functions. We use FDPs to build a new breed of generative models in function spaces, which do not require specialized network architectures, and that can work with any kind of continuous data. Our results on real data show that FDPs achieve high-quality image generation, using a simple MLP architecture with orders of magnitude fewer parameters than existing diffusion models.",True,True,"Franzese, Giulio and Corallo, Giulio and Rossi, Simone and Heinonen, Markus and Filippone, Maurizio and Michiardi, Pietro",2024.0,,,,Advances in Neural Information Processing Systems
Functional Adjoint Sampler: Scalable Sampling on Infinite Dimensional Spaces,2511.06239v1,lipman2023flow,\cite{lipman2023flow},Flow Matching for Generative Modeling,,,True,False,Yaron Lipman and Ricky T. Q. Chen and Heli Ben-Hamu and Maximilian Nickel and Matthew Le,2023.0,,https://openreview.net/forum?id=PqvMRDCJT9t,,
Functional Adjoint Sampler: Scalable Sampling on Infinite Dimensional Spaces,2511.06239v1,kerrigan2024functional,\cite{kerrigan2024functional},Functional Flow Matching,https://arxiv.org/abs/2305.17209v2,"We propose Functional Flow Matching (FFM), a function-space generative model that generalizes the recently-introduced Flow Matching model to operate in infinite-dimensional spaces. Our approach works by first defining a path of probability measures that interpolates between a fixed Gaussian measure and the data distribution, followed by learning a vector field on the underlying space of functions that generates this path of measures. Our method does not rely on likelihoods or simulations, making it well-suited to the function space setting. We provide both a theoretical framework for building such models and an empirical evaluation of our techniques. We demonstrate through experiments on several real-world benchmarks that our proposed FFM method outperforms several recently proposed function-space generative models.",True,True,"Kerrigan, Gavin and Migliorini, Giosue and Smyth, Padhraic",2024.0,,,,
Functional Adjoint Sampler: Scalable Sampling on Infinite Dimensional Spaces,2511.06239v1,shi2024diffusion,\cite{shi2024diffusion},"Diffusion Schr{\""o}dinger bridge matching",,,True,False,"Shi, Yuyang and De Bortoli, Valentin and Campbell, Andrew and Doucet, Arnaud",2024.0,,,,Advances in Neural Information Processing Systems
Functional Adjoint Sampler: Scalable Sampling on Infinite Dimensional Spaces,2511.06239v1,park2024stochastic,\cite{park2024stochastic},Stochastic Optimal Control for Diffusion Bridges in Function Spaces,,,True,False,Byoungwoo Park and Jungwon Choi and Sungbin Lim and Juho Lee,2024.0,,https://openreview.net/forum?id=WyQW4G57Zd,,
Functional Adjoint Sampler: Scalable Sampling on Infinite Dimensional Spaces,2511.06239v1,zhang2022path,\cite{zhang2022path},Path Integral Sampler: a stochastic control approach for sampling,https://arxiv.org/abs/2111.15141v2,"We present Path Integral Sampler~(PIS), a novel algorithm to draw samples from unnormalized probability density functions. The PIS is built on the Schrödinger bridge problem which aims to recover the most likely evolution of a diffusion process given its initial distribution and terminal distribution. The PIS draws samples from the initial distribution and then propagates the samples through the Schrödinger bridge to reach the terminal distribution. Applying the Girsanov theorem, with a simple prior diffusion, we formulate the PIS as a stochastic optimal control problem whose running cost is the control energy and terminal cost is chosen according to the target distribution. By modeling the control as a neural network, we establish a sampling algorithm that can be trained end-to-end. We provide theoretical justification of the sampling quality of PIS in terms of Wasserstein distance when sub-optimal control is used. Moreover, the path integrals theory is used to compute importance weights of the samples to compensate for the bias induced by the sub-optimality of the controller and time-discretization. We experimentally demonstrate the advantages of PIS compared with other start-of-the-art sampling methods on a variety of tasks.",True,True,Qinsheng Zhang and Yongxin Chen,2022.0,,https://openreview.net/forum?id=_uCb2ynRu7Y,,
Functional Adjoint Sampler: Scalable Sampling on Infinite Dimensional Spaces,2511.06239v1,vargas2023bayesian,\cite{vargas2023bayesian},"Bayesian learning via neural {Schr{\""o}dinger--F{\""o}llmer} flows",,,True,False,"Vargas, Francisco and Ovsianas, Andrius and Fernandes, David and Girolami, Mark and Lawrence, Neil D and N{\""u}sken, Nikolas",2023.0,,,,Statistics and Computing
Functional Adjoint Sampler: Scalable Sampling on Infinite Dimensional Spaces,2511.06239v1,berner2022optimal,\cite{berner2022optimal},An optimal control perspective on diffusion-based generative modeling,https://arxiv.org/abs/2211.01364v3,"We establish a connection between stochastic optimal control and generative models based on stochastic differential equations (SDEs), such as recently developed diffusion probabilistic models. In particular, we derive a Hamilton-Jacobi-Bellman equation that governs the evolution of the log-densities of the underlying SDE marginals. This perspective allows to transfer methods from optimal control theory to generative modeling. First, we show that the evidence lower bound is a direct consequence of the well-known verification theorem from control theory. Further, we can formulate diffusion-based generative modeling as a minimization of the Kullback-Leibler divergence between suitable measures in path space. Finally, we develop a novel diffusion-based method for sampling from unnormalized densities -- a problem frequently occurring in statistics and computational sciences. We demonstrate that our time-reversed diffusion sampler (DIS) can outperform other diffusion-based sampling approaches on multiple numerical examples.",True,True,Julius Berner and Lorenz Richter and Karen Ullrich,2024.0,,https://openreview.net/forum?id=oYIjw37pTP,,Transactions on Machine Learning Research
Functional Adjoint Sampler: Scalable Sampling on Infinite Dimensional Spaces,2511.06239v1,havens2025adjoint,\cite{havens2025adjoint},{A}djoint {S}ampling: Highly Scalable Diffusion Samplers via {A}djoint {M}atching,,,True,False,"Havens, Aaron and Miller, Benjamin Kurt and Yan, Bing and Domingo-Enrich, Carles and Sriram, Anuroop and Wood, Brandon and Levine, Daniel and Hu, Bin and Amos, Brandon and Karrer, Brian and Fu, Xiang and Liu, Guan-Horng and Chen, Ricky T. Q.",2025.0,,,,International Conference on Machine Learning
Functional Adjoint Sampler: Scalable Sampling on Infinite Dimensional Spaces,2511.06239v1,liu2025adjoint,\cite{liu2025adjoint},"Adjoint Schr$\backslash$"" odinger Bridge Sampler",,,True,False,"Liu, Guan-Horng and Choi, Jaemoo and Chen, Yongxin and Miller, Benjamin Kurt and Chen, Ricky TQ",2025.0,,,,arXiv preprint arXiv:2506.22565
Robust Experimental Design via Generalised Bayesian Inference,2511.07671v1,cavagnaro,\cite{cavagnaro},Adaptive Design Optimization: A Mutual Information-Based Approach to Model Discrimination in Cognitive Science,,,True,False,"Cavagnaro, Daniel R. and Myung, Jay I. and Pitt, Mark A. and Kujala, Janne V.",2010.0,04,https://doi.org/10.1162/neco.2009.02-09-959,10.1162/neco.2009.02-09-959,Neural Computation
Robust Experimental Design via Generalised Bayesian Inference,2511.07671v1,Hainy_2022,\cite{Hainy_2022},Optimal Bayesian design for model discrimination via classification,,,True,False,"Hainy, Markus and Price, David J. and Restif, Olivier and Drovandi, Christopher",2022.0,,http://dx.doi.org/10.1007/s11222-022-10078-2,10.1007/s11222-022-10078-2,Statistics and Computing
Robust Experimental Design via Generalised Bayesian Inference,2511.07671v1,catanach2023metrics,\cite{catanach2023metrics},Metrics for bayesian optimal experiment design under model misspecification,,,True,False,"Catanach, Tommie A and Das, Niladri",2023.0,,,,
Robust Experimental Design via Generalised Bayesian Inference,2511.07671v1,overstall2022bayesian,\cite{overstall2022bayesian},Bayesian decision-theoretic design of experiments under an alternative model,https://arxiv.org/abs/1909.12570v4,"Traditionally Bayesian decision-theoretic design of experiments proceeds by choosing a design to minimise expectation of a given loss function over the space of all designs. The loss function encapsulates the aim of the experiment, and the expectation is taken with respect to the joint distribution of all unknown quantities implied by the statistical model that will be fitted to observed responses. In this paper, an extended framework is proposed whereby the expectation of the loss is taken with respect to a joint distribution implied by an alternative statistical model. Motivation for this includes promoting robustness, ensuring computational feasibility and for allowing realistic prior specification when deriving a design. To aid in exploring the new framework, an asymptotic approximation to the expected loss under an alternative model is derived, and the properties of different loss functions are established. The framework is then demonstrated on a linear regression versus full-treatment model scenario, on estimating parameters of a non-linear model under model discrepancy and a cubic spline model under an unknown number of basis functions.",True,True,"Overstall, Antony and McGree, James",2022.0,,,,Bayesian Analysis
Robust Experimental Design via Generalised Bayesian Inference,2511.07671v1,forstermisspec,\cite{forstermisspec},Improving Robustness to Model Misspecification in Bayesian Experimental Design,,,True,False,Alexander J. Forster and Desi R. Ivanova and Tom Rainforth,2025.0,,,,Workshop at the 7th Symposium on Advances in Approximate Bayesian Inference
Robust Experimental Design via Generalised Bayesian Inference,2511.07671v1,tang2025generalizationanalysisbayesianoptimal,\cite{tang2025generalizationanalysisbayesianoptimal},Generalization Analysis for Bayesian Optimal Experiment Design under Model Misspecification,,,True,False,Roubing Tang and Sabina J. Sloman and Samuel Kaski,2025.0,,https://arxiv.org/abs/2506.07805,,
Robust Experimental Design via Generalised Bayesian Inference,2511.07671v1,overstall2023gibbsoptimaldesignexperiments,\cite{overstall2023gibbsoptimaldesignexperiments},Gibbs optimal design of experiments,https://arxiv.org/abs/2310.17440v2,"Bayesian optimal design is a well-established approach to planning experiments. A distribution for the responses, i.e. a statistical model, is assumed which is dependent on unknown parameters. A utility function is then specified giving gain in information in estimating the true values of the parameters, using the Bayesian posterior distribution. A Bayesian optimal design is given by maximising expectation of the utility with respect to the distribution implied by statistical model and prior distribution for the true parameter values. The approach accounts for the experimental aim, via specification of the utility, and of assumed sources of uncertainty. However, it is predicated on the statistical model being correct. Recently, a new type of statistical inference, known as Gibbs inference, has been proposed. This is Bayesian-like, i.e. uncertainty for unknown quantities is represented by a posterior distribution, but does not necessarily require specification of a statistical model. The resulting inference is less sensitive to misspecification of the statistical model. This paper introduces Gibbs optimal design: a framework for optimal design of experiments under Gibbs inference. A computational approach to find designs in practice is outlined and the framework is demonstrated on exemplars including linear models, and experiments with count and time-to-event responses.",True,True,Antony M. Overstall and Jacinta Holloway-Brown and James M. McGree,2023.0,,https://arxiv.org/abs/2310.17440,,
Robust Experimental Design via Generalised Bayesian Inference,2511.07671v1,dawid2014theory,\cite{dawid2014theory},Theory and applications of proper scoring rules,,,True,False,"Dawid, Alexander Philip and Musio, Monica",2014.0,,,,Metron
Robust Experimental Design via Generalised Bayesian Inference,2511.07671v1,Giummol__2018,\cite{Giummol__2018},Objective Bayesian inference with proper scoring rules,,,True,False,"Giummolè, F. and Mameli, V. and Ruli, E. and Ventura, L.",2018.0,,http://dx.doi.org/10.1007/s11749-018-0597-z,10.1007/s11749-018-0597-z,TEST
Robust Experimental Design via Generalised Bayesian Inference,2511.07671v1,bochkina2023bernstein,\cite{bochkina2023bernstein},Bernstein--von Mises theorem and misspecified models: A review,,,True,False,"Bochkina, Natalia",2023.0,,,,Foundations of modern statistics
Global Convergence of Four-Layer Matrix Factorization under Random Initialization,2511.09925v1,jain2017globalconvergencenonconvexgradient,\cite{jain2017globalconvergencenonconvexgradient},Global Convergence of Non-Convex Gradient Descent for Computing Matrix Squareroot,,,True,False,Prateek Jain and Chi Jin and Sham M. Kakade and Praneeth Netrapalli,2017.0,,https://arxiv.org/abs/1507.05854,,
Global Convergence of Four-Layer Matrix Factorization under Random Initialization,2511.09925v1,li2019algorithmicregularizationoverparameterizedmatrix,\cite{li2019algorithmicregularizationoverparameterizedmatrix},Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations,https://arxiv.org/abs/1712.09203v5,"We show that the gradient descent algorithm provides an implicit regularization effect in the learning of over-parameterized matrix factorization models and one-hidden-layer neural networks with quadratic activations. Concretely, we show that given $\tilde{O}(dr^{2})$ random linear measurements of a rank $r$ positive semidefinite matrix $X^{\star}$, we can recover $X^{\star}$ by parameterizing it by $UU^\top$ with $U\in \mathbb R^{d\times d}$ and minimizing the squared loss, even if $r \ll d$. We prove that starting from a small initialization, gradient descent recovers $X^{\star}$ in $\tilde{O}(\sqrt{r})$ iterations approximately. The results solve the conjecture of Gunasekar et al.'17 under the restricted isometry property. The technique can be applied to analyzing neural networks with one-hidden-layer quadratic activations with some technical modifications.",True,True,Yuanzhi Li and Tengyu Ma and Hongyang Zhang,2019.0,,https://arxiv.org/abs/1712.09203,,
Global Convergence of Four-Layer Matrix Factorization under Random Initialization,2511.09925v1,Chen_2019,\cite{Chen_2019},Gradient descent with random initialization: fast global convergence for nonconvex phase retrieval,,,True,False,"Chen, Yuxin and Chi, Yuejie and Fan, Jianqing and Ma, Cong",2019.0,,http://dx.doi.org/10.1007/s10107-019-01363-6,10.1007/s10107-019-01363-6,Mathematical Programming
Global Convergence of Four-Layer Matrix Factorization under Random Initialization,2511.09925v1,lee2016gradientdescentconvergesminimizers,\cite{lee2016gradientdescentconvergesminimizers},Gradient Descent Converges to Minimizers,https://arxiv.org/abs/1602.04915v2,"We show that gradient descent converges to a local minimizer, almost surely with random initialization. This is proved by applying the Stable Manifold Theorem from dynamical systems theory.",True,True,Jason D. Lee and Max Simchowitz and Michael I. Jordan and Benjamin Recht,2016.0,,https://arxiv.org/abs/1602.04915,,
Global Convergence of Four-Layer Matrix Factorization under Random Initialization,2511.09925v1,tu2016lowranksolutionslinearmatrix,\cite{tu2016lowranksolutionslinearmatrix},Low-rank Solutions of Linear Matrix Equations via Procrustes Flow,https://arxiv.org/abs/1507.03566v2,"In this paper we study the problem of recovering a low-rank matrix from linear measurements. Our algorithm, which we call Procrustes Flow, starts from an initial estimate obtained by a thresholding scheme followed by gradient descent on a non-convex objective. We show that as long as the measurements obey a standard restricted isometry property, our algorithm converges to the unknown matrix at a geometric rate. In the case of Gaussian measurements, such convergence occurs for a $n_1 \times n_2$ matrix of rank $r$ when the number of measurements exceeds a constant times $(n_1+n_2)r$.",True,True,Stephen Tu and Ross Boczar and Max Simchowitz and Mahdi Soltanolkotabi and Benjamin Recht,2016.0,,https://arxiv.org/abs/1507.03566,,
Global Convergence of Four-Layer Matrix Factorization under Random Initialization,2511.09925v1,ge2017spuriouslocalminimanonconvex,\cite{ge2017spuriouslocalminimanonconvex},No Spurious Local Minima in Nonconvex Low Rank Problems: A Unified Geometric Analysis,https://arxiv.org/abs/1704.00708v1,"In this paper we develop a new framework that captures the common landscape underlying the common non-convex low-rank matrix problems including matrix sensing, matrix completion and robust PCA. In particular, we show for all above problems (including asymmetric cases): 1) all local minima are also globally optimal; 2) no high-order saddle points exists. These results explain why simple algorithms such as stochastic gradient descent have global converge, and efficiently optimize these non-convex objective functions in practice. Our framework connects and simplifies the existing analyses on optimization landscapes for matrix sensing and symmetric matrix completion. The framework naturally leads to new results for asymmetric matrix completion and robust PCA.",True,True,Rong Ge and Chi Jin and Yi Zheng,2017.0,,https://arxiv.org/abs/1704.00708,,
Global Convergence of Four-Layer Matrix Factorization under Random Initialization,2511.09925v1,du2018algorithmicregularizationlearningdeep,\cite{du2018algorithmicregularizationlearningdeep},Algorithmic Regularization in Learning Deep Homogeneous Models: Layers are Automatically Balanced,https://arxiv.org/abs/1806.00900v2,"We study the implicit regularization imposed by gradient descent for learning multi-layer homogeneous functions including feed-forward fully connected and convolutional deep neural networks with linear, ReLU or Leaky ReLU activation. We rigorously prove that gradient flow (i.e. gradient descent with infinitesimal step size) effectively enforces the differences between squared norms across different layers to remain invariant without any explicit regularization. This result implies that if the weights are initially small, gradient flow automatically balances the magnitudes of all layers. Using a discretization argument, we analyze gradient descent with positive step size for the non-convex low-rank asymmetric matrix factorization problem without any regularization. Inspired by our findings for gradient flow, we prove that gradient descent with step sizes $η_t = O\left(t^{-\left( \frac12+δ\right)} \right)$ ($0<δ\le\frac12$) automatically balances two low-rank factors and converges to a bounded global optimum. Furthermore, for rank-$1$ asymmetric matrix factorization we give a finer analysis showing gradient descent with constant step size converges to the global minimum at a globally linear rate. We believe that the idea of examining the invariance imposed by first order algorithms in learning homogeneous models could serve as a fundamental building block for studying optimization for learning deep models.",True,True,Simon S. Du and Wei Hu and Jason D. Lee,2018.0,,https://arxiv.org/abs/1806.00900,,
Global Convergence of Four-Layer Matrix Factorization under Random Initialization,2511.09925v1,ye2021globalconvergencegradientdescent,\cite{ye2021globalconvergencegradientdescent},Global Convergence of Gradient Descent for Asymmetric Low-Rank Matrix Factorization,,,True,False,Tian Ye and Simon S. Du,2021.0,,https://arxiv.org/abs/2106.14289,,
Global Convergence of Four-Layer Matrix Factorization under Random Initialization,2511.09925v1,kawaguchi2016deeplearningpoorlocal,\cite{kawaguchi2016deeplearningpoorlocal},Deep Learning without Poor Local Minima,https://arxiv.org/abs/1605.07110v3,"In this paper, we prove a conjecture published in 1989 and also partially address an open problem announced at the Conference on Learning Theory (COLT) 2015. With no unrealistic assumption, we first prove the following statements for the squared loss function of deep linear neural networks with any depth and any widths: 1) the function is non-convex and non-concave, 2) every local minimum is a global minimum, 3) every critical point that is not a global minimum is a saddle point, and 4) there exist ""bad"" saddle points (where the Hessian has no negative eigenvalue) for the deeper networks (with more than three layers), whereas there is no bad saddle point for the shallow networks (with three layers). Moreover, for deep nonlinear neural networks, we prove the same four statements via a reduction to a deep linear model under the independence assumption adopted from recent work. As a result, we present an instance, for which we can answer the following question: how difficult is it to directly train a deep model in theory? It is more difficult than the classical machine learning models (because of the non-convexity), but not too difficult (because of the nonexistence of poor local minima). Furthermore, the mathematically proven existence of bad saddle points for deeper models would suggest a possible open problem. We note that even though we have advanced the theoretical foundations of deep learning and non-convex optimization, there is still a gap between theory and practice.",True,True,Kenji Kawaguchi,2016.0,,https://arxiv.org/abs/1605.07110,,
Global Convergence of Four-Layer Matrix Factorization under Random Initialization,2511.09925v1,bartlett2018gradientdescentidentityinitialization,\cite{bartlett2018gradientdescentidentityinitialization},Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks,,,True,False,Peter L. Bartlett and David P. Helmbold and Philip M. Long,2018.0,,https://arxiv.org/abs/1802.06093,,
Global Convergence of Four-Layer Matrix Factorization under Random Initialization,2511.09925v1,arora2019convergenceanalysisgradientdescent,\cite{arora2019convergenceanalysisgradientdescent},A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks,,,True,False,Sanjeev Arora and Nadav Cohen and Noah Golowich and Wei Hu,2019.0,,https://arxiv.org/abs/1810.02281,,
Global Convergence of Four-Layer Matrix Factorization under Random Initialization,2511.09925v1,ji2019gradientdescentalignslayers,\cite{ji2019gradientdescentalignslayers},Gradient descent aligns the layers of deep linear networks,,,True,False,Ziwei Ji and Matus Telgarsky,2019.0,,https://arxiv.org/abs/1810.02032,,
Global Convergence of Four-Layer Matrix Factorization under Random Initialization,2511.09925v1,arora2019implicitregularizationdeepmatrix,\cite{arora2019implicitregularizationdeepmatrix},Implicit Regularization in Deep Matrix Factorization,https://arxiv.org/abs/1905.13655v3,"Efforts to understand the generalization mystery in deep learning have led to the belief that gradient-based optimization induces a form of implicit regularization, a bias towards models of low ""complexity."" We study the implicit regularization of gradient descent over deep linear neural networks for matrix completion and sensing, a model referred to as deep matrix factorization. Our first finding, supported by theory and experiments, is that adding depth to a matrix factorization enhances an implicit tendency towards low-rank solutions, oftentimes leading to more accurate recovery. Secondly, we present theoretical and empirical arguments questioning a nascent view by which implicit regularization in matrix factorization can be captured using simple mathematical norms. Our results point to the possibility that the language of standard regularizers may not be rich enough to fully encompass the implicit regularization brought forth by gradient-based optimization.",True,True,Sanjeev Arora and Nadav Cohen and Wei Hu and Yuping Luo,2019.0,,https://arxiv.org/abs/1905.13655,,
Global Convergence of Four-Layer Matrix Factorization under Random Initialization,2511.09925v1,du2019width,\cite{du2019width},Width provably matters in optimization for deep linear neural networks,,,True,False,"Du, Simon and Hu, Wei",2019.0,,,,
Global Convergence of Four-Layer Matrix Factorization under Random Initialization,2511.09925v1,min2023convergence,\cite{min2023convergence},On the convergence of gradient flow on multi-layer linear models,,,True,False,"Min, Hancheng and Vidal, Ren{\'e} and Mallada, Enrique",2023.0,,,,
Global Convergence of Four-Layer Matrix Factorization under Random Initialization,2511.09925v1,min2021explicit,\cite{min2021explicit},On the explicit role of initialization on the convergence and implicit bias of overparametrized linear networks,,,True,False,"Min, Hancheng and Tarmoun, Salma and Vidal, Ren{\'e} and Mallada, Enrique",2021.0,,,,
Global Convergence of Four-Layer Matrix Factorization under Random Initialization,2511.09925v1,xiong2023over,\cite{xiong2023over},How Over-Parameterization Slows Down Gradient Descent in Matrix Sensing: The Curses of Symmetry and Initialization,https://arxiv.org/abs/2310.01769v3,"This paper rigorously shows how over-parameterization changes the convergence behaviors of gradient descent (GD) for the matrix sensing problem, where the goal is to recover an unknown low-rank ground-truth matrix from near-isotropic linear measurements. First, we consider the symmetric setting with the symmetric parameterization where $M^* \in \mathbb{R}^{n \times n}$ is a positive semi-definite unknown matrix of rank $r \ll n$, and one uses a symmetric parameterization $XX^\top$ to learn $M^*$. Here $X \in \mathbb{R}^{n \times k}$ with $k > r$ is the factor matrix. We give a novel $Ω(1/T^2)$ lower bound of randomly initialized GD for the over-parameterized case ($k >r$) where $T$ is the number of iterations. This is in stark contrast to the exact-parameterization scenario ($k=r$) where the convergence rate is $\exp (-Ω(T))$. Next, we study asymmetric setting where $M^* \in \mathbb{R}^{n_1 \times n_2}$ is the unknown matrix of rank $r \ll \min\{n_1,n_2\}$, and one uses an asymmetric parameterization $FG^\top$ to learn $M^*$ where $F \in \mathbb{R}^{n_1 \times k}$ and $G \in \mathbb{R}^{n_2 \times k}$. Building on prior work, we give a global exact convergence result of randomly initialized GD for the exact-parameterization case ($k=r$) with an $\exp (-Ω(T))$ rate. Furthermore, we give the first global exact convergence result for the over-parameterization case ($k>r$) with an $\exp(-Ω(α^2 T))$ rate where $α$ is the initialization scale. This linear convergence result in the over-parameterization case is especially significant because one can apply the asymmetric parameterization to the symmetric setting to speed up from $Ω(1/T^2)$ to linear convergence. On the other hand, we propose a novel method that only modifies one step of GD and obtains a convergence rate independent of $α$, recovering the rate in the exact-parameterization case.",True,True,"Xiong, Nuoya and Ding, Lijun and Du, Simon S",2023.0,,,,arXiv preprint arXiv:2310.01769
Global Convergence of Four-Layer Matrix Factorization under Random Initialization,2511.09925v1,tarmoun2021understanding,\cite{tarmoun2021understanding},Understanding the dynamics of gradient flow in overparameterized linear models,,,True,False,"Tarmoun, Salma and Franca, Guilherme and Haeffele, Benjamin D and Vidal, Rene",2021.0,,,,
Global Convergence of Four-Layer Matrix Factorization under Random Initialization,2511.09925v1,chizat2024infinite,\cite{chizat2024infinite},Infinite-width limit of deep linear neural networks,https://arxiv.org/abs/2211.16980v1,"This paper studies the infinite-width limit of deep linear neural networks initialized with random parameters. We obtain that, when the number of neurons diverges, the training dynamics converge (in a precise sense) to the dynamics obtained from a gradient descent on an infinitely wide deterministic linear neural network. Moreover, even if the weights remain random, we get their precise law along the training dynamics, and prove a quantitative convergence result of the linear predictor in terms of the number of neurons.
  We finally study the continuous-time limit obtained for infinitely wide linear neural networks and show that the linear predictors of the neural network converge at an exponential rate to the minimal $\ell_2$-norm minimizer of the risk.",True,True,"Chizat, L{\'e}na{\""\i}c and Colombo, Maria and Fern{\'a}ndez-Real, Xavier and Figalli, Alessio",2024.0,,,,Communications on Pure and Applied Mathematics
Understanding the role of depth in the neural tangent kernel for overparameterized neural networks,2511.07272v1,jacot2018neural,\cite{jacot2018neural},Neural tangent kernel: Convergence and generalization in neural networks,,,True,False,"Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment",2018.0,,,,Advances in neural information processing systems
Understanding the role of depth in the neural tangent kernel for overparameterized neural networks,2511.07272v1,arora2019exact,\cite{arora2019exact},On exact computation with an infinitely wide neural net,,,True,False,"Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong",2019.0,,,,Advances in neural information processing systems
Understanding the role of depth in the neural tangent kernel for overparameterized neural networks,2511.07272v1,liu2020linearity,\cite{liu2020linearity},On the linearity of large non-linear models: when and why the tangent kernel is constant,,,True,False,"Liu, Chaoyue and Zhu, Libin and Belkin, Misha",2020.0,,,,Advances in Neural Information Processing Systems
Understanding the role of depth in the neural tangent kernel for overparameterized neural networks,2511.07272v1,liu2022loss,\cite{liu2022loss},Loss landscapes and optimization in over-parameterized non-linear systems and neural networks,,,True,False,"Liu, Chaoyue and Zhu, Libin and Belkin, Mikhail",2022.0,,,,Applied and Computational Harmonic Analysis
Understanding the role of depth in the neural tangent kernel for overparameterized neural networks,2511.07272v1,belkin2021fit,\cite{belkin2021fit},Fit without fear: remarkable mathematical phenomena of deep learning through the prism of interpolation,https://arxiv.org/abs/2105.14368v1,"In the past decade the mathematical theory of machine learning has lagged far behind the triumphs of deep neural networks on practical challenges. However, the gap between theory and practice is gradually starting to close. In this paper I will attempt to assemble some pieces of the remarkable and still incomplete mathematical mosaic emerging from the efforts to understand the foundations of deep learning. The two key themes will be interpolation, and its sibling, over-parameterization. Interpolation corresponds to fitting data, even noisy data, exactly. Over-parameterization enables interpolation and provides flexibility to select a right interpolating model.
  As we will see, just as a physical prism separates colors mixed within a ray of light, the figurative prism of interpolation helps to disentangle generalization and optimization properties within the complex picture of modern Machine Learning. This article is written with belief and hope that clearer understanding of these issues brings us a step closer toward a general theory of deep learning and machine learning.",True,True,"Belkin, Mikhail",2021.0,,,,Acta Numerica
Understanding the role of depth in the neural tangent kernel for overparameterized neural networks,2511.07272v1,lee2020finite,\cite{lee2020finite},Finite Versus Infinite Neural Networks: an Empirical Study,https://arxiv.org/abs/2007.15801v2,"We perform a careful, thorough, and large scale empirical study of the correspondence between wide neural networks and kernel methods. By doing so, we resolve a variety of open questions related to the study of infinitely wide neural networks. Our experimental results include: kernel methods outperform fully-connected finite-width networks, but underperform convolutional finite width networks; neural network Gaussian process (NNGP) kernels frequently outperform neural tangent (NT) kernels; centered and ensembled finite networks have reduced posterior variance and behave more similarly to infinite networks; weight decay and the use of a large learning rate break the correspondence between finite and infinite networks; the NTK parameterization outperforms the standard parameterization for finite width networks; diagonal regularization of kernels acts similarly to early stopping; floating point precision limits kernel performance beyond a critical dataset size; regularized ZCA whitening improves accuracy; finite network performance depends non-monotonically on width in ways not captured by double descent phenomena; equivariance of CNNs is only beneficial for narrow networks far from the kernel regime. Our experiments additionally motivate an improved layer-wise scaling for weight decay which improves generalization in finite-width networks. Finally, we develop improved best practices for using NNGP and NT kernels for prediction, including a novel ensembling technique. Using these best practices we achieve state-of-the-art results on CIFAR-10 classification for kernels corresponding to each architecture class we consider.",True,True,"Lee, Jaehoon and Schoenholz, Samuel and Pennington, Jeffrey and Adlam, Ben and Xiao, Lechao and Novak, Roman and Sohl-Dickstein, Jascha",2020.0,,,,Advances in Neural Information Processing Systems
Understanding the role of depth in the neural tangent kernel for overparameterized neural networks,2511.07272v1,tsuchida2018invariance,\cite{tsuchida2018invariance},Invariance of Weight Distributions in Rectified MLPs,https://arxiv.org/abs/1711.09090v3,"An interesting approach to analyzing neural networks that has received renewed attention is to examine the equivalent kernel of the neural network. This is based on the fact that a fully connected feedforward network with one hidden layer, a certain weight distribution, an activation function, and an infinite number of neurons can be viewed as a mapping into a Hilbert space. We derive the equivalent kernels of MLPs with ReLU or Leaky ReLU activations for all rotationally-invariant weight distributions, generalizing a previous result that required Gaussian weight distributions. Additionally, the Central Limit Theorem is used to show that for certain activation functions, kernels corresponding to layers with weight distributions having $0$ mean and finite absolute third moment are asymptotically universal, and are well approximated by the kernel corresponding to layers with spherical Gaussian weights. In deep networks, as depth increases the equivalent kernel approaches a pathological fixed point, which can be used to argue why training randomly initialized networks can be difficult. Our results also have implications for weight initialization.",True,True,"Tsuchida, Russell and Roosta, Fred and Gallagher, Marcus",2018.0,,,,
Understanding the role of depth in the neural tangent kernel for overparameterized neural networks,2511.07272v1,bietti2020deep,\cite{bietti2020deep},Deep equals shallow for ReLU networks in kernel regimes,,,True,False,"Bietti, Alberto and Bach, Francis",2020.0,,,,arXiv preprint arXiv:2009.14397
Understanding the role of depth in the neural tangent kernel for overparameterized neural networks,2511.07272v1,nguyen2021tight,\cite{nguyen2021tight},Tight bounds on the smallest eigenvalue of the neural tangent kernel for deep ReLU networks,,,True,False,"Nguyen, Quynh and Mondelli, Marco and Montufar, Guido F",2021.0,,,,
Understanding the role of depth in the neural tangent kernel for overparameterized neural networks,2511.07272v1,arora2019fine,\cite{arora2019fine},Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks,,,True,False,"Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong",2019.0,,,,
Understanding the role of depth in the neural tangent kernel for overparameterized neural networks,2511.07272v1,murray2023characterizing,\cite{murray2023characterizing},Characterizing the spectrum of the NTK via a power series expansion,,,True,False,"Murray, Michael and Jin, Hui and Bowman, Benjamin and Montufar, Guido",2023.0,,,,
Understanding the role of depth in the neural tangent kernel for overparameterized neural networks,2511.07272v1,li2024eigenvalue,\cite{li2024eigenvalue},On the eigenvalue decay rates of a class of neural-network related kernel functions defined on general domains,,,True,False,"Li, Yicheng and Yu, Zixiong and Chen, Guhan and Lin, Qian",2024.0,,,,Journal of Machine Learning Research
Understanding the role of depth in the neural tangent kernel for overparameterized neural networks,2511.07272v1,lee2019wide,\cite{lee2019wide},Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent,https://arxiv.org/abs/1902.06720v4,"A longstanding goal in deep learning research has been to precisely characterize training and generalization. However, the often complex loss landscapes of neural networks have made a theory of learning dynamics elusive. In this work, we show that for wide neural networks the learning dynamics simplify considerably and that, in the infinite width limit, they are governed by a linear model obtained from the first-order Taylor expansion of the network around its initial parameters. Furthermore, mirroring the correspondence between wide Bayesian neural networks and Gaussian processes, gradient-based training of wide neural networks with a squared loss produces test set predictions drawn from a Gaussian process with a particular compositional kernel. While these theoretical results are only exact in the infinite width limit, we nevertheless find excellent empirical agreement between the predictions of the original network and those of the linearized version even for finite practically-sized networks. This agreement is robust across different architectures, optimization methods, and loss functions.",True,True,"Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey",2019.0,,,,Advances in neural information processing systems
Understanding the role of depth in the neural tangent kernel for overparameterized neural networks,2511.07272v1,allen2019convergence,\cite{allen2019convergence},A convergence theory for deep learning via over-parameterization,,,True,False,"Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao",2019.0,,,,
Understanding the role of depth in the neural tangent kernel for overparameterized neural networks,2511.07272v1,hanin2020finite,\cite{hanin2020finite},Finite depth and width corrections to the neural tangent kernel,,,True,False,"Hanin, Boris and Nica, Mihai",2020.0,,,,
Synapse: Adaptive Arbitration of Complementary Expertise in Time Series Foundational Models,2511.05460v1,box2015time,\cite{box2015time},Time Series Analysis: Forecasting and Control,,,True,False,"Box, George EP and Jenkins, Gwilym M and Reinsel, Gregory C and Ljung, Greta M",2015.0,,,,
Synapse: Adaptive Arbitration of Complementary Expertise in Time Series Foundational Models,2511.05460v1,hyndman2008forecasting,\cite{hyndman2008forecasting},Forecasting with Exponential Smoothing: The State Space Approach,,,True,False,"Hyndman, Rob J and Koehler, Anne B and Ord, J Keith and Snyder, Ralph D",2008.0,,,,
Synapse: Adaptive Arbitration of Complementary Expertise in Time Series Foundational Models,2511.05460v1,hochreiter1997long,\cite{hochreiter1997long},Associative Long Short-Term Memory,https://arxiv.org/abs/1602.03032v2,"We investigate a new method to augment recurrent neural networks with extra memory without increasing the number of network parameters. The system has an associative memory based on complex-valued vectors and is closely related to Holographic Reduced Representations and Long Short-Term Memory networks. Holographic Reduced Representations have limited capacity: as they store more information, each retrieval becomes noisier due to interference. Our system in contrast creates redundant copies of stored information, which enables retrieval with reduced noise. Experiments demonstrate faster learning on multiple memorization tasks.",True,True,"Hochreiter, Sepp and Schmidhuber, J{\""u}rgen",1997.0,,,,Neural computation
Synapse: Adaptive Arbitration of Complementary Expertise in Time Series Foundational Models,2511.05460v1,zeng2023are,\cite{zeng2023are},Are transformers effective for time series forecasting?,,,True,False,"Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang",2023.0,,,,arXiv preprint arXiv:2205.13504
Synapse: Adaptive Arbitration of Complementary Expertise in Time Series Foundational Models,2511.05460v1,nie2023time,\cite{nie2023time},A Time Series is Worth 64 Words: Long-term Forecasting with Transformers,https://arxiv.org/abs/2211.14730v2,"We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. Patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history. Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning performance, which outperforms supervised training on large datasets. Transferring of masked pre-trained representation on one dataset to others also produces SOTA forecasting accuracy. Code is available at: https://github.com/yuqinie98/PatchTST.",True,True,"Nie, Yuqi and Nguyen, Nam H and Sinthong, Phanwadee and Kalchbrenner, Jan",2023.0,,,,arXiv preprint arXiv:2211.14730
Synapse: Adaptive Arbitration of Complementary Expertise in Time Series Foundational Models,2511.05460v1,chen2023tsmixer,\cite{chen2023tsmixer},TSMixer: An All-MLP Architecture for Time Series Forecasting,,,True,False,"Chen, Si-An and Chen, Chun-Liang and Wang, Zsolt and Tung, Kuan-Ting and Ho, Ching-I and Lee, Chien-Chun and Wang, Yu-Chiang Frank",2023.0,,,,arXiv preprint arXiv:2303.06053
Synapse: Adaptive Arbitration of Complementary Expertise in Time Series Foundational Models,2511.05460v1,ansari2024chronos,\cite{ansari2024chronos},Chronos: Learning the language of time series,,,True,False,"Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Sundar and Arango, Sebastian Pineda and Kapoor, Shubham and others",2024.0,,,,arXiv preprint arXiv:2403.07815
Synapse: Adaptive Arbitration of Complementary Expertise in Time Series Foundational Models,2511.05460v1,garza2023timegpt,\cite{garza2023timegpt},TimeGPT-1,https://arxiv.org/abs/2310.03589v3,"In this paper, we introduce TimeGPT, the first foundation model for time series, capable of generating accurate predictions for diverse datasets not seen during training. We evaluate our pre-trained model against established statistical, machine learning, and deep learning methods, demonstrating that TimeGPT zero-shot inference excels in performance, efficiency, and simplicity. Our study provides compelling evidence that insights from other domains of artificial intelligence can be effectively applied to time series analysis. We conclude that large-scale time series models offer an exciting opportunity to democratize access to precise predictions and reduce uncertainty by leveraging the capabilities of contemporary advancements in deep learning.",True,True,"Garza, Azul and Challu, Cristian and Mergenthaler-Canseco, Max",2023.0,,,,arXiv preprint arXiv:2310.03589
Synapse: Adaptive Arbitration of Complementary Expertise in Time Series Foundational Models,2511.05460v1,das2024decoderonlyfoundationmodeltimeseries,\cite{das2024decoderonlyfoundationmodeltimeseries},A decoder-only foundation model for time-series forecasting,,,True,False,Abhimanyu Das and Weihao Kong and Rajat Sen and Yichen Zhou,2024.0,,https://arxiv.org/abs/2310.10688,,
Synapse: Adaptive Arbitration of Complementary Expertise in Time Series Foundational Models,2511.05460v1,graf2025flowstate,\cite{graf2025flowstate},FlowState: Sampling Rate Invariant Time Series Forecasting,https://arxiv.org/abs/2508.05287v2,"Foundation models (FMs) have transformed natural language processing, but their success has not yet translated to time series forecasting. Existing time series foundation models (TSFMs), often based on transformer variants, struggle with generalization across varying context and target lengths, lack adaptability to different sampling rates, and are computationally inefficient. We introduce FlowState, a novel TSFM architecture that addresses these challenges through two key innovations: a state space model (SSM) based encoder and a functional basis decoder. This design enables continuous-time modeling and dynamic time-scale adjustment, allowing FlowState to inherently generalize across all possible temporal resolutions, and dynamically adjust the forecasting horizons. In contrast to other state-of-the-art TSFMs, which require training data across all possible sampling rates to memorize patterns at each scale, FlowState inherently adapts its internal dynamics to the input scale, enabling smaller models, reduced data requirements, and improved efficiency. We further propose an efficient pretraining strategy that improves robustness and accelerates training. Despite being the smallest model, FlowState outperforms all other models and is state-of-the-art for the GIFT-ZS and the Chronos-ZS benchmarks. Ablation studies confirm the effectiveness of its components, and we demonstrate its unique ability to adapt online to varying input sampling rates.",True,True,"Graf, Lars and Ortner, Thomas and Wo{\'L}{\c{s}}niak, Stanis{\'L} and Pantazi, Angeliki and others",2025.0,,,,arXiv preprint arXiv:2508.05287
Synapse: Adaptive Arbitration of Complementary Expertise in Time Series Foundational Models,2511.05460v1,woo2024unifiedtraininguniversaltime,\cite{woo2024unifiedtraininguniversaltime},Unified Training of Universal Time Series Forecasting Transformers,https://arxiv.org/abs/2402.02592v2,"Deep learning for time series forecasting has traditionally operated within a one-model-per-dataset framework, limiting its potential to leverage the game-changing impact of large pre-trained models. The concept of universal forecasting, emerging from pre-training on a vast collection of time series datasets, envisions a single Large Time Series Model capable of addressing diverse downstream forecasting tasks. However, constructing such a model poses unique challenges specific to time series data: i) cross-frequency learning, ii) accommodating an arbitrary number of variates for multivariate time series, and iii) addressing the varying distributional properties inherent in large-scale data. To address these challenges, we present novel enhancements to the conventional time series Transformer architecture, resulting in our proposed Masked Encoder-based Universal Time Series Forecasting Transformer (Moirai). Trained on our newly introduced Large-scale Open Time Series Archive (LOTSA) featuring over 27B observations across nine domains, Moirai achieves competitive or superior performance as a zero-shot forecaster when compared to full-shot models. Code, data, and model weights can be found at https://github.com/SalesforceAIResearch/uni2ts.",True,True,Gerald Woo and Chenghao Liu and Akshat Kumar and Caiming Xiong and Silvio Savarese and Doyen Sahoo,2024.0,,https://arxiv.org/abs/2402.02592,,
SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories,2511.08136v1,bc,\cite{bc},Efficient training of artificial neural networks for autonomous navigation,,,True,False,"Pomerleau, Dean A",1991.0,,,,Neural computation
SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories,2511.08136v1,demodice_iclr_2022,\cite{demodice_iclr_2022},Demo{DICE}: Offline Imitation Learning with Supplementary Imperfect Demonstrations,,,True,False,Geon-Hyeong Kim and Seokin Seo and Jongmin Lee and Wonseok Jeon and HyeongJoo Hwang and Hongseok Yang and Kee-Eung Kim,2022.0,,https://openreview.net/forum?id=BrPdX1bDZkQ,,
SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories,2511.08136v1,safedice_neurips_2023,\cite{safedice_neurips_2023},Safe{DICE}: Offline Safe Imitation Learning with Non-Preferred Demonstrations,,,True,False,Youngsoo Jang and Geon-Hyeong Kim and Jongmin Lee and Sungryull Sohn and Byoungjip Kim and Honglak Lee and Moontae Lee,2023.0,,https://openreview.net/forum?id=toEGuA9Qfn,,
SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories,2511.08136v1,trex_icml_2019,\cite{trex_icml_2019},Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations,https://arxiv.org/abs/1904.06387v5,"A critical flaw of existing inverse reinforcement learning (IRL) methods is their inability to significantly outperform the demonstrator. This is because IRL typically seeks a reward function that makes the demonstrator appear near-optimal, rather than inferring the underlying intentions of the demonstrator that may have been poorly executed in practice. In this paper, we introduce a novel reward-learning-from-observation algorithm, Trajectory-ranked Reward EXtrapolation (T-REX), that extrapolates beyond a set of (approximately) ranked demonstrations in order to infer high-quality reward functions from a set of potentially poor demonstrations. When combined with deep reinforcement learning, T-REX outperforms state-of-the-art imitation learning and IRL methods on multiple Atari and MuJoCo benchmark tasks and achieves performance that is often more than twice the performance of the best demonstration. We also demonstrate that T-REX is robust to ranking noise and can accurately extrapolate intention by simply watching a learner noisily improve at a task over time.",True,True,"Brown, Daniel and Goo, Wonjoon and Nagarajan, Prabhat and Niekum, Scott",2019.0,,,,
SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories,2511.08136v1,b_pref_neurips_2021,\cite{b_pref_neurips_2021},B-Pref: Benchmarking Preference-Based Reinforcement Learning,https://arxiv.org/abs/2111.03026v1,"Reinforcement learning (RL) requires access to a reward function that incentivizes the right behavior, but these are notoriously hard to specify for complex tasks. Preference-based RL provides an alternative: learning policies using a teacher's preferences without pre-defined rewards, thus overcoming concerns associated with reward engineering. However, it is difficult to quantify the progress in preference-based RL due to the lack of a commonly adopted benchmark. In this paper, we introduce B-Pref: a benchmark specially designed for preference-based RL. A key challenge with such a benchmark is providing the ability to evaluate candidate algorithms quickly, which makes relying on real human input for evaluation prohibitive. At the same time, simulating human input as giving perfect preferences for the ground truth reward function is unrealistic. B-Pref alleviates this by simulating teachers with a wide array of irrationalities, and proposes metrics not solely for performance but also for robustness to these potential irrationalities. We showcase the utility of B-Pref by using it to analyze algorithmic design choices, such as selecting informative queries, for state-of-the-art preference-based RL algorithms. We hope that B-Pref can serve as a common starting point to study preference-based RL more systematically. Source code is available at https://github.com/rll-research/B-Pref.",True,True,Kimin Lee and Laura Smith and Anca Dragan and Pieter Abbeel,2021.0,,https://openreview.net/forum?id=ps95-mkHF_,,
SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories,2511.08136v1,pebble_icml_2021,\cite{pebble_icml_2021},PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training,https://arxiv.org/abs/2106.05091v1,"Conveying complex objectives to reinforcement learning (RL) agents can often be difficult, involving meticulous design of reward functions that are sufficiently informative yet easy enough to provide. Human-in-the-loop RL methods allow practitioners to instead interactively teach agents through tailored feedback; however, such approaches have been challenging to scale since human feedback is very expensive. In this work, we aim to make this process more sample- and feedback-efficient. We present an off-policy, interactive RL algorithm that capitalizes on the strengths of both feedback and off-policy learning. Specifically, we learn a reward model by actively querying a teacher's preferences between two clips of behavior and use it to train an agent. To enable off-policy learning, we relabel all the agent's past experience when its reward model changes. We additionally show that pre-training our agents with unsupervised exploration substantially increases the mileage of its queries. We demonstrate that our approach is capable of learning tasks of higher complexity than previously considered by human-in-the-loop methods, including a variety of locomotion and robotic manipulation skills. We also show that our method is able to utilize real-time human feedback to effectively prevent reward exploitation and learn new behaviors that are difficult to specify with standard reward functions.",True,True,Kimin Lee and Laura M. Smith and P. Abbeel,2021.0,,https://api.semanticscholar.org/CorpusID:235377145,,
SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories,2511.08136v1,offline_pref_tmlr_2023,\cite{offline_pref_tmlr_2023},Benchmarks and Algorithms for Offline Preference-Based Reward Learning,,,True,False,Daniel Shin and Anca Dragan and Daniel S. Brown,2023.0,,https://openreview.net/forum?id=TGuXXlbKsn,,Transactions on Machine Learning Research
SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories,2511.08136v1,ppo,\cite{ppo},Proximal Policy Optimization Algorithms,https://arxiv.org/abs/1707.06347v2,"We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a ""surrogate"" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.",True,True,"Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg",2017.0,,,,arXiv preprint arXiv:1707.06347
SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories,2511.08136v1,sac,\cite{sac},Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor,,,True,False,"Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey",2018.0,,,,
SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories,2511.08136v1,cql_neurips_2020,\cite{cql_neurips_2020},Conservative q-learning for offline reinforcement learning,,,True,False,"Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey",2020.0,,,,Advances in Neural Information Processing Systems
SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories,2511.08136v1,neorl_neurips_2022,\cite{neorl_neurips_2022},NeoRL: A Near Real-World Benchmark for Offline Reinforcement Learning,https://arxiv.org/abs/2102.00714v2,"Offline reinforcement learning (RL) aims at learning a good policy from a batch of collected data, without extra interactions with the environment during training. However, current offline RL benchmarks commonly have a large reality gap, because they involve large datasets collected by highly exploratory policies, and the trained policy is directly evaluated in the environment. In real-world situations, running a highly exploratory policy is prohibited to ensure system safety, the data is commonly very limited, and a trained policy should be well validated before deployment. In this paper, we present a near real-world offline RL benchmark, named NeoRL, which contains datasets from various domains with controlled sizes, and extra test datasets for policy validation. We evaluate existing offline RL algorithms on NeoRL and argue that the performance of a policy should also be compared with the deterministic version of the behavior policy, instead of the dataset reward. The empirical results demonstrate that the tested offline RL algorithms become less competitive to the deterministic policy on many datasets, and the offline policy evaluation hardly helps. The NeoRL suit can be found at http://polixir.ai/research/neorl. We hope this work will shed some light on future research and draw more attention when deploying RL in real-world systems.",True,True,"Qin, Rong-Jun and Zhang, Xingyuan and Gao, Songyi and Chen, Xiong-Hui and Li, Zewen and Zhang, Weinan and Yu, Yang",2022.0,,,,Advances in Neural Information Processing Systems
SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories,2511.08136v1,drex_corl_2020,\cite{drex_corl_2020},Better-than-Demonstrator Imitation Learning via Automatically-Ranked Demonstrations,https://arxiv.org/abs/1907.03976v3,"The performance of imitation learning is typically upper-bounded by the performance of the demonstrator. While recent empirical results demonstrate that ranked demonstrations allow for better-than-demonstrator performance, preferences over demonstrations may be difficult to obtain, and little is known theoretically about when such methods can be expected to successfully extrapolate beyond the performance of the demonstrator. To address these issues, we first contribute a sufficient condition for better-than-demonstrator imitation learning and provide theoretical results showing why preferences over demonstrations can better reduce reward function ambiguity when performing inverse reinforcement learning. Building on this theory, we introduce Disturbance-based Reward Extrapolation (D-REX), a ranking-based imitation learning method that injects noise into a policy learned through behavioral cloning to automatically generate ranked demonstrations. These ranked demonstrations are used to efficiently learn a reward function that can then be optimized using reinforcement learning. We empirically validate our approach on simulated robot and Atari imitation learning benchmarks and show that D-REX outperforms standard imitation learning approaches and can significantly surpass the performance of the demonstrator. D-REX is the first imitation learning approach to achieve significant extrapolation beyond the demonstrator's performance without additional side-information or supervision, such as rewards or human preferences. By generating rankings automatically, we show that preference-based inverse reinforcement learning can be applied in traditional imitation learning settings where only unlabeled demonstrations are available.",True,True,"Brown, Daniel S and Goo, Wonjoon and Niekum, Scott",2020.0,,,,
SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories,2511.08136v1,ssrr_corl_2020,\cite{ssrr_corl_2020},Learning from Suboptimal Demonstration via Self-Supervised Reward Regression,https://arxiv.org/abs/2010.11723v3,"Learning from Demonstration (LfD) seeks to democratize robotics by enabling non-roboticist end-users to teach robots to perform a task by providing a human demonstration. However, modern LfD techniques, e.g. inverse reinforcement learning (IRL), assume users provide at least stochastically optimal demonstrations. This assumption fails to hold in most real-world scenarios. Recent attempts to learn from sub-optimal demonstration leverage pairwise rankings and following the Luce-Shepard rule. However, we show these approaches make incorrect assumptions and thus suffer from brittle, degraded performance. We overcome these limitations in developing a novel approach that bootstraps off suboptimal demonstrations to synthesize optimality-parameterized data to train an idealized reward function. We empirically validate we learn an idealized reward function with ~0.95 correlation with ground-truth reward versus ~0.75 for prior work. We can then train policies achieving ~200% improvement over the suboptimal demonstration and ~90% improvement over prior work. We present a physical demonstration of teaching a robot a topspin strike in table tennis that achieves 32% faster returns and 40% more topspin than user demonstration.",True,True,"Chen, Letian and Paleja, Rohan and Gombolay, Matthew",2020.0,,,,
SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories,2511.08136v1,dwbc_icml_2022,\cite{dwbc_icml_2022},Discriminator-Weighted Offline Imitation Learning from Suboptimal Demonstrations,https://arxiv.org/abs/2207.10050v1,"We study the problem of offline Imitation Learning (IL) where an agent aims to learn an optimal expert behavior policy without additional online environment interactions. Instead, the agent is provided with a supplementary offline dataset from suboptimal behaviors. Prior works that address this problem either require that expert data occupies the majority proportion of the offline dataset, or need to learn a reward function and perform offline reinforcement learning (RL) afterwards. In this paper, we aim to address the problem without additional steps of reward learning and offline RL training for the case when demonstrations contain a large proportion of suboptimal data. Built upon behavioral cloning (BC), we introduce an additional discriminator to distinguish expert and non-expert data. We propose a cooperation framework to boost the learning of both tasks, Based on this framework, we design a new IL algorithm, where the outputs of discriminator serve as the weights of the BC loss. Experimental results show that our proposed algorithm achieves higher returns and faster training speed compared to baseline algorithms.",True,True,"Xu, Haoran and Zhan, Xianyuan and Yin, Honglei and Qin, Huiling",2022.0,,,,
SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories,2511.08136v1,pu_learning_1_2008,\cite{pu_learning_1_2008},Learning classifiers from only positive and unlabeled data,,,True,False,"Elkan, Charles and Noto, Keith",2008.0,,https://doi.org/10.1145/1401890.1401920,10.1145/1401890.1401920,
SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories,2511.08136v1,pu_learning_2_2021,\cite{pu_learning_2_2021},Combating False Negatives in Adversarial Imitation Learning,https://arxiv.org/abs/2002.00412v1,"In adversarial imitation learning, a discriminator is trained to differentiate agent episodes from expert demonstrations representing the desired behavior. However, as the trained policy learns to be more successful, the negative examples (the ones produced by the agent) become increasingly similar to expert ones. Despite the fact that the task is successfully accomplished in some of the agent's trajectories, the discriminator is trained to output low values for them. We hypothesize that this inconsistent training signal for the discriminator can impede its learning, and consequently leads to worse overall performance of the agent. We show experimental evidence for this hypothesis and that the 'False Negatives' (i.e. successful agent episodes) significantly hinder adversarial imitation learning, which is the first contribution of this paper. Then, we propose a method to alleviate the impact of false negatives and test it on the BabyAI environment. This method consistently improves sample efficiency over the baselines by at least an order of magnitude.",True,True,"{\.Z}o{\l}na, Konrad and Saharia, Chitwan and Boussioux, Leonard and Hui, David Yu-Tung and Chevalier-Boisvert, Maxime and Bahdanau, Dzmitry and Bengio, Yoshua",2021.0,,,,
Approximating Heavy-Tailed Distributions with a Mixture of Bernstein Phase-Type and Hyperexponential Models,2510.26524v1,[NEUT75],\cite{[NEUT75]},Probability distributions of phase type,,,True,False,M. Neuts,1975.0,,,,
Approximating Heavy-Tailed Distributions with a Mixture of Bernstein Phase-Type and Hyperexponential Models,2510.26524v1,johnson1989matching,\cite{johnson1989matching},Matching moments to phase distributions: Mixtures of {E}rlang distributions of common order,,,True,False,"Johnson, Mark A. and Taaffe, Michael R.",1989.0,,,,Stochastic Models
Approximating Heavy-Tailed Distributions with a Mixture of Bernstein Phase-Type and Hyperexponential Models,2510.26524v1,bobbio2005matching,\cite{bobbio2005matching},Matching three moments with minimal acyclic phase type distributions,,,True,False,"Bobbio, Andrea and Horv{\'a}th, Andr{\'a}s and Telek, Mikl{\'o}s",2005.0,,,,Stochastic Models
Approximating Heavy-Tailed Distributions with a Mixture of Bernstein Phase-Type and Hyperexponential Models,2510.26524v1,horvath2007matching,\cite{horvath2007matching},Matching more than three moments with acyclic phase type distributions,,,True,False,"Horv{\'a}th, Andr{\'a}s and Telek, Mikl{\'o}s",2007.0,,,10.1080/15326340701300712,Stochastic Models
Approximating Heavy-Tailed Distributions with a Mixture of Bernstein Phase-Type and Hyperexponential Models,2510.26524v1,BuKr09,\cite{BuKr09},A Heuristic Approach for Fitting MAPs to Moments and Joint Moments,,,True,False,"Buchholz, Peter and Kriege, Jan.",2009.0,,,10.1109/QEST.2009.36,
Approximating Heavy-Tailed Distributions with a Mixture of Bernstein Phase-Type and Hyperexponential Models,2510.26524v1,sherzer2025unconstrainedoptimizationapproachmoment,\cite{sherzer2025unconstrainedoptimizationapproachmoment},An Unconstrained Optimization Approach to Moment Fitting with Phase Type Distributions,https://arxiv.org/abs/2505.20379v1,"Phase type (PH) distributions are widely used in modeling and simulation due to their generality and analytical properties. In such settings, it is often necessary to construct a PH distribution that aligns with real-world data by matching a set of prescribed moments. Existing approaches provide either exact closed-form solutions or iterative procedures that may yield exact or approximate results. However, these methods are limited to matching a small number of moments using PH distributions with a small number of phases, or are restricted to narrow subclasses within the PH family. We address the problem of approximately fitting a larger set of given moments using potentially large PH distributions. We introduce an optimization methodology that relies on a re-parametrization of the Markovian representation, formulated in a space that enables unconstrained optimization of the moment-matching objective. This reformulation allows us to scale to significantly larger PH distributions and capture higher moments. Results on a large and diverse set of moment targets show that the proposed method is, in the vast majority of cases, capable of fitting as many as 20 moments to PH distributions with as many as 100 phases, with small relative errors on the order of under 0.5% from each target. We further demonstrate an application of the optimization framework where we search for a PH distribution that conforms not only to a given set of moments but also to a given shape. Finally, we illustrate the practical utility of this approach through a queueing application, presenting a case study that examines the influence of the i^{th} moment of the inter-arrival and service time distributions on the steady-state probabilities of the GI/GI/1 queue length.",True,True,Eliran Sherzer and Yehezkel Resheff and Miklos Telek,2025.0,,,,
Approximating Heavy-Tailed Distributions with a Mixture of Bernstein Phase-Type and Hyperexponential Models,2510.26524v1,horvath2024phase,\cite{horvath2024phase},Phase Type Distributions: Theory and Application,,,True,False,"Horv{\'a}th, Andr{\'a}s and Telek, Mikl{\'o}s",2024.0,,,,
Approximating Heavy-Tailed Distributions with a Mixture of Bernstein Phase-Type and Hyperexponential Models,2510.26524v1,asmussen1996fitting,\cite{asmussen1996fitting},Fitting phase-type distributions via the {EM} algorithm,,,True,False,"Asmussen, S{\o}ren and Nerman, Olle and Olsson, Marita",1996.0,,,,Scandinavian Journal of Statistics
Approximating Heavy-Tailed Distributions with a Mixture of Bernstein Phase-Type and Hyperexponential Models,2510.26524v1,okamura2011refined,\cite{okamura2011refined},A refined {EM} algorithm for {PH} distributions,,,True,False,"Okamura, Hiroyuki and Dohi, Tadashi and Trivedi, Kishor S.",2011.0,,,,Performance Evaluation
Approximating Heavy-Tailed Distributions with a Mixture of Bernstein Phase-Type and Hyperexponential Models,2510.26524v1,[BOBB94a],\cite{[BOBB94a]},"A benchmark for {PH} estimation algorithms: results for
{A}cyclic-{PH}",,,True,False,A. Bobbio and M. Telek,1994.0,,,,Stochastic Models
Approximating Heavy-Tailed Distributions with a Mixture of Bernstein Phase-Type and Hyperexponential Models,2510.26524v1,[FELD98a],\cite{[FELD98a]},"Fitting mixtures of exponentials to long-tail distributions
to analyze network performance models",,,True,False,A. Feldman and W. Whitt,1998.0,,,,Performance Evaluation
Approximating Heavy-Tailed Distributions with a Mixture of Bernstein Phase-Type and Hyperexponential Models,2510.26524v1,HoTe00,\cite{HoTe00},Approximating heavy tailed behavior with Phase-type distributions,,,True,False,A. Horv\'ath AND M. Telek,,June,,,
Approximating Heavy-Tailed Distributions with a Mixture of Bernstein Phase-Type and Hyperexponential Models,2510.26524v1,Haddad1997,\cite{Haddad1997},Efficient Handling of Phase-Type Distributions in Generalized Stochastic Petri Nets,,,True,False,Serge Haddad and Patrice Moreaux and Giovanni Chiola,1997.0,,,10.1007/3-540-63139-9\_36,
Approximating Heavy-Tailed Distributions with a Mixture of Bernstein Phase-Type and Hyperexponential Models,2510.26524v1,stewart1994markov,\cite{stewart1994markov},Introduction to the Numerical Solution of Markov Chains,,,True,False,"Stewart, William J.",1994.0,,,,
Approximating Heavy-Tailed Distributions with a Mixture of Bernstein Phase-Type and Hyperexponential Models,2510.26524v1,neuts1981matrix,\cite{neuts1981matrix},Matrix-Geometric Solutions in Stochastic Models: An Algorithmic Approach,,,True,False,"Neuts, Marcel F.",1981.0,,,,
Approximating Heavy-Tailed Distributions with a Mixture of Bernstein Phase-Type and Hyperexponential Models,2510.26524v1,latouche1999introduction,\cite{latouche1999introduction},Introduction to Matrix Analytic Methods in Stochastic Modeling,,,True,False,"Latouche, Guy and Ramaswami, Vaidyanathan",1999.0,,,,
Approximating Heavy-Tailed Distributions with a Mixture of Bernstein Phase-Type and Hyperexponential Models,2510.26524v1,Cu85,\cite{Cu85},ESP - {A} Package for the Evaluation of Stochastic {P}etri Nets with Phase-Type Distributed Transition Times,,,True,False,"Cumani, Aldo",1985.0,,,,
Approximating Heavy-Tailed Distributions with a Mixture of Bernstein Phase-Type and Hyperexponential Models,2510.26524v1,Horvath2002PhFit,\cite{Horvath2002PhFit},PhFit: A General Phase-Type Fitting Tool,,,True,False,András Horváth and Miklós Telek,2002.0,,,10.1007/3-540-46029-2_5,
Approximating Heavy-Tailed Distributions with a Mixture of Bernstein Phase-Type and Hyperexponential Models,2510.26524v1,HyperStarTool,\cite{HyperStarTool},HyperStar: A Tool for Fitting Hyper-Erlang Distributions,,,True,False,,,,,,
Approximating Heavy-Tailed Distributions with a Mixture of Bernstein Phase-Type and Hyperexponential Models,2510.26524v1,Horvath2017BuTools,\cite{Horvath2017BuTools},BuTools 2: A Rich Toolbox for Markovian Performance Evaluation,,,True,False,Gábor Horváth and Miklós Telek,2017.0,,,10.4108/eai.25-10-2016.2266400,
Distributionally Robust Online Markov Game with Linear Function Approximation,2511.07831v1,yang2019linear_q_learning,\cite{yang2019linear_q_learning},Sample-Optimal Parametric Q-Learning Using Linearly Additive Features,https://arxiv.org/abs/1902.04779v2,"Consider a Markov decision process (MDP) that admits a set of state-action features, which can linearly express the process's probabilistic transition model. We propose a parametric Q-learning algorithm that finds an approximate-optimal policy using a sample size proportional to the feature dimension $K$ and invariant with respect to the size of the state space. To further improve its sample efficiency, we exploit the monotonicity property and intrinsic noise structure of the Bellman operator, provided the existence of anchor state-actions that imply implicit non-negativity in the feature space. We augment the algorithm using techniques of variance reduction, monotonicity preservation, and confidence bounds. It is proved to find a policy which is $ε$-optimal from any initial state with high probability using $\widetilde{O}(K/ε^2(1-γ)^3)$ sample transitions for arbitrarily large-scale MDP with a discount factor $γ\in(0,1)$. A matching information-theoretical lower bound is proved, confirming the sample optimality of the proposed method with respect to all parameters (up to polylog factors).",True,True,"Yang, Lin and Wang, Mengdi",2019.0,,,,
Distributionally Robust Online Markov Game with Linear Function Approximation,2511.07831v1,yang2020_linear_bandit,\cite{yang2020_linear_bandit},"Reinforcement Learning in Feature Space: Matrix Bandit, Kernels, and Regret Bound",https://arxiv.org/abs/1905.10389v2,"Exploration in reinforcement learning (RL) suffers from the curse of dimensionality when the state-action space is large. A common practice is to parameterize the high-dimensional value and policy functions using given features. However existing methods either have no theoretical guarantee or suffer a regret that is exponential in the planning horizon $H$. In this paper, we propose an online RL algorithm, namely the MatrixRL, that leverages ideas from linear bandit to learn a low-dimensional representation of the probability transition model while carefully balancing the exploitation-exploration tradeoff. We show that MatrixRL achieves a regret bound ${O}\big(H^2d\log T\sqrt{T}\big)$ where $d$ is the number of features. MatrixRL has an equivalent kernelized version, which is able to work with an arbitrary kernel Hilbert space without using explicit features. In this case, the kernelized MatrixRL satisfies a regret bound ${O}\big(H^2\widetilde{d}\log T\sqrt{T}\big)$, where $\widetilde{d}$ is the effective dimension of the kernel space. To our best knowledge, for RL using features or kernels, our results are the first regret bounds that are near-optimal in time $T$ and dimension $d$ (or $\widetilde{d}$) and polynomial in the planning horizon $H$.",True,True,"Yang, Lin and Wang, Mengdi",2020.0,,,,
Distributionally Robust Online Markov Game with Linear Function Approximation,2511.07831v1,zanette2020online_linear,\cite{zanette2020online_linear},Frequentist regret bounds for randomized least-squares value iteration,,,True,False,"Zanette, Andrea and Brandfonbrener, David and Brunskill, Emma and Pirotta, Matteo and Lazaric, Alessandro",2020.0,,,,
Distributionally Robust Online Markov Game with Linear Function Approximation,2511.07831v1,jin2020online_linear,\cite{jin2020online_linear},Provably efficient reinforcement learning with linear function approximation,,,True,False,"Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I",2020.0,,,,
Distributionally Robust Online Markov Game with Linear Function Approximation,2511.07831v1,he2021online_linear_pre,\cite{he2021online_linear_pre},Logarithmic regret for reinforcement learning with linear function approximation,,,True,False,"He, Jiafan and Zhou, Dongruo and Gu, Quanquan",2021.0,,,,
Distributionally Robust Online Markov Game with Linear Function Approximation,2511.07831v1,he2023online_linear_minimax,\cite{he2023online_linear_minimax},Nearly minimax optimal reinforcement learning for linear markov decision processes,,,True,False,"He, Jiafan and Zhao, Heyang and Zhou, Dongruo and Gu, Quanquan",2023.0,,,,
Distributionally Robust Online Markov Game with Linear Function Approximation,2511.07831v1,zhou2021nearly_mixture,\cite{zhou2021nearly_mixture},Nearly minimax optimal reinforcement learning for linear mixture markov decision processes,,,True,False,"Zhou, Dongruo and Gu, Quanquan and Szepesvari, Csaba",2021.0,,,,
Distributionally Robust Online Markov Game with Linear Function Approximation,2511.07831v1,liu2024DRL_online_linear,\cite{liu2024DRL_online_linear},Distributionally robust off-dynamics reinforcement learning: Provable efficiency with linear function approximation,,,True,False,"Liu, Zhishuai and Xu, Pan",2024.0,,,,
Distributionally Robust Online Markov Game with Linear Function Approximation,2511.07831v1,liu2024upper,\cite{liu2024upper},Upper and Lower Bounds for Distributionally Robust Off-Dynamics Reinforcement Learning,,,True,False,"Liu, Zhishuai and Wang, Weixin and Xu, Pan",2024.0,,,,arXiv preprint arXiv:2409.20521
Distributionally Robust Online Markov Game with Linear Function Approximation,2511.07831v1,liu2022distributionally_q_learning,\cite{liu2022distributionally_q_learning},Distributionally Robust Learning,https://arxiv.org/abs/2108.08993v1,"This monograph develops a comprehensive statistical learning framework that is robust to (distributional) perturbations in the data using Distributionally Robust Optimization (DRO) under the Wasserstein metric. Beginning with fundamental properties of the Wasserstein metric and the DRO formulation, we explore duality to arrive at tractable formulations and develop finite-sample, as well as asymptotic, performance guarantees. We consider a series of learning problems, including (i) distributionally robust linear regression; (ii) distributionally robust regression with group structure in the predictors; (iii) distributionally robust multi-output regression and multiclass classification, (iv) optimal decision making that combines distributionally robust regression with nearest-neighbor estimation; (v) distributionally robust semi-supervised learning, and (vi) distributionally robust reinforcement learning. A tractable DRO relaxation for each problem is being derived, establishing a connection between robustness and regularization, and obtaining bounds on the prediction and estimation errors of the solution. Beyond theory, we include numerical experiments and case studies using synthetic and real data. The real data experiments are all associated with various health informatics problems, an application area which provided the initial impetus for this work.",True,True,"Liu, Zijian and Bai, Qinxun and Blanchet, Jose and Dong, Perry and Xu, Wei and Zhou, Zhengqing and Zhou, Zhengyuan",2022.0,,,,
Distributionally Robust Online Markov Game with Linear Function Approximation,2511.07831v1,clavier2023Lp-Bal_Planning,\cite{clavier2023Lp-Bal_Planning},Towards Minimax Optimality of Model-based Robust Reinforcement Learning,https://arxiv.org/abs/2302.05372v3,"We study the sample complexity of obtaining an $ε$-optimal policy in \emph{Robust} discounted Markov Decision Processes (RMDPs), given only access to a generative model of the nominal kernel. This problem is widely studied in the non-robust case, and it is known that any planning approach applied to an empirical MDP estimated with $\tilde{\mathcal{O}}(\frac{H^3 \mid S \mid\mid A \mid}{ε^2})$ samples provides an $ε$-optimal policy, which is minimax optimal. Results in the robust case are much more scarce. For $sa$- (resp $s$-)rectangular uncertainty sets, the best known sample complexity is $\tilde{\mathcal{O}}(\frac{H^4 \mid S \mid^2\mid A \mid}{ε^2})$ (resp. $\tilde{\mathcal{O}}(\frac{H^4 \mid S \mid^2\mid A \mid^2}{ε^2})$), for specific algorithms and when the uncertainty set is based on the total variation (TV), the KL or the Chi-square divergences. In this paper, we consider uncertainty sets defined with an $L_p$-ball (recovering the TV case), and study the sample complexity of \emph{any} planning algorithm (with high accuracy guarantee on the solution) applied to an empirical RMDP estimated using the generative model. In the general case, we prove a sample complexity of $\tilde{\mathcal{O}}(\frac{H^4 \mid S \mid\mid A \mid}{ε^2})$ for both the $sa$- and $s$-rectangular cases (improvements of $\mid S \mid$ and $\mid S \mid\mid A \mid$ respectively). When the size of the uncertainty is small enough, we improve the sample complexity to $\tilde{\mathcal{O}}(\frac{H^3 \mid S \mid\mid A \mid }{ε^2})$, recovering the lower-bound for the non-robust case for the first time and a robust lower-bound when the size of the uncertainty is small enough.",True,True,"Clavier, Pierre and Pennec, Erwan Le and Geist, Matthieu",2023.0,,,,arXiv preprint arXiv:2302.05372
Distributionally Robust Online Markov Game with Linear Function Approximation,2511.07831v1,shi2024distributionally_offline,\cite{shi2024distributionally_offline},Distributionally robust model-based offline reinforcement learning with near-optimal sample complexity,,,True,False,"Shi, Laixi and Chi, Yuejie",2024.0,,,,Journal of Machine Learning Research
Distributionally Robust Online Markov Game with Linear Function Approximation,2511.07831v1,Shi2023_Curious_prize_generative,\cite{Shi2023_Curious_prize_generative},The Curious Price of Distributional Robustness in Reinforcement Learning with a Generative Model,,,True,False,"Shi, Laixi and Li, Gen and Wei, Yuting and Chen, Yuxin and Geist, Matthieu and Chi, Yuejie",2023.0,,,,
Distributionally Robust Online Markov Game with Linear Function Approximation,2511.07831v1,wang2023finite_dr_q_learning,\cite{wang2023finite_dr_q_learning},A finite sample complexity bound for distributionally robust q-learning,,,True,False,"Wang, Shengbo and Si, Nian and Blanchet, Jose and Zhou, Zhengyuan",2023.0,,,,
Distributionally Robust Online Markov Game with Linear Function Approximation,2511.07831v1,lu2024Dr_Interactive_Data_Collection,\cite{lu2024Dr_Interactive_Data_Collection},Distributionally Robust Reinforcement Learning with Interactive Data Collection: Fundamental Hardness and Near-Optimal Algorithm,,,True,False,Miao Lu and Han Zhong and Tong Zhang and Jose Blanchet,2024.0,,https://arxiv.org/abs/2404.03578,,
Distributionally Robust Online Markov Game with Linear Function Approximation,2511.07831v1,kardecs2011discounted,\cite{kardecs2011discounted},Discounted robust stochastic games and an application to queueing control,,,True,False,"Karde{\c{s}}, Erim and Ord{\'o}{\~n}ez, Fernando and Hall, Randolph W",2011.0,,,,Operations research
Distributionally Robust Online Markov Game with Linear Function Approximation,2511.07831v1,zhang2020robust_marl,\cite{zhang2020robust_marl},Robust multi-agent reinforcement learning with model uncertainty,,,True,False,"Zhang, Kaiqing and Sun, Tao and Tao, Yunzhe and Genc, Sahika and Mallya, Sunil and Basar, Tamer",2020.0,,,,Advances in neural information processing systems
Distributionally Robust Online Markov Game with Linear Function Approximation,2511.07831v1,blanchet2023double_marl,\cite{blanchet2023double_marl},Double pessimism is provably efficient for distributionally robust offline reinforcement learning: Generic algorithm and robust partial coverage,,,True,False,"Blanchet, Jose and Lu, Miao and Zhang, Tong and Zhong, Han",2023.0,,,,Advances in Neural Information Processing Systems
Distributionally Robust Online Markov Game with Linear Function Approximation,2511.07831v1,shi2024breaking,\cite{shi2024breaking},Breaking the Curse of Multiagency in Robust Multi-Agent Reinforcement Learning,,,True,False,"Shi, Laixi and Gai, Jingchu and Mazumdar, Eric and Chi, Yuejie and Wierman, Adam",2024.0,,,,arXiv preprint arXiv:2409.20067
Distributionally Robust Online Markov Game with Linear Function Approximation,2511.07831v1,shi2024multi-generative-joint,\cite{shi2024multi-generative-joint},Sample-Efficient Robust Multi-Agent Reinforcement Learning in the Face of Environmental Uncertainty,https://arxiv.org/abs/2404.18909v3,"To overcome the sim-to-real gap in reinforcement learning (RL), learned policies must maintain robustness against environmental uncertainties. While robust RL has been widely studied in single-agent regimes, in multi-agent environments, the problem remains understudied -- despite the fact that the problems posed by environmental uncertainties are often exacerbated by strategic interactions. This work focuses on learning in distributionally robust Markov games (RMGs), a robust variant of standard Markov games, wherein each agent aims to learn a policy that maximizes its own worst-case performance when the deployed environment deviates within its own prescribed uncertainty set. This results in a set of robust equilibrium strategies for all agents that align with classic notions of game-theoretic equilibria. Assuming a non-adaptive sampling mechanism from a generative model, we propose a sample-efficient model-based algorithm (DRNVI) with finite-sample complexity guarantees for learning robust variants of various notions of game-theoretic equilibria. We also establish an information-theoretic lower bound for solving RMGs, which confirms the near-optimal sample complexity of DRNVI with respect to problem-dependent factors such as the size of the state space, the target accuracy, and the horizon length.",True,True,"Shi, Laixi and Mazumdar, Eric and Chi, Yuejie and Wierman, Adam",2024.0,,,,arXiv preprint arXiv:2404.18909
Distributionally Robust Online Markov Game with Linear Function Approximation,2511.07831v1,jiao2024minimax_marl,\cite{jiao2024minimax_marl},Minimax-Optimal Multi-Agent Robust Reinforcement Learning,https://arxiv.org/abs/2412.19873v1,"Multi-agent robust reinforcement learning, also known as multi-player robust Markov games (RMGs), is a crucial framework for modeling competitive interactions under environmental uncertainties, with wide applications in multi-agent systems. However, existing results on sample complexity in RMGs suffer from at least one of three obstacles: restrictive range of uncertainty level or accuracy, the curse of multiple agents, and the barrier of long horizons, all of which cause existing results to significantly exceed the information-theoretic lower bound. To close this gap, we extend the Q-FTRL algorithm \citep{li2022minimax} to the RMGs in finite-horizon setting, assuming access to a generative model. We prove that the proposed algorithm achieves an $\varepsilon$-robust coarse correlated equilibrium (CCE) with a sample complexity (up to log factors) of $\widetilde{O}\left(H^3S\sum_{i=1}^mA_i\min\left\{H,1/R\right\}/\varepsilon^2\right)$, where $S$ denotes the number of states, $A_i$ is the number of actions of the $i$-th agent, $H$ is the finite horizon length, and $R$ is uncertainty level. We also show that this sample compelxity is minimax optimal by combining an information-theoretic lower bound. Additionally, in the special case of two-player zero-sum RMGs, the algorithm achieves an $\varepsilon$-robust Nash equilibrium (NE) with the same sample complexity.",True,True,"Jiao, Yuchen and Li, Gen",2024.0,,,,arXiv preprint arXiv:2412.19873
Distributionally Robust Online Markov Game with Linear Function Approximation,2511.07831v1,ma2023decentralized_v_robust,\cite{ma2023decentralized_v_robust},Decentralized robust v-learning for solving markov games with model uncertainty,,,True,False,"Ma, Shaocong and Chen, Ziyi and Zou, Shaofeng and Zhou, Yi",2023.0,,,,Journal of Machine Learning Research
Distributionally Robust Online Markov Game with Linear Function Approximation,2511.07831v1,xie2020two-player-simultaneous-move,\cite{xie2020two-player-simultaneous-move},Learning Zero-Sum Simultaneous-Move Markov Games Using Function Approximation and Correlated Equilibrium,https://arxiv.org/abs/2002.07066v3,"We develop provably efficient reinforcement learning algorithms for two-player zero-sum finite-horizon Markov games with simultaneous moves. To incorporate function approximation, we consider a family of Markov games where the reward function and transition kernel possess a linear structure. Both the offline and online settings of the problems are considered. In the offline setting, we control both players and aim to find the Nash Equilibrium by minimizing the duality gap. In the online setting, we control a single player playing against an arbitrary opponent and aim to minimize the regret. For both settings, we propose an optimistic variant of the least-squares minimax value iteration algorithm. We show that our algorithm is computationally efficient and provably achieves an $\tilde O(\sqrt{d^3 H^3 T} )$ upper bound on the duality gap and regret, where $d$ is the linear dimension, $H$ the horizon and $T$ the total number of timesteps. Our results do not require additional assumptions on the sampling model.
  Our setting requires overcoming several new challenges that are absent in Markov decision processes or turn-based Markov games. In particular, to achieve optimism with simultaneous moves, we construct both upper and lower confidence bounds of the value function, and then compute the optimistic policy by solving a general-sum matrix game with these bounds as the payoff matrices. As finding the Nash Equilibrium of a general-sum game is computationally hard, our algorithm instead solves for a Coarse Correlated Equilibrium (CCE), which can be obtained efficiently. To our best knowledge, such a CCE-based scheme for optimism has not appeared in the literature and might be of interest in its own right.",True,True,"Xie, Qiaomin and Chen, Yudong and Wang, Zhaoran and Yang, Zhuoran",2020.0,,,,
Distributionally Robust Online Markov Game with Linear Function Approximation,2511.07831v1,chen2022linear_mixture_marl,\cite{chen2022linear_mixture_marl},Almost optimal algorithms for two-player zero-sum linear mixture markov games,,,True,False,"Chen, Zixiang and Zhou, Dongruo and Gu, Quanquan",2022.0,,,,
Distributionally Robust Online Markov Game with Linear Function Approximation,2511.07831v1,cisneros2023finite,\cite{cisneros2023finite},Finite-sample guarantees for nash Q-learning with linear function approximation,,,True,False,"Cisneros-Velarde, Pedro and Koyejo, Sanmi",2023.0,,,,
Distributionally Robust Online Markov Game with Linear Function Approximation,2511.07831v1,cui2023breaking_linear_decentralized,\cite{cui2023breaking_linear_decentralized},Breaking the curse of multiagents in a large state space: Rl in markov games with independent linear function approximation,,,True,False,"Cui, Qiwen and Zhang, Kaiqing and Du, Simon",2023.0,,,,
Distributionally Robust Online Markov Game with Linear Function Approximation,2511.07831v1,wang2023breaking_linear_decentralized,\cite{wang2023breaking_linear_decentralized},Breaking the curse of multiagency: Provably efficient decentralized multi-agent rl with function approximation,,,True,False,"Wang, Yuanhao and Liu, Qinghua and Bai, Yu and Jin, Chi",2023.0,,,,
Distributionally Robust Online Markov Game with Linear Function Approximation,2511.07831v1,dai2024refined_decentralized_linear_marl,\cite{dai2024refined_decentralized_linear_marl},Refined sample complexity for markov games with independent linear function approximation,,,True,False,"Dai, Yan and Cui, Qiwen and Du, Simon S",2024.0,,,,arXiv preprint arXiv:2402.07082
Online Linear Regression with Paid Stochastic Features,2511.08073v1,fuller2009measurement,\cite{fuller2009measurement},Prediction in Measurement Error Models,https://arxiv.org/abs/2405.10461v1,"We study the well known difficult problem of prediction in measurement error models. By targeting directly at the prediction interval instead of the point prediction, we construct a prediction interval by providing estimators of both the center and the length of the interval which achieves a pre-determined prediction level. The constructing procedure requires a working model for the distribution of the variable prone to error. If the working model is correct, the prediction interval estimator obtains the smallest variability in terms of assessing the true center and length. If the working model is incorrect, the prediction interval estimation is still consistent. We further study how the length of the prediction interval depends on the choice of the true prediction interval center and provide guidance on obtaining minimal prediction interval length. Numerical experiments are conducted to illustrate the performance and we apply our method to predict concentration of Abeta1-12 in cerebrospinal fluid in an Alzheimer's disease data.",True,True,"Fuller, Wayne A",2009.0,,,,
Online Linear Regression with Paid Stochastic Features,2511.08073v1,agarwal2021causal,\cite{agarwal2021causal},"Causal Inference with Corrupted Data: Measurement Error, Missing Values, Discretization, and Differential Privacy",,,True,False,"Agarwal, Anish and Singh, Rahul",2021.0,,,,arXiv preprint arXiv:2107.02780
Online Linear Regression with Paid Stochastic Features,2511.08073v1,rosenbaum2010sparse,\cite{rosenbaum2010sparse},Sparse recovery under matrix uncertainty,https://arxiv.org/abs/0812.2818v3,"We consider the model {eqnarray*}y=Xθ^*+ξ, Z=X+Ξ,{eqnarray*} where the random vector $y\in\mathbb{R}^n$ and the random $n\times p$ matrix $Z$ are observed, the $n\times p$ matrix $X$ is unknown, $Ξ$ is an $n\times p$ random noise matrix, $ξ\in\mathbb{R}^n$ is a noise independent of $Ξ$, and $θ^*$ is a vector of unknown parameters to be estimated. The matrix uncertainty is in the fact that $X$ is observed with additive error. For dimensions $p$ that can be much larger than the sample size $n$, we consider the estimation of sparse vectors $θ^*$. Under matrix uncertainty, the Lasso and Dantzig selector turn out to be extremely unstable in recovering the sparsity pattern (i.e., of the set of nonzero components of $θ^*$), even if the noise level is very small. We suggest new estimators called matrix uncertainty selectors (or, shortly, the MU-selectors) which are close to $θ^*$ in different norms and in the prediction risk if the restricted eigenvalue assumption on $X$ is satisfied. We also show that under somewhat stronger assumptions, these estimators recover correctly the sparsity pattern.",True,True,"Rosenbaum, Mathieu and Tsybakov, Alexandre B",2010.0,,,,The Annals of Statistics
Online Linear Regression with Paid Stochastic Features,2511.08073v1,loh2011high,\cite{loh2011high},High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity,,,True,False,"Loh, Po-Ling and Wainwright, Martin J",2011.0,,,,Advances in neural information processing systems
Online Linear Regression with Paid Stochastic Features,2511.08073v1,chen2013noisy,\cite{chen2013noisy},Noisy and missing data regression: Distribution-oblivious support recovery,,,True,False,"Chen, Yudong and Caramanis, Constantine",2013.0,,,,
Online Linear Regression with Paid Stochastic Features,2511.08073v1,rosenbaum2013improved,\cite{rosenbaum2013improved},Improved Matrix Uncertainty Selector,https://arxiv.org/abs/1112.4413v1,"We consider the regression model with observation error in the design: y=Xθ* + e, Z=X+N. Here the random vector y in R^n and the random n*p matrix Z are observed, the n*p matrix X is unknown, N is an n*p random noise matrix, e in R^n is a random noise vector, and θ* is a vector of unknown parameters to be estimated. We consider the setting where the dimension p can be much larger than the sample size n and θ* is sparse. Because of the presence of the noise matrix N, the commonly used Lasso and Dantzig selector are unstable. An alternative procedure called the Matrix Uncertainty (MU) selector has been proposed in Rosenbaum and Tsybakov (2010) in order to account for the noise. The properties of the MU selector have been studied in Rosenbaum and Tsybakov (2010) for sparse θ* under the assumption that the noise matrix N is deterministic and its values are small. In this paper, we propose a modification of the MU selector when N is a random matrix with zero-mean entries having the variances that can be estimated. This is, for example, the case in the model where the entries of X are missing at random. We show both theoretically and numerically that, under these conditions, the new estimator called the Compensated MU selector achieves better accuracy of estimation than the original MU selector.",True,True,"Rosenbaum, Mathieu and Tsybakov, Alexandre B",2013.0,,,,
Online Linear Regression with Paid Stochastic Features,2511.08073v1,kaul2015weighted,\cite{kaul2015weighted},Weighted $\ell_1$-penalized corrected quantile regression for high dimensional measurement error models,,,True,False,"Kaul, Abhishek and Koul, Hira L",2015.0,,,,Journal of Multivariate Analysis
Online Linear Regression with Paid Stochastic Features,2511.08073v1,belloni2017linear,\cite{belloni2017linear},Linear and conic programming estimators in high dimensional errors-in-variables models,,,True,False,"Belloni, Alexandre and Rosenbaum, Mathieu and Tsybakov, Alexandre B",2017.0,,,,Journal of the Royal Statistical Society Series B: Statistical Methodology
Online Linear Regression with Paid Stochastic Features,2511.08073v1,datta2017cocolasso,\cite{datta2017cocolasso},Cocolasso for high-dimensional error-in-variables regression,,,True,False,"Datta, Abhirup and Zou, Hui",2017.0,,,,Annals of Statistics
Online Linear Regression with Paid Stochastic Features,2511.08073v1,reilly1981bayesian,\cite{reilly1981bayesian},A Bayesian study of the error-in-variables model,,,True,False,"Reilly, Park M and Patino-Leal, Hugo",1981.0,,,,Technometrics
Online Linear Regression with Paid Stochastic Features,2511.08073v1,ungarala2000multiscale,\cite{ungarala2000multiscale},"A multiscale, Bayesian and error-in-variables approach for linear dynamic data rectification",,,True,False,"Ungarala, Sridhar and Bakshi, Bhavik R",2000.0,,,,Computers \& Chemical Engineering
Online Linear Regression with Paid Stochastic Features,2511.08073v1,figueroa2022robust,\cite{figueroa2022robust},Robust beta regression modeling with errors-in-variables: a Bayesian approach and numerical applications,,,True,False,"Figueroa-Z{\'u}{\~n}iga, Jorge I and Bayes, Cristian L and Leiva, V{\'\i}ctor and Liu, Shuangzhe",2022.0,,,,Statistical Papers
Online Linear Regression with Paid Stochastic Features,2511.08073v1,agarwal2023adaptive,\cite{agarwal2023adaptive},Adaptive principal component regression with applications to panel data,,,True,False,"Agarwal, Anish and Harris, Keegan and Whitehouse, Justin and Wu, Steven Z",2023.0,,,,Advances in Neural Information Processing Systems
Online Linear Regression with Paid Stochastic Features,2511.08073v1,agarwal2019robustness,\cite{agarwal2019robustness},On Robustness of Principal Component Regression,https://arxiv.org/abs/1902.10920v10,"Principal component regression (PCR) is a simple, but powerful and ubiquitously utilized method. Its effectiveness is well established when the covariates exhibit low-rank structure. However, its ability to handle settings with noisy, missing, and mixed-valued, i.e., discrete and continuous, covariates is not understood and remains an important open challenge. As the main contribution of this work we establish the robustness of PCR, without any change, in this respect and provide meaningful finite-sample analysis. To do so, we establish that PCR is equivalent to performing linear regression after pre-processing the covariate matrix via hard singular value thresholding (HSVT). As a result, in the context of counterfactual analysis using observational data, we show PCR is equivalent to the recently proposed robust variant of the synthetic control method, known as robust synthetic control (RSC). As an immediate consequence, we obtain finite-sample analysis of the RSC estimator that was previously absent. As an important contribution to the synthetic controls literature, we establish that an (approximate) linear synthetic control exists in the setting of a generalized factor model or latent variable model; traditionally in the literature, the existence of a synthetic control needs to be assumed to exist as an axiom. We further discuss a surprising implication of the robustness property of PCR with respect to noise, i.e., PCR can learn a good predictive model even if the covariates are tactfully transformed to preserve differential privacy. Finally, this work advances the state-of-the-art analysis for HSVT by establishing stronger guarantees with respect to the $\ell_{2, \infty}$-norm rather than the Frobenius norm as is commonly done in the matrix estimation literature, which may be of interest in its own right.",True,True,"Agarwal, Anish and Shah, Devavrat and Shen, Dennis and Song, Dogyoon",2019.0,,,,Advances in Neural Information Processing Systems
Online Linear Regression with Paid Stochastic Features,2511.08073v1,kasiviswanathan2011can,\cite{kasiviswanathan2011can},What Can We Learn Privately?,https://arxiv.org/abs/0803.0924v3,"Learning problems form an important category of computational tasks that generalizes many of the computations researchers apply to large real-life data sets. We ask: what concept classes can be learned privately, namely, by an algorithm whose output does not depend too heavily on any one input or specific training example? More precisely, we investigate learning algorithms that satisfy differential privacy, a notion that provides strong confidentiality guarantees in contexts where aggregate information is released about a database containing sensitive information about individuals. We demonstrate that, ignoring computational constraints, it is possible to privately agnostically learn any concept class using a sample size approximately logarithmic in the cardinality of the concept class. Therefore, almost anything learnable is learnable privately: specifically, if a concept class is learnable by a (non-private) algorithm with polynomial sample complexity and output size, then it can be learned privately using a polynomial number of samples. We also present a computationally efficient private PAC learner for the class of parity functions. Local (or randomized response) algorithms are a practical class of private algorithms that have received extensive investigation. We provide a precise characterization of local private learning algorithms. We show that a concept class is learnable by a local algorithm if and only if it is learnable in the statistical query (SQ) model. Finally, we present a separation between the power of interactive and noninteractive local learning algorithms.",True,True,"Kasiviswanathan, Shiva Prasad and Lee, Homin K and Nissim, Kobbi and Raskhodnikova, Sofya and Smith, Adam",2011.0,,,,SIAM Journal on Computing
Online Linear Regression with Paid Stochastic Features,2511.08073v1,yang2024local,\cite{yang2024local},Local differential privacy and its applications: A comprehensive survey,,,True,False,"Yang, Mengmeng and Guo, Taolin and Zhu, Tianqing and Tjuawinata, Ivan and Zhao, Jun and Lam, Kwok-Yan",2024.0,,,,Computer Standards \& Interfaces
Online Linear Regression with Paid Stochastic Features,2511.08073v1,duchi2013local,\cite{duchi2013local},"Local Privacy, Data Processing Inequalities, and Statistical Minimax Rates",https://arxiv.org/abs/1302.3203v4,"Working under a model of privacy in which data remains private even from the statistician, we study the tradeoff between privacy guarantees and the utility of the resulting statistical estimators. We prove bounds on information-theoretic quantities, including mutual information and Kullback-Leibler divergence, that depend on the privacy guarantees. When combined with standard minimax techniques, including the Le Cam, Fano, and Assouad methods, these inequalities allow for a precise characterization of statistical rates under local privacy constraints. We provide a treatment of several canonical families of problems: mean estimation, parameter estimation in fixed-design regression, multinomial probability estimation, and nonparametric density estimation. For all of these families, we provide lower and upper bounds that match up to constant factors, and exhibit new (optimal) privacy-preserving mechanisms and computationally efficient estimators that achieve the bounds.",True,True,"Duchi, John C and Jordan, Michael I and Wainwright, Martin J",2013.0,,,,arXiv preprint arXiv:1302.3203
Online Linear Regression with Paid Stochastic Features,2511.08073v1,smith2017interaction,\cite{smith2017interaction},Is interaction necessary for distributed private learning?,,,True,False,"Smith, Adam and Thakurta, Abhradeep and Upadhyay, Jalaj",2017.0,,,,
Online Linear Regression with Paid Stochastic Features,2511.08073v1,wang2019sparse,\cite{wang2019sparse},On sparse linear regression in the local differential privacy model,,,True,False,"Wang, Di and Xu, Jinhui",2019.0,,,,
Online Linear Regression with Paid Stochastic Features,2511.08073v1,fukuchi2017differentially,\cite{fukuchi2017differentially},Differentially Private Empirical Risk Minimization,https://arxiv.org/abs/0912.0071v5,"Privacy-preserving machine learning algorithms are crucial for the increasingly common setting in which personal data, such as medical or financial records, are analyzed. We provide general techniques to produce privacy-preserving approximations of classifiers learned via (regularized) empirical risk minimization (ERM). These algorithms are private under the $ε$-differential privacy definition due to Dwork et al. (2006). First we apply the output perturbation ideas of Dwork et al. (2006), to ERM classification. Then we propose a new method, objective perturbation, for privacy-preserving machine learning algorithm design. This method entails perturbing the objective function before optimizing over classifiers. If the loss and regularizer satisfy certain convexity and differentiability criteria, we prove theoretical results showing that our algorithms preserve privacy, and provide generalization bounds for linear and nonlinear kernels. We further present a privacy-preserving technique for tuning the parameters in general machine learning algorithms, thereby providing end-to-end privacy guarantees for the training process. We apply these results to produce privacy-preserving analogues of regularized logistic regression and support vector machines. We obtain encouraging results from evaluating their performance on real demographic and benchmark data sets. Our results show that both theoretically and empirically, objective perturbation is superior to the previous state-of-the-art, output perturbation, in managing the inherent tradeoff between privacy and learning performance.",True,True,"Fukuchi, Kazuto and Tran, Quang Khai and Sakuma, Jun",2017.0,,,,
Online Linear Regression with Paid Stochastic Features,2511.08073v1,cesa2011online,\cite{cesa2011online},Online Learning of Noisy Data with Kernels,https://arxiv.org/abs/1005.2296v2,"We study online learning when individual instances are corrupted by adversarially chosen random noise. We assume the noise distribution is unknown, and may change over time with no restriction other than having zero mean and bounded variance. Our technique relies on a family of unbiased estimators for non-linear functions, which may be of independent interest. We show that a variant of online gradient descent can learn functions in any dot-product (e.g., polynomial) or Gaussian kernel space with any analytic convex loss function. Our variant uses randomized estimates that need to query a random number of noisy copies of each instance, where with high probability this number is upper bounded by a constant. Allowing such multiple queries cannot be avoided: Indeed, we show that online learning is in general impossible when only one noisy copy of each instance can be accessed.",True,True,"Cesa-Bianchi, Nicolo and Shalev-Shwartz, Shai and Shamir, Ohad",2011.0,,,,IEEE Transactions on Information Theory
Online Linear Regression with Paid Stochastic Features,2511.08073v1,van2023trading,\cite{van2023trading},Trading-off payments and accuracy in online classification with paid stochastic experts,,,True,False,"Van Der Hoeven, Dirk and Pike-Burke, Ciara and Qiu, Hao and Cesa-Bianchi, Nicolo",2023.0,,,,
Online Linear Regression with Paid Stochastic Features,2511.08073v1,kim2023contextual,\cite{kim2023contextual},Contextual Linear Bandits under Noisy Features: Towards Bayesian Oracles,https://arxiv.org/abs/1703.01347v4,"We study contextual linear bandit problems under feature uncertainty, where the features are noisy and have missing entries. To address the challenges posed by this noise, we analyze Bayesian oracles given the observed noisy features. Our Bayesian analysis reveals that the optimal hypothesis can significantly deviate from the underlying realizability function, depending on the noise characteristics. These deviations are highly non-intuitive and do not occur in classical noiseless setups. This implies that classical approaches cannot guarantee a non-trivial regret bound. Therefore, we propose an algorithm that aims to approximate the Bayesian oracle based on the observed information under this model, achieving $\tilde{O}(d\sqrt{T})$ regret bound when there is a large number of arms. We demonstrate the proposed algorithm using synthetic and real-world datasets.",True,True,"Kim, Jung-hun and Yun, Se-Young and Jeong, Minchan and Nam, Junhyun and Shin, Jinwoo and Combes, Richard",2023.0,,,,
Online Linear Regression with Paid Stochastic Features,2511.08073v1,zheng2020locally,\cite{zheng2020locally},Locally Differentially Private (Contextual) Bandits Learning,https://arxiv.org/abs/2006.00701v4,"We study locally differentially private (LDP) bandits learning in this paper. First, we propose simple black-box reduction frameworks that can solve a large family of context-free bandits learning problems with LDP guarantee. Based on our frameworks, we can improve previous best results for private bandits learning with one-point feedback, such as private Bandits Convex Optimization, and obtain the first result for Bandits Convex Optimization (BCO) with multi-point feedback under LDP. LDP guarantee and black-box nature make our frameworks more attractive in real applications compared with previous specifically designed and relatively weaker differentially private (DP) context-free bandits algorithms. Further, we extend our $(\varepsilon, δ)$-LDP algorithm to Generalized Linear Bandits, which enjoys a sub-linear regret $\tilde{O}(T^{3/4}/\varepsilon)$ and is conjectured to be nearly optimal. Note that given the existing $Ω(T)$ lower bound for DP contextual linear bandits (Shariff & Sheffe, 2018), our result shows a fundamental difference between LDP and DP contextual bandits learning.",True,True,"Zheng, Kai and Cai, Tianle and Huang, Weiran and Li, Zhenguo and Wang, Liwei",2020.0,,,,Advances in Neural Information Processing Systems
Online Linear Regression with Paid Stochastic Features,2511.08073v1,ghosh2011selling,\cite{ghosh2011selling},Selling Privacy at Auction,https://arxiv.org/abs/1011.1375v4,"We initiate the study of markets for private data, though the lens of differential privacy. Although the purchase and sale of private data has already begun on a large scale, a theory of privacy as a commodity is missing. In this paper, we propose to build such a theory. Specifically, we consider a setting in which a data analyst wishes to buy information from a population from which he can estimate some statistic. The analyst wishes to obtain an accurate estimate cheaply. On the other hand, the owners of the private data experience some cost for their loss of privacy, and must be compensated for this loss. Agents are selfish, and wish to maximize their profit, so our goal is to design truthful mechanisms. Our main result is that such auctions can naturally be viewed and optimally solved as variants of multi-unit procurement auctions. Based on this result, we derive auctions for two natural settings which are optimal up to small constant factors:
  1. In the setting in which the data analyst has a fixed accuracy goal, we show that an application of the classic Vickrey auction achieves the analyst's accuracy goal while minimizing his total payment.
  2. In the setting in which the data analyst has a fixed budget, we give a mechanism which maximizes the accuracy of the resulting estimate while guaranteeing that the resulting sum payments do not exceed the analysts budget.
  In both cases, our comparison class is the set of envy-free mechanisms, which correspond to the natural class of fixed-price mechanisms in our setting.
  In both of these results, we ignore the privacy cost due to possible correlations between an individuals private data and his valuation for privacy itself. We then show that generically, no individually rational mechanism can compensate individuals for the privacy loss incurred due to their reported valuations for privacy.",True,True,"Ghosh, Arpita and Roth, Aaron",2011.0,,,,
Online Linear Regression with Paid Stochastic Features,2511.08073v1,nissim2012privacy,\cite{nissim2012privacy},Privacy-Aware Mechanism Design,https://arxiv.org/abs/1111.3350v2,"In traditional mechanism design, agents only care about the utility they derive from the outcome of the mechanism. We look at a richer model where agents also assign non-negative dis-utility to the information about their private types leaked by the outcome of the mechanism.
  We present a new model for privacy-aware mechanism design, where we only assume an upper bound on the agents' loss due to leakage, as opposed to previous work where a full characterization of the loss was required.
  In this model, under a mild assumption on the distribution of how agents value their privacy, we show a generic construction of privacy-aware mechanisms and demonstrate its applicability to electronic polling and pricing of a digital good.",True,True,"Nissim, Kobbi and Orlandi, Claudio and Smorodinsky, Rann",2012.0,,,,
Online Linear Regression with Paid Stochastic Features,2511.08073v1,nissim2014redrawing,\cite{nissim2014redrawing},Redrawing the boundaries on purchasing data from privacy-sensitive individuals,,,True,False,"Nissim, Kobbi and Vadhan, Salil and Xiao, David",2014.0,,,,
Online Linear Regression with Paid Stochastic Features,2511.08073v1,wang2018value,\cite{wang2018value},"The value of privacy: Strategic data subjects, incentive mechanisms, and fundamental limits",,,True,False,"Wang, Weina and Ying, Lei and Zhang, Junshan",2018.0,,,,ACM Transactions on Economics and Computation (TEAC)
Online Linear Regression with Paid Stochastic Features,2511.08073v1,fallah2024optimal,\cite{fallah2024optimal},Optimal and differentially private data acquisition: Central and local mechanisms,,,True,False,"Fallah, Alireza and Makhdoumi, Ali and Malekian, Azarakhsh and Ozdaglar, Asuman",2024.0,,,,Operations Research
Online Linear Regression with Paid Stochastic Features,2511.08073v1,hsu2014differential,\cite{hsu2014differential},Differential privacy: An economic method for choosing epsilon,,,True,False,"Hsu, Justin and Gaboardi, Marco and Haeberlen, Andreas and Khanna, Sanjeev and Narayan, Arjun and Pierce, Benjamin C and Roth, Aaron",2014.0,,,,
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,stachenfeld2021learned,\cite{stachenfeld2021learned},Learned coarse models for efficient turbulence simulation,,,True,False,"Stachenfeld, Kimberly and Fielding, Drummond B and Kochkov, Dmitrii and Cranmer, Miles and Pfaff, Tobias and Godwin, Jonathan and Cui, Can and Ho, Shirley and Battaglia, Peter and Sanchez-Gonzalez, Alvaro",2021.0,,,,arXiv preprint arXiv:2112.15275
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,mccabe2023multiple,\cite{mccabe2023multiple},Multiple physics pretraining for physical surrogate models,,,True,False,"McCabe, Michael and Blancard, Bruno R{\'e}galdo-Saint and Parker, Liam Holden and Ohana, Ruben and Cranmer, Miles and Bietti, Alberto and Eickenberg, Michael and Golkar, Siavash and Krawezik, Geraud and Lanusse, Francois and others",2023.0,,,,arXiv preprint arXiv:2310.02994
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,yang2023context,\cite{yang2023context},In-context operator learning with data prompts for differential equation problems,,,True,False,"Yang, Liu and Liu, Siting and Meng, Tingwei and Osher, Stanley J",2023.0,,,,Proceedings of the National Academy of Sciences
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,rahman2024pretraining,\cite{rahman2024pretraining},Pretraining Codomain Attention Neural Operators for Solving Multiphysics PDEs,https://arxiv.org/abs/2403.12553v3,"Existing neural operator architectures face challenges when solving multiphysics problems with coupled partial differential equations (PDEs) due to complex geometries, interactions between physical variables, and the limited amounts of high-resolution training data. To address these issues, we propose Codomain Attention Neural Operator (CoDA-NO), which tokenizes functions along the codomain or channel space, enabling self-supervised learning or pretraining of multiple PDE systems. Specifically, we extend positional encoding, self-attention, and normalization layers to function spaces. CoDA-NO can learn representations of different PDE systems with a single model. We evaluate CoDA-NO's potential as a backbone for learning multiphysics PDEs over multiple systems by considering few-shot learning settings. On complex downstream tasks with limited data, such as fluid flow simulations, fluid-structure interactions, and Rayleigh-Bénard convection, we found CoDA-NO to outperform existing methods by over 36%.",True,True,"Rahman, Md Ashiqur and George, Robert Joseph and Elleithy, Mogab and Leibovici, Daniel and Li, Zongyi and Bonev, Boris and White, Colin and Berner, Julius and Yeh, Raymond A and Kossaifi, Jean and others",2024.0,,,,Advances in Neural Information Processing Systems
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,sun2025towards,\cite{sun2025towards},Towards a foundation model for partial differential equations: Multioperator learning and extrapolation,,,True,False,"Sun, Jingmin and Liu, Yuxuan and Zhang, Zecheng and Schaeffer, Hayden",2025.0,,,,Physical Review E
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,shen2024ups,\cite{shen2024ups},UPS: Efficiently Building Foundation Models for PDE Solving via Cross-Modal Adaptation,https://arxiv.org/abs/2403.07187v4,"We present Unified PDE Solvers (UPS), a data- and compute-efficient approach to developing unified neural operators for diverse families of spatiotemporal PDEs from various domains, dimensions, and resolutions. UPS embeds different PDEs into a shared representation space and processes them using a FNO-transformer architecture. Rather than training the network from scratch, which is data-demanding and computationally expensive, we warm-start the transformer from pretrained LLMs and perform explicit alignment to reduce the modality gap while improving data and compute efficiency. The cross-modal UPS achieves state-of-the-art results on a wide range of 1D and 2D PDE families from PDEBench, outperforming existing unified models using 4 times less data and 26 times less compute. Meanwhile, it is capable of few-shot transfer to unseen PDE families and coefficients.",True,True,"Shen, Junhong and Marwah, Tanya and Talwalkar, Ameet",2024.0,,,,arXiv preprint arXiv:2403.07187
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,herde2024poseidon,\cite{herde2024poseidon},Poseidon: Efficient foundation models for pdes,,,True,False,"Herde, Maximilian and Raonic, Bogdan and Rohner, Tobias and K{\""a}ppeli, Roger and Molinaro, Roberto and de B{\'e}zenac, Emmanuel and Mishra, Siddhartha",2024.0,,,,Advances in Neural Information Processing Systems
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,chen2024omniarch,\cite{chen2024omniarch},OmniArch: Building Foundation Model For Scientific Computing,,,True,False,"Chen, Tianyu and Zhou, Haoyi and Li, Ying and Wang, Hao and Gao, Chonghan and Shi, Rongye and Zhang, Shanghang and Li, Jianxin",2024.0,,,,arXiv preprint arXiv:2402.16014
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,takamoto2022pdebench,\cite{takamoto2022pdebench},Pdebench: An extensive benchmark for scientific machine learning,,,True,False,"Takamoto, Makoto and Praditia, Timothy and Leiteritz, Raphael and MacKinlay, Daniel and Alesiani, Francesco and Pfl{\""u}ger, Dirk and Niepert, Mathias",2022.0,,,,Advances in Neural Information Processing Systems
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,ohana2024well,\cite{ohana2024well},The Well: A large-scale collection of diverse physics simulations for machine learning,,,True,False,"Ohana, Ruben and McCabe, Michael and Meyer, Lucas and Morel, Rudy and Agocs, Fruzsina and Beneitez, Miguel and Berger, Marsha and Burkhart, Blakesly and Dalziel, Stuart and Fielding, Drummond and others",2024.0,,,,Advances in Neural Information Processing Systems
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,audenaert2024multimodal,\cite{audenaert2024multimodal},The multimodal universe: enabling large-scale machine learning with 100TB of astronomical scientific data,,,True,False,"Audenaert, Jeroen and Bowles, Micah and Boyd, Benjamin M and Chemaly, David and Cherinka, Brian and Ciuc{\u{a}}, Ioana and Cranmer, Miles and Do, Aaron and Grayling, Matthew and Hayes, Erin E and others",2024.0,,,,arXiv preprint arXiv:2412.02527
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,shawe1989building,\cite{shawe1989building},Building symmetries into feedforward networks,,,True,False,"Shawe-Taylor, John",1989.0,,,,
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,cohen2016group,\cite{cohen2016group},Group Equivariant Convolutional Networks,https://arxiv.org/abs/1602.07576v3,"We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers. G-convolutions increase the expressive capacity of the network without increasing the number of parameters. Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups generated by translations, reflections and rotations. G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST.",True,True,"Cohen, Taco and Welling, Max",2016.0,,,,
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,kondor2018generalization,\cite{kondor2018generalization},On the generalization of equivariance and convolution in neural networks to the action of compact groups,,,True,False,"Kondor, Risi and Trivedi, Shubhendu",2018.0,,,,
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,worrall2019deep,\cite{worrall2019deep},Deep Scale-spaces: Equivariance Over Scale,https://arxiv.org/abs/1905.11697v1,"We introduce deep scale-spaces (DSS), a generalization of convolutional neural networks, exploiting the scale symmetry structure of conventional image recognition tasks. Put plainly, the class of an image is invariant to the scale at which it is viewed. We construct scale equivariant cross-correlations based on a principled extension of convolutions, grounded in the theory of scale-spaces and semigroups. As a very basic operation, these cross-correlations can be used in almost any modern deep learning architecture in a plug-and-play manner. We demonstrate our networks on the Patch Camelyon and Cityscapes datasets, to prove their utility and perform introspective studies to further understand their properties.",True,True,"Worrall, Daniel and Welling, Max",2019.0,,,,Advances in Neural Information Processing Systems
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,wang2020incorporating,\cite{wang2020incorporating},Incorporating symmetry into deep dynamics models for improved generalization,,,True,False,"Wang, Rui and Walters, Robin and Yu, Rose",2020.0,,,,arXiv preprint arXiv:2002.03061
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,huang2023approximately,\cite{huang2023approximately},Approximately Equivariant Graph Networks,https://arxiv.org/abs/2308.10436v3,"Graph neural networks (GNNs) are commonly described as being permutation equivariant with respect to node relabeling in the graph. This symmetry of GNNs is often compared to the translation equivariance of Euclidean convolution neural networks (CNNs). However, these two symmetries are fundamentally different: The translation equivariance of CNNs corresponds to symmetries of the fixed domain acting on the image signals (sometimes known as active symmetries), whereas in GNNs any permutation acts on both the graph signals and the graph domain (sometimes described as passive symmetries). In this work, we focus on the active symmetries of GNNs, by considering a learning setting where signals are supported on a fixed graph. In this case, the natural symmetries of GNNs are the automorphisms of the graph. Since real-world graphs tend to be asymmetric, we relax the notion of symmetries by formalizing approximate symmetries via graph coarsening. We present a bias-variance formula that quantifies the tradeoff between the loss in expressivity and the gain in the regularity of the learned estimator, depending on the chosen symmetry group. To illustrate our approach, we conduct extensive experiments on image inpainting, traffic flow prediction, and human pose estimation with different choices of symmetries. We show theoretically and empirically that the best generalization performance can be achieved by choosing a suitably larger group than the graph automorphism, but smaller than the permutation group.",True,True,"Huang, Ningyuan and Levie, Ron and Villar, Soledad",2023.0,,,,Advances in Neural Information Processing Systems
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,geiger2022e3nn,\cite{geiger2022e3nn},e3nn: Euclidean Neural Networks,https://arxiv.org/abs/2207.09453v1,"We present e3nn, a generalized framework for creating E(3) equivariant trainable functions, also known as Euclidean neural networks. e3nn naturally operates on geometry and geometric tensors that describe systems in 3D and transform predictably under a change of coordinate system. The core of e3nn are equivariant operations such as the TensorProduct class or the spherical harmonics functions that can be composed to create more complex modules such as convolutions and attention mechanisms. These core operations of e3nn can be used to efficiently articulate Tensor Field Networks, 3D Steerable CNNs, Clebsch-Gordan Networks, SE(3) Transformers and other E(3) equivariant networks.",True,True,"Geiger, Mario and Smidt, Tess",2022.0,,,,arXiv preprint arXiv:2207.09453
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,gregory2024robust,\cite{gregory2024robust},Robust Emulator for Compressible Navier-Stokes using Equivariant Geometric Convolutions,,,True,False,"Gregory, Wilson G and Wong, Kaze WK and Villar, Soledad",2024.0,,,,
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,blum2023machine,\cite{blum2023machine},Machine learning and invariant theory,,,True,False,"Blum-Smith, Ben and Villar, Soledad",2023.0,,,,Notices of the American Mathematical Society
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,villar2021scalars,\cite{villar2021scalars},"Scalars are universal: Equivariant machine learning, structured like classical physics",,,True,False,"Villar, Soledad and Hogg, David W and Storey-Fisher, Kate and Yao, Weichi and Blum-Smith, Ben",2021.0,,,,Advances in neural information processing systems
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,blum2024galois,\cite{blum2024galois},A Galois theorem for machine learning: Functions on symmetric matrices and point clouds via lightweight invariant features,,,True,False,"Blum-Smith, Ben and Huang, Ningyuan and Cuturi, Marco and Villar, Soledad",2024.0,,,,arXiv preprint arXiv:2405.08097
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,gregory2024learning,\cite{gregory2024learning},Learning equivariant tensor functions with applications to sparse vector recovery,,,True,False,"Gregory, Wilson G and Tonelli-Cueto, Josu{\'e} and Marshall, Nicholas F and Lee, Andrew S and Villar, Soledad",2024.0,,,,arXiv preprint arXiv:2406.01552
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,cohen2019general,\cite{cohen2019general},A General Theory of Equivariant CNNs on Homogeneous Spaces,https://arxiv.org/abs/1811.02017v2,"We present a general theory of Group equivariant Convolutional Neural Networks (G-CNNs) on homogeneous spaces such as Euclidean space and the sphere. Feature maps in these networks represent fields on a homogeneous base space, and layers are equivariant maps between spaces of fields. The theory enables a systematic classification of all existing G-CNNs in terms of their symmetry group, base space, and field type. We also consider a fundamental question: what is the most general kind of equivariant linear map between feature spaces (fields) of given types? Following Mackey, we show that such maps correspond one-to-one with convolutions using equivariant kernels, and characterize the space of such kernels.",True,True,"Cohen, Taco S and Geiger, Mario and Weiler, Maurice",2019.0,,,,Advances in Neural Information Processing Systems
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,kaba2023equivariance,\cite{kaba2023equivariance},Equivariance with learned canonicalization functions,,,True,False,"Kaba, S{\'e}kou-Oumar and Mondal, Arnab Kumar and Zhang, Yan and Bengio, Yoshua and Ravanbakhsh, Siamak",2023.0,,,,
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,cohen2016steerable,\cite{cohen2016steerable},Steerable CNNs,https://arxiv.org/abs/1612.08498v1,"It has long been recognized that the invariance and equivariance properties of a representation are critically important for success in many vision tasks. In this paper we present Steerable Convolutional Neural Networks, an efficient and flexible class of equivariant convolutional networks. We show that steerable CNNs achieve state of the art results on the CIFAR image classification benchmark. The mathematical theory of steerable representations reveals a type system in which any steerable representation is a composition of elementary feature types, each one associated with a particular kind of symmetry. We show how the parameter cost of a steerable filter bank depends on the types of the input and output features, and show how to use this knowledge to construct CNNs that utilize parameters effectively.",True,True,"Cohen, Taco S and Welling, Max",2016.0,,,,arXiv preprint arXiv:1612.08498
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,kondor2018n,\cite{kondor2018n},N-body Networks: a Covariant Hierarchical Neural Network Architecture for Learning Atomic Potentials,https://arxiv.org/abs/1803.01588v1,"We describe N-body networks, a neural network architecture for learning the behavior and properties of complex many body physical systems. Our specific application is to learn atomic potential energy surfaces for use in molecular dynamics simulations. Our architecture is novel in that (a) it is based on a hierarchical decomposition of the many body system into subsytems, (b) the activations of the network correspond to the internal state of each subsystem, (c) the ""neurons"" in the network are constructed explicitly so as to guarantee that each of the activations is covariant to rotations, (d) the neurons operate entirely in Fourier space, and the nonlinearities are realized by tensor products followed by Clebsch-Gordan decompositions. As part of the description of our network, we give a characterization of what way the weights of the network may interact with the activations so as to ensure that the covariance property is maintained.",True,True,"Kondor, Risi",2018.0,,,,arXiv preprint arXiv:1803.01588
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,yarotsky2022universal,\cite{yarotsky2022universal},Universal approximations of invariant maps by neural networks,https://arxiv.org/abs/1804.10306v1,"We describe generalizations of the universal approximation theorem for neural networks to maps invariant or equivariant with respect to linear representations of groups. Our goal is to establish network-like computational models that are both invariant/equivariant and provably complete in the sense of their ability to approximate any continuous invariant/equivariant map. Our contribution is three-fold. First, in the general case of compact groups we propose a construction of a complete invariant/equivariant network using an intermediate polynomial layer. We invoke classical theorems of Hilbert and Weyl to justify and simplify this construction; in particular, we describe an explicit complete ansatz for approximation of permutation-invariant maps. Second, we consider groups of translations and prove several versions of the universal approximation theorem for convolutional networks in the limit of continuous signals on euclidean spaces. Finally, we consider 2D signal transformations equivariant with respect to the group SE(2) of rigid euclidean motions. In this case we introduce the ""charge--conserving convnet"" -- a convnet-like computational model based on the decomposition of the feature space into isotypic representations of SO(2). We prove this model to be a universal approximator for continuous SE(2)--equivariant signal transformations.",True,True,"Yarotsky, Dmitry",2022.0,,,,Constructive Approximation
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,murphy2019relational,\cite{murphy2019relational},Relational pooling for graph representations,,,True,False,"Murphy, Ryan and Srinivasan, Balasubramaniam and Rao, Vinayak and Ribeiro, Bruno",2019.0,,,,
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,puny2022frame,\cite{puny2022frame},Frame Averaging for Invariant and Equivariant Network Design,,,True,False,"Puny, Omri and Atzmon, Matan and Ben-Hamu, Heli and Misra, Ishan and Grover, Aditya and Smith, Edward J and Lipman, Yaron",2022.0,,,,
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,elesedy2021provably,\cite{elesedy2021provably},Provably strict generalisation benefit for equivariant models,,,True,False,"Elesedy, Bryn and Zaidi, Sheheryar",2021.0,,,,
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,petrache2023approximation,\cite{petrache2023approximation},Approximation-Generalization Trade-offs under (Approximate) Group Equivariance,https://arxiv.org/abs/2305.17592v2,"The explicit incorporation of task-specific inductive biases through symmetry has emerged as a general design precept in the development of high-performance machine learning models. For example, group equivariant neural networks have demonstrated impressive performance across various domains and applications such as protein and drug design. A prevalent intuition about such models is that the integration of relevant symmetry results in enhanced generalization. Moreover, it is posited that when the data and/or the model may only exhibit $\textit{approximate}$ or $\textit{partial}$ symmetry, the optimal or best-performing model is one where the model symmetry aligns with the data symmetry. In this paper, we conduct a formal unified investigation of these intuitions. To begin, we present general quantitative bounds that demonstrate how models capturing task-specific symmetries lead to improved generalization. In fact, our results do not require the transformations to be finite or even form a group and can work with partial or approximate equivariance. Utilizing this quantification, we examine the more general question of model mis-specification i.e. when the model symmetries don't align with the data symmetries. We establish, for a given symmetry group, a quantitative comparison between the approximate/partial equivariance of the model and that of the data distribution, precisely connecting model equivariance error and data equivariance error. Our result delineates conditions under which the model equivariance error is optimal, thereby yielding the best-performing model for the given task and data. Our results are the most general results of their type in the literature.",True,True,"Petrache, Mircea and Trivedi, Shubhendu",2023.0,,,,Advances in Neural Information Processing Systems
Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost,2511.09573v1,tahmasebi2023exact,\cite{tahmasebi2023exact},The exact sample complexity gain from invariances for kernel regression,,,True,False,"Tahmasebi, Behrooz and Jegelka, Stefanie",2023.0,,,,Advances in Neural Information Processing Systems
Fair Bayesian Data Selection via Generalized Discrepancy Measures,2511.07032v1,grangier2023adaptivetrainingdistributionsscalable,\cite{grangier2023adaptivetrainingdistributionsscalable},Adaptive Training Distributions with Scalable Online Bilevel Optimization,,,True,False,David Grangier and Pierre Ablin and Awni Hannun,2023.0,,https://arxiv.org/abs/2311.11973,,
Fair Bayesian Data Selection via Generalized Discrepancy Measures,2511.07032v1,pmlr-v80-ren18a,\cite{pmlr-v80-ren18a},Learning to Reweight Examples for Robust Deep Learning,,,True,False,"Ren, Mengye and Zeng, Wenyuan and Yang, Bin and Urtasun, Raquel",2018.0,10--15 Jul,https://proceedings.mlr.press/v80/ren18a.html,,
Fair Bayesian Data Selection via Generalized Discrepancy Measures,2511.07032v1,10.5555/3454287.3454459,\cite{10.5555/3454287.3454459},Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting,https://arxiv.org/abs/1902.07379v6,"Current deep neural networks (DNNs) can easily overfit to biased training data with corrupted labels or class imbalance. Sample re-weighting strategy is commonly used to alleviate this issue by designing a weighting function mapping from training loss to sample weight, and then iterating between weight recalculating and classifier updating. Current approaches, however, need manually pre-specify the weighting function as well as its additional hyper-parameters. It makes them fairly hard to be generally applied in practice due to the significant variation of proper weighting schemes relying on the investigated problem and training data. To address this issue, we propose a method capable of adaptively learning an explicit weighting function directly from data. The weighting function is an MLP with one hidden layer, constituting a universal approximator to almost any continuous functions, making the method able to fit a wide range of weighting functions including those assumed in conventional research. Guided by a small amount of unbiased meta-data, the parameters of the weighting function can be finely updated simultaneously with the learning process of the classifiers. Synthetic and real experiments substantiate the capability of our method for achieving proper weighting functions in class imbalance and noisy label cases, fully complying with the common settings in traditional methods, and more complicated scenarios beyond conventional cases. This naturally leads to its better accuracy than other state-of-the-art methods.",True,True,"Shu, Jun and Xie, Qi and Yi, Lixuan and Zhao, Qian and Zhou, Sanping and Xu, Zongben and Meng, Deyu",2019.0,,,,Advances in neural information processing systems
Fair Bayesian Data Selection via Generalized Discrepancy Measures,2511.07032v1,zhang2021learningfastsamplereweighting,\cite{zhang2021learningfastsamplereweighting},Learning Fast Sample Re-weighting Without Reward Data,https://arxiv.org/abs/2109.03216v1,"Training sample re-weighting is an effective approach for tackling data biases such as imbalanced and corrupted labels. Recent methods develop learning-based algorithms to learn sample re-weighting strategies jointly with model training based on the frameworks of reinforcement learning and meta learning. However, depending on additional unbiased reward data is limiting their general applicability. Furthermore, existing learning-based sample re-weighting methods require nested optimizations of models and weighting parameters, which requires expensive second-order computation. This paper addresses these two problems and presents a novel learning-based fast sample re-weighting (FSR) method that does not require additional reward data. The method is based on two key ideas: learning from history to build proxy reward data and feature sharing to reduce the optimization cost. Our experiments show the proposed method achieves competitive results compared to state of the arts on label noise robustness and long-tailed recognition, and does so while achieving significantly improved training efficiency. The source code is publicly available at https://github.com/google-research/google-research/tree/master/ieg.",True,True,Zizhao Zhang and Tomas Pfister,2021.0,,https://arxiv.org/abs/2109.03216,,
Fair Bayesian Data Selection via Generalized Discrepancy Measures,2511.07032v1,fan2017learningdatalearn,\cite{fan2017learningdatalearn},Learning What Data to Learn,https://arxiv.org/abs/1702.08635v1,"Machine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter} (\textbf{NDF}), to explore automatic and adaptive data selection in the training process. In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks. Taking neural network training with stochastic gradient descent (SGD) as an example, comprehensive experiments with respect to various neural network modeling (e.g., multi-layer perceptron networks, convolutional neural networks and recurrent neural networks) and several applications (e.g., image classification and text understanding) demonstrate that NDF powered SGD can achieve comparable accuracy with standard SGD process by using less data and fewer iterations.",True,True,Yang Fan and Fei Tian and Tao Qin and Jiang Bian and Tie-Yan Liu,2017.0,,https://arxiv.org/abs/1702.08635,,
Fair Bayesian Data Selection via Generalized Discrepancy Measures,2511.07032v1,Curriculum/10.1145/1553374.1553380,\cite{Curriculum/10.1145/1553374.1553380},Curriculum Abductive Learning,https://arxiv.org/abs/2505.12275v2,"Abductive Learning (ABL) integrates machine learning with logical reasoning in a loop: a learning model predicts symbolic concept labels from raw inputs, which are revised through abduction using domain knowledge and then fed back for retraining. However, due to the nondeterminism of abduction, the training process often suffers from instability, especially when the knowledge base is large and complex, resulting in a prohibitively large abduction space. While prior works focus on improving candidate selection within this space, they typically treat the knowledge base as a static black box. In this work, we propose Curriculum Abductive Learning (C-ABL), a method that explicitly leverages the internal structure of the knowledge base to address the ABL training challenges. C-ABL partitions the knowledge base into a sequence of sub-bases, progressively introduced during training. This reduces the abduction space throughout training and enables the model to incorporate logic in a stepwise, smooth way. Experiments across multiple tasks show that C-ABL outperforms previous ABL implementations, significantly improves training stability, convergence speed, and final accuracy, especially under complex knowledge setting.",True,True,"Bengio, Yoshua and Louradour, J\'{e}r\^{o}me and Collobert, Ronan and Weston, Jason",2009.0,,,,
Fair Bayesian Data Selection via Generalized Discrepancy Measures,2511.07032v1,online_batch,\cite{online_batch},Online Batch Selection for Faster Training of Neural Networks,,,True,False,"Ilya Loshchilov and
               Frank Hutter",2015.0,,,,CoRR
Fair Bayesian Data Selection via Generalized Discrepancy Measures,2511.07032v1,importance_sampling,\cite{importance_sampling},Not All Samples Are Created Equal: Deep Learning with Importance Sampling,,,True,False,"Katharopoulos, Angelos and Fleuret, Francois",2018.0,10--15 Jul,,,
Fair Bayesian Data Selection via Generalized Discrepancy Measures,2511.07032v1,accelerating_deep_learning,\cite{accelerating_deep_learning},Accelerating Deep Learning by Focusing on the Biggest Losers,,,True,False,"Angela H. Jiang and
               Daniel L.{-}K. Wong and
               Giulio Zhou and
               David G. Andersen and
               Jeffrey Dean and
               Gregory R. Ganger and
               Gauri Joshi and
               Michael Kaminsky and
               Michael Kozuch and
               Zachary C. Lipton and
               Padmanabhan Pillai",2019.0,,,,CoRR
Fair Bayesian Data Selection via Generalized Discrepancy Measures,2511.07032v1,LongReMix,\cite{LongReMix},LongReMix: Robust learning with high confidence samples in a noisy label environment,,,True,False,Filipe R. Cordeiro and Ragav Sachdeva and Vasileios Belagiannis and Ian Reid and Gustavo Carneiro,2023.0,,,https://doi.org/10.1016/j.patcog.2022.109013,Pattern Recognition
Fair Bayesian Data Selection via Generalized Discrepancy Measures,2511.07032v1,confidence_score_idn,\cite{confidence_score_idn},Confidence Scores Make Instance-dependent Label-noise Learning Possible,https://arxiv.org/abs/2001.03772v2,"In learning with noisy labels, for every instance, its label can randomly walk to other classes following a transition distribution which is named a noise model. Well-studied noise models are all instance-independent, namely, the transition depends only on the original label but not the instance itself, and thus they are less practical in the wild. Fortunately, methods based on instance-dependent noise have been studied, but most of them have to rely on strong assumptions on the noise models. To alleviate this issue, we introduce confidence-scored instance-dependent noise (CSIDN), where each instance-label pair is equipped with a confidence score. We find with the help of confidence scores, the transition distribution of each instance can be approximately estimated. Similarly to the powerful forward correction for instance-independent noise, we propose a novel instance-level forward correction for CSIDN. We demonstrate the utility and effectiveness of our method through multiple experiments under synthetic label noise and real-world unknown noise.",True,True,"Berthon, Antonin and Han, Bo and Niu, Gang and Liu, Tongliang and Sugiyama, Masashi",2021.0,18--24 Jul,https://proceedings.mlr.press/v139/berthon21a.html,,
Fair Bayesian Data Selection via Generalized Discrepancy Measures,2511.07032v1,Roh0WS21_fairbatch,\cite{Roh0WS21_fairbatch},FairBatch: Batch Selection for Model Fairness,,,True,False,"Yuji Roh and
               Kangwook Lee and
               Steven Euijong Whang and
               Changho Suh",2021.0,,,,
Fair Bayesian Data Selection via Generalized Discrepancy Measures,2511.07032v1,2013_iclr_ae,\cite{2013_iclr_ae},{The Variational Fair Autoencoder},,,True,False,"{Louizos}, Christos and {Swersky}, Kevin and {Li}, Yujia and
         {Welling}, Max and {Zemel}, Richard",2015.0,Nov,,,arXiv e-prints
Fair Bayesian Data Selection via Generalized Discrepancy Measures,2511.07032v1,icml_2013,\cite{icml_2013},Learning fair representations,,,True,False,"Zemel, Rich and Wu, Yu and Swersky, Kevin and Pitassi, Toni and Dwork, Cynthia",2013.0,,,,
Fair Bayesian Data Selection via Generalized Discrepancy Measures,2511.07032v1,2016_lum,\cite{2016_lum},{A statistical framework for fair predictive algorithms},,,True,False,"{Lum}, Kristian and {Johndrow}, James",2016.0,Oct,,,arXiv e-prints
Fair Bayesian Data Selection via Generalized Discrepancy Measures,2511.07032v1,DBLP:flex,\cite{DBLP:flex},Flexibly Fair Representation Learning by Disentanglement,https://arxiv.org/abs/1906.02589v1,"We consider the problem of learning representations that achieve group and subgroup fairness with respect to multiple sensitive attributes. Taking inspiration from the disentangled representation learning literature, we propose an algorithm for learning compact representations of datasets that are useful for reconstruction and prediction, but are also \emph{flexibly fair}, meaning they can be easily modified at test time to achieve subgroup demographic parity with respect to multiple sensitive attributes and their conjunctions. We show empirically that the resulting encoder---which does not require the sensitive attributes for inference---enables the adaptation of a single representation to a variety of fair classification tasks with new target labels and subgroup definitions.",True,True,"Elliot Creager and
               David Madras and
               J{\""{o}}rn{-}Henrik Jacobsen and
               Marissa A. Weis and
               Kevin Swersky and
               Toniann Pitassi and
               Richard S. Zemel",2019.0,,,,CoRR
Fair Bayesian Data Selection via Generalized Discrepancy Measures,2511.07032v1,Jang_Zheng_Wang_2021,\cite{Jang_Zheng_Wang_2021},Constructing a Fair Classifier with Generated Fair Data,,,True,False,"Jang, Taeuk and Zheng, Feng and Wang, Xiaoqian",2021.0,May,,10.1609/aaai.v35i9.16965,Proceedings of the AAAI Conference on Artificial Intelligence
Fair Bayesian Data Selection via Generalized Discrepancy Measures,2511.07032v1,NIPS2017_optimised_preprocessing,\cite{NIPS2017_optimised_preprocessing},Optimized Pre-Processing for Discrimination Prevention,,,True,False,"Calmon, Flavio and Wei, Dennis and Vinzamuri, Bhanukiran and Natesan Ramamurthy, Karthikeyan and Varshney, Kush R",2017.0,,,,
Fair Bayesian Data Selection via Generalized Discrepancy Measures,2511.07032v1,pmlr-v119-roh20a,\cite{pmlr-v119-roh20a},FR-Train: A Mutual Information-Based Approach to Fair and Robust Training,https://arxiv.org/abs/2002.10234v2,"Trustworthy AI is a critical issue in machine learning where, in addition to training a model that is accurate, one must consider both fair and robust training in the presence of data bias and poisoning. However, the existing model fairness techniques mistakenly view poisoned data as an additional bias to be fixed, resulting in severe performance degradation. To address this problem, we propose FR-Train, which holistically performs fair and robust model training. We provide a mutual information-based interpretation of an existing adversarial training-based fairness-only method, and apply this idea to architect an additional discriminator that can identify poisoned data using a clean validation set and reduce its influence. In our experiments, FR-Train shows almost no decrease in fairness and accuracy in the presence of data poisoning by both mitigating the bias and defending against poisoning. We also demonstrate how to construct clean validation sets using crowdsourcing, and release new benchmark datasets.",True,True,"Roh, Yuji and Lee, Kangwook and Whang, Steven and Suh, Changho",2020.0,13--18 Jul,,,
Fair Bayesian Data Selection via Generalized Discrepancy Measures,2511.07032v1,baharlouei2024fferm,\cite{baharlouei2024fferm},f-{FERM}: A  Scalable Framework for  Robust Fair Empirical Risk Minimization,,,True,False,Sina Baharlouei and Shivam Patel and Meisam Razaviyayn,2024.0,,,,
Fair Bayesian Data Selection via Generalized Discrepancy Measures,2511.07032v1,donini2020empirical,\cite{donini2020empirical},Empirical Risk Minimization under Fairness Constraints,https://arxiv.org/abs/1802.08626v3,"We address the problem of algorithmic fairness: ensuring that sensitive variables do not unfairly influence the outcome of a classifier. We present an approach based on empirical risk minimization, which incorporates a fairness constraint into the learning problem. It encourages the conditional risk of the learned classifier to be approximately constant with respect to the sensitive variable. We derive both risk and fairness bounds that support the statistical consistency of our approach. We specify our approach to kernel methods and observe that the fairness requirement implies an orthogonality constraint which can be easily added to these methods. We further observe that for linear models the constraint translates into a simple data preprocessing step. Experiments indicate that the method is empirically effective and performs favorably against state-of-the-art approaches.",True,True,"Donini, Michele and Oneto, Luca and Ben-David, Shai and Shawe-Taylor, John and Pontil, Massimiliano",2018.0,,,,
Fair Bayesian Data Selection via Generalized Discrepancy Measures,2511.07032v1,pmlr-v97-gordaliza19a,\cite{pmlr-v97-gordaliza19a},Obtaining fairness using optimal transport theory,https://arxiv.org/abs/1806.03195v2,"Statistical algorithms are usually helping in making decisions in many aspects of our lives. But, how do we know if these algorithms are biased and commit unfair discrimination of a particular group of people, typically a minority? \textit{Fairness} is generally studied in a probabilistic framework where it is assumed that there exists a protected variable, whose use as an input of the algorithm may imply discrimination. There are different definitions of Fairness in the literature. In this paper we focus on two of them which are called Disparate Impact (DI) and Balanced Error Rate (BER). Both are based on the outcome of the algorithm across the different groups determined by the protected variable. The relationship between these two notions is also studied. The goals of this paper are to detect when a binary classification rule lacks fairness and to try to fight against the potential discrimination attributable to it. This can be done by modifying either the classifiers or the data itself. Our work falls into the second category and modifies the input data using optimal transport theory.",True,True,"Gordaliza, Paula and Barrio, Eustasio Del and Fabrice, Gamboa and Loubes, Jean-Michel",2019.0,09--15 Jun,,,
Fair Bayesian Data Selection via Generalized Discrepancy Measures,2511.07032v1,conf/aaai/ChiappaJSPJA20,\cite{conf/aaai/ChiappaJSPJA20},A General Approach to Fairness with Optimal Transport.,,,True,False,"Chiappa, Silvia and Jiang, Ray and Stepleton, Tom and Pacchiano, Aldo and Jiang, Heinrich and Aslanides, John",,,,,
Fair Bayesian Data Selection via Generalized Discrepancy Measures,2511.07032v1,zhang_adversarial,\cite{zhang_adversarial},Mitigating Unwanted Biases with Adversarial Learning,,,True,False,"Zhang, Brian Hu and Lemoine, Blake and Mitchell, Margaret",2018.0,,,10.1145/3278721.3278779,
Fair Bayesian Data Selection via Generalized Discrepancy Measures,2511.07032v1,fair_constraints,\cite{fair_constraints},{Fairness Constraints: Mechanisms for Fair Classification},,,True,False,"{Bilal Zafar}, Muhammad and {Valera}, Isabel and
         {Gomez Rodriguez}, Manuel and {Gummadi}, Krishna P.",2015.0,Jul,,,arXiv e-prints
Fair Bayesian Data Selection via Generalized Discrepancy Measures,2511.07032v1,5360534,\cite{5360534},Building Classifiers with Independency Constraints,,,True,False,T. {Calders} and F. {Kamiran} and M. {Pechenizkiy},2009.0,Dec,,10.1109/ICDMW.2009.83,
Fair Bayesian Data Selection via Generalized Discrepancy Measures,2511.07032v1,2018_icml_reductions,\cite{2018_icml_reductions},A Reductions Approach to Fair Classification,https://arxiv.org/abs/1803.02453v3,"We present a systematic approach for achieving fairness in a binary classification setting. While we focus on two well-known quantitative definitions of fairness, our approach encompasses many other previously studied definitions as special cases. The key idea is to reduce fair classification to a sequence of cost-sensitive classification problems, whose solutions yield a randomized classifier with the lowest (empirical) error subject to the desired constraints. We introduce two reductions that work for any representation of the cost-sensitive classifier and compare favorably to prior baselines on a variety of data sets, while overcoming several of their disadvantages.",True,True,"Agarwal, Alekh and Beygelzimer, Alina and Dudik, Miroslav and Langford, John and Wallach, Hanna",2018.0,10--15 Jul,,,
Fair Bayesian Data Selection via Generalized Discrepancy Measures,2511.07032v1,lowy2022a,\cite{lowy2022a},A Stochastic Optimization Framework for Fair Risk Minimization,,,True,False,Andrew Lowy and Sina Baharlouei and Rakesh Pavan and Meisam Razaviyayn and Ahmad Beirami,2022.0,,,,Transactions on Machine Learning Research
Fair Bayesian Data Selection via Generalized Discrepancy Measures,2511.07032v1,equal_opportunity,\cite{equal_opportunity},Equality of Opportunity in Supervised Learning,https://arxiv.org/abs/1610.02413v1,"We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy.
  In line with other studies, our notion is oblivious: it depends only on the joint statistics of the predictor, the target and the protected attribute, but not on interpretation of individualfeatures. We study the inherent limits of defining and identifying biases based on such oblivious measures, outlining what can and cannot be inferred from different oblivious tests.
  We illustrate our notion using a case study of FICO credit scores.",True,True,"Moritz Hardt and
               Eric Price and
               Nathan Srebro",2016.0,,,,CoRR
Fair Bayesian Data Selection via Generalized Discrepancy Measures,2511.07032v1,fairness_through_aleatoric,\cite{fairness_through_aleatoric},Fairness through Aleatoric Uncertainty,https://arxiv.org/abs/2304.03646v2,"We propose a simple yet effective solution to tackle the often-competing goals of fairness and utility in classification tasks. While fairness ensures that the model's predictions are unbiased and do not discriminate against any particular group or individual, utility focuses on maximizing the model's predictive performance. This work introduces the idea of leveraging aleatoric uncertainty (e.g., data ambiguity) to improve the fairness-utility trade-off. Our central hypothesis is that aleatoric uncertainty is a key factor for algorithmic fairness and samples with low aleatoric uncertainty are modeled more accurately and fairly than those with high aleatoric uncertainty. We then propose a principled model to improve fairness when aleatoric uncertainty is high and improve utility elsewhere. Our approach first intervenes in the data distribution to better decouple aleatoric uncertainty and epistemic uncertainty. It then introduces a fairness-utility bi-objective loss defined based on the estimated aleatoric uncertainty. Our approach is theoretically guaranteed to improve the fairness-utility trade-off. Experimental results on both tabular and image datasets show that the proposed approach outperforms state-of-the-art methods w.r.t. the fairness-utility trade-off and w.r.t. both group and individual fairness metrics. This work presents a fresh perspective on the trade-off between utility and algorithmic fairness and opens a key avenue for the potential of using prediction uncertainty in fair machine learning.",True,True,"Tahir, Anique and Cheng, Lu and Liu, Huan",2023.0,,https://doi.org/10.1145/3583780.3614875,10.1145/3583780.3614875,
Probably Approximately Global Robustness Certification,2511.06495v1,Szegedy_intriguing_properties_robustness_first_paper,\cite{Szegedy_intriguing_properties_robustness_first_paper},Intriguing properties of neural networks,https://arxiv.org/abs/1312.6199v4,"Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.
  First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks.
  Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",True,True,"Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob",2014.0,,,,
Probably Approximately Global Robustness Certification,2511.06495v1,Goodfellow_harnessing_adversarial_examples_FGSM,\cite{Goodfellow_harnessing_adversarial_examples_FGSM},Explaining and Harnessing Adversarial Examples,,,True,False,"Ian J. Goodfellow and
                  Jonathon Shlens and
                  Christian Szegedy",2015.0,,,,
Probably Approximately Global Robustness Certification,2511.06495v1,madry_resistant_to_adversarial_attacks_PGD,\cite{madry_resistant_to_adversarial_attacks_PGD},Towards Deep Learning Models Resistant to Adversarial Attacks,https://arxiv.org/abs/1706.06083v4,"Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist_challenge and https://github.com/MadryLab/cifar10_challenge.",True,True,Aleksander Madry and Aleksandar Makelov and Ludwig Schmidt and Dimitris Tsipras and Adrian Vladu,2018.0,,,,
Probably Approximately Global Robustness Certification,2511.06495v1,CW_attack_carlini_and_wagner,\cite{CW_attack_carlini_and_wagner},Towards Evaluating the Robustness of Neural Networks,,,True,False,"Carlini, Nicholas and Wagner, David",2017.0,,,,
Probably Approximately Global Robustness Certification,2511.06495v1,webb_assessing_neural_network_robustness,\cite{webb_assessing_neural_network_robustness},A Statistical Approach to Assessing Neural Network Robustness,https://arxiv.org/abs/1811.07209v4,"We present a new approach to assessing the robustness of neural networks based on estimating the proportion of inputs for which a property is violated. Specifically, we estimate the probability of the event that the property is violated under an input model. Our approach critically varies from the formal verification framework in that when the property can be violated, it provides an informative notion of how robust the network is, rather than just the conventional assertion that the network is not verifiable. Furthermore, it provides an ability to scale to larger networks than formal verification approaches. Though the framework still provides a formal guarantee of satisfiability whenever it successfully finds one or more violations, these advantages do come at the cost of only providing a statistical estimate of unsatisfiability whenever no violation is found. Key to the practical success of our approach is an adaptation of multi-level splitting, a Monte Carlo approach for estimating the probability of rare events, to our statistical robustness framework. We demonstrate that our approach is able to emulate formal verification procedures on benchmark problems, while scaling to larger networks and providing reliable additional information in the form of accurate estimates of the violation probability.",True,True,"Stefan Webb and
                  Tom Rainforth and
                  Yee Whye Teh and
                  M. Pawan Kumar",2019.0,,,,
Probably Approximately Global Robustness Certification,2511.06495v1,Baluta_Kuldeep_Scalable_Quantitative_Verification_chernoff,\cite{Baluta_Kuldeep_Scalable_Quantitative_Verification_chernoff},Scalable Quantitative Verification for Deep Neural Networks,,,True,False,"Baluta, Teodora and Chua, Zheng Leong and Meel, Kuldeep S. and Saxena, Prateek",2021.0,,,,
Probably Approximately Global Robustness Certification,2511.06495v1,kim_robust_generalization_MAIR,\cite{kim_robust_generalization_MAIR},Fantastic Robustness Measures: The Secrets of Robust Generalization,,,True,False,Hoki Kim and Jinseong Park and Yujin Choi and Jaewook Lee,2023.0,,,,
Probably Approximately Global Robustness Certification,2511.06495v1,li2023sok,\cite{li2023sok},{SoK: Certified Robustness for Deep Neural Networks},,,True,False,"Li, Linyi and Xie, Tao and Li, Bo",2023.0,,,,
Probably Approximately Global Robustness Certification,2511.06495v1,marabou_guy_katz,\cite{marabou_guy_katz},The Marabou Framework for Verification and Analysis of Deep Neural Networks,,,True,False,"Katz, Guy
and Huang, Derek A.
and Ibeling, Duligur
and Julian, Kyle
and Lazarus, Christopher
and Lim, Rachel
and Shah, Parth
and Thakoor, Shantanu
and Wu, Haoze
and Zelji{\'{c}}, Aleksandar
and Dill, David L.
and Kochenderfer, Mykel J.
and Barrett, Clark",2019.0,,,,
Probably Approximately Global Robustness Certification,2511.06495v1,chen2021learning,\cite{chen2021learning},Learning Security Classifiers with Verified Global Robustness Properties,,,True,False,"Chen, Yizheng and Wang, Shiqi and Qin, Yue and Liao, Xiaojing and Jana, Suman and Wagner, David",2021.0,,,,IEEE Transactions on Dependable and Secure Computing
Probably Approximately Global Robustness Certification,2511.06495v1,xu2020automatic,\cite{xu2020automatic},Automatic perturbation analysis for scalable certified robustness and beyond,,,True,False,"Xu, Kaidi and Shi, Zhouxing and Zhang, Huan and Wang, Yihan and Chang, Kai-Wei and Huang, Minlie and Kailkhura, Bhavya and Lin, Xue and Hsieh, Cho-Jui",2020.0,,,,Advances in Neural Information Processing Systems
Probably Approximately Global Robustness Certification,2511.06495v1,casadio2022neural,\cite{casadio2022neural},Neural Network Robustness as a Verification Property: A Principled Case Study,https://arxiv.org/abs/2104.01396v2,"Neural networks are very successful at detecting patterns in noisy data, and have become the technology of choice in many fields. However, their usefulness is hampered by their susceptibility to adversarial attacks. Recently, many methods for measuring and improving a network's robustness to adversarial perturbations have been proposed, and this growing body of research has given rise to numerous explicit or implicit notions of robustness. Connections between these notions are often subtle, and a systematic comparison between them is missing in the literature. In this paper we begin addressing this gap, by setting up general principles for the empirical analysis and evaluation of a network's robustness as a mathematical property - during the network's training phase, its verification, and after its deployment. We then apply these principles and conduct a case study that showcases the practical benefits of our general approach.",True,True,"Casadio, Marco and Komendantskaya, Ekaterina and Daggitt, Matthew L and Kokke, Wen and Katz, Guy and Amir, Guy and Refaeli, Idan",2022.0,,,,
Probably Approximately Global Robustness Certification,2511.06495v1,formal_verification_robustness_survey_meng,\cite{formal_verification_robustness_survey_meng},Adversarial Robustness of Deep Neural Networks: A Survey from a Formal Verification Perspective,,,True,False,"Meng, Mark Huasong and Bai, Guangdong and Teo, Sin Gee and Hou, Zhe and Xiao, Yan and Lin, Yun and Dong, Jin Song",2022.0,,,,IEEE Transactions on Dependable and Secure Computing
Probably Approximately Global Robustness Certification,2511.06495v1,katz2017reluplex,\cite{katz2017reluplex},Reluplex: An efficient SMT solver for verifying deep neural networks,,,True,False,"Katz, Guy and Barrett, Clark and Dill, David L and Julian, Kyle and Kochenderfer, Mykel J",2017.0,,,,
Probably Approximately Global Robustness Certification,2511.06495v1,carlini_wagner_bypassing_adversarial_detection_methods,\cite{carlini_wagner_bypassing_adversarial_detection_methods},Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods,,,True,False,"Carlini, Nicholas and Wagner, David",2017.0,,,,
Probably Approximately Global Robustness Certification,2511.06495v1,wu2020game,\cite{wu2020game},A Game-Based Approximate Verification of Deep Neural Networks with Provable Guarantees,https://arxiv.org/abs/1807.03571v2,"Despite the improved accuracy of deep neural networks, the discovery of adversarial examples has raised serious safety concerns. In this paper, we study two variants of pointwise robustness, the maximum safe radius problem, which for a given input sample computes the minimum distance to an adversarial example, and the feature robustness problem, which aims to quantify the robustness of individual features to adversarial perturbations. We demonstrate that, under the assumption of Lipschitz continuity, both problems can be approximated using finite optimisation by discretising the input space, and the approximation has provable guarantees, i.e., the error is bounded. We then show that the resulting optimisation problems can be reduced to the solution of two-player turn-based games, where the first player selects features and the second perturbs the image within the feature. While the second player aims to minimise the distance to an adversarial example, depending on the optimisation objective the first player can be cooperative or competitive. We employ an anytime approach to solve the games, in the sense of approximating the value of a game by monotonically improving its upper and lower bounds. The Monte Carlo tree search algorithm is applied to compute upper bounds for both games, and the Admissible A* and the Alpha-Beta Pruning algorithms are, respectively, used to compute lower bounds for the maximum safety radius and feature robustness games. When working on the upper bound of the maximum safe radius problem, our tool demonstrates competitive performance against existing adversarial example crafting algorithms. Furthermore, we show how our framework can be deployed to evaluate pointwise robustness of neural networks in safety-critical applications such as traffic sign recognition in self-driving cars.",True,True,Min Wu and Matthew Wicker and Wenjie Ruan and Xiaowei Huang and Marta Kwiatkowska,2020.0,,,,Theoretical Computer Science
Probably Approximately Global Robustness Certification,2511.06495v1,cohen2019certified,\cite{cohen2019certified},Certified Adversarial Robustness via Randomized Smoothing,https://arxiv.org/abs/1902.02918v2,"We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the $\ell_2$ norm. This ""randomized smoothing"" technique has been proposed recently in the literature, but existing guarantees are loose. We prove a tight robustness guarantee in $\ell_2$ norm for smoothing with Gaussian noise. We use randomized smoothing to obtain an ImageNet classifier with e.g. a certified top-1 accuracy of 49% under adversarial perturbations with $\ell_2$ norm less than 0.5 (=127/255). No certified defense has been shown feasible on ImageNet except for smoothing. On smaller-scale datasets where competing approaches to certified $\ell_2$ robustness are viable, smoothing delivers higher certified accuracies. Our strong empirical results suggest that randomized smoothing is a promising direction for future research into adversarially robust classification. Code and models are available at http://github.com/locuslab/smoothing.",True,True,"Cohen, Jeremy and Rosenfeld, Elan and Kolter, Zico",2019.0,,,,
Probably Approximately Global Robustness Certification,2511.06495v1,robustness_certification_with_differential_privacy_lecuyer,\cite{robustness_certification_with_differential_privacy_lecuyer},Certified Robustness to Adversarial Examples with Differential Privacy,,,True,False,"Lecuyer, Mathias and Atlidakis, Vaggelis and Geambasu, Roxana and Hsu, Daniel and Jana, Suman",2019.0,,,,
Probably Approximately Global Robustness Certification,2511.06495v1,certifiable_distributional_robustness_distributional,\cite{certifiable_distributional_robustness_distributional},Certifiable Distributional Robustness with Principled Adversarial Training,,,True,False,Aman Sinha and Hongseok Namkoong and John Duchi,2018.0,,,,stat
Probably Approximately Global Robustness Certification,2511.06495v1,mixtrain_wang_training_verifiable,\cite{mixtrain_wang_training_verifiable},MixTrain: Scalable Training of Verifiably Robust Neural Networks,https://arxiv.org/abs/1811.02625v2,"Making neural networks robust against adversarial inputs has resulted in an arms race between new defenses and attacks. The most promising defenses, adversarially robust training and verifiably robust training, have limitations that restrict their practical applications. The adversarially robust training only makes the networks robust against a subclass of attackers and we reveal such weaknesses by developing a new attack based on interval gradients. By contrast, verifiably robust training provides protection against any L-p norm-bounded attacker but incurs orders of magnitude more computational and memory overhead than adversarially robust training.
  We propose two novel techniques, stochastic robust approximation and dynamic mixed training, to drastically improve the efficiency of verifiably robust training without sacrificing verified robustness. We leverage two critical insights: (1) instead of over the entire training set, sound over-approximations over randomly subsampled training data points are sufficient for efficiently guiding the robust training process; and (2) We observe that the test accuracy and verifiable robustness often conflict after certain training epochs. Therefore, we use a dynamic loss function to adaptively balance them for each epoch.
  We designed and implemented our techniques as part of MixTrain and evaluated it on six networks trained on three popular datasets including MNIST, CIFAR, and ImageNet-200. Our evaluations show that MixTrain can achieve up to $95.2\%$ verified robust accuracy against $L_\infty$ norm-bounded attackers while taking $15$ and $3$ times less training time than state-of-the-art verifiably robust training and adversarially robust training schemes, respectively. Furthermore, MixTrain easily scales to larger networks like the one trained on ImageNet-200, significantly outperforming the existing verifiably robust training methods.",True,True,"Wang, Shiqi and Chen, Yizheng and Abdou, Ahmed and Jana, Suman",2018.0,,,,arXiv preprint arXiv:1811.02625
Probably Approximately Global Robustness Certification,2511.06495v1,athavale2024verifying,\cite{athavale2024verifying},Verifying Global Two-Safety Properties in Neural Networks with Confidence,https://arxiv.org/abs/2405.14400v3,"We present the first automated verification technique for confidence-based 2-safety properties, such as global robustness and global fairness, in deep neural networks (DNNs). Our approach combines self-composition to leverage existing reachability analysis techniques and a novel abstraction of the softmax function, which is amenable to automated verification. We characterize and prove the soundness of our static analysis technique. Furthermore, we implement it on top of Marabou, a safety analysis tool for neural networks, conducting a performance evaluation on several publicly available benchmarks for DNN verification.",True,True,"Athavale, Anagha and Bartocci, Ezio and Christakis, Maria and Maffei, Matteo and Nickovic, Dejan and Weissenbacher, Georg",2024.0,,,,
Probably Approximately Global Robustness Certification,2511.06495v1,indri2024distillation,\cite{indri2024distillation},Distillation based Robustness Verification with PAC Guarantees,,,True,False,"Indri, Patrick and Blohm, Peter and Athavale, Anagha and Bartocci, Ezio and Weissenbacher, Georg and Maffei, Matteo and Nickovic, Dejan and G{\""a}rtner, Thomas and Malhotra, Sagar",2024.0,,,,
Probably Approximately Global Robustness Certification,2511.06495v1,kabaha2024verification,\cite{kabaha2024verification},Verification of Neural Networks' Global Robustness,https://arxiv.org/abs/2402.19322v2,"Neural networks are successful in various applications but are also susceptible to adversarial attacks. To show the safety of network classifiers, many verifiers have been introduced to reason about the local robustness of a given input to a given perturbation. While successful, local robustness cannot generalize to unseen inputs. Several works analyze global robustness properties, however, neither can provide a precise guarantee about the cases where a network classifier does not change its classification. In this work, we propose a new global robustness property for classifiers aiming at finding the minimal globally robust bound, which naturally extends the popular local robustness property for classifiers. We introduce VHAGaR, an anytime verifier for computing this bound. VHAGaR relies on three main ideas: encoding the problem as a mixed-integer programming and pruning the search space by identifying dependencies stemming from the perturbation or the network's computation and generalizing adversarial attacks to unknown inputs. We evaluate VHAGaR on several datasets and classifiers and show that, given a three hour timeout, the average gap between the lower and upper bound on the minimal globally robust bound computed by VHAGaR is 1.9, while the gap of an existing global robustness verifier is 154.7. Moreover, VHAGaR is 130.6x faster than this verifier. Our results further indicate that leveraging dependencies and adversarial attacks makes VHAGaR 78.6x faster.",True,True,"Kabaha, Anan and Cohen, Dana Drachsler",2024.0,,,,Proceedings of the ACM on Programming Languages
Probably Approximately Global Robustness Certification,2511.06495v1,haussler1986epsilon,\cite{haussler1986epsilon},Epsilon-nets and simplex range queries,,,True,False,"Haussler, D and Welzl, E",1986.0,,,,
Masked Mineral Modeling: Continent-Scale Mineral Prospecting via Geospatial Infilling,2511.09722v1,yousefi2021data,\cite{yousefi2021data},Data analysis methods for prospectivity modelling as applied to mineral exploration targeting: State-of-the-art and outlook,,,True,False,"Yousefi, Mahyar and Carranza, Emmanuel John M and Kreuzer, Oliver P and Nyk{\""a}nen, Vesa and Hronsky, Jon MA and Mihalasky, Mark J",2021.0,,,,Journal of Geochemical Exploration
Masked Mineral Modeling: Continent-Scale Mineral Prospecting via Geospatial Infilling,2511.09722v1,chen2024hyperspectral,\cite{chen2024hyperspectral},Hyperspectral Remote Sensing Inversion of Mineral Abundance Based on Sparse Unmixing Method,,,True,False,"Chen, Weitao and Li, Xianju and Qin, Xuwen and Wang, Lizhe",2024.0,,,,
Masked Mineral Modeling: Continent-Scale Mineral Prospecting via Geospatial Infilling,2511.09722v1,franks2023mineral,\cite{franks2023mineral},Mineral security essential to achieving the Sustainable Development Goals,,,True,False,"Franks, Daniel M and Keenan, Julia and Hailu, Degol",2023.0,,,,Nature Sustainability
Masked Mineral Modeling: Continent-Scale Mineral Prospecting via Geospatial Infilling,2511.09722v1,misra2019machine,\cite{misra2019machine},Machine learning for subsurface characterization,,,True,False,"Misra, Siddharth and Li, Hao and He, Jiabo",2019.0,,,,
Masked Mineral Modeling: Continent-Scale Mineral Prospecting via Geospatial Infilling,2511.09722v1,vohra2024automated,\cite{vohra2024automated},Automated Underground Mapping of Buried Utilities: A Review of Robotic Solutions and Sensor Technologies,,,True,False,"Vohra, Mohit and Gupta, Ayush and Umair, Mian Muhammad and Shukla, Amit and Karunamurthy, Jayakumar Vandavasi and Gupta, Aditi",2024.0,,,,
Masked Mineral Modeling: Continent-Scale Mineral Prospecting via Geospatial Infilling,2511.09722v1,liu2020petrophysical,\cite{liu2020petrophysical},Petrophysical characterization of deep saline aquifers for CO2 storage using ensemble smoother and deep convolutional autoencoder,,,True,False,"Liu, Mingliang and Grana, Dario",2020.0,,,,Advances in Water Resources
Masked Mineral Modeling: Continent-Scale Mineral Prospecting via Geospatial Infilling,2511.09722v1,narayan2024machine,\cite{narayan2024machine},"Machine learning assisted reservoir characterization for CO2 sequestration: A case study from the Penobscot field, Canada offshore",,,True,False,"Narayan, Satya and Kumar, Vijay and Mukherjee, Bappa and Sahoo, SD and Pal, SK",2024.0,,,,Marine and Petroleum Geology
Masked Mineral Modeling: Continent-Scale Mineral Prospecting via Geospatial Infilling,2511.09722v1,wang2024machine,\cite{wang2024machine},Machine learning and deep learning for mineralogy interpretation and CO2 saturation estimation in geological carbon Storage: A case study in the Illinois Basin,,,True,False,"Wang, Hongsheng and Williams-Stroud, Sherilyn and Crandall, Dustin and Chen, Cheng",2024.0,,,,Fuel
Masked Mineral Modeling: Continent-Scale Mineral Prospecting via Geospatial Infilling,2511.09722v1,wang2025machine,\cite{wang2025machine},Machine and deep learning-based prediction of potential geothermal areas in Hangjiahu Plain by integrating remote sensing data and GIS,,,True,False,"Wang, Yuhan and Zhang, Xuan and Qian, Junfeng and Li, Xiang and Liu, Yangui and Wu, Wenyuan and Lu, Zhe and Xie, Bin",2025.0,,,,Energy
Masked Mineral Modeling: Continent-Scale Mineral Prospecting via Geospatial Infilling,2511.09722v1,dong2024fusion,\cite{dong2024fusion},Fusion of GaoFen-5 and Sentinel-2B data for lithological mapping using vision transformer dynamic graph convolutional network,,,True,False,"Dong, Yanni and Yang, Zhenzhen and Liu, Quanwei and Zuo, Renguang and Wang, Ziye",2024.0,,,,International Journal of Applied Earth Observation and Geoinformation
Masked Mineral Modeling: Continent-Scale Mineral Prospecting via Geospatial Infilling,2511.09722v1,jiang2024estimation,\cite{jiang2024estimation},Estimation of lithium content in rock debris based on spectral feature coefficients,,,True,False,"Jiang, Guo and Chen, Xi and Zhou, Kefa and Wang, Jinlin and Zhou, Shuguang and Bai, Yong",2024.0,,,,Ore Geology Reviews
Masked Mineral Modeling: Continent-Scale Mineral Prospecting via Geospatial Infilling,2511.09722v1,dumakor2021machine,\cite{dumakor2021machine},Machine learning—a review of applications in mineral resource estimation,,,True,False,"Dumakor-Dupey, Nelson K and Arya, Sampurna",2021.0,,,,Energies
Masked Mineral Modeling: Continent-Scale Mineral Prospecting via Geospatial Infilling,2511.09722v1,hajaj2024review,\cite{hajaj2024review},A review on hyperspectral imagery application for lithological mapping and mineral prospecting: Machine learning techniques and future prospects,,,True,False,"Hajaj, Soufiane and El Harti, Abderrazak and Pour, Amin Beiranvand and Jellouli, Amine and Adiri, Zakaria and Hashim, Mazlan",2024.0,,,,Remote Sensing Applications: Society and Environment
Masked Mineral Modeling: Continent-Scale Mineral Prospecting via Geospatial Infilling,2511.09722v1,wan2021application,\cite{wan2021application},"Application of hyperspectral remote sensing for supplementary investigation of polymetallic deposits in Huaniushan ore region, northwestern China",,,True,False,"Wan, Yu-qing and Fan, Yu-hai and Jin, Mou-shun",2021.0,,,,Scientific Reports
Masked Mineral Modeling: Continent-Scale Mineral Prospecting via Geospatial Infilling,2511.09722v1,qian2021hyperspectral,\cite{qian2021hyperspectral},"Hyperspectral satellites, evolution, and development history",,,True,False,"Qian, Shen-En",2021.0,,,,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Masked Mineral Modeling: Continent-Scale Mineral Prospecting via Geospatial Infilling,2511.09722v1,cong2022satmae,\cite{cong2022satmae},Satmae: Pre-training transformers for temporal and multi-spectral satellite imagery,,,True,False,"Cong, Yezhen and Khanna, Samar and Meng, Chenlin and Liu, Patrick and Rozi, Erik and He, Yutong and Burke, Marshall and Lobell, David and Ermon, Stefano",2022.0,,,,Advances in Neural Information Processing Systems
Masked Mineral Modeling: Continent-Scale Mineral Prospecting via Geospatial Infilling,2511.09722v1,reed2023scale,\cite{reed2023scale},Scale-MAE: A Scale-Aware Masked Autoencoder for Multiscale Geospatial Representation Learning,https://arxiv.org/abs/2212.14532v4,"Large, pretrained models are commonly finetuned with imagery that is heavily augmented to mimic different conditions and scales, with the resulting models used for various tasks with imagery from a range of spatial scales. Such models overlook scale-specific information in the data for scale-dependent domains, such as remote sensing. In this paper, we present Scale-MAE, a pretraining method that explicitly learns relationships between data at different, known scales throughout the pretraining process. Scale-MAE pretrains a network by masking an input image at a known input scale, where the area of the Earth covered by the image determines the scale of the ViT positional encoding, not the image resolution. Scale-MAE encodes the masked image with a standard ViT backbone, and then decodes the masked image through a bandpass filter to reconstruct low/high frequency images at lower/higher scales. We find that tasking the network with reconstructing both low/high frequency images leads to robust multiscale representations for remote sensing imagery. Scale-MAE achieves an average of a $2.4 - 5.6\%$ non-parametric kNN classification improvement across eight remote sensing datasets compared to current state-of-the-art and obtains a $0.9$ mIoU to $1.7$ mIoU improvement on the SpaceNet building segmentation transfer task for a range of evaluation scales.",True,True,"Reed, Colorado J and Gupta, Ritwik and Li, Shufan and Brockman, Sarah and Funk, Christopher and Clipp, Brian and Keutzer, Kurt and Candido, Salvatore and Uyttendaele, Matt and Darrell, Trevor",2023.0,,,,
Masked Mineral Modeling: Continent-Scale Mineral Prospecting via Geospatial Infilling,2511.09722v1,tang2023cross,\cite{tang2023cross},Cross-scale mae: A tale of multiscale exploitation in remote sensing,,,True,False,"Tang, Maofeng and Cozma, Andrei and Georgiou, Konstantinos and Qi, Hairong",2023.0,,,,Advances in Neural Information Processing Systems
Masked Mineral Modeling: Continent-Scale Mineral Prospecting via Geospatial Infilling,2511.09722v1,he2021mae,\cite{he2021mae},Mae: Masked autoencoders are scalable vision learners,,,True,False,"He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross",2021.0,,,,arXiv preprint arXiv:2111.06377
Masked Mineral Modeling: Continent-Scale Mineral Prospecting via Geospatial Infilling,2511.09722v1,schweitzer2019record,\cite{schweitzer2019record},Record quality tables for the mineral resources data system,,,True,False,"Schweitzer, PN",2019.0,,,,US Geological Survey data release
Air Pollution Forecasting in Bucharest,2511.00532v1,mohammadzadeh2024spatiotemporal,\cite{mohammadzadeh2024spatiotemporal},"Spatiotemporal integration of GCN and E-LSTM networks for PM2.5
               forecasting",,,True,False,"Mohammadzadeh, Ali Kamali and Salah, Halima and Jahanmahin,
               Roohollah and Hussain, Abd E Ali and Masoud, Sara and Huang,
               Yaoxian",2024.0,,,,Machine Learning with Applications
Air Pollution Forecasting in Bucharest,2511.00532v1,DBLP:conf/iclr/KipfW17,\cite{DBLP:conf/iclr/KipfW17},Semi-Supervised Classification with Graph Convolutional Networks,,,True,False,"Thomas N. Kipf and
               Max Welling",2017.0,,https://openreview.net/forum?id=SJU4ayYgl,,
Air Pollution Forecasting in Bucharest,2511.00532v1,hochreiter1997long,\cite{hochreiter1997long},Associative Long Short-Term Memory,https://arxiv.org/abs/1602.03032v2,"We investigate a new method to augment recurrent neural networks with extra memory without increasing the number of network parameters. The system has an associative memory based on complex-valued vectors and is closely related to Holographic Reduced Representations and Long Short-Term Memory networks. Holographic Reduced Representations have limited capacity: as they store more information, each retrieval becomes noisier due to interference. Our system in contrast creates redundant copies of stored information, which enables retrieval with reduced noise. Experiments demonstrate faster learning on multiple memorization tasks.",True,True,"Hochreiter, Sepp and Schmidhuber, J{\""u}rgen",1997.0,,,,Neural computation
Air Pollution Forecasting in Bucharest,2511.00532v1,zhang2019graph,\cite{zhang2019graph},Graph convolutional networks: a comprehensive review,,,True,False,"Zhang, Si and Tong, Hanghang and Xu, Jiejun and Maciejewski, Ross",2019.0,,,,Computational Social Networks
Air Pollution Forecasting in Bucharest,2511.00532v1,zajkeckaautomated,\cite{zajkeckaautomated},"Automated Prediction of Air Pollution Conditions in Environment
               Monitoring Systems",,,True,False,"Dawid Bialka and Malgorzata Zajecka and Ada Brzoza{-}Zajecka and
               Tomasz Pelech{-}Pilichowski",2024.0,,https://doi.org/10.1007/978-3-031-63783-4\_17,10.1007/978-3-031-63783-4\_17,
Air Pollution Forecasting in Bucharest,2511.00532v1,jaeger2007echo,\cite{jaeger2007echo},Echo state network,,,True,False,"Jaeger, Herbert",2007.0,,,,scholarpedia
Air Pollution Forecasting in Bucharest,2511.00532v1,caceres2024analysis,\cite{caceres2024analysis},"Analysis and Prediction of PM2. 5 Pollution in Madrid: The Use of
               Prophet--Long Short-Term Memory Hybrid Models",,,True,False,"C{\'a}ceres-Tello, Jes{\'u}s and Gal{\'a}n-Hern{\'a}ndez, Jos{\'e}
               Javier",2024.0,,,,AppliedMath
Air Pollution Forecasting in Bucharest,2511.00532v1,liu2025deep,\cite{liu2025deep},"A deep learning-based hybrid method for PM2. 5 prediction in
               central and western China",,,True,False,"Liu, Zuhan and Fang, Zihai and Hu, Yuanhao",2025.0,,,,Scientific Reports
Air Pollution Forecasting in Bucharest,2511.00532v1,peng2022machine,\cite{peng2022machine},"Machine learning and deep learning modeling and simulation for
               predicting PM2. 5 concentrations",,,True,False,"Peng, Jian and Han, Haisheng and Yi, Yong and Huang, Huimin and
               Xie, Le",2022.0,,,,Chemosphere
Air Pollution Forecasting in Bucharest,2511.00532v1,bai2019hourly,\cite{bai2019hourly},"Hourly PM2. 5 concentration forecast using stacked autoencoder
               model with emphasis on seasonality",,,True,False,"Bai, Yun and Li, Yong and Zeng, Bo and Li, Chuan and Zhang, Jin",2019.0,,,,Journal of Cleaner Production
Air Pollution Forecasting in Bucharest,2511.00532v1,qin2025sfdformer,\cite{qin2025sfdformer},"SFDformer: a frequency-based sparse decomposition transformer for
               air pollution time series prediction",,,True,False,"Qin, Zhenkai and Wei, Baozhong and Gao, Caifeng and Chen, Xiaolong
               and Zhang, Hongfeng and In Wong, Cora Un",2025.0,,,,Frontiers in Environmental Science
LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows,2511.07585v1,thinkingmachines2025,\cite{thinkingmachines2025},Defeating Nondeterminism in {LLM} Inference,,,True,False,{Thinking Machines Lab},2025.0,,https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/,,
LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows,2511.07585v1,belcak2025slm,\cite{belcak2025slm},Small Language Models are the Future of Agentic {AI},,,True,False,Peter Belcak and others,2025.0,,https://arxiv.org/abs/2506.02153,,arXiv preprint arXiv:2506.02153
LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows,2511.07585v1,shi2024decoding,\cite{shi2024decoding},A Thorough Examination of Decoding Methods in the Era of {LLMs},,,True,False,Chufan Shi and Haoran Yang and Deng Cai and Zhisong Zhang and Yifan Wang and Yujiu Yang and Wai Lam,2024.0,,https://arxiv.org/abs/2402.06925,,arXiv preprint arXiv:2402.06925
LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows,2511.07585v1,antunes2021reproducibility,\cite{antunes2021reproducibility},"Reproducibility, Replicability and Repeatability: A survey of reproducible research with a focus on high performance computing",,,True,False,"Antunes, Bruno and Hill, David R. C.",2021.0,,https://doi.org/10.1016/j.future.2020.11.016,10.1016/j.future.2020.11.016,Future Generation Computer Systems
LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows,2511.07585v1,baldwin2024,\cite{baldwin2024},"Non-Determinism of ""Deterministic"" LLM Settings",https://arxiv.org/abs/2408.04667v5,"LLM (large language model) practitioners commonly notice that outputs can vary for the same inputs under settings expected to be deterministic. Yet the questions of how pervasive this is, and with what impact on results, have not to our knowledge been systematically investigated. We investigate non-determinism in five LLMs configured to be deterministic when applied to eight common tasks in across 10 runs, in both zero-shot and few-shot settings. We see accuracy variations up to 15% across naturally occurring runs with a gap of best possible performance to worst possible performance up to 70%. In fact, none of the LLMs consistently delivers repeatable accuracy across all tasks, much less identical output strings. Sharing preliminary results with insiders has revealed that non-determinism perhaps essential to the efficient use of compute resources via co-mingled data in input buffers so this issue is not going away anytime soon. To better quantify our observations, we introduce metrics focused on quantifying determinism, TARr@N for the total agreement rate at N runs over raw output, and TARa@N for total agreement rate of parsed-out answers. Our code and data are publicly available at https://github.com/breckbaldwin/llm-stability.",True,True,Jacob Baldwin and others,2024.0,,https://arxiv.org/abs/2408.04667,,arXiv preprint arXiv:2408.04667
LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows,2511.07585v1,tokenizer-drift,\cite{tokenizer-drift},When Tokenizers Drift: Hidden Costs and Security Risks in {LLM} Deployments,,,True,False,{Trend Micro Research},2025.0,,https://www.trendmicro.com/vinfo/us/security/news/cybercrime-and-digital-threats/when-tokenizers-drift-hidden-costs-and-security-risks-in-llm-deployments,,
LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows,2511.07585v1,model-drift-crises,\cite{model-drift-crises},Model Drift: How Subtle Shifts in {AI} Responses Could Undermine Crisis Response,,,True,False,{The New Humanitarian},2025.0,,https://www.thenewhumanitarian.org/opinion/2025/10/08/model-drift-how-subtle-shifts-ai-responses-could-undermine-crisis-response,,
LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows,2511.07585v1,llm-observability,\cite{llm-observability},Top 10 {LLM} Observability Tools: Complete Guide for 2025,,,True,False,{Braintrust},2025.0,,https://www.braintrust.dev/articles/top-10-llm-observability-tools-2025,,
LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows,2511.07585v1,kwon2023,\cite{kwon2023},Efficient Memory Management for Large Language Model Serving with PagedAttention,,,True,False,Woosuk Kwon and others,2023.0,,,10.1145/3600006.3613165,
LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows,2511.07585v1,redhat2025,\cite{redhat2025},Run {Qwen3{-}Next} on {vLLM} with Red Hat {AI}: A step-by-step guide,,,True,False,{Red Hat},2025.0,,https://developers.redhat.com/articles/2025/09/12/run-qwen3-next-vllm-red-hat-ai-step-step-guide,,
LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows,2511.07585v1,xie2024finben,\cite{xie2024finben},{FinBen}: A Holistic Financial Benchmark for Large Language Models,,,True,False,Qianqian Xie and others,2024.0,,https://arxiv.org/abs/2402.12659,,
LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows,2511.07585v1,agarwal2024,\cite{agarwal2024},{SEC{-}QA}: A Systematic Evaluation Corpus for Financial {QA},,,True,False,Viet Dac Lai and Mahesh Nadendla and others,2024.0,,https://doi.org/10.48550/arXiv.2406.14394,,arXiv preprint arXiv:2406.14394
LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows,2511.07585v1,zhu2024,\cite{zhu2024},DocFinQA: A Long-Context Financial Reasoning Dataset,https://arxiv.org/abs/2401.06915v3,"For large language models (LLMs) to be effective in the financial domain -- where each decision can have a significant impact -- it is necessary to investigate realistic tasks and data. Financial professionals often interact with documents that are hundreds of pages long, but most financial research datasets only deal with short excerpts from these documents. To address this, we introduce a long-document financial QA task. We augment 7,437 questions from the existing FinQA dataset with the full-document context, extending the average context length from under 700 words in FinQA to 123k words in DocFinQA. We conduct extensive experiments over retrieval-based QA pipelines and long-context language models. DocFinQA proves a significant challenge for even state-of-the-art systems. We also provide a case-study on the longest documents in DocFinQA and find that models particularly struggle on these documents. Addressing these challenges may have a wide reaching impact across applications where specificity and long-range contexts are critical, like gene sequences and legal document contract analysis.",True,True,Varshini Reddy and others,2024.0,,https://aclanthology.org/2024.acl-short.42,10.18653/v1/2024.acl-short.42,
LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows,2511.07585v1,fsb2024,\cite{fsb2024},The Financial Stability Implications of Artificial Intelligence,,,True,False,{Financial Stability Board},2024.0,,https://www.fsb.org/2024/11/the-financial-stability-implications-of-artificial-intelligence/,,
LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows,2511.07585v1,fsb2023ai,\cite{fsb2023ai},Artificial Intelligence and Machine Learning in Financial Services: Market Developments and Financial Stability Implications,,,True,False,{Financial Stability Board},2017.0,,https://www.fsb.org/2017/11/artificial-intelligence-and-machine-learning-in-financial-service/,,
LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows,2511.07585v1,bis2024,\cite{bis2024},Regulating {AI} in the financial sector: recent developments and main challenges,,,True,False,{Bank for International Settlements},2024.0,,https://www.bis.org/fsi/publ/insights63.htm,,
LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows,2511.07585v1,bis2023ml,\cite{bis2023ml},Humans Keeping {AI} in Check — Emerging Regulatory Expectations in the Financial Sector,,,True,False,"Prenio, Jermy and Yong, Jeffery",2021.0,,https://www.bis.org/fsi/publ/insights35.pdf,,
LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows,2511.07585v1,cftc2024,\cite{cftc2024},Staff Letter No. 24-17: Digital Asset and {AI} Guidelines,,,True,False,{CFTC},2024.0,,https://www.cftc.gov/csl/24-17/download,,
LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows,2511.07585v1,cftc2024ai,\cite{cftc2024ai},Artificial Intelligence in Financial Markets,,,True,False,"{Commodity Futures Trading Commission, Technology Advisory Committee}",2024.0,,https://www.cftc.gov/media/10626/TAC_AIReport050224/download,,
LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows,2511.07585v1,fed2024ai,\cite{fed2024ai},Artificial Intelligence Program,,,True,False,{Federal Reserve Board},2024.0,,https://www.federalreserve.gov/ai.htm,,
LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows,2511.07585v1,occ2024ai,\cite{occ2024ai},Interpretive Letter 1183: Superseding Interpretive Letter 1179,,,True,False,{Office of the Comptroller of the Currency},2025.0,,https://www.occ.gov/topics/charters-and-licensing/interpretations-and-actions/2025/int1183.pdf,,
LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows,2511.07585v1,pytorchrepro,\cite{pytorchrepro},{PyTorch} Reproducibility,,,True,False,{PyTorch Team},2025.0,,https://pytorch.org/docs/stable/notes/randomness.html,,
LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows,2511.07585v1,vllmrepro,\cite{vllmrepro},{vLLM} Documentation: Reproducibility and Determinism,,,True,False,{vLLM Team},2025.0,,https://docs.vllm.ai/,,
Boosted GFlowNets: Improving Exploration via Sequential Learning,2511.09677v1,madan2025-sa-gfn,\cite{madan2025-sa-gfn},Towards Improving Exploration through Sibling Augmented {GF}lowNets,,,True,False,Kanika Madan and Alex Lamb and Emmanuel Bengio and Glen Berseth and Yoshua Bengio,2025.0,,https://openreview.net/forum?id=HH4KWP8RP5,,
Boosted GFlowNets: Improving Exploration via Sequential Learning,2511.09677v1,kim2025-adaptive-teacher,\cite{kim2025-adaptive-teacher},Adaptive teachers for amortized samplers,,,True,False,Minsu Kim and Sanghyeok Choi and Taeyoung Yun and Emmanuel Bengio and Leo Feng and Jarrid Rector-Brooks and Sungsoo Ahn and Jinkyoo Park and Nikolay Malkin and Yoshua Bengio,2025.0,,https://arxiv.org/abs/2410.01432,,
Boosted GFlowNets: Improving Exploration via Sequential Learning,2511.09677v1,hu2025-beyond-squared,\cite{hu2025-beyond-squared},Beyond Squared Error: Exploring Loss Design for Enhanced Training of Generative Flow Networks,https://arxiv.org/abs/2410.02596v1,"Generative Flow Networks (GFlowNets) are a novel class of generative models designed to sample from unnormalized distributions and have found applications in various important tasks, attracting great research interest in their training algorithms. In general, GFlowNets are trained by fitting the forward flow to the backward flow on sampled training objects. Prior work focused on the choice of training objects, parameterizations, sampling and resampling strategies, and backward policies, aiming to enhance credit assignment, exploration, or exploitation of the training process. However, the choice of regression loss, which can highly influence the exploration and exploitation behavior of the under-training policy, has been overlooked. Due to the lack of theoretical understanding for choosing an appropriate regression loss, most existing algorithms train the flow network by minimizing the squared error of the forward and backward flows in log-space, i.e., using the quadratic regression loss. In this work, we rigorously prove that distinct regression losses correspond to specific divergence measures, enabling us to design and analyze regression losses according to the desired properties of the corresponding divergence measures. Specifically, we examine two key properties: zero-forcing and zero-avoiding, where the former promotes exploitation and higher rewards, and the latter encourages exploration and enhances diversity. Based on our theoretical framework, we propose three novel regression losses, namely, Shifted-Cosh, Linex(1/2), and Linex(1). We evaluate them across three benchmarks: hyper-grid, bit-sequence generation, and molecule generation. Our proposed losses are compatible with most existing training algorithms, and significantly improve the performances of the algorithms concerning convergence speed, sample diversity, and robustness.",True,True,Rui Hu and Yifan Zhang and Zhuoran Li and Longbo Huang,2025.0,,https://openreview.net/forum?id=4NTrco82W0,,
Boosted GFlowNets: Improving Exploration via Sequential Learning,2511.09677v1,silva2025generalization,\cite{silva2025generalization},Generalization and Distributed Learning of {GF}lowNets,,,True,False,Tiago Silva and Amauri H Souza and Omar Rivasplata and Vikas Garg and Samuel Kaski and Diego Mesquita,2025.0,,https://openreview.net/forum?id=PJNhZoCjLh,,
Boosted GFlowNets: Improving Exploration via Sequential Learning,2511.09677v1,silva2025when,\cite{silva2025when},When do {GF}lowNets learn the right distribution?,,,True,False,Tiago Silva and Rodrigo Barreto Alves and Eliezer de Souza da Silva and Amauri H Souza and Vikas Garg and Samuel Kaski and Diego Mesquita,2025.0,,https://openreview.net/forum?id=9GsgCUJtic,,
Boosted GFlowNets: Improving Exploration via Sequential Learning,2511.09677v1,guo2016boosting,\cite{guo2016boosting},Boosting Variational Inference,https://arxiv.org/abs/1611.05559v2,"Variational inference (VI) provides fast approximations of a Bayesian posterior in part because it formulates posterior approximation as an optimization problem: to find the closest distribution to the exact posterior over some family of distributions. For practical reasons, the family of distributions in VI is usually constrained so that it does not include the exact posterior, even as a limit point. Thus, no matter how long VI is run, the resulting approximation will not approach the exact posterior. We propose to instead consider a more flexible approximating family consisting of all possible finite mixtures of a parametric base distribution (e.g., Gaussian). For efficient inference, we borrow ideas from gradient boosting to develop an algorithm we call boosting variational inference (BVI). BVI iteratively improves the current approximation by mixing it with a new component from the base distribution family and thereby yields progressively more accurate posterior approximations as more computing time is spent. Unlike a number of common VI variants including mean-field VI, BVI is able to capture multimodality, general posterior covariance, and nonstandard posterior shapes.",True,True,"Guo, Fangjian and Wang, Xiangyu and Fan, Kai and Broderick, Tamara and Dunson, David B",2016.0,,,,arXiv preprint arXiv:1611.05559
Boosted GFlowNets: Improving Exploration via Sequential Learning,2511.09677v1,miller17boosting,\cite{miller17boosting},Variational Boosting: Iteratively Refining Posterior Approximations,https://arxiv.org/abs/1611.06585v2,"We propose a black-box variational inference method to approximate intractable distributions with an increasingly rich approximating class. Our method, termed variational boosting, iteratively refines an existing variational approximation by solving a sequence of optimization problems, allowing the practitioner to trade computation time for accuracy. We show how to expand the variational approximating class by incorporating additional covariance structure and by introducing new components to form a mixture. We apply variational boosting to synthetic and real statistical models, and show that resulting posterior inferences compare favorably to existing posterior approximation algorithms in both accuracy and efficiency.",True,True,Andrew C. Miller and Nicholas J. Foti and Ryan P. Adams,2017.0,06--11 Aug,https://proceedings.mlr.press/v70/miller17a.html,,
Boosted GFlowNets: Improving Exploration via Sequential Learning,2511.09677v1,locatello18boosting,\cite{locatello18boosting},Boosting Variational Inference: an Optimization Perspective,https://arxiv.org/abs/1708.01733v2,"Variational inference is a popular technique to approximate a possibly intractable Bayesian posterior with a more tractable one. Recently, boosting variational inference has been proposed as a new paradigm to approximate the posterior by a mixture of densities by greedily adding components to the mixture. However, as is the case with many other variational inference algorithms, its theoretical properties have not been studied. In the present work, we study the convergence properties of this approach from a modern optimization viewpoint by establishing connections to the classic Frank-Wolfe algorithm. Our analyses yields novel theoretical insights regarding the sufficient conditions for convergence, explicit rates, and algorithmic simplifications. Since a lot of focus in previous works for variational inference has been on tractability, our work is especially important as a much needed attempt to bridge the gap between probabilistic models and their corresponding theoretical properties.",True,True,"Locatello, Francesco and Khanna, Rajiv and Ghosh, Joydeep and Ratsch, Gunnar",2018.0,09--11 Apr,https://proceedings.mlr.press/v84/locatello18a.html,,
Fast Measuring Pavement Crack Width by Cascading Principal Component Analysis,2511.02144v1,Ronneberger2015Biomedical,\cite{Ronneberger2015Biomedical},U-Net: Convolutional Networks for Biomedical Image Segmentation,,,True,False,"Ronneberger, Olaf  and  Fischer, Philipp  and  Brox, Thomas",2015.0,,,,
Fast Measuring Pavement Crack Width by Cascading Principal Component Analysis,2511.02144v1,David2018Semantic,\cite{David2018Semantic},A Deep Convolutional Neural Network for Semantic Pixel-Wise Segmentation of Road and Pavement Surface Cracks,,,True,False,"David Jenkins, Mark and Carr, Thomas Arthur and Iglesias, Maria Insa and Buggy, Tom and Morison, Gordon",2018.0,,,,
Fast Measuring Pavement Crack Width by Cascading Principal Component Analysis,2511.02144v1,Qin2018DeepCrack,\cite{Qin2018DeepCrack},DeepCrack: Learning Hierarchical Convolutional Features for Crack Detection,,,True,False,Qin and Zou and Zheng and Zhang and Qingquan and Li and Xianbiao and Qi and Qian and Wang,2018.0,,,,IEEE Transactions on Image Processing A Publication of the IEEE Signal Processing Society
Fast Measuring Pavement Crack Width by Cascading Principal Component Analysis,2511.02144v1,Yang2017Estimation,\cite{Yang2017Estimation},Estimation of signal-dependent noise level function using multi-column convolutional neural network,,,True,False,"Yang, Jingyu  and  Liu, Xin  and  Song, Xiaolin  and  Li, Kun",2017.0,,,,
Fast Measuring Pavement Crack Width by Cascading Principal Component Analysis,2511.02144v1,Qu2021Pavement,\cite{Qu2021Pavement},A Deeply Supervised Convolutional Neural Network for Pavement Crack Detection With Multiscale Feature Fusion,,,True,False,"Qu, Zhong  and  Cao, Chong  and  Liu, Ling  and  Zhou, Dong Yang",2021.0,,,,IEEE Transactions on Neural Networks and Learning Systems
Fast Measuring Pavement Crack Width by Cascading Principal Component Analysis,2511.02144v1,Xiang2022concrete,\cite{Xiang2022concrete},Crack detection algorithm for concrete structures based on super-resolution reconstruction and segmentation network,,,True,False,"Xiang, Chao  and  Wang, Wei  and  Deng, Lu  and  Shi, Peng  and  Kong, Xuan",2022.0,,,,Automation in construction
Fast Measuring Pavement Crack Width by Cascading Principal Component Analysis,2511.02144v1,Song2020automated,\cite{Song2020automated},Automated pavement crack damage detection using deep multiscale convolutional features,,,True,False,"Song, Weidong and Jia, Guohui and Zhu, Hong and Jia, Di and Gao, Lin and others",2020.0,,,,Journal of Advanced Transportation
Fast Measuring Pavement Crack Width by Cascading Principal Component Analysis,2511.02144v1,liu2019deepcrack,\cite{liu2019deepcrack},DeepCrack: A deep hierarchical feature learning architecture for crack segmentation,,,True,False,"Liu, Yahui and Yao, Jian and Lu, Xiaohu and Xie, Renping and Li, Li",2019.0,,,,Neurocomputing
Fast Measuring Pavement Crack Width by Cascading Principal Component Analysis,2511.02144v1,Yamaguchi2010Practical,\cite{Yamaguchi2010Practical},Practical image measurement of crack width for real concrete structure,,,True,False,"Yamaguchi, Tomoyuki  and  Hashimoto, Shuji",2010.0,,,,Electronics  Communications in Japan
Fast Measuring Pavement Crack Width by Cascading Principal Component Analysis,2511.02144v1,WeixingWangA2019Pavement,\cite{WeixingWangA2019Pavement},Pavement crack image acquisition methods and crack extraction algorithms: A review,,,True,False,"B, Weixing Wang A  and  A, Mengfei Wang  and  A, Hongxia Li  and  A, Heng Zhao  and  B, Kevin Wang  and  C, Changtao He  and  D, Jun Wang  and  E, Sifan Zheng  and  E, Jiabin Chen",2019.0,,,,Journal of Traffic and Transportation Engineering (English Edition)
Fast Measuring Pavement Crack Width by Cascading Principal Component Analysis,2511.02144v1,liu2021crackformer,\cite{liu2021crackformer},Crackformer: Transformer network for fine-grained crack detection,,,True,False,"Liu, Huajun and Miao, Xiangyu and Mertz, Christoph and Xu, Chengzhong and Kong, Hui",2021.0,,,,
Fast Measuring Pavement Crack Width by Cascading Principal Component Analysis,2511.02144v1,Benz2021Vision,\cite{Benz2021Vision},Model-based Crack Width Estimation using Rectangle Transform,,,True,False,"Benz, Christian and Rodehorst, Volker",2021.0,,,,
Fast Measuring Pavement Crack Width by Cascading Principal Component Analysis,2511.02144v1,Takafumi2011Concrete,\cite{Takafumi2011Concrete},Concrete Crack Detection by Multiple Sequential Image Filtering,,,True,False,Takafumi and Nishikawa and Junji and Yoshida and Toshiyuki and Sugiyama and Yozo and Fujino,2011.0,,,,Computer-Aided Civil and Infrastructure Engineering
Fast Measuring Pavement Crack Width by Cascading Principal Component Analysis,2511.02144v1,Wang2017Methodology,\cite{Wang2017Methodology},Methodology for Accurate AASHTO PP67-10-Based Cracking Quantification Using 1-mm 3D Pavement Images,,,True,False,Wang and Kelvin and C. and P. and Shaofan and Qiu and Shi and Wenjuan,2017.0,,,,Journal of Computing in Civil Engineering
Fast Measuring Pavement Crack Width by Cascading Principal Component Analysis,2511.02144v1,Weng2019Segment,\cite{Weng2019Segment},Segment-based pavement crack quantification,,,True,False,"Weng, Xingxing  and  Huang, Yuchun  and  Wang, Wenzong",2019.0,,,,Automation in construction
Fast Measuring Pavement Crack Width by Cascading Principal Component Analysis,2511.02144v1,Ni2019Zernike,\cite{Ni2019Zernike},Zernike-moment measurement of thin-crack width in images enabled by dual-scale deep learning,,,True,False,"Ni, Fu Tao  and  Zhang, Jian  and  Chen, Zhi Qiang",2019.0,,,,Computer-Aided Civil and Infrastructure Engineering
Fast Measuring Pavement Crack Width by Cascading Principal Component Analysis,2511.02144v1,Kim2019Image-based,\cite{Kim2019Image-based},Image-based concrete crack assessment using mask and region-based convolutional neural network,,,True,False,"Kim, Byunghyun and Cho, Soojin",2019.0,,,,Structural Control and Health Monitoring
Fast Measuring Pavement Crack Width by Cascading Principal Component Analysis,2511.02144v1,Payab2019Review,\cite{Payab2019Review},A Brief Review and a New Graph-Based Image Analysis for Concrete Crack Quantification,,,True,False,"Payab, Mahsa  and  Abbasina, Reza  and  Khanzadi, Mostafa",2019.0,,,,Springer Netherlands
Fast Measuring Pavement Crack Width by Cascading Principal Component Analysis,2511.02144v1,Wang2018PavementCrack,\cite{Wang2018PavementCrack},Pavement Crack Width Measurement Based on Laplace's Equation for Continuity and Unambiguity,,,True,False,"Wang, Wenjuan  and  Zhang, Allen  and  Wang, Kelvin C. P.  and  Braham, Andrew F.  and  Qiu, Shi",2018.0,,,,Computer‐Aided Civil and Infrastructure Engineering
PrAda-GAN: A Private Adaptive Generative Adversarial Network with Bayes Network Structure,2511.07997v1,goodfellow2014generative,\cite{goodfellow2014generative},Lipschitz Generative Adversarial Nets,https://arxiv.org/abs/1902.05687v4,"In this paper, we study the convergence of generative adversarial networks (GANs) from the perspective of the informativeness of the gradient of the optimal discriminative function. We show that GANs without restriction on the discriminative function space commonly suffer from the problem that the gradient produced by the discriminator is uninformative to guide the generator. By contrast, Wasserstein GAN (WGAN), where the discriminative function is restricted to 1-Lipschitz, does not suffer from such a gradient uninformativeness problem. We further show in the paper that the model with a compact dual form of Wasserstein distance, where the Lipschitz condition is relaxed, may also theoretically suffer from this issue. This implies the importance of Lipschitz condition and motivates us to study the general formulation of GANs with Lipschitz constraint, which leads to a new family of GANs that we call Lipschitz GANs (LGANs). We show that LGANs guarantee the existence and uniqueness of the optimal discriminative function as well as the existence of a unique Nash equilibrium. We prove that LGANs are generally capable of eliminating the gradient uninformativeness problem. According to our empirical analysis, LGANs are more stable and generate consistently higher quality samples compared with WGAN.",True,True,"Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua",2014.0,,,,NeurIPS
PrAda-GAN: A Private Adaptive Generative Adversarial Network with Bayes Network Structure,2511.07997v1,jordon2018pate,\cite{jordon2018pate},PATE-GAN: Generating synthetic data with differential privacy guarantees,,,True,False,"Jordon, James and Yoon, Jinsung and Van Der Schaar, Mihaela",2018.0,,,,
PrAda-GAN: A Private Adaptive Generative Adversarial Network with Bayes Network Structure,2511.07997v1,liu2019ppgan,\cite{liu2019ppgan},PPGAN: Privacy-preserving Generative Adversarial Network,https://arxiv.org/abs/1910.02007v1,"Generative Adversarial Network (GAN) and its variants serve as a perfect representation of the data generation model, providing researchers with a large amount of high-quality generated data. They illustrate a promising direction for research with limited data availability. When GAN learns the semantic-rich data distribution from a dataset, the density of the generated distribution tends to concentrate on the training data. Due to the gradient parameters of the deep neural network contain the data distribution of the training samples, they can easily remember the training samples. When GAN is applied to private or sensitive data, for instance, patient medical records, as private information may be leakage. To address this issue, we propose a Privacy-preserving Generative Adversarial Network (PPGAN) model, in which we achieve differential privacy in GANs by adding well-designed noise to the gradient during the model learning procedure. Besides, we introduced the Moments Accountant strategy in the PPGAN training process to improve the stability and compatibility of the model by controlling privacy loss. We also give a mathematical proof of the differential privacy discriminator. Through extensive case studies of the benchmark datasets, we demonstrate that PPGAN can generate high-quality synthetic data while retaining the required data available under a reasonable privacy budget.",True,True,"Liu, Yi and Peng, Jialiang and James, JQ and Wu, Yi",2019.0,,,,
PrAda-GAN: A Private Adaptive Generative Adversarial Network with Bayes Network Structure,2511.07997v1,chen2020gs,\cite{chen2020gs},Gs-wgan: A gradient-sanitized approach for learning differentially private generators,,,True,False,"Chen, Dingfan and Orekondy, Tribhuvanesh and Fritz, Mario",2020.0,,,,NeurIPS
PrAda-GAN: A Private Adaptive Generative Adversarial Network with Bayes Network Structure,2511.07997v1,long2021g,\cite{long2021g},G-PATE: Scalable Differentially Private Data Generator via Private Aggregation of Teacher Discriminators,https://arxiv.org/abs/1906.09338v2,"Recent advances in machine learning have largely benefited from the massive accessible training data. However, large-scale data sharing has raised great privacy concerns. In this work, we propose a novel privacy-preserving data Generative model based on the PATE framework (G-PATE), aiming to train a scalable differentially private data generator that preserves high generated data utility. Our approach leverages generative adversarial nets to generate data, combined with private aggregation among different discriminators to ensure strong privacy guarantees. Compared to existing approaches, G-PATE significantly improves the use of privacy budgets. In particular, we train a student data generator with an ensemble of teacher discriminators and propose a novel private gradient aggregation mechanism to ensure differential privacy on all information that flows from teacher discriminators to the student generator. In addition, with random projection and gradient discretization, the proposed gradient aggregation mechanism is able to effectively deal with high-dimensional gradient vectors. Theoretically, we prove that G-PATE ensures differential privacy for the data generator. Empirically, we demonstrate the superiority of G-PATE over prior work through extensive experiments. We show that G-PATE is the first work being able to generate high-dimensional image data with high data utility under limited privacy budgets ($ε\le 1$). Our code is available at https://github.com/AI-secure/G-PATE.",True,True,"Long, Yunhui and Wang, Boxin and Yang, Zhuolin and Kailkhura, Bhavya and Zhang, Aston and Gunter, Carl and Li, Bo",2021.0,,,,NeurIPS
PrAda-GAN: A Private Adaptive Generative Adversarial Network with Bayes Network Structure,2511.07997v1,bie2023private,\cite{bie2023private},"Private GANs, Revisited",https://arxiv.org/abs/2302.02936v2,"We show that the canonical approach for training differentially private GANs -- updating the discriminator with differentially private stochastic gradient descent (DPSGD) -- can yield significantly improved results after modifications to training. Specifically, we propose that existing instantiations of this approach neglect to consider how adding noise only to discriminator updates inhibits discriminator training, disrupting the balance between the generator and discriminator necessary for successful GAN training. We show that a simple fix -- taking more discriminator steps between generator steps -- restores parity between the generator and discriminator and improves results.
  Additionally, with the goal of restoring parity, we experiment with other modifications -- namely, large batch sizes and adaptive discriminator update frequency -- to improve discriminator training and see further improvements in generation quality. Our results demonstrate that on standard image synthesis benchmarks, DPSGD outperforms all alternative GAN privatization schemes. Code: https://github.com/alexbie98/dpgan-revisit.",True,True,"Bie, Alex and Kamath, Gautam and Zhang, Guojun",2023.0,,,,arXiv preprint arXiv:2302.02936
PrAda-GAN: A Private Adaptive Generative Adversarial Network with Bayes Network Structure,2511.07997v1,ma2023rdp,\cite{ma2023rdp},RDP-GAN: A R{\'e}nyi-differential privacy based generative adversarial network,,,True,False,"Ma, Chuan and Li, Jun and Ding, Ming and Liu, Bo and Wei, Kang and Weng, Jian and Poor, H Vincent",2023.0,,,,TDSC
PrAda-GAN: A Private Adaptive Generative Adversarial Network with Bayes Network Structure,2511.07997v1,abadi2016deep,\cite{abadi2016deep},Deep learning with differential privacy,,,True,False,"Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li",2016.0,,,,
PrAda-GAN: A Private Adaptive Generative Adversarial Network with Bayes Network Structure,2511.07997v1,xie2018differentially,\cite{xie2018differentially},Differentially Private Generative Adversarial Network,https://arxiv.org/abs/1802.06739v1,"Generative Adversarial Network (GAN) and its variants have recently attracted intensive research interests due to their elegant theoretical foundation and excellent empirical performance as generative models. These tools provide a promising direction in the studies where data availability is limited. One common issue in GANs is that the density of the learned generative distribution could concentrate on the training data points, meaning that they can easily remember training samples due to the high model complexity of deep networks. This becomes a major concern when GANs are applied to private or sensitive data such as patient medical records, and the concentration of distribution may divulge critical patient information. To address this issue, in this paper we propose a differentially private GAN (DPGAN) model, in which we achieve differential privacy in GANs by adding carefully designed noise to gradients during the learning procedure. We provide rigorous proof for the privacy guarantee, as well as comprehensive empirical evidence to support our analysis, where we demonstrate that our method can generate high quality data points at a reasonable privacy level.",True,True,"Xie, Liyang and Lin, Kaixiang and Wang, Shu and Wang, Fei and Zhou, Jiayu",2018.0,,,,arXiv preprint arXiv:1802.06739
PrAda-GAN: A Private Adaptive Generative Adversarial Network with Bayes Network Structure,2511.07997v1,zhang2018differentially,\cite{zhang2018differentially},Differentially Private Releasing via Deep Generative Model (Technical Report),https://arxiv.org/abs/1801.01594v2,"Privacy-preserving releasing of complex data (e.g., image, text, audio) represents a long-standing challenge for the data mining research community. Due to rich semantics of the data and lack of a priori knowledge about the analysis task, excessive sanitization is often necessary to ensure privacy, leading to significant loss of the data utility. In this paper, we present dp-GAN, a general private releasing framework for semantic-rich data. Instead of sanitizing and then releasing the data, the data curator publishes a deep generative model which is trained using the original data in a differentially private manner; with the generative model, the analyst is able to produce an unlimited amount of synthetic data for arbitrary analysis tasks. In contrast of alternative solutions, dp-GAN highlights a set of key features: (i) it provides theoretical privacy guarantee via enforcing the differential privacy principle; (ii) it retains desirable utility in the released model, enabling a variety of otherwise impossible analyses; and (iii) most importantly, it achieves practical training scalability and stability by employing multi-fold optimization strategies. Through extensive empirical evaluation on benchmark datasets and analyses, we validate the efficacy of dp-GAN.",True,True,"Zhang, Xinyang and Ji, Shouling and Wang, Ting",2018.0,,,,arXiv preprint arXiv:1801.01594
PrAda-GAN: A Private Adaptive Generative Adversarial Network with Bayes Network Structure,2511.07997v1,torkzadehmahani2019dp,\cite{torkzadehmahani2019dp},DP-CGAN: Differentially Private Synthetic Data and Label Generation,https://arxiv.org/abs/2001.09700v1,"Generative Adversarial Networks (GANs) are one of the well-known models to generate synthetic data including images, especially for research communities that cannot use original sensitive datasets because they are not publicly accessible. One of the main challenges in this area is to preserve the privacy of individuals who participate in the training of the GAN models. To address this challenge, we introduce a Differentially Private Conditional GAN (DP-CGAN) training framework based on a new clipping and perturbation strategy, which improves the performance of the model while preserving privacy of the training dataset. DP-CGAN generates both synthetic data and corresponding labels and leverages the recently introduced Renyi differential privacy accountant to track the spent privacy budget. The experimental results show that DP-CGAN can generate visually and empirically promising results on the MNIST dataset with a single-digit epsilon parameter in differential privacy.",True,True,"Torkzadehmahani, Reihaneh and Kairouz, Peter and Paten, Benedict",2019.0,,,,
PrAda-GAN: A Private Adaptive Generative Adversarial Network with Bayes Network Structure,2511.07997v1,zhao2024ctab,\cite{zhao2024ctab},CTAB-GAN+: Enhancing Tabular Data Synthesis,https://arxiv.org/abs/2204.00401v1,"While data sharing is crucial for knowledge development, privacy concerns and strict regulation (e.g., European General Data Protection Regulation (GDPR)) limit its full effectiveness. Synthetic tabular data emerges as alternative to enable data sharing while fulfilling regulatory and privacy constraints. State-of-the-art tabular data synthesizers draw methodologies from Generative Adversarial Networks (GAN). As GANs improve the synthesized data increasingly resemble the real data risking to leak privacy. Differential privacy (DP) provides theoretical guarantees on privacy loss but degrades data utility. Striking the best trade-off remains yet a challenging research question. We propose CTAB-GAN+ a novel conditional tabular GAN. CTAB-GAN+ improves upon state-of-the-art by (i) adding downstream losses to conditional GANs for higher utility synthetic data in both classification and regression domains; (ii) using Wasserstein loss with gradient penalty for better training convergence; (iii) introducing novel encoders targeting mixed continuous-categorical variables and variables with unbalanced or skewed data; and (iv) training with DP stochastic gradient descent to impose strict privacy guarantees. We extensively evaluate CTAB-GAN+ on data similarity and analysis utility against state-of-the-art tabular GANs. The results show that CTAB-GAN+ synthesizes privacy-preserving data with at least 48.16% higher utility across multiple datasets and learning tasks under different privacy budgets.",True,True,"Zhao, Zilong and Kunar, Aditya and Birke, Robert and Van der Scheer, Hiek and Chen, Lydia Y",2024.0,,,,Frontiers in big Data
PrAda-GAN: A Private Adaptive Generative Adversarial Network with Bayes Network Structure,2511.07997v1,hu2024sok,\cite{hu2024sok},SoK: Privacy-Preserving Data Synthesis,https://arxiv.org/abs/2307.02106v2,"As the prevalence of data analysis grows, safeguarding data privacy has become a paramount concern. Consequently, there has been an upsurge in the development of mechanisms aimed at privacy-preserving data analyses. However, these approaches are task-specific; designing algorithms for new tasks is a cumbersome process. As an alternative, one can create synthetic data that is (ideally) devoid of private information. This paper focuses on privacy-preserving data synthesis (PPDS) by providing a comprehensive overview, analysis, and discussion of the field. Specifically, we put forth a master recipe that unifies two prominent strands of research in PPDS: statistical methods and deep learning (DL)-based methods. Under the master recipe, we further dissect the statistical methods into choices of modeling and representation, and investigate the DL-based methods by different generative modeling principles. To consolidate our findings, we provide comprehensive reference tables, distill key takeaways, and identify open problems in the existing literature. In doing so, we aim to answer the following questions: What are the design principles behind different PPDS methods? How can we categorize these methods, and what are the advantages and disadvantages associated with each category? Can we provide guidelines for method selection in different real-world scenarios? We proceed to benchmark several prominent DL-based methods on the task of private image synthesis and conclude that DP-MERF is an all-purpose approach. Finally, upon systematizing the work over the past decade, we identify future directions and call for actions from researchers.",True,True,"Hu, Yuzheng and Wu, Fan and Li, Qinbin and Long, Yunhui and Garrido, Gonzalo Munilla and Ge, Chang and Ding, Bolin and Forsyth, David and Li, Bo and Song, Dawn",2024.0,,,,
PrAda-GAN: A Private Adaptive Generative Adversarial Network with Bayes Network Structure,2511.07997v1,zhang2017privbayes,\cite{zhang2017privbayes},Privbayes: Private data release via bayesian networks,,,True,False,"Zhang, Jun and Cormode, Graham and Procopiuc, Cecilia M and Srivastava, Divesh and Xiao, Xiaokui",2017.0,,,,TODS
PrAda-GAN: A Private Adaptive Generative Adversarial Network with Bayes Network Structure,2511.07997v1,mckenna2022aim,\cite{mckenna2022aim},AIM: an adaptive and iterative mechanism for differentially private synthetic data,,,True,False,"McKenna, Ryan and Mullins, Brett and Sheldon, Daniel and Miklau, Gerome",2022.0,,,,PVLDB
PrAda-GAN: A Private Adaptive Generative Adversarial Network with Bayes Network Structure,2511.07997v1,li2023statistical,\cite{li2023statistical},Statistical Theory of Differentially Private Marginal-based Data Synthesis Algorithms,https://arxiv.org/abs/2301.08844v2,"Marginal-based methods achieve promising performance in the synthetic data competition hosted by the National Institute of Standards and Technology (NIST). To deal with high-dimensional data, the distribution of synthetic data is represented by a probabilistic graphical model (e.g., a Bayesian network), while the raw data distribution is approximated by a collection of low-dimensional marginals. Differential privacy (DP) is guaranteed by introducing random noise to each low-dimensional marginal distribution. Despite its promising performance in practice, the statistical properties of marginal-based methods are rarely studied in the literature. In this paper, we study DP data synthesis algorithms based on Bayesian networks (BN) from a statistical perspective. We establish a rigorous accuracy guarantee for BN-based algorithms, where the errors are measured by the total variation (TV) distance or the $L^2$ distance. Related to downstream machine learning tasks, an upper bound for the utility error of the DP synthetic data is also derived. To complete the picture, we establish a lower bound for TV accuracy that holds for every $ε$-DP synthetic data generator.",True,True,"Li, Ximing and Wang, Chendi and Cheng, Guang",2023.0,,,,arXiv preprint arXiv:2301.08844
PrAda-GAN: A Private Adaptive Generative Adversarial Network with Bayes Network Structure,2511.07997v1,chen2015differentially,\cite{chen2015differentially},Differentially private high-dimensional data publication via sampling-based inference,,,True,False,"Chen, Rui and Xiao, Qian and Zhang, Yu and Xu, Jianliang",2015.0,,,,
PrAda-GAN: A Private Adaptive Generative Adversarial Network with Bayes Network Structure,2511.07997v1,mckenna2019graphical,\cite{mckenna2019graphical},Graphical-model based estimation and inference for differential privacy,,,True,False,"McKenna, Ryan and Sheldon, Daniel and Miklau, Gerome",2019.0,,,,
PrAda-GAN: A Private Adaptive Generative Adversarial Network with Bayes Network Structure,2511.07997v1,cai2021data,\cite{cai2021data},Data synthesis via differentially private markov random fields,,,True,False,"Cai, Kuntai and Lei, Xiaoyu and Wei, Jianxin and Xiao, Xiaokui",2021.0,,,,PVLDB
Reducing normalizing flow complexity for MCMC preconditioning,2511.02345v1,betancourt_hamiltonian_2015,\cite{betancourt_hamiltonian_2015},The Fundamental Incompatibility of Hamiltonian Monte Carlo and Data Subsampling,https://arxiv.org/abs/1502.01510v1,"Leveraging the coherent exploration of Hamiltonian flow, Hamiltonian Monte Carlo produces computationally efficient Monte Carlo estimators, even with respect to complex and high-dimensional target distributions. When confronted with data-intensive applications, however, the algorithm may be too expensive to implement, leaving us to consider the utility of approximations such as data subsampling. In this paper I demonstrate how data subsampling fundamentally compromises the efficient exploration of Hamiltonian flow and hence the scalable performance of Hamiltonian Monte Carlo itself.",True,True,,,,,,
Reducing normalizing flow complexity for MCMC preconditioning,2511.02345v1,papaspiliopoulos_general_2007,\cite{papaspiliopoulos_general_2007},A General Framework for the Parametrization of Hierarchical Models,https://arxiv.org/abs/0708.3797v1,"In this paper, we describe centering and noncentering methodology as complementary techniques for use in parametrization of broad classes of hierarchical models, with a view to the construction of effective MCMC algorithms for exploring posterior distributions from these models. We give a clear qualitative understanding as to when centering and noncentering work well, and introduce theory concerning the convergence time complexity of Gibbs samplers using centered and noncentered parametrizations. We give general recipes for the construction of noncentered parametrizations, including an auxiliary variable technique called the state-space expansion technique. We also describe partially noncentered methods, and demonstrate their use in constructing robust Gibbs sampler algorithms whose convergence properties are not overly sensitive to the data.",True,True,,,,,,
Reducing normalizing flow complexity for MCMC preconditioning,2511.02345v1,parno_transport_2018,\cite{parno_transport_2018},COHERENT 2018 at the Spallation Neutron Source,https://arxiv.org/abs/1803.09183v2,"The primary goal of the COHERENT collaboration is to measure and study coherent elastic neutrino-nucleus scattering (CEvNS) using the high-power, few-tens-of-MeV, pulsed source of neutrinos provided by the Spallation Neutron Source (SNS) at Oak Ridge National Laboratory (ORNL). The COHERENT collaboration reported the first detection of CEvNS [Akimov:2017ade] using a CsI[Na] detector. At present the collaboration is deploying four detector technologies: a CsI[Na] scintillating crystal, p-type point-contact germanium detectors, single-phase liquid argon, and NaI[Tl] crystals. All detectors are located in the neutron-quiet basement of the SNS target building at distances 20-30 m from the SNS neutrino source. The simultaneous measurement in all four COHERENT detector subsystems will test the $N^2$ dependence of the cross section and search for new physics. In addition, COHERENT is measuring neutrino-induced neutrons from charged- and neutral-current neutrino interactions on nuclei in shielding materials, which represent a non-negligible background for CEvNS as well as being of intrinsic interest. The Collaboration is planning as well to look for charged-current interactions of relevance to supernova and weak-interaction physics. This document describes concisely the COHERENT physics motivations, sensitivity, and next plans for measurements at the SNS to be accomplished on a few-year timescale.",True,True,,,,,,
Reducing normalizing flow complexity for MCMC preconditioning,2511.02345v1,gabrie_efficient_2021,\cite{gabrie_efficient_2021},Efficient Bayesian Sampling Using Normalizing Flows to Assist Markov Chain Monte Carlo Methods,https://arxiv.org/abs/2107.08001v1,"Normalizing flows can generate complex target distributions and thus show promise in many applications in Bayesian statistics as an alternative or complement to MCMC for sampling posteriors. Since no data set from the target posterior distribution is available beforehand, the flow is typically trained using the reverse Kullback-Leibler (KL) divergence that only requires samples from a base distribution. This strategy may perform poorly when the posterior is complicated and hard to sample with an untrained normalizing flow. Here we explore a distinct training strategy, using the direct KL divergence as loss, in which samples from the posterior are generated by (i) assisting a local MCMC algorithm on the posterior with a normalizing flow to accelerate its mixing rate and (ii) using the data generated this way to train the flow. The method only requires a limited amount of \textit{a~priori} input about the posterior, and can be used to estimate the evidence required for model validation, as we illustrate on examples.",True,True,,,,,,
Reducing normalizing flow complexity for MCMC preconditioning,2511.02345v1,karamanis_accelerating_2022,\cite{karamanis_accelerating_2022},Accelerating astronomical and cosmological inference with Preconditioned Monte Carlo,https://arxiv.org/abs/2207.05652v1,"We introduce Preconditioned Monte Carlo (PMC), a novel Monte Carlo method for Bayesian inference that facilitates efficient sampling of probability distributions with non-trivial geometry. PMC utilises a Normalising Flow (NF) in order to decorrelate the parameters of the distribution and then proceeds by sampling from the preconditioned target distribution using an adaptive Sequential Monte Carlo (SMC) scheme. The results produced by PMC include samples from the posterior distribution and an estimate of the model evidence that can be used for parameter inference and model comparison respectively. The aforementioned framework has been thoroughly tested in a variety of challenging target distributions achieving state-of-the-art sampling performance. In the cases of primordial feature analysis and gravitational wave inference, PMC is approximately 50 and 25 times faster respectively than Nested Sampling (NS). We found that in higher dimensional applications the acceleration is even greater. Finally, PMC is directly parallelisable, manifesting linear scaling up to thousands of CPUs.",True,True,,,,,,
Reducing normalizing flow complexity for MCMC preconditioning,2511.02345v1,grumitt_sequential_2024,\cite{grumitt_sequential_2024},"Sequential Kalman Tuning of the $t$-preconditioned Crank-Nicolson algorithm: efficient, adaptive and gradient-free inference for Bayesian inverse problems",https://arxiv.org/abs/2407.07781v2,"Ensemble Kalman Inversion (EKI) has been proposed as an efficient method for the approximate solution of Bayesian inverse problems with expensive forward models. However, when applied to the Bayesian inverse problem EKI is only exact in the regime of Gaussian target measures and linear forward models. In this work we propose embedding EKI and Flow Annealed Kalman Inversion (FAKI), its normalizing flow (NF) preconditioned variant, within a Bayesian annealing scheme as part of an adaptive implementation of the $t$-preconditioned Crank-Nicolson (tpCN) sampler. The tpCN sampler differs from standard pCN in that its proposal is reversible with respect to the multivariate $t$-distribution. The more flexible tail behaviour allows for better adaptation to sampling from non-Gaussian targets. Within our Sequential Kalman Tuning (SKT) adaptation scheme, EKI is used to initialize and precondition the tpCN sampler for each annealed target. The subsequent tpCN iterations ensure particles are correctly distributed according to each annealed target, avoiding the accumulation of errors that would otherwise impact EKI. We demonstrate the performance of SKT for tpCN on three challenging numerical benchmarks, showing significant improvements in the rate of convergence compared to adaptation within standard SMC with importance weighted resampling at each temperature level, and compared to similar adaptive implementations of standard pCN. The SKT scheme applied to tpCN offers an efficient, practical solution for solving the Bayesian inverse problem when gradients of the forward model are not available. Code implementing the SKT schemes for tpCN is available at \url{https://github.com/RichardGrumitt/KalmanMC}.",True,True,,,,,,
Reducing normalizing flow complexity for MCMC preconditioning,2511.02345v1,arbel_annealed_2021,\cite{arbel_annealed_2021},Annealed Flow Transport Monte Carlo,https://arxiv.org/abs/2102.07501v2,"Annealed Importance Sampling (AIS) and its Sequential Monte Carlo (SMC) extensions are state-of-the-art methods for estimating normalizing constants of probability distributions. We propose here a novel Monte Carlo algorithm, Annealed Flow Transport (AFT), that builds upon AIS and SMC and combines them with normalizing flows (NFs) for improved performance. This method transports a set of particles using not only importance sampling (IS), Markov chain Monte Carlo (MCMC) and resampling steps - as in SMC, but also relies on NFs which are learned sequentially to push particles towards the successive annealed targets. We provide limit theorems for the resulting Monte Carlo estimates of the normalizing constant and expectations with respect to the target distribution. Additionally, we show that a continuous-time scaling limit of the population version of AFT is given by a Feynman--Kac measure which simplifies to the law of a controlled diffusion for expressive NFs. We demonstrate experimentally the benefits and limitations of our methodology on a variety of applications.",True,True,,,,,,
Reducing normalizing flow complexity for MCMC preconditioning,2511.02345v1,grumitt_deterministic_2022,\cite{grumitt_deterministic_2022},Deterministic Langevin Monte Carlo with Normalizing Flows for Bayesian Inference,https://arxiv.org/abs/2205.14240v2,"We propose a general purpose Bayesian inference algorithm for expensive likelihoods, replacing the stochastic term in the Langevin equation with a deterministic density gradient term. The particle density is evaluated from the current particle positions using a Normalizing Flow (NF), which is differentiable and has good generalization properties in high dimensions. We take advantage of NF preconditioning and NF based Metropolis-Hastings updates for a faster convergence. We show on various examples that the method is competitive against state of the art sampling methods.",True,True,,,,,,
Reducing normalizing flow complexity for MCMC preconditioning,2511.02345v1,nabergoj_empirical_2024,\cite{nabergoj_empirical_2024},Empirical evaluation of normalizing flows in Markov Chain Monte Carlo,https://arxiv.org/abs/2412.17136v2,"Recent advances in MCMC use normalizing flows to precondition target distributions and enable jumps to distant regions. However, there is currently no systematic comparison of different normalizing flow architectures for MCMC. As such, many works choose simple flow architectures that are readily available and do not consider other models. Guidelines for choosing an appropriate architecture would reduce analysis time for practitioners and motivate researchers to take the recommended models as foundations to be improved. We provide the first such guideline by extensively evaluating many normalizing flow architectures on various flow-based MCMC methods and target distributions. When the target density gradient is available, we show that flow-based MCMC outperforms classic MCMC for suitable NF architecture choices with minor hyperparameter tuning. When the gradient is unavailable, flow-based MCMC wins with off-the-shelf architectures. We find contractive residual flows to be the best general-purpose models with relatively low sensitivity to hyperparameter choice. We also provide various insights into normalizing flow behavior within MCMC when varying their hyperparameters, properties of target distributions, and the overall computational budget.",True,True,,,,,,
Reducing normalizing flow complexity for MCMC preconditioning,2511.02345v1,grenioux_sampling_2023a,\cite{grenioux_sampling_2023a},Balanced Training of Energy-Based Models with Adaptive Flow Sampling,https://arxiv.org/abs/2306.00684v4,"Energy-based models (EBMs) are versatile density estimation models that directly parameterize an unnormalized log density. Although very flexible, EBMs lack a specified normalization constant of the model, making the likelihood of the model computationally intractable. Several approximate samplers and variational inference techniques have been proposed to estimate the likelihood gradients for training. These techniques have shown promising results in generating samples, but little attention has been paid to the statistical accuracy of the estimated density, such as determining the relative importance of different classes in a dataset. In this work, we propose a new maximum likelihood training algorithm for EBMs that uses a different type of generative model, normalizing flows (NF), which have recently been proposed to facilitate sampling. Our method fits an NF to an EBM during training so that an NF-assisted sampling scheme provides an accurate gradient for the EBMs at all times, ultimately leading to a fast sampler for generating new data.",True,True,,,,,,
Reducing normalizing flow complexity for MCMC preconditioning,2511.02345v1,mouton_graphical_2022a,\cite{mouton_graphical_2022a},Graphical Residual Flows,https://arxiv.org/abs/2204.11846v1,"Graphical flows add further structure to normalizing flows by encoding non-trivial variable dependencies. Previous graphical flow models have focused primarily on a single flow direction: the normalizing direction for density estimation, or the generative direction for inference. However, to use a single flow to perform tasks in both directions, the model must exhibit stable and efficient flow inversion. This work introduces graphical residual flows, a graphical flow based on invertible residual networks. Our approach to incorporating dependency information in the flow, means that we are able to calculate the Jacobian determinant of these flows exactly. Our experiments confirm that graphical residual flows provide stable and accurate inversion that is also more time-efficient than alternative flows with similar task performance. Furthermore, our model provides performance competitive with other graphical flows for both density estimation and inference tasks.",True,True,,,,,,
Reducing normalizing flow complexity for MCMC preconditioning,2511.02345v1,balgi_personalized_2022,\cite{balgi_personalized_2022},Personalized Public Policy Analysis in Social Sciences using Causal-Graphical Normalizing Flows,https://arxiv.org/abs/2202.03281v2,"Structural Equation/Causal Models (SEMs/SCMs) are widely used in epidemiology and social sciences to identify and analyze the average causal effect (ACE) and conditional ACE (CACE). Traditional causal effect estimation methods such as Inverse Probability Weighting (IPW) and more recently Regression-With-Residuals (RWR) are widely used - as they avoid the challenging task of identifying the SCM parameters - to estimate ACE and CACE. However, much work remains before traditional estimation methods can be used for counterfactual inference, and for the benefit of Personalized Public Policy Analysis (P$^3$A) in the social sciences. While doctors rely on personalized medicine to tailor treatments to patients in laboratory settings (relatively closed systems), P$^3$A draws inspiration from such tailoring but adapts it for open social systems. In this article, we develop a method for counterfactual inference that we name causal-Graphical Normalizing Flow (c-GNF), facilitating P$^3$A. First, we show how c-GNF captures the underlying SCM without making any assumption about functional forms. Second, we propose a novel dequantization trick to deal with discrete variables, which is a limitation of normalizing flows in general. Third, we demonstrate in experiments that c-GNF performs on-par with IPW and RWR in terms of bias and variance for estimating the ATE, when the true functional forms are known, and better when they are unknown. Fourth and most importantly, we conduct counterfactual inference with c-GNFs, demonstrating promising empirical performance. Because IPW and RWR, like other traditional methods, lack the capability of counterfactual inference, c-GNFs will likely play a major role in tailoring personalized treatment, facilitating P$^3$A, optimizing social interventions - in contrast to the current `one-size-fits-all' approach of existing methods.",True,True,,,,,,
Reducing normalizing flow complexity for MCMC preconditioning,2511.02345v1,javaloy_causal_2023a,\cite{javaloy_causal_2023a},Causal normalizing flows: from theory to practice,https://arxiv.org/abs/2306.05415v2,"In this work, we deepen on the use of normalizing flows for causal reasoning. Specifically, we first leverage recent results on non-linear ICA to show that causal models are identifiable from observational data given a causal ordering, and thus can be recovered using autoregressive normalizing flows (NFs). Second, we analyze different design and learning choices for causal normalizing flows to capture the underlying causal data-generating process. Third, we describe how to implement the do-operator in causal NFs, and thus, how to answer interventional and counterfactual questions. Finally, in our experiments, we validate our design and training choices through a comprehensive ablation study; compare causal NFs to other approaches for approximating causal models; and empirically demonstrate that causal NFs can be used to address real-world problems, where the presence of mixed discrete-continuous data and partial knowledge on the causal graph is the norm. The code for this work can be found at https://github.com/psanch21/causal-flows.",True,True,,,,,,
Reducing normalizing flow complexity for MCMC preconditioning,2511.02345v1,gundersen_escaping_2025,\cite{gundersen_escaping_2025},Escaping Neal's Funnel: a multi-stage sampling method for hierarchical models,https://arxiv.org/abs/2510.12917v1,"Neal's funnel refers to an exponential tapering in probability densities common to Bayesian hierarchical models. Usual sampling methods, such as Markov Chain Monte Carlo, struggle to efficiently sample the funnel. Reparameterizing the model or analytically marginalizing local parameters are common techniques to remedy sampling pathologies in distributions exhibiting Neal's funnel. In this paper, we show that the challenges of Neal's funnel can be avoided by performing the hierarchical analysis, well, hierarchically. That is, instead of sampling all parameters of the hierarchical model jointly, we break the sampling into multiple stages. The first stage samples a generalized (higher-dimensional) hierarchical model which is parameterized to lessen the sharpness of the funnel. The next stage samples from the estimated density of the first stage, but under a constraint which restricts the sampling to recover the marginal distributions on the hyper-parameters of the original (lower-dimensional) hierarchical model. A normalizing flow can be used to represent the distribution from the first stage, such that it can easily be sampled from for the second stage of the analysis. This technique is useful when effective reparameterizations are computationally expensive to calculate, or a generalized hierarchical model already exists from which it is easy to sample.",True,True,,,,,,
Bridging Constraints and Stochasticity: A Fully First-Order Method for Stochastic Bilevel Optimization with Linear Constraints,2511.09845v1,kwon2023,\cite{kwon2023},A Fully First-Order Method for Stochastic Bilevel Optimization,,,True,False,Jeongyeol Kwon and Dohyun Kwon and Stephen Wright and Robert Nowak,2023.0,,https://arxiv.org/abs/2301.10945,,
Bridging Constraints and Stochasticity: A Fully First-Order Method for Stochastic Bilevel Optimization with Linear Constraints,2511.09845v1,liu2022bome,\cite{liu2022bome},BOME! Bilevel Optimization Made Easy: A Simple First-Order Approach,https://arxiv.org/abs/2209.08709v1,"Bilevel optimization (BO) is useful for solving a variety of important machine learning problems including but not limited to hyperparameter optimization, meta-learning, continual learning, and reinforcement learning. Conventional BO methods need to differentiate through the low-level optimization process with implicit differentiation, which requires expensive calculations related to the Hessian matrix. There has been a recent quest for first-order methods for BO, but the methods proposed to date tend to be complicated and impractical for large-scale deep learning applications. In this work, we propose a simple first-order BO algorithm that depends only on first-order gradient information, requires no implicit differentiation, and is practical and efficient for large-scale non-convex functions in deep learning. We provide non-asymptotic convergence analysis of the proposed method to stationary points for non-convex objectives and present empirical results that show its superior practical performance.",True,True,"Liu, Bo and Ye, Mao and Wright, Stephen and Stone, Peter and Liu, Qiang",2022.0,,,,Advances in neural information processing systems
Bridging Constraints and Stochasticity: A Fully First-Order Method for Stochastic Bilevel Optimization with Linear Constraints,2511.09845v1,chen2024optimal,\cite{chen2024optimal},Optimal algorithms for stochastic bilevel optimization under relaxed smoothness conditions,,,True,False,"Chen, Xuxing and Xiao, Tesi and Balasubramanian, Krishnakumar",2024.0,,,,Journal of Machine Learning Research
Bridging Constraints and Stochasticity: A Fully First-Order Method for Stochastic Bilevel Optimization with Linear Constraints,2511.09845v1,yang2023achieving,\cite{yang2023achieving},Achieving {$O(\epsilon^{-1.5})$} Complexity in Hessian/Jacobian-free Stochastic Bilevel Optimization,,,True,False,"Yang, Yifan and Xiao, Peiyao and Ji, Kaiyi",2023.0,,,,Advances in Neural Information Processing Systems
Bridging Constraints and Stochasticity: A Fully First-Order Method for Stochastic Bilevel Optimization with Linear Constraints,2511.09845v1,kornowski2024,\cite{kornowski2024},First-Order Methods for Linearly Constrained Bilevel Optimization,,,True,False,"Kornowski, Guy and Padmanabhan, Swati and Wang, Kai and Zhang, Zhe and Sra, Suvrit",2024.0,,,,
Bridging Constraints and Stochasticity: A Fully First-Order Method for Stochastic Bilevel Optimization with Linear Constraints,2511.09845v1,goldstein1977,\cite{goldstein1977},Optimization of Lipschitz continuous functions,,,True,False,"Goldstein, A. A.",1977.0,,,,Mathematical Programming
Bridging Constraints and Stochasticity: A Fully First-Order Method for Stochastic Bilevel Optimization with Linear Constraints,2511.09845v1,yao2024constrained,\cite{yao2024constrained},Constrained Bi-Level Optimization: Proximal Lagrangian Value function Approach and Hessian-free Algorithm,https://arxiv.org/abs/2401.16164v1,"This paper presents a new approach and algorithm for solving a class of constrained Bi-Level Optimization (BLO) problems in which the lower-level problem involves constraints coupling both upper-level and lower-level variables. Such problems have recently gained significant attention due to their broad applicability in machine learning. However, conventional gradient-based methods unavoidably rely on computationally intensive calculations related to the Hessian matrix. To address this challenge, we begin by devising a smooth proximal Lagrangian value function to handle the constrained lower-level problem. Utilizing this construct, we introduce a single-level reformulation for constrained BLOs that transforms the original BLO problem into an equivalent optimization problem with smooth constraints. Enabled by this reformulation, we develop a Hessian-free gradient-based algorithm-termed proximal Lagrangian Value function-based Hessian-free Bi-level Algorithm (LV-HBA)-that is straightforward to implement in a single loop manner. Consequently, LV-HBA is especially well-suited for machine learning applications. Furthermore, we offer non-asymptotic convergence analysis for LV-HBA, eliminating the need for traditional strong convexity assumptions for the lower-level problem while also being capable of accommodating non-singleton scenarios. Empirical results substantiate the algorithm's superior practical performance.",True,True,"Yao, Wei and Yu, Chengming and Zeng, Shangzhi and Zhang, Jin",2024.0,,,,arXiv preprint arXiv:2401.16164
Bridging Constraints and Stochasticity: A Fully First-Order Method for Stochastic Bilevel Optimization with Linear Constraints,2511.09845v1,lu2024first,\cite{lu2024first},First-order penalty methods for bilevel optimization,,,True,False,"Lu, Zhaosong and Mei, Sanyou",2024.0,,,,SIAM Journal on Optimization
Bridging Constraints and Stochasticity: A Fully First-Order Method for Stochastic Bilevel Optimization with Linear Constraints,2511.09845v1,zhang2020complexity,\cite{zhang2020complexity},Complexity of Finding Stationary Points of Nonsmooth Nonconvex Functions,https://arxiv.org/abs/2002.04130v3,"We provide the first non-asymptotic analysis for finding stationary points of nonsmooth, nonconvex functions. In particular, we study the class of Hadamard semi-differentiable functions, perhaps the largest class of nonsmooth functions for which the chain rule of calculus holds. This class contains examples such as ReLU neural networks and others with non-differentiable activation functions. We first show that finding an $ε$-stationary point with first-order methods is impossible in finite time. We then introduce the notion of $(δ, ε)$-stationarity, which allows for an $ε$-approximate gradient to be the convex combination of generalized gradients evaluated at points within distance $δ$ to the solution. We propose a series of randomized first-order methods and analyze their complexity of finding a $(δ, ε)$-stationary point. Furthermore, we provide a lower bound and show that our stochastic algorithm has min-max optimal dependence on $δ$. Empirically, our methods perform well for training ReLU neural networks.",True,True,"Zhang, Jingzhao and Lin, Hongzhou and Jegelka, Stefanie and Sra, Suvrit and Jadbabaie, Ali",2020.0,,,,
Neyman-Pearson Classification under Both Null and Alternative Distributions Shift,2511.06641v1,li2022transfer,\cite{li2022transfer},"Transfer learning for high-dimensional linear regression: Prediction, estimation and minimax optimality",,,True,False,"Li, Sai and Cai, T Tony and Li, Hongzhe",2022.0,,,,Journal of the Royal Statistical Society Series B: Statistical Methodology
Neyman-Pearson Classification under Both Null and Alternative Distributions Shift,2511.06641v1,cai2021transfer,\cite{cai2021transfer},Transfer learning for nonparametric classification,,,True,False,"Cai, T Tony and Wei, Hongji",2021.0,,,,The Annals of Statistics
Neyman-Pearson Classification under Both Null and Alternative Distributions Shift,2511.06641v1,kpotufe2021marginal,\cite{kpotufe2021marginal},Marginal singularity and the benefits of labels in covariate-shift,,,True,False,"Kpotufe, Samory and Martinet, Guillaume",2021.0,,,,The Annals of Statistics
Neyman-Pearson Classification under Both Null and Alternative Distributions Shift,2511.06641v1,tripuraneni2020theory,\cite{tripuraneni2020theory},On the theory of transfer learning: The importance of task diversity,,,True,False,"Tripuraneni, Nilesh and Jordan, Michael and Jin, Chi",2020.0,,,,Advances in neural information processing systems
Neyman-Pearson Classification under Both Null and Alternative Distributions Shift,2511.06641v1,mousavi2020minimax,\cite{mousavi2020minimax},Minimax lower bounds for transfer learning with linear and one-hidden layer neural networks,,,True,False,"Mousavi Kalan, Mohammadreza and Fabian, Zalan and Avestimehr, Salman and Soltanolkotabi, Mahdi",2020.0,,,,Advances in Neural Information Processing Systems
Neyman-Pearson Classification under Both Null and Alternative Distributions Shift,2511.06641v1,ben2010theory,\cite{ben2010theory},A theory of learning from different domains,,,True,False,"Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman",2010.0,,,,Machine learning
Neyman-Pearson Classification under Both Null and Alternative Distributions Shift,2511.06641v1,ben2006analysis,\cite{ben2006analysis},Analysis of representations for domain adaptation,,,True,False,"Ben-David, Shai and Blitzer, John and Crammer, Koby and Pereira, Fernando",2006.0,,,,Advances in neural information processing systems
Neyman-Pearson Classification under Both Null and Alternative Distributions Shift,2511.06641v1,mansour2009domain,\cite{mansour2009domain},Domain adaptation: Learning bounds and algorithms,,,True,False,"Mansour, Yishay and Mohri, Mehryar and Rostamizadeh, Afshin",2009.0,,,,arXiv preprint arXiv:0902.3430
Neyman-Pearson Classification under Both Null and Alternative Distributions Shift,2511.06641v1,hanneke2019value,\cite{hanneke2019value},On the value of target data in transfer learning,,,True,False,"Hanneke, Steve and Kpotufe, Samory",2019.0,,,,Advances in Neural Information Processing Systems
Neyman-Pearson Classification under Both Null and Alternative Distributions Shift,2511.06641v1,cannon2002learning,\cite{cannon2002learning},Learning with the Neyman-Pearson and min-max criteria,,,True,False,"Cannon, Adam and Howse, James and Hush, Don and Scovel, Clint",2002.0,,,,
Neyman-Pearson Classification under Both Null and Alternative Distributions Shift,2511.06641v1,scott2005neyman,\cite{scott2005neyman},A Neyman-Pearson approach to statistical learning,,,True,False,"Scott, Clayton and Nowak, Robert",2005.0,,,,IEEE Transactions on Information Theory
Neyman-Pearson Classification under Both Null and Alternative Distributions Shift,2511.06641v1,rigollet2011neyman,\cite{rigollet2011neyman},"Neyman-Pearson classification, convexity and stochastic constraints.",,,True,False,"Rigollet, Philippe and Tong, Xin",2011.0,,,,Journal of machine learning research
Neyman-Pearson Classification under Both Null and Alternative Distributions Shift,2511.06641v1,tong2013plug,\cite{tong2013plug},A plug-in approach to neyman-pearson classification,,,True,False,"Tong, Xin",2013.0,,,,The Journal of Machine Learning Research
Neyman-Pearson Classification under Both Null and Alternative Distributions Shift,2511.06641v1,kalan2024distribution,\cite{kalan2024distribution},Distribution-Free Rates in Neyman-Pearson Classification,https://arxiv.org/abs/2402.09560v1,"We consider the problem of Neyman-Pearson classification which models unbalanced classification settings where error w.r.t. a distribution $μ_1$ is to be minimized subject to low error w.r.t. a different distribution $μ_0$. Given a fixed VC class $\mathcal{H}$ of classifiers to be minimized over, we provide a full characterization of possible distribution-free rates, i.e., minimax rates over the space of all pairs $(μ_0, μ_1)$. The rates involve a dichotomy between hard and easy classes $\mathcal{H}$ as characterized by a simple geometric condition, a three-points-separation condition, loosely related to VC dimension.",True,True,"Kalan, Mohammadreza M and Kpotufe, Samory",2024.0,,,,arXiv e-prints
Neyman-Pearson Classification under Both Null and Alternative Distributions Shift,2511.06641v1,kalantight,\cite{kalantight},Tight Rates in Supervised Outlier Transfer Learning,https://arxiv.org/abs/2310.04686v1,"A critical barrier to learning an accurate decision rule for outlier detection is the scarcity of outlier data. As such, practitioners often turn to the use of similar but imperfect outlier data from which they might transfer information to the target outlier detection task. Despite the recent empirical success of transfer learning approaches in outlier detection, a fundamental understanding of when and how knowledge can be transferred from a source to a target outlier detection task remains elusive. In this work, we adopt the traditional framework of Neyman-Pearson classification -- which formalizes supervised outlier detection -- with the added assumption that one has access to some related but imperfect outlier data. Our main results are as follows:
  We first determine the information-theoretic limits of the problem under a measure of discrepancy that extends some existing notions from traditional balanced classification; interestingly, unlike in balanced classification, seemingly very dissimilar sources can provide much information about a target, thus resulting in fast transfer.
  We then show that, in principle, these information-theoretic limits are achievable by adaptive procedures, i.e., procedures with no a priori information on the discrepancy between source and target outlier distributions.",True,True,"Kalan, Mohammadreza Mousavi and Kpotufe, Samory",2024.0,,,,
Neyman-Pearson Classification under Both Null and Alternative Distributions Shift,2511.06641v1,kalan2025transfer,\cite{kalan2025transfer},Transfer Neyman-Pearson Algorithm for Outlier Detection,,,True,False,"Kalan, Mohammadreza Mousavi and Neugut, Eitan J and Kpotufe, Samory",2025.0,,,,
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,chaudhuri2013near,\cite{chaudhuri2013near},A near-optimal algorithm for differentially-private principal components,,,True,False,"Chaudhuri, Kamalika and Sarwate, Anand D and Sinha, Kaushik",2013.0,,,,The Journal of Machine Learning Research
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,kapralov2013differentially,\cite{kapralov2013differentially},On differentially private low rank approximation,,,True,False,"Kapralov, Michael and Talwar, Kunal",2013.0,,,,
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,amin2019differentially,\cite{amin2019differentially},Differentially private covariance estimation,,,True,False,"Amin, Kareem and Dick, Travis and Kulesza, Alex and Munoz, Andres and Vassilvitskii, Sergei",2019.0,,,,Advances in Neural Information Processing Systems
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,leake2021sampling,\cite{leake2021sampling},Sampling matrices from {H}arish-{C}handra--{I}tzykson--{Z}uber densities with applications to quantum inference and differential privacy,,,True,False,"Leake, Jonathan and McSwiggen, Colin and Vishnoi, Nisheeth K",2021.0,,,,
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,mangoubi2022re,\cite{mangoubi2022re},Re-analyze {G}auss: Bounds for private matrix approximation via Dyson Brownian motion,,,True,False,"Mangoubi, Oren and Vishnoi, Nisheeth",2022.0,,,,Advances in Neural Information Processing Systems
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,dwork2014analyze,\cite{dwork2014analyze},Analyze {G}auss: optimal bounds for privacy-preserving principal component analysis,,,True,False,"Dwork, Cynthia and Talwar, Kunal and Thakurta, Abhradeep and Zhang, Li",2014.0,,,,
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,wei2016analysis,\cite{wei2016analysis},Analysis of a privacy-preserving {PCA} algorithm using random matrix theory,,,True,False,"Wei, Lu and Sarwate, Anand D and Corander, Jukka and Hero, Alfred and Tarokh, Vahid",2016.0,,,,
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,gonem2018smooth,\cite{gonem2018smooth},Smooth sensitivity based approach for differentially private {PCA},,,True,False,"Gonem, Alon and Gilad-Bachrach, Ram",2018.0,,,,
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,hardt2013beyond,\cite{hardt2013beyond},Beyond Worst-Case Analysis in Private Singular Vector Computation,https://arxiv.org/abs/1211.0975v1,"We consider differentially private approximate singular vector computation. Known worst-case lower bounds show that the error of any differentially private algorithm must scale polynomially with the dimension of the singular vector. We are able to replace this dependence on the dimension by a natural parameter known as the coherence of the matrix that is often observed to be significantly smaller than the dimension both theoretically and empirically. We also prove a matching lower bound showing that our guarantee is nearly optimal for every setting of the coherence parameter. Notably, we achieve our bounds by giving a robust analysis of the well-known power iteration algorithm, which may be of independent interest. Our algorithm also leads to improvements in worst-case settings and to better low-rank approximations in the spectral norm.",True,True,"Hardt, Moritz and Roth, Aaron",2013.0,,,,
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,hardt2012beating,\cite{hardt2012beating},Beating Randomized Response on Incoherent Matrices,https://arxiv.org/abs/1111.0623v1,"Computing accurate low rank approximations of large matrices is a fundamental data mining task. In many applications however the matrix contains sensitive information about individuals. In such case we would like to release a low rank approximation that satisfies a strong privacy guarantee such as differential privacy. Unfortunately, to date the best known algorithm for this task that satisfies differential privacy is based on naive input perturbation or randomized response: Each entry of the matrix is perturbed independently by a sufficiently large random noise variable, a low rank approximation is then computed on the resulting matrix.
  We give (the first) significant improvements in accuracy over randomized response under the natural and necessary assumption that the matrix has low coherence. Our algorithm is also very efficient and finds a constant rank approximation of an m x n matrix in time O(mn). Note that even generating the noise matrix required for randomized response already requires time O(mn).",True,True,"Hardt, Moritz and Roth, Aaron",2012.0,,,,
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,hardt2014noisy,\cite{hardt2014noisy},The noisy power method: A meta algorithm with applications,,,True,False,"Hardt, Moritz and Price, Eric",2014.0,,,,Advances in neural information processing systems
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,tran2025spectral,\cite{tran2025spectral},Spectral Perturbation Bounds for Low-Rank Approximation with Applications to Privacy,,,True,False,"Tran, Phuc and Vishnoi, Nisheeth K and Vu, Van H",2025.0,,,,arXiv preprint arXiv:2510.25670
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,d2025tight,\cite{d2025tight},Tight Differentially Private PCA via Matrix Coherence,https://arxiv.org/abs/2510.26679v1,"We revisit the task of computing the span of the top $r$ singular vectors $u_1, \ldots, u_r$ of a matrix under differential privacy. We show that a simple and efficient algorithm -- based on singular value decomposition and standard perturbation mechanisms -- returns a private rank-$r$ approximation whose error depends only on the \emph{rank-$r$ coherence} of $u_1, \ldots, u_r$ and the spectral gap $σ_r - σ_{r+1}$. This resolves a question posed by Hardt and Roth~\cite{hardt2013beyond}. Our estimator outperforms the state of the art -- significantly so in some regimes. In particular, we show that in the dense setting, it achieves the same guarantees for single-spike PCA in the Wishart model as those attained by optimal non-private algorithms, whereas prior private algorithms failed to do so.
  In addition, we prove that (rank-$r$) coherence does not increase under Gaussian perturbations. This implies that any estimator based on the Gaussian mechanism -- including ours -- preserves the coherence of the input. We conjecture that similar behavior holds for other structured models, including planted problems in graphs.
  We also explore applications of coherence to graph problems. In particular, we present a differentially private algorithm for Max-Cut and other constraint satisfaction problems under low coherence assumptions.",True,True,"d'Orsi, Tommaso and Novikov, Gleb",2025.0,,,,arXiv preprint arXiv:2510.26679
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,dwork2006calibrating,\cite{dwork2006calibrating},Calibrating noise to sensitivity in private data analysis,,,True,False,"Dwork, Cynthia and McSherry, Frank and Nissim, Kobbi and Smith, Adam",2006.0,,,,
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,dwork2006our,\cite{dwork2006our},"Our data, ourselves: Privacy via distributed noise generation",,,True,False,"Dwork, Cynthia and Kenthapadi, Krishnaram and McSherry, Frank and Mironov, Ilya and Naor, Moni",2006.0,,,,
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,dong2022gaussian,\cite{dong2022gaussian},Gaussian Differential Privacy,https://arxiv.org/abs/1905.02383v3,"Differential privacy has seen remarkable success as a rigorous and practical formalization of data privacy in the past decade. This privacy definition and its divergence based relaxations, however, have several acknowledged weaknesses, either in handling composition of private algorithms or in analyzing important primitives like privacy amplification by subsampling. Inspired by the hypothesis testing formulation of privacy, this paper proposes a new relaxation, which we term `$f$-differential privacy' ($f$-DP). This notion of privacy has a number of appealing properties and, in particular, avoids difficulties associated with divergence based relaxations. First, $f$-DP preserves the hypothesis testing interpretation. In addition, $f$-DP allows for lossless reasoning about composition in an algebraic fashion. Moreover, we provide a powerful technique to import existing results proven for original DP to $f$-DP and, as an application, obtain a simple subsampling theorem for $f$-DP.
  In addition to the above findings, we introduce a canonical single-parameter family of privacy notions within the $f$-DP class that is referred to as `Gaussian differential privacy' (GDP), defined based on testing two shifted Gaussians. GDP is focal among the $f$-DP class because of a central limit theorem we prove. More precisely, the privacy guarantees of \emph{any} hypothesis testing based definition of privacy (including original DP) converges to GDP in the limit under composition. The CLT also yields a computationally inexpensive tool for analyzing the exact composition of private algorithms.
  Taken together, this collection of attractive properties render $f$-DP a mathematically coherent, analytically tractable, and versatile framework for private data analysis. Finally, we demonstrate the use of the tools we develop by giving an improved privacy analysis of noisy stochastic gradient descent.",True,True,"Dong, Jinshuo and Roth, Aaron and Su, Weijie J",2022.0,,,,Journal of the Royal Statistical Society: Series B (Statistical Methodology)
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,mironov2017renyi,\cite{mironov2017renyi},R{\'e}nyi differential privacy,,,True,False,"Mironov, Ilya",2017.0,,,,
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,cai2024optimal,\cite{cai2024optimal},Optimal Differentially Private {PCA} and Estimation for Spiked Covariance Matrices,,,True,False,"Cai, T Tony and Xia, Dong and Zha, Mengyue",2024.0,,,,arXiv preprint arXiv:2401.03820
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,liu2022dp,\cite{liu2022dp},{DP-PCA}: Statistically optimal and differentially private {PCA},,,True,False,"Liu, Xiyang and Kong, Weihao and Jain, Prateek and Oh, Sewoong",2022.0,,,,Advances in neural information processing systems
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,singhal2021privately,\cite{singhal2021privately},Privately Learning Subspaces,https://arxiv.org/abs/2106.00001v3,"Private data analysis suffers a costly curse of dimensionality. However, the data often has an underlying low-dimensional structure. For example, when optimizing via gradient descent, the gradients often lie in or near a low-dimensional subspace. If that low-dimensional structure can be identified, then we can avoid paying (in terms of privacy or accuracy) for the high ambient dimension.
  We present differentially private algorithms that take input data sampled from a low-dimensional linear subspace (possibly with a small amount of error) and output that subspace (or an approximation to it). These algorithms can serve as a pre-processing step for other procedures.",True,True,"Singhal, Vikrant and Steinke, Thomas",2021.0,,,,Advances in Neural Information Processing Systems
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,dungler2025iterative,\cite{dungler2025iterative},An Iterative Algorithm for Differentially Private $ k $-{PCA} with Adaptive Noise,,,True,False,"D{\""u}ngler, Johanna and Sanyal, Amartya",2025.0,,,,arXiv preprint arXiv:2508.10879
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,dwork2024differentially,\cite{dwork2024differentially},Differentially private learning beyond the classical dimensionality regime,,,True,False,"Dwork, Cynthia and Tankala, Pranay and Zhang, Linjun",2024.0,,,,arXiv preprint arXiv:2411.13682
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,bombari2025better,\cite{bombari2025better},Better Rates for Private Linear Regression in the Proportional Regime via Aggressive Clipping,,,True,False,"Bombari, Simone and Seroussi, Inbar and Mondelli, Marco",2025.0,,,,arXiv preprint arXiv:2505.16329
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,kosterlitz1976spherical,\cite{kosterlitz1976spherical},The p-spin spherical spin glass model,https://arxiv.org/abs/cond-mat/9701031v1,"This review presents various aspects of a mean-field spin glass model known as the p-spin spherical spin glass model, which has raised a lot of interest in the study of spin glasses, and also for its possible links with a mean-field theory of structural glasses.
  This preprint contains no new results and is therefore not intended to be published, but its aim is to present a collection of results and formulas concerning this very rich model. It is in fact the english translation of one of the chapters of my PhD thesis (``Quelques aspects de la dynamique hors d'equilibre des verres de spin'', ``Some aspects of the out of equilibrium dynamics of spin glasses''). A postscript version (in french) of this PhD thesis will soon be available at http://www.lpt.ens.fr .",True,True,"Kosterlitz, John M and Thouless, David J and Jones, Raymund C",1976.0,,,,Physical Review Letters
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,guionnet2005fourier,\cite{guionnet2005fourier},A {F}ourier view on the {R}-transform and related asymptotics of spherical integrals,,,True,False,A. Guionnet and M. Maida,2005.0,,,,Journal of Functional Analysis
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,baik2016fluctuations,\cite{baik2016fluctuations},Fluctuations of the free energy of the spherical {S}herrington--{K}irkpatrick model,,,True,False,"Baik, Jinho and Lee, Ji Oon",2016.0,,,,Journal of Statistical Physics
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,baik2017fluctuations,\cite{baik2017fluctuations},Fluctuations of the free energy of the spherical {S}herrington--{K}irkpatrick model with ferromagnetic interaction,,,True,False,"Baik, Jinho and Lee, Ji Oon",2017.0,,,,
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,baik2021spherical,\cite{baik2021spherical},Spherical spin glass model with external field,,,True,False,"Baik, Jinho and Collins-Woodfin, Elizabeth and Le Doussal, Pierre and Wu, Hao",2021.0,,,,Journal of Statistical Physics
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,baik2018ferromagnetic,\cite{baik2018ferromagnetic},Ferromagnetic to paramagnetic transition in spherical spin glass,https://arxiv.org/abs/1805.05630v1,"We consider the spherical spin glass model defined by a combination of the pure 2-spin spherical Sherrington-Kirkpatrick Hamiltonian and the ferromagnetic Curie-Weiss Hamiltonian. In the large system limit, there is a two-dimensional phase diagram with respect to the temperature and the coupling strength. The phase diagram is divided into three regimes; ferromagnetic, paramagnetic, and spin glass regimes. The fluctuations of the free energy are known in each regime. In this paper, we study the transition between the ferromagnetic regime and the paramagnetic regime in a critical scale.",True,True,"Baik, Jinho and Lee, Ji Oon and Wu, Hao",2018.0,,,,Journal of Statistical Physics
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,landon2020fluctuations,\cite{landon2020fluctuations},Fluctuations of the 2-spin {SSK} model with magnetic field,,,True,False,"Landon, Benjamin and Sosoe, Philippe",2020.0,,,,arXiv preprint arXiv:2009.12514
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,guionnet2021asymptotics,\cite{guionnet2021asymptotics},Asymptotics of $k$-dimensional spherical integrals and applications,,,True,False,"Guionnet, Alice and Husson, Jonathan",2021.0,,,,arXiv preprint arXiv:2101.01983
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,husson2025spherical,\cite{husson2025spherical},Spherical Integrals of Sublinear Rank,https://arxiv.org/abs/2208.03642v2,"We consider the asymptotics of $k$-dimensional spherical integrals when $k = o(N)$. We prove that the $o(N)$-dimensional spherical integrals are approximately the products of $1$-dimensional spherical integrals. Our formulas extend the results for $k$-dimensional spherical integrals proved by Guionnet and Maïda in [29] and Husson and Guionnet in [34] which are only valid for $k$ finite and independent of $N$. These approximations will be used to prove a large deviation principle for the joint $2k(N)$ extreme eigenvalues for sharp sub-Gaussian Wigner matrices and for additive deformations of GOE/GUE matrices. Furthermore, our results will be used to compute the free energies of spherical SK vector spin glasses and the mutual information for matrix estimation problems when the dimensions of the spins or signals have sublinear growth.",True,True,"Husson, Jonathan and Ko, Justin",2025.0,,,,Probability Theory and Related Fields
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,hoff2009simulation,\cite{hoff2009simulation},"Simulation of the matrix {B}ingham--von {M}ises--{F}isher distribution, with applications to multivariate and relational data",,,True,False,"Hoff, Peter D",2009.0,,,,Journal of Computational and Graphical Statistics
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,kume2006sampling,\cite{kume2006sampling},Sampling from compositional and directional distributions,,,True,False,"Kume, Alfred and Walker, Stephen G",2006.0,,,,Statistics and Computing
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,kent2018new,\cite{kent2018new},A new unified approach for the simulation of a wide class of directional distributions,,,True,False,"Kent, John T and Ganeiber, Asaad M and Mardia, Kanti V",2018.0,,,,Journal of Computational and Graphical Statistics
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,kent2004simulation,\cite{kent2004simulation},Simulation for the complex {B}ingham distribution,,,True,False,"Kent, John T and Constable, Patrick DL and Er, Fikret",2004.0,,,,Statistics and Computing
High-Dimensional Asymptotics of Differentially Private PCA,2511.07270v1,ge2021efficient,\cite{ge2021efficient},Efficient sampling from the Bingham distribution,,,True,False,"Ge, Rong and Lee, Holden and Lu, Jianfeng and Risteski, Andrej",2021.0,,,,
Robust Causal Discovery under Imperfect Structural Constraints,2511.06790v1,de2007bayesian,\cite{de2007bayesian},Bayesian network learning algorithms using structural restrictions,,,True,False,"Luis M. de Campos and
                  Francisco Javier Garc{\'{\i}}a Castellano",2007.0,,https://doi.org/10.1016/j.ijar.2006.06.009,10.1016/J.IJAR.2006.06.009,Int. J. Approx. Reason.
Robust Causal Discovery under Imperfect Structural Constraints,2511.06790v1,colombo2014order,\cite{colombo2014order},Order-independent constraint-based causal structure learning,https://arxiv.org/abs/1211.3295v2,"We consider constraint-based methods for causal structure learning, such as the PC-, FCI-, RFCI- and CCD- algorithms (Spirtes et al. (2000, 1993), Richardson (1996), Colombo et al. (2012), Claassen et al. (2013)). The first step of all these algorithms consists of the PC-algorithm. This algorithm is known to be order-dependent, in the sense that the output can depend on the order in which the variables are given. This order-dependence is a minor issue in low-dimensional settings. We show, however, that it can be very pronounced in high-dimensional settings, where it can lead to highly variable results. We propose several modifications of the PC-algorithm (and hence also of the other algorithms) that remove part or all of this order-dependence. All proposed modifications are consistent in high-dimensional settings under the same conditions as their original counterparts. We compare the PC-, FCI-, and RFCI-algorithms and their modifications in simulation studies and on a yeast gene expression data set. We show that our modifications yield similar performance in low-dimensional settings and improved performance in high-dimensional settings. All software is implemented in the R-package pcalg.",True,True,"Diego Colombo and
                  Marloes H. Maathuis",2014.0,,https://dl.acm.org/doi/10.5555/2627435.2750365,10.5555/2627435.2750365,J. Mach. Learn. Res.
Robust Causal Discovery under Imperfect Structural Constraints,2511.06790v1,constantinou2023impact,\cite{constantinou2023impact},The impact of prior knowledge on causal structure learning,,,True,False,"Anthony C. Constantinou and
                  Zhigao Guo and
                  Neville Kenneth Kitson",2023.0,,https://doi.org/10.1007/s10115-023-01858-x,10.1007/S10115-023-01858-X,Knowl. Inf. Syst.
Robust Causal Discovery under Imperfect Structural Constraints,2511.06790v1,chen2016learning,\cite{chen2016learning},Learning Bayesian networks with ancestral constraints,,,True,False,"Chen, Eunice Yuh-Jie and Shen, Yujia and Choi, Arthur and Darwiche, Adnan",2016.0,,,,
Robust Causal Discovery under Imperfect Structural Constraints,2511.06790v1,wang2021learning,\cite{wang2021learning},Learning Bayesian networks based on order graph with ancestral constraints,,,True,False,"Zidong Wang and
                  Xiaoguang Gao and
                  Yu Yang and
                  Xiangyuan Tan and
                  Daqing Chen",2021.0,,https://doi.org/10.1016/j.knosys.2020.106515,10.1016/J.KNOSYS.2020.106515,Knowl. Based Syst.
Robust Causal Discovery under Imperfect Structural Constraints,2511.06790v1,wang2025large,\cite{wang2025large},Large-Scale Hierarchical Causal Discovery via Weak Prior Knowledge,,,True,False,"Xiangyu Wang and
                  Taiyu Ban and
                  Lyuzhou Chen and
                  Derui Lyu and
                  Qinrui Zhu and
                  Huanhuan Chen",2025.0,,https://doi.org/10.1109/TKDE.2025.3537832,10.1109/TKDE.2025.3537832,{IEEE} Trans. Knowl. Data Eng.
Robust Causal Discovery under Imperfect Structural Constraints,2511.06790v1,hasan2022kcrl,\cite{hasan2022kcrl},Kcrl: A prior knowledge based causal discovery framework with reinforcement learning,,,True,False,"Hasan, Uzma and Gani, Md Osman",2022.0,,,,
Robust Causal Discovery under Imperfect Structural Constraints,2511.06790v1,sun2023nts,\cite{sun2023nts},NTS-NOTEARS: Learning Nonparametric DBNs With Prior Knowledge,https://arxiv.org/abs/2109.04286v3,"We describe NTS-NOTEARS, a score-based structure learning method for time-series data to learn dynamic Bayesian networks (DBNs) that captures nonlinear, lagged (inter-slice) and instantaneous (intra-slice) relations among variables. NTS-NOTEARS utilizes 1D convolutional neural networks (CNNs) to model the dependence of child variables on their parents; 1D CNN is a neural function approximation model well-suited for sequential data. DBN-CNN structure learning is formulated as a continuous optimization problem with an acyclicity constraint, following the NOTEARS DAG learning approach. We show how prior knowledge of dependencies (e.g., forbidden and required edges) can be included as additional optimization constraints. Empirical evaluation on simulated and benchmark data show that NTS-NOTEARS achieves state-of-the-art DAG structure quality compared to both parametric and nonparametric baseline methods, with improvement in the range of 10-20% on the F1-score. We also evaluate NTS-NOTEARS on complex real-world data acquired from professional ice hockey games that contain a mixture of continuous and discrete variables. The code is available online.",True,True,"Sun, Xiangyu and Schulte, Oliver and Liu, Guiliang and Poupart, Pascal",2023.0,,,,
Robust Causal Discovery under Imperfect Structural Constraints,2511.06790v1,wang2024incorporating,\cite{wang2024incorporating},"Incorporating structural constraints into continuous optimization
                  for causal discovery",,,True,False,"Zidong Wang and
                  Xiaoguang Gao and
                  Xiaohan Liu and
                  Xinxin Ru and
                  Qingfu Zhang",2024.0,,https://doi.org/10.1016/j.neucom.2024.127902,10.1016/J.NEUCOM.2024.127902,Neurocomputing
Robust Causal Discovery under Imperfect Structural Constraints,2511.06790v1,bello2022dagma,\cite{bello2022dagma},DAGMA: Learning DAGs via M-matrices and a Log-Determinant Acyclicity Characterization,https://arxiv.org/abs/2209.08037v3,"The combinatorial problem of learning directed acyclic graphs (DAGs) from data was recently framed as a purely continuous optimization problem by leveraging a differentiable acyclicity characterization of DAGs based on the trace of a matrix exponential function. Existing acyclicity characterizations are based on the idea that powers of an adjacency matrix contain information about walks and cycles. In this work, we propose a new acyclicity characterization based on the log-determinant (log-det) function, which leverages the nilpotency property of DAGs. To deal with the inherent asymmetries of a DAG, we relate the domain of our log-det characterization to the set of $\textit{M-matrices}$, which is a key difference to the classical log-det function defined over the cone of positive definite matrices. Similar to acyclicity functions previously proposed, our characterization is also exact and differentiable. However, when compared to existing characterizations, our log-det function: (1) Is better at detecting large cycles; (2) Has better-behaved gradients; and (3) Its runtime is in practice about an order of magnitude faster. From the optimization side, we drop the typically used augmented Lagrangian scheme and propose DAGMA ($\textit{DAGs via M-matrices for Acyclicity}$), a method that resembles the central path for barrier methods. Each point in the central path of DAGMA is a solution to an unconstrained problem regularized by our log-det function, then we show that at the limit of the central path the solution is guaranteed to be a DAG. Finally, we provide extensive experiments for $\textit{linear}$ and $\textit{nonlinear}$ SEMs and show that our approach can reach large speed-ups and smaller structural Hamming distances against state-of-the-art methods. Code implementing the proposed method is open-source and publicly available at https://github.com/kevinsbello/dagma.",True,True,"Bello, Kevin and Aragam, Bryon and Ravikumar, Pradeep",2022.0,,,,
Robust Causal Discovery under Imperfect Structural Constraints,2511.06790v1,li2024weakly,\cite{li2024weakly},Weakly-supervised causal discovery based on fuzzy knowledge and complex data complementarity,https://arxiv.org/abs/2405.08699v1,"Causal discovery based on observational data is important for deciphering the causal mechanism behind complex systems. However, the effectiveness of existing causal discovery methods is limited due to inferior prior knowledge, domain inconsistencies, and the challenges of high-dimensional datasets with small sample sizes. To address this gap, we propose a novel weakly-supervised fuzzy knowledge and data co-driven causal discovery method named KEEL. KEEL adopts a fuzzy causal knowledge schema to encapsulate diverse types of fuzzy knowledge, and forms corresponding weakened constraints. This schema not only lessens the dependency on expertise but also allows various types of limited and error-prone fuzzy knowledge to guide causal discovery. It can enhance the generalization and robustness of causal discovery, especially in high-dimensional and small-sample scenarios. In addition, we integrate the extended linear causal model (ELCM) into KEEL for dealing with the multi-distribution and incomplete data. Extensive experiments with different datasets demonstrate the superiority of KEEL over several state-of-the-art methods in accuracy, robustness and computational efficiency. For causal discovery in real protein signal transduction processes, KEEL outperforms the benchmark method with limited data. In summary, KEEL is effective to tackle the causal discovery tasks with higher accuracy while alleviating the requirement for extensive domain expertise.",True,True,"Wenrui Li and
                  Wei Zhang and
                  Qinghao Zhang and
                  Xuegong Zhang and
                  Xiaowo Wang",2024.0,,https://doi.org/10.1109/TFUZZ.2024.3471187,10.1109/TFUZZ.2024.3471187,{IEEE} Trans. Fuzzy Syst.
Robust Causal Discovery under Imperfect Structural Constraints,2511.06790v1,chen2025continuous,\cite{chen2025continuous},Continuous Structure Constraint Integration for Robust Causal Discovery,,,True,False,"Chen, Lyuzhou and Ban, Taiyu and Lyu, Derui and Sun, Yijia and Hu, Kangtao and Wang, Xiangyu and Chen, Huanhuan",2025.0,,,,
Robust Causal Discovery under Imperfect Structural Constraints,2511.06790v1,bandifferentiable,\cite{bandifferentiable},Differentiable Structure Learning with Ancestral Constraints,,,True,False,"Ban, Taiyu and Rong, Changxin and Wang, Xiangyu and Chen, Lyuzhou and Wang, Xin and Lyu, Derui and Zhu, Qinrui and Chen, Huanhuan",2025.0,,,,
Robust Causal Discovery under Imperfect Structural Constraints,2511.06790v1,kiciman2023causal,\cite{kiciman2023causal},"Causal Reasoning and Large Language Models: Opening a New Frontier
                  for Causality",,,True,False,"Emre Kiciman and
                  Robert Osazuwa Ness and
                  Amit Sharma and
                  Chenhao Tan",2024.0,,https://openreview.net/forum?id=mqoxLkX210,,Trans. Mach. Learn. Res.
Robust Causal Discovery under Imperfect Structural Constraints,2511.06790v1,ban2025integrating,\cite{ban2025integrating},Integrating large language model for improved causal discovery,,,True,False,"Ban, Taiyu and Chen, Lyuzhou and Lyu, Derui and Wang, Xiangyu and Zhu, Qinrui and Tu, Qiang and Chen, Huanhuan",2025.0,,,,IEEE Trans. Artif. Intell.
Robust Causal Discovery under Imperfect Structural Constraints,2511.06790v1,khatibi2024alcm,\cite{khatibi2024alcm},ALCM: Autonomous LLM-Augmented Causal Discovery Framework,https://arxiv.org/abs/2405.01744v2,"To perform effective causal inference in high-dimensional datasets, initiating the process with causal discovery is imperative, wherein a causal graph is generated based on observational data. However, obtaining a complete and accurate causal graph poses a formidable challenge, recognized as an NP- hard problem. Recently, the advent of Large Language Models (LLMs) has ushered in a new era, indicating their emergent capabilities and widespread applicability in facilitating causal reasoning across diverse domains, such as medicine, finance, and science. The expansive knowledge base of LLMs holds the potential to elevate the field of causal reasoning by offering interpretability, making inferences, generalizability, and uncovering novel causal structures. In this paper, we introduce a new framework, named Autonomous LLM-Augmented Causal Discovery Framework (ALCM), to synergize data-driven causal discovery algorithms and LLMs, automating the generation of a more resilient, accurate, and explicable causal graph. The ALCM consists of three integral components: causal structure learning, causal wrapper, and LLM-driven causal refiner. These components autonomously collaborate within a dynamic environment to address causal discovery questions and deliver plausible causal graphs. We evaluate the ALCM framework by implementing two demonstrations on seven well-known datasets. Experimental results demonstrate that ALCM outperforms existing LLM methods and conventional data-driven causal reasoning mechanisms. This study not only shows the effectiveness of the ALCM but also underscores new research directions in leveraging the causal reasoning capabilities of LLMs.",True,True,"Elahe Khatibi and
                  Mahyar Abbasian and
                  Zhongqi Yang and
                  Iman Azimi and
                  Amir M. Rahmani",2024.0,,https://doi.org/10.48550/arXiv.2405.01744,10.48550/ARXIV.2405.01744,CoRR
Robust Causal Discovery under Imperfect Structural Constraints,2511.06790v1,zhou2024causalbench,\cite{zhou2024causalbench},"CausalBench: {A} Comprehensive Benchmark for Causal Learning Capability
                  of Large Language Models",,,True,False,"Yu Zhou and
                  Xingyu Wu and
                  Beicheng Huang and
                  Jibin Wu and
                  Liang Feng and
                  Kay Chen Tan",2024.0,,https://doi.org/10.48550/arXiv.2404.06349,10.48550/ARXIV.2404.06349,CoRR
Robust Causal Discovery under Imperfect Structural Constraints,2511.06790v1,ban2025llm,\cite{ban2025llm},LLM-Driven Causal Discovery via Harmonized Prior,,,True,False,"Taiyu Ban and
                  Lyuzhou Chen and
                  Derui Lyu and
                  Xiangyu Wang and
                  Qinrui Zhu and
                  Huanhuan Chen",2025.0,,https://doi.org/10.1109/TKDE.2025.3528461,10.1109/TKDE.2025.3528461,{IEEE} Trans. Knowl. Data Eng.
Robust Causal Discovery under Imperfect Structural Constraints,2511.06790v1,zhang2021survey,\cite{zhang2021survey},A Survey on Multi-Task Learning,https://arxiv.org/abs/1707.08114v3,"Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL from the perspective of algorithmic modeling, applications and theoretical analyses. For algorithmic modeling, we give a definition of MTL and then classify different MTL algorithms into five categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach and decomposition approach as well as discussing the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, we review online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works in this paper. Finally, we present theoretical analyses and discuss several future directions for MTL.",True,True,"Yu Zhang and
                  Qiang Yang",2022.0,,https://doi.org/10.1109/TKDE.2021.3070203,10.1109/TKDE.2021.3070203,{IEEE} Trans. Knowl. Data Eng.
Robust Causal Discovery under Imperfect Structural Constraints,2511.06790v1,zhao2022inherent,\cite{zhao2022inherent},Inherent Tradeoffs in Learning Fair Representations,https://arxiv.org/abs/1906.08386v6,"Real-world applications of machine learning tools in high-stakes domains are often regulated to be fair, in the sense that the predicted target should satisfy some quantitative notion of parity with respect to a protected attribute. However, the exact tradeoff between fairness and accuracy is not entirely clear, even for the basic paradigm of classification problems. In this paper, we characterize an inherent tradeoff between statistical parity and accuracy in the classification setting by providing a lower bound on the sum of group-wise errors of any fair classifiers. Our impossibility theorem could be interpreted as a certain uncertainty principle in fairness: if the base rates differ among groups, then any fair classifier satisfying statistical parity has to incur a large error on at least one of the groups. We further extend this result to give a lower bound on the joint error of any (approximately) fair classifiers, from the perspective of learning fair representations. To show that our lower bound is tight, assuming oracle access to Bayes (potentially unfair) classifiers, we also construct an algorithm that returns a randomized classifier that is both optimal (in terms of accuracy) and fair. Interestingly, when the protected attribute can take more than two values, an extension of this lower bound does not admit an analytic solution. Nevertheless, in this case, we show that the lower bound can be efficiently computed by solving a linear program, which we term as the TV-Barycenter problem, a barycenter problem under the TV-distance. On the upside, we prove that if the group-wise Bayes optimal classifiers are close, then learning fair representations leads to an alternative notion of fairness, known as the accuracy parity, which states that the error rates are close between groups. Finally, we also conduct experiments on real-world datasets to confirm our theoretical findings.",True,True,"Han Zhao and
                  Geoffrey J. Gordon",2022.0,,https://jmlr.org/papers/v23/21-1427.html,,J. Mach. Learn. Res.
Robust Causal Discovery under Imperfect Structural Constraints,2511.06790v1,lin2023libmtl,\cite{lin2023libmtl},LibMTL: {A} Python Library for Deep Multi-Task Learning,,,True,False,"Baijiong Lin and
                  Yu Zhang",2023.0,,https://jmlr.org/papers/v24/22-0347.html,,J. Mach. Learn. Res.
Robust Causal Discovery under Imperfect Structural Constraints,2511.06790v1,zhang2024libmoon,\cite{zhang2024libmoon},LibMOON: A Gradient-based MultiObjective OptimizatioN Library in PyTorch,https://arxiv.org/abs/2409.02969v3,"Multiobjective optimization problems (MOPs) are prevalent in machine learning, with applications in multi-task learning, learning under fairness or robustness constraints, etc. Instead of reducing multiple objective functions into a scalar objective, MOPs aim to optimize for the so-called Pareto optimality or Pareto set learning, which involves optimizing more than one objective function simultaneously, over models with thousands / millions of parameters. Existing benchmark libraries for MOPs mainly focus on evolutionary algorithms, most of which are zeroth-order / meta-heuristic methods that do not effectively utilize higher-order information from objectives and cannot scale to large-scale models with thousands / millions of parameters. In light of the above gap, this paper introduces LibMOON, the first multiobjective optimization library that supports state-of-the-art gradient-based methods, provides a fair benchmark, and is open-sourced for the community.",True,True,"Zhang, Xiaoyuan and Zhao, Liang and Yu, Yingying and Lin, Xi and Chen, Yifan and Zhao, Han and Zhang, Qingfu",2024.0,,,,
Robust Causal Discovery under Imperfect Structural Constraints,2511.06790v1,miettinen1999nonlinear,\cite{miettinen1999nonlinear},Nonlinear multiobjective optimization,,,True,False,Kaisa Miettinen,1998.0,,,,
Robust Causal Discovery under Imperfect Structural Constraints,2511.06790v1,zhang2007moea,\cite{zhang2007moea},{MOEA/D:} {A} Multiobjective Evolutionary Algorithm Based on Decomposition,,,True,False,"Qingfu Zhang and
                  Hui Li",2007.0,,https://doi.org/10.1109/TEVC.2007.892759,10.1109/TEVC.2007.892759,{IEEE} Trans. Evol. Comput.
Robust Causal Discovery under Imperfect Structural Constraints,2511.06790v1,lin2024smooth,\cite{lin2024smooth},Smooth Tchebycheff Scalarization for Multi-Objective Optimization,,,True,False,"Lin, Xi and Zhang, Xiaoyuan and Yang, Zhiyuan and Liu, Fei and Wang, Zhenkun and Zhang, Qingfu",2024.0,,,,
Robust Causal Discovery under Imperfect Structural Constraints,2511.06790v1,sener2018multi,\cite{sener2018multi},Multi-Task Learning as Multi-Objective Optimization,https://arxiv.org/abs/1810.04650v2,"In multi-task learning, multiple tasks are solved jointly, sharing inductive bias between them. Multi-task learning is inherently a multi-objective problem because different tasks may conflict, necessitating a trade-off. A common compromise is to optimize a proxy objective that minimizes a weighted linear combination of per-task losses. However, this workaround is only valid when the tasks do not compete, which is rarely the case. In this paper, we explicitly cast multi-task learning as multi-objective optimization, with the overall objective of finding a Pareto optimal solution. To this end, we use algorithms developed in the gradient-based multi-objective optimization literature. These algorithms are not directly applicable to large-scale learning problems since they scale poorly with the dimensionality of the gradients and the number of tasks. We therefore propose an upper bound for the multi-objective loss and show that it can be optimized efficiently. We further prove that optimizing this upper bound yields a Pareto optimal solution under realistic assumptions. We apply our method to a variety of multi-task deep learning problems including digit classification, scene understanding (joint semantic segmentation, instance segmentation, and depth estimation), and multi-label classification. Our method produces higher-performing models than recent multi-task learning formulations or per-task training.",True,True,"Sener, Ozan and Koltun, Vladlen",2018.0,,,,
Robust Causal Discovery under Imperfect Structural Constraints,2511.06790v1,lin2019pareto,\cite{lin2019pareto},Pareto Multi-Task Learning,https://arxiv.org/abs/1912.12854v1,"Multi-task learning is a powerful method for solving multiple correlated tasks simultaneously. However, it is often impossible to find one single solution to optimize all the tasks, since different tasks might conflict with each other. Recently, a novel method is proposed to find one single Pareto optimal solution with good trade-off among different tasks by casting multi-task learning as multiobjective optimization. In this paper, we generalize this idea and propose a novel Pareto multi-task learning algorithm (Pareto MTL) to find a set of well-distributed Pareto solutions which can represent different trade-offs among different tasks. The proposed algorithm first formulates a multi-task learning problem as a multiobjective optimization problem, and then decomposes the multiobjective optimization problem into a set of constrained subproblems with different trade-off preferences. By solving these subproblems in parallel, Pareto MTL can find a set of well-representative Pareto optimal solutions with different trade-off among all tasks. Practitioners can easily select their preferred solution from these Pareto solutions, or use different trade-off solutions for different situations. Experimental results confirm that the proposed algorithm can generate well-representative solutions and outperform some state-of-the-art algorithms on many multi-task learning applications.",True,True,"Lin, Xi and Zhen, Hui-Ling and Li, Zhenhua and Zhang, Qing-Fu and Kwong, Sam",2019.0,,,,
Robust Causal Discovery under Imperfect Structural Constraints,2511.06790v1,chen2018gradnorm,\cite{chen2018gradnorm},Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks,,,True,False,"Chen, Zhao and Badrinarayanan, Vijay and Lee, Chen-Yu and Rabinovich, Andrew",2018.0,,,,
Causal Regime Detection in Energy Markets With Augmented Time Series Structural Causal Models,2511.04361v1,gao2018causal,\cite{gao2018causal},Causal data science for financial stress testing,,,True,False,"Gao, Gelin and Mishra, Bud and Ramazzotti, Daniele",2018.0,,,,Journal of computational science
Causal Regime Detection in Energy Markets With Augmented Time Series Structural Causal Models,2511.04361v1,rigana2024navigating,\cite{rigana2024navigating},Navigating Market Turbulence: Insights from Causal Network Contagion Value at Risk,https://arxiv.org/abs/2402.06032v1,"Accurately defining, measuring and mitigating risk is a cornerstone of financial risk management, especially in the presence of financial contagion. Traditional correlation-based risk assessment methods often struggle under volatile market conditions, particularly in the face of external shocks, highlighting the need for a more robust and invariant predictive approach.
  This paper introduces the Causal Network Contagion Value at Risk (Causal-NECO VaR), a novel methodology that significantly advances causal inference in financial risk analysis. Embracing a causal network framework, this method adeptly captures and analyses volatility and spillover effects, effectively setting it apart from conventional contagion-based VaR models. Causal-NECO VaR's key innovation lies in its ability to derive directional influences among assets from observational data, thereby offering robust risk predictions that remain invariant to market shocks and systemic changes.
  A comprehensive simulation study and the application to the Forex market show the robustness of the method. Causal-NECO VaR not only demonstrates predictive accuracy, but also maintains its reliability in unstable financial environments, offering clearer risk assessments even amidst unforeseen market disturbances. This research makes a significant contribution to the field of risk management and financial stability, presenting a causal approach to the computation of VaR. It emphasises the model's superior resilience and invariant predictive power, essential for navigating the complexities of today's ever-evolving financial markets.",True,True,Katerina Rigana and Ernst C. Wit and Samantha Cook,2024.0,,https://arxiv.org/abs/2402.06032,,
Causal Regime Detection in Energy Markets With Augmented Time Series Structural Causal Models,2511.04361v1,lago2021forecasting,\cite{lago2021forecasting},"Forecasting day-ahead electricity prices: A review of state-of-the-art algorithms, best practices and an open-access benchmark",,,True,False,"Lago, Jesus and Marcjasz, Grzegorz and De Schutter, Bart and Weron, Rafał",2021.0,,,10.1016/j.apenergy.2021.116983,Applied Energy
Causal Regime Detection in Energy Markets With Augmented Time Series Structural Causal Models,2511.04361v1,ziel2018day,\cite{ziel2018day},Day-ahead electricity price forecasting with high-dimensional structures: Univariate vs. multivariate modeling frameworks,,,True,False,"Ziel, Florian and Weron, Rafał",2018.0,,,10.1016/j.eneco.2017.12.016,Energy Economics
Causal Regime Detection in Energy Markets With Augmented Time Series Structural Causal Models,2511.04361v1,pan2024counterfactual,\cite{pan2024counterfactual},Counterfactual Image Editing,https://arxiv.org/abs/2403.09683v1,"Counterfactual image editing is an important task in generative AI, which asks how an image would look if certain features were different. The current literature on the topic focuses primarily on changing individual features while remaining silent about the causal relationships between these features, as present in the real world. In this paper, we formalize the counterfactual image editing task using formal language, modeling the causal relationships between latent generative factors and images through a special type of model called augmented structural causal models (ASCMs). Second, we show two fundamental impossibility results: (1) counterfactual editing is impossible from i.i.d. image samples and their corresponding labels alone; (2) even when the causal relationships between the latent generative factors and images are available, no guarantees regarding the output of the model can be provided. Third, we propose a relaxation for this challenging problem by approximating non-identifiable counterfactual distributions with a new family of counterfactual-consistent estimators. This family exhibits the desirable property of preserving features that the user cares about across both factual and counterfactual worlds. Finally, we develop an efficient algorithm to generate counterfactual images by leveraging neural causal models.",True,True,"Pan, Yushu and Bareinboim, Elias",2024.0,21--27 Jul,https://proceedings.mlr.press/v235/pan24a.html,,
Causal Regime Detection in Energy Markets With Augmented Time Series Structural Causal Models,2511.04361v1,pan2025counterfactual,\cite{pan2025counterfactual},Counterfactual Image Editing with Disentangled Causal Latent Space,,,True,False,"Pan, Yushu and Bareinboim, Elias",2025.0,May,https://cs.columbia.edu/~eb,,
Causal Regime Detection in Energy Markets With Augmented Time Series Structural Causal Models,2511.04361v1,thumm2025towards,\cite{thumm2025towards},Towards Causal Market Simulators,https://arxiv.org/abs/2511.04469v1,"Market generators using deep generative models have shown promise for synthetic financial data generation, but existing approaches lack causal reasoning capabilities essential for counterfactual analysis and risk assessment. We propose a Time-series Neural Causal Model VAE (TNCM-VAE) that combines variational autoencoders with structural causal models to generate counterfactual financial time series while preserving both temporal dependencies and causal relationships. Our approach enforces causal constraints through directed acyclic graphs in the decoder architecture and employs the causal Wasserstein distance for training. We validate our method on synthetic autoregressive models inspired by the Ornstein-Uhlenbeck process, demonstrating superior performance in counterfactual probability estimation with L1 distances as low as 0.03-0.10 compared to ground truth. The model enables financial stress testing, scenario analysis, and enhanced backtesting by generating plausible counterfactual market trajectories that respect underlying causal mechanisms.",True,True,"Thumm, Dennis and Mijares, Luis Ontaneda",2025.0,,https://icaif-25-rfts.github.io,,
Causal Regime Detection in Energy Markets With Augmented Time Series Structural Causal Models,2511.04361v1,bica2020estimatingthe,\cite{bica2020estimatingthe},Estimating the effects of continuous-valued interventions using generative adversarial networks,,,True,False,"Bica, Ioana and Jordon, James and van der Schaar, Mihaela",2020.0,,,,Advances in Neural Information Processing Systems
Causal Regime Detection in Energy Markets With Augmented Time Series Structural Causal Models,2511.04361v1,melnychuk2022causal,\cite{melnychuk2022causal},Causal transformer for estimating counterfactual outcomes,,,True,False,"Melnychuk, Valentyn and Frauen, Dennis and Feuerriegel, Stefan",2022.0,,,,
Applying Time Series Deep Learning Models to Forecast the Growth of Perennial Ryegrass in Ireland,2511.03749v1,Ngo:2021,\cite{Ngo:2021},Electronic farming records – A framework for normalising agronomic knowledge discovery,,,True,False,Vuong M. Ngo and M-Tahar Kechadi,2021.0,,,10.1016/j.compag.2021.106074,Computers and Electronics in Agriculture
Applying Time Series Deep Learning Models to Forecast the Growth of Perennial Ryegrass in Ireland,2511.03749v1,Ngo:2023,\cite{Ngo:2023},A big data smart agricultural system: recommending optimum fertilisers for crops,,,True,False,"Vuong M. Ngo and  Duong, Thuy-Van T. and Nguyen, Tat-Bao-Thien and Dang, Cach N. and Conlan, Owen",2023.0,,,10.1007/s41870-022-01150-1,International Journal of Information Technology
Applying Time Series Deep Learning Models to Forecast the Growth of Perennial Ryegrass in Ireland,2511.03749v1,Benedict:2023,\cite{Benedict:2023},Extracting exotic annual grass phenology and climate relations in western U.S. rangeland ecoregions,,,True,False,"Benedict, Trenton D. and Boyte, Stephen P. and Dahal, Devendra and Shrestha, Dinesh and Parajuli, Sujan  and Megard, Logan J.",2023.0,,,10.1007/s10530-023-03021-7,Biological Invasions
Applying Time Series Deep Learning Models to Forecast the Growth of Perennial Ryegrass in Ireland,2511.03749v1,ngo2019designing,\cite{ngo2019designing},Designing and implementing data warehouse for agricultural big data,,,True,False,"Ngo, Vuong M and Le-Khac, Nhien-An and Kechadi, M-Tahar",2019.0,,,,
Applying Time Series Deep Learning Models to Forecast the Growth of Perennial Ryegrass in Ireland,2511.03749v1,Sapkota:2020,\cite{Sapkota:2020},Detection of Italian Ryegrass in Wheat and Prediction of Competitive Interactions Using Remote-Sensing and Machine-Learning Techniques,,,True,False,"Sapkota, Bishwa and Singh, Vijay and Neely, Clark and Rajan, Nithya and Bagavathiannan, Muthukumar",2020.0,,,10.3390/rs12182977,Remote Sensing
Applying Time Series Deep Learning Models to Forecast the Growth of Perennial Ryegrass in Ireland,2511.03749v1,Holtgrave:2023,\cite{Holtgrave:2023},"Grassland mowing event detection using combined optical, SAR, and weather time series",,,True,False,Ann-Kathrin Holtgrave and Felix Lobert and Stefan Erasmi and Norbert Röder and Birgit Kleinschmit,2023.0,,,10.1016/j.rse.2023.113680,Remote Sensing of Environment
Applying Time Series Deep Learning Models to Forecast the Growth of Perennial Ryegrass in Ireland,2511.03749v1,Defalque:2024,\cite{Defalque:2024},Machine learning models for dry matter and biomass estimates on cattle grazing systems,,,True,False,Guilherme Defalque and Ricardo Santos and Davi Bungenstab and Diego Echeverria and Alexandre Dias and Cristiane Defalque,2024.0,,,10.1016/j.compag.2023.108520,Computers and Electronics in Agriculture
Applying Time Series Deep Learning Models to Forecast the Growth of Perennial Ryegrass in Ireland,2511.03749v1,KRGS19,\cite{KRGS19},"Predicting grass growth for Sustainable Dairy Farming: A CBR system using bayesian case-exclusion and Post-Hoc, personalized explanation-by-example (XAI)",,,True,False,"Kenny, Eoin M. and Ruelle, Elodie and Geoghegan, Anne and Shalloo, Laurence and O’Leary, Micheál and O’Donovan, Michael and Keane, Mark T.",2019.0,,,10.1007/978-3-030-29249-2_12,Case-Based Reasoning Research and Development
Applying Time Series Deep Learning Models to Forecast the Growth of Perennial Ryegrass in Ireland,2511.03749v1,MBLJ20,\cite{MBLJ20},A decision analytic framework and exploratory statistical case study analysis of grass growth in Northern Ireland,,,True,False,"McHugh, Orla and Browne, Fiona and Liu, Jun and Jordan, Philip",2020.0,,,10.12720/jait.11.1.15-20,Journal of Advances in Information Technology
Applying Time Series Deep Learning Models to Forecast the Growth of Perennial Ryegrass in Ireland,2511.03749v1,PBADGQ21,\cite{PBADGQ21},Improving accuracy of herbage yield predictions in perennial ryegrass with UAV-based structural and spectral data fusion and machine learning,,,True,False,"Pranga, Joanna and Borra-Serrano, Irene and Aper, Jonas and De Swaef, Tom and Ghesquiere, An and Quataert, Paul and Roldán-Ruiz, Isabel and Janssens, Ivan A. and Ruysschaert, Greet and Lootens, Peter and et al.",2021.0,,,10.3390/rs13173459,Remote Sensing
Applying Time Series Deep Learning Models to Forecast the Growth of Perennial Ryegrass in Ireland,2511.03749v1,Yoo:2020,\cite{Yoo:2020},Time Series Forecasting of Agricultural Products’ Sales Volumes Based on Seasonal Long Short-Term Memory,,,True,False,"Yoo, Tae-Woong and Oh, Il-Seok",2020.0,,,10.3390/app10228169,Applied Sciences
Applying Time Series Deep Learning Models to Forecast the Growth of Perennial Ryegrass in Ireland,2511.03749v1,Yuan:2023,\cite{Yuan:2023},Bridging optical and SAR satellite image time series via contrastive feature extraction for crop classification,,,True,False,Yuan Yuan and Lei Lin and Zeng-Guang Zhou and Houjun Jiang and Qingshan Liu,2023.0,,,10.1016/j.isprsjprs.2022.11.020,ISPRS Journal of Photogrammetry and Remote Sensing
Applying Time Series Deep Learning Models to Forecast the Growth of Perennial Ryegrass in Ireland,2511.03749v1,Quan:2023,\cite{Quan:2023},Multimodal remote sensing application for weed competition time series analysis in maize farmland ecosystems,,,True,False,Longzhe Quan and Zhaoxia Lou and Xiaolan Lv and Deng Sun and Fulin Xia and Hailong Li and Wenfeng Sun,2023.0,,,10.1016/j.jenvman.2023.118376,Journal of Environmental Management
Siegel Neural Networks,2511.09577v1,chen2024riemannian,\cite{chen2024riemannian},{Riemannian Multinomial Logistics Regression for SPD Neural Networks},,,True,False,Ziheng Chen and Yue Song and Gaowen Liu and Ramana Rao Kompella and Xiaojun Wu and Nicu Sebe,2024.0,,http://arxiv.org/abs/2305.11288,,CoRR
Siegel Neural Networks,2511.09577v1,NguyenGyroMatMans23,\cite{NguyenGyroMatMans23},Building Neural Networks on Matrix Manifolds: A Gyrovector Space Approach,https://arxiv.org/abs/2305.04560v3,"Matrix manifolds, such as manifolds of Symmetric Positive Definite (SPD) matrices and Grassmann manifolds, appear in many applications. Recently, by applying the theory of gyrogroups and gyrovector spaces that is a powerful framework for studying hyperbolic geometry, some works have attempted to build principled generalizations of Euclidean neural networks on matrix manifolds. However, due to the lack of many concepts in gyrovector spaces for the considered manifolds, e.g., the inner product and gyroangles, techniques and mathematical tools provided by these works are still limited compared to those developed for studying hyperbolic geometry. In this paper, we generalize some notions in gyrovector spaces for SPD and Grassmann manifolds, and propose new models and layers for building neural networks on these manifolds. We show the effectiveness of our approach in two applications, i.e., human action recognition and knowledge graph completion.",True,True,Xuan Son Nguyen and Shuo Yang,2023.0,,,,
Siegel Neural Networks,2511.09577v1,NguyenICLR24,\cite{NguyenICLR24},Matrix Manifold Neural Networks++,https://arxiv.org/abs/2405.19206v1,"Deep neural networks (DNNs) on Riemannian manifolds have garnered increasing interest in various applied areas. For instance, DNNs on spherical and hyperbolic manifolds have been designed to solve a wide range of computer vision and nature language processing tasks. One of the key factors that contribute to the success of these networks is that spherical and hyperbolic manifolds have the rich algebraic structures of gyrogroups and gyrovector spaces. This enables principled and effective generalizations of the most successful DNNs to these manifolds. Recently, some works have shown that many concepts in the theory of gyrogroups and gyrovector spaces can also be generalized to matrix manifolds such as Symmetric Positive Definite (SPD) and Grassmann manifolds. As a result, some building blocks for SPD and Grassmann neural networks, e.g., isometric models and multinomial logistic regression (MLR) can be derived in a way that is fully analogous to their spherical and hyperbolic counterparts. Building upon these works, we design fully-connected (FC) and convolutional layers for SPD neural networks. We also develop MLR on Symmetric Positive Semi-definite (SPSD) manifolds, and propose a method for performing backpropagation with the Grassmann logarithmic map in the projector perspective. We demonstrate the effectiveness of the proposed approach in the human action recognition and node classification tasks.",True,True,Xuan Son Nguyen and Shuo Yang and Aymeric Histace,2024.0,,,,
Siegel Neural Networks,2511.09577v1,BdeirFullyHnnCv24,\cite{BdeirFullyHnnCv24},Fully Hyperbolic Convolutional Neural Networks,https://arxiv.org/abs/1905.10484v3,"Convolutional Neural Networks (CNN) have recently seen tremendous success in various computer vision tasks. However, their application to problems with high dimensional input and output, such as high-resolution image and video segmentation or 3D medical imaging, has been limited by various factors. Primarily, in the training stage, it is necessary to store network activations for back propagation. In these settings, the memory requirements associated with storing activations can exceed what is feasible with current hardware, especially for problems in 3D. Motivated by the propagation of signals over physical networks, that are governed by the hyperbolic Telegraph equation, in this work we introduce a fully conservative hyperbolic network for problems with high dimensional input and output. We introduce a coarsening operation that allows completely reversible CNNs by using a learnable Discrete Wavelet Transform and its inverse to both coarsen and interpolate the network state and change the number of channels. We show that fully reversible networks are able to achieve results comparable to the state of the art in 4D time-lapse hyper spectral image segmentation and full 3D video segmentation, with a much lower memory footprint that is a constant independent of the network depth. We also extend the use of such networks to Variational Auto Encoders with high resolution input and output.",True,True,Ahmad Bdeir and Kristian Schwethelm and Niels Landwehr,2024.0,,,,
Siegel Neural Networks,2511.09577v1,NEURIPS2018_dbab2adc,\cite{NEURIPS2018_dbab2adc},Hyperbolic Neural Networks++,https://arxiv.org/abs/2006.08210v3,"Hyperbolic spaces, which have the capacity to embed tree structures without distortion owing to their exponential volume growth, have recently been applied to machine learning to better capture the hierarchical nature of data. In this study, we generalize the fundamental components of neural networks in a single hyperbolic geometry model, namely, the Poincaré ball model. This novel methodology constructs a multinomial logistic regression, fully-connected layers, convolutional layers, and attention mechanisms under a unified mathematical interpretation, without increasing the parameters. Experiments show the superior parameter efficiency of our methods compared to conventional hyperbolic components, and stability and outperformance over their Euclidean counterparts.",True,True,Octavian-Eugen Ganea and Gary B\'ecigneul and Thomas Hofmann,2018.0,,,,
Siegel Neural Networks,2511.09577v1,LebanonMarginClassifierICML04,\cite{LebanonMarginClassifierICML04},{Hyperplane Margin Classifiers on the Multinomial Manifold},,,True,False,"Lebanon, Guy and Lafferty, John",2004.0,,,,
Siegel Neural Networks,2511.09577v1,shimizu2021hyperbolic,\cite{shimizu2021hyperbolic},Hyperbolic Neural Networks++,https://arxiv.org/abs/2006.08210v3,"Hyperbolic spaces, which have the capacity to embed tree structures without distortion owing to their exponential volume growth, have recently been applied to machine learning to better capture the hierarchical nature of data. In this study, we generalize the fundamental components of neural networks in a single hyperbolic geometry model, namely, the Poincaré ball model. This novel methodology constructs a multinomial logistic regression, fully-connected layers, convolutional layers, and attention mechanisms under a unified mathematical interpretation, without increasing the parameters. Experiments show the superior parameter efficiency of our methods compared to conventional hyperbolic components, and stability and outperformance over their Euclidean counterparts.",True,True,Ryohei Shimizu and Yusuke Mukuta and Tatsuya Harada,2021.0,,https://arxiv.org/abs/2006.08210,,CoRR
Siegel Neural Networks,2511.09577v1,NguyenICLR25,\cite{NguyenICLR25},{Neural Networks on Symmetric Spaces of Noncompact Type},,,True,False,Xuan Son Nguyen and Shuo Yang and Aymeric Histace,2025.0,,,,
Siegel Neural Networks,2511.09577v1,Sonoda2022FCRidgele,\cite{Sonoda2022FCRidgele},Fully-Connected Network on Noncompact Symmetric Space and Ridgelet Transform based on Helgason-Fourier Analysis,https://arxiv.org/abs/2203.01631v2,"Neural network on Riemannian symmetric space such as hyperbolic space and the manifold of symmetric positive definite (SPD) matrices is an emerging subject of research in geometric deep learning. Based on the well-established framework of the Helgason-Fourier transform on the noncompact symmetric space, we present a fully-connected network and its associated ridgelet transform on the noncompact symmetric space, covering the hyperbolic neural network (HNN) and the SPDNet as special cases. The ridgelet transform is an analysis operator of a depth-2 continuous network spanned by neurons, namely, it maps an arbitrary given function to the weights of a network. Thanks to the coordinate-free reformulation, the role of nonlinear activation functions is revealed to be a wavelet function, and the reconstruction formula directly yields the universality of the proposed networks.",True,True,Sho Sonoda and Isao Ishikawa and Masahiro Ikeda,2022.0,,,,
Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games,2511.05640v1,Ahuja&Orlin2001,\cite{Ahuja&Orlin2001},Deep Inverse Optimization,https://arxiv.org/abs/1812.00804v1,"Given a set of observations generated by an optimization process, the goal of inverse optimization is to determine likely parameters of that process. We cast inverse optimization as a form of deep learning. Our method, called deep inverse optimization, is to unroll an iterative optimization process and then use backpropagation to learn parameters that generate the observations. We demonstrate that by backpropagating through the interior point algorithm we can learn the coefficients determining the cost vector and the constraints, independently or jointly, for both non-parametric and parametric linear programs, starting from one or multiple observations. With this approach, inverse optimization can leverage concepts and algorithms from deep learning.",True,True,"Ahuja, Ravindra K and Orlin, James B",2001.0,,,,Operations research
Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games,2511.05640v1,Chan2022,\cite{Chan2022},Inverse optimization: Theory and applications,,,True,False,"Chan, Timothy C. Y. and Mahmood, Rafid and Zhu, Ian Yihang",2023.0,,,,Operations Research
Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games,2511.05640v1,Ng&Russell2000,\cite{Ng&Russell2000},Algorithms for inverse reinforcement learning,,,True,False,"Ng, Andrew Y and Russell, Stuart J",2000.0,,,,
Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games,2511.05640v1,Ziebart2008,\cite{Ziebart2008},Weighted Maximum Entropy Inverse Reinforcement Learning,https://arxiv.org/abs/2208.09611v1,"We study inverse reinforcement learning (IRL) and imitation learning (IM), the problems of recovering a reward or policy function from expert's demonstrated trajectories. We propose a new way to improve the learning process by adding a weight function to the maximum entropy framework, with the motivation of having the ability to learn and recover the stochasticity (or the bounded rationality) of the expert policy. Our framework and algorithms allow to learn both a reward (or policy) function and the structure of the entropy terms added to the Markov Decision Processes, thus enhancing the learning procedure. Our numerical experiments using human and simulated demonstrations and with discrete and continuous IRL/IM tasks show that our approach outperforms prior algorithms.",True,True,"Ziebart, Brian D and Maas, Andrew and Bagnell, J Andrew and Dey, Anind K",2008.0,,,,
Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games,2511.05640v1,Wulfmeier2016,\cite{Wulfmeier2016},Maximum Entropy Deep Inverse Reinforcement Learning,https://arxiv.org/abs/1507.04888v3,"This paper presents a general framework for exploiting the representational capacity of neural networks to approximate complex, nonlinear reward functions in the context of solving the inverse reinforcement learning (IRL) problem. We show in this context that the Maximum Entropy paradigm for IRL lends itself naturally to the efficient training of deep architectures. At test time, the approach leads to a computational complexity independent of the number of demonstrations, which makes it especially well-suited for applications in life-long learning scenarios. Our approach achieves performance commensurate to the state-of-the-art on existing benchmarks while exceeding on an alternative benchmark based on highly varying reward structures. Finally, we extend the basic architecture - which is equivalent to a simplified subclass of Fully Convolutional Neural Networks (FCNNs) with width one - to include larger convolutions in order to eliminate dependency on precomputed spatial features and work on raw input representations.",True,True,"Wulfmeier, Markus and Ondruska, Peter and Posner, Ingmar",2016.0,,,,arXiv preprint arXiv:1507.04888
Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games,2511.05640v1,Finn2016,\cite{Finn2016},Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization,https://arxiv.org/abs/1603.00448v3,"Reinforcement learning can acquire complex behaviors from high-level specifications. However, defining a cost function that can be optimized effectively and encodes the correct task is challenging in practice. We explore how inverse optimal control (IOC) can be used to learn behaviors from demonstrations, with applications to torque control of high-dimensional robotic systems. Our method addresses two key challenges in inverse optimal control: first, the need for informative features and effective regularization to impose structure on the cost, and second, the difficulty of learning the cost function under unknown dynamics for high-dimensional continuous systems. To address the former challenge, we present an algorithm capable of learning arbitrary nonlinear cost functions, such as neural networks, without meticulous feature engineering. To address the latter challenge, we formulate an efficient sample-based approximation for MaxEnt IOC. We evaluate our method on a series of simulated tasks and real-world robotic manipulation problems, demonstrating substantial improvement over prior methods both in terms of task complexity and sample efficiency.",True,True,"Finn, Chelsea and Levine, Sergey and Abbeel, Pieter",2016.0,,,,
Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games,2511.05640v1,Cao2021,\cite{Cao2021},Identifiability in inverse reinforcement learning,https://arxiv.org/abs/2106.03498v3,"Inverse reinforcement learning attempts to reconstruct the reward function in a Markov decision problem, using observations of agent actions. As already observed in Russell [1998] the problem is ill-posed, and the reward function is not identifiable, even under the presence of perfect information about optimal behavior. We provide a resolution to this non-identifiability for problems with entropy regularization. For a given environment, we fully characterize the reward functions leading to a given policy and demonstrate that, given demonstrations of actions for the same reward under two distinct discount factors, or under sufficiently different environments, the unobserved reward can be recovered up to a constant. We also give general necessary and sufficient conditions for reconstruction of time-homogeneous rewards on finite horizons, and for action-independent rewards, generalizing recent results of Kim et al. [2021] and Fu et al. [2018].",True,True,"Cao, Haoyang and Cohen, Samuel and Szpruch, Lukasz",2021.0,,,,
Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games,2511.05640v1,Rolland2022,\cite{Rolland2022},Identifiability and generalizability from multiple experts in inverse reinforcement learning,,,True,False,"Rolland, Paul and Viano, Luca and Sch{\""u}rhoff, Norman and Nikolov, Boris and Cevher, Volkan",2022.0,,,,
Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games,2511.05640v1,Vorobeychik2007,\cite{Vorobeychik2007},Learning payoff functions in infinite games,,,True,False,"Vorobeychik, Yevgeniy and Wellman, Michael P and Singh, Satinder",2007.0,,,,Machine Learning
Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games,2511.05640v1,Arora&Doshi2021,\cite{Arora&Doshi2021},"A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress",https://arxiv.org/abs/1806.06877v3,"Inverse reinforcement learning (IRL) is the problem of inferring the reward function of an agent, given its policy or observed behavior. Analogous to RL, IRL is perceived both as a problem and as a class of methods. By categorically surveying the current literature in IRL, this article serves as a reference for researchers and practitioners of machine learning and beyond to understand the challenges of IRL and select the approaches best suited for the problem on hand. The survey formally introduces the IRL problem along with its central challenges such as the difficulty in performing accurate inference and its generalizability, its sensitivity to prior knowledge, and the disproportionate growth in solution complexity with problem size. The article elaborates how the current methods mitigate these challenges. We further discuss the extensions to traditional IRL methods for handling: inaccurate and incomplete perception, an incomplete model, multiple reward functions, and nonlinear reward functions. This survey concludes the discussion with some broad advances in the research area and currently open research questions.",True,True,"Arora, Saurabh and Doshi, Prashant",2021.0,,,,Artificial Intelligence
Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games,2511.05640v1,Lin2014,\cite{Lin2014},Multiagent Inverse Reinforcement Learning for Two-Person Zero-Sum Games,,,True,False,"Lin, Xiaomin and Beling, Peter A. and Cogill, Randy",2018.0,,,,IEEE Transactions on Games
Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games,2511.05640v1,Kuleshov&Schrijvers2015,\cite{Kuleshov&Schrijvers2015},Inverse game theory: Learning utilities in succinct games,,,True,False,"Kuleshov, Volodymyr and Schrijvers, Okke",2015.0,,,,
Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games,2511.05640v1,Waugh2011,\cite{Waugh2011},Computational Rationalization: The Inverse Equilibrium Problem,,,True,False,"Waugh, Kevin and Ziebart, Brian D and Bagnell, J Andrew",2011.0,,,,
Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games,2511.05640v1,Wang&Klabjan2018,\cite{Wang&Klabjan2018},Competitive Multi-agent Inverse Reinforcement Learning with Sub-optimal Demonstrations,https://arxiv.org/abs/1801.02124v2,"This paper considers the problem of inverse reinforcement learning in zero-sum stochastic games when expert demonstrations are known to be not optimal. Compared to previous works that decouple agents in the game by assuming optimality in expert strategies, we introduce a new objective function that directly pits experts against Nash Equilibrium strategies, and we design an algorithm to solve for the reward function in the context of inverse reinforcement learning with deep neural networks as model approximations. In our setting the model and algorithm do not decouple by agent. In order to find Nash Equilibrium in large-scale games, we also propose an adversarial training algorithm for zero-sum stochastic games, and show the theoretical appeal of non-existence of local optima in its objective function. In our numerical experiments, we demonstrate that our Nash Equilibrium and inverse reinforcement learning algorithms address games that are not amenable to previous approaches using tabular representations. Moreover, with sub-optimal expert demonstrations our algorithms recover both reward functions and strategies with good quality.",True,True,"Wang, Xingyu and Klabjan, Diego",2018.0,,,,
Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games,2511.05640v1,Wu2022,\cite{Wu2022},Inverse game theory for stackelberg games: the blessing of bounded rationality,,,True,False,"Wu, Jibang and Shen, Weiran and Fang, Fei and Xu, Haifeng",2022.0,,,,
Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games,2511.05640v1,Konstantakopoulos2017,\cite{Konstantakopoulos2017},A Robust Utility Learning Framework via Inverse Optimization,https://arxiv.org/abs/1704.07933v1,"In many smart infrastructure applications flexibility in achieving sustainability goals can be gained by engaging end-users. However, these users often have heterogeneous preferences that are unknown to the decision-maker tasked with improving operational efficiency. Modeling user interaction as a continuous game between non-cooperative players, we propose a robust parametric utility learning framework that employs constrained feasible generalized least squares estimation with heteroskedastic inference. To improve forecasting performance, we extend the robust utility learning scheme by employing bootstrapping with bagging, bumping, and gradient boosting ensemble methods. Moreover, we estimate the noise covariance which provides approximated correlations between players which we leverage to develop a novel correlated utility learning framework. We apply the proposed methods both to a toy example arising from Bertrand-Nash competition between two firms as well as to data from a social game experiment designed to encourage energy efficient behavior amongst smart building occupants. Using occupant voting data for shared resources such as lighting, we simulate the game defined by the estimated utility functions to demonstrate the performance of the proposed methods.",True,True,"Konstantakopoulos, Ioannis C and Ratliff, Lillian J and Jin, Ming and Sastry, S Shankar and Spanos, Costas J",2018.0,,,,IEEE Transactions on Control Systems Technology
Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games,2511.05640v1,McKelvey&Palfrey1995,\cite{McKelvey&Palfrey1995},Quantal response equilibria for normal form games,,,True,False,"McKelvey, Richard D and Palfrey, Thomas R",1995.0,,,,Games and economic behavior
Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games,2511.05640v1,Mertikopoulos&Sandholm2016,\cite{Mertikopoulos&Sandholm2016},Learning in games via reinforcement and regularization,,,True,False,"Mertikopoulos, Panayotis and Sandholm, William H",2016.0,,,,Mathematics of Operations Research
Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games,2511.05640v1,Goeree2016,\cite{Goeree2016},Quantal Response Equilibrium: A Stochastic Theory of Games,,,True,False,"Goeree, Jacob K and Holt, Charles A and Palfrey, Thomas R",2016.0,,,,
Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games,2511.05640v1,Camerer2003,\cite{Camerer2003},Behavioral game theory: Experiments in strategic interaction,,,True,False,"Camerer, Colin F",2003.0,,,,
Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games,2511.05640v1,Haarnoja2018,\cite{Haarnoja2018},Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor,,,True,False,"Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey",2018.0,,,,
Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games,2511.05640v1,Cen2021,\cite{Cen2021},Fast Global Convergence of Natural Policy Gradient Methods with Entropy Regularization,https://arxiv.org/abs/2007.06558v5,"Natural policy gradient (NPG) methods are among the most widely used policy optimization algorithms in contemporary reinforcement learning. This class of methods is often applied in conjunction with entropy regularization -- an algorithmic scheme that encourages exploration -- and is closely related to soft policy iteration and trust region policy optimization. Despite the empirical success, the theoretical underpinnings for NPG methods remain limited even for the tabular setting. This paper develops $\textit{non-asymptotic}$ convergence guarantees for entropy-regularized NPG methods under softmax parameterization, focusing on discounted Markov decision processes (MDPs). Assuming access to exact policy evaluation, we demonstrate that the algorithm converges linearly -- or even quadratically once it enters a local region around the optimal policy -- when computing optimal value functions of the regularized MDP. Moreover, the algorithm is provably stable vis-à-vis inexactness of policy evaluation. Our convergence results accommodate a wide range of learning rates, and shed light upon the role of entropy regularization in enabling fast convergence.",True,True,"Cen, Shicong and Cheng, Chen and Chen, Yuxin and Wei, Yuting and Chi, Yuejie",2021.0,,,,Operations Research
Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games,2511.05640v1,Guan2021,\cite{Guan2021},Learning Nash Equilibria in Zero-Sum Stochastic Games via Entropy-Regularized Policy Approximation,https://arxiv.org/abs/2009.00162v2,"We explore the use of policy approximations to reduce the computational cost of learning Nash equilibria in zero-sum stochastic games. We propose a new Q-learning type algorithm that uses a sequence of entropy-regularized soft policies to approximate the Nash policy during the Q-function updates. We prove that under certain conditions, by updating the regularized Q-function, the algorithm converges to a Nash equilibrium. We also demonstrate the proposed algorithm's ability to transfer previous training experiences, enabling the agents to adapt quickly to new environments. We provide a dynamic hyper-parameter scheduling scheme to further expedite convergence. Empirical results applied to a number of stochastic games verify that the proposed algorithm converges to the Nash equilibrium, while exhibiting a major speed-up over existing algorithms.",True,True,"Guan, Yue and Zhang, Qifan and Tsiotras, Panagiotis",2021.0,,,,arXiv preprint arXiv:2009.00162
Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games,2511.05640v1,Ahmed2019,\cite{Ahmed2019},Understanding the impact of entropy on policy optimization,,,True,False,"Ahmed, Zafarali and Le Roux, Nicolas and Norouzi, Mohammad and Schuurmans, Dale",2019.0,,,,
Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games,2511.05640v1,Zhan2023,\cite{Zhan2023},Policy mirror descent for regularized reinforcement learning: A generalized framework with linear convergence,,,True,False,"Zhan, Wenhao and Cen, Shicong and Huang, Baihe and Chen, Yuxin and Lee, Jason D and Chi, Yuejie",2023.0,,,,SIAM Journal on Optimization
Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games,2511.05640v1,Liao2025,\cite{Liao2025},Decoding rewards in competitive games: Inverse game theory with entropy regularization,,,True,False,"Liao, Junyi and Zhu, Zihan and Fang, Ethan X and Yang, Zhuoran and Tarokh, Vahid",2025.0,,,,
Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games,2511.05640v1,Ahmed2014,\cite{Ahmed2014},Blind Deconvolution using Convex Programming,https://arxiv.org/abs/1211.5608v3,"We consider the problem of recovering two unknown vectors, $\boldsymbol{w}$ and $\boldsymbol{x}$, of length $L$ from their circular convolution. We make the structural assumption that the two vectors are members of known subspaces, one with dimension $N$ and the other with dimension $K$. Although the observed convolution is nonlinear in both $\boldsymbol{w}$ and $\boldsymbol{x}$, it is linear in the rank-1 matrix formed by their outer product $\boldsymbol{w}\boldsymbol{x}^*$. This observation allows us to recast the deconvolution problem as low-rank matrix recovery problem from linear measurements, whose natural convex relaxation is a nuclear norm minimization program.
  We prove the effectiveness of this relaxation by showing that for ""generic"" signals, the program can deconvolve $\boldsymbol{w}$ and $\boldsymbol{x}$ exactly when the maximum of $N$ and $K$ is almost on the order of $L$. That is, we show that if $\boldsymbol{x}$ is drawn from a random subspace of dimension $N$, and $\boldsymbol{w}$ is a vector in a subspace of dimension $K$ whose basis vectors are ""spread out"" in the frequency domain, then nuclear norm minimization recovers $\boldsymbol{w}\boldsymbol{x}^*$ without error.
  We discuss this result in the context of blind channel estimation in communications. If we have a message of length $N$ which we code using a random $L\times N$ coding matrix, and the encoded message travels through an unknown linear time-invariant channel of maximum length $K$, then the receiver can recover both the channel response and the message when $L\gtrsim N+K$, to within constant and log factors.",True,True,"Ahmed, Ali and Recht, Benjamin and Romberg, Justin",2014.0,,,,IEEE Transactions on Information Theory
Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games,2511.05640v1,Ling&Strohmer2015,\cite{Ling&Strohmer2015},Self-Calibration and Biconvex Compressive Sensing,,,True,False,"Ling, Shuyang and Strohmer, Thomas",2015.0,,,,Inverse Problems
