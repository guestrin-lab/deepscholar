parent_paper_title,parent_arxiv_link,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal
"AI Knows Best? The Paradox of Expertise, AI-Reliance, and Performance in Educational Tutoring Decision-Making Tasks",http://arxiv.org/abs/2509.16772v1,vasconcelos2023explanations,\cite{vasconcelos2023explanations},"Explanations Can Reduce Overreliance on AI Systems During
  Decision-Making",http://arxiv.org/abs/2212.06823v2,"Prior work has identified a resilient phenomenon that threatens the
performance of human-AI decision-making teams: overreliance, when people agree
with an AI, even when it is incorrect. Surprisingly, overreliance does not
reduce when the AI produces explanations for its predictions, compared to only
providing predictions. Some have argued that overreliance results from
cognitive biases or uncalibrated trust, attributing overreliance to an
inevitability of human cognition. By contrast, our paper argues that people
strategically choose whether or not to engage with an AI explanation,
demonstrating empirically that there are scenarios where AI explanations reduce
overreliance. To achieve this, we formalize this strategic choice in a
cost-benefit framework, where the costs and benefits of engaging with the task
are weighed against the costs and benefits of relying on the AI. We manipulate
the costs and benefits in a maze task, where participants collaborate with a
simulated AI to find the exit of a maze. Through 5 studies (N = 731), we find
that costs such as task difficulty (Study 1), explanation difficulty (Study 2,
3), and benefits such as monetary compensation (Study 4) affect overreliance.
Finally, Study 5 adapts the Cognitive Effort Discounting paradigm to quantify
the utility of different explanations, providing further support for our
framework. Our results suggest that some of the null effects found in
literature could be due in part to the explanation not sufficiently reducing
the costs of verifying the AI's prediction.","Vasconcelos, Helena and J{\""o}rke, Matthew and Grunde-McLaughlin, Madeleine and Gerstenberg, Tobias and Bernstein, Michael S and Krishna, Ranjay",2023,,,,Proceedings of the ACM on Human-Computer Interaction
"AI Knows Best? The Paradox of Expertise, AI-Reliance, and Performance in Educational Tutoring Decision-Making Tasks",http://arxiv.org/abs/2509.16772v1,buccinca2021trust,\cite{buccinca2021trust},"To Trust or to Think: Cognitive Forcing Functions Can Reduce
  Overreliance on AI in AI-assisted Decision-making",http://arxiv.org/abs/2102.09692v1,"People supported by AI-powered decision support tools frequently overrely on
the AI: they accept an AI's suggestion even when that suggestion is wrong.
Adding explanations to the AI decisions does not appear to reduce the
overreliance and some studies suggest that it might even increase it. Informed
by the dual-process theory of cognition, we posit that people rarely engage
analytically with each individual AI recommendation and explanation, and
instead develop general heuristics about whether and when to follow the AI
suggestions. Building on prior research on medical decision-making, we designed
three cognitive forcing interventions to compel people to engage more
thoughtfully with the AI-generated explanations. We conducted an experiment
(N=199), in which we compared our three cognitive forcing designs to two simple
explainable AI approaches and to a no-AI baseline. The results demonstrate that
cognitive forcing significantly reduced overreliance compared to the simple
explainable AI approaches. However, there was a trade-off: people assigned the
least favorable subjective ratings to the designs that reduced the overreliance
the most. To audit our work for intervention-generated inequalities, we
investigated whether our interventions benefited equally people with different
levels of Need for Cognition (i.e., motivation to engage in effortful mental
activities). Our results show that, on average, cognitive forcing interventions
benefited participants higher in Need for Cognition more. Our research suggests
that human cognitive motivation moderates the effectiveness of explainable AI
solutions.","Bu{\c{c}}inca, Zana and Malaya, Maja Barbara and Gajos, Krzysztof Z",2021,,,,Proceedings of the ACM on Human-Computer Interaction
"AI Knows Best? The Paradox of Expertise, AI-Reliance, and Performance in Educational Tutoring Decision-Making Tasks",http://arxiv.org/abs/2509.16772v1,dietvorst2015algorithm,\cite{dietvorst2015algorithm},Algorithm aversion: people erroneously avoid algorithms after seeing them err.,,,"Dietvorst, Berkeley J and Simmons, Joseph P and Massey, Cade",2015,,,,Journal of Experimental Psychology: General
"AI Knows Best? The Paradox of Expertise, AI-Reliance, and Performance in Educational Tutoring Decision-Making Tasks",http://arxiv.org/abs/2509.16772v1,morrison2023evaluating,\cite{morrison2023evaluating},Evaluating the impact of human explanation strategies on human-AI visual decision-making,,,"Morrison, Katelyn and Shin, Donghoon and Holstein, Kenneth and Perer, Adam",2023,,,,Proceedings of the ACM on Human-Computer Interaction
"AI Knows Best? The Paradox of Expertise, AI-Reliance, and Performance in Educational Tutoring Decision-Making Tasks",http://arxiv.org/abs/2509.16772v1,bansal2021does,\cite{bansal2021does},"Does the Whole Exceed its Parts? The Effect of AI Explanations on
  Complementary Team Performance",http://arxiv.org/abs/2006.14779v3,"Many researchers motivate explainable AI with studies showing that human-AI
team performance on decision-making tasks improves when the AI explains its
recommendations. However, prior studies observed improvements from explanations
only when the AI, alone, outperformed both the human and the best team. Can
explanations help lead to complementary performance, where team accuracy is
higher than either the human or the AI working solo? We conduct mixed-method
user studies on three datasets, where an AI with accuracy comparable to humans
helps participants solve a task (explaining itself in some conditions). While
we observed complementary improvements from AI augmentation, they were not
increased by explanations. Rather, explanations increased the chance that
humans will accept the AI's recommendation, regardless of its correctness. Our
result poses new challenges for human-centered AI: Can we develop explanatory
approaches that encourage appropriate trust in AI, and therefore help generate
(or improve) complementary performance?","Bansal, Gagan and Wu, Tongshuang and Zhou, Joyce and Fok, Raymond and Nushi, Besmira and Kamar, Ece and Ribeiro, Marco Tulio and Weld, Daniel",2021,,,,
"AI Knows Best? The Paradox of Expertise, AI-Reliance, and Performance in Educational Tutoring Decision-Making Tasks",http://arxiv.org/abs/2509.16772v1,vereschak2021evaluate,\cite{vereschak2021evaluate},How to evaluate trust in AI-assisted decision making? A survey of empirical methodologies,,,"Vereschak, Oleksandra and Bailly, Gilles and Caramiaux, Baptiste",2021,,,,Proceedings of the ACM on Human-Computer Interaction
"AI Knows Best? The Paradox of Expertise, AI-Reliance, and Performance in Educational Tutoring Decision-Making Tasks",http://arxiv.org/abs/2509.16772v1,morrison2023impact,\cite{morrison2023impact},The Impact of Imperfect XAI on Human-AI Decision-Making,http://arxiv.org/abs/2307.13566v4,"Explainability techniques are rapidly being developed to improve human-AI
decision-making across various cooperative work settings. Consequently,
previous research has evaluated how decision-makers collaborate with imperfect
AI by investigating appropriate reliance and task performance with the aim of
designing more human-centered computer-supported collaborative tools. Several
human-centered explainable AI (XAI) techniques have been proposed in hopes of
improving decision-makers' collaboration with AI; however, these techniques are
grounded in findings from previous studies that primarily focus on the impact
of incorrect AI advice. Few studies acknowledge the possibility of the
explanations being incorrect even if the AI advice is correct. Thus, it is
crucial to understand how imperfect XAI affects human-AI decision-making. In
this work, we contribute a robust, mixed-methods user study with 136
participants to evaluate how incorrect explanations influence humans'
decision-making behavior in a bird species identification task, taking into
account their level of expertise and an explanation's level of assertiveness.
Our findings reveal the influence of imperfect XAI and humans' level of
expertise on their reliance on AI and human-AI team performance. We also
discuss how explanations can deceive decision-makers during human-AI
collaboration. Hence, we shed light on the impacts of imperfect XAI in the
field of computer-supported cooperative work and provide guidelines for
designers of human-AI collaboration systems.","Morrison, Katelyn and Spitzer, Philipp and Turri, Violet and Feng, Michelle and K{\""u}hl, Niklas and Perer, Adam",2023,,,,arXiv preprint arXiv:2307.13566
"AI Knows Best? The Paradox of Expertise, AI-Reliance, and Performance in Educational Tutoring Decision-Making Tasks",http://arxiv.org/abs/2509.16772v1,thomas2023tutor,\cite{thomas2023tutor},When the Tutor Becomes the Student: Design and Evaluation of Efficient Scenario-based Lessons for Tutors,,,"Thomas, Danielle and Yang, Xinyu and Gupta, Shivang and Adeniran, Adetunji and Mclaughlin, Elizabeth and Koedinger, Kenneth",2023,,,,
"AI Knows Best? The Paradox of Expertise, AI-Reliance, and Performance in Educational Tutoring Decision-Making Tasks",http://arxiv.org/abs/2509.16772v1,lin2023using,\cite{lin2023using},Using large language models to provide explanatory feedback to human tutors,,,"Lin, Jionghao and Thomas, Danielle R and Han, Feifei and Gupta, Shivang and Tan, Wei and Nguyen, Ngoc Dang and Koedinger, Kenneth R",2023,,,,arXiv preprint arXiv:2306.15498
"AI Knows Best? The Paradox of Expertise, AI-Reliance, and Performance in Educational Tutoring Decision-Making Tasks",http://arxiv.org/abs/2509.16772v1,openai2023gpt4,\cite{openai2023gpt4},GPT-4 Technical Report,,,OpenAI,2023,,,,
"AI Knows Best? The Paradox of Expertise, AI-Reliance, and Performance in Educational Tutoring Decision-Making Tasks",http://arxiv.org/abs/2509.16772v1,brown2020language,\cite{brown2020language},Language Models are Few-Shot Learners,http://arxiv.org/abs/2005.14165v4,"Recent work has demonstrated substantial gains on many NLP tasks and
benchmarks by pre-training on a large corpus of text followed by fine-tuning on
a specific task. While typically task-agnostic in architecture, this method
still requires task-specific fine-tuning datasets of thousands or tens of
thousands of examples. By contrast, humans can generally perform a new language
task from only a few examples or from simple instructions - something which
current NLP systems still largely struggle to do. Here we show that scaling up
language models greatly improves task-agnostic, few-shot performance, sometimes
even reaching competitiveness with prior state-of-the-art fine-tuning
approaches. Specifically, we train GPT-3, an autoregressive language model with
175 billion parameters, 10x more than any previous non-sparse language model,
and test its performance in the few-shot setting. For all tasks, GPT-3 is
applied without any gradient updates or fine-tuning, with tasks and few-shot
demonstrations specified purely via text interaction with the model. GPT-3
achieves strong performance on many NLP datasets, including translation,
question-answering, and cloze tasks, as well as several tasks that require
on-the-fly reasoning or domain adaptation, such as unscrambling words, using a
novel word in a sentence, or performing 3-digit arithmetic. At the same time,
we also identify some datasets where GPT-3's few-shot learning still struggles,
as well as some datasets where GPT-3 faces methodological issues related to
training on large web corpora. Finally, we find that GPT-3 can generate samples
of news articles which human evaluators have difficulty distinguishing from
articles written by humans. We discuss broader societal impacts of this finding
and of GPT-3 in general.","Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",2020,,,,Advances in neural information processing systems
"AI Knows Best? The Paradox of Expertise, AI-Reliance, and Performance in Educational Tutoring Decision-Making Tasks",http://arxiv.org/abs/2509.16772v1,holtzman2019curious,\cite{holtzman2019curious},The Curious Case of Neural Text Degeneration,http://arxiv.org/abs/1904.09751v2,"Despite considerable advancements with deep neural language models, the
enigma of neural text degeneration persists when these models are tested as
text generators. The counter-intuitive empirical observation is that even
though the use of likelihood as training objective leads to high quality models
for a broad range of language understanding tasks, using likelihood as a
decoding objective leads to text that is bland and strangely repetitive.
  In this paper, we reveal surprising distributional differences between human
text and machine text. In addition, we find that decoding strategies alone can
dramatically effect the quality of machine text, even when generated from
exactly the same neural language model. Our findings motivate Nucleus Sampling,
a simple but effective method to draw the best out of neural generation. By
sampling text from the dynamic nucleus of the probability distribution, which
allows for diversity while effectively truncating the less reliable tail of the
distribution, the resulting text better demonstrates the quality of human text,
yielding enhanced diversity without sacrificing fluency and coherence.","Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin",2019,,,,arXiv preprint arXiv:1904.09751
