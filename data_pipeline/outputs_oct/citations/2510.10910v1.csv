parent_paper_title,parent_arxiv_link,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal
SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model,http://arxiv.org/abs/2510.10910v1,gatys2016image,\cite{gatys2016image},Image style transfer using convolutional neural networks,,,"Gatys, Leon A and Ecker, Alexander S and Bethge, Matthias",2016,,,,
SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model,http://arxiv.org/abs/2510.10910v1,huang2017arbitrary,\cite{huang2017arbitrary},"Arbitrary Style Transfer in Real-time with Adaptive Instance
  Normalization",http://arxiv.org/abs/1703.06868v2,"Gatys et al. recently introduced a neural algorithm that renders a content
image in the style of another image, achieving so-called style transfer.
However, their framework requires a slow iterative optimization process, which
limits its practical application. Fast approximations with feed-forward neural
networks have been proposed to speed up neural style transfer. Unfortunately,
the speed improvement comes at a cost: the network is usually tied to a fixed
set of styles and cannot adapt to arbitrary new styles. In this paper, we
present a simple yet effective approach that for the first time enables
arbitrary style transfer in real-time. At the heart of our method is a novel
adaptive instance normalization (AdaIN) layer that aligns the mean and variance
of the content features with those of the style features. Our method achieves
speed comparable to the fastest existing approach, without the restriction to a
pre-defined set of styles. In addition, our approach allows flexible user
controls such as content-style trade-off, style interpolation, color & spatial
controls, all using a single feed-forward neural network.","Huang, Xun and Belongie, Serge",2017,,,,
SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model,http://arxiv.org/abs/2510.10910v1,karras2019style,\cite{karras2019style},A Style-Based Generator Architecture for Generative Adversarial Networks,http://arxiv.org/abs/1812.04948v3,"We propose an alternative generator architecture for generative adversarial
networks, borrowing from style transfer literature. The new architecture leads
to an automatically learned, unsupervised separation of high-level attributes
(e.g., pose and identity when trained on human faces) and stochastic variation
in the generated images (e.g., freckles, hair), and it enables intuitive,
scale-specific control of the synthesis. The new generator improves the
state-of-the-art in terms of traditional distribution quality metrics, leads to
demonstrably better interpolation properties, and also better disentangles the
latent factors of variation. To quantify interpolation quality and
disentanglement, we propose two new, automated methods that are applicable to
any generator architecture. Finally, we introduce a new, highly varied and
high-quality dataset of human faces.","Karras, Tero and Laine, Samuli and Aila, Timo",2019,,,,
SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model,http://arxiv.org/abs/2510.10910v1,deng2022stytr2,\cite{deng2022stytr2},Stytr2: Image style transfer with transformers,,,"Deng, Yingying and Tang, Fan and Dong, Weiming and Ma, Chongyang and Pan, Xingjia and Wang, Lei and Xu, Changsheng",2022,,,,
SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model,http://arxiv.org/abs/2510.10910v1,goodfellow2014generative,\cite{goodfellow2014generative},Conditional Generative Adversarial Nets,http://arxiv.org/abs/1411.1784v1,"Generative Adversarial Nets [8] were recently introduced as a novel way to
train generative models. In this work we introduce the conditional version of
generative adversarial nets, which can be constructed by simply feeding the
data, y, we wish to condition on to both the generator and discriminator. We
show that this model can generate MNIST digits conditioned on class labels. We
also illustrate how this model could be used to learn a multi-modal model, and
provide preliminary examples of an application to image tagging in which we
demonstrate how this approach can generate descriptive tags which are not part
of training labels.","Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua",2014,,,,
SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model,http://arxiv.org/abs/2510.10910v1,vaswani2017attention,\cite{vaswani2017attention},Attention Is All You Need,http://arxiv.org/abs/1706.03762v7,"The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks in an encoder-decoder configuration. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer, based
solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to be
superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014
English-to-German translation task, improving over the existing best results,
including ensembles by over 2 BLEU. On the WMT 2014 English-to-French
translation task, our model establishes a new single-model state-of-the-art
BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction
of the training costs of the best models from the literature. We show that the
Transformer generalizes well to other tasks by applying it successfully to
English constituency parsing both with large and limited training data.","Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia",2017,,,,
SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model,http://arxiv.org/abs/2510.10910v1,kwon2022clipstyler,\cite{kwon2022clipstyler},CLIPstyler: Image Style Transfer with a Single Text Condition,http://arxiv.org/abs/2112.00374v3,"Existing neural style transfer methods require reference style images to
transfer texture information of style images to content images. However, in
many practical situations, users may not have reference style images but still
be interested in transferring styles by just imagining them. In order to deal
with such applications, we propose a new framework that enables a style
transfer `without' a style image, but only with a text description of the
desired style. Using the pre-trained text-image embedding model of CLIP, we
demonstrate the modulation of the style of content images only with a single
text condition. Specifically, we propose a patch-wise text-image matching loss
with multiview augmentations for realistic texture transfer. Extensive
experimental results confirmed the successful image style transfer with
realistic textures that reflect semantic query texts.","Kwon, Gihyun and Ye, Jong Chul",2022,,,,
SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model,http://arxiv.org/abs/2510.10910v1,radford2021learning,\cite{radford2021learning},Learning Transferable Visual Models From Natural Language Supervision,http://arxiv.org/abs/2103.00020v1,"State-of-the-art computer vision systems are trained to predict a fixed set
of predetermined object categories. This restricted form of supervision limits
their generality and usability since additional labeled data is needed to
specify any other visual concept. Learning directly from raw text about images
is a promising alternative which leverages a much broader source of
supervision. We demonstrate that the simple pre-training task of predicting
which caption goes with which image is an efficient and scalable way to learn
SOTA image representations from scratch on a dataset of 400 million (image,
text) pairs collected from the internet. After pre-training, natural language
is used to reference learned visual concepts (or describe new ones) enabling
zero-shot transfer of the model to downstream tasks. We study the performance
of this approach by benchmarking on over 30 different existing computer vision
datasets, spanning tasks such as OCR, action recognition in videos,
geo-localization, and many types of fine-grained object classification. The
model transfers non-trivially to most tasks and is often competitive with a
fully supervised baseline without the need for any dataset specific training.
For instance, we match the accuracy of the original ResNet-50 on ImageNet
zero-shot without needing to use any of the 1.28 million training examples it
was trained on. We release our code and pre-trained model weights at
https://github.com/OpenAI/CLIP.","Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",2021,,,,
SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model,http://arxiv.org/abs/2510.10910v1,wang2023stylediffusion,\cite{wang2023stylediffusion},"StyleDiffusion: Controllable Disentangled Style Transfer via Diffusion
  Models",http://arxiv.org/abs/2308.07863v1,"Content and style (C-S) disentanglement is a fundamental problem and critical
challenge of style transfer. Existing approaches based on explicit definitions
(e.g., Gram matrix) or implicit learning (e.g., GANs) are neither interpretable
nor easy to control, resulting in entangled representations and less satisfying
results. In this paper, we propose a new C-S disentangled framework for style
transfer without using previous assumptions. The key insight is to explicitly
extract the content information and implicitly learn the complementary style
information, yielding interpretable and controllable C-S disentanglement and
style transfer. A simple yet effective CLIP-based style disentanglement loss
coordinated with a style reconstruction prior is introduced to disentangle C-S
in the CLIP image space. By further leveraging the powerful style removal and
generative ability of diffusion models, our framework achieves superior results
than state of the art and flexible C-S disentanglement and trade-off control.
Our work provides new insights into the C-S disentanglement in style transfer
and demonstrates the potential of diffusion models for learning
well-disentangled C-S characteristics.","Wang, Zhizhong and Zhao, Lei and Xing, Wei",2023,,,,
SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model,http://arxiv.org/abs/2510.10910v1,zhang2023inversion,\cite{zhang2023inversion},Inversion-Based Style Transfer with Diffusion Models,http://arxiv.org/abs/2211.13203v3,"The artistic style within a painting is the means of expression, which
includes not only the painting material, colors, and brushstrokes, but also the
high-level attributes including semantic elements, object shapes, etc. Previous
arbitrary example-guided artistic image generation methods often fail to
control shape changes or convey elements. The pre-trained text-to-image
synthesis diffusion probabilistic models have achieved remarkable quality, but
it often requires extensive textual descriptions to accurately portray
attributes of a particular painting. We believe that the uniqueness of an
artwork lies precisely in the fact that it cannot be adequately explained with
normal language. Our key idea is to learn artistic style directly from a single
painting and then guide the synthesis without providing complex textual
descriptions. Specifically, we assume style as a learnable textual description
of a painting. We propose an inversion-based style transfer method (InST),
which can efficiently and accurately learn the key information of an image,
thus capturing and transferring the artistic style of a painting. We
demonstrate the quality and efficiency of our method on numerous paintings of
various artists and styles. Code and models are available at
https://github.com/zyxElsa/InST.","Zhang, Yuxin and Huang, Nisha and Tang, Fan and Huang, Haibin and Ma, Chongyang and Dong, Weiming and Xu, Changsheng",2023,,,,
SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model,http://arxiv.org/abs/2510.10910v1,yang2023zero,\cite{yang2023zero},"Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style
  Transfer",http://arxiv.org/abs/2303.08622v2,"Diffusion models have shown great promise in text-guided image style
transfer, but there is a trade-off between style transformation and content
preservation due to their stochastic nature. Existing methods require
computationally expensive fine-tuning of diffusion models or additional neural
network. To address this, here we propose a zero-shot contrastive loss for
diffusion models that doesn't require additional fine-tuning or auxiliary
networks. By leveraging patch-wise contrastive loss between generated samples
and original image embeddings in the pre-trained diffusion model, our method
can generate images with the same semantic content as the source image in a
zero-shot manner. Our approach outperforms existing methods while preserving
content and requiring no additional training, not only for image style transfer
but also for image-to-image translation and manipulation. Our experimental
results validate the effectiveness of our proposed method.","Yang, Serin and Hwang, Hyunmin and Ye, Jong Chul",2023,,,,
SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model,http://arxiv.org/abs/2510.10910v1,chung2024style,\cite{chung2024style},"Style Injection in Diffusion: A Training-free Approach for Adapting
  Large-scale Diffusion Models for Style Transfer",http://arxiv.org/abs/2312.09008v2,"Despite the impressive generative capabilities of diffusion models, existing
diffusion model-based style transfer methods require inference-stage
optimization (e.g. fine-tuning or textual inversion of style) which is
time-consuming, or fails to leverage the generative ability of large-scale
diffusion models. To address these issues, we introduce a novel artistic style
transfer method based on a pre-trained large-scale diffusion model without any
optimization. Specifically, we manipulate the features of self-attention layers
as the way the cross-attention mechanism works; in the generation process,
substituting the key and value of content with those of style image. This
approach provides several desirable characteristics for style transfer
including 1) preservation of content by transferring similar styles into
similar image patches and 2) transfer of style based on similarity of local
texture (e.g. edge) between content and style images. Furthermore, we introduce
query preservation and attention temperature scaling to mitigate the issue of
disruption of original content, and initial latent Adaptive Instance
Normalization (AdaIN) to deal with the disharmonious color (failure to transfer
the colors of style). Our experimental results demonstrate that our proposed
method surpasses state-of-the-art methods in both conventional and
diffusion-based style transfer baselines.","Chung, Jiwoo and Hyun, Sangeek and Heo, Jae-Pil",2024,,,,
SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model,http://arxiv.org/abs/2510.10910v1,huang2024diffstyler,\cite{huang2024diffstyler},"DiffStyler: Controllable Dual Diffusion for Text-Driven Image
  Stylization",http://arxiv.org/abs/2211.10682v2,"Despite the impressive results of arbitrary image-guided style transfer
methods, text-driven image stylization has recently been proposed for
transferring a natural image into a stylized one according to textual
descriptions of the target style provided by the user. Unlike the previous
image-to-image transfer approaches, text-guided stylization progress provides
users with a more precise and intuitive way to express the desired style.
However, the huge discrepancy between cross-modal inputs/outputs makes it
challenging to conduct text-driven image stylization in a typical feed-forward
CNN pipeline. In this paper, we present DiffStyler, a dual diffusion processing
architecture to control the balance between the content and style of the
diffused results. The cross-modal style information can be easily integrated as
guidance during the diffusion process step-by-step. Furthermore, we propose a
content image-based learnable noise on which the reverse denoising process is
based, enabling the stylization results to better preserve the structure
information of the content image. We validate the proposed DiffStyler beyond
the baseline methods through extensive qualitative and quantitative
experiments. Code is available at
\url{https://github.com/haha-lisa/Diffstyler}.","Huang, Nisha and Zhang, Yuxin and Tang, Fan and Ma, Chongyang and Huang, Haibin and Dong, Weiming and Xu, Changsheng",2024,,,,IEEE Transactions on Neural Networks and Learning Systems
SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model,http://arxiv.org/abs/2510.10910v1,roy2020stefann,\cite{roy2020stefann},STEFANN: Scene Text Editor using Font Adaptive Neural Network,http://arxiv.org/abs/1903.01192v4,"Textual information in a captured scene plays an important role in scene
interpretation and decision making. Though there exist methods that can
successfully detect and interpret complex text regions present in a scene, to
the best of our knowledge, there is no significant prior work that aims to
modify the textual information in an image. The ability to edit text directly
on images has several advantages including error correction, text restoration
and image reusability. In this paper, we propose a method to modify text in an
image at character-level. We approach the problem in two stages. At first, the
unobserved character (target) is generated from an observed character (source)
being modified. We propose two different neural network architectures - (a)
FANnet to achieve structural consistency with source font and (b) Colornet to
preserve source color. Next, we replace the source character with the generated
character maintaining both geometric and visual consistency with neighboring
characters. Our method works as a unified platform for modifying text in
images. We present the effectiveness of our method on COCO-Text and ICDAR
datasets both qualitatively and quantitatively.","Roy, Prasun and Bhattacharya, Saumik and Ghosh, Subhankar and Pal, Umapada",2020,,,,
SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model,http://arxiv.org/abs/2510.10910v1,wu2019editing,\cite{wu2019editing},Editing Text in the Wild,http://arxiv.org/abs/1908.03047v1,"In this paper, we are interested in editing text in natural images, which
aims to replace or modify a word in the source image with another one while
maintaining its realistic look. This task is challenging, as the styles of both
background and text need to be preserved so that the edited image is visually
indistinguishable from the source image. Specifically, we propose an end-to-end
trainable style retention network (SRNet) that consists of three modules: text
conversion module, background inpainting module and fusion module. The text
conversion module changes the text content of the source image into the target
text while keeping the original text style. The background inpainting module
erases the original text, and fills the text region with appropriate texture.
The fusion module combines the information from the two former modules, and
generates the edited text images. To our knowledge, this work is the first
attempt to edit text in natural images at the word level. Both visual effects
and quantitative results on synthetic and real-world dataset (ICDAR 2013) fully
confirm the importance and necessity of modular decomposition. We also conduct
extensive experiments to validate the usefulness of our method in various
real-world applications such as text image synthesis, augmented reality (AR)
translation, information hiding, etc.","Wu, Liang and Zhang, Chengquan and Liu, Jiaming and Han, Junyu and Liu, Jingtuo and Ding, Errui and Bai, Xiang",2019,,,,
SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model,http://arxiv.org/abs/2510.10910v1,yang2020swaptext,\cite{yang2020swaptext},SwapText: Image Based Texts Transfer in Scenes,http://arxiv.org/abs/2003.08152v1,"Swapping text in scene images while preserving original fonts, colors, sizes
and background textures is a challenging task due to the complex interplay
between different factors. In this work, we present SwapText, a three-stage
framework to transfer texts across scene images. First, a novel text swapping
network is proposed to replace text labels only in the foreground image.
Second, a background completion network is learned to reconstruct background
images. Finally, the generated foreground image and background image are used
to generate the word image by the fusion network. Using the proposing
framework, we can manipulate the texts of the input images even with severe
geometric distortion. Qualitative and quantitative results are presented on
several scene text datasets, including regular and irregular text datasets. We
conducted extensive experiments to prove the usefulness of our method such as
image based text translation, text image synthesis, etc.","Yang, Qiangpeng and Huang, Jun and Lin, Wei",2020,,,,
SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model,http://arxiv.org/abs/2510.10910v1,luo2022siman,\cite{luo2022siman},"SimAN: Exploring Self-Supervised Representation Learning of Scene Text
  via Similarity-Aware Normalization",http://arxiv.org/abs/2203.10492v2,"Recently self-supervised representation learning has drawn considerable
attention from the scene text recognition community. Different from previous
studies using contrastive learning, we tackle the issue from an alternative
perspective, i.e., by formulating the representation learning scheme in a
generative manner. Typically, the neighboring image patches among one text line
tend to have similar styles, including the strokes, textures, colors, etc.
Motivated by this common sense, we augment one image patch and use its
neighboring patch as guidance to recover itself. Specifically, we propose a
Similarity-Aware Normalization (SimAN) module to identify the different
patterns and align the corresponding styles from the guiding patch. In this
way, the network gains representation capability for distinguishing complex
patterns such as messy strokes and cluttered backgrounds. Experiments show that
the proposed SimAN significantly improves the representation quality and
achieves promising performance. Moreover, we surprisingly find that our
self-supervised generative network has impressive potential for data synthesis,
text image editing, and font interpolation, which suggests that the proposed
SimAN has a wide range of practical applications.","Luo, Canjie and Jin, Lianwen and Chen, Jingdong",2022,,,,
SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model,http://arxiv.org/abs/2510.10910v1,krishnan2023textstylebrush,\cite{krishnan2023textstylebrush},TextStyleBrush: Transfer of Text Aesthetics from a Single Example,http://arxiv.org/abs/2106.08385v1,"We present a novel approach for disentangling the content of a text image
from all aspects of its appearance. The appearance representation we derive can
then be applied to new content, for one-shot transfer of the source style to
new content. We learn this disentanglement in a self-supervised manner. Our
method processes entire word boxes, without requiring segmentation of text from
background, per-character processing, or making assumptions on string lengths.
We show results in different text domains which were previously handled by
specialized methods, e.g., scene text, handwritten text. To these ends, we make
a number of technical contributions: (1) We disentangle the style and content
of a textual image into a non-parametric, fixed-dimensional vector. (2) We
propose a novel approach inspired by StyleGAN but conditioned over the example
style at different resolution and content. (3) We present novel self-supervised
training criteria which preserve both source style and target content using a
pre-trained font classifier and text recognizer. Finally, (4) we also introduce
Imgur5K, a new challenging dataset for handwritten word images. We offer
numerous qualitative photo-realistic results of our method. We further show
that our method surpasses previous work in quantitative tests on scene text and
handwriting datasets, as well as in a user study.","Krishnan, Praveen and Kovvuri, Rama and Pang, Guan and Vassilev, Boris and Hassner, Tal",2023,,,,
SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model,http://arxiv.org/abs/2510.10910v1,qu2023exploring,\cite{qu2023exploring},Exploring Stroke-Level Modifications for Scene Text Editing,http://arxiv.org/abs/2212.01982v1,"Scene text editing (STE) aims to replace text with the desired one while
preserving background and styles of the original text. However, due to the
complicated background textures and various text styles, existing methods fall
short in generating clear and legible edited text images. In this study, we
attribute the poor editing performance to two problems: 1) Implicit decoupling
structure. Previous methods of editing the whole image have to learn different
translation rules of background and text regions simultaneously. 2) Domain gap.
Due to the lack of edited real scene text images, the network can only be well
trained on synthetic pairs and performs poorly on real-world images. To handle
the above problems, we propose a novel network by MOdifying Scene Text image at
strokE Level (MOSTEL). Firstly, we generate stroke guidance maps to explicitly
indicate regions to be edited. Different from the implicit one by directly
modifying all the pixels at image level, such explicit instructions filter out
the distractions from background and guide the network to focus on editing
rules of text regions. Secondly, we propose a Semi-supervised Hybrid Learning
to train the network with both labeled synthetic images and unpaired real scene
text images. Thus, the STE model is adapted to real-world datasets
distributions. Moreover, two new datasets (Tamper-Syn2k and Tamper-Scene) are
proposed to fill the blank of public evaluation datasets. Extensive experiments
demonstrate that our MOSTEL outperforms previous methods both qualitatively and
quantitatively. Datasets and code will be available at
https://github.com/qqqyd/MOSTEL.","Qu, Yadong and Tan, Qingfeng and Xie, Hongtao and Xu, Jianjun and Wang, Yuxin and Zhang, Yongdong",2023,,,,
SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model,http://arxiv.org/abs/2510.10910v1,ji2023improving,\cite{ji2023improving},Improving Diffusion Models for Scene Text Editing with Dual Encoders,http://arxiv.org/abs/2304.05568v1,"Scene text editing is a challenging task that involves modifying or inserting
specified texts in an image while maintaining its natural and realistic
appearance. Most previous approaches to this task rely on style-transfer models
that crop out text regions and feed them into image transfer models, such as
GANs. However, these methods are limited in their ability to change text style
and are unable to insert texts into images. Recent advances in diffusion models
have shown promise in overcoming these limitations with text-conditional image
editing. However, our empirical analysis reveals that state-of-the-art
diffusion models struggle with rendering correct text and controlling text
style. To address these problems, we propose DIFFSTE to improve pre-trained
diffusion models with a dual encoder design, which includes a character encoder
for better text legibility and an instruction encoder for better style control.
An instruction tuning framework is introduced to train our model to learn the
mapping from the text instruction to the corresponding image with either the
specified style or the style of the surrounding texts in the background. Such a
training method further brings our method the zero-shot generalization ability
to the following three scenarios: generating text with unseen font variation,
e.g., italic and bold, mixing different fonts to construct a new font, and
using more relaxed forms of natural language as the instructions to guide the
generation task. We evaluate our approach on five datasets and demonstrate its
superior performance in terms of text correctness, image naturalness, and style
controllability. Our code is publicly available.
https://github.com/UCSB-NLP-Chang/DiffSTE","Ji, Jiabao and Zhang, Guanhua and Wang, Zhaowen and Hou, Bairu and Zhang, Zhifei and Price, Brian and Chang, Shiyu",2023,,,,arXiv preprint arXiv:2304.05568
SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model,http://arxiv.org/abs/2510.10910v1,chen2024diffute,\cite{chen2024diffute},DiffUTE: Universal Text Editing Diffusion Model,http://arxiv.org/abs/2305.10825v3,"Diffusion model based language-guided image editing has achieved great
success recently. However, existing state-of-the-art diffusion models struggle
with rendering correct text and text style during generation. To tackle this
problem, we propose a universal self-supervised text editing diffusion model
(DiffUTE), which aims to replace or modify words in the source image with
another one while maintaining its realistic appearance. Specifically, we build
our model on a diffusion model and carefully modify the network structure to
enable the model for drawing multilingual characters with the help of glyph and
position information. Moreover, we design a self-supervised learning framework
to leverage large amounts of web data to improve the representation ability of
the model. Experimental results show that our method achieves an impressive
performance and enables controllable editing on in-the-wild images with high
fidelity. Our code will be avaliable in
\url{https://github.com/chenhaoxing/DiffUTE}.","Chen, Haoxing and Xu, Zhuoer and Gu, Zhangxuan and Li, Yaohui and Meng, Changhua and Zhu, Huijia and Wang, Weiqiang and others",2024,,,,Advances in Neural Information Processing Systems
SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model,http://arxiv.org/abs/2510.10910v1,ma2023glyphdraw,\cite{ma2023glyphdraw},GlyphDraw: Learning to Draw Chinese Characters in Image Synthesis Models Coherently,,,"Ma, Jian and Zhao, Mingjun and Chen, Chen and Wang, Ruichen and Niu, Di and Lu, Haonan and Lin, Xiaodong",2023,,,,arXiv preprint arXiv:2303.17870
SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model,http://arxiv.org/abs/2510.10910v1,yang2024glyphcontrol,\cite{yang2024glyphcontrol},GlyphControl: Glyph Conditional Control for Visual Text Generation,http://arxiv.org/abs/2305.18259v2,"Recently, there has been an increasing interest in developing diffusion-based
text-to-image generative models capable of generating coherent and well-formed
visual text. In this paper, we propose a novel and efficient approach called
GlyphControl to address this task. Unlike existing methods that rely on
character-aware text encoders like ByT5 and require retraining of text-to-image
models, our approach leverages additional glyph conditional information to
enhance the performance of the off-the-shelf Stable-Diffusion model in
generating accurate visual text. By incorporating glyph instructions, users can
customize the content, location, and size of the generated text according to
their specific requirements. To facilitate further research in visual text
generation, we construct a training benchmark dataset called LAION-Glyph. We
evaluate the effectiveness of our approach by measuring OCR-based metrics, CLIP
score, and FID of the generated visual text. Our empirical evaluations
demonstrate that GlyphControl outperforms the recent DeepFloyd IF approach in
terms of OCR accuracy, CLIP score, and FID, highlighting the efficacy of our
method.","Yang, Yukang and Gui, Dongnan and Yuan, Yuhui and Liang, Weicong and Ding, Haisong and Hu, Han and Chen, Kai",2024,,,,Advances in Neural Information Processing Systems
SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model,http://arxiv.org/abs/2510.10910v1,chen2024textdiffuser,\cite{chen2024textdiffuser},TextDiffuser: Diffusion Models as Text Painters,http://arxiv.org/abs/2305.10855v5,"Diffusion models have gained increasing attention for their impressive
generation abilities but currently struggle with rendering accurate and
coherent text. To address this issue, we introduce TextDiffuser, focusing on
generating images with visually appealing text that is coherent with
backgrounds. TextDiffuser consists of two stages: first, a Transformer model
generates the layout of keywords extracted from text prompts, and then
diffusion models generate images conditioned on the text prompt and the
generated layout. Additionally, we contribute the first large-scale text images
dataset with OCR annotations, MARIO-10M, containing 10 million image-text pairs
with text recognition, detection, and character-level segmentation annotations.
We further collect the MARIO-Eval benchmark to serve as a comprehensive tool
for evaluating text rendering quality. Through experiments and user studies, we
show that TextDiffuser is flexible and controllable to create high-quality text
images using text prompts alone or together with text template images, and
conduct text inpainting to reconstruct incomplete images with text. The code,
model, and dataset will be available at \url{https://aka.ms/textdiffuser}.","Chen, Jingye and Huang, Yupan and Lv, Tengchao and Cui, Lei and Chen, Qifeng and Wei, Furu",2024,,,,Advances in Neural Information Processing Systems
SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model,http://arxiv.org/abs/2510.10910v1,zeng2024textctrl,\cite{zeng2024textctrl},TextCtrl: Diffusion-based Scene Text Editing with Prior Guidance Control,http://arxiv.org/abs/2410.10133v1,"Centred on content modification and style preservation, Scene Text Editing
(STE) remains a challenging task despite considerable progress in text-to-image
synthesis and text-driven image manipulation recently. GAN-based STE methods
generally encounter a common issue of model generalization, while
Diffusion-based STE methods suffer from undesired style deviations. To address
these problems, we propose TextCtrl, a diffusion-based method that edits text
with prior guidance control. Our method consists of two key components: (i) By
constructing fine-grained text style disentanglement and robust text glyph
structure representation, TextCtrl explicitly incorporates Style-Structure
guidance into model design and network training, significantly improving text
style consistency and rendering accuracy. (ii) To further leverage the style
prior, a Glyph-adaptive Mutual Self-attention mechanism is proposed which
deconstructs the implicit fine-grained features of the source image to enhance
style consistency and vision quality during inference. Furthermore, to fill the
vacancy of the real-world STE evaluation benchmark, we create the first
real-world image-pair dataset termed ScenePair for fair comparisons.
Experiments demonstrate the effectiveness of TextCtrl compared with previous
methods concerning both style fidelity and text accuracy.","Zeng, Weichao and Shu, Yan and Li, Zhenhang and Yang, Dongbao and Zhou, Yu",2024,,,,
SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model,http://arxiv.org/abs/2510.10910v1,zhang2024choose,\cite{zhang2024choose},"Choose What You Need: Disentangled Representation Learning for Scene
  Text Recognition, Removal and Editing",http://arxiv.org/abs/2405.04377v1,"Scene text images contain not only style information (font, background) but
also content information (character, texture). Different scene text tasks need
different information, but previous representation learning methods use tightly
coupled features for all tasks, resulting in sub-optimal performance. We
propose a Disentangled Representation Learning framework (DARLING) aimed at
disentangling these two types of features for improved adaptability in better
addressing various downstream tasks (choose what you really need).
Specifically, we synthesize a dataset of image pairs with identical style but
different content. Based on the dataset, we decouple the two types of features
by the supervision design. Clearly, we directly split the visual representation
into style and content features, the content features are supervised by a text
recognition loss, while an alignment loss aligns the style features in the
image pairs. Then, style features are employed in reconstructing the
counterpart image via an image decoder with a prompt that indicates the
counterpart's content. Such an operation effectively decouples the features
based on their distinctive properties. To the best of our knowledge, this is
the first time in the field of scene text that disentangles the inherent
properties of the text images. Our method achieves state-of-the-art performance
in Scene Text Recognition, Removal, and Editing.","Zhang, Boqiang and Xie, Hongtao and Gao, Zuan and Wang, Yuxin",2024,,,,
SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model,http://arxiv.org/abs/2510.10910v1,wang2025glyphmastero,\cite{wang2025glyphmastero},GlyphMastero: A Glyph Encoder for High-Fidelity Scene Text Editing,http://arxiv.org/abs/2505.04915v1,"Scene text editing, a subfield of image editing, requires modifying texts in
images while preserving style consistency and visual coherence with the
surrounding environment. While diffusion-based methods have shown promise in
text generation, they still struggle to produce high-quality results. These
methods often generate distorted or unrecognizable characters, particularly
when dealing with complex characters like Chinese. In such systems, characters
are composed of intricate stroke patterns and spatial relationships that must
be precisely maintained. We present GlyphMastero, a specialized glyph encoder
designed to guide the latent diffusion model for generating texts with
stroke-level precision. Our key insight is that existing methods, despite using
pretrained OCR models for feature extraction, fail to capture the hierarchical
nature of text structures - from individual strokes to stroke-level
interactions to overall character-level structure. To address this, our glyph
encoder explicitly models and captures the cross-level interactions between
local-level individual characters and global-level text lines through our novel
glyph attention module. Meanwhile, our model implements a feature pyramid
network to fuse the multi-scale OCR backbone features at the global-level.
Through these cross-level and multi-scale fusions, we obtain more detailed
glyph-aware guidance, enabling precise control over the scene text generation
process. Our method achieves an 18.02\% improvement in sentence accuracy over
the state-of-the-art multi-lingual scene text editing baseline, while
simultaneously reducing the text-region Fr\'echet inception distance by
53.28\%.","Wang, Tong and Liu, Ting and Qu, Xiaochao and Wu, Chengjing and Liu, Luoqi and Hu, Xiaolin",2025,,,,
SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model,http://arxiv.org/abs/2510.10910v1,fang2025recognition,\cite{fang2025recognition},Recognition-Synergistic Scene Text Editing,http://arxiv.org/abs/2503.08387v2,"Scene text editing aims to modify text content within scene images while
maintaining style consistency. Traditional methods achieve this by explicitly
disentangling style and content from the source image and then fusing the style
with the target content, while ensuring content consistency using a pre-trained
recognition model. Despite notable progress, these methods suffer from complex
pipelines, leading to suboptimal performance in complex scenarios. In this
work, we introduce Recognition-Synergistic Scene Text Editing (RS-STE), a novel
approach that fully exploits the intrinsic synergy of text recognition for
editing. Our model seamlessly integrates text recognition with text editing
within a unified framework, and leverages the recognition model's ability to
implicitly disentangle style and content while ensuring content consistency.
Specifically, our approach employs a multi-modal parallel decoder based on
transformer architecture, which predicts both text content and stylized images
in parallel. Additionally, our cyclic self-supervised fine-tuning strategy
enables effective training on unpaired real-world data without ground truth,
enhancing style and content consistency through a twice-cyclic generation
process. Built on a relatively simple architecture, RS-STE achieves
state-of-the-art performance on both synthetic and real-world benchmarks, and
further demonstrates the effectiveness of leveraging the generated hard cases
to boost the performance of downstream recognition tasks. Code is available at
https://github.com/ZhengyaoFang/RS-STE.","Fang, Zhengyao and Lyu, Pengyuan and Wu, Jingjing and Zhang, Chengquan and Yu, Jun and Lu, Guangming and Pei, Wenjie",2025,,,,
