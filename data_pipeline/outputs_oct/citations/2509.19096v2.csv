parent_paper_title,parent_arxiv_link,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal
Investigating Traffic Accident Detection Using Multimodal Large Language Models,http://arxiv.org/abs/2509.19096v2,fang2023vision,\cite{fang2023vision},Vision-based traffic accident detection and anticipation: A survey,,,"Fang, Jianwu and Qiao, Jiahuan and Xue, Jianru and Li, Zhengguo",2023,,,,IEEE Transactions on Circuits and Systems for Video Technology
Investigating Traffic Accident Detection Using Multimodal Large Language Models,http://arxiv.org/abs/2509.19096v2,you2020traffic,\cite{you2020traffic},Traffic accident benchmark for causality recognition,,,"You, Tackgeun and Han, Bohyung",2020,,,,
Investigating Traffic Accident Detection Using Multimodal Large Language Models,http://arxiv.org/abs/2509.19096v2,7780460,\cite{7780460},"You Only Look Once: Unified, Real-Time Object Detection",http://arxiv.org/abs/1506.02640v5,"We present YOLO, a new approach to object detection. Prior work on object
detection repurposes classifiers to perform detection. Instead, we frame object
detection as a regression problem to spatially separated bounding boxes and
associated class probabilities. A single neural network predicts bounding boxes
and class probabilities directly from full images in one evaluation. Since the
whole detection pipeline is a single network, it can be optimized end-to-end
directly on detection performance.
  Our unified architecture is extremely fast. Our base YOLO model processes
images in real-time at 45 frames per second. A smaller version of the network,
Fast YOLO, processes an astounding 155 frames per second while still achieving
double the mAP of other real-time detectors. Compared to state-of-the-art
detection systems, YOLO makes more localization errors but is far less likely
to predict false detections where nothing exists. Finally, YOLO learns very
general representations of objects. It outperforms all other detection methods,
including DPM and R-CNN, by a wide margin when generalizing from natural images
to artwork on both the Picasso Dataset and the People-Art Dataset.","Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali",2016,,,10.1109/CVPR.2016.91,
Investigating Traffic Accident Detection Using Multimodal Large Language Models,http://arxiv.org/abs/2509.19096v2,ghahremannezhad2022real,\cite{ghahremannezhad2022real},Real-Time Accident Detection in Traffic Surveillance Using Deep Learning,http://arxiv.org/abs/2208.06461v1,"Automatic detection of traffic accidents is an important emerging topic in
traffic monitoring systems. Nowadays many urban intersections are equipped with
surveillance cameras connected to traffic management systems. Therefore,
computer vision techniques can be viable tools for automatic accident
detection. This paper presents a new efficient framework for accident detection
at intersections for traffic surveillance applications. The proposed framework
consists of three hierarchical steps, including efficient and accurate object
detection based on the state-of-the-art YOLOv4 method, object tracking based on
Kalman filter coupled with the Hungarian algorithm for association, and
accident detection by trajectory conflict analysis. A new cost function is
applied for object association to accommodate for occlusion, overlapping
objects, and shape changes in the object tracking step. The object trajectories
are analyzed in terms of velocity, angle, and distance in order to detect
different types of trajectory conflicts including vehicle-to-vehicle,
vehicle-to-pedestrian, and vehicle-to-bicycle. Experimental results using real
traffic video data show the feasibility of the proposed method in real-time
applications of traffic surveillance. In particular, trajectory conflicts,
including near-accidents and accidents occurring at urban intersections are
detected with a low false alarm rate and a high detection rate. The robustness
of the proposed framework is evaluated using video sequences collected from
YouTube with diverse illumination conditions. The dataset is publicly available
at: http://github.com/hadi-ghnd/AccidentDetection.","Ghahremannezhad, Hadi and Shi, Hang and Liu, Chengjun",2022,,,,
Investigating Traffic Accident Detection Using Multimodal Large Language Models,http://arxiv.org/abs/2509.19096v2,xia2022research,\cite{xia2022research},Research on urban traffic incident detection based on vehicle cameras,,,"Xia, Zhuofei and Gong, Jiayuan and Yu, Hailong and Ren, Wenbo and Wang, Jingnan",2022,,,,Future Internet
Investigating Traffic Accident Detection Using Multimodal Large Language Models,http://arxiv.org/abs/2509.19096v2,karim2024visual,\cite{karim2024visual},Visual Detection of Traffic Incident through Automatic Monitoring of Vehicle Activities,,,"Karim, Abdul and Raza, Muhammad Amir and Alharthi, Yahya Z and Abbas, Ghulam and Othmen, Salwa and Hossain, Md Shouquat and Nahar, Afroza and Mercorelli, Paolo",2024,,,,World Electric Vehicle Journal
Investigating Traffic Accident Detection Using Multimodal Large Language Models,http://arxiv.org/abs/2509.19096v2,wojke2017simple,\cite{wojke2017simple},Simple online and realtime tracking with a deep association metric,,,"Wojke, Nicolai and Bewley, Alex and Paulus, Dietrich",2017,,,,
Investigating Traffic Accident Detection Using Multimodal Large Language Models,http://arxiv.org/abs/2509.19096v2,yao2022dota,\cite{yao2022dota},DoTA: Unsupervised detection of traffic anomaly in driving videos,,,"Yao, Yu and Wang, Xizi and Xu, Mingze and Pu, Zelin and Wang, Yuchen and Atkins, Ella and Crandall, David J",2022,,,,IEEE transactions on pattern analysis and machine intelligence
Investigating Traffic Accident Detection Using Multimodal Large Language Models,http://arxiv.org/abs/2509.19096v2,lee2017crash,\cite{lee2017crash},Crash to not crash: Playing video games to predict vehicle collisions,,,"Lee, Kangwook and Kim, Hoon and Suh, Changho",2017,,,,Place holder journal
Investigating Traffic Accident Detection Using Multimodal Large Language Models,http://arxiv.org/abs/2509.19096v2,wang2024deepaccident,\cite{wang2024deepaccident},"DeepAccident: A Motion and Accident Prediction Benchmark for V2X
  Autonomous Driving",http://arxiv.org/abs/2304.01168v5,"Safety is the primary priority of autonomous driving. Nevertheless, no
published dataset currently supports the direct and explainable safety
evaluation for autonomous driving. In this work, we propose DeepAccident, a
large-scale dataset generated via a realistic simulator containing diverse
accident scenarios that frequently occur in real-world driving. The proposed
DeepAccident dataset includes 57K annotated frames and 285K annotated samples,
approximately 7 times more than the large-scale nuScenes dataset with 40k
annotated samples. In addition, we propose a new task, end-to-end motion and
accident prediction, which can be used to directly evaluate the accident
prediction ability for different autonomous driving algorithms. Furthermore,
for each scenario, we set four vehicles along with one infrastructure to record
data, thus providing diverse viewpoints for accident scenarios and enabling V2X
(vehicle-to-everything) research on perception and prediction tasks. Finally,
we present a baseline V2X model named V2XFormer that demonstrates superior
performance for motion and accident prediction and 3D object detection compared
to the single-vehicle model.","Wang, Tianqi and Kim, Sukmin and Wenxuan, Ji and Xie, Enze and Ge, Chongjian and Chen, Junsong and Li, Zhenguo and Luo, Ping",2024,,,,
Investigating Traffic Accident Detection Using Multimodal Large Language Models,http://arxiv.org/abs/2509.19096v2,dosovitskiy2017carla,\cite{dosovitskiy2017carla},CARLA: An Open Urban Driving Simulator,http://arxiv.org/abs/1711.03938v1,"We introduce CARLA, an open-source simulator for autonomous driving research.
CARLA has been developed from the ground up to support development, training,
and validation of autonomous urban driving systems. In addition to open-source
code and protocols, CARLA provides open digital assets (urban layouts,
buildings, vehicles) that were created for this purpose and can be used freely.
The simulation platform supports flexible specification of sensor suites and
environmental conditions. We use CARLA to study the performance of three
approaches to autonomous driving: a classic modular pipeline, an end-to-end
model trained via imitation learning, and an end-to-end model trained via
reinforcement learning. The approaches are evaluated in controlled scenarios of
increasing difficulty, and their performance is examined via metrics provided
by CARLA, illustrating the platform's utility for autonomous driving research.
The supplementary video can be viewed at https://youtu.be/Hp8Dz-Zek2E","Dosovitskiy, Alexey and Ros, German and Codevilla, Felipe and Lopez, Antonio and Koltun, Vladlen",2017,,,,
Investigating Traffic Accident Detection Using Multimodal Large Language Models,http://arxiv.org/abs/2509.19096v2,tong2024connectgpt,\cite{tong2024connectgpt},Connectgpt: Connect large language models with connected and automated vehicles,,,"Tong, Kailin and Solmaz, Selim",2024,,,,
Investigating Traffic Accident Detection Using Multimodal Large Language Models,http://arxiv.org/abs/2509.19096v2,wu2024accidentgpt,\cite{wu2024accidentgpt},"AccidentGPT: Large Multi-Modal Foundation Model for Traffic Accident
  Analysis",http://arxiv.org/abs/2401.03040v1,"Traffic accident analysis is pivotal for enhancing public safety and
developing road regulations. Traditional approaches, although widely used, are
often constrained by manual analysis processes, subjective decisions, uni-modal
outputs, as well as privacy issues related to sensitive data. This paper
introduces the idea of AccidentGPT, a foundation model of traffic accident
analysis, which incorporates multi-modal input data to automatically
reconstruct the accident process video with dynamics details, and furthermore
provide multi-task analysis with multi-modal outputs. The design of the
AccidentGPT is empowered with a multi-modality prompt with feedback for
task-oriented adaptability, a hybrid training schema to leverage labelled and
unlabelled data, and a edge-cloud split configuration for data privacy. To
fully realize the functionalities of this model, we proposes several research
opportunities. This paper serves as the stepping stone to fill the gaps in
traditional approaches of traffic accident analysis and attract the research
community attention for automatic, objective, and privacy-preserving traffic
accident analysis.","Wu, Kebin and Li, Wenbin and Xiao, Xiaofei",2024,,,,arXiv preprint arXiv:2401.03040
Investigating Traffic Accident Detection Using Multimodal Large Language Models,http://arxiv.org/abs/2509.19096v2,zhang2025language,\cite{zhang2025language},When language and vision meet road safety: leveraging multimodal large language models for video-based traffic accident analysis,,,"Zhang, Ruixuan and Wang, Beichen and Zhang, Juexiao and Bian, Zilin and Feng, Chen and Ozbay, Kaan",2025,,,,arXiv preprint arXiv:2501.10604
Investigating Traffic Accident Detection Using Multimodal Large Language Models,http://arxiv.org/abs/2509.19096v2,lohner2024enhancing,\cite{lohner2024enhancing},Enhancing Vision-Language Models with Scene Graphs for Traffic Accident Understanding,,,"Lohner, Aaron and Compagno, Francesco and Francis, Jonathan and Oltramari, Alessandro",2024,,,,
