parent_paper_title,parent_arxiv_link,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal
FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers,http://arxiv.org/abs/2509.16518v1,bsa,\cite{bsa},{Block Sparse Attention},,,"Guo, Junxian and Tang, Haotian and Yang, Shang and Zhang, Zhekai and Liu, Zhijian and Han, Song",2024,,,,GitHub repository
FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers,http://arxiv.org/abs/2509.16518v1,flashattn,\cite{flashattn},"FlashAttention: Fast and Memory-Efficient Exact Attention with
  IO-Awareness",http://arxiv.org/abs/2205.14135v2,"Transformers are slow and memory-hungry on long sequences, since the time and
memory complexity of self-attention are quadratic in sequence length.
Approximate attention methods have attempted to address this problem by trading
off model quality to reduce the compute complexity, but often do not achieve
wall-clock speedup. We argue that a missing principle is making attention
algorithms IO-aware -- accounting for reads and writes between levels of GPU
memory. We propose FlashAttention, an IO-aware exact attention algorithm that
uses tiling to reduce the number of memory reads/writes between GPU high
bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of
FlashAttention, showing that it requires fewer HBM accesses than standard
attention, and is optimal for a range of SRAM sizes. We also extend
FlashAttention to block-sparse attention, yielding an approximate attention
algorithm that is faster than any existing approximate attention method.
FlashAttention trains Transformers faster than existing baselines: 15%
end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the
MLPerf 1.1 training speed record, 3$\times$ speedup on GPT-2 (seq. length 1K),
and 2.4$\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention
and block-sparse FlashAttention enable longer context in Transformers, yielding
higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on
long-document classification) and entirely new capabilities: the first
Transformers to achieve better-than-chance performance on the Path-X challenge
(seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1%
accuracy).","Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher",2022,,,,Advances in neural information processing systems
FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers,http://arxiv.org/abs/2509.16518v1,flexattn,\cite{flexattn},"Flex Attention: A Programming Model for Generating Optimized Attention
  Kernels",http://arxiv.org/abs/2412.05496v1,"Over the past 7 years, attention has become one of the most important
primitives in deep learning. The primary approach to optimize attention is
FlashAttention, which fuses the operation together, drastically improving both
the runtime and the memory consumption. However, the importance of
FlashAttention combined with its monolithic nature poses a problem for
researchers aiming to try new attention variants -- a ""software lottery"". This
problem is exacerbated by the difficulty of writing efficient fused attention
kernels, resisting traditional compiler-based approaches. We introduce
FlexAttention, a novel compiler-driven programming model that allows
implementing the majority of attention variants in a few lines of idiomatic
PyTorch code. We demonstrate that many existing attention variants (e.g. Alibi,
Document Masking, PagedAttention, etc.) can be implemented via FlexAttention,
and that we achieve competitive performance compared to these handwritten
kernels. Finally, we demonstrate how FlexAttention allows for easy composition
of attention variants, solving the combinatorial explosion of attention
variants.","Dong, Juechu and Feng, Boyuan and Guessous, Driss and Liang, Yanbo and He, Horace",2024,,,,arXiv preprint arXiv:2412.05496
FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers,http://arxiv.org/abs/2509.16518v1,flashinfer,\cite{flashinfer},"FlashInfer: Efficient and Customizable Attention Engine for LLM
  Inference Serving",http://arxiv.org/abs/2501.01005v2,"Transformers, driven by attention mechanisms, form the foundation of large
language models (LLMs). As these models scale up, efficient GPU attention
kernels become essential for high-throughput and low-latency inference. Diverse
LLM applications demand flexible and high-performance attention solutions. We
present FlashInfer: a customizable and efficient attention engine for LLM
serving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse
format and composable formats to optimize memory access and reduce redundancy.
It also offers a customizable attention template, enabling adaptation to
various settings through Just-In-Time (JIT) compilation. Additionally,
FlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user
requests while maintaining compatibility with CUDAGraph which requires static
configuration. FlashInfer have been integrated into leading LLM serving
frameworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and
end-to-end evaluations demonstrate FlashInfer's ability to significantly boost
kernel performance across diverse inference scenarios: compared to
state-of-the-art LLM serving solutions, FlashInfer achieve 29-69%
inter-token-latency reduction compared to compiler backends for LLM serving
benchmark, 28-30% latency reduction for long-context inference, and 13-17%
speedup for LLM serving with parallel generation.","Ye, Zihao and Chen, Lequn and Lai, Ruihang and Lin, Wuwei and Zhang, Yineng and Wang, Stephanie and Chen, Tianqi and Kasikci, Baris and Grover, Vinod and Krishnamurthy, Arvind and others",2025,,,,arXiv preprint arXiv:2501.01005
FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers,http://arxiv.org/abs/2509.16518v1,flashmask,\cite{flashmask},FlashMask: Efficient and Rich Mask Extension of FlashAttention,http://arxiv.org/abs/2410.01359v2,"The computational and memory demands of vanilla attention scale quadratically
with the sequence length $N$, posing significant challenges for processing long
sequences in Transformer models. FlashAttention alleviates these challenges by
eliminating the $O(N^2)$ memory dependency and reducing attention latency
through IO-aware memory optimizations. However, its native support for certain
attention mask types is limited, and it does not inherently accommodate more
complex masking requirements. Previous approaches resort to using dense masks
with $O(N^2)$ memory complexity, leading to inefficiencies. In this paper, we
propose FlashMask, an extension of FlashAttention that introduces a column-wise
sparse representation of attention masks. This approach efficiently represents
a wide range of mask types and facilitates the development of optimized kernel
implementations. By adopting this novel representation, FlashMask achieves
linear memory complexity $O(N)$, suitable for modeling long-context sequences.
Moreover, this representation enables kernel optimizations that eliminate
unnecessary computations by leveraging sparsity in the attention mask, without
sacrificing computational accuracy, resulting in higher computational
efficiency. We evaluate FlashMask's performance in fine-tuning and alignment
training of LLMs such as SFT, LoRA, DPO, and RM. FlashMask achieves significant
throughput improvements, with end-to-end speedups ranging from 1.65x to 3.22x
compared to existing FlashAttention dense method. Additionally, our
kernel-level comparisons demonstrate that FlashMask surpasses the latest
counterpart, FlexAttention, by 12.1% to 60.7% in terms of kernel TFLOPs/s,
achieving 37.8% to 62.3% of the theoretical maximum FLOPs/s on the A100 GPU.
The code is open-sourced on PaddlePaddle and integrated into PaddleNLP,
supporting models with over 100 billion parameters for contexts up to 128K
tokens.","Wang, Guoxia and Zeng, Jinle and Xiao, Xiyuan and Wu, Siming and Yang, Jiabin and Zheng, Lujing and Chen, Zeyu and Bian, Jiang and Yu, Dianhai and Wang, Haifeng",2024,,,,arXiv preprint arXiv:2410.01359
FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers,http://arxiv.org/abs/2509.16518v1,minference,\cite{minference},"MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via
  Dynamic Sparse Attention",http://arxiv.org/abs/2407.02490v2,"The computational challenges of Large Language Model (LLM) inference remain a
significant barrier to their widespread deployment, especially as prompt
lengths continue to increase. Due to the quadratic complexity of the attention
computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens
(i.e., the pre-filling stage) on a single A100 GPU. Existing methods for
speeding up prefilling often fail to maintain acceptable accuracy or efficiency
when applied to long-context LLMs. To address this gap, we introduce MInference
(Milliontokens Inference), a sparse calculation method designed to accelerate
pre-filling of long-sequence processing. Specifically, we identify three unique
patterns in long-context attention matrices-the A-shape, Vertical-Slash, and
Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. We
determine the optimal pattern for each attention head offline and dynamically
build sparse indices based on the assigned pattern during inference. With the
pattern and sparse indices, we perform efficient sparse attention calculations
via our optimized GPU kernels to significantly reduce the latency in the
pre-filling stage of long-context LLMs. Our proposed technique can be directly
applied to existing LLMs without any modifications to the pre-training setup or
additional fine-tuning. By evaluating on a wide range of downstream tasks,
including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models
including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we
demonstrate that MInference effectively reduces inference latency by up to 10x
for pre-filling on an A100, while maintaining accuracy. Our code is available
at https://aka.ms/MInference.","Jiang, Huiqiang and Li, Yucheng and Zhang, Chengruidong and Wu, Qianhui and Luo, Xufang and Ahn, Surin and Han, Zhenhua and Abdi, Amir H and Li, Dongsheng and Lin, Chin-Yew and others",2024,,,,Advances in Neural Information Processing Systems
FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers,http://arxiv.org/abs/2509.16518v1,xattn,\cite{xattn},XAttention: Block Sparse Attention with Antidiagonal Scoring,http://arxiv.org/abs/2503.16428v1,"Long-Context Transformer Models (LCTMs) are vital for real-world applications
but suffer high computational costs due to attention's quadratic complexity.
Block-sparse attention mitigates this by focusing computation on critical
regions, yet existing methods struggle with balancing accuracy and efficiency
due to costly block importance measurements. In this paper, we introduce
XAttention, a plug-and-play framework that dramatically accelerates
long-context inference in Transformers models using sparse attention.
XAttention's key innovation is the insight that the sum of antidiagonal values
(i.e., from the lower-left to upper-right) in the attention matrix provides a
powerful proxy for block importance. This allows for precise identification and
pruning of non-essential blocks, resulting in high sparsity and dramatically
accelerated inference. Across comprehensive evaluations on demanding
long-context benchmarks-including RULER and LongBench for language, VideoMME
for video understanding, and VBench for video generation. XAttention achieves
accuracy comparable to full attention while delivering substantial
computational gains. We demonstrate up to 13.5x acceleration in attention
computation. These results underscore XAttention's ability to unlock the
practical potential of block sparse attention, paving the way for scalable and
efficient deployment of LCTMs in real-world applications. Code is available at
https://github.com/mit-han-lab/x-attention.","Xu, Ruyi and Xiao, Guangxuan and Huang, Haofeng and Guo, Junxian and Han, Song",2025,,,,arXiv preprint arXiv:2503.16428
FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers,http://arxiv.org/abs/2509.16518v1,flashdecode,\cite{flashdecode},Flashdecoding++: Faster large language model inference on gpus,,,"Hong, Ke and Dai, Guohao and Xu, Jiaming and Mao, Qiuli and Li, Xiuhong and Liu, Jun and Chen, Kangdi and Dong, Yuhan and Wang, Yu",2023,,,,arXiv preprint arXiv:2311.01282
FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers,http://arxiv.org/abs/2509.16518v1,nsa,\cite{nsa},"Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse
  Attention",http://arxiv.org/abs/2502.11089v2,"Long-context modeling is crucial for next-generation language models, yet the
high computational cost of standard attention mechanisms poses significant
computational challenges. Sparse attention offers a promising direction for
improving efficiency while maintaining model capabilities. We present NSA, a
Natively trainable Sparse Attention mechanism that integrates algorithmic
innovations with hardware-aligned optimizations to achieve efficient
long-context modeling. NSA employs a dynamic hierarchical sparse strategy,
combining coarse-grained token compression with fine-grained token selection to
preserve both global context awareness and local precision. Our approach
advances sparse attention design with two key innovations: (1) We achieve
substantial speedups through arithmetic intensity-balanced algorithm design,
with implementation optimizations for modern hardware. (2) We enable end-to-end
training, reducing pretraining computation without sacrificing model
performance. As shown in Figure 1, experiments show the model pretrained with
NSA maintains or exceeds Full Attention models across general benchmarks,
long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves
substantial speedups over Full Attention on 64k-length sequences across
decoding, forward propagation, and backward propagation, validating its
efficiency throughout the model lifecycle.","Yuan, Jingyang and Gao, Huazuo and Dai, Damai and Luo, Junyu and Zhao, Liang and Zhang, Zhengyan and Xie, Zhenda and Wei, YX and Wang, Lean and Xiao, Zhiping and others",2025,,,,arXiv preprint arXiv:2502.11089
FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers,http://arxiv.org/abs/2509.16518v1,seerattn,\cite{seerattn},SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs,http://arxiv.org/abs/2410.13276v4,"Attention is the cornerstone of modern Large Language Models (LLMs). Yet its
quadratic complexity hinders efficiency and scalability, especially for
long-context processing. A promising approach is to leverage sparsity in
attention. However, existing sparsity-based solutions predominantly rely on
predefined patterns or heuristics at the attention head level, struggling to
adapt dynamically to different contexts efficiently.
  We propose SeerAttention, a simple yet effective attention mechanism that
directly learns the block-level attention sparsity from the LLM itself.
Inspired by the gating mechanism in Mixture of Experts (MoE), SeerAttention
augments the conventional attention with a learnable gate that selectively
activates important blocks within the attention map. Specifically, the gate
first pools the query (Q) and key (K) tensors along the sequence dimension and
processes them through learnable linear layers. The resulting matrices are then
multiplied together to produce the gating scores, which are used to predict
block-level attention sparsity. Combined with our block-sparse FlashAttention
kernel, SeerAttention can achieve significant speedup on GPUs. When applied to
pre-trained LLMs, SeerAttention only requires training the gate parameters in a
lightweight self-distillation manner, allowing rapid convergence. Our
evaluation results demonstrate that SeerAttention achieves better model
accuracy and lower latency for long-context pre-filling compared to prior
methods. Code is available at: https://github.com/microsoft/SeerAttention","Gao, Yizhao and Zeng, Zhichen and Du, Dayou and Cao, Shijie and Zhou, Peiyuan and Qi, Jiaxing and Lai, Junjie and So, Hayden Kwok-Hay and Cao, Ting and Yang, Fan and others",2024,,,,arXiv preprint arXiv:2410.13276
FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers,http://arxiv.org/abs/2509.16518v1,radialattn,\cite{radialattn},Radial Attention: $ O (nlog n) $ Sparse Attention with Energy Decay for Long Video Generation,,,"Li, Xingyang and Li, Muyang and Cai, Tianle and Xi, Haocheng and Yang, Shuo and Lin, Yujun and Zhang, Lvmin and Yang, Songlin and Hu, Jinbo and Peng, Kelly and others",2025,,,,arXiv preprint arXiv:2506.19852
FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers,http://arxiv.org/abs/2509.16518v1,sparsevideogen,\cite{sparsevideogen},"Sparse VideoGen: Accelerating Video Diffusion Transformers with
  Spatial-Temporal Sparsity",http://arxiv.org/abs/2502.01776v2,"Diffusion Transformers (DiTs) dominate video generation but their high
computational cost severely limits real-world applicability, usually requiring
tens of minutes to generate a few seconds of video even on high-performance
GPUs. This inefficiency primarily arises from the quadratic computational
complexity of 3D Full Attention with respect to the context length. In this
paper, we propose a training-free framework termed Sparse VideoGen (SVG) that
leverages the inherent sparsity in 3D Full Attention to boost inference
efficiency. We reveal that the attention heads can be dynamically classified
into two groups depending on distinct sparse patterns: (1) Spatial Head, where
only spatially-related tokens within each frame dominate the attention output,
and (2) Temporal Head, where only temporally-related tokens across different
frames dominate. Based on this insight, SVG proposes an online profiling
strategy to capture the dynamic sparse patterns and predicts the type of
attention head. Combined with a novel hardware-efficient tensor layout
transformation and customized kernel implementations, SVG achieves up to 2.28x
and 2.33x end-to-end speedup on CogVideoX-v1.5 and HunyuanVideo, respectively,
while preserving generation quality. Our code is open-sourced and is available
at https://github.com/svg-project/Sparse-VideoGen","Xi, Haocheng and Yang, Shuo and Zhao, Yilong and Xu, Chenfeng and Li, Muyang and Li, Xiuyu and Lin, Yujun and Cai, Han and Zhang, Jintao and Li, Dacheng and others",2025,,,,arXiv preprint arXiv:2502.01776
FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers,http://arxiv.org/abs/2509.16518v1,sparsevideogen2,\cite{sparsevideogen2},"Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via
  Semantic-Aware Permutation",http://arxiv.org/abs/2505.18875v3,"Diffusion Transformers (DiTs) are essential for video generation but suffer
from significant latency due to the quadratic complexity of attention. By
computing only critical tokens, sparse attention reduces computational costs
and offers a promising acceleration approach. However, we identify that
existing methods fail to approach optimal generation quality under the same
computation budget for two reasons: (1) Inaccurate critical token
identification: current methods cluster tokens based on position rather than
semantics, leading to imprecise aggregated representations. (2) Excessive
computation waste: critical tokens are scattered among non-critical ones,
leading to wasted computation on GPUs, which are optimized for processing
contiguous tokens. In this paper, we propose SVG2, a training-free framework
that maximizes identification accuracy and minimizes computation waste,
achieving a Pareto frontier trade-off between generation quality and
efficiency. The core of SVG2 is semantic-aware permutation, which clusters and
reorders tokens based on semantic similarity using k-means. This approach
ensures both a precise cluster representation, improving identification
accuracy, and a densified layout of critical tokens, enabling efficient
computation without padding. Additionally, SVG2 integrates top-p dynamic budget
control and customized kernel implementations, achieving up to 2.30x and 1.89x
speedup while maintaining a PSNR of up to 30 and 26 on HunyuanVideo and Wan
2.1, respectively. Our code is open-sourced at
\href{https://github.com/svg-project/Sparse-VideoGen}{https://github.com/svg-project/Sparse-VideoGen}.","Yang, Shuo and Xi, Haocheng and Zhao, Yilong and Li, Muyang and Zhang, Jintao and Cai, Han and Lin, Yujun and Li, Xiuyu and Xu, Chenfeng and Peng, Kelly and others",2025,,,,arXiv preprint arXiv:2505.18875
FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers,http://arxiv.org/abs/2509.16518v1,vsa,\cite{vsa},VSA: Faster Video Diffusion with Trainable Sparse Attention,http://arxiv.org/abs/2505.13389v4,"Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D
attention, even though most of the attention mass concentrates on a small
subset of positions. We turn this observation into VSA, a trainable,
hardware-efficient sparse attention that replaces full attention at \emph{both}
training and inference. In VSA, a lightweight coarse stage pools tokens into
tiles and identifies high-weight \emph{critical tokens}; a fine stage computes
token-level attention only inside those tiles subjecting to block computing
layout to ensure hard efficiency. This leads to a single differentiable kernel
that trains end-to-end, requires no post-hoc profiling, and sustains 85\% of
FlashAttention3 MFU. We perform a large sweep of ablation studies and
scaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA
reaches a Pareto point that cuts training FLOPS by 2.53$\times$ with no drop in
diffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention
time by 6$\times$ and lowers end-to-end generation time from 31s to 18s with
comparable quality. These results establish trainable sparse attention as a
practical alternative to full attention and a key enabler for further scaling
of video diffusion models. Code will be available at
https://github.com/hao-ai-lab/FastVideo.","Zhang, Peiyuan and Huang, Haofeng and Chen, Yongqi and Lin, Will and Liu, Zhengzhong and Stoica, Ion and Xing, Eric and Zhang, Hao",2025,,,,arXiv preprint arXiv:2505.13389
FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers,http://arxiv.org/abs/2509.16518v1,spargeattn,\cite{spargeattn},"SpargeAttention: Accurate and Training-free Sparse Attention
  Accelerating Any Model Inference",http://arxiv.org/abs/2502.18137v7,"An efficient attention implementation is essential for large models due to
its quadratic time complexity. Fortunately, attention commonly exhibits
sparsity, i.e., many values in the attention map are near zero, allowing for
the omission of corresponding computations. Many studies have utilized the
sparse pattern to accelerate attention. However, most existing works focus on
optimizing attention within specific models by exploiting certain sparse
patterns of the attention map. A universal sparse attention that guarantees
both the speedup and end-to-end performance of diverse models remains elusive.
In this paper, we propose SpargeAttn, a universal sparse and quantized
attention for any model. Our method uses a two-stage online filter: in the
first stage, we rapidly and accurately predict the attention map, enabling the
skip of some matrix multiplications in attention. In the second stage, we
design an online softmax-aware filter that incurs no extra overhead and further
skips some matrix multiplications. Experiments show that our method
significantly accelerates diverse models, including language, image, and video
generation, without sacrificing end-to-end metrics. The codes are available at
https://github.com/thu-ml/SpargeAttn.","Zhang, Jintao and Xiang, Chendong and Huang, Haofeng and Xi, Haocheng and Zhu, Jun and Chen, Jianfei and others",2025,,,,
FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers,http://arxiv.org/abs/2509.16518v1,sageattn,\cite{sageattn},Sageattention: Accurate 8-bit attention for plug-and-play inference acceleration,,,"Zhang, Jintao and Wei, Jia and Huang, Haofeng and Zhang, Pengle and Zhu, Jun and Chen, Jianfei",2024,,,,arXiv preprint arXiv:2410.02367
FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers,http://arxiv.org/abs/2509.16518v1,sageattn2,\cite{sageattn2},"SageAttention2: Efficient Attention with Thorough Outlier Smoothing and
  Per-thread INT4 Quantization",http://arxiv.org/abs/2411.10958v7,"Although quantization for linear layers has been widely used, its application
to accelerate the attention process remains limited. To further enhance the
efficiency of attention computation compared to SageAttention while maintaining
precision, we propose SageAttention2, which utilizes significantly faster 4-bit
matrix multiplication (Matmul) alongside additional precision-enhancing
techniques. First, we propose to quantize matrices $(Q, K)$ to INT4 in a
hardware-friendly thread-level granularity and quantize matrices $(\widetilde
P, V)$ to FP8. Second, we propose a method to smooth $Q$, enhancing the
accuracy of INT4 $QK^\top$. Third, we propose a two-level accumulation strategy
for $\widetilde PV$ to enhance the accuracy of FP8 $\widetilde PV$. The
operations per second (OPS) of SageAttention2 surpass FlashAttention2 and
xformers by about 3x and 4.5x on RTX4090, respectively. Moreover,
SageAttention2 matches the speed of FlashAttention3(fp8) on the Hopper GPUs,
while delivering much higher accuracy. Comprehensive experiments confirm that
our approach incurs negligible end-to-end metrics loss across diverse models,
including those for language, image, and video generation. The code is
available at https://github.com/thu-ml/SageAttention.","Zhang, Jintao and Huang, Haofeng and Zhang, Pengle and Wei, Jia and Zhu, Jun and Chen, Jianfei",2024,,,,arXiv preprint arXiv:2411.10958
