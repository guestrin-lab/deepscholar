parent_paper_title,parent_arxiv_link,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,http://arxiv.org/abs/2510.08783v1,gu2024survey,\cite{gu2024survey},A survey on llm-as-a-judge,,,"Gu, Jiawei and Jiang, Xuhui and Shi, Zhichao and Tan, Hexiang and Zhai, Xuehao and Xu, Chengjin and Li, Wei and Shen, Yinghan and Ma, Shengjie and Liu, Honghao and others",2024,,,,arXiv preprint arXiv:2411.15594
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,http://arxiv.org/abs/2510.08783v1,achiam2023gpt,\cite{achiam2023gpt},Gpt-4 technical report,,,"Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others",2023,,,,arXiv preprint arXiv:2303.08774
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,http://arxiv.org/abs/2510.08783v1,li2023generative,\cite{li2023generative},Generative Judge for Evaluating Alignment,http://arxiv.org/abs/2310.05470v2,"The rapid development of Large Language Models (LLMs) has substantially
expanded the range of tasks they can address. In the field of Natural Language
Processing (NLP), researchers have shifted their focus from conventional NLP
tasks (e.g., sequence tagging and parsing) towards tasks that revolve around
aligning with human needs (e.g., brainstorming and email writing). This shift
in task distribution imposes new requirements on evaluating these aligned
models regarding generality (i.e., assessing performance across diverse
scenarios), flexibility (i.e., examining under different protocols), and
interpretability (i.e., scrutinizing models with explanations). In this paper,
we propose a generative judge with 13B parameters, Auto-J, designed to address
these challenges. Our model is trained on user queries and LLM-generated
responses under massive real-world scenarios and accommodates diverse
evaluation protocols (e.g., pairwise response comparison and single-response
evaluation) with well-structured natural language critiques. To demonstrate the
efficacy of our approach, we construct a new testbed covering 58 different
scenarios. Experimentally, Auto-J outperforms a series of strong competitors,
including both open-source and closed-source models, by a large margin. We also
provide detailed analysis and case studies to further reveal the potential of
our method and make a variety of resources public at
https://github.com/GAIR-NLP/auto-j.","Li, Junlong and Sun, Shichao and Yuan, Weizhe and Fan, Run-Ze and Zhao, Hai and Liu, Pengfei",2023,,,,arXiv preprint arXiv:2310.05470
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,http://arxiv.org/abs/2510.08783v1,wang2023large,\cite{wang2023large},Large Language Models are not Fair Evaluators,http://arxiv.org/abs/2305.17926v2,"In this paper, we uncover a systematic bias in the evaluation paradigm of
adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and
compare the quality of responses generated by candidate models. We find that
the quality ranking of candidate responses can be easily hacked by simply
altering their order of appearance in the context. This manipulation allows us
to skew the evaluation result, making one model appear considerably superior to
the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries
with ChatGPT as an evaluator. To address this issue, we propose a calibration
framework with three simple yet effective strategies: 1) Multiple Evidence
Calibration, which requires the evaluator model to generate multiple evaluation
evidence before assigning ratings; 2) Balanced Position Calibration, which
aggregates results across various orders to determine the final score; 3)
Human-in-the-Loop Calibration, which introduces a balanced position diversity
entropy to measure the difficulty of each example and seeks human assistance
when needed. We also manually annotate the ""win/tie/lose"" outcomes of responses
from ChatGPT and Vicuna-13B in the Vicuna Benchmark's question prompt, and
extensive experiments demonstrate that our approach successfully mitigates
evaluation bias, resulting in closer alignment with human judgments. We release
our code and human annotation at \url{https://github.com/i-Eval/FairEval} to
facilitate future research.","Wang, Peiyi and Li, Lei and Chen, Liang and Cai, Zefan and Zhu, Dawei and Lin, Binghuai and Cao, Yunbo and Liu, Qi and Liu, Tianyu and Sui, Zhifang",2023,,,,arXiv preprint arXiv:2305.17926
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,http://arxiv.org/abs/2510.08783v1,zhang2023wider,\cite{zhang2023wider},Wider and Deeper LLM Networks are Fairer LLM Evaluators,http://arxiv.org/abs/2308.01862v1,"Measuring the quality of responses generated by LLMs is a challenging task,
particularly when it comes to evaluating whether the response is aligned with
human preference. A novel approach involves using the LLM itself to make
evaluation and stabilizing the results through multiple independent
evaluations, similar to a single-layer narrow LLM network. This network
consists of a fixed number of neurons, with each neuron being the same LLM. In
this paper, we draw upon the extensive research on deep neural networks to
explore whether deeper and wider networks can lead to fairer evaluations.
Specifically, inspired by the observation that different neurons in a neural
network are responsible for detecting different concepts, we first adaptively
generate as many neuron roles as possible for each evaluation sample. Each
perspective corresponds to the role of a specific LLM neuron in the first
layer. In subsequent layers, we follow the idea that higher layers in deep
networks are responsible for more comprehensive features, each layer receives
representations from all neurons in the previous layer, integrating the locally
learned evaluation information to obtain a more comprehensive evaluation
result. Interestingly, this network design resembles the process of academic
paper reviewing. To validate the effectiveness of our method, we construct the
largest and most diverse English evaluation benchmark LLMEval$^2$ for LLM
evaluators, comprising 15 tasks, 8 abilities, and 2,553 samples. Experimental
results demonstrate that a wider network (involving many reviewers) with 2
layers (one round of discussion) performs the best, improving kappa correlation
coefficient from 0.28 to 0.34. We also leverage WideDeep to aid in the
assessment of Chinese LLMs, which has accelerated the evaluation time by 4.6
times, resulting in a 60% cost saving. WideDeep achieves a remarkable 93%
agreement level among humans.","Zhang, Xinghua and Yu, Bowen and Yu, Haiyang and Lv, Yangyu and Liu, Tingwen and Huang, Fei and Xu, Hongbo and Li, Yongbin",2023,,,,arXiv preprint arXiv:2308.01862
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,http://arxiv.org/abs/2510.08783v1,zheng2023judging,\cite{zheng2023judging},Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena,http://arxiv.org/abs/2306.05685v4,"Evaluating large language model (LLM) based chat assistants is challenging
due to their broad capabilities and the inadequacy of existing benchmarks in
measuring human preferences. To address this, we explore using strong LLMs as
judges to evaluate these models on more open-ended questions. We examine the
usage and limitations of LLM-as-a-judge, including position, verbosity, and
self-enhancement biases, as well as limited reasoning ability, and propose
solutions to mitigate some of them. We then verify the agreement between LLM
judges and human preferences by introducing two benchmarks: MT-bench, a
multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our
results reveal that strong LLM judges like GPT-4 can match both controlled and
crowdsourced human preferences well, achieving over 80% agreement, the same
level of agreement between humans. Hence, LLM-as-a-judge is a scalable and
explainable way to approximate human preferences, which are otherwise very
expensive to obtain. Additionally, we show our benchmark and traditional
benchmarks complement each other by evaluating several variants of LLaMA and
Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with
human preferences are publicly available at
https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.","Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others",2023,,,,Advances in Neural Information Processing Systems
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,http://arxiv.org/abs/2510.08783v1,wang2022self,\cite{wang2022self},Self-Instruct: Aligning Language Models with Self-Generated Instructions,http://arxiv.org/abs/2212.10560v2,"Large ""instruction-tuned"" language models (i.e., finetuned to respond to
instructions) have demonstrated a remarkable ability to generalize zero-shot to
new tasks. Nevertheless, they depend heavily on human-written instruction data
that is often limited in quantity, diversity, and creativity, therefore
hindering the generality of the tuned model. We introduce Self-Instruct, a
framework for improving the instruction-following capabilities of pretrained
language models by bootstrapping off their own generations. Our pipeline
generates instructions, input, and output samples from a language model, then
filters invalid or similar ones before using them to finetune the original
model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute
improvement over the original model on Super-NaturalInstructions, on par with
the performance of InstructGPT-001, which was trained with private user data
and human annotations. For further evaluation, we curate a set of
expert-written instructions for novel tasks, and show through human evaluation
that tuning GPT3 with Self-Instruct outperforms using existing public
instruction datasets by a large margin, leaving only a 5% absolute gap behind
InstructGPT-001. Self-Instruct provides an almost annotation-free method for
aligning pre-trained language models with instructions, and we release our
large synthetic dataset to facilitate future studies on instruction tuning. Our
code and data are available at https://github.com/yizhongw/self-instruct.","Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh",2022,,,,arXiv preprint arXiv:2212.10560
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,http://arxiv.org/abs/2510.08783v1,ramamurthy2022reinforcement,\cite{ramamurthy2022reinforcement},"Is Reinforcement Learning (Not) for Natural Language Processing:
  Benchmarks, Baselines, and Building Blocks for Natural Language Policy
  Optimization",http://arxiv.org/abs/2210.01241v3,"We tackle the problem of aligning pre-trained large language models (LMs)
with human preferences. If we view text generation as a sequential
decision-making problem, reinforcement learning (RL) appears to be a natural
conceptual framework. However, using RL for LM-based generation faces empirical
challenges, including training instability due to the combinatorial action
space, as well as a lack of open-source libraries and benchmarks customized for
LM alignment. Thus, a question rises in the research community: is RL a
practical paradigm for NLP?
  To help answer this, we first introduce an open-source modular library,
RL4LMs (Reinforcement Learning for Language Models), for optimizing language
generators with RL. The library consists of on-policy RL algorithms that can be
used to train any encoder or encoder-decoder LM in the HuggingFace library
(Wolf et al. 2020) with an arbitrary reward function. Next, we present the GRUE
(General Reinforced-language Understanding Evaluation) benchmark, a set of 6
language generation tasks which are supervised not by target strings, but by
reward functions which capture automated measures of human preference. GRUE is
the first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally,
we introduce an easy-to-use, performant RL algorithm, NLPO (Natural Language
Policy Optimization) that learns to effectively reduce the combinatorial action
space in language generation. We show 1) that RL techniques are generally
better than supervised methods at aligning LMs to human preferences; and 2)
that NLPO exhibits greater stability and performance than previous policy
gradient methods (e.g., PPO (Schulman et al. 2017)), based on both automatic
and human evaluations.","Ramamurthy, Rajkumar and Ammanabrolu, Prithviraj and Brantley, Kiant{\'e} and Hessel, Jack and Sifa, Rafet and Bauckhage, Christian and Hajishirzi, Hannaneh and Choi, Yejin",2022,,,,arXiv preprint arXiv:2210.01241
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,http://arxiv.org/abs/2510.08783v1,ouyang2022training,\cite{ouyang2022training},Training language models to follow instructions with human feedback,,,"Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",2022,,,,Advances in neural information processing systems
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,http://arxiv.org/abs/2510.08783v1,kim2025chart,\cite{kim2025chart},Chart-to-Experience: Benchmarking Multimodal LLMs for Predicting Experiential Impact of Charts,,,"Kim, Seon Gyeom and Choi, Jae Young and Rossi, Ryan and Koh, Eunyee and Lee, Tak Yeon",2025,,,,
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,http://arxiv.org/abs/2510.08783v1,team2023gemini,\cite{team2023gemini},Gemini: A Family of Highly Capable Multimodal Models,http://arxiv.org/abs/2312.11805v5,"This report introduces a new family of multimodal models, Gemini, that
exhibit remarkable capabilities across image, audio, video, and text
understanding. The Gemini family consists of Ultra, Pro, and Nano sizes,
suitable for applications ranging from complex reasoning tasks to on-device
memory-constrained use-cases. Evaluation on a broad range of benchmarks shows
that our most-capable Gemini Ultra model advances the state of the art in 30 of
32 of these benchmarks - notably being the first model to achieve human-expert
performance on the well-studied exam benchmark MMLU, and improving the state of
the art in every one of the 20 multimodal benchmarks we examined. We believe
that the new capabilities of the Gemini family in cross-modal reasoning and
language understanding will enable a wide variety of use cases. We discuss our
approach toward post-training and deploying Gemini models responsibly to users
through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud
Vertex AI.","Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie and others",2023,,,,arXiv preprint arXiv:2312.11805
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,http://arxiv.org/abs/2510.08783v1,guo2025deepseek,\cite{guo2025deepseek},Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,,,"Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others",2025,,,,arXiv preprint arXiv:2501.12948
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,http://arxiv.org/abs/2510.08783v1,thakur2025judgingjudgesevaluatingalignment,\cite{thakur2025judgingjudgesevaluatingalignment},"Judging the Judges: Evaluating Alignment and Vulnerabilities in
  LLMs-as-Judges",http://arxiv.org/abs/2406.12624v6,"Offering a promising solution to the scalability challenges associated with
human evaluation, the LLM-as-a-judge paradigm is rapidly gaining traction as an
approach to evaluating large language models (LLMs). However, there are still
many open questions about the strengths and weaknesses of this paradigm, and
what potential biases it may hold. In this paper, we present a comprehensive
study of the performance of various LLMs acting as judges, focusing on a clean
scenario in which inter-human agreement is high. Investigating thirteen judge
models of different model sizes and families, judging answers of nine different
'examtaker models' - both base and instruction-tuned - we find that only the
best (and largest) models achieve reasonable alignment with humans. However,
they are still quite far behind inter-human agreement and their assigned scores
may still differ with up to 5 points from human-assigned scores. In terms of
their ranking of the nine exam-taker models, instead, also smaller models and
even the lexical metric contains may provide a reasonable signal. Through error
analysis and other studies, we identify vulnerabilities in judge models, such
as their sensitivity to prompt complexity and length, and a tendency toward
leniency. The fact that even the best judges differ from humans in this
comparatively simple setup suggest that caution may be wise when using judges
in more complex setups. Lastly, our research rediscovers the importance of
using alignment metrics beyond simple percent alignment, showing that judges
with high percent agreement can still assign vastly different scores.",Aman Singh Thakur and Kartik Choudhary and Venkat Srinik Ramayapally and Sankaran Vaidyanathan and Dieuwke Hupkes,2025,,https://arxiv.org/abs/2406.12624,,
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,http://arxiv.org/abs/2510.08783v1,liu2024aligning,\cite{liu2024aligning},"Aligning with Human Judgement: The Role of Pairwise Preference in Large
  Language Model Evaluators",http://arxiv.org/abs/2403.16950v5,"Large Language Models (LLMs) have demonstrated promising capabilities as
automatic evaluators in assessing the quality of generated natural language.
However, LLMs still exhibit biases in evaluation and often struggle to generate
coherent evaluations that align with human assessments. In this work, we first
conduct a systematic study of the misalignment between LLM evaluators and human
evaluation, revealing that existing calibration methods aimed at mitigating
biases of LLMs are insufficient for effectively aligning LLM evaluators.
Inspired by the use of preference data in RLHF, we formulate the evaluation as
a ranking problem and introduce Pairwise-preference Search (PAIRS), an
uncertainty-guided search-based rank aggregation method that employs LLMs to
conduct pairwise comparisons locally and efficiently ranks candidate texts
globally. PAIRS achieves state-of-the-art performance on representative
evaluation tasks in long-form generations and demonstrates significant
improvements over direct scoring. Furthermore, we provide insights into the
role of pairwise preference in quantifying the transitivity of LLMs and
demonstrate how PAIRS benefits from calibration using debiased pairwise
evaluations.","Liu, Yinhong and Zhou, Han and Guo, Zhijiang and Shareghi, Ehsan and Vuli{\'c}, Ivan and Korhonen, Anna and Collier, Nigel",2024,,,,arXiv preprint arXiv:2403.16950
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,http://arxiv.org/abs/2510.08783v1,panickssery2024llmevaluatorsrecognizefavor,\cite{panickssery2024llmevaluatorsrecognizefavor},LLM Evaluators Recognize and Favor Their Own Generations,http://arxiv.org/abs/2404.13076v1,"Self-evaluation using large language models (LLMs) has proven valuable not
only in benchmarking but also methods like reward modeling, constitutional AI,
and self-refinement. But new biases are introduced due to the same LLM acting
as both the evaluator and the evaluatee. One such bias is self-preference,
where an LLM evaluator scores its own outputs higher than others' while human
annotators consider them of equal quality. But do LLMs actually recognize their
own outputs when they give those texts higher scores, or is it just a
coincidence? In this paper, we investigate if self-recognition capability
contributes to self-preference. We discover that, out of the box, LLMs such as
GPT-4 and Llama 2 have non-trivial accuracy at distinguishing themselves from
other LLMs and humans. By fine-tuning LLMs, we discover a linear correlation
between self-recognition capability and the strength of self-preference bias;
using controlled experiments, we show that the causal explanation resists
straightforward confounders. We discuss how self-recognition can interfere with
unbiased evaluations and AI safety more generally.",Arjun Panickssery and Samuel R. Bowman and Shi Feng,2024,,https://arxiv.org/abs/2404.13076,,
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,http://arxiv.org/abs/2510.08783v1,qin2024largelanguagemodelseffective,\cite{qin2024largelanguagemodelseffective},Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting,,,Zhen Qin and Rolf Jagerman and Kai Hui and Honglei Zhuang and Junru Wu and Le Yan and Jiaming Shen and Tianqi Liu and Jialu Liu and Donald Metzler and Xuanhui Wang and Michael Bendersky,2024,,https://arxiv.org/abs/2306.17563,,
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,http://arxiv.org/abs/2510.08783v1,bertao2021artificial,\cite{bertao2021artificial},Artificial intelligence in UX/UI design: a survey on current adoption and [future] practices,,,"Bert{\~a}o, Renato Antonio and Joo, Jaewoo",2021,,,,Safe Harbors for Design Research
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,http://arxiv.org/abs/2510.08783v1,luera2024survey,\cite{luera2024survey},"Survey of User Interface Design and Interaction Techniques in Generative
  AI Applications",http://arxiv.org/abs/2410.22370v1,"The applications of generative AI have become extremely impressive, and the
interplay between users and AI is even more so. Current human-AI interaction
literature has taken a broad look at how humans interact with generative AI,
but it lacks specificity regarding the user interface designs and patterns used
to create these applications. Therefore, we present a survey that
comprehensively presents taxonomies of how a human interacts with AI and the
user interaction patterns designed to meet the needs of a variety of relevant
use cases. We focus primarily on user-guided interactions, surveying
interactions that are initiated by the user and do not include any implicit
signals given by the user. With this survey, we aim to create a compendium of
different user-interaction patterns that can be used as a reference for
designers and developers alike. In doing so, we also strive to lower the entry
barrier for those attempting to learn more about the design of generative AI
applications.","Luera, Reuben and Rossi, Ryan A and Siu, Alexa and Dernoncourt, Franck and Yu, Tong and Kim, Sungchul and Zhang, Ruiyi and Chen, Xiang and Salehy, Hanieh and Zhao, Jian and others",2024,,,,arXiv preprint arXiv:2410.22370
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,http://arxiv.org/abs/2510.08783v1,10.1145/3613904.3642168,\cite{10.1145/3613904.3642168},Enhancing UX Evaluation Through Collaboration with Conversational AI Assistants: Effects of Proactive Dialogue and Timing,,,"Kuang, Emily and Li, Minghao and Fan, Mingming and Shinohara, Kristen",2024,,https://doi.org/10.1145/3613904.3642168,10.1145/3613904.3642168,
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,http://arxiv.org/abs/2510.08783v1,cao2025survey,\cite{cao2025survey},Learning to Simulate Survey Distributions with Fine-Tuned LLMs,,,"Cao, Renjie and Zhang, Mei and Huang, Yu",2025,,,,
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,http://arxiv.org/abs/2510.08783v1,rosala2024synthetic,\cite{rosala2024synthetic},Synthetic Users and AI Personas in UX Research: A Cautionary Note,,,"Rosala, Tina and Moran, Kate",2024,,https://www.nngroup.com/articles/synthetic-users-ai/,,Nielsen Norman Group Reports
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,http://arxiv.org/abs/2510.08783v1,10.1145/3701716.3715452,\cite{10.1145/3701716.3715452},Personalizing Data Delivery: Investigating User Characteristics and Enhancing LLM Predictions,,,"Luera, Reuben and Rossi, Ryan and Dernoncourt, Franck and Siu, Alexa and Kim, Sungchul and Yu, Tong and Zhang, Ruiyi and Chen, Xiang and Lipka, Nedim and Zhang, Zhehao and Gyeom Kim, Seon and Yeon Lee, Tak",2025,,https://doi.org/10.1145/3701716.3715452,10.1145/3701716.3715452,
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,http://arxiv.org/abs/2510.08783v1,DBLP:journals/nlpj/JansenJS23,\cite{DBLP:journals/nlpj/JansenJS23},Employing large language models in survey research,,,Bernard J. Jansen and Soon-Gyo Jung and Joni Salminen,2023,,https://doi.org/10.1016/j.nlp.2023.100020,,Nat. Lang. Process. J.
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,http://arxiv.org/abs/2510.08783v1,10.1145/3544548.3580688,\cite{10.1145/3544548.3580688},Evaluating Large Language Models in Generating Synthetic HCI Research Data: a Case Study,,,"H\""{a}m\""{a}l\""{a}inen, Perttu and Tavast, Mikke and Kunnari, Anton",2023,,https://doi.org/10.1145/3544548.3580688,10.1145/3544548.3580688,
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,http://arxiv.org/abs/2510.08783v1,li2024frontiers,\cite{li2024frontiers},Frontiers: Determining the validity of large language models for automated perceptual analysis,,,"Li, Peiyao and Castelo, Noah and Katona, Zsolt and Sarvary, Miklos",2024,,,,Marketing Science
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,http://arxiv.org/abs/2510.08783v1,duan2024generating,\cite{duan2024generating},Generating Automatic Feedback on UI Mockups with Large Language Models,http://arxiv.org/abs/2403.13139v1,"Feedback on user interface (UI) mockups is crucial in design. However, human
feedback is not always readily available. We explore the potential of using
large language models for automatic feedback. Specifically, we focus on
applying GPT-4 to automate heuristic evaluation, which currently entails a
human expert assessing a UI's compliance with a set of design guidelines. We
implemented a Figma plugin that takes in a UI design and a set of written
heuristics, and renders automatically-generated feedback as constructive
suggestions. We assessed performance on 51 UIs using three sets of guidelines,
compared GPT-4-generated design suggestions with those from human experts, and
conducted a study with 12 expert designers to understand fit with existing
practice. We found that GPT-4-based feedback is useful for catching subtle
errors, improving text, and considering UI semantics, but feedback also
decreased in utility over iterations. Participants described several uses for
this plugin despite its imperfect suggestions.","Duan, Peitong and Warner, Jeremy and Li, Yang and Hartmann, Bjoern",2024,,,,
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,http://arxiv.org/abs/2510.08783v1,duan2024uicrit,\cite{duan2024uicrit},UICrit: Enhancing Automated Design Evaluation with a UICritique Dataset,http://arxiv.org/abs/2407.08850v3,"Automated UI evaluation can be beneficial for the design process; for
example, to compare different UI designs, or conduct automated heuristic
evaluation. LLM-based UI evaluation, in particular, holds the promise of
generalizability to a wide variety of UI types and evaluation tasks. However,
current LLM-based techniques do not yet match the performance of human
evaluators. We hypothesize that automatic evaluation can be improved by
collecting a targeted UI feedback dataset and then using this dataset to
enhance the performance of general-purpose LLMs. We present a targeted dataset
of 3,059 design critiques and quality ratings for 983 mobile UIs, collected
from seven experienced designers. We carried out an in-depth analysis to
characterize the dataset's features. We then applied this dataset to achieve a
55% performance gain in LLM-generated UI feedback via various few-shot and
visual prompting techniques. We also discuss future applications of this
dataset, including training a reward model for generative UI techniques, and
fine-tuning a tool-agnostic multi-modal LLM that automates UI evaluation.","Duan, Peitong and Cheng, Chin-Yi and Li, Gang and Hartmann, Bjoern and Li, Yang",2024,,,,
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,http://arxiv.org/abs/2510.08783v1,wu2024uiclip,\cite{wu2024uiclip},UIClip: A Data-driven Model for Assessing User Interface Design,http://arxiv.org/abs/2404.12500v1,"User interface (UI) design is a difficult yet important task for ensuring the
usability, accessibility, and aesthetic qualities of applications. In our
paper, we develop a machine-learned model, UIClip, for assessing the design
quality and visual relevance of a UI given its screenshot and natural language
description. To train UIClip, we used a combination of automated crawling,
synthetic augmentation, and human ratings to construct a large-scale dataset of
UIs, collated by description and ranked by design quality. Through training on
the dataset, UIClip implicitly learns properties of good and bad designs by i)
assigning a numerical score that represents a UI design's relevance and quality
and ii) providing design suggestions. In an evaluation that compared the
outputs of UIClip and other baselines to UIs rated by 12 human designers, we
found that UIClip achieved the highest agreement with ground-truth rankings.
Finally, we present three example applications that demonstrate how UIClip can
facilitate downstream applications that rely on instantaneous assessment of UI
design quality: i) UI code generation, ii) UI design tips generation, and iii)
quality-aware UI example search.","Wu, Jason and Peng, Yi-Hao and Li, Xin Yue Amanda and Swearngin, Amanda and Bigham, Jeffrey P and Nichols, Jeffrey",2024,,,,
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,http://arxiv.org/abs/2510.08783v1,Schoop_2022,\cite{Schoop_2022},"Predicting and Explaining Mobile UI Tappability with Vision Modeling and
  Saliency Analysis",http://arxiv.org/abs/2204.02448v1,"We use a deep learning based approach to predict whether a selected element
in a mobile UI screenshot will be perceived by users as tappable, based on
pixels only instead of view hierarchies required by previous work. To help
designers better understand model predictions and to provide more actionable
design feedback than predictions alone, we additionally use ML interpretability
techniques to help explain the output of our model. We use XRAI to highlight
areas in the input screenshot that most strongly influence the tappability
prediction for the selected region, and use k-Nearest Neighbors to present the
most similar mobile UIs from the dataset with opposing influences on
tappability perception.","Schoop, Eldon and Zhou, Xin and Li, Gang and Chen, Zhourong and Hartmann, Bjoern and Li, Yang",2022,,http://dx.doi.org/10.1145/3491102.3517497,10.1145/3491102.3517497,
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,http://arxiv.org/abs/2510.08783v1,10.1145/3613904.3642481,\cite{10.1145/3613904.3642481},SimUser: Generating Usability Feedback by Simulating Various Users Interacting with Mobile Applications,,,"Xiang, Wei and Zhu, Hanfei and Lou, Suqi and Chen, Xinli and Pan, Zhenghua and Jin, Yuping and Chen, Shi and Sun, Lingyun",2024,,https://doi.org/10.1145/3613904.3642481,10.1145/3613904.3642481,
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,http://arxiv.org/abs/2510.08783v1,wang2025agenta,\cite{wang2025agenta},"AgentA/B: Automated and Scalable Web A/BTesting with Interactive LLM
  Agents",http://arxiv.org/abs/2504.09723v3,"A/B testing experiment is a widely adopted method for evaluating UI/UX design
decisions in modern web applications. Yet, traditional A/B testing remains
constrained by its dependence on the large-scale and live traffic of human
participants, and the long time of waiting for the testing result. Through
formative interviews with six experienced industry practitioners, we identified
critical bottlenecks in current A/B testing workflows. In response, we present
AgentA/B, a novel system that leverages Large Language Model-based autonomous
agents (LLM Agents) to automatically simulate user interaction behaviors with
real webpages. AgentA/B enables scalable deployment of LLM agents with diverse
personas, each capable of navigating the dynamic webpage and interactively
executing multi-step interactions like search, clicking, filtering, and
purchasing. In a demonstrative controlled experiment, we employ AgentA/B to
simulate a between-subject A/B testing with 1,000 LLM agents Amazon.com, and
compare agent behaviors with real human shopping behaviors at a scale. Our
findings suggest AgentA/B can emulate human-like behavior patterns.","Wang, Dakuo et al.",2025,,,,arXiv preprint arXiv:2504.09723
