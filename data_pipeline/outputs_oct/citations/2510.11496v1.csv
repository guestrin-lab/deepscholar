parent_paper_title,parent_arxiv_link,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal
AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model,http://arxiv.org/abs/2510.11496v1,bai2023qwenvl,\cite{bai2023qwenvl},Qwen-vl: A frontier large vision-language model with versatile abilities,,,"Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren",2023,,,,arXiv preprint arXiv:2308.12966
AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model,http://arxiv.org/abs/2510.11496v1,wang2024qwen2vl,\cite{wang2024qwen2vl},Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution,,,"Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others",2024,,,,arXiv preprint arXiv:2409.12191
AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model,http://arxiv.org/abs/2510.11496v1,bai2025qwen2,\cite{bai2025qwen2},Qwen2 Technical Report,http://arxiv.org/abs/2407.10671v4,"This report introduces the Qwen2 series, the latest addition to our large
language models and large multimodal models. We release a comprehensive suite
of foundational and instruction-tuned language models, encompassing a parameter
range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts
model. Qwen2 surpasses most prior open-weight models, including its predecessor
Qwen1.5, and exhibits competitive performance relative to proprietary models
across diverse benchmarks on language understanding, generation, multilingual
proficiency, coding, mathematics, and reasoning.
  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on
MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base
language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1
on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2
demonstrates robust multilingual capabilities, proficient in approximately 30
languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,
Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and
global reach.
  To foster community innovation and accessibility, we have made the Qwen2
model weights openly available on Hugging Face and ModelScope, and the
supplementary materials including example code on GitHub. These platforms also
include resources for quantization, fine-tuning, and deployment, facilitating a
wide range of applications and research endeavors.","Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and others",2025,,,,arXiv preprint arXiv:2502.13923
AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model,http://arxiv.org/abs/2510.11496v1,chen2023internvl,\cite{chen2023internvl},"InternVL: Scaling up Vision Foundation Models and Aligning for Generic
  Visual-Linguistic Tasks",http://arxiv.org/abs/2312.14238v3,"The exponential growth of large language models (LLMs) has opened up numerous
possibilities for multimodal AGI systems. However, the progress in vision and
vision-language foundation models, which are also critical elements of
multi-modal AGI, has not kept pace with LLMs. In this work, we design a
large-scale vision-language foundation model (InternVL), which scales up the
vision foundation model to 6 billion parameters and progressively aligns it
with the LLM, using web-scale image-text data from various sources. This model
can be broadly applied to and achieve state-of-the-art performance on 32
generic visual-linguistic benchmarks including visual perception tasks such as
image-level or pixel-level recognition, vision-language tasks such as zero-shot
image/video classification, zero-shot image/video-text retrieval, and link with
LLMs to create multi-modal dialogue systems. It has powerful visual
capabilities and can be a good alternative to the ViT-22B. We hope that our
research could contribute to the development of multi-modal large models. Code
and models are available at https://github.com/OpenGVLab/InternVL.","Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others",2024,,,,
AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model,http://arxiv.org/abs/2510.11496v1,chen2024internvl_1_5,\cite{chen2024internvl_1_5},"How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal
  Models with Open-Source Suites",http://arxiv.org/abs/2404.16821v2,"In this report, we introduce InternVL 1.5, an open-source multimodal large
language model (MLLM) to bridge the capability gap between open-source and
proprietary commercial models in multimodal understanding. We introduce three
simple improvements: (1) Strong Vision Encoder: we explored a continuous
learning strategy for the large-scale vision foundation model -- InternViT-6B,
boosting its visual understanding capabilities, and making it can be
transferred and reused in different LLMs. (2) Dynamic High-Resolution: we
divide images into tiles ranging from 1 to 40 of 448$\times$448 pixels
according to the aspect ratio and resolution of the input images, which
supports up to 4K resolution input. (3) High-Quality Bilingual Dataset: we
carefully collected a high-quality bilingual dataset that covers common scenes,
document images, and annotated them with English and Chinese question-answer
pairs, significantly enhancing performance in OCR- and Chinese-related tasks.
We evaluate InternVL 1.5 through a series of benchmarks and comparative
studies. Compared to both open-source and proprietary models, InternVL 1.5
shows competitive performance, achieving state-of-the-art results in 8 of 18
benchmarks. Code has been released at https://github.com/OpenGVLab/InternVL.","Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others",2024,,,,arXiv preprint arXiv:2404.16821
AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model,http://arxiv.org/abs/2510.11496v1,chen2024expanding,\cite{chen2024expanding},"Expanding Performance Boundaries of Open-Source Multimodal Models with
  Model, Data, and Test-Time Scaling",http://arxiv.org/abs/2412.05271v5,"We introduce InternVL 2.5, an advanced multimodal large language model (MLLM)
series that builds upon InternVL 2.0, maintaining its core model architecture
while introducing significant enhancements in training and testing strategies
as well as data quality. In this work, we delve into the relationship between
model scaling and performance, systematically exploring the performance trends
in vision encoders, language models, dataset sizes, and test-time
configurations. Through extensive evaluations on a wide range of benchmarks,
including multi-discipline reasoning, document understanding, multi-image /
video understanding, real-world comprehension, multimodal hallucination
detection, visual grounding, multilingual capabilities, and pure language
processing, InternVL 2.5 exhibits competitive performance, rivaling leading
commercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model is
the first open-source MLLMs to surpass 70% on the MMMU benchmark, achieving a
3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasing
strong potential for test-time scaling. We hope this model contributes to the
open-source community by setting new standards for developing and applying
multimodal AI systems. HuggingFace demo see
https://huggingface.co/spaces/OpenGVLab/InternVL","Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others",2024,,,,arXiv preprint arXiv:2412.05271
AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model,http://arxiv.org/abs/2510.11496v1,zhu2025internvl3,\cite{zhu2025internvl3},"InternVL3: Exploring Advanced Training and Test-Time Recipes for
  Open-Source Multimodal Models",http://arxiv.org/abs/2504.10479v3,"We introduce InternVL3, a significant advancement in the InternVL series
featuring a native multimodal pre-training paradigm. Rather than adapting a
text-only large language model (LLM) into a multimodal large language model
(MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and
linguistic capabilities from both diverse multimodal data and pure-text corpora
during a single pre-training stage. This unified training paradigm effectively
addresses the complexities and alignment challenges commonly encountered in
conventional post-hoc training pipelines for MLLMs. To further improve
performance and scalability, InternVL3 incorporates variable visual position
encoding (V2PE) to support extended multimodal contexts, employs advanced
post-training techniques such as supervised fine-tuning (SFT) and mixed
preference optimization (MPO), and adopts test-time scaling strategies
alongside an optimized training infrastructure. Extensive empirical evaluations
demonstrate that InternVL3 delivers superior performance across a wide range of
multi-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the
MMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its
capabilities remain highly competitive with leading proprietary models,
including ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also
maintaining strong pure-language proficiency. In pursuit of open-science
principles, we will publicly release both the training data and model weights
to foster further research and development in next-generation MLLMs.",Jinguo Zhu and Weiyun Wang and Zhe Chen and Zhaoyang Liu and Shenglong Ye and Lixin Gu and Hao Tian and Yuchen Duan and Weijie Su and Jie Shao and Zhangwei Gao and Erfei Cui and Xuehui Wang and Yue Cao and Yangzhou Liu and Xingguang Wei and Hongjie Zhang and Haomin Wang and Weiye Xu and Hao Li and Jiahao Wang and Nianchen Deng and Songze Li and Yinan He and Tan Jiang and Jiapeng Luo and Yi Wang and Conghui He and Botian Shi and Xingcheng Zhang and Wenqi Shao and Junjun He and Yingtong Xiong and Wenwen Qu and Peng Sun and Penglong Jiao and Han Lv and Lijun Wu and Kaipeng Zhang and Huipeng Deng and Jiaye Ge and Kai Chen and Limin Wang and Min Dou and Lewei Lu and Xizhou Zhu and Tong Lu and Dahua Lin and Yu Qiao and Jifeng Dai and Wenhai Wang,2025,,https://arxiv.org/abs/2504.10479,,
AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model,http://arxiv.org/abs/2510.11496v1,wang2025internvl3,\cite{wang2025internvl3},"Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency",,,"Wang, Weiyun and Gao, Zhangwei and Gu, Lixin and Pu, Hengjun and Cui, Long and Wei, Xingguang and Liu, Zhaoyang and Jing, Linglin and Ye, Shenglong and Shao, Jie and others",2025,,,,arXiv preprint arXiv:2508.18265
AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model,http://arxiv.org/abs/2510.11496v1,chatgpt4o,\cite{chatgpt4o},GPT-4o System Card,http://arxiv.org/abs/2410.21276v1,"GPT-4o is an autoregressive omni model that accepts as input any combination
of text, audio, image, and video, and generates any combination of text, audio,
and image outputs. It's trained end-to-end across text, vision, and audio,
meaning all inputs and outputs are processed by the same neural network. GPT-4o
can respond to audio inputs in as little as 232 milliseconds, with an average
of 320 milliseconds, which is similar to human response time in conversation.
It matches GPT-4 Turbo performance on text in English and code, with
significant improvement on text in non-English languages, while also being much
faster and 50\% cheaper in the API. GPT-4o is especially better at vision and
audio understanding compared to existing models. In line with our commitment to
building AI safely and consistent with our voluntary commitments to the White
House, we are sharing the GPT-4o System Card, which includes our Preparedness
Framework evaluations. In this System Card, we provide a detailed look at
GPT-4o's capabilities, limitations, and safety evaluations across multiple
categories, focusing on speech-to-speech while also evaluating text and image
capabilities, and measures we've implemented to ensure the model is safe and
aligned. We also include third-party assessments on dangerous capabilities, as
well as discussion of potential societal impacts of GPT-4o's text and vision
capabilities.",OpenAI,2025,,,,
AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model,http://arxiv.org/abs/2510.11496v1,team2023gemini,\cite{team2023gemini},Gemini: A Family of Highly Capable Multimodal Models,http://arxiv.org/abs/2312.11805v5,"This report introduces a new family of multimodal models, Gemini, that
exhibit remarkable capabilities across image, audio, video, and text
understanding. The Gemini family consists of Ultra, Pro, and Nano sizes,
suitable for applications ranging from complex reasoning tasks to on-device
memory-constrained use-cases. Evaluation on a broad range of benchmarks shows
that our most-capable Gemini Ultra model advances the state of the art in 30 of
32 of these benchmarks - notably being the first model to achieve human-expert
performance on the well-studied exam benchmark MMLU, and improving the state of
the art in every one of the 20 multimodal benchmarks we examined. We believe
that the new capabilities of the Gemini family in cross-modal reasoning and
language understanding will enable a wide variety of use cases. We discuss our
approach toward post-training and deploying Gemini models responsibly to users
through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud
Vertex AI.","Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others",2023,,,,arXiv preprint arXiv:2312.11805
AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model,http://arxiv.org/abs/2510.11496v1,reid2024gemini1_5,\cite{reid2024gemini1_5},"Gemini 1.5: Unlocking multimodal understanding across millions of tokens
  of context",http://arxiv.org/abs/2403.05530v5,"In this report, we introduce the Gemini 1.5 family of models, representing
the next generation of highly compute-efficient multimodal models capable of
recalling and reasoning over fine-grained information from millions of tokens
of context, including multiple long documents and hours of video and audio. The
family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds
the February version on the great majority of capabilities and benchmarks; (2)
Gemini 1.5 Flash, a more lightweight variant designed for efficiency with
minimal regression in quality. Gemini 1.5 models achieve near-perfect recall on
long-context retrieval tasks across modalities, improve the state-of-the-art in
long-document QA, long-video QA and long-context ASR, and match or surpass
Gemini 1.0 Ultra's state-of-the-art performance across a broad set of
benchmarks. Studying the limits of Gemini 1.5's long-context ability, we find
continued improvement in next-token prediction and near-perfect retrieval
(>99%) up to at least 10M tokens, a generational leap over existing models such
as Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world
use cases, such as Gemini 1.5 collaborating with professionals on completing
their tasks achieving 26 to 75% time savings across 10 different job
categories, as well as surprising new capabilities of large language models at
the frontier; when given a grammar manual for Kalamang, a language with fewer
than 200 speakers worldwide, the model learns to translate English to Kalamang
at a similar level to a person who learned from the same content.","Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and others",2024,,,,arXiv preprint arXiv:2403.05530
AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model,http://arxiv.org/abs/2510.11496v1,gemini2_0,\cite{gemini2_0},Introducing gemini 2.0: our new ai model for the agentic era,,,Google Deepmind,2024,,,,
AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model,http://arxiv.org/abs/2510.11496v1,gemini2_0pro,\cite{gemini2_0pro},Gemini 2.0 is now available to everyone,,,Google Deepmind,202,,,,
AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model,http://arxiv.org/abs/2510.11496v1,geminipro2.5,\cite{geminipro2.5},Gemini 2.5 Pro,,,v DeepMind,2025,,,,
AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model,http://arxiv.org/abs/2510.11496v1,claude3series2024,\cite{claude3series2024},"The Claude 3 Model Family: Opus, Sonnet, Haiku",,,{Anthropic},2024,,https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model\_Card\_Claude\_3.pdf,,
AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model,http://arxiv.org/abs/2510.11496v1,chu2023mobilevlm,\cite{chu2023mobilevlm},"MobileVLM : A Fast, Strong and Open Vision Language Assistant for Mobile
  Devices",http://arxiv.org/abs/2312.16886v2,"We present MobileVLM, a competent multimodal vision language model (MMVLM)
targeted to run on mobile devices. It is an amalgamation of a myriad of
architectural designs and techniques that are mobile-oriented, which comprises
a set of language models at the scale of 1.4B and 2.7B parameters, trained from
scratch, a multimodal vision model that is pre-trained in the CLIP fashion,
cross-modality interaction via an efficient projector. We evaluate MobileVLM on
several typical VLM benchmarks. Our models demonstrate on par performance
compared with a few much larger models. More importantly, we measure the
inference speed on both a Qualcomm Snapdragon 888 CPU and an NVIDIA Jeston Orin
GPU, and we obtain state-of-the-art performance of 21.5 tokens and 65.3 tokens
per second, respectively. Our code will be made available at:
https://github.com/Meituan-AutoML/MobileVLM.","Chu, Xiangxiang and Qiao, Limeng and Lin, Xinyang and Xu, Shuang and Yang, Yang and Hu, Yiming and Wei, Fei and Zhang, Xinyu and Zhang, Bo and Wei, Xiaolin and others",2023,,,,arXiv preprint arXiv:2312.16886
AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model,http://arxiv.org/abs/2510.11496v1,liu2023llava,\cite{liu2023llava},Visual Instruction Tuning,http://arxiv.org/abs/2304.08485v2,"Instruction tuning large language models (LLMs) using machine-generated
instruction-following data has improved zero-shot capabilities on new tasks,
but the idea is less explored in the multimodal field. In this paper, we
present the first attempt to use language-only GPT-4 to generate multimodal
language-image instruction-following data. By instruction tuning on such
generated data, we introduce LLaVA: Large Language and Vision Assistant, an
end-to-end trained large multimodal model that connects a vision encoder and
LLM for general-purpose visual and language understanding.Our early experiments
show that LLaVA demonstrates impressive multimodel chat abilities, sometimes
exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and
yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal
instruction-following dataset. When fine-tuned on Science QA, the synergy of
LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make
GPT-4 generated visual instruction tuning data, our model and code base
publicly available.","Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",2023,,,,
AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model,http://arxiv.org/abs/2510.11496v1,chu2024mobilevlm,\cite{chu2024mobilevlm},MobileVLM V2: Faster and Stronger Baseline for Vision Language Model,http://arxiv.org/abs/2402.03766v1,"We introduce MobileVLM V2, a family of significantly improved vision language
models upon MobileVLM, which proves that a delicate orchestration of novel
architectural design, an improved training scheme tailored for mobile VLMs, and
rich high-quality dataset curation can substantially benefit VLMs' performance.
Specifically, MobileVLM V2 1.7B achieves better or on-par performance on
standard VLM benchmarks compared with much larger VLMs at the 3B scale.
Notably, our 3B model outperforms a large variety of VLMs at the 7B+ scale. Our
models will be released at https://github.com/Meituan-AutoML/MobileVLM .","Chu, Xiangxiang and Qiao, Limeng and Zhang, Xinyu and Xu, Shuang and Wei, Fei and Yang, Yang and Sun, Xiaofei and Hu, Yiming and Lin, Xinyang and Zhang, Bo and others",2024,,,,arXiv preprint arXiv:2402.03766
AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model,http://arxiv.org/abs/2510.11496v1,mckinzie2024mm1,\cite{mckinzie2024mm1},"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training",http://arxiv.org/abs/2403.09611v4,"In this work, we discuss building performant Multimodal Large Language Models
(MLLMs). In particular, we study the importance of various architecture
components and data choices. Through careful and comprehensive ablations of the
image encoder, the vision language connector, and various pre-training data
choices, we identified several crucial design lessons. For example, we
demonstrate that for large-scale multimodal pre-training using a careful mix of
image-caption, interleaved image-text, and text-only data is crucial for
achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,
compared to other published pre-training results. Further, we show that the
image encoder together with image resolution and the image token count has
substantial impact, while the vision-language connector design is of
comparatively negligible importance. By scaling up the presented recipe, we
build MM1, a family of multimodal models up to 30B parameters, including both
dense models and mixture-of-experts (MoE) variants, that are SOTA in
pre-training metrics and achieve competitive performance after supervised
fine-tuning on a range of established multimodal benchmarks. Thanks to
large-scale pre-training, MM1 enjoys appealing properties such as enhanced
in-context learning, and multi-image reasoning, enabling few-shot
chain-of-thought prompting.","McKinzie, Brandon and Gan, Zhe and Fauconnier, Jean-Philippe and Dodge, Sam and Zhang, Bowen and Dufter, Philipp and Shah, Dhruti and Du, Xianzhi and Peng, Futang and Weers, Floris and others",2024,,,,arXiv preprint arXiv:2403.09611
AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model,http://arxiv.org/abs/2510.11496v1,zhang2024mm1_5,\cite{zhang2024mm1_5},"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training",http://arxiv.org/abs/2403.09611v4,"In this work, we discuss building performant Multimodal Large Language Models
(MLLMs). In particular, we study the importance of various architecture
components and data choices. Through careful and comprehensive ablations of the
image encoder, the vision language connector, and various pre-training data
choices, we identified several crucial design lessons. For example, we
demonstrate that for large-scale multimodal pre-training using a careful mix of
image-caption, interleaved image-text, and text-only data is crucial for
achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,
compared to other published pre-training results. Further, we show that the
image encoder together with image resolution and the image token count has
substantial impact, while the vision-language connector design is of
comparatively negligible importance. By scaling up the presented recipe, we
build MM1, a family of multimodal models up to 30B parameters, including both
dense models and mixture-of-experts (MoE) variants, that are SOTA in
pre-training metrics and achieve competitive performance after supervised
fine-tuning on a range of established multimodal benchmarks. Thanks to
large-scale pre-training, MM1 enjoys appealing properties such as enhanced
in-context learning, and multi-image reasoning, enabling few-shot
chain-of-thought prompting.","Zhang, Haotian and Gao, Mingfei and Gan, Zhe and Dufter, Philipp and Wenzel, Nina and Huang, Forrest and Shah, Dhruti and Du, Xianzhi and Zhang, Bowen and Li, Yanghao and others",2024,,,,arXiv preprint arXiv:2409.20566
AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model,http://arxiv.org/abs/2510.11496v1,you2024ferret,\cite{you2024ferret},Ferret-ui: Grounded mobile ui understanding with multimodal llms,,,"You, Keen and Zhang, Haotian and Schoop, Eldon and Weers, Floris and Swearngin, Amanda and Nichols, Jeffrey and Yang, Yinfei and Gan, Zhe",2024,,,,
AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model,http://arxiv.org/abs/2510.11496v1,li2024ferret,\cite{li2024ferret},"Ferret-UI 2: Mastering Universal User Interface Understanding Across
  Platforms",http://arxiv.org/abs/2410.18967v2,"Building a generalist model for user interface (UI) understanding is
challenging due to various foundational issues, such as platform diversity,
resolution variation, and data limitation. In this paper, we introduce
Ferret-UI 2, a multimodal large language model (MLLM) designed for universal UI
understanding across a wide range of platforms, including iPhone, Android,
iPad, Webpage, and AppleTV. Building on the foundation of Ferret-UI, Ferret-UI
2 introduces three key innovations: support for multiple platform types,
high-resolution perception through adaptive scaling, and advanced task training
data generation powered by GPT-4o with set-of-mark visual prompting. These
advancements enable Ferret-UI 2 to perform complex, user-centered interactions,
making it highly versatile and adaptable for the expanding diversity of
platform ecosystems. Extensive empirical experiments on referring, grounding,
user-centric advanced tasks (comprising 9 subtasks $\times$ 5 platforms), GUIDE
next-action prediction dataset, and GUI-World multi-platform benchmark
demonstrate that Ferret-UI 2 significantly outperforms Ferret-UI, and also
shows strong cross-platform transfer capabilities.","Li, Zhangheng and You, Keen and Zhang, Haotian and Feng, Di and Agrawal, Harsh and Li, Xiujun and Moorthy, Mohana Prasad Sathya and Nichols, Jeff and Yang, Yinfei and Gan, Zhe",2024,,,,arXiv preprint arXiv:2410.18967
AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model,http://arxiv.org/abs/2510.11496v1,wu2024mobilevlm,\cite{wu2024mobilevlm},Mobilevlm: A vision-language model for better intra-and inter-ui understanding,,,"Wu, Qinzhuo and Xu, Weikai and Liu, Wei and Tan, Tao and Liu, Jianfeng and Li, Ang and Luan, Jian and Wang, Bin and Shang, Shuo",2024,,,,arXiv preprint arXiv:2409.14818
AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model,http://arxiv.org/abs/2510.11496v1,lu2024bluelm,\cite{lu2024bluelm},BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices,,,"Lu, Xudong and Chen, Yinghao and Chen, Cheng and Tan, Hui and Chen, Boheng and Xie, Yina and Hu, Rui and Tan, Guanxin and Wu, Renshou and Hu, Yan and others",2024,,,,arXiv preprint arXiv:2411.10640
AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model,http://arxiv.org/abs/2510.11496v1,xiong2025bluelm,\cite{xiong2025bluelm},BlueLM-2.5-3B Technical Report,,,"Xiong, Baojiao and Chen, Boheng and Wang, Chengzhi and Luo, Daxiong and Xu, Dongsheng and Liu, Dongyang and Yang, Fan and Li, Fangyuan and Teng, Fei and Wang, Feng and others",2025,,,,arXiv preprint arXiv:2507.05934
