parent_paper_title,parent_arxiv_link,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal
CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval,http://arxiv.org/abs/2510.05586v1,Lai_2024_CVPR,\cite{Lai_2024_CVPR},LISA: Reasoning Segmentation via Large Language Model,http://arxiv.org/abs/2308.00692v3,"Although perception systems have made remarkable advancements in recent
years, they still rely on explicit human instruction or pre-defined categories
to identify the target objects before executing visual recognition tasks. Such
systems cannot actively reason and comprehend implicit user intention. In this
work, we propose a new segmentation task -- reasoning segmentation. The task is
designed to output a segmentation mask given a complex and implicit query text.
Furthermore, we establish a benchmark comprising over one thousand
image-instruction-mask data samples, incorporating intricate reasoning and
world knowledge for evaluation purposes. Finally, we present LISA: large
Language Instructed Segmentation Assistant, which inherits the language
generation capabilities of multimodal Large Language Models (LLMs) while also
possessing the ability to produce segmentation masks. We expand the original
vocabulary with a <SEG> token and propose the embedding-as-mask paradigm to
unlock the segmentation capability. Remarkably, LISA can handle cases involving
complex reasoning and world knowledge. Also, it demonstrates robust zero-shot
capability when trained exclusively on reasoning-free datasets. In addition,
fine-tuning the model with merely 239 reasoning segmentation data samples
results in further performance enhancement. Both quantitative and qualitative
experiments show our method effectively unlocks new reasoning segmentation
capabilities for multimodal LLMs. Code, models, and data are available at
https://github.com/dvlab-research/LISA.","Lai, Xin and Tian, Zhuotao and Chen, Yukang and Li, Yanwei and Yuan, Yuhui and Liu, Shu and Jia, Jiaya",2024,June,,,
CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval,http://arxiv.org/abs/2510.05586v1,Chen_2024_CVPR,\cite{Chen_2024_CVPR},"How to Make Cross Encoder a Good Teacher for Efficient Image-Text
  Retrieval?",http://arxiv.org/abs/2407.07479v1,"Dominant dual-encoder models enable efficient image-text retrieval but suffer
from limited accuracy while the cross-encoder models offer higher accuracy at
the expense of efficiency. Distilling cross-modality matching knowledge from
cross-encoder to dual-encoder provides a natural approach to harness their
strengths. Thus we investigate the following valuable question: how to make
cross-encoder a good teacher for dual-encoder? Our findings are threefold:(1)
Cross-modal similarity score distribution of cross-encoder is more concentrated
while the result of dual-encoder is nearly normal making vanilla logit
distillation less effective. However ranking distillation remains practical as
it is not affected by the score distribution.(2) Only the relative order
between hard negatives conveys valid knowledge while the order information
between easy negatives has little significance.(3) Maintaining the coordination
between distillation loss and dual-encoder training loss is beneficial for
knowledge transfer. Based on these findings we propose a novel Contrastive
Partial Ranking Distillation (CPRD) method which implements the objective of
mimicking relative order between hard negative samples with contrastive
learning. This approach coordinates with the training of the dual-encoder
effectively transferring valid knowledge from the cross-encoder to the
dual-encoder. Extensive experiments on image-text retrieval and ranking tasks
show that our method surpasses other distillation methods and significantly
improves the accuracy of dual-encoder.","Chen, Yuxin and Ma, Zongyang and Zhang, Ziqi and Qi, Zhongang and Yuan, Chunfeng and Li, Bing and Pu, Junfu and Shan, Ying and Qi, Xiaojuan and Hu, Weiming",2024,June,,,
CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval,http://arxiv.org/abs/2510.05586v1,Zhai_2023_ICCV,\cite{Zhai_2023_ICCV},Sigmoid Loss for Language Image Pre-Training,http://arxiv.org/abs/2303.15343v4,"We propose a simple pairwise Sigmoid loss for Language-Image Pre-training
(SigLIP). Unlike standard contrastive learning with softmax normalization, the
sigmoid loss operates solely on image-text pairs and does not require a global
view of the pairwise similarities for normalization. The sigmoid loss
simultaneously allows further scaling up the batch size, while also performing
better at smaller batch sizes. Combined with Locked-image Tuning, with only
four TPUv4 chips, we train a SigLiT model that achieves 84.5% ImageNet
zero-shot accuracy in two days. The disentanglement of the batch size from the
loss further allows us to study the impact of examples vs pairs and negative to
positive ratio. Finally, we push the batch size to the extreme, up to one
million, and find that the benefits of growing batch size quickly diminish,
with a more reasonable batch size of 32k being sufficient. We release our
models at https://github.com/google-research/big_vision and hope our research
motivates further explorations in improving the quality and efficiency of
language-image pre-training.","Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas",2023,October,,,
CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval,http://arxiv.org/abs/2510.05586v1,Guan_2024_CVPR,\cite{Guan_2024_CVPR},"HallusionBench: An Advanced Diagnostic Suite for Entangled Language
  Hallucination and Visual Illusion in Large Vision-Language Models",http://arxiv.org/abs/2310.14566v5,"We introduce HallusionBench, a comprehensive benchmark designed for the
evaluation of image-context reasoning. This benchmark presents significant
challenges to advanced large visual-language models (LVLMs), such as
GPT-4V(Vision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, by emphasizing
nuanced understanding and interpretation of visual data. The benchmark
comprises 346 images paired with 1129 questions, all meticulously crafted by
human experts. We introduce a novel structure for these visual questions
designed to establish control groups. This structure enables us to conduct a
quantitative analysis of the models' response tendencies, logical consistency,
and various failure modes. In our evaluation on HallusionBench, we benchmarked
15 different models, highlighting a 31.42% question-pair accuracy achieved by
the state-of-the-art GPT-4V. Notably, all other evaluated models achieve
accuracy below 16%. Moreover, our analysis not only highlights the observed
failure modes, including language hallucination and visual illusion, but also
deepens an understanding of these pitfalls. Our comprehensive case studies
within HallusionBench shed light on the challenges of hallucination and
illusion in LVLMs. Based on these insights, we suggest potential pathways for
their future improvement. The benchmark and codebase can be accessed at
https://github.com/tianyi-lab/HallusionBench.","Guan, Tianrui and Liu, Fuxiao and Wu, Xiyang and Xian, Ruiqi and Li, Zongxia and Liu, Xiaoyu and Wang, Xijun and Chen, Lichang and Huang, Furong and Yacoob, Yaser and Manocha, Dinesh and Zhou, Tianyi",2024,June,,,
CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval,http://arxiv.org/abs/2510.05586v1,NEURIPS2024_a0303731,\cite{NEURIPS2024_a0303731},What matters when building vision-language models?,http://arxiv.org/abs/2405.02246v1,"The growing interest in vision-language models (VLMs) has been driven by
improvements in large language models and vision transformers. Despite the
abundance of literature on this subject, we observe that critical decisions
regarding the design of VLMs are often not justified. We argue that these
unsupported decisions impede progress in the field by making it difficult to
identify which choices improve model performance. To address this issue, we
conduct extensive experiments around pre-trained models, architecture choice,
data, and training methods. Our consolidation of findings includes the
development of Idefics2, an efficient foundational VLM of 8 billion parameters.
Idefics2 achieves state-of-the-art performance within its size category across
various multimodal benchmarks, and is often on par with models four times its
size. We release the model (base, instructed, and chat) along with the datasets
created for its training.","Lauren\c{c}on, Hugo and Tronchon, L\'{e}o and Cord, Matthieu and Sanh, Victor",2024,,,,
CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval,http://arxiv.org/abs/2510.05586v1,Zhang_2024_CVPR,\cite{Zhang_2024_CVPR},SSR-Encoder: Encoding Selective Subject Representation for Subject-Driven Generation,,,"Zhang, Yuxuan and Song, Yiren and Liu, Jiaming and Wang, Rui and Yu, Jinpeng and Tang, Hao and Li, Huaxia and Tang, Xu and Hu, Yao and Pan, Han and Jing, Zhongliang",2024,June,,,
CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval,http://arxiv.org/abs/2510.05586v1,Shao-eccv-2024,\cite{Shao-eccv-2024},Explore thePotential of CLIP for Training-Free Open Vocabulary Semantic Segmentation,,,"Shao, Tong
    and Tian, Zhuotao
    and Zhao, Hang
    and Su, Jingyong",2025,,,,
CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval,http://arxiv.org/abs/2510.05586v1,Hamilton_2024_CVPR,\cite{Hamilton_2024_CVPR},"Separating the ""Chirp"" from the ""Chat"": Self-supervised Visual Grounding
  of Sound and Language",http://arxiv.org/abs/2406.05629v1,"We present DenseAV, a novel dual encoder grounding architecture that learns
high-resolution, semantically meaningful, and audio-visually aligned features
solely through watching videos. We show that DenseAV can discover the
``meaning'' of words and the ``location'' of sounds without explicit
localization supervision. Furthermore, it automatically discovers and
distinguishes between these two types of associations without supervision. We
show that DenseAV's localization abilities arise from a new multi-head feature
aggregation operator that directly compares dense image and audio
representations for contrastive learning. In contrast, many other systems that
learn ``global'' audio and video representations cannot localize words and
sound. Finally, we contribute two new datasets to improve the evaluation of AV
representations through speech and sound prompted semantic segmentation. On
these and other datasets we show DenseAV dramatically outperforms the prior art
on speech and sound prompted semantic segmentation. DenseAV outperforms the
previous state-of-the-art, ImageBind, on cross-modal retrieval using fewer than
half of the parameters. Project Page:
\href{https://aka.ms/denseav}{https://aka.ms/denseav}","Hamilton, Mark and Zisserman, Andrew and Hershey, John R. and Freeman, William T.",2024,June,,,
CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval,http://arxiv.org/abs/2510.05586v1,longlora,\cite{longlora},LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models,http://arxiv.org/abs/2309.12307v3,"We present LongLoRA, an efficient fine-tuning approach that extends the
context sizes of pre-trained large language models (LLMs), with limited
computation cost. Typically, training LLMs with long context sizes is
computationally expensive, requiring extensive training hours and GPU
resources. For example, training on the context length of 8192 needs 16x
computational costs in self-attention layers as that of 2048. In this paper, we
speed up the context extension of LLMs in two aspects. On the one hand,
although dense global attention is needed during inference, fine-tuning the
model can be effectively and efficiently done by sparse local attention. The
proposed shifted sparse attention effectively enables context extension,
leading to non-trivial computation saving with similar performance to
fine-tuning with vanilla attention. Particularly, it can be implemented with
only two lines of code in training, while being optional in inference. On the
other hand, we revisit the parameter-efficient fine-tuning regime for context
expansion. Notably, we find that LoRA for context extension works well under
the premise of trainable embedding and normalization. LongLoRA combines this
improved LoRA with S^2-Attn. LongLoRA demonstrates strong empirical results on
various tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends Llama2 7B
from 4k context to 100k, or Llama2 70B to 32k on a single 8x A100 machine.
LongLoRA extends models' context while retaining their original architectures,
and is compatible with most existing techniques, like Flash-Attention2. In
addition, we further conduct supervised fine-tuning with LongLoRA and our long
instruction-following LongAlpaca dataset.",Yukang Chen and Shengju Qian and Haotian Tang and Xin Lai and Zhijian Liu and Song Han and Jiaya Jia,2024,,,,
CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval,http://arxiv.org/abs/2510.05586v1,bai2023qwentechnicalreport,\cite{bai2023qwentechnicalreport},Qwen Technical Report,http://arxiv.org/abs/2309.16609v1,"Large language models (LLMs) have revolutionized the field of artificial
intelligence, enabling natural language processing tasks that were previously
thought to be exclusive to humans. In this work, we introduce Qwen, the first
installment of our large language model series. Qwen is a comprehensive
language model series that encompasses distinct models with varying parameter
counts. It includes Qwen, the base pretrained language models, and Qwen-Chat,
the chat models finetuned with human alignment techniques. The base language
models consistently demonstrate superior performance across a multitude of
downstream tasks, and the chat models, particularly those trained using
Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The
chat models possess advanced tool-use and planning capabilities for creating
agent applications, showcasing impressive performance even when compared to
bigger models on complex tasks like utilizing a code interpreter. Furthermore,
we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as
well as mathematics-focused models, Math-Qwen-Chat, which are built upon base
language models. These models demonstrate significantly improved performance in
comparison with open-source models, and slightly fall behind the proprietary
models.",Jinze Bai and Shuai Bai and Yunfei Chu et al.,2023,,,,
CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval,http://arxiv.org/abs/2510.05586v1,Xu2024Invisible,\cite{Xu2024Invisible},"Invisible Relevance Bias: Text-Image Retrieval Models Prefer
  AI-Generated Images",http://arxiv.org/abs/2311.14084v4,"With the advancement of generation models, AI-generated content (AIGC) is
becoming more realistic, flooding the Internet. A recent study suggests that
this phenomenon causes source bias in text retrieval for web search.
Specifically, neural retrieval models tend to rank generated texts higher than
human-written texts. In this paper, we extend the study of this bias to
cross-modal retrieval. Firstly, we successfully construct a suitable benchmark
to explore the existence of the bias. Subsequent extensive experiments on this
benchmark reveal that AI-generated images introduce an invisible relevance bias
to text-image retrieval models. Specifically, our experiments show that
text-image retrieval models tend to rank the AI-generated images higher than
the real images, even though the AI-generated images do not exhibit more
visually relevant features to the query than real images. This invisible
relevance bias is prevalent across retrieval models with varying training data
and architectures. Furthermore, our subsequent exploration reveals that the
inclusion of AI-generated images in the training data of the retrieval models
exacerbates the invisible relevance bias. The above phenomenon triggers a
vicious cycle, which makes the invisible relevance bias become more and more
serious. To elucidate the potential causes of invisible relevance and address
the aforementioned issues, we introduce an effective training method aimed at
alleviating the invisible relevance bias. Subsequently, we apply our proposed
debiasing method to retroactively identify the causes of invisible relevance,
revealing that the AI-generated images induce the image encoder to embed
additional information into their representation. This information exhibits a
certain consistency across generated images with different semantics and can
make the retriever estimate a higher relevance score.",Shicheng Xu and Ding Hou and Liang Pang and Yiqun Liu and Shaoping Ma,2024,,,,
CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval,http://arxiv.org/abs/2510.05586v1,Liao2024Selection,\cite{Liao2024Selection},Selection and Reconstruction of Key Locals: A Novel Specific Domain Image-Text Retrieval Method,,,Yuanze Liao and Xinyu Zhang and Rui Yang and Jie Liu and Yi Yang,2024,,,,
CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval,http://arxiv.org/abs/2510.05586v1,liu2024candidate,\cite{liu2024candidate},"Candidate Set Re-ranking for Composed Image Retrieval with Dual
  Multi-modal Encoder",http://arxiv.org/abs/2305.16304v3,"Composed image retrieval aims to find an image that best matches a given
multi-modal user query consisting of a reference image and text pair. Existing
methods commonly pre-compute image embeddings over the entire corpus and
compare these to a reference image embedding modified by the query text at test
time. Such a pipeline is very efficient at test time since fast vector
distances can be used to evaluate candidates, but modifying the reference image
embedding guided only by a short textual description can be difficult,
especially independent of potential candidates. An alternative approach is to
allow interactions between the query and every possible candidate, i.e.,
reference-text-candidate triplets, and pick the best from the entire set.
Though this approach is more discriminative, for large-scale datasets the
computational cost is prohibitive since pre-computation of candidate embeddings
is no longer possible. We propose to combine the merits of both schemes using a
two-stage model. Our first stage adopts the conventional vector distancing
metric and performs a fast pruning among candidates. Meanwhile, our second
stage employs a dual-encoder architecture, which effectively attends to the
input triplet of reference-text-candidate and re-ranks the candidates. Both
stages utilize a vision-and-language pre-trained network, which has proven
beneficial for various downstream tasks. Our method consistently outperforms
state-of-the-art approaches on standard benchmarks for the task. Our
implementation is available at
https://github.com/Cuberick-Orion/Candidate-Reranking-CIR.",Zheyuan Liu and Weixuan Sun and Damien Teney and Stephen Gould,2024,,,,Transactions on Machine Learning Research
CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval,http://arxiv.org/abs/2510.05586v1,bai_2023_sentence,\cite{bai_2023_sentence},Sentence-level Prompts Benefit Composed Image Retrieval,http://arxiv.org/abs/2310.05473v1,"Composed image retrieval (CIR) is the task of retrieving specific images by
using a query that involves both a reference image and a relative caption. Most
existing CIR models adopt the late-fusion strategy to combine visual and
language features. Besides, several approaches have also been suggested to
generate a pseudo-word token from the reference image, which is further
integrated into the relative caption for CIR. However, these pseudo-word-based
prompting methods have limitations when target image encompasses complex
changes on reference image, e.g., object removal and attribute modification. In
this work, we demonstrate that learning an appropriate sentence-level prompt
for the relative caption (SPRC) is sufficient for achieving effective composed
image retrieval. Instead of relying on pseudo-word-based prompts, we propose to
leverage pretrained V-L models, e.g., BLIP-2, to generate sentence-level
prompts. By concatenating the learned sentence-level prompt with the relative
caption, one can readily use existing text-based image retrieval models to
enhance CIR performance. Furthermore, we introduce both image-text contrastive
loss and text prompt alignment loss to enforce the learning of suitable
sentence-level prompts. Experiments show that our proposed method performs
favorably against the state-of-the-art CIR methods on the Fashion-IQ and CIRR
datasets. The source code and pretrained model are publicly available at
https://github.com/chunmeifeng/SPRC",Yang Bai and Xinxing Xu and Yong Liu and Salman Khan and Fahad Khan and Wangmeng Zuo and Rick Siow Mong Goh and Chun-Mei Feng,2023,,,,
CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval,http://arxiv.org/abs/2510.05586v1,APTM_MM_2023,\cite{APTM_MM_2023},"Towards Unified Text-based Person Retrieval: A Large-scale
  Multi-Attribute and Language Search Benchmark",http://arxiv.org/abs/2306.02898v4,"In this paper, we introduce a large Multi-Attribute and Language Search
dataset for text-based person retrieval, called MALS, and explore the
feasibility of performing pre-training on both attribute recognition and
image-text matching tasks in one stone. In particular, MALS contains 1,510,330
image-text pairs, which is about 37.5 times larger than prevailing CUHK-PEDES,
and all images are annotated with 27 attributes. Considering the privacy
concerns and annotation costs, we leverage the off-the-shelf diffusion models
to generate the dataset. To verify the feasibility of learning from the
generated data, we develop a new joint Attribute Prompt Learning and Text
Matching Learning (APTM) framework, considering the shared knowledge between
attribute and text. As the name implies, APTM contains an attribute prompt
learning stream and a text matching learning stream. (1) The attribute prompt
learning leverages the attribute prompts for image-attribute alignment, which
enhances the text matching learning. (2) The text matching learning facilitates
the representation learning on fine-grained details, and in turn, boosts the
attribute prompt learning. Extensive experiments validate the effectiveness of
the pre-training on MALS, achieving state-of-the-art retrieval performance via
APTM on three challenging real-world benchmarks. In particular, APTM achieves a
consistent improvement of +6.96%, +7.68%, and +16.95% Recall@1 accuracy on
CUHK-PEDES, ICFG-PEDES, and RSTPReid datasets by a clear margin, respectively.","Yang, Shuyu and Zhou, Yinan and Zheng, Zhedong and Wang, Yaxiong and Zhu, Li and Wu, Yujiao",2023,,,,
CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval,http://arxiv.org/abs/2510.05586v1,Feng_ACMMM_2024,\cite{Feng_ACMMM_2024},"Improving Composed Image Retrieval via Contrastive Learning with Scaling
  Positives and Negatives",http://arxiv.org/abs/2404.11317v2,"The Composed Image Retrieval (CIR) task aims to retrieve target images using
a composed query consisting of a reference image and a modified text. Advanced
methods often utilize contrastive learning as the optimization objective, which
benefits from adequate positive and negative examples. However, the triplet for
CIR incurs high manual annotation costs, resulting in limited positive
examples. Furthermore, existing methods commonly use in-batch negative
sampling, which reduces the negative number available for the model. To address
the problem of lack of positives, we propose a data generation method by
leveraging a multi-modal large language model to construct triplets for CIR. To
introduce more negatives during fine-tuning, we design a two-stage fine-tuning
framework for CIR, whose second stage introduces plenty of static
representations of negatives to optimize the representation space rapidly. The
above two improvements can be effectively stacked and designed to be
plug-and-play, easily applied to existing CIR models without changing their
original architectures. Extensive experiments and ablation analysis demonstrate
that our method effectively scales positives and negatives and achieves
state-of-the-art results on both FashionIQ and CIRR datasets. In addition, our
method also performs well in zero-shot composed image retrieval, providing a
new CIR solution for the low-resources scenario. Our code and data are released
at https://github.com/BUAADreamer/SPN4CIR.","Feng, Zhangchi and Zhang, Richong and Nie, Zhijie",2024,,,,
CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval,http://arxiv.org/abs/2510.05586v1,Wan_2024_CVPR,\cite{Wan_2024_CVPR},Cross-modal Feature Alignment and Fusion for Composed Image Retrieval,,,"Wan, Yongquan and Wang, Wenhai and Zou, Guobing and Zhang, Bofeng",2024,June,,,
CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval,http://arxiv.org/abs/2510.05586v1,Fu_2023_CVPR,\cite{Fu_2023_CVPR},Learning Semantic Relationship Among Instances for Image-Text Matching,,,"Fu, Zheren and Mao, Zhendong and Song, Yan and Zhang, Yongdong",2023,June,,,
CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval,http://arxiv.org/abs/2510.05586v1,tcsvt_2023_CTLG,\cite{tcsvt_2023_CTLG},"Contrastive Transformer Learning with Proximity Data Generation for
  Text-Based Person Search",http://arxiv.org/abs/2311.09084v1,"Given a descriptive text query, text-based person search (TBPS) aims to
retrieve the best-matched target person from an image gallery. Such a
cross-modal retrieval task is quite challenging due to significant modality
gap, fine-grained differences and insufficiency of annotated data. To better
align the two modalities, most existing works focus on introducing
sophisticated network structures and auxiliary tasks, which are complex and
hard to implement. In this paper, we propose a simple yet effective dual
Transformer model for text-based person search. By exploiting a hardness-aware
contrastive learning strategy, our model achieves state-of-the-art performance
without any special design for local feature alignment or side information.
Moreover, we propose a proximity data generation (PDG) module to automatically
produce more diverse data for cross-modal training. The PDG module first
introduces an automatic generation algorithm based on a text-to-image diffusion
model, which generates new text-image pair samples in the proximity space of
original ones. Then it combines approximate text generation and feature-level
mixup during training to further strengthen the data diversity. The PDG module
can largely guarantee the reasonability of the generated samples that are
directly used for training without any human inspection for noise rejection. It
improves the performance of our model significantly, providing a feasible
solution to the data insufficiency problem faced by such fine-grained
visual-linguistic tasks. Extensive experiments on two popular datasets of the
TBPS task (i.e., CUHK-PEDES and ICFG-PEDES) show that the proposed approach
outperforms state-of-the-art approaches evidently, e.g., improving by 3.88%,
4.02%, 2.92% in terms of Top1, Top5, Top10 on CUHK-PEDES. The codes will be
available at https://github.com/HCPLab-SYSU/PersonSearch-CTLG","Wu, Hefeng and Chen, Weifeng and Liu, Zhibin and Chen, Tianshui and Chen, Zhiguang and Lin, Liang",2024,,,,IEEE Transactions on Circuits and Systems for Video Technology
CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval,http://arxiv.org/abs/2510.05586v1,tnnls_2024_EAIB,\cite{tnnls_2024_EAIB},Improving Text-Based Person Retrieval by Excavating All-Round Information Beyond Color,,,"Zhu, Aichun and Wang, Zijie and Xue, Jingyi and Wan, Xili and Jin, Jing and Wang, Tian and Snoussi, Hichem",2025,,,,IEEE Transactions on Neural Networks and Learning Systems
CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval,http://arxiv.org/abs/2510.05586v1,Liu2024Causality,\cite{Liu2024Causality},Causality-Inspired Invariant Representation Learning for Text-Based Person Retrieval,,,Yifan Liu and Guoliang Qin and Hao Chen and Zhaoyang Zeng and Xiatian Yang,2024,,,,
CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval,http://arxiv.org/abs/2510.05586v1,Pan_2023_CVPR,\cite{Pan_2023_CVPR},Fine-Grained Image-Text Matching by Cross-Modal Hard Aligning Network,,,"Pan, Zhengxin and Wu, Fangyu and Zhang, Bailing",2023,June,,,
CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval,http://arxiv.org/abs/2510.05586v1,2024_SIGIR_ACM,\cite{2024_SIGIR_ACM},"CaLa: Complementary Association Learning for Augmenting Composed Image
  Retrieval",http://arxiv.org/abs/2405.19149v2,"Composed Image Retrieval (CIR) involves searching for target images based on
an image-text pair query. While current methods treat this as a query-target
matching problem, we argue that CIR triplets contain additional associations
beyond this primary relation. In our paper, we identify two new relations
within triplets, treating each triplet as a graph node. Firstly, we introduce
the concept of text-bridged image alignment, where the query text serves as a
bridge between the query image and the target image. We propose a hinge-based
cross-attention mechanism to incorporate this relation into network learning.
Secondly, we explore complementary text reasoning, considering CIR as a form of
cross-modal retrieval where two images compose to reason about complementary
text. To integrate these perspectives effectively, we design a twin
attention-based compositor. By combining these complementary associations with
the explicit query pair-target image relation, we establish a comprehensive set
of constraints for CIR. Our framework, CaLa (Complementary Association Learning
for Augmenting Composed Image Retrieval), leverages these insights. We evaluate
CaLa on CIRR and FashionIQ benchmarks with multiple backbones, demonstrating
its superiority in composed image retrieval.","Jiang, Xintong and Wang, Yaxiong and Li, Mengjian and Wu, Yujiao and Hu, Bingwen and Qian, Xueming",2024,,,,
