parent_paper_title,parent_arxiv_link,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal
Beyond Western Politics: Cross-Cultural Benchmarks for Evaluating Partisan Associations in LLMs,http://arxiv.org/abs/2509.22711v1,feng-etal-2023-pretraining,\cite{feng-etal-2023-pretraining},From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair {NLP} Models,,,"Feng, Shangbin  and
      Park, Chan Young  and
      Liu, Yuhan  and
      Tsvetkov, Yulia",2023,,https://aclanthology.org/2023.acl-long.656/,10.18653/v1/2023.acl-long.656,
Beyond Western Politics: Cross-Cultural Benchmarks for Evaluating Partisan Associations in LLMs,http://arxiv.org/abs/2509.22711v1,Yangetal,\cite{Yangetal},"Accuracy and Political Bias of News Source Credibility Ratings by Large
  Language Models",http://arxiv.org/abs/2304.00228v3,"Search engines increasingly leverage large language models (LLMs) to generate
direct answers, and AI chatbots now access the Internet for fresh data. As
information curators for billions of users, LLMs must assess the accuracy and
reliability of different sources. This paper audits nine widely used LLMs from
three leading providers -- OpenAI, Google, and Meta -- to evaluate their
ability to discern credible and high-quality information sources from
low-credibility ones. We find that while LLMs can rate most tested news
outlets, larger models more frequently refuse to provide ratings due to
insufficient information, whereas smaller models are more prone to making
errors in their ratings. For sources where ratings are provided, LLMs exhibit a
high level of agreement among themselves (average Spearman's $\rho = 0.79$),
but their ratings align only moderately with human expert evaluations (average
$\rho = 0.50$). Analyzing news sources with different political leanings in the
US, we observe a liberal bias in credibility ratings yielded by all LLMs in
default configurations. Additionally, assigning partisan roles to LLMs
consistently induces strong politically congruent bias in their ratings. These
findings have important implications for the use of LLMs in curating news and
political information.","Yang, Kai-Cheng and Menczer, Filippo",2025,,https://doi.org/10.1145/3717867.3717903,10.1145/3717867.3717903,
Beyond Western Politics: Cross-Cultural Benchmarks for Evaluating Partisan Associations in LLMs,http://arxiv.org/abs/2509.22711v1,Rozado2023,\cite{Rozado2023},The Political Biases of ChatGPT,,,David Rozado,2023,,https://www.mdpi.com/2076-0760/12/3/148,10.3390/socsci12030148,Social Sciences
Beyond Western Politics: Cross-Cultural Benchmarks for Evaluating Partisan Associations in LLMs,http://arxiv.org/abs/2509.22711v1,Rotaru2024,\cite{Rotaru2024},How Artificial Intelligence Can Influence Elections: Analyzing the Large Language Models (LLMs) Political Bias,,,George-Cristinel Rotaru and Sorin Anagnoste and Vasile-Marian Oancea,2024,6,,10.2478/picbe-2024-0158,Proceedings of the International Conference on Business Excellence
Beyond Western Politics: Cross-Cultural Benchmarks for Evaluating Partisan Associations in LLMs,http://arxiv.org/abs/2509.22711v1,yuksel2025languagedependentpoliticalbiasai,\cite{yuksel2025languagedependentpoliticalbiasai},Language-Dependent Political Bias in AI: A Study of ChatGPT and Gemini,http://arxiv.org/abs/2504.06436v1,"As leading examples of large language models, ChatGPT and Gemini claim to
provide accurate and unbiased information, emphasizing their commitment to
political neutrality and avoidance of personal bias. This research investigates
the political tendency of large language models and the existence of
differentiation according to the query language. For this purpose, ChatGPT and
Gemini were subjected to a political axis test using 14 different languages.
The findings of the study suggest that these large language models do exhibit
political tendencies, with both models demonstrating liberal and leftist
biases. A comparative analysis revealed that Gemini exhibited a more pronounced
liberal and left-wing tendency compared to ChatGPT. The study also found that
these political biases varied depending on the language used for inquiry. The
study delves into the factors that constitute political tendencies and
linguistic differentiation, exploring differences in the sources and scope of
educational data, structural and grammatical features of languages, cultural
and political contexts, and the model's response to linguistic features. From
this standpoint, and an ethical perspective, it is proposed that artificial
intelligence tools should refrain from asserting a lack of political tendencies
and neutrality, instead striving for political neutrality and executing user
queries by incorporating these tendencies.",Dogus Yuksel and Mehmet Cem Catalbas and Bora Oc,2025,,https://arxiv.org/abs/2504.06436,,
Beyond Western Politics: Cross-Cultural Benchmarks for Evaluating Partisan Associations in LLMs,http://arxiv.org/abs/2509.22711v1,Motoki2025,\cite{Motoki2025},Assessing political bias and value misalignment in generative artificial intelligence,,,Fabio Y S Motoki and Valdemar Pinho Neto and Victor Rangel,2025,,https://doi.org/10.7910/DVN/VZ,10.7910/DVN/VZ,
Beyond Western Politics: Cross-Cultural Benchmarks for Evaluating Partisan Associations in LLMs,http://arxiv.org/abs/2509.22711v1,Faulborn2025,\cite{Faulborn2025},"Only a Little to the Left: A Theory-grounded Measure of Political Bias
  in Large Language Models",http://arxiv.org/abs/2503.16148v2,"Prompt-based language models like GPT4 and LLaMa have been used for a wide
variety of use cases such as simulating agents, searching for information, or
for content analysis. For all of these applications and others, political
biases in these models can affect their performance. Several researchers have
attempted to study political bias in language models using evaluation suites
based on surveys, such as the Political Compass Test (PCT), often finding a
particular leaning favored by these models. However, there is some variation in
the exact prompting techniques, leading to diverging findings, and most
research relies on constrained-answer settings to extract model responses.
Moreover, the Political Compass Test is not a scientifically valid survey
instrument. In this work, we contribute a political bias measured informed by
political science theory, building on survey design principles to test a wide
variety of input prompts, while taking into account prompt sensitivity. We then
prompt 11 different open and commercial models, differentiating between
instruction-tuned and non-instruction-tuned models, and automatically classify
their political stances from 88,110 responses. Leveraging this dataset, we
compute political bias profiles across different prompt variations and find
that while PCT exaggerates bias in certain models like GPT3.5, measures of
political bias are often unstable, but generally more left-leaning for
instruction-tuned models. Code and data are available on:
https://github.com/MaFa211/theory_grounded_pol_bias",Mats Faulborn and Indira Sen and Max Pellert and Andreas Spitz and David Garcia,2025,7,http://arxiv.org/abs/2503.16148,,
Beyond Western Politics: Cross-Cultural Benchmarks for Evaluating Partisan Associations in LLMs,http://arxiv.org/abs/2509.22711v1,Peng2024,\cite{Peng2024},"Beyond Partisan Leaning: A Comparative Analysis of Political Bias in
  Large Language Models",http://arxiv.org/abs/2412.16746v4,"As large language models (LLMs) become increasingly embedded in civic,
educational, and political information environments, concerns about their
potential political bias have grown. Prior research often evaluates such bias
through simulated personas or predefined ideological typologies, which may
introduce artificial framing effects or overlook how models behave in general
use scenarios. This study adopts a persona-free, topic-specific approach to
evaluate political behavior in LLMs, reflecting how users typically interact
with these systems-without ideological role-play or conditioning. We introduce
a two-dimensional framework: one axis captures partisan orientation on highly
polarized topics (e.g., abortion, immigration), and the other assesses
sociopolitical engagement on less polarized issues (e.g., climate change,
foreign policy). Using survey-style prompts drawn from the ANES and Pew
Research Center, we analyze responses from 43 LLMs developed in the U.S.,
Europe, China, and the Middle East. We propose an entropy-weighted bias score
to quantify both the direction and consistency of partisan alignment, and
identify four behavioral clusters through engagement profiles. Findings show
most models lean center-left or left ideologically and vary in their
nonpartisan engagement patterns. Model scale and openness are not strong
predictors of behavior, suggesting that alignment strategy and institutional
context play a more decisive role in shaping political expression.",Tai-Quan Peng and Kaiqi Yang and Sanguk Lee and Hang Li and Yucheng Chu and Yuping Lin and Hui Liu,2024,,,,
Beyond Western Politics: Cross-Cultural Benchmarks for Evaluating Partisan Associations in LLMs,http://arxiv.org/abs/2509.22711v1,Fisher2025,\cite{Fisher2025},Biased LLMs can Influence Political Decision-Making,,,Jillian Fisher and Shangbin Feng and Robert Aron and Thomas Richardson and Yejin Choi and Daniel W Fisher and Jennifer Pan and Yulia Tsvetkov and Katharina Reinecke,2025,,,10.18653/v1/2025.acl-long.328,
Beyond Western Politics: Cross-Cultural Benchmarks for Evaluating Partisan Associations in LLMs,http://arxiv.org/abs/2509.22711v1,Messer2025,\cite{Messer2025},How do people react to political bias in generative artificial intelligence (AI)?,,,Uwe Messer,2025,3,,10.1016/j.chbah.2024.100108,Computers in Human Behavior: Artificial Humans
Beyond Western Politics: Cross-Cultural Benchmarks for Evaluating Partisan Associations in LLMs,http://arxiv.org/abs/2509.22711v1,GoodmanFacultyAdvisor2024,\cite{GoodmanFacultyAdvisor2024},How harmful is the political bias in ChatGPT?,,,Neomi Goodman Faculty Advisor and Professor Susanne Lohmann,2024,,,,
