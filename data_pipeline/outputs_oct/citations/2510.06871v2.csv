parent_paper_title,parent_arxiv_link,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,MLLM-flamingo,\cite{MLLM-flamingo},Flamingo: a Visual Language Model for Few-Shot Learning,http://arxiv.org/abs/2204.14198v2,"Building models that can be rapidly adapted to novel tasks using only a
handful of annotated examples is an open challenge for multimodal machine
learning research. We introduce Flamingo, a family of Visual Language Models
(VLM) with this ability. We propose key architectural innovations to: (i)
bridge powerful pretrained vision-only and language-only models, (ii) handle
sequences of arbitrarily interleaved visual and textual data, and (iii)
seamlessly ingest images or videos as inputs. Thanks to their flexibility,
Flamingo models can be trained on large-scale multimodal web corpora containing
arbitrarily interleaved text and images, which is key to endow them with
in-context few-shot learning capabilities. We perform a thorough evaluation of
our models, exploring and measuring their ability to rapidly adapt to a variety
of image and video tasks. These include open-ended tasks such as visual
question-answering, where the model is prompted with a question which it has to
answer; captioning tasks, which evaluate the ability to describe a scene or an
event; and close-ended tasks such as multiple-choice visual question-answering.
For tasks lying anywhere on this spectrum, a single Flamingo model can achieve
a new state of the art with few-shot learning, simply by prompting the model
with task-specific examples. On numerous benchmarks, Flamingo outperforms
models fine-tuned on thousands of times more task-specific data.","Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others",2022,,,,Advances in neural information processing systems
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,MLLM-gpt4o,\cite{MLLM-gpt4o},GPT-4o System Card,http://arxiv.org/abs/2410.21276v1,"GPT-4o is an autoregressive omni model that accepts as input any combination
of text, audio, image, and video, and generates any combination of text, audio,
and image outputs. It's trained end-to-end across text, vision, and audio,
meaning all inputs and outputs are processed by the same neural network. GPT-4o
can respond to audio inputs in as little as 232 milliseconds, with an average
of 320 milliseconds, which is similar to human response time in conversation.
It matches GPT-4 Turbo performance on text in English and code, with
significant improvement on text in non-English languages, while also being much
faster and 50\% cheaper in the API. GPT-4o is especially better at vision and
audio understanding compared to existing models. In line with our commitment to
building AI safely and consistent with our voluntary commitments to the White
House, we are sharing the GPT-4o System Card, which includes our Preparedness
Framework evaluations. In this System Card, we provide a detailed look at
GPT-4o's capabilities, limitations, and safety evaluations across multiple
categories, focusing on speech-to-speech while also evaluating text and image
capabilities, and measures we've implemented to ensure the model is safe and
aligned. We also include third-party assessments on dangerous capabilities, as
well as discussion of potential societal impacts of GPT-4o's text and vision
capabilities.","Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others",2024,,,,arXiv preprint arXiv:2410.21276
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,MLLM-cogvlm,\cite{MLLM-cogvlm},CogVLM: Visual Expert for Pretrained Language Models,http://arxiv.org/abs/2311.03079v2,"We introduce CogVLM, a powerful open-source visual language foundation model.
Different from the popular shallow alignment method which maps image features
into the input space of language model, CogVLM bridges the gap between the
frozen pretrained language model and image encoder by a trainable visual expert
module in the attention and FFN layers. As a result, CogVLM enables deep fusion
of vision language features without sacrificing any performance on NLP tasks.
CogVLM-17B achieves state-of-the-art performance on 10 classic cross-modal
benchmarks, including NoCaps, Flicker30k captioning, RefCOCO, RefCOCO+,
RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA and TDIUC, and ranks the 2nd on
VQAv2, OKVQA, TextVQA, COCO captioning, etc., surpassing or matching PaLI-X
55B. Codes and checkpoints are available at https://github.com/THUDM/CogVLM.","Wang, Weihan and Lv, Qingsong and Yu, Wenmeng and Hong, Wenyi and Qi, Ji and Wang, Yan and Ji, Junhui and Yang, Zhuoyi and Zhao, Lei and XiXuan, Song and others",2024,,,,Advances in Neural Information Processing Systems
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,MLLM-qwen2vl,\cite{MLLM-qwen2vl},Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution,,,"Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others",2024,,,,arXiv preprint arXiv:2409.12191
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,MLLM-qwen2.5vl,\cite{MLLM-qwen2.5vl},Qwen2 Technical Report,http://arxiv.org/abs/2407.10671v4,"This report introduces the Qwen2 series, the latest addition to our large
language models and large multimodal models. We release a comprehensive suite
of foundational and instruction-tuned language models, encompassing a parameter
range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts
model. Qwen2 surpasses most prior open-weight models, including its predecessor
Qwen1.5, and exhibits competitive performance relative to proprietary models
across diverse benchmarks on language understanding, generation, multilingual
proficiency, coding, mathematics, and reasoning.
  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on
MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base
language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1
on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2
demonstrates robust multilingual capabilities, proficient in approximately 30
languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,
Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and
global reach.
  To foster community innovation and accessibility, we have made the Qwen2
model weights openly available on Hugging Face and ModelScope, and the
supplementary materials including example code on GitHub. These platforms also
include resources for quantization, fine-tuning, and deployment, facilitating a
wide range of applications and research endeavors.","Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and others",2025,,,,arXiv preprint arXiv:2502.13923
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,MLRM-o1,\cite{MLRM-o1},OpenAI o1 System Card,http://arxiv.org/abs/2412.16720v1,"The o1 model series is trained with large-scale reinforcement learning to
reason using chain of thought. These advanced reasoning capabilities provide
new avenues for improving the safety and robustness of our models. In
particular, our models can reason about our safety policies in context when
responding to potentially unsafe prompts, through deliberative alignment. This
leads to state-of-the-art performance on certain benchmarks for risks such as
generating illicit advice, choosing stereotyped responses, and succumbing to
known jailbreaks. Training models to incorporate a chain of thought before
answering has the potential to unlock substantial benefits, while also
increasing potential risks that stem from heightened intelligence. Our results
underscore the need for building robust alignment methods, extensively
stress-testing their efficacy, and maintaining meticulous risk management
protocols. This report outlines the safety work carried out for the OpenAI o1
and OpenAI o1-mini models, including safety evaluations, external red teaming,
and Preparedness Framework evaluations.","Jaech, Aaron and Kalai, Adam and Lerer, Adam and Richardson, Adam and El-Kishky, Ahmed and Low, Aiden and Helyar, Alec and Madry, Aleksander and Beutel, Alex and Carney, Alex and others",2024,,,,arXiv preprint arXiv:2412.16720
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,MLRM-r1,\cite{MLRM-r1},DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning,,,"Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Zhang, Ruoyu and Ma, Shirong and Bi, Xiao and others",2025,,,,Nature
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,MLRM-grpo,\cite{MLRM-grpo},"DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open
  Language Models",http://arxiv.org/abs/2402.03300v3,"Mathematical reasoning poses a significant challenge for language models due
to its complex and structured nature. In this paper, we introduce DeepSeekMath
7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B
math-related tokens sourced from Common Crawl, together with natural language
and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the
competition-level MATH benchmark without relying on external toolkits and
voting techniques, approaching the performance level of Gemini-Ultra and GPT-4.
Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH.
The mathematical reasoning capability of DeepSeekMath is attributed to two key
factors: First, we harness the significant potential of publicly available web
data through a meticulously engineered data selection pipeline. Second, we
introduce Group Relative Policy Optimization (GRPO), a variant of Proximal
Policy Optimization (PPO), that enhances mathematical reasoning abilities while
concurrently optimizing the memory usage of PPO.","Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Y and others",2024,,,,arXiv preprint arXiv:2402.03300
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,MLRM-math-lmmr1,\cite{MLRM-math-lmmr1},"LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through
  Two-Stage Rule-Based RL",http://arxiv.org/abs/2503.07536v2,"Enhancing reasoning in Large Multimodal Models (LMMs) faces unique challenges
from the complex interplay between visual perception and logical reasoning,
particularly in compact 3B-parameter architectures where architectural
constraints limit reasoning capacity and modality alignment.
  While rule-based reinforcement learning (RL) excels in text-only domains, its
multimodal extension confronts two critical barriers: (1) data limitations due
to ambiguous answers and scarce complex reasoning examples, and (2) degraded
foundational reasoning induced by multimodal pretraining. To address these
challenges, we propose \textbf{LMM-R1}, a two-stage framework adapting
rule-based RL for multimodal reasoning through \textbf{Foundational Reasoning
Enhancement (FRE)} followed by \textbf{Multimodal Generalization Training
(MGT)}. The FRE stage first strengthens reasoning abilities using text-only
data with rule-based RL, then the MGT stage generalizes these reasoning
capabilities to multimodal domains.
  Experiments on Qwen2.5-VL-Instruct-3B demonstrate that LMM-R1 achieves 4.83\%
and 4.5\% average improvements over baselines in multimodal and text-only
benchmarks, respectively, with a 3.63\% gain in complex Football Game tasks.
These results validate that text-based reasoning enhancement enables effective
multimodal generalization, offering a data-efficient paradigm that bypasses
costly high-quality multimodal training data.","Peng, Yingzhe and Zhang, Gongrui and Zhang, Miaosen and You, Zhiyuan and Liu, Jie and Zhu, Qipeng and Yang, Kai and Xu, Xingzhong and Geng, Xin and Yang, Xu",2025,,,,arXiv preprint arXiv:2503.07536
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,MLRM-math-ursa,\cite{MLRM-math-ursa},Ursa: Understanding and verifying chain-of-thought reasoning in multimodal mathematics,,,"Luo, Ruilin and Zheng, Zhuofan and Wang, Yifan and Ni, Xinzhe and Lin, Zicheng and Jiang, Songtao and Yu, Yiyao and Shi, Chufan and Chu, Ruihang and Zeng, Jin and others",2025,,,,arXiv preprint arXiv:2501.04686
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,MLRM-spatial-star,\cite{MLRM-spatial-star},STAR-R1: Spatial TrAnsformation Reasoning by Reinforcing Multimodal LLMs,,,"Li, Zongzhao and Ma, Zongyang and Li, Mingze and Li, Songyou and Rong, Yu and Xu, Tingyang and Zhang, Ziqi and Zhao, Deli and Huang, Wenbing",2025,,,,arXiv preprint arXiv:2505.15804
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,MLRM-per-perception,\cite{MLRM-per-perception},Perception-R1: Pioneering Perception Policy with Reinforcement Learning,http://arxiv.org/abs/2504.07954v1,"Inspired by the success of DeepSeek-R1, we explore the potential of
rule-based reinforcement learning (RL) in MLLM post-training for perception
policy learning. While promising, our initial experiments reveal that
incorporating a thinking process through RL does not consistently lead to
performance gains across all visual perception tasks. This leads us to delve
into the essential role of RL in the context of visual perception. In this
work, we return to the fundamentals and explore the effects of RL on different
perception tasks. We observe that the perceptual complexity is a major factor
in determining the effectiveness of RL. We also observe that reward design
plays a crucial role in further approching the upper limit of model perception.
To leverage these findings, we propose Perception-R1, a scalable RL framework
using GRPO during MLLM post-training. With a standard Qwen2.5-VL-3B-Instruct,
Perception-R1 achieves +4.2% on RefCOCO+, +17.9% on PixMo-Count, +4.2% on
PageOCR, and notably, 31.9% AP on COCO2017 val for the first time, establishing
a strong baseline for perception policy learning.","Yu, En and Lin, Kangheng and Zhao, Liang and Yin, Jisheng and Wei, Yana and Peng, Yuang and Wei, Haoran and Sun, Jianjian and Han, Chunrui and Ge, Zheng and others",2025,,,,arXiv preprint arXiv:2504.07954
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,MLRM-per-visionr1,\cite{MLRM-per-visionr1},Vision-r1: Incentivizing reasoning capability in multimodal large language models,,,"Huang, Wenxuan and Jia, Bohan and Zhai, Zijie and Cao, Shaosheng and Ye, Zheyu and Zhao, Fei and Xu, Zhe and Hu, Yao and Lin, Shaohui",2025,,,,arXiv preprint arXiv:2503.06749
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,MLRM-per-vrft,\cite{MLRM-per-vrft},Visual-rft: Visual reinforcement fine-tuning,,,"Liu, Ziyu and Sun, Zeyi and Zang, Yuhang and Dong, Xiaoyi and Cao, Yuhang and Duan, Haodong and Lin, Dahua and Wang, Jiaqi",2025,,,,arXiv preprint arXiv:2503.01785
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,MLRM-mcot-chain,\cite{MLRM-mcot-chain},Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL,,,"Zhang, Xintong and Gao, Zhi and Zhang, Bofei and Li, Pengxiang and Zhang, Xiaowen and Liu, Yang and Yuan, Tao and Wu, Yuwei and Jia, Yunde and Zhu, Song-Chun and others",2025,,,,arXiv preprint arXiv:2505.15436
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,MLRM-mcot-grit,\cite{MLRM-mcot-grit},GRIT: Teaching MLLMs to Think with Images,http://arxiv.org/abs/2505.15879v1,"Recent studies have demonstrated the efficacy of using Reinforcement Learning
(RL) in building reasoning models that articulate chains of thoughts prior to
producing final answers. However, despite ongoing advances that aim at enabling
reasoning for vision-language tasks, existing open-source visual reasoning
models typically generate reasoning content with pure natural language, lacking
explicit integration of visual information. This limits their ability to
produce clearly articulated and visually grounded reasoning chains. To this
end, we propose Grounded Reasoning with Images and Texts (GRIT), a novel method
for training MLLMs to think with images. GRIT introduces a grounded reasoning
paradigm, in which models generate reasoning chains that interleave natural
language and explicit bounding box coordinates. These coordinates point to
regions of the input image that the model consults during its reasoning
process. Additionally, GRIT is equipped with a reinforcement learning approach,
GRPO-GR, built upon the GRPO algorithm. GRPO-GR employs robust rewards focused
on the final answer accuracy and format of the grounded reasoning output, which
eliminates the need for data with reasoning chain annotations or explicit
bounding box labels. As a result, GRIT achieves exceptional data efficiency,
requiring as few as 20 image-question-answer triplets from existing datasets.
Comprehensive evaluations demonstrate that GRIT effectively trains MLLMs to
produce coherent and visually grounded reasoning chains, showing a successful
unification of reasoning and grounding abilities.","Fan, Yue and He, Xuehai and Yang, Diji and Zheng, Kaizhi and Kuo, Ching-Chen and Zheng, Yuting and Narayanaraju, Sravana Jyothi and Guan, Xinze and Wang, Xin Eric",2025,,,,arXiv preprint arXiv:2505.15879
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,MLRM-mcot-rex,\cite{MLRM-mcot-rex},Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning,,,"Jiang, Qing and Chen, Xingyu and Zeng, Zhaoyang and Yu, Junzhi and Zhang, Lei",2025,,,,arXiv preprint arXiv:2506.04034
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,MLRM-mcot-deepeyes,\cite{MLRM-mcot-deepeyes},"DeepEyes: Incentivizing ""Thinking with Images"" via Reinforcement
  Learning",http://arxiv.org/abs/2505.14362v2,"Large Vision-Language Models (VLMs) have shown strong capabilities in
multimodal understanding and reasoning, yet they are primarily constrained by
text-based reasoning processes. However, achieving seamless integration of
visual and textual reasoning which mirrors human cognitive processes remains a
significant challenge. In particular, effectively incorporating advanced visual
input processing into reasoning mechanisms is still an open question. Thus, in
this paper, we explore the interleaved multimodal reasoning paradigm and
introduce DeepEyes, a model with ""thinking with images"" capabilities
incentivized through end-to-end reinforcement learning without the need for
cold-start SFT. Notably, this ability emerges natively within the model itself,
leveraging its inherent grounding ability as a tool instead of depending on
separate specialized models. Specifically, we propose a tool-use-oriented data
selection mechanism and a reward strategy to encourage successful tool-assisted
reasoning trajectories. DeepEyes achieves significant performance gains on
fine-grained perception and reasoning benchmarks and also demonstrates
improvement in grounding, hallucination, and mathematical reasoning tasks.
Interestingly, we observe the distinct evolution of tool-calling behavior from
initial exploration to efficient and accurate exploitation, and diverse
thinking patterns that closely mirror human visual reasoning processes. Code is
available at https://github.com/Visual-Agent/DeepEyes.","Zheng, Ziwei and Yang, Michael and Hong, Jack and Zhao, Chenxiao and Xu, Guohai and Yang, Le and Shen, Chao and Yu, Xing",2025,,,,arXiv preprint arXiv:2505.14362
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,MLRM-reflect-mulberry,\cite{MLRM-reflect-mulberry},"Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via
  Collective Monte Carlo Tree Search",http://arxiv.org/abs/2412.18319v2,"In this work, we aim to develop an MLLM that understands and solves questions
by learning to create each intermediate step of the reasoning involved till the
final answer. To this end, we propose Collective Monte Carlo Tree Search
(CoMCTS), a new learning-to-reason method for MLLMs, which introduces the
concept of collective learning into ``tree search'' for effective and efficient
reasoning-path searching and learning. The core idea of CoMCTS is to leverage
collective knowledge from multiple models to collaboratively conjecture, search
and identify effective reasoning paths toward correct answers via four
iterative operations including Expansion, Simulation and Error Positioning,
Backpropagation, and Selection. Using CoMCTS, we construct Mulberry-260k, a
multimodal dataset with a tree of rich, explicit and well-defined reasoning
nodes for each question. With Mulberry-260k, we perform collective SFT to train
our model, Mulberry, a series of MLLMs with o1-like step-by-step Reasoning and
Reflection capabilities. Extensive experiments demonstrate the superiority of
our proposed methods on various benchmarks. Code will be available at
https://github.com/HJYao00/Mulberry","Yao, Huanjin and Huang, Jiaxing and Wu, Wenhao and Zhang, Jingyi and Wang, Yibo and Liu, Shunyu and Wang, Yingjie and Song, Yuxin and Feng, Haocheng and Shen, Li and others",2024,,,,arXiv preprint arXiv:2412.18319
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,MLRM-reflect-r3,\cite{MLRM-reflect-r3},"Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",http://arxiv.org/abs/2505.24726v1,"We explore a method for improving the performance of large language models
through self-reflection and reinforcement learning. By incentivizing the model
to generate better self-reflections when it answers incorrectly, we demonstrate
that a model's ability to solve complex, verifiable tasks can be enhanced even
when generating synthetic data is infeasible and only binary feedback is
available. Our framework operates in two stages: first, upon failing a given
task, the model generates a self-reflective commentary analyzing its previous
attempt; second, the model is given another attempt at the task with the
self-reflection in context. If the subsequent attempt succeeds, the tokens
generated during the self-reflection phase are rewarded. Our experimental
results show substantial performance gains across a variety of model
architectures, as high as 34.7% improvement at math equation writing and 18.1%
improvement at function calling. Notably, smaller fine-tuned models (1.5
billion to 7 billion parameters) outperform models in the same family that are
10 times larger. Our novel paradigm is thus an exciting pathway to more useful
and reliable language models that can self-improve on challenging tasks with
limited external feedback.","Bensal, Shelly and Jamil, Umar and Bryant, Christopher and Russak, Melisa and Kamble, Kiran and Mozolevskyi, Dmytro and Ali, Muayad and AlShikh, Waseem",2025,,,,arXiv preprint arXiv:2505.24726
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,MLRM-reflect-srpo,\cite{MLRM-reflect-srpo},"SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware
  Reinforcement Learning",http://arxiv.org/abs/2506.01713v3,"Multimodal large language models (MLLMs) have shown promising capabilities in
reasoning tasks, yet still struggle with complex problems requiring explicit
self-reflection and self-correction, especially compared to their unimodal
text-based counterparts. Existing reflection methods are simplistic and
struggle to generate meaningful and instructive feedback, as the reasoning
ability and knowledge limits of pre-trained models are largely fixed during
initial training. To overcome these challenges, we propose Multimodal
Self-Reflection enhanced reasoning with Group Relative Policy Optimization
(SRPO), a two-stage reflection-aware reinforcement learning (RL) framework
explicitly designed to enhance multimodal LLM reasoning. In the first stage, we
construct a high-quality, reflection-focused dataset under the guidance of an
advanced MLLM, which generates reflections based on initial responses to help
the policy model learn both reasoning and self-reflection. In the second stage,
we introduce a novel reward mechanism within the GRPO framework that encourages
concise and cognitively meaningful reflection while avoiding redundancy.
Extensive experiments across multiple multimodal reasoning benchmarks,
including MathVista, MathVision, MathVerse, and MMMU-Pro, using Qwen-2.5-VL-7B
and Qwen-2.5-VL-32B demonstrate that SRPO significantly outperforms
state-of-the-art models, achieving notable improvements in both reasoning
accuracy and reflection quality.","Wan, Zhongwei and Dou, Zhihao and Liu, Che and Zhang, Yu and Cui, Dongfei and Zhao, Qinjian and Shen, Hui and Xiong, Jing and Xin, Yi and Jiang, Yifan and others",2025,,,,arXiv preprint arXiv:2506.01713
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,MLRM-reflect-vl-rethinker,\cite{MLRM-reflect-vl-rethinker},Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning,,,"Wang, Haozhe and Qu, Chao and Huang, Zuming and Chu, Wei and Lin, Fangzhen and Chen, Wenhu",2025,,,,arXiv preprint arXiv:2504.08837
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,MLRM-aug-visionmatters,\cite{MLRM-aug-visionmatters},Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math Reasoning,,,"Li, Yuting and Wei, Lai and Zheng, Kaipeng and Huang, Jingyuan and Kong, Linghe and Sun, Lichao and Huang, Weiran",2025,,,,arXiv preprint arXiv:2506.09736
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,MLRM-aug-shareVL,\cite{MLRM-aug-shareVL},R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO,,,"Yao, Huanjin and Yin, Qixiang and Zhang, Jingyi and Yang, Min and Wang, Yibo and Wu, Wenhao and Su, Fei and Shen, Li and Qiu, Minghui and Tao, Dacheng and others",2025,,,,arXiv preprint arXiv:2505.16673
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,MLRM-aug-thinknot,\cite{MLRM-aug-thinknot},"Think or Not? Selective Reasoning via Reinforcement Learning for
  Vision-Language Models",http://arxiv.org/abs/2505.16854v2,"Reinforcement Learning (RL) has proven to be an effective post-training
strategy for enhancing reasoning in vision-language models (VLMs). Group
Relative Policy Optimization (GRPO) is a recent prominent method that
encourages models to generate complete reasoning traces before answering,
leading to increased token usage and computational cost. Inspired by the
human-like thinking process-where people skip reasoning for easy questions but
think carefully when needed-we explore how to enable VLMs to first decide when
reasoning is necessary. To realize this, we propose TON, a two-stage training
strategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective
'thought dropout' operation, where reasoning traces are randomly replaced with
empty thoughts. This introduces a think-or-not format that serves as a cold
start for selective reasoning; (ii) a GRPO stage that enables the model to
freely explore when to think or not, while maximizing task-aware outcome
rewards. Experimental results show that TON can reduce the completion length by
up to 90% compared to vanilla GRPO, without sacrificing performance or even
improving it. Further evaluations across diverse vision-language tasks-covering
a range of reasoning difficulties under both 3B and 7B models-consistently
reveal that the model progressively learns to bypass unnecessary reasoning
steps as training advances. These findings shed light on the path toward
human-like reasoning patterns in reinforcement learning approaches. Our code is
available at https://github.com/kokolerk/TON.","Wang, Jiaqi and Lin, Kevin Qinghong and Cheng, James and Shou, Mike Zheng",2025,,,,arXiv preprint arXiv:2505.16854
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,MLRM-reward-got-r1,\cite{MLRM-reward-got-r1},Got-r1: Unleashing reasoning capability of mllm for visual generation with reinforcement learning,,,"Duan, Chengqi and Fang, Rongyao and Wang, Yuqing and Wang, Kun and Huang, Linjiang and Zeng, Xingyu and Li, Hongsheng and Liu, Xihui",2025,,,,arXiv preprint arXiv:2505.17022
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,MLRM-reward-pixel,\cite{MLRM-reward-pixel},"Pixel Reasoner: Incentivizing Pixel-Space Reasoning with
  Curiosity-Driven Reinforcement Learning",http://arxiv.org/abs/2505.15966v2,"Chain-of-thought reasoning has significantly improved the performance of
Large Language Models (LLMs) across various domains. However, this reasoning
process has been confined exclusively to textual space, limiting its
effectiveness in visually intensive tasks. To address this limitation, we
introduce the concept of reasoning in the pixel-space. Within this novel
framework, Vision-Language Models (VLMs) are equipped with a suite of visual
reasoning operations, such as zoom-in and select-frame. These operations enable
VLMs to directly inspect, interrogate, and infer from visual evidences, thereby
enhancing reasoning fidelity for visual tasks. Cultivating such pixel-space
reasoning capabilities in VLMs presents notable challenges, including the
model's initially imbalanced competence and its reluctance to adopt the newly
introduced pixel-space operations. We address these challenges through a
two-phase training approach. The first phase employs instruction tuning on
synthesized reasoning traces to familiarize the model with the novel visual
operations. Following this, a reinforcement learning (RL) phase leverages a
curiosity-driven reward scheme to balance exploration between pixel-space
reasoning and textual reasoning. With these visual operations, VLMs can
interact with complex visual inputs, such as information-rich images or videos
to proactively gather necessary information. We demonstrate that this approach
significantly improves VLM performance across diverse visual reasoning
benchmarks. Our 7B model, \model, achieves 84\% on V* bench, 74\% on
TallyQA-Complex, and 84\% on InfographicsVQA, marking the highest accuracy
achieved by any open-source model to date. These results highlight the
importance of pixel-space reasoning and the effectiveness of our framework.","Su, Alex and Wang, Haozhe and Ren, Weimin and Lin, Fangzhen and Chen, Wenhu",2025,,,,arXiv preprint arXiv:2505.15966
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,MLRM-reward-sophiavl,\cite{MLRM-reward-sophiavl},SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward,,,"Fan, Kaixuan and Feng, Kaituo and Lyu, Haoming and Zhou, Dongzhan and Yue, Xiangyu",2025,,,,arXiv preprint arXiv:2505.17018
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,kumar2024training,\cite{kumar2024training},Training Language Models to Self-Correct via Reinforcement Learning,http://arxiv.org/abs/2409.12917v2,"Self-correction is a highly desirable capability of large language models
(LLMs), yet it has consistently been found to be largely ineffective in modern
LLMs. Current methods for training self-correction typically depend on either
multiple models, a more advanced model, or additional forms of supervision. To
address these shortcomings, we develop a multi-turn online reinforcement
learning (RL) approach, SCoRe, that significantly improves an LLM's
self-correction ability using entirely self-generated data. To build SCoRe, we
first show that variants of supervised fine-tuning (SFT) on offline
model-generated correction traces are often insufficient for instilling
self-correction behavior. In particular, we observe that training via SFT falls
prey to either a distribution mismatch between mistakes made by the
data-collection policy and the model's own responses, or to behavior collapse,
where learning implicitly prefers only a certain mode of correction behavior
that is often not effective at self-correction on test problems. SCoRe
addresses these challenges by training under the model's own distribution of
self-generated correction traces and using appropriate regularization to steer
the learning process into learning a self-correction behavior that is effective
at test time as opposed to fitting high-reward responses for a given prompt.
This regularization process includes an initial phase of multi-turn RL on a
base model to generate a policy initialization that is less susceptible to
collapse, followed by using a reward bonus to amplify self-correction. With
Gemini 1.0 Pro and 1.5 Flash models, we find that SCoRe achieves
state-of-the-art self-correction performance, improving the base models'
self-correction by 15.6% and 9.1% respectively on MATH and HumanEval.","Kumar, Aviral and Zhuang, Vincent and Agarwal, Rishabh and Su, Yi and Co-Reyes, John D and Singh, Avi and Baumli, Kate and Iqbal, Shariq and Bishop, Colton and Roelofs, Rebecca and others",2024,,,,arXiv preprint arXiv:2409.12917
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,safe-safemlrm,\cite{safe-safemlrm},SafeMLRM: Demystifying Safety in Multi-modal Large Reasoning Models,http://arxiv.org/abs/2504.08813v1,"The rapid advancement of multi-modal large reasoning models (MLRMs) --
enhanced versions of multimodal language models (MLLMs) equipped with reasoning
capabilities -- has revolutionized diverse applications. However, their safety
implications remain underexplored. While prior work has exposed critical
vulnerabilities in unimodal reasoning models, MLRMs introduce distinct risks
from cross-modal reasoning pathways. This work presents the first systematic
safety analysis of MLRMs through large-scale empirical studies comparing MLRMs
with their base MLLMs. Our experiments reveal three critical findings: (1) The
Reasoning Tax: Acquiring reasoning capabilities catastrophically degrades
inherited safety alignment. MLRMs exhibit 37.44% higher jailbreaking success
rates than base MLLMs under adversarial attacks. (2) Safety Blind Spots: While
safety degradation is pervasive, certain scenarios (e.g., Illegal Activity)
suffer 25 times higher attack rates -- far exceeding the average 3.4 times
increase, revealing scenario-specific vulnerabilities with alarming cross-model
and datasets consistency. (3) Emergent Self-Correction: Despite tight
reasoning-answer safety coupling, MLRMs demonstrate nascent self-correction --
16.9% of jailbroken reasoning steps are overridden by safe answers, hinting at
intrinsic safeguards. These findings underscore the urgency of scenario-aware
safety auditing and mechanisms to amplify MLRMs' self-correction potential. To
catalyze research, we open-source OpenSafeMLRM, the first toolkit for MLRM
safety evaluation, providing unified interface for mainstream models, datasets,
and jailbreaking methods. Our work calls for immediate efforts to harden
reasoning-augmented AI, ensuring its transformative potential aligns with
ethical safeguards.","Fang, Junfeng and Wang, Yukai and Wang, Ruipeng and Yao, Zijun and Wang, Kun and Zhang, An and Wang, Xiang and Chua, Tat-Seng",2025,,,,arXiv preprint arXiv:2504.08813
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,safe-risk-safety,\cite{safe-risk-safety},Safety Alignment Should Be Made More Than Just a Few Tokens Deep,http://arxiv.org/abs/2406.05946v1,"The safety alignment of current Large Language Models (LLMs) is vulnerable.
Relatively simple attacks, or even benign fine-tuning, can jailbreak aligned
models. We argue that many of these vulnerabilities are related to a shared
underlying issue: safety alignment can take shortcuts, wherein the alignment
adapts a model's generative distribution primarily over only its very first few
output tokens. We refer to this issue as shallow safety alignment. In this
paper, we present case studies to explain why shallow safety alignment can
exist and provide evidence that current aligned LLMs are subject to this issue.
We also show how these findings help explain multiple recently discovered
vulnerabilities in LLMs, including the susceptibility to adversarial suffix
attacks, prefilling attacks, decoding parameter attacks, and fine-tuning
attacks. Importantly, we discuss how this consolidated notion of shallow safety
alignment sheds light on promising research directions for mitigating these
vulnerabilities. For instance, we show that deepening the safety alignment
beyond just the first few tokens can often meaningfully improve robustness
against some common exploits. Finally, we design a regularized finetuning
objective that makes the safety alignment more persistent against fine-tuning
attacks by constraining updates on initial tokens. Overall, we advocate that
future safety alignment should be made more than just a few tokens deep.","Qi, Xiangyu and Panda, Ashwinee and Lyu, Kaifeng and Ma, Xiao and Roy, Subhrajit and Beirami, Ahmad and Mittal, Prateek and Henderson, Peter",2024,,,,arXiv preprint arXiv:2406.05946
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,safe-guard-eta,\cite{safe-guard-eta},"ETA: Evaluating Then Aligning Safety of Vision Language Models at
  Inference Time",http://arxiv.org/abs/2410.06625v2,"Vision Language Models (VLMs) have become essential backbones for multimodal
intelligence, yet significant safety challenges limit their real-world
application. While textual inputs are often effectively safeguarded,
adversarial visual inputs can easily bypass VLM defense mechanisms. Existing
defense methods are either resource-intensive, requiring substantial data and
compute, or fail to simultaneously ensure safety and usefulness in responses.
To address these limitations, we propose a novel two-phase inference-time
alignment framework, Evaluating Then Aligning (ETA): 1) Evaluating input visual
contents and output responses to establish a robust safety awareness in
multimodal settings, and 2) Aligning unsafe behaviors at both shallow and deep
levels by conditioning the VLMs' generative distribution with an interference
prefix and performing sentence-level best-of-N to search the most harmless and
helpful generation paths. Extensive experiments show that ETA outperforms
baseline methods in terms of harmlessness, helpfulness, and efficiency,
reducing the unsafe rate by 87.5% in cross-modality attacks and achieving 96.6%
win-ties in GPT-4 helpfulness evaluation. The code is publicly available at
https://github.com/DripNowhy/ETA.","Ding, Yi and Li, Bolian and Zhang, Ruqi",2024,,,,arXiv preprint arXiv:2410.06625
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,safe-risk-figstep,\cite{safe-risk-figstep},"FigStep: Jailbreaking Large Vision-Language Models via Typographic
  Visual Prompts",http://arxiv.org/abs/2311.05608v3,"Large Vision-Language Models (LVLMs) signify a groundbreaking paradigm shift
within the Artificial Intelligence (AI) community, extending beyond the
capabilities of Large Language Models (LLMs) by assimilating additional
modalities (e.g., images). Despite this advancement, the safety of LVLMs
remains adequately underexplored, with a potential overreliance on the safety
assurances purported by their underlying LLMs. In this paper, we propose
FigStep, a straightforward yet effective black-box jailbreak algorithm against
LVLMs. Instead of feeding textual harmful instructions directly, FigStep
converts the prohibited content into images through typography to bypass the
safety alignment. The experimental results indicate that FigStep can achieve an
average attack success rate of 82.50% on six promising open-source LVLMs. Not
merely to demonstrate the efficacy of FigStep, we conduct comprehensive
ablation studies and analyze the distribution of the semantic embeddings to
uncover that the reason behind the success of FigStep is the deficiency of
safety alignment for visual embeddings. Moreover, we compare FigStep with five
text-only jailbreaks and four image-based jailbreaks to demonstrate the
superiority of FigStep, i.e., negligible attack costs and better attack
performance. Above all, our work reveals that current LVLMs are vulnerable to
jailbreak attacks, which highlights the necessity of novel cross-modality
safety alignment techniques. Our code and datasets are available at
https://github.com/ThuCCSLab/FigStep .","Gong, Yichen and Ran, Delong and Liu, Jinyuan and Wang, Conglei and Cong, Tianshuo and Wang, Anyu and Duan, Sisi and Wang, Xiaoyun",2025,,,,
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,safe-risk-mllmguard,\cite{safe-risk-mllmguard},"MLLMGuard: A Multi-dimensional Safety Evaluation Suite for Multimodal
  Large Language Models",http://arxiv.org/abs/2406.07594v2,"Powered by remarkable advancements in Large Language Models (LLMs),
Multimodal Large Language Models (MLLMs) demonstrate impressive capabilities in
manifold tasks. However, the practical application scenarios of MLLMs are
intricate, exposing them to potential malicious instructions and thereby posing
safety risks. While current benchmarks do incorporate certain safety
considerations, they often lack comprehensive coverage and fail to exhibit the
necessary rigor and robustness. For instance, the common practice of employing
GPT-4V as both the evaluator and a model to be evaluated lacks credibility, as
it tends to exhibit a bias toward its own responses. In this paper, we present
MLLMGuard, a multidimensional safety evaluation suite for MLLMs, including a
bilingual image-text evaluation dataset, inference utilities, and a lightweight
evaluator. MLLMGuard's assessment comprehensively covers two languages (English
and Chinese) and five important safety dimensions (Privacy, Bias, Toxicity,
Truthfulness, and Legality), each with corresponding rich subtasks. Focusing on
these dimensions, our evaluation dataset is primarily sourced from platforms
such as social media, and it integrates text-based and image-based red teaming
techniques with meticulous annotation by human experts. This can prevent
inaccurate evaluation caused by data leakage when using open-source datasets
and ensures the quality and challenging nature of our benchmark. Additionally,
a fully automated lightweight evaluator termed GuardRank is developed, which
achieves significantly higher evaluation accuracy than GPT-4. Our evaluation
results across 13 advanced models indicate that MLLMs still have a substantial
journey ahead before they can be considered safe and responsible.","Gu, Tianle and Zhou, Zeyang and Huang, Kexin and Dandan, Liang and Wang, Yixu and Zhao, Haiquan and Yao, Yuanqi and Yang, Yujiu and Teng, Yan and Qiao, Yu and others",2024,,,,Advances in Neural Information Processing Systems
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,mmsafetybench,\cite{mmsafetybench},Mm-safetybench: A benchmark for safety evaluation of multimodal large language models,,,"Liu, Xin and Zhu, Yichen and Gu, Jindong and Lan, Yunshi and Yang, Chao and Qiao, Yu",2024,,,,
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,safe-risk-usb,\cite{safe-risk-usb},"USB: A Comprehensive and Unified Safety Evaluation Benchmark for
  Multimodal Large Language Models",http://arxiv.org/abs/2505.23793v1,"Despite their remarkable achievements and widespread adoption, Multimodal
Large Language Models (MLLMs) have revealed significant security
vulnerabilities, highlighting the urgent need for robust safety evaluation
benchmarks. Existing MLLM safety benchmarks, however, fall short in terms of
data quality and coverge, and modal risk combinations, resulting in inflated
and contradictory evaluation results, which hinders the discovery and
governance of security concerns. Besides, we argue that vulnerabilities to
harmful queries and oversensitivity to harmless ones should be considered
simultaneously in MLLMs safety evaluation, whereas these were previously
considered separately. In this paper, to address these shortcomings, we
introduce Unified Safety Benchmarks (USB), which is one of the most
comprehensive evaluation benchmarks in MLLM safety. Our benchmark features
high-quality queries, extensive risk categories, comprehensive modal
combinations, and encompasses both vulnerability and oversensitivity
evaluations. From the perspective of two key dimensions: risk categories and
modality combinations, we demonstrate that the available benchmarks -- even the
union of the vast majority of them -- are far from being truly comprehensive.
To bridge this gap, we design a sophisticated data synthesis pipeline that
generates extensive, high-quality complementary data addressing previously
unexplored aspects. By combining open-source datasets with our synthetic data,
our benchmark provides 4 distinct modality combinations for each of the 61 risk
sub-categories, covering both English and Chinese across both vulnerability and
oversensitivity dimensions.","Zheng, Baolin and Chen, Guanlin and Zhong, Hongqiong and Teng, Qingyang and Tan, Yingshui and Liu, Zhendong and Wang, Weixun and Liu, Jiaheng and Yang, Jian and Jing, Huiyun and others",2025,,,,arXiv preprint arXiv:2505.23793
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,safe-risk-aialign,\cite{safe-risk-aialign},AI Alignment: A Comprehensive Survey,http://arxiv.org/abs/2310.19852v6,"AI alignment aims to make AI systems behave in line with human intentions and
values. As AI systems grow more capable, so do risks from misalignment. To
provide a comprehensive and up-to-date overview of the alignment field, in this
survey, we delve into the core concepts, methodology, and practice of
alignment. First, we identify four principles as the key objectives of AI
alignment: Robustness, Interpretability, Controllability, and Ethicality
(RICE). Guided by these four principles, we outline the landscape of current
alignment research and decompose them into two key components: forward
alignment and backward alignment. The former aims to make AI systems aligned
via alignment training, while the latter aims to gain evidence about the
systems' alignment and govern them appropriately to avoid exacerbating
misalignment risks. On forward alignment, we discuss techniques for learning
from feedback and learning under distribution shift. On backward alignment, we
discuss assurance techniques and governance practices.
  We also release and continually update the website (www.alignmentsurvey.com)
which features tutorials, collections of papers, blog posts, and other
resources.","Ji, Jiaming and Qiu, Tianyi and Chen, Boyuan and Zhang, Borong and Lou, Hantao and Wang, Kaile and Duan, Yawen and He, Zhonghao and Zhou, Jiayi and Zhang, Zhaowei and others",2023,,,,arXiv preprint arXiv:2310.19852
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,safe-risk-red,\cite{safe-risk-red},Red Teaming Visual Language Models,http://arxiv.org/abs/2401.12915v1,"VLMs (Vision-Language Models) extend the capabilities of LLMs (Large Language
Models) to accept multimodal inputs. Since it has been verified that LLMs can
be induced to generate harmful or inaccurate content through specific test
cases (termed as Red Teaming), how VLMs perform in similar scenarios,
especially with their combination of textual and visual inputs, remains a
question. To explore this problem, we present a novel red teaming dataset
RTVLM, which encompasses 10 subtasks (e.g., image misleading, multi-modal
jail-breaking, face fairness, etc) under 4 primary aspects (faithfulness,
privacy, safety, fairness). Our RTVLM is the first red-teaming dataset to
benchmark current VLMs in terms of these 4 different aspects. Detailed analysis
shows that 10 prominent open-sourced VLMs struggle with the red teaming in
different degrees and have up to 31% performance gap with GPT-4V. Additionally,
we simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning
(SFT) using RTVLM, and this bolsters the models' performance with 10% in RTVLM
test set, 13% in MM-Hal, and without noticeable decline in MM-Bench,
overpassing other LLaVA-based models with regular alignment data. This reveals
that current open-sourced VLMs still lack red teaming alignment. Our code and
datasets will be open-source.","Li, Mukai and Li, Lei and Yin, Yuwei and Ahmed, Masood and Liu, Zhenguang and Liu, Qi",2024,,,,arXiv preprint arXiv:2401.12915
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,safe-3h,\cite{safe-3h},A General Language Assistant as a Laboratory for Alignment,http://arxiv.org/abs/2112.00861v3,"Given the broad capabilities of large language models, it should be possible
to work towards a general-purpose, text-based assistant that is aligned with
human values, meaning that it is helpful, honest, and harmless. As an initial
foray in this direction we study simple baseline techniques and evaluations,
such as prompting. We find that the benefits from modest interventions increase
with model size, generalize to a variety of alignment evaluations, and do not
compromise the performance of large models. Next we investigate scaling trends
for several training objectives relevant to alignment, comparing imitation
learning, binary discrimination, and ranked preference modeling. We find that
ranked preference modeling performs much better than imitation learning, and
often scales more favorably with model size. In contrast, binary discrimination
typically performs and scales very similarly to imitation learning. Finally we
study a `preference model pre-training' stage of training, with the goal of
improving sample efficiency when finetuning on human preferences.","Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and others",2021,,,,arXiv preprint arXiv:2112.00861
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,vlguard,\cite{vlguard},"Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large
  Language Models",http://arxiv.org/abs/2402.02207v2,"Current vision large language models (VLLMs) exhibit remarkable capabilities
yet are prone to generate harmful content and are vulnerable to even the
simplest jailbreaking attacks. Our initial analysis finds that this is due to
the presence of harmful data during vision-language instruction fine-tuning,
and that VLLM fine-tuning can cause forgetting of safety alignment previously
learned by the underpinning LLM. To address this issue, we first curate a
vision-language safe instruction-following dataset VLGuard covering various
harmful categories. Our experiments demonstrate that integrating this dataset
into standard vision-language fine-tuning or utilizing it for post-hoc
fine-tuning effectively safety aligns VLLMs. This alignment is achieved with
minimal impact on, or even enhancement of, the models' helpfulness. The
versatility of our safety fine-tuning dataset makes it a valuable resource for
safety-testing existing VLLMs, training new models or safeguarding pre-trained
VLLMs. Empirical results demonstrate that fine-tuned VLLMs effectively reject
unsafe instructions and substantially reduce the success rates of several
black-box adversarial attacks, which approach zero in many cases. The code and
dataset are available at https://github.com/ys-zong/VLGuard.","Zong, Yongshuo and Bohdal, Ondrej and Yu, Tingyang and Yang, Yongxin and Hospedales, Timothy",2024,,,,arXiv preprint arXiv:2402.02207
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,safe-algin-think,\cite{safe-algin-think},"Think in Safety: Unveiling and Mitigating Safety Alignment Collapse in
  Multimodal Large Reasoning Model",http://arxiv.org/abs/2505.06538v4,"The rapid development of Multimodal Large Reasoning Models (MLRMs) has
demonstrated broad application potential, yet their safety and reliability
remain critical concerns that require systematic exploration. To address this
gap, we conduct a comprehensive and systematic safety evaluation of 11 MLRMs
across 5 benchmarks and unveil prevalent safety degradation phenomena in most
advanced models. Moreover, our analysis reveals distinct safety patterns across
different benchmarks: significant safety degradation is observed across
jailbreak robustness benchmarks, whereas safety-awareness benchmarks
demonstrate less pronounced degradation. In particular, the long thought
process in some scenarios even enhances safety performance. Therefore, it is a
potential approach to address safety issues in MLRMs by leveraging the
intrinsic reasoning capabilities of the model to detect unsafe intent. To
operationalize this insight, we construct a multimodal tuning dataset that
incorporates a safety-oriented thought process. Experimental results from
fine-tuning existing MLRMs with this dataset effectively enhances the safety on
both jailbreak robustness and safety-awareness benchmarks. This study provides
a new perspective for developing safe MLRMs. Our dataset is available at
https://github.com/xinyuelou/Think-in-Safety.","Lou, Xinyue and Li, You and Xu, Jinan and Shi, Xiangyu and Chen, Chi and Huang, Kaiyu",2025,,,,arXiv preprint arXiv:2505.06538
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,safe-algin-saferlhfv,\cite{safe-algin-saferlhfv},Safe RLHF: Safe Reinforcement Learning from Human Feedback,http://arxiv.org/abs/2310.12773v1,"With the development of large language models (LLMs), striking a balance
between the performance and safety of AI systems has never been more critical.
However, the inherent tension between the objectives of helpfulness and
harmlessness presents a significant challenge during LLM training. To address
this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe
RLHF), a novel algorithm for human value alignment. Safe RLHF explicitly
decouples human preferences regarding helpfulness and harmlessness, effectively
avoiding the crowdworkers' confusion about the tension and allowing us to train
separate reward and cost models. We formalize the safety concern of LLMs as an
optimization task of maximizing the reward function while satisfying specified
cost constraints. Leveraging the Lagrangian method to solve this constrained
problem, Safe RLHF dynamically adjusts the balance between the two objectives
during fine-tuning. Through a three-round fine-tuning using Safe RLHF, we
demonstrate a superior ability to mitigate harmful responses while enhancing
model performance compared to existing value-aligned algorithms.
Experimentally, we fine-tuned the Alpaca-7B using Safe RLHF and aligned it with
collected human preferences, significantly improving its helpfulness and
harmlessness according to human evaluations.","Ji, Jiaming and Chen, Xinyu and Pan, Rui and Zhang, Conghui and Zhu, Han and Li, Jiahao and Hong, Donghai and Chen, Boyuan and Zhou, Jiayi and Wang, Kaile and others",2025,,,,arXiv preprint arXiv:2503.17682
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,safe-algin-G-RLHF-V,\cite{safe-algin-G-RLHF-V},Generative RLHF-V: Learning Principles from Multi-modal Human Preference,,,"Zhou, Jiayi and Ji, Jiaming and Chen, Boyuan and Sun, Jiapeng and Chen, Wenqi and Hong, Donghai and Han, Sirui and Guo, Yike and Yang, Yaodong",2025,,,,arXiv preprint arXiv:2505.18531
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,safe-algin-ADPO,\cite{safe-algin-ADPO},"Adversary-Aware DPO: Enhancing Safety Alignment in Vision Language
  Models via Adversarial Training",http://arxiv.org/abs/2502.11455v1,"Safety alignment is critical in pre-training large language models (LLMs) to
generate responses aligned with human values and refuse harmful queries. Unlike
LLM, the current safety alignment of VLMs is often achieved with post-hoc
safety fine-tuning. However, these methods are less effective to white-box
attacks. To address this, we propose $\textit{Adversary-aware DPO (ADPO)}$, a
novel training framework that explicitly considers adversarial.
$\textit{Adversary-aware DPO (ADPO)}$ integrates adversarial training into DPO
to enhance the safety alignment of VLMs under worst-case adversarial
perturbations. $\textit{ADPO}$ introduces two key components: (1) an
adversarial-trained reference model that generates human-preferred responses
under worst-case perturbations, and (2) an adversarial-aware DPO loss that
generates winner-loser pairs accounting for adversarial distortions. By
combining these innovations, $\textit{ADPO}$ ensures that VLMs remain robust
and reliable even in the presence of sophisticated jailbreak attacks. Extensive
experiments demonstrate that $\textit{ADPO}$ outperforms baselines in the
safety alignment and general utility of VLMs.","Weng, Fenghua and Lou, Jian and Feng, Jun and Huang, Minlie and Wang, Wenjie",2025,,,,arXiv preprint arXiv:2502.11455
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,safe-algin-SafeVid,\cite{safe-algin-SafeVid},SafeVid: Toward Safety Aligned Video Large Multimodal Models,http://arxiv.org/abs/2505.11926v1,"As Video Large Multimodal Models (VLMMs) rapidly advance, their inherent
complexity introduces significant safety challenges, particularly the issue of
mismatched generalization where static safety alignments fail to transfer to
dynamic video contexts. We introduce SafeVid, a framework designed to instill
video-specific safety principles in VLMMs. SafeVid uniquely transfers robust
textual safety alignment capabilities to the video domain by employing detailed
textual video descriptions as an interpretive bridge, facilitating LLM-based
rule-driven safety reasoning. This is achieved through a closed-loop system
comprising: 1) generation of SafeVid-350K, a novel 350,000-pair video-specific
safety preference dataset; 2) targeted alignment of VLMMs using Direct
Preference Optimization (DPO); and 3) comprehensive evaluation via our new
SafeVidBench benchmark. Alignment with SafeVid-350K significantly enhances VLMM
safety, with models like LLaVA-NeXT-Video demonstrating substantial
improvements (e.g., up to 42.39%) on SafeVidBench. SafeVid provides critical
resources and a structured approach, demonstrating that leveraging textual
descriptions as a conduit for safety reasoning markedly improves the safety
alignment of VLMMs. We have made SafeVid-350K dataset
(https://huggingface.co/datasets/yxwang/SafeVid-350K) publicly available.","Wang, Yixu and Song, Jiaxin and Gao, Yifeng and Wang, Xin and Yao, Yang and Teng, Yan and Ma, Xingjun and Wang, Yingchun and Jiang, Yu-Gang",2025,,,,arXiv preprint arXiv:2505.11926
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,safe-guard-vlmguard,\cite{safe-guard-vlmguard},VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven Prompt Optimization,,,"Chen, Menglan and Pang, Xianghe and Dong, Jingjing and Wang, WenHao and Du, Yaxin and Chen, Siheng",2025,,,,arXiv preprint arXiv:2504.12661
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,safe-guard-rapguard,\cite{safe-guard-rapguard},"RapGuard: Safeguarding Multimodal Large Language Models via
  Rationale-aware Defensive Prompting",http://arxiv.org/abs/2412.18826v1,"While Multimodal Large Language Models (MLLMs) have made remarkable progress
in vision-language reasoning, they are also more susceptible to producing
harmful content compared to models that focus solely on text. Existing
defensive prompting techniques rely on a static, unified safety guideline that
fails to account for the specific risks inherent in different multimodal
contexts. To address these limitations, we propose RapGuard, a novel framework
that uses multimodal chain-of-thought reasoning to dynamically generate
scenario-specific safety prompts. RapGuard enhances safety by adapting its
prompts to the unique risks of each input, effectively mitigating harmful
outputs while maintaining high performance on benign tasks. Our experimental
results across multiple MLLM benchmarks demonstrate that RapGuard achieves
state-of-the-art safety performance, significantly reducing harmful content
without degrading the quality of responses.","Jiang, Yilei and Tan, Yingshui and Yue, Xiangyu",2024,,,,arXiv preprint arXiv:2412.18826
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,safe-guard-adashield,\cite{safe-guard-adashield},"AdaShield: Safeguarding Multimodal Large Language Models from
  Structure-based Attack via Adaptive Shield Prompting",http://arxiv.org/abs/2403.09513v1,"With the advent and widespread deployment of Multimodal Large Language Models
(MLLMs), the imperative to ensure their safety has become increasingly
pronounced. However, with the integration of additional modalities, MLLMs are
exposed to new vulnerabilities, rendering them prone to structured-based
jailbreak attacks, where semantic content (e.g., ""harmful text"") has been
injected into the images to mislead MLLMs. In this work, we aim to defend
against such threats. Specifically, we propose \textbf{Ada}ptive
\textbf{Shield} Prompting (\textbf{AdaShield}), which prepends inputs with
defense prompts to defend MLLMs against structure-based jailbreak attacks
without fine-tuning MLLMs or training additional modules (e.g., post-stage
content detector). Initially, we present a manually designed static defense
prompt, which thoroughly examines the image and instruction content step by
step and specifies response methods to malicious queries. Furthermore, we
introduce an adaptive auto-refinement framework, consisting of a target MLLM
and a LLM-based defense prompt generator (Defender). These components
collaboratively and iteratively communicate to generate a defense prompt.
Extensive experiments on the popular structure-based jailbreak attacks and
benign datasets show that our methods can consistently improve MLLMs'
robustness against structure-based jailbreak attacks without compromising the
model's general capabilities evaluated on standard benign tasks. Our code is
available at https://github.com/rain305f/AdaShield.","Wang, Yu and Liu, Xiaogeng and Li, Yu and Chen, Muhao and Xiao, Chaowei",2024,,,,
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,safe-guard-mllmprotector,\cite{safe-guard-mllmprotector},Mllm-protector: Ensuring mllm's safety without hurting performance,,,"Pi, Renjie and Han, Tianyang and Zhang, Jianshu and Xie, Yueqi and Pan, Rui and Lian, Qing and Dong, Hanze and Zhang, Jipeng and Zhang, Tong",2024,,,,arXiv preprint arXiv:2401.02906
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,safe-guard-guardreasoner,\cite{safe-guard-guardreasoner},Guardreasoner-vl: Safeguarding vlms via reinforced reasoning,,,"Liu, Yue and Zhai, Shengfang and Du, Mingzhe and Chen, Yulin and Cao, Tri and Gao, Hongcheng and Wang, Cheng and Li, Xinfeng and Wang, Kun and Fang, Junfeng and others",2025,,,,arXiv preprint arXiv:2505.11049
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,safe-guard-coca,\cite{safe-guard-coca},"CoCA: Regaining Safety-awareness of Multimodal Large Language Models
  with Constitutional Calibration",http://arxiv.org/abs/2409.11365v2,"The deployment of multimodal large language models (MLLMs) has demonstrated
remarkable success in engaging in conversations involving visual inputs, thanks
to the superior power of large language models (LLMs). Those MLLMs are
typically built based on the LLMs, with an image encoder to process images into
the token embedding space of the LLMs. However, the integration of visual
modality has introduced a unique vulnerability: the MLLM becomes susceptible to
malicious visual inputs and prone to generating sensitive or harmful responses,
even though the LLM has been trained on textual dataset to align with human
value. In this paper, we first raise the question: ``Do the MLLMs possess
safety-awareness against malicious image inputs?"". We find that after adding a
principle that specifies the safety requirement into the input of the MLLM, the
model's safety awareness becomes boosted. This phenomenon verifies the
existence of MLLM's safety-awareness against image inputs, it is only weakened
by the modality gap. We then introduce a simple yet effective technique termed
CoCA, which amplifies the safety-awareness of the MLLM by calibrating its
output distribution. Our proposed strategy helps the model reclaim its original
safety awareness without losing its original capabilities. We verify the
effectiveness of our approach on both multimodal safety and understanding
benchmarks.","Gao, Jiahui and Pi, Renjie and Han, Tianyang and Wu, Han and Hong, Lanqing and Kong, Lingpeng and Jiang, Xin and Li, Zhenguo",2024,,,,arXiv preprint arXiv:2409.11365
SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,http://arxiv.org/abs/2510.06871v2,safe-guard-immune,\cite{safe-guard-immune},"Immune: Improving Safety Against Jailbreaks in Multi-modal LLMs via
  Inference-Time Alignment",http://arxiv.org/abs/2411.18688v5,"With the widespread deployment of Multimodal Large Language Models (MLLMs)
for visual-reasoning tasks, improving their safety has become crucial. Recent
research indicates that despite training-time safety alignment, these models
remain vulnerable to jailbreak attacks. In this work, we first highlight an
important safety gap to describe that alignment achieved solely through safety
training may be insufficient against jailbreak attacks. To address this
vulnerability, we propose Immune, an inference-time defense framework that
leverages a safe reward model through controlled decoding to defend against
jailbreak attacks. Additionally, we provide a mathematical characterization of
Immune, offering insights on why it improves safety against jailbreaks.
Extensive evaluations on diverse jailbreak benchmarks using recent MLLMs reveal
that Immune effectively enhances model safety while preserving the model's
original capabilities. For instance, against text-based jailbreak attacks on
LLaVA-1.6, Immune reduces the attack success rate by 57.82% and 16.78% compared
to the base MLLM and state-of-the-art defense strategy, respectively.","Ghosal, Soumya Suvra and Chakraborty, Souradip and Singh, Vaibhav and Guan, Tianrui and Wang, Mengdi and Beirami, Ahmad and Huang, Furong and Velasquez, Alvaro and Manocha, Dinesh and Bedi, Amrit Singh",2025,,,,
