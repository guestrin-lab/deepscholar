parent_paper_title,parent_arxiv_link,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,cheynel2023sparse,\cite{cheynel2023sparse},Sparse Motion Semantics for Contact-Aware Retargeting,,,"Cheynel, Th{\'e}o and Rossi, Thomas and Bellot-Gurlet, Baptiste and Rohmer, Damien and Cani, Marie-Paule",2023,,,,
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,Ho2010Spatial,\cite{Ho2010Spatial},Spatial Relationship Preserving Character Motion Adaptation,,,"Ho, Edmond S. L. and Komura, Taku and Tai, Chiew-Lan",2010,,,,ACM Transactions on Graphics
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,kim2016retargeting,\cite{kim2016retargeting},Retargeting human-object interaction to virtual avatars,,,"Kim, Yeonjoon and Park, Hangil and Bang, Seungbae and Lee, Sung-Hee",2016,,,,IEEE transactions on visualization and computer graphics
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,gleicher1998retargetting,\cite{gleicher1998retargetting},Retargetting motion to new characters,,,"Gleicher, Michael",1998,,,,
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,aberman2020skeleton,\cite{aberman2020skeleton},Skeleton-Aware Networks for Deep Motion Retargeting,http://arxiv.org/abs/2005.05732v1,"We introduce a novel deep learning framework for data-driven motion
retargeting between skeletons, which may have different structure, yet
corresponding to homeomorphic graphs. Importantly, our approach learns how to
retarget without requiring any explicit pairing between the motions in the
training set. We leverage the fact that different homeomorphic skeletons may be
reduced to a common primal skeleton by a sequence of edge merging operations,
which we refer to as skeletal pooling. Thus, our main technical contribution is
the introduction of novel differentiable convolution, pooling, and unpooling
operators. These operators are skeleton-aware, meaning that they explicitly
account for the skeleton's hierarchical structure and joint adjacency, and
together they serve to transform the original motion into a collection of deep
temporal features associated with the joints of the primal skeleton. In other
words, our operators form the building blocks of a new deep motion processing
framework that embeds the motion into a common latent space, shared by a
collection of homeomorphic skeletons. Thus, retargeting can be achieved simply
by encoding to, and decoding from this latent space. Our experiments show the
effectiveness of our framework for motion retargeting, as well as motion
processing in general, compared to existing approaches. Our approach is also
quantitatively evaluated on a synthetic dataset that contains pairs of motions
applied to different skeletons. To the best of our knowledge, our method is the
first to perform retargeting between skeletons with differently sampled
kinematic chains, without any paired examples.","Aberman, Kfir and Li, Peizhuo and Lischinski, Dani and Sorkine-Hornung, Olga and Cohen-Or, Daniel and Chen, Baoquan",2020,,,,ACM Transactions on Graphics (TOG)
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,villegas2018neural,\cite{villegas2018neural},Neural Kinematic Networks for Unsupervised Motion Retargetting,http://arxiv.org/abs/1804.05653v1,"We propose a recurrent neural network architecture with a Forward Kinematics
layer and cycle consistency based adversarial training objective for
unsupervised motion retargetting. Our network captures the high-level
properties of an input motion by the forward kinematics layer, and adapts them
to a target character with different skeleton bone lengths (e.g., shorter,
longer arms etc.). Collecting paired motion training sequences from different
characters is expensive. Instead, our network utilizes cycle consistency to
learn to solve the Inverse Kinematics problem in an unsupervised manner. Our
method works online, i.e., it adapts the motion sequence on-the-fly as new
frames are received. In our experiments, we use the Mixamo animation data to
test our method for a variety of motions and characters and achieve
state-of-the-art results. We also demonstrate motion retargetting from
monocular human videos to 3D characters using an off-the-shelf 3D pose
estimator.","Villegas, Ruben and Yang, Jimei and Ceylan, Duygu and Lee, Honglak",2018,,,,
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,zhang2023simulation,\cite{zhang2023simulation},Simulation and Retargeting of Complex Multi-Character Interactions,http://arxiv.org/abs/2305.20041v1,"We present a method for reproducing complex multi-character interactions for
physically simulated humanoid characters using deep reinforcement learning. Our
method learns control policies for characters that imitate not only individual
motions, but also the interactions between characters, while maintaining
balance and matching the complexity of reference data. Our approach uses a
novel reward formulation based on an interaction graph that measures distances
between pairs of interaction landmarks. This reward encourages control policies
to efficiently imitate the character's motion while preserving the spatial
relationships of the interactions in the reference motion. We evaluate our
method on a variety of activities, from simple interactions such as a high-five
greeting to more complex interactions such as gymnastic exercises, Salsa
dancing, and box carrying and throwing. This approach can be used to
``clean-up'' existing motion capture data to produce physically plausible
interactions or to retarget motion to new characters with different sizes,
kinematics or morphologies while maintaining the interactions in the original
data.","Zhang, Yunbo and Gopinath, Deepak and Ye, Yuting and Hodgins, Jessica and Turk, Greg and Won, Jungdam",2023,,,,
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,Luo2023PerpetualHC,\cite{Luo2023PerpetualHC},Perpetual Humanoid Control for Real-time Simulated Avatars,http://arxiv.org/abs/2305.06456v3,"We present a physics-based humanoid controller that achieves high-fidelity
motion imitation and fault-tolerant behavior in the presence of noisy input
(e.g. pose estimates from video or generated from language) and unexpected
falls. Our controller scales up to learning ten thousand motion clips without
using any external stabilizing forces and learns to naturally recover from
fail-state. Given reference motion, our controller can perpetually control
simulated avatars without requiring resets. At its core, we propose the
progressive multiplicative control policy (PMCP), which dynamically allocates
new network capacity to learn harder and harder motion sequences. PMCP allows
efficient scaling for learning from large-scale motion databases and adding new
tasks, such as fail-state recovery, without catastrophic forgetting. We
demonstrate the effectiveness of our controller by using it to imitate noisy
poses from video-based pose estimators and language-based motion generators in
a live and real-time multi-person avatar use case.",Zhengyi Luo and Jinkun Cao and Alexander W. Winkler and Kris Kitani and Weipeng Xu,2023,,,,
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,he2025asap,\cite{he2025asap},"ASAP: Aligning Simulation and Real-World Physics for Learning Agile
  Humanoid Whole-Body Skills",http://arxiv.org/abs/2502.01143v3,"Humanoid robots hold the potential for unparalleled versatility in performing
human-like, whole-body skills. However, achieving agile and coordinated
whole-body motions remains a significant challenge due to the dynamics mismatch
between simulation and the real world. Existing approaches, such as system
identification (SysID) and domain randomization (DR) methods, often rely on
labor-intensive parameter tuning or result in overly conservative policies that
sacrifice agility. In this paper, we present ASAP (Aligning Simulation and
Real-World Physics), a two-stage framework designed to tackle the dynamics
mismatch and enable agile humanoid whole-body skills. In the first stage, we
pre-train motion tracking policies in simulation using retargeted human motion
data. In the second stage, we deploy the policies in the real world and collect
real-world data to train a delta (residual) action model that compensates for
the dynamics mismatch. Then, ASAP fine-tunes pre-trained policies with the
delta action model integrated into the simulator to align effectively with
real-world dynamics. We evaluate ASAP across three transfer scenarios: IsaacGym
to IsaacSim, IsaacGym to Genesis, and IsaacGym to the real-world Unitree G1
humanoid robot. Our approach significantly improves agility and whole-body
coordination across various dynamic motions, reducing tracking error compared
to SysID, DR, and delta dynamics learning baselines. ASAP enables highly agile
motions that were previously difficult to achieve, demonstrating the potential
of delta action learning in bridging simulation and real-world dynamics. These
results suggest a promising sim-to-real direction for developing more
expressive and agile humanoids.","He, Tairan and Gao, Jiawei and Xiao, Wenli and Zhang, Yuanhang and Wang, Zi and Wang, Jiashun and Luo, Zhengyi and He, Guanqi and Sobanbabu, Nikhil and Pan, Chaoyi and Yi, Zeji and Qu, Guannan and Kitani, Kris and Hodgins, Jessica and Fan, Linxi ""Jim"" and Zhu, Yuke and Liu, Changliu and Shi, Guanya",2025,,,,arXiv
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,he2024omnih2o,\cite{he2024omnih2o},"OmniH2O: Universal and Dexterous Human-to-Humanoid Whole-Body
  Teleoperation and Learning",http://arxiv.org/abs/2406.08858v1,"We present OmniH2O (Omni Human-to-Humanoid), a learning-based system for
whole-body humanoid teleoperation and autonomy. Using kinematic pose as a
universal control interface, OmniH2O enables various ways for a human to
control a full-sized humanoid with dexterous hands, including using real-time
teleoperation through VR headset, verbal instruction, and RGB camera. OmniH2O
also enables full autonomy by learning from teleoperated demonstrations or
integrating with frontier models such as GPT-4. OmniH2O demonstrates
versatility and dexterity in various real-world whole-body tasks through
teleoperation or autonomy, such as playing multiple sports, moving and
manipulating objects, and interacting with humans. We develop an RL-based
sim-to-real pipeline, which involves large-scale retargeting and augmentation
of human motion datasets, learning a real-world deployable policy with sparse
sensor input by imitating a privileged teacher policy, and reward designs to
enhance robustness and stability. We release the first humanoid whole-body
control dataset, OmniH2O-6, containing six everyday tasks, and demonstrate
humanoid whole-body skill learning from teleoperated datasets.","He, Tairan and Luo, Zhengyi and He, Xialin and Xiao, Wenli and Zhang, Chong and Zhang, Weinan and Kitani, Kris M and Liu, Changliu and Shi, Guanya",2025,,,,
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,ze2025twist,\cite{ze2025twist},TWIST: Teleoperated Whole-Body Imitation System,http://arxiv.org/abs/2505.02833v1,"Teleoperating humanoid robots in a whole-body manner marks a fundamental step
toward developing general-purpose robotic intelligence, with human motion
providing an ideal interface for controlling all degrees of freedom. Yet, most
current humanoid teleoperation systems fall short of enabling coordinated
whole-body behavior, typically limiting themselves to isolated locomotion or
manipulation tasks. We present the Teleoperated Whole-Body Imitation System
(TWIST), a system for humanoid teleoperation through whole-body motion
imitation. We first generate reference motion clips by retargeting human motion
capture data to the humanoid robot. We then develop a robust, adaptive, and
responsive whole-body controller using a combination of reinforcement learning
and behavior cloning (RL+BC). Through systematic analysis, we demonstrate how
incorporating privileged future motion frames and real-world motion capture
(MoCap) data improves tracking accuracy. TWIST enables real-world humanoid
robots to achieve unprecedented, versatile, and coordinated whole-body motor
skills--spanning whole-body manipulation, legged manipulation, locomotion, and
expressive movement--using a single unified neural network controller. Our
project website: https://humanoid-teleop.github.io","Ze, Yanjie and Chen, Zixuan and Ara{\~A}{\v{s}}jo, Jo{\~A}{\c{G}}o Pedro and Cao, Zi-ang and Peng, Xue Bin and Wu, Jiajun and Liu, C Karen",2025,,,,CoRL
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,videomimic,\cite{videomimic},Visual Imitation Enables Contextual Humanoid Control,http://arxiv.org/abs/2505.03729v5,"How can we teach humanoids to climb staircases and sit on chairs using the
surrounding environment context? Arguably, the simplest way is to just show
them-casually capture a human motion video and feed it to humanoids. We
introduce VIDEOMIMIC, a real-to-sim-to-real pipeline that mines everyday
videos, jointly reconstructs the humans and the environment, and produces
whole-body control policies for humanoid robots that perform the corresponding
skills. We demonstrate the results of our pipeline on real humanoid robots,
showing robust, repeatable contextual control such as staircase ascents and
descents, sitting and standing from chairs and benches, as well as other
dynamic whole-body skills-all from a single policy, conditioned on the
environment and global root commands. VIDEOMIMIC offers a scalable path towards
teaching humanoids to operate in diverse real-world environments.","Allshire, Arthur and Choi, Hongsuk and Zhang, Junyi and McAllister, David 
                       and Zhang, Anthony and Kim, Chung Min and Darrell, Trevor and Abbeel, 
                       Pieter and Malik, Jitendra and Kanazawa, Angjoo",2025,,,,CoRL
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,Nakaoka2012Interaction,\cite{Nakaoka2012Interaction},Interaction mesh based motion adaptation for biped humanoid robots,,,"Nakaoka, Shin'ichiro and Komura, Taku",2012,,,,
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,dao2024sim,\cite{dao2024sim},Sim-to-Real Learning for Humanoid Box Loco-Manipulation,http://arxiv.org/abs/2310.03191v1,"In this work we propose a learning-based approach to box loco-manipulation
for a humanoid robot. This is a particularly challenging problem due to the
need for whole-body coordination in order to lift boxes of varying weight,
position, and orientation while maintaining balance. To address this challenge,
we present a sim-to-real reinforcement learning approach for training general
box pickup and carrying skills for the bipedal robot Digit. Our reward
functions are designed to produce the desired interactions with the box while
also valuing balance and gait quality. We combine the learned skills into a
full system for box loco-manipulation to achieve the task of moving boxes from
one table to another with a variety of sizes, weights, and initial
configurations. In addition to quantitative simulation results, we demonstrate
successful sim-to-real transfer on the humanoid r","Dao, Jeremy and Duan, Helei and Fern, Alan",2024,,,,
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,long2024learning,\cite{long2024learning},Learning Humanoid Locomotion with Perceptive Internal Model,http://arxiv.org/abs/2411.14386v1,"In contrast to quadruped robots that can navigate diverse terrains using a
""blind"" policy, humanoid robots require accurate perception for stable
locomotion due to their high degrees of freedom and inherently unstable
morphology. However, incorporating perceptual signals often introduces
additional disturbances to the system, potentially reducing its robustness,
generalizability, and efficiency. This paper presents the Perceptive Internal
Model (PIM), which relies on onboard, continuously updated elevation maps
centered around the robot to perceive its surroundings. We train the policy
using ground-truth obstacle heights surrounding the robot in simulation,
optimizing it based on the Hybrid Internal Model (HIM), and perform inference
with heights sampled from the constructed elevation map. Unlike previous
methods that directly encode depth maps or raw point clouds, our approach
allows the robot to perceive the terrain beneath its feet clearly and is less
affected by camera movement or noise. Furthermore, since depth map rendering is
not required in simulation, our method introduces minimal additional
computational costs and can train the policy in 3 hours on an RTX 4090 GPU. We
verify the effectiveness of our method across various humanoid robots, various
indoor and outdoor terrains, stairs, and various sensor configurations. Our
method can enable a humanoid robot to continuously climb stairs and has the
potential to serve as a foundational algorithm for the development of future
humanoid control methods.","Long, Junfeng and Ren, Junli and Shi, Moji and Wang, Zirui and Huang, Tao and Luo, Ping and Pang, Jiangmiao",2024,,,,arXiv
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,he2025attention,\cite{he2025attention},Attention-Based Map Encoding for Learning Generalized Legged Locomotion,http://arxiv.org/abs/2506.09588v1,"Dynamic locomotion of legged robots is a critical yet challenging topic in
expanding the operational range of mobile robots. It requires precise planning
when possible footholds are sparse, robustness against uncertainties and
disturbances, and generalizability across diverse terrains. While traditional
model-based controllers excel at planning on complex terrains, they struggle
with real-world uncertainties. Learning-based controllers offer robustness to
such uncertainties but often lack precision on terrains with sparse steppable
areas. Hybrid methods achieve enhanced robustness on sparse terrains by
combining both methods but are computationally demanding and constrained by the
inherent limitations of model-based planners. To achieve generalized legged
locomotion on diverse terrains while preserving the robustness of
learning-based controllers, this paper proposes to learn an attention-based map
encoding conditioned on robot proprioception, which is trained as part of the
end-to-end controller using reinforcement learning. We show that the network
learns to focus on steppable areas for future footholds when the robot
dynamically navigates diverse and challenging terrains. We synthesize behaviors
that exhibit robustness against uncertainties while enabling precise and agile
traversal of sparse terrains. Additionally, our method offers a way to
interpret the topographical perception of a neural network. We have trained two
controllers for a 12-DoF quadrupedal robot and a 23-DoF humanoid robot
respectively and tested the resulting controllers in the real world under
various challenging indoor and outdoor scenarios, including ones unseen during
training.","He, Junzhe and Zhang, Chong and Jenelten, Fabian and Grandia, Ruben and B{\""a}cher, Moritz and Hutter, Marco",2025,,,,Science Robotics
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,he2025learning,\cite{he2025learning},Learning Getting-Up Policies for Real-World Humanoid Robots,http://arxiv.org/abs/2502.12152v2,"Automatic fall recovery is a crucial prerequisite before humanoid robots can
be reliably deployed. Hand-designing controllers for getting up is difficult
because of the varied configurations a humanoid can end up in after a fall and
the challenging terrains humanoid robots are expected to operate on. This paper
develops a learning framework to produce controllers that enable humanoid
robots to get up from varying configurations on varying terrains. Unlike
previous successful applications of learning to humanoid locomotion, the
getting-up task involves complex contact patterns (which necessitates
accurately modeling of the collision geometry) and sparser rewards. We address
these challenges through a two-phase approach that induces a curriculum. The
first stage focuses on discovering a good getting-up trajectory under minimal
constraints on smoothness or speed / torque limits. The second stage then
refines the discovered motions into deployable (i.e. smooth and slow) motions
that are robust to variations in initial configuration and terrains. We find
these innovations enable a real-world G1 humanoid robot to get up from two main
situations that we considered: a) lying face up and b) lying face down, both
tested on flat, deformable, slippery surfaces and slopes (e.g., sloppy grass
and snowfield). This is one of the first successful demonstrations of learned
getting-up policies for human-sized humanoid robots in the real world.","He, Xialin and Dong, Runpei and Chen, Zixuan and Gupta, Saurabh",2025,,,,RSS
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,kuang2025skillblender,\cite{kuang2025skillblender},"SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation
  via Skill Blending",http://arxiv.org/abs/2506.09366v1,"Humanoid robots hold significant potential in accomplishing daily tasks
across diverse environments thanks to their flexibility and human-like
morphology. Recent works have made significant progress in humanoid whole-body
control and loco-manipulation leveraging optimal control or reinforcement
learning. However, these methods require tedious task-specific tuning for each
task to achieve satisfactory behaviors, limiting their versatility and
scalability to diverse tasks in daily scenarios. To that end, we introduce
SkillBlender, a novel hierarchical reinforcement learning framework for
versatile humanoid loco-manipulation. SkillBlender first pretrains
goal-conditioned task-agnostic primitive skills, and then dynamically blends
these skills to accomplish complex loco-manipulation tasks with minimal
task-specific reward engineering. We also introduce SkillBench, a parallel,
cross-embodiment, and diverse simulated benchmark containing three embodiments,
four primitive skills, and eight challenging loco-manipulation tasks,
accompanied by a set of scientific evaluation metrics balancing accuracy and
feasibility. Extensive simulated experiments show that our method significantly
outperforms all baselines, while naturally regularizing behaviors to avoid
reward hacking, resulting in more accurate and feasible movements for diverse
loco-manipulation tasks in our daily scenarios. Our code and benchmark will be
open-sourced to the community to facilitate future research. Project page:
https://usc-gvl.github.io/SkillBlender-web/.","Kuang, Yuxuan and Geng, Haoran and Elhafsi, Amine and Do, Tan-Dzung and Abbeel, Pieter and Malik, Jitendra and Pavone, Marco and Wang, Yue",2025,,,,arXiv
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,zhang2025unleashing,\cite{zhang2025unleashing},Unleashing Humanoid Reaching Potential via Real-world-Ready Skill Space,http://arxiv.org/abs/2505.10918v1,"Humans possess a large reachable space in the 3D world, enabling interaction
with objects at varying heights and distances. However, realizing such
large-space reaching on humanoids is a complex whole-body control problem and
requires the robot to master diverse skills simultaneously-including base
positioning and reorientation, height and body posture adjustments, and
end-effector pose control. Learning from scratch often leads to optimization
difficulty and poor sim2real transferability. To address this challenge, we
propose Real-world-Ready Skill Space (R2S2). Our approach begins with a
carefully designed skill library consisting of real-world-ready primitive
skills. We ensure optimal performance and robust sim2real transfer through
individual skill tuning and sim2real evaluation. These skills are then
ensembled into a unified latent space, serving as a structured prior that helps
task execution in an efficient and sim2real transferable manner. A high-level
planner, trained to sample skills from this space, enables the robot to
accomplish real-world goal-reaching tasks. We demonstrate zero-shot sim2real
transfer and validate R2S2 in multiple challenging goal-reaching scenarios.","Zhang, Zhikai and Chen, Chao and Xue, Han and Wang, Jilong and Liang, Sikai and Liu, Yun and Zhang, Zongzhang and Wang, He and Yi, Li",2025,,,,arXiv preprint arXiv:2505.10918
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,xue2025unified,\cite{xue2025unified},"A Unified and General Humanoid Whole-Body Controller for Versatile
  Locomotion",http://arxiv.org/abs/2502.03206v3,"Locomotion is a fundamental skill for humanoid robots. However, most existing
works make locomotion a single, tedious, unextendable, and unconstrained
movement. This limits the kinematic capabilities of humanoid robots. In
contrast, humans possess versatile athletic abilities-running, jumping,
hopping, and finely adjusting gait parameters such as frequency and foot
height. In this paper, we investigate solutions to bring such versatility into
humanoid locomotion and thereby propose HugWBC: a unified and general humanoid
whole-body controller for versatile locomotion. By designing a general command
space in the aspect of tasks and behaviors, along with advanced techniques like
symmetrical loss and intervention training for learning a whole-body humanoid
controlling policy in simulation, HugWBC enables real-world humanoid robots to
produce various natural gaits, including walking, jumping, standing, and
hopping, with customizable parameters such as frequency, foot swing height,
further combined with different body height, waist rotation, and body pitch.
Beyond locomotion, HugWBC also supports real-time interventions from external
upper-body controllers like teleoperation, enabling loco-manipulation with
precision under any locomotive behavior. Extensive experiments validate the
high tracking accuracy and robustness of HugWBC with/without upper-body
intervention for all commands, and we further provide an in-depth analysis of
how the various commands affect humanoid movement and offer insights into the
relationships between these commands. To our knowledge, HugWBC is the first
humanoid whole-body controller that supports such versatile locomotion
behaviors with high robustness and flexibility.","Xue, Yufei and Dong, Wentao and Liu, Minghuan and Zhang, Weinan and Pang, Jiangmiao",2025,,,,RSS
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,zhang2406wococo,\cite{zhang2406wococo},WoCoCo: Learning Whole-Body Humanoid Control with Sequential Contacts,http://arxiv.org/abs/2406.06005v2,"Humanoid activities involving sequential contacts are crucial for complex
robotic interactions and operations in the real world and are traditionally
solved by model-based motion planning, which is time-consuming and often relies
on simplified dynamics models. Although model-free reinforcement learning (RL)
has become a powerful tool for versatile and robust whole-body humanoid
control, it still requires tedious task-specific tuning and state machine
design and suffers from long-horizon exploration issues in tasks involving
contact sequences. In this work, we propose WoCoCo (Whole-Body Control with
Sequential Contacts), a unified framework to learn whole-body humanoid control
with sequential contacts by naturally decomposing the tasks into separate
contact stages. Such decomposition facilitates simple and general policy
learning pipelines through task-agnostic reward and sim-to-real designs,
requiring only one or two task-related terms to be specified for each task. We
demonstrated that end-to-end RL-based controllers trained with WoCoCo enable
four challenging whole-body humanoid tasks involving diverse contact sequences
in the real world without any motion priors: 1) versatile parkour jumping, 2)
box loco-manipulation, 3) dynamic clap-and-tap dancing, and 4) cliffside
climbing. We further show that WoCoCo is a general framework beyond humanoid by
applying it in 22-DoF dinosaur robot loco-manipulation tasks.","Zhang, Chong and Xiao, Wenli and He, Tairan and Shi, Guanya",2024,,,,arXiv
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,zhang2025falcon,\cite{zhang2025falcon},FALCON: Learning Force-Adaptive Humanoid Loco-Manipulation,http://arxiv.org/abs/2505.06776v1,"Humanoid loco-manipulation holds transformative potential for daily service
and industrial tasks, yet achieving precise, robust whole-body control with 3D
end-effector force interaction remains a major challenge. Prior approaches are
often limited to lightweight tasks or quadrupedal/wheeled platforms. To
overcome these limitations, we propose FALCON, a dual-agent
reinforcement-learning-based framework for robust force-adaptive humanoid
loco-manipulation. FALCON decomposes whole-body control into two specialized
agents: (1) a lower-body agent ensuring stable locomotion under external force
disturbances, and (2) an upper-body agent precisely tracking end-effector
positions with implicit adaptive force compensation. These two agents are
jointly trained in simulation with a force curriculum that progressively
escalates the magnitude of external force exerted on the end effector while
respecting torque limits. Experiments demonstrate that, compared to the
baselines, FALCON achieves 2x more accurate upper-body joint tracking, while
maintaining robust locomotion under force disturbances and achieving faster
training convergence. Moreover, FALCON enables policy training without
embodiment-specific reward or curriculum tuning. Using the same training setup,
we obtain policies that are deployed across multiple humanoids, enabling
forceful loco-manipulation tasks such as transporting payloads (0-20N force),
cart-pulling (0-100N), and door-opening (0-40N) in the real world.","Zhang, Yuanhang and Yuan, Yifu and Gurunath, Prajwal and He, Tairan and Omidshafiei, Shayegan and Agha-mohammadi, Ali-akbar and Vazquez-Chanlatte, Marcell and Pedersen, Liam and Shi, Guanya",2025,,,,arXiv
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,peng2018deepmimic,\cite{peng2018deepmimic},"DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based
  Character Skills",http://arxiv.org/abs/1804.02717v3,"A longstanding goal in character animation is to combine data-driven
specification of behavior with a system that can execute a similar behavior in
a physical simulation, thus enabling realistic responses to perturbations and
environmental variation. We show that well-known reinforcement learning (RL)
methods can be adapted to learn robust control policies capable of imitating a
broad range of example motion clips, while also learning complex recoveries,
adapting to changes in morphology, and accomplishing user-specified goals. Our
method handles keyframed motions, highly-dynamic actions such as
motion-captured flips and spins, and retargeted motions. By combining a
motion-imitation objective with a task objective, we can train characters that
react intelligently in interactive settings, e.g., by walking in a desired
direction or throwing a ball at a user-specified target. This approach thus
combines the convenience and motion quality of using motion clips to define the
desired style and appearance, with the flexibility and generality afforded by
RL methods and physics-based animation. We further explore a number of methods
for integrating multiple clips into the learning process to develop
multi-skilled agents capable of performing a rich repertoire of diverse skills.
We demonstrate results using multiple characters (human, Atlas robot, bipedal
dinosaur, dragon) and a large variety of skills, including locomotion,
acrobatics, and martial arts.","Peng, Xue Bin and Abbeel, Pieter and Levine, Sergey and Van de Panne, Michiel",2018,,,,ACM Transactions On Graphics (TOG)
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,zhang2025hub,\cite{zhang2025hub},HuB: Learning Extreme Humanoid Balance,http://arxiv.org/abs/2505.07294v2,"The human body demonstrates exceptional motor capabilities-such as standing
steadily on one foot or performing a high kick with the leg raised over 1.5
meters-both requiring precise balance control. While recent research on
humanoid control has leveraged reinforcement learning to track human motions
for skill acquisition, applying this paradigm to balance-intensive tasks
remains challenging. In this work, we identify three key obstacles: instability
from reference motion errors, learning difficulties due to morphological
mismatch, and the sim-to-real gap caused by sensor noise and unmodeled
dynamics. To address these challenges, we propose HuB (Humanoid Balance), a
unified framework that integrates reference motion refinement, balance-aware
policy learning, and sim-to-real robustness training, with each component
targeting a specific challenge. We validate our approach on the Unitree G1
humanoid robot across challenging quasi-static balance tasks, including extreme
single-legged poses such as Swallow Balance and Bruce Lee's Kick. Our policy
remains stable even under strong physical disturbances-such as a forceful
soccer strike-while baseline methods consistently fail to complete these tasks.
Project website: https://hub-robot.github.io","Zhang, Tong and Zheng, Boyuan and Nai, Ruiqian and Hu, Yingdong and Wang, Yen-Jen and Chen, Geng and Lin, Fanqi and Li, Jiongye and Hong, Chuye and Sreenath, Koushil and others",2025,,,,CoRL
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,li2025reinforcement,\cite{li2025reinforcement},"Reinforcement Learning for Versatile, Dynamic, and Robust Bipedal
  Locomotion Control",http://arxiv.org/abs/2401.16889v2,"This paper presents a comprehensive study on using deep reinforcement
learning (RL) to create dynamic locomotion controllers for bipedal robots.
Going beyond focusing on a single locomotion skill, we develop a general
control solution that can be used for a range of dynamic bipedal skills, from
periodic walking and running to aperiodic jumping and standing. Our RL-based
controller incorporates a novel dual-history architecture, utilizing both a
long-term and short-term input/output (I/O) history of the robot. This control
architecture, when trained through the proposed end-to-end RL approach,
consistently outperforms other methods across a diverse range of skills in both
simulation and the real world. The study also delves into the adaptivity and
robustness introduced by the proposed RL system in developing locomotion
controllers. We demonstrate that the proposed architecture can adapt to both
time-invariant dynamics shifts and time-variant changes, such as contact
events, by effectively using the robot's I/O history. Additionally, we identify
task randomization as another key source of robustness, fostering better task
generalization and compliance to disturbances. The resulting control policies
can be successfully deployed on Cassie, a torque-controlled human-sized bipedal
robot. This work pushes the limits of agility for bipedal robots through
extensive real-world experiments. We demonstrate a diverse range of locomotion
skills, including: robust standing, versatile walking, fast running with a
demonstration of a 400-meter dash, and a diverse set of jumping skills, such as
standing long jumps and high jumps.","Li, Zhongyu and Peng, Xue Bin and Abbeel, Pieter and Levine, Sergey and Berseth, Glen and Sreenath, Koushil",2025,,,,IJRR
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,liao2025beyondmimic,\cite{liao2025beyondmimic},"BeyondMimic: From Motion Tracking to Versatile Humanoid Control via
  Guided Diffusion",http://arxiv.org/abs/2508.08241v3,"Learning skills from human motions offers a promising path toward
generalizable policies for versatile humanoid whole-body control, yet two key
cornerstones are missing: (1) a high-quality motion tracking framework that
faithfully transforms large-scale kinematic references into robust and
extremely dynamic motions on real hardware, and (2) a distillation approach
that can effectively learn these motion primitives and compose them to solve
downstream tasks. We address these gaps with BeyondMimic, a real-world
framework to learn from human motions for versatile and naturalistic humanoid
control via guided diffusion. Our framework provides a motion tracking pipeline
capable of challenging skills such as jumping spins, sprinting, and cartwheels
with state-of-the-art motion quality. Moving beyond simply mimicking existing
motions, we further introduce a unified diffusion policy that enables zero-shot
task-specific control at test time using simple cost functions. Deployed on
hardware, BeyondMimic performs diverse tasks at test time, including waypoint
navigation, joystick teleoperation, and obstacle avoidance, bridging
sim-to-real motion tracking and flexible synthesis of human motion primitives
for whole-body control. https://beyondmimic.github.io/.","Liao, Qiayuan and Truong, Takara E and Huang, Xiaoyu and Tevet, Guy and Sreenath, Koushil and Liu, C Karen",2025,,,,arXiv e-prints
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,unitree_lafan1_retargeting_dataset,\cite{unitree_lafan1_retargeting_dataset},Unitree LAFAN1 Retargeting Dataset,,,Unitree Robotics and Contributors,2025,,,,
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,xu2025parc,\cite{xu2025parc},"PARC: Physics-based Augmentation with Reinforcement Learning for
  Character Controllers",http://arxiv.org/abs/2505.04002v1,"Humans excel in navigating diverse, complex environments with agile motor
skills, exemplified by parkour practitioners performing dynamic maneuvers, such
as climbing up walls and jumping across gaps. Reproducing these agile movements
with simulated characters remains challenging, in part due to the scarcity of
motion capture data for agile terrain traversal behaviors and the high cost of
acquiring such data. In this work, we introduce PARC (Physics-based
Augmentation with Reinforcement Learning for Character Controllers), a
framework that leverages machine learning and physics-based simulation to
iteratively augment motion datasets and expand the capabilities of terrain
traversal controllers. PARC begins by training a motion generator on a small
dataset consisting of core terrain traversal skills. The motion generator is
then used to produce synthetic data for traversing new terrains. However, these
generated motions often exhibit artifacts, such as incorrect contacts or
discontinuities. To correct these artifacts, we train a physics-based tracking
controller to imitate the motions in simulation. The corrected motions are then
added to the dataset, which is used to continue training the motion generator
in the next iteration. PARC's iterative process jointly expands the
capabilities of the motion generator and tracker, creating agile and versatile
models for interacting with complex environments. PARC provides an effective
approach to develop controllers for agile terrain traversal, which bridges the
gap between the scarcity of motion data and the need for versatile character
controllers.","Xu, Michael and Shi, Yi and Yin, KangKang and Peng, Xue Bin",2025,,,,
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,wu2024human,\cite{wu2024human},Human-Object Interaction from Human-Level Instructions,http://arxiv.org/abs/2406.17840v3,"Intelligent agents must autonomously interact with the environments to
perform daily tasks based on human-level instructions. They need a foundational
understanding of the world to accurately interpret these instructions, along
with precise low-level movement and interaction skills to execute the derived
actions. In this work, we propose the first complete system for synthesizing
physically plausible, long-horizon human-object interactions for object
manipulation in contextual environments, driven by human-level instructions. We
leverage large language models (LLMs) to interpret the input instructions into
detailed execution plans. Unlike prior work, our system is capable of
generating detailed finger-object interactions, in seamless coordination with
full-body movements. We also train a policy to track generated motions in
physics simulation via reinforcement learning (RL) to ensure physical
plausibility of the motion. Our experiments demonstrate the effectiveness of
our system in synthesizing realistic interactions with diverse objects in
complex environments, highlighting its potential for real-world applications.","Wu, Zhen and Li, Jiaman and Xu, Pei and Liu, C Karen",2024,,,,arXiv preprint arXiv:2406.17840
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,seo2023deep,\cite{seo2023deep},"Deep Imitation Learning for Humanoid Loco-manipulation through Human
  Teleoperation",http://arxiv.org/abs/2309.01952v2,"We tackle the problem of developing humanoid loco-manipulation skills with
deep imitation learning. The difficulty of collecting task demonstrations and
training policies for humanoids with a high degree of freedom presents
substantial challenges. We introduce TRILL, a data-efficient framework for
training humanoid loco-manipulation policies from human demonstrations. In this
framework, we collect human demonstration data through an intuitive Virtual
Reality (VR) interface. We employ the whole-body control formulation to
transform task-space commands by human operators into the robot's joint-torque
actuation while stabilizing its dynamics. By employing high-level action
abstractions tailored for humanoid loco-manipulation, our method can
efficiently learn complex sensorimotor skills. We demonstrate the effectiveness
of TRILL in simulation and on a real-world robot for performing various
loco-manipulation tasks. Videos and additional materials can be found on the
project page: https://ut-austin-rpl.github.io/TRILL.","Seo, Mingyo and Han, Steve and Sim, Kyutae and Bang, Seung Hyeon and Gonzalez, Carlos and Sentis, Luis and Zhu, Yuke",2023,,,,
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,fu2024humanplus,\cite{fu2024humanplus},HumanPlus: Humanoid Shadowing and Imitation from Humans,http://arxiv.org/abs/2406.10454v1,"One of the key arguments for building robots that have similar form factors
to human beings is that we can leverage the massive human data for training.
Yet, doing so has remained challenging in practice due to the complexities in
humanoid perception and control, lingering physical gaps between humanoids and
humans in morphologies and actuation, and lack of a data pipeline for humanoids
to learn autonomous skills from egocentric vision. In this paper, we introduce
a full-stack system for humanoids to learn motion and autonomous skills from
human data. We first train a low-level policy in simulation via reinforcement
learning using existing 40-hour human motion datasets. This policy transfers to
the real world and allows humanoid robots to follow human body and hand motion
in real time using only a RGB camera, i.e. shadowing. Through shadowing, human
operators can teleoperate humanoids to collect whole-body data for learning
different tasks in the real world. Using the data collected, we then perform
supervised behavior cloning to train skill policies using egocentric vision,
allowing humanoids to complete different tasks autonomously by imitating human
skills. We demonstrate the system on our customized 33-DoF 180cm humanoid,
autonomously completing tasks such as wearing a shoe to stand up and walk,
unloading objects from warehouse racks, folding a sweatshirt, rearranging
objects, typing, and greeting another robot with 60-100% success rates using up
to 40 demonstrations. Project website: https://humanoid-ai.github.io/","Fu, Zipeng and Zhao, Qingqing and Wu, Qi and Wetzstein, Gordon and Finn, Chelsea",2024,,,,CoRL
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,ben2025homie,\cite{ben2025homie},HOMIE: Humanoid Loco-Manipulation with Isomorphic Exoskeleton Cockpit,http://arxiv.org/abs/2502.13013v2,"Generalizable humanoid loco-manipulation poses significant challenges,
requiring coordinated whole-body control and precise, contact-rich object
manipulation. To address this, this paper introduces HOMIE, a semi-autonomous
teleoperation system that combines a reinforcement learning policy for body
control mapped to a pedal, an isomorphic exoskeleton arm for arm control, and
motion-sensing gloves for hand control, forming a unified cockpit to freely
operate humanoids and establish a data flywheel. The policy incorporates novel
designs, including an upper-body pose curriculum, a height-tracking reward, and
symmetry utilization. These features enable the system to perform walking and
squatting to specific heights while seamlessly adapting to arbitrary upper-body
poses. The exoskeleton, by eliminating the reliance on inverse dynamics,
delivers faster and more precise arm control. The gloves utilize Hall sensors
instead of servos, allowing even compact devices to achieve 15 or more degrees
of freedom and freely adapt to any model of dexterous hands. Compared to
previous teleoperation systems, HOMIE stands out for its exceptional
efficiency, completing tasks in half the time; its expanded working range,
allowing users to freely reach high and low areas as well as interact with any
objects; and its affordability, with a price of just $500. The system is fully
open-source, demos and code can be found in our https://homietele.github.io/.","Ben, Qingwei and Jia, Feiyu and Zeng, Jia and Dong, Junting and Lin, Dahua and Pang, Jiangmiao",2025,,,,RSS
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,zhang2024diffusion,\cite{zhang2024diffusion},Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning,http://arxiv.org/abs/2402.17768v2,"A common failure mode for policies trained with imitation is compounding
execution errors at test time. When the learned policy encounters states that
are not present in the expert demonstrations, the policy fails, leading to
degenerate behavior. The Dataset Aggregation, or DAgger approach to this
problem simply collects more data to cover these failure states. However, in
practice, this is often prohibitively expensive. In this work, we propose
Diffusion Meets DAgger (DMD), a method to reap the benefits of DAgger without
the cost for eye-in-hand imitation learning problems. Instead of collecting new
samples to cover out-of-distribution states, DMD uses recent advances in
diffusion models to synthesize these samples. This leads to robust performance
from few demonstrations. We compare DMD against behavior cloning baseline
across four tasks: pushing, stacking, pouring, and shirt hanging. In pushing,
DMD achieves 80% success rate with as few as 8 expert demonstrations, where
naive behavior cloning reaches only 20%. In stacking, DMD succeeds on average
92% of the time across 5 cups, versus 40% for BC. When pouring coffee beans,
DMD transfers to another cup successfully 80% of the time. Finally, DMD attains
90% success rate for hanging shirt on a clothing rack.","Zhang, Xiaoyu and Chang, Matthew and Kumar, Pranav and Gupta, Saurabh",2024,,,,
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,tian2024view,\cite{tian2024view},View-Invariant Policy Learning via Zero-Shot Novel View Synthesis,http://arxiv.org/abs/2409.03685v3,"Large-scale visuomotor policy learning is a promising approach toward
developing generalizable manipulation systems. Yet, policies that can be
deployed on diverse embodiments, environments, and observational modalities
remain elusive. In this work, we investigate how knowledge from large-scale
visual data of the world may be used to address one axis of variation for
generalizable manipulation: observational viewpoint. Specifically, we study
single-image novel view synthesis models, which learn 3D-aware scene-level
priors by rendering images of the same scene from alternate camera viewpoints
given a single input image. For practical application to diverse robotic data,
these models must operate zero-shot, performing view synthesis on unseen tasks
and environments. We empirically analyze view synthesis models within a simple
data-augmentation scheme that we call View Synthesis Augmentation (VISTA) to
understand their capabilities for learning viewpoint-invariant policies from
single-viewpoint demonstration data. Upon evaluating the robustness of policies
trained with our method to out-of-distribution camera viewpoints, we find that
they outperform baselines in both simulated and real-world manipulation tasks.
Videos and additional visualizations are available at
https://s-tian.github.io/projects/vista.","Tian, Stephen and Wulfe, Blake and Sargent, Kyle and Liu, Katherine and Zakharov, Sergey and Guizilini, Vitor Campagnolo and Wu, Jiajun",2025,,,,
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,chen2024rovi,\cite{chen2024rovi},RoVi-Aug: Robot and Viewpoint Augmentation for Cross-Embodiment Robot Learning,,,"Chen, Lawrence Yunliang and Xu, Chenfeng and Dharmarajan, Karthik and Irshad, Zubair and Cheng, Richard and Keutzer, Kurt and Tomizuka, Masayoshi and Vuong, Quan and Goldberg, Ken",2024,,,,
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,mandi2022cacti,\cite{mandi2022cacti},"CACTI: A Framework for Scalable Multi-Task Multi-Scene Visual Imitation
  Learning",http://arxiv.org/abs/2212.05711v2,"Large-scale training have propelled significant progress in various
sub-fields of AI such as computer vision and natural language processing.
However, building robot learning systems at a comparable scale remains
challenging. To develop robots that can perform a wide range of skills and
adapt to new scenarios, efficient methods for collecting vast and diverse
amounts of data on physical robot systems are required, as well as the
capability to train high-capacity policies using such datasets. In this work,
we propose a framework for scaling robot learning, with specific focus on
multi-task and multi-scene manipulation in kitchen environments, both in
simulation and in the real world. Our proposed framework, CACTI, comprises four
stages that separately handle: data collection, data augmentation, visual
representation learning, and imitation policy training, to enable scalability
in robot learning . We make use of state-of-the-art generative models as part
of the data augmentation stage, and use pre-trained out-of-domain visual
representations to improve training efficiency. Experimental results
demonstrate the effectiveness of our approach. On a real robot setup, CACTI
enables efficient training of a single policy that can perform 10 manipulation
tasks involving kitchen objects, and is robust to varying layouts of
distractors. In a simulated kitchen environment, CACTI trains a single policy
to perform 18 semantic tasks across 100 layout variations for each individual
task. We will release the simulation task benchmark and augmented datasets in
both real and simulated environments to facilitate future research.","Mandi, Zhao and Bharadhwaj, Homanga and Moens, Vincent and Song, Shuran and Rajeswaran, Aravind and Kumar, Vikash",2022,,,,arXiv preprint arXiv:2212.05711
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,chen2023genaug,\cite{chen2023genaug},"GenAug: Retargeting behaviors to unseen situations via Generative
  Augmentation",http://arxiv.org/abs/2302.06671v2,"Robot learning methods have the potential for widespread generalization
across tasks, environments, and objects. However, these methods require large
diverse datasets that are expensive to collect in real-world robotics settings.
For robot learning to generalize, we must be able to leverage sources of data
or priors beyond the robot's own experience. In this work, we posit that
image-text generative models, which are pre-trained on large corpora of
web-scraped data, can serve as such a data source. We show that despite these
generative models being trained on largely non-robotics data, they can serve as
effective ways to impart priors into the process of robot learning in a way
that enables widespread generalization. In particular, we show how pre-trained
generative models can serve as effective tools for semantically meaningful data
augmentation. By leveraging these pre-trained models for generating appropriate
""semantic"" data augmentations, we propose a system GenAug that is able to
significantly improve policy generalization. We apply GenAug to tabletop
manipulation tasks, showing the ability to re-target behavior to novel
scenarios, while only requiring marginal amounts of real-world data. We
demonstrate the efficacy of this system on a number of object manipulation
problems in the real world, showing a 40% improvement in generalization to
novel scenes and objects.","Chen, Zoey and Kiami, Sho and Gupta, Abhishek and Kumar, Vikash",2023,,,,RSS
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,yu2023scaling,\cite{yu2023scaling},Scaling Robot Learning with Semantically Imagined Experience,http://arxiv.org/abs/2302.11550v1,"Recent advances in robot learning have shown promise in enabling robots to
perform a variety of manipulation tasks and generalize to novel scenarios. One
of the key contributing factors to this progress is the scale of robot data
used to train the models. To obtain large-scale datasets, prior approaches have
relied on either demonstrations requiring high human involvement or
engineering-heavy autonomous data collection schemes, both of which are
challenging to scale. To mitigate this issue, we propose an alternative route
and leverage text-to-image foundation models widely used in computer vision and
natural language processing to obtain meaningful data for robot learning
without requiring additional robot data. We term our method Robot Learning with
Semantically Imagened Experience (ROSIE). Specifically, we make use of the
state of the art text-to-image diffusion models and perform aggressive data
augmentation on top of our existing robotic manipulation datasets via
inpainting various unseen objects for manipulation, backgrounds, and
distractors with text guidance. Through extensive real-world experiments, we
show that manipulation policies trained on data augmented this way are able to
solve completely unseen tasks with new objects and can behave more robustly
w.r.t. novel distractors. In addition, we find that we can improve the
robustness and generalization of high-level robot learning tasks such as
success detection through training with the diffusion-based data augmentation.
The project's website and videos can be found at diffusion-rosie.github.io","Yu, Tianhe and Xiao, Ted and Stone, Austin and Tompson, Jonathan and Brohan, Anthony and Wang, Su and Singh, Jaspiar and Tan, Clayton and Peralta, Jodilyn and Ichter, Brian and others",2023,,,,RSS
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,mandlekar2023mimicgen,\cite{mandlekar2023mimicgen},"MimicGen: A Data Generation System for Scalable Robot Learning using
  Human Demonstrations",http://arxiv.org/abs/2310.17596v1,"Imitation learning from a large set of human demonstrations has proved to be
an effective paradigm for building capable robot agents. However, the
demonstrations can be extremely costly and time-consuming to collect. We
introduce MimicGen, a system for automatically synthesizing large-scale, rich
datasets from only a small number of human demonstrations by adapting them to
new contexts. We use MimicGen to generate over 50K demonstrations across 18
tasks with diverse scene configurations, object instances, and robot arms from
just ~200 human demonstrations. We show that robot agents can be effectively
trained on this generated dataset by imitation learning to achieve strong
performance in long-horizon and high-precision tasks, such as multi-part
assembly and coffee preparation, across broad initial state distributions. We
further demonstrate that the effectiveness and utility of MimicGen data compare
favorably to collecting additional human demonstrations, making it a powerful
and economical approach towards scaling up robot learning. Datasets, simulation
environments, videos, and more at https://mimicgen.github.io .","Mandlekar, Ajay and Nasiriany, Soroush and Wen, Bowen and Akinola, Iretiayo and Narang, Yashraj and Fan, Linxi and Zhu, Yuke and Fox, Dieter",2023,,,,
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,jiang2024dexmimicgen,\cite{jiang2024dexmimicgen},"DexMimicGen: Automated Data Generation for Bimanual Dexterous
  Manipulation via Imitation Learning",http://arxiv.org/abs/2410.24185v2,"Imitation learning from human demonstrations is an effective means to teach
robots manipulation skills. But data acquisition is a major bottleneck in
applying this paradigm more broadly, due to the amount of cost and human effort
involved. There has been significant interest in imitation learning for
bimanual dexterous robots, like humanoids. Unfortunately, data collection is
even more challenging here due to the challenges of simultaneously controlling
multiple arms and multi-fingered hands. Automated data generation in simulation
is a compelling, scalable alternative to fuel this need for data. To this end,
we introduce DexMimicGen, a large-scale automated data generation system that
synthesizes trajectories from a handful of human demonstrations for humanoid
robots with dexterous hands. We present a collection of simulation environments
in the setting of bimanual dexterous manipulation, spanning a range of
manipulation behaviors and different requirements for coordination among the
two arms. We generate 21K demos across these tasks from just 60 source human
demos and study the effect of several data generation and policy learning
decisions on agent performance. Finally, we present a real-to-sim-to-real
pipeline and deploy it on a real-world humanoid can sorting task. Generated
datasets, simulation environments and additional results are at
https://dexmimicgen.github.io/","Jiang, Zhenyu and Xie, Yuqi and Lin, Kevin and Xu, Zhenjia and Wan, Weikang and Mandlekar, Ajay and Fan, Linxi and Zhu, Yuke",2025,,,,ICRA
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,garrett2024skillmimicgen,\cite{garrett2024skillmimicgen},"SkillMimicGen: Automated Demonstration Generation for Efficient Skill
  Learning and Deployment",http://arxiv.org/abs/2410.18907v1,"Imitation learning from human demonstrations is an effective paradigm for
robot manipulation, but acquiring large datasets is costly and
resource-intensive, especially for long-horizon tasks. To address this issue,
we propose SkillMimicGen (SkillGen), an automated system for generating
demonstration datasets from a few human demos. SkillGen segments human demos
into manipulation skills, adapts these skills to new contexts, and stitches
them together through free-space transit and transfer motion. We also propose a
Hybrid Skill Policy (HSP) framework for learning skill initiation, control, and
termination components from SkillGen datasets, enabling skills to be sequenced
using motion planning at test-time. We demonstrate that SkillGen greatly
improves data generation and policy learning performance over a
state-of-the-art data generation framework, resulting in the capability to
produce data for large scene variations, including clutter, and agents that are
on average 24% more successful. We demonstrate the efficacy of SkillGen by
generating over 24K demonstrations across 18 task variants in simulation from
just 60 human demonstrations, and training proficient, often near-perfect, HSP
agents. Finally, we apply SkillGen to 3 real-world manipulation tasks and also
demonstrate zero-shot sim-to-real transfer on a long-horizon assembly task.
Videos, and more at https://skillgen.github.io.","Garrett, Caelan and Mandlekar, Ajay and Wen, Bowen and Fox, Dieter",2024,,,,
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,yang2025physics,\cite{yang2025physics},Physics-driven data generation for contact-rich manipulation via trajectory optimization,,,"Yang, Lujie and Suh, HJ and Zhao, Tong and Graesdal, Bernhard Paus and Kelestemur, Tarik and Wang, Jiuguang and Pang, Tao and Tedrake, Russ",2025,,,,RSS
OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,http://arxiv.org/abs/2509.26633v2,starke2019neural,\cite{starke2019neural},Neural state machine for character-scene interactions,,,"Starke, Sebastian and Zhang, He and Komura, Taku and Saito, Jun",2019,,,,ACM Transactions on Graphics
