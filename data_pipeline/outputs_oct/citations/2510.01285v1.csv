parent_paper_title,parent_arxiv_link,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal
LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,http://arxiv.org/abs/2510.01285v1,10.5555/3618408.3619164,\cite{10.5555/3618408.3619164},DS-1000: a natural and reliable benchmark for data science code generation,,,"Lai, Yuhang and Li, Chengxi and Wang, Yiming and Zhang, Tianyi and Zhong, Ruiqi and Zettlemoyer, Luke and Yih, Wen-tau and Fried, Daniel and Wang, Sida and Yu, Tao",2023,,,,
LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,http://arxiv.org/abs/2510.01285v1,yin-etal-2023-natural,\cite{yin-etal-2023-natural},"Natural Language to Code Generation in Interactive Data Science
  Notebooks",http://arxiv.org/abs/2212.09248v1,"Computational notebooks, such as Jupyter notebooks, are interactive computing
environments that are ubiquitous among data scientists to perform data
wrangling and analytic tasks. To measure the performance of AI pair programmers
that automatically synthesize programs for those tasks given natural language
(NL) intents from users, we build ARCADE, a benchmark of 1082 code generation
problems using the pandas data analysis framework in data science notebooks.
ARCADE features multiple rounds of NL-to-code problems from the same notebook.
It requires a model to understand rich multi-modal contexts, such as existing
notebook cells and their execution states as well as previous turns of
interaction. To establish a strong baseline on this challenging task, we
develop PaChiNCo, a 62B code language model (LM) for Python computational
notebooks, which significantly outperforms public code LMs. Finally, we explore
few-shot prompting strategies to elicit better code with step-by-step
decomposition and NL explanation, showing the potential to improve the
diversity and explainability of model predictions.","Yin, Pengcheng  and
      Li, Wen-Ding  and
      Xiao, Kefan  and
      Rao, Abhishek  and
      Wen, Yeming  and
      Shi, Kensen  and
      Howland, Joshua  and
      Bailey, Paige  and
      Catasta, Michele  and
      Michalewski, Henryk  and
      Polozov, Oleksandr  and
      Sutton, Charles",2023,,https://aclanthology.org/2023.acl-long.9/,10.18653/v1/2023.acl-long.9,
LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,http://arxiv.org/abs/2510.01285v1,zhang2025datascibenchllmagentbenchmark,\cite{zhang2025datascibenchllmagentbenchmark},DataSciBench: An LLM Agent Benchmark for Data Science,http://arxiv.org/abs/2502.13897v1,"This paper presents DataSciBench, a comprehensive benchmark for evaluating
Large Language Model (LLM) capabilities in data science. Recent related
benchmarks have primarily focused on single tasks, easily obtainable ground
truth, and straightforward evaluation metrics, which limits the scope of tasks
that can be evaluated. In contrast, DataSciBench is constructed based on a more
comprehensive and curated collection of natural and challenging prompts for
uncertain ground truth and evaluation metrics. We develop a semi-automated
pipeline for generating ground truth (GT) and validating evaluation metrics.
This pipeline utilizes and implements an LLM-based self-consistency and human
verification strategy to produce accurate GT by leveraging collected prompts,
predefined task types, and aggregate functions (metrics). Furthermore, we
propose an innovative Task - Function - Code (TFC) framework to assess each
code execution outcome based on precisely defined metrics and programmatic
rules. Our experimental framework involves testing 6 API-based models, 8
open-source general models, and 9 open-source code generation models using the
diverse set of prompts we have gathered. This approach aims to provide a more
comprehensive and rigorous evaluation of LLMs in data science, revealing their
strengths and weaknesses. Experimental results demonstrate that API-based
models outperform open-sourced models on all metrics and
Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced
models. We release all code and data at https://github.com/THUDM/DataSciBench.",Dan Zhang and Sining Zhoubian and Min Cai and Fengzu Li and Lekang Yang and Wei Wang and Tianjiao Dong and Ziniu Hu and Jie Tang and Yisong Yue,2025,,https://arxiv.org/abs/2502.13897,,
LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,http://arxiv.org/abs/2510.01285v1,zhang-etal-2024-benchmarking-data,\cite{zhang-etal-2024-benchmarking-data},Benchmarking Data Science Agents,http://arxiv.org/abs/2402.17168v1,"In the era of data-driven decision-making, the complexity of data analysis
necessitates advanced expertise and tools of data science, presenting
significant challenges even for specialists. Large Language Models (LLMs) have
emerged as promising aids as data science agents, assisting humans in data
analysis and processing. Yet their practical efficacy remains constrained by
the varied demands of real-world applications and complicated analytical
process. In this paper, we introduce DSEval -- a novel evaluation paradigm, as
well as a series of innovative benchmarks tailored for assessing the
performance of these agents throughout the entire data science lifecycle.
Incorporating a novel bootstrapped annotation method, we streamline dataset
preparation, improve the evaluation coverage, and expand benchmarking
comprehensiveness. Our findings uncover prevalent obstacles and provide
critical insights to inform future advancements in the field.","Zhang, Yuge  and
      Jiang, Qiyang  and
      XingyuHan, XingyuHan  and
      Chen, Nan  and
      Yang, Yuqing  and
      Ren, Kan",2024,,https://aclanthology.org/2024.acl-long.308/,10.18653/v1/2024.acl-long.308,
LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,http://arxiv.org/abs/2510.01285v1,jimenez2024swebench,\cite{jimenez2024swebench},{SWE}-bench: Can Language Models Resolve Real-world Github Issues?,,,Carlos E Jimenez and John Yang and Alexander Wettig and Shunyu Yao and Kexin Pei and Ofir Press and Karthik R Narasimhan,2024,,https://openreview.net/forum?id=VTF8yNQM66,,
LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,http://arxiv.org/abs/2510.01285v1,tang2025mlbench,\cite{tang2025mlbench},{ML}-Bench: Evaluating Large Language Models for Code Generation in Repository-Level Machine Learning Tasks,,,Xiangru Tang and Yuliang Liu and Zefan Cai and Yanjun Shao and Junjie Lu and Yichi Zhang and Zexuan Deng and Helan Hu and Kaikai An and Ruijun Huang and Shuzheng Si and Chen Sheng and Haozhe Zhao and Liang Chen and Tianyu Liu and Yin Fang and Yujia Qin and Wangchunshu Zhou and Yilun Zhao and Zhiwei Jiang and Baobao Chang and Arman Cohan and Mark Gerstein,2025,,https://openreview.net/forum?id=sf1u3vTRjm,,
LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,http://arxiv.org/abs/2510.01285v1,zhuo2025bigcodebench,\cite{zhuo2025bigcodebench},"BigCodeBench: Benchmarking Code Generation with Diverse Function Calls
  and Complex Instructions",http://arxiv.org/abs/2406.15877v4,"Task automation has been greatly empowered by the recent advances in Large
Language Models (LLMs) via Python code, where the tasks ranging from software
engineering development to general-purpose reasoning. While current benchmarks
have shown that LLMs can solve tasks using programs like human developers, the
majority of their evaluations are limited to short and self-contained
algorithmic tasks or standalone function calls. Solving challenging and
practical tasks requires the capability of utilizing diverse function calls as
tools to efficiently implement functionalities like data analysis and web
development. In addition, using multiple tools to solve a task needs
compositional reasoning by accurately understanding complex instructions.
Fulfilling both of these characteristics can pose a great challenge for LLMs.To
assess how well LLMs can solve challenging and practical tasks via programs, we
introduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple
function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained
tasks. To evaluate LLMs rigorously, each task encompasses 5.6 test cases with
an average branch coverage of 99%. In addition, we propose a
natural-language-oriented variant of BigCodeBench, BigCodeBench-Instruct, that
automatically transforms the original docstrings into short instructions only
with essential information. Our extensive evaluation of 60 LLMs shows that LLMs
are not yet capable of following complex instructions to use function calls
precisely, with scores up to 60%, significantly lower than the human
performance of 97%. The results underscore the need for further advancements in
this area.",Terry Yue Zhuo and Vu Minh Chien and Jenny Chim and Han Hu and Wenhao Yu and Ratnadira Widyasari and Imam Nur Bani Yusuf and Haolan Zhan and Junda He and Indraneil Paul and Simon Brunner and Chen GONG and James Hoang and Armel Randy Zebaze and Xiaoheng Hong and Wen-Ding Li and Jean Kaddour and Ming Xu and Zhihan Zhang and Prateek Yadav and Naman Jain and Alex Gu and Zhoujun Cheng and Jiawei Liu and Qian Liu and Zijian Wang and David Lo and Binyuan Hui and Niklas Muennighoff and Daniel Fried and Xiaoning Du and Harm de Vries and Leandro Von Werra,2025,,https://openreview.net/forum?id=YrycTjllL0,,
LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,http://arxiv.org/abs/2510.01285v1,jing2025dsbench,\cite{jing2025dsbench},{DSB}ench: How Far Are Data Science Agents from Becoming Data Science Experts?,,,Liqiang Jing and Zhehui Huang and Xiaoyang Wang and Wenlin Yao and Wenhao Yu and Kaixin Ma and Hongming Zhang and Xinya Du and Dong Yu,2025,,https://openreview.net/forum?id=DSsSPr0RZJ,,
LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,http://arxiv.org/abs/2510.01285v1,gu-etal-2024-blade,\cite{gu-etal-2024-blade},BLADE: Benchmarking Language Model Agents for Data-Driven Science,http://arxiv.org/abs/2408.09667v2,"Data-driven scientific discovery requires the iterative integration of
scientific domain knowledge, statistical expertise, and an understanding of
data semantics to make nuanced analytical decisions, e.g., about which
variables, transformations, and statistical models to consider. LM-based agents
equipped with planning, memory, and code execution capabilities have the
potential to support data-driven science. However, evaluating agents on such
open-ended tasks is challenging due to multiple valid approaches, partially
correct steps, and different ways to express the same decisions. To address
these challenges, we present BLADE, a benchmark to automatically evaluate
agents' multifaceted approaches to open-ended research questions. BLADE
consists of 12 datasets and research questions drawn from existing scientific
literature, with ground truth collected from independent analyses by expert
data scientists and researchers. To automatically evaluate agent responses, we
developed corresponding computational methods to match different
representations of analyses to this ground truth. Though language models
possess considerable world knowledge, our evaluation shows that they are often
limited to basic analyses. However, agents capable of interacting with the
underlying data demonstrate improved, but still non-optimal, diversity in their
analytical decision making. Our work enables the evaluation of agents for
data-driven science and provides researchers deeper insights into agents'
analysis approaches.","Gu, Ken  and
      Shang, Ruoxi  and
      Jiang, Ruien  and
      Kuang, Keying  and
      Lin, Richard-John  and
      Lyu, Donghe  and
      Mao, Yue  and
      Pan, Youran  and
      Wu, Teng  and
      Yu, Jiaqian  and
      Zhang, Yikun  and
      Zhang, Tianmai M.  and
      Zhu, Lanyi  and
      Merrill, Mike A  and
      Heer, Jeffrey  and
      Althoff, Tim",2024,,https://aclanthology.org/2024.findings-emnlp.815/,10.18653/v1/2024.findings-emnlp.815,
LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,http://arxiv.org/abs/2510.01285v1,chen2025scienceagentbench,\cite{chen2025scienceagentbench},"ScienceAgentBench: Toward Rigorous Assessment of Language Agents for
  Data-Driven Scientific Discovery",http://arxiv.org/abs/2410.05080v3,"The advancements of large language models (LLMs) have piqued growing interest
in developing LLM-based language agents to automate scientific discovery
end-to-end, which has sparked both excitement and skepticism about their true
capabilities. In this work, we call for rigorous assessment of agents on
individual tasks in a scientific workflow before making bold claims on
end-to-end automation. To this end, we present ScienceAgentBench, a new
benchmark for evaluating language agents for data-driven scientific discovery.
To ensure the scientific authenticity and real-world relevance of our
benchmark, we extract 102 tasks from 44 peer-reviewed publications in four
disciplines and engage nine subject matter experts to validate them. We unify
the target output for every task to a self-contained Python program file and
employ an array of evaluation metrics to examine the generated programs,
execution results, and costs. Each task goes through multiple rounds of manual
validation by annotators and subject matter experts to ensure its annotation
quality and scientific plausibility. We also propose two effective strategies
to mitigate data contamination concerns. Using ScienceAgentBench, we evaluate
five open-weight and proprietary LLMs, each with three frameworks: direct
prompting, OpenHands CodeAct, and self-debug. Given three attempts for each
task, the best-performing agent can only solve 32.4% of the tasks independently
and 34.3% with expert-provided knowledge. In addition, we evaluate OpenAI
o1-preview with direct prompting and self-debug, which can boost the
performance to 42.2%, demonstrating the effectiveness of increasing
inference-time compute but with more than 10 times the cost of other LLMs.
Still, our results underscore the limitations of current language agents in
generating code for data-driven discovery, let alone end-to-end automation for
scientific research.",Ziru Chen and Shijie Chen and Yuting Ning and Qianheng Zhang and Boshi Wang and Botao Yu and Yifei Li and Zeyi Liao and Chen Wei and Zitong Lu and Vishal Dey and Mingyi Xue and Frazier N. Baker and Benjamin Burns and Daniel Adu-Ampratwum and Xuhui Huang and Xia Ning and Song Gao and Yu Su and Huan Sun,2025,,https://openreview.net/forum?id=6z4YKr0GK6,,
LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,http://arxiv.org/abs/2510.01285v1,mitchener2025bixbenchcomprehensivebenchmarkllmbased,\cite{mitchener2025bixbenchcomprehensivebenchmarkllmbased},"BixBench: a Comprehensive Benchmark for LLM-based Agents in
  Computational Biology",http://arxiv.org/abs/2503.00096v3,"Large Language Models (LLMs) and LLM-based agents show great promise in
accelerating scientific research. Existing benchmarks for measuring this
potential and guiding future development continue to evolve from pure recall
and rote knowledge tasks, towards more practical work such as literature review
and experimental planning. Bioinformatics is a domain where fully autonomous
AI-driven discovery may be near, but no extensive benchmarks for measuring
progress have been introduced to date. We therefore present the Bioinformatics
Benchmark (BixBench), a dataset comprising over 50 real-world scenarios of
practical biological data analysis with nearly 300 associated open-answer
questions designed to measure the ability of LLM-based agents to explore
biological datasets, perform long, multi-step analytical trajectories, and
interpret the nuanced results of those analyses. We evaluate the performance of
two frontier LLMs (GPT-4o and Claude 3.5 Sonnet) using a custom agent framework
we open source. We find that even the latest frontier models only achieve 17%
accuracy in the open-answer regime, and no better than random in a
multiple-choice setting. By exposing the current limitations of frontier
models, we hope BixBench can spur the development of agents capable of
conducting rigorous bioinformatic analysis and accelerate scientific discovery.",Ludovico Mitchener and Jon M Laurent and Benjamin Tenmann and Siddharth Narayanan and Geemi P Wellawatte and Andrew White and Lorenzo Sani and Samuel G Rodriques,2025,,https://arxiv.org/abs/2503.00096,,
LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,http://arxiv.org/abs/2510.01285v1,lai2025kramabenchbenchmarkaisystems,\cite{lai2025kramabenchbenchmarkaisystems},"KramaBench: A Benchmark for AI Systems on Data-to-Insight Pipelines over
  Data Lakes",http://arxiv.org/abs/2506.06541v2,"Constructing real-world data-to-insight pipelines often involves data
extraction from data lakes, data integration across heterogeneous data sources,
and diverse operations from data cleaning to analysis. The design and
implementation of data science pipelines require domain knowledge, technical
expertise, and even project-specific insights. AI systems have shown remarkable
reasoning, coding, and understanding capabilities. However, it remains unclear
to what extent these capabilities translate into successful design and
execution of such complex pipelines. We introduce KRAMABENCH: a benchmark
composed of 104 manually-curated real-world data science pipelines spanning
1700 data files from 24 data sources in 6 different domains. We show that these
pipelines test the end-to-end capabilities of AI systems on data processing,
requiring data discovery, wrangling and cleaning, efficient processing,
statistical reasoning, and orchestrating data processing steps given a
high-level task. Our evaluation tests 5 general models and 3 code generation
models using our reference framework, DS-GURU, which instructs the AI model to
decompose a question into a sequence of subtasks, reason through each step, and
synthesize Python code that implements the proposed design. Our results on
KRAMABENCH show that, although the models are sufficiently capable of solving
well-specified data science code generation tasks, when extensive data
processing and domain knowledge are required to construct real-world data
science pipelines, existing out-of-box models fall short. Progress on
KramaBench represents crucial steps towards developing autonomous data science
agents for real-world applications. Our code, reference framework, and data are
available at https://github.com/mitdbg/KramaBench.",Eugenie Lai and Gerardo Vitagliano and Ziyu Zhang and Sivaprasad Sudhir and Om Chabra and Anna Zeng and Anton A. Zabreyko and Chenning Li and Ferdi Kossmann and Jialin Ding and Jun Chen and Markos Markakis and Matthew Russo and Weiyang Wang and Ziniu Wu and Michael J. Cafarella and Lei Cao and Samuel Madden and Tim Kraska,2025,,https://arxiv.org/abs/2506.06541,,
LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,http://arxiv.org/abs/2510.01285v1,10.5555/3495724.3495883,\cite{10.5555/3495724.3495883},Language Models are Few-Shot Learners,http://arxiv.org/abs/2005.14165v4,"Recent work has demonstrated substantial gains on many NLP tasks and
benchmarks by pre-training on a large corpus of text followed by fine-tuning on
a specific task. While typically task-agnostic in architecture, this method
still requires task-specific fine-tuning datasets of thousands or tens of
thousands of examples. By contrast, humans can generally perform a new language
task from only a few examples or from simple instructions - something which
current NLP systems still largely struggle to do. Here we show that scaling up
language models greatly improves task-agnostic, few-shot performance, sometimes
even reaching competitiveness with prior state-of-the-art fine-tuning
approaches. Specifically, we train GPT-3, an autoregressive language model with
175 billion parameters, 10x more than any previous non-sparse language model,
and test its performance in the few-shot setting. For all tasks, GPT-3 is
applied without any gradient updates or fine-tuning, with tasks and few-shot
demonstrations specified purely via text interaction with the model. GPT-3
achieves strong performance on many NLP datasets, including translation,
question-answering, and cloze tasks, as well as several tasks that require
on-the-fly reasoning or domain adaptation, such as unscrambling words, using a
novel word in a sentence, or performing 3-digit arithmetic. At the same time,
we also identify some datasets where GPT-3's few-shot learning still struggles,
as well as some datasets where GPT-3 faces methodological issues related to
training on large web corpora. Finally, we find that GPT-3 can generate samples
of news articles which human evaluators have difficulty distinguishing from
articles written by humans. We discuss broader societal impacts of this finding
and of GPT-3 in general.","Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario",2020,,,,
LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,http://arxiv.org/abs/2510.01285v1,rubavicius2025conversationalcodegenerationcase,\cite{rubavicius2025conversationalcodegenerationcase},"Conversational Code Generation: a Case Study of Designing a Dialogue
  System for Generating Driving Scenarios for Testing Autonomous Vehicles",http://arxiv.org/abs/2410.09829v3,"Cyber-physical systems like autonomous vehicles are tested in simulation
before deployment, using domain-specific programs for scenario specification.
To aid the testing of autonomous vehicles in simulation, we design a natural
language interface, using an instruction-following large language model, to
assist a non-coding domain expert in synthesising the desired scenarios and
vehicle behaviours. We show that using it to convert utterances to the symbolic
program is feasible, despite the very small training dataset. Human experiments
show that dialogue is critical to successful simulation generation, leading to
a 4.5 times higher success rate than a generation without engaging in extended
conversation.",Rimvydas Rubavicius and Antonio Valerio Miceli-Barone and Alex Lascarides and Subramanian Ramamoorthy,2025,,https://arxiv.org/abs/2410.09829,,
LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,http://arxiv.org/abs/2510.01285v1,li2023starcoder,\cite{li2023starcoder},StarCoder: may the source be with you!,http://arxiv.org/abs/2305.06161v2,"The BigCode community, an open-scientific collaboration working on the
responsible development of Large Language Models for Code (Code LLMs),
introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context
length, infilling capabilities and fast large-batch inference enabled by
multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced
from The Stack, a large collection of permissively licensed GitHub repositories
with inspection tools and an opt-out process. We fine-tuned StarCoderBase on
35B Python tokens, resulting in the creation of StarCoder. We perform the most
comprehensive evaluation of Code LLMs to date and show that StarCoderBase
outperforms every open Code LLM that supports multiple programming languages
and matches or outperforms the OpenAI code-cushman-001 model. Furthermore,
StarCoder outperforms every model that is fine-tuned on Python, can be prompted
to achieve 40\% pass@1 on HumanEval, and still retains its performance on other
programming languages. We take several important steps towards a safe
open-access model release, including an improved PII redaction pipeline and a
novel attribution tracing tool, and make the StarCoder models publicly
available under a more commercially viable version of the Open Responsible AI
Model license.",Raymond Li and Loubna Ben allal and Yangtian Zi and Niklas Muennighoff and Denis Kocetkov and Chenghao Mou and Marc Marone and Christopher Akiki and Jia LI and Jenny Chim and Qian Liu and Evgenii Zheltonozhskii and Terry Yue Zhuo and Thomas Wang and Olivier Dehaene and Joel Lamy-Poirier and Joao Monteiro and Nicolas Gontier and Ming-Ho Yee and Logesh Kumar Umapathi and Jian Zhu and Ben Lipkin and Muhtasham Oblokulov and Zhiruo Wang and Rudra Murthy and Jason T Stillerman and Siva Sankalp Patel and Dmitry Abulkhanov and Marco Zocca and Manan Dey and Zhihan Zhang and Urvashi Bhattacharyya and Wenhao Yu and Sasha Luccioni and Paulo Villegas and Fedor Zhdanov and Tony Lee and Nadav Timor and Jennifer Ding and Claire S Schlesinger and Hailey Schoelkopf and Jan Ebert and Tri Dao and Mayank Mishra and Alex Gu and Carolyn Jane Anderson and Brendan Dolan-Gavitt and Danish Contractor and Siva Reddy and Daniel Fried and Dzmitry Bahdanau and Yacine Jernite and Carlos Mu{\~n}oz Ferrandis and Sean Hughes and Thomas Wolf and Arjun Guha and Leandro Von Werra and Harm de Vries,2023,,https://openreview.net/forum?id=KoFOg41haE,,Transactions on Machine Learning Research
LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,http://arxiv.org/abs/2510.01285v1,codellama,\cite{codellama},Code Llama: Open Foundation Models for Code,http://arxiv.org/abs/2308.12950v3,"We release Code Llama, a family of large language models for code based on
Llama 2 providing state-of-the-art performance among open models, infilling
capabilities, support for large input contexts, and zero-shot instruction
following ability for programming tasks. We provide multiple flavors to cover a
wide range of applications: foundation models (Code Llama), Python
specializations (Code Llama - Python), and instruction-following models (Code
Llama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are
trained on sequences of 16k tokens and show improvements on inputs with up to
100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants
support infilling based on surrounding content. Code Llama reaches
state-of-the-art performance among open models on several code benchmarks, with
scores of up to 67% and 65% on HumanEval and MBPP, respectively. Notably, Code
Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our
models outperform every other publicly available model on MultiPL-E. We release
Code Llama under a permissive license that allows for both research and
commercial use.",Baptiste Rozière and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Romain Sauvestre and Tal Remez and Jérémy Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre Défossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve,2024,,https://arxiv.org/abs/2308.12950,,
LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,http://arxiv.org/abs/2510.01285v1,yao2023react,\cite{yao2023react},ReAct: Synergizing Reasoning and Acting in Language Models,http://arxiv.org/abs/2210.03629v3,"While large language models (LLMs) have demonstrated impressive capabilities
across tasks in language understanding and interactive decision making, their
abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.
action plan generation) have primarily been studied as separate topics. In this
paper, we explore the use of LLMs to generate both reasoning traces and
task-specific actions in an interleaved manner, allowing for greater synergy
between the two: reasoning traces help the model induce, track, and update
action plans as well as handle exceptions, while actions allow it to interface
with external sources, such as knowledge bases or environments, to gather
additional information. We apply our approach, named ReAct, to a diverse set of
language and decision making tasks and demonstrate its effectiveness over
state-of-the-art baselines, as well as improved human interpretability and
trustworthiness over methods without reasoning or acting components.
Concretely, on question answering (HotpotQA) and fact verification (Fever),
ReAct overcomes issues of hallucination and error propagation prevalent in
chain-of-thought reasoning by interacting with a simple Wikipedia API, and
generates human-like task-solving trajectories that are more interpretable than
baselines without reasoning traces. On two interactive decision making
benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and
reinforcement learning methods by an absolute success rate of 34% and 10%
respectively, while being prompted with only one or two in-context examples.
Project site with code: https://react-lm.github.io","Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan",2023,,,,
LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,http://arxiv.org/abs/2510.01285v1,chen2018executionguided,\cite{chen2018executionguided},Execution-Guided Neural Program Synthesis,,,Xinyun Chen and Chang Liu and Dawn Song,2019,,https://openreview.net/forum?id=H1gfOiAqYm,,
LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,http://arxiv.org/abs/2510.01285v1,schick2023toolformer,\cite{schick2023toolformer},Toolformer: Language Models Can Teach Themselves to Use Tools,http://arxiv.org/abs/2302.04761v1,"Language models (LMs) exhibit remarkable abilities to solve new tasks from
just a few examples or textual instructions, especially at scale. They also,
paradoxically, struggle with basic functionality, such as arithmetic or factual
lookup, where much simpler and smaller models excel. In this paper, we show
that LMs can teach themselves to use external tools via simple APIs and achieve
the best of both worlds. We introduce Toolformer, a model trained to decide
which APIs to call, when to call them, what arguments to pass, and how to best
incorporate the results into future token prediction. This is done in a
self-supervised way, requiring nothing more than a handful of demonstrations
for each API. We incorporate a range of tools, including a calculator, a Q\&A
system, two different search engines, a translation system, and a calendar.
Toolformer achieves substantially improved zero-shot performance across a
variety of downstream tasks, often competitive with much larger models, without
sacrificing its core language modeling abilities.",Timo Schick and Jane Dwivedi-Yu and Roberto Dessi and Roberta Raileanu and Maria Lomeli and Eric Hambro and Luke Zettlemoyer and Nicola Cancedda and Thomas Scialom,2023,,https://openreview.net/forum?id=Yacmpz84TH,,
LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,http://arxiv.org/abs/2510.01285v1,patil2024gorilla,\cite{patil2024gorilla},Gorilla: Large Language Model Connected with Massive APIs,http://arxiv.org/abs/2305.15334v1,"Large Language Models (LLMs) have seen an impressive wave of advances
recently, with models now excelling in a variety of tasks, such as mathematical
reasoning and program synthesis. However, their potential to effectively use
tools via API calls remains unfulfilled. This is a challenging task even for
today's state-of-the-art LLMs such as GPT-4, largely due to their inability to
generate accurate input arguments and their tendency to hallucinate the wrong
usage of an API call. We release Gorilla, a finetuned LLaMA-based model that
surpasses the performance of GPT-4 on writing API calls. When combined with a
document retriever, Gorilla demonstrates a strong capability to adapt to
test-time document changes, enabling flexible user updates or version changes.
It also substantially mitigates the issue of hallucination, commonly
encountered when prompting LLMs directly. To evaluate the model's ability, we
introduce APIBench, a comprehensive dataset consisting of HuggingFace,
TorchHub, and TensorHub APIs. The successful integration of the retrieval
system with Gorilla demonstrates the potential for LLMs to use tools more
accurately, keep up with frequently updated documentation, and consequently
increase the reliability and applicability of their outputs. Gorilla's code,
model, data, and demo are available at https://gorilla.cs.berkeley.edu",Shishir G Patil and Tianjun Zhang and Xin Wang and Joseph E. Gonzalez,2024,,https://openreview.net/forum?id=tBRNC6YemY,,
LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,http://arxiv.org/abs/2510.01285v1,chen2024teaching,\cite{chen2024teaching},Teaching Large Language Models to Self-Debug,http://arxiv.org/abs/2304.05128v2,"Large language models (LLMs) have achieved impressive performance on code
generation. However, for complex programming tasks, generating the correct
solution in one go becomes challenging, thus some prior works have designed
program repair approaches to improve code generation performance. In this work,
we propose Self-Debugging, which teaches a large language model to debug its
predicted program via few-shot demonstrations. In particular, we demonstrate
that Self-Debugging can teach the large language model to perform rubber duck
debugging; i.e., without any human feedback on the code correctness or error
messages, the model is able to identify its mistakes by investigating the
execution results and explaining the generated code in natural language.
Self-Debugging achieves the state-of-the-art performance on several code
generation benchmarks, including the Spider dataset for text-to-SQL generation,
TransCoder for C++-to-Python translation, and MBPP for text-to-Python
generation. On the Spider benchmark where there are no unit tests to verify the
correctness of predictions, Self-Debugging with code explanation consistently
improves the baseline by 2-3%, and improves the prediction accuracy on problems
of the hardest level by 9%. On TransCoder and MBPP where unit tests are
available, Self-Debugging improves the baseline accuracy by up to 12%.
Meanwhile, by leveraging feedback messages and reusing failed predictions,
Self-Debugging notably improves sample efficiency, and can match or outperform
baseline models that generate more than 10x candidate programs.","Xinyun Chen and Maxwell Lin and Nathanael Sch{\""a}rli and Denny Zhou",2024,,https://openreview.net/forum?id=KuPixIqPiq,,
LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,http://arxiv.org/abs/2510.01285v1,shinn2023reflexion,\cite{shinn2023reflexion},Reflexion: Language Agents with Verbal Reinforcement Learning,http://arxiv.org/abs/2303.11366v4,"Large language models (LLMs) have been increasingly used to interact with
external environments (e.g., games, compilers, APIs) as goal-driven agents.
However, it remains challenging for these language agents to quickly and
efficiently learn from trial-and-error as traditional reinforcement learning
methods require extensive training samples and expensive model fine-tuning. We
propose Reflexion, a novel framework to reinforce language agents not by
updating weights, but instead through linguistic feedback. Concretely,
Reflexion agents verbally reflect on task feedback signals, then maintain their
own reflective text in an episodic memory buffer to induce better
decision-making in subsequent trials. Reflexion is flexible enough to
incorporate various types (scalar values or free-form language) and sources
(external or internally simulated) of feedback signals, and obtains significant
improvements over a baseline agent across diverse tasks (sequential
decision-making, coding, language reasoning). For example, Reflexion achieves a
91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous
state-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis
studies using different feedback signals, feedback incorporation methods, and
agent types, and provide insights into how they affect performance.",Noah Shinn and Federico Cassano and Ashwin Gopinath and Karthik R Narasimhan and Shunyu Yao,2023,,https://openreview.net/forum?id=vAElhFcKW6,,
LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,http://arxiv.org/abs/2510.01285v1,10.5555/3495724.3496517,\cite{10.5555/3495724.3496517},Retrieval-augmented generation for knowledge-intensive NLP tasks,,,"Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\""{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\""{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe",2020,,,,
LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,http://arxiv.org/abs/2510.01285v1,salemi2025planandrefinediversecomprehensiveretrievalaugmented,\cite{salemi2025planandrefinediversecomprehensiveretrievalaugmented},Plan-and-Refine: Diverse and Comprehensive Retrieval-Augmented Generation,,,Alireza Salemi and Chris Samarinas and Hamed Zamani,2025,,https://arxiv.org/abs/2504.07794,,
LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,http://arxiv.org/abs/2510.01285v1,10.1145/3731120.3744584,\cite{10.1145/3731120.3744584},"Learning to Rank for Multiple Retrieval-Augmented Models through
  Iterative Utility Maximization",http://arxiv.org/abs/2410.09942v2,"This paper investigates the design of a unified search engine to serve
multiple retrieval-augmented generation (RAG) agents, each with a distinct
task, backbone large language model (LLM), and RAG strategy. We introduce an
iterative approach where the search engine generates retrieval results for the
RAG agents and gathers feedback on the quality of the retrieved documents
during an offline phase. This feedback is then used to iteratively optimize the
search engine using an expectation-maximization algorithm, with the goal of
maximizing each agent's utility function. Additionally, we adapt this to an
online setting, allowing the search engine to refine its behavior based on
real-time individual agents feedback to better serve the results for each of
them. Experiments on datasets from the Knowledge-Intensive Language Tasks
(KILT) benchmark demonstrates that our approach significantly on average
outperforms baselines across 18 RAG models. We demonstrate that our method
effectively ``personalizes'' the retrieval for each RAG agent based on the
collected feedback. Finally, we provide a comprehensive ablation study to
explore various aspects of our method.","Salemi, Alireza and Zamani, Hamed",2025,,https://doi.org/10.1145/3731120.3744584,10.1145/3731120.3744584,
LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,http://arxiv.org/abs/2510.01285v1,10.1145/3626772.3657733,\cite{10.1145/3626772.3657733},"Towards a Search Engine for Machines: Unified Ranking for Multiple
  Retrieval-Augmented Large Language Models",http://arxiv.org/abs/2405.00175v1,"This paper introduces uRAG--a framework with a unified retrieval engine that
serves multiple downstream retrieval-augmented generation (RAG) systems. Each
RAG system consumes the retrieval results for a unique purpose, such as
open-domain question answering, fact verification, entity linking, and relation
extraction. We introduce a generic training guideline that standardizes the
communication between the search engine and the downstream RAG systems that
engage in optimizing the retrieval model. This lays the groundwork for us to
build a large-scale experimentation ecosystem consisting of 18 RAG systems that
engage in training and 18 unknown RAG systems that use the uRAG as the new
users of the search engine. Using this experimentation ecosystem, we answer a
number of fundamental research questions that improve our understanding of
promises and challenges in developing search engines for machines.","Salemi, Alireza and Zamani, Hamed",2024,,https://doi.org/10.1145/3626772.3657733,10.1145/3626772.3657733,
LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,http://arxiv.org/abs/2510.01285v1,li2025autokaggle,\cite{li2025autokaggle},"AutoKaggle: A Multi-Agent Framework for Autonomous Data Science
  Competitions",http://arxiv.org/abs/2410.20424v3,"Data science tasks involving tabular data present complex challenges that
require sophisticated problem-solving approaches. We propose AutoKaggle, a
powerful and user-centric framework that assists data scientists in completing
daily data pipelines through a collaborative multi-agent system. AutoKaggle
implements an iterative development process that combines code execution,
debugging, and comprehensive unit testing to ensure code correctness and logic
consistency. The framework offers highly customizable workflows, allowing users
to intervene at each phase, thus integrating automated intelligence with human
expertise. Our universal data science toolkit, comprising validated functions
for data cleaning, feature engineering, and modeling, forms the foundation of
this solution, enhancing productivity by streamlining common tasks. We selected
8 Kaggle competitions to simulate data processing workflows in real-world
application scenarios. Evaluation results demonstrate that AutoKaggle achieves
a validation submission rate of 0.85 and a comprehensive score of 0.82 in
typical data science pipelines, fully proving its effectiveness and
practicality in handling complex data science tasks.",Ziming Li and Qianbo Zang and David Ma and Jiawei Guo and Tuney Zheng and Minghao Liu and Xinyao Niu and Yue Wang and Jian Yang and Jiaheng Liu and Wanjun Zhong and Wangchunshu Zhou and Wenhao Huang and Ge Zhang,2024,,https://arxiv.org/abs/2410.20424,,
LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,http://arxiv.org/abs/2510.01285v1,10.1145/356810.356816,\cite{10.1145/356810.356816},The Hearsay-II Speech-Understanding System: Integrating Knowledge to Resolve Uncertainty,,,"Erman, Lee D. and Hayes-Roth, Frederick and Lesser, Victor R. and Reddy, D. Raj",1980,,https://doi.org/10.1145/356810.356816,10.1145/356810.356816,ACM Comput. Surv.
LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,http://arxiv.org/abs/2510.01285v1,Nii_1986,\cite{Nii_1986},The Blackboard Model of Problem Solving and the Evolution of Blackboard Architectures,,,"Nii, H. Penny",1986,Jun.,https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/537,10.1609/aimag.v7i2.537,AI Magazine
LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,http://arxiv.org/abs/2510.01285v1,Nii_Feigenbaum_Anton_1982,\cite{Nii_Feigenbaum_Anton_1982},Signal-to-Symbol Transformation: HASP/SIAP Case Study,,,"Nii, H. Penny and Feigenbaum, Edward A. and Anton, John J.",1982,Jun.,https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/368,10.1609/aimag.v3i2.368,AI Magazine
