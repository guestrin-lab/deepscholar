parent_paper_title,parent_arxiv_link,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal
Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,http://arxiv.org/abs/2509.21170v1,wang2021codet5,\cite{wang2021codet5},"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for
  Code Understanding and Generation",http://arxiv.org/abs/2109.00859v1,"Pre-trained models for Natural Languages (NL) like BERT and GPT have been
recently shown to transfer well to Programming Languages (PL) and largely
benefit a broad set of code-related tasks. Despite their success, most current
methods either rely on an encoder-only (or decoder-only) pre-training that is
suboptimal for generation (resp. understanding) tasks or process the code
snippet in the same way as NL, neglecting the special characteristics of PL
such as token types. We present CodeT5, a unified pre-trained encoder-decoder
Transformer model that better leverages the code semantics conveyed from the
developer-assigned identifiers. Our model employs a unified framework to
seamlessly support both code understanding and generation tasks and allows for
multi-task learning. Besides, we propose a novel identifier-aware pre-training
task that enables the model to distinguish which code tokens are identifiers
and to recover them when they are masked. Furthermore, we propose to exploit
the user-written code comments with a bimodal dual generation task for better
NL-PL alignment. Comprehensive experiments show that CodeT5 significantly
outperforms prior methods on understanding tasks such as code defect detection
and clone detection, and generation tasks across various directions including
PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better
capture semantic information from code. Our code and pre-trained models are
released at https: //github.com/salesforce/CodeT5 .","Wang, Yue and Wang, Weishi and Joty, Shafiq and Hoi, Steven CH",2021,,,,arXiv preprint arXiv:2109.00859
Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,http://arxiv.org/abs/2509.21170v1,feng2020codebert,\cite{feng2020codebert},CodeBERT: A Pre-Trained Model for Programming and Natural Languages,http://arxiv.org/abs/2002.08155v4,"We present CodeBERT, a bimodal pre-trained model for programming language
(PL) and nat-ural language (NL). CodeBERT learns general-purpose
representations that support downstream NL-PL applications such as natural
language codesearch, code documentation generation, etc. We develop CodeBERT
with Transformer-based neural architecture, and train it with a hybrid
objective function that incorporates the pre-training task of replaced token
detection, which is to detect plausible alternatives sampled from generators.
This enables us to utilize both bimodal data of NL-PL pairs and unimodal data,
where the former provides input tokens for model training while the latter
helps to learn better generators. We evaluate CodeBERT on two NL-PL
applications by fine-tuning model parameters. Results show that CodeBERT
achieves state-of-the-art performance on both natural language code search and
code documentation generation tasks. Furthermore, to investigate what type of
knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and
evaluate in a zero-shot setting where parameters of pre-trained models are
fixed. Results show that CodeBERT performs better than previous pre-trained
models on NL-PL probing.","Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and others",2020,,,,arXiv preprint arXiv:2002.08155
Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,http://arxiv.org/abs/2509.21170v1,shi2019automatic,\cite{shi2019automatic},Automatic code review by learning the revision of source code,,,"Shi, Shu-Ting and Li, Ming and Lo, David and Thung, Ferdian and Huo, Xuan",2019,,,,
Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,http://arxiv.org/abs/2509.21170v1,zhang2024logicode,\cite{zhang2024logicode},LogiCode: an LLM-Driven Framework for Logical Anomaly Detection,http://arxiv.org/abs/2406.04687v1,"This paper presents LogiCode, a novel framework that leverages Large Language
Models (LLMs) for identifying logical anomalies in industrial settings, moving
beyond traditional focus on structural inconsistencies. By harnessing LLMs for
logical reasoning, LogiCode autonomously generates Python codes to pinpoint
anomalies such as incorrect component quantities or missing elements, marking a
significant leap forward in anomaly detection technologies. A custom dataset
""LOCO-Annotations"" and a benchmark ""LogiBench"" are introduced to evaluate the
LogiCode's performance across various metrics including binary classification
accuracy, code generation success rate, and precision in reasoning. Findings
demonstrate LogiCode's enhanced interpretability, significantly improving the
accuracy of logical anomaly detection and offering detailed explanations for
identified anomalies. This represents a notable shift towards more intelligent,
LLM-driven approaches in industrial anomaly detection, promising substantial
impacts on industry-specific applications.","Zhang, Yiheng and Cao, Yunkang and Xu, Xiaohao and Shen, Weiming",2024,,,,IEEE Transactions on Automation Science and Engineering
Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,http://arxiv.org/abs/2509.21170v1,lu2023llama,\cite{lu2023llama},LLaMA-Reviewer: Advancing code review automation with large language models through parameter-efficient fine-tuning,,,"Lu, Junyi and Yu, Lei and Li, Xiaojia and Yang, Li and Zuo, Chun",2023,,,,
Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,http://arxiv.org/abs/2509.21170v1,li2022codereviewer,\cite{li2022codereviewer},Automating Code Review Activities by Large-Scale Pre-training,http://arxiv.org/abs/2203.09095v2,"Code review is an essential part to software development lifecycle since it
aims at guaranteeing the quality of codes. Modern code review activities
necessitate developers viewing, understanding and even running the programs to
assess logic, functionality, latency, style and other factors. It turns out
that developers have to spend far too much time reviewing the code of their
peers. Accordingly, it is in significant demand to automate the code review
process. In this research, we focus on utilizing pre-training techniques for
the tasks in the code review scenario. We collect a large-scale dataset of
real-world code changes and code reviews from open-source projects in nine of
the most popular programming languages. To better understand code diffs and
reviews, we propose CodeReviewer, a pre-trained model that utilizes four
pre-training tasks tailored specifically for the code review scenario. To
evaluate our model, we focus on three key tasks related to code review
activities, including code change quality estimation, review comment generation
and code refinement. Furthermore, we establish a high-quality benchmark dataset
based on our collected data for these three tasks and conduct comprehensive
experiments on it. The experimental results demonstrate that our model
outperforms the previous state-of-the-art pre-training approaches in all tasks.
Further analysis show that our proposed pre-training tasks and the multilingual
pre-training dataset benefit the model on the understanding of code changes and
reviews.","Li, Zhiyu and Lu, Shuai and Guo, Daya and Duan, Nan and Jannu, Shailesh and Jenks, Grant and Majumder, Deep and Green, Jared and Svyatkovskiy, Alexey and Fu, Shengyu and others",2022,,,,
Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,http://arxiv.org/abs/2509.21170v1,gupta2018intelligent,\cite{gupta2018intelligent},Intelligent code reviews using deep learning,,,"Gupta, Anshul and Sundaresan, Neel",2018,,,,
Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,http://arxiv.org/abs/2509.21170v1,hong2022commentfinder,\cite{hong2022commentfinder},"Commentfinder: a simpler, faster, more accurate code review comments recommendation",,,"Hong, Yang and Tantithamthavorn, Chakkrit and Thongtanunam, Patanamon and Aleti, Aldeida",2022,,,,
Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,http://arxiv.org/abs/2509.21170v1,ning2024defining,\cite{ning2024defining},"Defining and Detecting the Defects of the Large Language Model-based
  Autonomous Agents",http://arxiv.org/abs/2412.18371v2,"AI agents are systems capable of perceiving their environment, autonomously
planning and executing tasks. Recent advancements in LLM have introduced a
transformative paradigm for AI agents, enabling them to interact with external
resources and tools through prompts. In such agents, the workflow integrates
developer-written code, which manages framework construction and logic control,
with LLM-generated natural language that enhances dynamic decision-making and
interaction. However, discrepancies between developer-implemented logic and the
dynamically generated content of LLMs in terms of behavior and expected
outcomes can lead to defects, such as tool invocation failures and task
execution errors. These issues introduce specific risks, leading to various
defects in LLM-based AI Agents, such as service interruptions. Despite the
importance of these issues, there is a lack of systematic work that focuses on
analyzing LLM-based AI Agents to uncover defects in their code. In this paper,
we present the first study focused on identifying and detecting defects in LLM
Agents. We collected and analyzed 6,854 relevant posts from StackOverflow to
define 8 types of agent defects. For each type, we provided detailed
descriptions with an example. Then, we designed a static analysis tool, named
Agentable, to detect the defects. Agentable leverages Code Property Graphs and
LLMs to analyze Agent workflows by efficiently identifying specific code
patterns and analyzing natural language descriptions. To evaluate Agentable, we
constructed two datasets: AgentSet, consists of 84 real-world Agents, and
AgentTest, which contains 78 Agents specifically designed to include various
types of defects. Our results show that Agentable achieved an overall accuracy
of 88.79% and a recall rate of 91.03%. Furthermore, our analysis reveals the
889 defects of the AgentSet, highlighting the prevalence of these defects.","Ning, Kaiwen and Chen, Jiachi and Zhang, Jingwen and Li, Wei and Wang, Zexu and Feng, Yuming and Zhang, Weizhe and Zheng, Zibin",2024,,,,arXiv preprint arXiv:2412.18371
Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,http://arxiv.org/abs/2509.21170v1,wei2022chain,\cite{wei2022chain},Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,http://arxiv.org/abs/2201.11903v6,"We explore how generating a chain of thought -- a series of intermediate
reasoning steps -- significantly improves the ability of large language models
to perform complex reasoning. In particular, we show how such reasoning
abilities emerge naturally in sufficiently large language models via a simple
method called chain of thought prompting, where a few chain of thought
demonstrations are provided as exemplars in prompting. Experiments on three
large language models show that chain of thought prompting improves performance
on a range of arithmetic, commonsense, and symbolic reasoning tasks. The
empirical gains can be striking. For instance, prompting a 540B-parameter
language model with just eight chain of thought exemplars achieves state of the
art accuracy on the GSM8K benchmark of math word problems, surpassing even
finetuned GPT-3 with a verifier.","Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others",2022,,,,Advances in neural information processing systems
Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,http://arxiv.org/abs/2509.21170v1,10.1145/3690635,\cite{10.1145/3690635},Structured Chain-of-Thought Prompting for Code Generation,http://arxiv.org/abs/2305.06599v3,"Large Language Models (LLMs) (e.g., ChatGPT) have shown impressive
performance in code generation. LLMs take prompts as inputs, and
Chain-of-Thought (CoT) prompting is the state-of-the-art prompting technique.
CoT prompting asks LLMs first to generate CoTs (i.e., intermediate natural
language reasoning steps) and then output the code. However, CoT prompting is
designed for natural language generation and has low accuracy in code
generation.
  In this paper, we propose Structured CoTs (SCoTs) and present a novel
prompting technique for code generation, named SCoT prompting. Our motivation
is source code contains rich structural information and any code can be
composed of three program structures (i.e., sequence, branch, and loop
structures). Intuitively, structured intermediate reasoning steps make for
structured source code. Thus, we ask LLMs to use program structures to build
CoTs, obtaining SCoTs. Then, LLMs generate the final code based on SCoTs.
Compared to CoT prompting, SCoT prompting explicitly constrains LLMs to think
about how to solve requirements from the view of source code and further the
performance of LLMs in code generation. We apply SCoT prompting to two LLMs
(i.e., ChatGPT and Codex) and evaluate it on three benchmarks (i.e., HumanEval,
MBPP, and MBCPP). (1) SCoT prompting outperforms the state-of-the-art baseline
- CoT prompting by up to 13.79% in Pass@1. (2) Human evaluation shows human
developers prefer programs from SCoT prompting. (3) SCoT prompting is robust to
examples and achieves substantial improvements.","Li, Jia and Li, Ge and Li, Yongmin and Jin, Zhi",2025,,https://doi.org/10.1145/3690635,10.1145/3690635,ACM Trans. Softw. Eng. Methodol.
Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,http://arxiv.org/abs/2509.21170v1,yu2024fine,\cite{yu2024fine},Fine-tuning large language models to improve accuracy and comprehensibility of automated code review,,,"Yu, Yongda and Rong, Guoping and Shen, Haifeng and Zhang, He and Shao, Dong and Wang, Min and Wei, Zhao and Xu, Yong and Wang, Juhong",2024,,,,ACM transactions on software engineering and methodology
Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,http://arxiv.org/abs/2509.21170v1,nong2024chain,\cite{nong2024chain},Chain-of-thought prompting of large language models for discovering and fixing software vulnerabilities,,,"Nong, Yu and Aldeen, Mohammed and Cheng, Long and Hu, Hongxin and Chen, Feng and Cai, Haipeng",2024,,,,arXiv preprint arXiv:2402.17230
Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,http://arxiv.org/abs/2509.21170v1,chen2025towards,\cite{chen2025towards},"Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning
  Large Language Models",http://arxiv.org/abs/2503.09567v5,"Recent advancements in reasoning with large language models (RLLMs), such as
OpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in
complex domains like mathematics and coding. A central factor in their success
lies in the application of long chain-of-thought (Long CoT) characteristics,
which enhance reasoning abilities and enable the solution of intricate
problems. However, despite these developments, a comprehensive survey on Long
CoT is still lacking, limiting our understanding of its distinctions from
traditional short chain-of-thought (Short CoT) and complicating ongoing debates
on issues like ""overthinking"" and ""inference-time scaling."" This survey seeks
to fill this gap by offering a unified perspective on Long CoT. (1) We first
distinguish Long CoT from Short CoT and introduce a novel taxonomy to
categorize current reasoning paradigms. (2) Next, we explore the key
characteristics of Long CoT: deep reasoning, extensive exploration, and
feasible reflection, which enable models to handle more complex tasks and
produce more efficient, coherent outcomes compared to the shallower Short CoT.
(3) We then investigate key phenomena such as the emergence of Long CoT with
these characteristics, including overthinking, and inference-time scaling,
offering insights into how these processes manifest in practice. (4) Finally,
we identify significant research gaps and highlight promising future
directions, including the integration of multi-modal reasoning, efficiency
improvements, and enhanced knowledge frameworks. By providing a structured
overview, this survey aims to inspire future research and further the
development of logical reasoning in artificial intelligence.","Chen, Qiguang and Qin, Libo and Liu, Jinhao and Peng, Dengyun and Guan, Jiannan and Wang, Peng and Hu, Mengkang and Zhou, Yuhang and Gao, Te and Che, Wanxiang",2025,,,,arXiv preprint arXiv:2503.09567
Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,http://arxiv.org/abs/2509.21170v1,xia2024beyond,\cite{xia2024beyond},Beyond chain-of-thought: A survey of chain-of-x paradigms for llms,,,"Xia, Yu and Wang, Rui and Liu, Xu and Li, Mingyan and Yu, Tong and Chen, Xiang and McAuley, Julian and Li, Shuai",2024,,,,arXiv preprint arXiv:2404.15676
Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,http://arxiv.org/abs/2509.21170v1,wang2025multimodal,\cite{wang2025multimodal},Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey,http://arxiv.org/abs/2503.12605v2,"By extending the advantage of chain-of-thought (CoT) reasoning in human-like
step-by-step processes to multimodal contexts, multimodal CoT (MCoT) reasoning
has recently garnered significant research attention, especially in the
integration with multimodal large language models (MLLMs). Existing MCoT
studies design various methodologies and innovative reasoning paradigms to
address the unique challenges of image, video, speech, audio, 3D, and
structured data across different modalities, achieving extensive success in
applications such as robotics, healthcare, autonomous driving, and multimodal
generation. However, MCoT still presents distinct challenges and opportunities
that require further focus to ensure consistent thriving in this field, where,
unfortunately, an up-to-date review of this domain is lacking. To bridge this
gap, we present the first systematic survey of MCoT reasoning, elucidating the
relevant foundational concepts and definitions. We offer a comprehensive
taxonomy and an in-depth analysis of current methodologies from diverse
perspectives across various application scenarios. Furthermore, we provide
insights into existing challenges and future research directions, aiming to
foster innovation toward multimodal AGI.","Wang, Yaoting and Wu, Shengqiong and Zhang, Yuecheng and Yan, Shuicheng and Liu, Ziwei and Luo, Jiebo and Fei, Hao",2025,,,,arXiv preprint arXiv:2503.12605
Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,http://arxiv.org/abs/2509.21170v1,yeo2502demystifying,\cite{yeo2502demystifying},Demystifying Long Chain-of-Thought Reasoning in LLMs,http://arxiv.org/abs/2502.03373v1,"Scaling inference compute enhances reasoning in large language models (LLMs),
with long chains-of-thought (CoTs) enabling strategies like backtracking and
error correction. Reinforcement learning (RL) has emerged as a crucial method
for developing these capabilities, yet the conditions under which long CoTs
emerge remain unclear, and RL training requires careful design choices. In this
study, we systematically investigate the mechanics of long CoT reasoning,
identifying the key factors that enable models to generate long CoT
trajectories. Through extensive supervised fine-tuning (SFT) and RL
experiments, we present four main findings: (1) While SFT is not strictly
necessary, it simplifies training and improves efficiency; (2) Reasoning
capabilities tend to emerge with increased training compute, but their
development is not guaranteed, making reward shaping crucial for stabilizing
CoT length growth; (3) Scaling verifiable reward signals is critical for RL. We
find that leveraging noisy, web-extracted solutions with filtering mechanisms
shows strong potential, particularly for out-of-distribution (OOD) tasks such
as STEM reasoning; and (4) Core abilities like error correction are inherently
present in base models, but incentivizing these skills effectively for complex
tasks via RL demands significant compute, and measuring their emergence
requires a nuanced approach. These insights provide practical guidance for
optimizing training strategies to enhance long CoT reasoning in LLMs. Our code
is available at: https://github.com/eddycmu/demystify-long-cot.","Yeo, Edward and Tong, Yuxuan and Niu, Morry and Neubig, Graham and Yue, Xiang",2025,,,,URL https://arxiv. org/abs/2502.03373
Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,http://arxiv.org/abs/2509.21170v1,wang2024drt,\cite{wang2024drt},Drt-o1: Optimized deep reasoning translation via long chain-of-thought,,,"Wang, Jiaan and Meng, Fandong and Liang, Yunlong and Zhou, Jie",2024,,,,arXiv e-prints
Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,http://arxiv.org/abs/2509.21170v1,zhang2024improve,\cite{zhang2024improve},Improve vision language model chain-of-thought reasoning,,,"Zhang, Ruohong and Zhang, Bowen and Li, Yanghao and Zhang, Haotian and Sun, Zhiqing and Gan, Zhe and Yang, Yinfei and Pang, Ruoming and Yang, Yiming",2024,,,,arXiv preprint arXiv:2410.16198
Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,http://arxiv.org/abs/2509.21170v1,kaelbling1996reinforcement,\cite{kaelbling1996reinforcement},Reinforcement Learning: A Survey,http://arxiv.org/abs/cs/9605103v1,"This paper surveys the field of reinforcement learning from a
computer-science perspective. It is written to be accessible to researchers
familiar with machine learning. Both the historical basis of the field and a
broad selection of current work are summarized. Reinforcement learning is the
problem faced by an agent that learns behavior through trial-and-error
interactions with a dynamic environment. The work described here has a
resemblance to work in psychology, but differs considerably in the details and
in the use of the word ``reinforcement.'' The paper discusses central issues of
reinforcement learning, including trading off exploration and exploitation,
establishing the foundations of the field via Markov decision theory, learning
from delayed reinforcement, constructing empirical models to accelerate
learning, making use of generalization and hierarchy, and coping with hidden
state. It concludes with a survey of some implemented systems and an assessment
of the practical utility of current methods for reinforcement learning.","Kaelbling, Leslie Pack and Littman, Michael L and Moore, Andrew W",1996,,,,Journal of artificial intelligence research
Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,http://arxiv.org/abs/2509.21170v1,arulkumaran2017deep,\cite{arulkumaran2017deep},A Brief Survey of Deep Reinforcement Learning,http://arxiv.org/abs/1708.05866v2,"Deep reinforcement learning is poised to revolutionise the field of AI and
represents a step towards building autonomous systems with a higher level
understanding of the visual world. Currently, deep learning is enabling
reinforcement learning to scale to problems that were previously intractable,
such as learning to play video games directly from pixels. Deep reinforcement
learning algorithms are also applied to robotics, allowing control policies for
robots to be learned directly from camera inputs in the real world. In this
survey, we begin with an introduction to the general field of reinforcement
learning, then progress to the main streams of value-based and policy-based
methods. Our survey will cover central algorithms in deep reinforcement
learning, including the deep $Q$-network, trust region policy optimisation, and
asynchronous advantage actor-critic. In parallel, we highlight the unique
advantages of deep neural networks, focusing on visual understanding via
reinforcement learning. To conclude, we describe several current areas of
research within the field.","Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony",2017,,,,IEEE Signal Processing Magazine
Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,http://arxiv.org/abs/2509.21170v1,haarnoja2017reinforcement,\cite{haarnoja2017reinforcement},Reinforcement Learning with Deep Energy-Based Policies,http://arxiv.org/abs/1702.08165v2,"We propose a method for learning expressive energy-based policies for
continuous states and actions, which has been feasible only in tabular domains
before. We apply our method to learning maximum entropy policies, resulting
into a new algorithm, called soft Q-learning, that expresses the optimal policy
via a Boltzmann distribution. We use the recently proposed amortized Stein
variational gradient descent to learn a stochastic sampling network that
approximates samples from this distribution. The benefits of the proposed
algorithm include improved exploration and compositionality that allows
transferring skills between tasks, which we confirm in simulated experiments
with swimming and walking robots. We also draw a connection to actor-critic
methods, which can be viewed performing approximate inference on the
corresponding energy-based model.","Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey",2017,,,,
Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,http://arxiv.org/abs/2509.21170v1,schulman2017equivalence,\cite{schulman2017equivalence},Equivalence Between Policy Gradients and Soft Q-Learning,http://arxiv.org/abs/1704.06440v4,"Two of the leading approaches for model-free reinforcement learning are
policy gradient methods and $Q$-learning methods. $Q$-learning methods can be
effective and sample-efficient when they work, however, it is not
well-understood why they work, since empirically, the $Q$-values they estimate
are very inaccurate. A partial explanation may be that $Q$-learning methods are
secretly implementing policy gradient updates: we show that there is a precise
equivalence between $Q$-learning and policy gradient methods in the setting of
entropy-regularized reinforcement learning, that ""soft"" (entropy-regularized)
$Q$-learning is exactly equivalent to a policy gradient method. We also point
out a connection between $Q$-learning methods and natural policy gradient
methods. Experimentally, we explore the entropy-regularized versions of
$Q$-learning and policy gradients, and we find them to perform as well as (or
slightly better than) the standard variants on the Atari benchmark. We also
show that the equivalence holds in practical settings by constructing a
$Q$-learning method that closely matches the learning dynamics of A3C without
using a target network or $\epsilon$-greedy exploration schedule.","Schulman, John and Chen, Xi and Abbeel, Pieter",2017,,,,arXiv preprint arXiv:1704.06440
Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,http://arxiv.org/abs/2509.21170v1,haarnoja2018soft,\cite{haarnoja2018soft},Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor,,,"Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey",2018,,,,
Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,http://arxiv.org/abs/2509.21170v1,jaynes1957information,\cite{jaynes1957information},Information Theory and Statistical Mechanics Revisited,http://arxiv.org/abs/1105.5662v1,"The statistical mechanics of Gibbs is a juxtaposition of subjective,
probabilistic ideas on the one hand and objective, mechanical ideas on the
other. In this paper, we follow the path set out by Jaynes, including elements
added subsequently to that original work, to explore the consequences of the
purely statistical point of view. We show how standard methods in the
equilibrium theory could have been derived simply from a description of the
available problem information. In addition, our presentation leads to novel
insights into questions associated with symmetry and non-equilibrium
statistical mechanics. Two surprising consequences to be explored in further
work are that (in)distinguishability factors are automatically predicted from
the problem formulation and that a quantity related to the thermodynamic
entropy production is found by considering information loss in non-equilibrium
processes. Using the problem of ion channel thermodynamics as an example, we
illustrate the idea of building up complexity by successively adding
information to create progressively more complex descriptions of a physical
system. Our result is that such statistical mechanical descriptions can be used
to create transparent, computable, experimentally-relevant models that may be
informed by more detailed atomistic simulations. We also derive a theory for
the kinetic behavior of this system, identifying the nonequilibrium `process'
free energy functional. The Gibbs relation for this functional is a
fluctuation-dissipation theorem applicable arbitrarily far from equilibrium,
that captures the effect of non-local and time-dependent behavior from
transient driving forces. Based on this work, it is clear that statistical
mechanics is a general tool for constructing the relationships between
constraints on system information.","Jaynes, Edwin T",1957,,,,Physical review
Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,http://arxiv.org/abs/2509.21170v1,levine2018reinforcement,\cite{levine2018reinforcement},"Reinforcement Learning and Control as Probabilistic Inference: Tutorial
  and Review",http://arxiv.org/abs/1805.00909v3,"The framework of reinforcement learning or optimal control provides a
mathematical formalization of intelligent decision making that is powerful and
broadly applicable. While the general form of the reinforcement learning
problem enables effective reasoning about uncertainty, the connection between
reinforcement learning and inference in probabilistic models is not immediately
obvious. However, such a connection has considerable value when it comes to
algorithm design: formalizing a problem as probabilistic inference in principle
allows us to bring to bear a wide array of approximate inference tools, extend
the model in flexible and powerful ways, and reason about compositionality and
partial observability. In this article, we will discuss how a generalization of
the reinforcement learning or optimal control problem, which is sometimes
termed maximum entropy reinforcement learning, is equivalent to exact
probabilistic inference in the case of deterministic dynamics, and variational
inference in the case of stochastic dynamics. We will present a detailed
derivation of this framework, overview prior work that has drawn on this and
related ideas to propose new reinforcement learning and control algorithms, and
describe perspectives on future research.","Levine, Sergey",2018,,,,arXiv preprint arXiv:1805.00909
Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,http://arxiv.org/abs/2509.21170v1,nigam1999using,\cite{nigam1999using},Using maximum entropy for text classification,,,"Nigam, Kamal and Lafferty, John and McCallum, Andrew",1999,,,,
Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,http://arxiv.org/abs/2509.21170v1,el2015arabic,\cite{el2015arabic},Arabic text classification using maximum entropy,,,"El-Halees, Alaa M",2015,,,,IUG Journal of Natural Studies
Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,http://arxiv.org/abs/2509.21170v1,och2002discriminative,\cite{och2002discriminative},Discriminative training and maximum entropy models for statistical machine translation,,,"Och, Franz Josef and Ney, Hermann",2002,,,,
Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,http://arxiv.org/abs/2509.21170v1,ittycheriah2005maximum,\cite{ittycheriah2005maximum},A maximum entropy word aligner for arabic-english machine translation,,,"Ittycheriah, Abraham and Roukos, Salim",2005,,,,
