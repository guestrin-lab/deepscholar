parent_paper_title,parent_arxiv_link,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,Anderson01041995,\cite{Anderson01041995},Cognitive Tutors: Lessons Learned,,,John R. Anderson and Albert T. Corbett and Kenneth R. Koedinger and Ray Pelletier,1995,,https://doi.org/10.1207/s15327809jls0402_2,10.1207/s15327809jls0402_2,Journal of the Learning Sciences
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,article,\cite{article},Cognitive Anatomy of Tutor Learning: Lessons Learned With SimStudent,,,Noboru Matsuda and Evelyn Yarzebinski and Victoria Keiser and Rohan Raizada and William Cohen and Gabriel Stylianides and Kenneth Koedinger,2013,sep,,10.1037/a0031955,Journal of Educational Psychology
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,christensen2011simschool,\cite{christensen2011simschool},SimSchool: An online dynamic simulator for enhancing teacher preparation,,,Rhonda Christensen and Gerald Knezek and Tandra Tyler-Wood and David Gibson,2011,,,,International Journal of Learning Technology
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,foley2005making,\cite{foley2005making},Making it real: Sim-school a backdrop for contextualizing teacher preparation,,,Jean Ann Foley and Gretchen McAllister,2005,,,,AACE Review
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,carrington2011enhancing,\cite{carrington2011enhancing},Enhancing the development of pre-service teacher professional identity via an online classroom simulation,,,Lisa Carrington and Lisa Kervin and Brian Ferry,2011,,,,Journal of Technology and Teacher Education
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,dotger2010medicine,\cite{dotger2010medicine},From medicine to teaching: The evolution of the simulated interaction model,,,Benjamin H. Dotger and Sharon C. Dotger and Michael J. Maher,2010,,,,Innovative Higher Education
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,kervin2006classsim,\cite{kervin2006classsim},ClassSim: Preparing tomorrows teachers for classroom reality,,,Lisa Kervin and Brian Ferry and Lisa Carrington,2006,,,,
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,dieker2015tle,\cite{dieker2015tle},TLE TeachLivE™: Using technology to provide quality professional development in rural schools,,,Lisa A. Dieker and Michael C. Hynes and Charles E. Hughes and Stacey Hardin and Kathleen Becht,2015,,,,Rural Special Education Quarterly
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,delamarre2021interactive,\cite{delamarre2021interactive},The interactive virtual training for teachers (IVT-T) to practice classroom behavior management,,,A. Delamarre and Elisa Shernoff and Cédric Buche and Stacy Frazier and Joe Gabbard and C. Lisetti,2021,,,,International Journal of Human-Computer Studies
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,shernoff2018early,\cite{shernoff2018early},Early career teacher professional development: Bridging simulation technology with evidence-based behavior management,,,Elisa Shernoff and Stacy Frazier and Christine Lisetti and Cedric Buche and Stephanie Lunn and Claire Brown and Alban Delamarre and Tommy Chou and Joseph Gabbard and Emily Morgan,2018,,,,Journal of Technology and Teacher Education
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,kelleci2021using,\cite{kelleci2021using},Using game-based virtual classroom simulation in teacher training: User experience research,,,Özge Kelleci and Nuri Can Aksoy,2021,,,,Simulation \& Gaming
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,zhang_simulating_2024,\cite{zhang_simulating_2024},Simulating Classroom Education with LLM-Empowered Agents,http://arxiv.org/abs/2406.19226v2,"Large language models (LLMs) have been applied across various intelligent
educational tasks to assist teaching. While preliminary studies have focused on
task-specific, independent LLM-empowered agents, the potential of LLMs within a
multi-agent collaborative framework for classroom simulation with real user
participation remains unexplored. In this work, we propose SimClass, a
multi-agent classroom simulation teaching framework. We recognize
representative class roles and introduce a novel class control mechanism for
automatic classroom teaching, and conduct user experiments in two real-world
courses. Using the Flanders Interactive Analysis System and Community of
Inquiry theoretical frameworks from educational analysis, we demonstrate that
LLMs can simulate a dynamic learning environment for users with active
teacher-student and student-student interactions. We also observe group
behaviors among agents in SimClass, where agents collaborate to create
enlivening interactions in classrooms to improve user learning process. We hope
this work pioneers the application of LLM-empowered multi-agent systems in
virtual classroom teaching.",Zheyuan Zhang and Daniel Zhang-Li and Jifan Yu and Linlu Gong and Jinchang Zhou and Zhanxin Hao and Jianxiao Jiang and Jie Cao and Huiqin Liu and Zhiyuan Liu and Lei Hou and Juanzi Li,2024,nov,https://arxiv.org/abs/2406.19226,10.48550/arXiv.2406.19226,
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,lee_generative_nodate,\cite{lee_generative_nodate},"Generative Agent for Teacher Training: Designing Educational Problem-Solving Simulations
               with Large Language Model–based Agents for Pre-Service Teachers",,,"Unggi Lee and Sanghyeok Lee and Junbo Koh and Yeil Jeong and Haewon Jung
               and Gyuri Byun and Yunseo Lee and Jewoong Moon and Jieun Lim and Hyeoncheol Kim",2023,dec,https://gaied.org/neurips2023/files/8/8_poster.pdf,,
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,yue_mathvc_2025,\cite{yue_mathvc_2025},"MathVC: An LLM-Simulated Multi-Character Virtual Classroom for
  Mathematics Education",http://arxiv.org/abs/2404.06711v3,"Collaborative problem solving (CPS) is essential in mathematics education,
fostering deeper learning through the exchange of ideas. Yet, classrooms often
lack the resources, time, and peer dynamics needed to sustain productive CPS.
Recent advancements in Large Language Models (LLMs) offer a promising avenue to
enhance CPS in mathematical education. We designed and developed MathVC, a
multi-persona LLM simulated virtual classroom platform to facilitate CPS in
mathematics. MathVC combines a meta planning controller that monitors CPS
stages-sense-making, team organization, planning, execution, validation, and
predicts the next speaker, with a persona simulation stack that encodes
mathematical thinking via a task schema and error-injected persona schemas
seeded from teacher-specified misconceptions. We evaluated MathVC with 14 U.S.
middle schoolers. Students reported constructive interaction and reaching
shared solutions, describing gains in engagement, motivation, and confidence
through diverse perspectives, immediate scaffolding, and human-like
fallibility. Our findings also provide insights into simulating peers via
LLM-based technologies for collaboration to support learning.",Murong Yue and Wenhan Lyu and Wijdane Mifdal and Jennifer Suh and Yixuan Zhang and Ziyu Yao,2025,jan,https://arxiv.org/abs/2404.06711,,
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,mollick_ai_2024,\cite{mollick_ai_2024},AI Agents and Education: Simulated Practice at Scale,http://arxiv.org/abs/2407.12796v1,"This paper explores the potential of generative AI in creating adaptive
educational simulations. By leveraging a system of multiple AI agents,
simulations can provide personalized learning experiences, offering students
the opportunity to practice skills in scenarios with AI-generated mentors,
role-players, and instructor-facing evaluators. We describe a prototype,
PitchQuest, a venture capital pitching simulator that showcases the
capabilities of AI in delivering instruction, facilitating practice, and
providing tailored feedback. The paper discusses the pedagogy behind the
simulation, the technology powering it, and the ethical considerations in using
AI for education. While acknowledging the limitations and need for rigorous
testing, we propose that generative AI can significantly lower the barriers to
creating effective, engaging simulations, opening up new possibilities for
experiential learning at scale.",Ethan Mollick and Lilach Mollick and Natalie Bach and L.~J. Ciccarelli and Ben Przystanski and Daniel Ravipinto,2024,jun,https://arxiv.org/abs/2407.12796,10.48550/arXiv.2407.12796,
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,markel_gpteach_2023,\cite{markel_gpteach_2023},GPTeach: Interactive TA Training with GPT-based Students,,,Julia M. Markel and Steven G. Opferman and James A. Landay and Chris Piech,2023,jul,https://dl.acm.org/doi/10.1145/3573051.3593393,10.1145/3573051.3593393,
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,wang_generative_2025,\cite{wang_generative_2025},Generative Co-Learners: Enhancing Cognitive and Social Presence of Students in Asynchronous Learning with Generative AI,,,Tianjia Wang and Tong Wu and Huayi Liu and Chris Brown and Yan Chen,2025,jan,https://dl.acm.org/doi/10.1145/3701198,10.1145/3701198,Proc. ACM Hum.-Comput. Interact.
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,fahid_online_2024,\cite{fahid_online_2024},Online Reinforcement Learning–Based Pedagogical Planning for Narrative-Centered Learning Environments,,,Fahmid Morshed Fahid and Jonathan Rowe and Yeojin Kim and Shashank Srivastava and James Lester,2024,mar,https://ojs.aaai.org/index.php/AAAI/article/view/30365,10.1609/aaai.v38i21.30365,Proceedings of the AAAI Conference on Artificial Intelligence
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,park2023generative,\cite{park2023generative},Generative Agents: Interactive Simulacra of Human Behavior,http://arxiv.org/abs/2304.03442v2,"Believable proxies of human behavior can empower interactive applications
ranging from immersive environments to rehearsal spaces for interpersonal
communication to prototyping tools. In this paper, we introduce generative
agents--computational software agents that simulate believable human behavior.
Generative agents wake up, cook breakfast, and head to work; artists paint,
while authors write; they form opinions, notice each other, and initiate
conversations; they remember and reflect on days past as they plan the next
day. To enable generative agents, we describe an architecture that extends a
large language model to store a complete record of the agent's experiences
using natural language, synthesize those memories over time into higher-level
reflections, and retrieve them dynamically to plan behavior. We instantiate
generative agents to populate an interactive sandbox environment inspired by
The Sims, where end users can interact with a small town of twenty five agents
using natural language. In an evaluation, these generative agents produce
believable individual and emergent social behaviors: for example, starting with
only a single user-specified notion that one agent wants to throw a Valentine's
Day party, the agents autonomously spread invitations to the party over the
next two days, make new acquaintances, ask each other out on dates to the
party, and coordinate to show up for the party together at the right time. We
demonstrate through ablation that the components of our agent
architecture--observation, planning, and reflection--each contribute critically
to the believability of agent behavior. By fusing large language models with
computational, interactive agents, this work introduces architectural and
interaction patterns for enabling believable simulations of human behavior.",Joon Sung Park and Joseph O’Brien and Carrie Jun Cai and Meredith Ringel Morris and Percy Liang and Michael S. Bernstein,2023,,,,
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,li2023camelcommunicativeagentsmind,\cite{li2023camelcommunicativeagentsmind},"CAMEL: Communicative Agents for ""Mind"" Exploration of Large Language
  Model Society",http://arxiv.org/abs/2303.17760v2,"The rapid advancement of chat-based language models has led to remarkable
progress in complex task-solving. However, their success heavily relies on
human input to guide the conversation, which can be challenging and
time-consuming. This paper explores the potential of building scalable
techniques to facilitate autonomous cooperation among communicative agents, and
provides insight into their ""cognitive"" processes. To address the challenges of
achieving autonomous cooperation, we propose a novel communicative agent
framework named role-playing. Our approach involves using inception prompting
to guide chat agents toward task completion while maintaining consistency with
human intentions. We showcase how role-playing can be used to generate
conversational data for studying the behaviors and capabilities of a society of
agents, providing a valuable resource for investigating conversational language
models. In particular, we conduct comprehensive studies on
instruction-following cooperation in multi-agent settings. Our contributions
include introducing a novel communicative agent framework, offering a scalable
approach for studying the cooperative behaviors and capabilities of multi-agent
systems, and open-sourcing our library to support research on communicative
agents and beyond: https://github.com/camel-ai/camel.",Guohao Li and Hasan Abed Al Kader Hammoud and Hani Itani and Dmitrii Khizbullin and Bernard Ghanem,2023,,https://arxiv.org/abs/2303.17760,,
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,chen2023agentversefacilitatingmultiagentcollaboration,\cite{chen2023agentversefacilitatingmultiagentcollaboration},"AgentVerse: Facilitating Multi-Agent Collaboration and Exploring
  Emergent Behaviors",http://arxiv.org/abs/2308.10848v3,"Autonomous agents empowered by Large Language Models (LLMs) have undergone
significant improvements, enabling them to generalize across a broad spectrum
of tasks. However, in real-world scenarios, cooperation among individuals is
often required to enhance the efficiency and effectiveness of task
accomplishment. Hence, inspired by human group dynamics, we propose a
multi-agent framework \framework that can collaboratively and dynamically
adjust its composition as a greater-than-the-sum-of-its-parts system. Our
experiments demonstrate that \framework framework can effectively deploy
multi-agent groups that outperform a single agent. Furthermore, we delve into
the emergence of social behaviors among individual agents within a group during
collaborative task accomplishment. In view of these behaviors, we discuss some
possible strategies to leverage positive ones and mitigate negative ones for
improving the collaborative potential of multi-agent groups. Our codes for
\framework will soon be released at
\url{https://github.com/OpenBMB/AgentVerse}.",Weize Chen and Yusheng Su and Jingwei Zuo and Cheng Yang and Chenfei Yuan and Chi-Min Chan and Heyang Yu and Yaxi Lu and Yi-Hsin Hung and Chen Qian and Yujia Qin and Xin Cong and Ruobing Xie and Zhiyuan Liu and Maosong Sun and Jie Zhou,2023,,https://arxiv.org/abs/2308.10848,,
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,jinxin_cgmi_2023,\cite{jinxin_cgmi_2023},CGMI: Configurable General Multi-Agent Interaction Framework,http://arxiv.org/abs/2308.12503v2,"Benefiting from the powerful capabilities of large language models (LLMs),
agents based on LLMs have shown the potential to address domain-specific tasks
and emulate human behaviors. However, the content generated by these agents
remains somewhat superficial, owing to their limited domain expertise and the
absence of an effective cognitive architecture. To address this, we present the
Configurable General Multi-Agent Interaction (CGMI) framework, designed to
replicate human interactions in real-world scenarios. Specifically, we propose
a tree-structured methodology for the assignment, detection, and maintenance of
agent personality. Additionally, we designed a cognitive architecture equipped
with a skill library based on the ACT* model, which contains memory,
reflection, and planning modules. We have also integrated general agents to
augment the virtual environment's realism. Using the CGMI framework, we
simulated numerous classroom interactions between teacher and students. The
experiments indicate that aspects such as the teaching methodology, curriculum,
and student performance closely mirror real classroom settings. We will open
source our work.",Shi Jinxin and Zhao Jiabao and Wang Yilei and Wu Xingjiao and Li Jiawen and He Liang,2023,aug,https://arxiv.org/abs/2308.12503,10.48550/arXiv.2308.12503,
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,shao_character-llm_2023,\cite{shao_character-llm_2023},Character-LLM: A Trainable Agent for Role-Playing,,,Yunfan Shao and Linyang Li and Junqi Dai and Xipeng Qiu,2023,dec,https://arxiv.org/abs/2310.10158,,
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,jiang2024personallminvestigatingabilitylarge,\cite{jiang2024personallminvestigatingabilitylarge},"PersonaLLM: Investigating the Ability of Large Language Models to
  Express Personality Traits",http://arxiv.org/abs/2305.02547v5,"Despite the many use cases for large language models (LLMs) in creating
personalized chatbots, there has been limited research on evaluating the extent
to which the behaviors of personalized LLMs accurately and consistently reflect
specific personality traits. We consider studying the behavior of LLM-based
agents which we refer to as LLM personas and present a case study with GPT-3.5
and GPT-4 to investigate whether LLMs can generate content that aligns with
their assigned personality profiles. To this end, we simulate distinct LLM
personas based on the Big Five personality model, have them complete the
44-item Big Five Inventory (BFI) personality test and a story writing task, and
then assess their essays with automatic and human evaluations. Results show
that LLM personas' self-reported BFI scores are consistent with their
designated personality types, with large effect sizes observed across five
traits. Additionally, LLM personas' writings have emerging representative
linguistic patterns for personality traits when compared with a human writing
corpus. Furthermore, human evaluation shows that humans can perceive some
personality traits with an accuracy of up to 80%. Interestingly, the accuracy
drops significantly when the annotators were informed of AI authorship.",Hang Jiang and Xiajie Zhang and Xubo Cao and Cynthia Breazeal and Deb Roy and Jad Kabbara,2024,,https://arxiv.org/abs/2305.02547,,
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,wang2024rolellmbenchmarkingelicitingenhancing,\cite{wang2024rolellmbenchmarkingelicitingenhancing},"RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities
  of Large Language Models",http://arxiv.org/abs/2310.00746v3,"The advent of Large Language Models (LLMs) has paved the way for complex
tasks such as role-playing, which enhances user interactions by enabling models
to imitate various characters. However, the closed-source nature of
state-of-the-art LLMs and their general-purpose training limit role-playing
optimization. In this paper, we introduce RoleLLM, a framework to benchmark,
elicit, and enhance role-playing abilities in LLMs. RoleLLM comprises four
stages: (1) Role Profile Construction for 100 roles; (2) Context-Based
Instruction Generation (Context-Instruct) for role-specific knowledge
extraction; (3) Role Prompting using GPT (RoleGPT) for speaking style
imitation; and (4) Role-Conditioned Instruction Tuning (RoCIT) for fine-tuning
open-source models along with role customization. By Context-Instruct and
RoleGPT, we create RoleBench, the first systematic and fine-grained
character-level benchmark dataset for role-playing with 168,093 samples.
Moreover, RoCIT on RoleBench yields RoleLLaMA (English) and RoleGLM (Chinese),
significantly enhancing role-playing abilities and even achieving comparable
results with RoleGPT (using GPT-4).",Zekun Moore Wang and Zhongyuan Peng and Haoran Que and Jiaheng Liu and Wangchunshu Zhou and Yuhan Wu and Hongcheng Guo and Ruitong Gan and Zehao Ni and Jian Yang and Man Zhang and Zhaoxiang Zhang and Wanli Ouyang and Ke Xu and Stephen W. Huang and Jie Fu and Junran Peng,2024,,https://arxiv.org/abs/2310.00746,,
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,tseng-etal-2024-two,\cite{tseng-etal-2024-two},"Two Tales of Persona in LLMs: A Survey of Role-Playing and
  Personalization",http://arxiv.org/abs/2406.01171v3,"The concept of persona, originally adopted in dialogue literature, has
re-surged as a promising framework for tailoring large language models (LLMs)
to specific context (e.g., personalized search, LLM-as-a-judge). However, the
growing research on leveraging persona in LLMs is relatively disorganized and
lacks a systematic taxonomy. To close the gap, we present a comprehensive
survey to categorize the current state of the field. We identify two lines of
research, namely (1) LLM Role-Playing, where personas are assigned to LLMs, and
(2) LLM Personalization, where LLMs take care of user personas. Additionally,
we introduce existing methods for LLM personality evaluation. To the best of
our knowledge, we present the first survey for role-playing and personalization
in LLMs under the unified view of persona. We continuously maintain a paper
collection to foster future endeavors:
https://github.com/MiuLab/PersonaLLM-Survey",Yu-Min Tseng and Yu-Chao Huang and Teng-Yun Hsiao and Wei-Lin Chen and Chao-Wei Huang and Yu Meng and Yun-Nung Chen,2024,nov,https://aclanthology.org/2024.findings-emnlp.969/,10.18653/v1/2024.findings-emnlp.969,
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,zhong2023memorybankenhancinglargelanguage,\cite{zhong2023memorybankenhancinglargelanguage},MemoryBank: Enhancing Large Language Models with Long-Term Memory,http://arxiv.org/abs/2305.10250v3,"Revolutionary advancements in Large Language Models have drastically reshaped
our interactions with artificial intelligence systems. Despite this, a notable
hindrance remains-the deficiency of a long-term memory mechanism within these
models. This shortfall becomes increasingly evident in situations demanding
sustained interaction, such as personal companion systems and psychological
counseling. Therefore, we propose MemoryBank, a novel memory mechanism tailored
for LLMs. MemoryBank enables the models to summon relevant memories,
continually evolve through continuous memory updates, comprehend, and adapt to
a user personality by synthesizing information from past interactions. To mimic
anthropomorphic behaviors and selectively preserve memory, MemoryBank
incorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting
Curve theory, which permits the AI to forget and reinforce memory based on time
elapsed and the relative significance of the memory, thereby offering a
human-like memory mechanism. MemoryBank is versatile in accommodating both
closed-source models like ChatGPT and open-source models like ChatGLM. We
exemplify application of MemoryBank through the creation of an LLM-based
chatbot named SiliconFriend in a long-term AI Companion scenario. Further tuned
with psychological dialogs, SiliconFriend displays heightened empathy in its
interactions. Experiment involves both qualitative analysis with real-world
user dialogs and quantitative analysis with simulated dialogs. In the latter,
ChatGPT acts as users with diverse characteristics and generates long-term
dialog contexts covering a wide array of topics. The results of our analysis
reveal that SiliconFriend, equipped with MemoryBank, exhibits a strong
capability for long-term companionship as it can provide emphatic response,
recall relevant memories and understand user personality.",Wanjun Zhong and Lianghong Guo and Qiqi Gao and He Ye and Yanlin Wang,2023,,https://arxiv.org/abs/2305.10250,,
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,roy2023conversationstyletransferusing,\cite{roy2023conversationstyletransferusing},Conversation Style Transfer using Few-Shot Learning,http://arxiv.org/abs/2302.08362v2,"Conventional text style transfer approaches focus on sentence-level style
transfer without considering contextual information, and the style is described
with attributes (e.g., formality). When applying style transfer in
conversations such as task-oriented dialogues, existing approaches suffer from
these limitations as context can play an important role and the style
attributes are often difficult to define in conversations. In this paper, we
introduce conversation style transfer as a few-shot learning problem, where the
model learns to perform style transfer by observing only a few example
dialogues in the target style. We propose a novel in-context learning approach
to solve the task with style-free dialogues as a pivot. Human evaluation shows
that by incorporating multi-turn context, the model is able to match the target
style while having better appropriateness and semantic correctness compared to
utterance/sentence-level style transfer. Additionally, we show that
conversation style transfer can also benefit downstream tasks. For example, in
multi-domain intent classification tasks, the F1 scores improve after
transferring the style of training data to match the style of the test data.",Shamik Roy and Raphael Shu and Nikolaos Pappas and Elman Mansimov and Yi Zhang and Saab Mansour and Dan Roth,2023,,https://arxiv.org/abs/2302.08362,,
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,madaan2023selfrefineiterativerefinementselffeedback,\cite{madaan2023selfrefineiterativerefinementselffeedback},Self-Refine: Iterative Refinement with Self-Feedback,http://arxiv.org/abs/2303.17651v2,"Like humans, large language models (LLMs) do not always generate the best
output on their first try. Motivated by how humans refine their written text,
we introduce Self-Refine, an approach for improving initial outputs from LLMs
through iterative feedback and refinement. The main idea is to generate an
initial output using an LLMs; then, the same LLMs provides feedback for its
output and uses it to refine itself, iteratively. Self-Refine does not require
any supervised training data, additional training, or reinforcement learning,
and instead uses a single LLM as the generator, refiner, and feedback provider.
We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response
generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT,
and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine
are preferred by humans and automatic metrics over those generated with the
same LLM using conventional one-step generation, improving by ~20% absolute on
average in task performance. Our work demonstrates that even state-of-the-art
LLMs like GPT-4 can be further improved at test time using our simple,
standalone approach.",Aman Madaan and Niket Tandon and Prakhar Gupta and Skyler Hallinan and Luyu Gao and Sarah Wiegreffe and Uri Alon and Nouha Dziri and Shrimai Prabhumoye and Yiming Yang and Shashank Gupta and Bodhisattwa Prasad Majumder and Katherine Hermann and Sean Welleck and Amir Yazdanbakhsh and Peter Clark,2023,,https://arxiv.org/abs/2303.17651,,
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,li_exploring_2025,\cite{li_exploring_2025},Exploring LLM-based Student Simulation for Metacognitive Cultivation,http://arxiv.org/abs/2502.11678v2,"Metacognitive education plays a crucial role in cultivating students'
self-regulation and reflective thinking, providing essential support for those
with learning difficulties through academic advising. Simulating students with
insufficient learning capabilities using large language models offers a
promising approach to refining pedagogical methods without ethical concerns.
However, existing simulations often fail to authentically represent students'
learning struggles and face challenges in evaluation due to the lack of
reliable metrics and ethical constraints in data collection. To address these
issues, we propose a pipeline for automatically generating and filtering
high-quality simulated student agents. Our approach leverages a two-round
automated scoring system validated by human experts and employs a score
propagation module to obtain more consistent scores across the student graph.
Experimental results demonstrate that our pipeline efficiently identifies
high-quality student agents, and we discuss the traits that influence the
simulation's effectiveness. By simulating students with varying degrees of
learning difficulties, our work paves the way for broader applications in
personalized learning and educational assessment.",Haoxuan Li and Jifan Yu and Xin Cong and Yang Dang and Zhang-li Daniel and Yisi Zhan and Huiqin Liu and Zhiyuan Liu,2025,apr,https://arxiv.org/abs/2502.11678,,
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,didolkar2024metacognitivecapabilitiesllmsexploration,\cite{didolkar2024metacognitivecapabilitiesllmsexploration},"Metacognitive Capabilities of LLMs: An Exploration in Mathematical
  Problem Solving",http://arxiv.org/abs/2405.12205v1,"Metacognitive knowledge refers to humans' intuitive knowledge of their own
thinking and reasoning processes. Today's best LLMs clearly possess some
reasoning processes. The paper gives evidence that they also have metacognitive
knowledge, including ability to name skills and procedures to apply given a
task. We explore this primarily in context of math reasoning, developing a
prompt-guided interaction procedure to get a powerful LLM to assign sensible
skill labels to math questions, followed by having it perform semantic
clustering to obtain coarser families of skill labels. These coarse skill
labels look interpretable to humans.
  To validate that these skill labels are meaningful and relevant to the LLM's
reasoning processes we perform the following experiments. (a) We ask GPT-4 to
assign skill labels to training questions in math datasets GSM8K and MATH. (b)
When using an LLM to solve the test questions, we present it with the full list
of skill labels and ask it to identify the skill needed. Then it is presented
with randomly selected exemplar solved questions associated with that skill
label. This improves accuracy on GSM8k and MATH for several strong LLMs,
including code-assisted models. The methodology presented is domain-agnostic,
even though this article applies it to math problems.",Aniket Didolkar and Anirudh Goyal and Nan Rosemary Ke and Siyuan Guo and Michal Valko and Timothy Lillicrap and Danilo Rezende and Yoshua Bengio and Michael Mozer and Sanjeev Arora,2024,,https://arxiv.org/abs/2405.12205,,
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,wang2024comprehensivesurveycontinuallearning,\cite{wang2024comprehensivesurveycontinuallearning},"A Comprehensive Survey of Continual Learning: Theory, Method and
  Application",http://arxiv.org/abs/2302.00487v3,"To cope with real-world dynamics, an intelligent system needs to
incrementally acquire, update, accumulate, and exploit knowledge throughout its
lifetime. This ability, known as continual learning, provides a foundation for
AI systems to develop themselves adaptively. In a general sense, continual
learning is explicitly limited by catastrophic forgetting, where learning a new
task usually results in a dramatic performance degradation of the old tasks.
Beyond this, increasingly numerous advances have emerged in recent years that
largely extend the understanding and application of continual learning. The
growing and widespread interest in this direction demonstrates its realistic
significance as well as complexity. In this work, we present a comprehensive
survey of continual learning, seeking to bridge the basic settings, theoretical
foundations, representative methods, and practical applications. Based on
existing theoretical and empirical results, we summarize the general objectives
of continual learning as ensuring a proper stability-plasticity trade-off and
an adequate intra/inter-task generalizability in the context of resource
efficiency. Then we provide a state-of-the-art and elaborated taxonomy,
extensively analyzing how representative methods address continual learning,
and how they are adapted to particular challenges in realistic applications.
Through an in-depth discussion of promising directions, we believe that such a
holistic perspective can greatly facilitate subsequent exploration in this
field and beyond.",Liyuan Wang and Xingxing Zhang and Hang Su and Jun Zhu,2024,,https://arxiv.org/abs/2302.00487,,
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,parisi_continual_2019,\cite{parisi_continual_2019},Continual Lifelong Learning with Neural Networks: A Review,,,German I. Parisi and Ronald Kemker and Jose L. Part and Christopher Kanan and Stefan Wermter,2019,may,https://doi.org/10.1016/j.neunet.2019.01.012,10.1016/j.neunet.2019.01.012,Neural Networks
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,zheng2024lifelonglearninglargelanguage,\cite{zheng2024lifelonglearninglargelanguage},Towards Lifelong Learning of Large Language Models: A Survey,http://arxiv.org/abs/2406.06391v1,"As the applications of large language models (LLMs) expand across diverse
fields, the ability of these models to adapt to ongoing changes in data, tasks,
and user preferences becomes crucial. Traditional training methods, relying on
static datasets, are increasingly inadequate for coping with the dynamic nature
of real-world information. Lifelong learning, also known as continual or
incremental learning, addresses this challenge by enabling LLMs to learn
continuously and adaptively over their operational lifetime, integrating new
knowledge while retaining previously learned information and preventing
catastrophic forgetting. This survey delves into the sophisticated landscape of
lifelong learning, categorizing strategies into two primary groups: Internal
Knowledge and External Knowledge. Internal Knowledge includes continual
pretraining and continual finetuning, each enhancing the adaptability of LLMs
in various scenarios. External Knowledge encompasses retrieval-based and
tool-based lifelong learning, leveraging external data sources and
computational tools to extend the model's capabilities without modifying core
parameters. The key contributions of our survey are: (1) Introducing a novel
taxonomy categorizing the extensive literature of lifelong learning into 12
scenarios; (2) Identifying common techniques across all lifelong learning
scenarios and classifying existing literature into various technique groups
within each scenario; (3) Highlighting emerging techniques such as model
expansion and data selection, which were less explored in the pre-LLM era.
Through a detailed examination of these groups and their respective categories,
this survey aims to enhance the adaptability, reliability, and overall
performance of LLMs in real-world applications.",Junhao Zheng and Shengjie Qiu and Chengming Shi and Qianli Ma,2024,,https://arxiv.org/abs/2406.06391,,
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,chen_lifelong_nodate,\cite{chen_lifelong_nodate},Lifelong Machine Learning,,,Zhiyuan Chen and Bing Liu,2018,,,,
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,zheng2025lifelonglearninglargelanguage,\cite{zheng2025lifelonglearninglargelanguage},Lifelong Learning of Large Language Model based Agents: A Roadmap,,,Junhao Zheng and Chengming Shi and Xidi Cai and Qiuke Li and Duzhen Zhang and Chenxing Li and Dong Yu and Qianli Ma,2025,,https://arxiv.org/abs/2501.07278,,
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,maharana2024evaluatinglongtermconversationalmemory,\cite{maharana2024evaluatinglongtermconversationalmemory},Evaluating Very Long-Term Conversational Memory of LLM Agents,http://arxiv.org/abs/2402.17753v1,"Existing works on long-term open-domain dialogues focus on evaluating model
responses within contexts spanning no more than five chat sessions. Despite
advancements in long-context large language models (LLMs) and retrieval
augmented generation (RAG) techniques, their efficacy in very long-term
dialogues remains unexplored. To address this research gap, we introduce a
machine-human pipeline to generate high-quality, very long-term dialogues by
leveraging LLM-based agent architectures and grounding their dialogues on
personas and temporal event graphs. Moreover, we equip each agent with the
capability of sharing and reacting to images. The generated conversations are
verified and edited by human annotators for long-range consistency and
grounding to the event graphs. Using this pipeline, we collect LoCoMo, a
dataset of very long-term conversations, each encompassing 300 turns and 9K
tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a
comprehensive evaluation benchmark to measure long-term memory in models,
encompassing question answering, event summarization, and multi-modal dialogue
generation tasks. Our experimental results indicate that LLMs exhibit
challenges in understanding lengthy conversations and comprehending long-range
temporal and causal dynamics within dialogues. Employing strategies like
long-context LLMs or RAG can offer improvements but these models still
substantially lag behind human performance.",Adyasha Maharana and Dong-Ho Lee and Sergey Tulyakov and Mohit Bansal and Francesco Barbieri and Yuwei Fang,2024,,https://arxiv.org/abs/2402.17753,,
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,wang2025recursivelysummarizingenableslongterm,\cite{wang2025recursivelysummarizingenableslongterm},"Recursively Summarizing Enables Long-Term Dialogue Memory in Large
  Language Models",http://arxiv.org/abs/2308.15022v4,"Recently, large language models (LLMs), such as GPT-4, stand out remarkable
conversational abilities, enabling them to engage in dynamic and contextually
relevant dialogues across a wide range of topics. However, given a long
conversation, these chatbots fail to recall past information and tend to
generate inconsistent responses. To address this, we propose to recursively
generate summaries/ memory using large language models (LLMs) to enhance
long-term memory ability. Specifically, our method first stimulates LLMs to
memorize small dialogue contexts and then recursively produce new memory using
previous memory and following contexts. Finally, the chatbot can easily
generate a highly consistent response with the help of the latest memory. We
evaluate our method on both open and closed LLMs, and the experiments on the
widely-used public dataset show that our method can generate more consistent
responses in a long-context conversation. Also, we show that our strategy could
nicely complement both long-context (e.g., 8K and 16K) and retrieval-enhanced
LLMs, bringing further long-term dialogue performance. Notably, our method is a
potential solution to enable the LLM to model the extremely long context. The
code and scripts are released.",Qingyue Wang and Yanhe Fu and Yanan Cao and Shuai Wang and Zhiliang Tian and Liang Ding,2025,,https://arxiv.org/abs/2308.15022,,
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,tan2025prospectretrospectreflectivememory,\cite{tan2025prospectretrospectreflectivememory},"In Prospect and Retrospect: Reflective Memory Management for Long-term
  Personalized Dialogue Agents",http://arxiv.org/abs/2503.08026v2,"Large Language Models (LLMs) have made significant progress in open-ended
dialogue, yet their inability to retain and retrieve relevant information from
long-term interactions limits their effectiveness in applications requiring
sustained personalization. External memory mechanisms have been proposed to
address this limitation, enabling LLMs to maintain conversational continuity.
However, existing approaches struggle with two key challenges. First, rigid
memory granularity fails to capture the natural semantic structure of
conversations, leading to fragmented and incomplete representations. Second,
fixed retrieval mechanisms cannot adapt to diverse dialogue contexts and user
interaction patterns. In this work, we propose Reflective Memory Management
(RMM), a novel mechanism for long-term dialogue agents, integrating forward-
and backward-looking reflections: (1) Prospective Reflection, which dynamically
summarizes interactions across granularities-utterances, turns, and
sessions-into a personalized memory bank for effective future retrieval, and
(2) Retrospective Reflection, which iteratively refines the retrieval in an
online reinforcement learning (RL) manner based on LLMs' cited evidence.
Experiments show that RMM demonstrates consistent improvement across various
metrics and benchmarks. For example, RMM shows more than 10% accuracy
improvement over the baseline without memory management on the LongMemEval
dataset.",Zhen Tan and Jun Yan and I-Hung Hsu and Rujun Han and Zifeng Wang and Long T. Le and Yiwen Song and Yanfei Chen and Hamid Palangi and George Lee and Anand Iyer and Tianlong Chen and Huan Liu and Chen-Yu Lee and Tomas Pfister,2025,,https://arxiv.org/abs/2503.08026,,
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,li2025helloagainllmpoweredpersonalized,\cite{li2025helloagainllmpoweredpersonalized},Hello Again! LLM-powered Personalized Agent for Long-term Dialogue,http://arxiv.org/abs/2406.05925v2,"Open-domain dialogue systems have seen remarkable advancements with the
development of large language models (LLMs). Nonetheless, most existing
dialogue systems predominantly focus on brief single-session interactions,
neglecting the real-world demands for long-term companionship and personalized
interactions with chatbots. Crucial to addressing this real-world need are
event summary and persona management, which enable reasoning for appropriate
long-term dialogue responses. Recent progress in the human-like cognitive and
reasoning capabilities of LLMs suggests that LLM-based agents could
significantly enhance automated perception, decision-making, and
problem-solving. In response to this potential, we introduce a model-agnostic
framework, the Long-term Dialogue Agent (LD-Agent), which incorporates three
independently tunable modules dedicated to event perception, persona
extraction, and response generation. For the event memory module, long and
short-term memory banks are employed to separately focus on historical and
ongoing sessions, while a topic-based retrieval mechanism is introduced to
enhance the accuracy of memory retrieval. Furthermore, the persona module
conducts dynamic persona modeling for both users and agents. The integration of
retrieved memories and extracted personas is subsequently fed into the
generator to induce appropriate responses. The effectiveness, generality, and
cross-domain capabilities of LD-Agent are empirically demonstrated across
various illustrative benchmarks, models, and tasks. The code is released at
https://github.com/leolee99/LD-Agent.",Hao Li and Chenghao Yang and An Zhang and Yang Deng and Xiang Wang and Tat-Seng Chua,2025,,https://arxiv.org/abs/2406.05925,,
EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,http://arxiv.org/abs/2510.05650v1,xu_eduagent_2024,\cite{xu_eduagent_2024},EduAgent: Generative Student Agents in Learning,http://arxiv.org/abs/2404.07963v1,"Student simulation in online education is important to address dynamic
learning behaviors of students with diverse backgrounds. Existing simulation
models based on deep learning usually need massive training data, lacking prior
knowledge in educational contexts. Large language models (LLMs) may contain
such prior knowledge since they are pre-trained from a large corpus. However,
because student behaviors are dynamic and multifaceted with individual
differences, directly prompting LLMs is not robust nor accurate enough to
capture fine-grained interactions among diverse student personas, learning
behaviors, and learning outcomes. This work tackles this problem by presenting
a newly annotated fine-grained large-scale dataset and proposing EduAgent, a
novel generative agent framework incorporating cognitive prior knowledge (i.e.,
theoretical findings revealed in cognitive science) to guide LLMs to first
reason correlations among various behaviors and then make simulations. Our two
experiments show that EduAgent could not only mimic and predict learning
behaviors of real students but also generate realistic learning behaviors of
virtual students without real data.",Songlin Xu and Xinyu Zhang and Lianhui Qin,2024,mar,https://arxiv.org/abs/2404.07963,,
