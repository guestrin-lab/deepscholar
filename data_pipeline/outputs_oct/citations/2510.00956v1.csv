parent_paper_title,parent_arxiv_link,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal
Bridging the Gap Between Simulated and Real Network Data Using Transfer Learning,http://arxiv.org/abs/2510.00956v1,10.5555/2998687.2998769,\cite{10.5555/2998687.2998769},Learning many related tasks at the same time with backpropagation,,,"Caruana, Rich",1994,,,,
Bridging the Gap Between Simulated and Real Network Data Using Transfer Learning,http://arxiv.org/abs/2510.00956v1,pmlr-v15-bengio11b,\cite{pmlr-v15-bengio11b},Deep learners benefit more from out-of-distribution examples,,,"Bengio, Yoshua and others",2011,,,,
Bridging the Gap Between Simulated and Real Network Data Using Transfer Learning,http://arxiv.org/abs/2510.00956v1,10.5555/2969033.2969197,\cite{10.5555/2969033.2969197},How transferable are features in deep neural networks?,http://arxiv.org/abs/1411.1792v1,"Many deep neural networks trained on natural images exhibit a curious
phenomenon in common: on the first layer they learn features similar to Gabor
filters and color blobs. Such first-layer features appear not to be specific to
a particular dataset or task, but general in that they are applicable to many
datasets and tasks. Features must eventually transition from general to
specific by the last layer of the network, but this transition has not been
studied extensively. In this paper we experimentally quantify the generality
versus specificity of neurons in each layer of a deep convolutional neural
network and report a few surprising results. Transferability is negatively
affected by two distinct issues: (1) the specialization of higher layer neurons
to their original task at the expense of performance on the target task, which
was expected, and (2) optimization difficulties related to splitting networks
between co-adapted neurons, which was not expected. In an example network
trained on ImageNet, we demonstrate that either of these two issues may
dominate, depending on whether features are transferred from the bottom,
middle, or top of the network. We also document that the transferability of
features decreases as the distance between the base task and target task
increases, but that transferring features even from distant tasks can be better
than using random features. A final surprising result is that initializing a
network with transferred features from almost any number of layers can produce
a boost to generalization that lingers even after fine-tuning to the target
dataset.","Yosinski, Jason and others",2014,,,,
Bridging the Gap Between Simulated and Real Network Data Using Transfer Learning,http://arxiv.org/abs/2510.00956v1,zeiler2013visualizingunderstandingconvolutionalnetworks,\cite{zeiler2013visualizingunderstandingconvolutionalnetworks},Visualizing and Understanding Convolutional Networks,http://arxiv.org/abs/1311.2901v3,"Large Convolutional Network models have recently demonstrated impressive
classification performance on the ImageNet benchmark. However there is no clear
understanding of why they perform so well, or how they might be improved. In
this paper we address both issues. We introduce a novel visualization technique
that gives insight into the function of intermediate feature layers and the
operation of the classifier. We also perform an ablation study to discover the
performance contribution from different model layers. This enables us to find
model architectures that outperform Krizhevsky \etal on the ImageNet
classification benchmark. We show our ImageNet model generalizes well to other
datasets: when the softmax classifier is retrained, it convincingly beats the
current state-of-the-art results on Caltech-101 and Caltech-256 datasets.",Matthew D Zeiler and Rob Fergus,2013,,https://arxiv.org/abs/1311.2901,,
Bridging the Gap Between Simulated and Real Network Data Using Transfer Learning,http://arxiv.org/abs/2510.00956v1,ferriolgalmés2022routenetfermi,\cite{ferriolgalmés2022routenetfermi},RouteNet-Fermi: Network Modeling with Graph Neural Networks,,,Miquel Ferriol-Galmés and others,2022,,,,
Bridging the Gap Between Simulated and Real Network Data Using Transfer Learning,http://arxiv.org/abs/2510.00956v1,pmlr-v70-gilmer17a,\cite{pmlr-v70-gilmer17a},Neural Message Passing for Quantum Chemistry,http://arxiv.org/abs/1704.01212v2,"Supervised learning on molecules has incredible potential to be useful in
chemistry, drug discovery, and materials science. Luckily, several promising
and closely related neural network models invariant to molecular symmetries
have already been described in the literature. These models learn a message
passing algorithm and aggregation procedure to compute a function of their
entire input graph. At this point, the next step is to find a particularly
effective variant of this general approach and apply it to chemical prediction
benchmarks until we either solve them or reach the limits of the approach. In
this paper, we reformulate existing models into a single common framework we
call Message Passing Neural Networks (MPNNs) and explore additional novel
variations within this framework. Using MPNNs we demonstrate state of the art
results on an important molecular property prediction benchmark; these results
are strong enough that we believe future work should focus on datasets with
larger molecules or more accurate ground truth labels.","Gilmer, Justin and Schoenholz, Samuel S and Riley, Patrick F and Vinyals, Oriol and Dahl, George E",2017,,,,
