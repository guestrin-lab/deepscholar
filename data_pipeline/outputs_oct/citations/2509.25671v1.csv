parent_paper_title,parent_arxiv_link,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,kiela2021dynabenchrethinkingbenchmarkingnlp,\cite{kiela2021dynabenchrethinkingbenchmarkingnlp},Dynabench: Rethinking Benchmarking in NLP,http://arxiv.org/abs/2104.14337v1,"We introduce Dynabench, an open-source platform for dynamic dataset creation
and model benchmarking. Dynabench runs in a web browser and supports
human-and-model-in-the-loop dataset creation: annotators seek to create
examples that a target model will misclassify, but that another person will
not. In this paper, we argue that Dynabench addresses a critical need in our
community: contemporary models quickly achieve outstanding performance on
benchmark tasks but nonetheless fail on simple challenge examples and falter in
real-world scenarios. With Dynabench, dataset creation, model development, and
model assessment can directly inform each other, leading to more robust and
informative benchmarks. We report on four initial NLP tasks, illustrating these
concepts and highlighting the promise of the platform, and address potential
objections to dynamic benchmarking as a new standard for the field.",Douwe Kiela and Max Bartolo and Yixin Nie and Divyansh Kaushik and Atticus Geiger and Zhengxuan Wu and Bertie Vidgen and Grusha Prasad and Amanpreet Singh and Pratik Ringshia and Zhiyi Ma and Tristan Thrush and Sebastian Riedel and Zeerak Waseem and Pontus Stenetorp and Robin Jia and Mohit Bansal and Christopher Potts and Adina Williams,2021,,https://arxiv.org/abs/2104.14337,,
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,chiang2024chatbot,\cite{chiang2024chatbot},Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference,http://arxiv.org/abs/2403.04132v1,"Large Language Models (LLMs) have unlocked new capabilities and applications;
however, evaluating the alignment with human preferences still poses
significant challenges. To address this issue, we introduce Chatbot Arena, an
open platform for evaluating LLMs based on human preferences. Our methodology
employs a pairwise comparison approach and leverages input from a diverse user
base through crowdsourcing. The platform has been operational for several
months, amassing over 240K votes. This paper describes the platform, analyzes
the data we have collected so far, and explains the tried-and-true statistical
methods we are using for efficient and accurate evaluation and ranking of
models. We confirm that the crowdsourced questions are sufficiently diverse and
discriminating and that the crowdsourced human votes are in good agreement with
those of expert raters. These analyses collectively establish a robust
foundation for the credibility of Chatbot Arena. Because of its unique value
and openness, Chatbot Arena has emerged as one of the most referenced LLM
leaderboards, widely cited by leading LLM developers and companies. Our demo is
publicly available at \url{https://chat.lmsys.org}.",Wei-Lin Chiang and Lianmin Zheng and Ying Sheng and Anastasios Nikolas Angelopoulos and Tianle Li and Dacheng Li and Hao Zhang and Banghua Zhu and Michael Jordan and Joseph E. Gonzalez and Ion Stoica,2024,,https://arxiv.org/abs/2403.04132,,
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,nie2020adversarialnlinewbenchmark,\cite{nie2020adversarialnlinewbenchmark},Adversarial NLI: A New Benchmark for Natural Language Understanding,http://arxiv.org/abs/1910.14599v2,"We introduce a new large-scale NLI benchmark dataset, collected via an
iterative, adversarial human-and-model-in-the-loop procedure. We show that
training models on this new dataset leads to state-of-the-art performance on a
variety of popular NLI benchmarks, while posing a more difficult challenge with
its new test set. Our analysis sheds light on the shortcomings of current
state-of-the-art models, and shows that non-expert annotators are successful at
finding their weaknesses. The data collection method can be applied in a
never-ending learning scenario, becoming a moving target for NLU, rather than a
static benchmark that will quickly saturate.",Yixin Nie and Adina Williams and Emily Dinan and Mohit Bansal and Jason Weston and Douwe Kiela,2020,,https://arxiv.org/abs/1910.14599,,
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,croce2021robustbenchstandardizedadversarialrobustness,\cite{croce2021robustbenchstandardizedadversarialrobustness},RobustBench: a standardized adversarial robustness benchmark,http://arxiv.org/abs/2010.09670v3,"As a research community, we are still lacking a systematic understanding of
the progress on adversarial robustness which often makes it hard to identify
the most promising ideas in training robust models. A key challenge in
benchmarking robustness is that its evaluation is often error-prone leading to
robustness overestimation. Our goal is to establish a standardized benchmark of
adversarial robustness, which as accurately as possible reflects the robustness
of the considered models within a reasonable computational budget. To this end,
we start by considering the image classification task and introduce
restrictions (possibly loosened in the future) on the allowed models. We
evaluate adversarial robustness with AutoAttack, an ensemble of white- and
black-box attacks, which was recently shown in a large-scale study to improve
almost all robustness evaluations compared to the original publications. To
prevent overadaptation of new defenses to AutoAttack, we welcome external
evaluations based on adaptive attacks, especially where AutoAttack flags a
potential overestimation of robustness. Our leaderboard, hosted at
https://robustbench.github.io/, contains evaluations of 120+ models and aims at
reflecting the current state of the art in image classification on a set of
well-defined tasks in $\ell_\infty$- and $\ell_2$-threat models and on common
corruptions, with possible extensions in the future. Additionally, we
open-source the library https://github.com/RobustBench/robustbench that
provides unified access to 80+ robust models to facilitate their downstream
applications. Finally, based on the collected models, we analyze the impact of
robustness on the performance on distribution shifts, calibration,
out-of-distribution detection, fairness, privacy leakage, smoothness, and
transferability.",Francesco Croce and Maksym Andriushchenko and Vikash Sehwag and Edoardo Debenedetti and Nicolas Flammarion and Mung Chiang and Prateek Mittal and Matthias Hein,2021,,https://arxiv.org/abs/2010.09670,,
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,deng2024investigatingdatacontaminationmodern,\cite{deng2024investigatingdatacontaminationmodern},"Investigating Data Contamination in Modern Benchmarks for Large Language
  Models",http://arxiv.org/abs/2311.09783v2,"Recent observations have underscored a disparity between the inflated
benchmark scores and the actual performance of LLMs, raising concerns about
potential contamination of evaluation benchmarks. This issue is especially
critical for closed-source models and certain open-source models where training
data transparency is lacking. In this paper we study data contamination by
proposing two methods tailored for both open-source and proprietary LLMs. We
first introduce a retrieval-based system to explore potential overlaps between
evaluation benchmarks and pretraining corpora. We further present a novel
investigation protocol named \textbf{T}estset \textbf{S}lot Guessing
(\textit{TS-Guessing}), applicable to both open and proprietary models. This
approach entails masking a wrong answer in a multiple-choice question and
prompting the model to fill in the gap. Additionally, it involves obscuring an
unlikely word in an evaluation example and asking the model to produce it. We
find that certain commercial LLMs could surprisingly guess the missing option
in various test sets. Specifically, in the TruthfulQA benchmark, we find that
LLMs exhibit notable performance improvement when provided with additional
metadata in the benchmark. Further, in the MMLU benchmark, ChatGPT and GPT-4
demonstrated an exact match rate of 52\% and 57\%, respectively, in guessing
the missing options in benchmark test data. We hope these results underscore
the need for more robust evaluation methodologies and benchmarks in the field.",Chunyuan Deng and Yilun Zhao and Xiangru Tang and Mark Gerstein and Arman Cohan,2024,,https://arxiv.org/abs/2311.09783,,
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,golchin2024timetravelllmstracing,\cite{golchin2024timetravelllmstracing},Time Travel in LLMs: Tracing Data Contamination in Large Language Models,http://arxiv.org/abs/2308.08493v3,"Data contamination, i.e., the presence of test data from downstream tasks in
the training data of large language models (LLMs), is a potential major issue
in measuring LLMs' real effectiveness on other tasks. We propose a
straightforward yet effective method for identifying data contamination within
LLMs. At its core, our approach starts by identifying potential contamination
at the instance level; using this information, our approach then assesses wider
contamination at the partition level. To estimate contamination of individual
instances, we employ ""guided instruction:"" a prompt consisting of the dataset
name, partition type, and the random-length initial segment of a reference
instance, asking the LLM to complete it. An instance is flagged as contaminated
if the LLM's output either exactly or nearly matches the latter segment of the
reference. To understand if an entire partition is contaminated, we propose two
ideas. The first idea marks a dataset partition as contaminated if the average
overlap score with the reference instances (as measured by ROUGE-L or BLEURT)
is statistically significantly better with the completions from guided
instruction compared to a ""general instruction"" that does not include the
dataset and partition name. The second idea marks a dataset partition as
contaminated if a classifier based on GPT-4 with few-shot in-context learning
prompt marks multiple generated completions as exact/near-exact matches of the
corresponding reference instances. Our best method achieves an accuracy between
92% and 100% in detecting if an LLM is contaminated with seven datasets,
containing train and test/validation partitions, when contrasted with manual
evaluation by human experts. Further, our findings indicate that GPT-4 is
contaminated with AG News, WNLI, and XSum datasets.",Shahriar Golchin and Mihai Surdeanu,2024,,https://arxiv.org/abs/2308.08493,,
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,roberts2023datacontaminationlenstime,\cite{roberts2023datacontaminationlenstime},Data Contamination Through the Lens of Time,http://arxiv.org/abs/2310.10628v1,"Recent claims about the impressive abilities of large language models (LLMs)
are often supported by evaluating publicly available benchmarks. Since LLMs
train on wide swaths of the internet, this practice raises concerns of data
contamination, i.e., evaluating on examples that are explicitly or implicitly
included in the training data. Data contamination remains notoriously
challenging to measure and mitigate, even with partial attempts like controlled
experimentation of training data, canary strings, or embedding similarities. In
this work, we conduct the first thorough longitudinal analysis of data
contamination in LLMs by using the natural experiment of training cutoffs in
GPT models to look at benchmarks released over time. Specifically, we consider
two code/mathematical problem-solving datasets, Codeforces and Project Euler,
and find statistically significant trends among LLM pass rate vs. GitHub
popularity and release date that provide strong evidence of contamination. By
open-sourcing our dataset, raw results, and evaluation framework, our work
paves the way for rigorous analyses of data contamination in modern models. We
conclude with a discussion of best practices and future steps for publicly
releasing benchmarks in the age of LLMs that train on webscale data.",Manley Roberts and Himanshu Thakur and Christine Herlihy and Colin White and Samuel Dooley,2023,,https://arxiv.org/abs/2310.10628,,
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,dong-etal-2024-generalization,\cite{dong-etal-2024-generalization},"Generalization or Memorization: Data Contamination and Trustworthy
  Evaluation for Large Language Models",http://arxiv.org/abs/2402.15938v3,"Recent statements about the impressive capabilities of large language models
(LLMs) are usually supported by evaluating on open-access benchmarks.
Considering the vast size and wide-ranging sources of LLMs' training data, it
could explicitly or implicitly include test data, leading to LLMs being more
susceptible to data contamination. However, due to the opacity of training
data, the black-box access of models, and the rapid growth of synthetic
training data, detecting and mitigating data contamination for LLMs faces
significant challenges. In this paper, we propose CDD, which stands for
Contamination Detection via output Distribution for LLMs. CDD necessitates only
the sampled texts to detect data contamination, by identifying the peakedness
of LLM's output distribution. To mitigate the impact of data contamination in
evaluation, we also present TED: Trustworthy Evaluation via output
Distribution, based on the correction of LLM's output distribution. To
facilitate this study, we introduce two benchmarks, i.e., DetCon and ComiEval,
for data contamination detection and contamination mitigation evaluation tasks.
Extensive experimental results show that CDD achieves the average relative
improvements of 21.8\%-30.2\% over other contamination detection approaches in
terms of Accuracy, F1 Score, and AUC metrics, and can effectively detect
implicit contamination. TED substantially mitigates performance improvements up
to 66.9\% attributed to data contamination across various contamination setups.
In real-world applications, we reveal that ChatGPT exhibits a high potential to
suffer from data contamination on HumanEval benchmark.","Dong, Yihong  and
      Jiang, Xue  and
      Liu, Huanyu  and
      Jin, Zhi  and
      Gu, Bin  and
      Yang, Mengfei  and
      Li, Ge",2024,,https://aclanthology.org/2024.findings-acl.716/,10.18653/v1/2024.findings-acl.716,
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,dbevalgauntlet,\cite{dbevalgauntlet},Calibrating the Mosaic Evaluation Gauntlet,,,Tessa Barton,2025,,https://www.databricks.com/blog/author/tessa-barton,,
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,zellers2019hellaswagmachinereallyfinish,\cite{zellers2019hellaswagmachinereallyfinish},HellaSwag: Can a Machine Really Finish Your Sentence?,http://arxiv.org/abs/1905.07830v1,"Recent work by Zellers et al. (2018) introduced a new task of commonsense
natural language inference: given an event description such as ""A woman sits at
a piano,"" a machine must select the most likely followup: ""She sets her fingers
on the keys."" With the introduction of BERT, near human-level performance was
reached. Does this mean that machines can perform human level commonsense
inference?
  In this paper, we show that commonsense inference still proves difficult for
even state-of-the-art models, by presenting HellaSwag, a new challenge dataset.
Though its questions are trivial for humans (>95% accuracy), state-of-the-art
models struggle (<48%). We achieve this via Adversarial Filtering (AF), a data
collection paradigm wherein a series of discriminators iteratively select an
adversarial set of machine-generated wrong answers. AF proves to be
surprisingly robust. The key insight is to scale up the length and complexity
of the dataset examples towards a critical 'Goldilocks' zone wherein generated
text is ridiculous to humans, yet often misclassified by state-of-the-art
models.
  Our construction of HellaSwag, and its resulting difficulty, sheds light on
the inner workings of deep pretrained models. More broadly, it suggests a new
path forward for NLP research, in which benchmarks co-evolve with the evolving
state-of-the-art in an adversarial way, so as to present ever-harder
challenges.",Rowan Zellers and Ari Holtzman and Yonatan Bisk and Ali Farhadi and Yejin Choi,2019,,https://arxiv.org/abs/1905.07830,,
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,talmor2018commonsenseqa,\cite{talmor2018commonsenseqa},"CommonsenseQA: A Question Answering Challenge Targeting Commonsense
  Knowledge",http://arxiv.org/abs/1811.00937v2,"When answering a question, people often draw upon their rich world knowledge
in addition to the particular context. Recent work has focused primarily on
answering questions given some relevant document or context, and required very
little general background. To investigate question answering with prior
knowledge, we present CommonsenseQA: a challenging new dataset for commonsense
question answering. To capture common sense beyond associations, we extract
from ConceptNet (Speer et al., 2017) multiple target concepts that have the
same semantic relation to a single source concept. Crowd-workers are asked to
author multiple-choice questions that mention the source concept and
discriminate in turn between each of the target concepts. This encourages
workers to create questions with complex semantics that often require prior
knowledge. We create 12,247 questions through this procedure and demonstrate
the difficulty of our task with a large number of strong baselines. Our best
baseline is based on BERT-large (Devlin et al., 2018) and obtains 56% accuracy,
well below human performance, which is 89%.",Alon Talmor and Jonathan Herzig and Nicholas Lourie and Jonathan Berant,2018,,,,
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,recht2019imagenetclassifiersgeneralizeimagenet,\cite{recht2019imagenetclassifiersgeneralizeimagenet},Do ImageNet Classifiers Generalize to ImageNet?,http://arxiv.org/abs/1902.10811v2,"We build new test sets for the CIFAR-10 and ImageNet datasets. Both
benchmarks have been the focus of intense research for almost a decade, raising
the danger of overfitting to excessively re-used test sets. By closely
following the original dataset creation processes, we test to what extent
current classification models generalize to new data. We evaluate a broad range
of models and find accuracy drops of 3% - 15% on CIFAR-10 and 11% - 14% on
ImageNet. However, accuracy gains on the original test sets translate to larger
gains on the new test sets. Our results suggest that the accuracy drops are not
caused by adaptivity, but by the models' inability to generalize to slightly
""harder"" images than those found in the original test sets.",Benjamin Recht and Rebecca Roelofs and Ludwig Schmidt and Vaishaal Shankar,2019,,https://arxiv.org/abs/1902.10811,,
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,taori2020measuringrobustnessnaturaldistribution,\cite{taori2020measuringrobustnessnaturaldistribution},"Measuring Robustness to Natural Distribution Shifts in Image
  Classification",http://arxiv.org/abs/2007.00644v2,"We study how robust current ImageNet models are to distribution shifts
arising from natural variations in datasets. Most research on robustness
focuses on synthetic image perturbations (noise, simulated weather artifacts,
adversarial examples, etc.), which leaves open how robustness on synthetic
distribution shift relates to distribution shift arising in real data. Informed
by an evaluation of 204 ImageNet models in 213 different test conditions, we
find that there is often little to no transfer of robustness from current
synthetic to natural distribution shift. Moreover, most current techniques
provide no robustness to the natural distribution shifts in our testbed. The
main exception is training on larger and more diverse datasets, which in
multiple cases increases robustness, but is still far from closing the
performance gaps. Our results indicate that distribution shifts arising in real
data are currently an open research problem. We provide our testbed and data as
a resource for future work at https://modestyachts.github.io/imagenet-testbed/ .",Rohan Taori and Achal Dave and Vaishaal Shankar and Nicholas Carlini and Benjamin Recht and Ludwig Schmidt,2020,,https://arxiv.org/abs/2007.00644,,
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,teney2020valueoutofdistributiontestingexample,\cite{teney2020valueoutofdistributiontestingexample},"On the Value of Out-of-Distribution Testing: An Example of Goodhart's
  Law",http://arxiv.org/abs/2005.09241v1,"Out-of-distribution (OOD) testing is increasingly popular for evaluating a
machine learning system's ability to generalize beyond the biases of a training
set. OOD benchmarks are designed to present a different joint distribution of
data and labels between training and test time. VQA-CP has become the standard
OOD benchmark for visual question answering, but we discovered three troubling
practices in its current use. First, most published methods rely on explicit
knowledge of the construction of the OOD splits. They often rely on
``inverting'' the distribution of labels, e.g. answering mostly 'yes' when the
common training answer is 'no'. Second, the OOD test set is used for model
selection. Third, a model's in-domain performance is assessed after retraining
it on in-domain splits (VQA v2) that exhibit a more balanced distribution of
labels. These three practices defeat the objective of evaluating
generalization, and put into question the value of methods specifically
designed for this dataset. We show that embarrassingly-simple methods,
including one that generates answers at random, surpass the state of the art on
some question types. We provide short- and long-term solutions to avoid these
pitfalls and realize the benefits of OOD evaluation.",Damien Teney and Kushal Kafle and Robik Shrestha and Ehsan Abbasnejad and Christopher Kanan and Anton van den Hengel,2020,,https://arxiv.org/abs/2005.09241,,
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,northcutt2021pervasivelabelerrorstest,\cite{northcutt2021pervasivelabelerrorstest},"Pervasive Label Errors in Test Sets Destabilize Machine Learning
  Benchmarks",http://arxiv.org/abs/2103.14749v4,"We identify label errors in the test sets of 10 of the most commonly-used
computer vision, natural language, and audio datasets, and subsequently study
the potential for these label errors to affect benchmark results. Errors in
test sets are numerous and widespread: we estimate an average of at least 3.3%
errors across the 10 datasets, where for example label errors comprise at least
6% of the ImageNet validation set. Putative label errors are identified using
confident learning algorithms and then human-validated via crowdsourcing (51%
of the algorithmically-flagged candidates are indeed erroneously labeled, on
average across the datasets). Traditionally, machine learning practitioners
choose which model to deploy based on test accuracy - our findings advise
caution here, proposing that judging models over correctly labeled test sets
may be more useful, especially for noisy real-world datasets. Surprisingly, we
find that lower capacity models may be practically more useful than higher
capacity models in real-world datasets with high proportions of erroneously
labeled data. For example, on ImageNet with corrected labels: ResNet-18
outperforms ResNet-50 if the prevalence of originally mislabeled test examples
increases by just 6%. On CIFAR-10 with corrected labels: VGG-11 outperforms
VGG-19 if the prevalence of originally mislabeled test examples increases by
just 5%. Test set errors across the 10 datasets can be viewed at
https://labelerrors.com and all label errors can be reproduced by
https://github.com/cleanlab/label-errors.",Curtis G. Northcutt and Anish Athalye and Jonas Mueller,2021,,https://arxiv.org/abs/2103.14749,,
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,gema2025mmlu,\cite{gema2025mmlu},Are We Done with MMLU?,http://arxiv.org/abs/2406.04127v3,"Maybe not. We identify and analyse errors in the popular Massive Multitask
Language Understanding (MMLU) benchmark. Even though MMLU is widely adopted,
our analysis demonstrates numerous ground truth errors that obscure the true
capabilities of LLMs. For example, we find that 57% of the analysed questions
in the Virology subset contain errors. To address this issue, we introduce a
comprehensive framework for identifying dataset errors using a novel error
annotation protocol. Then, we create MMLU-Redux, which is a subset of 5,700
manually re-annotated questions across all 57 MMLU subjects. We estimate that
6.49% of MMLU questions contain errors. Using MMLU-Redux, we demonstrate
significant discrepancies with the model performance metrics that were
originally reported. Our results strongly advocate for revising MMLU's
error-ridden questions to enhance its future utility and reliability as a
benchmark. https://huggingface.co/datasets/edinburgh-dawg/mmlu-redux-2.0.","Gema, Aryo Pradipta  and
      Leang, Joshua Ong Jun  and
      Hong, Giwon  and
      Devoto, Alessio  and
      Mancino, Alberto Carlo Maria  and
      Saxena, Rohit  and
      He, Xuanli  and
      Zhao, Yu  and
      Du, Xiaotang  and
      Ghasemi Madani, Mohammad Reza  and
      Barale, Claire  and
      McHardy, Robert  and
      Harris, Joshua  and
      Kaddour, Jean  and
      Van Krieken, Emile  and
      Minervini, Pasquale",2025,,https://aclanthology.org/2025.naacl-long.262/,,
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,reuel2024betterbenchassessingaibenchmarks,\cite{reuel2024betterbenchassessingaibenchmarks},"BetterBench: Assessing AI Benchmarks, Uncovering Issues, and
  Establishing Best Practices",http://arxiv.org/abs/2411.12990v1,"AI models are increasingly prevalent in high-stakes environments,
necessitating thorough assessment of their capabilities and risks. Benchmarks
are popular for measuring these attributes and for comparing model performance,
tracking progress, and identifying weaknesses in foundation and non-foundation
models. They can inform model selection for downstream tasks and influence
policy initiatives. However, not all benchmarks are the same: their quality
depends on their design and usability. In this paper, we develop an assessment
framework considering 46 best practices across an AI benchmark's lifecycle and
evaluate 24 AI benchmarks against it. We find that there exist large quality
differences and that commonly used benchmarks suffer from significant issues.
We further find that most benchmarks do not report statistical significance of
their results nor allow for their results to be easily replicated. To support
benchmark developers in aligning with best practices, we provide a checklist
for minimum quality assurance based on our assessment. We also develop a living
repository of benchmark assessments to support benchmark comparability,
accessible at betterbench.stanford.edu.",Anka Reuel and Amelia Hardy and Chandler Smith and Max Lamparth and Malcolm Hardy and Mykel J. Kochenderfer,2024,,https://arxiv.org/abs/2411.12990,,
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,mazumder2023dataperfbenchmarksdatacentricai,\cite{mazumder2023dataperfbenchmarksdatacentricai},DataPerf: Benchmarks for Data-Centric AI Development,http://arxiv.org/abs/2207.10062v4,"Machine learning research has long focused on models rather than datasets,
and prominent datasets are used for common ML tasks without regard to the
breadth, difficulty, and faithfulness of the underlying problems. Neglecting
the fundamental importance of data has given rise to inaccuracy, bias, and
fragility in real-world applications, and research is hindered by saturation
across existing dataset benchmarks. In response, we present DataPerf, a
community-led benchmark suite for evaluating ML datasets and data-centric
algorithms. We aim to foster innovation in data-centric AI through competition,
comparability, and reproducibility. We enable the ML community to iterate on
datasets, instead of just architectures, and we provide an open, online
platform with multiple rounds of challenges to support this iterative
development. The first iteration of DataPerf contains five benchmarks covering
a wide spectrum of data-centric techniques, tasks, and modalities in vision,
speech, acquisition, debugging, and diffusion prompting, and we support hosting
new contributed benchmarks from the community. The benchmarks, online
evaluation platform, and baseline implementations are open source, and the
MLCommons Association will maintain DataPerf to ensure long-term benefits to
academia and industry.",Mark Mazumder and Colby Banbury and Xiaozhe Yao and Bojan Karlaš and William Gaviria Rojas and Sudnya Diamos and Greg Diamos and Lynn He and Alicia Parrish and Hannah Rose Kirk and Jessica Quaye and Charvi Rastogi and Douwe Kiela and David Jurado and David Kanter and Rafael Mosquera and Juan Ciro and Lora Aroyo and Bilge Acun and Lingjiao Chen and Mehul Smriti Raje and Max Bartolo and Sabri Eyuboglu and Amirata Ghorbani and Emmett Goodman and Oana Inel and Tariq Kane and Christine R. Kirkpatrick and Tzu-Sheng Kuo and Jonas Mueller and Tristan Thrush and Joaquin Vanschoren and Margaret Warren and Adina Williams and Serena Yeung and Newsha Ardalani and Praveen Paritosh and Lilith Bat-Leah and Ce Zhang and James Zou and Carole-Jean Wu and Cody Coleman and Andrew Ng and Peter Mattson and Vijay Janapa Reddi,2023,,https://arxiv.org/abs/2207.10062,,
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,gebru2021datasheetsdatasets,\cite{gebru2021datasheetsdatasets},Datasheets for Datasets,http://arxiv.org/abs/1803.09010v8,"The machine learning community currently has no standardized process for
documenting datasets, which can lead to severe consequences in high-stakes
domains. To address this gap, we propose datasheets for datasets. In the
electronics industry, every component, no matter how simple or complex, is
accompanied with a datasheet that describes its operating characteristics, test
results, recommended uses, and other information. By analogy, we propose that
every dataset be accompanied with a datasheet that documents its motivation,
composition, collection process, recommended uses, and so on. Datasheets for
datasets will facilitate better communication between dataset creators and
dataset consumers, and encourage the machine learning community to prioritize
transparency and accountability.",Timnit Gebru and Jamie Morgenstern and Briana Vecchione and Jennifer Wortman Vaughan and Hanna Wallach and Hal Daumé III and Kate Crawford,2021,,https://arxiv.org/abs/1803.09010,,
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,Ott_2022,\cite{Ott_2022},"Mapping global dynamics of benchmark creation and saturation in
  artificial intelligence",http://arxiv.org/abs/2203.04592v4,"Benchmarks are crucial to measuring and steering progress in artificial
intelligence (AI). However, recent studies raised concerns over the state of AI
benchmarking, reporting issues such as benchmark overfitting, benchmark
saturation and increasing centralization of benchmark dataset creation. To
facilitate monitoring of the health of the AI benchmarking ecosystem, we
introduce methodologies for creating condensed maps of the global dynamics of
benchmark creation and saturation. We curated data for 3765 benchmarks covering
the entire domains of computer vision and natural language processing, and show
that a large fraction of benchmarks quickly trended towards near-saturation,
that many benchmarks fail to find widespread utilization, and that benchmark
performance gains for different AI tasks were prone to unforeseen bursts. We
analyze attributes associated with benchmark popularity, and conclude that
future benchmarks should emphasize versatility, breadth and real-world utility.","Ott, Simon and Barbosa-Silva, Adriano and Blagec, Kathrin and Brauner, Jan and Samwald, Matthias",2022,,http://dx.doi.org/10.1038/s41467-022-34591-0,10.1038/s41467-022-34591-0,Nature Communications
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,dehghani2021benchmarklottery,\cite{dehghani2021benchmarklottery},The Benchmark Lottery,http://arxiv.org/abs/2107.07002v1,"The world of empirical machine learning (ML) strongly relies on benchmarks in
order to determine the relative effectiveness of different algorithms and
methods. This paper proposes the notion of ""a benchmark lottery"" that describes
the overall fragility of the ML benchmarking process. The benchmark lottery
postulates that many factors, other than fundamental algorithmic superiority,
may lead to a method being perceived as superior. On multiple benchmark setups
that are prevalent in the ML community, we show that the relative performance
of algorithms may be altered significantly simply by choosing different
benchmark tasks, highlighting the fragility of the current paradigms and
potential fallacious interpretation derived from benchmarking ML methods. Given
that every benchmark makes a statement about what it perceives to be important,
we argue that this might lead to biased progress in the community. We discuss
the implications of the observed phenomena and provide recommendations on
mitigating them using multiple machine learning domains and communities as use
cases, including natural language processing, computer vision, information
retrieval, recommender systems, and reinforcement learning.",Mostafa Dehghani and Yi Tay and Alexey A. Gritsenko and Zhe Zhao and Neil Houlsby and Fernando Diaz and Donald Metzler and Oriol Vinyals,2021,,https://arxiv.org/abs/2107.07002,,
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,singh2025leaderboardillusion,\cite{singh2025leaderboardillusion},The Leaderboard Illusion,http://arxiv.org/abs/2504.20879v2,"Measuring progress is fundamental to the advancement of any scientific field.
As benchmarks play an increasingly central role, they also grow more
susceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard
for ranking the most capable AI systems. Yet, in this work we identify
systematic issues that have resulted in a distorted playing field. We find that
undisclosed private testing practices benefit a handful of providers who are
able to test multiple variants before public release and retract scores if
desired. We establish that the ability of these providers to choose the best
score leads to biased Arena scores due to selective disclosure of performance
results. At an extreme, we identify 27 private LLM variants tested by Meta in
the lead-up to the Llama-4 release. We also establish that proprietary closed
models are sampled at higher rates (number of battles) and have fewer models
removed from the arena than open-weight and open-source alternatives. Both
these policies lead to large data access asymmetries over time. Providers like
Google and OpenAI have received an estimated 19.2% and 20.4% of all data on the
arena, respectively. In contrast, a combined 83 open-weight models have only
received an estimated 29.7% of the total data. We show that access to Chatbot
Arena data yields substantial benefits; even limited additional data can result
in relative performance gains of up to 112% on the arena distribution, based on
our conservative estimates. Together, these dynamics result in overfitting to
Arena-specific dynamics rather than general model quality. The Arena builds on
the substantial efforts of both the organizers and an open community that
maintains this valuable evaluation platform. We offer actionable
recommendations to reform the Chatbot Arena's evaluation framework and promote
fairer, more transparent benchmarking for the field",Shivalika Singh and Yiyang Nan and Alex Wang and Daniel D'Souza and Sayash Kapoor and Ahmet Üstün and Sanmi Koyejo and Yuntian Deng and Shayne Longpre and Noah A. Smith and Beyza Ermis and Marzieh Fadaee and Sara Hooker,2025,,https://arxiv.org/abs/2504.20879,,
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,madaan2024quantifyingvarianceevaluationbenchmarks,\cite{madaan2024quantifyingvarianceevaluationbenchmarks},Quantifying Variance in Evaluation Benchmarks,http://arxiv.org/abs/2406.10229v1,"Evaluation benchmarks are the cornerstone of measuring capabilities of large
language models (LLMs), as well as driving progress in said capabilities.
Originally designed to make claims about capabilities (or lack thereof) in
fully pretrained models, evaluation benchmarks are now also extensively used to
decide between various training choices. Despite this widespread usage, we
rarely quantify the variance in our evaluation benchmarks, which dictates
whether differences in performance are meaningful. Here, we define and measure
a range of metrics geared towards measuring variance in evaluation benchmarks,
including seed variance across initialisations, and monotonicity during
training. By studying a large number of models -- both openly available and
pretrained from scratch -- we provide empirical estimates for a variety of
variance metrics, with considerations and recommendations for practitioners. We
also evaluate the utility and tradeoffs of continuous versus discrete
performance measures and explore options for better understanding and reducing
this variance. We find that simple changes, such as framing choice tasks (like
MMLU) as completion tasks, can often reduce variance for smaller scale
($\sim$7B) models, while more involved methods inspired from human testing
literature (such as item analysis and item response theory) struggle to
meaningfully reduce variance. Overall, our work provides insights into variance
in evaluation benchmarks, suggests LM-specific techniques to reduce variance,
and more generally encourages practitioners to carefully factor in variance
when comparing models.",Lovish Madaan and Aaditya K. Singh and Rylan Schaeffer and Andrew Poulton and Sanmi Koyejo and Pontus Stenetorp and Sharan Narang and Dieuwke Hupkes,2024,,https://arxiv.org/abs/2406.10229,,
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,evalarena,\cite{evalarena},{E}val-{A}rena: noise and errors on LLM evaluations,,,Sida I. Wang and Alex Gu and Lovish Madaan and Dieuwke Hupkes and Jiawei Liu and Yuxiang Wei and Naman Jain and Yuhang Lai and Sten Sootla and Ofir Press and Baptiste Rozière and Gabriel Synnaeve,2024,,,,GitHub repository
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,heineman2025signalnoiseframeworkreducing,\cite{heineman2025signalnoiseframeworkreducing},"Signal and Noise: A Framework for Reducing Uncertainty in Language Model
  Evaluation",http://arxiv.org/abs/2508.13144v1,"Developing large language models is expensive and involves making decisions
with small experiments, typically by evaluating on large, multi-task evaluation
suites. In this work, we analyze specific properties which make a benchmark
more reliable for such decisions, and interventions to design higher-quality
evaluation benchmarks. We introduce two key metrics that show differences in
current benchmarks: signal, a benchmark's ability to separate better models
from worse models, and noise, a benchmark's sensitivity to random variability
between training steps. We demonstrate that benchmarks with a better
signal-to-noise ratio are more reliable when making decisions at small scale,
and those with less noise have lower scaling law prediction error. These
results suggest that improving signal or noise will lead to more useful
benchmarks, so we introduce three interventions designed to directly affect
signal or noise. For example, we propose that switching to a metric that has
better signal and noise (e.g., perplexity rather than accuracy) leads to better
reliability and improved scaling law error. We also find that filtering noisy
subtasks, to improve an aggregate signal-to-noise ratio, leads to more reliable
multi-task evaluations. We also find that averaging the output of a model's
intermediate checkpoints to reduce noise leads to consistent improvements. We
conclude by recommending that those creating new benchmarks, or selecting which
existing benchmarks to use, aim for high signal and low noise. We use 30
benchmarks for these experiments, and 375 open-weight language models from 60M
to 32B parameters, resulting in a new, publicly available dataset of 900K
evaluation benchmark results, totaling 200M instances.",David Heineman and Valentin Hofmann and Ian Magnusson and Yuling Gu and Noah A. Smith and Hannaneh Hajishirzi and Kyle Lo and Jesse Dodge,2025,,,,
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,kaplan2020scalinglawsneurallanguage,\cite{kaplan2020scalinglawsneurallanguage},Scaling Laws for Neural Language Models,http://arxiv.org/abs/2001.08361v1,"We study empirical scaling laws for language model performance on the
cross-entropy loss. The loss scales as a power-law with model size, dataset
size, and the amount of compute used for training, with some trends spanning
more than seven orders of magnitude. Other architectural details such as
network width or depth have minimal effects within a wide range. Simple
equations govern the dependence of overfitting on model/dataset size and the
dependence of training speed on model size. These relationships allow us to
determine the optimal allocation of a fixed compute budget. Larger models are
significantly more sample-efficient, such that optimally compute-efficient
training involves training very large models on a relatively modest amount of
data and stopping significantly before convergence.",Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei,2020,,https://arxiv.org/abs/2001.08361,,
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,perlitz-etal-2024-efficient,\cite{perlitz-etal-2024-efficient},Efficient Benchmarking of Language Models,http://arxiv.org/abs/2308.11696v5,"The increasing versatility of language models (LMs) has given rise to a new
class of benchmarks that comprehensively assess a broad range of capabilities.
Such benchmarks are associated with massive computational costs, extending to
thousands of GPU hours per model. However, the efficiency aspect of these
evaluation efforts had raised little discussion in the literature. In this
work, we present the problem of Efficient Benchmarking, namely, intelligently
reducing the computation costs of LM evaluation without compromising
reliability. Using the HELM benchmark as a test case, we investigate how
different benchmark design choices affect the computation-reliability
trade-off. We propose to evaluate the reliability of such decisions, by using a
new measure -- Decision Impact on Reliability, DIoR for short. We find, for
example, that a benchmark leader may change by merely removing a low-ranked
model from the benchmark, and observe that a correct benchmark ranking can be
obtained by considering only a fraction of the evaluation examples. Based on
our findings, we outline a set of concrete recommendations for efficient
benchmark design and utilization practices. To take a step further, we use our
findings to propose an evaluation algorithm, that, when applied to the HELM
benchmark, leads to dramatic cost savings with minimal loss of benchmark
reliability, often reducing computation by x100 or more.","Perlitz, Yotam  and
      Bandel, Elron  and
      Gera, Ariel  and
      Arviv, Ofir  and
      Ein-Dor, Liat  and
      Shnarch, Eyal  and
      Slonim, Noam  and
      Shmueli-Scheuer, Michal  and
      Choshen, Leshem",2024,,https://aclanthology.org/2024.naacl-long.139/,10.18653/v1/2024.naacl-long.139,
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,rodriguez-etal-2021-evaluation,\cite{rodriguez-etal-2021-evaluation},Evaluation Examples are not Equally Informative: How should that change {NLP} Leaderboards?,,,"Rodriguez, Pedro  and
      Barrow, Joe  and
      Hoyle, Alexander Miserlis  and
      Lalor, John P.  and
      Jia, Robin  and
      Boyd-Graber, Jordan",2021,,https://aclanthology.org/2021.acl-long.346/,10.18653/v1/2021.acl-long.346,
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,Tatsuoka1971StatisticalTO,\cite{Tatsuoka1971StatisticalTO},Statistical Theories of Mental Test Scores.,,,Maurice M. Tatsuoka and Frederic M. Lord and Melvin R. Novick and Allan Birnbaum,1971,,https://api.semanticscholar.org/CorpusID:124110050,,Journal of the American Statistical Association
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,polo2024tinybenchmarksevaluatingllmsfewer,\cite{polo2024tinybenchmarksevaluatingllmsfewer},tinyBenchmarks: evaluating LLMs with fewer examples,http://arxiv.org/abs/2402.14992v2,"The versatility of large language models (LLMs) led to the creation of
diverse benchmarks that thoroughly test a variety of language models'
abilities. These benchmarks consist of tens of thousands of examples making
evaluation of LLMs very expensive. In this paper, we investigate strategies to
reduce the number of evaluations needed to assess the performance of an LLM on
several key benchmarks. For example, we show that to accurately estimate the
performance of an LLM on MMLU, a popular multiple-choice QA benchmark
consisting of 14K examples, it is sufficient to evaluate this LLM on 100
curated examples. We release evaluation tools and tiny versions of popular
benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical
analysis demonstrates that these tools and tiny benchmarks are sufficient to
reliably and efficiently reproduce the original evaluation results.",Felipe Maia Polo and Lucas Weber and Leshem Choshen and Yuekai Sun and Gongjun Xu and Mikhail Yurochkin,2024,,https://arxiv.org/abs/2402.14992,,
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,vivek2024anchorpointsbenchmarkingmodels,\cite{vivek2024anchorpointsbenchmarkingmodels},Anchor Points: Benchmarking Models with Much Fewer Examples,http://arxiv.org/abs/2309.08638v2,"Modern language models often exhibit powerful but brittle behavior, leading
to the development of larger and more diverse benchmarks to reliably assess
their behavior. Here, we suggest that model performance can be benchmarked and
elucidated with much smaller evaluation sets. We first show that in six popular
language classification benchmarks, model confidence in the correct class on
many pairs of points is strongly correlated across models. We build upon this
phenomenon to propose Anchor Point Selection, a technique to select small
subsets of datasets that capture model behavior across the entire dataset.
Anchor points reliably rank models: across 87 diverse language model-prompt
pairs, evaluating models using 1-30 anchor points outperforms uniform sampling
and other baselines at accurately ranking models. Moreover, just several anchor
points can be used to estimate model per-class predictions on all other points
in a dataset with low mean absolute error, sufficient for gauging where the
model is likely to fail. Lastly, we present Anchor Point Maps for visualizing
these insights and facilitating comparisons of the performance of different
models on various regions within the dataset distribution.",Rajan Vivek and Kawin Ethayarajh and Diyi Yang and Douwe Kiela,2024,,https://arxiv.org/abs/2309.08638,,
The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,http://arxiv.org/abs/2509.25671v1,ethayarajh2022understandingdatasetdifficultymathcalvusable,\cite{ethayarajh2022understandingdatasetdifficultymathcalvusable},Understanding Dataset Difficulty with $\mathcal{V}$-Usable Information,,,Kawin Ethayarajh and Yejin Choi and Swabha Swayamdipta,2022,,https://arxiv.org/abs/2110.08420,,
