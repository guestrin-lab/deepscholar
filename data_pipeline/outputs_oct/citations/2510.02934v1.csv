parent_paper_title,parent_arxiv_link,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,zhang2025systematic,\cite{zhang2025systematic},"A Systematic Survey of Text Summarization: From Statistical Methods to
  Large Language Models",http://arxiv.org/abs/2406.11289v1,"Text summarization research has undergone several significant transformations
with the advent of deep neural networks, pre-trained language models (PLMs),
and recent large language models (LLMs). This survey thus provides a
comprehensive review of the research progress and evolution in text
summarization through the lens of these paradigm shifts. It is organized into
two main parts: (1) a detailed overview of datasets, evaluation metrics, and
summarization methods before the LLM era, encompassing traditional statistical
methods, deep learning approaches, and PLM fine-tuning techniques, and (2) the
first detailed examination of recent advancements in benchmarking, modeling,
and evaluating summarization in the LLM era. By synthesizing existing
literature and presenting a cohesive overview, this survey also discusses
research trends, open challenges, and proposes promising research directions in
summarization, aiming to guide researchers through the evolving landscape of
summarization research.","Zhang, Haopeng and Yu, Philip S and Zhang, Jiawei",2025,,,,ACM Computing Surveys
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,li2024flexkbqa,\cite{li2024flexkbqa},"FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base
  Question Answering",http://arxiv.org/abs/2308.12060v3,"Knowledge base question answering (KBQA) is a critical yet challenging task
due to the vast number of entities within knowledge bases and the diversity of
natural language questions posed by users. Unfortunately, the performance of
most KBQA models tends to decline significantly in real-world scenarios where
high-quality annotated data is insufficient. To mitigate the burden associated
with manual annotation, we introduce FlexKBQA by utilizing Large Language
Models (LLMs) as program translators for addressing the challenges inherent in
the few-shot KBQA task. Specifically, FlexKBQA leverages automated algorithms
to sample diverse programs, such as SPARQL queries, from the knowledge base,
which are subsequently converted into natural language questions via LLMs. This
synthetic dataset facilitates training a specialized lightweight model for the
KB. Additionally, to reduce the barriers of distribution shift between
synthetic data and real user questions, FlexKBQA introduces an executionguided
self-training method to iterative leverage unlabeled user questions.
Furthermore, we explore harnessing the inherent reasoning capability of LLMs to
enhance the entire framework. Consequently, FlexKBQA delivers substantial
flexibility, encompassing data annotation, deployment, and being domain
agnostic. Through extensive experiments on GrailQA, WebQSP, and KQA Pro, we
observe that under the few-shot even the more challenging zero-shot scenarios,
FlexKBQA achieves impressive results with a few annotations, surpassing all
previous baselines and even approaching the performance of supervised models,
achieving a remarkable 93% performance relative to the fully-supervised models.
We posit that FlexKBQA represents a significant advancement towards exploring
better integration of large and lightweight models. The code is open-sourced.","Li, Zhenyu and Fan, Sunqi and Gu, Yu and Li, Xiuxing and Duan, Zhichao and Dong, Bowen and Liu, Ning and Wang, Jianyong",2024,,,,
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,wei2023empirical,\cite{wei2023empirical},Empirical study of llm fine-tuning for text classification in legal document review,,,"Wei, Fusheng and Keeling, Robert and Huber-Fliflet, Nathaniel and Zhang, Jianping and Dabrowski, Adam and Yang, Jingchao and Mao, Qiang and Qin, Han",2023,,,,
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,zhang2025teleclass,\cite{zhang2025teleclass},"TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text
  Classification with Minimal Supervision",http://arxiv.org/abs/2403.00165v3,"Hierarchical text classification aims to categorize each document into a set
of classes in a label taxonomy, which is a fundamental web text mining task
with broad applications such as web content analysis and semantic indexing.
Most earlier works focus on fully or semi-supervised methods that require a
large amount of human annotated data which is costly and time-consuming to
acquire. To alleviate human efforts, in this paper, we work on hierarchical
text classification with a minimal amount of supervision: using the sole class
name of each node as the only supervision. Recently, large language models
(LLM) have shown competitive performance on various tasks through zero-shot
prompting, but this method performs poorly in the hierarchical setting because
it is ineffective to include the large and structured label space in a prompt.
On the other hand, previous weakly-supervised hierarchical text classification
methods only utilize the raw taxonomy skeleton and ignore the rich information
hidden in the text corpus that can serve as additional class-indicative
features. To tackle the above challenges, we propose TELEClass, Taxonomy
Enrichment and LLM-Enhanced weakly-supervised hierarchical text Classification,
which combines the general knowledge of LLMs and task-specific features mined
from an unlabeled corpus. TELEClass automatically enriches the raw taxonomy
with class-indicative features for better label space understanding and
utilizes novel LLM-based data annotation and generation methods specifically
tailored for the hierarchical setting. Experiments show that TELEClass can
significantly outperform previous baselines while achieving comparable
performance to zero-shot prompting of LLMs with drastically less inference
cost.","Zhang, Yunyi and Yang, Ruozhen and Xu, Xueqiang and Li, Rui and Xiao, Jinfeng and Shen, Jiaming and Han, Jiawei",2025,,,,
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,zhang2024vision,\cite{zhang2024vision},Vision-Language Models for Vision Tasks: A Survey,http://arxiv.org/abs/2304.00685v2,"Most visual recognition studies rely heavily on crowd-labelled data in deep
neural networks (DNNs) training, and they usually train a DNN for each single
visual recognition task, leading to a laborious and time-consuming visual
recognition paradigm. To address the two challenges, Vision-Language Models
(VLMs) have been intensively investigated recently, which learns rich
vision-language correlation from web-scale image-text pairs that are almost
infinitely available on the Internet and enables zero-shot predictions on
various visual recognition tasks with a single VLM. This paper provides a
systematic review of visual language models for various visual recognition
tasks, including: (1) the background that introduces the development of visual
recognition paradigms; (2) the foundations of VLM that summarize the
widely-adopted network architectures, pre-training objectives, and downstream
tasks; (3) the widely-adopted datasets in VLM pre-training and evaluations; (4)
the review and categorization of existing VLM pre-training methods, VLM
transfer learning methods, and VLM knowledge distillation methods; (5) the
benchmarking, analysis and discussion of the reviewed methods; (6) several
research challenges and potential research directions that could be pursued in
the future VLM studies for visual recognition. A project associated with this
survey has been created at https://github.com/jingyi0000/VLM_survey.","Zhang, Jingyi and Huang, Jiaxing and Jin, Sheng and Lu, Shijian",2024,,,,IEEE transactions on pattern analysis and machine intelligence
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,chen2024vitamin,\cite{chen2024vitamin},ViTamin: Designing Scalable Vision Models in the Vision-Language Era,http://arxiv.org/abs/2404.02132v2,"Recent breakthroughs in vision-language models (VLMs) start a new page in the
vision community. The VLMs provide stronger and more generalizable feature
embeddings compared to those from ImageNet-pretrained models, thanks to the
training on the large-scale Internet image-text pairs. However, despite the
amazing achievement from the VLMs, vanilla Vision Transformers (ViTs) remain
the default choice for the image encoder. Although pure transformer proves its
effectiveness in the text encoding area, it remains questionable whether it is
also the case for image encoding, especially considering that various types of
networks are proposed on the ImageNet benchmark, which, unfortunately, are
rarely studied in VLMs. Due to small data/model scale, the original conclusions
of model design on ImageNet can be limited and biased. In this paper, we aim at
building an evaluation protocol of vision models in the vision-language era
under the contrastive language-image pretraining (CLIP) framework. We provide a
comprehensive way to benchmark different vision models, covering their
zero-shot performance and scalability in both model and training data sizes. To
this end, we introduce ViTamin, a new vision models tailored for VLMs.
ViTamin-L significantly outperforms ViT-L by 2.0% ImageNet zero-shot accuracy,
when using the same publicly available DataComp-1B dataset and the same
OpenCLIP training scheme. ViTamin-L presents promising results on 60 diverse
benchmarks, including classification, retrieval, open-vocabulary detection and
segmentation, and large multi-modal models. When further scaling up the model
size, our ViTamin-XL with only 436M parameters attains 82.9% ImageNet zero-shot
accuracy, surpassing 82.0% achieved by EVA-E that has ten times more parameters
(4.4B).","Chen, Jieneng and Yu, Qihang and Shen, Xiaohui and Yuille, Alan and Chen, Liang-Chieh",2024,,,,
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,zheng2025towards,\cite{zheng2025towards},Towards an understanding of large language models in software engineering tasks,,,"Zheng, Zibin and Ning, Kaiwen and Zhong, Qingyuan and Chen, Jiachi and Chen, Wenqing and Guo, Lianghong and Wang, Weicheng and Wang, Yanlin",2025,,,,Empirical Software Engineering
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,wang2025can,\cite{wang2025can},"Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge
  in Software Engineering",http://arxiv.org/abs/2502.06193v3,"Recently, large language models (LLMs) have been deployed to tackle various
software engineering (SE) tasks like code generation, significantly advancing
the automation of SE tasks. However, assessing the quality of these
LLM-generated code and text remains challenging. The commonly used Pass@k
metric necessitates extensive unit tests and configured environments, demands a
high labor cost, and is not suitable for evaluating LLM-generated text.
Conventional metrics like BLEU, which measure only lexical rather than semantic
similarity, have also come under scrutiny. In response, a new trend has emerged
to employ LLMs for automated evaluation, known as LLM-as-a-judge. These
LLM-as-a-judge methods are claimed to better mimic human assessment than
conventional metrics without relying on high-quality reference answers.
Nevertheless, their exact human alignment in SE tasks remains unexplored.
  In this paper, we empirically explore LLM-as-a-judge methods for evaluating
SE tasks, focusing on their alignment with human judgments. We select seven
LLM-as-a-judge methods that utilize general-purpose LLMs, alongside two LLMs
specifically fine-tuned for evaluation. After generating and manually scoring
LLM responses on three recent SE datasets of code translation, code generation,
and code summarization, we then prompt these methods to evaluate each response.
Finally, we compare the scores generated by these methods with human
evaluation. The results indicate that output-based methods reach the highest
Pearson correlation of 81.32 and 68.51 with human scores in code translation
and generation, achieving near-human evaluation, noticeably outperforming
ChrF++, one of the best conventional metrics, at 34.23 and 64.92. Such
output-based methods prompt LLMs to output judgments directly, and exhibit more
balanced score distributions that resemble human score patterns. Finally, we
provide...","Wang, Ruiqi and Guo, Jiyu and Gao, Cuiyun and Fan, Guodong and Chong, Chun Yong and Xia, Xin",2025,,,,Proceedings of the ACM on Software Engineering
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,chen2024chatunitest,\cite{chen2024chatunitest},ChatUniTest: A Framework for LLM-Based Test Generation,http://arxiv.org/abs/2305.04764v2,"Unit testing is an essential yet frequently arduous task. Various automated
unit test generation tools have been introduced to mitigate this challenge.
Notably, methods based on large language models (LLMs) have garnered
considerable attention and exhibited promising results in recent years.
Nevertheless, LLM-based tools encounter limitations in generating accurate unit
tests. This paper presents ChatUniTest, an LLM-based automated unit test
generation framework. ChatUniTest incorporates an adaptive focal context
mechanism to encompass valuable context in prompts and adheres to a
generation-validation-repair mechanism to rectify errors in generated unit
tests. Subsequently, we have developed ChatUniTest Core, a common library that
implements core workflow, complemented by the ChatUniTest Toolchain, a suite of
seamlessly integrated tools enhancing the capabilities of ChatUniTest. Our
effectiveness evaluation reveals that ChatUniTest outperforms TestSpark and
EvoSuite in half of the evaluated projects, achieving the highest overall line
coverage. Furthermore, insights from our user study affirm that ChatUniTest
delivers substantial value to various stakeholders in the software testing
domain. ChatUniTest is available at
https://github.com/ZJU-ACES-ISE/ChatUniTest, and the demo video is available at
https://www.youtube.com/watch?v=GmfxQUqm2ZQ.","Chen, Yinghao and Hu, Zehao and Zhi, Chen and Han, Junxiao and Deng, Shuiguang and Yin, Jianwei",2024,,,,
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,zhang2023repocoder,\cite{zhang2023repocoder},"RepoCoder: Repository-Level Code Completion Through Iterative Retrieval
  and Generation",http://arxiv.org/abs/2303.12570v3,"The task of repository-level code completion is to continue writing the
unfinished code based on a broader context of the repository. While for
automated code completion tools, it is difficult to utilize the useful
information scattered in different files. We propose RepoCoder, a simple,
generic, and effective framework to address the challenge. It streamlines the
repository-level code completion process by incorporating a similarity-based
retriever and a pre-trained code language model in an iterative
retrieval-generation pipeline. RepoCoder makes effective utilization of
repository-level information for code completion and has the ability to
generate code at various levels of granularity. Moreover, we propose a new
benchmark RepoEval, which consists of the latest and high-quality real-world
repositories covering line, API invocation, and function body completion
scenarios. Experimental results indicate that RepoCoder significantly improves
the In-File completion baseline by over 10% in all settings and consistently
outperforms the vanilla retrieval-augmented code completion approach.
Furthermore, we validate the effectiveness of RepoCoder through comprehensive
analysis, providing valuable insights for future research. Our source code and
benchmark are publicly available:
https://github.com/microsoft/CodeT/tree/main/RepoCoder","Zhang, Fengji and Chen, Bei and Zhang, Yue and Keung, Jacky and Liu, Jin and Zan, Daoguang and Mao, Yi and Lou, Jian-Guang and Chen, Weizhu",2023,,,,
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,rambo,\cite{rambo},RAMBO: Enhancing RAG-based Repository-Level Method Body Completion,http://arxiv.org/abs/2409.15204v3,"Code completion is essential in software development, helping developers by
predicting code snippets based on context. Among completion tasks, Method Body
Completion (MBC) is particularly challenging as it involves generating complete
method bodies based on their signatures and context. This task becomes
significantly harder in large repositories, where method bodies must integrate
repositoryspecific elements such as custom APIs, inter-module dependencies, and
project-specific conventions. In this paper, we introduce RAMBO, a novel
RAG-based approach for repository-level MBC. Instead of retrieving similar
method bodies, RAMBO identifies essential repository-specific elements, such as
classes, methods, and variables/fields, and their relevant usages. By
incorporating these elements and their relevant usages into the code generation
process, RAMBO ensures more accurate and contextually relevant method bodies.
Our experimental results with leading code LLMs across 40 Java projects show
that RAMBO significantly outperformed the state-of-the-art repository-level MBC
approaches, with the improvements of up to 46% in BLEU, 57% in CodeBLEU, 36% in
Compilation Rate, and up to 3X in Exact Match. Notably, RAMBO surpassed
RepoCoder Oracle method by up to 12% in Exact Match, setting a new benchmark
for repository-level MBC.","Bui, Tuan-Dung and Luu-Van, Duc-Thieu and Nguyen, Thanh-Phat and Nguyen, Thu-Trang and Nguyen, Son and Vo, Hieu Dinh",2024,,,,arXiv preprint arXiv:2409.15204
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,deepseek-coder,\cite{deepseek-coder},DeepSeek Coder: Let the Code Write Itself,,,DeepSeek,2023,,,,GitHub repository
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,codellama,\cite{codellama},Code Llama: Open Foundation Models for Code,http://arxiv.org/abs/2308.12950v3,"We release Code Llama, a family of large language models for code based on
Llama 2 providing state-of-the-art performance among open models, infilling
capabilities, support for large input contexts, and zero-shot instruction
following ability for programming tasks. We provide multiple flavors to cover a
wide range of applications: foundation models (Code Llama), Python
specializations (Code Llama - Python), and instruction-following models (Code
Llama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are
trained on sequences of 16k tokens and show improvements on inputs with up to
100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants
support infilling based on surrounding content. Code Llama reaches
state-of-the-art performance among open models on several code benchmarks, with
scores of up to 67% and 65% on HumanEval and MBPP, respectively. Notably, Code
Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our
models outperform every other publicly available model on MultiPL-E. We release
Code Llama under a permissive license that allows for both research and
commercial use.","Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\'e}r{\'e}my and others",2023,,,,arXiv preprint arXiv:2308.12950
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,magicoder,\cite{magicoder},Magicoder: Source code is all you need,,,"Wei, Yuxiang and Wang, Zhe and Liu, Jiawei and Ding, Yifeng and Zhang, Lingming",2023,,,,arXiv preprint arXiv:2312.02120
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,mu2024clarifygpt,\cite{mu2024clarifygpt},Clarifygpt: A framework for enhancing llm-based code generation via requirements clarification,,,"Mu, Fangwen and Shi, Lin and Wang, Song and Yu, Zhuohao and Zhang, Binquan and Wang, ChenXue and Liu, Shichao and Wang, Qing",2024,,,,Proceedings of the ACM on Software Engineering
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,wang2024hits,\cite{wang2024hits},HITS: High-coverage LLM-based Unit Test Generation via Method Slicing,http://arxiv.org/abs/2408.11324v1,"Large language models (LLMs) have behaved well in generating unit tests for
Java projects. However, the performance for covering the complex focal methods
within the projects is poor. Complex methods comprise many conditions and
loops, requiring the test cases to be various enough to cover all lines and
branches. However, existing test generation methods with LLMs provide the whole
method-to-test to the LLM without assistance on input analysis. The LLM has
difficulty inferring the test inputs to cover all conditions, resulting in
missing lines and branches. To tackle the problem, we propose decomposing the
focal methods into slices and asking the LLM to generate test cases slice by
slice. Our method simplifies the analysis scope, making it easier for the LLM
to cover more lines and branches in each slice. We build a dataset comprising
complex focal methods collected from the projects used by existing
state-of-the-art approaches. Our experiment results show that our method
significantly outperforms current test case generation methods with LLMs and
the typical SBST method Evosuite regarding both line and branch coverage
scores.","Wang, Zejun and Liu, Kaibo and Li, Ge and Jin, Zhi",2024,,,,
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,sun2024source,\cite{sun2024source},Source Code Summarization in the Era of Large Language Models,http://arxiv.org/abs/2407.07959v2,"To support software developers in understanding and maintaining programs,
various automatic (source) code summarization techniques have been proposed to
generate a concise natural language summary (i.e., comment) for a given code
snippet. Recently, the emergence of large language models (LLMs) has led to a
great boost in the performance of code-related tasks. In this paper, we
undertake a systematic and comprehensive study on code summarization in the era
of LLMs, which covers multiple aspects involved in the workflow of LLM-based
code summarization. Specifically, we begin by examining prevalent automated
evaluation methods for assessing the quality of summaries generated by LLMs and
find that the results of the GPT-4 evaluation method are most closely aligned
with human evaluation. Then, we explore the effectiveness of five prompting
techniques (zero-shot, few-shot, chain-of-thought, critique, and expert) in
adapting LLMs to code summarization tasks. Contrary to expectations, advanced
prompting techniques may not outperform simple zero-shot prompting. Next, we
investigate the impact of LLMs' model settings (including top\_p and
temperature parameters) on the quality of generated summaries. We find the
impact of the two parameters on summary quality varies by the base LLM and
programming language, but their impacts are similar. Moreover, we canvass LLMs'
abilities to summarize code snippets in distinct types of programming
languages. The results reveal that LLMs perform suboptimally when summarizing
code written in logic programming languages compared to other language types.
Finally, we unexpectedly find that CodeLlama-Instruct with 7B parameters can
outperform advanced GPT-4 in generating summaries describing code
implementation details and asserting code properties. We hope that our findings
can provide a comprehensive understanding of code summarization in the era of
LLMs.","Sun, Weisong and Miao, Yun and Li, Yuekang and Zhang, Hongyu and Fang, Chunrong and Liu, Yi and Deng, Gelei and Liu, Yang and Chen, Zhenyu",2024,,,,
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,jin2023inferfix,\cite{jin2023inferfix},InferFix: End-to-End Program Repair with LLMs,http://arxiv.org/abs/2303.07263v1,"Software development life cycle is profoundly influenced by bugs: their
introduction, identification, and eventual resolution account for a significant
portion of software cost. This has motivated software engineering researchers
and practitioners to propose different approaches for automating the
identification and repair of software defects. Large language models have been
adapted to the program repair task through few-shot demonstration learning and
instruction prompting, treating this as an infilling task. However, these
models have only focused on learning general bug-fixing patterns for
uncategorized bugs mined from public repositories. In this paper, we propose
InferFix: a transformer-based program repair framework paired with a
state-of-the-art static analyzer to fix critical security and performance bugs.
InferFix combines a Retriever -- transformer encoder model pretrained via
contrastive learning objective, which aims at searching for semantically
equivalent bugs and corresponding fixes; and a Generator -- a large language
model (Codex Cushman) finetuned on supervised bug-fix data with prompts
augmented via bug type annotations and semantically similar fixes retrieved
from an external non-parametric memory. To train and evaluate our approach, we
curated InferredBugs, a novel, metadata-rich dataset of bugs extracted by
executing the Infer static analyzer on the change histories of thousands of
Java and C# repositories. Our evaluation demonstrates that InferFix outperforms
strong LLM baselines, with a top-1 accuracy of 65.6% for generating fixes in C#
and 76.8% in Java. We discuss the deployment of InferFix alongside Infer at
Microsoft which offers an end-to-end solution for detection, classification,
and localization of bugs, as well as fixing and validation of candidate
patches, integrated in the continuous integration pipeline to automate the
software development workflow.","Jin, Matthew and Shahriar, Syed and Tufano, Michele and Shi, Xin and Lu, Shuai and Sundaresan, Neel and Svyatkovskiy, Alexey",2023,,,,
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,github_copilot,\cite{github_copilot},GitHub Copilot,,,,,,https://github.com/features/copilot,,
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,codegeex2,\cite{codegeex2},CodeGeeX2,,,,,,https://github.com/zai-org/CodeGeeX2,,
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,zhang2025llm,\cite{zhang2025llm},"LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism,
  and Mitigation",http://arxiv.org/abs/2409.20550v2,"Code generation aims to automatically generate code from input requirements,
significantly enhancing development efficiency. Recent large language models
(LLMs) based approaches have shown promising results and revolutionized code
generation task. Despite the promising performance, LLMs often generate
contents with hallucinations, especially for the code generation scenario
requiring the handling of complex contextual dependencies in practical
development process. Although previous study has analyzed hallucinations in
LLM-powered code generation, the study is limited to standalone function
generation. In this paper, we conduct an empirical study to study the
phenomena, mechanism, and mitigation of LLM hallucinations within more
practical and complex development contexts in repository-level generation
scenario. First, we manually examine the code generation results from six
mainstream LLMs to establish a hallucination taxonomy of LLM-generated code.
Next, we elaborate on the phenomenon of hallucinations, analyze their
distribution across different models. We then analyze causes of hallucinations
and identify four potential factors contributing to hallucinations. Finally, we
propose an RAG-based mitigation method, which demonstrates consistent
effectiveness in all studied LLMs. The replication package including code,
data, and experimental results is available at
https://github.com/DeepSoftwareAnalytics/LLMCodingHallucination","Zhang, Ziyao and Wang, Chong and Wang, Yanlin and Shi, Ensheng and Ma, Yuchi and Zhong, Wanjun and Chen, Jiachi and Mao, Mingzhi and Zheng, Zibin",2025,,,,Proceedings of the ACM on Software Engineering
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,liu2024exploring,\cite{liu2024exploring},Exploring and Evaluating Hallucinations in LLM-Powered Code Generation,http://arxiv.org/abs/2404.00971v2,"The rise of Large Language Models (LLMs) has significantly advanced many
applications on software engineering tasks, particularly in code generation.
Despite the promising performance, LLMs are prone to generate hallucinations,
which means LLMs might produce outputs that deviate from users' intent, exhibit
internal inconsistencies, or misalign with the factual knowledge, making the
deployment of LLMs potentially risky in a wide range of applications. Existing
work mainly focuses on investing the hallucination in the domain of natural
language generation (NLG), leaving a gap in understanding the types and extent
of hallucinations in the context of code generation. To bridge the gap, we
conducted a thematic analysis of the LLM-generated code to summarize and
categorize the hallucinations present in it. Our study established a
comprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5
primary categories of hallucinations depending on the conflicting objectives
and varying degrees of deviation observed in code generation. Furthermore, we
systematically analyzed the distribution of hallucinations, exploring
variations among different LLMs and their correlation with code correctness.
Based on the results, we proposed HalluCode, a benchmark for evaluating the
performance of code LLMs in recognizing hallucinations. Hallucination
recognition and mitigation experiments with HalluCode and HumanEval show
existing LLMs face great challenges in recognizing hallucinations, particularly
in identifying their types, and are hardly able to mitigate hallucinations. We
believe our findings will shed light on future research about hallucination
evaluation, detection, and mitigation, ultimately paving the way for building
more effective and reliable code LLMs in the future.","Liu, Fang and Liu, Yang and Shi, Lin and Huang, Houkun and Wang, Ruifeng and Yang, Zhen and Zhang, Li and Li, Zhongqi and Ma, Yuchi",2024,,,,arXiv preprint arXiv:2404.00971
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,liu2023your,\cite{liu2023your},"Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of
  Large Language Models for Code Generation",http://arxiv.org/abs/2305.01210v3,"Program synthesis has been long studied with recent approaches focused on
directly using the power of Large Language Models (LLMs) to generate code.
Programming benchmarks, with curated synthesis problems and test-cases, are
used to measure the performance of various LLMs on code synthesis. However,
these test-cases can be limited in both quantity and quality for fully
assessing the functional correctness of the generated code. Such limitation in
the existing benchmarks begs the following question: In the era of LLMs, is the
code generated really correct? To answer this, we propose EvalPlus -- a code
synthesis evaluation framework to rigorously benchmark the functional
correctness of LLM-synthesized code. EvalPlus augments a given evaluation
dataset with large amounts of test-cases newly produced by an automatic test
input generator, powered by both LLM- and mutation-based strategies. While
EvalPlus is general, we extend the test-cases of the popular HumanEval
benchmark by 80x to build HumanEval+. Our extensive evaluation across 26
popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to
catch significant amounts of previously undetected wrong code synthesized by
LLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that
test insufficiency can lead to mis-ranking. For example, both
WizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+,
while none of them could on HumanEval. Our work not only indicates that prior
popular code synthesis evaluation results do not accurately reflect the true
performance of LLMs for code synthesis, but also opens up a new direction to
improve such programming benchmarks through automated testing. We have
open-sourced our tools, enhanced datasets as well as all LLM-generated code at
https://github.com/evalplus/evalplus to facilitate and accelerate future
LLM-for-code research.","Liu, Jiawei and Xia, Chunqiu Steven and Wang, Yuyao and Zhang, Lingming",2023,,,,Advances in Neural Information Processing Systems
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,wang2025towards,\cite{wang2025towards},"Towards Understanding the Characteristics of Code Generation Errors Made
  by Large Language Models",http://arxiv.org/abs/2406.08731v3,"Large Language Models (LLMs) have demonstrated unprecedented capabilities in
code generation. However, there remains a limited understanding of code
generation errors that LLMs can produce. To bridge the gap, we conducted an
in-depth analysis of code generation errors across six representative LLMs on
the HumanEval dataset. Specifically, we first employed open coding and thematic
analysis to distill a comprehensive taxonomy of code generation errors. We
analyzed two dimensions of error characteristics -- semantic characteristics
and syntactic characteristics. Our analysis revealed that LLMs often made
non-trivial, multi-line code generation errors in various locations and with
various root causes. We further analyzed the correlation between these errors
and task complexity as well as test pass rate. Our findings highlighted several
challenges in locating and fixing code generation errors made by LLMs. In the
end, we discussed several future directions to address these challenges.","Wang, Zhijie and Zhou, Zijie and Song, Da and Huang, Yuheng and Chen, Shengmai and Ma, Lei and Zhang, Tianyi",2025,,,,
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,llmcheck,\cite{llmcheck},Llm-check: Investigating detection of hallucinations in large language models,,,"Sriramanan, Gaurang and Bharti, Siddhant and Sadasivan, Vinu Sankar and Saha, Shoumik and Kattakinda, Priyatham and Feizi, Soheil",2024,,,,Advances in Neural Information Processing Systems
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,inside,\cite{inside},"INSIDE: LLMs' Internal States Retain the Power of Hallucination
  Detection",http://arxiv.org/abs/2402.03744v2,"Knowledge hallucination have raised widespread concerns for the security and
reliability of deployed LLMs. Previous efforts in detecting hallucinations have
been employed at logit-level uncertainty estimation or language-level
self-consistency evaluation, where the semantic information is inevitably lost
during the token-decoding procedure. Thus, we propose to explore the dense
semantic information retained within LLMs' \textbf{IN}ternal \textbf{S}tates
for halluc\textbf{I}nation \textbf{DE}tection (\textbf{INSIDE}). In particular,
a simple yet effective \textbf{EigenScore} metric is proposed to better
evaluate responses' self-consistency, which exploits the eigenvalues of
responses' covariance matrix to measure the semantic consistency/diversity in
the dense embedding space. Furthermore, from the perspective of self-consistent
hallucination detection, a test time feature clipping approach is explored to
truncate extreme activations in the internal states, which reduces
overconfident generations and potentially benefits the detection of
overconfident hallucinations. Extensive experiments and ablation studies are
performed on several popular LLMs and question-answering (QA) benchmarks,
showing the effectiveness of our proposal.","Chen, Chao and Liu, Kai and Chen, Ze and Gu, Yi and Wu, Yue and Tao, Mingyuan and Fu, Zhihang and Ye, Jieping",2024,,,,arXiv preprint arXiv:2402.03744
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,inner-working,\cite{inner-working},A Primer on the Inner Workings of Transformer-based Language Models,http://arxiv.org/abs/2405.00208v3,"The rapid progress of research aimed at interpreting the inner workings of
advanced language models has highlighted a need for contextualizing the
insights gained from years of work in this area. This primer provides a concise
technical introduction to the current techniques used to interpret the inner
workings of Transformer-based language models, focusing on the generative
decoder-only architecture. We conclude by presenting a comprehensive overview
of the known internal mechanisms implemented by these models, uncovering
connections across popular approaches and active research directions in this
area.","Ferrando, Javier and Sarti, Gabriele and Bisazza, Arianna and Costa-juss{\`a}, Marta R",2024,,,,arXiv preprint arXiv:2405.00208
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,manakul2023selfcheckgpt,\cite{manakul2023selfcheckgpt},"SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for
  Generative Large Language Models",http://arxiv.org/abs/2303.08896v3,"Generative Large Language Models (LLMs) such as GPT-3 are capable of
generating highly fluent responses to a wide variety of user prompts. However,
LLMs are known to hallucinate facts and make non-factual statements which can
undermine trust in their output. Existing fact-checking approaches either
require access to the output probability distribution (which may not be
available for systems such as ChatGPT) or external databases that are
interfaced via separate, often complex, modules. In this work, we propose
""SelfCheckGPT"", a simple sampling-based approach that can be used to fact-check
the responses of black-box models in a zero-resource fashion, i.e. without an
external database. SelfCheckGPT leverages the simple idea that if an LLM has
knowledge of a given concept, sampled responses are likely to be similar and
contain consistent facts. However, for hallucinated facts, stochastically
sampled responses are likely to diverge and contradict one another. We
investigate this approach by using GPT-3 to generate passages about individuals
from the WikiBio dataset, and manually annotate the factuality of the generated
passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and
factual sentences; and ii) rank passages in terms of factuality. We compare our
approach to several baselines and show that our approach has considerably
higher AUC-PR scores in sentence-level hallucination detection and higher
correlation scores in passage-level factuality assessment compared to grey-box
methods.","Manakul, Potsawee and Liusie, Adian and Gales, Mark",2023,,,,
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,huang2025survey,\cite{huang2025survey},"A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions",,,"Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and others",2025,,,,ACM Transactions on Information Systems
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,farquhar2024detecting,\cite{farquhar2024detecting},Detecting hallucinations in large language models using semantic entropy,,,"Farquhar, Sebastian and Kossen, Jannik and Kuhn, Lorenz and Gal, Yarin",2024,,,,Nature
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,zhang2024self,\cite{zhang2024self},"Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via
  Self-Evaluation",http://arxiv.org/abs/2402.09267v2,"Despite showing increasingly human-like abilities, large language models
(LLMs) often struggle with factual inaccuracies, i.e. ""hallucinations"", even
when they hold relevant knowledge. To address these hallucinations, current
approaches typically necessitate high-quality human factuality annotations. In
this work, we explore Self-Alignment for Factuality, where we leverage the
self-evaluation capability of an LLM to provide training signals that steer the
model towards factuality. Specifically, we incorporate Self-Eval, a
self-evaluation component, to prompt an LLM to validate the factuality of its
own generated responses solely based on its internal knowledge. Additionally,
we design Self-Knowledge Tuning (SK-Tuning) to augment the LLM's
self-evaluation ability by improving the model's confidence estimation and
calibration. We then utilize these self-annotated responses to fine-tune the
model via Direct Preference Optimization algorithm. We show that the proposed
self-alignment approach substantially enhances factual accuracy over Llama
family models across three key knowledge-intensive tasks on TruthfulQA and
BioGEN.","Zhang, Xiaoying and Peng, Baolin and Tian, Ye and Zhou, Jingyan and Jin, Lifeng and Song, Linfeng and Mi, Haitao and Meng, Helen",2024,,,,
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,zhang2024knowhalu,\cite{zhang2024knowhalu},"KnowHalu: Hallucination Detection via Multi-Form Knowledge Based Factual
  Checking",http://arxiv.org/abs/2404.02935v1,"This paper introduces KnowHalu, a novel approach for detecting hallucinations
in text generated by large language models (LLMs), utilizing step-wise
reasoning, multi-formulation query, multi-form knowledge for factual checking,
and fusion-based detection mechanism. As LLMs are increasingly applied across
various domains, ensuring that their outputs are not hallucinated is critical.
Recognizing the limitations of existing approaches that either rely on the
self-consistency check of LLMs or perform post-hoc fact-checking without
considering the complexity of queries or the form of knowledge, KnowHalu
proposes a two-phase process for hallucination detection. In the first phase,
it identifies non-fabrication hallucinations--responses that, while factually
correct, are irrelevant or non-specific to the query. The second phase,
multi-form based factual checking, contains five key steps: reasoning and query
decomposition, knowledge retrieval, knowledge optimization, judgment
generation, and judgment aggregation. Our extensive evaluations demonstrate
that KnowHalu significantly outperforms SOTA baselines in detecting
hallucinations across diverse tasks, e.g., improving by 15.65% in QA tasks and
5.50% in summarization tasks, highlighting its effectiveness and versatility in
detecting hallucinations in LLM-generated content.","Zhang, Jiawei and Xu, Chejian and Gai, Yu and Lecue, Freddy and Song, Dawn and Li, Bo",2024,,,,arXiv preprint arXiv:2404.02935
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,zhou2021detecting,\cite{zhou2021detecting},Detecting Hallucinated Content in Conditional Neural Sequence Generation,http://arxiv.org/abs/2011.02593v3,"Neural sequence models can generate highly fluent sentences, but recent
studies have also shown that they are also prone to hallucinate additional
content not supported by the input. These variety of fluent but wrong outputs
are particularly problematic, as it will not be possible for users to tell they
are being presented incorrect content. To detect these errors, we propose a
task to predict whether each token in the output sequence is hallucinated (not
contained in the input) and collect new manually annotated evaluation sets for
this task. We also introduce a method for learning to detect hallucinations
using pretrained language models fine tuned on synthetic data that includes
automatically inserted hallucinations Experiments on machine translation (MT)
and abstractive summarization demonstrate that our proposed approach
consistently outperforms strong baselines on all benchmark datasets. We further
demonstrate how to use the token-level hallucination labels to define a
fine-grained loss over the target sequence in low-resource MT and achieve
significant improvements over strong baseline methods. We also apply our method
to word-level quality estimation for MT and show its effectiveness in both
supervised and unsupervised settings. Codes and data available at
https://github.com/violet-zct/fairseq-detect-hallucination.","Zhou, Chunting and Neubig, Graham and Gu, Jiatao and Diab, Mona and Guzm{\'a}n, Francisco and Zettlemoyer, Luke and Ghazvininejad, Marjan",2021,,,,
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,huang2023look,\cite{huang2023look},"Look Before You Leap: An Exploratory Study of Uncertainty Measurement
  for Large Language Models",http://arxiv.org/abs/2307.10236v4,"The recent performance leap of Large Language Models (LLMs) opens up new
opportunities across numerous industrial applications and domains. However,
erroneous generations, such as false predictions, misinformation, and
hallucination made by LLMs, have also raised severe concerns for the
trustworthiness of LLMs', especially in safety-, security- and
reliability-sensitive scenarios, potentially hindering real-world adoptions.
While uncertainty estimation has shown its potential for interpreting the
prediction risks made by general machine learning (ML) models, little is known
about whether and to what extent it can help explore an LLM's capabilities and
counteract its undesired behavior. To bridge the gap, in this paper, we
initiate an exploratory study on the risk assessment of LLMs from the lens of
uncertainty. In particular, we experiment with twelve uncertainty estimation
methods and four LLMs on four prominent natural language processing (NLP) tasks
to investigate to what extent uncertainty estimation techniques could help
characterize the prediction risks of LLMs. Our findings validate the
effectiveness of uncertainty estimation for revealing LLMs'
uncertain/non-factual predictions. In addition to general NLP tasks, we
extensively conduct experiments with four LLMs for code generation on two
datasets. We find that uncertainty estimation can potentially uncover buggy
programs generated by LLMs. Insights from our study shed light on future design
and development for reliable LLMs, facilitating further research toward
enhancing the trustworthiness of LLMs.","Huang, Yuheng and Song, Jiayang and Wang, Zhijie and Zhao, Shengming and Chen, Huaming and Juefei-Xu, Felix and Ma, Lei",2023,,,,arXiv preprint arXiv:2307.10236
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,augenstein2024factuality,\cite{augenstein2024factuality},Factuality challenges in the era of large language models and opportunities for fact-checking,,,"Augenstein, Isabelle and Baldwin, Timothy and Cha, Meeyoung and Chakraborty, Tanmoy and Ciampaglia, Giovanni Luca and Corney, David and DiResta, Renee and Ferrara, Emilio and Hale, Scott and Halevy, Alon and others",2024,,,,Nature Machine Intelligence
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,hu2024knowledge,\cite{hu2024knowledge},Knowledge-centric hallucination detection,,,"Hu, Xiangkun and Ru, Dongyu and Qiu, Lin and Guo, Qipeng and Zhang, Tianhang and Xu, Yang and Luo, Yun and Liu, Pengfei and Zhang, Yue and Zhang, Zheng",2024,,,,
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,probing,\cite{probing},"Probing the Decision Boundaries of In-context Learning in Large Language
  Models",http://arxiv.org/abs/2406.11233v3,"In-context learning is a key paradigm in large language models (LLMs) that
enables them to generalize to new tasks and domains by simply prompting these
models with a few exemplars without explicit parameter updates. Many attempts
have been made to understand in-context learning in LLMs as a function of model
scale, pretraining data, and other factors. In this work, we propose a new
mechanism to probe and understand in-context learning from the lens of decision
boundaries for in-context binary classification. Decision boundaries are
straightforward to visualize and provide important information about the
qualitative behavior of the inductive biases of standard classifiers. To our
surprise, we find that the decision boundaries learned by current LLMs in
simple binary classification tasks are often irregular and non-smooth,
regardless of linear separability in the underlying task. This paper
investigates the factors influencing these decision boundaries and explores
methods to enhance their generalizability. We assess various approaches,
including training-free and fine-tuning methods for LLMs, the impact of model
architecture, and the effectiveness of active prompting techniques for
smoothing decision boundaries in a data-efficient manner. Our findings provide
a deeper understanding of in-context learning dynamics and offer practical
improvements for enhancing robustness and generalizability of in-context
learning.","Zhao, Siyan and Nguyen, Tung and Grover, Aditya",2024,,,,Advances in Neural Information Processing Systems
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,internal-state,\cite{internal-state},The Internal State of an LLM Knows When It's Lying,http://arxiv.org/abs/2304.13734v2,"While Large Language Models (LLMs) have shown exceptional performance in
various tasks, one of their most prominent drawbacks is generating inaccurate
or false information with a confident tone. In this paper, we provide evidence
that the LLM's internal state can be used to reveal the truthfulness of
statements. This includes both statements provided to the LLM, and statements
that the LLM itself generates. Our approach is to train a classifier that
outputs the probability that a statement is truthful, based on the hidden layer
activations of the LLM as it reads or generates the statement. Experiments
demonstrate that given a set of test sentences, of which half are true and half
false, our trained classifier achieves an average of 71\% to 83\% accuracy
labeling which sentences are true versus false, depending on the LLM base
model. Furthermore, we explore the relationship between our classifier's
performance and approaches based on the probability assigned to the sentence by
the LLM. We show that while LLM-assigned sentence probability is related to
sentence truthfulness, this probability is also dependent on sentence length
and the frequencies of words in the sentence, resulting in our trained
classifier providing a more reliable approach to detecting truthfulness,
highlighting its potential to enhance the reliability of LLM-generated content
and its practical applicability in real-world scenarios.","Azaria, Amos and Mitchell, Tom",2023,,,,
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,factoscope,\cite{factoscope},"LLM Factoscope: Uncovering LLMs' Factual Discernment through Inner
  States Analysis",http://arxiv.org/abs/2312.16374v3,"Large Language Models (LLMs) have revolutionized various domains with
extensive knowledge and creative capabilities. However, a critical issue with
LLMs is their tendency to produce outputs that diverge from factual reality.
This phenomenon is particularly concerning in sensitive applications such as
medical consultation and legal advice, where accuracy is paramount. In this
paper, we introduce the LLM factoscope, a novel Siamese network-based model
that leverages the inner states of LLMs for factual detection. Our
investigation reveals distinguishable patterns in LLMs' inner states when
generating factual versus non-factual content. We demonstrate the LLM
factoscope's effectiveness across various architectures, achieving over 96%
accuracy in factual detection. Our work opens a new avenue for utilizing LLMs'
inner states for factual detection and encourages further exploration into
LLMs' inner workings for enhanced reliability and transparency.","He, Jinwen and Gong, Yujia and Lin, Zijin and Zhao, Yue and Chen, Kai and others",2024,,,,
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,internal-state-2,\cite{internal-state-2},LLM Internal States Reveal Hallucination Risk Faced With a Query,http://arxiv.org/abs/2407.03282v2,"The hallucination problem of Large Language Models (LLMs) significantly
limits their reliability and trustworthiness. Humans have a self-awareness
process that allows us to recognize what we don't know when faced with queries.
Inspired by this, our paper investigates whether LLMs can estimate their own
hallucination risk before response generation. We analyze the internal
mechanisms of LLMs broadly both in terms of training data sources and across 15
diverse Natural Language Generation (NLG) tasks, spanning over 700 datasets.
Our empirical analysis reveals two key insights: (1) LLM internal states
indicate whether they have seen the query in training data or not; and (2) LLM
internal states show they are likely to hallucinate or not regarding the query.
Our study explores particular neurons, activation layers, and tokens that play
a crucial role in the LLM perception of uncertainty and hallucination risk. By
a probing estimator, we leverage LLM self-assessment, achieving an average
hallucination estimation accuracy of 84.32\% at run time.","Ji, Ziwei and Chen, Delong and Ishii, Etsuko and Cahyawijaya, Samuel and Bang, Yejin and Wilie, Bryan and Fung, Pascale",2024,,,,
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,croft2021empirical,\cite{croft2021empirical},"An Empirical Study of Rule-Based and Learning-Based Approaches for
  Static Application Security Testing",http://arxiv.org/abs/2107.01921v2,"Background: Static Application Security Testing (SAST) tools purport to
assist developers in detecting security issues in source code. These tools
typically use rule-based approaches to scan source code for security
vulnerabilities. However, due to the significant shortcomings of these tools
(i.e., high false positive rates), learning-based approaches for Software
Vulnerability Prediction (SVP) are becoming a popular approach. Aims: Despite
the similar objectives of these two approaches, their comparative value is
unexplored. We provide an empirical analysis of SAST tools and SVP models, to
identify their relative capabilities for source code security analysis. Method:
We evaluate the detection and assessment performance of several common SAST
tools and SVP models on a variety of vulnerability datasets. We further assess
the viability and potential benefits of combining the two approaches. Results:
SAST tools and SVP models provide similar detection capabilities, but SVP
models exhibit better overall performance for both detection and assessment.
Unification of the two approaches is difficult due to lacking synergies.
Conclusions: Our study generates 12 main findings which provide insights into
the capabilities and synergy of these two approaches. Through these
observations we provide recommendations for use and improvement.","Croft, Roland and Newlands, Dominic and Chen, Ziyu and Babar, M Ali",2021,,,,
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,codebert,\cite{codebert},{C}ode{BERT}: A Pre-Trained Model for Programming and Natural Languages,,,"Feng, Zhangyin  and
      Guo, Daya  and
      Tang, Duyu  and
      Duan, Nan  and
      Feng, Xiaocheng  and
      Gong, Ming  and
      Shou, Linjun  and
      Qin, Bing  and
      Liu, Ting  and
      Jiang, Daxin  and
      Zhou, Ming",2020,,,,
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,zhang2020graph,\cite{zhang2020graph},Graph-bert: Only attention is needed for learning graph representations,,,"Zhang, Jiawei and Zhang, Haopeng and Xia, Congying and Sun, Li",2020,,,,arXiv preprint arXiv:2001.05140
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,codet5,\cite{codet5},"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for
  Code Understanding and Generation",http://arxiv.org/abs/2109.00859v1,"Pre-trained models for Natural Languages (NL) like BERT and GPT have been
recently shown to transfer well to Programming Languages (PL) and largely
benefit a broad set of code-related tasks. Despite their success, most current
methods either rely on an encoder-only (or decoder-only) pre-training that is
suboptimal for generation (resp. understanding) tasks or process the code
snippet in the same way as NL, neglecting the special characteristics of PL
such as token types. We present CodeT5, a unified pre-trained encoder-decoder
Transformer model that better leverages the code semantics conveyed from the
developer-assigned identifiers. Our model employs a unified framework to
seamlessly support both code understanding and generation tasks and allows for
multi-task learning. Besides, we propose a novel identifier-aware pre-training
task that enables the model to distinguish which code tokens are identifiers
and to recover them when they are masked. Furthermore, we propose to exploit
the user-written code comments with a bimodal dual generation task for better
NL-PL alignment. Comprehensive experiments show that CodeT5 significantly
outperforms prior methods on understanding tasks such as code defect detection
and clone detection, and generation tasks across various directions including
PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better
capture semantic information from code. Our code and pre-trained models are
released at https: //github.com/salesforce/CodeT5 .","Wang, Yue and Wang, Weishi and Joty, Shafiq and Hoi, Steven CH",2021,,,,
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,ivdetect,\cite{ivdetect},Vulnerability Detection with Fine-grained Interpretations,http://arxiv.org/abs/2106.10478v1,"Despite the successes of machine learning (ML) and deep learning (DL) based
vulnerability detectors (VD), they are limited to providing only the decision
on whether a given code is vulnerable or not, without details on what part of
the code is relevant to the detected vulnerability. We present IVDetect an
interpretable vulnerability detector with the philosophy of using Artificial
Intelligence (AI) to detect vulnerabilities, while using Intelligence Assistant
(IA) via providing VD interpretations in terms of vulnerable statements.
  For vulnerability detection, we separately consider the vulnerable statements
and their surrounding contexts via data and control dependencies. This allows
our model better discriminate vulnerable statements than using the mixture of
vulnerable code and~contextual code as in existing approaches. In addition to
the coarse-grained vulnerability detection result, we leverage interpretable AI
to provide users with fine-grained interpretations that include the sub-graph
in the Program Dependency Graph (PDG) with the crucial statements that are
relevant to the detected vulnerability. Our empirical evaluation on
vulnerability databases shows that IVDetect outperforms the existing DL-based
approaches by 43%--84% and 105%--255% in top-10 nDCG and MAP ranking scores.
IVDetect correctly points out the vulnerable statements relevant to the
vulnerability via its interpretation~in 67% of the cases with a top-5 ranked
list. It improves over baseline interpretation models by 12.3%--400% and
9%--400% in accuracy.","Li, Yi and Wang, Shaohua and Nguyen, Tien N",2021,,,,
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,COSTA,\cite{COSTA},Context-based statement-level vulnerability localization,,,"Nguyen, Thu-Trang and Vo, Hieu Dinh",2024,,,,Information and Software Technology
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,velvet,\cite{velvet},"VELVET: a noVel Ensemble Learning approach to automatically locate
  VulnErable sTatements",http://arxiv.org/abs/2112.10893v2,"Automatically locating vulnerable statements in source code is crucial to
assure software security and alleviate developers' debugging efforts. This
becomes even more important in today's software ecosystem, where vulnerable
code can flow easily and unwittingly within and across software repositories
like GitHub. Across such millions of lines of code, traditional static and
dynamic approaches struggle to scale. Although existing machine-learning-based
approaches look promising in such a setting, most work detects vulnerable code
at a higher granularity -- at the method or file level. Thus, developers still
need to inspect a significant amount of code to locate the vulnerable
statement(s) that need to be fixed.
  This paper presents VELVET, a novel ensemble learning approach to locate
vulnerable statements. Our model combines graph-based and sequence-based neural
networks to successfully capture the local and global context of a program
graph and effectively understand code semantics and vulnerable patterns. To
study VELVET's effectiveness, we use an off-the-shelf synthetic dataset and a
recently published real-world dataset. In the static analysis setting, where
vulnerable functions are not detected in advance, VELVET achieves 4.5x better
performance than the baseline static analyzers on the real-world data. For the
isolated vulnerability localization task, where we assume the vulnerability of
a function is known while the specific vulnerable statement is unknown, we
compare VELVET with several neural networks that also attend to local and
global context of code. VELVET achieves 99.6% and 43.6% top-1 accuracy over
synthetic data and real-world data, respectively, outperforming the baseline
deep-learning models by 5.3-29.0%.","Ding, Yangruibo and Suneja, Sahil and Zheng, Yunhui and Laredo, Jim and Morari, Alessandro and Kaiser, Gail and Ray, Baishakhi",2022,,,,
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,linevd,\cite{linevd},"LineVD: Statement-level Vulnerability Detection using Graph Neural
  Networks",http://arxiv.org/abs/2203.05181v2,"Current machine-learning based software vulnerability detection methods are
primarily conducted at the function-level. However, a key limitation of these
methods is that they do not indicate the specific lines of code contributing to
vulnerabilities. This limits the ability of developers to efficiently inspect
and interpret the predictions from a learnt model, which is crucial for
integrating machine-learning based tools into the software development
workflow. Graph-based models have shown promising performance in function-level
vulnerability detection, but their capability for statement-level vulnerability
detection has not been extensively explored. While interpreting function-level
predictions through explainable AI is one promising direction, we herein
consider the statement-level software vulnerability detection task from a fully
supervised learning perspective. We propose a novel deep learning framework,
LineVD, which formulates statement-level vulnerability detection as a node
classification task. LineVD leverages control and data dependencies between
statements using graph neural networks, and a transformer-based model to encode
the raw source code tokens. In particular, by addressing the conflicting
outputs between function-level and statement-level information, LineVD
significantly improve the prediction performance without vulnerability status
for function code. We have conducted extensive experiments against a
large-scale collection of real-world C/C++ vulnerabilities obtained from
multiple real-world projects, and demonstrate an increase of 105\% in F1-score
over the current state-of-the-art.","David Hin and
              Andrey Kan and
              Huaming Chen and
              Muhammad Ali Babar",2022,,,,
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,linevul,\cite{linevul},LineVul: A Transformer-based Line-Level Vulnerability Prediction,,,M. Fu and C. Tantithamthavorn,2022,may,,,
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,lost-at-c,\cite{lost-at-c},"Lost at C: A User Study on the Security Implications of Large Language
  Model Code Assistants",http://arxiv.org/abs/2208.09727v4,"Large Language Models (LLMs) such as OpenAI Codex are increasingly being used
as AI-based coding assistants. Understanding the impact of these tools on
developers' code is paramount, especially as recent work showed that LLMs may
suggest cybersecurity vulnerabilities. We conduct a security-driven user study
(N=58) to assess code written by student programmers when assisted by LLMs.
Given the potential severity of low-level bugs as well as their relative
frequency in real-world projects, we tasked participants with implementing a
singly-linked 'shopping list' structure in C. Our results indicate that the
security impact in this setting (low-level C with pointer and array
manipulations) is small: AI-assisted users produce critical security bugs at a
rate no greater than 10% more than the control, indicating the use of LLMs does
not introduce new security risks.","Sandoval, Gustavo and Pearce, Hammond and Nys, Teo and Karri, Ramesh and Garg, Siddharth and Dolan-Gavitt, Brendan",2023,,,,
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,empirical-study-2,\cite{empirical-study-2},An empirical study of code generation errors made by large language models,,,"Song, Da and Zhou, Zijie and Wang, Zhijie and Huang, Yuheng and Chen, Shengmai and Kou, Bonan and Ma, Lei and Zhang, Tianyi",2023,,,,
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,llm-gen-code-emp-study,\cite{llm-gen-code-emp-study},"Can We Trust Large Language Models Generated Code? A Framework for In-Context Learning, Security Patterns, and Code Evaluations Across Diverse LLMs",,,"Mohsin, Ahmad and Janicke, Helge and Wood, Adrian and Sarker, Iqbal H and Maglaras, Leandros and Janjua, Naeem",2024,,,,arXiv preprint arXiv:2406.12513
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,calibration,\cite{calibration},Calibration and Correctness of Language Models for Code,http://arxiv.org/abs/2402.02047v4,"Machine learning models are widely used, but can also often be wrong. Users
would benefit from a reliable indication of whether a given output from a given
model should be trusted, so a rational decision can be made whether to use the
output or not. For example, outputs can be associated with a confidence
measure; if this confidence measure is strongly associated with likelihood of
correctness, then the model is said to be well-calibrated.
  A well-calibrated confidence measure can serve as a basis for rational,
graduated decision-making on how much review and care is needed when using
generated code. Calibration has so far been studied in mostly non-generative
(e.g. classification) settings, especially in software engineering. However,
generated code can quite often be wrong: Given generated code, developers must
decide whether to use directly, use after varying intensity of careful review,
or discard model-generated code. Thus, calibration is vital in generative
settings.
  We make several contributions. We develop a framework for evaluating the
calibration of code-generating models. We consider several tasks, correctness
criteria, datasets, and approaches, and find that, by and large, generative
code models we test are not well-calibrated out of the box. We then show how
calibration can be improved using standard methods, such as Platt scaling.
Since Platt scaling relies on the prior availability of correctness data, we
evaluate the applicability and generalizability of Platt scaling in software
engineering, discuss settings where it has good potential for practical use,
and settings where it does not. Our contributions will lead to
better-calibrated decision-making in the current use of code generated by
language models, and offers a framework for future research to further improve
calibration methods for generative models in software engineering.","Spiess, Claudio and Gros, David and Pai, Kunal Suresh and Pradel, Michael and Rabin, Md Rafiqul Islam and Alipour, Amin and Jha, Susmit and Devanbu, Prem and Ahmed, Toufique",2024,,,,arXiv preprint arXiv:2402.02047
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,vulnerabilities-copilot,\cite{vulnerabilities-copilot},"Security Weaknesses of Copilot-Generated Code in GitHub Projects: An
  Empirical Study",http://arxiv.org/abs/2310.02059v4,"Modern code generation tools utilizing AI models like Large Language Models
(LLMs) have gained increased popularity due to their ability to produce
functional code. However, their usage presents security challenges, often
resulting in insecure code merging into the code base. Thus, evaluating the
quality of generated code, especially its security, is crucial. While prior
research explored various aspects of code generation, the focus on security has
been limited, mostly examining code produced in controlled environments rather
than open source development scenarios. To address this gap, we conducted an
empirical study, analyzing code snippets generated by GitHub Copilot and two
other AI code generation tools (i.e., CodeWhisperer and Codeium) from GitHub
projects. Our analysis identified 733 snippets, revealing a high likelihood of
security weaknesses, with 29.5% of Python and 24.2% of JavaScript snippets
affected. These issues span 43 Common Weakness Enumeration (CWE) categories,
including significant ones like CWE-330: Use of Insufficiently Random Values,
CWE-94: Improper Control of Generation of Code, and CWE-79: Cross-site
Scripting. Notably, eight of those CWEs are among the 2023 CWE Top-25,
highlighting their severity. We further examined using Copilot Chat to fix
security issues in Copilot-generated code by providing Copilot Chat with
warning messages from the static analysis tools, and up to 55.5% of the
security issues can be fixed. We finally provide the suggestions for mitigating
security issues in generated code.","Fu, Yujia and Liang, Peng and Tahir, Amjed and Li, Zengyang and Shahin, Mojtaba and Yu, Jiaxin and Chen, Jinfu",2023,,,,arXiv preprint arXiv:2310.02059
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,llm-security-guard,\cite{llm-security-guard},LLM Security Guard for Code,http://arxiv.org/abs/2405.01103v2,"Many developers rely on Large Language Models (LLMs) to facilitate software
development. Nevertheless, these models have exhibited limited capabilities in
the security domain. We introduce LLMSecGuard, a framework to offer enhanced
code security through the synergy between static code analyzers and LLMs.
LLMSecGuard is open source and aims to equip developers with code solutions
that are more secure than the code initially generated by LLMs. This framework
also has a benchmarking feature, aimed at providing insights into the evolving
security attributes of these models.","Kavian, Arya and Pourhashem Kallehbasti, Mohammad Mehdi and Kazemi, Sajjad and Firouzi, Ehsan and Ghafari, Mohammad",2024,,,,
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,autosafecoder,\cite{autosafecoder},"AutoSafeCoder: A Multi-Agent Framework for Securing LLM Code Generation
  through Static Analysis and Fuzz Testing",http://arxiv.org/abs/2409.10737v2,"Recent advancements in automatic code generation using large language models
(LLMs) have brought us closer to fully automated secure software development.
However, existing approaches often rely on a single agent for code generation,
which struggles to produce secure, vulnerability-free code. Traditional program
synthesis with LLMs has primarily focused on functional correctness, often
neglecting critical dynamic security implications that happen during runtime.
To address these challenges, we propose AutoSafeCoder, a multi-agent framework
that leverages LLM-driven agents for code generation, vulnerability analysis,
and security enhancement through continuous collaboration. The framework
consists of three agents: a Coding Agent responsible for code generation, a
Static Analyzer Agent identifying vulnerabilities, and a Fuzzing Agent
performing dynamic testing using a mutation-based fuzzing approach to detect
runtime errors. Our contribution focuses on ensuring the safety of multi-agent
code generation by integrating dynamic and static testing in an iterative
process during code generation by LLM that improves security. Experiments using
the SecurityEval dataset demonstrate a 13% reduction in code vulnerabilities
compared to baseline LLMs, with no compromise in functionality.","Nunez, Ana and Islam, Nafis Tanveer and Jha, Sumit Kumar and Najafirad, Peyman",2024,,,,arXiv preprint arXiv:2409.10737
Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,http://arxiv.org/abs/2510.02934v1,openia,\cite{openia},"Correctness Assessment of Code Generated by Large Language Models Using
  Internal Representations",http://arxiv.org/abs/2501.12934v2,"Ensuring the correctness of code generated by Large Language Models (LLMs)
presents a significant challenge in AI-driven software development. Existing
approaches predominantly rely on black-box (closed-box) approaches that
evaluate correctness post-generation, failing to utilize the rich insights
embedded in the LLMs' internal states during code generation. In this paper, we
introduce OPENIA, a novel white-box (open-box) framework that leverages these
internal representations to assess the correctness of LLM-generated code.
OPENIA systematically analyzes the intermediate states of representative
open-source LLMs specialized for code, including DeepSeek-Coder, CodeLlama, and
MagicCoder, across diverse code generation benchmarks. Our empirical analysis
reveals that these internal representations encode latent information, which
strongly correlates with the correctness of the generated code. Building on
these insights, OPENIA uses a white-box/open-box approach to make informed
predictions about code correctness, offering significant advantages in
adaptability and robustness over traditional classification-based methods and
zero-shot approaches. Experimental results demonstrate that OPENIA consistently
outperforms baseline models, achieving higher accuracy, precision, recall, and
F1-Scores with up to a 2X improvement in standalone code generation and a 46%
enhancement in repository-specific scenarios. By unlocking the potential of
in-process signals, OPENIA paves the way for more proactive and efficient
quality assurance mechanisms in LLM-assisted code generation.",,2025,,,https://doi.org/10.1016/j.jss.2025.112570,Journal of Systems and Software
