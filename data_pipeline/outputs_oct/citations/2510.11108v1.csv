parent_paper_title,parent_arxiv_link,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal
A Vision for Access Control in LLM-based Agent Systems,http://arxiv.org/abs/2510.11108v1,yao2023reactsynergizingreasoningacting,\cite{yao2023reactsynergizingreasoningacting},ReAct: Synergizing Reasoning and Acting in Language Models,http://arxiv.org/abs/2210.03629v3,"While large language models (LLMs) have demonstrated impressive capabilities
across tasks in language understanding and interactive decision making, their
abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.
action plan generation) have primarily been studied as separate topics. In this
paper, we explore the use of LLMs to generate both reasoning traces and
task-specific actions in an interleaved manner, allowing for greater synergy
between the two: reasoning traces help the model induce, track, and update
action plans as well as handle exceptions, while actions allow it to interface
with external sources, such as knowledge bases or environments, to gather
additional information. We apply our approach, named ReAct, to a diverse set of
language and decision making tasks and demonstrate its effectiveness over
state-of-the-art baselines, as well as improved human interpretability and
trustworthiness over methods without reasoning or acting components.
Concretely, on question answering (HotpotQA) and fact verification (Fever),
ReAct overcomes issues of hallucination and error propagation prevalent in
chain-of-thought reasoning by interacting with a simple Wikipedia API, and
generates human-like task-solving trajectories that are more interpretable than
baselines without reasoning traces. On two interactive decision making
benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and
reinforcement learning methods by an absolute success rate of 34% and 10%
respectively, while being prompted with only one or two in-context examples.
Project site with code: https://react-lm.github.io",Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik Narasimhan and Yuan Cao,2023,,https://arxiv.org/abs/2210.03629,,
A Vision for Access Control in LLM-based Agent Systems,http://arxiv.org/abs/2510.11108v1,nakano2022webgptbrowserassistedquestionansweringhuman,\cite{nakano2022webgptbrowserassistedquestionansweringhuman},WebGPT: Browser-assisted question-answering with human feedback,http://arxiv.org/abs/2112.09332v3,"We fine-tune GPT-3 to answer long-form questions using a text-based
web-browsing environment, which allows the model to search and navigate the
web. By setting up the task so that it can be performed by humans, we are able
to train models on the task using imitation learning, and then optimize answer
quality with human feedback. To make human evaluation of factual accuracy
easier, models must collect references while browsing in support of their
answers. We train and evaluate our models on ELI5, a dataset of questions asked
by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior
cloning, and then performing rejection sampling against a reward model trained
to predict human preferences. This model's answers are preferred by humans 56%
of the time to those of our human demonstrators, and 69% of the time to the
highest-voted answer from Reddit.",Reiichiro Nakano and Jacob Hilton and Suchir Balaji and Jeff Wu and Long Ouyang and Christina Kim and Christopher Hesse and Shantanu Jain and Vineet Kosaraju and William Saunders and Xu Jiang and Karl Cobbe and Tyna Eloundou and Gretchen Krueger and Kevin Button and Matthew Knight and Benjamin Chess and John Schulman,2022,,https://arxiv.org/abs/2112.09332,,
A Vision for Access Control in LLM-based Agent Systems,http://arxiv.org/abs/2510.11108v1,wang2023voyageropenendedembodiedagent,\cite{wang2023voyageropenendedembodiedagent},Voyager: An Open-Ended Embodied Agent with Large Language Models,http://arxiv.org/abs/2305.16291v2,"We introduce Voyager, the first LLM-powered embodied lifelong learning agent
in Minecraft that continuously explores the world, acquires diverse skills, and
makes novel discoveries without human intervention. Voyager consists of three
key components: 1) an automatic curriculum that maximizes exploration, 2) an
ever-growing skill library of executable code for storing and retrieving
complex behaviors, and 3) a new iterative prompting mechanism that incorporates
environment feedback, execution errors, and self-verification for program
improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses
the need for model parameter fine-tuning. The skills developed by Voyager are
temporally extended, interpretable, and compositional, which compounds the
agent's abilities rapidly and alleviates catastrophic forgetting. Empirically,
Voyager shows strong in-context lifelong learning capability and exhibits
exceptional proficiency in playing Minecraft. It obtains 3.3x more unique
items, travels 2.3x longer distances, and unlocks key tech tree milestones up
to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill
library in a new Minecraft world to solve novel tasks from scratch, while other
techniques struggle to generalize. We open-source our full codebase and prompts
at https://voyager.minedojo.org/.",Guanzhi Wang and Yuqi Xie and Yunfan Jiang and Ajay Mandlekar and Chaowei Xiao and Yuke Zhu and Linxi Fan and Anima Anandkumar,2023,,https://arxiv.org/abs/2305.16291,,
A Vision for Access Control in LLM-based Agent Systems,http://arxiv.org/abs/2510.11108v1,greshake2023youvesignedforcompromising,\cite{greshake2023youvesignedforcompromising},Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection,,,Kai Greshake and Sahar Abdelnabi and Shailesh Mishra and Christoph Endres and Thorsten Holz and Mario Fritz,2023,,https://arxiv.org/abs/2302.12173,,
A Vision for Access Control in LLM-based Agent Systems,http://arxiv.org/abs/2510.11108v1,Zhang_2025,\cite{Zhang_2025},"From Allies to Adversaries: Manipulating LLM Tool-Calling through
  Adversarial Injection",http://arxiv.org/abs/2412.10198v2,"Tool-calling has changed Large Language Model (LLM) applications by
integrating external tools, significantly enhancing their functionality across
diverse tasks. However, this integration also introduces new security
vulnerabilities, particularly in the tool scheduling mechanisms of LLM, which
have not been extensively studied. To fill this gap, we present ToolCommander,
a novel framework designed to exploit vulnerabilities in LLM tool-calling
systems through adversarial tool injection. Our framework employs a
well-designed two-stage attack strategy. Firstly, it injects malicious tools to
collect user queries, then dynamically updates the injected tools based on the
stolen information to enhance subsequent attacks. These stages enable
ToolCommander to execute privacy theft, launch denial-of-service attacks, and
even manipulate business competition by triggering unscheduled tool-calling.
Notably, the ASR reaches 91.67% for privacy theft and hits 100% for
denial-of-service and unscheduled tool calling in certain cases. Our work
demonstrates that these vulnerabilities can lead to severe consequences beyond
simple misuse of tool-calling systems, underscoring the urgent need for robust
defensive strategies to secure LLM Tool-calling systems.","Zhang, Rupeng and Wang, Haowei and Wang, Junjie and Li, Mingyang and Huang, Yuekai and Wang, Dandan and Wang, Qing",2025,,http://dx.doi.org/10.18653/v1/2025.naacl-long.101,10.18653/v1/2025.naacl-long.101,
A Vision for Access Control in LLM-based Agent Systems,http://arxiv.org/abs/2510.11108v1,li2026webcloak,\cite{li2026webcloak},WebCloak: Characterizing and Mitigating the Threats of LLM-Driven Web Agents as Intelligent Scrapers,,,"Li, Xinfeng and Qiu, Tianze and Jin, Yingbin and Wang, Lixu and Guo, Hanqing and Jia, Xiaojun and Wang, Xiaofeng and Dong, Wei",2026,,,,
A Vision for Access Control in LLM-based Agent Systems,http://arxiv.org/abs/2510.11108v1,he2025redteamingllmmultiagentsystems,\cite{he2025redteamingllmmultiagentsystems},Red-Teaming LLM Multi-Agent Systems via Communication Attacks,http://arxiv.org/abs/2502.14847v2,"Large Language Model-based Multi-Agent Systems (LLM-MAS) have revolutionized
complex problem-solving capability by enabling sophisticated agent
collaboration through message-based communications. While the communication
framework is crucial for agent coordination, it also introduces a critical yet
unexplored security vulnerability. In this work, we introduce
Agent-in-the-Middle (AiTM), a novel attack that exploits the fundamental
communication mechanisms in LLM-MAS by intercepting and manipulating
inter-agent messages. Unlike existing attacks that compromise individual
agents, AiTM demonstrates how an adversary can compromise entire multi-agent
systems by only manipulating the messages passing between agents. To enable the
attack under the challenges of limited control and role-restricted
communication format, we develop an LLM-powered adversarial agent with a
reflection mechanism that generates contextually-aware malicious instructions.
Our comprehensive evaluation across various frameworks, communication
structures, and real-world applications demonstrates that LLM-MAS is vulnerable
to communication-based attacks, highlighting the need for robust security
measures in multi-agent systems.",Pengfei He and Yupin Lin and Shen Dong and Han Xu and Yue Xing and Hui Liu,2025,,https://arxiv.org/abs/2502.14847,,
A Vision for Access Control in LLM-based Agent Systems,http://arxiv.org/abs/2510.11108v1,bolukbasi2016mancomputerprogrammerwoman,\cite{bolukbasi2016mancomputerprogrammerwoman},"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word
  Embeddings",http://arxiv.org/abs/1607.06520v1,"The blind application of machine learning runs the risk of amplifying biases
present in data. Such a danger is facing us with word embedding, a popular
framework to represent text data as vectors which has been used in many machine
learning and natural language processing tasks. We show that even word
embeddings trained on Google News articles exhibit female/male gender
stereotypes to a disturbing extent. This raises concerns because their
widespread use, as we describe, often tends to amplify these biases.
Geometrically, gender bias is first shown to be captured by a direction in the
word embedding. Second, gender neutral words are shown to be linearly separable
from gender definition words in the word embedding. Using these properties, we
provide a methodology for modifying an embedding to remove gender stereotypes,
such as the association between between the words receptionist and female,
while maintaining desired associations such as between the words queen and
female. We define metrics to quantify both direct and indirect gender biases in
embeddings, and develop algorithms to ""debias"" the embedding. Using
crowd-worker evaluation as well as standard benchmarks, we empirically
demonstrate that our algorithms significantly reduce gender bias in embeddings
while preserving the its useful properties such as the ability to cluster
related concepts and to solve analogy tasks. The resulting embeddings can be
used in applications without amplifying gender bias.",Tolga Bolukbasi and Kai-Wei Chang and James Zou and Venkatesh Saligrama and Adam Kalai,2016,,https://arxiv.org/abs/1607.06520,,
A Vision for Access Control in LLM-based Agent Systems,http://arxiv.org/abs/2510.11108v1,henderson2017ethicalchallengesdatadrivendialogue,\cite{henderson2017ethicalchallengesdatadrivendialogue},Ethical Challenges in Data-Driven Dialogue Systems,http://arxiv.org/abs/1711.09050v1,"The use of dialogue systems as a medium for human-machine interaction is an
increasingly prevalent paradigm. A growing number of dialogue systems use
conversation strategies that are learned from large datasets. There are well
documented instances where interactions with these system have resulted in
biased or even offensive conversations due to the data-driven training process.
Here, we highlight potential ethical issues that arise in dialogue systems
research, including: implicit biases in data-driven systems, the rise of
adversarial examples, potential sources of privacy violations, safety concerns,
special considerations for reinforcement learning systems, and reproducibility
concerns. We also suggest areas stemming from these issues that deserve further
investigation. Through this initial survey, we hope to spur research leading to
robust, safe, and ethically sound dialogue systems.",Peter Henderson and Koustuv Sinha and Nicolas Angelard-Gontier and Nan Rosemary Ke and Genevieve Fried and Ryan Lowe and Joelle Pineau,2017,,https://arxiv.org/abs/1711.09050,,
A Vision for Access Control in LLM-based Agent Systems,http://arxiv.org/abs/2510.11108v1,Caliskan_2017,\cite{Caliskan_2017},"Semantics derived automatically from language corpora contain human-like
  biases",http://arxiv.org/abs/1608.07187v4,"Artificial intelligence and machine learning are in a period of astounding
growth. However, there are concerns that these technologies may be used, either
with or without intention, to perpetuate the prejudice and unfairness that
unfortunately characterizes many human institutions. Here we show for the first
time that human-like semantic biases result from the application of standard
machine learning to ordinary language---the same sort of language humans are
exposed to every day. We replicate a spectrum of standard human biases as
exposed by the Implicit Association Test and other well-known psychological
studies. We replicate these using a widely used, purely statistical
machine-learning model---namely, the GloVe word embedding---trained on a corpus
of text from the Web. Our results indicate that language itself contains
recoverable and accurate imprints of our historic biases, whether these are
morally neutral as towards insects or flowers, problematic as towards race or
gender, or even simply veridical, reflecting the {\em status quo} for the
distribution of gender with respect to careers or first names. These
regularities are captured by machine learning along with the rest of semantics.
In addition to our empirical findings concerning language, we also contribute
new methods for evaluating bias in text, the Word Embedding Association Test
(WEAT) and the Word Embedding Factual Association Test (WEFAT). Our results
have implications not only for AI and machine learning, but also for the fields
of psychology, sociology, and human ethics, since they raise the possibility
that mere exposure to everyday language can account for the biases we replicate
here.","Caliskan, Aylin and Bryson, Joanna J. and Narayanan, Arvind",2017,,http://dx.doi.org/10.1126/science.aal4230,10.1126/science.aal4230,Science
A Vision for Access Control in LLM-based Agent Systems,http://arxiv.org/abs/2510.11108v1,li2024safegenmitigatingsexual,\cite{li2024safegenmitigatingsexual},"SafeGen: Mitigating Sexually Explicit Content Generation in
  Text-to-Image Models",http://arxiv.org/abs/2404.06666v3,"Text-to-image (T2I) models, such as Stable Diffusion, have exhibited
remarkable performance in generating high-quality images from text descriptions
in recent years. However, text-to-image models may be tricked into generating
not-safe-for-work (NSFW) content, particularly in sexually explicit scenarios.
Existing countermeasures mostly focus on filtering inappropriate inputs and
outputs, or suppressing improper text embeddings, which can block sexually
explicit content (e.g., naked) but may still be vulnerable to adversarial
prompts -- inputs that appear innocent but are ill-intended. In this paper, we
present SafeGen, a framework to mitigate sexual content generation by
text-to-image models in a text-agnostic manner. The key idea is to eliminate
explicit visual representations from the model regardless of the text input. In
this way, the text-to-image model is resistant to adversarial prompts since
such unsafe visual representations are obstructed from within. Extensive
experiments conducted on four datasets and large-scale user studies demonstrate
SafeGen's effectiveness in mitigating sexually explicit content generation
while preserving the high-fidelity of benign images. SafeGen outperforms eight
state-of-the-art baseline methods and achieves 99.4% sexual content removal
performance. Furthermore, our constructed benchmark of adversarial prompts
provides a basis for future development and evaluation of anti-NSFW-generation
methods.","Li, Xinfeng and Yang, Yuchen and Deng, Jiangyi and Yan, Chen and Chen, Yanjiao and Ji, Xiaoyu and Xu, Wenyuan",2024,,,,
A Vision for Access Control in LLM-based Agent Systems,http://arxiv.org/abs/2510.11108v1,durante2024agentaisurveyinghorizons,\cite{durante2024agentaisurveyinghorizons},Agent AI: Surveying the Horizons of Multimodal Interaction,http://arxiv.org/abs/2401.03568v2,"Multi-modal AI systems will likely become a ubiquitous presence in our
everyday lives. A promising approach to making these systems more interactive
is to embody them as agents within physical and virtual environments. At
present, systems leverage existing foundation models as the basic building
blocks for the creation of embodied agents. Embedding agents within such
environments facilitates the ability of models to process and interpret visual
and contextual data, which is critical for the creation of more sophisticated
and context-aware AI systems. For example, a system that can perceive user
actions, human behavior, environmental objects, audio expressions, and the
collective sentiment of a scene can be used to inform and direct agent
responses within the given environment. To accelerate research on agent-based
multimodal intelligence, we define ""Agent AI"" as a class of interactive systems
that can perceive visual stimuli, language inputs, and other
environmentally-grounded data, and can produce meaningful embodied actions. In
particular, we explore systems that aim to improve agents based on
next-embodied action prediction by incorporating external knowledge,
multi-sensory inputs, and human feedback. We argue that by developing agentic
AI systems in grounded environments, one can also mitigate the hallucinations
of large foundation models and their tendency to generate environmentally
incorrect outputs. The emerging field of Agent AI subsumes the broader embodied
and agentic aspects of multimodal interactions. Beyond agents acting and
interacting in the physical world, we envision a future where people can easily
create any virtual reality or simulated scene and interact with agents embodied
within the virtual environment.",Zane Durante and Qiuyuan Huang and Naoki Wake and Ran Gong and Jae Sung Park and Bidipta Sarkar and Rohan Taori and Yusuke Noda and Demetri Terzopoulos and Yejin Choi and Katsushi Ikeuchi and Hoi Vo and Li Fei-Fei and Jianfeng Gao,2024,,https://arxiv.org/abs/2401.03568,,
