parent_paper_title,parent_arxiv_link,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal
Task-Level Insights from Eigenvalues across Sequence Models,http://arxiv.org/abs/2510.09379v1,helfrich2019eigenvaluenormalizedrecurrentneural,\cite{helfrich2019eigenvaluenormalizedrecurrentneural},Eigenvalue Normalized Recurrent Neural Networks for Short Term Memory,http://arxiv.org/abs/1911.07964v1,"Several variants of recurrent neural networks (RNNs) with orthogonal or
unitary recurrent matrices have recently been developed to mitigate the
vanishing/exploding gradient problem and to model long-term dependencies of
sequences. However, with the eigenvalues of the recurrent matrix on the unit
circle, the recurrent state retains all input information which may
unnecessarily consume model capacity. In this paper, we address this issue by
proposing an architecture that expands upon an orthogonal/unitary RNN with a
state that is generated by a recurrent matrix with eigenvalues in the unit
disc. Any input to this state dissipates in time and is replaced with new
inputs, simulating short-term memory. A gradient descent algorithm is derived
for learning such a recurrent matrix. The resulting method, called the
Eigenvalue Normalized RNN (ENRNN), is shown to be highly competitive in several
experiments.",Kyle Helfrich and Qiang Ye,2019,,https://arxiv.org/abs/1911.07964,,
Task-Level Insights from Eigenvalues across Sequence Models,http://arxiv.org/abs/2510.09379v1,Jarne_2022,\cite{Jarne_2022},"Different eigenvalue distributions encode the same temporal tasks in
  recurrent neural networks",http://arxiv.org/abs/2005.13074v5,"Different brain areas, such as the cortex and, more specifically, the
prefrontal cortex, show great recurrence in their connections, even in early
sensory areas. {Several approaches and methods based on trained networks have
been proposed to model and describe these regions. It is essential to
understand the dynamics behind the models because they are used to build
different hypotheses about the functioning of brain areas and to explain
experimental results. The main contribution here is the description of the
dynamics through the classification and interpretation carried out with a set
of numerical simulations. This study sheds light on the multiplicity of
solutions obtained for the same tasks and shows the link between the spectra of
linearized trained networks and the dynamics of the counterparts. The patterns
in the distribution of the eigenvalues of the recurrent weight matrix were
studied and properly related to the dynamics in each task.","Jarne, Cecilia",2022,,http://dx.doi.org/10.1007/s11571-022-09802-5,10.1007/s11571-022-09802-5,Cognitive Neurodynamics
Task-Level Insights from Eigenvalues across Sequence Models,http://arxiv.org/abs/2510.09379v1,Gu2020,\cite{Gu2020},HiPPO: Recurrent Memory with Optimal Polynomial Projections,http://arxiv.org/abs/2008.07669v2,"A central problem in learning from sequential data is representing cumulative
history in an incremental fashion as more data is processed. We introduce a
general framework (HiPPO) for the online compression of continuous signals and
discrete time series by projection onto polynomial bases. Given a measure that
specifies the importance of each time step in the past, HiPPO produces an
optimal solution to a natural online function approximation problem. As special
cases, our framework yields a short derivation of the recent Legendre Memory
Unit (LMU) from first principles, and generalizes the ubiquitous gating
mechanism of recurrent neural networks such as GRUs. This formal framework
yields a new memory update mechanism (HiPPO-LegS) that scales through time to
remember all history, avoiding priors on the timescale. HiPPO-LegS enjoys the
theoretical benefits of timescale robustness, fast updates, and bounded
gradients. By incorporating the memory dynamics into recurrent neural networks,
HiPPO RNNs can empirically capture complex temporal dependencies. On the
benchmark permuted MNIST dataset, HiPPO-LegS sets a new state-of-the-art
accuracy of 98.3%. Finally, on a novel trajectory classification task testing
robustness to out-of-distribution timescales and missing data, HiPPO-LegS
outperforms RNN and neural ODE baselines by 25-40% accuracy.","Gu, Albert and Dao, Tri and Ermon, Stefano and Rudra, Atri and R\'{e}, Christopher",2020,,,,
Task-Level Insights from Eigenvalues across Sequence Models,http://arxiv.org/abs/2510.09379v1,Fu2023,\cite{Fu2023},Hungry Hungry Hippos: Towards Language Modeling with State Space Models,http://arxiv.org/abs/2212.14052v3,"State space models (SSMs) have demonstrated state-of-the-art sequence
modeling performance in some modalities, but underperform attention in language
modeling. Moreover, despite scaling nearly linearly in sequence length instead
of quadratically, SSMs are still slower than Transformers due to poor hardware
utilization. In this paper, we make progress on understanding the expressivity
gap between SSMs and attention in language modeling, and on reducing the
hardware barrier between SSMs and attention. First, we use synthetic language
modeling tasks to understand the gap between SSMs and attention. We find that
existing SSMs struggle with two capabilities: recalling earlier tokens in the
sequence and comparing tokens across the sequence. To understand the impact on
language modeling, we propose a new SSM layer, H3, that is explicitly designed
for these abilities. H3 matches attention on the synthetic languages and comes
within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid
125M-parameter H3-attention model that retains two attention layers
surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to
improve the efficiency of training SSMs on modern hardware, we propose
FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on
sequences up to 8K, and introduces a novel state passing algorithm that
exploits the recurrent properties of SSMs to scale to longer sequences.
FlashConv yields 2$\times$ speedup on the long-range arena benchmark and allows
hybrid language models to generate text 2.4$\times$ faster than Transformers.
Using FlashConv, we scale hybrid H3-attention language models up to 2.7B
parameters on the Pile and find promising initial results, achieving lower
perplexity than Transformers and outperforming Transformers in zero- and
few-shot learning on a majority of tasks in the SuperGLUE benchmark.",Daniel Y. Fu and Tri Dao and Khaled K. Saab and Armin W. Thomas and Atri Rudra and Christopher Ré,2023,,https://arxiv.org/abs/2212.14052,,
Task-Level Insights from Eigenvalues across Sequence Models,http://arxiv.org/abs/2510.09379v1,wang2024stablessmalleviatingcursememory,\cite{wang2024stablessmalleviatingcursememory},"StableSSM: Alleviating the Curse of Memory in State-space Models through
  Stable Reparameterization",http://arxiv.org/abs/2311.14495v4,"In this paper, we investigate the long-term memory learning capabilities of
state-space models (SSMs) from the perspective of parameterization. We prove
that state-space models without any reparameterization exhibit a memory
limitation similar to that of traditional RNNs: the target relationships that
can be stably approximated by state-space models must have an exponential
decaying memory. Our analysis identifies this ""curse of memory"" as a result of
the recurrent weights converging to a stability boundary, suggesting that a
reparameterization technique can be effective. To this end, we introduce a
class of reparameterization techniques for SSMs that effectively lift its
memory limitations. Besides improving approximation capabilities, we further
illustrate that a principled choice of reparameterization scheme can also
enhance optimization stability. We validate our findings using synthetic
datasets, language models and image classifications.",Shida Wang and Qianxiao Li,2024,,https://arxiv.org/abs/2311.14495,,
Task-Level Insights from Eigenvalues across Sequence Models,http://arxiv.org/abs/2510.09379v1,grazzi2025unlockingstatetrackinglinearrnns,\cite{grazzi2025unlockingstatetrackinglinearrnns},Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues,http://arxiv.org/abs/2411.12537v5,"Linear Recurrent Neural Networks (LRNNs) such as Mamba, RWKV, GLA, mLSTM, and
DeltaNet have emerged as efficient alternatives to Transformers for long
sequences. However, both Transformers and LRNNs struggle to perform
state-tracking, which may impair performance in tasks such as code evaluation.
In one forward pass, current architectures are unable to solve even parity, the
simplest state-tracking task, which non-linear RNNs can handle effectively.
Recently, Sarrof et al. (2024) demonstrated that the failure of LRNNs like
Mamba to solve parity stems from restricting the value range of their diagonal
state-transition matrices to $[0, 1]$ and that incorporating negative values
can resolve this issue. We extend this result to non-diagonal LRNNs such as
DeltaNet. We prove that finite precision LRNNs with state-transition matrices
having only positive eigenvalues cannot solve parity, while non-triangular
matrices are needed to count modulo $3$. Notably, we also prove that LRNNs can
learn any regular language when their state-transition matrices are products of
identity minus vector outer product matrices, each with eigenvalues in the
range $[-1, 1]$. Our experiments confirm that extending the eigenvalue range of
Mamba and DeltaNet to include negative values not only enables them to solve
parity but consistently improves their performance on state-tracking tasks. We
also show that state-tracking enabled LRNNs can be pretrained stably and
efficiently at scale (1.3B parameters), achieving competitive performance on
language modeling and showing promise on code and math tasks.",Riccardo Grazzi and Julien Siems and Arber Zela and Jörg K. H. Franke and Frank Hutter and Massimiliano Pontil,2025,,https://arxiv.org/abs/2411.12537,,
Task-Level Insights from Eigenvalues across Sequence Models,http://arxiv.org/abs/2510.09379v1,oppenheim1997signals,\cite{oppenheim1997signals},Signals \& systems,,,"Oppenheim, Alan V and Willsky, Alan S and Nawab, Syed Hamid",1997,,,,
Task-Level Insights from Eigenvalues across Sequence Models,http://arxiv.org/abs/2510.09379v1,wang2023state,\cite{wang2023state},"State-space Models with Layer-wise Nonlinearity are Universal
  Approximators with Exponential Decaying Memory",http://arxiv.org/abs/2309.13414v3,"State-space models have gained popularity in sequence modelling due to their
simple and efficient network structures. However, the absence of nonlinear
activation along the temporal direction limits the model's capacity. In this
paper, we prove that stacking state-space models with layer-wise nonlinear
activation is sufficient to approximate any continuous sequence-to-sequence
relationship. Our findings demonstrate that the addition of layer-wise
nonlinear activation enhances the model's capacity to learn complex sequence
patterns. Meanwhile, it can be seen both theoretically and empirically that the
state-space models do not fundamentally resolve the issue of exponential
decaying memory. Theoretical results are justified by numerical verifications.","Wang, Shida and Xue, Beichen",2023,,,,Advances in Neural Information Processing Systems
Task-Level Insights from Eigenvalues across Sequence Models,http://arxiv.org/abs/2510.09379v1,Su_2019,\cite{Su_2019},On extended long short-term memory and dependent bidirectional recurrent neural network,,,"Su, Yuanhang and Kuo, C.-C. Jay",2019,,http://dx.doi.org/10.1016/j.neucom.2019.04.044,10.1016/j.neucom.2019.04.044,Neurocomputing
Task-Level Insights from Eigenvalues across Sequence Models,http://arxiv.org/abs/2510.09379v1,raffel2016feedforwardnetworksattentionsolve,\cite{raffel2016feedforwardnetworksattentionsolve},"Feed-Forward Networks with Attention Can Solve Some Long-Term Memory
  Problems",http://arxiv.org/abs/1512.08756v5,"We propose a simplified model of attention which is applicable to
feed-forward neural networks and demonstrate that the resulting model can solve
the synthetic ""addition"" and ""multiplication"" long-term memory problems for
sequence lengths which are both longer and more widely varying than the best
published results for these tasks.",Colin Raffel and Daniel P. W. Ellis,2016,,https://arxiv.org/abs/1512.08756,,
Task-Level Insights from Eigenvalues across Sequence Models,http://arxiv.org/abs/2510.09379v1,sukhbaatar2019augmentingselfattentionpersistentmemory,\cite{sukhbaatar2019augmentingselfattentionpersistentmemory},Augmenting Self-attention with Persistent Memory,http://arxiv.org/abs/1907.01470v1,"Transformer networks have lead to important progress in language modeling and
machine translation. These models include two consecutive modules, a
feed-forward layer and a self-attention layer. The latter allows the network to
capture long term dependencies and are often regarded as the key ingredient in
the success of Transformers. Building upon this intuition, we propose a new
model that solely consists of attention layers. More precisely, we augment the
self-attention layers with persistent memory vectors that play a similar role
as the feed-forward layer. Thanks to these vectors, we can remove the
feed-forward layer without degrading the performance of a transformer. Our
evaluation shows the benefits brought by our model on standard character and
word level language modeling benchmarks.",Sainbayar Sukhbaatar and Edouard Grave and Guillaume Lample and Herve Jegou and Armand Joulin,2019,,https://arxiv.org/abs/1907.01470,,
Task-Level Insights from Eigenvalues across Sequence Models,http://arxiv.org/abs/2510.09379v1,bao2024self,\cite{bao2024self},Self-attention Networks Localize When QK-eigenspectrum Concentrates,http://arxiv.org/abs/2402.02098v1,"The self-attention mechanism prevails in modern machine learning. It has an
interesting functionality of adaptively selecting tokens from an input sequence
by modulating the degree of attention localization, which many researchers
speculate is the basis of the powerful model performance but complicates the
underlying mechanism of the learning dynamics. In recent years, mainly two
arguments have connected attention localization to the model performances. One
is the rank collapse, where the embedded tokens by a self-attention block
become very similar across different tokens, leading to a less expressive
network. The other is the entropy collapse, where the attention probability
approaches non-uniform and entails low entropy, making the learning dynamics
more likely to be trapped in plateaus. These two failure modes may apparently
contradict each other because the rank and entropy collapses are relevant to
uniform and non-uniform attention, respectively. To this end, we characterize
the notion of attention localization by the eigenspectrum of query-key
parameter matrices and reveal that a small eigenspectrum variance leads
attention to be localized. Interestingly, the small eigenspectrum variance
prevents both rank and entropy collapse, leading to better model expressivity
and trainability.","Bao, Han and Hataya, Ryuichiro and Karakida, Ryo",2024,,,,
Task-Level Insights from Eigenvalues across Sequence Models,http://arxiv.org/abs/2510.09379v1,bhojanapalli2021eigen,\cite{bhojanapalli2021eigen},"Eigen Analysis of Self-Attention and its Reconstruction from Partial
  Computation",http://arxiv.org/abs/2106.08823v1,"State-of-the-art transformer models use pairwise dot-product based
self-attention, which comes at a computational cost quadratic in the input
sequence length. In this paper, we investigate the global structure of
attention scores computed using this dot product mechanism on a typical
distribution of inputs, and study the principal components of their variation.
Through eigen analysis of full attention score matrices, as well as of their
individual rows, we find that most of the variation among attention scores lie
in a low-dimensional eigenspace. Moreover, we find significant overlap between
these eigenspaces for different layers and even different transformer models.
Based on this, we propose to compute scores only for a partial subset of token
pairs, and use them to estimate scores for the remaining pairs. Beyond
investigating the accuracy of reconstructing attention scores themselves, we
investigate training transformer models that employ these approximations, and
analyze the effect on overall accuracy. Our analysis and the proposed method
provide insights into how to balance the benefits of exact pair-wise attention
and its significant computational expense.","Bhojanapalli, Srinadh and Chakrabarti, Ayan and Jain, Himanshu and Kumar, Sanjiv and Lukasik, Michal and Veit, Andreas",2021,,,,arXiv preprint arXiv:2106.08823
Task-Level Insights from Eigenvalues across Sequence Models,http://arxiv.org/abs/2510.09379v1,mamba2,\cite{mamba2},"Transformers are SSMs: Generalized Models and Efficient Algorithms
  Through Structured State Space Duality",http://arxiv.org/abs/2405.21060v1,"While Transformers have been the main architecture behind deep learning's
success in language modeling, state-space models (SSMs) such as Mamba have
recently been shown to match or outperform Transformers at small to medium
scale. We show that these families of models are actually quite closely
related, and develop a rich framework of theoretical connections between SSMs
and variants of attention, connected through various decompositions of a
well-studied class of structured semiseparable matrices. Our state space
duality (SSD) framework allows us to design a new architecture (Mamba-2) whose
core layer is an a refinement of Mamba's selective SSM that is 2-8X faster,
while continuing to be competitive with Transformers on language modeling.",Tri Dao and Albert Gu,2024,,,,
