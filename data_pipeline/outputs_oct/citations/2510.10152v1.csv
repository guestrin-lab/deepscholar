parent_paper_title,parent_arxiv_link,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,mildenhall2021nerf,\cite{mildenhall2021nerf},NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis,http://arxiv.org/abs/2003.08934v2,"We present a method that achieves state-of-the-art results for synthesizing
novel views of complex scenes by optimizing an underlying continuous volumetric
scene function using a sparse set of input views. Our algorithm represents a
scene using a fully-connected (non-convolutional) deep network, whose input is
a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing
direction $(\theta, \phi)$) and whose output is the volume density and
view-dependent emitted radiance at that spatial location. We synthesize views
by querying 5D coordinates along camera rays and use classic volume rendering
techniques to project the output colors and densities into an image. Because
volume rendering is naturally differentiable, the only input required to
optimize our representation is a set of images with known camera poses. We
describe how to effectively optimize neural radiance fields to render
photorealistic novel views of scenes with complicated geometry and appearance,
and demonstrate results that outperform prior work on neural rendering and view
synthesis. View synthesis results are best viewed as videos, so we urge readers
to view our supplementary video for convincing comparisons.","Mildenhall, Ben and Srinivasan, Pratul P and Tancik, Matthew and Barron, Jonathan T and Ramamoorthi, Ravi and Ng, Ren",2021,,,,Communications of the ACM
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,kerbl20233d,\cite{kerbl20233d},3D Gaussian Splatting for Real-Time Radiance Field Rendering,http://arxiv.org/abs/2308.04079v1,"Radiance Field methods have recently revolutionized novel-view synthesis of
scenes captured with multiple photos or videos. However, achieving high visual
quality still requires neural networks that are costly to train and render,
while recent faster methods inevitably trade off speed for quality. For
unbounded and complete scenes (rather than isolated objects) and 1080p
resolution rendering, no current method can achieve real-time display rates. We
introduce three key elements that allow us to achieve state-of-the-art visual
quality while maintaining competitive training times and importantly allow
high-quality real-time (>= 30 fps) novel-view synthesis at 1080p resolution.
First, starting from sparse points produced during camera calibration, we
represent the scene with 3D Gaussians that preserve desirable properties of
continuous volumetric radiance fields for scene optimization while avoiding
unnecessary computation in empty space; Second, we perform interleaved
optimization/density control of the 3D Gaussians, notably optimizing
anisotropic covariance to achieve an accurate representation of the scene;
Third, we develop a fast visibility-aware rendering algorithm that supports
anisotropic splatting and both accelerates training and allows realtime
rendering. We demonstrate state-of-the-art visual quality and real-time
rendering on several established datasets.","Kerbl, Bernhard and Kopanas, Georgios and Leimk{\""u}hler, Thomas and Drettakis, George",2023,,,,ACM Trans. Graph.
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,barron2021mip,\cite{barron2021mip},Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields,,,"Barron, Jonathan T and Mildenhall, Ben and Tancik, Matthew and Hedman, Peter and Martin-Brualla, Ricardo and Srinivasan, Pratul P",2021,,,,
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,barron2022mip,\cite{barron2022mip},Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields,http://arxiv.org/abs/2111.12077v3,"Though neural radiance fields (NeRF) have demonstrated impressive view
synthesis results on objects and small bounded regions of space, they struggle
on ""unbounded"" scenes, where the camera may point in any direction and content
may exist at any distance. In this setting, existing NeRF-like models often
produce blurry or low-resolution renderings (due to the unbalanced detail and
scale of nearby and distant objects), are slow to train, and may exhibit
artifacts due to the inherent ambiguity of the task of reconstructing a large
scene from a small set of images. We present an extension of mip-NeRF (a NeRF
variant that addresses sampling and aliasing) that uses a non-linear scene
parameterization, online distillation, and a novel distortion-based regularizer
to overcome the challenges presented by unbounded scenes. Our model, which we
dub ""mip-NeRF 360"" as we target scenes in which the camera rotates 360 degrees
around a point, reduces mean-squared error by 57% compared to mip-NeRF, and is
able to produce realistic synthesized views and detailed depth maps for highly
intricate, unbounded real-world scenes.","Barron, Jonathan T and Mildenhall, Ben and Verbin, Dor and Srinivasan, Pratul P and Hedman, Peter",2022,,,,
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,barron2023zip,\cite{barron2023zip},Zip-nerf: Anti-aliased grid-based neural radiance fields,,,"Barron, Jonathan T and Mildenhall, Ben and Verbin, Dor and Srinivasan, Pratul P and Hedman, Peter",2023,,,,
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,verbin2022ref,\cite{verbin2022ref},Ref-nerf: Structured view-dependent appearance for neural radiance fields,,,"Verbin, Dor and Hedman, Peter and Mildenhall, Ben and Zickler, Todd and Barron, Jonathan T and Srinivasan, Pratul P",2022,,,,
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,chen2022tensorf,\cite{chen2022tensorf},TensoRF: Tensorial Radiance Fields,http://arxiv.org/abs/2203.09517v2,"We present TensoRF, a novel approach to model and reconstruct radiance
fields. Unlike NeRF that purely uses MLPs, we model the radiance field of a
scene as a 4D tensor, which represents a 3D voxel grid with per-voxel
multi-channel features. Our central idea is to factorize the 4D scene tensor
into multiple compact low-rank tensor components. We demonstrate that applying
traditional CP decomposition -- that factorizes tensors into rank-one
components with compact vectors -- in our framework leads to improvements over
vanilla NeRF. To further boost performance, we introduce a novel vector-matrix
(VM) decomposition that relaxes the low-rank constraints for two modes of a
tensor and factorizes tensors into compact vector and matrix factors. Beyond
superior rendering quality, our models with CP and VM decompositions lead to a
significantly lower memory footprint in comparison to previous and concurrent
works that directly optimize per-voxel features. Experimentally, we demonstrate
that TensoRF with CP decomposition achieves fast reconstruction (<30 min) with
better rendering quality and even a smaller model size (<4 MB) compared to
NeRF. Moreover, TensoRF with VM decomposition further boosts rendering quality
and outperforms previous state-of-the-art methods, while reducing the
reconstruction time (<10 min) and retaining a compact model size (<75 MB).","Chen, Anpei and Xu, Zexiang and Geiger, Andreas and Yu, Jingyi and Su, Hao",2022,,,,
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,fridovich2022plenoxels,\cite{fridovich2022plenoxels},Plenoxels: Radiance Fields without Neural Networks,http://arxiv.org/abs/2112.05131v1,"We introduce Plenoxels (plenoptic voxels), a system for photorealistic view
synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical
harmonics. This representation can be optimized from calibrated images via
gradient methods and regularization without any neural components. On standard,
benchmark tasks, Plenoxels are optimized two orders of magnitude faster than
Neural Radiance Fields with no loss in visual quality.","Fridovich-Keil, Sara and Yu, Alex and Tancik, Matthew and Chen, Qinhong and Recht, Benjamin and Kanazawa, Angjoo",2022,,,,
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,garbin2021fastnerf,\cite{garbin2021fastnerf},FastNeRF: High-Fidelity Neural Rendering at 200FPS,http://arxiv.org/abs/2103.10380v2,"Recent work on Neural Radiance Fields (NeRF) showed how neural networks can
be used to encode complex 3D environments that can be rendered
photorealistically from novel viewpoints. Rendering these images is very
computationally demanding and recent improvements are still a long way from
enabling interactive rates, even on high-end hardware. Motivated by scenarios
on mobile and mixed reality devices, we propose FastNeRF, the first NeRF-based
system capable of rendering high fidelity photorealistic images at 200Hz on a
high-end consumer GPU. The core of our method is a graphics-inspired
factorization that allows for (i) compactly caching a deep radiance map at each
position in space, (ii) efficiently querying that map using ray directions to
estimate the pixel values in the rendered image. Extensive experiments show
that the proposed method is 3000 times faster than the original NeRF algorithm
and at least an order of magnitude faster than existing work on accelerating
NeRF, while maintaining visual quality and extensibility.","Garbin, Stephan J and Kowalski, Marek and Johnson, Matthew and Shotton, Jamie and Valentin, Julien",2021,,,,
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,muller2022instant,\cite{muller2022instant},Instant Neural Graphics Primitives with a Multiresolution Hash Encoding,http://arxiv.org/abs/2201.05989v2,"Neural graphics primitives, parameterized by fully connected neural networks,
can be costly to train and evaluate. We reduce this cost with a versatile new
input encoding that permits the use of a smaller network without sacrificing
quality, thus significantly reducing the number of floating point and memory
access operations: a small neural network is augmented by a multiresolution
hash table of trainable feature vectors whose values are optimized through
stochastic gradient descent. The multiresolution structure allows the network
to disambiguate hash collisions, making for a simple architecture that is
trivial to parallelize on modern GPUs. We leverage this parallelism by
implementing the whole system using fully-fused CUDA kernels with a focus on
minimizing wasted bandwidth and compute operations. We achieve a combined
speedup of several orders of magnitude, enabling training of high-quality
neural graphics primitives in a matter of seconds, and rendering in tens of
milliseconds at a resolution of ${1920\!\times\!1080}$.","M{\""u}ller, Thomas and Evans, Alex and Schied, Christoph and Keller, Alexander",2022,,,,ACM transactions on graphics (TOG)
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,park2021nerfies,\cite{park2021nerfies},Nerfies: Deformable Neural Radiance Fields,http://arxiv.org/abs/2011.12948v5,"We present the first method capable of photorealistically reconstructing
deformable scenes using photos/videos captured casually from mobile phones. Our
approach augments neural radiance fields (NeRF) by optimizing an additional
continuous volumetric deformation field that warps each observed point into a
canonical 5D NeRF. We observe that these NeRF-like deformation fields are prone
to local minima, and propose a coarse-to-fine optimization method for
coordinate-based models that allows for more robust optimization. By adapting
principles from geometry processing and physical simulation to NeRF-like
models, we propose an elastic regularization of the deformation field that
further improves robustness. We show that our method can turn casually captured
selfie photos/videos into deformable NeRF models that allow for photorealistic
renderings of the subject from arbitrary viewpoints, which we dub ""nerfies."" We
evaluate our method by collecting time-synchronized data using a rig with two
mobile phones, yielding train/validation images of the same pose at different
viewpoints. We show that our method faithfully reconstructs non-rigidly
deforming scenes and reproduces unseen views with high fidelity.","Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M and Martin-Brualla, Ricardo",2021,,,,
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,park2021hypernerf,\cite{park2021hypernerf},"HyperNeRF: A Higher-Dimensional Representation for Topologically Varying
  Neural Radiance Fields",http://arxiv.org/abs/2106.13228v2,"Neural Radiance Fields (NeRF) are able to reconstruct scenes with
unprecedented fidelity, and various recent works have extended NeRF to handle
dynamic scenes. A common approach to reconstruct such non-rigid scenes is
through the use of a learned deformation field mapping from coordinates in each
input image into a canonical template coordinate space. However, these
deformation-based approaches struggle to model changes in topology, as
topological changes require a discontinuity in the deformation field, but these
deformation fields are necessarily continuous. We address this limitation by
lifting NeRFs into a higher dimensional space, and by representing the 5D
radiance field corresponding to each individual input image as a slice through
this ""hyper-space"". Our method is inspired by level set methods, which model
the evolution of surfaces as slices through a higher dimensional surface. We
evaluate our method on two tasks: (i) interpolating smoothly between ""moments"",
i.e., configurations of the scene, seen in the input images while maintaining
visual plausibility, and (ii) novel-view synthesis at fixed moments. We show
that our method, which we dub HyperNeRF, outperforms existing methods on both
tasks. Compared to Nerfies, HyperNeRF reduces average error rates by 4.1% for
interpolation and 8.6% for novel-view synthesis, as measured by LPIPS.
Additional videos, results, and visualizations are available at
https://hypernerf.github.io.","Park, Keunhong and Sinha, Utkarsh and Hedman, Peter and Barron, Jonathan T and Bouaziz, Sofien and Goldman, Dan B and Martin-Brualla, Ricardo and Seitz, Steven M",2021,,,,arXiv preprint arXiv:2106.13228
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,wang2023masked,\cite{wang2023masked},"Masked Space-Time Hash Encoding for Efficient Dynamic Scene
  Reconstruction",http://arxiv.org/abs/2310.17527v1,"In this paper, we propose the Masked Space-Time Hash encoding (MSTH), a novel
method for efficiently reconstructing dynamic 3D scenes from multi-view or
monocular videos. Based on the observation that dynamic scenes often contain
substantial static areas that result in redundancy in storage and computations,
MSTH represents a dynamic scene as a weighted combination of a 3D hash encoding
and a 4D hash encoding. The weights for the two components are represented by a
learnable mask which is guided by an uncertainty-based objective to reflect the
spatial and temporal importance of each 3D position. With this design, our
method can reduce the hash collision rate by avoiding redundant queries and
modifications on static areas, making it feasible to represent a large number
of space-time voxels by hash tables with small size.Besides, without the
requirements to fit the large numbers of temporally redundant features
independently, our method is easier to optimize and converge rapidly with only
twenty minutes of training for a 300-frame dynamic scene.As a result, MSTH
obtains consistently better results than previous methods with only 20 minutes
of training time and 130 MB of memory storage. Code is available at
https://github.com/masked-spacetime-hashing/msth","Wang, Feng and Chen, Zilong and Wang, Guokang and Song, Yafei and Liu, Huaping",2023,,,,Advances in neural information processing systems
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,fang2022fast,\cite{fang2022fast},Fast Dynamic Radiance Fields with Time-Aware Neural Voxels,http://arxiv.org/abs/2205.15285v2,"Neural radiance fields (NeRF) have shown great success in modeling 3D scenes
and synthesizing novel-view images. However, most previous NeRF methods take
much time to optimize one single scene. Explicit data structures, e.g. voxel
features, show great potential to accelerate the training process. However,
voxel features face two big challenges to be applied to dynamic scenes, i.e.
modeling temporal information and capturing different scales of point motions.
We propose a radiance field framework by representing scenes with time-aware
voxel features, named as TiNeuVox. A tiny coordinate deformation network is
introduced to model coarse motion trajectories and temporal information is
further enhanced in the radiance network. A multi-distance interpolation method
is proposed and applied on voxel features to model both small and large
motions. Our framework significantly accelerates the optimization of dynamic
radiance fields while maintaining high rendering quality. Empirical evaluation
is performed on both synthetic and real scenes. Our TiNeuVox completes training
with only 8 minutes and 8-MB storage cost while showing similar or even better
rendering performance than previous dynamic NeRF methods.","Fang, Jiemin and Yi, Taoran and Wang, Xinggang and Xie, Lingxi and Zhang, Xiaopeng and Liu, Wenyu and Nie{\ss}ner, Matthias and Tian, Qi",2022,,,,
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,liu2023robust,\cite{liu2023robust},Robust Dynamic Radiance Fields,http://arxiv.org/abs/2301.02239v2,"Dynamic radiance field reconstruction methods aim to model the time-varying
structure and appearance of a dynamic scene. Existing methods, however, assume
that accurate camera poses can be reliably estimated by Structure from Motion
(SfM) algorithms. These methods, thus, are unreliable as SfM algorithms often
fail or produce erroneous poses on challenging videos with highly dynamic
objects, poorly textured surfaces, and rotating camera motion. We address this
robustness issue by jointly estimating the static and dynamic radiance fields
along with the camera parameters (poses and focal length). We demonstrate the
robustness of our approach via extensive quantitative and qualitative
experiments. Our results show favorable performance over the state-of-the-art
dynamic view synthesis methods.","Liu, Yu-Lun and Gao, Chen and Meuleman, Andreas and Tseng, Hung-Yu and Saraf, Ayush and Kim, Changil and Chuang, Yung-Yu and Kopf, Johannes and Huang, Jia-Bin",2023,,,,
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,guo2023forward,\cite{guo2023forward},Forward Flow for Novel View Synthesis of Dynamic Scenes,http://arxiv.org/abs/2309.17390v1,"This paper proposes a neural radiance field (NeRF) approach for novel view
synthesis of dynamic scenes using forward warping. Existing methods often adopt
a static NeRF to represent the canonical space, and render dynamic images at
other time steps by mapping the sampled 3D points back to the canonical space
with the learned backward flow field. However, this backward flow field is
non-smooth and discontinuous, which is difficult to be fitted by commonly used
smooth motion models. To address this problem, we propose to estimate the
forward flow field and directly warp the canonical radiance field to other time
steps. Such forward flow field is smooth and continuous within the object
region, which benefits the motion model learning. To achieve this goal, we
represent the canonical radiance field with voxel grids to enable efficient
forward warping, and propose a differentiable warping process, including an
average splatting operation and an inpaint network, to resolve the many-to-one
and one-to-many mapping issues. Thorough experiments show that our method
outperforms existing methods in both novel view rendering and motion modeling,
demonstrating the effectiveness of our forward flow motion modeling. Project
page: https://npucvr.github.io/ForwardFlowDNeRF","Guo, Xiang and Sun, Jiadai and Dai, Yuchao and Chen, Guanying and Ye, Xiaoqing and Tan, Xiao and Ding, Errui and Zhang, Yumeng and Wang, Jingdong",2023,,,,
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,shao2023tensor4d,\cite{shao2023tensor4d},"Tensor4D : Efficient Neural 4D Decomposition for High-fidelity Dynamic
  Reconstruction and Rendering",http://arxiv.org/abs/2211.11610v2,"We present Tensor4D, an efficient yet effective approach to dynamic scene
modeling. The key of our solution is an efficient 4D tensor decomposition
method so that the dynamic scene can be directly represented as a 4D
spatio-temporal tensor. To tackle the accompanying memory issue, we decompose
the 4D tensor hierarchically by projecting it first into three time-aware
volumes and then nine compact feature planes. In this way, spatial information
over time can be simultaneously captured in a compact and memory-efficient
manner. When applying Tensor4D for dynamic scene reconstruction and rendering,
we further factorize the 4D fields to different scales in the sense that
structural motions and dynamic detailed changes can be learned from coarse to
fine. The effectiveness of our method is validated on both synthetic and
real-world scenes. Extensive experiments show that our method is able to
achieve high-quality dynamic reconstruction and rendering from sparse-view
camera rigs or even a monocular camera. The code and dataset will be released
at https://liuyebin.com/tensor4d/tensor4d.html.","Shao, Ruizhi and Zheng, Zerong and Tu, Hanzhang and Liu, Boning and Zhang, Hongwen and Liu, Yebin",2023,,,,
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,duan20244d,\cite{duan20244d},"4D-Rotor Gaussian Splatting: Towards Efficient Novel View Synthesis for
  Dynamic Scenes",http://arxiv.org/abs/2402.03307v3,"We consider the problem of novel-view synthesis (NVS) for dynamic scenes.
Recent neural approaches have accomplished exceptional NVS results for static
3D scenes, but extensions to 4D time-varying scenes remain non-trivial. Prior
efforts often encode dynamics by learning a canonical space plus implicit or
explicit deformation fields, which struggle in challenging scenarios like
sudden movements or generating high-fidelity renderings. In this paper, we
introduce 4D Gaussian Splatting (4DRotorGS), a novel method that represents
dynamic scenes with anisotropic 4D XYZT Gaussians, inspired by the success of
3D Gaussian Splatting in static scenes. We model dynamics at each timestamp by
temporally slicing the 4D Gaussians, which naturally compose dynamic 3D
Gaussians and can be seamlessly projected into images. As an explicit
spatial-temporal representation, 4DRotorGS demonstrates powerful capabilities
for modeling complicated dynamics and fine details--especially for scenes with
abrupt motions. We further implement our temporal slicing and splatting
techniques in a highly optimized CUDA acceleration framework, achieving
real-time inference rendering speeds of up to 277 FPS on an RTX 3090 GPU and
583 FPS on an RTX 4090 GPU. Rigorous evaluations on scenes with diverse motions
showcase the superior efficiency and effectiveness of 4DRotorGS, which
consistently outperforms existing methods both quantitatively and
qualitatively.","Duan, Yuanxing and Wei, Fangyin and Dai, Qiyu and He, Yuhang and Chen, Wenzheng and Chen, Baoquan",2024,,,,
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,li2024spacetime,\cite{li2024spacetime},"Spacetime Gaussian Feature Splatting for Real-Time Dynamic View
  Synthesis",http://arxiv.org/abs/2312.16812v2,"Novel view synthesis of dynamic scenes has been an intriguing yet challenging
problem. Despite recent advancements, simultaneously achieving high-resolution
photorealistic results, real-time rendering, and compact storage remains a
formidable task. To address these challenges, we propose Spacetime Gaussian
Feature Splatting as a novel dynamic scene representation, composed of three
pivotal components. First, we formulate expressive Spacetime Gaussians by
enhancing 3D Gaussians with temporal opacity and parametric motion/rotation.
This enables Spacetime Gaussians to capture static, dynamic, as well as
transient content within a scene. Second, we introduce splatted feature
rendering, which replaces spherical harmonics with neural features. These
features facilitate the modeling of view- and time-dependent appearance while
maintaining small size. Third, we leverage the guidance of training error and
coarse depth to sample new Gaussians in areas that are challenging to converge
with existing pipelines. Experiments on several established real-world datasets
demonstrate that our method achieves state-of-the-art rendering quality and
speed, while retaining compact storage. At 8K resolution, our lite-version
model can render at 60 FPS on an Nvidia RTX 4090 GPU. Our code is available at
https://github.com/oppo-us-research/SpacetimeGaussians.","Li, Zhan and Chen, Zhang and Li, Zhong and Xu, Yi",2024,,,,
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,yang2023real,\cite{yang2023real},"Real-time Photorealistic Dynamic Scene Representation and Rendering with
  4D Gaussian Splatting",http://arxiv.org/abs/2310.10642v3,"Reconstructing dynamic 3D scenes from 2D images and generating diverse views
over time is challenging due to scene complexity and temporal dynamics. Despite
advancements in neural implicit models, limitations persist: (i) Inadequate
Scene Structure: Existing methods struggle to reveal the spatial and temporal
structure of dynamic scenes from directly learning the complex 6D plenoptic
function. (ii) Scaling Deformation Modeling: Explicitly modeling scene element
deformation becomes impractical for complex dynamics. To address these issues,
we consider the spacetime as an entirety and propose to approximate the
underlying spatio-temporal 4D volume of a dynamic scene by optimizing a
collection of 4D primitives, with explicit geometry and appearance modeling.
Learning to optimize the 4D primitives enables us to synthesize novel views at
any desired time with our tailored rendering routine. Our model is conceptually
simple, consisting of a 4D Gaussian parameterized by anisotropic ellipses that
can rotate arbitrarily in space and time, as well as view-dependent and
time-evolved appearance represented by the coefficient of 4D spherindrical
harmonics. This approach offers simplicity, flexibility for variable-length
video and end-to-end training, and efficient real-time rendering, making it
suitable for capturing complex dynamic scene motions. Experiments across
various benchmarks, including monocular and multi-view scenarios, demonstrate
our 4DGS model's superior visual quality and efficiency.","Yang, Zeyu and Yang, Hongye and Pan, Zijie and Zhang, Li",2023,,,,arXiv preprint arXiv:2310.10642
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,wu20244d,\cite{wu20244d},3D Gaussian Splatting for Real-Time Radiance Field Rendering,http://arxiv.org/abs/2308.04079v1,"Radiance Field methods have recently revolutionized novel-view synthesis of
scenes captured with multiple photos or videos. However, achieving high visual
quality still requires neural networks that are costly to train and render,
while recent faster methods inevitably trade off speed for quality. For
unbounded and complete scenes (rather than isolated objects) and 1080p
resolution rendering, no current method can achieve real-time display rates. We
introduce three key elements that allow us to achieve state-of-the-art visual
quality while maintaining competitive training times and importantly allow
high-quality real-time (>= 30 fps) novel-view synthesis at 1080p resolution.
First, starting from sparse points produced during camera calibration, we
represent the scene with 3D Gaussians that preserve desirable properties of
continuous volumetric radiance fields for scene optimization while avoiding
unnecessary computation in empty space; Second, we perform interleaved
optimization/density control of the 3D Gaussians, notably optimizing
anisotropic covariance to achieve an accurate representation of the scene;
Third, we develop a fast visibility-aware rendering algorithm that supports
anisotropic splatting and both accelerates training and allows realtime
rendering. We demonstrate state-of-the-art visual quality and real-time
rendering on several established datasets.","Wu, Guanjun and Yi, Taoran and Fang, Jiemin and Xie, Lingxi and Zhang, Xiaopeng and Wei, Wei and Liu, Wenyu and Tian, Qi and Wang, Xinggang",2024,,,,
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,yang2024deformable,\cite{yang2024deformable},"Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene
  Reconstruction",http://arxiv.org/abs/2309.13101v2,"Implicit neural representation has paved the way for new approaches to
dynamic scene reconstruction and rendering. Nonetheless, cutting-edge dynamic
neural rendering methods rely heavily on these implicit representations, which
frequently struggle to capture the intricate details of objects in the scene.
Furthermore, implicit methods have difficulty achieving real-time rendering in
general dynamic scenes, limiting their use in a variety of tasks. To address
the issues, we propose a deformable 3D Gaussians Splatting method that
reconstructs scenes using 3D Gaussians and learns them in canonical space with
a deformation field to model monocular dynamic scenes. We also introduce an
annealing smoothing training mechanism with no extra overhead, which can
mitigate the impact of inaccurate poses on the smoothness of time interpolation
tasks in real-world datasets. Through a differential Gaussian rasterizer, the
deformable 3D Gaussians not only achieve higher rendering quality but also
real-time rendering speed. Experiments show that our method outperforms
existing methods significantly in terms of both rendering quality and speed,
making it well-suited for tasks such as novel-view synthesis, time
interpolation, and real-time rendering.","Yang, Ziyi and Gao, Xinyu and Zhou, Wen and Jiao, Shaohui and Zhang, Yuqing and Jin, Xiaogang",2024,,,,
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,zhang2017real,\cite{zhang2017real},Real-Time User-Guided Image Colorization with Learned Deep Priors,http://arxiv.org/abs/1705.02999v1,"We propose a deep learning approach for user-guided image colorization. The
system directly maps a grayscale image, along with sparse, local user ""hints""
to an output colorization with a Convolutional Neural Network (CNN). Rather
than using hand-defined rules, the network propagates user edits by fusing
low-level cues along with high-level semantic information, learned from
large-scale data. We train on a million images, with simulated user inputs. To
guide the user towards efficient input selection, the system recommends likely
colors based on the input image and current user inputs. The colorization is
performed in a single feed-forward pass, enabling real-time use. Even with
randomly simulated user inputs, we show that the proposed system helps novice
users quickly create realistic colorizations, and offers large improvements in
colorization quality with just a minute of use. In addition, we demonstrate
that the framework can incorporate other user ""hints"" to the desired
colorization, showing an application to color histogram transfer. Our code and
models are available at https://richzhang.github.io/ideepcolor.","Zhang, Richard and Zhu, Jun-Yan and Isola, Phillip and Geng, Xinyang and Lin, Angela S and Yu, Tianhe and Efros, Alexei A",2017,,,,arXiv preprint arXiv:1705.02999
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,wu2021towards,\cite{wu2021towards},Towards Vivid and Diverse Image Colorization with Generative Color Prior,http://arxiv.org/abs/2108.08826v2,"Colorization has attracted increasing interest in recent years. Classic
reference-based methods usually rely on external color images for plausible
results. A large image database or online search engine is inevitably required
for retrieving such exemplars. Recent deep-learning-based methods could
automatically colorize images at a low cost. However, unsatisfactory artifacts
and incoherent colors are always accompanied. In this work, we propose
GCP-Colorization that leverages the rich and diverse color priors encapsulated
in a pretrained Generative Adversarial Networks (GAN) for automatic
colorization. Specifically, we first ""retrieve"" matched features (similar to
exemplars) via a GAN encoder and then incorporate these features into the
colorization process with feature modulations. Thanks to the powerful
generative color prior (GCP) and delicate designs, our GCP-Colorization could
produce vivid colors with a single forward pass. Moreover, it is highly
convenient to obtain diverse results by modifying GAN latent codes.
GCP-Colorization also inherits the merit of interpretable controls of GANs and
could attain controllable and smooth transitions by walking through GAN latent
space. Extensive experiments and user studies demonstrate that GCP-Colorization
achieves superior performance than previous works. Codes are available at
https://github.com/ToTheBeginning/GCP-Colorization.","Wu, Yanze and Wang, Xintao and Li, Yu and Zhang, Honglun and Zhao, Xun and Shan, Ying",2021,,,,
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,kim2022bigcolor,\cite{kim2022bigcolor},BigColor: Colorization using a Generative Color Prior for Natural Images,http://arxiv.org/abs/2207.09685v1,"For realistic and vivid colorization, generative priors have recently been
exploited. However, such generative priors often fail for in-the-wild complex
images due to their limited representation space. In this paper, we propose
BigColor, a novel colorization approach that provides vivid colorization for
diverse in-the-wild images with complex structures. While previous generative
priors are trained to synthesize both image structures and colors, we learn a
generative color prior to focus on color synthesis given the spatial structure
of an image. In this way, we reduce the burden of synthesizing image structures
from the generative prior and expand its representation space to cover diverse
images. To this end, we propose a BigGAN-inspired encoder-generator network
that uses a spatial feature map instead of a spatially-flattened BigGAN latent
code, resulting in an enlarged representation space. Our method enables robust
colorization for diverse inputs in a single forward pass, supports arbitrary
input resolutions, and provides multi-modal colorization results. We
demonstrate that BigColor significantly outperforms existing methods especially
on in-the-wild images with complex structures.","Kim, Geonung and Kang, Kyoungkook and Kim, Seongtae and Lee, Hwayoon and Kim, Sehoon and Kim, Jonghyun and Baek, Seung-Hwan and Cho, Sunghyun",2022,,,,
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,kang2023ddcolor,\cite{kang2023ddcolor},DDColor: Towards Photo-Realistic Image Colorization via Dual Decoders,http://arxiv.org/abs/2212.11613v5,"Image colorization is a challenging problem due to multi-modal uncertainty
and high ill-posedness. Directly training a deep neural network usually leads
to incorrect semantic colors and low color richness. While transformer-based
methods can deliver better results, they often rely on manually designed
priors, suffer from poor generalization ability, and introduce color bleeding
effects. To address these issues, we propose DDColor, an end-to-end method with
dual decoders for image colorization. Our approach includes a pixel decoder and
a query-based color decoder. The former restores the spatial resolution of the
image, while the latter utilizes rich visual features to refine color queries,
thus avoiding hand-crafted priors. Our two decoders work together to establish
correlations between color and multi-scale semantic representations via
cross-attention, significantly alleviating the color bleeding effect.
Additionally, a simple yet effective colorfulness loss is introduced to enhance
the color richness. Extensive experiments demonstrate that DDColor achieves
superior performance to existing state-of-the-art works both quantitatively and
qualitatively. The codes and models are publicly available at
https://github.com/piddnad/DDColor.","Kang, Xiaoyang and Yang, Tao and Ouyang, Wenqi and Ren, Peiran and Li, Lingzhi and Xie, Xuansong",2023,,,,
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,ji2022colorformer,\cite{ji2022colorformer},ColorFormer: Image colorization via color memory assisted hybrid-attention transformer,,,"Ji, Xiaozhong and Jiang, Boyuan and Luo, Donghao and Tao, Guangpin and Chu, Wenqing and Xie, Zhifeng and Wang, Chengjie and Tai, Ying",2022,,,,
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,he2018deep,\cite{he2018deep},Deep Exemplar-based Colorization,http://arxiv.org/abs/1807.06587v2,"We propose the first deep learning approach for exemplar-based local
colorization. Given a reference color image, our convolutional neural network
directly maps a grayscale image to an output colorized image. Rather than using
hand-crafted rules as in traditional exemplar-based methods, our end-to-end
colorization network learns how to select, propagate, and predict colors from
the large-scale data. The approach performs robustly and generalizes well even
when using reference images that are unrelated to the input grayscale image.
More importantly, as opposed to other learning-based colorization methods, our
network allows the user to achieve customizable results by simply feeding
different references. In order to further reduce manual effort in selecting the
references, the system automatically recommends references with our proposed
image retrieval algorithm, which considers both semantic and luminance
information. The colorization can be performed fully automatically by simply
picking the top reference suggestion. Our approach is validated through a user
study and favorable quantitative comparisons to the-state-of-the-art methods.
Furthermore, our approach can be naturally extended to video colorization. Our
code and models will be freely available for public use.","He, Mingming and Chen, Dongdong and Liao, Jing and Sander, Pedro V and Yuan, Lu",2018,,,,ACM Transactions on Graphics (TOG)
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,huang2022unicolor,\cite{huang2022unicolor},"UniColor: A Unified Framework for Multi-Modal Colorization with
  Transformer",http://arxiv.org/abs/2209.11223v1,"We propose the first unified framework UniColor to support colorization in
multiple modalities, including both unconditional and conditional ones, such as
stroke, exemplar, text, and even a mix of them. Rather than learning a separate
model for each type of condition, we introduce a two-stage colorization
framework for incorporating various conditions into a single model. In the
first stage, multi-modal conditions are converted into a common representation
of hint points. Particularly, we propose a novel CLIP-based method to convert
the text to hint points. In the second stage, we propose a Transformer-based
network composed of Chroma-VQGAN and Hybrid-Transformer to generate diverse and
high-quality colorization results conditioned on hint points. Both qualitative
and quantitative comparisons demonstrate that our method outperforms
state-of-the-art methods in every control modality and further enables
multi-modal colorization that was not feasible before. Moreover, we design an
interactive interface showing the effectiveness of our unified framework in
practical usage, including automatic colorization, hybrid-control colorization,
local recolorization, and iterative color editing. Our code and models are
available at https://luckyhzt.github.io/unicolor.","Huang, Zhitong and Zhao, Nanxuan and Liao, Jing",2022,,,,ACM Transactions on Graphics (TOG)
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,zhao2021color2embed,\cite{zhao2021color2embed},"Color2Embed: Fast Exemplar-Based Image Colorization using Color
  Embeddings",http://arxiv.org/abs/2106.08017v3,"In this paper, we present a fast exemplar-based image colorization approach
using color embeddings named Color2Embed. Generally, due to the difficulty of
obtaining input and ground truth image pairs, it is hard to train a
exemplar-based colorization model with unsupervised and unpaired training
manner. Current algorithms usually strive to achieve two procedures: i)
retrieving a large number of reference images with high similarity for
preparing training dataset, which is inevitably time-consuming and tedious; ii)
designing complicated modules to transfer the colors of the reference image to
the target image, by calculating and leveraging the deep semantic
correspondence between them (e.g., non-local operation), which is
computationally expensive during testing. Contrary to the previous methods, we
adopt a self-augmented self-reference learning scheme, where the reference
image is generated by graphical transformations from the original colorful one
whereby the training can be formulated in a paired manner. Second, in order to
reduce the process time, our method explicitly extracts the color embeddings
and exploits a progressive style feature Transformation network, which injects
the color embeddings into the reconstruction of the final image. Such design is
much more lightweight and intelligible, achieving appealing performance with
fast processing speed.","Zhao, Hengyuan and Wu, Wenhao and Liu, Yihao and He, Dongliang",2021,,,,arXiv preprint arXiv:2106.08017
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,weng2023cad,\cite{weng2023cad},L-CAD: Language-based colorization with any-level descriptions using diffusion priors,,,"Weng, Shuchen and Zhang, Peixuan and Li, Yu and Li, Si and Shi, Boxin and others",2023,,,,Advances in Neural Information Processing Systems
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,zabari2023diffusing,\cite{zabari2023diffusing},Diffusing Colors: Image Colorization with Text Guided Diffusion,http://arxiv.org/abs/2312.04145v1,"The colorization of grayscale images is a complex and subjective task with
significant challenges. Despite recent progress in employing large-scale
datasets with deep neural networks, difficulties with controllability and
visual quality persist. To tackle these issues, we present a novel image
colorization framework that utilizes image diffusion techniques with granular
text prompts. This integration not only produces colorization outputs that are
semantically appropriate but also greatly improves the level of control users
have over the colorization process. Our method provides a balance between
automation and control, outperforming existing techniques in terms of visual
quality and semantic coherence. We leverage a pretrained generative Diffusion
Model, and show that we can finetune it for the colorization task without
losing its generative power or attention to text prompts. Moreover, we present
a novel CLIP-based ranking model that evaluates color vividness, enabling
automatic selection of the most suitable level of vividness based on the
specific scene semantics. Our approach holds potential particularly for color
enhancement and historical image colorization.","Zabari, Nir and Azulay, Aharon and Gorkor, Alexey and Halperin, Tavi and Fried, Ohad",2023,,,,
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,weng2022code,\cite{weng2022code},L-CoDe: Language-based colorization using color-object decoupled conditions,,,"Weng, Shuchen and Wu, Hao and Chang, Zheng and Tang, Jiajun and Li, Si and Shi, Boxin",2022,,,,
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,chang2023coins,\cite{chang2023coins},L-CoIns: Language-based colorization with instance awareness,,,"Chang, Zheng and Weng, Shuchen and Zhang, Peixuan and Li, Yu and Li, Si and Shi, Boxin",2023,,,,
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,liang2024control,\cite{liang2024control},Control Color: Multimodal Diffusion-based Interactive Image Colorization,http://arxiv.org/abs/2402.10855v1,"Despite the existence of numerous colorization methods, several limitations
still exist, such as lack of user interaction, inflexibility in local
colorization, unnatural color rendering, insufficient color variation, and
color overflow. To solve these issues, we introduce Control Color (CtrlColor),
a multi-modal colorization method that leverages the pre-trained Stable
Diffusion (SD) model, offering promising capabilities in highly controllable
interactive image colorization. While several diffusion-based methods have been
proposed, supporting colorization in multiple modalities remains non-trivial.
In this study, we aim to tackle both unconditional and conditional image
colorization (text prompts, strokes, exemplars) and address color overflow and
incorrect color within a unified framework. Specifically, we present an
effective way to encode user strokes to enable precise local color manipulation
and employ a practical way to constrain the color distribution similar to
exemplars. Apart from accepting text prompts as conditions, these designs add
versatility to our approach. We also introduce a novel module based on
self-attention and a content-guided deformable autoencoder to address the
long-standing issues of color overflow and inaccurate coloring. Extensive
comparisons show that our model outperforms state-of-the-art image colorization
methods both qualitatively and quantitatively.","Liang, Zhexin and Li, Zhaochen and Zhou, Shangchen and Li, Chongyi and Loy, Chen Change",2024,,,,arXiv preprint arXiv:2402.10855
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,yang2024bistnet,\cite{yang2024bistnet},"BiSTNet: Semantic Image Prior Guided Bidirectional Temporal Feature
  Fusion for Deep Exemplar-based Video Colorization",http://arxiv.org/abs/2212.02268v1,"How to effectively explore the colors of reference exemplars and propagate
them to colorize each frame is vital for exemplar-based video colorization. In
this paper, we present an effective BiSTNet to explore colors of reference
exemplars and utilize them to help video colorization by a bidirectional
temporal feature fusion with the guidance of semantic image prior. We first
establish the semantic correspondence between each frame and the reference
exemplars in deep feature space to explore color information from reference
exemplars. Then, to better propagate the colors of reference exemplars into
each frame and avoid the inaccurate matches colors from exemplars we develop a
simple yet effective bidirectional temporal feature fusion module to better
colorize each frame. We note that there usually exist color-bleeding artifacts
around the boundaries of the important objects in videos. To overcome this
problem, we further develop a mixed expert block to extract semantic
information for modeling the object boundaries of frames so that the semantic
image prior can better guide the colorization process for better performance.
In addition, we develop a multi-scale recurrent block to progressively colorize
frames in a coarse-to-fine manner. Extensive experimental results demonstrate
that the proposed BiSTNet performs favorably against state-of-the-art methods
on the benchmark datasets. Our code will be made available at
\url{https://yyang181.github.io/BiSTNet/}","Yang, Yixin and Pan, Jinshan and Peng, Zhongzheng and Du, Xiaoyu and Tao, Zhulin and Tang, Jinhui",2024,,,,IEEE Transactions on Pattern Analysis and Machine Intelligence
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,wang2025consistent,\cite{wang2025consistent},Consistent Video Colorization via Palette Guidance,http://arxiv.org/abs/2501.19331v1,"Colorization is a traditional computer vision task and it plays an important
role in many time-consuming tasks, such as old film restoration. Existing
methods suffer from unsaturated color and temporally inconsistency. In this
paper, we propose a novel pipeline to overcome the challenges. We regard the
colorization task as a generative task and introduce Stable Video Diffusion
(SVD) as our base model. We design a palette-based color guider to assist the
model in generating vivid and consistent colors. The color context introduced
by the palette not only provides guidance for color generation, but also
enhances the stability of the generated colors through a unified color context
across multiple sequences. Experiments demonstrate that the proposed method can
provide vivid and stable colors for videos, surpassing previous methods.","Wang, Han and Zhang, Yuang and Zhang, Yuhong and Lu, Lingxiao and Song, Li",2025,,,,arXiv preprint arXiv:2501.19331
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,li2024towards,\cite{li2024towards},Towards photorealistic video colorization via gated color-guided image diffusion models,,,"Li, Jiaxing and Zhao, Hongbo and Wang, Yijun and Lin, Jianxin",2024,,,,
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,yang2024colormnet,\cite{yang2024colormnet},"ColorMNet: A Memory-based Deep Spatial-Temporal Feature Propagation
  Network for Video Colorization",http://arxiv.org/abs/2404.06251v1,"How to effectively explore spatial-temporal features is important for video
colorization. Instead of stacking multiple frames along the temporal dimension
or recurrently propagating estimated features that will accumulate errors or
cannot explore information from far-apart frames, we develop a memory-based
feature propagation module that can establish reliable connections with
features from far-apart frames and alleviate the influence of inaccurately
estimated features. To extract better features from each frame for the
above-mentioned feature propagation, we explore the features from
large-pretrained visual models to guide the feature estimation of each frame so
that the estimated features can model complex scenarios. In addition, we note
that adjacent frames usually contain similar contents. To explore this property
for better spatial and temporal feature utilization, we develop a local
attention module to aggregate the features from adjacent frames in a
spatial-temporal neighborhood. We formulate our memory-based feature
propagation module, large-pretrained visual model guided feature estimation
module, and local attention module into an end-to-end trainable network (named
ColorMNet) and show that it performs favorably against state-of-the-art methods
on both the benchmark datasets and real-world scenarios. The source code and
pre-trained models will be available at
\url{https://github.com/yyang181/colormnet}.","Yang, Yixin and Dong, Jiangxin and Tang, Jinhui and Pan, Jinshan",2024,,,,
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,liao2024gbc,\cite{liao2024gbc},GBC: Gaussian-Based Colorization and Super-Resolution for 3D Reconstruction,,,"Liao, Jiecheng and He, Shi and Yuan, Yichen and Zhang, Hui",2024,,,,
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,dhiman2023corf,\cite{dhiman2023corf},Corf: Colorizing radiance fields using knowledge distillation,,,"Dhiman, Ankit and Srinath, R and Sarkar, Srinjay and Boregowda, Lokesh R and Babu, R Venkatesh",2023,,,,arXiv preprint arXiv:2309.07668
Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,http://arxiv.org/abs/2510.10152v1,cheng2024colorizing,\cite{cheng2024colorizing},Colorizing Monochromatic Radiance Fields,http://arxiv.org/abs/2402.12184v1,"Though Neural Radiance Fields (NeRF) can produce colorful 3D representations
of the world by using a set of 2D images, such ability becomes non-existent
when only monochromatic images are provided. Since color is necessary in
representing the world, reproducing color from monochromatic radiance fields
becomes crucial. To achieve this goal, instead of manipulating the
monochromatic radiance fields directly, we consider it as a
representation-prediction task in the Lab color space. By first constructing
the luminance and density representation using monochromatic images, our
prediction stage can recreate color representation on the basis of an image
colorization module. We then reproduce a colorful implicit model through the
representation of luminance, density, and color. Extensive experiments have
been conducted to validate the effectiveness of our approaches. Our project
page: https://liquidammonia.github.io/color-nerf.","Cheng, Yean and Wan, Renjie and Weng, Shuchen and Zhu, Chengxuan and Chang, Yakun and Shi, Boxin",2024,,,,
