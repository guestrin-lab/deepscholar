parent_paper_title,parent_arxiv_link,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal
No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials,http://arxiv.org/abs/2509.21816v1,li2023sheetcopilot,\cite{li2023sheetcopilot},"SheetCopilot: Bringing Software Productivity to the Next Level through
  Large Language Models",http://arxiv.org/abs/2305.19308v2,"Computer end users have spent billions of hours completing daily tasks like
tabular data processing and project timeline scheduling. Most of these tasks
are repetitive and error-prone, yet most end users lack the skill to automate
these burdensome works. With the advent of large language models (LLMs),
directing software with natural language user requests become a reachable goal.
In this work, we propose a SheetCopilot agent that takes natural language task
and control spreadsheet to fulfill the requirements. We propose a set of atomic
actions as an abstraction of spreadsheet software functionalities. We further
design a state machine-based task planning framework for LLMs to robustly
interact with spreadsheets. We curate a representative dataset containing 221
spreadsheet control tasks and establish a fully automated evaluation pipeline
for rigorously benchmarking the ability of LLMs in software control tasks. Our
SheetCopilot correctly completes 44.3\% of tasks for a single generation,
outperforming the strong code generation baseline by a wide margin. Our project
page:https://sheetcopilot.github.io/.","Li, Hongxin and Su, Jingran and Chen, Yuntao and Li, Qing and ZHANG, ZHAO-XIANG",2023,,,,Advances in Neural Information Processing Systems
No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials,http://arxiv.org/abs/2509.21816v1,chen2025sheetagent,\cite{chen2025sheetagent},"SheetAgent: Towards A Generalist Agent for Spreadsheet Reasoning and
  Manipulation via Large Language Models",http://arxiv.org/abs/2403.03636v3,"Spreadsheets are ubiquitous across the World Wide Web, playing a critical
role in enhancing work efficiency across various domains. Large language model
(LLM) has been recently attempted for automatic spreadsheet manipulation but
has not yet been investigated in complicated and realistic tasks where
reasoning challenges exist (e.g., long horizon manipulation with multi-step
reasoning and ambiguous requirements). To bridge the gap with the real-world
requirements, we introduce SheetRM, a benchmark featuring long-horizon and
multi-category tasks with reasoning-dependent manipulation caused by real-life
challenges. To mitigate the above challenges, we further propose SheetAgent, a
novel autonomous agent that utilizes the power of LLMs. SheetAgent consists of
three collaborative modules: Planner, Informer, and Retriever, achieving both
advanced reasoning and accurate manipulation over spreadsheets without human
interaction through iterative task reasoning and reflection. Extensive
experiments demonstrate that SheetAgent delivers 20--40\% pass rate
improvements on multiple benchmarks over baselines, achieving enhanced
precision in spreadsheet manipulation and demonstrating superior table
reasoning abilities. More details and visualizations are available at the
project website: https://sheetagent.github.io/. The datasets and source code
are available at https://anonymous.4open.science/r/SheetAgent.","Chen, Yibin and Yuan, Yifu and Zhang, Zeyu and Zheng, Yan and Liu, Jinyi and Ni, Fei and Hao, Jianye and Mao, Hangyu and Zhang, Fuzheng",2025,,,,
No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials,http://arxiv.org/abs/2509.21816v1,zhu2025sheetmind,\cite{zhu2025sheetmind},"SheetMind: An End-to-End LLM-Powered Multi-Agent Framework for
  Spreadsheet Automation",http://arxiv.org/abs/2506.12339v1,"We present SheetMind, a modular multi-agent framework powered by large
language models (LLMs) for spreadsheet automation via natural language
instructions. The system comprises three specialized agents: a Manager Agent
that decomposes complex user instructions into subtasks; an Action Agent that
translates these into structured commands using a Backus Naur Form (BNF)
grammar; and a Reflection Agent that validates alignment between generated
actions and the user's original intent. Integrated into Google Sheets via a
Workspace extension, SheetMind supports real-time interaction without requiring
scripting or formula knowledge. Experiments on benchmark datasets demonstrate
an 80 percent success rate on single step tasks and approximately 70 percent on
multi step instructions, outperforming ablated and baseline variants. Our
results highlight the effectiveness of multi agent decomposition and grammar
based execution for bridging natural language and spreadsheet functionalities.","Zhu, Ruiyan and Cheng, Xi and Liu, Ke and Zhu, Brian and Jin, Daniel and Parihar, Neeraj and Xu, Zhoutian and Gao, Oliver",2025,,,,arXiv preprint arXiv:2506.12339
No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials,http://arxiv.org/abs/2509.21816v1,zhang2024ufo,\cite{zhang2024ufo},UFO: A UI-Focused Agent for Windows OS Interaction,http://arxiv.org/abs/2402.07939v5,"We introduce UFO, an innovative UI-Focused agent to fulfill user requests
tailored to applications on Windows OS, harnessing the capabilities of
GPT-Vision. UFO employs a dual-agent framework to meticulously observe and
analyze the graphical user interface (GUI) and control information of Windows
applications. This enables the agent to seamlessly navigate and operate within
individual applications and across them to fulfill user requests, even when
spanning multiple applications. The framework incorporates a control
interaction module, facilitating action grounding without human intervention
and enabling fully automated execution. Consequently, UFO transforms arduous
and time-consuming processes into simple tasks achievable solely through
natural language commands. We conducted testing of UFO across 9 popular Windows
applications, encompassing a variety of scenarios reflective of users' daily
usage. The results, derived from both quantitative metrics and real-case
studies, underscore the superior effectiveness of UFO in fulfilling user
requests. To the best of our knowledge, UFO stands as the first UI agent
specifically tailored for task completion within the Windows OS environment.
The open-source code for UFO is available on https://github.com/microsoft/UFO.","Zhang, Chaoyun and Li, Liqun and He, Shilin and Zhang, Xu and Qiao, Bo and Qin, Si and Ma, Minghua and Kang, Yu and Lin, Qingwei and Rajmohan, Saravan and others",2024,,,,arXiv preprint arXiv:2402.07939
No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials,http://arxiv.org/abs/2509.21816v1,bonatti2024windows,\cite{bonatti2024windows},Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale,http://arxiv.org/abs/2409.08264v2,"Large language models (LLMs) show remarkable potential to act as computer
agents, enhancing human productivity and software accessibility in multi-modal
tasks that require planning and reasoning. However, measuring agent performance
in realistic environments remains a challenge since: (i) most benchmarks are
limited to specific modalities or domains (e.g. text-only, web navigation, Q&A,
coding) and (ii) full benchmark evaluations are slow (on order of magnitude of
days) given the multi-step sequential nature of tasks. To address these
challenges, we introduce the Windows Agent Arena: a reproducible, general
environment focusing exclusively on the Windows operating system (OS) where
agents can operate freely within a real Windows OS and use the same wide range
of applications, tools, and web browsers available to human users when solving
tasks. We adapt the OSWorld framework (Xie et al., 2024) to create 150+ diverse
Windows tasks across representative domains that require agent abilities in
planning, screen understanding, and tool usage. Our benchmark is scalable and
can be seamlessly parallelized in Azure for a full benchmark evaluation in as
little as 20 minutes. To demonstrate Windows Agent Arena's capabilities, we
also introduce a new multi-modal agent, Navi. Our agent achieves a success rate
of 19.5% in the Windows domain, compared to 74.5% performance of an unassisted
human. Navi also demonstrates strong performance on another popular web-based
benchmark, Mind2Web. We offer extensive quantitative and qualitative analysis
of Navi's performance, and provide insights into the opportunities for future
research in agent development and data generation using Windows Agent Arena.
  Webpage: https://microsoft.github.io/WindowsAgentArena
  Code: https://github.com/microsoft/WindowsAgentArena","Bonatti, Rogerio and Zhao, Dan and Bonacci, Francesco and Dupont, Dillon and Abdali, Sara and Li, Yinheng and Lu, Yadong and Wagle, Justin and Koishida, Kazuhito and Bucker, Arthur and others",2024,,,,arXiv preprint arXiv:2409.08264
No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials,http://arxiv.org/abs/2509.21816v1,lu2024omniparser,\cite{lu2024omniparser},OmniParser for Pure Vision Based GUI Agent,http://arxiv.org/abs/2408.00203v1,"The recent success of large vision language models shows great potential in
driving the agent system operating on user interfaces. However, we argue that
the power multimodal models like GPT-4V as a general agent on multiple
operating systems across different applications is largely underestimated due
to the lack of a robust screen parsing technique capable of: 1) reliably
identifying interactable icons within the user interface, and 2) understanding
the semantics of various elements in a screenshot and accurately associate the
intended action with the corresponding region on the screen. To fill these
gaps, we introduce \textsc{OmniParser}, a comprehensive method for parsing user
interface screenshots into structured elements, which significantly enhances
the ability of GPT-4V to generate actions that can be accurately grounded in
the corresponding regions of the interface. We first curated an interactable
icon detection dataset using popular webpages and an icon description dataset.
These datasets were utilized to fine-tune specialized models: a detection model
to parse interactable regions on the screen and a caption model to extract the
functional semantics of the detected elements. \textsc{OmniParser}
significantly improves GPT-4V's performance on ScreenSpot benchmark. And on
Mind2Web and AITW benchmark, \textsc{OmniParser} with screenshot only input
outperforms the GPT-4V baselines requiring additional information outside of
screenshot.","Lu, Yadong and Yang, Jianwei and Shen, Yelong and Awadallah, Ahmed",2024,,,,arXiv preprint arXiv:2408.00203
No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials,http://arxiv.org/abs/2509.21816v1,agashe2024agent,\cite{agashe2024agent},Agent S: An Open Agentic Framework that Uses Computers Like a Human,http://arxiv.org/abs/2410.08164v1,"We present Agent S, an open agentic framework that enables autonomous
interaction with computers through a Graphical User Interface (GUI), aimed at
transforming human-computer interaction by automating complex, multi-step
tasks. Agent S aims to address three key challenges in automating computer
tasks: acquiring domain-specific knowledge, planning over long task horizons,
and handling dynamic, non-uniform interfaces. To this end, Agent S introduces
experience-augmented hierarchical planning, which learns from external
knowledge search and internal experience retrieval at multiple levels,
facilitating efficient task planning and subtask execution. In addition, it
employs an Agent-Computer Interface (ACI) to better elicit the reasoning and
control capabilities of GUI agents based on Multimodal Large Language Models
(MLLMs). Evaluation on the OSWorld benchmark shows that Agent S outperforms the
baseline by 9.37% on success rate (an 83.6% relative improvement) and achieves
a new state-of-the-art. Comprehensive analysis highlights the effectiveness of
individual components and provides insights for future improvements.
Furthermore, Agent S demonstrates broad generalizability to different operating
systems on a newly-released WindowsAgentArena benchmark. Code available at
https://github.com/simular-ai/Agent-S.","Agashe, Saaket and Han, Jiuzhou and Gan, Shuyu and Yang, Jiachen and Li, Ang and Wang, Xin Eric",2024,,,,arXiv preprint arXiv:2410.08164
No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials,http://arxiv.org/abs/2509.21816v1,openai2025computer,\cite{openai2025computer},Computer-using Agent: Introducing a Universal Interface for AI to Interact with the Digital World,,,OpenAI,2025,,,,
No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials,http://arxiv.org/abs/2509.21816v1,chi2012mixt,\cite{chi2012mixt},MixT: automatic generation of step-by-step mixed media tutorials,,,"Chi, Pei-Yu and Ahn, Sally and Ren, Amanda and Dontcheva, Mira and Li, Wilmot and Hartmann, Bj{\""o}rn",2012,,,,
No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials,http://arxiv.org/abs/2509.21816v1,truong2021automatic,\cite{truong2021automatic},Automatic generation of two-level hierarchical tutorials from instructional makeup videos,,,"Truong, Anh and Chi, Peggy and Salesin, David and Essa, Irfan and Agrawala, Maneesh",2021,,,,
No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials,http://arxiv.org/abs/2509.21816v1,grabler2009generating,\cite{grabler2009generating},Generating photo manipulation tutorials by demonstration,,,"Grabler, Floraine and Agrawala, Maneesh and Li, Wilmot and Dontcheva, Mira and Igarashi, Takeo",2009,,,,
No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials,http://arxiv.org/abs/2509.21816v1,denning2011meshflow,\cite{denning2011meshflow},Meshflow: interactive visualization of mesh construction sequences,,,"Denning, Jonathan D and Kerr, William B and Pellacini, Fabio",2011,,,,
No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials,http://arxiv.org/abs/2509.21816v1,grossman2010chronicle,\cite{grossman2010chronicle},"Chronicle: capture, exploration, and playback of document workflow histories",,,"Grossman, Tovi and Matejka, Justin and Fitzmaurice, George",2010,,,,
No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials,http://arxiv.org/abs/2509.21816v1,zhong2021helpviz,\cite{zhong2021helpviz},"HelpViz: Automatic Generation of Contextual Visual MobileTutorials from
  Text-Based Instructions",http://arxiv.org/abs/2108.03356v1,"We present HelpViz, a tool for generating contextual visual mobile tutorials
from text-based instructions that are abundant on the web. HelpViz transforms
text instructions to graphical tutorials in batch, by extracting a sequence of
actions from each text instruction through an instruction parsing model, and
executing the extracted actions on a simulation infrastructure that manages an
array of Android emulators. The automatic execution of each instruction
produces a set of graphical and structural assets, including images, videos,
and metadata such as clicked elements for each step. HelpViz then synthesizes a
tutorial by combining parsed text instructions with the generated assets, and
contextualizes the tutorial to user interaction by tracking the user's progress
and highlighting the next step.
  Our experiments with HelpViz indicate that our pipeline improved tutorial
execution robustness and that participants preferred tutorials generated by
HelpViz over text-based instructions. HelpViz promises a cost-effective
approach for generating contextual visual tutorials for mobile interaction at
scale.","Zhong, Mingyuan and Li, Gang and Chi, Peggy and Li, Yang",2021,,,,
No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials,http://arxiv.org/abs/2509.21816v1,liu2024having,\cite{liu2024having},"Having Difficulty Understanding Manuals? Automatically Converting User
  Manuals into Instructional Videos",http://arxiv.org/abs/2311.11031v2,"While users tend to perceive instructional videos as an experience rather
than a lesson with a set of instructions, instructional videos are more
effective and appealing than textual user manuals and eliminate the ambiguity
in text-based descriptions. However, most software vendors only offer document
manuals that describe how to install and use their software, leading burden for
non-professionals to comprehend the instructions. In this paper, we present a
framework called M2V to generate instructional videos automatically based on
the provided instructions and images in user manuals. M2V is a two-step
framework. First, an action sequence is extracted from the given user manual
via natural language processing and computer vision techniques. Second, M2V
operates the software sequentially based on the extracted actions; meanwhile,
the operation procedure is recorded into an instructional video. We evaluate
the usability of automatically generated instructional videos via user studies
and an online survey. The evaluation results show, with our toolkit, the
generated instructional videos can better assist non-professional end users
with the software operations. Moreover, more than 85% of survey participants
prefer to use the instructional videos rather than the original user manuals.","Liu, Songsong and Wang, Shu and Sun, Kun",2024,,,,Proceedings of the ACM on Human-Computer Interaction
No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials,http://arxiv.org/abs/2509.21816v1,chi2021automatic,\cite{chi2021automatic},Automatic instructional video creation from a markdown-formatted tutorial,,,"Chi, Peggy and Frey, Nathan and Panovich, Katrina and Essa, Irfan",2021,,,,
