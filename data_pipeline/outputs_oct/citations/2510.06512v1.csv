parent_paper_title,parent_arxiv_link,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal
LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval,http://arxiv.org/abs/2510.06512v1,yang2023specification,\cite{yang2023specification},"Specification-Driven Video Search via Foundation Models and Formal
  Verification",http://arxiv.org/abs/2309.10171v1,"The increasing abundance of video data enables users to search for events of
interest, e.g., emergency incidents. Meanwhile, it raises new concerns, such as
the need for preserving privacy. Existing approaches to video search require
either manual inspection or a deep learning model with massive training. We
develop a method that uses recent advances in vision and language models, as
well as formal methods, to search for events of interest in video clips
automatically and efficiently. The method consists of an algorithm to map
text-based event descriptions into linear temporal logic over finite traces
(LTL$_f$) and an algorithm to construct an automaton encoding the video
information. Then, the method formally verifies the automaton representing the
video against the LTL$_f$ specifications and adds the pertinent video clips to
the search result if the automaton satisfies the specifications. We provide
qualitative and quantitative analysis to demonstrate the video-searching
capability of the proposed method. It achieves over 90 percent precision in
searching over privacy-sensitive videos and a state-of-the-art autonomous
driving dataset.","Yang, Yunhao and Gaglione, Jean-Rapha{\""e}l and Chinchali, Sandeep and Topcu, Ufuk",2023,,,,arXiv preprint arXiv:2309.10171
LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval,http://arxiv.org/abs/2510.06512v1,Choi2024TowardsNV,\cite{Choi2024TowardsNV},Towards Neuro-Symbolic Video Understanding,http://arxiv.org/abs/2403.11021v3,"The unprecedented surge in video data production in recent years necessitates
efficient tools to extract meaningful frames from videos for downstream tasks.
Long-term temporal reasoning is a key desideratum for frame retrieval systems.
While state-of-the-art foundation models, like VideoLLaMA and ViCLIP, are
proficient in short-term semantic understanding, they surprisingly fail at
long-term reasoning across frames. A key reason for this failure is that they
intertwine per-frame perception and temporal reasoning into a single deep
network. Hence, decoupling but co-designing semantic understanding and temporal
reasoning is essential for efficient scene identification. We propose a system
that leverages vision-language models for semantic understanding of individual
frames but effectively reasons about the long-term evolution of events using
state machines and temporal logic (TL) formulae that inherently capture memory.
Our TL-based reasoning improves the F1 score of complex event identification by
9-15% compared to benchmarks that use GPT4 for reasoning on state-of-the-art
self-driving datasets such as Waymo and NuScenes.",Minkyu Choi and Harsh Goel and Mohammad Omama and Yunhao Yang and Sahil Shah and Sandeep Chinchali,2024,,https://api.semanticscholar.org/CorpusID:268513042,,
LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval,http://arxiv.org/abs/2510.06512v1,videomme,\cite{videomme},Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis,,,"Fu, Chaoyou and Dai, Yuhan and Luo, Yongdong and Li, Lei and Ren, Shuhuai and Zhang, Renrui and Wang, Zihan and Zhou, Chenyu and Shen, Yunhang and Zhang, Mengdan and others",2024,,,,arXiv preprint arXiv:2405.21075
LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval,http://arxiv.org/abs/2510.06512v1,chen2024rextime,\cite{chen2024rextime},ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos,http://arxiv.org/abs/2406.19392v2,"We introduce ReXTime, a benchmark designed to rigorously test AI models'
ability to perform temporal reasoning within video events. Specifically,
ReXTime focuses on reasoning across time, i.e. human-like understanding when
the question and its corresponding answer occur in different video segments.
This form of reasoning, requiring advanced understanding of cause-and-effect
relationships across video segments, poses significant challenges to even the
frontier multimodal large language models. To facilitate this evaluation, we
develop an automated pipeline for generating temporal reasoning question-answer
pairs, significantly reducing the need for labor-intensive manual annotations.
Our benchmark includes 921 carefully vetted validation samples and 2,143 test
samples, each manually curated for accuracy and relevance. Evaluation results
show that while frontier large language models outperform academic models, they
still lag behind human performance by a significant 14.3% accuracy gap.
Additionally, our pipeline creates a training dataset of 9,695 machine
generated samples without manual effort, which empirical studies suggest can
enhance the across-time reasoning via fine-tuning.","Chen, Jr-Jen and Liao, Yu-Chien and Lin, Hsi-Che and Yu, Yu-Chu and Chen, Yen-Chun and Wang, Frank",2024,,,,Advances in Neural Information Processing Systems
LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval,http://arxiv.org/abs/2510.06512v1,nextqa,\cite{nextqa},NExT-QA:Next Phase of Question-Answering to Explaining Temporal Actions,http://arxiv.org/abs/2105.08276v2,"We introduce NExT-QA, a rigorously designed video question answering
(VideoQA) benchmark to advance video understanding from describing to
explaining the temporal actions. Based on the dataset, we set up multi-choice
and open-ended QA tasks targeting causal action reasoning, temporal action
reasoning, and common scene comprehension. Through extensive analysis of
baselines and established VideoQA techniques, we find that top-performing
methods excel at shallow scene descriptions but are weak in causal and temporal
action reasoning. Furthermore, the models that are effective on multi-choice
QA, when adapted to open-ended QA, still struggle in generalizing the answers.
This raises doubt on the ability of these models to reason and highlights
possibilities for improvement. With detailed results for different question
types and heuristic observations for future works, we hope NExT-QA will guide
the next generation of VQA research to go beyond superficial scene description
towards a deeper understanding of videos. (The dataset and related resources
are available at https://github.com/doc-doc/NExT-QA.git)","Xiao, Junbin and Shang, Xindi and Yao, Angela and Chua, Tat-Seng",2021,,,,
LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval,http://arxiv.org/abs/2510.06512v1,qvhighlights,\cite{qvhighlights},Detecting moments and highlights in videos via natural language queries,,,"Lei, Jie and Berg, Tamara L and Bansal, Mohit",2021,,,,Advances in Neural Information Processing Systems
LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval,http://arxiv.org/abs/2510.06512v1,cai2024temporalbench,\cite{cai2024temporalbench},"TemporalBench: Benchmarking Fine-grained Temporal Understanding for
  Multimodal Video Models",http://arxiv.org/abs/2410.10818v2,"Understanding fine-grained temporal dynamics is crucial for multimodal video
comprehension and generation. Due to the lack of fine-grained temporal
annotations, existing video benchmarks mostly resemble static image benchmarks
and are incompetent at evaluating models for temporal understanding. In this
paper, we introduce TemporalBench, a new benchmark dedicated to evaluating
fine-grained temporal understanding in videos. TemporalBench consists of ~10K
video question-answer pairs, derived from ~2K high-quality human annotations
detailing the temporal dynamics in video clips. As a result, our benchmark
provides a unique testbed for evaluating various temporal understanding and
reasoning abilities such as action frequency, motion magnitude, event order,
etc. Moreover, it enables evaluations on various tasks like both video question
answering and captioning, both short and long video understanding, as well as
different models such as multimodal video embedding models and text generation
models. Results show that state-of-the-art models like GPT-4o achieve only
38.5% question answering accuracy on TemporalBench, demonstrating a significant
gap (~30%) between humans and AI in temporal understanding. Furthermore, we
notice a critical pitfall for multi-choice QA where LLMs can detect the subtle
changes in negative captions and find a centralized description as a cue for
its prediction, where we propose Multiple Binary Accuracy (MBA) to correct such
bias. We hope that TemporalBench can foster research on improving models'
temporal reasoning capabilities. Both dataset and evaluation code will be made
available.","Cai, Mu and Tan, Reuben and Zhang, Jianrui and Zou, Bocheng and Zhang, Kai and Yao, Feng and Zhu, Fangrui and Gu, Jing and Zhong, Yiwu and Shang, Yuzhang and others",2024,,,,arXiv preprint arXiv:2410.10818
LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval,http://arxiv.org/abs/2510.06512v1,liu2024tempcompass,\cite{liu2024tempcompass},TempCompass: Do Video LLMs Really Understand Videos?,http://arxiv.org/abs/2403.00476v3,"Recently, there is a surge in interest surrounding video large language
models (Video LLMs). However, existing benchmarks fail to provide a
comprehensive feedback on the temporal perception ability of Video LLMs. On the
one hand, most of them are unable to distinguish between different temporal
aspects (e.g., speed, direction) and thus cannot reflect the nuanced
performance on these specific aspects. On the other hand, they are limited in
the diversity of task formats (e.g., only multi-choice QA), which hinders the
understanding of how temporal perception performance may vary across different
types of tasks. Motivated by these two problems, we propose the
\textbf{TempCompass} benchmark, which introduces a diversity of temporal
aspects and task formats. To collect high-quality test data, we devise two
novel strategies: (1) In video collection, we construct conflicting videos that
share the same static content but differ in a specific temporal aspect, which
prevents Video LLMs from leveraging single-frame bias or language priors. (2)
To collect the task instructions, we propose a paradigm where humans first
annotate meta-information for a video and then an LLM generates the
instruction. We also design an LLM-based approach to automatically and
accurately evaluate the responses from Video LLMs. Based on TempCompass, we
comprehensively evaluate 8 state-of-the-art (SOTA) Video LLMs and 3 Image LLMs,
and reveal the discerning fact that these models exhibit notably poor temporal
perception ability. Our data will be available at
https://github.com/llyx97/TempCompass.","Liu, Yuanxin and Li, Shicheng and Liu, Yi and Wang, Yuxiang and Ren, Shuhuai and Li, Lei and Chen, Sishuo and Sun, Xu and Hou, Lu",2024,,,,arXiv preprint arXiv:2403.00476
LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval,http://arxiv.org/abs/2510.06512v1,sakshi2024mmau,\cite{sakshi2024mmau},MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark,http://arxiv.org/abs/2410.19168v1,"The ability to comprehend audio--which includes speech, non-speech sounds,
and music--is crucial for AI agents to interact effectively with the world. We
present MMAU, a novel benchmark designed to evaluate multimodal audio
understanding models on tasks requiring expert-level knowledge and complex
reasoning. MMAU comprises 10k carefully curated audio clips paired with
human-annotated natural language questions and answers spanning speech,
environmental sounds, and music. It includes information extraction and
reasoning questions, requiring models to demonstrate 27 distinct skills across
unique and challenging tasks. Unlike existing benchmarks, MMAU emphasizes
advanced perception and reasoning with domain-specific knowledge, challenging
models to tackle tasks akin to those faced by experts. We assess 18 open-source
and proprietary (Large) Audio-Language Models, demonstrating the significant
challenges posed by MMAU. Notably, even the most advanced Gemini Pro v1.5
achieves only 52.97% accuracy, and the state-of-the-art open-source Qwen2-Audio
achieves only 52.50%, highlighting considerable room for improvement. We
believe MMAU will drive the audio and multimodal research community to develop
more advanced audio understanding models capable of solving complex audio
tasks.","Sakshi, S and Tyagi, Utkarsh and Kumar, Sonal and Seth, Ashish and Selvakumar, Ramaneswaran and Nieto, Oriol and Duraiswami, Ramani and Ghosh, Sreyan and Manocha, Dinesh",2024,,,,arXiv preprint arXiv:2410.19168
LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval,http://arxiv.org/abs/2510.06512v1,ghosh2023compa,\cite{ghosh2023compa},"CompA: Addressing the Gap in Compositional Reasoning in Audio-Language
  Models",http://arxiv.org/abs/2310.08753v4,"A fundamental characteristic of audio is its compositional nature.
Audio-language models (ALMs) trained using a contrastive approach (e.g., CLAP)
that learns a shared representation between audio and language modalities have
improved performance in many downstream applications, including zero-shot audio
classification, audio retrieval, etc. However, the ability of these models to
effectively perform compositional reasoning remains largely unexplored and
necessitates additional research. In this paper, we propose CompA, a collection
of two expert-annotated benchmarks with a majority of real-world audio samples,
to evaluate compositional reasoning in ALMs. Our proposed CompA-order evaluates
how well an ALM understands the order or occurrence of acoustic events in
audio, and CompA-attribute evaluates attribute-binding of acoustic events. An
instance from either benchmark consists of two audio-caption pairs, where both
audios have the same acoustic events but with different compositions. An ALM is
evaluated on how well it matches the right audio to the right caption. Using
this benchmark, we first show that current ALMs perform only marginally better
than random chance, thereby struggling with compositional reasoning. Next, we
propose CompA-CLAP, where we fine-tune CLAP using a novel learning method to
improve its compositional reasoning abilities. To train CompA-CLAP, we first
propose improvements to contrastive training with composition-aware hard
negatives, allowing for more focused training. Next, we propose a novel modular
contrastive loss that helps the model learn fine-grained compositional
understanding and overcomes the acute scarcity of openly available
compositional audios. CompA-CLAP significantly improves over all our baseline
models on the CompA benchmark, indicating its superior compositional reasoning
capabilities.","Ghosh, Sreyan and Seth, Ashish and Kumar, Sonal and Tyagi, Utkarsh and Evuru, Chandra Kiran and Ramaneswaran, S and Sakshi, S and Nieto, Oriol and Duraiswami, Ramani and Manocha, Dinesh",2023,,,,arXiv preprint arXiv:2310.08753
LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval,http://arxiv.org/abs/2510.06512v1,krishna2017dense,\cite{krishna2017dense},Dense-Captioning Events in Videos,http://arxiv.org/abs/1705.00754v1,"Most natural videos contain numerous events. For example, in a video of a
""man playing a piano"", the video might also contain ""another man dancing"" or ""a
crowd clapping"". We introduce the task of dense-captioning events, which
involves both detecting and describing events in a video. We propose a new
model that is able to identify all events in a single pass of the video while
simultaneously describing the detected events with natural language. Our model
introduces a variant of an existing proposal module that is designed to capture
both short as well as long events that span minutes. To capture the
dependencies between the events in a video, our model introduces a new
captioning module that uses contextual information from past and future events
to jointly describe all events. We also introduce ActivityNet Captions, a
large-scale benchmark for dense-captioning events. ActivityNet Captions
contains 20k videos amounting to 849 video hours with 100k total descriptions,
each with it's unique start and end time. Finally, we report performances of
our model for dense-captioning events, video retrieval and localization.","Krishna, Ranjay and Hata, Kenji and Ren, Frederic and Fei-Fei, Li and Niebles, Juan Carlos",2017,,,,
LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval,http://arxiv.org/abs/2510.06512v1,didemo,\cite{didemo},Localizing Moments in Video with Natural Language,http://arxiv.org/abs/1708.01641v1,"We consider retrieving a specific temporal segment, or moment, from a video
given a natural language text description. Methods designed to retrieve whole
video clips with natural language determine what occurs in a video but not
when. To address this issue, we propose the Moment Context Network (MCN) which
effectively localizes natural language queries in videos by integrating local
and global video features over time. A key obstacle to training our MCN model
is that current video datasets do not include pairs of localized video segments
and referring expressions, or text descriptions which uniquely identify a
corresponding moment. Therefore, we collect the Distinct Describable Moments
(DiDeMo) dataset which consists of over 10,000 unedited, personal videos in
diverse visual settings with pairs of localized video segments and referring
expressions. We demonstrate that MCN outperforms several baseline methods and
believe that our initial results together with the release of DiDeMo will
inspire further research on localizing video moments with natural language.","Anne Hendricks, Lisa and Wang, Oliver and Shechtman, Eli and Sivic, Josef and Darrell, Trevor and Russell, Bryan",2017,,,,
LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval,http://arxiv.org/abs/2510.06512v1,luo2021clip4clip,\cite{luo2021clip4clip},"CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip
  Retrieval",http://arxiv.org/abs/2104.08860v2,"Video-text retrieval plays an essential role in multi-modal research and has
been widely used in many real-world web applications. The CLIP (Contrastive
Language-Image Pre-training), an image-language pre-training model, has
demonstrated the power of visual concepts learning from web collected
image-text datasets. In this paper, we propose a CLIP4Clip model to transfer
the knowledge of the CLIP model to video-language retrieval in an end-to-end
manner. Several questions are investigated via empirical studies: 1) Whether
image feature is enough for video-text retrieval? 2) How a post-pretraining on
a large-scale video-text dataset based on the CLIP affect the performance? 3)
What is the practical mechanism to model temporal dependency between video
frames? And 4) The Hyper-parameters sensitivity of the model on video-text
retrieval task. Extensive experimental results present that the CLIP4Clip model
transferred from the CLIP can achieve SOTA results on various video-text
retrieval datasets, including MSR-VTT, MSVC, LSMDC, ActivityNet, and DiDeMo. We
release our code at https://github.com/ArrowLuo/CLIP4Clip.","Luo, Huaishao and Ji, Lei and Zhong, Ming and Chen, Yang and Lei, Wen and Duan, Nan and Li, Tianrui",2021,,,,arXiv preprint arXiv:2104.08860
LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval,http://arxiv.org/abs/2510.06512v1,liu2022ts2,\cite{liu2022ts2},Ts2-net: Token shift and selection transformer for text-video retrieval,,,"Liu, Yuqi and Xiong, Pengfei and Xu, Luhui and Cao, Shengming and Jin, Qin",2022,,,,
LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval,http://arxiv.org/abs/2510.06512v1,Bain21,\cite{Bain21},Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval,http://arxiv.org/abs/2104.00650v2,"Our objective in this work is video-text retrieval - in particular a joint
embedding that enables efficient text-to-video retrieval. The challenges in
this area include the design of the visual architecture and the nature of the
training data, in that the available large scale video-text training datasets,
such as HowTo100M, are noisy and hence competitive performance is achieved only
at scale through large amounts of compute. We address both these challenges in
this paper. We propose an end-to-end trainable model that is designed to take
advantage of both large-scale image and video captioning datasets. Our model is
an adaptation and extension of the recent ViT and Timesformer architectures,
and consists of attention in both space and time. The model is flexible and can
be trained on both image and video text datasets, either independently or in
conjunction. It is trained with a curriculum learning schedule that begins by
treating images as 'frozen' snapshots of video, and then gradually learns to
attend to increasing temporal context when trained on video datasets. We also
provide a new video-text pretraining dataset WebVid-2M, comprised of over two
million videos with weak captions scraped from the internet. Despite training
on datasets that are an order of magnitude smaller, we show that this approach
yields state-of-the-art results on standard downstream video-retrieval
benchmarks including MSR-VTT, MSVD, DiDeMo and LSMDC.",Max Bain and Arsha Nagrani and G{\,2021,,,,
LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval,http://arxiv.org/abs/2510.06512v1,li2022mplug,\cite{li2022mplug},"mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal
  Skip-connections",http://arxiv.org/abs/2205.12005v2,"Large-scale pretrained foundation models have been an emerging paradigm for
building artificial intelligence (AI) systems, which can be quickly adapted to
a wide range of downstream tasks. This paper presents mPLUG, a new
vision-language foundation model for both cross-modal understanding and
generation. Most existing pre-trained models suffer from the problems of low
computational efficiency and information asymmetry brought by the long visual
sequence in cross-modal alignment. To address these problems, mPLUG introduces
an effective and efficient vision-language architecture with novel cross-modal
skip-connections, which creates inter-layer shortcuts that skip a certain
number of layers for time-consuming full self-attention on the vision side.
mPLUG is pre-trained end-to-end on large-scale image-text pairs with both
discriminative and generative objectives. It achieves state-of-the-art results
on a wide range of vision-language downstream tasks, such as image captioning,
image-text retrieval, visual grounding and visual question answering. mPLUG
also demonstrates strong zero-shot transferability when directly transferred to
multiple video-language tasks.","Li, Chenliang and Xu, Haiyang and Tian, Junfeng and Wang, Wei and Yan, Ming and Bi, Bin and Ye, Jiabo and Chen, Hehong and Xu, Guohai and Cao, Zheng and others",2022,,,,arXiv preprint arXiv:2205.12005
LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval,http://arxiv.org/abs/2510.06512v1,liu-etal-2025-eliot,\cite{liu-etal-2025-eliot},{ELIOT}: Zero-Shot Video-Text Retrieval through Relevance-Boosted Captioning and Structural Information Extraction,,,"Liu, Xuye  and
      Wang, Yimu  and
      Zhao, Jian",2025,,https://aclanthology.org/2025.naacl-srw.37/,10.18653/v1/2025.naacl-srw.37,
