parent_paper_title,parent_arxiv_link,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal
Seeing Culture: A Benchmark for Visual Reasoning and Grounding,http://arxiv.org/abs/2509.16517v1,thapliyal-etal-2022-crossmodal3600,\cite{thapliyal-etal-2022-crossmodal3600},Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset,,,"Thapliyal, Ashish V.  and
      Pont Tuset, Jordi  and
      Chen, Xi  and
      Soricut, Radu",2022,,https://aclanthology.org/2022.emnlp-main.45/,10.18653/v1/2022.emnlp-main.45,
Seeing Culture: A Benchmark for Visual Reasoning and Grounding,http://arxiv.org/abs/2509.16517v1,IPAS_mosaic_2025,\cite{IPAS_mosaic_2025},How Culturally Aware are Vision-Language Models?,http://arxiv.org/abs/2405.17475v2,"An image is often considered worth a thousand words, and certain images can
tell rich and insightful stories. Can these stories be told via image
captioning? Images from folklore genres, such as mythology, folk dance,
cultural signs, and symbols, are vital to every culture. Our research compares
the performance of four popular vision-language models (GPT-4V, Gemini Pro
Vision, LLaVA, and OpenFlamingo) in identifying culturally specific information
in such images and creating accurate and culturally sensitive image captions.
We also propose a new evaluation metric, the Cultural Awareness Score (CAS),
which measures the degree of cultural awareness in image captions. We provide a
dataset MOSAIC-1.5k labeled with ground truth for images containing cultural
background and context and a labeled dataset with assigned Cultural Awareness
Scores that can be used with unseen data. Creating culturally appropriate image
captions is valuable for scientific research and can be beneficial for many
practical applications. We envision our work will promote a deeper integration
of cultural sensitivity in AI applications worldwide. By making the dataset and
Cultural Awareness Score available to the public, we aim to facilitate further
research in this area, encouraging the development of more culturally aware AI
systems that respect and celebrate global diversity.","Burda-Lassen, Olena and Chadha, Aman and Goswami, Shashank and Jain, Vinija",2025,,,10.1109/IPAS63548.2025.10924504,
Seeing Culture: A Benchmark for Visual Reasoning and Grounding,http://arxiv.org/abs/2509.16517v1,bai-etal-2025-power_MosAIC,\cite{bai-etal-2025-power_MosAIC},"The Power of Many: Multi-Agent Multimodal Models for Cultural Image
  Captioning",http://arxiv.org/abs/2411.11758v1,"Large Multimodal Models (LMMs) exhibit impressive performance across various
multimodal tasks. However, their effectiveness in cross-cultural contexts
remains limited due to the predominantly Western-centric nature of most data
and models. Conversely, multi-agent models have shown significant capability in
solving complex tasks. Our study evaluates the collective performance of LMMs
in a multi-agent interaction setting for the novel task of cultural image
captioning. Our contributions are as follows: (1) We introduce MosAIC, a
Multi-Agent framework to enhance cross-cultural Image Captioning using LMMs
with distinct cultural personas; (2) We provide a dataset of culturally
enriched image captions in English for images from China, India, and Romania
across three datasets: GeoDE, GD-VCR, CVQA; (3) We propose a culture-adaptable
metric for evaluating cultural information within image captions; and (4) We
show that the multi-agent interaction outperforms single-agent models across
different metrics, and offer valuable insights for future research. Our dataset
and models can be accessed at https://github.com/MichiganNLP/MosAIC.","Bai, Longju  and
      Borah, Angana  and
      Ignat, Oana  and
      Mihalcea, Rada",2025,,https://aclanthology.org/2025.naacl-long.152/,,
Seeing Culture: A Benchmark for Visual Reasoning and Grounding,http://arxiv.org/abs/2509.16517v1,cahyawijaya2025crowdsourcecrawlgeneratecreating_SEA-VL,\cite{cahyawijaya2025crowdsourcecrawlgeneratecreating_SEA-VL},"Crowdsource, Crawl, or Generate? Creating SEA-VL, a Multicultural
  Vision-Language Dataset for Southeast Asia",http://arxiv.org/abs/2503.07920v2,"Southeast Asia (SEA) is a region of extraordinary linguistic and cultural
diversity, yet it remains significantly underrepresented in vision-language
(VL) research. This often results in artificial intelligence (AI) models that
fail to capture SEA cultural nuances. To fill this gap, we present SEA-VL, an
open-source initiative dedicated to developing high-quality, culturally
relevant data for SEA languages. By involving contributors from SEA countries,
SEA-VL aims to ensure better cultural relevance and diversity, fostering
greater inclusivity of underrepresented languages in VL research. Beyond
crowdsourcing, our initiative goes one step further in the exploration of the
automatic collection of culturally relevant images through crawling and image
generation. First, we find that image crawling achieves approximately ~85%
cultural relevance while being more cost- and time-efficient than
crowdsourcing. Second, despite the substantial progress in generative vision
models, synthetic images remain unreliable in accurately reflecting SEA
cultures. The generated images often fail to reflect the nuanced traditions and
cultural contexts of the region. Collectively, we gather 1.28M SEA
culturally-relevant images, more than 50 times larger than other existing
datasets. Through SEA-VL, we aim to bridge the representation gap in SEA,
fostering the development of more inclusive AI systems that authentically
represent diverse cultures across SEA.",Samuel Cahyawijaya and Holy Lovenia and Joel Ruben Antony Moniz and Tack Hwa Wong and Mohammad Rifqi Farhansyah and Thant Thiri Maung and Frederikus Hudi and David Anugraha and Muhammad Ravi Shulthan Habibi and Muhammad Reza Qorib and Amit Agarwal and Joseph Marvin Imperial and Hitesh Laxmichand Patel and Vicky Feliren and Bahrul Ilmi Nasution and Manuel Antonio Rufino and Genta Indra Winata and Rian Adam Rajagede and Carlos Rafael Catalan and Mohamed Fazli Imam and Priyaranjan Pattnayak and Salsabila Zahirah Pranida and Kevin Pratama and Yeshil Bangera and Adisai Na-Thalang and Patricia Nicole Monderin and Yueqi Song and Christian Simon and Lynnette Hui Xian Ng and Richardy Lobo' Sapan and Taki Hasan Rafi and Bin Wang and Supryadi and Kanyakorn Veerakanjana and Piyalitt Ittichaiwong and Matthew Theodore Roque and Karissa Vincentio and Takdanai Kreangphet and Phakphum Artkaew and Kadek Hendrawan Palgunadi and Yanzhi Yu and Rochana Prih Hastuti and William Nixon and Mithil Bangera and Adrian Xuan Wei Lim and Aye Hninn Khine and Hanif Muhammad Zhafran and Teddy Ferdinan and Audra Aurora Izzani and Ayushman Singh and Evan and Jauza Akbar Krito and Michael Anugraha and Fenal Ashokbhai Ilasariya and Haochen Li and John Amadeo Daniswara and Filbert Aurelian Tjiaranata and Eryawan Presma Yulianrifat and Can Udomcharoenchaikit and Fadil Risdian Ansori and Mahardika Krisna Ihsani and Giang Nguyen and Anab Maulana Barik and Dan John Velasco and Rifo Ahmad Genadi and Saptarshi Saha and Chengwei Wei and Isaiah Flores and Kenneth Ko Han Chen and Anjela Gail Santos and Wan Shen Lim and Kaung Si Phyo and Tim Santos and Meisyarah Dwiastuti and Jiayun Luo and Jan Christian Blaise Cruz and Ming Shan Hee and Ikhlasul Akmal Hanif and M. Alif Al Hakim and Muhammad Rizky Sya'ban and Kun Kerdthaisong and Lester James V. Miranda and Fajri Koto and Tirana Noor Fatyanosa and Alham Fikri Aji and Jostin Jerico Rosal and Jun Kevin and Robert Wijaya and Onno P. Kampman and Ruochen Zhang and BÃ¶rje F. Karlsson and Peerat Limkonchotiwat,2025,,https://arxiv.org/abs/2503.07920,,
Seeing Culture: A Benchmark for Visual Reasoning and Grounding,http://arxiv.org/abs/2509.16517v1,bhalerao2025multiagentmultimodalmodelsmulticultural_mosaig,\cite{bhalerao2025multiagentmultimodalmodelsmulticultural_mosaig},Multi-Agent Multimodal Models for Multicultural Text to Image Generation,http://arxiv.org/abs/2502.15972v1,"Large Language Models (LLMs) demonstrate impressive performance across
various multimodal tasks. However, their effectiveness in cross-cultural
contexts remains limited due to the predominantly Western-centric nature of
existing data and models. Meanwhile, multi-agent models have shown strong
capabilities in solving complex tasks. In this paper, we evaluate the
performance of LLMs in a multi-agent interaction setting for the novel task of
multicultural image generation. Our key contributions are: (1) We introduce
MosAIG, a Multi-Agent framework that enhances multicultural Image Generation by
leveraging LLMs with distinct cultural personas; (2) We provide a dataset of
9,000 multicultural images spanning five countries, three age groups, two
genders, 25 historical landmarks, and five languages; and (3) We demonstrate
that multi-agent interactions outperform simple, no-agent models across
multiple evaluation metrics, offering valuable insights for future research.
Our dataset and models are available at https://github.com/OanaIgnat/MosAIG.",Parth Bhalerao and Mounika Yalamarty and Brian Trinh and Oana Ignat,2025,,https://arxiv.org/abs/2502.15972,,
Seeing Culture: A Benchmark for Visual Reasoning and Grounding,http://arxiv.org/abs/2509.16517v1,tang2024mtvqa,\cite{tang2024mtvqa},MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering,http://arxiv.org/abs/2405.11985v5,"Text-Centric Visual Question Answering (TEC-VQA) in its proper format not
only facilitates human-machine interaction in text-centric visual environments
but also serves as a de facto gold proxy to evaluate AI models in the domain of
text-centric scene understanding. Nonetheless, most existing TEC-VQA benchmarks
have focused on high-resource languages like English and Chinese. Despite
pioneering works to expand multilingual QA pairs in non-text-centric VQA
datasets through translation engines, the translation-based protocol encounters
a substantial ""visual-textual misalignment"" problem when applied to TEC-VQA.
Specifically, it prioritizes the text in question-answer pairs while
disregarding the visual text present in images. Moreover, it fails to address
complexities related to nuanced meaning, contextual distortion, language bias,
and question-type diversity. In this work, we tackle multilingual TEC-VQA by
introducing MTVQA, the first benchmark featuring high-quality human expert
annotations across 9 diverse languages, consisting of 6,778 question-answer
pairs across 2,116 images. Further, by comprehensively evaluating numerous
state-of-the-art Multimodal Large Language Models~(MLLMs), including Qwen2-VL,
GPT-4o, GPT-4V, Claude3, and Gemini, on the MTVQA benchmark, it is evident that
there is still a large room for performance improvement (Qwen2-VL scoring 30.9
versus 79.7 for human performance), underscoring the value of MTVQA.
Additionally, we supply multilingual training data within the MTVQA dataset,
demonstrating that straightforward fine-tuning with this data can substantially
enhance multilingual TEC-VQA performance. We aspire that MTVQA will offer the
research community fresh insights and stimulate further exploration in
multilingual visual text comprehension. The project homepage is available at
https://bytedance.github.io/MTVQA/.",Jingqun Tang and Qi Liu and Yongjie Ye and Jinghui Lu and Shu Wei and Chunhui Lin and Wanqing Li and Mohamad Fitri Faiz Bin Mahmood and Hao Feng and Zhen Zhao and Yanjie Wang and Yuliang Liu and Hao Liu and Xiang Bai and Can Huang,2024,,https://arxiv.org/abs/2405.11985,,
Seeing Culture: A Benchmark for Visual Reasoning and Grounding,http://arxiv.org/abs/2509.16517v1,nayak-etal-2024-benchmarking_CulturalVQA,\cite{nayak-etal-2024-benchmarking_CulturalVQA},Benchmarking Vision Language Models for Cultural Understanding,http://arxiv.org/abs/2407.10920v3,"Foundation models and vision-language pre-training have notably advanced
Vision Language Models (VLMs), enabling multimodal processing of visual and
linguistic data. However, their performance has been typically assessed on
general scene understanding - recognizing objects, attributes, and actions -
rather than cultural comprehension. This study introduces CulturalVQA, a visual
question-answering benchmark aimed at assessing VLM's geo-diverse cultural
understanding. We curate a collection of 2,378 image-question pairs with 1-5
answers per question representing cultures from 11 countries across 5
continents. The questions probe understanding of various facets of culture such
as clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on
CulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of
cultural understanding across regions, with strong cultural understanding
capabilities for North America while significantly lower performance for
Africa. We observe disparity in their performance across cultural facets too,
with clothing, rituals, and traditions seeing higher performances than food and
drink. These disparities help us identify areas where VLMs lack cultural
understanding and demonstrate the potential of CulturalVQA as a comprehensive
evaluation set for gauging VLM progress in understanding diverse cultures.","Nayak, Shravan  and
      Jain, Kanishk  and
      Awal, Rabiul  and
      Reddy, Siva  and
      Steenkiste, Sjoerd Van  and
      Hendricks, Lisa Anne  and
      Stanczak, Karolina  and
      Agrawal, Aishwarya",2024,,https://aclanthology.org/2024.emnlp-main.329/,10.18653/v1/2024.emnlp-main.329,
Seeing Culture: A Benchmark for Visual Reasoning and Grounding,http://arxiv.org/abs/2509.16517v1,Wang_CVLUE_2025,\cite{Wang_CVLUE_2025},"CVLUE: A New Benchmark Dataset for Chinese Vision-Language Understanding
  Evaluation",http://arxiv.org/abs/2407.01081v1,"Despite the rapid development of Chinese vision-language models (VLMs), most
existing Chinese vision-language (VL) datasets are constructed on
Western-centric images from existing English VL datasets. The cultural bias in
the images makes these datasets unsuitable for evaluating VLMs in Chinese
culture. To remedy this issue, we present a new Chinese Vision- Language
Understanding Evaluation (CVLUE) benchmark dataset, where the selection of
object categories and images is entirely driven by Chinese native speakers,
ensuring that the source images are representative of Chinese culture. The
benchmark contains four distinct VL tasks ranging from image-text retrieval to
visual question answering, visual grounding and visual dialogue. We present a
detailed statistical analysis of CVLUE and provide a baseline performance
analysis with several open-source multilingual VLMs on CVLUE and its English
counterparts to reveal their performance gap between English and Chinese. Our
in-depth category-level analysis reveals a lack of Chinese cultural knowledge
in existing VLMs. We also find that fine-tuning on Chinese culture-related VL
datasets effectively enhances VLMs' understanding of Chinese culture.","Wang, Yuxuan and Liu, Yijun and Yu, Fei and Huang, Chen and Li, Kexin and Wan, Zhiguo and Che, Wanxiang and Chen, Hongyang",2025,,,10.1609/aaai.v39i8.32884,Proceedings of the AAAI Conference on Artificial Intelligence
Seeing Culture: A Benchmark for Visual Reasoning and Grounding,http://arxiv.org/abs/2509.16517v1,nikandrou_2025_crope,\cite{nikandrou_2025_crope},"CROPE: Evaluating In-Context Adaptation of Vision and Language Models to
  Culture-Specific Concepts",http://arxiv.org/abs/2410.15453v2,"As Vision and Language models (VLMs) are reaching users across the globe,
assessing their cultural understanding has become a critical challenge. In this
paper, we introduce CROPE, a visual question answering benchmark designed to
probe the knowledge of culture-specific concepts and evaluate the capacity for
cultural adaptation through contextual information. This allows us to
distinguish between parametric knowledge acquired during training and
contextual knowledge provided during inference via visual and textual
descriptions. Our evaluation of several state-of-the-art open VLMs shows large
performance disparities between culture-specific and common concepts in the
parametric setting. Moreover, experiments with contextual knowledge indicate
that models struggle to effectively utilize multimodal information and bind
culture-specific concepts to their depictions. Our findings reveal limitations
in the cultural understanding and adaptability of current VLMs that need to be
addressed toward more culturally inclusive models.","Nikandrou, Malvina  and
      Pantazopoulos, Georgios  and
      Vitsakis, Nikolas  and
      Konstas, Ioannis  and
      Suglia, Alessandro",2025,,https://aclanthology.org/2025.naacl-long.402/,,
Seeing Culture: A Benchmark for Visual Reasoning and Grounding,http://arxiv.org/abs/2509.16517v1,yin2021broaden_GDVCR,\cite{yin2021broaden_GDVCR},Broaden the Vision: Geo-Diverse Visual Commonsense Reasoning,http://arxiv.org/abs/2109.06860v1,"Commonsense is defined as the knowledge that is shared by everyone. However,
certain types of commonsense knowledge are correlated with culture and
geographic locations and they are only shared locally. For example, the
scenarios of wedding ceremonies vary across regions due to different customs
influenced by historical and religious factors. Such regional characteristics,
however, are generally omitted in prior work. In this paper, we construct a
Geo-Diverse Visual Commonsense Reasoning dataset (GD-VCR) to test
vision-and-language models' ability to understand cultural and
geo-location-specific commonsense. In particular, we study two state-of-the-art
Vision-and-Language models, VisualBERT and ViLBERT trained on VCR, a standard
multimodal commonsense benchmark with images primarily from Western regions. We
then evaluate how well the trained models can generalize to answering the
questions in GD-VCR. We find that the performance of both models for
non-Western regions including East Asia, South Asia, and Africa is
significantly lower than that for Western region. We analyze the reasons behind
the performance disparity and find that the performance gap is larger on QA
pairs that: 1) are concerned with culture-related scenarios, e.g., weddings,
religious activities, and festivals; 2) require high-level geo-diverse
commonsense reasoning rather than low-order perception and recognition. Dataset
and code are released at https://github.com/WadeYin9712/GD-VCR.","Yin, Da and Li, Liunian Harold and Hu, Ziniu and Peng, Nanyun and Chang, Kai-Wei",2021,,,,
Seeing Culture: A Benchmark for Visual Reasoning and Grounding,http://arxiv.org/abs/2509.16517v1,mogrovejo2024cvqa,\cite{mogrovejo2024cvqa},"CVQA: Culturally-diverse Multilingual Visual Question Answering
  Benchmark",http://arxiv.org/abs/2406.05967v2,"Visual Question Answering (VQA) is an important task in multimodal AI, and it
is often used to test the ability of vision-language models to understand and
reason on knowledge present in both visual and textual data. However, most of
the current VQA models use datasets that are primarily focused on English and a
few major world languages, with images that are typically Western-centric.
While recent efforts have tried to increase the number of languages covered on
VQA datasets, they still lack diversity in low-resource languages. More
importantly, although these datasets often extend their linguistic range via
translation or some other approaches, they usually keep images the same,
resulting in narrow cultural representation. To address these limitations, we
construct CVQA, a new Culturally-diverse multilingual Visual Question Answering
benchmark, designed to cover a rich set of languages and cultures, where we
engage native speakers and cultural experts in the data collection process. As
a result, CVQA includes culturally-driven images and questions from across 30
countries on four continents, covering 31 languages with 13 scripts, providing
a total of 10k questions. We then benchmark several Multimodal Large Language
Models (MLLMs) on CVQA, and show that the dataset is challenging for the
current state-of-the-art models. This benchmark can serve as a probing
evaluation suite for assessing the cultural capability and bias of multimodal
models and hopefully encourage more research efforts toward increasing cultural
awareness and linguistic diversity in this field.",David Orlando Romero Mogrovejo and Chenyang Lyu and Haryo Akbarianto Wibowo and Santiago G{\'o}ngora and Aishik Mandal and Sukannya Purkayastha and Jesus-German Ortiz-Barajas and Emilio Villa Cueva and Jinheon Baek and Soyeong Jeong and Injy Hamed and Zheng Xin Yong and Zheng Wei Lim and Paula M{\'o}nica Silva and Jocelyn Dunstan and M{\'e}lanie Jouitteau and David LE MEUR and Joan Nwatu and Ganzorig Batnasan and Munkh-Erdene Otgonbold and Munkhjargal Gochoo and Guido Ivetta and Luciana Benotti and Laura Alonso Alemany and Hern{\'a}n Maina and Jiahui Geng and Tiago Timponi Torrent and Frederico Belcavello and Marcelo Viridiano and Jan Christian Blaise Cruz and Dan John Velasco and Oana Ignat and Zara Burzo and Chenxi Whitehouse and Artem Abzaliev and Teresa Clifford and Gr{\'a}inne Caulfield and Teresa Lynn and Christian Salamea-Palacios and Vladimir Araujo and Yova Kementchedjhieva and Mihail Minkov Mihaylov and Israel Abebe Azime and Henok Biadglign Ademtew and Bontu Fufa Balcha and Naome A Etori and David Ifeoluwa Adelani and Rada Mihalcea and Atnafu Lambebo Tonja and Maria Camila Buitrago Cabrera and Gisela Vallejo and Holy Lovenia and Ruochen Zhang and Marcos Estecha-Garitagoitia and Mario Rodr{\'\i}guez-Cantelar and Toqeer Ehsan and Rendi Chevi and Muhammad Farid Adilazuarda and Ryandito Diandaru and Samuel Cahyawijaya and Fajri Koto and Tatsuki Kuribayashi and Haiyue Song and Aditya Nanda Kishore Khandavally and Thanmay Jayakumar and Raj Dabre and Mohamed Fazli Mohamed Imam and Kumaranage Ravindu Yasas Nagasinghe and Alina Dragonetti and Luis Fernando D'Haro and Olivier NIYOMUGISHA and Jay Gala and Pranjal A Chitale and Fauzan Farooqui and Thamar Solorio and Alham Fikri Aji,2024,,https://openreview.net/forum?id=E18kRXTGmV,,
Seeing Culture: A Benchmark for Visual Reasoning and Grounding,http://arxiv.org/abs/2509.16517v1,liu2025culturevlm,\cite{liu2025culturevlm},Culture{VLM}: Characterizing and Improving Cultural Understanding of Vision-Language Models for over 100 Countries,,,"Liu, Shudong and Jin, Yiqiao and Li, Cheng and Wong, Derek F and Wen, Qingsong and Sun, Lichao and Chen, Haipeng and Xie, Xing and Wang, Jindong",2025,,,,arXiv preprint arXiv:2501.01282
Seeing Culture: A Benchmark for Visual Reasoning and Grounding,http://arxiv.org/abs/2509.16517v1,schneider2025gimmickgloballyinclusive,\cite{schneider2025gimmickgloballyinclusive},"GIMMICK -- Globally Inclusive Multimodal Multitask Cultural Knowledge
  Benchmarking",http://arxiv.org/abs/2502.13766v1,"Large Vision-Language Models (LVLMs) have recently gained attention due to
their distinctive performance and broad applicability. While it has been
previously shown that their efficacy in usage scenarios involving non-Western
contexts falls short, existing studies are limited in scope, covering just a
narrow range of cultures, focusing exclusively on a small number of cultural
aspects, or evaluating a limited selection of models on a single task only.
Towards globally inclusive LVLM research, we introduce GIMMICK, an extensive
multimodal benchmark designed to assess a broad spectrum of cultural knowledge
across 144 countries representing six global macro-regions. GIMMICK comprises
six tasks built upon three new datasets that span 728 unique cultural events or
facets on which we evaluated 20 LVLMs and 11 LLMs, including five proprietary
and 26 open-weight models of all sizes. We systematically examine (1) regional
cultural biases, (2) the influence of model size, (3) input modalities, and (4)
external cues. Our analyses reveal strong biases toward Western cultures across
models and tasks and highlight strong correlations between model size and
performance, as well as the effectiveness of multimodal input and external
geographic cues. We further find that models have more knowledge of tangible
than intangible aspects (e.g., food vs. rituals) and that they excel in
recognizing broad cultural origins but struggle with a more nuanced
understanding.",Florian Schneider and Carolin Holtermann and Chris Biemann and Anne Lauscher,2025,,https://arxiv.org/abs/2502.13766,,
Seeing Culture: A Benchmark for Visual Reasoning and Grounding,http://arxiv.org/abs/2509.16517v1,bhatia-etal-2024-local_GlobalRG,\cite{bhatia-etal-2024-local_GlobalRG},"From Local Concepts to Universals: Evaluating the Multicultural
  Understanding of Vision-Language Models",http://arxiv.org/abs/2407.00263v1,"Despite recent advancements in vision-language models, their performance
remains suboptimal on images from non-western cultures due to
underrepresentation in training datasets. Various benchmarks have been proposed
to test models' cultural inclusivity, but they have limited coverage of
cultures and do not adequately assess cultural diversity across universal as
well as culture-specific local concepts. To address these limitations, we
introduce the GlobalRG benchmark, comprising two challenging tasks: retrieval
across universals and cultural visual grounding. The former task entails
retrieving culturally diverse images for universal concepts from 50 countries,
while the latter aims at grounding culture-specific concepts within images from
15 countries. Our evaluation across a wide range of models reveals that the
performance varies significantly across cultures -- underscoring the necessity
for enhancing multicultural understanding in vision-language models.","Bhatia, Mehar  and
      Ravi, Sahithya  and
      Chinchure, Aditya  and
      Hwang, EunJeong  and
      Shwartz, Vered",2024,,https://aclanthology.org/2024.emnlp-main.385/,10.18653/v1/2024.emnlp-main.385,
Seeing Culture: A Benchmark for Visual Reasoning and Grounding,http://arxiv.org/abs/2509.16517v1,liu-etal-2021-visually_MaRVL,\cite{liu-etal-2021-visually_MaRVL},Visually Grounded Reasoning across Languages and Cultures,http://arxiv.org/abs/2109.13238v2,"The design of widespread vision-and-language datasets and pre-trained
encoders directly adopts, or draws inspiration from, the concepts and images of
ImageNet. While one can hardly overestimate how much this benchmark contributed
to progress in computer vision, it is mostly derived from lexical databases and
image queries in English, resulting in source material with a North American or
Western European bias. Therefore, we devise a new protocol to construct an
ImageNet-style hierarchy representative of more languages and cultures. In
particular, we let the selection of both concepts and images be entirely driven
by native speakers, rather than scraping them automatically. Specifically, we
focus on a typologically diverse set of languages, namely, Indonesian, Mandarin
Chinese, Swahili, Tamil, and Turkish. On top of the concepts and images
obtained through this new protocol, we create a multilingual dataset for
{M}ulticultur{a}l {R}easoning over {V}ision and {L}anguage (MaRVL) by eliciting
statements from native speaker annotators about pairs of images. The task
consists of discriminating whether each grounded statement is true or false. We
establish a series of baselines using state-of-the-art models and find that
their cross-lingual transfer performance lags dramatically behind supervised
performance in English. These results invite us to reassess the robustness and
accuracy of current state-of-the-art models beyond a narrow domain, but also
open up new exciting challenges for the development of truly multilingual and
multicultural systems.","Liu, Fangyu  and
      Bugliarello, Emanuele  and
      Ponti, Edoardo Maria  and
      Reddy, Siva  and
      Collier, Nigel  and
      Elliott, Desmond",2021,,https://aclanthology.org/2021.emnlp-main.818,,
Seeing Culture: A Benchmark for Visual Reasoning and Grounding,http://arxiv.org/abs/2509.16517v1,2024_seavqa,\cite{2024_seavqa},{SEA}-{VQA}: {S}outheast {A}sian Cultural Context Dataset For Visual Question Answering,,,"Urailertprasert, Norawit  and
      Limkonchotiwat, Peerat  and
      Suwajanakorn, Supasorn  and
      Nutanong, Sarana",2024,,https://aclanthology.org/2024.alvr-1.15/,10.18653/v1/2024.alvr-1.15,
Seeing Culture: A Benchmark for Visual Reasoning and Grounding,http://arxiv.org/abs/2509.16517v1,corr_kviscuit,\cite{corr_kviscuit},"Evaluating Visual and Cultural Interpretation: The K-Viscuit Benchmark
  with Human-VLM Collaboration",http://arxiv.org/abs/2406.16469v3,"To create culturally inclusive vision-language models (VLMs), developing a
benchmark that tests their ability to address culturally relevant questions is
essential. Existing approaches typically rely on human annotators, making the
process labor-intensive and creating a cognitive burden in generating diverse
questions. To address this, we propose a semi-automated framework for
constructing cultural VLM benchmarks, specifically targeting multiple-choice
QA. This framework combines human-VLM collaboration, where VLMs generate
questions based on guidelines, a small set of annotated examples, and relevant
knowledge, followed by a verification process by native speakers. We
demonstrate the effectiveness of this framework through the creation of
\texttt{K-Viscuit}, a dataset focused on Korean culture. Our experiments on
this dataset reveal that open-source models lag behind proprietary ones in
understanding Korean culture, highlighting key areas for improvement. We also
present a series of further analyses, including human evaluation, augmenting
VLMs with external knowledge, and the evaluation beyond multiple-choice QA. Our
dataset is available at https://huggingface.co/datasets/ddehun/k-viscuit.",Yujin Baek and ChaeHun Park and Jaeseok Kim and Yu-Jung Heo and Du-Seong Chang and Jaegul Choo,2024,,https://arxiv.org/abs/2406.16469,,
Seeing Culture: A Benchmark for Visual Reasoning and Grounding,http://arxiv.org/abs/2509.16517v1,li-etal-2024-foodieqa,\cite{li-etal-2024-foodieqa},{F}oodie{QA}: A Multimodal Dataset for Fine-Grained Understanding of {C}hinese Food Culture,,,"Li, Wenyan  and
      Zhang, Crystina  and
      Li, Jiaang  and
      Peng, Qiwei  and
      Tang, Raphael  and
      Zhou, Li  and
      Zhang, Weijia  and
      Hu, Guimin  and
      Yuan, Yifei  and
      S{\o}gaard, Anders  and
      Hershcovich, Daniel  and
      Elliott, Desmond",2024,,https://aclanthology.org/2024.emnlp-main.1063,,
