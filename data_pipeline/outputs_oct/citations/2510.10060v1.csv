parent_paper_title,parent_arxiv_link,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal
Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling,http://arxiv.org/abs/2510.10060v1,DBLP:conf/nips/VaswaniSPUJGKP17,\cite{DBLP:conf/nips/VaswaniSPUJGKP17},Attention Is All You Need,http://arxiv.org/abs/1706.03762v7,"The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks in an encoder-decoder configuration. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer, based
solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to be
superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014
English-to-German translation task, improving over the existing best results,
including ensembles by over 2 BLEU. On the WMT 2014 English-to-French
translation task, our model establishes a new single-model state-of-the-art
BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction
of the training costs of the best models from the literature. We show that the
Transformer generalizes well to other tasks by applying it successfully to
English constituency parsing both with large and limited training data.","Ashish Vaswani and
                  Noam Shazeer and
                  Niki Parmar and
                  Jakob Uszkoreit and
                  Llion Jones and
                  Aidan N. Gomez and
                  Lukasz Kaiser and
                  Illia Polosukhin",2017,,,,
Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling,http://arxiv.org/abs/2510.10060v1,radford2018improving,\cite{radford2018improving},Improving language understanding by generative pre-training,,,"Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya",2018,,,,
Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling,http://arxiv.org/abs/2510.10060v1,DBLP:conf/naacl/DevlinCLT19,\cite{DBLP:conf/naacl/DevlinCLT19},"BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding",http://arxiv.org/abs/1810.04805v2,"We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations from unlabeled text by jointly conditioning on
both left and right context in all layers. As a result, the pre-trained BERT
model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering
and language inference, without substantial task-specific architecture
modifications.
  BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI
accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering
Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).","Jacob Devlin and
                  Ming{-}Wei Chang and
                  Kenton Lee and
                  Kristina Toutanova",2019,,,,
Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling,http://arxiv.org/abs/2510.10060v1,DBLP:conf/iclr/DosovitskiyB0WZ21,\cite{DBLP:conf/iclr/DosovitskiyB0WZ21},"An Image is Worth 16x16 Words: Transformers for Image Recognition at
  Scale",http://arxiv.org/abs/2010.11929v2,"While the Transformer architecture has become the de-facto standard for
natural language processing tasks, its applications to computer vision remain
limited. In vision, attention is either applied in conjunction with
convolutional networks, or used to replace certain components of convolutional
networks while keeping their overall structure in place. We show that this
reliance on CNNs is not necessary and a pure transformer applied directly to
sequences of image patches can perform very well on image classification tasks.
When pre-trained on large amounts of data and transferred to multiple mid-sized
or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision
Transformer (ViT) attains excellent results compared to state-of-the-art
convolutional networks while requiring substantially fewer computational
resources to train.","Alexey Dosovitskiy and
                  Lucas Beyer and
                  Alexander Kolesnikov and
                  Dirk Weissenborn and
                  Xiaohua Zhai and
                  Thomas Unterthiner and
                  Mostafa Dehghani and
                  Matthias Minderer and
                  Georg Heigold and
                  Sylvain Gelly and
                  Jakob Uszkoreit and
                  Neil Houlsby",2021,,,,
Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling,http://arxiv.org/abs/2510.10060v1,DBLP:conf/iccv/LiuL00W0LG21,\cite{DBLP:conf/iccv/LiuL00W0LG21},Swin Transformer: Hierarchical Vision Transformer using Shifted Windows,http://arxiv.org/abs/2103.14030v2,"This paper presents a new vision Transformer, called Swin Transformer, that
capably serves as a general-purpose backbone for computer vision. Challenges in
adapting Transformer from language to vision arise from differences between the
two domains, such as large variations in the scale of visual entities and the
high resolution of pixels in images compared to words in text. To address these
differences, we propose a hierarchical Transformer whose representation is
computed with \textbf{S}hifted \textbf{win}dows. The shifted windowing scheme
brings greater efficiency by limiting self-attention computation to
non-overlapping local windows while also allowing for cross-window connection.
This hierarchical architecture has the flexibility to model at various scales
and has linear computational complexity with respect to image size. These
qualities of Swin Transformer make it compatible with a broad range of vision
tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and
dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP
on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its
performance surpasses the previous state-of-the-art by a large margin of +2.7
box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the
potential of Transformer-based models as vision backbones. The hierarchical
design and the shifted window approach also prove beneficial for all-MLP
architectures. The code and models are publicly available
at~\url{https://github.com/microsoft/Swin-Transformer}.","Ze Liu and
                  Yutong Lin and
                  Yue Cao and
                  Han Hu and
                  Yixuan Wei and
                  Zheng Zhang and
                  Stephen Lin and
                  Baining Guo",2021,,,,
Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling,http://arxiv.org/abs/2510.10060v1,DBLP:conf/icml/TouvronCDMSJ21,\cite{DBLP:conf/icml/TouvronCDMSJ21},"Training data-efficient image transformers & distillation through
  attention",http://arxiv.org/abs/2012.12877v2,"Recently, neural networks purely based on attention were shown to address
image understanding tasks such as image classification. However, these visual
transformers are pre-trained with hundreds of millions of images using an
expensive infrastructure, thereby limiting their adoption.
  In this work, we produce a competitive convolution-free transformer by
training on Imagenet only. We train them on a single computer in less than 3
days. Our reference vision transformer (86M parameters) achieves top-1 accuracy
of 83.1% (single-crop evaluation) on ImageNet with no external data.
  More importantly, we introduce a teacher-student strategy specific to
transformers. It relies on a distillation token ensuring that the student
learns from the teacher through attention. We show the interest of this
token-based distillation, especially when using a convnet as a teacher. This
leads us to report results competitive with convnets for both Imagenet (where
we obtain up to 85.2% accuracy) and when transferring to other tasks. We share
our code and models.","Hugo Touvron and
                  Matthieu Cord and
                  Matthijs Douze and
                  Francisco Massa and
                  Alexandre Sablayrolles and
                  Herv{\'{e}} J{\'{e}}gou",2021,,,,
Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling,http://arxiv.org/abs/2510.10060v1,DBLP:conf/naacl/ShawUV18,\cite{DBLP:conf/naacl/ShawUV18},Self-Attention with Relative Position Representations,http://arxiv.org/abs/1803.02155v2,"Relying entirely on an attention mechanism, the Transformer introduced by
Vaswani et al. (2017) achieves state-of-the-art results for machine
translation. In contrast to recurrent and convolutional neural networks, it
does not explicitly model relative or absolute position information in its
structure. Instead, it requires adding representations of absolute positions to
its inputs. In this work we present an alternative approach, extending the
self-attention mechanism to efficiently consider representations of the
relative positions, or distances between sequence elements. On the WMT 2014
English-to-German and English-to-French translation tasks, this approach yields
improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations,
respectively. Notably, we observe that combining relative and absolute position
representations yields no further improvement in translation quality. We
describe an efficient implementation of our method and cast it as an instance
of relation-aware self-attention mechanisms that can generalize to arbitrary
graph-labeled inputs.","Peter Shaw and
                  Jakob Uszkoreit and
                  Ashish Vaswani",2018,,,,
Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling,http://arxiv.org/abs/2510.10060v1,DBLP:conf/iclr/HuangVUSHSDHDE19,\cite{DBLP:conf/iclr/HuangVUSHSDHDE19},Music Transformer: Generating Music with Long-Term Structure,,,"Cheng{-}Zhi Anna Huang and
                  Ashish Vaswani and
                  Jakob Uszkoreit and
                  Ian Simon and
                  Curtis Hawthorne and
                  Noam Shazeer and
                  Andrew M. Dai and
                  Matthew D. Hoffman and
                  Monica Dinculescu and
                  Douglas Eck",2019,,,,
Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling,http://arxiv.org/abs/2510.10060v1,DBLP:conf/nips/ParmarRVBLS19,\cite{DBLP:conf/nips/ParmarRVBLS19},Stand-Alone Self-Attention in Vision Models,http://arxiv.org/abs/1906.05909v1,"Convolutions are a fundamental building block of modern computer vision
systems. Recent approaches have argued for going beyond convolutions in order
to capture long-range dependencies. These efforts focus on augmenting
convolutional models with content-based interactions, such as self-attention
and non-local means, to achieve gains on a number of vision tasks. The natural
question that arises is whether attention can be a stand-alone primitive for
vision models instead of serving as just an augmentation on top of
convolutions. In developing and testing a pure self-attention vision model, we
verify that self-attention can indeed be an effective stand-alone layer. A
simple procedure of replacing all instances of spatial convolutions with a form
of self-attention applied to ResNet model produces a fully self-attentional
model that outperforms the baseline on ImageNet classification with 12% fewer
FLOPS and 29% fewer parameters. On COCO object detection, a pure self-attention
model matches the mAP of a baseline RetinaNet while having 39% fewer FLOPS and
34% fewer parameters. Detailed ablation studies demonstrate that self-attention
is especially impactful when used in later layers. These results establish that
stand-alone self-attention is an important addition to the vision
practitioner's toolbox.","Niki Parmar and
                  Prajit Ramachandran and
                  Ashish Vaswani and
                  Irwan Bello and
                  Anselm Levskaya and
                  Jonathon Shlens",2019,,,,
Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling,http://arxiv.org/abs/2510.10060v1,DBLP:conf/acl/DaiYYCLS19,\cite{DBLP:conf/acl/DaiYYCLS19},Transformer-XL: Attentive Language Models beyond a Fixed-Length Context,,,"Zihang Dai and
                  Zhilin Yang and
                  Yiming Yang and
                  Jaime G. Carbonell and
                  Quoc Viet Le and
                  Ruslan Salakhutdinov",2019,,,,
Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling,http://arxiv.org/abs/2510.10060v1,DBLP:conf/emnlp/TsaiBYMS19,\cite{DBLP:conf/emnlp/TsaiBYMS19},"Transformer Dissection: An Unified Understanding for Transformer's
                  Attention via the Lens of Kernel",,,"Yao{-}Hung Hubert Tsai and
                  Shaojie Bai and
                  Makoto Yamada and
                  Louis{-}Philippe Morency and
                  Ruslan Salakhutdinov",2019,,,,
Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling,http://arxiv.org/abs/2510.10060v1,DBLP:journals/jmlr/RaffelSRLNMZLL20,\cite{DBLP:journals/jmlr/RaffelSRLNMZLL20},Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,,,"Colin Raffel and
                  Noam Shazeer and
                  Adam Roberts and
                  Katherine Lee and
                  Sharan Narang and
                  Michael Matena and
                  Yanqi Zhou and
                  Wei Li and
                  Peter J. Liu",2020,,,,J. Mach. Learn. Res.
Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling,http://arxiv.org/abs/2510.10060v1,DBLP:conf/nips/DaiLLT21,\cite{DBLP:conf/nips/DaiLLT21},CoAtNet: Marrying Convolution and Attention for All Data Sizes,http://arxiv.org/abs/2106.04803v2,"Transformers have attracted increasing interests in computer vision, but they
still fall behind state-of-the-art convolutional networks. In this work, we
show that while Transformers tend to have larger model capacity, their
generalization can be worse than convolutional networks due to the lack of the
right inductive bias. To effectively combine the strengths from both
architectures, we present CoAtNets(pronounced ""coat"" nets), a family of hybrid
models built from two key insights: (1) depthwise Convolution and
self-Attention can be naturally unified via simple relative attention; (2)
vertically stacking convolution layers and attention layers in a principled way
is surprisingly effective in improving generalization, capacity and efficiency.
Experiments show that our CoAtNets achieve state-of-the-art performance under
different resource constraints across various datasets: Without extra data,
CoAtNet achieves 86.0% ImageNet top-1 accuracy; When pre-trained with 13M
images from ImageNet-21K, our CoAtNet achieves 88.56% top-1 accuracy, matching
ViT-huge pre-trained with 300M images from JFT-300M while using 23x less data;
Notably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88% top-1
accuracy on ImageNet, establishing a new state-of-the-art result.","Zihang Dai and
                  Hanxiao Liu and
                  Quoc V. Le and
                  Mingxing Tan",2021,,,,
Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling,http://arxiv.org/abs/2510.10060v1,DBLP:conf/cvpr/SrinivasLPSAV21,\cite{DBLP:conf/cvpr/SrinivasLPSAV21},Bottleneck Transformers for Visual Recognition,http://arxiv.org/abs/2101.11605v2,"We present BoTNet, a conceptually simple yet powerful backbone architecture
that incorporates self-attention for multiple computer vision tasks including
image classification, object detection and instance segmentation. By just
replacing the spatial convolutions with global self-attention in the final
three bottleneck blocks of a ResNet and no other changes, our approach improves
upon the baselines significantly on instance segmentation and object detection
while also reducing the parameters, with minimal overhead in latency. Through
the design of BoTNet, we also point out how ResNet bottleneck blocks with
self-attention can be viewed as Transformer blocks. Without any bells and
whistles, BoTNet achieves 44.4% Mask AP and 49.7% Box AP on the COCO Instance
Segmentation benchmark using the Mask R-CNN framework; surpassing the previous
best published single model and single scale results of ResNeSt evaluated on
the COCO validation set. Finally, we present a simple adaptation of the BoTNet
design for image classification, resulting in models that achieve a strong
performance of 84.7% top-1 accuracy on the ImageNet benchmark while being up to
1.64x faster in compute time than the popular EfficientNet models on TPU-v3
hardware. We hope our simple and effective approach will serve as a strong
baseline for future research in self-attention models for vision","Aravind Srinivas and
                  Tsung{-}Yi Lin and
                  Niki Parmar and
                  Jonathon Shlens and
                  Pieter Abbeel and
                  Ashish Vaswani",2021,,,,
Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling,http://arxiv.org/abs/2510.10060v1,DBLP:conf/cvpr/VaswaniRSPHS21,\cite{DBLP:conf/cvpr/VaswaniRSPHS21},Scaling Local Self-Attention for Parameter Efficient Visual Backbones,http://arxiv.org/abs/2103.12731v3,"Self-attention has the promise of improving computer vision systems due to
parameter-independent scaling of receptive fields and content-dependent
interactions, in contrast to parameter-dependent scaling and
content-independent interactions of convolutions. Self-attention models have
recently been shown to have encouraging improvements on accuracy-parameter
trade-offs compared to baseline convolutional models such as ResNet-50. In this
work, we aim to develop self-attention models that can outperform not just the
canonical baseline models, but even the high-performing convolutional models.
We propose two extensions to self-attention that, in conjunction with a more
efficient implementation of self-attention, improve the speed, memory usage,
and accuracy of these models. We leverage these improvements to develop a new
self-attention model family, HaloNets, which reach state-of-the-art accuracies
on the parameter-limited setting of the ImageNet classification benchmark. In
preliminary transfer learning experiments, we find that HaloNet models
outperform much larger models and have better inference performance. On harder
tasks such as object detection and instance segmentation, our simple local
self-attention and convolutional hybrids show improvements over very strong
baselines. These results mark another step in demonstrating the efficacy of
self-attention models on settings traditionally dominated by convolutional
models.","Ashish Vaswani and
                  Prajit Ramachandran and
                  Aravind Srinivas and
                  Niki Parmar and
                  Blake A. Hechtman and
                  Jonathon Shlens",2021,,,,
Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling,http://arxiv.org/abs/2510.10060v1,DBLP:conf/icml/dAscoliTLMBS21,\cite{DBLP:conf/icml/dAscoliTLMBS21},"ConViT: Improving Vision Transformers with Soft Convolutional Inductive
  Biases",http://arxiv.org/abs/2103.10697v2,"Convolutional architectures have proven extremely successful for vision
tasks. Their hard inductive biases enable sample-efficient learning, but come
at the cost of a potentially lower performance ceiling. Vision Transformers
(ViTs) rely on more flexible self-attention layers, and have recently
outperformed CNNs for image classification. However, they require costly
pre-training on large external datasets or distillation from pre-trained
convolutional networks. In this paper, we ask the following question: is it
possible to combine the strengths of these two architectures while avoiding
their respective limitations? To this end, we introduce gated positional
self-attention (GPSA), a form of positional self-attention which can be
equipped with a ``soft"" convolutional inductive bias. We initialise the GPSA
layers to mimic the locality of convolutional layers, then give each attention
head the freedom to escape locality by adjusting a gating parameter regulating
the attention paid to position versus content information. The resulting
convolutional-like ViT architecture, ConViT, outperforms the DeiT on ImageNet,
while offering a much improved sample efficiency. We further investigate the
role of locality in learning by first quantifying how it is encouraged in
vanilla self-attention layers, then analysing how it is escaped in GPSA layers.
We conclude by presenting various ablations to better understand the success of
the ConViT. Our code and models are released publicly at
https://github.com/facebookresearch/convit.","St{\'{e}}phane d'Ascoli and
                  Hugo Touvron and
                  Matthew L. Leavitt and
                  Ari S. Morcos and
                  Giulio Biroli and
                  Levent Sagun",2021,,,,
Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling,http://arxiv.org/abs/2510.10060v1,DBLP:journals/ijon/SuALPBL24,\cite{DBLP:journals/ijon/SuALPBL24},RoFormer: Enhanced Transformer with Rotary Position Embedding,http://arxiv.org/abs/2104.09864v5,"Position encoding recently has shown effective in the transformer
architecture. It enables valuable supervision for dependency modeling between
elements at different positions of the sequence. In this paper, we first
investigate various methods to integrate positional information into the
learning process of transformer-based language models. Then, we propose a novel
method named Rotary Position Embedding(RoPE) to effectively leverage the
positional information. Specifically, the proposed RoPE encodes the absolute
position with a rotation matrix and meanwhile incorporates the explicit
relative position dependency in self-attention formulation. Notably, RoPE
enables valuable properties, including the flexibility of sequence length,
decaying inter-token dependency with increasing relative distances, and the
capability of equipping the linear self-attention with relative position
encoding. Finally, we evaluate the enhanced transformer with rotary position
embedding, also called RoFormer, on various long text classification benchmark
datasets. Our experiments show that it consistently overcomes its alternatives.
Furthermore, we provide a theoretical analysis to explain some experimental
results. RoFormer is already integrated into Huggingface:
\url{https://huggingface.co/docs/transformers/model_doc/roformer}.","Jianlin Su and
                  Murtadha H. M. Ahmed and
                  Yu Lu and
                  Shengfeng Pan and
                  Wen Bo and
                  Yunfeng Liu",2024,,,10.1016/J.NEUCOM.2023.127063,Neurocomputing
Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling,http://arxiv.org/abs/2510.10060v1,DBLP:journals/pieee/LeCunBBH98,\cite{DBLP:journals/pieee/LeCunBBH98},Gradient-based learning applied to document recognition,,,"Yann LeCun and
                  L{\'{e}}on Bottou and
                  Yoshua Bengio and
                  Patrick Haffner",1998,,,10.1109/5.726791,Proc. {IEEE}
Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling,http://arxiv.org/abs/2510.10060v1,DBLP:conf/nips/KrizhevskySH12,\cite{DBLP:conf/nips/KrizhevskySH12},ImageNet Classification with Deep Convolutional Neural Networks,,,"Alex Krizhevsky and
                  Ilya Sutskever and
                  Geoffrey E. Hinton",2012,,,,
Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling,http://arxiv.org/abs/2510.10060v1,DBLP:journals/corr/SimonyanZ14a,\cite{DBLP:journals/corr/SimonyanZ14a},Very Deep Convolutional Networks for Large-Scale Image Recognition,,,"Karen Simonyan and
                  Andrew Zisserman",2015,,,,
Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling,http://arxiv.org/abs/2510.10060v1,DBLP:conf/cvpr/SzegedyLJSRAEVR15,\cite{DBLP:conf/cvpr/SzegedyLJSRAEVR15},Going Deeper with Convolutions,http://arxiv.org/abs/1409.4842v1,"We propose a deep convolutional neural network architecture codenamed
""Inception"", which was responsible for setting the new state of the art for
classification and detection in the ImageNet Large-Scale Visual Recognition
Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the
improved utilization of the computing resources inside the network. This was
achieved by a carefully crafted design that allows for increasing the depth and
width of the network while keeping the computational budget constant. To
optimize quality, the architectural decisions were based on the Hebbian
principle and the intuition of multi-scale processing. One particular
incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22
layers deep network, the quality of which is assessed in the context of
classification and detection.","Christian Szegedy and
                  Wei Liu and
                  Yangqing Jia and
                  Pierre Sermanet and
                  Scott E. Reed and
                  Dragomir Anguelov and
                  Dumitru Erhan and
                  Vincent Vanhoucke and
                  Andrew Rabinovich",2015,,,,
Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling,http://arxiv.org/abs/2510.10060v1,DBLP:conf/cvpr/HeZRS16,\cite{DBLP:conf/cvpr/HeZRS16},Deep Residual Learning for Image Recognition,http://arxiv.org/abs/1512.03385v1,"Deeper neural networks are more difficult to train. We present a residual
learning framework to ease the training of networks that are substantially
deeper than those used previously. We explicitly reformulate the layers as
learning residual functions with reference to the layer inputs, instead of
learning unreferenced functions. We provide comprehensive empirical evidence
showing that these residual networks are easier to optimize, and can gain
accuracy from considerably increased depth. On the ImageNet dataset we evaluate
residual nets with a depth of up to 152 layers---8x deeper than VGG nets but
still having lower complexity. An ensemble of these residual nets achieves
3.57% error on the ImageNet test set. This result won the 1st place on the
ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100
and 1000 layers.
  The depth of representations is of central importance for many visual
recognition tasks. Solely due to our extremely deep representations, we obtain
a 28% relative improvement on the COCO object detection dataset. Deep residual
nets are foundations of our submissions to ILSVRC & COCO 2015 competitions,
where we also won the 1st places on the tasks of ImageNet detection, ImageNet
localization, COCO detection, and COCO segmentation.","Kaiming He and
                  Xiangyu Zhang and
                  Shaoqing Ren and
                  Jian Sun",2016,,,,
Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling,http://arxiv.org/abs/2510.10060v1,DBLP:conf/interspeech/GulatiQCPZYHWZW20,\cite{DBLP:conf/interspeech/GulatiQCPZYHWZW20},Conformer: Convolution-augmented Transformer for Speech Recognition,http://arxiv.org/abs/2005.08100v1,"Recently Transformer and Convolution neural network (CNN) based models have
shown promising results in Automatic Speech Recognition (ASR), outperforming
Recurrent neural networks (RNNs). Transformer models are good at capturing
content-based global interactions, while CNNs exploit local features
effectively. In this work, we achieve the best of both worlds by studying how
to combine convolution neural networks and transformers to model both local and
global dependencies of an audio sequence in a parameter-efficient way. To this
regard, we propose the convolution-augmented transformer for speech
recognition, named Conformer. Conformer significantly outperforms the previous
Transformer and CNN based models achieving state-of-the-art accuracies. On the
widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without
using a language model and 1.9%/3.9% with an external language model on
test/testother. We also observe competitive performance of 2.7%/6.3% with a
small model of only 10M parameters.","Anmol Gulati and
                  James Qin and
                  Chung{-}Cheng Chiu and
                  Niki Parmar and
                  Yu Zhang and
                  Jiahui Yu and
                  Wei Han and
                  Shibo Wang and
                  Zhengdong Zhang and
                  Yonghui Wu and
                  Ruoming Pang",2020,,,,
Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling,http://arxiv.org/abs/2510.10060v1,DBLP:conf/iccv/YuanG0ZYW21,\cite{DBLP:conf/iccv/YuanG0ZYW21},Incorporating Convolution Designs into Visual Transformers,http://arxiv.org/abs/2103.11816v2,"Motivated by the success of Transformers in natural language processing (NLP)
tasks, there emerge some attempts (e.g., ViT and DeiT) to apply Transformers to
the vision domain. However, pure Transformer architectures often require a
large amount of training data or extra supervision to obtain comparable
performance with convolutional neural networks (CNNs). To overcome these
limitations, we analyze the potential drawbacks when directly borrowing
Transformer architectures from NLP. Then we propose a new
\textbf{Convolution-enhanced image Transformer (CeiT)} which combines the
advantages of CNNs in extracting low-level features, strengthening locality,
and the advantages of Transformers in establishing long-range dependencies.
Three modifications are made to the original Transformer: \textbf{1)} instead
of the straightforward tokenization from raw input images, we design an
\textbf{Image-to-Tokens (I2T)} module that extracts patches from generated
low-level features; \textbf{2)} the feed-froward network in each encoder block
is replaced with a \textbf{Locally-enhanced Feed-Forward (LeFF)} layer that
promotes the correlation among neighboring tokens in the spatial dimension;
\textbf{3)} a \textbf{Layer-wise Class token Attention (LCA)} is attached at
the top of the Transformer that utilizes the multi-level representations.
  Experimental results on ImageNet and seven downstream tasks show the
effectiveness and generalization ability of CeiT compared with previous
Transformers and state-of-the-art CNNs, without requiring a large amount of
training data and extra CNN teachers. Besides, CeiT models also demonstrate
better convergence with $3\times$ fewer training iterations, which can reduce
the training cost significantly\footnote{Code and models will be released upon
acceptance.}.","Kun Yuan and
                  Shaopeng Guo and
                  Ziwei Liu and
                  Aojun Zhou and
                  Fengwei Yu and
                  Wei Wu",2021,,,,
