parent_paper_title,parent_arxiv_link,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal
Context-Driven Performance Modeling for Causal Inference Operators on Neural Processing Units,http://arxiv.org/abs/2509.25155v1,zhang2024edgeshardefficientllminference,\cite{zhang2024edgeshardefficientllminference},EdgeShard: Efficient LLM Inference via Collaborative Edge Computing,http://arxiv.org/abs/2405.14371v1,"Large language models (LLMs) have shown great potential in natural language
processing and content generation. However, current LLMs heavily rely on cloud
computing, leading to prolonged latency, high bandwidth cost, and privacy
concerns. Edge computing is promising to address such concerns by deploying
LLMs on edge devices, closer to data sources. Some works try to leverage model
quantization to reduce the model size to fit the resource-constraint edge
devices, but they lead to accuracy loss. Other works use cloud-edge
collaboration, suffering from unstable network connections. In this work, we
leverage collaborative edge computing to facilitate the collaboration among
edge devices and cloud servers for jointly performing efficient LLM inference.
We propose a general framework to partition the LLM model into shards and
deploy on distributed devices. To achieve efficient LLM inference, we formulate
an adaptive joint device selection and model partition problem and design an
efficient dynamic programming algorithm to optimize the inference latency and
throughput, respectively. Experiments of Llama2 serial models on a
heterogeneous physical prototype demonstrate that EdgeShard achieves up to 50%
latency reduction and 2x throughput improvement over baseline methods.",Mingjin Zhang and Jiannong Cao and Xiaoming Shen and Zeyang Cui,2024,,https://arxiv.org/abs/2405.14371,,
Context-Driven Performance Modeling for Causal Inference Operators on Neural Processing Units,http://arxiv.org/abs/2509.25155v1,10.1145/3700410.3702126,\cite{10.1145/3700410.3702126},MNN-LLM: A Generic Inference Engine for Fast Large Language Model Deployment on Mobile Devices,,,"Wang, Zhaode and Yang, Jingbang and Qian, Xinyu and Xing, Shiwen and Jiang, Xiaotang and Lv, Chengfei and Zhang, Shengyu",2024,,,,
Context-Driven Performance Modeling for Causal Inference Operators on Neural Processing Units,http://arxiv.org/abs/2509.25155v1,llamacpp,\cite{llamacpp},Llama.cpp,,,,,,https://github.com/ggerganov/llama.cpp,,
Context-Driven Performance Modeling for Causal Inference Operators on Neural Processing Units,http://arxiv.org/abs/2509.25155v1,haris2024designing,\cite{haris2024designing},Designing Efficient LLM Accelerators for Edge Devices,http://arxiv.org/abs/2408.00462v1,"The increase in open-source availability of Large Language Models (LLMs) has
enabled users to deploy them on more and more resource-constrained edge devices
to reduce reliance on network connections and provide more privacy. However,
the high computation and memory demands of LLMs make their execution on
resource-constrained edge devices challenging and inefficient. To address this
issue, designing new and efficient edge accelerators for LLM inference is
crucial. FPGA-based accelerators are ideal for LLM acceleration due to their
reconfigurability, as they enable model-specific optimizations and higher
performance per watt. However, creating and integrating FPGA-based accelerators
for LLMs (particularly on edge devices) has proven challenging, mainly due to
the limited hardware design flows for LLMs in existing FPGA platforms.
  To tackle this issue, in this paper we first propose a new design platform,
named SECDA-LLM, that utilizes the SECDA methodology to streamline the process
of designing, integrating, and deploying efficient FPGA-based LLM accelerators
for the llama.cpp inference framework. We then demonstrate, through a case
study, the potential benefits of SECDA-LLM by creating a new MatMul accelerator
that supports block floating point quantized operations for LLMs. Our initial
accelerator design, deployed on the PYNQ-Z1 board, reduces latency 1.7 seconds
per token or ~2 seconds per word) by 11x over the dual-core Arm NEON-based CPU
execution for the TinyLlama model.","Haris, Jude and Saha, Rappy and Hu, Wenhao and Cano, Jos{\'e}",2024,,,,arXiv preprint arXiv:2408.00462
Context-Driven Performance Modeling for Causal Inference Operators on Neural Processing Units,http://arxiv.org/abs/2509.25155v1,xu2025fast,\cite{xu2025fast},Fast On-device LLM Inference with NPUs,http://arxiv.org/abs/2407.05858v2,"On-device inference for Large Language Models (LLMs), driven by increasing
privacy concerns and advancements of mobile-sized models, has gained
significant interest. However, even mobile-sized LLMs (e.g., Gemma-2B)
encounter unacceptably high inference latency, often bottlenecked by the
prefill stage in tasks like screen UI understanding.
  We present llm.npu, the first LLM inference system utilizing on-device Neural
Processing Unit (NPU) offloading to reduce prefill latency. llm.npu enhances
NPU offloading efficiency by re-constructing the prompt and model in three
levels: (1) At prompt level, it divides variable-length prompts into multiple
fixed-sized chunks while maintaining data dependencies; (2) At tensor level, it
identifies and extracts significant outliers to run on the CPU/GPU in parallel
with minimal overhead; (3) At block level, it schedules Transformer blocks in
an out-of-order manner to the CPU/GPU and NPU based on their hardware affinity
and sensitivity to accuracy. Compared to competitive baselines, llm.npu
achieves 22.4x faster prefill speed and 30.7$\times$ energy savings on average,
and up to 32.8x speedup in an end-to-end real-world application. For the first
time, llm.npu achieves more than 1,000 tokens/sec prefilling for a
billion-sized model.","Xu, Daliang and Zhang, Hao and Yang, Liming and Liu, Ruiqi and Huang, Gang and Xu, Mengwei and Liu, Xuanzhe",2025,,,,
Context-Driven Performance Modeling for Causal Inference Operators on Neural Processing Units,http://arxiv.org/abs/2509.25155v1,zhu2025edge,\cite{zhu2025edge},Edge-side NPU inference optimization: Adaptation research of multimodal large models on qualcomm platforms,,,"Zhu, Yajie and Lu, Hongtao",2025,,,,Intelligent Data Analysis
Context-Driven Performance Modeling for Causal Inference Operators on Neural Processing Units,http://arxiv.org/abs/2509.25155v1,das2025xamba,\cite{das2025xamba},"XAMBA: Enabling Efficient State Space Models on Resource-Constrained
  Neural Processing Units",http://arxiv.org/abs/2502.06924v4,"State-Space Models (SSMs) have emerged as efficient alternatives to
transformers for sequential data tasks, offering linear or near-linear
scalability with sequence length, making them ideal for long-sequence
applications in NLP, vision, and edge AI, including real-time transcription,
translation, and contextual search. These applications require lightweight,
high-performance models for deployment on resource-constrained devices like
laptops and PCs. Designing specialized accelerators for every emerging neural
network is costly and impractical; instead, optimizing models for existing NPUs
in AI PCs provides a scalable solution. To this end, we propose XAMBA, the
first framework to enable and optimize SSMs on commercial off-the-shelf (COTS)
state-of-the-art (SOTA) NPUs. XAMBA follows a three-step methodology: (1)
enabling SSMs on NPUs, (2) optimizing performance to meet KPI requirements, and
(3) trading accuracy for additional performance gains. After enabling SSMs on
NPUs, XAMBA mitigates key bottlenecks using CumBA and ReduBA, replacing
sequential CumSum and ReduceSum operations with matrix-based computations,
significantly improving execution speed and memory efficiency. Additionally,
ActiBA enhances performance by approximating expensive activation functions
(e.g., Swish, Softplus) using piecewise linear mappings, reducing latency with
minimal accuracy loss. Evaluations on an Intel Core Ultra Series 2 AI PC show
that XAMBA achieves up to 4.8X speed-up over the baseline. Our implementation
is available at https://github.com/arghadippurdue/XAMBA.",Arghadip Das and Arnab Raha and Shamik Kundu and Soumendu Kumar Ghosh and Deepak Mathaikutty and Vijay Raghunathan,2025,,https://arxiv.org/abs/2502.06924,,
Context-Driven Performance Modeling for Causal Inference Operators on Neural Processing Units,http://arxiv.org/abs/2509.25155v1,aalishah2025mambalitesr,\cite{aalishah2025mambalitesr},"MambaLiteSR: Image Super-Resolution with Low-Rank Mamba using Knowledge
  Distillation",http://arxiv.org/abs/2502.14090v1,"Generative Artificial Intelligence (AI) has gained significant attention in
recent years, revolutionizing various applications across industries. Among
these, advanced vision models for image super-resolution are in high demand,
particularly for deployment on edge devices where real-time processing is
crucial. However, deploying such models on edge devices is challenging due to
limited computing power and memory. In this paper, we present MambaLiteSR, a
novel lightweight image Super-Resolution (SR) model that utilizes the
architecture of Vision Mamba. It integrates State Space Blocks and a
reconstruction module for efficient feature extraction. To optimize efficiency
without affecting performance, MambaLiteSR employs knowledge distillation to
transfer key insights from a larger Mamba-based teacher model to a smaller
student model via hyperparameter tuning. Through mathematical analysis of model
parameters and their impact on PSNR, we identify key factors and adjust them
accordingly. Our comprehensive evaluation shows that MambaLiteSR outperforms
state-of-the-art edge SR methods by reducing power consumption while
maintaining competitive PSNR and SSIM scores across benchmark datasets. It also
reduces power usage during training via low-rank approximation. Moreover,
MambaLiteSR reduces parameters with minimal performance loss, enabling
efficient deployment of generative AI models on resource-constrained devices.
Deployment on the embedded NVIDIA Jetson Orin Nano confirms the superior
balance of MambaLiteSR size, latency, and efficiency. Experiments show that
MambaLiteSR achieves performance comparable to both the baseline and other edge
models while using 15% fewer parameters. It also improves power consumption by
up to 58% compared to state-of-the-art SR edge models, all while maintaining
low energy use during training.","Aalishah, Romina and Navardi, Mozhgan and Mohsenin, Tinoosh",2025,,,,
