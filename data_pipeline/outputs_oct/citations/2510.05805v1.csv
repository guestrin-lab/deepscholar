parent_paper_title,parent_arxiv_link,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal
Improving Clinical Dataset Condensation with Mode Connectivity-based Trajectory Surrogates,http://arxiv.org/abs/2510.05805v1,wang2018dataset,\cite{wang2018dataset},Dataset Distillation,http://arxiv.org/abs/1811.10959v3,"Model distillation aims to distill the knowledge of a complex model into a
simpler one. In this paper, we consider an alternative formulation called
dataset distillation: we keep the model fixed and instead attempt to distill
the knowledge from a large training dataset into a small one. The idea is to
synthesize a small number of data points that do not need to come from the
correct data distribution, but will, when given to the learning algorithm as
training data, approximate the model trained on the original data. For example,
we show that it is possible to compress 60,000 MNIST training images into just
10 synthetic distilled images (one per class) and achieve close to original
performance with only a few gradient descent steps, given a fixed network
initialization. We evaluate our method in various initialization settings and
with different learning objectives. Experiments on multiple datasets show the
advantage of our approach compared to alternative methods.","Wang, Tongzhou and Zhu, Jun-Yan and Torralba, Antonio and Efros, Alexei A.",2018,,,,arXiv preprint arXiv:1811.10959
Improving Clinical Dataset Condensation with Mode Connectivity-based Trajectory Surrogates,http://arxiv.org/abs/2510.05805v1,zhao2021dataset,\cite{zhao2021dataset},Dataset Condensation with Gradient Matching,http://arxiv.org/abs/2006.05929v3,"As the state-of-the-art machine learning methods in many fields rely on
larger datasets, storing datasets and training models on them become
significantly more expensive. This paper proposes a training set synthesis
technique for data-efficient learning, called Dataset Condensation, that learns
to condense large dataset into a small set of informative synthetic samples for
training deep neural networks from scratch. We formulate this goal as a
gradient matching problem between the gradients of deep neural network weights
that are trained on the original and our synthetic data. We rigorously evaluate
its performance in several computer vision benchmarks and demonstrate that it
significantly outperforms the state-of-the-art methods. Finally we explore the
use of our method in continual learning and neural architecture search and
report promising gains when limited memory and computations are available.","Zhao, Bo and Bilen, Hakan",2021,,,,
Improving Clinical Dataset Condensation with Mode Connectivity-based Trajectory Surrogates,http://arxiv.org/abs/2510.05805v1,sucholutsky2019soft,\cite{sucholutsky2019soft},Soft-Label Dataset Distillation and Text Dataset Distillation,http://arxiv.org/abs/1910.02551v3,"Dataset distillation is a method for reducing dataset sizes by learning a
small number of synthetic samples containing all the information of a large
dataset. This has several benefits like speeding up model training, reducing
energy consumption, and reducing required storage space. Currently, each
synthetic sample is assigned a single `hard' label, and also, dataset
distillation can currently only be used with image data.
  We propose to simultaneously distill both images and their labels, thus
assigning each synthetic sample a `soft' label (a distribution of labels). Our
algorithm increases accuracy by 2-4% over the original algorithm for several
image classification tasks. Using `soft' labels also enables distilled datasets
to consist of fewer samples than there are classes as each sample can encode
information for multiple classes. For example, training a LeNet model with 10
distilled images (one per class) results in over 96% accuracy on MNIST, and
almost 92% accuracy when trained on just 5 distilled images.
  We also extend the dataset distillation algorithm to distill sequential
datasets including texts. We demonstrate that text distillation outperforms
other methods across multiple datasets. For example, models attain almost their
original accuracy on the IMDB sentiment analysis task using just 20 distilled
sentences.
  Our code can be found at
$\href{https://github.com/ilia10000/dataset-distillation}{\text{https://github.com/ilia10000/dataset-distillation}}$.","Sucholutsky, Ilia and Schonlau, Matthias",2019,,,,arXiv preprint arXiv:1910.02551
Improving Clinical Dataset Condensation with Mode Connectivity-based Trajectory Surrogates,http://arxiv.org/abs/2510.05805v1,bohdal2020flexible,\cite{bohdal2020flexible},Flexible Dataset Distillation: Learn Labels Instead of Images,http://arxiv.org/abs/2006.08572v3,"We study the problem of dataset distillation - creating a small set of
synthetic examples capable of training a good model. In particular, we study
the problem of label distillation - creating synthetic labels for a small set
of real images, and show it to be more effective than the prior image-based
approach to dataset distillation. Methodologically, we introduce a more robust
and flexible meta-learning algorithm for distillation, as well as an effective
first-order strategy based on convex optimization layers. Distilling labels
with our new algorithm leads to improved results over prior image-based
distillation. More importantly, it leads to clear improvements in flexibility
of the distilled dataset in terms of compatibility with off-the-shelf
optimizers and diverse neural architectures. Interestingly, label distillation
can also be applied across datasets, for example enabling learning Japanese
character recognition by training only on synthetically labeled English
letters.","Bohdal, Ondrej and Yang, Yongxin and Hospedales, Timothy",2020,,,,arXiv preprint arXiv:2006.08572
Improving Clinical Dataset Condensation with Mode Connectivity-based Trajectory Surrogates,http://arxiv.org/abs/2510.05805v1,nguyen2021dataset,\cite{nguyen2021dataset},Dataset Meta-Learning from Kernel Ridge-Regression,http://arxiv.org/abs/2011.00050v3,"One of the most fundamental aspects of any machine learning algorithm is the
training data used by the algorithm. We introduce the novel concept of
$\epsilon$-approximation of datasets, obtaining datasets which are much smaller
than or are significant corruptions of the original training data while
maintaining similar model performance. We introduce a meta-learning algorithm
called Kernel Inducing Points (KIP) for obtaining such remarkable datasets,
inspired by the recent developments in the correspondence between
infinitely-wide neural networks and kernel ridge-regression (KRR). For KRR
tasks, we demonstrate that KIP can compress datasets by one or two orders of
magnitude, significantly improving previous dataset distillation and subset
selection methods while obtaining state of the art results for MNIST and
CIFAR-10 classification. Furthermore, our KIP-learned datasets are transferable
to the training of finite-width neural networks even beyond the lazy-training
regime, which leads to state of the art results for neural network dataset
distillation with potential applications to privacy-preservation.","Nguyen, Timothy and Chen, Zhourong and Lee, Jaehoon",2021,,,,
Improving Clinical Dataset Condensation with Mode Connectivity-based Trajectory Surrogates,http://arxiv.org/abs/2510.05805v1,nguyen2022dataset,\cite{nguyen2022dataset},Dataset Distillation with Infinitely Wide Convolutional Networks,http://arxiv.org/abs/2107.13034v3,"The effectiveness of machine learning algorithms arises from being able to
extract useful features from large amounts of data. As model and dataset sizes
increase, dataset distillation methods that compress large datasets into
significantly smaller yet highly performant ones will become valuable in terms
of training efficiency and useful feature extraction. To that end, we apply a
novel distributed kernel based meta-learning framework to achieve
state-of-the-art results for dataset distillation using infinitely wide
convolutional neural networks. For instance, using only 10 datapoints (0.02% of
original dataset), we obtain over 65% test accuracy on CIFAR-10 image
classification task, a dramatic improvement over the previous best test
accuracy of 40%. Our state-of-the-art results extend across many other settings
for MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100, and SVHN. Furthermore, we
perform some preliminary analyses of our distilled datasets to shed light on
how they differ from naturally occurring data.","Nguyen, Timothy and Novak, Roman and Xiao, Lechao and Lee, Jaehoon",2022,,,,
Improving Clinical Dataset Condensation with Mode Connectivity-based Trajectory Surrogates,http://arxiv.org/abs/2510.05805v1,jacot2018neural,\cite{jacot2018neural},Neural Tangent Kernel: Convergence and Generalization in Neural Networks,http://arxiv.org/abs/1806.07572v4,"At initialization, artificial neural networks (ANNs) are equivalent to
Gaussian processes in the infinite-width limit, thus connecting them to kernel
methods. We prove that the evolution of an ANN during training can also be
described by a kernel: during gradient descent on the parameters of an ANN, the
network function $f_\theta$ (which maps input vectors to output vectors)
follows the kernel gradient of the functional cost (which is convex, in
contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel
(NTK). This kernel is central to describe the generalization features of ANNs.
While the NTK is random at initialization and varies during training, in the
infinite-width limit it converges to an explicit limiting kernel and it stays
constant during training. This makes it possible to study the training of ANNs
in function space instead of parameter space. Convergence of the training can
then be related to the positive-definiteness of the limiting NTK. We prove the
positive-definiteness of the limiting NTK when the data is supported on the
sphere and the non-linearity is non-polynomial. We then focus on the setting of
least-squares regression and show that in the infinite-width limit, the network
function $f_\theta$ follows a linear differential equation during training. The
convergence is fastest along the largest kernel principal components of the
input data with respect to the NTK, hence suggesting a theoretical motivation
for early stopping. Finally we study the NTK numerically, observe its behavior
for wide networks, and compare it to the infinite-width limit.","Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment",2018,,,,Advances in Neural Information Processing Systems
Improving Clinical Dataset Condensation with Mode Connectivity-based Trajectory Surrogates,http://arxiv.org/abs/2510.05805v1,zhao2023dataset,\cite{zhao2023dataset},Dataset Condensation with Distribution Matching,http://arxiv.org/abs/2110.04181v3,"Computational cost of training state-of-the-art deep models in many learning
problems is rapidly increasing due to more sophisticated models and larger
datasets. A recent promising direction for reducing training cost is dataset
condensation that aims to replace the original large training set with a
significantly smaller learned synthetic set while preserving the original
information. While training deep models on the small set of condensed images
can be extremely fast, their synthesis remains computationally expensive due to
the complex bi-level optimization and second-order derivative computation. In
this work, we propose a simple yet effective method that synthesizes condensed
images by matching feature distributions of the synthetic and original training
images in many sampled embedding spaces. Our method significantly reduces the
synthesis cost while achieving comparable or better performance. Thanks to its
efficiency, we apply our method to more realistic and larger datasets with
sophisticated neural architectures and obtain a significant performance boost.
We also show promising practical benefits of our method in continual learning
and neural architecture search.","Zhao, Bo and Bilen, Hakan",2023,,,,
Improving Clinical Dataset Condensation with Mode Connectivity-based Trajectory Surrogates,http://arxiv.org/abs/2510.05805v1,wang2022cafe,\cite{wang2022cafe},CAFE: Learning to Condense Dataset by Aligning Features,http://arxiv.org/abs/2203.01531v2,"Dataset condensation aims at reducing the network training effort through
condensing a cumbersome training set into a compact synthetic one.
State-of-the-art approaches largely rely on learning the synthetic data by
matching the gradients between the real and synthetic data batches. Despite the
intuitive motivation and promising results, such gradient-based methods, by
nature, easily overfit to a biased set of samples that produce dominant
gradients, and thus lack global supervision of data distribution. In this
paper, we propose a novel scheme to Condense dataset by Aligning FEatures
(CAFE), which explicitly attempts to preserve the real-feature distribution as
well as the discriminant power of the resulting synthetic set, lending itself
to strong generalization capability to various architectures. At the heart of
our approach is an effective strategy to align features from the real and
synthetic data across various scales, while accounting for the classification
of real samples. Our scheme is further backed up by a novel dynamic bi-level
optimization, which adaptively adjusts parameter updates to prevent
over-/under-fitting. We validate the proposed CAFE across various datasets, and
demonstrate that it generally outperforms the state of the art: on the SVHN
dataset, for example, the performance gain is up to 11%. Extensive experiments
and analyses verify the effectiveness and necessity of proposed designs.","Wang, Kai and Zhao, Bo and Peng, Xiangyu and Zhu, Zheng and Yang, Shuo and Wang, Shuo and Huang, Guan and Bilen, Hakan and Wang, Xinchao and You, Yang",2022,,,,
Improving Clinical Dataset Condensation with Mode Connectivity-based Trajectory Surrogates,http://arxiv.org/abs/2510.05805v1,zhao2023improved,\cite{zhao2023improved},Improved Distribution Matching for Dataset Condensation,http://arxiv.org/abs/2307.09742v1,"Dataset Condensation aims to condense a large dataset into a smaller one
while maintaining its ability to train a well-performing model, thus reducing
the storage cost and training effort in deep learning applications. However,
conventional dataset condensation methods are optimization-oriented and
condense the dataset by performing gradient or parameter matching during model
optimization, which is computationally intensive even on small datasets and
models. In this paper, we propose a novel dataset condensation method based on
distribution matching, which is more efficient and promising. Specifically, we
identify two important shortcomings of naive distribution matching (i.e.,
imbalanced feature numbers and unvalidated embeddings for distance computation)
and address them with three novel techniques (i.e., partitioning and expansion
augmentation, efficient and enriched model sampling, and class-aware
distribution regularization). Our simple yet effective method outperforms most
previous optimization-oriented methods with much fewer computational resources,
thereby scaling data condensation to larger datasets and models. Extensive
experiments demonstrate the effectiveness of our method. Codes are available at
https://github.com/uitrbn/IDM","Zhao, Ganlong and Li, Guanbin and Qin, Yipeng and Yu, Yizhou",2023,,,,arXiv preprint arXiv:2307.09742
Improving Clinical Dataset Condensation with Mode Connectivity-based Trajectory Surrogates,http://arxiv.org/abs/2510.05805v1,zhang2024m3d,\cite{zhang2024m3d},M3D: Dataset Condensation by Minimizing Maximum Mean Discrepancy,http://arxiv.org/abs/2312.15927v3,"Training state-of-the-art (SOTA) deep models often requires extensive data,
resulting in substantial training and storage costs. To address these
challenges, dataset condensation has been developed to learn a small synthetic
set that preserves essential information from the original large-scale dataset.
Nowadays, optimization-oriented methods have been the primary method in the
field of dataset condensation for achieving SOTA results. However, the bi-level
optimization process hinders the practical application of such methods to
realistic and larger datasets. To enhance condensation efficiency, previous
works proposed Distribution-Matching (DM) as an alternative, which
significantly reduces the condensation cost. Nonetheless, current DM-based
methods still yield less comparable results to SOTA optimization-oriented
methods. In this paper, we argue that existing DM-based methods overlook the
higher-order alignment of the distributions, which may lead to sub-optimal
matching results. Inspired by this, we present a novel DM-based method named
M3D for dataset condensation by Minimizing the Maximum Mean Discrepancy between
feature representations of the synthetic and real images. By embedding their
distributions in a reproducing kernel Hilbert space, we align all orders of
moments of the distributions of real and synthetic images, resulting in a more
generalized condensed set. Notably, our method even surpasses the SOTA
optimization-oriented method IDC on the high-resolution ImageNet dataset.
Extensive analysis is conducted to verify the effectiveness of the proposed
method. Source codes are available at https://github.com/Hansong-Zhang/M3D.","Zhang, Hansong and Li, Shikun and Wang, Pengju and Zeng, Dan and Ge, Shiming",2024,,,,arXiv preprint arXiv:2312.15927
Improving Clinical Dataset Condensation with Mode Connectivity-based Trajectory Surrogates,http://arxiv.org/abs/2510.05805v1,yin2023sre2l,\cite{yin2023sre2l},"Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale
  From A New Perspective",http://arxiv.org/abs/2306.13092v3,"We present a new dataset condensation framework termed Squeeze, Recover and
Relabel (SRe$^2$L) that decouples the bilevel optimization of model and
synthetic data during training, to handle varying scales of datasets, model
architectures and image resolutions for efficient dataset condensation. The
proposed method demonstrates flexibility across diverse dataset scales and
exhibits multiple advantages in terms of arbitrary resolutions of synthesized
images, low training cost and memory consumption with high-resolution
synthesis, and the ability to scale up to arbitrary evaluation network
architectures. Extensive experiments are conducted on Tiny-ImageNet and full
ImageNet-1K datasets. Under 50 IPC, our approach achieves the highest 42.5% and
60.8% validation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming all
previous state-of-the-art methods by margins of 14.5% and 32.9%, respectively.
Our approach also surpasses MTT in terms of speed by approximately 52$\times$
(ConvNet-4) and 16$\times$ (ResNet-18) faster with less memory consumption of
11.6$\times$ and 6.4$\times$ during data synthesis. Our code and condensed
datasets of 50, 200 IPC with 4K recovery budget are available at
https://github.com/VILA-Lab/SRe2L.","Yin, Zeyuan and Xing, Eric and Shen, Zhiqiang",2023,,,,
Improving Clinical Dataset Condensation with Mode Connectivity-based Trajectory Surrogates,http://arxiv.org/abs/2510.05805v1,sun2023rded,\cite{sun2023rded},"On the Diversity and Realism of Distilled Dataset: An Efficient Dataset
  Distillation Paradigm",http://arxiv.org/abs/2312.03526v2,"Contemporary machine learning requires training large neural networks on
massive datasets and thus faces the challenges of high computational demands.
Dataset distillation, as a recent emerging strategy, aims to compress
real-world datasets for efficient training. However, this line of research
currently struggle with large-scale and high-resolution datasets, hindering its
practicality and feasibility. To this end, we re-examine the existing dataset
distillation methods and identify three properties required for large-scale
real-world applications, namely, realism, diversity, and efficiency. As a
remedy, we propose RDED, a novel computationally-efficient yet effective data
distillation paradigm, to enable both diversity and realism of the distilled
data. Extensive empirical results over various neural architectures and
datasets demonstrate the advancement of RDED: we can distill the full
ImageNet-1K to a small dataset comprising 10 images per class within 7 minutes,
achieving a notable 42% top-1 accuracy with ResNet-18 on a single RTX-4090 GPU
(while the SOTA only achieves 21% but requires 6 hours).","Sun, Peng and Shi, Bei and Yu, Daiwei and Lin, Tao",2023,,,,
Improving Clinical Dataset Condensation with Mode Connectivity-based Trajectory Surrogates,http://arxiv.org/abs/2510.05805v1,cazenavette2022dataset,\cite{cazenavette2022dataset},Dataset Distillation by Matching Training Trajectories,http://arxiv.org/abs/2203.11932v1,"Dataset distillation is the task of synthesizing a small dataset such that a
model trained on the synthetic set will match the test accuracy of the model
trained on the full dataset. In this paper, we propose a new formulation that
optimizes our distilled data to guide networks to a similar state as those
trained on real data across many training steps. Given a network, we train it
for several iterations on our distilled data and optimize the distilled data
with respect to the distance between the synthetically trained parameters and
the parameters trained on real data. To efficiently obtain the initial and
target network parameters for large-scale datasets, we pre-compute and store
training trajectories of expert networks trained on the real dataset. Our
method handily outperforms existing methods and also allows us to distill
higher-resolution visual data.","Cazenavette, George and Wang, Tongzhou and Torralba, Antonio and Efros, Alexei A. and Zhu, Jun-Yan",2022,,,,
Improving Clinical Dataset Condensation with Mode Connectivity-based Trajectory Surrogates,http://arxiv.org/abs/2510.05805v1,du2023ftd,\cite{du2023ftd},"Minimizing the Accumulated Trajectory Error to Improve Dataset
  Distillation",http://arxiv.org/abs/2211.11004v3,"Model-based deep learning has achieved astounding successes due in part to
the availability of large-scale real-world data. However, processing such
massive amounts of data comes at a considerable cost in terms of computations,
storage, training and the search for good neural architectures. Dataset
distillation has thus recently come to the fore. This paradigm involves
distilling information from large real-world datasets into tiny and compact
synthetic datasets such that processing the latter ideally yields similar
performances as the former. State-of-the-art methods primarily rely on learning
the synthetic dataset by matching the gradients obtained during training
between the real and synthetic data. However, these gradient-matching methods
suffer from the so-called accumulated trajectory error caused by the
discrepancy between the distillation and subsequent evaluation. To mitigate the
adverse impact of this accumulated trajectory error, we propose a novel
approach that encourages the optimization algorithm to seek a flat trajectory.
We show that the weights trained on synthetic data are robust against the
accumulated errors perturbations with the regularization towards the flat
trajectory. Our method, called Flat Trajectory Distillation (FTD), is shown to
boost the performance of gradient-matching methods by up to 4.7% on a subset of
images of the ImageNet dataset with higher resolution images. We also validate
the effectiveness and generalizability of our method with datasets of different
resolutions and demonstrate its applicability to neural architecture search.
Code is available at https://github.com/AngusDujw/FTD-distillation.","Du, Jiawei and Jiang, Yidi and Tan, Vincent YF and Zhou, Joey Tianyi and Li, Haizhou",2023,,,,arXiv preprint arXiv:2211.11004
Improving Clinical Dataset Condensation with Mode Connectivity-based Trajectory Surrogates,http://arxiv.org/abs/2510.05805v1,guo2024datm,\cite{guo2024datm},"Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory
  Matching",http://arxiv.org/abs/2310.05773v2,"The ultimate goal of Dataset Distillation is to synthesize a small synthetic
dataset such that a model trained on this synthetic set will perform equally
well as a model trained on the full, real dataset. Until now, no method of
Dataset Distillation has reached this completely lossless goal, in part due to
the fact that previous methods only remain effective when the total number of
synthetic samples is extremely small. Since only so much information can be
contained in such a small number of samples, it seems that to achieve truly
loss dataset distillation, we must develop a distillation method that remains
effective as the size of the synthetic dataset grows. In this work, we present
such an algorithm and elucidate why existing methods fail to generate larger,
high-quality synthetic sets. Current state-of-the-art methods rely on
trajectory-matching, or optimizing the synthetic data to induce similar
long-term training dynamics as the real data. We empirically find that the
training stage of the trajectories we choose to match (i.e., early or late)
greatly affects the effectiveness of the distilled dataset. Specifically, early
trajectories (where the teacher network learns easy patterns) work well for a
low-cardinality synthetic set since there are fewer examples wherein to
distribute the necessary information. Conversely, late trajectories (where the
teacher network learns hard patterns) provide better signals for larger
synthetic sets since there are now enough samples to represent the necessary
complex patterns. Based on our findings, we propose to align the difficulty of
the generated patterns with the size of the synthetic dataset. In doing so, we
successfully scale trajectory matching-based methods to larger synthetic
datasets, achieving lossless dataset distillation for the very first time. Code
and distilled datasets are available at https://gzyaftermath.github.io/DATM.","Guo, Ziyao and Wang, Kai and Cazenavette, George and Li, Hui and Zhang, Kaipeng and You, Yang",2024,,,,
Improving Clinical Dataset Condensation with Mode Connectivity-based Trajectory Surrogates,http://arxiv.org/abs/2510.05805v1,liu2023tesla,\cite{liu2023tesla},Scaling Up Dataset Distillation to ImageNet-1K with Constant Memory,http://arxiv.org/abs/2211.10586v4,"Dataset Distillation is a newly emerging area that aims to distill large
datasets into much smaller and highly informative synthetic ones to accelerate
training and reduce storage. Among various dataset distillation methods,
trajectory-matching-based methods (MTT) have achieved SOTA performance in many
tasks, e.g., on CIFAR-10/100. However, due to exorbitant memory consumption
when unrolling optimization through SGD steps, MTT fails to scale to
large-scale datasets such as ImageNet-1K. Can we scale this SOTA method to
ImageNet-1K and does its effectiveness on CIFAR transfer to ImageNet-1K? To
answer these questions, we first propose a procedure to exactly compute the
unrolled gradient with constant memory complexity, which allows us to scale MTT
to ImageNet-1K seamlessly with ~6x reduction in memory footprint. We further
discover that it is challenging for MTT to handle datasets with a large number
of classes, and propose a novel soft label assignment that drastically improves
its convergence. The resulting algorithm sets new SOTA on ImageNet-1K: we can
scale up to 50 IPCs (Image Per Class) on ImageNet-1K on a single GPU (all
previous methods can only scale to 2 IPCs on ImageNet-1K), leading to the best
accuracy (only 5.9% accuracy drop against full dataset training) while
utilizing only 4.2% of the number of data points - an 18.2% absolute gain over
prior SOTA. Our code is available at https://github.com/justincui03/tesla","Liu, Xialei and Tapia, Mauricio and Mallya, Arun and Davis, Larry and Shrivastava, Abhinav",2023,,,,arXiv preprint arXiv:2211.10586
Improving Clinical Dataset Condensation with Mode Connectivity-based Trajectory Surrogates,http://arxiv.org/abs/2510.05805v1,garipov2018loss,\cite{garipov2018loss},"Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs",http://arxiv.org/abs/1802.10026v4,"The loss functions of deep neural networks are complex and their geometric
properties are not well understood. We show that the optima of these complex
loss functions are in fact connected by simple curves over which training and
test accuracy are nearly constant. We introduce a training procedure to
discover these high-accuracy pathways between modes. Inspired by this new
geometric insight, we also propose a new ensembling method entitled Fast
Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in
the time required to train a single model. We achieve improved performance
compared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10,
CIFAR-100, and ImageNet.","Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry P and Wilson, Andrew Gordon",2018,,,,
Improving Clinical Dataset Condensation with Mode Connectivity-based Trajectory Surrogates,http://arxiv.org/abs/2510.05805v1,draxler2018essentially,\cite{draxler2018essentially},Essentially No Barriers in Neural Network Energy Landscape,http://arxiv.org/abs/1803.00885v5,"Training neural networks involves finding minima of a high-dimensional
non-convex loss function. Knowledge of the structure of this energy landscape
is sparse. Relaxing from linear interpolations, we construct continuous paths
between minima of recent neural network architectures on CIFAR10 and CIFAR100.
Surprisingly, the paths are essentially flat in both the training and test
landscapes. This implies that neural networks have enough capacity for
structural changes, or that these changes are small between minima. Also, each
minimum has at least one vanishing Hessian eigenvalue in addition to those
resulting from trivial invariance.","Draxler, Felix and Veschgini, Kambis and Salmhofer, Manfred and Hamprecht, Fred A",2018,,,,
Improving Clinical Dataset Condensation with Mode Connectivity-based Trajectory Surrogates,http://arxiv.org/abs/2510.05805v1,izmailov2018averaging,\cite{izmailov2018averaging},Averaging Weights Leads to Wider Optima and Better Generalization,http://arxiv.org/abs/1803.05407v3,"Deep neural networks are typically trained by optimizing a loss function with
an SGD variant, in conjunction with a decaying learning rate, until
convergence. We show that simple averaging of multiple points along the
trajectory of SGD, with a cyclical or constant learning rate, leads to better
generalization than conventional training. We also show that this Stochastic
Weight Averaging (SWA) procedure finds much flatter solutions than SGD, and
approximates the recent Fast Geometric Ensembling (FGE) approach with a single
model. Using SWA we achieve notable improvement in test accuracy over
conventional SGD training on a range of state-of-the-art residual networks,
PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and
ImageNet. In short, SWA is extremely easy to implement, improves
generalization, and has almost no computational overhead.","Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon",2018,,,,
Improving Clinical Dataset Condensation with Mode Connectivity-based Trajectory Surrogates,http://arxiv.org/abs/2510.05805v1,thakur2023clinical,\cite{thakur2023clinical},Mode Connections for Clinical Incremental Learning: Lessons From the COVID-19 Pandemic,,,"Thakur, Anshul and Wang, Cheng and Ceritli, Turgay and Clifton, David and Eyre, David",2023,,,10.1101/2023.05.05.23289583,medRxiv
