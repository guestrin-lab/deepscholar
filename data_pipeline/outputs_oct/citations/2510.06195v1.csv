parent_paper_title,parent_arxiv_link,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal
Latent Speech-Text Transformer,http://arxiv.org/abs/2510.06195v1,van2016wavenet,\cite{van2016wavenet},WaveNet: A Generative Model for Raw Audio,http://arxiv.org/abs/1609.03499v2,"This paper introduces WaveNet, a deep neural network for generating raw audio
waveforms. The model is fully probabilistic and autoregressive, with the
predictive distribution for each audio sample conditioned on all previous ones;
nonetheless we show that it can be efficiently trained on data with tens of
thousands of samples per second of audio. When applied to text-to-speech, it
yields state-of-the-art performance, with human listeners rating it as
significantly more natural sounding than the best parametric and concatenative
systems for both English and Mandarin. A single WaveNet can capture the
characteristics of many different speakers with equal fidelity, and can switch
between them by conditioning on the speaker identity. When trained to model
music, we find that it generates novel and often highly realistic musical
fragments. We also show that it can be employed as a discriminative model,
returning promising results for phoneme recognition.","van den Oord, A{\""a}ron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray",2016,,,,
Latent Speech-Text Transformer,http://arxiv.org/abs/2510.06195v1,kong2020hifi,\cite{kong2020hifi},Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis,,,"Kong, Jungil and Kim, Jaehyeon and Bae, Jaekyoung",2020,,,,Advances in neural information processing systems
Latent Speech-Text Transformer,http://arxiv.org/abs/2510.06195v1,lakhotia2021generative,\cite{lakhotia2021generative},Generative Spoken Language Modeling from Raw Audio,http://arxiv.org/abs/2102.01192v2,"We introduce Generative Spoken Language Modeling, the task of learning the
acoustic and linguistic characteristics of a language from raw audio (no text,
no labels), and a set of metrics to automatically evaluate the learned
representations at acoustic and linguistic levels for both encoding and
generation. We set up baseline systems consisting of a discrete speech encoder
(returning pseudo-text units), a generative language model (trained on
pseudo-text), and a speech decoder (generating a waveform from pseudo-text) all
trained without supervision and validate the proposed metrics with human
evaluation. Across 3 speech encoders (CPC, wav2vec 2.0, HuBERT), we find that
the number of discrete units (50, 100, or 200) matters in a task-dependent and
encoder-dependent way, and that some combinations approach text-based systems.","Lakhotia, Kushal and Kharitonov, Eugene and Hsu, Wei-Ning and Adi, Yossi and Polyak, Adam and Bolte, Benjamin and Nguyen, Tu-Anh and Copet, Jade and Baevski, Alexei and Mohamed, Abdelrahman and others",2021,,,,Transactions of the Association for Computational Linguistics
Latent Speech-Text Transformer,http://arxiv.org/abs/2510.06195v1,borsos2023audiolm,\cite{borsos2023audiolm},AudioLM: a Language Modeling Approach to Audio Generation,http://arxiv.org/abs/2209.03143v2,"We introduce AudioLM, a framework for high-quality audio generation with
long-term consistency. AudioLM maps the input audio to a sequence of discrete
tokens and casts audio generation as a language modeling task in this
representation space. We show how existing audio tokenizers provide different
trade-offs between reconstruction quality and long-term structure, and we
propose a hybrid tokenization scheme to achieve both objectives. Namely, we
leverage the discretized activations of a masked language model pre-trained on
audio to capture long-term structure and the discrete codes produced by a
neural audio codec to achieve high-quality synthesis. By training on large
corpora of raw audio waveforms, AudioLM learns to generate natural and coherent
continuations given short prompts. When trained on speech, and without any
transcript or annotation, AudioLM generates syntactically and semantically
plausible speech continuations while also maintaining speaker identity and
prosody for unseen speakers. Furthermore, we demonstrate how our approach
extends beyond speech by generating coherent piano music continuations, despite
being trained without any symbolic representation of music.","Borsos, Zal{\'a}n and Marinier, Rapha{\""e}l and Vincent, Damien and Kharitonov, Eugene and Pietquin, Olivier and Sharifi, Matt and Roblek, Dominik and Teboul, Olivier and Grangier, David and Tagliasacchi, Marco and others",2023,,,,"IEEE/ACM transactions on audio, speech, and language processing"
Latent Speech-Text Transformer,http://arxiv.org/abs/2510.06195v1,zeghidour2021soundstream,\cite{zeghidour2021soundstream},SoundStream: An End-to-End Neural Audio Codec,http://arxiv.org/abs/2107.03312v1,"We present SoundStream, a novel neural audio codec that can efficiently
compress speech, music and general audio at bitrates normally targeted by
speech-tailored codecs. SoundStream relies on a model architecture composed by
a fully convolutional encoder/decoder network and a residual vector quantizer,
which are trained jointly end-to-end. Training leverages recent advances in
text-to-speech and speech enhancement, which combine adversarial and
reconstruction losses to allow the generation of high-quality audio content
from quantized embeddings. By training with structured dropout applied to
quantizer layers, a single model can operate across variable bitrates from
3kbps to 18kbps, with a negligible quality loss when compared with models
trained at fixed bitrates. In addition, the model is amenable to a low latency
implementation, which supports streamable inference and runs in real time on a
smartphone CPU. In subjective evaluations using audio at 24kHz sampling rate,
SoundStream at 3kbps outperforms Opus at 12kbps and approaches EVS at 9.6kbps.
Moreover, we are able to perform joint compression and enhancement either at
the encoder or at the decoder side with no additional latency, which we
demonstrate through background noise suppression for speech.","Zeghidour, Neil and Luebs, Alejandro and Omran, Ahmed and Skoglund, Jan and Tagliasacchi, Marco",2021,,,,"IEEE/ACM Transactions on Audio, Speech, and Language Processing"
Latent Speech-Text Transformer,http://arxiv.org/abs/2510.06195v1,nguyen2025spirit,\cite{nguyen2025spirit},Spirit LM: Interleaved Spoken and Written Language Model,http://arxiv.org/abs/2402.05755v2,"We introduce Spirit LM, a foundation multimodal language model that freely
mixes text and speech. Our model is based on a 7B pretrained text language
model that we extend to the speech modality by continuously training it on text
and speech units. Speech and text sequences are concatenated as a single stream
of tokens, and trained with a word-level interleaving method using a small
automatically-curated speech-text parallel corpus. Spirit LM comes in two
versions: a Base version that uses speech phonetic units (HuBERT) and an
Expressive version that models expressivity using pitch and style units in
addition to the phonetic units. For both versions, the text is encoded with
subword BPE tokens. The resulting model displays both the semantic abilities of
text models and the expressive abilities of speech models. Additionally, we
demonstrate that Spirit LM can learn new tasks in a few-shot fashion across
modalities (i.e. ASR, TTS, Speech Classification). We make available model
weights and inference code.","Nguyen, Tu Anh and Muller, Benjamin and Yu, Bokai and Costa-Jussa, Marta R and Elbayad, Maha and Popuri, Sravya and Ropers, Christophe and Duquenne, Paul-Ambroise and Algayres, Robin and Mavlyutov, Ruslan and others",2025,,,,Transactions of the Association for Computational Linguistics
Latent Speech-Text Transformer,http://arxiv.org/abs/2510.06195v1,defossez2024moshi,\cite{defossez2024moshi},Moshi: a speech-text foundation model for real-time dialogue,http://arxiv.org/abs/2410.00037v2,"We introduce Moshi, a speech-text foundation model and full-duplex spoken
dialogue framework. Current systems for spoken dialogue rely on pipelines of
independent components, namely voice activity detection, speech recognition,
textual dialogue and text-to-speech. Such frameworks cannot emulate the
experience of real conversations. First, their complexity induces a latency of
several seconds between interactions. Second, text being the intermediate
modality for dialogue, non-linguistic information that modifies meaning -- such
as emotion or non-speech sounds -- is lost in the interaction. Finally, they
rely on a segmentation into speaker turns, which does not take into account
overlapping speech, interruptions and interjections. Moshi solves these
independent issues altogether by casting spoken dialogue as speech-to-speech
generation. Starting from a text language model backbone, Moshi generates
speech as tokens from the residual quantizer of a neural audio codec, while
modeling separately its own speech and that of the user into parallel streams.
This allows for the removal of explicit speaker turns, and the modeling of
arbitrary conversational dynamics. We moreover extend the hierarchical
semantic-to-acoustic token generation of previous work to first predict
time-aligned text tokens as a prefix to audio tokens. Not only this ""Inner
Monologue"" method significantly improves the linguistic quality of generated
speech, but we also illustrate how it can provide streaming speech recognition
and text-to-speech. Our resulting model is the first real-time full-duplex
spoken large language model, with a theoretical latency of 160ms, 200ms in
practice, and is available at https://github.com/kyutai-labs/moshi.","D{\'e}fossez, Alexandre and Mazar{\'e}, Laurent and Orsini, Manu and Royer, Am{\'e}lie and P{\'e}rez, Patrick and J{\'e}gou, Herv{\'e} and Grave, Edouard and Zeghidour, Neil",2024,,,,arXiv e-prints
Latent Speech-Text Transformer,http://arxiv.org/abs/2510.06195v1,hoffmann2022training,\cite{hoffmann2022training},Training Compute-Optimal Large Language Models,http://arxiv.org/abs/2203.15556v1,"We investigate the optimal model size and number of tokens for training a
transformer language model under a given compute budget. We find that current
large language models are significantly undertrained, a consequence of the
recent focus on scaling language models whilst keeping the amount of training
data constant. By training over 400 language models ranging from 70 million to
over 16 billion parameters on 5 to 500 billion tokens, we find that for
compute-optimal training, the model size and the number of training tokens
should be scaled equally: for every doubling of model size the number of
training tokens should also be doubled. We test this hypothesis by training a
predicted compute-optimal model, Chinchilla, that uses the same compute budget
as Gopher but with 70B parameters and 4$\times$ more more data. Chinchilla
uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1
(178B), and Megatron-Turing NLG (530B) on a large range of downstream
evaluation tasks. This also means that Chinchilla uses substantially less
compute for fine-tuning and inference, greatly facilitating downstream usage.
As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%
on the MMLU benchmark, greater than a 7% improvement over Gopher.","Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others",2022,,,,
Latent Speech-Text Transformer,http://arxiv.org/abs/2510.06195v1,cuervo2024scaling,\cite{cuervo2024scaling},Scaling Properties of Speech Language Models,http://arxiv.org/abs/2404.00685v2,"Speech Language Models (SLMs) aim to learn language from raw audio, without
textual resources. Despite significant advances, our current models exhibit
weak syntax and semantic abilities. However, if the scaling properties of
neural language models hold for the speech modality, these abilities will
improve as the amount of compute used for training increases. In this paper, we
use models of this scaling behavior to estimate the scale at which our current
methods will yield a SLM with the English proficiency of text-based Large
Language Models (LLMs). We establish a strong correlation between pre-training
loss and downstream syntactic and semantic performance in SLMs and LLMs, which
results in predictable scaling of linguistic performance. We show that the
linguistic performance of SLMs scales up to three orders of magnitude more
slowly than that of text-based LLMs. Additionally, we study the benefits of
synthetic data designed to boost semantic understanding and the effects of
coarser speech tokenization.","Cuervo, Santiago and Marxer, Ricard",2024,,,,arXiv preprint arXiv:2404.00685
Latent Speech-Text Transformer,http://arxiv.org/abs/2510.06195v1,rubenstein2023audiopalm,\cite{rubenstein2023audiopalm},AudioPaLM: A Large Language Model That Can Speak and Listen,http://arxiv.org/abs/2306.12925v1,"We introduce AudioPaLM, a large language model for speech understanding and
generation. AudioPaLM fuses text-based and speech-based language models, PaLM-2
[Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified
multimodal architecture that can process and generate text and speech with
applications including speech recognition and speech-to-speech translation.
AudioPaLM inherits the capability to preserve paralinguistic information such
as speaker identity and intonation from AudioLM and the linguistic knowledge
present only in text large language models such as PaLM-2. We demonstrate that
initializing AudioPaLM with the weights of a text-only large language model
improves speech processing, successfully leveraging the larger quantity of text
training data used in pretraining to assist with the speech tasks. The
resulting model significantly outperforms existing systems for speech
translation tasks and has the ability to perform zero-shot speech-to-text
translation for many languages for which input/target language combinations
were not seen in training. AudioPaLM also demonstrates features of audio
language models, such as transferring a voice across languages based on a short
spoken prompt. We release examples of our method at
https://google-research.github.io/seanet/audiopalm/examples","Rubenstein, Paul K and Asawaroengchai, Chulayuth and Nguyen, Duc Dung and Bapna, Ankur and Borsos, Zal{\'a}n and Quitry, F{\'e}lix de Chaumont and Chen, Peter and Badawy, Dalia El and Han, Wei and Kharitonov, Eugene and others",2023,,,,arXiv preprint arXiv:2306.12925
Latent Speech-Text Transformer,http://arxiv.org/abs/2510.06195v1,hassid2023textually,\cite{hassid2023textually},Textually Pretrained Speech Language Models,http://arxiv.org/abs/2305.13009v3,"Speech language models (SpeechLMs) process and generate acoustic data only,
without textual supervision. In this work, we propose TWIST, a method for
training SpeechLMs using a warm-start from a pretrained textual language
models. We show using both automatic and human evaluations that TWIST
outperforms a cold-start SpeechLM across the board. We empirically analyze the
effect of different model design choices such as the speech tokenizer, the
pretrained textual model, and the dataset size. We find that model and dataset
scale both play an important role in constructing better-performing SpeechLMs.
Based on our observations, we present the largest (to the best of our
knowledge) SpeechLM both in terms of number of parameters and training data. We
additionally introduce two spoken versions of the StoryCloze textual benchmark
to further improve model evaluation and advance future research in the field.
We make speech samples, code and models publicly available:
https://pages.cs.huji.ac.il/adiyoss-lab/twist/ .","Hassid, Michael and Remez, Tal and Nguyen, Tu Anh and Gat, Itai and Conneau, Alexis and Kreuk, Felix and Copet, Jade and Defossez, Alexandre and Synnaeve, Gabriel and Dupoux, Emmanuel and others",2023,,,,Advances in Neural Information Processing Systems
Latent Speech-Text Transformer,http://arxiv.org/abs/2510.06195v1,nachmanispoken,\cite{nachmanispoken},"Spoken Question Answering and Speech Continuation Using
  Spectrogram-Powered LLM",http://arxiv.org/abs/2305.15255v4,"We present Spectron, a novel approach to adapting pre-trained large language
models (LLMs) to perform spoken question answering (QA) and speech
continuation. By endowing the LLM with a pre-trained speech encoder, our model
becomes able to take speech inputs and generate speech outputs. The entire
system is trained end-to-end and operates directly on spectrograms, simplifying
our architecture. Key to our approach is a training objective that jointly
supervises speech recognition, text continuation, and speech synthesis using
only paired speech-text pairs, enabling a `cross-modal' chain-of-thought within
a single decoding pass. Our method surpasses existing spoken language models in
speaker preservation and semantic coherence. Furthermore, the proposed model
improves upon direct initialization in retaining the knowledge of the original
LLM as demonstrated through spoken QA datasets. We release our audio samples
(https://michelleramanovich.github.io/spectron/spectron) and spoken QA dataset
(https://github.com/google-research-datasets/LLAMA1-Test-Set).","Nachmani, Eliya and Levkovitch, Alon and Hirsch, Roy and Salazar, Julian and Asawaroengchai, Chulayuth and Mariooryad, Soroosh and Rivlin, Ehud and Skerry-Ryan, RJ and Ramanovich, Michelle Tadmor",,,,,
Latent Speech-Text Transformer,http://arxiv.org/abs/2510.06195v1,fang2024llama,\cite{fang2024llama},Llama-omni: Seamless speech interaction with large language models,,,"Fang, Qingkai and Guo, Shoutao and Zhou, Yan and Ma, Zhengrui and Zhang, Shaolei and Feng, Yang",2024,,,,arXiv preprint arXiv:2409.06666
Latent Speech-Text Transformer,http://arxiv.org/abs/2510.06195v1,baadesyllablelm,\cite{baadesyllablelm},SyllableLM: Learning Coarse Semantic Units for Speech Language Models,http://arxiv.org/abs/2410.04029v1,"Language models require tokenized inputs. However, tokenization strategies
for continuous data like audio and vision are often based on simple heuristics
such as fixed sized convolutions or discrete clustering, which do not
necessarily align with the semantic structure of the data. For speech in
particular, the high resolution of waveforms (16,000 samples/second or more)
presents a significant challenge as speech-based language models have had to
use several times more tokens per word than text-based language models. In this
work, we introduce a controllable self-supervised technique to merge speech
representations into coarser syllable-like units while still preserving
semantic information. We do this by 1) extracting noisy boundaries through
analyzing correlations in pretrained encoder losses and 2) iteratively
improving model representations with a novel distillation technique. Our method
produces controllable-rate semantic units at as low as 5Hz and 60bps and
achieves SotA in syllabic segmentation and clustering. Using these coarse
tokens, we successfully train SyllableLM, a Speech Language Model (SpeechLM)
that matches or outperforms current SotA SpeechLMs on a range of spoken
language modeling tasks. SyllableLM also achieves significant improvements in
efficiency with a 30x reduction in training compute and a 4x wall-clock
inference speedup.","Baade, Alan and Peng, Puyuan and Harwath, David",,,,,
Latent Speech-Text Transformer,http://arxiv.org/abs/2510.06195v1,tseng2025taste,\cite{tseng2025taste},"TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken
  Language Modeling",http://arxiv.org/abs/2504.07053v2,"Recent efforts target spoken language models (SLMs) that not only listen but
also speak for more natural human-LLM interaction. Joint speech-text modeling
is a promising direction to achieve this. However, the effectiveness of recent
speech tokens for joint modeling remains underexplored. To address this, we
introduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that
directly addresses the modality gap by aligning speech token with the
corresponding text transcription during the tokenization stage. We propose a
method that can achieve this through a attention-based aggregation mechanism
and with speech reconstruction as the training objective. We conduct extensive
experiments and show that TASTE can preserve essential paralinguistic
information while dramatically reducing the token sequence length. With TASTE,
we perform straightforward joint spoken language modeling by using Low-Rank
Adaptation on the pre-trained text LLM. Experimental results show that
TASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze;
while significantly outperform other pre-trained SLMs on speech continuation
across subjective and objective evaluations. To our knowledge, TASTE is the
first end-to-end approach that utilizes a reconstruction objective to
automatically learn a text-aligned speech tokenization and embedding suitable
for spoken language modeling. Our demo, code, and model are available at
https://mtkresearch.github.io/TASTE-SpokenLM.github.io.","Tseng, Liang-Hsuan and Chen, Yi-Chang and Lee, Kuan-Yi and Shiu, Da-Shan and Lee, Hung-yi",2025,,,,arXiv preprint arXiv:2504.07053
Latent Speech-Text Transformer,http://arxiv.org/abs/2510.06195v1,copet2023simple,\cite{copet2023simple},Simple and Controllable Music Generation,http://arxiv.org/abs/2306.05284v3,"We tackle the task of conditional music generation. We introduce MusicGen, a
single Language Model (LM) that operates over several streams of compressed
discrete music representation, i.e., tokens. Unlike prior work, MusicGen is
comprised of a single-stage transformer LM together with efficient token
interleaving patterns, which eliminates the need for cascading several models,
e.g., hierarchically or upsampling. Following this approach, we demonstrate how
MusicGen can generate high-quality samples, both mono and stereo, while being
conditioned on textual description or melodic features, allowing better
controls over the generated output. We conduct extensive empirical evaluation,
considering both automatic and human studies, showing the proposed approach is
superior to the evaluated baselines on a standard text-to-music benchmark.
Through ablation studies, we shed light over the importance of each of the
components comprising MusicGen. Music samples, code, and models are available
at https://github.com/facebookresearch/audiocraft","Copet, Jade and Kreuk, Felix and Gat, Itai and Remez, Tal and Kant, David and Synnaeve, Gabriel and Adi, Yossi and D{\'e}fossez, Alexandre",2023,,,,Advances in Neural Information Processing Systems
Latent Speech-Text Transformer,http://arxiv.org/abs/2510.06195v1,ren2022speech,\cite{ren2022speech},Speech Pre-training with Acoustic Piece,http://arxiv.org/abs/2204.03240v1,"Previous speech pre-training methods, such as wav2vec2.0 and HuBERT,
pre-train a Transformer encoder to learn deep representations from audio data,
with objectives predicting either elements from latent vector quantized space
or pre-generated labels (known as target codes) with offline clustering.
However, those training signals (quantized elements or codes) are independent
across different tokens without considering their relations. According to our
observation and analysis, the target codes share obvious patterns aligned with
phonemized text data. Based on that, we propose to leverage those patterns to
better pre-train the model considering the relations among the codes. The
patterns we extracted, called ""acoustic piece""s, are from the sentence piece
result of HuBERT codes. With the acoustic piece as the training signal, we can
implicitly bridge the input audio and natural language, which benefits
audio-to-text tasks, such as automatic speech recognition (ASR). Simple but
effective, our method ""HuBERT-AP"" significantly outperforms strong baselines on
the LibriSpeech ASR task.","Ren, Shuo and Liu, Shujie and Wu, Yu and Zhou, Long and Wei, Furu",2022,,,,
Latent Speech-Text Transformer,http://arxiv.org/abs/2510.06195v1,li2024effectiveness,\cite{li2024effectiveness},On the Effectiveness of Acoustic BPE in Decoder-Only TTS,http://arxiv.org/abs/2407.03892v1,"Discretizing speech into tokens and generating them by a decoder-only model
have been a promising direction for text-to-speech (TTS) and spoken language
modeling (SLM). To shorten the sequence length of speech tokens, acoustic
byte-pair encoding (BPE) has emerged in SLM that treats speech tokens from
self-supervised semantic representations as characters to further compress the
token sequence. But the gain in TTS has not been fully investigated, and the
proper choice of acoustic BPE remains unclear. In this work, we conduct a
comprehensive study on various settings of acoustic BPE to explore its
effectiveness in decoder-only TTS models with semantic speech tokens.
Experiments on LibriTTS verify that acoustic BPE uniformly increases the
intelligibility and diversity of synthesized speech, while showing different
features across BPE settings. Hence, acoustic BPE is a favorable tool for
decoder-only TTS.","Li, Bohan and Shen, Feiyu and Guo, Yiwei and Wang, Shuai and Chen, Xie and Yu, Kai",2024,,,,
Latent Speech-Text Transformer,http://arxiv.org/abs/2510.06195v1,pagnoni2024byte,\cite{pagnoni2024byte},Byte Latent Transformer: Patches Scale Better Than Tokens,http://arxiv.org/abs/2412.09871v1,"We introduce the Byte Latent Transformer (BLT), a new byte-level LLM
architecture that, for the first time, matches tokenization-based LLM
performance at scale with significant improvements in inference efficiency and
robustness. BLT encodes bytes into dynamically sized patches, which serve as
the primary units of computation. Patches are segmented based on the entropy of
the next byte, allocating more compute and model capacity where increased data
complexity demands it. We present the first FLOP controlled scaling study of
byte-level models up to 8B parameters and 4T training bytes. Our results
demonstrate the feasibility of scaling models trained on raw bytes without a
fixed vocabulary. Both training and inference efficiency improve due to
dynamically selecting long patches when data is predictable, along with
qualitative improvements on reasoning and long tail generalization. Overall,
for fixed inference costs, BLT shows significantly better scaling than
tokenization-based models, by simultaneously growing both patch and model size.","Pagnoni, Artidoro and Pasunuru, Ram and Rodriguez, Pedro and Nguyen, John and Muller, Benjamin and Li, Margaret and Zhou, Chunting and Yu, Lili and Weston, Jason and Zettlemoyer, Luke and others",2024,,,,arXiv preprint arXiv:2412.09871
Latent Speech-Text Transformer,http://arxiv.org/abs/2510.06195v1,yu2023megabyte,\cite{yu2023megabyte},MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers,http://arxiv.org/abs/2305.07185v2,"Autoregressive transformers are spectacular models for short sequences but
scale poorly to long sequences such as high-resolution images, podcasts, code,
or books. We proposed Megabyte, a multi-scale decoder architecture that enables
end-to-end differentiable modeling of sequences of over one million bytes.
Megabyte segments sequences into patches and uses a local submodel within
patches and a global model between patches. This enables sub-quadratic
self-attention, much larger feedforward layers for the same compute, and
improved parallelism during decoding -- unlocking better performance at reduced
cost for both training and generation. Extensive experiments show that Megabyte
allows byte-level models to perform competitively with subword models on long
context language modeling, achieve state-of-the-art density estimation on
ImageNet, and model audio from raw files. Together, these results establish the
viability of tokenization-free autoregressive sequence modeling at scale.","Yu, Lili and Simig, D{\'a}niel and Flaherty, Colin and Aghajanyan, Armen and Zettlemoyer, Luke and Lewis, Mike",2023,,,,Advances in Neural Information Processing Systems
Latent Speech-Text Transformer,http://arxiv.org/abs/2510.06195v1,videau2025bytes,\cite{videau2025bytes},From Bytes to Ideas: Language Modeling with Autoregressive U-Nets,http://arxiv.org/abs/2506.14761v1,"Tokenization imposes a fixed granularity on the input text, freezing how a
language model operates on data and how far in the future it predicts. Byte
Pair Encoding (BPE) and similar schemes split text once, build a static
vocabulary, and leave the model stuck with that choice. We relax this rigidity
by introducing an autoregressive U-Net that learns to embed its own tokens as
it trains. The network reads raw bytes, pools them into words, then pairs of
words, then up to 4 words, giving it a multi-scale view of the sequence. At
deeper stages, the model must predict further into the future -- anticipating
the next few words rather than the next byte -- so deeper stages focus on
broader semantic patterns while earlier stages handle fine details. When
carefully tuning and controlling pretraining compute, shallow hierarchies tie
strong BPE baselines, and deeper hierarchies have a promising trend. Because
tokenization now lives inside the model, the same system can handle
character-level tasks and carry knowledge across low-resource languages.","Videau, Mathurin and Idrissi, Badr Youbi and Leite, Alessandro and Schoenauer, Marc and Teytaud, Olivier and Lopez-Paz, David",2025,,,,arXiv preprint arXiv:2506.14761
Latent Speech-Text Transformer,http://arxiv.org/abs/2510.06195v1,pang2024next,\cite{pang2024next},Next Patch Prediction for Autoregressive Visual Generation,http://arxiv.org/abs/2412.15321v3,"Autoregressive models, built based on the Next Token Prediction (NTP)
paradigm, show great potential in developing a unified framework that
integrates both language and vision tasks. Pioneering works introduce NTP to
autoregressive visual generation tasks. In this work, we rethink the NTP for
autoregressive image generation and extend it to a novel Next Patch Prediction
(NPP) paradigm. Our key idea is to group and aggregate image tokens into patch
tokens with higher information density. By using patch tokens as a more compact
input sequence, the autoregressive model is trained to predict the next patch,
significantly reducing computational costs. To further exploit the natural
hierarchical structure of image data, we propose a multi-scale coarse-to-fine
patch grouping strategy. With this strategy, the training process begins with a
large patch size and ends with vanilla NTP where the patch size is 1$\times$1,
thus maintaining the original inference process without modifications.
Extensive experiments across a diverse range of model sizes demonstrate that
NPP could reduce the training cost to around 0.6 times while improving image
generation quality by up to 1.0 FID score on the ImageNet 256x256 generation
benchmark. Notably, our method retains the original autoregressive model
architecture without introducing additional trainable parameters or
specifically designing a custom image tokenizer, offering a flexible and
plug-and-play solution for enhancing autoregressive visual generation.","Pang, Yatian and Jin, Peng and Yang, Shuo and Lin, Bin and Zhu, Bin and Tang, Zhenyu and Chen, Liuhan and Tay, Francis EH and Lim, Ser-Nam and Yang, Harry and others",2024,,,,CoRR
Latent Speech-Text Transformer,http://arxiv.org/abs/2510.06195v1,beyer2023flexivit,\cite{beyer2023flexivit},FlexiViT: One Model for All Patch Sizes,http://arxiv.org/abs/2212.08013v2,"Vision Transformers convert images to sequences by slicing them into patches.
The size of these patches controls a speed/accuracy tradeoff, with smaller
patches leading to higher accuracy at greater computational cost, but changing
the patch size typically requires retraining the model. In this paper, we
demonstrate that simply randomizing the patch size at training time leads to a
single set of weights that performs well across a wide range of patch sizes,
making it possible to tailor the model to different compute budgets at
deployment time. We extensively evaluate the resulting model, which we call
FlexiViT, on a wide range of tasks, including classification, image-text
retrieval, open-world detection, panoptic segmentation, and semantic
segmentation, concluding that it usually matches, and sometimes outperforms,
standard ViT models trained at a single patch size in an otherwise identical
setup. Hence, FlexiViT training is a simple drop-in improvement for ViT that
makes it easy to add compute-adaptive capabilities to most models relying on a
ViT backbone architecture. Code and pre-trained models are available at
https://github.com/google-research/big_vision","Beyer, Lucas and Izmailov, Pavel and Kolesnikov, Alexander and Caron, Mathilde and Kornblith, Simon and Zhai, Xiaohua and Minderer, Matthias and Tschannen, Michael and Alabdulmohsin, Ibrahim and Pavetic, Filip",2023,,,,
Latent Speech-Text Transformer,http://arxiv.org/abs/2510.06195v1,kahn2020libri,\cite{kahn2020libri},Libri-light: A benchmark for asr with limited or no supervision,,,"Kahn, Jacob and Riviere, Morgane and Zheng, Weiyi and Kharitonov, Evgeny and Xu, Qiantong and Mazar{\'e}, Pierre-Emmanuel and Karadayi, Julien and Liptchinsky, Vitaliy and Collobert, Ronan and Fuegen, Christian and others",2020,,,,
Latent Speech-Text Transformer,http://arxiv.org/abs/2510.06195v1,nguyen2020zero,\cite{nguyen2020zero},"The Zero Resource Speech Benchmark 2021: Metrics and baselines for
  unsupervised spoken language modeling",http://arxiv.org/abs/2011.11588v2,"We introduce a new unsupervised task, spoken language modeling: the learning
of linguistic representations from raw audio signals without any labels, along
with the Zero Resource Speech Benchmark 2021: a suite of 4 black-box, zero-shot
metrics probing for the quality of the learned models at 4 linguistic levels:
phonetics, lexicon, syntax and semantics. We present the results and analyses
of a composite baseline made of the concatenation of three unsupervised
systems: self-supervised contrastive representation learning (CPC), clustering
(k-means) and language modeling (LSTM or BERT). The language models learn on
the basis of the pseudo-text derived from clustering the learned
representations. This simple pipeline shows better than chance performance on
all four metrics, demonstrating the feasibility of spoken language modeling
from raw speech. It also yields worse performance compared to text-based
'topline' systems trained on the same data, delineating the space to be
explored by more sophisticated end-to-end models.","Nguyen, Tu Anh and de Seyssel, Maureen and Roz{\'e}, Patricia and Rivi{\`e}re, Morgane and Kharitonov, Evgeny and Baevski, Alexei and Dunbar, Ewan and Dupoux, Emmanuel",2020,,,,
