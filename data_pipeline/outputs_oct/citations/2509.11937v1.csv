parent_paper_title,parent_arxiv_link,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal
MMORE: Massive Multimodal Open RAG & Extraction,http://arxiv.org/abs/2509.11937v1,nvingest,\cite{nvingest},NVIDIA Ingest: An accelerated pipeline for document ingestion,,,NVIDIA Ingest Development Team,2024,,https://github.com/NVIDIA/nv-ingest,,
MMORE: Massive Multimodal Open RAG & Extraction,http://arxiv.org/abs/2509.11937v1,auer2024docling,\cite{auer2024docling},Docling Technical Report,http://arxiv.org/abs/2408.09869v5,"This technical report introduces Docling, an easy to use, self-contained,
MIT-licensed open-source package for PDF document conversion. It is powered by
state-of-the-art specialized AI models for layout analysis (DocLayNet) and
table structure recognition (TableFormer), and runs efficiently on commodity
hardware in a small resource budget. The code interface allows for easy
extensibility and addition of new features and models.","Auer, Christoph and Lysak, Maksym and Nassar, Ahmed and Dolfi, Michele and Livathinos, Nikolaos and Vagenas, Panos and Ramis, Cesar Berrospi and Omenetti, Matteo and Lindlbauer, Fabian and Dinkla, Kasper and others",2024,,,,arXiv preprint arXiv:2408.09869
MMORE: Massive Multimodal Open RAG & Extraction,http://arxiv.org/abs/2509.11937v1,doctr2021,\cite{doctr2021},docTR: Document Text Recognition,,,Mindee,2021,,,,
MMORE: Massive Multimodal Open RAG & Extraction,http://arxiv.org/abs/2509.11937v1,paruchuri2025surya,\cite{paruchuri2025surya},Surya: A lightweight document OCR and analysis toolkit,,,Vikas Paruchuri and Datalab Team,2025,,,,
MMORE: Massive Multimodal Open RAG & Extraction,http://arxiv.org/abs/2509.11937v1,llmwhisperer2025,\cite{llmwhisperer2025},LLMWhisperer,,,Unstract,2025,,,,
MMORE: Massive Multimodal Open RAG & Extraction,http://arxiv.org/abs/2509.11937v1,Chase_LangChain_2022,\cite{Chase_LangChain_2022},{LangChain},,,"Chase, Harrison",2022,,,,
MMORE: Massive Multimodal Open RAG & Extraction,http://arxiv.org/abs/2509.11937v1,Liu_LlamaIndex_2022,\cite{Liu_LlamaIndex_2022},LlamaIndex,,,"Liu, Jerry",2022,,,,
MMORE: Massive Multimodal Open RAG & Extraction,http://arxiv.org/abs/2509.11937v1,unstructured2025,\cite{unstructured2025},Unstructured: Open-Source Pre-Processing Tools for Unstructured Data,,,{Unstructured-IO},2025,,,,
MMORE: Massive Multimodal Open RAG & Extraction,http://arxiv.org/abs/2509.11937v1,haystack2019,\cite{haystack2019},Haystack: the end-to-end NLP framework for pragmatic builders,,,Malte Pietsch and Timo MÃ¶ller and Bogdan Kostic and Julian Risch and Massimiliano Pippi and Mayank Jobanputra and Sara Zanzottera and Silvano Cerza and Vladimir Blagojevic and Thomas Stadelmann and Tanay Soni and Sebastian Lee,2019,,https://github.com/deepset-ai/haystack,,
MMORE: Massive Multimodal Open RAG & Extraction,http://arxiv.org/abs/2509.11937v1,li2023m3it,\cite{li2023m3it},M$^3$IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning,,,Lei Li and Yuwei Yin and Shicheng Li and Liang Chen and Peiyi Wang and Shuhuai Ren and Mukai Li and Yazheng Yang and Jingjing Xu and Xu Sun and Lingpeng Kong and Qi Liu,2023,,,,arXiv preprint arXiv:2306.04387
MMORE: Massive Multimodal Open RAG & Extraction,http://arxiv.org/abs/2509.11937v1,awadalla2023openflamingo,\cite{awadalla2023openflamingo},"OpenFlamingo: An Open-Source Framework for Training Large Autoregressive
  Vision-Language Models",http://arxiv.org/abs/2308.01390v2,"We introduce OpenFlamingo, a family of autoregressive vision-language models
ranging from 3B to 9B parameters. OpenFlamingo is an ongoing effort to produce
an open-source replication of DeepMind's Flamingo models. On seven
vision-language datasets, OpenFlamingo models average between 80 - 89% of
corresponding Flamingo performance. This technical report describes our models,
training data, hyperparameters, and evaluation suite. We share our models and
code at https://github.com/mlfoundations/open_flamingo.","Awadalla, Anas and Gao, Irena and Gardner, Josh and Hessel, Jack and Hanafy, Yusuf and Zhu, Wanrong and Marathe, Kalyani and Bitton, Yonatan and Gadre, Samir and Sagawa, Shiori and others",2023,,,,arXiv preprint arXiv:2308.01390
