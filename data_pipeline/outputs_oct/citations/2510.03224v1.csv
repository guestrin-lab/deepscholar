parent_paper_title,parent_arxiv_link,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,goodfellow2014explaining,\cite{goodfellow2014explaining},Explaining and Harnessing Adversarial Examples,http://arxiv.org/abs/1412.6572v3,"Several machine learning models, including neural networks, consistently
misclassify adversarial examples---inputs formed by applying small but
intentionally worst-case perturbations to examples from the dataset, such that
the perturbed input results in the model outputting an incorrect answer with
high confidence. Early attempts at explaining this phenomenon focused on
nonlinearity and overfitting. We argue instead that the primary cause of neural
networks' vulnerability to adversarial perturbation is their linear nature.
This explanation is supported by new quantitative results while giving the
first explanation of the most intriguing fact about them: their generalization
across architectures and training sets. Moreover, this view yields a simple and
fast method of generating adversarial examples. Using this approach to provide
examples for adversarial training, we reduce the test set error of a maxout
network on the MNIST dataset.","Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian",2014,,,,arXiv preprint arXiv:1412.6572
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,madry2017towards,\cite{madry2017towards},Towards deep learning models resistant to adversarial attacks,,,"Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian",2017,,,,arXiv preprint arXiv:1706.06083
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,kannan2018adversarial,\cite{kannan2018adversarial},Adversarial Logit Pairing,http://arxiv.org/abs/1803.06373v1,"In this paper, we develop improved techniques for defending against
adversarial examples at scale. First, we implement the state of the art version
of adversarial training at unprecedented scale on ImageNet and investigate
whether it remains effective in this setting - an important open scientific
question (Athalye et al., 2018). Next, we introduce enhanced defenses using a
technique we call logit pairing, a method that encourages logits for pairs of
examples to be similar. When applied to clean examples and their adversarial
counterparts, logit pairing improves accuracy on adversarial examples over
vanilla adversarial training; we also find that logit pairing on clean examples
only is competitive with adversarial training in terms of accuracy on two
datasets. Finally, we show that adversarial logit pairing achieves the state of
the art defense on ImageNet against PGD white box attacks, with an accuracy
improvement from 1.5% to 27.9%. Adversarial logit pairing also successfully
damages the current state of the art defense against black box attacks on
ImageNet (Tramer et al., 2018), dropping its accuracy from 66.6% to 47.1%. With
this new accuracy drop, adversarial logit pairing ties with Tramer et al.(2018)
for the state of the art on black box attacks on ImageNet.","Kannan, Harini and Kurakin, Alexey and Goodfellow, Ian",2018,,,,arXiv preprint arXiv:1803.06373
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,zhang2019theoretically,\cite{zhang2019theoretically},Theoretically Principled Trade-off between Robustness and Accuracy,http://arxiv.org/abs/1901.08573v3,"We identify a trade-off between robustness and accuracy that serves as a
guiding principle in the design of defenses against adversarial examples.
Although this problem has been widely studied empirically, much remains unknown
concerning the theory underlying this trade-off. In this work, we decompose the
prediction error for adversarial examples (robust error) as the sum of the
natural (classification) error and boundary error, and provide a differentiable
upper bound using the theory of classification-calibrated loss, which is shown
to be the tightest possible upper bound uniform over all probability
distributions and measurable predictors. Inspired by our theoretical analysis,
we also design a new defense method, TRADES, to trade adversarial robustness
off against accuracy. Our proposed algorithm performs well experimentally in
real-world datasets. The methodology is the foundation of our entry to the
NeurIPS 2018 Adversarial Vision Challenge in which we won the 1st place out of
~2,000 submissions, surpassing the runner-up approach by $11.41\%$ in terms of
mean $\ell_2$ perturbation distance.","Zhang, Hongyang and Yu, Yaodong and Jiao, Jiantao and Xing, Eric and El Ghaoui, Laurent and Jordan, Michael",2019,,,,
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,wang2019improving,\cite{wang2019improving},Improving adversarial robustness requires revisiting misclassified examples,,,"Wang, Yisen and Zou, Difan and Yi, Jinfeng and Bailey, James and Ma, Xingjun and Gu, Quanquan",2019,,,,
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,cai2018curriculum,\cite{cai2018curriculum},Curriculum Adversarial Training,http://arxiv.org/abs/1805.04807v1,"Recently, deep learning has been applied to many security-sensitive
applications, such as facial authentication. The existence of adversarial
examples hinders such applications. The state-of-the-art result on defense
shows that adversarial training can be applied to train a robust model on MNIST
against adversarial examples; but it fails to achieve a high empirical
worst-case accuracy on a more complex task, such as CIFAR-10 and SVHN. In our
work, we propose curriculum adversarial training (CAT) to resolve this issue.
The basic idea is to develop a curriculum of adversarial examples generated by
attacks with a wide range of strengths. With two techniques to mitigate the
forgetting and the generalization issues, we demonstrate that CAT can improve
the prior art's empirical worst-case accuracy by a large margin of 25% on
CIFAR-10 and 35% on SVHN. At the same, the model's performance on
non-adversarial inputs is comparable to the state-of-the-art models.","Cai, Qi-Zhi and Du, Min and Liu, Chang and Song, Dawn",2018,,,,arXiv preprint arXiv:1805.04807
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,zhang2020attacks,\cite{zhang2020attacks},Attacks Which Do Not Kill Training Make Adversarial Learning Stronger,http://arxiv.org/abs/2002.11242v2,"Adversarial training based on the minimax formulation is necessary for
obtaining adversarial robustness of trained models. However, it is conservative
or even pessimistic so that it sometimes hurts the natural generalization. In
this paper, we raise a fundamental question---do we have to trade off natural
generalization for adversarial robustness? We argue that adversarial training
is to employ confident adversarial data for updating the current model. We
propose a novel approach of friendly adversarial training (FAT): rather than
employing most adversarial data maximizing the loss, we search for least
adversarial (i.e., friendly adversarial) data minimizing the loss, among the
adversarial data that are confidently misclassified. Our novel formulation is
easy to implement by just stopping the most adversarial data searching
algorithms such as PGD (projected gradient descent) early, which we call
early-stopped PGD. Theoretically, FAT is justified by an upper bound of the
adversarial risk. Empirically, early-stopped PGD allows us to answer the
earlier question negatively---adversarial robustness can indeed be achieved
without compromising the natural generalization.","Zhang, Jingfeng and Xu, Xilie and Han, Bo and Niu, Gang and Cui, Lizhen and Sugiyama, Masashi and Kankanhalli, Mohan",2020,,,,
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,wang2022self,\cite{wang2022self},Self-Ensemble Adversarial Training for Improved Robustness,http://arxiv.org/abs/2203.09678v2,"Due to numerous breakthroughs in real-world applications brought by machine
intelligence, deep neural networks (DNNs) are widely employed in critical
applications. However, predictions of DNNs are easily manipulated with
imperceptible adversarial perturbations, which impedes the further deployment
of DNNs and may result in profound security and privacy implications. By
incorporating adversarial samples into the training data pool, adversarial
training is the strongest principled strategy against various adversarial
attacks among all sorts of defense methods. Recent works mainly focus on
developing new loss functions or regularizers, attempting to find the unique
optimal point in the weight space. But none of them taps the potentials of
classifiers obtained from standard adversarial training, especially states on
the searching trajectory of training. In this work, we are dedicated to the
weight states of models through the training process and devise a simple but
powerful \emph{Self-Ensemble Adversarial Training} (SEAT) method for yielding a
robust classifier by averaging weights of history models. This considerably
improves the robustness of the target model against several well known
adversarial attacks, even merely utilizing the naive cross-entropy loss to
supervise. We also discuss the relationship between the ensemble of predictions
from different adversarially trained models and the prediction of
weight-ensembled models, as well as provide theoretical and empirical evidence
that the proposed self-ensemble method provides a smoother loss landscape and
better robustness than both individual models and the ensemble of predictions
from different classifiers. We further analyze a subtle but fatal issue in the
general settings for the self-ensemble model, which causes the deterioration of
the weight-ensembled method in the late phases.","Wang, Hongjun and Wang, Yisen",2022,,,,arXiv preprint arXiv:2203.09678
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,jin2022enhancing,\cite{jin2022enhancing},Enhancing Adversarial Training with Second-Order Statistics of Weights,http://arxiv.org/abs/2203.06020v1,"Adversarial training has been shown to be one of the most effective
approaches to improve the robustness of deep neural networks. It is formalized
as a min-max optimization over model weights and adversarial perturbations,
where the weights can be optimized through gradient descent methods like SGD.
In this paper, we show that treating model weights as random variables allows
for enhancing adversarial training through \textbf{S}econd-Order
\textbf{S}tatistics \textbf{O}ptimization (S$^2$O) with respect to the weights.
By relaxing a common (but unrealistic) assumption of previous PAC-Bayesian
frameworks that all weights are statistically independent, we derive an
improved PAC-Bayesian adversarial generalization bound, which suggests that
optimizing second-order statistics of weights can effectively tighten the
bound. In addition to this theoretical insight, we conduct an extensive set of
experiments, which show that S$^2$O not only improves the robustness and
generalization of the trained neural networks when used in isolation, but also
integrates easily in state-of-the-art adversarial training techniques like
TRADES, AWP, MART, and AVMixup, leading to a measurable improvement of these
techniques. The code is available at \url{https://github.com/Alexkael/S2O}.","Jin, Gaojie and Yi, Xinping and Huang, Wei and Schewe, Sven and Huang, Xiaowei",2022,,,,
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,ghiasvand2024robust,\cite{ghiasvand2024robust},Robust Decentralized Learning with Local Updates and Gradient Tracking,http://arxiv.org/abs/2405.00965v2,"As distributed learning applications such as Federated Learning, the Internet
of Things (IoT), and Edge Computing grow, it is critical to address the
shortcomings of such technologies from a theoretical perspective. As an
abstraction, we consider decentralized learning over a network of communicating
clients or nodes and tackle two major challenges: data heterogeneity and
adversarial robustness. We propose a decentralized minimax optimization method
that employs two important modules: local updates and gradient tracking.
Minimax optimization is the key tool to enable adversarial training for
ensuring robustness. Having local updates is essential in Federated Learning
(FL) applications to mitigate the communication bottleneck, and utilizing
gradient tracking is essential to proving convergence in the case of data
heterogeneity. We analyze the performance of the proposed algorithm,
Dec-FedTrack, in the case of nonconvex-strongly concave minimax optimization,
and prove that it converges a stationary point. We also conduct numerical
experiments to support our theoretical findings.","Ghiasvand, Sajjad and Reisizadeh, Amirhossein and Alizadeh, Mahnoosh and Pedarsani, Ramtin",2024,,,,arXiv preprint arXiv:2405.00965
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,bengio2009curriculum,\cite{bengio2009curriculum},Curriculum learning,,,"Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason",2009,,,,
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,galloway2019batch,\cite{galloway2019batch},Batch Normalization is a Cause of Adversarial Vulnerability,http://arxiv.org/abs/1905.02161v2,"Batch normalization (batch norm) is often used in an attempt to stabilize and
accelerate training in deep neural networks. In many cases it indeed decreases
the number of parameter updates required to achieve low training error.
However, it also reduces robustness to small adversarial input perturbations
and noise by double-digit percentages, as we show on five standard datasets.
Furthermore, substituting weight decay for batch norm is sufficient to nullify
the relationship between adversarial vulnerability and the input dimension. Our
work is consistent with a mean-field analysis that found that batch norm causes
exploding gradients.","Galloway, Angus and Golubeva, Anna and Tanay, Thomas and Moussa, Medhat and Taylor, Graham W",2019,,,,arXiv preprint arXiv:1905.02161
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,benz2021batch,\cite{benz2021batch},"Batch Normalization Increases Adversarial Vulnerability and Decreases
  Adversarial Transferability: A Non-Robust Feature Perspective",http://arxiv.org/abs/2010.03316v2,"Batch normalization (BN) has been widely used in modern deep neural networks
(DNNs) due to improved convergence. BN is observed to increase the model
accuracy while at the cost of adversarial robustness. There is an increasing
interest in the ML community to understand the impact of BN on DNNs, especially
related to the model robustness. This work attempts to understand the impact of
BN on DNNs from a non-robust feature perspective. Straightforwardly, the
improved accuracy can be attributed to the better utilization of useful
features. It remains unclear whether BN mainly favors learning robust features
(RFs) or non-robust features (NRFs). Our work presents empirical evidence that
supports that BN shifts a model towards being more dependent on NRFs. To
facilitate the analysis of such a feature robustness shift, we propose a
framework for disentangling robust usefulness into robustness and usefulness.
Extensive analysis under the proposed framework yields valuable insight on the
DNN behavior regarding robustness, e.g. DNNs first mainly learn RFs and then
NRFs. The insight that RFs transfer better than NRFs, further inspires simple
techniques to strengthen transfer-based black-box attacks.","Benz, Philipp and Zhang, Chaoning and Kweon, In So",2021,,,,
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,wang2022removing,\cite{wang2022removing},Removing Batch Normalization Boosts Adversarial Training,http://arxiv.org/abs/2207.01156v1,"Adversarial training (AT) defends deep neural networks against adversarial
attacks. One challenge that limits its practical application is the performance
degradation on clean samples. A major bottleneck identified by previous works
is the widely used batch normalization (BN), which struggles to model the
different statistics of clean and adversarial training samples in AT. Although
the dominant approach is to extend BN to capture this mixture of distribution,
we propose to completely eliminate this bottleneck by removing all BN layers in
AT. Our normalizer-free robust training (NoFrost) method extends recent
advances in normalizer-free networks to AT for its unexplored advantage on
handling the mixture distribution challenge. We show that NoFrost achieves
adversarial robustness with only a minor sacrifice on clean sample accuracy. On
ImageNet with ResNet50, NoFrost achieves $74.06\%$ clean accuracy, which drops
merely $2.00\%$ from standard training. In contrast, BN-based AT obtains
$59.28\%$ clean accuracy, suffering a significant $16.78\%$ drop from standard
training. In addition, NoFrost achieves a $23.56\%$ adversarial robustness
against PGD attack, which improves the $13.57\%$ robustness in BN-based AT. We
observe better model smoothness and larger decision margins from NoFrost, which
make the models less sensitive to input perturbations and thus more robust.
Moreover, when incorporating more data augmentations into NoFrost, it achieves
comprehensive robustness against multiple distribution shifts. Code and
pre-trained models are public at
https://github.com/amazon-research/normalizer-free-robust-training.","Wang, Haotao and Zhang, Aston and Zheng, Shuai and Shi, Xingjian and Li, Mu and Wang, Zhangyang",2022,,,,
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,dhillon2018stochastic,\cite{dhillon2018stochastic},Stochastic Activation Pruning for Robust Adversarial Defense,http://arxiv.org/abs/1803.01442v1,"Neural networks are known to be vulnerable to adversarial examples. Carefully
chosen perturbations to real images, while imperceptible to humans, induce
misclassification and threaten the reliability of deep learning systems in the
wild. To guard against adversarial examples, we take inspiration from game
theory and cast the problem as a minimax zero-sum game between the adversary
and the model. In general, for such games, the optimal strategy for both
players requires a stochastic policy, also known as a mixed strategy. In this
light, we propose Stochastic Activation Pruning (SAP), a mixed strategy for
adversarial defense. SAP prunes a random subset of activations (preferentially
pruning those with smaller magnitude) and scales up the survivors to
compensate. We can apply SAP to pretrained networks, including adversarially
trained models, without fine-tuning, providing robustness against adversarial
examples. Experiments demonstrate that SAP confers robustness against attacks,
increasing accuracy and preserving calibration.","Dhillon, Guneet S and Azizzadenesheli, Kamyar and Lipton, Zachary C and Bernstein, Jeremy and Kossaifi, Jean and Khanna, Aran and Anandkumar, Anima",2018,,,,arXiv preprint arXiv:1803.01442
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,madaan2020adversarial,\cite{madaan2020adversarial},Adversarial Neural Pruning with Latent Vulnerability Suppression,http://arxiv.org/abs/1908.04355v4,"Despite the remarkable performance of deep neural networks on various
computer vision tasks, they are known to be susceptible to adversarial
perturbations, which makes it challenging to deploy them in real-world
safety-critical applications. In this paper, we conjecture that the leading
cause of adversarial vulnerability is the distortion in the latent feature
space, and provide methods to suppress them effectively. Explicitly, we define
\emph{vulnerability} for each latent feature and then propose a new loss for
adversarial learning, \emph{Vulnerability Suppression (VS)} loss, that aims to
minimize the feature-level vulnerability during training. We further propose a
Bayesian framework to prune features with high vulnerability to reduce both
vulnerability and loss on adversarial samples. We validate our
\emph{Adversarial Neural Pruning with Vulnerability Suppression (ANP-VS)}
method on multiple benchmark datasets, on which it not only obtains
state-of-the-art adversarial robustness but also improves the performance on
clean examples, using only a fraction of the parameters used by the full
network. Further qualitative analysis suggests that the improvements come from
the suppression of feature-level vulnerability.","Madaan, Divyam and Shin, Jinwoo and Hwang, Sung Ju",2020,,,,
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,xiao2019enhancing,\cite{xiao2019enhancing},Enhancing adversarial defense by k-winners-take-all,,,"Xiao, Chang and Zhong, Peilin and Zheng, Changxi",2019,,,,arXiv preprint arXiv:1905.10510
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,zoran2020towards,\cite{zoran2020towards},Towards Robust Image Classification Using Sequential Attention Models,http://arxiv.org/abs/1912.02184v1,"In this paper we propose to augment a modern neural-network architecture with
an attention model inspired by human perception. Specifically, we adversarially
train and analyze a neural model incorporating a human inspired, visual
attention component that is guided by a recurrent top-down sequential process.
Our experimental evaluation uncovers several notable findings about the
robustness and behavior of this new model. First, introducing attention to the
model significantly improves adversarial robustness resulting in
state-of-the-art ImageNet accuracies under a wide range of random targeted
attack strengths. Second, we show that by varying the number of attention steps
(glances/fixations) for which the model is unrolled, we are able to make its
defense capabilities stronger, even in light of stronger attacks --- resulting
in a ""computational race"" between the attacker and the defender. Finally, we
show that some of the adversarial examples generated by attacking our model are
quite different from conventional adversarial examples --- they contain global,
salient and spatially coherent structures coming from the target class that
would be recognizable even to a human, and work by distracting the attention of
the model away from the main object in the original image.","Zoran, Daniel and Chrzanowski, Mike and Huang, Po-Sen and Gowal, Sven and Mott, Alex and Kohli, Pushmeet",2020,,,,
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,xie2019feature,\cite{xie2019feature},Feature Denoising for Improving Adversarial Robustness,http://arxiv.org/abs/1812.03411v2,"Adversarial attacks to image classification systems present challenges to
convolutional networks and opportunities for understanding them. This study
suggests that adversarial perturbations on images lead to noise in the features
constructed by these networks. Motivated by this observation, we develop new
network architectures that increase adversarial robustness by performing
feature denoising. Specifically, our networks contain blocks that denoise the
features using non-local means or other filters; the entire networks are
trained end-to-end. When combined with adversarial training, our feature
denoising networks substantially improve the state-of-the-art in adversarial
robustness in both white-box and black-box attack settings. On ImageNet, under
10-iteration PGD white-box attacks where prior art has 27.9% accuracy, our
method achieves 55.7%; even under extreme 2000-iteration PGD white-box attacks,
our method secures 42.6% accuracy. Our method was ranked first in Competition
on Adversarial Attacks and Defenses (CAAD) 2018 --- it achieved 50.6%
classification accuracy on a secret, ImageNet-like test dataset against 48
unknown attackers, surpassing the runner-up approach by ~10%. Code is available
at https://github.com/facebookresearch/ImageNet-Adversarial-Training.","Xie, Cihang and Wu, Yuxin and Maaten, Laurens van der and Yuille, Alan L and He, Kaiming",2019,,,,
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,bai2021improving,\cite{bai2021improving},Improving Adversarial Robustness via Channel-wise Activation Suppressing,http://arxiv.org/abs/2103.08307v2,"The study of adversarial examples and their activation has attracted
significant attention for secure and robust learning with deep neural networks
(DNNs). Different from existing works, in this paper, we highlight two new
characteristics of adversarial examples from the channel-wise activation
perspective: 1) the activation magnitudes of adversarial examples are higher
than that of natural examples; and 2) the channels are activated more uniformly
by adversarial examples than natural examples. We find that the
state-of-the-art defense adversarial training has addressed the first issue of
high activation magnitudes via training on adversarial examples, while the
second issue of uniform activation remains. This motivates us to suppress
redundant activation from being activated by adversarial perturbations via a
Channel-wise Activation Suppressing (CAS) strategy. We show that CAS can train
a model that inherently suppresses adversarial activation, and can be easily
applied to existing defense methods to further improve their robustness. Our
work provides a simple but generic training strategy for robustifying the
intermediate layer activation of DNNs.","Bai, Yang and Zeng, Yuyuan and Jiang, Yong and Xia, Shu-Tao and Ma, Xingjun and Wang, Yisen",2021,,,,arXiv preprint arXiv:2103.08307
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,yan2021cifs,\cite{yan2021cifs},"CIFS: Improving Adversarial Robustness of CNNs via Channel-wise
  Importance-based Feature Selection",http://arxiv.org/abs/2102.05311v4,"We investigate the adversarial robustness of CNNs from the perspective of
channel-wise activations. By comparing \textit{non-robust} (normally trained)
and \textit{robustified} (adversarially trained) models, we observe that
adversarial training (AT) robustifies CNNs by aligning the channel-wise
activations of adversarial data with those of their natural counterparts.
However, the channels that are \textit{negatively-relevant} (NR) to predictions
are still over-activated when processing adversarial data. Besides, we also
observe that AT does not result in similar robustness for all classes. For the
robust classes, channels with larger activation magnitudes are usually more
\textit{positively-relevant} (PR) to predictions, but this alignment does not
hold for the non-robust classes. Given these observations, we hypothesize that
suppressing NR channels and aligning PR ones with their relevances further
enhances the robustness of CNNs under AT. To examine this hypothesis, we
introduce a novel mechanism, i.e., \underline{C}hannel-wise
\underline{I}mportance-based \underline{F}eature \underline{S}election (CIFS).
The CIFS manipulates channels' activations of certain layers by generating
non-negative multipliers to these channels based on their relevances to
predictions. Extensive experiments on benchmark datasets including CIFAR10 and
SVHN clearly verify the hypothesis and CIFS's effectiveness of robustifying
CNNs. \url{https://github.com/HanshuYAN/CIFS}","Yan, Hanshu and Zhang, Jingfeng and Niu, Gang and Feng, Jiashi and Tan, Vincent and Sugiyama, Masashi",2021,,,,
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,kim2023feature,\cite{kim2023feature},Feature Separation and Recalibration for Adversarial Robustness,http://arxiv.org/abs/2303.13846v1,"Deep neural networks are susceptible to adversarial attacks due to the
accumulation of perturbations in the feature level, and numerous works have
boosted model robustness by deactivating the non-robust feature activations
that cause model mispredictions. However, we claim that these malicious
activations still contain discriminative cues and that with recalibration, they
can capture additional useful information for correct model predictions. To
this end, we propose a novel, easy-to-plugin approach named Feature Separation
and Recalibration (FSR) that recalibrates the malicious, non-robust activations
for more robust feature maps through Separation and Recalibration. The
Separation part disentangles the input feature map into the robust feature with
activations that help the model make correct predictions and the non-robust
feature with activations that are responsible for model mispredictions upon
adversarial attack. The Recalibration part then adjusts the non-robust
activations to restore the potentially useful cues for model predictions.
Extensive experiments verify the superiority of FSR compared to traditional
deactivation techniques and demonstrate that it improves the robustness of
existing adversarial training methods by up to 8.57% with small computational
overhead. Codes are available at https://github.com/wkim97/FSR.","Kim, Woo Jae and Cho, Yoonki and Jung, Junsik and Yoon, Sung-Eui",2023,,,,
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,tang2024robust,\cite{tang2024robust},"Robust Overfitting Does Matter: Test-Time Adversarial Purification With
  FGSM",http://arxiv.org/abs/2403.11448v1,"Numerous studies have demonstrated the susceptibility of deep neural networks
(DNNs) to subtle adversarial perturbations, prompting the development of many
advanced adversarial defense methods aimed at mitigating adversarial attacks.
Current defense strategies usually train DNNs for a specific adversarial attack
method and can achieve good robustness in defense against this type of
adversarial attack. Nevertheless, when subjected to evaluations involving
unfamiliar attack modalities, empirical evidence reveals a pronounced
deterioration in the robustness of DNNs. Meanwhile, there is a trade-off
between the classification accuracy of clean examples and adversarial examples.
Most defense methods often sacrifice the accuracy of clean examples in order to
improve the adversarial robustness of DNNs. To alleviate these problems and
enhance the overall robust generalization of DNNs, we propose the Test-Time
Pixel-Level Adversarial Purification (TPAP) method. This approach is based on
the robust overfitting characteristic of DNNs to the fast gradient sign method
(FGSM) on training and test datasets. It utilizes FGSM for adversarial
purification, to process images for purifying unknown adversarial perturbations
from pixels at testing time in a ""counter changes with changelessness"" manner,
thereby enhancing the defense capability of DNNs against various unknown
adversarial attacks. Extensive experimental results show that our method can
effectively improve both overall robust generalization of DNNs, notably over
previous methods.","Tang, Linyu and Zhang, Lei",2024,,,,
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,yeh2024test,\cite{yeh2024test},"Test-time Adversarial Defense with Opposite Adversarial Path and High
  Attack Time Cost",http://arxiv.org/abs/2410.16805v2,"Deep learning models are known to be vulnerable to adversarial attacks by
injecting sophisticated designed perturbations to input data. Training-time
defenses still exhibit a significant performance gap between natural accuracy
and robust accuracy. In this paper, we investigate a new test-time adversarial
defense method via diffusion-based recovery along opposite adversarial paths
(OAPs). We present a purifier that can be plugged into a pre-trained model to
resist adversarial attacks. Different from prior arts, the key idea is
excessive denoising or purification by integrating the opposite adversarial
direction with reverse diffusion to push the input image further toward the
opposite adversarial direction. For the first time, we also exemplify the
pitfall of conducting AutoAttack (Rand) for diffusion-based defense methods.
Through the lens of time complexity, we examine the trade-off between the
effectiveness of adaptive attack and its computation complexity against our
defense. Experimental evaluation along with time cost analysis verifies the
effectiveness of the proposed method.","Yeh, Cheng-Han and Yu, Kuanchun and Lu, Chun-Shien",2024,,,,arXiv preprint arXiv:2410.16805
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,tsai2023test,\cite{tsai2023test},Test-time detection and repair of adversarial samples via masked autoencoder,,,"Tsai, Yun-Yun and Chao, Ju-Chin and Wen, Albert and Yang, Zhaoyuan and Mao, Chengzhi and Shah, Tapan and Yang, Junfeng",2023,,,,arXiv preprint arXiv:2303.12848
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,wang2021fighting,\cite{wang2021fighting},"Fighting Gradients with Gradients: Dynamic Defenses against Adversarial
  Attacks",http://arxiv.org/abs/2105.08714v1,"Adversarial attacks optimize against models to defeat defenses. Existing
defenses are static, and stay the same once trained, even while attacks change.
We argue that models should fight back, and optimize their defenses against
attacks at test time. We propose dynamic defenses, to adapt the model and input
during testing, by defensive entropy minimization (dent). Dent alters testing,
but not training, for compatibility with existing models and train-time
defenses. Dent improves the robustness of adversarially-trained defenses and
nominally-trained models against white-box, black-box, and adaptive attacks on
CIFAR-10/100 and ImageNet. In particular, dent boosts state-of-the-art defenses
by 20+ points absolute against AutoAttack on CIFAR-10 at $\epsilon_\infty$ =
8/255.","Wang, Dequan and Ju, An and Shelhamer, Evan and Wagner, David and Darrell, Trevor",2021,,,,arXiv preprint arXiv:2105.08714
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,cohen2024simple,\cite{cohen2024simple},"Simple Post-Training Robustness Using Test Time Augmentations and Random
  Forest",http://arxiv.org/abs/2109.08191v2,"Although Deep Neural Networks (DNNs) achieve excellent performance on many
real-world tasks, they are highly vulnerable to adversarial attacks. A leading
defense against such attacks is adversarial training, a technique in which a
DNN is trained to be robust to adversarial attacks by introducing adversarial
noise to its input. This procedure is effective but must be done during the
training phase. In this work, we propose Augmented Random Forest (ARF), a
simple and easy-to-use strategy for robustifying an existing pretrained DNN
without modifying its weights. For every image, we generate randomized test
time augmentations by applying diverse color, blur, noise, and geometric
transforms. Then we use the DNN's logits output to train a simple random forest
to predict the real class label. Our method achieves state-of-the-art
adversarial robustness on a diversity of white and black box attacks with
minimal compromise on the natural images' classification. We test ARF also
against numerous adaptive white-box attacks and it shows excellent results when
combined with adversarial training. Code is available at
https://github.com/giladcohen/ARF.","Cohen, Gilad and Giryes, Raja",2024,,,,
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,perez2021enhancing,\cite{perez2021enhancing},Enhancing Adversarial Robustness via Test-time Transformation Ensembling,http://arxiv.org/abs/2107.14110v1,"Deep learning models are prone to being fooled by imperceptible perturbations
known as adversarial attacks. In this work, we study how equipping models with
Test-time Transformation Ensembling (TTE) can work as a reliable defense
against such attacks. While transforming the input data, both at train and test
times, is known to enhance model performance, its effects on adversarial
robustness have not been studied. Here, we present a comprehensive empirical
study of the impact of TTE, in the form of widely-used image transforms, on
adversarial robustness. We show that TTE consistently improves model robustness
against a variety of powerful attacks without any need for re-training, and
that this improvement comes at virtually no trade-off with accuracy on clean
samples. Finally, we show that the benefits of TTE transfer even to the
certified robustness domain, in which TTE provides sizable and consistent
improvements.","P{\'e}rez, Juan C and Alfarra, Motasem and Jeanneret, Guillaume and Rueda, Laura and Thabet, Ali and Ghanem, Bernard and Arbel{\'a}ez, Pablo",2021,,,,
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,benzi1981mechanism,\cite{benzi1981mechanism},The mechanism of stochastic resonance,,,"Benzi, Roberto and Sutera, Alfonso and Vulpiani, Angelo",1981,,,,Journal of Physics A: mathematical and general
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,benzi1982stochastic,\cite{benzi1982stochastic},Stochastic resonance in climatic change,,,"Benzi, Roberto and Parisi, Giorgio and Sutera, Alfonso and Vulpiani, Angelo",1982,,,,Tellus
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,wellens2003stochastic,\cite{wellens2003stochastic},Stochastic Resonance,http://arxiv.org/abs/chao-dyn/9307006v1,"Stochastic resonance (SR) - a counter-intuitive phenomenon in which the
signal due to a weak periodic force in a nonlinear system can be {\it enhanced}
by the addition of external noise - is reviewed. A theoretical approach based
on linear response theory (LRT) is described. It is pointed out that, although
the LRT theory of SR is by definition restricted to the small signal limit, it
possesses substantial advantages in terms of simplicity, generality and
predictive power. The application of LRT to overdamped motion in a bistable
potential, the most commonly studied form of SR, is outlined. Two new forms of
SR, predicted on the basis of LRT and subsequently observed in analogue
electronic experiments, are described.","Wellens, Thomas and Shatokhin, Vyacheslav and Buchleitner, Andreas",2003,,,,Reports on progress in physics
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,kosko2001robust,\cite{kosko2001robust},Robust stochastic resonance: Signal detection and adaptation in impulsive noise,,,"Kosko, Bart and Mitaim, Sanya",2001,,,,Physical review E
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,chen2007theory,\cite{chen2007theory},Theory of the stochastic resonance effect in signal detection: Part Iâ€”Fixed detectors,,,"Chen, Hao and Varshney, Pramod K and Kay, Steven M and Michels, James H",2007,,,,IEEE transactions on Signal Processing
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,shu2016application,\cite{shu2016application},Application of stochastic resonance technology in underwater acoustic weak signal detection,,,"Shu-Yao, Ji and Fei, Yuan and Ke-Yu, Chen and En, Cheng",2016,,,,
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,wang2014adaptive,\cite{wang2014adaptive},Adaptive multiscale noise tuning stochastic resonance for health diagnosis of rolling element bearings,,,"Wang, Jun and He, Qingbo and Kong, Fanrang",2014,,,,IEEE Transactions on instrumentation and measurement
Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,http://arxiv.org/abs/2510.03224v1,lao2024sub,\cite{lao2024sub},Sub-token ViT Embedding via Stochastic Resonance Transformers,http://arxiv.org/abs/2310.03967v2,"Vision Transformer (ViT) architectures represent images as collections of
high-dimensional vectorized tokens, each corresponding to a rectangular
non-overlapping patch. This representation trades spatial granularity for
embedding dimensionality, and results in semantically rich but spatially
coarsely quantized feature maps. In order to retrieve spatial details
beneficial to fine-grained inference tasks we propose a training-free method
inspired by ""stochastic resonance"". Specifically, we perform sub-token spatial
transformations to the input data, and aggregate the resulting ViT features
after applying the inverse transformation. The resulting ""Stochastic Resonance
Transformer"" (SRT) retains the rich semantic information of the original
representation, but grounds it on a finer-scale spatial domain, partly
mitigating the coarse effect of spatial tokenization. SRT is applicable across
any layer of any ViT architecture, consistently boosting performance on several
tasks including segmentation, classification, depth estimation, and others by
up to 14.9% without the need for any fine-tuning.","Lao, Dong and Wu, Yangchao and Liu, Tian Yu and Wong, Alex and Soatto, Stefano",2024,,,,
