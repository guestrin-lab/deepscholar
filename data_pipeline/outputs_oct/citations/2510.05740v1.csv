parent_paper_title,parent_arxiv_link,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,gan,\cite{gan},Conditional Generative Adversarial Nets,http://arxiv.org/abs/1411.1784v1,"Generative Adversarial Nets [8] were recently introduced as a novel way to
train generative models. In this work we introduce the conditional version of
generative adversarial nets, which can be constructed by simply feeding the
data, y, we wish to condition on to both the generator and discriminator. We
show that this model can generate MNIST digits conditioned on class labels. We
also illustrate how this model could be used to learn a multi-modal model, and
provide preliminary examples of an application to image tagging in which we
demonstrate how this approach can generate descriptive tags which are not part
of training labels.","Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua",2014,,,,Advances in neural information processing systems
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,progan,\cite{progan},"Progressive Growing of GANs for Improved Quality, Stability, and
  Variation",http://arxiv.org/abs/1710.10196v3,"We describe a new training methodology for generative adversarial networks.
The key idea is to grow both the generator and discriminator progressively:
starting from a low resolution, we add new layers that model increasingly fine
details as training progresses. This both speeds the training up and greatly
stabilizes it, allowing us to produce images of unprecedented quality, e.g.,
CelebA images at 1024^2. We also propose a simple way to increase the variation
in generated images, and achieve a record inception score of 8.80 in
unsupervised CIFAR10. Additionally, we describe several implementation details
that are important for discouraging unhealthy competition between the generator
and discriminator. Finally, we suggest a new metric for evaluating GAN results,
both in terms of image quality and variation. As an additional contribution, we
construct a higher-quality version of the CelebA dataset.","Karras, Tero",2017,,,,arXiv preprint arXiv:1710.10196
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,biggan,\cite{biggan},Large Scale GAN Training for High Fidelity Natural Image Synthesis,http://arxiv.org/abs/1809.11096v2,"Despite recent progress in generative image modeling, successfully generating
high-resolution, diverse samples from complex datasets such as ImageNet remains
an elusive goal. To this end, we train Generative Adversarial Networks at the
largest scale yet attempted, and study the instabilities specific to such
scale. We find that applying orthogonal regularization to the generator renders
it amenable to a simple ""truncation trick,"" allowing fine control over the
trade-off between sample fidelity and variety by reducing the variance of the
Generator's input. Our modifications lead to models which set the new state of
the art in class-conditional image synthesis. When trained on ImageNet at
128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of
166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous
best IS of 52.52 and FID of 18.6.","Brock, Andrew and Donahue, Jeff and Simonyan, Karen",2018,,,,arXiv preprint arXiv:1809.11096
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,adm,\cite{adm},Diffusion Models Beat GANs on Image Synthesis,http://arxiv.org/abs/2105.05233v4,"We show that diffusion models can achieve image sample quality superior to
the current state-of-the-art generative models. We achieve this on
unconditional image synthesis by finding a better architecture through a series
of ablations. For conditional image synthesis, we further improve sample
quality with classifier guidance: a simple, compute-efficient method for
trading off diversity for fidelity using gradients from a classifier. We
achieve an FID of 2.97 on ImageNet 128$\times$128, 4.59 on ImageNet
256$\times$256, and 7.72 on ImageNet 512$\times$512, and we match BigGAN-deep
even with as few as 25 forward passes per sample, all while maintaining better
coverage of the distribution. Finally, we find that classifier guidance
combines well with upsampling diffusion models, further improving FID to 3.94
on ImageNet 256$\times$256 and 3.85 on ImageNet 512$\times$512. We release our
code at https://github.com/openai/guided-diffusion","Dhariwal, Prafulla and Nichol, Alexander",2021,,,,Advances in neural information processing systems
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,ldm,\cite{ldm},High-Resolution Image Synthesis with Latent Diffusion Models,,,Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and BjÃ¶rn Ommer,2022,,,,
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,glide,\cite{glide},"GLIDE: Towards Photorealistic Image Generation and Editing with
  Text-Guided Diffusion Models",http://arxiv.org/abs/2112.10741v3,"Diffusion models have recently been shown to generate high-quality synthetic
images, especially when paired with a guidance technique to trade off diversity
for fidelity. We explore diffusion models for the problem of text-conditional
image synthesis and compare two different guidance strategies: CLIP guidance
and classifier-free guidance. We find that the latter is preferred by human
evaluators for both photorealism and caption similarity, and often produces
photorealistic samples. Samples from a 3.5 billion parameter text-conditional
diffusion model using classifier-free guidance are favored by human evaluators
to those from DALL-E, even when the latter uses expensive CLIP reranking.
Additionally, we find that our models can be fine-tuned to perform image
inpainting, enabling powerful text-driven image editing. We train a smaller
model on a filtered dataset and release the code and weights at
https://github.com/openai/glide-text2im.","Nichol, Alex and Dhariwal, Prafulla and Ramesh, Aditya and Shyam, Pranav and Mishkin, Pamela and McGrew, Bob and Sutskever, Ilya and Chen, Mark",2021,,,,arXiv preprint arXiv:2112.10741
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,stylegan,\cite{stylegan},A Style-Based Generator Architecture for Generative Adversarial Networks,http://arxiv.org/abs/1812.04948v3,"We propose an alternative generator architecture for generative adversarial
networks, borrowing from style transfer literature. The new architecture leads
to an automatically learned, unsupervised separation of high-level attributes
(e.g., pose and identity when trained on human faces) and stochastic variation
in the generated images (e.g., freckles, hair), and it enables intuitive,
scale-specific control of the synthesis. The new generator improves the
state-of-the-art in terms of traditional distribution quality metrics, leads to
demonstrably better interpolation properties, and also better disentangles the
latent factors of variation. To quantify interpolation quality and
disentanglement, we propose two new, automated methods that are applicable to
any generator architecture. Finally, we introduce a new, highly varied and
high-quality dataset of human faces.","Karras, Tero and Laine, Samuli and Aila, Timo",2019,,,,
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,ddpm,\cite{ddpm},Denoising Diffusion Probabilistic Models,http://arxiv.org/abs/2006.11239v2,"We present high quality image synthesis results using diffusion probabilistic
models, a class of latent variable models inspired by considerations from
nonequilibrium thermodynamics. Our best results are obtained by training on a
weighted variational bound designed according to a novel connection between
diffusion probabilistic models and denoising score matching with Langevin
dynamics, and our models naturally admit a progressive lossy decompression
scheme that can be interpreted as a generalization of autoregressive decoding.
On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and
a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality
similar to ProgressiveGAN. Our implementation is available at
https://github.com/hojonathanho/diffusion","Ho, Jonathan and Jain, Ajay and Abbeel, Pieter",2020,,,,Advances in neural information processing systems
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,sdxl,\cite{sdxl},"SDXL: Improving Latent Diffusion Models for High-Resolution Image
  Synthesis",http://arxiv.org/abs/2307.01952v1,"We present SDXL, a latent diffusion model for text-to-image synthesis.
Compared to previous versions of Stable Diffusion, SDXL leverages a three times
larger UNet backbone: The increase of model parameters is mainly due to more
attention blocks and a larger cross-attention context as SDXL uses a second
text encoder. We design multiple novel conditioning schemes and train SDXL on
multiple aspect ratios. We also introduce a refinement model which is used to
improve the visual fidelity of samples generated by SDXL using a post-hoc
image-to-image technique. We demonstrate that SDXL shows drastically improved
performance compared the previous versions of Stable Diffusion and achieves
results competitive with those of black-box state-of-the-art image generators.
In the spirit of promoting open research and fostering transparency in large
model training and evaluation, we provide access to code and model weights at
https://github.com/Stability-AI/generative-models","Podell, Dustin and English, Zion and Lacey, Kyle and Blattmann, Andreas and Dockhorn, Tim and M{\""u}ller, Jonas and Penna, Joe and Rombach, Robin",2023,,,,arXiv preprint arXiv:2307.01952
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,unet,\cite{unet},U-net: Convolutional networks for biomedical image segmentation,,,"Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas",2015,,,,
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,flux,\cite{flux},FLUX.1 Kontext: Flow Matching for In-Context Image Generation and Editing in Latent Space,,,Black Forest Labs and Stephen Batifol and Andreas Blattmann and Frederic Boesel and Saksham Consul and Cyril Diagne and Tim Dockhorn and Jack English and Zion English and Patrick Esser and Sumith Kulal and Kyle Lacey and Yam Levi and Cheng Li and Dominik Lorenz and Jonas Müller and Dustin Podell and Robin Rombach and Harry Saini and Axel Sauer and Luke Smith,2025,,https://arxiv.org/abs/2506.15742,,
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,sd3.5,\cite{sd3.5},Scaling Rectified Flow Transformers for High-Resolution Image Synthesis,http://arxiv.org/abs/2403.03206v1,"Diffusion models create data from noise by inverting the forward paths of
data towards noise and have emerged as a powerful generative modeling technique
for high-dimensional, perceptual data such as images and videos. Rectified flow
is a recent generative model formulation that connects data and noise in a
straight line. Despite its better theoretical properties and conceptual
simplicity, it is not yet decisively established as standard practice. In this
work, we improve existing noise sampling techniques for training rectified flow
models by biasing them towards perceptually relevant scales. Through a
large-scale study, we demonstrate the superior performance of this approach
compared to established diffusion formulations for high-resolution
text-to-image synthesis. Additionally, we present a novel transformer-based
architecture for text-to-image generation that uses separate weights for the
two modalities and enables a bidirectional flow of information between image
and text tokens, improving text comprehension, typography, and human preference
ratings. We demonstrate that this architecture follows predictable scaling
trends and correlates lower validation loss to improved text-to-image synthesis
as measured by various metrics and human evaluations. Our largest models
outperform state-of-the-art models, and we will make our experimental data,
code, and model weights publicly available.","Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and M{\""u}ller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and others",2024,,,,
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,hidream,\cite{hidream},HiDream-I1: A High-Efficient Image Generative Foundation Model with Sparse Diffusion Transformer,,,"Cai, Qi and Chen, Jingwen and Chen, Yang and Li, Yehao and Long, Fuchen and Pan, Yingwei and Qiu, Zhaofan and Zhang, Yiheng and Gao, Fengbin and Xu, Peihan and others",2025,,,,arXiv preprint arXiv:2505.22705
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,cogview,\cite{cogview},CogView3: Finer and Faster Text-to-Image Generation via Relay Diffusion,http://arxiv.org/abs/2403.05121v1,"Recent advancements in text-to-image generative systems have been largely
driven by diffusion models. However, single-stage text-to-image diffusion
models still face challenges, in terms of computational efficiency and the
refinement of image details. To tackle the issue, we propose CogView3, an
innovative cascaded framework that enhances the performance of text-to-image
diffusion. CogView3 is the first model implementing relay diffusion in the
realm of text-to-image generation, executing the task by first creating
low-resolution images and subsequently applying relay-based super-resolution.
This methodology not only results in competitive text-to-image outputs but also
greatly reduces both training and inference costs. Our experimental results
demonstrate that CogView3 outperforms SDXL, the current state-of-the-art
open-source text-to-image diffusion model, by 77.0\% in human evaluations, all
while requiring only about 1/2 of the inference time. The distilled variant of
CogView3 achieves comparable performance while only utilizing 1/10 of the
inference time by SDXL.","Zheng, Wendi and Teng, Jiayan and Yang, Zhuoyi and Wang, Weihan and Chen, Jidong and Gu, Xiaotao and Dong, Yuxiao and Ding, Ming and Tang, Jie",2024,,,,
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,kandinsky,\cite{kandinsky},"Kandinsky 3: Text-to-Image Synthesis for Multifunctional Generative
  Framework",http://arxiv.org/abs/2410.21061v1,"Text-to-image (T2I) diffusion models are popular for introducing image
manipulation methods, such as editing, image fusion, inpainting, etc. At the
same time, image-to-video (I2V) and text-to-video (T2V) models are also built
on top of T2I models. We present Kandinsky 3, a novel T2I model based on latent
diffusion, achieving a high level of quality and photorealism. The key feature
of the new architecture is the simplicity and efficiency of its adaptation for
many types of generation tasks. We extend the base T2I model for various
applications and create a multifunctional generation system that includes
text-guided inpainting/outpainting, image fusion, text-image fusion, image
variations generation, I2V and T2V generation. We also present a distilled
version of the T2I model, evaluating inference in 4 steps of the reverse
process without reducing image quality and 3 times faster than the base model.
We deployed a user-friendly demo system in which all the features can be tested
in the public domain. Additionally, we released the source code and checkpoints
for the Kandinsky 3 and extended models. Human evaluations show that Kandinsky
3 demonstrates one of the highest quality scores among open source generation
systems.","Vladimir, Arkhipkin and Vasilev, Viacheslav and Filatov, Andrei and Pavlov, Igor and Agafonova, Julia and Gerasimenko, Nikolai and Averchenkova, Anna and Mironova, Evelina and Anton, Bukashkin and Kulikov, Konstantin and others",2024,,,,
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,imagen,\cite{imagen},"Photorealistic Text-to-Image Diffusion Models with Deep Language
  Understanding",http://arxiv.org/abs/2205.11487v1,"We present Imagen, a text-to-image diffusion model with an unprecedented
degree of photorealism and a deep level of language understanding. Imagen
builds on the power of large transformer language models in understanding text
and hinges on the strength of diffusion models in high-fidelity image
generation. Our key discovery is that generic large language models (e.g. T5),
pretrained on text-only corpora, are surprisingly effective at encoding text
for image synthesis: increasing the size of the language model in Imagen boosts
both sample fidelity and image-text alignment much more than increasing the
size of the image diffusion model. Imagen achieves a new state-of-the-art FID
score of 7.27 on the COCO dataset, without ever training on COCO, and human
raters find Imagen samples to be on par with the COCO data itself in image-text
alignment. To assess text-to-image models in greater depth, we introduce
DrawBench, a comprehensive and challenging benchmark for text-to-image models.
With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP,
Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen
over other models in side-by-side comparisons, both in terms of sample quality
and image-text alignment. See https://imagen.research.google/ for an overview
of the results.","Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily L and Ghasemipour, Kamyar and Gontijo Lopes, Raphael and Karagol Ayan, Burcu and Salimans, Tim and others",2022,,,,Advances in neural information processing systems
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,midjourney,\cite{midjourney},MidJourney: AI Image Generation Platform,,,{MidJourney},2025,,,,
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,juggernaut,\cite{juggernaut},Juggernaut XI v11 – text-to-image model,,,{RunDiffusion},2025,,,,
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,dreamshaper,\cite{dreamshaper},DreamShaper – Stable Diffusion 1.5 fine-tuned model,,,{Lykon},2025,,,,
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,dif,\cite{dif},"Deep Image Fingerprint: Towards Low Budget Synthetic Image Detection and
  Model Lineage Analysis",http://arxiv.org/abs/2303.10762v4,"The generation of high-quality images has become widely accessible and is a
rapidly evolving process. As a result, anyone can generate images that are
indistinguishable from real ones. This leads to a wide range of applications,
including malicious usage with deceptive intentions. Despite advances in
detection techniques for generated images, a robust detection method still
eludes us. Furthermore, model personalization techniques might affect the
detection capabilities of existing methods. In this work, we utilize the
architectural properties of convolutional neural networks (CNNs) to develop a
new detection method. Our method can detect images from a known generative
model and enable us to establish relationships between fine-tuned generative
models. We tested the method on images produced by both Generative Adversarial
Networks (GANs) and recent large text-to-image models (LTIMs) that rely on
Diffusion Models. Our approach outperforms others trained under identical
conditions and achieves comparable performance to state-of-the-art pre-trained
detection methods on images generated by Stable Diffusion and MidJourney, with
significantly fewer required train samples.","Sinitsa, Sergey and Fried, Ohad",2024,,,,
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,fredect,\cite{fredect},Leveraging Frequency Analysis for Deep Fake Image Recognition,http://arxiv.org/abs/2003.08685v3,"Deep neural networks can generate images that are astonishingly realistic, so
much so that it is often hard for humans to distinguish them from actual
photos. These achievements have been largely made possible by Generative
Adversarial Networks (GANs). While deep fake images have been thoroughly
investigated in the image domain - a classical approach from the area of image
forensics - an analysis in the frequency domain has been missing so far. In
this paper, we address this shortcoming and our results reveal that in
frequency space, GAN-generated images exhibit severe artifacts that can be
easily identified. We perform a comprehensive analysis, showing that these
artifacts are consistent across different neural network architectures, data
sets, and resolutions. In a further investigation, we demonstrate that these
artifacts are caused by upsampling operations found in all current GAN
architectures, indicating a structural and fundamental problem in the way
images are generated via GANs. Based on this analysis, we demonstrate how the
frequency representation can be used to identify deep fake images in an
automated way, surpassing state-of-the-art methods.","Frank, Joel and Eisenhofer, Thorsten and Sch{\""o}nherr, Lea and Fischer, Asja and Kolossa, Dorothea and Holz, Thorsten",2020,,,,
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,npr,\cite{npr},"Rethinking the Up-Sampling Operations in CNN-based Generative Network
  for Generalizable Deepfake Detection",http://arxiv.org/abs/2312.10461v2,"Recently, the proliferation of highly realistic synthetic images, facilitated
through a variety of GANs and Diffusions, has significantly heightened the
susceptibility to misuse. While the primary focus of deepfake detection has
traditionally centered on the design of detection algorithms, an investigative
inquiry into the generator architectures has remained conspicuously absent in
recent years. This paper contributes to this lacuna by rethinking the
architectures of CNN-based generators, thereby establishing a generalized
representation of synthetic artifacts. Our findings illuminate that the
up-sampling operator can, beyond frequency-based artifacts, produce generalized
forgery artifacts. In particular, the local interdependence among image pixels
caused by upsampling operators is significantly demonstrated in synthetic
images generated by GAN or diffusion. Building upon this observation, we
introduce the concept of Neighboring Pixel Relationships(NPR) as a means to
capture and characterize the generalized structural artifacts stemming from
up-sampling operations. A comprehensive analysis is conducted on an open-world
dataset, comprising samples generated by \tft{28 distinct generative models}.
This analysis culminates in the establishment of a novel state-of-the-art
performance, showcasing a remarkable \tft{11.6\%} improvement over existing
methods. The code is available at
https://github.com/chuangchuangtan/NPR-DeepfakeDetection.","Tan, Chuangchuang and Zhao, Yao and Wei, Shikui and Gu, Guanghua and Liu, Ping and Wei, Yunchao",2024,,,,
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,f3net,\cite{f3net},"Thinking in Frequency: Face Forgery Detection by Mining Frequency-aware
  Clues",http://arxiv.org/abs/2007.09355v2,"As realistic facial manipulation technologies have achieved remarkable
progress, social concerns about potential malicious abuse of these technologies
bring out an emerging research topic of face forgery detection. However, it is
extremely challenging since recent advances are able to forge faces beyond the
perception ability of human eyes, especially in compressed images and videos.
We find that mining forgery patterns with the awareness of frequency could be a
cure, as frequency provides a complementary viewpoint where either subtle
forgery artifacts or compression errors could be well described. To introduce
frequency into the face forgery detection, we propose a novel Frequency in Face
Forgery Network (F3-Net), taking advantages of two different but complementary
frequency-aware clues, 1) frequency-aware decomposed image components, and 2)
local frequency statistics, to deeply mine the forgery patterns via our
two-stream collaborative learning framework. We apply DCT as the applied
frequency-domain transformation. Through comprehensive studies, we show that
the proposed F3-Net significantly outperforms competing state-of-the-art
methods on all compression qualities in the challenging FaceForensics++
dataset, especially wins a big lead upon low-quality media.","Qian, Yuyang and Yin, Guojun and Sheng, Lu and Chen, Zixuan and Shao, Jing",2020,,,,
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,patchcraft,\cite{patchcraft},"PatchCraft: Exploring Texture Patch for Efficient AI-generated Image
  Detection",http://arxiv.org/abs/2311.12397v3,"Recent generative models show impressive performance in generating
photographic images. Humans can hardly distinguish such incredibly
realistic-looking AI-generated images from real ones. AI-generated images may
lead to ubiquitous disinformation dissemination. Therefore, it is of utmost
urgency to develop a detector to identify AI generated images. Most existing
detectors suffer from sharp performance drops over unseen generative models. In
this paper, we propose a novel AI-generated image detector capable of
identifying fake images created by a wide range of generative models. We
observe that the texture patches of images tend to reveal more traces left by
generative models compared to the global semantic information of the images. A
novel Smash&Reconstruction preprocessing is proposed to erase the global
semantic information and enhance texture patches. Furthermore, pixels in rich
texture regions exhibit more significant fluctuations than those in poor
texture regions. Synthesizing realistic rich texture regions proves to be more
challenging for existing generative models. Based on this principle, we
leverage the inter-pixel correlation contrast between rich and poor texture
regions within an image to further boost the detection performance.
  In addition, we build a comprehensive AI-generated image detection benchmark,
which includes 17 kinds of prevalent generative models, to evaluate the
effectiveness of existing baselines and our approach. Our benchmark provides a
leaderboard for follow-up studies. Extensive experimental results show that our
approach outperforms state-of-the-art baselines by a significant margin. Our
project: https://fdmas.github.io/AIGCDetect","Zhong, Nan and Xu, Yiran and Li, Sheng and Qian, Zhenxing and Zhang, Xinpeng",2023,,,,arXiv preprint arXiv:2311.12397
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,lgrad,\cite{lgrad},Learning on gradients: Generalized artifacts representation for gan-generated images detection,,,"Tan, Chuangchuang and Zhao, Yao and Wei, Shikui and Gu, Guanghua and Wei, Yunchao",2023,,,,
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,ssp,\cite{ssp},A Single Simple Patch is All You Need for AI-generated Image Detection,http://arxiv.org/abs/2402.01123v2,"The recent development of generative models unleashes the potential of
generating hyper-realistic fake images. To prevent the malicious usage of fake
images, AI-generated image detection aims to distinguish fake images from real
images. However, existing method suffer from severe performance drop when
detecting images generated by unseen generators. We find that generative models
tend to focus on generating the patches with rich textures to make the images
more realistic while neglecting the hidden noise caused by camera capture
present in simple patches. In this paper, we propose to exploit the noise
pattern of a single simple patch to identify fake images. Furthermore, due to
the performance decline when handling low-quality generated images, we
introduce an enhancement module and a perception module to remove the
interfering information. Extensive experiments demonstrate that our method can
achieve state-of-the-art performance on public benchmarks.","Chen, Jiaxuan and Yao, Jieteng and Niu, Li",2024,,,,arXiv preprint arXiv:2402.01123
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,dnf,\cite{dnf},Diffusion Noise Feature: Accurate and Fast Generated Image Detection,http://arxiv.org/abs/2312.02625v3,"Generative models now produce images with such stunning realism that they can
easily deceive the human eye. While this progress unlocks vast creative
potential, it also presents significant risks, such as the spread of
misinformation. Consequently, detecting generated images has become a critical
research challenge. However, current detection methods are often plagued by low
accuracy and poor generalization. In this paper, to address these limitations
and enhance the detection of generated images, we propose a novel
representation, Diffusion Noise Feature (DNF). Derived from the inverse process
of diffusion models, DNF effectively amplifies the subtle, high-frequency
artifacts that act as fingerprints of artificial generation. Our key insight is
that real and generated images exhibit distinct DNF signatures, providing a
robust basis for differentiation. By training a simple classifier such as
ResNet-50 on DNF, our approach achieves remarkable accuracy, robustness, and
generalization in detecting generated images, including those from unseen
generators or with novel content. Extensive experiments across four training
datasets and five test sets confirm that DNF establishes a new state-of-the-art
in generated image detection. The code is available at
https://github.com/YichiCS/Diffusion-Noise-Feature.","Zhang, Y and Xu, X",2023,,,,arXiv preprint arXiv:2312.02625
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,lnp,\cite{lnp},Detecting Generated Images by Real Images Only,http://arxiv.org/abs/2311.00962v1,"As deep learning technology continues to evolve, the images yielded by
generative models are becoming more and more realistic, triggering people to
question the authenticity of images. Existing generated image detection methods
detect visual artifacts in generated images or learn discriminative features
from both real and generated images by massive training. This learning paradigm
will result in efficiency and generalization issues, making detection methods
always lag behind generation methods. This paper approaches the generated image
detection problem from a new perspective: Start from real images. By finding
the commonality of real images and mapping them to a dense subspace in feature
space, the goal is that generated images, regardless of their generative model,
are then projected outside the subspace. As a result, images from different
generative models can be detected, solving some long-existing problems in the
field. Experimental results show that although our method was trained only by
real images and uses 99.9\% less training data than other deep learning-based
methods, it can compete with state-of-the-art methods and shows excellent
performance in detecting emerging generative models with high inference
efficiency. Moreover, the proposed method shows robustness against various
post-processing. These advantages allow the method to be used in real-world
scenarios.","Liu, Bo and Yang, Fan and Bi, Xiuli and Xiao, Bin and Li, Weisheng and Gao, Xinbo",2022,,,,
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,dire,\cite{dire},DIRE for Diffusion-Generated Image Detection,http://arxiv.org/abs/2303.09295v1,"Diffusion models have shown remarkable success in visual synthesis, but have
also raised concerns about potential abuse for malicious purposes. In this
paper, we seek to build a detector for telling apart real images from
diffusion-generated images. We find that existing detectors struggle to detect
images generated by diffusion models, even if we include generated images from
a specific diffusion model in their training data. To address this issue, we
propose a novel image representation called DIffusion Reconstruction Error
(DIRE), which measures the error between an input image and its reconstruction
counterpart by a pre-trained diffusion model. We observe that
diffusion-generated images can be approximately reconstructed by a diffusion
model while real images cannot. It provides a hint that DIRE can serve as a
bridge to distinguish generated and real images. DIRE provides an effective way
to detect images generated by most diffusion models, and it is general for
detecting generated images from unseen diffusion models and robust to various
perturbations. Furthermore, we establish a comprehensive diffusion-generated
benchmark including images generated by eight diffusion models to evaluate the
performance of diffusion-generated image detectors. Extensive experiments on
our collected benchmark demonstrate that DIRE exhibits superiority over
previous generated-image detectors. The code and dataset are available at
https://github.com/ZhendongWang6/DIRE.","Wang, Zhendong and Bao, Jianmin and Zhou, Wengang and Wang, Weilun and Hu, Hezhen and Chen, Hong and Li, Houqiang",2023,,,,
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,sedid,\cite{sedid},Exposing the Fake: Effective Diffusion-Generated Images Detection,http://arxiv.org/abs/2307.06272v1,"Image synthesis has seen significant advancements with the advent of
diffusion-based generative models like Denoising Diffusion Probabilistic Models
(DDPM) and text-to-image diffusion models. Despite their efficacy, there is a
dearth of research dedicated to detecting diffusion-generated images, which
could pose potential security and privacy risks. This paper addresses this gap
by proposing a novel detection method called Stepwise Error for
Diffusion-generated Image Detection (SeDID). Comprising statistical-based
$\text{SeDID}_{\text{Stat}}$ and neural network-based
$\text{SeDID}_{\text{NNs}}$, SeDID exploits the unique attributes of diffusion
models, namely deterministic reverse and deterministic denoising computation
errors. Our evaluations demonstrate SeDID's superior performance over existing
methods when applied to diffusion models. Thus, our work makes a pivotal
contribution to distinguishing diffusion model-generated images, marking a
significant step in the domain of artificial intelligence security.","Ma, RuiPeng and Duan, Jinhao and Kong, Fei and Shi, Xiaoshuang and Xu, Kaidi",2023,,,,
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,aeroblade,\cite{aeroblade},"AEROBLADE: Training-Free Detection of Latent Diffusion Images Using
  Autoencoder Reconstruction Error",http://arxiv.org/abs/2401.17879v2,"With recent text-to-image models, anyone can generate deceptively realistic
images with arbitrary contents, fueling the growing threat of visual
disinformation. A key enabler for generating high-resolution images with low
computational cost has been the development of latent diffusion models (LDMs).
In contrast to conventional diffusion models, LDMs perform the denoising
process in the low-dimensional latent space of a pre-trained autoencoder (AE)
instead of the high-dimensional image space. Despite their relevance, the
forensic analysis of LDMs is still in its infancy. In this work we propose
AEROBLADE, a novel detection method which exploits an inherent component of
LDMs: the AE used to transform images between image and latent space. We find
that generated images can be more accurately reconstructed by the AE than real
images, allowing for a simple detection approach based on the reconstruction
error. Most importantly, our method is easy to implement and does not require
any training, yet nearly matches the performance of detectors that rely on
extensive training. We empirically demonstrate that AEROBLADE is effective
against state-of-the-art LDMs, including Stable Diffusion and Midjourney.
Beyond detection, our approach allows for the qualitative analysis of images,
which can be leveraged for identifying inpainted regions. We release our code
and data at https://github.com/jonasricker/aeroblade .","Ricker, Jonas and Lukovnikov, Denis and Fischer, Asja",2024,,,,
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,lare,\cite{lare},"LaRE$^2$: Latent Reconstruction Error Based Method for
  Diffusion-Generated Image Detection",http://arxiv.org/abs/2403.17465v4,"The evolution of Diffusion Models has dramatically improved image generation
quality, making it increasingly difficult to differentiate between real and
generated images. This development, while impressive, also raises significant
privacy and security concerns. In response to this, we propose a novel Latent
REconstruction error guided feature REfinement method (LaRE^2) for detecting
the diffusion-generated images. We come up with the Latent Reconstruction Error
(LaRE), the first reconstruction-error based feature in the latent space for
generated image detection. LaRE surpasses existing methods in terms of feature
extraction efficiency while preserving crucial cues required to differentiate
between the real and the fake. To exploit LaRE, we propose an Error-Guided
feature REfinement module (EGRE), which can refine the image feature guided by
LaRE to enhance the discriminativeness of the feature. Our EGRE utilizes an
align-then-refine mechanism, which effectively refines the image feature for
generated-image detection from both spatial and channel perspectives. Extensive
experiments on the large-scale GenImage benchmark demonstrate the superiority
of our LaRE^2, which surpasses the best SoTA method by up to 11.9%/12.1%
average ACC/AP across 8 different image generators. LaRE also surpasses
existing methods in terms of feature extraction cost, delivering an impressive
speed enhancement of 8 times. Code is available.","Luo, Yunpeng and Du, Junlong and Yan, Ke and Ding, Shouhong",2024,,,,
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,drct,\cite{drct},Drct: Diffusion reconstruction contrastive training towards universal detection of diffusion generated images,,,"Chen, Baoying and Zeng, Jishen and Yang, Jianquan and Yang, Rui",2024,,,,
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,fakeinversion,\cite{fakeinversion},"FakeInversion: Learning to Detect Images from Unseen Text-to-Image
  Models by Inverting Stable Diffusion",http://arxiv.org/abs/2406.08603v1,"Due to the high potential for abuse of GenAI systems, the task of detecting
synthetic images has recently become of great interest to the research
community. Unfortunately, existing image-space detectors quickly become
obsolete as new high-fidelity text-to-image models are developed at blinding
speed. In this work, we propose a new synthetic image detector that uses
features obtained by inverting an open-source pre-trained Stable Diffusion
model. We show that these inversion features enable our detector to generalize
well to unseen generators of high visual fidelity (e.g., DALL-E 3) even when
the detector is trained only on lower fidelity fake images generated via Stable
Diffusion. This detector achieves new state-of-the-art across multiple training
and evaluation setups. Moreover, we introduce a new challenging evaluation
protocol that uses reverse image search to mitigate stylistic and thematic
biases in the detector evaluation. We show that the resulting evaluation scores
align well with detectors' in-the-wild performance, and release these datasets
as public benchmarks for future research.","Cazenavette, George and Sud, Avneesh and Leung, Thomas and Usman, Ben",2024,,,,
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,distildire,\cite{distildire},"DistilDIRE: A Small, Fast, Cheap and Lightweight Diffusion Synthesized
  Deepfake Detection",http://arxiv.org/abs/2406.00856v1,"A dramatic influx of diffusion-generated images has marked recent years,
posing unique challenges to current detection technologies. While the task of
identifying these images falls under binary classification, a seemingly
straightforward category, the computational load is significant when employing
the ""reconstruction then compare"" technique. This approach, known as DIRE
(Diffusion Reconstruction Error), not only identifies diffusion-generated
images but also detects those produced by GANs, highlighting the technique's
broad applicability. To address the computational challenges and improve
efficiency, we propose distilling the knowledge embedded in diffusion models to
develop rapid deepfake detection models. Our approach, aimed at creating a
small, fast, cheap, and lightweight diffusion synthesized deepfake detector,
maintains robust performance while significantly reducing operational demands.
Maintaining performance, our experimental results indicate an inference speed
3.2 times faster than the existing DIRE framework. This advance not only
enhances the practicality of deploying these systems in real-world settings but
also paves the way for future research endeavors that seek to leverage
diffusion model knowledge.","Lim, Yewon and Lee, Changyeon and Kim, Aerin and Etzioni, Oren",2024,,,,
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,aide,\cite{aide},A Sanity Check for AI-generated Image Detection,http://arxiv.org/abs/2406.19435v3,"With the rapid development of generative models, discerning AI-generated
content has evoked increasing attention from both industry and academia. In
this paper, we conduct a sanity check on ""whether the task of AI-generated
image detection has been solved"". To start with, we present Chameleon dataset,
consisting AIgenerated images that are genuinely challenging for human
perception. To quantify the generalization of existing methods, we evaluate 9
off-the-shelf AI-generated image detectors on Chameleon dataset. Upon analysis,
almost all models classify AI-generated images as real ones. Later, we propose
AIDE (AI-generated Image DEtector with Hybrid Features), which leverages
multiple experts to simultaneously extract visual artifacts and noise patterns.
Specifically, to capture the high-level semantics, we utilize CLIP to compute
the visual embedding. This effectively enables the model to discern
AI-generated images based on semantics or contextual information; Secondly, we
select the highest frequency patches and the lowest frequency patches in the
image, and compute the low-level patchwise features, aiming to detect
AI-generated images by low-level artifacts, for example, noise pattern,
anti-aliasing, etc. While evaluating on existing benchmarks, for example,
AIGCDetectBenchmark and GenImage, AIDE achieves +3.5% and +4.6% improvements to
state-of-the-art methods, and on our proposed challenging Chameleon benchmarks,
it also achieves the promising results, despite this problem for detecting
AI-generated images is far from being solved.","Yan, Shilin and Li, Ouxiang and Cai, Jiayin and Hao, Yanbin and Jiang, Xiaolong and Hu, Yao and Xie, Weidi",2025,,,,
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,unifd,\cite{unifd},"Towards Universal Fake Image Detectors that Generalize Across Generative
  Models",http://arxiv.org/abs/2302.10174v2,"With generative models proliferating at a rapid rate, there is a growing need
for general purpose fake image detectors. In this work, we first show that the
existing paradigm, which consists of training a deep network for real-vs-fake
classification, fails to detect fake images from newer breeds of generative
models when trained to detect GAN fake images. Upon analysis, we find that the
resulting classifier is asymmetrically tuned to detect patterns that make an
image fake. The real class becomes a sink class holding anything that is not
fake, including generated images from models not accessible during training.
Building upon this discovery, we propose to perform real-vs-fake classification
without learning; i.e., using a feature space not explicitly trained to
distinguish real from fake images. We use nearest neighbor and linear probing
as instantiations of this idea. When given access to the feature space of a
large pretrained vision-language model, the very simple baseline of nearest
neighbor classification has surprisingly good generalization ability in
detecting fake images from a wide variety of generative models; e.g., it
improves upon the SoTA by +15.07 mAP and +25.90% acc when tested on unseen
diffusion and autoregressive models.","Ojha, Utkarsh and Li, Yuheng and Lee, Yong Jae",2023,,,,
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,bilora,\cite{bilora},Bi-LORA: A Vision-Language Approach for Synthetic Image Detection,,,"Keita, Mamadou and Hamidouche, Wassim and Eutamene, Hessen Bougueffa and Taleb-Ahmed, Abdelmalik and Camacho, David and Hadid, Abdenour",2025,,,,Expert Systems
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,clip,\cite{clip},Learning Transferable Visual Models From Natural Language Supervision,http://arxiv.org/abs/2103.00020v1,"State-of-the-art computer vision systems are trained to predict a fixed set
of predetermined object categories. This restricted form of supervision limits
their generality and usability since additional labeled data is needed to
specify any other visual concept. Learning directly from raw text about images
is a promising alternative which leverages a much broader source of
supervision. We demonstrate that the simple pre-training task of predicting
which caption goes with which image is an efficient and scalable way to learn
SOTA image representations from scratch on a dataset of 400 million (image,
text) pairs collected from the internet. After pre-training, natural language
is used to reference learned visual concepts (or describe new ones) enabling
zero-shot transfer of the model to downstream tasks. We study the performance
of this approach by benchmarking on over 30 different existing computer vision
datasets, spanning tasks such as OCR, action recognition in videos,
geo-localization, and many types of fine-grained object classification. The
model transfers non-trivially to most tasks and is often competitive with a
fully supervised baseline without the need for any dataset specific training.
For instance, we match the accuracy of the original ResNet-50 on ImageNet
zero-shot without needing to use any of the 1.28 million training examples it
was trained on. We release our code and pre-trained model weights at
https://github.com/OpenAI/CLIP.","Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",2021,,,,
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,lasted,\cite{lasted},"Generalizable Synthetic Image Detection via Language-guided Contrastive
  Learning",http://arxiv.org/abs/2305.13800v2,"The heightened realism of AI-generated images can be attributed to the rapid
development of synthetic models, including generative adversarial networks
(GANs) and diffusion models (DMs). The malevolent use of synthetic images, such
as the dissemination of fake news or the creation of fake profiles, however,
raises significant concerns regarding the authenticity of images. Though many
forensic algorithms have been developed for detecting synthetic images, their
performance, especially the generalization capability, is still far from being
adequate to cope with the increasing number of synthetic models. In this work,
we propose a simple yet very effective synthetic image detection method via a
language-guided contrastive learning. Specifically, we augment the training
images with carefully-designed textual labels, enabling us to use a joint
visual-language contrastive supervision for learning a forensic feature space
with better generalization. It is shown that our proposed LanguAge-guided
SynThEsis Detection (LASTED) model achieves much improved generalizability to
unseen image generation models and delivers promising performance that far
exceeds state-of-the-art competitors over four datasets. The code is available
at https://github.com/HighwayWu/LASTED.","Wu, Haiwei and Zhou, Jiantao and Zhang, Shile",2023,,,,arXiv preprint arXiv:2305.13800
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,dct,\cite{dct},On discrete cosine transform,http://arxiv.org/abs/1109.0337v1,"The discrete cosine transform (DCT), introduced by Ahmed, Natarajan and Rao,
has been used in many applications of digital signal processing, data
compression and information hiding. There are four types of the discrete cosine
transform. In simulating the discrete cosine transform, we propose a
generalized discrete cosine transform with three parameters, and prove its
orthogonality for some new cases. A new type of discrete cosine transform is
proposed and its orthogonality is proved. Finally, we propose a generalized
discrete W transform with three parameters, and prove its orthogonality for
some new cases.","Ahmed, Nasir and Natarajan, T\_ and Rao, Kamisetty R",2006,,,,IEEE transactions on Computers
Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,http://arxiv.org/abs/2510.05740v1,srm,\cite{srm},Rich models for steganalysis of digital images,,,"Fridrich, Jessica and Kodovsky, Jan",2012,,,,IEEE Transactions on information Forensics and Security
