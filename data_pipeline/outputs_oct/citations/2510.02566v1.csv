parent_paper_title,parent_arxiv_link,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,SMPL:2015,\cite{SMPL:2015},{SMPL}: A Skinned Multi-Person Linear Model,,,"Loper, Matthew and Mahmood, Naureen and Romero, Javier and Pons-Moll, Gerard and Black, Michael J.",2015,,,,ACM Transactions on Graphics (TOG)
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,STAR:2020,\cite{STAR:2020},STAR: Sparse Trained Articulated Human Body Regressor,http://arxiv.org/abs/2008.08535v1,"The SMPL body model is widely used for the estimation, synthesis, and
analysis of 3D human pose and shape. While popular, we show that SMPL has
several limitations and introduce STAR, which is quantitatively and
qualitatively superior to SMPL. First, SMPL has a huge number of parameters
resulting from its use of global blend shapes. These dense pose-corrective
offsets relate every vertex on the mesh to all the joints in the kinematic
tree, capturing spurious long-range correlations. To address this, we define
per-joint pose correctives and learn the subset of mesh vertices that are
influenced by each joint movement. This sparse formulation results in more
realistic deformations and significantly reduces the number of model parameters
to 20% of SMPL. When trained on the same data as SMPL, STAR generalizes better
despite having many fewer parameters. Second, SMPL factors pose-dependent
deformations from body shape while, in reality, people with different shapes
deform differently. Consequently, we learn shape-dependent pose-corrective
blend shapes that depend on both body pose and BMI. Third, we show that the
shape space of SMPL is not rich enough to capture the variation in the human
population. We address this by training STAR with an additional 10,000 scans of
male and female subjects, and show that this results in better model
generalization. STAR is compact, generalizes better to new bodies and is a
drop-in replacement for SMPL. STAR is publicly available for research purposes
at http://star.is.tue.mpg.de.",Ahmed A. A. Osman and Timo Bolkart and Michael J. Black,2020,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,SMPL-X:2019,\cite{SMPL-X:2019},"Expressive Body Capture: 3D Hands, Face, and Body from a Single Image",http://arxiv.org/abs/1904.05866v1,"To facilitate the analysis of human actions, interactions and emotions, we
compute a 3D model of human body pose, hand pose, and facial expression from a
single monocular image. To achieve this, we use thousands of 3D scans to train
a new, unified, 3D model of the human body, SMPL-X, that extends SMPL with
fully articulated hands and an expressive face. Learning to regress the
parameters of SMPL-X directly from images is challenging without paired images
and 3D ground truth. Consequently, we follow the approach of SMPLify, which
estimates 2D features and then optimizes model parameters to fit the features.
We improve on SMPLify in several significant ways: (1) we detect 2D features
corresponding to the face, hands, and feet and fit the full SMPL-X model to
these; (2) we train a new neural network pose prior using a large MoCap
dataset; (3) we define a new interpenetration penalty that is both fast and
accurate; (4) we automatically detect gender and the appropriate body models
(male, female, or neutral); (5) our PyTorch implementation achieves a speedup
of more than 8x over Chumpy. We use the new method, SMPLify-X, to fit SMPL-X to
both controlled images and images in the wild. We evaluate 3D accuracy on a new
curated dataset comprising 100 images with pseudo ground-truth. This is a step
towards automatic expressive human capture from monocular RGB data. The models,
code, and data are available for research purposes at
https://smpl-x.is.tue.mpg.de.",Georgios Pavlakos and Vasileios Choutas and Nima Ghorbani and Timo Bolkart and Ahmed A. A. Osman and Dimitrios Tzionas and Michael J. Black,2019,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,50649,\cite{50649},GHUM \& GHUML: Generative 3D Human Shape and Articulated Pose Models,,,Hongyi Xu and Eduard Gabriel Bazavan and Andrei Zanfir and Bill Freeman and Rahul Sukthankar and Cristian Sminchisescu,2020,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,10.1007/978-3-319-46454-1_34,\cite{10.1007/978-3-319-46454-1_34},"Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a
  Single Image",http://arxiv.org/abs/1607.08128v1,"We describe the first method to automatically estimate the 3D pose of the
human body as well as its 3D shape from a single unconstrained image. We
estimate a full 3D mesh and show that 2D joints alone carry a surprising amount
of information about body shape. The problem is challenging because of the
complexity of the human body, articulation, occlusion, clothing, lighting, and
the inherent ambiguity in inferring 3D from 2D. To solve this, we first use a
recently published CNN-based method, DeepCut, to predict (bottom-up) the 2D
body joint locations. We then fit (top-down) a recently published statistical
body shape model, called SMPL, to the 2D joints. We do so by minimizing an
objective function that penalizes the error between the projected 3D model
joints and detected 2D joints. Because SMPL captures correlations in human
shape across the population, we are able to robustly fit it to very little
data. We further leverage the 3D model to prevent solutions that cause
interpenetration. We evaluate our method, SMPLify, on the Leeds Sports,
HumanEva, and Human3.6M datasets, showing superior pose accuracy with respect
to the state of the art.","Bogo, Federica
and Kanazawa, Angjoo
and Lassner, Christoph
and Gehler, Peter
and Romero, Javier
and Black, Michael J.",2016,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,Arnab_CVPR_2019,\cite{Arnab_CVPR_2019},Exploiting temporal context for 3D human pose estimation in the wild,http://arxiv.org/abs/1905.04266v1,"We present a bundle-adjustment-based algorithm for recovering accurate 3D
human pose and meshes from monocular videos. Unlike previous algorithms which
operate on single frames, we show that reconstructing a person over an entire
sequence gives extra constraints that can resolve ambiguities. This is because
videos often give multiple views of a person, yet the overall body shape does
not change and 3D positions vary slowly. Our method improves not only on
standard mocap-based datasets like Human 3.6M -- where we show quantitative
improvements -- but also on challenging in-the-wild datasets such as Kinetics.
Building upon our algorithm, we present a new dataset of more than 3 million
frames of YouTube videos from Kinetics with automatically generated 3D poses
and meshes. We show that retraining a single-frame 3D pose estimator on this
data improves accuracy on both real-world and mocap data by evaluating on the
3DPW and HumanEVA datasets.",Anurag Arnab and Carl Doersch and Andrew Zisserman,2019,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,MuVS:3DV:2017,\cite{MuVS:3DV:2017},Towards Accurate Marker-less Human Shape and Pose Estimation over Time,,,Yinghao Huang and Federica Bogo and Christoph Lassner and Angjoo Kanazawa and Peter V. Gehler and Javier Romero and Ijaz Akhter and Michael J. Black,2017,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,xiang2019monocular,\cite{xiang2019monocular},"Monocular Total Capture: Posing Face, Body, and Hands in the Wild",http://arxiv.org/abs/1812.01598v1,"We present the first method to capture the 3D total motion of a target person
from a monocular view input. Given an image or a monocular video, our method
reconstructs the motion from body, face, and fingers represented by a 3D
deformable mesh model. We use an efficient representation called 3D Part
Orientation Fields (POFs), to encode the 3D orientations of all body parts in
the common 2D image space. POFs are predicted by a Fully Convolutional Network
(FCN), along with the joint confidence maps. To train our network, we collect a
new 3D human motion dataset capturing diverse total body motion of 40 subjects
in a multiview system. We leverage a 3D deformable human model to reconstruct
total body pose from the CNN outputs by exploiting the pose and shape prior in
the model. We also present a texture-based tracking method to obtain temporally
coherent motion capture output. We perform thorough quantitative evaluations
including comparison with the existing body-specific and hand-specific methods,
and performance analysis on camera viewpoint and human pose changes. Finally,
we demonstrate the results of our total body motion capture on various
challenging in-the-wild videos. Our code and newly collected human motion
dataset will be publicly shared.",Donglai Xiang and Hanbyul Joo and Yaser Sheikh,2019,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,goel2023humans,\cite{goel2023humans},Humans in 4D: Reconstructing and Tracking Humans with Transformers,http://arxiv.org/abs/2305.20091v3,"We present an approach to reconstruct humans and track them over time. At the
core of our approach, we propose a fully ""transformerized"" version of a network
for human mesh recovery. This network, HMR 2.0, advances the state of the art
and shows the capability to analyze unusual poses that have in the past been
difficult to reconstruct from single images. To analyze video, we use 3D
reconstructions from HMR 2.0 as input to a tracking system that operates in 3D.
This enables us to deal with multiple people and maintain identities through
occlusion events. Our complete approach, 4DHumans, achieves state-of-the-art
results for tracking people from monocular video. Furthermore, we demonstrate
the effectiveness of HMR 2.0 on the downstream task of action recognition,
achieving significant improvements over previous pose-based action recognition
approaches. Our code and models are available on the project website:
https://shubham-goel.github.io/4dhumans/.",Shubham Goel and Georgios Pavlakos and Jathushan Rajasegaran and Angjoo Kanazawa and Jitendra Malik,2023,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,cai2023smplerx,\cite{cai2023smplerx},{SMPLer-X}: Scaling Up Expressive Human Pose and Shape Estimation,,,Zhongang Cai and Wanqi Yin and Ailing Zeng and Chen Wei and Qingping Sun and Yanjun Wang and Hui En Pang and Haiyi Mei and Mingyuan Zhang and Lei Zhang and Chen Change Loy and Lei Yang and Ziwei Liu,2023,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,yin2025smplest,\cite{yin2025smplest},{SMPLest-X}: Ultimate Scaling for Expressive Human Pose and Shape Estimation,,,Wanqi Yin and Zhongang Cai and Ruisi Wang and Ailing Zeng and Chen Wei and Qingping Sun and Haiyi Mei and Yanjun Wang and Hui En Pang and Mingyuan Zhang and Lei Zhang and Chen Change Loy and Atsushi Yamashita and Lei Yang and Ziwei Liu,2025,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,TRACE,\cite{TRACE},"TRACE: 5D Temporal Regression of Avatars with Dynamic Cameras in 3D
  Environments",http://arxiv.org/abs/2306.02850v2,"Although the estimation of 3D human pose and shape (HPS) is rapidly
progressing, current methods still cannot reliably estimate moving humans in
global coordinates, which is critical for many applications. This is
particularly challenging when the camera is also moving, entangling human and
camera motion. To address these issues, we adopt a novel 5D representation
(space, time, and identity) that enables end-to-end reasoning about people in
scenes. Our method, called TRACE, introduces several novel architectural
components. Most importantly, it uses two new ""maps"" to reason about the 3D
trajectory of people over time in camera, and world, coordinates. An additional
memory unit enables persistent tracking of people even during long occlusions.
TRACE is the first one-stage method to jointly recover and track 3D humans in
global coordinates from dynamic cameras. By training it end-to-end, and using
full image information, TRACE achieves state-of-the-art performance on tracking
and HPS benchmarks. The code and dataset are released for research purposes.",Yu Sun and Qian Bao and Wu Liu and Tao Mei and Michael J. Black,2023,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,WHAM,\cite{WHAM},WHAM: Reconstructing World-grounded Humans with Accurate 3D Motion,http://arxiv.org/abs/2312.07531v2,"The estimation of 3D human motion from video has progressed rapidly but
current methods still have several key limitations. First, most methods
estimate the human in camera coordinates. Second, prior work on estimating
humans in global coordinates often assumes a flat ground plane and produces
foot sliding. Third, the most accurate methods rely on computationally
expensive optimization pipelines, limiting their use to offline applications.
Finally, existing video-based methods are surprisingly less accurate than
single-frame methods. We address these limitations with WHAM (World-grounded
Humans with Accurate Motion), which accurately and efficiently reconstructs 3D
human motion in a global coordinate system from video. WHAM learns to lift 2D
keypoint sequences to 3D using motion capture data and fuses this with video
features, integrating motion context and visual information. WHAM exploits
camera angular velocity estimated from a SLAM method together with human motion
to estimate the body's global trajectory. We combine this with a contact-aware
trajectory refinement method that lets WHAM capture human motion in diverse
conditions, such as climbing stairs. WHAM outperforms all existing 3D human
motion recovery methods across multiple in-the-wild benchmarks. Code will be
available for research purposes at http://wham.is.tue.mpg.de/",Soyong Shin and Juyong Kim and Eni Halilaj and Michael J. Black,2024,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,yuan2022glamr,\cite{yuan2022glamr},GLAMR: Global Occlusion-Aware Human Mesh Recovery with Dynamic Cameras,http://arxiv.org/abs/2112.01524v2,"We present an approach for 3D global human mesh recovery from monocular
videos recorded with dynamic cameras. Our approach is robust to severe and
long-term occlusions and tracks human bodies even when they go outside the
camera's field of view. To achieve this, we first propose a deep generative
motion infiller, which autoregressively infills the body motions of occluded
humans based on visible motions. Additionally, in contrast to prior work, our
approach reconstructs human meshes in consistent global coordinates even with
dynamic cameras. Since the joint reconstruction of human motions and camera
poses is underconstrained, we propose a global trajectory predictor that
generates global human trajectories based on local body movements. Using the
predicted trajectories as anchors, we present a global optimization framework
that refines the predicted trajectories and optimizes the camera poses to match
the video evidence such as 2D keypoints. Experiments on challenging indoor and
in-the-wild datasets with dynamic cameras demonstrate that the proposed
approach outperforms prior methods significantly in terms of motion infilling
and global mesh recovery.",Ye Yuan and Umar Iqbal and Pavlo Molchanov and Kris Kitani and Jan Kautz,2022,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,TRAM,\cite{TRAM},TRAM: Global Trajectory and Motion of 3D Humans from in-the-wild Videos,http://arxiv.org/abs/2403.17346v2,"We propose TRAM, a two-stage method to reconstruct a human's global
trajectory and motion from in-the-wild videos. TRAM robustifies SLAM to recover
the camera motion in the presence of dynamic humans and uses the scene
background to derive the motion scale. Using the recovered camera as a
metric-scale reference frame, we introduce a video transformer model (VIMO) to
regress the kinematic body motion of a human. By composing the two motions, we
achieve accurate recovery of 3D humans in the world space, reducing global
motion errors by a large margin from prior work.
https://yufu-wang.github.io/tram4d/",Yufu Wang and Ziyun Wang and Lingjie Liu and Kostas Daniilidis,2024,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,rempe2021humor,\cite{rempe2021humor},HuMoR: 3D Human Motion Model for Robust Pose Estimation,http://arxiv.org/abs/2105.04668v2,"We introduce HuMoR: a 3D Human Motion Model for Robust Estimation of temporal
pose and shape. Though substantial progress has been made in estimating 3D
human motion and shape from dynamic observations, recovering plausible pose
sequences in the presence of noise and occlusions remains a challenge. For this
purpose, we propose an expressive generative model in the form of a conditional
variational autoencoder, which learns a distribution of the change in pose at
each step of a motion sequence. Furthermore, we introduce a flexible
optimization-based approach that leverages HuMoR as a motion prior to robustly
estimate plausible pose and shape from ambiguous observations. Through
extensive evaluations, we demonstrate that our model generalizes to diverse
motions and body shapes after training on a large motion capture dataset, and
enables motion reconstruction from multiple input modalities including 3D
keypoints and RGB(-D) videos.",Davis Rempe and Tolga Birdal and Aaron Hertzmann and Jimei Yang and Srinath Sridhar and Leonidas J. Guibas,2021,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,zhang2024physpt,\cite{zhang2024physpt},"PhysPT: Physics-aware Pretrained Transformer for Estimating Human
  Dynamics from Monocular Videos",http://arxiv.org/abs/2404.04430v1,"While current methods have shown promising progress on estimating 3D human
motion from monocular videos, their motion estimates are often physically
unrealistic because they mainly consider kinematics. In this paper, we
introduce Physics-aware Pretrained Transformer (PhysPT), which improves
kinematics-based motion estimates and infers motion forces. PhysPT exploits a
Transformer encoder-decoder backbone to effectively learn human dynamics in a
self-supervised manner. Moreover, it incorporates physics principles governing
human motion. Specifically, we build a physics-based body representation and
contact force model. We leverage them to impose novel physics-inspired training
losses (i.e., force loss, contact loss, and Euler-Lagrange loss), enabling
PhysPT to capture physical properties of the human body and the forces it
experiences. Experiments demonstrate that, once trained, PhysPT can be directly
applied to kinematics-based estimates to significantly enhance their physical
plausibility and generate favourable motion forces. Furthermore, we show that
these physically meaningful quantities translate into improved accuracy of an
important downstream task: human action recognition.",Yufei Zhang and Jeffrey O. Kephart and Zijun Cui and Qiang Ji,2024,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,tripathi2023intuitivephysics,\cite{tripathi2023intuitivephysics},3D Human Pose Estimation via Intuitive Physics,http://arxiv.org/abs/2303.18246v3,"Estimating 3D humans from images often produces implausible bodies that lean,
float, or penetrate the floor. Such methods ignore the fact that bodies are
typically supported by the scene. A physics engine can be used to enforce
physical plausibility, but these are not differentiable, rely on unrealistic
proxy bodies, and are difficult to integrate into existing optimization and
learning frameworks. In contrast, we exploit novel intuitive-physics (IP) terms
that can be inferred from a 3D SMPL body interacting with the scene. Inspired
by biomechanics, we infer the pressure heatmap on the body, the Center of
Pressure (CoP) from the heatmap, and the SMPL body's Center of Mass (CoM). With
these, we develop IPMAN, to estimate a 3D body from a color image in a ""stable""
configuration by encouraging plausible floor contact and overlapping CoP and
CoM. Our IP terms are intuitive, easy to implement, fast to compute,
differentiable, and can be integrated into existing optimization and regression
methods. We evaluate IPMAN on standard datasets and MoYo, a new dataset with
synchronized multi-view images, ground-truth 3D bodies with complex poses,
body-floor contact, CoM and pressure. IPMAN produces more plausible results
than the state of the art, improving accuracy for static poses, while not
hurting dynamic ones. Code and data are available for research at
https://ipman.is.tue.mpg.de.","Shashank Tripathi and Lea M{\""u}ller and Chun-Hao P. Huang and Omid Taheri and Michael J. Black and Dimitrios Tzionas",2023,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,li2022dnd,\cite{li2022dnd},D&D: Learning Human Dynamics from Dynamic Camera,http://arxiv.org/abs/2209.08790v1,"3D human pose estimation from a monocular video has recently seen significant
improvements. However, most state-of-the-art methods are kinematics-based,
which are prone to physically implausible motions with pronounced artifacts.
Current dynamics-based methods can predict physically plausible motion but are
restricted to simple scenarios with static camera view. In this work, we
present D&D (Learning Human Dynamics from Dynamic Camera), which leverages the
laws of physics to reconstruct 3D human motion from the in-the-wild videos with
a moving camera. D&D introduces inertial force control (IFC) to explain the 3D
human motion in the non-inertial local frame by considering the inertial forces
of the dynamic camera. To learn the ground contact with limited annotations, we
develop probabilistic contact torque (PCT), which is computed by differentiable
sampling from contact probabilities and used to generate motions. The contact
state can be weakly supervised by encouraging the model to generate correct
motions. Furthermore, we propose an attentive PD controller that adjusts target
pose states using temporal information to obtain smooth and accurate pose
control. Our approach is entirely neural-based and runs without offline
optimization or simulation in physics engines. Experiments on large-scale 3D
human motion benchmarks demonstrate the effectiveness of D&D, where we exhibit
superior performance against both state-of-the-art kinematics-based and
dynamics-based methods. Code is available at https://github.com/Jeffsjtu/DnD",Jiefeng Li and Siyuan Bian and Chao Xu and Gang Liu and Gang Yu and Cewu Lu,2022,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,makoviychuk2021isaac,\cite{makoviychuk2021isaac},"Isaac Gym: High Performance GPU-Based Physics Simulation For Robot
  Learning",http://arxiv.org/abs/2108.10470v2,"Isaac Gym offers a high performance learning platform to train policies for
wide variety of robotics tasks directly on GPU. Both physics simulation and the
neural network policy training reside on GPU and communicate by directly
passing data from physics buffers to PyTorch tensors without ever going through
any CPU bottlenecks. This leads to blazing fast training times for complex
robotics tasks on a single GPU with 2-3 orders of magnitude improvements
compared to conventional RL training that uses a CPU based simulator and GPU
for neural networks. We host the results and videos at
\url{https://sites.google.com/view/isaacgym-nvidia} and isaac gym can be
downloaded at \url{https://developer.nvidia.com/isaac-gym}.",Viktor Makoviychuk and Lukasz Wawrzyniak and Yunrong Guo and Michelle Lu and Kier Storey and Miles Macklin and David Hoeller and Nikita Rudin and Arthur Allshire and Ankur Handa and Gavriel State,2021,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,todorov2012mujoco,\cite{todorov2012mujoco},MuJoCo: A Physics Engine for Model-Based Control,,,Emanuel Todorov and Tom Erez and Yuval Tassa,2012,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,2018-TOG-deepMimic,\cite{2018-TOG-deepMimic},"DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based
  Character Skills",http://arxiv.org/abs/1804.02717v3,"A longstanding goal in character animation is to combine data-driven
specification of behavior with a system that can execute a similar behavior in
a physical simulation, thus enabling realistic responses to perturbations and
environmental variation. We show that well-known reinforcement learning (RL)
methods can be adapted to learn robust control policies capable of imitating a
broad range of example motion clips, while also learning complex recoveries,
adapting to changes in morphology, and accomplishing user-specified goals. Our
method handles keyframed motions, highly-dynamic actions such as
motion-captured flips and spins, and retargeted motions. By combining a
motion-imitation objective with a task objective, we can train characters that
react intelligently in interactive settings, e.g., by walking in a desired
direction or throwing a ball at a user-specified target. This approach thus
combines the convenience and motion quality of using motion clips to define the
desired style and appearance, with the flexibility and generality afforded by
RL methods and physics-based animation. We further explore a number of methods
for integrating multiple clips into the learning process to develop
multi-skilled agents capable of performing a rich repertoire of diverse skills.
We demonstrate results using multiple characters (human, Atlas robot, bipedal
dinosaur, dragon) and a large variety of skills, including locomotion,
acrobatics, and martial arts.","Peng, Xue Bin and Abbeel, Pieter and Levine, Sergey and van de Panne, Michiel",2018,,,,ACM Transactions on Graphics (TOG)
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,wang2024skillmimic,\cite{wang2024skillmimic},SkillMimic: Learning Reusable Basketball Skills from Demonstrations,,,Yinhuai Wang and Qihan Zhao and Runyi Yu and Ailing Zeng and Jing Lin and Zhengyi Luo and Hok Wai Tsui and Jiwen Yu and Xiu Li and Qifeng Chen and Jian Zhang and Lei Zhang and Ping Tan,2024,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,AMP,\cite{AMP},"AMP: Adversarial Motion Priors for Stylized Physics-Based Character
  Control",http://arxiv.org/abs/2104.02180v2,"Synthesizing graceful and life-like behaviors for physically simulated
characters has been a fundamental challenge in computer animation. Data-driven
methods that leverage motion tracking are a prominent class of techniques for
producing high fidelity motions for a wide range of behaviors. However, the
effectiveness of these tracking-based methods often hinges on carefully
designed objective functions, and when applied to large and diverse motion
datasets, these methods require significant additional machinery to select the
appropriate motion for the character to track in a given scenario. In this
work, we propose to obviate the need to manually design imitation objectives
and mechanisms for motion selection by utilizing a fully automated approach
based on adversarial imitation learning. High-level task objectives that the
character should perform can be specified by relatively simple reward
functions, while the low-level style of the character's behaviors can be
specified by a dataset of unstructured motion clips, without any explicit clip
selection or sequencing. These motion clips are used to train an adversarial
motion prior, which specifies style-rewards for training the character through
reinforcement learning (RL). The adversarial RL procedure automatically selects
which motion to perform, dynamically interpolating and generalizing from the
dataset. Our system produces high-quality motions that are comparable to those
achieved by state-of-the-art tracking-based techniques, while also being able
to easily accommodate large datasets of unstructured motion clips. Composition
of disparate skills emerges automatically from the motion prior, without
requiring a high-level motion planner or other task-specific annotations of the
motion clips. We demonstrate the effectiveness of our framework on a diverse
cast of complex simulated characters and a challenging suite of motor control
tasks.","Peng, Xue Bin and Ma, Ze and Abbeel, Pieter and Levine, Sergey and Kanazawa, Angjoo",2021,,,,ACM Transactions on Graphics (TOG)
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,2022-TOG-ASE,\cite{2022-TOG-ASE},"ASE: Large-Scale Reusable Adversarial Skill Embeddings for Physically
  Simulated Characters",http://arxiv.org/abs/2205.01906v2,"The incredible feats of athleticism demonstrated by humans are made possible
in part by a vast repertoire of general-purpose motor skills, acquired through
years of practice and experience. These skills not only enable humans to
perform complex tasks, but also provide powerful priors for guiding their
behaviors when learning new tasks. This is in stark contrast to what is common
practice in physics-based character animation, where control policies are most
typically trained from scratch for each task. In this work, we present a
large-scale data-driven framework for learning versatile and reusable skill
embeddings for physically simulated characters. Our approach combines
techniques from adversarial imitation learning and unsupervised reinforcement
learning to develop skill embeddings that produce life-like behaviors, while
also providing an easy to control representation for use on new downstream
tasks. Our models can be trained using large datasets of unstructured motion
clips, without requiring any task-specific annotation or segmentation of the
motion data. By leveraging a massively parallel GPU-based simulator, we are
able to train skill embeddings using over a decade of simulated experiences,
enabling our model to learn a rich and versatile repertoire of skills. We show
that a single pre-trained model can be effectively applied to perform a diverse
set of new tasks. Our system also allows users to specify tasks through simple
reward functions, and the skill embedding then enables the character to
automatically synthesize complex and naturalistic strategies in order to
achieve the task objectives.","Peng, Xue Bin and Guo, Yunrong and Halper, Lina and Levine, Sergey and Fidler, Sanja",2022,,,,ACM Transactions on Graphics (TOG)
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,dou2022case,\cite{dou2022case},"C$\cdot$ASE: Learning Conditional Adversarial Skill Embeddings for
  Physics-based Characters",http://arxiv.org/abs/2309.11351v1,"We present C$\cdot$ASE, an efficient and effective framework that learns
conditional Adversarial Skill Embeddings for physics-based characters. Our
physically simulated character can learn a diverse repertoire of skills while
providing controllability in the form of direct manipulation of the skills to
be performed. C$\cdot$ASE divides the heterogeneous skill motions into distinct
subsets containing homogeneous samples for training a low-level conditional
model to learn conditional behavior distribution. The skill-conditioned
imitation learning naturally offers explicit control over the character's
skills after training. The training course incorporates the focal skill
sampling, skeletal residual forces, and element-wise feature masking to balance
diverse skills of varying complexities, mitigate dynamics mismatch to master
agile motions and capture more general behavior characteristics, respectively.
Once trained, the conditional model can produce highly diverse and realistic
skills, outperforming state-of-the-art models, and can be repurposed in various
downstream tasks. In particular, the explicit skill control handle allows a
high-level policy or user to direct the character with desired skill
specifications, which we demonstrate is advantageous for interactive character
animation.",Zhiyang Dou and Xuelin Chen and Qingnan Fan and Taku Komura and Wenping Wang,2023,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,tessler2023calm,\cite{tessler2023calm},"CALM: Conditional Adversarial Latent Models for Directable Virtual
  Characters",http://arxiv.org/abs/2305.02195v1,"In this work, we present Conditional Adversarial Latent Models (CALM), an
approach for generating diverse and directable behaviors for user-controlled
interactive virtual characters. Using imitation learning, CALM learns a
representation of movement that captures the complexity and diversity of human
motion, and enables direct control over character movements. The approach
jointly learns a control policy and a motion encoder that reconstructs key
characteristics of a given motion without merely replicating it. The results
show that CALM learns a semantic motion representation, enabling control over
the generated motions and style-conditioning for higher-level task training.
Once trained, the character can be controlled using intuitive interfaces, akin
to those found in video games.",Chen Tessler and Yoni Kasten and Yunrong Guo and Shie Mannor and Gal Chechik and Xue Bin Peng,2023,,,,ACM Transactions on Graphics (TOG)
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,yang2023ppr,\cite{yang2023ppr},PPR: Physically Plausible Reconstruction from Monocular Videos,,,Gengshan Yang and Shuo Yang and John Z. Zhang and Zachary Manchester and Deva Ramanan,2023,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,gartner2022diffphy,\cite{gartner2022diffphy},Differentiable Dynamics for Articulated 3d Human Motion Reconstruction,http://arxiv.org/abs/2205.12256v1,"We introduce DiffPhy, a differentiable physics-based model for articulated 3d
human motion reconstruction from video. Applications of physics-based reasoning
in human motion analysis have so far been limited, both by the complexity of
constructing adequate physical models of articulated human motion, and by the
formidable challenges of performing stable and efficient inference with physics
in the loop. We jointly address such modeling and inference challenges by
proposing an approach that combines a physically plausible body representation
with anatomical joint limits, a differentiable physics simulator, and
optimization techniques that ensure good performance and robustness to
suboptimal local optima. In contrast to several recent methods, our approach
readily supports full-body contact including interactions with objects in the
scene. Most importantly, our model connects end-to-end with images, thus
supporting direct gradient-based physics optimization by means of image-based
loss functions. We validate the model by demonstrating that it can accurately
reconstruct physically plausible 3d human motion from monocular video, both on
public benchmarks with available 3d ground-truth, and on videos from the
internet.","Erik G{\""a}rtner and Mykhaylo Andriluka and Erwin Coumans and Cristian Sminchisescu",2022,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,AMASS:ICCV:2019,\cite{AMASS:ICCV:2019},AMASS: Archive of Motion Capture as Surface Shapes,http://arxiv.org/abs/1904.03278v1,"Large datasets are the cornerstone of recent advances in computer vision
using deep learning. In contrast, existing human motion capture (mocap)
datasets are small and the motions limited, hampering progress on learning
models of human motion. While there are many different datasets available, they
each use a different parameterization of the body, making it difficult to
integrate them into a single meta dataset. To address this, we introduce AMASS,
a large and varied database of human motion that unifies 15 different optical
marker-based mocap datasets by representing them within a common framework and
parameterization. We achieve this using a new method, MoSh++, that converts
mocap data into realistic 3D human meshes represented by a rigged body model;
here we use SMPL [doi:10.1145/2816795.2818013], which is widely used and
provides a standard skeletal representation as well as a fully rigged surface
mesh. The method works for arbitrary marker sets, while recovering soft-tissue
dynamics and realistic hand motion. We evaluate MoSh++ and tune its
hyperparameters using a new dataset of 4D body scans that are jointly recorded
with marker-based mocap. The consistent representation of AMASS makes it
readily useful for animation, visualization, and generating training data for
deep learning. Our dataset is significantly richer than previous human motion
collections, having more than 40 hours of motion data, spanning over 300
subjects, more than 11,000 motions, and will be publicly available to the
research community.",Naureen Mahmood and Nima Ghorbani and Nikolaus F. Troje and Gerard Pons-Moll and Michael J. Black,2019,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,peng2021neural,\cite{peng2021neural},Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans,,,Sida Peng and Yuanqing Zhang and Yinghao Xu and Qianqian Wang and Qing Shuai and Hujun Bao and Xiaowei Zhou,2021,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,kobayashi2023motion,\cite{kobayashi2023motion},"Motion Capture Dataset for Practical Use of AI-based Motion Editing and
  Stylization",http://arxiv.org/abs/2306.08861v2,"In this work, we proposed a new style-diverse dataset for the domain of
motion style transfer. The motion dataset uses an industrial-standard human
bone structure and thus is industry-ready to be plugged into 3D characters for
many projects. We claim the challenges in motion style transfer and encourage
future work in this domain by releasing the proposed motion dataset both to the
public and the market. We conduct a comprehensive study on motion style
transfer in the experiment using the state-of-the-art method, and the results
show the proposed dataset's validity for the motion style transfer task.",Makito Kobayashi and Chen-Chieh Liao and Keito Inoue and Sentaro Yojima and Masafumi Takahashi,2023,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,wagener2022mocapact,\cite{wagener2022mocapact},MoCapAct: A Multi-Task Dataset for Simulated Humanoid Control,http://arxiv.org/abs/2208.07363v3,"Simulated humanoids are an appealing research domain due to their physical
capabilities. Nonetheless, they are also challenging to control, as a policy
must drive an unstable, discontinuous, and high-dimensional physical system.
One widely studied approach is to utilize motion capture (MoCap) data to teach
the humanoid agent low-level skills (e.g., standing, walking, and running) that
can then be re-used to synthesize high-level behaviors. However, even with
MoCap data, controlling simulated humanoids remains very hard, as MoCap data
offers only kinematic information. Finding physical control inputs to realize
the demonstrated motions requires computationally intensive methods like
reinforcement learning. Thus, despite the publicly available MoCap data, its
utility has been limited to institutions with large-scale compute. In this
work, we dramatically lower the barrier for productive research on this topic
by training and releasing high-quality agents that can track over three hours
of MoCap data for a simulated humanoid in the dm_control physics-based
environment. We release MoCapAct (Motion Capture with Actions), a dataset of
these expert agents and their rollouts, which contain proprioceptive
observations and actions. We demonstrate the utility of MoCapAct by using it to
train a single hierarchical policy capable of tracking the entire MoCap dataset
within dm_control and show the learned low-level component can be re-used to
efficiently learn downstream high-level tasks. Finally, we use MoCapAct to
train an autoregressive GPT model and show that it can control a simulated
humanoid to perform natural motion completion given a motion prompt.
  Videos of the results and links to the code and dataset are available at
https://microsoft.github.io/MoCapAct.",Nolan Wagener and Andrey Kolobov and Felipe Vieira Frujeri and Ricky Loynd and Ching-An Cheng and Matthew Hausknecht,2022,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,Luo2022EmbodiedSH,\cite{Luo2022EmbodiedSH},Embodied Scene-aware Human Pose Estimation,http://arxiv.org/abs/2206.09106v3,"We propose embodied scene-aware human pose estimation where we estimate 3D
poses based on a simulated agent's proprioception and scene awareness, along
with external third-person observations. Unlike prior methods that often resort
to multistage optimization, non-causal inference, and complex contact modeling
to estimate human pose and human scene interactions, our method is one-stage,
causal, and recovers global 3D human poses in a simulated environment. Since 2D
third-person observations are coupled with the camera pose, we propose to
disentangle the camera pose and use a multi-step projection gradient defined in
the global coordinate frame as the movement cue for our embodied agent.
Leveraging a physics simulation and prescanned scenes (e.g., 3D mesh), we
simulate our agent in everyday environments (library, office, bedroom, etc.)
and equip our agent with environmental sensors to intelligently navigate and
interact with the geometries of the scene. Our method also relies only on 2D
keypoints and can be trained on synthetic datasets derived from popular human
motion databases. To evaluate, we use the popular H36M and PROX datasets and
achieve high quality pose estimation on the challenging PROX dataset without
ever using PROX motion sequences for training. Code and videos are available on
the project page.",Zhengyi Luo and Shun Iwase and Ye Yuan and Kris Kitani,2022,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,luo2024universal,\cite{luo2024universal},Universal Humanoid Motion Representations for Physics-Based Control,http://arxiv.org/abs/2310.04582v2,"We present a universal motion representation that encompasses a comprehensive
range of motor skills for physics-based humanoid control. Due to the high
dimensionality of humanoids and the inherent difficulties in reinforcement
learning, prior methods have focused on learning skill embeddings for a narrow
range of movement styles (e.g. locomotion, game characters) from specialized
motion datasets. This limited scope hampers their applicability in complex
tasks. We close this gap by significantly increasing the coverage of our motion
representation space. To achieve this, we first learn a motion imitator that
can imitate all of human motion from a large, unstructured motion dataset. We
then create our motion representation by distilling skills directly from the
imitator. This is achieved by using an encoder-decoder structure with a
variational information bottleneck. Additionally, we jointly learn a prior
conditioned on proprioception (humanoid's own pose and velocities) to improve
model expressiveness and sampling efficiency for downstream tasks. By sampling
from the prior, we can generate long, stable, and diverse human motions. Using
this latent space for hierarchical RL, we show that our policies solve tasks
using human-like behavior. We demonstrate the effectiveness of our motion
representation by solving generative tasks (e.g. strike, terrain traversal) and
motion tracking using VR controllers.",Zhengyi Luo and Jinkun Cao and Josh Merel and Alexander Winkler and Jing Huang and Kris M. Kitani and Weipeng Xu,2024,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,Luo2023PerpetualHC,\cite{Luo2023PerpetualHC},Perpetual Humanoid Control for Real-time Simulated Avatars,http://arxiv.org/abs/2305.06456v3,"We present a physics-based humanoid controller that achieves high-fidelity
motion imitation and fault-tolerant behavior in the presence of noisy input
(e.g. pose estimates from video or generated from language) and unexpected
falls. Our controller scales up to learning ten thousand motion clips without
using any external stabilizing forces and learns to naturally recover from
fail-state. Given reference motion, our controller can perpetually control
simulated avatars without requiring resets. At its core, we propose the
progressive multiplicative control policy (PMCP), which dynamically allocates
new network capacity to learn harder and harder motion sequences. PMCP allows
efficient scaling for learning from large-scale motion databases and adding new
tasks, such as fail-state recovery, without catastrophic forgetting. We
demonstrate the effectiveness of our controller by using it to imitate noisy
poses from video-based pose estimators and language-based motion generators in
a live and real-time multi-person avatar use case.",Zhengyi Luo and Jinkun Cao and Alexander W. Winkler and Kris Kitani and Weipeng Xu,2023,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,tessler2024maskedmimic,\cite{tessler2024maskedmimic},"MaskedMimic: Unified Physics-Based Character Control Through Masked
  Motion Inpainting",http://arxiv.org/abs/2409.14393v1,"Crafting a single, versatile physics-based controller that can breathe life
into interactive characters across a wide spectrum of scenarios represents an
exciting frontier in character animation. An ideal controller should support
diverse control modalities, such as sparse target keyframes, text instructions,
and scene information. While previous works have proposed physically simulated,
scene-aware control models, these systems have predominantly focused on
developing controllers that each specializes in a narrow set of tasks and
control modalities. This work presents MaskedMimic, a novel approach that
formulates physics-based character control as a general motion inpainting
problem. Our key insight is to train a single unified model to synthesize
motions from partial (masked) motion descriptions, such as masked keyframes,
objects, text descriptions, or any combination thereof. This is achieved by
leveraging motion tracking data and designing a scalable training method that
can effectively utilize diverse motion descriptions to produce coherent
animations. Through this process, our approach learns a physics-based
controller that provides an intuitive control interface without requiring
tedious reward engineering for all behaviors of interest. The resulting
controller supports a wide range of control modalities and enables seamless
transitions between disparate tasks. By unifying character control through
motion inpainting, MaskedMimic creates versatile virtual characters. These
characters can dynamically adapt to complex scenes and compose diverse motions
on demand, enabling more interactive and immersive experiences.",Chen Tessler and Yunrong Guo and Ofir Nabati and Gal Chechik and Xue Bin Peng,2024,,,,ACM Transactions on Graphics (TOG)
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,peng2018sfv,\cite{peng2018sfv},SFV: Reinforcement Learning of Physical Skills from Videos,http://arxiv.org/abs/1810.03599v2,"Data-driven character animation based on motion capture can produce highly
naturalistic behaviors and, when combined with physics simulation, can provide
for natural procedural responses to physical perturbations, environmental
changes, and morphological discrepancies. Motion capture remains the most
popular source of motion data, but collecting mocap data typically requires
heavily instrumented environments and actors. In this paper, we propose a
method that enables physically simulated characters to learn skills from videos
(SFV). Our approach, based on deep pose estimation and deep reinforcement
learning, allows data-driven animation to leverage the abundance of publicly
available video clips from the web, such as those from YouTube. This has the
potential to enable fast and easy design of character controllers simply by
querying for video recordings of the desired behavior. The resulting
controllers are robust to perturbations, can be adapted to new settings, can
perform basic object interactions, and can be retargeted to new morphologies
via reinforcement learning. We further demonstrate that our method can predict
potential human motions from still images, by forward simulation of learned
controllers initialized from the observed pose. Our framework is able to learn
a broad range of dynamic skills, including locomotion, acrobatics, and martial
arts.",Xue Bin Peng and Angjoo Kanazawa and Jitendra Malik and Pieter Abbeel and Sergey Levine,2018,,,,ACM Transactions on Graphics (TOG)
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,winkler2022questsim,\cite{winkler2022questsim},"QuestSim: Human Motion Tracking from Sparse Sensors with Simulated
  Avatars",http://arxiv.org/abs/2209.09391v1,"Real-time tracking of human body motion is crucial for interactive and
immersive experiences in AR/VR. However, very limited sensor data about the
body is available from standalone wearable devices such as HMDs (Head Mounted
Devices) or AR glasses. In this work, we present a reinforcement learning
framework that takes in sparse signals from an HMD and two controllers, and
simulates plausible and physically valid full body motions. Using high quality
full body motion as dense supervision during training, a simple policy network
can learn to output appropriate torques for the character to balance, walk, and
jog, while closely following the input signals. Our results demonstrate
surprisingly similar leg motions to ground truth without any observations of
the lower body, even when the input is only the 6D transformations of the HMD.
We also show that a single policy can be robust to diverse locomotion styles,
different body sizes, and novel environments.",Alexander Winkler and Jungdam Won and Yuting Ye,2022,,,,
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,shimada2020physcap,\cite{shimada2020physcap},PhysCap: Physically Plausible Monocular 3D Motion Capture in Real Time,http://arxiv.org/abs/2008.08880v2,"Marker-less 3D human motion capture from a single colour camera has seen
significant progress. However, it is a very challenging and severely ill-posed
problem. In consequence, even the most accurate state-of-the-art approaches
have significant limitations. Purely kinematic formulations on the basis of
individual joints or skeletons, and the frequent frame-wise reconstruction in
state-of-the-art methods greatly limit 3D accuracy and temporal stability
compared to multi-view or marker-based motion capture. Further, captured 3D
poses are often physically incorrect and biomechanically implausible, or
exhibit implausible environment interactions (floor penetration, foot skating,
unnatural body leaning and strong shifting in depth), which is problematic for
any use case in computer graphics. We, therefore, present PhysCap, the first
algorithm for physically plausible, real-time and marker-less human 3D motion
capture with a single colour camera at 25 fps. Our algorithm first captures 3D
human poses purely kinematically. To this end, a CNN infers 2D and 3D joint
positions, and subsequently, an inverse kinematics step finds space-time
coherent joint angles and global 3D pose. Next, these kinematic reconstructions
are used as constraints in a real-time physics-based pose optimiser that
accounts for environment constraints (e.g., collision handling and floor
placement), gravity, and biophysical plausibility of human postures. Our
approach employs a combination of ground reaction force and residual force for
plausible root control, and uses a trained neural network to detect foot
contact events in images. Our method captures physically plausible and
temporally stable global 3D human motion, without physically implausible
postures, floor penetrations or foot skating, from video in real time and in
general scenes. The video is available at
http://gvv.mpi-inf.mpg.de/projects/PhysCap",Soshi Shimada and Vladislav Golyanik and Weipeng Xu and Christian Theobalt,2020,,,,ACM Transactions on Graphics (TOG)
PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,http://arxiv.org/abs/2510.02566v1,Luo_2024_CVPR,\cite{Luo_2024_CVPR},Real-Time Simulated Avatar from Head-Mounted Sensors,http://arxiv.org/abs/2403.06862v2,"We present SimXR, a method for controlling a simulated avatar from
information (headset pose and cameras) obtained from AR / VR headsets. Due to
the challenging viewpoint of head-mounted cameras, the human body is often
clipped out of view, making traditional image-based egocentric pose estimation
challenging. On the other hand, headset poses provide valuable information
about overall body motion, but lack fine-grained details about the hands and
feet. To synergize headset poses with cameras, we control a humanoid to track
headset movement while analyzing input images to decide body movement. When
body parts are seen, the movements of hands and feet will be guided by the
images; when unseen, the laws of physics guide the controller to generate
plausible motion. We design an end-to-end method that does not rely on any
intermediate representations and learns to directly map from images and headset
poses to humanoid control signals. To train our method, we also propose a
large-scale synthetic dataset created using camera configurations compatible
with a commercially available VR headset (Quest 2) and show promising results
on real-world captures. To demonstrate the applicability of our framework, we
also test it on an AR headset with a forward-facing camera.",Zhengyi Luo and Jinkun Cao and Rawal Khirodkar and Alexander Winkler and Kris Kitani and Weipeng Xu,2024,,,,
