arxiv_link,arxiv_id,publication_date,paper_title,abstract,related_works_section,related_works_length
http://arxiv.org/abs/2510.05707v1,2510.05707v1,2025-10-07T09:16:48+00:00,Stable Robot Motions on Manifolds: Learning Lyapunov-Constrained Neural Manifold ODEs,"Learning stable dynamical systems from data is crucial for safe and reliable
robot motion planning and control. However, extending stability guarantees to
trajectories defined on Riemannian manifolds poses significant challenges due
to the manifold's geometric constraints. To address this, we propose a general
framework for learning stable dynamical systems on Riemannian manifolds using
neural ordinary differential equations. Our method guarantees stability by
projecting the neural vector field evolving on the manifold so that it strictly
satisfies the Lyapunov stability criterion, ensuring stability at every system
state. By leveraging a flexible neural parameterisation for both the base
vector field and the Lyapunov function, our framework can accurately represent
complex trajectories while respecting manifold constraints by evolving
solutions directly on the manifold. We provide an efficient training strategy
for applying our framework and demonstrate its utility by solving Riemannian
LASA datasets on the unit quaternion (S^3) and symmetric positive-definite
matrix manifolds, as well as robotic motions evolving on \mathbb{R}^3 \times
S^3. We demonstrate the performance, scalability, and practical applicability
of our approach through extensive simulations and by learning robot motions in
a real-world experiment.","\newcommand{\ck}{\textcolor{bettergreen}{\(\pmb{\checkmark}\)}}
\newcommand{\no}{\textcolor{betterred}{\(\pmb{\times}\)}}
\newcommand{\na}{--}
\begin{table}[b]
    \centering
    \caption{\textbf{Comparison with Existing LfD Frameworks}.}
        \begin{tabular}{>{\centering}p{.13\columnwidth} @{} >{\centering}p{.18\columnwidth} @{} >{\centering}p{.18\columnwidth} @{} >{\centering}p{.18\columnwidth} @{} >{\centering\arraybackslash}p{.12\columnwidth}}
        \ \newline\textbf{Reference} & \textbf{Riemannian}\newline\textbf{Manifold} & \ \textbf{Stability}\newline\textbf{Guarantee} & \ \textbf{Learned}\newline\textbf{Vector Field} & \textbf{Neural}\newline\textbf{ODE} \\ \midrule
        
        \cite{AuddyEtAl2023Continual}
        & \no & \no & \ck &\ck \\
        
        \cite{SochopoulosGiengerVijayakumar2024,AuddyEtAl2023Scalable}
        & \no & \(\textcolor{betterorange}{\pmb{\checkmark}}^{\;\!\!\ast}\) & \ck &\ck \\
        
        \cite{LemmeEtAl2014,NawazEtAl2024}
        & \no & \ck & \ck & \ck \\

        \cite{KhansariZadehBillard11}
        & \no & \ck & \ck & \no \\
        
        \cite{WangSaverianoAbuDakka2022}
        & \ck & \no & \ck & \ck \\
        
        \cite{SaverianoAbuDakkaKyrki2023}
        & \ck & \ck & \no &\no \\
        
        \cite{ZhangBeikMohammadiRozo2022}
        & \ck & \(\textcolor{betterorange}{\pmb{\checkmark}}^{\;\!\!\dagger}\)  & \no & \ck \\
        
        \textbf{Ours}
        & \ck & \ck &\ck & \ck \\
    \end{tabular}
    
    {\footnotesize \({}^\ast\)Affected by the equilibrium point issue that we address in this paper.}\\
    {\footnotesize \({}^\dagger\)Unlike the other approaches, the equilibrium point is not user-defined.}
\end{table}
In this section, we review existing works related to stable neural ODEs (NODEs) on Riemannian manifolds. \Cref{tab:related-work-comparison} provides a concise comparison of our \ourframework{} framework to existing learning from demonstrations (LfD) frameworks.
We first discuss approaches in Euclidean space.

\paragraph{NODEs and Stability}
NODEs~\cite{NODEs} are a powerful paradigm for modelling complex continuous-time dynamical systems, from image processing~\cite{NODEs,KangEtAl2021,LuoEtAl2025} to learning robotic skills~\cite{LemmeEtAl2014,WhiteEtAl2023,NawazEtAl2024,SochopoulosGiengerVijayakumar2024,AuddyEtAl2023Continual,AuddyEtAl2023Scalable}.

Stability in NODEs is addressed using Lyapunov functions~\cite{ManekKolter2019,AuddyEtAl2023Scalable,KangEtAl2021,SochopoulosGiengerVijayakumar2024,LuoEtAl2025} and contraction metrics~\cite{KhansariZadehBillard11, NawazEtAl2024}. 

However, these stability frameworks are fundamentally limited to Euclidean spaces and cannot handle data evolving on more general Riemannian manifolds. Additionally, we identify a critical equilibrium consistency issue in~\cite{ManekKolter2019} that invalidates theoretical stability guarantees and causes numerical instability in practice. This issue also affects subsequent works~\cite{AuddyEtAl2023Scalable,SochopoulosGiengerVijayakumar2024} that build upon~\cite{ManekKolter2019}. We address this critical issue in \cref{sec:main-stability-issue}.







\paragraph{NODEs on Riemannian Manifolds}
Neural Manifold ODEs (NMODEs)~\cite{DynamicChartMethod} provide an expressive framework for learning dynamical systems while respecting manifold geometry. 
While~\cite{DynamicChartMethod} employs dynamic coordinate charts to solve NMODEs,~\cite{WangSaverianoAbuDakka2022} projects the manifold data into a single tangent space.




Both~\cite{DynamicChartMethod} and~\cite{WangSaverianoAbuDakka2022} lack stability guarantees.


\paragraph{Stable Dynamical Systems on Riemannian Manifolds} 




Learning stable dynamical systems on Riemannian manifolds combines differential geometry, control theory, and machine learning challenges. Recent approaches~\cite{ZhangBeikMohammadiRozo2022,SaverianoAbuDakkaKyrki2023} focus on learning diffeomorphic mappings to transform stable canonical dynamics into complex behaviours using NMODEs and Gaussian mixture models (GMMs).

RSDS~\cite{ZhangBeikMohammadiRozo2022} uses NMODEs to construct diffeomorphisms for Riemannian stable dynamical systems, providing theoretical stability guarantees through pullback operations. However, this method is costly, as it solves an NMODE at each time step, hindering real-time applications. 
Additionally, a fundamental issue in~\cite{ZhangBeikMohammadiRozo2022} is that the equilibrium of the learned diffeomorphism is assumed to automatically correspond to the desired equilibrium point, without enforcing this constraint during training. While~\cite{ZhangBeikMohammadiRozo2022} ensures convergence to \emph{some} equilibrium point, users have no control over the location of this equilibrium point.

In contrast, SDS-RM~\cite{SaverianoAbuDakkaKyrki2023} employs GMMs to effectively learn diffeomorphisms. Compared to RSDS, SDS-RM ensures computational efficiency while guaranteeing convergence to a user-defined equilibrium point. Nevertheless, GMMs struggle with scalability in high-dimensional manifolds~\cite{abu-dakka2018force} and oftentimes fail to capture the complex nonlinear dynamics that neural networks handle well.

Alternative approaches~\cite{ravichandar2019learning,mukadam2020riemannian} explore contraction theory on manifolds~\cite{ravichandar2019learning} and Riemannian motion policies~\cite{mukadam2020riemannian}, but focus on tracking a reference trajectory rather than learning stable dynamics from demonstrations.

\paragraph{Positioning Our Approach}
Our work addresses key limitations in current methods by merging the expressiveness of NODEs with stability guarantees on Riemannian manifolds.
Unlike diffeomorphism-based methods~\cite{ZhangBeikMohammadiRozo2022,SaverianoAbuDakkaKyrki2023}, we learn stable vector fields on manifolds directly, avoiding expensive inverse mappings. 

Our approach retains the computational benefits of NODEs while offering the geometric awareness crucial for robotic applications involving orientation, impedance, or other manifold-valued quantities.",6110
http://arxiv.org/abs/2510.07319v1,2510.07319v1,2025-10-08T17:59:57+00:00,Temporal Prompting Matters: Rethinking Referring Video Object Segmentation,"Referring Video Object Segmentation (RVOS) aims to segment the object
referred to by the query sentence in the video. Most existing methods require
end-to-end training with dense mask annotations, which could be
computation-consuming and less scalable. In this work, we rethink the RVOS
problem and aim to investigate the key to this task. Based on existing
foundation segmentation models, we decompose the RVOS task into referring,
video, and segmentation factors, and propose a Temporal Prompt Generation and
Selection (Tenet) framework to address the referring and video factors while
leaving the segmentation problem to foundation models. To efficiently adapt
image-based foundation segmentation models to referring video object
segmentation, we leverage off-the-shelf object detectors and trackers to
produce temporal prompts associated with the referring sentence. While
high-quality temporal prompts could be produced, they can not be easily
identified from confidence scores. To tackle this issue, we propose Prompt
Preference Learning to evaluate the quality of the produced temporal prompts.
By taking such prompts to instruct image-based foundation segmentation models,
we would be able to produce high-quality masks for the referred object,
enabling efficient model adaptation to referring video object segmentation.
Experiments on RVOS benchmarks demonstrate the effectiveness of the Tenet
framework.","\vspace{-2mm}
\subsection{Referring Image/Video Segmentation}

\vspace{-2mm}
Referring image segmentation (RIS)~\citep{xu2023bridging, yang2022lavt, yu2023zero, liu2023polyformer} learns to segment the corresponding object in an image given a free-form text query. For example, PolyFormer~\citep{liu2023polyformer} re-formulates the RIS problem as a sequential polygon generation and then converts it to a segmentation mask. Also, PolyFormer further performs zero-shot transfer on the RVOS task to show its generalization ability for the video domain. However, the challenging issues for RVOS such as position and size variation, pose deformation, object occlusion, \etc, may limit the performance of RIS methods. 


Referring Video Object Segmentation (RVOS)~\citep{wu2022language,han2023html,wu2023onlinerefer,miao2023spectrum,tang2023temporal,luo2024soc,wu2023segment} strives to segment the object described by a free-form sentence query across the entire video duration. 
Recently, ReferFormer~\citep{wu2022language} views language as queries to pay attention to the referred object by adopting an encoder-decoder style in the transformer. However, this work only supports offline training and inference, limiting its usage in real-world scenarios. More recently, 

OnlineRefer~\citep{wu2023onlinerefer} further proposes an online RVOS setting to deal with the issues about offline limits, which makes it more possible to adapt to real-world scenarios. 
Nevertheless, most existing methods require end-to-end training for vision-language models, which could be computationally expensive and time-consuming. Moreover, the requirement of dense mask annotations for training impedes the scalability of those approaches.
Instead, we propose to exploit foundation segmentation models without text- and temporal-aware prompting, which is trained without mask annotations and supports online settings. Very recently, several methods~\citep{zhu2023tracking,lai2024lisa,yan2024visa,bai2024onetoken} are proposed to leverage the knowledge learned in large language models to address RVOS. Nevertheless, the results of these LLM-based methods are still inferior to traditional ones.


\vspace{-2mm}

\subsection{Foundation Segmentation Models}

\vspace{-2mm}
In recent years, foundation vision models have gained massive attention given their remarkable generalization capabilities on various downstream tasks. More recently, SAM~\citep{kirillov2023segment} has introduced a foundation model specifically tailored for segmentation tasks. SAM allows specific position prompts (\eg, points, boxes, \etc.) to demonstrate the zero-shot ability on the open vocabulary segmentation tasks with novel image distributions. Several works have studied the versatility of SAM, including remote sensing images~\citep{chen2024rsprompter, SAMRS}, medical image analysis~\citep{MedSAM, chen2023sam, wu2023medical, cheng2023sam}, and adaptation to video-based tracking task~\citep{cheng2023segment, yang2023track, sam-pt}, \etc.


In addition to SAM, SegGPT~\citep{wang2023seggpt} and SEEM~\citep{zou2024segment} have also emerged as generalized foundation segmentation models, showcasing comparable concepts. SegGPT exploits the concept of an in-context learning scheme to treat the classic segmentation problems as an in-context coloring problem. With this design, SegGPT is able to focus on more contextual information when training. On the other hand, SEEM extends the versatility of a single segmentation model by broadening the range of tasks. Similar to SAM, SEEM also supports various prompts including points, boxes, masks, \etc. Specifically, SEEM proposes to align visual-semantic space to accommodate flexible multi-prompt input. However, both SegGPT and  SEEM are not directly feasible for our RVOS task due to no specific adaptation to the video domain or enhancement of tracking ability.



For adaptation to tracking tasks with SAM, SAM-PT~\citep{sam-pt} 

designs a point-based prompt enhancement for the original SAM point prompt to support classic video object segmentation tasks, while neglecting the importance of text prompt for advanced referring video object segmentation. Another example SAM-Track~\citep{cheng2023segment} attempts to utilize SAM for segmentation and detection of objects while the DeAOT~\citep{yang2022decoupling} module captures the motion across frames for tracking the objects. On the other hand, SAM 2~\citep{ravi2024sam} introduces a memory attention mechanism on SAM to produce masklets for videos. Though it is possible to combine text-grounding detection models (\eg, Grounding DINO~\citep{liu2024grounding}) with SAM-Track to tackle RVOS, RefSAM~\citep{li2023refsam} has studied the possible concerns and indicates the unsatisfactory performance compared with current SOTAs in RVOS tasks. 

Different from the above, we propose

temporal-aware prompting with foundation segmentation models (\eg, SAM) to tackle RVOS problems.",4954
http://arxiv.org/abs/2510.09489v1,2510.09489v1,2025-10-10T15:52:23+00:00,Two-Stage Gaussian Splatting Optimization for Outdoor Scene Reconstruction,"Outdoor scene reconstruction remains challenging due to the stark contrast
between well-textured, nearby regions and distant backgrounds dominated by low
detail, uneven illumination, and sky effects. We introduce a two-stage Gaussian
Splatting framework that explicitly separates and optimizes these regions,
yielding higher-fidelity novel view synthesis. In stage one, background
primitives are initialized within a spherical shell and optimized using a loss
that combines a background-only photometric term with two geometric
regularizers: one constraining Gaussians to remain inside the shell, and
another aligning them with local tangential planes. In stage two, foreground
Gaussians are initialized from a Structure-from-Motion reconstruction, added
and refined using the standard rendering loss, while the background set remains
fixed but contributes to the final image formation. Experiments on diverse
outdoor datasets show that our method reduces background artifacts and improves
perceptual quality compared to state-of-the-art baselines. Moreover, the
explicit background separation enables automatic, object-free environment map
estimation, opening new possibilities for photorealistic outdoor rendering and
mixed-reality applications.","Gaussian Splatting (GS) has emerged as a powerful real-time scene
representation technique, achieving high-fidelity rendering through
optimized anisotropic Gaussian primitives (Kerbl et al. [6]). Be-
yond object-centric scenes, recent works have extended GS to handle
large-scale and outdoor environments using different approaches to
the problem.
Kulhanek et al. identified a relevant issue with outdoor scenes re-
construction, as the Gaussians couldn’t be created in the sky with
Structure-from-Motion (SfM) initialization [9]. To overcome this
problem, the authors sampled points on a sphere around the scene and
added them to the rest of the points used to initialize the Gaussians.
Wang et al. adopted another strategy and implemented a pipeline
with different neural modules. They separate the sky from more de-
tailed elements in the scene (e.g., buildings) and generate a cubemap
to render the sky [16]. However, while the resulting sky reconstruc-
tion looks visually realistic, in some instances (i.e., sky with clouds),
it struggles to reproduce images close to the ground truth.
Cheng et al. introduced GaussianPro, using progressive densifi-
cation strategies to bolster robustness in texture-poor and large out-
door regions [4]. Lin et al. proposed VastGaussian, partitioning large
scenes into parallel-optimized volumes, allowing real-time splatting
across large environments [10]. Ren et al. developed Octree-GS,
employing hierarchical levels of detail (LoD) via octree structures
to adaptively manage rendering fidelity in large landscapes [12]. In
some works, authors tried to tackle the side effects of handling large
environments. Zhang et al. presented GaussianSpa, an optimization-
based sparsification framework that reduces point count while pre-
serving visual quality, critical for memory-efficient large-scale GS
[18]. Pateux et al. designed BOGausS, a better-optimized training
regime with confidence-aware updates and rate-distortion densifica-
tion to enhance GS performance under real-world capture variabil-
ity [11]. Franke et al. introduced a trilinear point splatting schemeblending GS and neural rendering, showing effective large-scale land-
scape rendering at real-time rates [5].
Despite these advances, current GS approaches often rely on dense
viewpoint coverage, structured capture, or point priors. Modeling dis-
tant background regions (e.g., sky, distant mountains, etc.) remains
challenging. In Kerbl et al. [7], the authors introduce a hierarchy of
3D Gaussians to preserve the visual quality for extensive scenarios.
Furthermore, they use a small set of Gaussians to define a spherical
skybox just for the sky. With a similar strategy, our method addresses
the gaps between nearby and distant elements (including but not lim-
ited to the sky) by combining ""standard"" Gaussians within a defined
foreground depth for detailed local fidelity, with a spherical shell of
Gaussians (SSG) designed to model the background elements.",2974
http://arxiv.org/abs/2510.09012v1,2510.09012v1,2025-10-10T05:26:11+00:00,Towards Better & Faster Autoregressive Image Generation: From the Perspective of Entropy,"In this work, we first revisit the sampling issues in current autoregressive
(AR) image generation models and identify that image tokens, unlike text
tokens, exhibit lower information density and non-uniform spatial distribution.
Accordingly, we present an entropy-informed decoding strategy that facilitates
higher autoregressive generation quality with faster synthesis speed.
Specifically, the proposed method introduces two main innovations: 1) dynamic
temperature control guided by spatial entropy of token distributions, enhancing
the balance between content diversity, alignment accuracy, and structural
coherence in both mask-based and scale-wise models, without extra computational
overhead, and 2) entropy-aware acceptance rules in speculative decoding,
achieving near-lossless generation at about 85\% of the inference cost of
conventional acceleration methods. Extensive experiments across multiple
benchmarks using diverse AR image generation models demonstrate the
effectiveness and generalizability of our approach in enhancing both generation
quality and sampling speed.","2.1 Autoregressive image generation
Early work [ 27] generates images directly at pixel level. Later approaches adopt a two-stage pipeline:
images are first quantized into discrete tokens [ 10,11], then generated with Transformers in raster
order [ 28,29,30,31,32,33]. Recent efforts scale this paradign with larger models and stronger
conditioning. LlamaGen [ 1] provides class and text-conditioned baselines; Lumina-mGPT [ 2] and
2
Top-k
Top-p
Temp.CFG
FIDCLIPCFG
(1) k=16384; CFG=2(2) k=16384; CFG=7.5
(3) k=1; CFG<2
(3) k=1; CFG=7.5
Top-kGreedyRandom
(c)Quantitative Evaluation Across Settings(b)Qualitative Comparison Across SettingsAvg.Spectral of Segments
TextImage
(a) Difference between Text & ImageFrequency (%)Figure 2: (a) Comparison of information density between image and text. Histogram of average
frequency-domain embeddings from LlamaGen [ 1] (image) and Qwen2 [ 6] (text) show the uneven
spatial distribution in images with a large amount of low-frequency components. (b) Qualitative
results under various configurations. High CFG (Classfier-Free Guidance) or low top- Koften harms
fidelity, while lower CFG with higher top- Kimproves fidelity but may reduce text-image consistency.
(c) Quantitative evaluation of LlamaGen under different sampling settings.
Anole [ 34] fine-tune Chameleon [ 12] for improved text-conditioned generation. Unified frameworks
further bridge understanding and generation [ 8,9,35,36] in a single Transformer. Meanwhile, image
tokenizers have evolved for better reconstruction [ 37,38,39,40] or multimodal integration [ 41,42].
While proven effective, the vanilla autoregressive paradigm suffers from slow and rigid next-token pre-
diction. To improve efficiency, recent studies explore more strategies, including multi-token prediction
via random masking [ 14,43,44], coarse-to-fine modeling [ 7,13,45,46]or hybrid approaches [ 47,48].
Nonetheless, vector-quantized models still rely on sampling from token distributions, making genera-
tion quality sensitive to the sampling strategy.
2.2 Sampling strategies in autoregressive models
Transformers model the probability distribution over tokens, requiring specific sampling strategies
to obtain concrete outputs. Common approaches in language modeling include top-k[ 49] and top-
p[18] sampling, which truncate the candidate space by rank or cumulative probability. EDT [ 20]
dynamically adjusts temperature based on entropy to balance diversity and precision. Other ap-
proaches explore repetition penalties [ 50], contrastive decoding [ 22], speculative decoding [ 51,52],
and search-based techniques [53, 54, 25, 55, 56] to reduce hallucination or speed up inference.
In visual generation, a higher degree of randomness is often needed to produce more realistic and
detailed content. LlamaGen [ 1] and Lumina-mGPT [ 2] demonstrate that much larger top-kvalues
than those used in language models help avoid over-smoothed and low-detail outputs. Recent
methods [ 26,57] apply speculative [ 58] or parallel decoding [ 59,60] to accelerate image synthesis.
However, they overlook the highly uneven spatial information distribution in images compared to
text, and do not tailor decoding for autoregressive image generation.",3220
http://arxiv.org/abs/2510.08409v1,2510.08409v1,2025-10-09T16:28:48+00:00,Optimal Stopping in Latent Diffusion Models,"We identify and analyze a surprising phenomenon of Latent Diffusion Models
(LDMs) where the final steps of the diffusion can degrade sample quality. In
contrast to conventional arguments that justify early stopping for numerical
stability, this phenomenon is intrinsic to the dimensionality reduction in
LDMs. We provide a principled explanation by analyzing the interaction between
latent dimension and stopping time. Under a Gaussian framework with linear
autoencoders, we characterize the conditions under which early stopping is
needed to minimize the distance between generated and target distributions.
More precisely, we show that lower-dimensional representations benefit from
earlier termination, whereas higher-dimensional latent spaces require later
stopping time. We further establish that the latent dimension interplays with
other hyperparameters of the problem such as constraints in the parameters of
score matching. Experiments on synthetic and real datasets illustrate these
properties, underlining that early stopping can improve generative quality.
Together, our results offer a theoretical foundation for understanding how the
latent dimension influences the sample quality, and highlight stopping time as
a key hyperparameter in LDMs.","Learning low-dimensional data with diffusion models.Riemannian Diffusion Models, introduced
by Huang et al. (2022); De Bortoli et al. (2022), generalize the diffusion process to operate on
Riemannian manifolds and preserve a known geometric structure by design. Subsequent theoretical
work has analyzed the behavior of standard Denoising Diffusion Probabilistic Models (DDPMs)
under the manifold hypothesis, demonstrating that they can implicitly adapt to the data’s intrinsic
dimension without explicit knowledge of the manifold (Tang & Yang, 2024; George et al., 2025).
Further improvements in computational and memory efficiency were introduced by LDMs (Rombach
et al., 2022) by first training a compression model to transform images into a lower-dimensional latent
space, from which the original data can be reconstructed at high fidelity. In practice, this approach is
implemented with a regularized V AE (Esser et al., 2021). The LDM is then trained in the latent space.
Building on this core concept, LDMs have been extended to new domains, such as the generation
of high-resolution videos (Blattmann et al., 2023). Furthermore, extensive research has focused on
improving LDM’s sampling quality, including methods like aligning encoded images with DINOv2
representations (Yu et al., 2024), and enhancing the robustness of the latent space through explicit or
implicit equivariance constraints (Kouzelis et al., 2025; Skorokhodov et al., 2025; Zhou et al., 2025).
In contrast to standard diffusion models, theoretical properties of LDMs have been little studied; in
this work, we investigate the connection of the latent dimension with diffusion stopping time and
score matching regularization.
Optimal stopping time of diffusion models.Focusing on a theoretical analysis of this phenomenon,
Achilli et al. (2025) investigate the optimal stopping time for diffusion models under the assumption
that the data is concentrated on a low-dimensional manifold, a concept formalized by the Hidden
Manifold Model (Goldt et al., 2020). Closer to our contribution is the work of Hurault et al. (2025).
They also investigate the scenario where the true data distribution is Gaussian. Their analysis focuses
on learning the score function using SGD, and allows them to determine an optimal stopping time.
However, the study of these authors is limited to the diffusion model and did not consider the
two-stage architecture of LDMs. Furthermore, the relationship between the data dimension and the
derived optimal stopping time remained unexplored in their findings. In contrast, our work directly
investigates the influence of the latent dimension on the optimal stopping time by incorporating an
autoencoder into the diffusion model framework. We also demonstrate the need of early stopping
without discretization of the backward diffusion process.",2844
http://arxiv.org/abs/2509.23449v1,2509.23449v1,2025-09-27T18:34:32+00:00,Beyond Embeddings: Interpretable Feature Extraction for Binary Code Similarity,"Binary code similarity detection is a core task in reverse engineering. It
supports malware analysis and vulnerability discovery by identifying
semantically similar code in different contexts. Modern methods have progressed
from manually engineered features to vector representations. Hand-crafted
statistics (e.g., operation ratios) are interpretable, but shallow and fail to
generalize. Embedding-based methods overcome this by learning robust
cross-setting representations, but these representations are opaque vectors
that prevent rapid verification. They also face a scalability-accuracy
trade-off, since high-dimensional nearest-neighbor search requires
approximations that reduce precision. Current approaches thus force a
compromise between interpretability, generalizability, and scalability.
  We bridge these gaps using a language model-based agent to conduct structured
reasoning analysis of assembly code and generate features such as input/output
types, side effects, notable constants, and algorithmic intent. Unlike
hand-crafted features, they are richer and adaptive. Unlike embeddings, they
are human-readable, maintainable, and directly searchable with inverted or
relational indexes. Without any matching training, our method respectively
achieves 42% and 62% for recall@1 in cross-architecture and cross-optimization
tasks, comparable to embedding methods with training (39% and 34%). Combined
with embeddings, it significantly outperforms the state-of-the-art,
demonstrating that accuracy, scalability, and interpretability can coexist.","Dynamic analysis.This method family consists of analyz-
ing the features of a binary or code fragment by monitoring
its runtime behavior. This method is compute intensive and
requires a cross-platform emulator, but completely sidesteps
the syntactic aspects of binary code and solely analyzes its
semantics. [34] As such, this method is highly resilient to
12
obfuscations, but requires a sandboxed environment and is
hard to generalize across architectures and application binary
interfaces [15, 17].
Code Representations.Our research focuses on methods
of binary similarity that use the assembly code (and possibly
the control flow graph) retrieved from the binary to perform
similarity detection. However, other approaches exist that use
an alternate representation language, so that binaries compiled
for all architectures are extracted into the same representation
language. Some methods turn to the well known LLVM-IR
[19, 20, 57] as representation language, and others prefer the
VEX intermediate representation [43, 50].
Full Program Similarity.An alternate approach to binary
matching is to compare whole binaries, instead of their indi-
vidual functions. This type of method constructs a fingerprint
for the whole binary as compared to having one embedding
per function. This approach is primarily used for malware
identification [27, 39, 40, 54], but also for identifying known
vulnerabilities [11, 30].
LLM-based software analysis. Researchers have found
many related applications of language models to binary and
software analysis. We give a sparse overview of the progress
that has been made in this area of research. For vulnerability
detection, FuncVul [21] is a language-model-based method
to detect whether a provided source function is vulnerable.
LLM4Decompile [45] tries to utilize LLMs for decompila-
tion. An open source LLM is fine-tuned to learn the source
code representation of an assembly function, and the model
is then evaluated on its decompilation capabilities. Fang et
al. [18] measure the competency of LLMs for source code
analysis, with a focus on obfuscated source code. Their results
match our findings, that LLMs are efficient at code analysis,
but they also demonstrate that language models still struggle
when faced with cases of advanced obfuscation techniques.
LLM-based fuzzing is another area that has recently gained
interest. For instance, Asmita et al. [4] use a LLM to generate
the initial seed of a fuzzing pipeline on the BusyBox [51]
executable.",2497
http://arxiv.org/abs/2509.23673v1,2509.23673v1,2025-09-28T06:26:11+00:00,RCI: A Score for Evaluating Global and Local Reasoning in Multimodal Benchmarks,"Multimodal Large Language Models (MLLMs) have achieved impressive results on
vision-language benchmarks, yet it remains unclear whether these benchmarks
assess genuine global reasoning or allow success via localized visual cues.
Existing evaluation methods do not explicitly measure this distinction,
hindering effective dataset curation and real-world focused model development.
  We introduce Region Comprehension Index (RCI), the first model-based score to
directly quantify a dataset's reliance on global versus local visual
information. RCI systematically compares reference-model performance on image
patches versus full images, revealing if tasks require holistic image
understanding or can be solved with partial or localized visual cues.
  When applying RCI to 13 widely used multimodal benchmarks, we observed that
most of them favor localized reasoning and exhibit significant spatial biases,
indicating potential risks in real-world applications. RCI equips researchers &
practitioners with an actionable tool for diagnosing & mitigating these biases,
enabling the construction of datasets and benchmarks to foster the development
of robust, enterprise-ready multimodal systems.","2.1 Vision-Language Benchmarks
Vision-language benchmarks such as MS
COCO (Chen et al., 2015), GQA (Hudson and
Manning, 2019), TextVQA & VizWiz have signifi-
cantly advanced multimodal model development.
However, recent works (Geirhos et al., 2020; Guan
et al., 2024; Woh et al., 2022; Huang et al., 2024;
Kamath et al., 2023) reveal critical limitations:
many tasks can be effectively addressed by
exploiting localized visual information, creating
anillusion of progress. For instance, models
often leverage minimal contextual clues & biased
spatial distributions to achieve deceptively high
benchmark scores (Woh et al., 2022; Kamath
et al., 2023). Benchmarks like SPEC (Peng
et al., 2024), AMBER, BLINK, MVTamperBench
(Agarwal et al., 2025a), & What’s Up (Kamath
et al., 2023) explicitly highlight these issues by
isolating fine-grained spatial-temporal & semantic
reasoning tasks, uncovering significant model
limitations. Such localized shortcuts undermine
robustness, interpretability, & generalization,
impacting real-world applications like medical
analysis (Pattnayak et al., 2025b), document analy-
sis (Meghwani et al., 2025; Agarwal et al., 2025b),
& autonomous navigation, where comprehensive
visual reasoning is essential.2.2 Spatial Reasoning and Dataset Quality
Assessment
Spatial reasoning remains a challenging yet es-
sential capability for vision-language models (Wu
et al., 2024; Kamath et al., 2023). Recent research
demonstrates widespread deficiencies in spatial re-
lation comprehension, even among advanced mod-
els (Kamath et al., 2023). For example, SPEC
explicitly diagnoses model comprehension of spa-
tial attributes, demonstrating near-random perfor-
mance even for state-of-the-art MLLMs (Peng
et al., 2024). Similarly, Zhao et al. (2023) high-
light the importance of dataset quality, revealing
substantial annotation issues that exacerbate spa-
tial reasoning deficiencies. To address these short-
comings, some researchers have proposed visual
prompting techniques, guiding model’s attention
explicitly through visual cues (Yu et al., 2024).
2.3 Metrics for Vision-Language Tasks
Traditional metrics like CIDEr, BLEU (Papineni
et al., 2002), and METEOR (Banerjee and Lavie,
2005) primarily assess linguistic properties such as
caption similarity and diversity, but fail to explic-
itly capture deeper reasoning capabilities or spatial
dependencies. Metrics like CLIPScore and FID
evaluate cross-modal alignment and image realism.
Recently, Tao et al. (2024) probed multimodal
models for global and local semantic representa-
tions, highlighting discrepancies in representation
across model layers. Although insightful, this work
does not quantify how datasets themselves struc-
ture or promote spatial reasoning explicitly.
Our ContributionOur research explicitly ad-
dresses these critical gaps by introducing RCI. Un-
like previous approaches that focus on isolated eval-
uation dimensions or linguistic alignment, RCI
systematically measures and reveals whether a
dataset’s tasks fundamentally depend on integrat-
ing information across the entire image, or can be
addressed using isolated regions.",3137
http://arxiv.org/abs/2510.05650v1,2510.05650v1,2025-10-07T07:58:32+00:00,EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,"Reproducing cognitive development, group interaction, and long-term evolution
in virtual classrooms remains a core challenge for educational AI, as real
classrooms integrate open-ended cognition, dynamic social interaction,
affective factors, and multi-session development rarely captured together.
Existing approaches mostly focus on short-term or single-agent settings,
limiting systematic study of classroom complexity and cross-task reuse. We
present EduVerse, the first user-defined multi-agent simulation space that
supports environment, agent, and session customization. A distinctive
human-in-the-loop interface further allows real users to join the space. Built
on a layered CIE (Cognition-Interaction-Evolution) architecture, EduVerse
ensures individual consistency, authentic interaction, and longitudinal
adaptation in cognition, emotion, and behavior-reproducing realistic classroom
dynamics with seamless human-agent integration. We validate EduVerse in
middle-school Chinese classes across three text genres, environments, and
multiple sessions. Results show: (1) Instructional alignment: simulated IRF
rates (0.28-0.64) closely match real classrooms (0.37-0.49), indicating
pedagogical realism; (2) Group interaction and role differentiation: network
density (0.27-0.40) with about one-third of peer links realized, while
human-agent tasks indicate a balance between individual variability and
instructional stability; (3) Cross-session evolution: the positive transition
rate R+ increase by 11.7% on average, capturing longitudinal shifts in
behavior, emotion, and cognition and revealing structured learning
trajectories. Overall, EduVerse balances realism, reproducibility, and
interpretability, providing a scalable platform for educational AI. The system
will be open-sourced to foster cross-disciplinary research.","\textbf{EduVerse} aims to build a user-defined multi-agent simulation space for education that systematically supports cognitive development, group interaction, and long-term evolution in virtual classrooms. Despite progress in educational agents, multi-agent simulation, and LLM-based generation, a unified platform integrating these dimensions remains absent. We therefore review three threads closely aligned with EduVerse’s dimensions (see App.~\ref{sec:more-relate-work} for details).

\textbf{Educational agents and virtual classrooms.}
Early systems such as \textit{Cognitive Tutor} \citep{Anderson01041995} and \textit{SimStudent} \citep{article} focused on skill acquisition and personalization, typically via rule- or model-based mechanisms \citep{christensen2011simschool, foley2005making, carrington2011enhancing, dotger2010medicine}. Teacher-training simulations used scripted virtual students as scaffolds but lacked adaptivity to feedback, peer influence, or classroom context \citep{kervin2006classsim, dieker2015tle, delamarre2021interactive, shernoff2018early, kelleci2021using}. Recent generative extensions enable task-level learning \citep{zhang_simulating_2024, lee_generative_nodate, yue_mathvc_2025, mollick_ai_2024, markel_gpteach_2023, wang_generative_2025, fahid_online_2024}, but often omit emotional modeling, stylistic progression, and multi-agent coupling, limiting their suitability for open, dynamic classrooms.


\textbf{Multi-agent social simulations.}
Works such as \textit{Generative Agents} show that LLMs enhanced with memory, planning, and reflection can generate human-like social behaviors in sandbox settings \citep{park2023generative, li2023camelcommunicativeagentsmind, chen2023agentversefacilitatingmultiagentcollaboration, jinxin_cgmi_2023}. However, these focus on adult roles and informal contexts, overlooking classroom-specific structures such as IRF discourse, teacher–student roles, and goal alignment. They also lack mechanisms for knowledge progression tracking and temporal adaptivity.

\textbf{Personalized modeling and long-term coherence.}
Persona conditioning and style control are widely used to maintain role consistency \citep{shao_character-llm_2023, jiang2024personallminvestigatingabilitylarge, wang2024rolellmbenchmarkingelicitingenhancing}, with design patterns surveyed by \citet{tseng-etal-2024-two}. Yet long-term interactions often suffer from persona drift, leading to memory-based prompting \citep{zhong2023memorybankenhancinglargelanguage}, style constraints \citep{roy2023conversationstyletransferusing}, and metacognitive or reflective mechanisms \citep{madaan2023selfrefineiterativerefinementselffeedback, li_exploring_2025, didolkar2024metacognitivecapabilitiesllmsexploration}. Research on continual and lifelong learning also contributes to longitudinal coherence \citep{wang2024comprehensivesurveycontinuallearning, parisi_continual_2019, zheng2024lifelonglearninglargelanguage, chen_lifelong_nodate, zheng2025lifelonglearninglargelanguage, maharana2024evaluatinglongtermconversationalmemory, wang2025recursivelysummarizingenableslongterm, tan2025prospectretrospectreflectivememory, li2025helloagainllmpoweredpersonalized}. However, these methods are mostly evaluated in single-agent or non-classroom contexts, rarely integrating group-level structures or pedagogically grounded evolution. \textit{EduAgent} \citep{xu_eduagent_2024} models individual cognitive and metacognitive processes but lacks multi-agent coordination and group dynamics.

Overall, Prior work offers key ingredients—feedback, generative behavior, and role consistency—but remains fragmented across time spans (short-term vs. longitudinal), modeling dimensions (individual vs. group), and educational contexts (informal vs. classroom-structured). \textbf{EduVerse} bridges these gaps by combining user-defined environments, agent, sessions and by adopting the \textbf{CIE} architecture to unify individual coherence, group dynamics, and cross-session evolution in a scalable and interpretable classroom simulation platform.",4076
http://arxiv.org/abs/2510.03224v1,2510.03224v1,2025-10-03T17:57:25+00:00,Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,"We propose a test-time defense mechanism against adversarial attacks:
imperceptible image perturbations that significantly alter the predictions of a
model. Unlike existing methods that rely on feature filtering or smoothing,
which can lead to information loss, we propose to ""combat noise with noise"" by
leveraging stochastic resonance to enhance robustness while minimizing
information loss. Our approach introduces small translational perturbations to
the input image, aligns the transformed feature embeddings, and aggregates them
before mapping back to the original reference image. This can be expressed in a
closed-form formula, which can be deployed on diverse existing network
architectures without introducing additional network modules or fine-tuning for
specific attack types. The resulting method is entirely training-free,
architecture-agnostic, and attack-agnostic. Empirical results show
state-of-the-art robustness on image classification and, for the first time,
establish a generic test-time defense for dense prediction tasks, including
stereo matching and optical flow, highlighting the method's versatility and
practicality. Specifically, relative to clean (unperturbed) performance, our
method recovers up to 68.1% of the accuracy loss on image classification, 71.9%
on stereo matching, and 29.2% on optical flow under various types of
adversarial attacks.","The literature on adversarial attack and defense is extensive. We highlight some of the advances. 

\noindent\textbf{Adversarial Training as Defense.}
Adversarial training increases the robustness of the model by training it with adversarially augmented images. The popular attack methods used are Fast Gradient Sign Method (FGSM) \cite{goodfellow2014explaining}, and Projected Gradient Descent (PGD) \cite{madry2017towards}. 
ALP \cite{kannan2018adversarial} Minimizes the difference between the logits of pairs of clean and adversarially augmented images. TRADES \cite{zhang2019theoretically} decomposes prediction error for adversarial examples into natural error and boundary error to improve adversarial robustness at the cost of accuracy. MART \cite{wang2019improving} improves adversarial robustness by considering misclassified natural examples during training. Subsequent work \cite{cai2018curriculum, zhang2020attacks, wang2022self, jin2022enhancing, ghiasvand2024robust} improves adversarial robustness with curriculum learning \cite{bengio2009curriculum}, model ensembling, second order statistics, and gradient tracking. 

On the other hand, some methods learn robust feature representation through a modified architecture or feature manipulation. \cite{galloway2019batch, benz2021batch, wang2022removing} investigate the effect of batch normalization on adversarial robustness. \cite{dhillon2018stochastic, madaan2020adversarial} prunes certain activations in the network that are susceptible to adversarial attacks.
\cite{xiao2019enhancing} keep k-features with the largest magnitude and deactivate everything else.
\cite{zoran2020towards} uses an attention mask to highlight robust regions on the feature.
Feature Denoising (FD) \cite{xie2019feature} uses classical denoising techniques to deactivate abnormal activations.
\cite{bai2021improving, yan2021cifs} proposed Channel Activation Suppression (CAS) and Channel-wise Importance-based Feature Selection (CIFS) to deactivate feature channels that are vulnerable to attacks. 
\cite{kim2023feature} improves the robustness with Feature Separation and Recalibration (FSR).
Our method also operates in feature space, but purely during test time. While we choose some of these works as baselines, our method works in conjunction with any aforementioned methods.



\noindent\textbf{Adversarial Purification as Defense}
Another line of work focuses on purifying or augmenting the images before they are used as input. \cite{tang2024robust, yeh2024test, tsai2023test}  train an FGSM robust classifier, a diffusion model, or a mask auto-encoder, respectively, to purify adversarial examples.
\cite{wang2021fighting} optimizes both the model and the input to minimize the entropy of model predictions to adapt to changing attacks. 
\cite{cohen2024simple} trains a random forest predictor to ensemble outputs from test-time augmented images.
These works involve training a new model or updating the original model, while our method is purely test-time and does not require any training. 
\cite{perez2021enhancing} ensembles model output from different augmentations, which is a special case of our method, as the ensemble is performed solely on the output, while we can ensemble at any layer, which both saves computational cost and achieves higher performance



Notably, we recognize that above methods focus solely on classification as a task for adversarial attacks. Through extensive experiments, we demonstrate that our method can not only perform well on classification, but also on dense prediction tasks like optical flow, and stereo matching.


\noindent\textbf{Stochastic Resonance (SR)} was proposed by \cite{benzi1981mechanism} and first applied in climate dynamics \citep{benzi1982stochastic} and later in signal processing \citep{wellens2003stochastic, kosko2001robust, chen2007theory} and acoustics \citep{shu2016application, wang2014adaptive}. Recently, Stochastic Resonance Transformer (SRT) \cite{lao2024sub} uses SR to ``super-resolve'' Vision Transformer (ViT) embeddings. In this work, we instead use SR to mitigate adversarial perturbations. Since SR has been developed specifically to address {\em quantization artifacts}, it has never before been used to mitigate classes of perturbations beyond aliasing. Our novel use of the technique leverages the fact that group transformations and spatial quantization preserve the statistics of natural images, which are heavy-tailed, but do not preserve the statistics of adversarial perturbations.",4524
http://arxiv.org/abs/2510.07626v1,2510.07626v1,2025-10-08T23:47:05+00:00,LLM Unlearning Under the Microscope: A Full-Stack View on Methods and Metrics,"Machine unlearning for large language models (LLMs) aims to remove undesired
data, knowledge, and behaviors (e.g., for safety, privacy, or copyright) while
preserving useful model capabilities. Despite rapid progress over the past two
years, research in LLM unlearning remains fragmented, with limited clarity on
what constitutes effective unlearning and how it should be rigorously
evaluated. In this work, we present a principled taxonomy of twelve recent
stateful unlearning methods, grouped into three methodological families:
divergence-driven optimization, representation misalignment, and
rejection-based targeted unlearning. Building on this taxonomy, we revisit the
evaluation of unlearning effectiveness (UE), utility retention (UT), and
robustness (Rob), focusing on the WMDP benchmark. Our analysis shows that
current evaluations, dominated by multiple-choice question (MCQ) accuracy,
offer only a narrow perspective, often overstating success while overlooking
the model's actual generation behavior. To address this gap, we introduce open
question-answering (Open-QA) metrics that better capture generative performance
and reveal the inherent UE-UT tradeoff across method families. Furthermore, we
demonstrate that robustness requires finer-grained analysis: for example,
vulnerabilities differ substantially between in-domain relearning and
out-of-domain fine-tuning, even though both fall under model-level attacks.
Through this study, we hope to deliver a full-stack revisit of LLM unlearning
and actionable guidance for designing and evaluating future methods.","LLM unlearning and benchmarking studies.Recent work on LLM unlearning (Liu et al., 2024b; Maini et al., 2024;
Liu et al., 2024a; Yao et al., 2023) has made progress in mitigating risks such as copyright infringement (Eldan &
Russinovich, 2023), privacy leakage (Hu et al., 2024; Wu et al., 2023), and harmful content generation (Li et al., 2024;
Lu et al., 2022). For detailed introductions to the diverse families of unlearning methods, we refer readers to Sec. 3.
Given the growing interest in LLM unlearning, several benchmarks have been proposed to systematically evaluate its
effectiveness (Li et al., 2024; Maini et al., 2024; Jin et al., 2024; Shi et al., 2024; Eldan & Russinovich, 2023). These
benchmarks can be grouped into two categories based on whether models are fine-tuned on the forget corpus. The first
2
category fine-tunes models on domain-specific corpora to introduce unlearning targets. WHP (Eldan & Russinovich,
2023) uses the Harry Potter series, TOFU (Maini et al., 2024) constructs synthetic author profiles, and MUSE (Shi et al.,
2024) provides Harry Potter and news corpora with privacy-leakage evaluation. A drawback is that domain-specific
fine-tuning can degrade general abilities such as reasoning, complicating fair utility assessment (Jin et al., 2024).
The second category avoids fine-tuning, aligning more closely with real-world use cases. WMDP (Li et al., 2024)
constructs forget corpora in biology, chemistry, and cybersecurity, and evaluates unlearning via multiple-choice QA
proxies for hazardous knowledge, alongside utility on MMLU (Hendrycks et al., 2020) and MT-Bench (Zheng et al.,
2023). RWKU (Jin et al., 2024) focuses on real-world entities, evaluating memorization on the forget set and reasoning,
truthfulness, factuality, and fluency on retain tests.
Adversarial robustness of LLM unlearning.Robustness has emerged as a central challenge for unlearning, as
unlearned models remain vulnerable to diverse attacks (Łucki et al., 2024; Che et al., 2025; Hu et al., 2024). Parameter-
level attacks can restore forgotten content through light fine-tuning (Łucki et al., 2024; Hu et al., 2024; Qi et al.,
2023; Halawi et al., 2024; Lermen et al., 2023; Huang et al., 2024), pruning (Jain et al., 2023; Wei et al., 2024;
Lee et al., 2018), or quantization (Zhang et al., 2024b), which may re-expose knowledge by compressing weights.
Representation-level attacks perturb embeddings or hidden activations to revive residual traces (Schwinn et al., 2024;
Sheshadri et al., 2024), while input-level attacks exploit adversarial prompting or query optimization (Chao et al., 2025;
Shin et al., 2020), ranging from gradient-guided search (Zou et al., 2023) to perplexity-based strategies (Sadasivan
et al., 2024). In response, several defenses have been proposed. Adversarial training strengthens robustness against such
attacks (Sheshadri et al., 2024), and meta-learning strategies further enhance defense (Tamirisa et al., 2024; Sondej
et al., 2025). Sharpness-aware minimization encourages flat minima to reduce susceptibility to relearning (Fan et al.,
2025). Invariance-based regularization introduces robustness through invariant risk principles (Wang et al., 2025b),
while distillation-based methods transfer knowledge into partially noised student models (Lee et al., 2025). Although
robustness benchmarks are emerging (Che et al., 2025; Hu et al., 2025), they remain limited: Che et al. (2025) evaluates
only with MCQ accuracy, and Hu et al. (2025) considers only in-domain relearning, leaving comprehensive assessment
open.",3571
http://arxiv.org/abs/2509.17914v1,2509.17914v1,2025-09-22T15:39:33+00:00,XaaS Containers: Performance-Portable Representation With Source and IR Containers,"High-performance computing (HPC) systems and cloud data centers are
converging, and containers are becoming the default method of portable software
deployment. Yet, while containers simplify software management, they face
significant performance challenges in HPC environments as they must sacrifice
hardware-specific optimizations to achieve portability. Although HPC containers
can use runtime hooks to access optimized MPI libraries and GPU devices, they
are limited by application binary interface (ABI) compatibility and cannot
overcome the effects of early-stage compilation decisions. Acceleration as a
Service (XaaS) proposes a vision of performance-portable containers, where a
containerized application should achieve peak performance across all HPC
systems. We present a practical realization of this vision through Source and
Intermediate Representation (IR) containers, where we delay
performance-critical decisions until the target system specification is known.
We analyze specialization mechanisms in HPC software and propose a new
LLM-assisted method for automatic discovery of specializations. By examining
the compilation pipeline, we develop a methodology to build containers
optimized for target architectures at deployment time. Our prototype
demonstrates that new XaaS containers combine the convenience of
containerization with the performance benefits of system-specialized builds.","Building: Languages common in HPC, like C++ and Fortran, are no-
tably missing in commonly used package managers. EasyBuild [ 44]
builds HPC applications from source using specific toolchains, sup-
porting hierarchical module creation [ 32]. Spack [ 31] is a package
manager that parameterizes builds with constraints and versioned
dependencies. Resolving dependencies can be reduced with declar-
ative programming [ 30] or machine learning [ 57]. E4S provides
curated HPC software stacks, including hardware-specific con-
tainers [ 41,78]. Binary distribution is possible: Spack uses binary
caches [ 71], and EESSI distributes EasyBuild stacks via network
filesystems [ 27]. XaaS complements these tools by addressing the
trade-off between container portability and performance.
Portable and HPC Containers: Injecting or replacing con-
tainer libraries with host counterparts can be achieved with many
container runtimes, but it can require expert knowledge of the
system. Apptainer [ 46] supports semi-manual mounting of host
MPI [ 6]. Charliecloud [ 65] uses heuristics to copy resource-specific
files (NVIDIA, libfabric) into images, permanently modifying them.
Sarus [ 13,51] and Podman-HPC [ 73] use OCI hooks to inject host
MPI and GPU libraries. XaaS can use the same hooks, but source con-
tainers can be compiled to use the version of the specialized library
compatible with the one available on the system. Vendor container
registries offer optimized but platform-specific images [5, 60].
Containers can already contain intermediate representation as
Python and Java bytecode [ 83]. Popcorn Linux [ 9] enables cross-ISA
live migration with a custom compiler and kernel that transform
LLVM IR into multi-ISA binaries with compatible data layouts [ 40].
H-Containers [ 8,79] achieve migration by decompiling to LLVM IR
and recompiling to different ISAs. To the best of our knowledge, this
is the only known use of IR for container deployment. However, it
differs fundamentally from XaaS: we use IR-based representations
to access customized performance features of each system.
Performance Portability: Performance portability often in-
volves rewriting applications using models like OpenMP, OpenACC,
or SYCL [ 17,63]. Frameworks provide new abstractions for memory
access (Kokkos [ 18]), loop parallelism (Raja [ 10]), and data-centric
programming (DaCe [ 84]). Compilers can translate programming id-
ioms to specialized libraries [ 36] and accelerators [ 53], and upgrade
applications to use newer and specialized implementations of linear
algebra libraries [22]. XaaS focuses on portable representations of
existing applications without rewriting or requiring single-source
code. We do not require applications to be single-source or use the
same set of source files across all systems and devices.
Emulation, Translation, and JIT: Cross-ISA emulation, such
as Docker with QEMU [ 26], is unsuitable for HPC due to
performance overheads. Runtime MPI ABI translation layers
like Wi4MPI [ 48] can incur performance overhead. Other tools
include mpixlate [23] (compatibility with Cray MPI), MPITrampo-
line [ 68], Mukautuva [ 38], and MPI-Adapter2 [ 74]. JIT compilation,
used in CUDA PTX, OpenCL, SYCL IR [ 4], allows for specialization
of the final implementation by compiling the code dynamically",3309
http://arxiv.org/abs/2509.25671v1,2509.25671v1,2025-09-30T02:14:30+00:00,The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,"Benchmarks shape scientific conclusions about model capabilities and steer
model development. This creates a feedback loop: stronger benchmarks drive
better models, and better models demand more discriminative benchmarks.
Ensuring benchmark reliability is therefore essential for trustworthy
evaluation and meaningful progress. In this work, we study benchmark
reliability from a distributional perspective and introduce benchmark harmony,
which measures how uniformly a model's performance is distributed across the
subdomains of a benchmark. We posit that high harmony is a desirable benchmark
property, indicating that the aggregate metric reflects uniform competence
across subdomains. Across 19 multiple-choice benchmarks and five model
families, we map each benchmark onto a mean-variance plane of harmony computed
across models, where high mean and low variance signal more reliable
evaluation. Our analysis shows that less harmonious benchmarks can give
misleading results, since overall accuracy may be disproportionately influenced
by specific subdomains. For instance, ARC-Easy is overwhelmed by questions on
Biological Concepts, overshadowing other critical subdomains such as Geography,
Physics, Chemistry, and Environmental Science. By recommending that harmony
should be reported alongside accuracy, we reframe evaluation from simple
performance averages to a more robust, distributionally reliable measurement of
performance.","\paragraph{Assessing Benchmark Reliability.} Beyond proposing new tasks, a growing body of work interrogates the \emph{reliability} of benchmarks themselves. A line of work targets the robustness of the test sets, focusing on building dynamic benchmarks to replace static benchmarks \citep{kiela2021dynabenchrethinkingbenchmarkingnlp, chiang2024chatbot} and building adversarial perturbations to eliminate spurious cues present in static benchmarks \citep{nie2020adversarialnlinewbenchmark, croce2021robustbenchstandardizedadversarialrobustness}. Closely related are concerns about overfitting to public test sets and contamination from pre-training corpora, which can inflate reported gains \citep{deng2024investigatingdatacontaminationmodern, golchin2024timetravelllmstracing, roberts2023datacontaminationlenstime, dong-etal-2024-generalization}. \citet{dbevalgauntlet} analyzes a collection of benchmarks, showing that some benchmarks (e.g. Hellaswag \citep{zellers2019hellaswagmachinereallyfinish}) scale smoothly with increased scale and compute, while others (e.g. CommonsenseQA \citep{talmor2018commonsenseqa}) do not. 
Another branch of literature audits data reliability and distributional coverage by introducing shifted test sets to probe generalization \citep{recht2019imagenetclassifiersgeneralizeimagenet, taori2020measuringrobustnessnaturaldistribution, teney2020valueoutofdistributiontestingexample} and correcting pervasive label errors in widely used benchmarks \citep{northcutt2021pervasivelabelerrorstest, gema2025mmlu}. Beyond individual datasets, meta-evaluation work proposes frameworks and documentation practices to systematically assess benchmark design, provenance, and intended use \citep{reuel2024betterbenchassessingaibenchmarks, mazumder2023dataperfbenchmarksdatacentricai, gebru2021datasheetsdatasets}. Another important topic is the external validity of benchmarks, such as how well leaderboard gains translate to real-world performance \citep{Ott_2022} and what reported scores actually measure \citep{dehghani2021benchmarklottery, singh2025leaderboardillusion}. Finally, a complementary line of work separates signal from noise in benchmark results by quantifying variance and prescribing protocols that stabilize rankings in order to make comparative conclusions more reliable \citep{madaan2024quantifyingvarianceevaluationbenchmarks, evalarena, heineman2025signalnoiseframeworkreducing}. Advancing this field of work, we contribute a distributional perspective on benchmark reliability. Rather than treating a benchmark evaluation as a single score, we model the benchmark as a mixture over the subdomains of the stated benchmark domain. We then measure how \emph{performance mass} is distributed across these subdomains. This perspective diagnoses whether aggregate metrics reflect a broad competence over the benchmark or are dominated by certain subdomains.



\textbf{Distributional Frameworks for Efficient Evaluation.} 
Scaling laws of neural language models suggest that performance improves with model size \citep{kaplan2020scalinglawsneurallanguage}, encouraging the development of increasingly larger and costlier models. Consequently, there has been growing interest in developing efficient evaluation methods that reduce computational and financial costs without compromising reliability. \citet{perlitz-etal-2024-efficient} introduce a reliability metric that dynamically adjusts compute by performance tier while preserving rank fidelity. \citet{rodriguez-etal-2021-evaluation} propose Item Response Theory \citep{Tatsuoka1971StatisticalTO} based leaderboards that jointly model difficulty and discrimination to identify examples that best differentiate model performance. Similarly, \citet{polo2024tinybenchmarksevaluatingllmsfewer} propose tinyBenchmarks, an efficient evaluation method that uses IRT to model the discriminative power of benchmark examples, allowing the selection of a small yet representative subset of items that can accurately estimate performance. \citet{vivek2024anchorpointsbenchmarkingmodels} propose anchor point selection to identify small, representative subsets by leveraging cross-model correlations in instance-level predictions. \citet{ethayarajh2022understandingdatasetdifficultymathcalvusable} identify informative data points via \emph{usable information} (how much input a model family can exploit) extending Shannon information to account for model constraints. Notably, these works introduce distinct metrics such as IRT item parameters, cross-model instance correlations, and information-theoretic usable information to characterize the benchmark distribution and guide principled compression of benchmarks. Ultimately, these metrics enable targeted downsampling (e.g., selecting maximally discriminative or most informative items) that preserves rankings and reduces evaluation cost while maintaining coverage. In contrast, we do not seek cheaper evaluations. We instead assess whether a benchmark reliably measures its stated domain and, where it does not, we question the original evaluation rather than preserve it.

Prior work mainly (i) proposes new or dynamic tests, (ii) stabilizes leaderboards through variance control and guidelines, and (iii) compresses evaluation via discriminative selection. We instead audit \emph{existing} benchmarks through a distributional lens, modeling a benchmark as a mixture over subdomains and measuring whether models spread accuracy uniformly. Unlike efficiency work that preserves overall scores while reducing cost, \method{} reveals where aggregate metrics fail to provide a representative understanding of model competency. Our method is post hoc and lightweight, complements robustness and contamination audits, and yields practical guidance: report \method{} with accuracy and rebalance low \method{} benchmarks.

We further discuss additional related work on language model evaluation in Appendix~\ref{sec:add_rel_work}.

\vspace{-0.5em}",5978
http://arxiv.org/abs/2509.23241v1,2509.23241v1,2025-09-27T10:44:38+00:00,Memory Efficient and Staleness Free Pipeline Parallel DNN Training Framework with Improved Convergence Speed,"High resource requirement for Deep Neural Network (DNN) training across
multiple GPUs necessitates development of various parallelism techniques. In
this paper, we introduce two interconnected DNN training frameworks, namely,
V-TiMePReSt and I-TiMePReSt, based on pipeline parallelism, a variant of model
parallelism. V-TiMePReSt is a completely staleness-free system which enables
the DNNs to be trained on the latest updated weights in each stage of all
forward and backward passes. Developing staleness-aware systems at the expense
of weight stashing reduces GPU-memory consumption, however, increases the
number of epochs to converge. Thus, we introduce I-TiMePReSt, which is also a
staleness-aware system, but not at the expense of weight stashing. It does not
rely solely on the stale weights or the latest updated weights. I-TiMePReSt
computes an intermediate weight towards the latter and performs backward pass
on it. Additionally, we formulate the significance of the stale weights
mathematically depending on the degree of staleness. In contrast to
V-TiMePReSt, I-TiMePReSt works based on the assumption that stale weights have
a significant contribution in training, which can be quantified mathematically
based on the degree of staleness, although there are other contributory factors
which should not be ignored. Experimental results show that V-TiMePReSt is
advantageous over existing models in terms of $1)$ the extent of staleness of
the weight parameter values and $2)$ GPU memory efficiency, while I-TiMePReSt
is superior in terms of $1)$ removing staleness of the weight parameters
without removing weight stashing and $2)$ maintaining the trade-off between GPU
memory consumption and convergence speed (number of epochs).","Two commonly used pipeline parallelism frameworks for distributed large-scale deep learning are PipeDream \cite{narayanan2019pipedream} and GPipe \cite{huang2019gpipe}. In order to achieve improved pipeline throughput, PipeDream integrates intra-batch and inter-batch parallelism. The integration enables the overlap of computation and communication, as well as reduced communication. It retains the same weight versions throughout the forward and backward passes of a mini-batch, as well as all the stages in a pass, leveraging both vertical and horizontal weight stashing. The need to maintain weight consistency increases the memory footprint for each stage of DNN training on a mini-batch. 

A backward pass begins as soon as a forward pass concludes, using the same set of workers in reverse order. This bidirectional training of DNNs is devised in PipeDream. PipeDream introduces a scheduling strategy, named as $1$F$1$B ($1$ Forward $1$ Backward) scheduling, which is capable of avoiding circular waiting between forward and backward passes. PipeDream is an asynchronous approach, sending the output activations of each stage in a forward pass asynchronously to the next stage. Simultaneously, another mini-batch starts being processed during this communication. Similarly, the gradients, computed in each stage of a backward pass, are sent asynchronously to the previous stage, while another mini-batch starts processing. PipeDream leverages both pipeline and data parallelism together. Thus, the system allows multiple mini-batches as input for parallel training. This asynchronous communication of activations and gradients results in significant overlap of computation and communication as they operate on different mini-batches.









GPipe introduces a novel batch splitting mechanism where each mini-batch is splitted in micro-batches. The underlying pipeline algorithm is utilized to process the micro-batches of a mini-batch in parallel. In contrast to PipeDream, GPipe is synchronous implementing uni-directional training of DNNs, and it performs synchronous mini-batch gradient descent for DNN training, where the computed gradients are gathered across all micro-batches in a mini-batch and the updates are applied on the weights at the end of a mini-batch. 

The scheduling policy of GPipe limits its applicability merely to networks that can be expressed as a sequence of layers. Both GPipe and PipeDream exploit pipeline and data parallelism together. In contrast to PipeDream, periodic pipeline flushes are performed in GPipe after each mini-batch, in order to maintain single weight version at a time, making GPipe more memory efficient. There exists a single version of weight parameters for all the micro-batches in a mini-batch. 



There are two memory-efficient versions of PipeDream, namely PipeDream-2BW \cite{narayanan2021memory} and PipeDream-Flush \cite{narayanan2021memory}. PipeDream-2BW is based on a double buffered weight updates (2BW) technique, which reduces the memory footprint by reducing the number of active weight versions to two - one for already in-flight micro-batches (also known as shadow version) and the other for newly admitted micro-batches. In contrast to GPipe, it increases pipeline throughput by avoiding pipeline flushes. 

PipeDream-Flush achieves a smaller memory footprint than PipeDream-2BW by performing periodic pipeline flush at the cost of pipeline throughput. It maintains a single weight version at a time, which reduces the memory footprint. Both of them follows 1F1B scheduling, similar to PipeDream.

vPipe \cite{zhao2021v} is a system that introduces dynamic layer partitioning and memory management mechanisms for pipeline parallelism by searching for a plan of near-optimal layer partitioning and memory management. vPipe acts as a virtual layer between pipeline parallel systems such as PipeDream, GPipe, and so forth, and their execution engines such as PyTorch, Tensorflow, and among others. 

BPipe \cite{kim2023bpipe} is another approach in literature for achieving memory balance in pipeline parallelism. By using an activation balancing technique, BPipe allows all GPUs to use similar amounts of memory during training by transferring intermediate activations across them. 

Hippie \cite{10.1145/3472456.3472497} is a hybrid parallel training framework integrating pipeline parallelism and data parallelism to increase the throughput and scalability of large DNN training by hiding gradient communication. In addition, it provides a last-stage pipeline scheduling and recomputation mechanism for specific layers. 



PipePar \cite{zhang2023pipepar} is a model partitioning and task placement algorithm for pipeline parallel DNN training in heterogeneous GPU clusters. PipePar is based on dynamic programming with search space pruning that takes into consideration both the heterogeneity of GPUs and network bandwidth. 

 Koala \cite{tangkoala} is an automatic, end-to-end and  globally optimal searching technique for an efficient scheduling strategy with optimal flexibility and training efficiency. 

It facilitates solving the pipeline schedule development as a Binary-Tree-Traversing optimization problem.


Mario \cite{liu2025mario} is a pipeline optimizer technique that uniquely incorporates activation checkpointing into pipelines in order to achieve reduced and balanced memory footprint across devices. Additionally, it also consists of an automatic scheduler which can determine better parameter configurations. WeiPipe \cite{lin2025weipipe}, also known as weight-pipeline parallelism, is a weight-passing pipeline technique, which decreases communication costs across devices,

 and also ensures scalability 


DualPipe \cite{liu2024deepseek} is a bidirectional pipeline parallelism framework first introduced in DeepSeek V3 Technical Report \cite{liu2024deepseek}. DualPipe eliminates pipeline bubbles through dual-channel processing enabling simultaneous occurrence of forward-backward computation-communication phases and complete synchronization between forward and backward passes. It achieves optimized resource utilization, reduces idle time between processing stages and reduces memory footprint across all the devices. It uses an adaptive task scheduling method based on computational demands. 


MEPipe \cite{sun2025mepipe} introduces a slice-level scheduling method, named as Sequence Virtual Pipeline Parallelism (SVPP), for sequence pipeline parallelism. This method democratizes the training of LLMs to inexpensive accelerators with low-bandwidth interconnection by reducing memory footprint without increasing communication overhead across all devices. 

Zero Bubble \cite{qi2024zero} pipeline parallelism technique introduces a scheduling strategy that achieves almost zero pipeline bubbles under synchronous training mechanisms. In Zero Bubble, the backward computation is splitted into two parts - gradient computation for the input and that for the parameters. 


GraphPipe \cite{jeon2025graphpipe} is another pipeline parallelism technique that enables the system to identify the dependencies between different pipeline stages of a DNN by a directed acyclic graph. In contrast to traditional sequential pipeline parallelism (SPP), GraphPipe enables concurrent execution of computationally independent components of a DNN.



TiMePReSt (Time and Memory Efficient Pipeline Parallel DNN Training with Removed Staleness) \cite{dutta2024timeprest} is a memory-efficient pipeline parallelism based DNN training framework that addresses the above issue of stale weights. It has introduced an intra-batch scheduling technique, named as $n$F$1$B scheduling, that makes the framework more time-efficient.",7712
http://arxiv.org/abs/2510.10806v1,2510.10806v1,2025-10-12T20:52:43+00:00,Is Implicit Knowledge Enough for LLMs? A RAG Approach for Tree-based Structures,"Large Language Models (LLMs) are adept at generating responses based on
information within their context. While this ability is useful for interacting
with structured data like code files, another popular method,
Retrieval-Augmented Generation (RAG), retrieves relevant documents to augment
the model's in-context learning. However, it is not well-explored how to best
represent this retrieved knowledge for generating responses on structured data,
particularly hierarchical structures like trees. In this work, we propose a
novel bottom-up method to linearize knowledge from tree-like structures (like a
GitHub repository) by generating implicit, aggregated summaries at each
hierarchical level. This approach enables the knowledge to be stored in a
knowledge base and used directly with RAG. We then compare our method to using
RAG on raw, unstructured code, evaluating the accuracy and quality of the
generated responses. Our results show that while response quality is comparable
across both methods, our approach generates over 68% fewer documents in the
retriever, a significant gain in efficiency. This finding suggests that
leveraging implicit, linearized knowledge may be a highly effective and
scalable strategy for handling complex, hierarchical data structures.","Existing graph-based LLM applications can be broadly categorized into two types. The first category focuses on solving
fundamental graph problems, such as those related to node properties (e.g., degree, centrality) and relational properties
(e.g., shortest path between two nodes) [ 3,6,7]. The second category, which is more relevant to our work, evaluates the
ability of LLMs to ""talk to a graph"" by answering natural language queries about its content and structure ([8, 4]).
Our work shares a conceptual similarity with the latter category, particularly the bottom-up knowledge aggregation
approach proposed by [ 4]. However, our primary objective is to distill knowledge from a hierarchical structure for a
more efficient Retrieval-Augmented Generation (RAG) pipeline. In contrast, their method focuses on retrieving similar
embedding vectors and summarizing clusters to provide answers, which represents a different approach to a similar
problem.
Another line of work centers on the construction of Knowledge Graphs (KGs) from text and using LLMs for subsequent
reasoning over these KGs ([ 9,10]). These methods primarily address the challenge of structuring unstructured text into
2
Technical ReportA PREPRINT
a graph format, whereas our work begins with an existing hierarchical structure and focuses on creating an optimal
representation of its knowledge for efficient retrieval.
Another line of work relevant to our research is knowledge distillation, which focuses on compressing knowledge from
a large model into a smaller one. Our concept of generating implicit knowledge shares commonality with this field.
For instance, some methods create explicit frameworks, such as using ""teacher-models"" to improve Chain-of-Thought
reasoning ([ 14]) or to distill knowledge for unlabeled data ([ 15]). Other works use customized loss functions to extract
subtler forms of implicit knowledge from a model’s activations or internal states ([ 16]). While these methods primarily
focus on model-level distillation, they underscore the broader principle that valuable knowledge can be distilled into a
more compact and efficient form. Our work applies this fundamental idea to the realm of RAG, demonstrating how
knowledge can be distilled from a data source itself, rather than from a model.
Finally, it has been demonstrated that the performance of RAG-based approaches can be significantly affected by the
number of documents stored in vector databases, often leading to performance degradation as the context size increases
([11,12]). This has spurred research into methods that optimize the retrieval process and manage large contexts more
effectively, such as the work on long-context RAG by Jiang et al. ([ 13]). This body of literature underscores the
critical need for developing efficient and scalable methods to manage the knowledge used in RAG frameworks. Our
work on generating implicit knowledge addresses this need directly by enabling more concise and effective storage of
information, thus mitigating the very performance issues identified in these studies.",3072
http://arxiv.org/abs/2510.03122v2,2510.03122v2,2025-10-03T15:50:52+00:00,HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided Versatile Diffusion,"The reconstruction of visual information from brain activity fosters
interdisciplinary integration between neuroscience and computer vision.
However, existing methods still face challenges in accurately recovering highly
complex visual stimuli. This difficulty stems from the characteristics of
natural scenes: low-level features exhibit heterogeneity, while high-level
features show semantic entanglement due to contextual overlaps. Inspired by the
hierarchical representation theory of the visual cortex, we propose the HAVIR
model, which separates the visual cortex into two hierarchical regions and
extracts distinct features from each. Specifically, the Structural Generator
extracts structural information from spatial processing voxels and converts it
into latent diffusion priors, while the Semantic Extractor converts semantic
processing voxels into CLIP embeddings. These components are integrated via the
Versatile Diffusion model to synthesize the final image. Experimental results
demonstrate that HAVIR enhances both the structural and semantic quality of
reconstructions, even in complex scenes, and outperforms existing models.","\subsection{Diffusion Models}
Diffusion Models (DMs)\cite{3-7,mdb8,Mdb13,14-6,4-9,11-3-23} are probabilistic generative models that generate samples through iterative denoising of Gaussian noise. To address computational limitations in pixel space processing, Latent Diffusion Models (LDMs) were developed to operate in compressed latent spaces. The framework utilizes a pretrained autoencoder that compresses images into low dimensional latent representations, significantly reducing computational costs while maintaining generation fidelity.

Expanding on LDM, the Versatile Diffusion (VD)\cite{mdb46} introduces a dual guidance mechanism that integrates CLIP features from images, text, or both, with a distinctive reverse process initialization strategy. VD is trained on datasets such as Laion2B-en\cite{uni31} and COYO-700M\cite{uni4}, and employs a pretrained ViT-L/14 CLIP backbone\cite{uni23-clip} for powerful multi-modal fusion. We chose VD for the final image reconstruction because it can integrate structural and semantic features during denoising.

\subsection{Visual Reconstruction from fMRI}
Decoding visual stimuli from brain activity remains a key challenge in computational neuroscience. Early approaches decoded visual stimuli from brain activity by extracting image features (e.g., multi-scale local bases \cite{11-3-08}, Gabor filters \cite{11-2}) and training linear mappings from fMRI signals to these features, demonstrating feasibility. However, fMRI’s low signal-to-noise ratio and limited datasets made natural scene reconstruction infeasible with linear methods.

Deep neural networks (DNNs) later enabled modeling complex brain-visual relationships through feature learning \cite{11-8}. Diverse DNN frameworks improved reconstruction, including deep belief networks \cite{10-4}, VAEs \cite{10-5-effenet}, feedforward architectures \cite{10-6,10-7}, GANs \cite{10-8,4-9}, and hybrid VAE/GANs \cite{7-12}, but often lacked semantic fidelity. Integrating CLIP \cite{uni23-clip} addressed this, Lin et al. \cite{2-7} aligned fMRI patterns with CLIP’s latent space via contrastive-adversarial learning, leveraging StyleGAN2 for semantically enhanced generation.

Recently, diffusion models has shown its ability to create high-resolution images with strong semantic consistency, leading to its successful use in various generative tasks\cite{3-27,3-28,3-29,3-7}. Takagi et al.\cite{9-1} was the first to map fMRI signals to diffusion latent space and CLIP text embeddings, generating images without training or fine-tuning deep networks, but the reconstructed images lacked sufficient semantic information and natural qualities. Later, MindEye\cite{12-4}, which optimized the semantic representations of fMRI features through contrastive learning, and MindDiffuser\cite{14-6}, which devised a two-stage diffusion process, addressed this issue. Furthermore, MindBridge\cite{13-5} introduces a cross-subject alignment strategy that enables the decoder to generalize across brain activity data from multiple individuals. 

Despite these advancements, existing methods still have limited capability in reconstructing visual stimuli of highly complex scenes, struggling to accurately capture their low-level structural features and high-level semantic information. This limitation hinders the generation of images that simultaneously achieve structural fidelity and semantic accuracy.",3405
http://arxiv.org/abs/2510.03952v1,2510.03952v1,2025-10-04T21:37:14+00:00,"Strategy Logic, Imperfect Information, and Hyperproperties","Strategy logic (SL) is a powerful temporal logic that enables first-class
reasoning over strategic behavior in multi-agent systems (MAS). In many MASs,
the agents (and their strategies) cannot observe the global state of the
system, leading to many extensions of SL centered around imperfect information,
such as strategy logic with imperfect information (SL$_\mathit{ii}$). Along
orthogonal lines, researchers have studied the combination of strategic
behavior and hyperproperties. Hyperproperties are system properties that relate
multiple executions in a system and commonly arise when specifying security
policies. Hyper Strategy Logic (HyperSL) is a temporal logic that combines
quantification over strategies with the ability to express hyperproperties on
the executions of different strategy profiles. In this paper, we study the
relation between SL$_\mathit{ii}$ and HyperSL. Our main result is that both
logics (restricted to formulas where no state formulas are nested within path
formulas) are equivalent in the sense that we can encode SL$_\mathit{ii}$
instances into HyperSL instances and vice versa. For the former direction, we
build on the well-known observation that imperfect information is a
hyperproperty. For the latter direction, we construct a self-composition of
MASs and show how we can simulate hyperproperties using imperfect information.","There exist many extensions of ATL∗and SL to reason
about imperfect information (Belardinelli, Lomuscio, and
Malvone 2019; Dima and Tiplea 2011; Jamroga, Malvone,
and Murano 2019; Knight and Maubert 2019; Belardinelli et
al. 2017; Berthon, Maubert, and Murano 2017; Huang and
van der Meyden 2014; Huang and van der Meyden 2018)
or hyperproperties (Beutner and Finkbeiner 2021; Beut-
ner and Finkbeiner 2023; Beutner and Finkbeiner 2024b;
Beutner and Finkbeiner 2024a). The connection between
knowledge and hyperproperties has been studied extensively
(Rabe 2016; Coenen et al. 2020; Beutner et al. 2023; Beutner
and Finkbeiner 2025a; Beutner and Finkbeiner 2025b). The
work most closely related to ours is the study of (Bozzelli,
Maubert, and Pinchinat 2015), who show that LTL Kand a
fragment of HyperCTL* are equally expressive (on standard
Kripke structures). We study the relation of hyperproperties
and knowledge in the setting of MASs and strategies. For
example, unlike LTL K, SLiidoes not use a knowledge op-
erator that we can directly encode. Instead, SL iiquantifies
over strategies with a given observation relation.",1128
http://arxiv.org/abs/2509.18800v1,2509.18800v1,2025-09-23T08:45:07+00:00,Security Evaluation of Android apps in budget African Mobile Devices,"Android's open-source nature facilitates widespread smartphone accessibility,
particularly in price-sensitive markets. System and vendor applications that
come pre-installed on budget Android devices frequently operate with elevated
privileges, yet they receive limited independent examination. To address this
gap, we developed a framework that extracts APKs from physical devices and
applies static analysis to identify privacy and security issues in embedded
software. Our study examined 1,544 APKs collected from seven African
smartphones. The analysis revealed that 145 applications (9%) disclose
sensitive data, 249 (16%) expose critical components without sufficient
safeguards, and many present additional risks: 226 execute privileged or
dangerous commands, 79 interact with SMS messages (read, send, or delete), and
33 perform silent installation operations. We also uncovered a vendor-supplied
package that appears to transmit device identifiers and location details to an
external third party. These results demonstrate that pre-installed applications
on widely distributed low-cost devices represent a significant and
underexplored threat to user security and privacy.","Zheng et al. [46] presented a tool named DroidRay that extracts statically and dynamically pre-installed apps from
250 firmware downloaded from forums and website. The static extraction consists of extracting pre-installed apps
directly from the firmware images. As for the dynamic extraction, it consists of flashing the image into a device and
then extracting the pre-installed apps using ADB commands. DroidRay performs pre-installed app analysis and system
analysis. For the pre-installed app analysis, they extracted the “SharedUserId” attribute from the AndroidManifest.xml
and the signature information from the RSA file. Then, they compare them with the default signatures they found from
the AOSP. They also analyzed the apps in VirusTotal do detect potential malware, and applied a filter to retain apps
having dangerous permissions or silent installation behavior. For the system analysis, DroidRay performs static and
dynamic analysis of the Android firmware by doing a system signature vulnerability detection, a network security
analysis, and a privilege escalation vulnerability detection. In our proposed pipeline, we leveraged the technique of
dynamic extraction to directly extract pre-installed apps from a physical device using ADB commands rather than
collecting firmware and flashing them into a device. We also used the same technique concerning malware detection,
consisting of analysing APK files in VirusTotal.
Mitchell et al. [36] designed DexDiff, a system for assessing the security impacts of vendor customization to the official
Android system. DexDiff helps the security analyst, who first retrieves the pre-installed apps and libraries from the
phone and then builds their corresponding base binaries from the release branch in AOSP on which the phone is based.
DexDiff compares each pair of these binaries obtained and evaluates the security impacts of individual modifications.
The bit and only similarity that this study has compared to our approach is that it extracts pre-installed apps from the
phone. However, the proposed tool did not automate this process. The apps are supposed to be extracted before using
DexDiff. Contrary to our approach, PiPLAnD automated the process of extraction and analysis.
Elsabagh et al. [15] proposed a static analysis tool named FIRMSCOPE, to identify unwanted functionality in pre-
installed apps by analyzing the Android firmware. FIRMSCOPE extracts pre-installed apps from the Android firmware
and then performs a taint analysis with context-sensitive, flow-sensitive, field-sensitive, and partially object-sensitive.
Specifically, it focuses exclusively on identifying the increase in privileges.
Gamba et al. [19] presented a large-scale study of Android pre-installed using crowd-sourcing methods. In this study,
the authors built an Android app, Firmware Scanner, that looks for and extracts pre-installed apps when installed on a
device. This study performs permission analysis using Androguard [14], static analysis leveraging existing tools such
as Androwarn, FlowDroid [3], and Amandroid [45], as well as apktool [2] and Androguard frameworks to identify
unwanted behaviors, and traffic analysis using the crowd-sourced Lumen mobile traffic dataset to see app real-world
behaviors. Compared to this work, we did the same by extracting pre-installed apps from the physical device but using
ADB commands rather than an installed app, and we also performed a taint analysis using FlowDroid.
Blázquez et al. [7] proposed FOTA (Firmware-Over-The-Air) Finder to automatically classify a given APK as FOTA
or not based on Androguard, using the dataset of pre-installed apps from Firmware Scanner [19]. Then, they performed
behavior analysis relying on FlowDroid and Amandroid for a taint analysis and a modification of Androwarn [13] to
analyze the use of API calls. This part of the work is a bit similar to our data leak detection using FlowDroid. However,
we considered all the pre-installed apps extracted from devices, and performed also malware detection.
Hou et al. [26] performed a study in which they collected firmware images from vendors, official websites and open
source repositories, and CVE data to link them with pre-installed apps. In this study, they proposed a tool named And-
Scanner that automates the extraction of pre-installed apps from firmware images before analyzing them. AndScanner
proposes an analysis of the security patches of the firmware to know if it has been patched in time and if the security
issues have been fixed. It also performs app analysis by analyzing the pre-installed apps using Androguard to identify
misconfiguration in the manifest file and CryptoGuard [11] to detect cryptography misuses. Our study is completely
different since we did not focus on analyzing the manifest files and cryptography misuses. However, we leveraged on
Androguard framework in our pipeline.
More recently, Sutter and Tellenbach [40] proposed FirmwareDroid, an automated static analysis tool for pre-installed
apps. This tool automates the process of extracting pre-installed apps from firmware images and their analysis using
existing tools including Androgurad and Exodus. The study identifies the advertising tracker libraries used with
Exodus and the permissions pre-installed apps inherited with Androguard. The authors have integrated 8 open source
static analysis tools in FirmwareDroid which could be used for more analysis.
Almost all of these studies are a bit similar since they follow almost the same approach, such as collecting firmware
from the Internet, extracting the pre-installed apps, and analyzing them. Our approach allows the extraction of pre-
installed apps from physical devices, the detection of malware and dangerous permissions, the detection of data leak-
age, and the use of malicious URLs in pre-installed apps. Furthermore, our approach is particularly tested on low-cost
devices sold in Africa. However, with this approach, every brand and every Android device can be inspected and
analyzed. Consequently, we do not have a limitation related to missing some brands or firmware.",6115
http://arxiv.org/abs/2510.09175v1,2510.09175v1,2025-10-10T09:17:21+00:00,Beyond Pairwise Connections: Extracting High-Order Functional Brain Network Structures under Global Constraints,"Functional brain network (FBN) modeling often relies on local pairwise
interactions, whose limitation in capturing high-order dependencies is
theoretically analyzed in this paper. Meanwhile, the computational burden and
heuristic nature of current hypergraph modeling approaches hinder end-to-end
learning of FBN structures directly from data distributions. To address this,
we propose to extract high-order FBN structures under global constraints, and
implement this as a Global Constraints oriented Multi-resolution (GCM) FBN
structure learning framework. It incorporates 4 types of global constraint
(signal synchronization, subject identity, expected edge numbers, and data
labels) to enable learning FBN structures for 4 distinct levels
(sample/subject/group/project) of modeling resolution. Experimental results
demonstrate that GCM achieves up to a 30.6% improvement in relative accuracy
and a 96.3% reduction in computational time across 5 datasets and 2 task
settings, compared to 9 baselines and 10 state-of-the-art methods. Extensive
experiments validate the contributions of individual components and highlight
the interpretability of GCM. This work offers a novel perspective on FBN
structure learning and provides a foundation for interdisciplinary applications
in cognitive neuroscience. Code is publicly available on
https://github.com/lzhan94swu/GCM.","Because the idea of high-order FBN structure is addressed in hyper network modeling, and the design
ofGCMis inspired by the ideas of Brain Graph Interpretation (BGI) [ 39] and Graph Structure
Learning (GSL) [40], we briefly introduce the related works along these aspects in this section.
Hyper Network ModelingResearch on FBNs aims to model and understand the cooperative
functions of the brain and has prospered with the development of complex network theory [ 6].
Pairwise-based network modeling is a relatively mature field and has been approached from numerous
perspectives utilizing diverse tools both in network science and machine learning [ 9,41,42,43,44].
Comparatively, hypergraph inference is still in its early stages, but the field is evolving rapidly [ 22,
45,46]. Recent efforts span probabilistic modeling grounded in historical connectivity [ 20,47] or
contagion traces [ 48], as well as optimization-based formulations adapted to MTS [ 24]. Despite
these advances, their computational overhead presents a major bottleneck for scaling to real-world
datasets [ 23,24]. More recently, a pioneering work proposes an end-to-end model for learning a fixed
number of hyperedges from individual samples [ 49]. In contrast, our proposedGCMframework is
explicitly designed to learn a single, generalizable FBN that represents an entire cohort (e.g., at the
subject or group resolution), a distinct goal focused on cross-sample interpretability.
Brain Graph InterpretationBrain Graph Interpretation (BGI) methods aim to identify and select
important edges in FBNs that contribute most to predictive outcomes [ 50]. These approaches
typically construct FBNs based on pairwise interactions and learn gradient-updated masks to filter
structures. Some methods refine the masks dynamically during training [ 51,52], others generate
predictive subgraphs to facilitate interpretation [ 39,53,54], integrate multiple modalities for cross-
3
validation [ 52,55,56], or employ attention mechanisms as trainable criteria for edge selection [ 57,
58,59]. These methods rely on pairwise-constructed FBNs and apply post-hoc explanations without
structural optimization. Consequently, their explanations are often resolution-agnostic, lacking
explicit differentiation across semantic levels. In contrast, our framework directly models functional
structures under global constraints at multiple modeling resolutions.
Graph Structure LearningGraph Structure Learning (GSL) treats input network structures as
noisy or incomplete and aims to refine them based on node features and initial graphs [ 40]. Traditional
GSL research focuses on defending against adversarial attacks [ 60,61], later evolving toward task-
specific structure optimization [ 62], mostly for node-level tasks on single graphs [ 63,64,65,66,
67,68]. Recent benchmarks [ 69] highlight that only a few models, such as VIB-GSL [ 70] and
HGP-SL [ 71], address structure learning across multiple graphs. In brain network modeling, Zong
et al. [ 8] introduced GSL to FBN learning by aligning brain regions using a diffusion model and
constructing structures based on feature correlations.
Unlike prior methods,GCMsupports end-to-end extraction of high-order FBN structures by explicitly
encoding modeling resolutions and global constraints, aspects often neglected in existing approaches.",3343
http://arxiv.org/abs/2509.19096v2,2509.19096v2,2025-09-23T14:47:33+00:00,Investigating Traffic Accident Detection Using Multimodal Large Language Models,"Traffic safety remains a critical global concern, with timely and accurate
accident detection essential for hazard reduction and rapid emergency response.
Infrastructure-based vision sensors offer scalable and efficient solutions for
continuous real-time monitoring, facilitating automated detection of accidents
directly from captured images. This research investigates the zero-shot
capabilities of multimodal large language models (MLLMs) for detecting and
describing traffic accidents using images from infrastructure cameras, thus
minimizing reliance on extensive labeled datasets. Main contributions include:
(1) Evaluation of MLLMs using the simulated DeepAccident dataset from CARLA,
explicitly addressing the scarcity of diverse, realistic, infrastructure-based
accident data through controlled simulations; (2) Comparative performance
analysis between Gemini 1.5 and 2.0, Gemma 3 and Pixtral models in accident
identification and descriptive capabilities without prior fine-tuning; and (3)
Integration of advanced visual analytics, specifically YOLO for object
detection, Deep SORT for multi-object tracking, and Segment Anything (SAM) for
instance segmentation, into enhanced prompts to improve model accuracy and
explainability. Key numerical results show Pixtral as the top performer with an
F1-score of 0.71 and 83% recall, while Gemini models gained precision with
enhanced prompts (e.g., Gemini 1.5 rose to 90%) but suffered notable F1 and
recall losses. Gemma 3 offered the most balanced performance with minimal
metric fluctuation. These findings demonstrate the substantial potential of
integrating MLLMs with advanced visual analytics techniques, enhancing their
applicability in real-world automated traffic monitoring systems.","Vision-based accident detection methods can be categorized by their model architectures into several categories, including both frame-level and object-centric approaches \cite{fang2023vision}. Frame-level accident detection relies on extracting features from individual frames and requires annotating accident windows in traffic recordings. You and Han \cite{you2020traffic} propose a dataset for traffic understanding, where accident windows are detected by classifying feature embeddings derived from video frames using a 3D-CNN model.
In contrast, methods that emphasize object detection and tracking aim to identify accident participants by detecting and localizing objects in images and monitoring their movement over time. Various approaches have used the YOLO detector \cite{7780460} previously, among which are YOLOv4 \cite{ghahremannezhad2022real} and YOLOv5 \cite{xia2022research}. Karim et al. \cite{karim2024visual} employed YOLOv8 to  detect vehicles and accidents in real time from surveillance video. They integrated Deep-SORT \cite{wojke2017simple} to track vehicle movement across frames, supporting temporal analysis for accident detection. While 3D-CNN based classification and YOLO with Deep-SORT tracking effectively pinpoint accident participants, they sometimes fall short in capturing the broader context needed for a more thorough analysis of accident scenarios.


The growth of artificial intelligence has increased the demand for extensive datasets to train models for specific applications, including accident detection. Consequently, numerous datasets have been developed to support research in this area. Yao et al. \cite{yao2022dota} propose an unsupervised traffic anomaly detection framework that employs future object localization and a novel  spatial-temporal area under curve (STAUC) on the richly annotated DoTA dataset to effectively identify abnormal events in dynamic, egocentric driving videos.
However, since the DoTA dataset primarily comprises dashcam video recordings with highly dynamic perspectives, it may not be ideally suited for applications requiring fixed, infrastructure-based scenes for accident detection.


Lee et al. \cite{lee2017crash} introduced GTACrash, a synthetic dataset derived from GTA V that enables training CNN-based collision prediction algorithms, and demonstrated that synthetic data is a valuable alternative to scarce real-world accident data, although the approach suffered from limited accident diversity and realism.
DeepAccident \cite{wang2024deepaccident} is a large-scale, simulator-generated V2X dataset for autonomous driving, developed using the CARLA simulator \cite{dosovitskiy2017carla}. It offers a wide variety of collision scenarios, accompanied by comprehensive sensor and annotation data from 6 infrastructure cameras, making it a robust resource for accident prediction and accident detection research.


ConnectGPT \cite{tong2024connectgpt} introduces a pipeline that integrates GPT-4 with infrastructure cameras and V2X communication to automatically analyze traffic conditions and generate standardized C-ITS messages, thereby enhancing real-time incident detection and road safety. Authors demonstrated its potential through practical experiments on a small dataset, highlighting improved responsiveness and reduced manual intervention in managing traffic hazards. However, its reliance solely on GPT-4 without evaluating alternative multimodal LLMs may limit its broader applicability.

AccidentGPT \cite{wu2024accidentgpt} presented a framework that combines V2X environmental perception with GPT-based reasoning to enable real-time accident prediction, prevention, and post-accident analysis. By fusing multi-sensor data and using collaborative perception techniques, the system delivers proactive alerts and detailed analyses of accident causation, aiding traffic management and enforcement agencies in improving road safety. 
Although AccidentGPT provides solid scene understanding, its reliance on complex sensor fusion and modular integration may restrict its scalability and limit its practical use in infrastructure-based accident detection. Furthermore, Zhang et al. \cite{zhang2025language} introduced a framework that leverages MLLMs for structured and scalable traffic accident analysis through video classification and visual grounding, incorporating severity-based aggregation, multimodal prompts, and a tailored evaluation metric. Lohner et al. \cite{lohner2024enhancing} introduced the idea of enhancing MLLM inputs with scene graphs.",4548
http://arxiv.org/abs/2509.24443v1,2509.24443v1,2025-09-29T08:26:23+00:00,"A Systematic Review of Digital Twin-Driven Predictive Maintenance in Industrial Engineering: Taxonomy, Architectural Elements, and Future Research Directions","With the increasing complexity of industrial systems, there is a pressing
need for predictive maintenance to avoid costly downtime and disastrous
outcomes that could be life-threatening in certain domains. With the growing
popularity of the Internet of Things, Artificial Intelligence, machine
learning, and real-time big data analytics, there is a unique opportunity for
efficient predictive maintenance to forecast equipment failures for real-time
intervention and optimize maintenance actions, as traditional reactive and
preventive maintenance practices are often inadequate to meet the requirements
for the industry to provide quality-of-services of operations. Central to this
evolution is digital twin technology, an adaptive virtual replica that
continuously monitors and integrates sensor data to simulate and improve asset
performance. Despite remarkable progress in digital twin implementations, such
as considering DT in predictive maintenance for industrial engineering. This
paper aims to address this void. We perform a retrospective analysis of the
temporal evolution of the digital twin in predictive maintenance for industrial
engineering to capture the applications, middleware, and technological
requirements that led to the development of the digital twin from its inception
to the AI-enabled digital twin and its self-learning models. We provide a
layered architecture of the digital twin technology, as well as a taxonomy of
the technology-enabled industrial engineering applications systems, middleware,
and the used Artificial Intelligence algorithms. We provide insights into these
systems for the realization of a trustworthy and efficient smart digital-twin
industrial engineering ecosystem. We discuss future research directions in
digital twin for predictive maintenance in industrial engineering.","Several surveys in the literature \cite{18wang2023review, 19zhong2023overview, 20van2022predictive, 21falekas2021digital, ma2024state, 37chen2023advance, mayr2024digital, cimino2024enhancing} emphasize the evolution of DT concepts and their integration with PdM. However, those works do not lay out the architectural elements of DT-PdM, which are necessary for the realization of a Quality-of-Services (QoS), real-time, AI-driven, and secure DT-PdM in a distributed IIoT, edge, and cloud computing integrated system. In addition, most of these studies do not provide a systematic methodology for collecting, including, excluding, and analyzing the relevant papers under study. Table~\ref{tab:table1} summarizes the comparisons across these works, in terms of the requirements considered (e.g., real-time performance, QoS, scalability, security, AI, and energy), search strategies, and the targeted application domains. Unlike earlier reviews, our review paper not only adheres to a systematic methodology but also extends DT-PdM applications beyond traditional settings, enriching it with AI and distributed architectural elements necessary for its efficient realization. We integrate these findings by addressing a
broader spectrum of industrial domains—including marine, automobile, energy, hydro, aviation, railway, and manufacturing industries—and establishes a unified framework for implementing DT-based PdM solutions. Furthermore, our work further integrates its findings by addressing a broader spectrum of industrial domains—including marine, automobile, energy, hydro, aviation, railway, and manufacturing industries—and establishes a unified framework for implementing DT-based PdM solutions. We categorize the related reviews in the literature based on the application domains they target: aviation and aerospace. manufacturing and process equipment, and other specialized domains such as electrical machine \cite{21falekas2021digital}, sheet metal binding \cite{mayr2024digital}, internal supply chain management \cite{cimino2024enhancing}.


Regarding, Aviation and Aerospace, Wang et al. \cite{18wang2023review} provide an in-depth review of DT development specifically for aircraft maintenance, repair, and overhaul (MRO) activities, exploring both data-driven digital twin (DDDT) and model-based digital twin (MBDT) for enhanced aircraft system management. Ma et al. \cite{ma2024state} propose a systematic approach aimed at standardizing PdM automation in aviation by explicitly defining informational and functional requirements. Furthermore, Chen et al. \cite{37chen2023advance} showcase a multi-plant production planning platform that leverages simulation-based DT capabilities in aerospace contexts, emphasizing the importance of standardized data frameworks and cloud-edge collaboration.

Regarding Manufacturing and Process equipment, Zhong et al. \cite{19zhong2023overview} critique the limitations of traditional maintenance approaches, advocating for DT-based PdM that incorporates material properties, operating conditions, and degradation laws to enable improved fault prediction and more accurate estimation of remaining useful life. Complementing this perspective, Van Dinter et al. \cite{20van2022predictive} perform a systematic review that categorizes the existing literature along various dimensions such as DT platform implementation, communication protocols, and complexity challenges across manufacturing, energy, aerospace, and marine PdM applications.

In Specialized domains of electrical machines, Falekas and Karlis \cite{21falekas2021digital} focuses on predictive maintenance for electrical machines, highlighting the predominant use of AI techniques and categorizing digital replicas into Digital Model, Digital Shadow, and Digital Twin. Mayr et al. \cite{mayr2024digital} addresses the challenges in sheet metal bending applications by proposing a hybrid approach that integrates both physics-based and data-driven methods. This method uniquely combines DT predictions with production data analysis to derive process parameters that are otherwise difficult to measure directly. Cimino et al. \cite{cimino2024enhancing} introduces a simulation-based DT platform to enhance internal supply chain management in the oil and gas sector. Their modular architecture supports dynamic what-if analyses, resulting in notable improvements in system flow time and tardiness reduction.

\begin{table*}[!htb]
\centering
\caption{Comparison of existing surveys on Digital Twin for Predictive Maintenance}
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{p{0.8cm}|p{0.4cm}|p{0.2cm}p{0.2cm}p{0.2cm}p{0.2cm}p{0.2cm}p{0.2cm}|p{3cm}|p{0.8cm}|p{1.2cm}|p{1.5cm}|p{3.2cm}}
\toprule
\textbf{Work} &  & \multicolumn{6}{c|}{\textbf{Requirements Considered}} & \textbf{Search Databases} & \textbf{Search String} & \textbf{Inclusion \& Exclusion Criteria} & \textbf{Challenges Considered} & \textbf{Application Domain} \\
 & \rotatebox{90}{\textbf{Architectural Elements}} & \rotatebox{90}{\textbf{Real-Time}} & \rotatebox{90}{\textbf{QoS}} & \rotatebox{90}{\textbf{Scalability}} & \rotatebox{90}{\textbf{Security}} & \rotatebox{90}{\textbf{AI}} & \rotatebox{90}{\textbf{Energy}}&  &  &  & &  \\ 
\midrule
\cite{18wang2023review} & $\times$ & \checkmark & \checkmark & $\times$ & $\times$ & \checkmark & $\times$ & NR & NR & NR & $\times$ & Aircraft Vehicular Predictive Maintenance Systems \\ 
\midrule
\cite{19zhong2023overview} & $\times$ & $\times$ & \checkmark & \checkmark & $\times$ & \checkmark & $\times$ & NR & NR & NR & $\times$ & Process Equipment Manufacturing, Automobile Manufacturing, Cyber-Physical Systems \\ 
\midrule
\cite{20van2022predictive} & $\times$ & $\times$ & \checkmark & \checkmark & $\times$ & \checkmark & \checkmark & Springer, Scopus, IEEE Xplore, ScienceDirect, ACM, Wiley, Taylor \& Francis & \checkmark & NR & $\times$ & Manufacturing, Energy, Aerospace, Marine PdMs \\ 
\midrule
\cite{21falekas2021digital} & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ & \checkmark & $\times$ & NR & NR & NR & \checkmark & Electrical machine control and Maintenance \\ 
\midrule
\cite{ma2024state} & $\times$ & \checkmark & \checkmark & \checkmark & $\times$ & $\times$ & $\times$ & IEEE Xplore, Science-Direct, Springer Link, ACM, Taylor \& Francis & \checkmark & \checkmark & $\times$ & Aviation, Manufacturing, HVAC Systems, Building Management\\
\midrule
\cite{37chen2023advance} & $\times$ & \checkmark & \checkmark & \checkmark & $\times$ & \checkmark & \checkmark & IEEE Xplore, Science-Direct, Springer Link, Scopus, Google Scholar, Taylor \& Francis & \checkmark & \checkmark & $\times$ & Manufacturing Systems (CNC, Bearings, Gearboxes), Aerospace Industry \\
\midrule
\cite{mayr2024digital} & $\times$ & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ & Springer, Scopus, IEEE Xplore, ScienceDirect, ACM, Wiley, Taylor \& Francis & \checkmark & NR & \checkmark & Sheet Metal Bending (Salvagnini panel benders) \\ 
\midrule
\cite{cimino2024enhancing} & $\times$ & $\times$ & \checkmark & \checkmark & $\times$ & \checkmark & $\times$ & Springer, Scopus, IEEE Xplore, ScienceDirect, ACM, Wiley, Taylor \& Francis & \checkmark & NR & $\times$ & Oil \& Gas Manufacturing Sector (Internal Supply Chain Management) \\ 
\midrule
\textbf{Ours} & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & Springer, Scopus, IEEE Xplore, ScienceDirect, ACM, Wiley, Taylor \& Francis & \checkmark & \checkmark & \checkmark & Marine Industry, Automobile Industry, Energy Industry, Hydro Industry, Aviation Industry, Railway Industry, Manufacturing Industry \\
\bottomrule
\end{tabular}
\\[1ex]
\footnotesize{\textbf{Note:} NR - Not Reported, $\times$ - Not considered, \checkmark\ - Considered.}
\end{table*}",7844
http://arxiv.org/abs/2510.00881v1,2510.00881v1,2025-10-01T13:28:26+00:00,Advancing Automated Ethical Profiling in SE: a Zero-Shot Evaluation of LLM Reasoning,"Large Language Models (LLMs) are increasingly integrated into software
engineering (SE) tools for tasks that extend beyond code synthesis, including
judgment under uncertainty and reasoning in ethically significant contexts. We
present a fully automated framework for assessing ethical reasoning
capabilities across 16 LLMs in a zero-shot setting, using 30 real-world
ethically charged scenarios. Each model is prompted to identify the most
applicable ethical theory to an action, assess its moral acceptability, and
explain the reasoning behind their choice. Responses are compared against
expert ethicists' choices using inter-model agreement metrics. Our results show
that LLMs achieve an average Theory Consistency Rate (TCR) of 73.3% and Binary
Agreement Rate (BAR) on moral acceptability of 86.7%, with interpretable
divergences concentrated in ethically ambiguous cases. A qualitative analysis
of free-text explanations reveals strong conceptual convergence across models
despite surface-level lexical diversity. These findings support the potential
viability of LLMs as ethical inference engines within SE pipelines, enabling
scalable, auditable, and adaptive integration of user-aligned ethical
reasoning. Our focus is the Ethical Interpreter component of a broader
profiling pipeline: we evaluate whether current LLMs exhibit sufficient
interpretive stability and theory-consistent reasoning to support automated
profiling.","\noindent\emph{\major{Ethics in autonomous and software-intensive systems.}}
The integration of ethical considerations into autonomous systems has been widely examined in software engineering. Prior work spans design-time approaches that encode ethics via codes of conduct, principles, and rules~\cite{suri2023software,alidoosti2021ethics,alidoosti2022incorporating,DBLP:journals/access/ShahinHNPSGW22,trailer2024ciniselli}, and verification-time approaches that formalize and check system decisions against ethical frameworks~\cite{jedlickova2024ensuring,dennis2016formal,cardoso2021implementing,inverardi2019ethics,de2024engineering,inverardi2022ethical,Machine_Ethics_in_Changing_Contexts:2021,karim2017ethical}. This body of research establishes both the need and the mechanisms for ensuring ethically compliant behavior in autonomous decision pipelines.


\noindent\emph{\major{LLMs in software engineering and the need for ethical reasoning.}}
Concurrently, LLMs have been adopted across SE tasks such as code generation, bug detection, and documentation~\cite{hou2024large}. As these models are integrated into SE workflows, it becomes important to assess whether they exhibit ethical reasoning and how they might be leveraged in systems with ethical implications~\cite{DBLP:conf/emnlp/RaoKTAC23,han2022aligning}.


\noindent\emph{\major{Model-centric evaluations of moral reasoning.}}
Han et al.~\cite{han2022aligning} evaluate LLM understanding of moral/ethical reasoning across five domains (justice, deontology, virtue ethics, utilitarianism, and commonsense morality). They fine-tune BERT-base, BERT-large, RoBERTa-large, and ALBERT-xxlarge on ETHICS datasets comprising over 13{,}000 scenarios, and then test the models’ ability to classify scenarios in line with the ethical theories used during fine-tuning. In contrast, the present work does not rely on fine-tuning; it evaluates whether pre-trained LLMs, in a zero-shot setup, display ethical reasoning.


\noindent\emph{\major{Value identification in scholarly texts vs. ethically charged situations.}}
A complementary thread leverages LLMs to extract values and structure from scientific writing. For example,~\cite{ICSE2025PAPER} studies ChatGPT’s ability to identify human values from titles and abstracts of SE publications using Schwartz’s theory~\cite{schwartz2012overview}, followed by manual verification by humans. Related efforts show that LLMs can assist in extracting insights from scholarly texts, classifying publications, and mining metadata for literature reviews~\cite{hou2024large,alshami2023harnessing}. These studies focus on value detection in academic prose rather than on evaluating ethical concrete reasoning, ethically charged scenarios.


\noindent\emph{\major{Reasoning with policies and learning moral rewards.}}
Rao et al.~\cite{DBLP:conf/emnlp/RaoKTAC23} argue against hard-coding specific moral values in LLMs and advocate for general ethical reasoning capacities that adapt to diverse contexts. They introduce in-context ethical policies defined at varying abstraction levels and grounded in deontology, virtue ethics, and consequentialism, reporting experiments across GPT-3, ChatGPT, and GPT-4 with GPT-4 showing stronger ethical reasoning. Tennant et al.~\cite{DBLP:conf/iclr/TennantHM25} incorporate intrinsic moral rewards, grounded in deontological and utilitarian theories, into reinforcement learning fine-tuning. Using the Iterated Prisoner’s Dilemma, they demonstrate that LLM agents can learn morally aligned strategies and even unlearn previously selfish behaviors. While these works shape model behavior through policies or moral rewards, the present study compares outputs across multiple models without additional tuning, treating ethical reasoning as a testing ground rather than a direct optimization target.


\noindent\emph{\major{Positioning of the present study.}}
Prior research establishes design/verification-time mechanisms for ethics in autonomous systems, documents LLM utility in SE, and explores both fine-tuned moral reasoning and in-context ethical policy use. The contribution here is orthogonal: a zero-shot assessment of pre-trained LLMs’ ethical reasoning on ethically charged scenarios, contrasting with fine-tuned or policy-conditioned settings, and distinct from value-mining in academic text.



\begin{comment}",4350
http://arxiv.org/abs/2510.09903v1,2510.09903v1,2025-10-10T22:27:13+00:00,An uncertainty-aware framework for data-efficient multi-view animal pose estimation,"Multi-view pose estimation is essential for quantifying animal behavior in
scientific research, yet current methods struggle to achieve accurate tracking
with limited labeled data and suffer from poor uncertainty estimates. We
address these challenges with a comprehensive framework combining novel
training and post-processing techniques, and a model distillation procedure
that leverages the strengths of these techniques to produce a more efficient
and effective pose estimator. Our multi-view transformer (MVT) utilizes
pretrained backbones and enables simultaneous processing of information across
all views, while a novel patch masking scheme learns robust cross-view
correspondences without camera calibration. For calibrated setups, we
incorporate geometric consistency through 3D augmentation and a triangulation
loss. We extend the existing Ensemble Kalman Smoother (EKS) post-processor to
the nonlinear case and enhance uncertainty quantification via a variance
inflation technique. Finally, to leverage the scaling properties of the MVT, we
design a distillation procedure that exploits improved EKS predictions and
uncertainty estimates to generate high-quality pseudo-labels, thereby reducing
dependence on manual labels. Our framework components consistently outperform
existing methods across three diverse animal species (flies, mice, chickadees),
with each component contributing complementary benefits. The result is a
practical, uncertainty-aware system for reliable pose estimation that enables
downstream behavioral analyses under real-world data constraints.","spoke architectural elements. While these architectures may provide good performance with enough
training data, they do not allow us to easily exploit general pretrained backbones that are useful when
training models with a small number of labels. Furthermore, algorithmic simplicity is desirable for
our application domain, where users are often experimental labs with little experience maintaining
and debugging exotic architectures. Here we propose a simple strategy that allows the model to take
advantage of multiple views and is also compatible with generic VIT backbones: rather than pro-
cess pixel patches from each view independently, we process all patches simultaneously, allowing
the standard self-attention mechanism to pool information within and across views.
The standard image VIT (Dosovitskiy et al., 2020) data pipeline (Fig. 1,top) starts with a 2D image
x∈RH×W×C(whereH,W,Care height, width, channels) and splits it into 2D patches, each
with shape(P×P×C), where the patch sizePis typically",1012
http://arxiv.org/abs/2510.05805v1,2510.05805v1,2025-10-07T11:22:27+00:00,Improving Clinical Dataset Condensation with Mode Connectivity-based Trajectory Surrogates,"Dataset condensation (DC) enables the creation of compact, privacy-preserving
synthetic datasets that can match the utility of real patient records,
supporting democratised access to highly regulated clinical data for developing
downstream clinical models. State-of-the-art DC methods supervise synthetic
data by aligning the training dynamics of models trained on real and those
trained on synthetic data, typically using full stochastic gradient descent
(SGD) trajectories as alignment targets; however, these trajectories are often
noisy, high-curvature, and storage-intensive, leading to unstable gradients,
slow convergence, and substantial memory overhead. We address these limitations
by replacing full SGD trajectories with smooth, low-loss parametric surrogates,
specifically quadratic B\'ezier curves that connect the initial and final model
states from real training trajectories. These mode-connected paths provide
noise-free, low-curvature supervision signals that stabilise gradients,
accelerate convergence, and eliminate the need for dense trajectory storage. We
theoretically justify B\'ezier-mode connections as effective surrogates for SGD
paths and empirically show that the proposed method outperforms
state-of-the-art condensation approaches across five clinical datasets,
yielding condensed datasets that enable clinically effective model development.","Dataset Condensation:Dataset Distillation (Wang
et al., 2018) pioneered synthetic dataset synthesis
through nested bilevel optimisation, addressing the
computational burden of training on massive real-world
datasets by creating dramatically smaller yet equally
effective alternatives. Gradient matching approaches
(Zhao and Bilen, 2021) improved computational ef-
ficiency by avoiding expensive inner-loop unrolling,
while soft-label extensions (Sucholutsky and Schonlau,
2019; Bohdal et al., 2020) explored label optimisation.
Kernel-based methods (Nguyen et al., 2021, 2022; Jacot
et al., 2018) reformulated the problem using neural tan-
gent kernel theory, offering closed-form solutions but
with heavy computational costs. Distribution match-
ing methods (Zhao and Bilen, 2023; Wang et al., 2022;
Zhao et al., 2023; Zhang et al., 2024) achieved efficiency
gains by aligning feature statistics without bilevel opti-
misation, though often requiring larger synthetic sets to
achieve strong performance. Recent image-focussed de-
coupling approaches like Squeeze, Recover and Relabel
(Yin et al., 2023) and Realistic, Diverse, and Efficient
Dataset Distillation (Sun et al., 2023) separate syn-
thesis from training optimisation. While effective on
vision datasets, these methods transfer poorly to struc-
tured clinical data, where performance gaps remain
significant.
Trajectory Matching:TM narrows this gap by
supervising synthetic data with the training dynamics
of real data. Matching Training Trajectories (MTT)
(Cazenavette et al., 2022) first aligned long-range tra-
jectories, capturing richer information than stepwise
gradient matching. Extensions such as Flat Trajec-
tory Distillation (FTD) (Du et al., 2023), Difficulty-
Aligned Trajectory Matching (DATM) (Guo et al.,
2024), and TrajEctory matching with Soft Label As-
signment (TESLA) (Liu et al., 2023) introduce cur-
vature regularisation, difficulty-based curricula, and
scalable soft-label assignment respectively. Despite
strong results, TM methods depend on dense trajecto-
ries—tens to hundreds of checkpoints—and inherit the
noise and curvature of SGD paths, inflating storage
and introducing instability that limits clinical use.
Mode Connectivity:Mode connectivity studies the
geometry of neural loss landscapes by constructing
smooth, low-loss paths between trained models, most
commonly parameterised as quadratic B´ ezier curves
(Garipov et al., 2018; Draxler et al., 2018; Izmailov
et al., 2018). These paths preserve endpoint perfor-
mance while bypassing high-loss regions, and have been
used to analyse generalisation and reparameterisation.
In clinical machine learning, they have supported in-
cremental learning and mitigated distribution shift(Thakur et al., 2023). Requiring only two endpoints
and a single control point, mode connections provide
compact yet expressive representations of optimisation
paths.
Comparison With Proposed Approach:These
approaches present distinct trade-offs: non-TM meth-
ods are efficient but struggle with structured clinical
data; TM captures richer dynamics but suffers from
SGD instability and heavy storage requirements; mode
connectivity provides compact, smooth paths but re-
mains unexplored for DC. This work bridges these
limitations by replacing noisy SGD trajectories with
smooth mode-connected surrogates, combining TM’s
supervisory power with mode connectivity’s efficiency
for effective clinical data synthesis.",3446
http://arxiv.org/abs/2510.04220v1,2510.04220v1,2025-10-05T14:23:51+00:00,MASC: Boosting Autoregressive Image Generation with a Manifold-Aligned Semantic Clustering,"Autoregressive (AR) models have shown great promise in image generation, yet
they face a fundamental inefficiency stemming from their core component: a
vast, unstructured vocabulary of visual tokens. This conventional approach
treats tokens as a flat vocabulary, disregarding the intrinsic structure of the
token embedding space where proximity often correlates with semantic
similarity. This oversight results in a highly complex prediction task, which
hinders training efficiency and limits final generation quality. To resolve
this, we propose Manifold-Aligned Semantic Clustering (MASC), a principled
framework that constructs a hierarchical semantic tree directly from the
codebook's intrinsic structure. MASC employs a novel geometry-aware distance
metric and a density-driven agglomerative construction to model the underlying
manifold of the token embeddings. By transforming the flat, high-dimensional
prediction task into a structured, hierarchical one, MASC introduces a
beneficial inductive bias that significantly simplifies the learning problem
for the AR model. MASC is designed as a plug-and-play module, and our extensive
experiments validate its effectiveness: it accelerates training by up to 57%
and significantly improves generation quality, reducing the FID of LlamaGen-XL
from 2.87 to 2.58. MASC elevates existing AR frameworks to be highly
competitive with state-of-the-art methods, establishing that structuring the
prediction space is as crucial as architectural innovation for scalable
generative modeling.","In summary, the choice of average-linkage is a deliberate design decision to create a clustering
hierarchy that is robust, deterministic, and best reflects the underlying semantic structure of the
codebook manifold by balancing compactness and shape-invariance.
15
Figure 5: A conceptual illustration of linkage criteria. (a) Single-linkage may incorrectly merge two
distinct semantic groups if they are connected by a bridge of a few close points. (b) Average-linkage,
as used in MASC, considers the overall distribution of points and is more robust, correctly identifying
distinct clusters.
Algorithm 1Manifold-Aligned Semantic Clustering (MASC) Construction
Require:Codebook embeddingsZ={v 1, . . . , v N} ∈RN×d, target number of clustersk.
Ensure:A mappingM:{1, . . . , N} → {1, . . . , k}from token indices to cluster indices.
1:▷Initialization
2:InitializeNactive clusters,C j← {v j}, and their sizes,|C j| ←1, forj= 1, . . . , N.
3:Pre-compute the fullN×Npairwise Euclidean distance matrixD, whereD st=∥v s−vt∥2.
4:Set diagonal elementsD ss← ∞to prevent self-merging.
5:▷Bottom-Up Hierarchical Construction
6:fori←1toN−kdo
7:Find the pair of active clusters with the minimum distance:(s∗, t∗)←arg mins,tDst.
8:▷Update distance matrix efficiently using a weighted average
9:foreach remaining active clusterC uwhereu̸=s∗, t∗do
10:D s∗,u←|Cs∗|Ds∗,u+|Ct∗|Dt∗,u
|Cs∗|+|C t∗|;D u,s∗←D s∗,u
11:end for
12:▷Update cluster size and deactivate the merged clusterC t∗
13:|C s∗| ← |C s∗|+|C t∗|.
14:Set row and columnt∗ofDto∞.
15:Keep track that all original tokens from clusterC t∗now belong to clusterC s∗.
16:end for
17:▷Final Mapping Construction
18:The remainingkactive clusters form the final coarse vocabulary.
19:Assign a unique index from{1, . . . , k}to each of the finalkactive clusters.
20:Construct the mappingMby assigning each original tokenv ito its final cluster index.
21:returnThe mappingM.
C EXTENDEDEXPERIMENTALRESULTS ANDANALYSES
This section expands upon the experimental results presented in the main paper. We provide detailed
analyses, including qualitative visualizations of cluster coherence, ablation studies on key hyperpa-
rameters, and a deeper look into the effects of different decoding strategies. These results collectively
offer a comprehensive validation of the MASC framework.
C",2312
http://arxiv.org/abs/2509.19459v1,2509.19459v1,2025-09-23T18:14:21+00:00,Automated Insertion of Flushes and Fences for Persistency,"CXL shared memory and persistent memory allow the contents of memory to
persist beyond crashes. Stores to persistent or CXL memory are typically not
immediately made persistent; developers must manually flush the corresponding
cache lines to force the data to be written to the underlying storage.
Correctly using flush and fence operations is known to be challenging. While
state-of-the-art tools can find missing flush instructions, they often require
bug-revealing test cases. No existing tools can ensure the absence of missing
flush bugs.
  In this paper, we present PMRobust, a compiler that automatically inserts
flush and fence operations to ensure that code using persistent memory is free
from missing flush and fence bugs. PMRobust employs a novel static analysis
with optimizations that target newly allocated objects. We have evaluated
PMRobust on persistent memory libraries and several persistent memory data
structures and measured a geometric mean overhead of 0.26% relative to the
original benchmarks with hand-placed flush and fence operations.","There is work on checking/testing PM programs to find bugs. In
particular, XFDetector~\cite{xfdetector} uses a finite state machine
to track the consistency and persistency of persistent
data. PMTest~\cite{pmtest} lets developers annotate a program with
checking rules to infer the persistency status of writes and ordering
constraints between writes. Pmemcheck~\cite{pmemcheck} checks how many
stores were not made persistent and detects memory overwrites using
binary rewriting. Yat~\cite{yat} model checks
PM programs. 
Agamotto~\cite{agamotto} finds bugs in PM programs by
using symbolic execution. An algorithm by Huang et al.~\cite{pminvariant} infers invariants from PM programs that are then used to check for bugs.
Although these tools are able to find many bugs, none of these tools
can assure the absence of flush/fence bugs like \tool can.  POG~\cite{pog} and
Pierogi~\cite{pierogi} provide logics that can be used to manually
reason about program behaviors.

A line of work~\cite{nv-htm,crafty-pldi20,persistent-htm-giles-2017,dudetm} uses (software or hardware) transactions to provide
(failure and thread) atomicity. NVL-C provides language and compiler
support for the use of transactions to access non-volatile
memory~\cite{nvlc}.  While NVL-C can provide crash consistency, it
does so by incurring the overheads of using transactions to provide
crash consistency.  Another line of
work~\cite{atlas-follow-up,atlas,nvthreads,justdo-logging,ido}
advocates use of locks or synchronization-free
regions~\cite{persistency-sfr}.  Memento~\cite{memento} provides
detectable checkpointing---it extends standard checkpoint with support
to allow the system to be able to detect the status of in flight
operations when the crash occurred.  These approaches typically incur
large overheads to support the necessary logging.

StaticPersist~\cite{staticpersist} is a static analysis to
determine which objects must be allocated in PM.  The
idea is to use annotations to declare a set of durable roots, and the
analysis determines which objects are reachable from the 
roots. AutoPersist~\cite{autopersist} is a Java extension to support
NVM.  Developers specify durable roots and when an
object becomes reachable from a durable root, AutoPersist moves it to
PM.  Both
StaticPersist and AutoPersist provide a higher-level API for
programming PM, while \tool targets a
lower-level model by automatically inserting flush and
fence operations.

Hippocrates~\cite{hippocrates-asplos21} and
PMBugAssist~\cite{pmbugassist} insert flushes
and fences to repair PM bugs.  Hippocrates chooses the placement of
flushes and fences using a reduction procedure, whereas PMBugAssist
uses a SMT solver. They both focus on
repairing specific PM bugs given as program input and rely on other PM
bug detection tools to produce bug traces, which is different from our
task of exhaustively detecting potential PM bugs and fixing them at
the same time.

Our work is related to a line of work on static fence
insertion for concurrent programs to ensure sequential consistency~\cite{musketeer2017, lee2000relaxed, fang2003automatic}. The static approach to fence insertion has so far not been applied to PM programs, a
gap which we bridge. In terms of techniques, previous work such as musketeer~\cite{musketeer2017} and pensieve~\cite{fang2003automatic} focused on variants of delay set analysis~\cite{delayset} combined with flow-insensitive escape analysis to minimise fence insertions, whereas we make use of a flow-sensitive escape analysis to avoid redundant fence insertions. 

FliT~\cite{flit} presents a technique that uses counters to
eliminate flush operations on atomic loads, which we applied in \tool
to further reduce the overhead it introduces.",3739
http://arxiv.org/abs/2510.08669v1,2510.08669v1,2025-10-09T17:22:23+00:00,FreqCa: Accelerating Diffusion Models via Frequency-Aware Caching,"The application of diffusion transformers is suffering from their significant
inference costs. Recently, feature caching has been proposed to solve this
problem by reusing features from previous timesteps, thereby skipping
computation in future timesteps. However, previous feature caching assumes that
features in adjacent timesteps are similar or continuous, which does not always
hold in all settings. To investigate this, this paper begins with an analysis
from the frequency domain, which reveal that different frequency bands in the
features of diffusion models exhibit different dynamics across timesteps.
Concretely, low-frequency components, which decide the structure of images,
exhibit higher similarity but poor continuity. In contrast, the high-frequency
bands, which decode the details of images, show significant continuity but poor
similarity. These interesting observations motivate us to propose
Frequency-aware Caching (FreqCa)
  which directly reuses features of low-frequency components based on their
similarity, while using a second-order Hermite interpolator to predict the
volatile high-frequency ones based on its continuity.
  Besides, we further propose to cache Cumulative Residual Feature (CRF)
instead of the features in all the layers, which reduces the memory footprint
of feature caching by 99%.
  Extensive experiments on FLUX.1-dev, FLUX.1-Kontext-dev, Qwen-Image, and
Qwen-Image-Edit demonstrate its effectiveness in both generation and editing.
Codes are available in the supplementary materials and will be released on
GitHub.","\vspace{-3mm}

Diffusion models have emerged as a cornerstone of modern generative AI, exhibiting state-of-the-art capabilities in synthesizing visual content~\citep{sohl2015deep,ho2020DDPM}. While early models were predominantly built upon U-Net architectures~\citep{ronneberger2015unet}, their scalability limitations paved the way for the Diffusion Transformer (DiT)~\citep{peebles2023dit}. The DiT architecture has since become foundational, catalyzing a wave of powerful models across diverse domains~\citep{opensora,yang2025cogvideox}. Nevertheless, the iterative nature of the diffusion sampling process imposes a significant computational burden during inference, making acceleration a critical area of research~\citep{ho2020DDPM,peebles2023dit}. Current efforts to enhance efficiency are largely focused on two complementary directions: reducing the number of sampling steps and accelerating the denoising network itself.

\vspace{-3mm}
\subsection{Sampling Timestep Reduction}
\vspace{-1mm}

One primary strategy seeks to minimize the number of required sampling iterations while preserving generation quality. Seminal work like DDIM introduced deterministic sampling to reduce step counts without significant fidelity loss~\citep{songDDIM}. This concept was further refined by the DPM-Solver series, which employed high-order ODE solvers to achieve faster convergence~\citep{lu2022dpm,lu2022dpm++,zheng2023dpmsolvervF}. Other notable approaches include knowledge distillation, which trains a student model to emulate multiple denoising steps of a larger teacher model~\citep{salimans2022progressive,meng2022on}, and Rectified Flow, which learns to straighten the generation path between noise and data distributions~\citep{refitiedflow}. More recently, Consistency Models have enabled high-quality synthesis in a single step by directly mapping noise to clean data, circumventing the need for a sequential path~\citep{song2023consistency}.

\vspace{-3mm}
\subsection{Denoising Network Acceleration}
\vspace{-1mm}

An alternative to reducing timesteps is to decrease the computational cost of each forward pass through the denoising network. This is typically achieved via model compression or feature caching.

\vspace{-2mm}
\paragraph{Model Compression-based Acceleration.} 
\vspace{-1mm}

One avenue involves model compression, which includes techniques such as network pruning~\citep{structural_pruning_diffusion, zhu2024dipgo}, quantization~\citep{10377259, shang2023post, kim2025ditto}, and various forms of token reduction that dynamically shorten the input sequence length~\citep{bolya2023tomesd, kim2024tofu, zhang2024tokenpruningcachingbetter, zhang2025sito}. While effective, these methods often necessitate a fine-tuning or retraining stage to mitigate the potential loss of expressive power inherent in model simplification~\citep{li2024snapfusion,10377259}.

\vspace{-2mm}
\paragraph{Feature Caching-based Acceleration.}
\vspace{-2mm}
A compelling training-free alternative is feature caching, which exploits temporal redundancies in the denoising process. Pioneered in U-Net architectures through FasterDiffusion and DeepCache, this paradigm was subsequently adapted to DiTs. Initial efforts focused on a ``cache then reuse'' strategy, while advanced techniques like FORA and $\Delta$-DiT refined this approach. This concept evolved with more sophisticated mechanisms, including dynamic token-level updates (ToCa), adaptive sampling (RAS~\citep{liu2025regionadaptivesamplingdiffusiontransformers}), and explicit error correction frameworks~\citep{qiu2025acceleratingdiffusiontransformererroroptimized, chenIncrementCalibrated2025, chuOmniCache2025}. A pivotal shift was the ``cache then forecast'' paradigm introduced by TaylorSeeer, which was further advanced by more robust numerical methods in FoCa~\citep{zhengFoCa2025}, HiCache~\citep{fengHiCache2025}, and SpeCa~\citep{Liu2025SpeCa}.

However, a crucial flaw underlies these sophisticated paradigms, as hinted at by preliminary frequency-domain analyses. For instance, PAB~\citep{zhao2024PAB} insightfully associated different attention mechanisms with distinct frequency bands but did not delve into token-level frequency dynamics. Similarly, while FasterCache~\citep{lvFasterCacheTrainingFreeVideo2025} examined the frequency-domain differences within Classifier-Free Guidance , its findings were confined to this specific context, not addressing the more universal dynamics of temporal feature evolution and thus showing limited practical acceleration.

In contrast to prior methods that treat features as a monolithic whole, we propose \textbf{FreqCa}, which resolves quality degradation in caching by decomposing features into their stable low-frequency and volatile high-frequency components for differentiated treatment. As an added benefit,  we introduce the Cumulative Residual Feature, collapsing the memory complexity from $\mathcal{O}(L)$ to $\mathcal{O}(1)$ to solve the resource inefficiency of prior ``layer-wise'' architectures.
\vspace{-4mm}",5039
http://arxiv.org/abs/2509.12441v1,2509.12441v1,2025-09-15T20:48:50+00:00,Automatic Network Planning with Digital Radio Twin,"Network planning seeks to determine base station parameters that maximize
coverage and capacity in cellular networks. However, achieving optimal planning
remains challenging due to the diversity of deployment scenarios and the
significant simulation-to-reality discrepancy. In this paper, we propose
\emph{AutoPlan}, a new automatic network planning framework by leveraging
digital radio twin (DRT) techniques. We derive the DRT by finetuning the
parameters of building materials to reduce the sim-to-real discrepancy based on
crowdsource real-world user data. Leveraging the DRT, we design a Bayesian
optimization based algorithm to optimize the deployment parameters of base
stations efficiently. Using the field measurement from Husker-Net, we
extensively evaluate \emph{AutoPlan} under various deployment scenarios, in
terms of both coverage and capacity. The evaluation results show that
\emph{AutoPlan} flexibly adapts to different scenarios and achieves performance
comparable to exhaustive search, while requiring less than 2\% of its
computation time.","Traditional approaches to BS deployment include both manual measurement and optimization-based methods. Early efforts often relied on drive testing or site surveys to evaluate signal quality at predefined locations~\cite{molisch2022wireless}, which are labor-intensive, time-consuming, and inflexible in adapting to changing user demands. To overcome these limitations, optimisation-based methods were introduced to automate the process using mathematical formulations or heuristics. 
Chiaraviglio et al.~\cite{chiaraviglio20215g} formulated BS planning as a mixed-integer linear program under service and EMF constraints, and proposed a heuristic solution named PLATEA. While effective in balancing coverage and EMF compliance, the method relies on static environmental models and sensitive parameters, limiting its adaptability in dynamic scenarios. Similarly, Philip et al.\cite{philip2023cellular} applied meta-heuristic algorithms such as particle swarm optimization (PSO) and genetic algorithms (GA) for BS placement. Although PSO achieved better coverage and efficiency than GA, both approaches depend on simplified propagation models and static user distributions, making them less suitable for realistic and data-driven deployments.


In contrast, AI/ML based methods aim to enhance scalability and adaptability by replacing manual engineering and handcrafted models with data-driven learning. 
AutoBS~\cite{lee2025autobs} employs reinforcement learning to select BS locations, enabling fast inference during deployment. However, it requires large-scale pretraining on labeled environment performance pairs, which is both computationally expensive and time-consuming to collect. 
OSSN~\cite{zheng2024transformer}, a Transformer-based framework, unifies radio map estimation and site selection to reduce reliance on exhaustive candidate evaluations. While it improves planning efficiency, its performance remains limited by the availability of training data and a lack of real-world validation. 
Similarly, Dai and Zhang~\cite{dai2020propagation} take a hybrid approach by integrating machine learning with heuristic optimization, using a dataset of over $0.76$ million measurements to predict signal strength without explicit propagation modeling. 


However, existing AI/ML-based methods require extensive labelled datasets and incur substantial data collection and training costs. 
More importantly, they tend to overlook the sim-to-real discrepancy introduced by inaccurate or incomplete modelling of physical environments, which hinders generalization in real-world deployments.",2591
http://arxiv.org/abs/2510.01285v1,2510.01285v1,2025-09-30T22:34:23+00:00,LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,"The rapid advancement of Large Language Models (LLMs) has opened new
opportunities in data science, yet their practical deployment is often
constrained by the challenge of discovering relevant data within large
heterogeneous data lakes. Existing methods struggle with this: single-agent
systems are quickly overwhelmed by large, heterogeneous files in the large data
lakes, while multi-agent systems designed based on a master-slave paradigm
depend on a rigid central controller for task allocation that requires precise
knowledge of each sub-agent's capabilities. To address these limitations, we
propose a novel multi-agent communication paradigm inspired by the blackboard
architecture for traditional AI models. In this framework, a central agent
posts requests to a shared blackboard, and autonomous subordinate agents --
either responsible for a partition of the data lake or general information
retrieval -- volunteer to respond based on their capabilities. This design
improves scalability and flexibility by eliminating the need for a central
coordinator to have prior knowledge of all sub-agents' expertise. We evaluate
our method on three benchmarks that require explicit data discovery: KramaBench
and modified versions of DS-Bench and DA-Code to incorporate data discovery.
Experimental results demonstrate that the blackboard architecture substantially
outperforms baselines, including RAG and the master-slave multi-agent paradigm,
achieving between 13% to 57% relative improvement in end-to-end task success
and up to a 9% relative gain in F1 score for data discovery over the
best-performing baselines across both proprietary and open-source LLMs. Our
findings establish the blackboard paradigm as a scalable and generalizable
communication framework for multi-agent systems.","LLMs for Data Science:Specialized benchmarks have emerged to evaluate LLMs in data science.
DS-1000 (Lai et al., 2023), ARCADE (Yin et al., 2023), DataSciBench (Zhang et al., 2025), and DSEval
(Zhang et al., 2024) assess the translation of natural language instructions into correct implementa-
tions, distinguishingthemfrombroaderprogrammingbenchmarkssuchasSWE-Bench(Jimenezetal.,
2024), ML-Bench (Tang et al., 2025), and BigCodeBench (Zhuo et al., 2025). While most assume that
the relevant data files are pre-specified, recent efforts address multi-step reasoning: DS-Bench (Jing
et al., 2025) and BLADE (Gu et al., 2024) evaluate implementation planning, and ScienceAgentBench
(Chen et al., 2025) and BixBench (Mitchener et al., 2025) focus on integrating domain knowledge.
These benchmarks, however, still overlook the practical challenge of discovering relevant data within
large, heterogeneous repositories—a gap addressed by KramaBench (Lai et al., 2025), which explicitly
evaluates data discovery. Building on this, we study how agents can autonomously identify and
leverage the correct data sources for end-to-end analysis.
Applications of LLMs in data science have evolved from single-turn code generation to interactive,
tool-augmented agents that exploit models specialized for code, including GPT (Brown et al., 2020),
CodeGen (Rubavicius et al., 2025), StarCoder (Li et al., 2023), and Code Llama (Rozière et al., 2024).
While few-shot prompting (Brown et al., 2020) remains effective, state-of-the-art approaches adopt
agentic or multi-agentic frameworks that combine iterative reasoning with external tool use. ReAct
(Yao et al., 2023) pioneered the interleaving of reasoning and action, later extended to execution
environments (Chen et al., 2019). Toolformer (Schick et al., 2023) and Gorilla (Patil et al., 2024)
10
LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science
explicitly train LLMs to call APIs, a capability critical for tasks relying on specialized libraries. Self-
correction is a another key feature: frameworks like Self-Debug (Chen et al., 2024) and Reflexion
(Shinn et al., 2023) refine generated code using execution feedback. To further enhance reliability,
many systems integrate RAG (Lewis et al., 2020; Salemi and Zamani, 2024a, 2025; Salemi et al.,
2025) to retrieve documentation or code examples, reducing hallucinations and ensuring up-to-date
library use. Additionally, multi-agent master-slave frameworks, such as AutoKaggle (Li et al., 2024),
have demonstrated promising results in addressing these challenges.
Blackboard Systems:The blackboard system is a seminal architectural model from classical AI,
developed for complex problems that require incremental and opportunistic reasoning. It was
implemented in the Hearsay-II speech understanding (Erman et al., 1980) and is characterized by
threecomponents: (1)aglobal,hierarchicaldatastructure(theblackboard)thatmaintainsthecurrent
state of the solution; (2) independent specialist modules, known as knowledge sources, which monitor
the blackboard and contribute partial solutions; and (3) a control mechanism that opportunistically
determines which knowledge source to activate next (Nii, 1986). Following successful applications in
domains such as sonar interpretation with the HASP/SIAP system (Nii et al., 1982), the architecture
evolved to incorporate more sophisticated control strategies. Inspired by this paradigm, we adapt the
blackboard architecture for multi-agent communication: rather than a central controller assigning
tasks, all agents operate autonomously, responding to requests posted on the blackboard. A central
main agent then leverages the information contributed by sub-agents to solve the problem.",3751
http://arxiv.org/abs/2510.10695v1,2510.10695v1,2025-10-12T16:48:25+00:00,Stock Prediction via a Dual Relation Fusion Network incorporating Static and Dynamic Relations,"Accurate modeling of inter-stock relationships is critical for stock price
forecasting. However, existing methods predominantly focus on single-state
relationships, neglecting the essential complementarity between dynamic and
static inter-stock relations. To solve this problem, we propose a Dual Relation
Fusion Network (DRFN) to capture the long-term relative stability of stock
relation structures while retaining the flexibility to respond to sudden market
shifts. Our approach features a novel relative static relation component that
models time-varying long-term patterns and incorporates overnight informational
influences. We capture dynamic inter-stock relationships through distance-aware
mechanisms, while evolving long-term structures via recurrent fusion of dynamic
relations from the prior day with the pre-defined static relations. Experiments
demonstrate that our method significantly outperforms the baselines across
different markets, with high sensitivity to the co-movement of relational
strength and stock price.","Earlier studies have used historical data and technical indicators of individual stocks for predictions [ 9,10]. Li et al. [ 2]
used online news text and stock price time-series market indicators as input, and they proposed a multimodal event-
driven LSTM model to perform the interaction and integration of the multimodal inputs to enhance the performance
of stock price prediction. However, these studies ignored correlations between stocks and deemed them independent
entities, making it difficult to capture their interactive influence on stock predictions. Subsequent studies have
recognized this issue and developed stock prediction methods based on relational modeling. Graph models are widely
used in relationship modeling. Ying et al. [ 11] designed a time-aware relational attention mechanism to capture
the time-varying correlation strength between stocks. However, the use of predefined relations lacks an effective
mechanism for handling attribute-related influences. The method in [ 12] infers latent firm relations from market
signals and, with its attribute-driven graph attention network, captures attribute-sensitive momentum spillovers and
complex market signal interactions. Huynh et al. [ 13] attempted to capture more sophisticated dynamic relations using
industry hypergraphs, wavelet hypergraph convolutions, and temporal generative filters to capture multi-order and
internal dynamic relationships. Relation modeling is further improved by capturing momentary and cross-time stock
correlations through intra- and inter-stock information aggregation, using a gating mechanism for automatic feature
selection based on market information [ 5]. However, this may not fully capture the long-term invariant relationships
between stocks. LSR-IGRU [ 6] constructed long-short-term relationship matrices using secondary industry and
overnight price information, enhancing GRU input to better integrate temporal and relationship information. Extending
LSR-IGRU, we propose DRFN to capture persistent relational structures while maintaining flexibility in responding to
sudden market shifts.",2105
http://arxiv.org/abs/2510.05740v1,2510.05740v1,2025-10-07T10:01:32+00:00,Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,"The rapid development of generative models has made it increasingly crucial
to develop detectors that can reliably detect synthetic images. Although most
of the work has now focused on cross-generator generalization, we argue that
this viewpoint is too limited. Detecting synthetic images involves another
equally important challenge: generalization across visual domains. To bridge
this gap,we present the OmniGen Benchmark. This comprehensive evaluation
dataset incorporates 12 state-of-the-art generators, providing a more realistic
way of evaluating detector performance under realistic conditions. In addition,
we introduce a new method, FusionDetect, aimed at addressing both vectors of
generalization. FusionDetect draws on the benefits of two frozen foundation
models: CLIP & Dinov2. By deriving features from both complementary models,we
develop a cohesive feature space that naturally adapts to changes in both
thecontent and design of the generator. Our extensive experiments demonstrate
that FusionDetect delivers not only a new state-of-the-art, which is 3.87% more
accurate than its closest competitor and 6.13% more precise on average on
established benchmarks, but also achieves a 4.48% increase in accuracy on
OmniGen,along with exceptional robustness to common image perturbations. We
introduce not only a top-performing detector, but also a new benchmark and
framework for furthering universal AI image detection. The code and dataset are
available at http://github.com/amir-aman/FusionDetect","The field of AI-generated image detection is in a constant race against generative technology. To provide context for our work, we'll first review the evolution of generative models, from older GANs \cite{gan, progan, biggan} to modern diffusion models \cite{adm, ldm, glide}. We'll then look at the detection methods, highlighting how each has responded to the shifting capabilities of generative architectures. Our review shows that existing detection methods have consistently lagged behind generative advancements, a critical gap that our work aims to close by addressing the ""two-axis generalization"" problem. 

\subsection{Image Generation}



The field of synthetic image generation has been reshaped over the last decade. It has transitioned from early breakthroughs with GANs \cite{progan, stylegan, biggan} to the current dominance of Diffusion Models (DMs). The advent of Denoising Diffusion Probabilistic Models (DDPMs) \cite{ddpm} marked a significant paradigm shift. Diffusion Models (DMs) and their subsequent variants have now surpassed GANs in terms of image quality, diversity, and text-to-image coherence \cite{adm}. The initial wave of practical diffusion models was led by the Latent Diffusion Model (LDM) architecture \cite{ldm}, which underpins the widely popular Stable Diffusion series. These models made high-fidelity generation accessible to the public and became a foundational tool for both research and creative applications.

The pace of innovation has since accelerated, leading to a new generation of even more sophisticated architectures. Architectural upgrades in models like Stable Diffusion XL (SDXL) \cite{sdxl}, such as a larger UNet \cite{unet} and dual text-encoders, have led to significant improvements in image quality and prompt fidelity. The field continues to evolve rapidly with new open-source models like FLUX \cite{flux}, SD3.5 \cite{sd3.5}, HiDream \cite{hidream}, CogView4 \cite{cogview}, Kandinsky3 \cite{kandinsky}, PixArt-$\delta$, alongside closed-source counterparts like Google's Imagen \cite{imagen} and Midjourney \cite{midjourney} and also community finetuned models such as Juggernaut \cite{juggernaut} and Dreamshaper \cite{dreamshaper}. This model shift from GANs to diffusions generates a new class of synthetic images with distinct statistical fingerprints that challenge existing detection methodologies, a primary focus of this work.



\subsection{Image Detection}
Detection methodologies can be broadly categorized into two main paradigms: those that seek to identify specific, inherent artifacts of the generation process, and those that leverage the general-purpose feature representations of large pretrained foundational models.


\subsubsection*{Artifact-Based Detection}

This paradigm is founded on the principle that the synthetic generation process, regardless of its sophistication, leaves behind subtle, machine-detectable traces or ""fingerprints"" \cite{dif}. Researchers have pursued these artifacts across various domains. A significant body of work targets universal image properties, analyzing inconsistencies in the frequency domain (\cite{fredect, npr, dif, f3net}), exploring local texture and patch-level correlations (\cite{patchcraft, lgrad, ssp}), or extracting unique residual noise patterns left by the generation process (\cite{dnf, lnp}). More recently, a modern class of artifact-based detectors leverages the internal mechanics of the diffusion process itself as a forensic tool. This approach is broadly divided into error-based and non-error-based methods. Error-based detectors operate on the principle that diffusion models reconstruct their own outputs with lower error than real images, using this discrepancy in pixel space (\cite{dire, sedid}), in latent space (\cite{aeroblade}), or as a guiding feature (\cite{lare}). In contrast, non-error-based methods use the diffusion pipeline in other ways, such as to generate hard negative training samples (\cite{drct}), to extract internal representations like noise maps as features (\cite{fakeinversion}), or to distill a slow, error-based model into a faster one (\cite{distildire}). A detailed overview of these detection paradigms is provided in Appendix \ref{previous_detectors}.


Despite their successes, our experiments indicate that artifact-based detection methods face significant limitations. First, their performance is often brittle, demonstrating poor cross-generator generalization. As generative models evolve, the specific artifacts these methods rely on change, making the detectors quickly outdated. Second, they are highly sensitive to common image perturbations, like compression, which can easily destroy the subtle fingerprints they detect.


\subsubsection*{Pretrained Feature-Based Detection}

A more recent and increasingly dominant paradigm moves away from specialized artifact detection and instead leverages the rich feature spaces of large-scale, pretrained foundational models \cite{aide, unifd, bilora}. The core idea is that these models, having been trained on web-scale data, have learned robust and generalizable representations of the visual world. A pioneering work in this area is \texttt{UniFD} \cite{unifd}, which demonstrated that a simple linear classifier trained on CLIP \cite{clip} features can achieve impressive generalization across unseen generators. This highlighted the power of semantic features for the detection task. Other works have explored this vision-language connection further; for example, \texttt{Bi-LoRa} \cite{bilora} reframes the detection problem as a visual question-answering or captioning task. Methods like \texttt{LASTED} \cite{lasted} also leverage language-guided contrastive learning. \texttt{AIDE} \cite{aide}, proposed a hybrid model that combined semantic features from a pretrained CLIP model with specialized, hand-crafted modules (DCT \cite{dct} and SRM \cite{srm} filters) to capture low-level texture statistics. 



The success of sophisticated hybrid approaches like AIDE \cite{aide}, raises a critical question: is it necessary to design hand-crafted modules for low-level features, or can a more effective and less complex solution be found by fusing the features of two distinct, general-purpose foundational models? To answer this question we proposed \texttt{FusionDetect} that utilized feature fusion of foundation models and experiment on the impact of such approach.",6414
http://arxiv.org/abs/2510.01675v1,2510.01675v1,2025-10-02T05:00:24+00:00,Geometric Backstepping Control of Omnidirectional Tiltrotors Incorporating Servo-Rotor Dynamics for Robustness against Sudden Disturbances,"This work presents a geometric backstepping controller for a variable-tilt
omnidirectional multirotor that explicitly accounts for both servo and rotor
dynamics. Considering actuator dynamics is essential for more effective and
reliable operation, particularly during aggressive flight maneuvers or recovery
from sudden disturbances. While prior studies have investigated actuator-aware
control for conventional and fixed-tilt multirotors, these approaches rely on
linear relationships between actuator input and wrench, which cannot capture
the nonlinearities induced by variable tilt angles. In this work, we exploit
the cascade structure between the rigid-body dynamics of the multirotor and its
nonlinear actuator dynamics to design the proposed backstepping controller and
establish exponential stability of the overall system. Furthermore, we reveal
parametric uncertainty in the actuator model through experiments, and we
demonstrate that the proposed controller remains robust against such
uncertainty. The controller was compared against a baseline that does not
account for actuator dynamics across three experimental scenarios: fast
translational tracking, rapid rotational tracking, and recovery from sudden
disturbance. The proposed method consistently achieved better tracking
performance, and notably, while the baseline diverged and crashed during the
fastest translational trajectory tracking and the recovery experiment, the
proposed controller maintained stability and successfully completed the tasks,
thereby demonstrating its effectiveness.","Many existing control strategies for conventional multirotors \cite{bouabdallah2005backstepping,lee2010geometric} and omnidirectional multirotors \cite{kamel2018voliro, lee2025autonomous, lee2024autonomous, bodie2021active} simplify the problem by assuming that actuators can respond instantaneously to commanded inputs. While effective for slow movements, this assumption fails during fast trajectory tracking or when recovering from external disturbances. 




To address this issue, recent works begin to incorporate actuator dynamics directly into the controller design. For conventional multirotors, incorporating first-order rotor dynamics improves high speed tracking and robustness \cite{faessler2016thrust,tal2020accurate}. For fixed-tilt platforms where servo dynamics are absent, research has focused on the impact of rotor dynamics. Some studies have modeled the rotor angular speed dynamics as a first-order system and implemented an angular velocity feedback controller \cite{Brescianini2018}. Similarly, a geometric tracking controller was proposed that incorporated a first-order wrench dynamics model \cite{lee2025geometric}. However, for variable-tilt platforms, the wrench dynamics cannot be represented by a single first-order system due to the nonlinearity of the control allocation map. Consequently, variable-tilt multirotors require control strategies that explicitly accommodate these additional nonlinearities.
 





Another challenge for variable-tilt platforms is the slower response of the tilt servos. To address this problem, a Smith predictor was employed to compensate for known servo delays \cite{ryll2015novel}. A quasi-decoupling controller was also developed, which uses the current servo angle for control allocation to achieve performance independent of the servo's response time \cite{Lee2021_carosq}. Similar approach of using current servo angle in control allocation was also devised in \cite{park2023design} from the observation of slower servomotor response than the rotor response. A common limitation of these approaches is their focus on either rotor or servo dynamics in isolation. 

More recently, efforts have been made to address both actuator dynamics simultaneously. For instance, an approach using Nonlinear Model Predictive Control (NMPC) modeled both rotor thrust dynamics and servo angle dynamics as first-order systems \cite{li2024servo}. However, the substantial computational burden of NMPC remains particularly challenging in scenarios that require high-frequency control and rapid, large changes in actuator inputs. A different strategy, known as differential allocation, addresses actuator velocity limits and dynamics directly within the control allocation module \cite{allenspach2020design}. This approach was later extended to incorporate the power dynamics of the actuators, providing a more comprehensive model at the allocation level \cite{cuniato2024allocation}. However, while these methods are effective, their focus remains on solving the allocation problem itself. Consequently, they do not provide a formal stability analysis for the entire closed-loop system, where the vehicle's rigid-body dynamics are fully coupled with the actuator dynamics, and stability claims in these works are confined to simplified or linearized models. Furthermore, they rely on the assumption of a perfectly known model and do not provide a robustness analysis against model uncertainties.",3446
http://arxiv.org/abs/2509.14126v1,2509.14126v1,2025-09-17T16:06:59+00:00,CrazyMARL: Decentralized Direct Motor Control Policies for Cooperative Aerial Transport of Cable-Suspended Payloads,"Collaborative transportation of cable-suspended payloads by teams of Unmanned
Aerial Vehicles (UAVs) has the potential to enhance payload capacity, adapt to
different payload shapes, and provide built-in compliance, making it attractive
for applications ranging from disaster relief to precision logistics. However,
multi-UAV coordination under disturbances, nonlinear payload dynamics, and
slack--taut cable modes remains a challenging control problem. To our
knowledge, no prior work has addressed these cable mode transitions in the
multi-UAV context, instead relying on simplifying rigid-link assumptions. We
propose CrazyMARL, a decentralized Reinforcement Learning (RL) framework for
multi-UAV cable-suspended payload transport. Simulation results demonstrate
that the learned policies can outperform classical decentralized controllers in
terms of disturbance rejection and tracking precision, achieving an 80%
recovery rate from harsh conditions compared to 44% for the baseline method. We
also achieve successful zero-shot sim-to-real transfer and demonstrate that our
policies are highly robust under harsh conditions, including wind, random
external disturbances, and transitions between slack and taut cable dynamics.
This work paves the way for autonomous, resilient UAV teams capable of
executing complex payload missions in unstructured environments.","We review control strategies for multirotor \glspl{uav} transporting cable-suspended payloads, covering classical model-based methods as well as learning-based approaches for both single- and multi-agent coordination. 
For a comprehensive survey of aerial cable transport, see~\cite{estevez_review_2024}.

\subsection{Traditional Model-based Approaches}


Model-based control approaches for aerial payload transport include centralized cascaded geometric controllers that provide stability guarantees for payload and manipulation control~\cite{sreenath_dynamics_2013}, as well as decentralized controllers that exploit internal cable tension for quasi-static attitude stabilization~\cite{tognon_aerial_2018}. 
However, these methods rely on noisy payload acceleration as a feedback signal and model cables as rigid rods, which limit the applicability in agile scenarios. 

Centralized and decentralized nonlinear model predictive control (NMPC) have advanced multi-\gls{uav} payload manipulation without acceleration feedback and can incorporate high-level objectives (e.g., obstacle collision avoidance)~\cite{ sun_nonlinear_2023,de2025distributed}. 
However, centralized NMPC is computationally expensive and suffers when scaled to large teams, while decentralized NMPC depends on iterative inter-robot communication and can suffer from deadlocks. Both approaches still adopt the rigid-rod cable assumption inherited from reactive controllers.  

The controllers devised by the previous methods adopt simplified models which restrict control performance to quasi-static regimes. Thus, other works have employed the full system dynamics in offline motion planning through optimization-based planners to account for more accurate models and enable agile maneuver planning~\cite{wahba2025pc, wahba_kinodynamic_2024,Wang2025SafeAA,sun2025agile}. Nevertheless, despite considering the full dynamics, these approaches still rely on the rigid-rod cable assumption.

To overcome the limitations of the rigid-rod assumption, more accurate cable models have been incorporated by switching the dynamics that explicitly capture the taut–slack transitions~\cite{wang2024impact, recalde2025hpc}. NMPC formulations with such hybrid models enable motions unattainable under rigid-rod assumptions but have so far been limited to single-\gls{uav} systems. Extending them to cooperative multi-robot manipulation remains challenging due to the added complexity of modeling, optimization, and coordination.

In contrast, our work explores reinforcement learning (RL) as a promising direction for achieving agile maneuvers and high robustness in multi-\gls{uav} payload transport while accounting for realistic cable dynamics.
\subsection{Reinforcement Learning-based Approaches}

Early application of \gls{rl} to \gls{uav} control focused on single-agent scenarios, and showed that RL agents trained with model-free \gls{rl} algorithms perform on par with or better than classical controllers~\cite{Koch2018ReinforcementLF}. 
Later works have demonstrated the effectiveness of \gls{rl}-trained \glspl{uav} in handling harsh initial conditions, executing aggressive maneuvers, and operating near the limits of their dynamic capabilities~\cite{Song2023ReachingTL, xing_multi-task_2024}. 
Other approaches have extended single-\gls{uav} control to payload transport and aerial manipulation, where RL-based controllers have also proven effective in adapting to unknown payload dynamics, maintaining stability and rejecting payload disturbances~\cite{hua_new_2022}.
Notably, \cite{cao2025flare} is capable of mode-switching and handling flexible cables for single \glspl{uav}.
However, compared to the single-\gls{uav}-payload-transportation problem, our current work on multi-\gls{uav} collaborative payload transport is considerably more challenging due to the coupling between vehicles, the need for precise coordination to regulate cable tensions, and the heightened risk of collisions.

\glsreset{marl}\gls{marl} has emerged as a powerful framework for cooperative tasks.
Some \Gls{marl} approaches employ \gls{ctde} by using a shared critic, such as \cite{Lowe2017MultiAgentAF, yu_surprising_2022}. Others, such as \gls{ippo}~\cite{witt_is_2020}, follow a completely decentralized regime. A decentralized scheme such as \gls{ippo} avoids scalability and communication overheads and is thus utilized in our work.
Many works have successfully used \gls{marl}, particularly methods such as \gls{mappo} and \gls{ippo}, in real-world, multi-robot collaborative tasks~\cite{Pandit2024LearningDM,  Chen2025DecentralizedNO}.
\gls{marl} strategies have also achieved success in swarm coordination, collaborative pursuit and evasion, and obstacle avoidance~\cite{huang_collision_2024, zhao_deep_2024}.

In the area of payload transportation, the adaptability and disturbance rejection capabilities of single-\gls{uav} methods have naturally led to multi-\gls{uav} extensions such as \cite{Lin2024PayloadTW, Estevez2024Reinforcement, xu_omnidrones_2024}. However, unlike the centralized approach presented in \cite{Lin2024PayloadTW}, we adopt a decentralized approach, thereby avoiding communication and scalability overheads.
While \cite{xu_omnidrones_2024} considers only rigid-link payloads and \cite{Estevez2024Reinforcement} assumes the payload cables are always taut, we make no such assumptions and model payload links with realistic flexible cables. Neither of these works reports transfer to the real world, whereas we achieve successful zero-shot sim2real transfer and demonstrate the robustness and agility of our robots under harsh real-world conditions. 
The closest to our current work is the approach from~\cite{zeng2025decentralized}, which also uses a decentralized training scheme and showcases robustness under harsh real-world settings. 
However, unlike our fully decentralized \gls{ippo}-based approach, this work uses the \gls{ctde}-based \gls{mappo} algorithm. It relies on a low-level controller, whereas we do not, and thanks to our highly parallelized training setup, our training speed is about ten times faster. In addition, \cite{zeng2025decentralized} still retains the rigid-rod assumption, where our work focuses on exploiting the hybrid cable model to achieve agile maneuvers and recovery from harsh configurations.",6319
http://arxiv.org/abs/2510.10113v1,2510.10113v1,2025-10-11T08:43:38+00:00,ImmerIris: A Large-Scale Dataset and Benchmark for Immersive Iris Recognition in Open Scenes,"In egocentric applications such as augmented and virtual reality, immersive
iris recognition is emerging as an accurate and seamless way to identify
persons. While classic systems acquire iris images on-axis, i.e., via dedicated
frontal sensors in controlled settings, the immersive setup primarily captures
off-axis irises through tilt-placed headset cameras, with only mild control in
open scenes. This yields unique challenges, including perspective distortion,
intensified quality degradations, and intra-class variations in iris texture.
Datasets capturing these challenges remain scarce. To fill this gap, this paper
introduces ImmerIris, a large-scale dataset collected via VR headsets,
containing 499,791 ocular images from 564 subjects. It is, to the best of
current knowledge, the largest public dataset and among the first dedicated to
off-axis acquisition. Based on ImmerIris, evaluation protocols are constructed
to benchmark recognition methods under different challenging factors. Current
methods, primarily designed for classic on-axis imagery, perform
unsatisfactorily on the immersive setup, mainly due to reliance on fallible
normalization. To this end, this paper further proposes a normalization-free
paradigm that directly learns from ocular images with minimal adjustment.
Despite its simplicity, this approach consistently outperforms
normalization-based counterparts, pointing to a promising direction for robust
immersive recognition.","\subsection{Iris Recognition}

Existing iris recognition methods mostly operate on normalized iris textures rather than raw ocular images~\cite{nguyen2024deep}. They first employ \textit{normalization} that segments the pupillary region~\cite{daugman2002high,he2008toward,vatsa2008improving}, parameterizes the iris contour~\cite{shah2009iris,proenca2009iris}, and usually unwraps it into a rectangular strip via polar transform~\cite{daugman2009iris}, followed by \textit{feature extraction} that generates identity-discriminative templates. \textit{Training-free} methods apply hand-crafted filters, \eg, Gabor~\cite{daugman2009iris}, to produce binarized \textit{iriscodes} and match them with Hamming distance~\cite{norouzi2012hamming}, with variants such as log-Gabor~\cite{ali2007recognition}, ordinal measure~\cite{sun2008ordinal}, sparse representation~\cite{pillai2011secure}, and phase correlation~\cite{miyazawa2008effective}. More recently, \textit{learning-based} methods employ DNNs for hierarchical feature extraction, improving robustness and accuracy with CNNs~\cite{gangwar2016deepirisnet,nguyen2017iris,zhang2018deep,wei2022towards}, FCNs~\cite{zhao2017towards}, Mask R-CNN~\cite{zhao2019deep}, DenseNet~\cite{wang2019toward,boutros2020benchmarking}, ResNet~\cite{boutros2020benchmarking}, and specialized backbones incorporating periocular cues~\cite{wei2022contextual,nguyen2022complex,zhao2018improving}. Though these methods achieve exciting results in controlled setups~\cite{nguyen2017long} and to some extent under non-ideal imaging~\cite{wang2020recognition}, they are not designed for the immersive setup and perform unsatisfactorily under off-axis distortion, quality degradation, and large variations, as discussed in~\cref{sec:baseline} and validated in~\cref{sec:benchmark}.










\subsection{Iris Recognition Datasets}

\input{tab/tab_rw_dataset}

Early iris recognition datasets were primarily collected in controlled setups using either visible light (VIS) or near-infrared (NIR) sensors, where both extrinsic and intrinsic conditions were strictly regulated~\cite{omelina2021survey}. Representative examples include CASIA-IrisV1~\cite{casia-iris-v1} and its update CASIA-IrisV4~\cite{casia-iris-v4}, the IIT Delhi database~\cite{kumar2010comparison}, the CUHK Iris dataset~\cite{chun2004iris}, and ND-CrossSensor~\cite{arora2012iris,xiao2013coupled}. Later efforts introduced semi-controlled scenarios with richer variations, such as acquisition via smartphones~\cite{rattani2016icip,zhang2016btas}, at-a-distance imaging~\cite{proencca2009ubiris}, or noise injection~\cite{proencca2005ubiris}. A recent work~\cite{wang2022human} is related to ours in that it also employs VR/AR devices for iris acquisition. However, their images exhibit much less off-axis distortion and lack diversity in such as illumination changes. Current datasets are briefly summarized in~\cref{tab:rw-dataset} and further discussed in the supplementary material. Overall, they fall short for the immersive setups due to 3 limitations: 1) small scale in images and subjects; 2) some being proprietary or unavailable; and 3) insufficient coverage of intra-class variation and off-axis geometric distortion. To address these gaps, we present ImmerIris, a large-scale, open-scene dataset dedicated to immersive iris recognition, which we believe will substantially advance research in this field.",3408
http://arxiv.org/abs/2510.07582v1,2510.07582v1,2025-10-08T22:01:00+00:00,"Type, Ability, and Effect Systems: Perspectives on Purity, Semantics, and Expressiveness","Programming benefits from a clear separation between pure, mathematical
computation and impure, effectful interaction with the world. Existing
approaches to enforce this separation include monads, type-and-effect systems,
and capability systems. All share a tension between precision and usability,
and each one has non-obvious strengths and weaknesses.
  This paper aims to raise the bar in assessing such systems. First, we propose
a semantic definition of purity, inspired by contextual equivalence, as a
baseline independent of any specific typing discipline. Second, we propose that
expressiveness should be measured by the degree of completeness, i.e., how many
semantically pure terms can be typed as pure. Using this measure, we focus on
minimal meaningful effect and capability systems and show that they are
incomparable, i.e., neither subsumes the other in terms of expressiveness.
  Based on this result, we propose a synthesis and show that type, ability, and
effect systems combine their respective strengths while avoiding their
weaknesses. As part of our formal model, we provide a logical relation to
facilitate proofs of purity and other properties for a variety of effect typing
disciplines.","\bfparagraph{Purity} 
In object-oriented programming languages, 
various definitions of purity have been proposed~\cite{stewart2014csse}, 
among which the definition of \citet{DBLP:journals/sigsoft/LeavensBR06} has been widely adopted in Java.
Under this definition, a location is modified when it exists in both the pre-state and post-state of an execution,
but stores different values in these two states.
This notion of observable purity has been adopted by Java analysis tools, including JPPA~\cite{DBLP:conf/vmcai/SalcianuR05}, ReIm and ReImInfer~\cite{DBLP:conf/oopsla/HuangMDE12,DBLP:conf/sigsoft/HuangM12},
JPure~\cite{DBLP:conf/cc/Pearce11}, as well as by Scala purity checking, \eg, \cite{DBLP:conf/ecoop/RytzAO13}.

Different from the above systems, Joe-E~\cite{DBLP:conf/ndss/MettlerWC10} defines pure methods as being side effect free and deterministic, 
\ie, their outputs should depend only on their inputs. 
This notation of purity is closer to the functional programming notion of purity.

In this paper, we use the definition of a pure function aligning with the mathematical definition of a function. 
That is, a pure function's behavior is entirely determined by its input argument, 
and its output does not depend on an external runtime environment that may change.
This definition is suitable for our purpose of equational reasoning. 

\citet{DBLP:journals/jfp/Sabry98} proposes to define purity for the Haskell language based on parameter-passing independence, 
\ie, different evaluation strategies (call-by-value, call-by-need, or call-by-name)
lead to equivalent values modulo divergence and runtime errors.
Our treatment of termination behavior differs from Sabry's approach. 
His equivalence is defined modulo termination, so his definition of pure
language describes Haskell more than Coq. 
In the age of Coq, Agda~\cite{DBLP:conf/tphol/BoveDN09}, Lean~\cite{DBLP:conf/cade/Moura021}, and especially also Liquid Haskell~\cite{DBLP:phd/basesearch/Vazou16}, 
we take the position that it is useful to tighten the definition and
recognize nontermination as impurity in closer correspondence with
logical consistency. 
In the presence of possibly-infinite codata, 
a syntactic guardedness checker is used to ensure productivity, \ie, even if a program generates an infinite amount of data, 
each piece will be generated in finite time~\cite{DBLP:journals/jucs/ATurner04}.



\bfparagraph{Type-and-Effect Systems} 
Effect systems are introduced to integrate imperative operations into functional languages~\cite{gifford1986integrating,DBLP:conf/popl/LucassenG88},
to track read and write effects on the heap~\cite{nielson1999type}, 
to check purity~\cite{DBLP:conf/cc/Pearce11,DBLP:conf/ecoop/RytzAO13,DBLP:conf/icfp/RompfMO09} and exceptions~\cite{DBLP:conf/popl/PessauxL99,DBLP:journals/toplas/LeroyP00,java},
and others. 
\cite{DBLP:conf/tldi/MarinoM09} introduce a generic type-and-effect system that allow programmers to easily define new effects systems. 
\citet{DBLP:journals/toplas/Gordon21} introduce a generic effect quantale framework
that  can model the effects of a range of sequential effect systems. 
In this work, we adopt a canonical binary effect system that distinguishes between computations that may induce observable effects and those that do not,
serving the purpose of reasoning about purity.

\bfparagraph{Algebraic Effects and Handlers} 
Algebraic effects~\cite{DBLP:journals/acs/PlotkinP03} and handlers~\cite{DBLP:journals/corr/PlotkinP13} are a purely functional composable approaches to modeling effects,
and inherit similar challenges in supporting effect polymorphism as those encountered in traditional effect systems.
These challenges are evident in languages that support algebraic effects, 
\eg, Koka~\cite{DBLP:conf/popl/Leijen17}, Helium~\cite{DBLP:journals/pacmpl/BiernackiPPS20}, the language in \citet{DBLP:conf/rta/HillerstromLAS17}'s work,
and the system developed by \citet{DBLP:journals/pacmpl/ZhangM19}'s work.
Frank~\cite{DBLP:conf/popl/LindleyMM17,DBLP:journals/jfp/ConventLMM20} reduces the annotation burden by leveraging ambient effects provided by the context, 
but relies on desugaring them to traditional effect polymorphism. This mechanism is fragile, and may result in unclear error messages.
\citet{DBLP:journals/pacmpl/TangWDHLL25,tang2025rowscapabilitiesmodaleffects} extend Frank's approach with multimodal type theory~\cite{DBLP:journals/lmcs/GratzerKNB21,694f77d247d34986bb82129b8d96206e} 
in tracking modes for types and terms.

\bfparagraph{Capabilities} 
Capability systems~\cite{DBLP:conf/ecoop/CastegrenW16,DBLP:conf/ecoop/BoylandNR01} have been used to reason about program resources
and external calls~\cite{drossopoulou2025reasoning},
and to ensure unique access with borrowing~\cite{haller2010capabilities},
Reference capabilities in Pony~\cite{DBLP:phd/ethos/Clebsch17} distinguish read and write capabilities.
\citet{DBLP:conf/ecoop/Gordon19} articulates the use-mention distinction when designing capabilities and effect systems. 
Our $\lambda_{ae}$-calculus uses ability to track ``mention'', and effects to track ``use''. 
\citet{DBLP:conf/icfem/CraigPGA18} use capabilities to bound the effects that an expression can induce for a language with 
a fixed set of global resources. Our systems can reason about dynamically allocated resources. 

\citet{osvald2016gentrification} distinguish 1st- and 2nd-class values via privilege qualifiers,
which can mediate access to different kinds of capabilities (\eg, read and write) with a fine-grained privilege lattice.
Effekt~\cite{DBLP:journals/pacmpl/BrachthauserSO20} treats effects as capabilities and treat all functions as second-class values,
which providing a lightweight of effect polymorphism, but impeding reasoning about purity.
Our systems track resource via abilities in a coarse-grained way and can recognize pure computation, 
which was not a goal of those prior systems.




\bfparagraph{Tracking Variables in Types} 
\citet{DBLP:journals/pacmpl/BaoWBJHR21} introduce reachability types to track aliasing and separation through reachability qualifiers.
Their work has been extended to support polymorphism~\cite{DBLP:journals/pacmpl/WeiBJBR24},
cyclic references~\cite{deng2025completecyclereachabilitytypes} and bidirectional typing~\cite{jia2024escape}.
Our semantic model is adapted from \citet{bao2025logrel}'s work, but we focus on a simplified setting restricted to Boolean references. 
In addition, \citet{bunting2025a} study contextual equivalence for \citet{DBLP:journals/pacmpl/BaoWBJHR21}'s system  
using operational game semantics and present a full abstraction model based on a labelled transition system.
In the future, we plan to extend our work to support fine-grained ability and effect tracking 
for programs with higher-order stores by leveraging reachability qualifiers as in \citet{bao2025logrel}'s work.

Capturing types~\cite{DBLP:journals/toplas/BoruchGruszeckiOLLB23,DBLP:journals/pacmpl/XuBO24} 
integrate capability tracking and escape checking into Scala 3.
Treating effects as capabilities provides a lightweight way to achieve effect polymorphism~\cite{DBLP:journals/pacmpl/BrachthauserSO20}.
\citet{DBLP:conf/programming/XuO24} introduce reach capabilities to address the challenges when capture checking and mutable variables interact.
Integrating mechanisms from our approach could potentially enable more precise purity checking for Scala.",7433
http://arxiv.org/abs/2509.25155v1,2509.25155v1,2025-09-29T17:55:43+00:00,Context-Driven Performance Modeling for Causal Inference Operators on Neural Processing Units,"The proliferation of large language models (LLMs) has driven demand for long
context inference on resource constrained edge devices. However, deploying
these models on Neural Processing Units (NPUs) presents significant challenges
due to the architectural mismatch: quadratic complexity of standard attention
mechanisms conflicts with memory and compute patterns of edge accelerators.
This paper presents a comprehensive performance analysis of various causal
inference operators on a modern NPU. We benchmark standard quadratic attention
against several sub-quadratic alternatives, including structured state-space
and linear attention models. Our analysis reveals that while sub-quadratic
methods offer superior scalability, they introduce distinct computational
bottlenecks on the NPU's specialized execution units. We identify that
quadratic attention becomes severely memory-bound, suffering from cache
inefficiency and pipeline stalls exceeding 95% at long contexts. In contrast,
sub-quadratic models can become compute-bound on programmable vector cores.
These findings provide critical insights for the co-design of hardware-aware
models and optimization strategies to enable on-device AI inference with
long-contexts.","\subsection{Long-Context Inference on Edge Platforms}
Prior work has explored deploying transformer-based causal large language models (LLMs) on edge platforms \cite{zhang2024edgeshardefficientllminference}, including hardware-specific optimizations for ARM CPUs \cite{10.1145/3700410.3702126} and FPGA-based execution through frameworks like \texttt{llama.cpp} \cite{llamacpp, haris2024designing}. While these efforts target on-device inference, they are not designed for long-context scenarios and do not address the associated memory and compute bottlenecks that emerge in attention-heavy models. Our work addresses this gap by empirically analyzing a range of causal inference mechanisms—including standard transformers and structured state-space models (SSMs)—under long-context settings. This enables us to derive architectural insights that inform the co-design of attention mechanisms for Neural Processing Units (NPUs), supporting more efficient hardware-aware deployment strategies.

\subsection{Acceleration of Sequence Models on NPUs}
Several efforts have investigated transformer acceleration on NPUs \cite{xu2025fast, zhu2025edge}, typically through operator-level scheduling or compiler-level block partitioning. However, these approaches fall short in capturing the fine-grained resource behavior required for efficient long-context inference. Other work has focused on optimizing SSMs for NPUs \cite{das2025xamba, aalishah2025mambalitesr}, leveraging architectural properties such as linear recurrence and memory compression. While successful within their respective domains, these strategies are not directly applicable to transformer-style causal attention. In contrast, our approach uses execution profiling and performance modeling—grounded in structured operator variants—to analyze architectural trade-offs across attention and SSM-style models. By leveraging structured state-space duality (SSD), we characterize how causal operators interact with NPU memory and compute hierarchies, enabling more informed co-design for future inference systems.",2070
http://arxiv.org/abs/2510.00956v1,2510.00956v1,2025-10-01T14:29:47+00:00,Bridging the Gap Between Simulated and Real Network Data Using Transfer Learning,"Machine Learning (ML)-based network models provide fast and accurate
predictions for complex network behaviors but require substantial training
data. Collecting such data from real networks is often costly and limited,
especially for critical scenarios like failures. As a result, researchers
commonly rely on simulated data, which reduces accuracy when models are
deployed in real environments. We propose a hybrid approach leveraging transfer
learning to combine simulated and real-world data. Using RouteNet-Fermi, we
show that fine-tuning a pre-trained model with a small real dataset
significantly improves performance. Our experiments with OMNeT++ and a custom
testbed reduce the Mean Absolute Percentage Error (MAPE) in packet delay
prediction by up to 88%. With just 10 real scenarios, MAPE drops by 37%, and
with 50 scenarios, by 48%.","\subsection{Transfer Learning}

Transfer learning enhances performance in a target task (receiver) by reusing knowledge from a related source task (donor) \cite{10.5555/2998687.2998769, pmlr-v15-bengio11b}. 


In this paper, we specifically employ fine-tuning~\cite{10.5555/2969033.2969197} to transfer useful knowledge from simulated to real-world network scenarios. Fine-tuning is a particularly popular transfer learning technique for neural networks (NNs). It involves reusing a pre-trained donor model by transferring some of its learned weights to initialize the receiver model. These encode knowledge from the donor's training process, providing the new model with a strong starting point, potentially reducing training time and improving accuracy. The benefits are higher if the target dataset is small.






Neural network-based models are typically composed of one or more layers that perform progressively complex transformations of the input data. Lower layers often focus on extracting general features from the input (e.g., basic statistical patterns), while higher layers specialize in learning task-specific features or making predictions~\cite{zeiler2013visualizingunderstandingconvolutionalnetworks}. 

Taking into account this, and how similar donor and receiver tasks are, the receiver model initialize its weights in one of three ways:
\begin{enumerate}
    \item Reused and frozen weights: The weights are transferred from the donor model and remain fixed during training. It decreases the training's computational cost.
    \item Reused and adjustable weights: The weights are transferred from the donor model but are allowed to update during training —and thus adapt to better fit the target.
    \item Randomly initialized weights: The layer's weights are trained from scratch if there is no useful knowledge to transfer from the donor (i.e., task-specific layers).
\end{enumerate}

\subsection{RouteNet-Fermi}

RouteNet-Fermi~\cite{ferriolgalmés2022routenetfermi} is a state-of-the-art network performance model. Specifically, it uses a custom representation of the network along with a modified Message-Passing Neural Network architecture \cite{pmlr-v70-gilmer17a} that exploits the interactions between traffic flows and the underlying devices. As a result, it has proven to generate accurate predictions at a fraction of the computational cost of alternatives such as DES. Its design also makes it robust to infer under unseen topologies, including larger than those seen during the model's training. It was also evaluated with both simulated and real-world network data.",2601
http://arxiv.org/abs/2510.06048v2,2510.06048v2,2025-10-07T15:42:33+00:00,BLISS: A Lightweight Bilevel Influence Scoring Method for Data Selection in Language Model Pretraining,"Effective data selection is essential for pretraining large language models
(LLMs), enhancing efficiency and improving generalization to downstream tasks.
However, existing approaches often require leveraging external pretrained
models, making it difficult to disentangle the effects of data selection from
those of the external pretrained models. In addition, they often overlook the
long-term impact of selected data if the model is trained to convergence,
primarily due to the prohibitive cost of full-scale LLM pretraining. In this
paper, we introduce BLISS (\textbf{B}ileve\textbf{L} \textbf{I}nfluence
\textbf{S}coring method for data \textbf{S}election): a lightweight data
selection method that operates entirely \emph{from scratch}, without relying on
any external pretrained oracle models, while explicitly accounting for the
long-term impact of selected data. BLISS leverages a small proxy model as a
surrogate for the LLM and employs a score model to estimate the long-term
influence of training samples if the proxy model is trained to convergence. We
formulate data selection as a bilevel optimization problem, where the
upper-level objective optimizes the score model to assign importance weights to
training samples, ensuring that minimizing the lower-level objective (i.e.,
training the proxy model over the weighted training loss until convergence)
leads to best validation performance. Once optimized, the trained score model
predicts influence scores for the dataset, enabling efficient selection of
high-quality samples for LLM pretraining. We validate BLISS by pretraining
410M/1B/2.8B Pythia and LLaMA-0.5B models on selected subsets of the C4
dataset. Notably, under the 1B model setting, BLISS achieves $1.7\times$
speedup in reaching the same performance as the state-of-the-art method,
demonstrating superior performance across multiple downstream tasks.","\vspace*{-0.05in}

\textbf{Data Selection for Language Model Training.} Early approaches to data selection primarily relied on rule-based methods as language filters for training data, employing utility functions tailored to specific datasets~\citep{conneau2019cross,raffel2020exploring,rae2021scaling,penedo2023refinedweb}. Another key category is data deduplication~\citep{lee2021deduplicating,sorscher2022beyond,penedo2023refinedweb,abbas2023semdedup,tirumala2023d4}, which eliminates redundant samples to optimize training efficiency and enhance performance on downstream tasks. A class of methods exist for performing data-quality filtering, which can select data similar to high-quality corpus of data points~\citep{brown2020language,du2022glam,gao2020pile,xie2023data}, with small perplexity~\citep{chowdhery2023palm,wenzek2019ccnet}. More recent methods leverage external pretrained LLMs to evaluate the pretraining data quality~\citep{wettig2024qurating,maini2024rephrasing}. In addition, a similar variant of data selection is domain reweighting for data mixtures~\citep{oren2019distributionally,sagawa2019distributionally,xie2023doremi,fan2023doge,albalak2023efficient,chen2024skill}, which re-scale the contribution of each domain to enhance generalization. Another recently emerged line of research leverages the tool of influence functions~\citep{hampel1974influence,cook1977detection,ling1984residuals,koh2017understanding} to evaluate the impact of individual training samples on a fixed LLM~\citep{park2023trak,engstrom2024dsdm,yu2024mates}. In contrast to these works, our work explicitly considers the long-term impact of selected data if the model is not simply fixed but trained to convergence. In addition, our method can train the model from scratch and does not need any extra information from any external pretrained models, making it a scalable and effective solution.

\vspace*{-0.05in}
\textbf{Bilevel Optimization and Data Selection.} Bilevel optimization provides a powerful framework for modeling optimization problems with a nested structure~\citep{bracken1973mathematical,dempe2002foundations}. Recent research has focused on developing efficient bilevel optimization algorithms with strong theoretical guarantees~\citep{ghadimi2018approximation,hong2023two,ji2021bilevel,kwon2023fully,dagreou2022framework,chen2023bilevel1,grazzi2022bilevel,hao2024bilevel,gong2024a}. This approach has been widely applied in various machine learning tasks, including meta-learning~\citep{finn2017model}, hyperparameter optimization~\citep{franceschi2018bilevel}, and natural language processing~\citep{somayajula2023bi,grangier2023bilevel}. For the application of data selection, bilevel optimization has been utilized for continual learning~\citep{borsos2020coresets,zhou2022probabilistic,hao2023bilevel1} and data reweighting in LLM fine-tuning~\citep{pan2024scalebio,shen2024seal}. Our work is most closely related to SEAL \citep{shen2024seal}, which focuses on selecting high-quality and safe data to fine-tune a pretrained LLM, with the goal of aligning the model with safety and ethical guidelines. However, our approach differs from SEAL in two key aspects:
(1) Problem setting. While SEAL operates in a fine-tuning context, our objective is to select data for \textbf{pretraining} an LLM \textbf{from scratch}, aiming to improve downstream performance \textbf{without relying on any external pretrained models}.
(2) Model update mechanism. SEAL utilizes the LoRA technique~\citep{hu2021lora} to update both the data selector and the LLM during fine-tuning. However, this approach is not directly applicable to our setting due to the following reasons. First, LoRA is only suitable for fine-tuning tasks but insufficient for full model pretraining. Second, their algorithm always updates the original large models directly, which is computationally expensive if all parameters are updated. In contrast, we propose a more efficient framework that introduces lightweight models (a score model and a proxy model) to guide data selection, while allowing full parameter updates within these smaller networks.
To the best of our knowledge, our proposed bilevel influence scoring method is the first to leverage bilevel optimization techniques for data selection in LLM pretraining. 


\vspace*{-0.1in}",4320
http://arxiv.org/abs/2510.10910v1,2510.10910v1,2025-10-13T02:11:57+00:00,SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model,"With the rapid development of diffusion models, style transfer has made
remarkable progress. However, flexible and localized style editing for scene
text remains an unsolved challenge. Although existing scene text editing
methods have achieved text region editing, they are typically limited to
content replacement and simple styles, which lack the ability of free-style
transfer. In this paper, we introduce SceneTextStylizer, a novel training-free
diffusion-based framework for flexible and high-fidelity style transfer of text
in scene images. Unlike prior approaches that either perform global style
transfer or focus solely on textual content modification, our method enables
prompt-guided style transformation specifically for text regions, while
preserving both text readability and stylistic consistency. To achieve this, we
design a feature injection module that leverages diffusion model inversion and
self-attention to transfer style features effectively. Additionally, a region
control mechanism is introduced by applying a distance-based changing mask at
each denoising step, enabling precise spatial control. To further enhance
visual quality, we incorporate a style enhancement module based on the Fourier
transform to reinforce stylistic richness. Extensive experiments demonstrate
that our method achieves superior performance in scene text style
transformation, outperforming existing state-of-the-art methods in both visual
fidelity and text preservation.","\subsection{Arbitrary Style Transfer}
Early neural style transfer methods focused on applying style from a reference image to a content image. Neural image style transfer~\cite{gatys2016image} was the first neural style transfer method that utilizes pre-trained neural networks to achieve style transfer based on style images. AdaIN~\cite{huang2017arbitrary} enabled arbitrary style transfer by aligning the mean and variance of content images and style images.
More recently, models such as StyleGAN~\cite{karras2019style} and StyTr2~\cite{deng2022stytr2} have explored style generation using GAN~\cite{goodfellow2014generative} and Transformer~\cite{vaswani2017attention} models. These methods require style images and focus on holistic stylization.
To overcome the reliance on style exemplars, recent approaches such as CLIPstyler~\cite{kwon2022clipstyler} have leveraged vision-language models, CLIP~\cite{radford2021learning}, to enable prompt-guided style transfer. 

With the emergence of diffusion models, a new text-guided image synthesis has emerged. Many style transformation methods based on the diffusion model have achieved high-quality results.
StyleDiffusion~\cite{wang2023stylediffusion} proposed a new content-style decoupling framework and introduced a CLIP-based style decoupling loss, which realizes interpretable and controllable style transformations by explicitly extracting content information and implicitly learning supplementary style information.
InST~\cite{zhang2023inversion} proposed an inversion-based style transformation method, in which style pictures are regarded as learnable text descriptions, and style transformation is realized through the attention layer of the diffusion model.
Yang et al.~\cite{yang2023zero} achieved style transformation without the need for fine-tuning and auxiliary networks by comparing the loss of the samples generated by the pre-trained diffusion network with the patches of the original images.
Chung et al.~\cite{chung2024style} also achieved style transformation without training by replacing the keys and values of the self-attention layer of the content image with the corresponding parts of the style image during the generation process.
Diffstyler~\cite{huang2024diffstyler} designed a dual diffusion model structure that utilized text embedding to control the generation of content and style.

While these approaches achieve impressive results in whole-image stylization, they are not designed for region-specific editing, such as selectively transforming only text regions. While also inversion-based, our method targets scene text stylization with precise regional control and zero training.

\subsection{Scene Text Editing}
Scene Text Editing (STE) aims to modify the textual regions of an image while preserving the rest of the scene. Traditional methods~\cite{roy2020stefann, wu2019editing, yang2020swaptext, luo2022siman} usually divide the task into background generation, text style generation, and reintegration modules that have a complicated network structure.
Subsequent works utilize GAN to improve editing fidelity like TextStyleBrush~\cite{krishnan2023textstylebrush} and Mostel~\cite{qu2023exploring}.
Recently, several diffusion-based methods such as DiffSTE~\cite{ji2023improving}, DiffUTE~\cite{chen2024diffute}, GlyphDraw~\cite{ma2023glyphdraw}, GlyphControl~\cite{yang2024glyphcontrol}, TextDiffuser~\cite{chen2024textdiffuser} significantly advanced in scene text generation and editing. However, many of these models still exhibit style inconsistencies.
To address this, TextCtrl~\cite{zeng2024textctrl} incorporates stylistic-structural guidance into the model design as well as the integration of a Glyph-adaptive Mutual Self-attention mechanism, which improves the stylistic consistency of the text.
DARLING~\cite{zhang2024choose} improved multitasking performance for text recognition, removal, and editing by decoupling content and style features and the Multi-task Decoder.
GlyphMastero~\cite{wang2025glyphmastero} targets editing tasks with complex characters, such as Chinese, by combining local character-level features and global text-line structures.
RS-STE~\cite{fang2025recognition} integration of text recognition and editing tasks, eliminating the complexity of modeling a design with a clear separation of background style and text content, enhancing the generation ability in real-world scenarios.

In contrast to the above methods, which either focus on content modification or make limited style changes based on in-image features, our approach targets prompt-guided, free-form style transformation of scene text without altering its content. This enables flexible and diverse stylization beyond the constraints of existing font or color attributes.",4768
http://arxiv.org/abs/2510.00895v1,2510.00895v1,2025-10-01T13:37:21+00:00,Visualizing Quantum Circuits: State Vector Difference Highlighting and the Half-Matrix,"Existing graphical user interfaces for circuit simulators often show small
visual summaries of the reduced state of each qubit, showing the probability,
phase, purity, and/or Bloch sphere coordinates associated with each qubit.
These necessarily provide an incomplete picture of the quantum state of the
qubits, and can sometimes be confusing for students or newcomers to quantum
computing. We contribute two novel visual approaches to provide more complete
information about small circuits. First, to complement information about each
qubit, we show the complete state vector, and illustrate the way that
amplitudes change from layer-to-layer under the effect of different gates, by
using a small set of colors, arrows, and symbols. We call this ``state vector
difference highlighting'', and show how it elucidates the effect of Hadamard,
X, Y, Z, S, T, Phase, and SWAP gates, where each gate may have an arbitrary
combination of control and anticontrol qubits. Second, we display pairwise
information about qubits (such as concurrence and correlation) in a triangular
``half-matrix'' visualization. Our open source software implementation, called
MuqcsCraft, is available as a live online demonstration that runs in a web
browser without installing any additional software, allowing a user to define a
circuit through drag-and-drop actions, and then simulate and visualize it.","\subsection{Visualizations for Quantum Computing}

A growing body of work applies principles of information visualization \cite{munzner2014book} and interaction design
to quantum computing, e.g.,
showing higher-level blocks in a circuit \cite{wen2023quantivine},
allowing users to sketch circuits \cite{arawjo2022},
visualizing and manipulating tensor networks \cite{kissinger2015quantomatic},
visualizing noise \cite{ruan2023vacsen}
and graph states \cite{miller2021},
or enabling a better understanding of
Grover's algorithm \cite{norrie2024qgrover},
Shor's algorithm \cite{tao2017shorvis},
quantum machine learning \cite{ruan2024violet},
and variational quantum algorithms (VQAs) \cite{rudolph2021orqviz}.


We now review topics related more specifically to our current work.

\subsection{Visualization of circuits}

Compared to previous work,
MuqcsCraft is the only graphical circuit simulator
that highlights differences in the state vector from one layer to the next,
as well as the only to display a triangular half-matrix,
and the only one that computes and shows pairwise concurrence values
to aid in understanding entanglement.
Below we discuss previous work and some of their specific features.


\subsubsection{Popular graphical interfaces}





IBM Quantum Composer \cite{ibm2023composer}
and Quirk \cite{gidney2020quirk} are both web-based, like MuqcsCraft.
Quantum Composer is limited to 4 qubits without an account and is closed source,
whereas Quirk and MuqcsCraft support 16 qubits and are both open source.
Quantum Composer lacks support for anticontrol qubits,
lacks support for control qubits on certain gates ($T$ and $S$),
supports a maximum of one control qubit on most gates,
and requires several clicks to add this control qubit,
whereas Quirk and MuqcsCraft allow arbitrary control and anticontrol qubits to be added to any gate
with a simple drag-and-drop.
Quantum Composer also has no support for continuously varying the parameters of gates,
whereas Quirk supports animated rotation gates,
and MuqcsCraft allows parameters of gates to be smoothly varied by dragging with the mouse
which triggers continuously updated visual feedback.














To visualize the evolution of the single-qubit reduced states,
both Quantum Composer and Quirk allow widgets to be drag-and-dropped onto the circuit
between each layer of gates,
however MuqcsCraft allows the visibility of reduced states for all layers to be toggled with a single click.

To visualize the evolution of the state vector,
Quantum Composer allows the user to step through the circuit, one layer at a time,
while a single colored barchart of the state vector is updated.
Quirk and MuqcsCraft both display the state vector with wrapping to save space
(and in MuqcsCraft, wrapping is an option that can be toggled).
Quirk allows state vector widgets to be drag-and-dropped onto the circuit
between each layer of gates.
MuqcsCraft instead allows the visibility of state vectors for all layers to be toggled with a single click,
and is the only package to highlight the differences from one layer to the next.


Comparing the two open-source packages,
Quirk is over 40k lines of code versus MuqcsCraft's less than 10k
making it easier to study and understand.
Quirk uses GPU acceleration to simulate circuits faster,
while MuqcsCraft avoids the complexity of GPU programming
and nevertheless simulates circuits of 16 qubits or less in under 10 ms per gate on a 2022 laptop.

\subsubsection{Academic work on circuit visualization}

Rainbow boxes \cite{lamy2019rainbow}
represent qubits with rectangles,
where height shows probability, color shows phase,
and entanglement is indicated by merging rectangles,
however this can only be done for adjacent qubits,
and makes no distinction between qubits that are
partially versus maximally entangled.

QuFlow \cite{lin2018quflow} shows causal relationships between amplitudes from one layer to the next,
and
QuantumEyes \cite{ruan2023quantumeyes} has a Probability Summary View that shows the evolution of
probabilities of base states layer-by-layer.
However, neither of these use
wrapping to make the state vectors consume less
vertical space,
making them less scalable than
Quirk and MuqcsCraft.

In addition, there is no obvious way to modify
these techniques to support wrapping.




Also, unlike these previous works,
we provide the source code for MuqcsCraft
and a working interactive online demo.












\subsection{Extensions to the Bloch sphere}

Methods for embedding sets of quantum states in space
can be developed through
the perspective of abstract algebra and topology,
discussed in depth by Bengtsson and {\.Z}yczkowski \cite{bengtsson2017geometry}.
Previous works have extended the Bloch sphere
to two qubits \cite{gamel2016,wie2020,chang2024},
e.g., to better understand entanglement,
and even to multiple qubits
\cite{altepeter2009,makela2010,bley2024}, 
however these extensions require 3D geometry
that is difficult to visualize,
and/or have limited scalability to many qubits.




In contrast, the triangular half-matrix used in MuqcsCraft
is purely 2D, and occupies an area that scales quadratically with
the number of qubits, allocating one cell for each pair of qubits
to show information about how the pair are related.",5279
http://arxiv.org/abs/2509.22711v1,2509.22711v1,2025-09-24T03:48:49+00:00,Beyond Western Politics: Cross-Cultural Benchmarks for Evaluating Partisan Associations in LLMs,"Partisan bias in LLMs has been evaluated to assess political leanings,
typically through a broad lens and largely in Western contexts. We move beyond
identifying general leanings to examine harmful, adversarial representational
associations around political leaders and parties. To do so, we create datasets
\textit{NeutQA-440} (non-adversarial prompts) and \textit{AdverQA-440}
(adversarial prompts), which probe models for comparative plausibility
judgments across the USA and India. Results show high susceptibility to biased
partisan associations and pronounced asymmetries (e.g., substantially more
favorable associations for U.S. Democrats than Republicans) alongside
mixed-polarity concentration around India's BJP, highlighting systemic risks
and motivating standardized, cross-cultural evaluation.","Partisan or political bias in LLMs has piqued researchers’ interest, and several studies have evaluated
the presence of this bias in various applications and frontier LLMs like ChatGPT, Google Gemini,
etc. Feng et al. [2023], Yang and Menczer [2025], Rozado [2023], Rotaru et al. [2024], Yuksel et al.
[2025]. Focusing on political bias in the American context, Motoki et al. studied the left-leaning
political bias in LLMs and underscored the existing value misalignment between ChatGPT, a popular
LLM application, and the average American Motoki et al. [2025]. Similarly, Faulborn et al. proposed
a survey-type political bias measure grounded in political science theory and used it to test various
commercial large language models, including multiple versions of ChatGPT Faulborn et al. [2025].
Going a step further from simply analysing partisan leaning, Peng et al. perform a comparative
study of political bias in LLMs. They design a two-dimensional framework that assesses the political
leaning of models on highly polarized topics while also assessing socio-political involvement on less
polarized ones Peng et al. [2024].
When examining the manifestations of partisan bias in different contexts, it is crucial to also highlight
the well-researched effects of interacting with a politically biased LLM and how it can influence
decisions and individual political ideologies. Fisher et al. conducted a study to understand whether
LLMs with a specific political leaning can influence the political decision-making of individuals
interacting with those models. The experiment highlights the significant extent to which interacting
with a biased model leads participants to adopt opinions and make decisions that match the model’s
Fisher et al. [2025]. Messer, in their study, uncovered a similar pattern where perceived alignment
between a user’s political orientation and bias in generated content was found to increase reliance
and acceptance of Generative AI systems by the user Messer [2025].
Research not only highlights the pervasive influence of biased LLMs but also demonstrates their
power to impact political conduct and public discourse around crucial topics. Goodman, in their
thesis, further elaborates on the impact and detrimental effects of the presence of political bias in
LLM applications like ChatGPT and reiterates how it can influence voting trends, especially the votes
of voters with low self-confidence Advisor and Lohmann [2024]. Therefore, it becomes imperative to
first understand the extent of the bias in a system and thoroughly examine the harms it is perpetuating
before aiming to mitigate the bias.",2635
http://arxiv.org/abs/2510.11682v1,2510.11682v1,2025-10-13T17:47:39+00:00,Ego-Vision World Model for Humanoid Contact Planning,"Enabling humanoid robots to exploit physical contact, rather than simply
avoid collisions, is crucial for autonomy in unstructured environments.
Traditional optimization-based planners struggle with contact complexity, while
on-policy reinforcement learning (RL) is sample-inefficient and has limited
multi-task ability. We propose a framework combining a learned world model with
sampling-based Model Predictive Control (MPC), trained on a demonstration-free
offline dataset to predict future outcomes in a compressed latent space. To
address sparse contact rewards and sensor noise, the MPC uses a learned
surrogate value function for dense, robust planning. Our single, scalable model
supports contact-aware tasks, including wall support after perturbation,
blocking incoming objects, and traversing height-limited arches, with improved
data efficiency and multi-task capability over on-policy RL. Deployed on a
physical humanoid, our system achieves robust, real-time contact planning from
proprioception and ego-centric depth images. Website:
https://ego-vcp.github.io/","\subsection{Model-Based Contact Planning}

For both locomotion and manipulation, robotics is replete with contact-rich problems, made challenging by the non-smooth dynamics induced by the impact ~\cite{raibert1986legged}. Optimization-based approaches address this by explicitly modeling these physical interactions, like linearizing the complex friction model into a Linear Complementarity Problem (LCP)~\cite{stewart1996implicit}, or relaxing it into a Cone Complementarity Problem (CCP)~\cite{anitescu2006optimization}. These formulations can then be embedded within a trajectory optimization framework~\cite{sleiman2021unified,winkler2018gait}. Another prominent paradigm is Hybrid Zero Dynamics (HZD)~\cite{westervelt2003hybrid, sreenath2011compliant}, which addresses the non-smooth contact dynamics of legged locomotion by enforcing virtual constraints whose associated zero dynamics surface remains invariant through impacts. However, such model-based approaches are often hindered by model inaccuracies and high computational costs~\cite{xue2024full}, which complicate real-time deployment. Furthermore, their reliance on predefined structures, such as periodic gaits~\cite{gong2019feedback} or reference foot-end trajectories~\cite{westervelt2003hybrid}, makes it difficult to scale them to more general, aperiodic whole-body contact scenarios.


\subsection{Learning-Based Contact
Planning}

Learning-based approaches for real-world contact planning have shown remarkable potential, enabling dynamic skills~\cite{li2025reinforcement, cheng2024extreme, jenelten2024dtc, liu2025discrete,margolis2024rapid, margolis2023walk}. However, three significant challenges remain largely unaddressed. First, interaction with both dynamic and static objects is limited. Most existing work on legged locomotion based on simplified 2.5D elevation maps~\cite{roth2025learned}, which cannot represent dynamic or overhanging obstacles such as a moving ball or an archway. Second, sample efficiency remains a bottleneck. Modern approaches rely heavily on synthetic data~\cite{NVIDIA_Isaac_Sim}, but the computational cost of rendering visual input makes on-policy RL algorithms prohibitively expensive~\cite{cheng2024extreme}. The sparse and discontinuous nature of contact events also poses significant challenges for model-free methods, which often require extensive training or carefully designed inductive biases to guide exploration effectively~\cite{liu2025discrete,zhang2024wococo}. Finally, multi-task capability is limited. While current policies can be trained to solve a single task with a specific reward function, they often fail to generalize across interactions with different objects or adapt to variations in task definitions~\cite{schulman2017proximal}.






\subsection{Planning with Robotic World Models}

The world model~\cite{ha2018recurrent} is a learned, internal model of an environment that allows an robot to predict future outcomes based on actions. The emphasis is not on explicitly predicting the future, but rather on predicting its abstract representation. Using a learned world model for model-based planning offers a path toward better generalization and data efficiency, presenting a promising solution for contact planning~\cite{hansen2022temporal,hafner2019learning}.

Early work on world models~\cite{ha2018recurrent} was defined as enabling efficient policy learning in reinforcement learning~\cite{hansen2022temporal, hafner2019learning}. Today, the definition has evolved as a generative AI system~\cite{assran2025v,bruce2024genie} capable of understanding and simulating the physical world's causal relationships, dynamics, and interactions. In control and robotics, a similar path is being explored by learning neural dynamics models to represent complex systems~\cite{o2022neural, zhang2024adaptigraph, li2025offline, li2025robotic}. These learned models could be integrated into frameworks such as sampling-based MPC~\cite{williams2017information, pan2024model, xue2024full} to achieve highly adaptive control~\cite{margolislearning, roth2025learned, xiao2024anycar}.


Nonetheless, enabling robotic world models to fully generalize remains an open problem. This is especially the case for contact planning, as the underlying whole-body contact state is not directly observable and is difficult to infer and predict from partial, noisy sensory data.",4387
http://arxiv.org/abs/2509.21170v1,2509.21170v1,2025-09-25T13:51:56+00:00,Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,"Large Language Models (LLMs) have shown great potential in supporting
automated code review due to their impressive capabilities in context
understanding and reasoning. However, these capabilities are still limited
compared to human-level cognition because they are heavily influenced by the
training data. Recent research has demonstrated significantly improved
performance through fine-tuning LLMs with code review data. However, compared
to human reviewers who often simultaneously analyze multiple dimensions of code
review to better identify issues, the full potential of these methods is
hampered by the limited or vague information used to fine-tune the models. This
paper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that
trains LLMs with an impressive reasoning ability to analyze multiple dimensions
of code review by harnessing long COT techniques to provide rich structured
information. To address context loss and reasoning logic loss issues that
frequently occur when LLMs process long COT prompts, we propose a solution that
combines the Maximum Entropy (ME) modeling principle with pre-defined reasoning
pathways in MelcotCR to enable more effective utilization of in-context
knowledge within long COT prompts while strengthening the logical tightness of
the reasoning process. Empirical evaluations on our curated MelcotCR dataset
and the public CodeReviewer dataset reveal that a low-parameter base model,
such as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art
methods in terms of the accuracy of detecting and describing code issues, with
its performance remarkably on par with that of the 671B DeepSeek-R1 model.","In this section, we review the related work that forms the foundation of this work, including
automated code review, chain of thought, and maximum entropy.
2.1 Automated Code Review (ACR)
ACR is an essential aspect of improving software development efficiency, which aims to reduce the
manual effort and time required for code assessment. The main focus of an ACR system is to detect
potential code defects and suggest or generate relevant review comments. It typically comprises
two components: defect detection and review comment recommendation/generation.
Defect detection aims to identify potential issues within code snippets under review. For instance,
CodeT5 [ 50] adopts a unified framework that supports both code understanding and generation
tasks, thus facilitating multi-task learning. CodeBert [ 10] is a bimodal pre-training model tailored
for programming languages and natural language, excelling in tasks such as natural language-based
code search and code documentation generation. DACE [ 44] employs CNN and LSTM techniques
to extract Diff features from the code, enabling the prediction of code quality in Diff patches.
LogiCode [ 62] leverages LLMs to detect logical anomalies, automatically generating Python code
to identify issues such as incorrect component quantities or missing elements.
Review comment recommendation/generation produces review comments through retrieval or
generation methods. For example, LLaMA-Reviewer [ 31] uses low-parameter fine-tuning techniques
to enhance LLaMA, leading to impressive results in generating review comments. CodeReviewer [ 30]
achieves notable success in detecting code defects, generating review comments, and performing
code repair tasks by developing pre-training tasks specifically designed for code review in an
end-to-end manner. Notably, both studies utilize the same dataset [ 30] for training and validating
their models, assuming that the existence of review comments represents the ground truth, without
evaluating whether these comments are genuinely related to the code fixes. DCR [ 14] learns the
similarity between code commit ‘diffs’ and review comments to retrieve comments relevant to
specific code commits. CommentFinder [ 17] employs deep learning techniques to retrieve pertinent
code review comments, thus minimizing the time reviewers spend crafting these comments. The
BitsAI-CR [ 33] framework enhances ACR through a two-stage approach that combines a rule-based
J. ACM, Vol. 37, No. 4, Article",2487
http://arxiv.org/abs/2510.10152v1,2510.10152v1,2025-10-11T10:21:19+00:00,Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,"In this work, we present Color3D, a highly adaptable framework for colorizing
both static and dynamic 3D scenes from monochromatic inputs, delivering
visually diverse and chromatically vibrant reconstructions with flexible
user-guided control. In contrast to existing methods that focus solely on
static scenarios and enforce multi-view consistency by averaging color
variations which inevitably sacrifice both chromatic richness and
controllability, our approach is able to preserve color diversity and
steerability while ensuring cross-view and cross-time consistency. In
particular, the core insight of our method is to colorize only a single key
view and then fine-tune a personalized colorizer to propagate its color to
novel views and time steps. Through personalization, the colorizer learns a
scene-specific deterministic color mapping underlying the reference view,
enabling it to consistently project corresponding colors to the content in
novel views and video frames via its inherent inductive bias. Once trained, the
personalized colorizer can be applied to infer consistent chrominance for all
other images, enabling direct reconstruction of colorful 3D scenes with a
dedicated Lab color space Gaussian splatting representation. The proposed
framework ingeniously recasts complicated 3D colorization as a more tractable
single image paradigm, allowing seamless integration of arbitrary image
colorization models with enhanced flexibility and controllability. Extensive
experiments across diverse static and dynamic 3D colorization benchmarks
substantiate that our method can deliver more consistent and chromatically rich
renderings with precise user control. Project Page
https://yecongwan.github.io/Color3D/.","\vspace{-6pt}
\noindent\textbf{Radiance Fields for Static and Dynamic Scenes.}
Radiance field representations \cite{mildenhall2021nerf,kerbl20233d} have driven a paradigm shift in novel view synthesis. Neural Radiance Fields (NeRF) \cite{mildenhall2021nerf} pioneered implicit volumetric modeling with coordinate-based neural networks, inspiring numerous extensions \cite{barron2021mip,barron2022mip,barron2023zip,verbin2022ref,chen2022tensorf,fridovich2022plenoxels,garbin2021fastnerf,muller2022instant} and subsequent work on dynamic scenes \cite{park2021nerfies,park2021hypernerf,wang2023masked,fang2022fast,liu2023robust,guo2023forward,shao2023tensor4d}. However, NeRF-based methods remain hindered by costly training and slow rendering. To overcome this, 3D Gaussian Splatting (3DGS) \cite{kerbl20233d} introduced an explicit Gaussian representation that enables real-time, photorealistic rendering. Recent advances further extend Gaussian splatting to dynamic reconstruction \cite{duan20244d,li2024spacetime,yang2023real,wu20244d,yang2024deformable}, combining efficiency and fidelity. Motivated by these developments, we employ 3DGS and 4DGS as canonical static and dynamic representations, building a unified framework for controllable 3D colorization with strong spatial-temporal consistency.

\noindent\textbf{From 2D Colorization to 3D.}
Image colorization aims to recover plausible chromatic content from grayscale inputs, with decades of progress improving realism and controllability \cite{zhang2017real,wu2021towards,kim2022bigcolor,kang2023ddcolor,ji2022colorformer}. Beyond automatic approaches, user-guided methods leverage reference images \cite{he2018deep,huang2022unicolor,zhao2021color2embed} or language prompts \cite{weng2023cad,zabari2023diffusing,weng2022code,chang2023coins,liang2024control}. Extending to videos introduces temporal consistency challenges, tackled by matching \cite{yang2024bistnet}, palette transfer \cite{wang2025consistent}, attention \cite{li2024towards}, and memory propagation \cite{yang2024colormnet}. Yet these remain limited to 2D domains, lacking cross-view consistency.
Efforts on 3D scene colorization are still nascent. GBC \cite{liao2024gbc} exploits video models for continuous inputs, while ChromaDistill \cite{dhiman2023corf} and ColorNeRF \cite{cheng2024colorizing} transfer knowledge from pretrained colorizers by averaging inconsistent predictions, often sacrificing controllability and vividness. Moreover, colorizing dynamic 3D scenes while ensuring spatial-temporal consistency remains unaddressed. Our work bridges this gap with a unified framework for both static and dynamic settings, achieving visually rich, consistent, and user-controllable 3D colorization.

\vspace{-6pt}
\vspace{-3pt}",2759
http://arxiv.org/abs/2510.04648v1,2510.04648v1,2025-10-06T09:52:18+00:00,EduPersona: Benchmarking Subjective Ability Boundaries of Virtual Student Agents,"As large language models are increasingly integrated into education, virtual
student agents are becoming vital for classroom simulation and teacher
training. Yet their classroom-oriented subjective abilities remain largely
unassessed, limiting understanding of model boundaries and hindering
trustworthy deployment. We present EduPersona, a large-scale benchmark spanning
two languages, three subjects, and ten persona types based on the Big Five
theory. The dataset contains 1,308 authentic classroom dialogue rounds,
corresponding to 12,814 teacher-student Q&A turns, and is further expanded
through persona stylization into roughly 10 times larger scale (128k turns),
providing a solid foundation for evaluation. Building on this resource, we
decompose hard-to-quantify subjective performance into three progressive tasks:
TASK1 basic coherence (whether behavior, emotion, expression, and voice align
with classroom context), TASK2 student realism, and TASK3 long-term persona
consistency, thereby establishing an evaluation framework grounded in
educational theory and research value. We conduct systematic experiments on
three representative LLMs, comparing their original versions with ten
persona-fine-tuned variants trained on EduPersona. Results show consistent and
significant average improvements across all tasks: TASK1 +33.6%, TASK2 +30.6%,
and TASK3 +14.9%. These improvements highlight the dataset's effectiveness and
research value, while also revealing the heterogeneous difficulty of persona
modeling. In summary, EduPersona delivers the first classroom benchmark
centered on subjective abilities, establishes a decoupled and verifiable
research paradigm, and we will open-source both the dataset and the framework
to support the broader research community in advancing trustworthy and
human-like AI for education.","We review prior research through the lens of our three evaluation tasks and highlight why systematic modeling of virtual student agents requires going beyond existing approaches.  
(1) \textbf{Basic Behavioral Coherence:}  
Existing educational datasets (e.g., ScienceQA~\citep{lu2022learn}, C-Eval~\citep{huang2023c}, SocraticQ~\citep{ang2023socratic}, MathQA~\citep{amini2019mathqa}) have advanced knowledge assessment but remain largely single-turn or exam-oriented, lacking modeling of the IRF (Initiation–Response–Feedback) structure central to classrooms. Recent multimodal efforts explore VQA~\citep{lee2025multimodality,xiao2025eduvqa}, emotion recognition~\citep{song2025emotional}, and engagement detection~\citep{xie2025msc}, yet they focus on perception rather than coherence across verbal and non-verbal dimensions. Task~1 (Sec.~\ref{subsec:task1}) addresses this gap.
(2) \textbf{Student Realism:}  
Persona-driven dialogue studies such as PersonaChat~\citep{zhang2018personalizing}, PersonalDialog~\citep{zheng2019personalized}, and MBTI-based generation~\citep{kar2025convergence} illustrate role-conditioned generation, but they rely on simplified tags and are situated in open-domain settings. They cannot answer the classroom-specific question: does a model’s response resemble that of a real student? Task~2 (Sec.~\ref{subsec:task2}) formalizes this evaluation.
(3) \textbf{Persona Consistency:}  
Maintaining stable traits over long interactions remains challenging. Traditional metrics (BLEU, ROUGE) correlate poorly with persona preservation, and alignment methods (RLHF~\citep{ouyang2022training}, Constitutional AI~\citep{bai2022constitutional}) or bias detection~\citep{chen2024persona} provide only partial insights. Systematic evaluation of persona stability in classroom dialogue is still absent, which Task~3 (Sec.~\ref{subsec:task3}) directly operationalizes.
Overall, while prior work has progressed in knowledge testing, role-conditioned generation, and multimodal analytics, it lacks a unified, pedagogically grounded framework for jointly evaluating \textit{basic coherence}, \textit{student realism}, and \textit{persona consistency}. EduPersona is designed to fill this gap.",2211
http://arxiv.org/abs/2510.09484v1,2510.09484v1,2025-10-10T15:48:31+00:00,CRPS-LAM: Regional ensemble weather forecasting from matching marginals,"Machine learning for weather prediction increasingly relies on ensemble
methods to provide probabilistic forecasts. Diffusion-based models have shown
strong performance in Limited-Area Modeling (LAM) but remain computationally
expensive at sampling time. Building on the success of global weather
forecasting models trained based on Continuous Ranked Probability Score (CRPS),
we introduce CRPS-LAM, a probabilistic LAM forecasting model trained with a
CRPS-based objective. By sampling and injecting a single latent noise vector
into the model, CRPS-LAM generates ensemble members in a single forward pass,
achieving sampling speeds up to 39 times faster than a diffusion-based model.
We evaluate the model on the MEPS regional dataset, where CRPS-LAM matches the
low errors of diffusion models. By retaining also fine-scale forecast details,
the method stands out as an effective approach for probabilistic regional
weather forecasting","MLWP models [Pacchiardi et al., 2024, Lang et al., 2024b, 2025, Bonev et al., 2025, Alet et al., 2025,
Oskarsson et al., 2024]. These approaches differ primarily in how the CRPS is estimated and how
stochasticity is introduced into the model. However, all these methods can produce skillful ensemble
forecasts with a single forward pass through the network. A particularly convenient formulation is that
of Alet et al. [2025], which employs a similar conditioning mechanism commonly used in diffusion
models, thereby enabling a simple transformation from a diffusion-based model to a CRPS-based one.
Previous approaches to probabilistic LAM have primarily relied on diffusion-based methods [Pathak
et al., 2024, Larsson et al., 2025, Abdi et al., 2025]. Diffusion models have demonstrated strong
forecasting performance, but at substantial computational cost during sampling of ensemble members.
The Graph-EFM latent variable model [Oskarsson et al., 2023] is capable of probabilistic LAM
forecasting in a single forward pass, similar to our method. Although it also incorporates a CRPS-
based regularization term in its training objective, the Graph-EFM training mainly relies on a more
involved variational framework.",1219
http://arxiv.org/abs/2510.10069v1,2510.10069v1,2025-10-11T07:12:44+00:00,SyncLipMAE: Contrastive Masked Pretraining for Audio-Visual Talking-Face Representation,"We introduce SyncLipMAE, a self-supervised pretraining framework for
talking-face video that learns synchronization-aware and transferable facial
dynamics from unlabeled audio-visual streams. Our approach couples masked
visual modeling with cross-modal contrastive alignment and employs three
per-frame prompt tokens that explicitly encode the essential factors of a
talking-face frame - identity, vocal motion (speech-synchronized facial
dynamics), and ambient motion (audio-agnostic movements such as blinks and head
pose). The contrastive objective uses time-aligned vocal-motion and audio
tokens as positives and misaligned pairs as negatives, driving both modalities
into a shared embedding space and yielding token-level audio-visual stream
synchronization. After pretraining, the aligned audio tokens together with the
visual prompt tokens (identity, vocal motion, ambient motion) form a unified
interface for four disparate downstream settings: (i) audio-visual stream
synchronization; (ii) facial emotion and head/face action recognition; (iii)
visual speech recognition; and (iv) visual dubbing, for which we enable
indistinguishable audio- or video-driven control within a single model. Across
four task families that require distinct capabilities, SyncLipMAE achieves
state-of-the-art results, underscoring the effectiveness of
synchronization-aware, factorized self-supervised pretraining.","\paragraph{Audio--visual synchronisation and correspondence.}
A long line of work formulates A/V learning as correspondence or temporal alignment. ~\cite{l3net} learns generic audio–visual correspondence from unlabelled video, while ~\cite{avts} casts synchronisation as in-time vs.\ out-of-time discrimination within the same clip. For faces, ~\cite{syncnet} introduces a two-stream embedding for lip–audio alignment and lag estimation; recent transformers such as ~\cite{vocalist} improve robustness across speech/singing with longer-range context. 

\paragraph{Masked modelling and contrastive learning at scale.}
Contrastive pretraining aligns paired modalities at scale (e.g., CLIP ~\cite{clip}), while masked autoencoders (MAE~\cite{mae}/VideoMAE~\cite{videomae}) show that reconstructing heavily masked inputs yields strong visual/video representations. These paradigms are complementary and now standard building blocks for multimodal pretraining. 


\paragraph{Facial video representation pretraining.} Closer to our setting, MARLIN~\cite{marlin} applies masked autoencoding to facial videos, using facial-region-guided masking to learn a universal face encoder transferable to expression recognition, deepfake detection, etc. Subsequent variants adapt MAE-style pretraining to dynamic facial expression recognition under limited labels~\cite{mae_dfer}. While these works focus on reconstruction-only objectives within the facial domain, we couple MAE with audio–visual contrast to align speech-driven dynamics, and structure the representation to separate identity and motion factors.


\paragraph{Visual speech recognition and audio--visual ASR.}
From early end-to-end lipreading (LipNet) to large-scale self-supervised A/V pretraining (AV-HuBERT) and automated VSR/AVSR recipe design (Auto-AVSR~\cite{autoavsr}), results consistently show that robust visual streams improve recognition and benefit from pretraining on unlabelled A/V data.

\paragraph{Talking-face generation and lip-sync synthesis.}
Audio-driven talking-head synthesis has progressed from GAN-based pipelines with explicit sync critics (e.g., Wav2Lip~\cite{wav2lip}) to diffusion in latent/image space (e.g., LatentSync~\cite{latentsync}) and efficient diffusion heads (MuseTalk~\cite{musetalk}). In parallel, foundation video generators and unified conditional DiT~\cite{dit} frameworks (WAN~\cite{wan}; VACE~\cite{vace}) provide scalable backbones and conditioning interfaces (Video Condition Unit, context adapters) for editing or T2V, which our dubbing setup leverages.

\input{depds/tab_avsync_hallo3}",2585
http://arxiv.org/abs/2510.02836v1,2510.02836v1,2025-10-03T09:18:36+00:00,"VR as a ""Drop-In"" Well-being Tool for Knowledge Workers","Virtual Reality (VR) is increasingly being used to support workplace
well-being, but many interventions focus narrowly on a single activity or goal.
Our work explores how VR can meet the diverse physical and mental needs of
knowledge workers. We developed Tranquil Loom, a VR app offering stretching,
guided meditation, and open exploration across four environments. The app
includes an AI assistant that suggests activities based on users' emotional
states. We conducted a two-phase mixed-methods study: (1) interviews with 10
knowledge workers to guide the app's design, and (2) deployment with 35
participants gathering usage data, well-being measures, and interviews. Results
showed increases in mindfulness and reductions in anxiety. Participants enjoyed
both structured and open-ended activities, often using the app playfully. While
AI suggestions were used infrequently, they prompted ideas for future
personalization. Overall, participants viewed VR as a flexible, ``drop-in''
tool, highlighting its value for situational rather than prescriptive
well-being support.","We surveyed various lines of research that our work draws upon, and grouped them into two
areas: \emph{i)} knowledge workers and well-being (\S\ref{subsec:rw_1}); and \emph{ii)} the role of VR in promoting well-being (\S\ref{subsec:rw_2}).

\subsection{Knowledge Work and Well-being}
Knowledge workers face a number of challenges due to technological advances, hybrid work, and shifting organizational structures~\cite{constantinides2022future, rudnicka2020eworklife}. While these changes bring flexibility, they also introduce stressors (e.g., long hours, lack of meaningful work, and poor work relationships) which can lead to alienation~\cite{nair_exploration_2010}. Constant connectivity fragments attention with frequent interruptions~\cite{soto_observing_2021}, while open-plan offices and remote work exacerbate distractions, isolation, and blur work-life boundaries~\cite{teevan_new_2021}. Together, these factors contribute to stress, burnout, and mental health problems, which ultimately reduces engagement and work outcomes~\cite{who_burnout_2019}. Therefore, employee well-being is vital to organizational success.

Knowledge work requires environments that support autonomy and creativity~\cite{mladkova_knowledge_2011} because it involves processing complex information and making decisions without clear guidelines~\cite{septiandri2024potential}. However, organizational tools and technologies focused on standardization and efficiency can hinder these needs~\cite{karr-wisniewski_when_2010}. Tools such as task managers and AI-driven automation might reduce cognitive load~\cite{das2023focused} but could also perpetuate overwork and stress~\cite{leshed_i_2011, mark_effects_2018}. HCI research has recently increasingly moved from productivity-focused agendas to the holistic experience of technology use in work settings \cite{guillou_is_2020, kim_understanding_2019}, through well-being-focused solutions such as mindfulness apps or tools for supporting emotional resilience~\cite{howe_design_2022}. However, these technologies often face challenges in adoption as they are treated as optional add-ons rather than integral parts of work processes. Moreover, they frequently fail to provide workers with opportunities to detach fully from their workplace and tasks, limiting their ability to recharge and mentally recover.



\subsection{The Role of VR in Promoting Well-being}
VR apps have been shown to support well-being through stress management~\cite{mazgelyte_effects_2021, pimentel_digital_2019}, emotional regulation~\cite{garcia-ballesteros_garden_2024, rodriguez_vr-based_2015}, and physical rehabilitation~\cite{camporesi_vr_2013}. Recent studies have explored their application in workplace well-being~\cite{Riches2023Virtual, Naylor2020A}, showing that VR interventions can reduce anxiety and negative mood states through nature-inspired or abstract environments and guided meditation~\cite{8252142, adhyaru2022virtual}. Thoondee and Oikonomou~\cite{8252142} demonstrated VR's stress-reducing effects for office workers, while Heyse et al.~\cite{heyse} enhanced relaxation by tailoring content to users’ emotions.

Nevertheless, many VR apps lack specificity for knowledge workers, often targeting general well-being instead of unique stressors and cognitive demands~\cite{8252142}. Hardware discomforts, such as prolonged headset use, limit adoption~\cite{kim_effect_2021}, and logistical issues in open-plan offices further complicate implementation. Privacy concerns surrounding data collection hinder trust~\cite{kaminska}, and sustaining engagement remains difficult, as the novelty of VR tools often wears off~\cite{Riches2023Virtual}. Despite these barriers, VR can provide distraction-free environments for relaxation and mindfulness~\cite{Naylor2019Augmented}. Features such as emotion-based adaptation~\cite{heyse} and biometric feedback~\cite{kaminska} can personalize experiences and boost engagement. Integrating VR within existing workplace tools such as calendars, can make tools more seamless and practical~\cite{chow_feeling_2024}. At the same time, transparent data practices are crucial for building trust~\cite{tahaei2023human}. By addressing these challenges, VR has the potential to become an effective and tailored solution for workplace well-being.
\smallskip

\noindent\textbf{Research Gaps.} 

Prior research on VR well-being often takes a solution-driven approach by evaluating predefined apps that target singular outcomes such as relaxation or focus \cite{adhyaru2022virtual}. However, this approach overlooks the multifaceted and fluctuating nature of workplace well-being, especially among knowledge workers whose needs can shift throughout the day. Moreover, many studies involve users only at the evaluation stage and exclude their perspectives from the design process \cite{Riches2023Virtual, heyse, 8252142}. As a result, existing tools may lack the flexibility and personal relevance needed for real-world uptake. In response, we adopted a human-centered approach integrating workers' experiences from early design to deployment.",5096
http://arxiv.org/abs/2510.11496v1,2510.11496v1,2025-10-13T15:04:38+00:00,AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model,"In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o,
Gemini, and Claude Sonnet have demonstrated outstanding performance with
enormous model sizes reaching hundreds of billions of parameters, they
significantly surpass the limitations in memory, power consumption, and
computing capacity of edge devices such as mobile phones. This paper introduces
AndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on
Qwen3's LLM and various visual encoders. We comprehensively outline the model
architectures, training pipeline, and training data of AndesVL, which achieves
first-tier performance across a wide range of open-source benchmarks, including
fields such as text-rich image understanding, reasoning and math, multi-image
comprehension, general VQA, hallucination mitigation, multilingual
understanding, and GUI-related tasks when compared with state-of-the-art models
of a similar scale. Furthermore, we introduce a 1+N LoR","2.1 Mobile-side MLLMs
Recent years have witnessed a proliferation of remarkable advances in MLLMs. Numerous remarkable
MLLMs [ 14,227,15,30,29,28,274,229,177,210,189,44,43,219,8] have been introduced, primarily driven
by the pursuit of exploring the upper bounds of model performance through scaling laws. This endeavor has
resulted in models with astronomically large parameter counts, reaching hundreds of billions or even trillions.
Nonetheless, this emphasis on large-scale models has left the development of mobile-side MLLMs relatively
underexplored.
Among the efforts towards more mobile-friendly MLLMs, the Qwen series has made notable progress. Qwen2-
VL [227] and Qwen2.5-VL [ 15] introduced model sizes of 2B and 3B, respectively, which are particularly suited
for deployment on mobile devices. These model sizes effectively balance performance and the computational
3
limitations of mobile hardware. Similarly, the InternVL series [ 28,274,229] presented a range of model
sizes—1B, 2B, and 4B—designed to fulfill various operational needs on mobile platforms.
In 2023, Meituan emerged as a pioneer in the mobile MLLM domain with the introduction of MobileVLM [ 36].
Built upon MobileLLaMA in a LLaV A-like [ 129] architecture, MobileVLM came in 1.7B and 3B model sizes. It
achieved SOTA results in some benchmarks for models of similar sizes at that time. Meituan offered significant
insights into the processing speeds on mobile and IoT platforms, reporting rates of 12.21 and 21.54 tokens per
second, respectively. In 2024, the release of MobileVLM V2 [ 37] further advanced the field by exploring the data
scaling law, improving training strategies, and optimizing the modality alignment design. These developments
contributed to a comprehensive enhancement in the performance of the MobileVLM framework.
In 2024, the Apple MM series [ 154,258] demonstrated that even relatively compact models, specifically those
with 1B and 3B parameters, could achieve impressive performance through meticulous data curation and
optimized training strategies. The Ferret UI series [ 249,123] marked a significant step forward, as it was the first
series extensively dedicated to improving the capabilities of screen UI understanding. It extended the capabilities
of MLLMs to tasks such as referring and grounding on mobile UI screens and answering questions related to
screen operations. However, Apple did not reveal the performance metrics for these models when deployed on
mobile platforms.
Xiaomi’s MobileVLM [ 233] also made important contributions by leveraging carefully constructed UI under-
standing and APP operation trajectory data. This enabled the model to expand its capabilities from understanding
within a single UI (intra-UI) to understanding and operating across multiple UIs (inter-UI). Nevertheless, Xi-
aomi’s 9.8B MobileVLM model was not successfully deployed on mobile devices.
Finally, vivo’s BlueLM-V-3B [ 146] and BlueLM-2.5-3B [ 238] achieved mobile-side deployment of an MLLM
through systematic optimizations in algorithms and hardware deployment. Specifically, BlueLM-V-3B achieved
a running memory of 2.2G and a token throughput speed of 24.4 tokens/s on MediaTek Dimensity 9300 NPUs.
This not only showcases its effectiveness but also provides practical performance metrics for mobile-side
MLLMs.
Despite these efforts, there remains a gap in comprehensively documenting training processes, deployment
solutions, and benchmark results for general and mobile-specific tasks of mobile-side MLLMs. Our work aims
to fill this void by presenting the AndesVL suite, which offers a comprehensive approach to mobile-side MLLMs,
including detailed training, deployment, and benchmarking aspects.
2.2 Mobile-Side Deployment of MLLM
The deployment of MLLMs on mobile devices presents unique challenges, including limited computational
resources, diverse hardware architectures, and stringent energy constraints. To address these issues, various
solutions [ 156,61,212,82,86,114,42,10] have been proposed that take advantage of CPUs, GPUs, and NPUs.
CPU-based DeploymentIn 2020, Alibaba developed the Mobile Neural Network (MNN) [ 86], an inference
engine tailored for mobile applications. It introduces a “pre-inference” mechanism for runtime optimization,
thorough kernel optimizations for optimal computation performance, and a back-end abstraction module that
enables hybrid scheduling while maintaining a lightweight engine footprint on mobile CPUs.
In 2023, Georgi Gerganov [ 61] introduced llama.cpp, a lightweight, dependency-free C/C++ implementation
designed for efficient LLM inference across diverse hardware platforms, including mobile CPUs. It includes
support for several quantization levels (ranging from 1.5-bit to 8-bit), enabling reduced memory consumption
and accelerated inference.
GPU-based DeploymentIn 2024, a machine learning compiler and high-performance deployment engine
for LLMs, MLC LLM [ 212], was developed, aiming to enable native deployment across various platforms,
including mobile GPUs. It compiles models into optimized binaries compatible with platforms such as iOS,
Android, and web browsers.
In addition, Li et al. [ 114] proposed Transformer-Lite, which focuses on the high-efficiency deployment of LLM
on mobile phone GPUs. It introduced four optimization techniques: a symbolic expression-based approach for
dynamic shape model inference, operator optimizations with execution priority settings, an FP4 quantization
method termed M0E4 to reduce dequantization overhead, and a sub-tensor-based technique to eliminate the
need for copying key-value (KV) cache after inference. These optimizations enable significant speedups in both
prefill and decoding phases compared to existing CPU-based and GPU-based inference engines.
4
Figure 2: The overall architecture of AndesVL mainly includes a visual encoder, an MLP projector, and an
LLM.
NPU-based DeploymentGemini Nano [ 42], developed by Google, is designed for on-device use cases,
running within Android’s AICore system service to leverage device hardware for low-latency inference. It is
accessible through the AI Edge SDK, which allows developers to customize the inference and prompts. Gemini
Nano models, such as Nano-1 (1.8B parameters) and Nano-2 (3.25B parameters), are distilled from larger
Gemini models and optimized for edge devices such as smartphones.
Finally, Apple’s On-Device Deployment utilizes the Core ML framework to optimize and deploy large language
models on Apple silicon [ 10]. Techniques such as grouped-query attention (GQA) mechanisms, mixed 2-bit
and 4-bit quantization, and efficient memory management strategies enable the deployment of models like
Llama-3.1-8B-Instruct on devices such as the iPhone",6746
http://arxiv.org/abs/2510.08783v1,2510.08783v1,2025-10-09T20:00:41+00:00,MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,"In an ideal design pipeline, user interface (UI) design is intertwined with
user research to validate decisions, yet studies are often resource-constrained
during early exploration. Recent advances in multimodal large language models
(MLLMs) offer a promising opportunity to act as early evaluators, helping
designers narrow options before formal testing. Unlike prior work that
emphasizes user behavior in narrow domains such as e-commerce with metrics like
clicks or conversions, we focus on subjective user evaluations across varied
interfaces. We investigate whether MLLMs can mimic human preferences when
evaluating individual UIs and comparing them. Using data from a crowdsourcing
platform, we benchmark GPT-4o, Claude, and Llama across 30 interfaces and
examine alignment with human judgments on multiple UI factors. Our results show
that MLLMs approximate human preferences on some dimensions but diverge on
others, underscoring both their potential and limitations in supplementing
early UX research.","2.1 LLM-as-a-Judge
Extensive work has been done in the field of ’LLM as a Judge’,
and within this paper, we aim to build upon the existing literature.
LLM as a judge is a concept that can be applied to a myriad of
different domains [ 16] and consists of using LLMs to evaluate the
outcomes of complex tasks. They can do so effectively because they
can quickly and efficiently process extensive datasets that can then
be used to make an informed decision [ 1]. LLMs have been used
as judges to evaluate everything from other models [ 28,55,60,61],
natural language and information [ 37,41,56], charts [ 24], and
reasoning and thinking [ 1,17,50]. Despite this, related literature
suggests that using LLMs as judges can offer cost-cutting and a
flexible alternative to human evaluation, but warns that they often
lack consistency while suffering from issues pertaining to bias and
reliability [16].
Across all studied domains, larger state-of-the art models (e.g.,
GPT-4o, Claude, etc.) perform better in LLM as a judge tasks than
smaller models [ 51,61]. Nonetheless, even well-performing models
deviate by up to five points on a 10-point scale, and LLM evaluators
are often excessively verbose, biased towards the existing positions,
and prefer their own outputs [ 30,39]. Given these limitations, LLMs
often are capable as approximators, but are far from being human
replacements in evaluation tasks.
We aim to distinguish our work by using a method that specifi-
cally explores how MLLMs can be used to evaluate two distinct UIs.
In the context of this paper, we use a pairwise MLLM evaluation
method [ 30], which consists of having the MLLM compare two
different options and selecting which is better aligned with a pre-
determined criterion [ 16,40]. This method closely aligns with A/B
testing, commonly used in user research to compare two interfaces.
We aim to run a pairwise test with human users looking at UIs and
a Likert-scale LLM evaluation with the same interfaces to explore
how close LLMs are to simulating human results.
2.2 MLLMs in User Research and Design
As in many other domains, AI is becoming increasingly intertwined
with user interface design and research [ 5,32]. Designers and re-
searchers alike are using MLLM to create and explore different
interfaces for their respective products. Specifically in design re-
search, LLMs are being used as assistants that are able to collaborate
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces 2025, , San Jose, CA, USA
Figure 1: User interfaces split by domains: An overview of a sample of the UIs evaluated by humans and MLLMs can be divided
into three domains: landing pages, digital receipts, and catalogs. Here we present low-fidelity versions of the screens that
we presented to the users. The users saw high-fidelity, branded, and in-product versions of these screens that have been
anonymized.
during UX evaluation sessions [ 25]. LLMs are occasionally being
used to create synthetic data that emulates human users [ 10,31,43].
For instance, Jansen et al . [21] explain how LLMs can use simulated
respondents to supplement the creation and analysis of survey re-
search. Further, Hämäläinen et al . [18] demonstrates that LLMs are
capable of simulating user survey responses and creating responses
that humans found plausible. Similarly, Li et al . [29] found that inlarger-scale market research surveys, LLM-generated survey re-
sponses achieved about a 75%-85% agreement with results from
actual humans. However, this paper also highlights the importance
of these models being trained on human data, as they cannot yet
capture all of the nuances of human users.
There have been some work focused on some aspect of UI eval-
uations [ 14,15,44,58]. In Duan et al . [15] , a Figma plugin was
2025, , San Jose, CA, USA Reuben Luera, et al.
created by assessing GPT-4’s alignment with a set of design heuris-
tics. Similarly, Duan et al . [14] utilized a dataset of design critiques
from several experienced designers to improve LLM-generated UI
feedback by 55%. Meanwhile Wu et al . [58] creates a design tool
based on several datasets and shows it performs closely to human
ground truth. In addition, Xiang et al . [59] created an LLM tool used
to evaluate smartwatch interfaces using simulated users. Our work
differs from these works as we create a benchmark that compares
several different state-of-the-art MLLMs to each other and to the
human judgments. Further, the benchmark reflects how MLLMs
perform in both pairwise and absolute scoring tasks based on a
list of nine UI factors. Agent A/B [ 53] asks a similar question and
explores whether or not its own model can complete A/B UX tests.
They train their agent specifically on UX click-through rate, focus-
ing solely on the filters and menus of e-commerce websites. We
aim to focus on a broader question, exploring UI judgments at a
higher level while testing an assortment of widely used MLLMs
(e.g., GPT-4o, Claude, Gemini) on an assortment of UI screens from
different domains.",5065
http://arxiv.org/abs/2510.06512v1,2510.06512v1,2025-10-07T23:05:20+00:00,LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval,"Neural models such as YOLO and HuBERT can be used to detect local properties
such as objects (""car"") and emotions (""angry"") in individual frames of videos
and audio clips respectively. The likelihood of these detections is indicated
by scores in [0, 1]. Lifting these scores to temporal properties over sequences
can be useful for several downstream applications such as query matching (e.g.,
""does the speaker eventually sound happy in this audio clip?""), and ranked
retrieval (e.g., ""retrieve top 5 videos with a 10 second scene where a car is
detected until a pedestrian is detected""). In this work, we formalize this
problem of assigning Scores for TempOral Properties (STOPs) over sequences,
given potentially noisy score predictors for local properties. We then propose
a scoring function called LogSTOP that can efficiently compute these scores for
temporal properties represented in Linear Temporal Logic. Empirically, LogSTOP,
with YOLO and HuBERT, outperforms Large Vision / Audio Language Models and
other Temporal Logic-based baselines by at least 16% on query matching with
temporal properties over objects-in-videos and emotions-in-speech respectively.
Similarly, on ranked retrieval with temporal properties over objects and
actions in videos, LogSTOP with Grounding DINO and SlowR50 reports at least a
19% and 16% increase in mean average precision and recall over zero-shot
text-to-video retrieval baselines respectively.","Temporal Logic for video and audio understanding.Yang et al. [44]and Choi et al. [9](NSVS-TL)
use the probabilistic model checker STORM to verify temporal properties over object detections in videos,
using LTL and PCTL to represent properties respectively.
9
Figure 5: Examples of video retrieval with different methods, from the TP2VR-objects and TP2VR-actions
datasets. The event length ranges in terms of number of frames are mentioned with the temporal properties.
Detailed discussion of these examples is in Appendix H
Benchmarks for video and audio understanding.Benchmarks for video understanding such as
Video-MME [ 17], RexTIME [ 8], Next-qa [ 43], QVHighlights [ 25], TemporalBench [ 7] and TempCompass [ 30]
include tasks that require temporal understanding of events in videos. Similarly, audio understanding datasets
such as MMAU [ 38] and CompA [ 18] evaluate temporal tasks such as detecting the order of two events. These
tasks are fundamentally different from the QMTP benchmark which focuses on more fine-grained temporal
properties.
Video retrieval with temporal queries.Popular text-to-video retrieval datasets such as Activity Net
Captions [ 24] and DiDeMo [ 3] focus on temporal segments within minute-long videos. Our TP2VR benchmark
focuses on fine-grained temporal queries over short events in videos, with many-to-many mapping between
queries and videos.
Popular text-video retrieval methods include CLIP4Clip [ 32], TS2-Net [ 31], [4], which employ training to
improve embeddings for retrieval, and zero-shot methods such as mPLUG [ 26] and ELIOT [ 29]. Since we use
off-the-shelf models with LogSTOP for retrieval, we only include the latter for comparison.",1685
http://arxiv.org/abs/2510.06195v1,2510.06195v1,2025-10-07T17:52:08+00:00,Latent Speech-Text Transformer,"Auto-regressive speech-text models are typically pre-trained on a large
number of interleaved sequences of text tokens and raw speech encoded as speech
tokens using vector quantization. These models have demonstrated
state-of-the-art performance in speech-to-speech understanding and generation
benchmarks, together with promising scaling laws, primarily enabled by the
representational alignment between text and speech. Nevertheless, they suffer
from shortcomings, partly owing to the disproportionately longer sequences of
speech tokens in contrast to textual tokens. This results in a large compute
imbalance between modalities during pre-training as well as during inference,
and a potential hindrance to effectively aligning speech and text, ultimately
translating to several orders of magnitude slower scaling laws. We introduce
the Latent Speech-Text Transformer (LST), which makes pre-training speech-text
models more data-efficient by dynamically and inexpensively aggregating speech
tokens into latent speech patches. These patches serve as higher-level units
that can either align with corresponding textual units to aid capability
transfer or even encapsulate common speech sequences like silences to be more
compute-efficient. We show that LST outperforms vanilla approaches on
speech-to-speech as well as text-to-text benchmarks in both data- and
compute-controlled settings, the former indicating more effective
representational alignment and the latter indicating steeper scaling laws for
speech-text models. On HellaSwag story completion, LST achieves 6.5% absolute
gain in speech accuracy under compute-controlled training and 5.3% under
data-controlled training, while also improving text performance. We will
release our models, code, and the evaluation data to facilitate further
research.","\textbf{LLMs using speech tokens.} Early neural audio generation methods included direct auto-regressive generation of the speech waveform \citep{van2016wavenet}, or using adversarial approaches \citep{kong2020hifi}. Following this, \textit{textless NLP} work \citep{lakhotia2021generative} showed that by using discrete speech tokens obtained from self-supervised speech encoders (CPC, wav2vec 2.0, HuBERT) as targets for language modeling, can enable fully spoken LLMs.   AudioLM \citep{borsos2023audiolm} further uses hierarchical generation, first predicts semantic tokens, and subsequent stages predict fine-grained acoustic tokens from SoundStream \citep{zeghidour2021soundstream}, to achieve both high audio quality as well as long-term consistency. In addition to augmenting semantic speech tokens with pitch and style tokens to explicitly model expressivity, SpiritLM \citep{nguyen2025spirit} also introduced interleaving speech modeling with text-tokens. More recently, Moshi \citep{defossez2024moshi} propose a hierarchical \textit{inner monologue} method, that jointly predicts time-aligned text and acoustic tokens (with distilled semantic information), together with modeling multiple-stream audio for handling full-duplex audio dialogues. Finally, similar to scaling laws for text LLMs \citep{hoffmann2022training}, \cite{cuervo2024scaling} fit scaling law curves to predict the performance of spoken LLMs, and find that they scale upto three order of magnitude more slowly than text LLMs.



\textbf{Transferring textual knowledge into speech LMs.} Slower scaling trends, together with a disproportionately lower amount of data, lead to a knowledge and reasoning gap between speech and text LLMs. To bridge this, AudioPaLM and TWIST \citep{rubenstein2023audiopalm,hassid2023textually} initialize a spoken LLM from a strong text model (PaLM-2, LLaMA), improving both speech understanding/generation and cross-lingual transfer. SpiritLM demonstrates that interleaved speech–text training significantly improves inter-modality knowledge transfer. Spectron \citep{nachmanispoken} uses a “Chain-of-Modality” pipeline to first produce text and then speech conditioned on the text, trading latency for stronger textual control, while Moshi \citep{defossez2024moshi} uses a similar approach but generates interleaved text and speech as an inner monologue. To improve latency, LLaMA-Omni \citep{fang2024llama} style systems decode text and speech simultaneously, by upsampling textual LLM hidden states to decode speech units, before proceeding to decode the next text token.



\begin{table}[t]
\centering
\begin{threeparttable}
\caption{Comparison of patching strategies with approximately matched patch sizes.  Static uses fixed patch lengths. Align (sil sep.) treats silence as separate patches, Align (sil merged) merges silence into words, and Curriculum starts with Align (sil sep.) and gradually shifts to Static during training.}



\begin{tabular}{lccc|cc|cc}
\toprule
\textbf{Model} & \textbf{Ave Patch Size} & \multicolumn{2}{c|}{HellaSwag} & \multicolumn{2}{c|}{StoryCloze} & \multicolumn{2}{c}{TopicStoryCloze} \\
 & (tokens) & S$\rightarrow$S & T$\rightarrow$T & S$\rightarrow$S & T$\rightarrow$T & S$\rightarrow$S & T$\rightarrow$T  \\
\midrule

LST (Static)  & 4 &   40.5&	48.8&	58.2&	\textbf{69.4}&	86.2&	95.1 \\
LST (\textbf{Curriculum})  & 5.8\tnote{*} $\rightarrow$ 4 &   \textbf{41.3}&	\textbf{49.2}&	\textbf{58.6} &	67.8&	\textbf{86.6}&	\textbf{95.4} \\
\midrule


LST (\textbf{Align, sil sep.})    & 5.8\tnote{*}  &    \textbf{39.9} &	\textbf{49.3}&	\textbf{60.3}&	\textbf{69.9}&	\textbf{85.7}&	\textbf{95.3} \\
LST (Static)  & 6 &  39.4&	49.2&	58.7&	69.6&	84.9&	94.9  \\
\midrule



LST (Static)  & 9 & 37.2 & \textbf{49.4} & 57.5 & \textbf{69.7} & 84.7 & 95.9 \\ 
LST (\textbf{Align, sil merged}) & 9.4  &  \textbf{38.5} & 49.0 & \textbf{58.8} & \textbf{69.7} & \textbf{86.9} & \textbf{96.0}  \\ 


\bottomrule
\end{tabular}
\begin{tablenotes}[para,flushleft]
\footnotesize
\item[*] The average patch length is 5.8 for words in Align (sil sep.), while silence has an average of 3.7.
\end{tablenotes}
\end{threeparttable}
\vspace{-2mm}
\end{table}


\textbf{Speech model efficiency.} Compared to text, speech yields much longer token sequences, owing to higher frequency audio codecs, that consume many times additional compute to pre-train and generate. Efforts to mitigate this include methods to produce coarser speech units \citep{baadesyllablelm,tseng2025taste}, hierarchical generation \citep{borsos2023audiolm}, and producing residual tokens using parallel streams \citep{copet2023simple}. Attempts at text-inspired approaches to compress token sequences such as BPE \citep{ren2022speech,li2024effectiveness} achieved limited success. In this paper, we take inspiration from recent dynamic patching approaches that have yielded improvements in other modalities such as text \citep{pagnoni2024byte,yu2023megabyte,videau2025bytes} and vision \citep{pang2024next,beyer2023flexivit}, and extend these methods to speech-text LLMs.

\textbf{Speech Understanding Benchmarks.} Going beyond measuring only acoustic and phonetic capabilities of speech models using scores such as ABX \citep{kahn2020libri}, \cite{nguyen2020zero} established the Zero Resource Speech Benchmark 2021, comprising datasets/metrics to evaluate   lexical (sWUGGY), syntactic (sBLIMP) and lexical-semantic (sSIMI) capabilities of spoken LLMs. Since these benchmarks contrast between very short speech segments, we found that dynamic compute  approaches such as ours, do not yield significant improvements. However, subsequently, \cite{hassid2023textually} introduced the sStoryCloze and TopicStoryCloze datasets, which are story completion benchmarks in the speech modality measuring commonsense/understanding abilities of Spoken LLMs. We use these benchmarks in this paper, together with a speech version of the popular HellaSWAG textual benchmark, also measuring commonsense reasoning capabilities.",6005
http://arxiv.org/abs/2509.17398v1,2509.17398v1,2025-09-22T06:57:46+00:00,Optimizing Split Federated Learning with Unstable Client Participation,"To enable training of large artificial intelligence (AI) models at the
network edge, split federated learning (SFL) has emerged as a promising
approach by distributing computation between edge devices and a server.
However, while unstable network environments pose significant challenges to
SFL, prior schemes often overlook such an effect by assuming perfect client
participation, rendering them impractical for real-world scenarios. In this
work, we develop an optimization framework for SFL with unstable client
participation. We theoretically derive the first convergence upper bound for
SFL with unstable client participation by considering activation uploading
failures, gradient downloading failures, and model aggregation failures. Based
on the theoretical results, we formulate a joint optimization problem for
client sampling and model splitting to minimize the upper bound. We then
develop an efficient solution approach to solve the problem optimally.
Extensive simulations on EMNIST and CIFAR-10 demonstrate the superiority of our
proposed framework compared to existing benchmarks.","\textbf{Convergence analysis of FL.}
FedAvg~\cite{mcmahan2017communication} is widely recognized as the first and the most commonly used FL algorithm. Several works have shown the convergence of FedAvg under different settings, e.g., IID setting~\cite{10.5555/3546258.3546471,NEURIPS2018_3ec27c2c}, non-IID setting~\cite{10292582,9261995} even with partial clients participation~\cite{cho2020clientselectionfederatedlearning}.





Under IID conditions, Wang et al. \cite{10.5555/3546258.3546471} present a unified framework for communication‑efficient strategies and establish convergence guarantees that both reduce communication overhead and achieve fast error convergence. 
Moreover, Woodworth et al.~\cite{NEURIPS2018_3ec27c2c} introduce a graph‑based oracle model for parallel stochastic optimization and derive the optimal lower bound that depends only on the graph depth and size, thereby clarifying fundamental limits of communication–parallelism trade-offs.
For non-IID settings, Rodio et al.~\cite{10292582} analyze heterogeneous and temporally/spatially correlated client availability, demonstrating that correlation deteriorates FedAvg’s convergence rate if not handled properly.
Dinh et al.~\cite{ 9261995} propose FEDL under strong convexity and smoothness, establish linear convergence by controlling local inexactness and learning rate. They derive closed-form solutions jointly tuning FL hyperparameters to balance wall-clock convergence and device energy costs. Meanwhile, Cho et al. \cite{cho2020clientselectionfederatedlearning} study biased client selection under partial participation and show that prioritizing high-loss clients accelerates FedAvg’s convergence but may introduce a small bias tied to data heterogeneity.












However, despite extensive convergence analyses in FL, existing FL convergence bounds do not directly apply to SFL due to architectural differences, as they often neglect SFL’s frequent communications and the impact of model splitting on convergence.




\textbf{Client sampling.} Client sampling is a critical design component in distributed machine learning across heterogeneous devices. In the existing FL literature, client sampling strategies primarily include uniform sampling \cite{mcmahan2017communication}, importance-aware sampling~\cite{chen2022optimalclientsamplingfederated,9904868,pmlr-v151-jee-cho22a}, clustering-based sampling~\cite{fraboni2021clustered,NEURIPS2024_7886b9ba}, resource-aware sampling \cite{10.1109/INFOCOM48880.2022.9796935}, and fairness-aware sampling \cite{9810502}. Uniform sampling refers to randomly selecting a fixed proportion of clients from the total pool during each training round. Despite its simplicity, uniform sampling does not account for system or data heterogeneity. Importance sampling mitigates the high variance inherent in stochastic optimization methods like stochastic gradient descent (SGD) by preferentially sampling clients with more ``important'' data. Specifically, these approaches assign higher sampling probabilities to clients whose contributions are quantified using local gradients (e.g., \cite{chen2022optimalclientsamplingfederated,9904868}) or local losses (e.g., \cite{pmlr-v151-jee-cho22a}). 

Clustering-based sampling first groups clients according to the similarity of their data features or model updates and then selects representative clients from each group~\cite{fraboni2021clustered,NEURIPS2024_7886b9ba}. Resource-aware sampling takes into account the available system resources, such as communication and computational resources, when choosing participating clients  for each training round~\cite{10.1109/INFOCOM48880.2022.9796935,9810502}.  However, the above methods are tailored for traditional FL, which is inapplicable to SFL because SFL has different training stages as alluded earlier.









\textbf{Model splitting.} Model splitting plays a critical role in SFL, especially in scenarios with unstable client participation. The selection of the split point influences both communication/computation latency and overall training performance~\cite{ZW2024ultra-LoLa,zw2025AIoutage}. Several studies have investigated how to determine the optimal model splitting point for SFL \cite{lin2024adaptsfl,10980018,10304624,shiranthika2023splitfedresiliencepacketloss}. 
Shiranthika et al. \cite{shiranthika2023splitfedresiliencepacketloss}  study the impact of model splitting strategies on the packet loss resilience
of SFL. 
Lin et al. \cite{lin2024adaptsfl,10980018} investigate how the model splitting point affects training convergence, revealing that a shallower split point leads to smaller convergence upper bound. Xu et al. \cite{10304624} propose a combined optimization approach to identify the best split point and bandwidth allocation, aiming to reduce total training latency. Unfortunately, these studies overlook how model splitting affects training convergence with unstable client participation. To the best of our knowledge, our research is the first to analyze convergence of SFL under unstable client participation and explore client sampling and model splitting under the context of SFL.",5151
http://arxiv.org/abs/2510.10682v1,2510.10682v1,2025-10-12T16:10:40+00:00,Action-Dynamics Modeling and Cross-Temporal Interaction for Online Action Understanding,"Action understanding, encompassing action detection and anticipation, plays a
crucial role in numerous practical applications. However, untrimmed videos are
often characterized by substantial redundant information and noise. Moreover,
in modeling action understanding, the influence of the agent's intention on the
action is often overlooked. Motivated by these issues, we propose a novel
framework called the State-Specific Model (SSM), designed to unify and enhance
both action detection and anticipation tasks. In the proposed framework, the
Critical State-Based Memory Compression module compresses frame sequences into
critical states, reducing information redundancy. The Action Pattern Learning
module constructs a state-transition graph with multi-dimensional edges to
model action dynamics in complex scenarios, on the basis of which potential
future cues can be generated to represent intention. Furthermore, our
Cross-Temporal Interaction module models the mutual influence between
intentions and past as well as current information through cross-temporal
interactions, thereby refining present and future features and ultimately
realizing simultaneous action detection and anticipation. Extensive experiments
on multiple benchmark datasets -- including EPIC-Kitchens-100, THUMOS'14,
TVSeries, and the introduced Parkinson's Disease Mouse Behaviour (PDMB) dataset
-- demonstrate the superior performance of our proposed framework compared to
other state-of-the-art approaches. These results highlight the importance of
action dynamics learning and cross-temporal interactions, laying a foundation
for future action understanding research.","method, Section IV reports the experimental results, and
Section V concludes.
II. RELATEDWORK
A. Online Action Detection
Online Action Detection (OAD) requires identifying and
classifying actions instantly, without access to future frames.
Contemporary OAD methods frequently center on memory
modeling to capture and leverage historical context from
observed frames. Early methods primarily relied on RNN or
CNN based models (e.g., [16]) to capture historical context.
TRN proposed by Xu et al. [17] explicitly modeled past frames
and their temporal context, while Eun et al. [18] extended
GRU [19] with a discriminative embedding model to more
effectively learn representations for detecting ongoing actions.
Zhao et al. [20] further improved learning efficiency through
knowledge distillation to mitigate inconsistent visual content.
With the success of Transformers [21] in modeling temporal
sequences, recent approaches have explored attention-based
architectures. Wang et al. [22], proposed an encoder-decoder
framework, referred to as OadTR, to jointly encode historical
information and predict future actions. LSTR proposed by
Xu et al. [9] expanded the memory horizon by introducing
segmented memory to analyze historical context in depth.
Yang et al. [23] adopted exemplary frames to guide attention
scheme learning representation sothat the detection accuracy
is improved. Chen et al. [10] introduced a gated history unit
and a future-augmented background suppression strategy to
better capture temporal cues. Despite these advances, OAD
still faces the inherent limitation of observed information,
which can reduce the effectiveness of modeling. On the other
hand, current popular methods exploit transformer’s capacity
for memory modeling, but the ever-growing length of the
memory sequence limits the effectiveness of these methods.
For the limitation of observed information, our proposed
SSM employs cross-temporal interactions to facilitate richer
temporal information learning. Moreover, by focusing on state-
based action dynamics, our method alleviates the limitations
brought by the ever-growing length of memory sequences.
B. Online Action Anticipation
Online action anticipation has received significant attention
in recent years, with its primary goal being the prediction of
future actions based solely on observations. Early works pre-
dominantly employed recurrent neural networks. For instance,
Furnari and Farinella [24] utilize a Dual-LSTM structure to en-
code and distill input sequences, generating cyclic predictions
for future frames. Their framework additionally incorporated a
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3
learnable attention module to fuse representations from RGB,
optical flow, and object-centric streams, thereby capturing a
wide range of visual cues. Similarly, Qi et al. [25] tackle
error accumulation in recurrent models by combining a con-
trastive loss with an attention mechanism, iteratively refining
intermediate feature embeddings. They also introduce verb and
noun classification for auxiliary guidance. Subsequently, Liu
and Lam [26] enhance the recurrent pipeline with an external
memory bank and a classification loss for observed content,
while employing contrastive learning to more closely align
anticipated features with ground-truth sequences.
Moving beyond recurrent networks, recent work has em-
braced Transformer architectures for action anticipation. Gird-
har and Grauman [12] developed the Anticipative Video Trans-
former (A VT), combining a Transformer encoder on raw video
frames with a masked decoder to jointly predict intermediate
and final representations. Osman et al. [27] took inspiration
from action recognition and devised a dual-stream approach
with different frame sampling rates, aiming to capture both
slow and fast dynamics in videos. Meanwhile, Roy et al. [28]
focused on human-object interactions, showing that modeling
object-specific cues through attention or Transformer modules
can effectively reveal which items are likely to be involved in
upcoming activities. Most of the previous works have tended
to focus solely on the single-task setting of action anticipation,
overlooking a key aspect: The outcomes of online action
detection and action anticipation mutually influence each other.
Consequently, they miss the potential benefit of integrating
complementary features from both tasks. Such complementar-
ity may yield richer and more robust feature representations,
which have the potential to guide the model to produce more
accurate detection and anticipation results. Building on this
insight, Our SSM addresses this limitation by enabling joint
training or inference for both tasks simultaneously.
III. METHOD
The proposed method aims to enable the model to perform
both action anticipation and detection within a video stream,
as illustrated in Fig",4879
http://arxiv.org/abs/2510.06190v1,2510.06190v1,2025-10-07T17:49:30+00:00,"On Powerful Ways to Generate: Autoregression, Diffusion, and Beyond","This paper formally studies generation processes, including auto-regressive
next-token prediction and masked diffusion, that abstract beyond architectural
specifics. At this level of abstraction, we quantify their benefits and
limitations through measurable criteria such as computational hardness and
learnability. In particular, we demonstrate that allowing generation to proceed
beyond autoregression and current masked diffusion, with capabilities to
rewrite and length-variable edit, can bring significant theoretical and
empirical advantages, with important implications for frontier LLMs that aspire
to tackle increasingly hard problems and work universally across domains beyond
natural language, such as coding and science.","Masked diffusion models~\citep{hoogeboom2021argmax,austin2021structured,lou2024discrete,sahoo2024simple,shi2024simplified} extend continuous diffusion models~\citep{sohl2015deep,ho2020denoising,song2020score} to discrete data. Early work applied these models to specialized domains such as graph generation~\citep{vignac2022digress,sun2023difusco}, protein design~\citep{gruver2023protein}, and drug discovery~\citep{lee2025genmol}, where non-sequential generation provides natural advantages. The field has evolved with recent commercial-scale language models like Gemini Diffusion~\citep{deepmind2025gemini} and Mercury~\citep{inceptionlabs2025mercury}, which demonstrate competitive performance on language generation, reasoning, and coding tasks. This suggests that MDMs can serve as viable alternatives to the autoregressive models that currently dominate LLMs. Against this background, this paper investigates the fundamental computational differences between generation paradigms and explores whether more powerful generation methods exist.

Several works have explored extensions to standard MDM through mechanisms that enable rewriting and editing~\citep{vonrutte2025generalized,wang2025remasking,peng2025path,havasi2025edit,wu2025dreamon,kim2025any}, which relate to our any-process generation framework. \citet{wang2025remasking} introduces random remasking during inference, though this capability is not learned from data. \citet{lou2024discrete,vonrutte2025generalized,sahoo2025diffusion} propose adding uniform noise in the forward process rather than using masks, with models learning to revert them in the backward process, but this approach generally underperforms since modifying tokens directly appears more difficult than unmasking. \citet{peng2025path} introduces path planning to control generation, though the planner is not trained end-to-end with the base model. Current with ours: \citet{havasi2025edit} introduces edit operations to flow matching frameworks but faces similar limitations as uniform noise approaches; \citet{kim2025any} introduces to insert tokens at any 
position while \citet{wu2025dreamon} proposes expansion 
and delete, but these capabilities per se are 
insufficient for handling hard reasoning tasks as 
discussed in \Cref{sec:theory}.",2286
http://arxiv.org/abs/2509.16517v1,2509.16517v1,2025-09-20T03:47:49+00:00,Seeing Culture: A Benchmark for Visual Reasoning and Grounding,"Multimodal vision-language models (VLMs) have made substantial progress in
various tasks that require a combined understanding of visual and textual
content, particularly in cultural understanding tasks, with the emergence of
new cultural datasets. However, these datasets frequently fall short of
providing cultural reasoning while underrepresenting many cultures. In this
paper, we introduce the Seeing Culture Benchmark (SCB), focusing on cultural
reasoning with a novel approach that requires VLMs to reason on culturally rich
images in two stages: i) selecting the correct visual option with
multiple-choice visual question answering (VQA), and ii) segmenting the
relevant cultural artifact as evidence of reasoning. Visual options in the
first stage are systematically organized into three types: those originating
from the same country, those from different countries, or a mixed group.
Notably, all options are derived from a singular category for each type.
Progression to the second stage occurs only after a correct visual option is
chosen. The SCB benchmark comprises 1,065 images that capture 138 cultural
artifacts across five categories from seven Southeast Asia countries, whose
diverse cultures are often overlooked, accompanied by 3,178 questions, of which
1,093 are unique and meticulously curated by human annotators. Our evaluation
of various VLMs reveals the complexities involved in cross-modal cultural
reasoning and highlights the disparity between visual reasoning and spatial
grounding in culturally nuanced scenarios. The SCB serves as a crucial
benchmark for identifying these shortcomings, thereby guiding future
developments in the field of cultural reasoning.
https://github.com/buraksatar/SeeingCulture","2.1 Benchmarks for Cultural Understanding
The domain has seen the emergence of various
recent multicultural vision-language datasets and
benchmarks that incorporate explicit cultural tax-
onomies and tailored tasks (e.g., culture-aware
VQA, grounding, and captioning), as shown inTable",285
http://arxiv.org/abs/2510.11108v1,2510.11108v1,2025-10-13T07:57:09+00:00,A Vision for Access Control in LLM-based Agent Systems,"The autonomy and contextual complexity of LLM-based agents render traditional
access control (AC) mechanisms insufficient. Static, rule-based systems
designed for predictable environments are fundamentally ill-equipped to manage
the dynamic information flows inherent in agentic interactions. This position
paper argues for a paradigm shift from binary access control to a more
sophisticated model of information governance, positing that the core challenge
is not merely about permission, but about governing the flow of information. We
introduce Agent Access Control (AAC), a novel framework that reframes AC as a
dynamic, context-aware process of information flow governance. AAC operates on
two core modules: (1) multi-dimensional contextual evaluation, which assesses
not just identity but also relationships, scenarios, and norms; and (2)
adaptive response formulation, which moves beyond simple allow/deny decisions
to shape information through redaction, summarization, and paraphrasing. This
vision, powered by a dedicated AC reasoning engine, aims to bridge the gap
between human-like nuanced judgment and scalable Al safety, proposing a new
conceptual lens for future research in trustworthy agent design.","LLM-Agent security and safety.Recent advances in Language Models (Yao
et al., 2023; Nakano et al., 2022; Wang et al., 2023) and tool use enable agents to
handleincreasinglycomplextasks.However,thesemethodssimultaneouslygrant
agents more and more permissions, expanding their attack surface. (Greshake
et al., 2023) exploit plugin permission vulnerabilities, using malicious prompts
to trick agents into actions like file deletion, data access, and system command
execution. (Zhang et al., 2025) involves injecting malicious tool permission vul-
nerabilities to carry out privacy theft, launch denial-of-service attacks, and even
manipulate business competition by triggering unscheduled tool-calling. (Li
et al., 2026) characterizes that LLM-driven agents can be easily weaponized
as intelligent scrapers, which defeat a website’s anti-bot access controls and illic-
itly exfiltrate high-value data. In a multi-agent system, (He et al., 2025) attack
can compromise entire multi-agent systems by intercepting and manipulating
inter-agent messages. So, in this paper, we propose a framework that aims to
A Vision for Access Control in LLM-based Agent Systems 3
comprehensively improve the safety of LLM-based agents by enhancing permis-
sion security.
ContextAwarenessandNormCompliance.Ourpermissionsecurityframe-
workfocusesonthekeyfactorsinfluencingAgentAccessControl(AAC),primar-
ily revolving around Context Awareness and Norm Compliance. Context aware-
ness highlights the necessity for agents to comprehend specific interaction sce-
narios, such as business meetings or private conversations, and the relationships
between agents and users, like friends or a superior-subordinate dynamic.These
factors dictate information sharing rules and trust/permission levels. Agents
must also maintain their digital identity in real-time, accurately understand-
ing their own and the interlocutor’s identity (roles, permissions, organizational
affiliations) to ensure self and user safety, preventing issues like “Sydney” form-
ing unexpected emotional connections due to identity perception errors. Conse-
quently, our AAC system integrates contextual awareness with dynamic rights
management. Norm compliance dictates that agents must adhere to legal, eth-
ical, and cultural frameworks. At the legal and regulatory level, agents must
strictly comply with global and regional data privacy laws and industry-specific
requirements. Ethically, agent actions must conform to universal moral prin-
ciples, ensuring fairness of outputs Bolukbasi et al. (2016), avoiding bias and
discrimination (Henderson et al., 2017; Caliskan et al., 2017), and suppressing
harmful content (Li et al., 2024). Additionally, agents need cultural adaptability,
understanding and respecting the nuances of different cultural backgrounds to
prevent clashes and misunderstandings (Durante et al., 2024). In summary, our
AAC system will comprehensively consider how to ensure agents comply with
laws and regulations, adhere to ethical principles, and adapt to multicultural
backgrounds, enabling them to serve human society more fairly and reliably.",3116
http://arxiv.org/abs/2510.08648v1,2510.08648v1,2025-10-09T06:41:18+00:00,Inverse-Free Wilson Loops for Transformers: A Practical Diagnostic for Invariance and Order Sensitivity,"Large language models can change answers under harmless edits that matter in
practice: RAG outputs flip when passages are reordered, fine-tuning erodes
invariances learned at pretraining, debate or chain-of-thought prompts take
path-dependent routes, and compiler fusion or reordering perturbs logits near
decision boundaries. These failures violate intended invariances, break
continuous integration, and force teams to trade safety for speed. The effects
are small yet distributed across layers and positions, sensitive to context
length and evaluation order, and costly to repair with retraining or formal
verification. We present WILSON, a minimal post-hoc diagnostic suite that
converts simple loop and reordering checks on internal representations into
system signals. WILSON combines an inverse-free curvature map over positions
and layers, computed with JVPs and Hutchinson probes, with activation-level
commutators that flag reorder risk. Signals are cheap to compute,
model-agnostic for standard Transformers, and exported as thresholds and CSV
artifacts for orchestrators. This enables concrete actions: guard RAG against
order effects, catch fine-tuning regressions, stabilize debate pathways and
long multi-turn contexts, and gate fusions or reorders in deployment. In short,
WILSON helps anticipate failures and approve safe optimizations so reliability
and throughput can improve together without changing model architecture or
training.","Missing invariance tests, fragile behavior, and
reorder risk.Behavioral test suites show that mod-
ern NLP systems often fail simple invariances and con-
trolled perturbations, motivatingdiagnosticevaluation
beyond accuracy (e.g., CheckList) (Ribeiro et al., 2020).
Robustness Gym systematizes stress tests and high-
lights brittleness under template-preserving edits (Goel
et al., 2021). For code LLMs, semantics-preserving
program transformations (notably identifier or variable
renaming) can spuriously change predictions, motivat-
ing invariance-oriented evaluation and augmentation
(Ankner et al., 2021; Wang et al., 2022). At the sys-
tems layer, ML compilers (TVM, XLA) aggressively
fuse and reorder operators to improve throughput, but
floating point non-associativity means reordering can
alter numerics (Chen et al., 2018; Higham, 2002). With-
out guards, such optimizations risk correctness regres-
sions. For long-context LLMs, input order effects are
well documented: models can underuse middle con-
text and flip answers when premise or evidence order
changes (Liu et al., 2023; Chen et al., 2024). In retrieval-
augmented pipelines, document order can interact with
position bias; recent studies report mixed magnitudes
(Cuconasu et al., 2025; Zhang et al., 2024).
Attempts to address the problems.Architectural
symmetry has a long history: group-equivariant models
(Cohen and Welling, 2016; 2017; Bronstein et al., 2021)
bake ininvariances via layer design. Our approach
ispost hocand model-agnostic: we diagnose order
sensitivity and invariance breaks at inference time. For
long-context stability, positional schemes such as ALiBi,
positional interpolation for RoPE, LongRoPE, and
YaRN reduce phase or length drift but modify training
or attention parameterization rather thandiagnose
order sensitivity online (Press et al., 2021; Chen et al.,
2023; Ding et al., 2024; Peng et al., 2024). In code
robustness, RECode and related work use semantics-
preserving transforms and augmentation to improve
invariance but again focus on training-time fixes (Wang
et al., 2022).",2086
http://arxiv.org/abs/2509.20160v1,2509.20160v1,2025-09-24T14:28:37+00:00,Characterizing the Performance of Accelerated Jetson Edge Devices for Training Deep Learning Models,"Deep Neural Networks (DNNs) have had a significant impact on domains like
autonomous vehicles and smart cities through low-latency inferencing on edge
computing devices close to the data source. However, DNN training on the edge
is poorly explored. Techniques like federated learning and the growing capacity
of GPU-accelerated edge devices like NVIDIA Jetson motivate the need for a
holistic characterization of DNN training on the edge. Training DNNs is
resource-intensive and can stress an edge's GPU, CPU, memory and storage
capacities. Edge devices also have different resources compared to workstations
and servers, such as slower shared memory and diverse storage media. Here, we
perform a principled study of DNN training on individual devices of three
contemporary Jetson device types: AGX Xavier, Xavier NX and Nano for three
diverse DNN model--dataset combinations. We vary device and training parameters
such as I/O pipelining and parallelism, storage media, mini-batch sizes and
power modes, and examine their effect on CPU and GPU utilization, fetch stalls,
training time, energy usage, and variability. Our analysis exposes several
resource inter-dependencies and counter-intuitive insights, while also helping
quantify known wisdom. Our rigorous study can help tune the training
performance on the edge, trade-off time and energy usage on constrained
devices, and even select an ideal edge hardware for a DNN workload, and, in
future, extend to federated learning too. As an illustration, we use these
results to build a simple model to predict the training time and energy per
epoch for any given DNN across different power modes, with minimal additional
profiling.","There is growing interest in training DNNs on the edge
and it offers several systems research challenges~\cite{paise22}.
However, there is a lack of rigorous empirical studies to characterize their performance and the specific challenges in effectively leveraging them. We address this gap in this paper. 

Liu et al.~\cite{tx2_training} examine DNN training workloads on the Jetson TX2 with respect to memory, CPU/GPU utilization and power consumption. They also correlate the analysis to lower level operations in DNN models. However, they do not experiment with varying power modes and framework configurations as we do, and the TX2 is an older architecture.
The Flower federated learning framework~\cite{beutel2020flower} supports heterogeneous environments including edge devices. They present results of deploying Flower on virtual Android devices and on Jetson TX2 edge accelerators. However, the edge is just a validation platform in their work and they do not offer any detailed analysis of the performance of the edge for training.

There exists literature on evaluating edge devices for model inferencing.
DeepEdgeBench~\cite{baller2021deepedgebench} compares the inference time and power consumption for edge devices such as NVIDIA Jetson Nano, Google Coral and Raspberry Pi 4 for \mobilenet v2 but training is not considered.
Others~\cite{tk1_europar} study Jetson TX1 and TK1 using roofline models for both the CPU and GPU with a matrix multiplication as the workload. While important, matrix multiplication is limited to the GPU. The training pipeline is I/O intensive and exercises disk, memory, CPU and GPU. We study this holistically.

MLPerf~\cite{MLSYS2020_02522a2b} is a community effort to provide a uniform framework for quantifying the performance of ML Hardware and Systems. The benchmark suite spans a number of application domains and datasets, and prescribes a quality threshold that must be met by any implementation. While such a suite is essential for measuring the overall impact of systems or optimizations on training, it does not measure low-level system metrics like the IO reads. MLPerf also lacks a training suite for edge devices. We adopt a similar training benchmark in our study, which is viable on edge devices.

There has also been specific attention on the energy usage and variability of edge devices. Holly, et al.~\cite{edge_config} correlate CPU and GPU frequencies and the number of CPU cores with the latency, power and energy for inferening on the Jetson Nano. Some~\cite{frqswitching} examine the effect of these on power consumption for stream processing workloads. We focus on the impact of such configurations, including storage media and DNN framework settings, on the end-to-end time and energy consumed for DNN training workloads on the Jetson AGX Xavier. Snowflakes~\cite{snowflakes} reports a detailed study on the latency and power variability observed across Jetson AGX Xaviers for inferencing. We too evaluate the variability, but for a training workload and we do not observe any variability.


There is a larger body of work on training on GPU workstations and servers.
Mohan, et al.~\cite{mohan2021analyzing} characterize the training data pipeline and how it affects training time on desktop GPUs. They also analyze the effect of the OS page cache on data access. However, their study only considers server-grade GPUs which are much more powerful and have exclusive and faster GPU RAM when compared to edge devices.
They also propose a modified caching mechanism that minimises I/O caused by thrashing. Once the page cache is full, all further accesses are sent to disk without evicting existing data in the cache. Quiver~\cite{kumar2020quiver} proposes a caching strategy based on substitutability. Accesses that cause a miss in the cache are substituted with other data that are present in the cache without interfering with training requirements and single access per epoch.
Our detailed study and analysis can help design such optimization strategies for training on edge accelerators.

Lastly, our paper enables accurate modeling of DNN training time and energy usage for the diverse power modes of these devices. This is key for federated learning when devices in a training round need to complete at about the same time~\cite{google-sysml}. Current techniques use simple approximations like over-sampling of devices~\cite{google-sysml}. We provide initial promising results in this direction.


\begin{table}[t]
\centering
\footnotesize
\vspace{-0.1in}
\caption{Specifications of NVIDIA Jetson Devices Evaluated}
\vspace{-0.1in}
\begin{tabular}{L{3.5cm}|R{1.5cm}|R{2.3cm}|R{2.3cm}|R{2.3cm}}
\toprule
\textbf{Feature} & \textbf{Nano}~\cite{powermodes_nano}  &  Xavier \textbf{NX}~\cite{powermodes_nxagx} & \textbf{AGX} Xavier~\cite{powermodes_nxagx} & AGX \textbf{Orin}~\cite{powermodes_orin}\\
\midrule
ARM CPU Architecture & A57 & Carmel & Carmel &  A78AE\\ 
\noalign{\global\arrayrulewidth=0.1pt}\arrayrulecolor{lightgray}\hline
\noalign{\global\arrayrulewidth=0.4pt}\arrayrulecolor{black}
CPU Cores$^\dagger$ & 4 & 6 & 8 & 12\\
\noalign{\global\arrayrulewidth=0.1pt}\arrayrulecolor{lightgray}\hline
\noalign{\global\arrayrulewidth=0.4pt}\arrayrulecolor{black}
CPU Frequency (MHz)$^\dagger$ & 1479 & 1900 & 2265 & 2200\\ \hline
GPU Architecture & Maxwell & Volta & Volta & Ampere\\
\noalign{\global\arrayrulewidth=0.1pt}\arrayrulecolor{lightgray}\hline
\noalign{\global\arrayrulewidth=0.4pt}\arrayrulecolor{black}
CUDA/Tensor Cores & 128/--& 384/48 & 512/64 & 2048/64\\
\noalign{\global\arrayrulewidth=0.1pt}\arrayrulecolor{lightgray}\hline
\noalign{\global\arrayrulewidth=0.4pt}\arrayrulecolor{black}
GPU Frequency (MHz)$^\dagger$ & 921 & 1100 & 1377 & 1300\\ \hline
RAM (GB) & 4 & 8 & 32 & 32\\ \hline
Storage Interfaces & $\mu$SD, USB & $\mu$SD, NVMe, USB & $\mu$SD, eMMC, eSATA, NVMe, USB & $\mu$SD, eMMC, NVMe, USB \\
\noalign{\global\arrayrulewidth=0.1pt}\arrayrulecolor{lightgray}\hline
\noalign{\global\arrayrulewidth=0.4pt}\arrayrulecolor{black}
Memory Frequency (MHz)$^\dagger$ & 1600 & 1600 &  2133 & 3200\\ \hline
Power (W)$^\dagger$ & 10 & 15$^*$ & 65$^\#$ & 60\\ \hline
Price (USD) & $\$129$ & $\$399$ & $\$999$ & $\$1999$  \\
\bottomrule
\multicolumn{5}{L{13cm}}{$^\dagger$~This is the maximum possible value across all power modes.
Actual value depends on the power mode used (Table~\ref{tbl:power}).
\quad 
$^*$ This peak power is for Jetpack release $v4.5.1$ and earlier. 
\quad 
$^\#$ The data sheet does not list the power for the MAXN peak power mode. We report the power adapter rating of $65W$.
}
\end{tabular}
\vspace{-0.15in}
\end{table}",6639
http://arxiv.org/abs/2509.21816v1,2509.21816v1,2025-09-26T03:21:39+00:00,No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials,"Excel is one of the most widely used productivity tools across domains,
offering rich functionality but also overwhelming users with its complexity.
This creates a persistent demand for tutorials to support effective usage.
However, existing tutorials are manually authored by experts, require frequent
updates after each software release, and incur substantial labor costs. Prior
work has not achieved fully automated tutorial generation, since existing
methods still depend on handcrafted operation sequences or example materials.
In this paper, we present the first framework for automatically generating
Excel tutorials directly from natural language task descriptions. Our framework
first instantiates the task. Then a central component of this framework,
Execution Agent, plans and executes the solution in Excel, and collects the
intermediate artifacts required for tutorial construction. These artifacts are
then transformed into both structured Excel documents and video demonstrations.
To build a comprehensive tutorial corpus, we collected 1,559 task descriptions
from real-world scenarios. In addition, we designed a systematic evaluation
framework that integrates assessments from both large language models (LLMs)
and human reviewers. Experimental results show that our framework improves task
execution success rates by 8.5% over state-of-the-art baselines. Moreover, the
generated tutorials demonstrate superior readability and instructional
effectiveness, often approaching or surpassing expert-authored materials.
Importantly, the automated pipeline eliminates manual labor and reduces time
costs to 1/20 of expert authoring, making scalable and high-quality tutorial
generation practical for the first time.","2.1 Spreadsheet Agent
With the rapid advancement of large language models (LLMs), recent spreadsheet agents have increas-
ingly leveraged LLM capabilities to address complex tasks involving spreadsheet data. SheetCopilot
Li et al. [2023] enables stable interaction between LLMs and spreadsheets by defining atomic actions
within a state machine-based task planning framework. SheetAgent Chen et al. [2025] introduces a
modular Planner–Informer–Retriever architecture that effectively addresses long-horizon spreadsheet
tasks through iterative reasoning. SheetMind Zhu et al. [2025] builds a multi-agent system with
2
clearly defined roles, enhancing the robustness and reliability of task execution. Although existing
spreadsheet agents have made progress in automating table-level tasks through code execution, their
operations remain opaque to end users and fail to provide transparent, interface-based learning paths.
2.2 Computer-Using Agents (CUAs)
Recent advances in desktop automation have produced a range of LLM-powered agent systems. UFO
Zhang et al. [2024b] represents one of the earliest multi-agent automation frameworks for Windows,
emphasizing GUI interaction through the integration of UI Automation and visual perception. NA VI
Bonatti et al. [2024], a single-agent baseline from WAA, leverages both screenshots and accessibility
metadata to facilitate GUI understanding. OmniAgent Lu et al. [2024] combines OmniParser for
visual grounding with GPT-based action planning, enabling robust multimodal reasoning. Agent
S Agashe et al. [2024] employs a multi-agent architecture with experience-driven hierarchical
planning, optimized for executing complex, multi-step tasks. Operator OpenAI [2025c], a recent
high-performance CUA from OpenAI, simulates human-like mouse and keyboard operations based
on screenshot inputs. While these systems demonstrate the promise of LLM-driven desktop agents,
they often suffer from shallow OS integration and heavy reliance on brittle visual inputs.
2.3 Automated Tutorial Generation
Prior work has explored semi-automated tutorial generation using pre-existing materials such as user
demonstrations and instructional videos. For instance, MixT Chi et al. [2012] produces mixed-media
tutorials by combining static instructions with video segments derived from user demonstrations.
In a similar vein, an approach Truong et al. [2021] has been proposed to automatically construct
hierarchical tutorials from makeup instructional videos. Other research Grabler et al. [2009] captures
software operations along with screencast recordings and converts them into document-style tutorials
enriched with text descriptions and annotated step images. In the domain of physical tasks, some
approaches Denning et al. [2011], Grossman et al. [2010] produce instructional videos through
semi-automatic editing of creator-annotated single-take or multi-take demonstrations.
In addition to leveraging demonstrations and videos, several studies have explored transforming
static textual content (such as user manuals) into richer, multimedia-based tutorials. HelpViz Zhong
et al. [2021] automatically converts text-based mobile instructions into contextual visual tutorials by
parsing action sequences, simulating interactions on Android emulators, and synthesizing step-by-step
visual assets, leading to higher user preference over plain text. Other systems such as M2V Liu
et al. [2024] and HowToCut Chi et al. [2021] focus on generating instructional videos from manuals
or markdown tutorials, combining NLP, computer vision, and automatic video editing to produce
engaging, easy-to-follow guidance preferred by most users over traditional documentation.",3690
http://arxiv.org/abs/2510.11188v1,2510.11188v1,2025-10-13T09:21:45+00:00,Protein as a Second Language for LLMs,"Deciphering the function of unseen protein sequences is a fundamental
challenge with broad scientific impact, yet most existing methods depend on
task-specific adapters or large-scale supervised fine-tuning. We introduce the
""Protein-as-Second-Language"" framework, which reformulates amino-acid sequences
as sentences in a novel symbolic language that large language models can
interpret through contextual exemplars. Our approach adaptively constructs
sequence-question-answer triples that reveal functional cues in a zero-shot
setting, without any further training. To support this process, we curate a
bilingual corpus of 79,926 protein-QA instances spanning attribute prediction,
descriptive understanding, and extended reasoning. Empirically, our method
delivers consistent gains across diverse open-source LLMs and GPT-4, achieving
up to 17.2% ROUGE-L improvement (average +7%) and even surpassing fine-tuned
protein-specific language models. These results highlight that generic LLMs,
when guided with protein-as-language cues, can outperform domain-specialized
models, offering a scalable pathway for protein understanding in foundation
models.","\subsection{Language Models in Protein}


Protein representation learning with protein language models (PLMs) extends the Transformer to amino-acid strings, producing dense embeddings for property prediction~\cite{hayes2025simulating,brandes2022proteinbert,elnaggar2021prottrans,ProtGO,cao2021tale, chen2024xtrimopglm,chen2024unifying} or generative design~\cite{madani2023large, nijkamp2023progen2, lv2024prollama, ferruz2022protgpt2}. 


Because these models are trained exclusively on amino acid sequences, their outputs remain latent vectors that external classifiers must translate into human-readable function.

To obviate this indirection, protein–language alignment modeling has emerged, which jointly connects sequences with textual descriptions via (i) contrastive objectives mapping proteins and sentences into a shared space~\cite{protst, wu2024proteinclip}, (ii) bioknowledge-augmented pre-training on curated protein–text corpora~\cite{ferruz2022protgpt2, taylor2022galactica, lv2024prollama, BioT5, zhuo2024protllm, liu2024evollama}, or (iii) multi-modal LLMs that graft protein encoders onto frozen language backbones~\cite{liu-etal-2024-prott3, abdine2024prot2text, wang2024protchatgpt, chen2024unifying, ma2025prottex, xiang2024fapm}.

While effective, these approaches entail costly retraining or gradient updates and risk catastrophic forgetting when scaled to larger LLMs~\cite{kirkpatrick2017overcoming,wu2025rethinking}, prompting a shift toward parameter-efficient adaptation.

\subsection{Protein QA Datesets}
Datasets that couple proteins with natural-language annotations have become the empirical bedrock for developing protein–text hybrid systems. At present, two complementary families of corpora dominate the landscape. The first centers on protein captioning: given an amino-acid sequence alone, the objective is to generate a concise textual description. Representative instances include the richly annotated Swiss-Prot collection~\cite{bairoch2000swiss}, the ProteinKG resource~\cite{zhang2022ontoprotein} and ProtDescribe~\cite{xu2023protst}. The second family targets protein question answering: here, both a sequence and a natural-language query are supplied, and the model is required to synthesize an answer grounded in the provided protein. Curated examples span Mol-Instructions~\cite{fang2023mol}, UniProtQA~\cite{luo2024biomedgpt}, ProteinLMBench~\cite{shen2024fine}, VenusX~\cite{tan2025venusxunlockingfinegrainedfunctional} and Protein2Text-QA~\cite{Protein2Text2025}.",2513
http://arxiv.org/abs/2509.12042v1,2509.12042v1,2025-09-15T15:25:26+00:00,FinGEAR: Financial Mapping-Guided Enhanced Answer Retrieval,"Financial disclosures such as 10-K filings present challenging retrieval
problems due to their length, regulatory section hierarchy, and domain-specific
language, which standard retrieval-augmented generation (RAG) models underuse.
We introduce FinGEAR (Financial Mapping-Guided Enhanced Answer Retrieval), a
retrieval framework tailored to financial documents. FinGEAR combines a finance
lexicon for Item-level guidance (FLAM), dual hierarchical indices for
within-Item search (Summary Tree and Question Tree), and a two-stage
cross-encoder reranker. This design aligns retrieval with disclosure structure
and terminology, enabling fine-grained, query-aware context selection.
Evaluated on full 10-Ks with queries aligned to the FinQA dataset, FinGEAR
delivers consistent gains in precision, recall, F1, and relevancy, improving F1
by up to 56.7% over flat RAG, 12.5% over graph-based RAGs, and 217.6% over
prior tree-based systems, while also increasing downstream answer accuracy with
a fixed reader. By jointly modeling section hierarchy and domain lexicon
signals, FinGEAR improves retrieval fidelity and provides a practical
foundation for high-stakes financial analysis.","2.1 Retrieval-Augmented Generation (RAG)
Retrieval-Augmented Generation (RAG) (Lewis
et al., 2021) augments language models by fetch-
ing relevant context from external corpora, re-
ducing the need for full-model fine-tuning (Guu
et al., 2020; Ram et al., 2023). Advanced variants
such as Self-RAG (Asai et al., 2023) and Adap-
tive RAG (Jeong et al., 2024) improve coordina-
tion between retrievers and generators but still use
fixed-size chunks. This makes it hard to preserve
document structure and can introduce drift in long
documents, as seen in long-form QA benchmarks
like ELI5 (Fan et al., 2019). Recent work addresses
context-length limits with longer-context models
(e.g., Transformer-XL (Dai et al., 2019)), retrieval-
aware chunking (Zhong et al., 2025), and studies
of position bias (Liu et al., 2023). These efforts
mainly target input length and do not solve struc-tured retrieval in financial domains.
2.2 Hierarchical and Graph-Based Retrieval
Hierarchical methods such as RAPTOR (Sarthi
et al., 2024) and HiQA (Chen et al., 2024) represent
documents as trees and retrieve recursively from
higher-level summaries. Graph-based systems, in-
cluding GraphRAG (Edge et al., 2024) and Ligh-
tRAG (Guo et al., 2024), model relations between
entities and sections to support multi-hop reason-
ing. In particular, GraphRAG builds local–global
graphs over LLM-extracted entities and commu-
nity summaries and retrieves via community-level
traversal, while LightRAG performs dual-level
query decomposition with lightweight neighbor-
hood expansion over section-aligned segments.
Longtriever (Yang et al., 2023b) targets long-
context retrieval by combining local and global se-
mantics at the block level. These graph approaches
differ from retrieval that uses section hierarchies
(e.g., SEC Items), so we include GraphRAG as a
representative graph baseline for community-level
traversal. Many graph/tree systems also depend on
LLM-generated summaries, which may hallucinate
content (Maynez et al., 2020; Li et al., 2023).
2.3 Financial NLP and Domain-Specific
Retrieval
Financial NLP supports applications such as senti-
ment analysis (Zhang et al., 2023; Wang and Ma,
2024), event prediction (Li et al., 2024; Wang et al.,
2024), and hybrid QA (Chen et al., 2021; Zhu
et al., 2021), often using domain-adapted mod-
els like FinBERT-QA (Yuan et al., 2021) and Fin-
GPT (Yang et al., 2023a). These models improve
semantic understanding but usually assume that
relevant context is provided and therefore lack re-
trieval. DocFinQA (Reddy et al., 2024) evaluates
QA over full filings but relies on an oracle retriever,
leaving retrieval design unaddressed. As a result,
prior work does not fully model retrieval architec-
tures that reflect hierarchical layouts, domain termi-
nology, and section-specific semantics. FinGEAR
addresses this gap by treating structure-aware re-
trieval as a core objective in financial document
understanding.
2.4 Guided and Interpretable Retrieval for
Financial Documents
In high-stakes settings like finance, retrieval should
be both relevant and interpretable because results
support regulatory or analytical decisions (Yu et al.,
2
Financial Terms from FinRAD
Weighted Lexicon MappingFinancial Lexicon-Aware Mapping (FLAM)Semantic ClusteringLexicon Clustering and MappingAssign keyterms to disclosure sections using financial lexicon-tuned embeddings. Score semantic similarity to the Items based on relative frequency across filings.
Item 1,w1= 0.22Item 1A, w2= 0.14Item 7, w3= 0.48…
Structure Extraction
a.Embed item-level content with financial semantics b.Cluster into latent disclosure topics c.Build summary hierarchy over topic clusters d.Construct question-guided tree from summary tree
SEC Disclosure Tree
Parsed 10-K FilingFLAM: Global Knowledge Lexicon MappingLexiconsare mapped to disclosure Itemsand assigned weights (w ∈[0, 1]) based on their relative frequency across filings.Specifically, financial terms (lexicons) are clustered into latent themes using domain-specific embeddings fine-tuned on FinRAD.
Hierarchical ExpansionDual-Tree GenerationDual-Tree Index (Summary + Question)Summary Tree (Disclosure-Level Indexing);Question Tree (Query-Driven Retrieval)Expanded Hierarchies via Financial Topic ClusteringOne disclosure tree is built for each Item, with nodes encoding the semantics of page-level segments. Figure 1: Pre-retrieval. From parsed 10-K filings, FinGEAR performs structure extraction and lexicon mapping
(FLAM). FLAM clusters domain terms and assigns Item weights; topic clustering builds a Summary Tree and a
mirrored Question Tree for each Item.
Identify the query’s closest financial keyword using FLAM, then retrieve disclosure items weighted by lexicon relevance. Financial Keyword Navigation
Query Question
Dual-TreeRanking
RerankedRetrievalsGenerateAnswer
Traverse Summary and Question Trees within each Item to locate both sparse and dense semantically aligned passages. Disclosure-Aware Structure Traversal
Item 1,w1= 0.22Item 1A, w2= 0.14Item 7, w3= 0.64…
“What was JPMorgan Chase & Co.’s CET1 ratio in 2008?”I. Within-Group (FLAM)II. Within-Item (tree traversal)Cross-rank top-k candidates from both trees by relevance, then merge across items to select highly relevant evidence.
Figure 2: In-retrieval. FLAM allocates the budget across Items (Within-Group). Within each Item, the Summary
and Question Trees are traversed (Within-Item). Candidates are jointly reranked and merged across Items. Example
query: CET1 ratio in 2008.
2024). Flat, dense-only pipelines can obscure why
passages were selected. Structured methods (hier-
archical and graph-based) reviewed above improve
traceability by encoding document structure and
relations, and they typically outperform flat chunk-
ing in both quality and transparency. However,
most remain domain-agnostic. For 10-Ks, where
a standardized section layout and stable terminol-
ogy are available, integrating domain signals (e.g.,
a finance lexicon and disclosure Item hierarchy)
can further align retrieval with analyst intent. Fin-
GEAR follows this principle by combining lexicon-
guided global navigation with Item-aligned hierar-
chical indexing, providing interpretable, section-
aware evidence selection tailored to financial dis-
closures.",6279
http://arxiv.org/abs/2510.06919v1,2510.06919v1,2025-10-08T11:52:39+00:00,Bayesian Nonparametric Dynamical Clustering of Time Series,"We present a method that models the evolution of an unbounded number of time
series clusters by switching among an unknown number of regimes with linear
dynamics. We develop a Bayesian non-parametric approach using a hierarchical
Dirichlet process as a prior on the parameters of a Switching Linear Dynamical
System and a Gaussian process prior to model the statistical variations in
amplitude and temporal alignment within each cluster. By modeling the evolution
of time series patterns, the method avoids unnecessary proliferation of
clusters in a principled manner. We perform inference by formulating a
variational lower bound for off-line and on-line scenarios, enabling efficient
learning through optimization. We illustrate the versatility and effectiveness
of the approach through several case studies of electrocardiogram analysis
using publicly available databases.","Probabilistic approaches to clustering time-series data have been widely explored, particularly through mixture models \cite{smyth1996, biernacki2000, zhong2003, bach2004}. These methods assume that the data is generated by a mixture of distributions with predefined forms, such as Gaussian, HMMs, or graphical models. With the aim of reducing the strong modeling assumptions, a nonparametric approach is proposed by \cite{kahlegi2016} in exchange for assuming stationary ergodicity over the data distribution. While effective for static data, these methods often fail to account for the evolving nature of time series, where distributional shifts can reduce precision, and where a fixed number of clusters is not flexible enough to deal with the emergence of new clusters. Trying to reduce this variability between observations, alignment based on GPs is developed in \cite{kazlauskaite_2019}, resulting in better clustering within the latent space. Also, works such as \cite{chiu2022, kolter2007} address this variability by detecting and reacting to concept drift. In contrast, the present approach integrates this dynamic transformation directly into the model, enabling clusters to adapt independently while parameterizing the evolving distributions over time.

Bayesian nonparametrics have provided a natural solution to overcome the limitation of fixed cardinality for the state space. In \cite{qi2007}, authors propose a HDP-HMM model for clustering fixed-length segments of a time series, with applications to music analysis. In \cite{chakraborty2023}, an infinite hidden Markov model (iHMM) on a
state space of multi-dimensional vector fields described by GPs is proposed to extract traffic patterns from multi-vehicle trajectories. In \cite{fox_2011}, HDP-SLDS has proven effective for modeling temporal dynamics in time-series data, with applications to segmentation of dancing honey bee trajectories and learning changes in the volatility of the IBOVESPA stock exchange. A sticky version of the original model increases the expected probability of self-transition, thus avoiding unrealistic fast changes between linear regimes caused by the intrinsic variability of raw data. The present proposal avoids this problem through pattern abstraction on the time-series data, and therefore the segmentation is performed using time-series segments as samples, increasing the stability of the required dynamics. 
    
Procedures for efficient learning of nonlinear state-space models and nonlinear mappings from latent states to observations have been proposed in \cite{damianou11, frigola14}, by placing proper GP priors. These methods exhibit good performance in applications such as reconstructing human motion capture data and high-dimensional video sequences, but they need more complex non-conjugate inference and learning algorithms. Notably, in \cite{alvarez09} the latent force model is introduced as a combination of mechanistic modeling principles with non-parametric data-driven components, using GPs as nonparametric models for unknown input functions in physical models, which are formulated as ordinary differential equations (ODEs). Applications in modeling human motion capture data, describing the time-dependent expression levels of genes, and predicting heavy metal concentrations in geostatistics show the versatility of the proposal.
\cite{hensman2015} proposed a Dirichlet Process Gaussian Process (DP-GP) model for clustering structured time-series data, effectively capturing both intra-group and inter-group variability. However, their approach focuses on clustering from a static perspective, without explicitly addressing the temporal dynamics underlying the time-series data.
Importantly, an extension to nonlinear (ODEs) and significant improvement in computational complexity for latent force modeling has been achieved in \cite{hartikainen2011, hartikainen2012}. However, their application to dynamic clustering with nonlinear mappings to observations given by GP priors faces identifiability and scalability issues due to the nested dependencies over complex GP structures.",4111
http://arxiv.org/abs/2509.15233v1,2509.15233v1,2025-09-17T02:50:54+00:00,Video2Roleplay: A Multimodal Dataset and Framework for Video-Guided Role-playing Agents,"Role-playing agents (RPAs) have attracted growing interest for their ability
to simulate immersive and interactive characters. However, existing approaches
primarily focus on static role profiles, overlooking the dynamic perceptual
abilities inherent to humans. To bridge this gap, we introduce the concept of
dynamic role profiles by incorporating video modality into RPAs. To support
this, we construct Role-playing-Video60k, a large-scale, high-quality dataset
comprising 60k videos and 700k corresponding dialogues. Based on this dataset,
we develop a comprehensive RPA framework that combines adaptive temporal
sampling with both dynamic and static role profile representations.
Specifically, the dynamic profile is created by adaptively sampling video
frames and feeding them to the LLM in temporal order, while the static profile
consists of (1) character dialogues from training videos during fine-tuning,
and (2) a summary context from the input video during inference. This joint
integration enables RPAs to generate greater responses. Furthermore, we propose
a robust evaluation method covering eight metrics. Experimental results
demonstrate the effectiveness of our framework, highlighting the importance of
dynamic role profiles in developing RPAs.","2.1 Static Role Playing
ChatHaruhi (Li et al., 2023a) provides a dataset
of over 54ksimulated dialogues for 32characters
spanning Chinese, English, and anime. Charac-
terGLM (Zhou et al., 2024) allows for personal-
izing a diverse range of agent personas and so-
cial agents through customizable attributes and
behaviors. CharacterLLM (Shao et al., 2023)
builds a dataset detailing specific character ex-
periences, then fine-tunes a base model with
2
the dataset to achieve target character portrayal.
RoleLLM (Wang et al., 2024a) improves LLM role-
playing via a multi-component framework (e.g.,
role profile construction, role-GPT, role-bench).
Ditto (Lu et al., 2024) introduces a self-alignment
method to enhance LLM role-playing capabilities
through knowledge augmentation and dialogue sim-
ulation. MMrole (Dai et al., 2024)introduces the
concept of multimodal role-playing agents and of-
fers a comprehensive framework for their develop-
ment and evaluation. RoleMRC (Lu et al., 2025)
provides a fine-grained composite benchmark for
role-playing and instruction-following, revealing
activation patterns linked to these distinct abilities.
CoSER (Wang et al., 2025b) provides a dataset
comprising 29,798 authentic conversations and
comprehensive data from 771 renowned books and
proposes a given-circumstance acting method for
training and evaluating role-playing LLMs.
2.2 Video Understanding
GPT4Video (Wang et al., 2024e) proposes a unified
framework for video understanding and generation
via pre-trained model integration and develops a
simple text-only fine-tuning method for instruction
following and safety alignment. LongVLM (Weng
et al., 2024) introduces a VideoLLM for long-term
video understanding, achieving affordability via
segment decomposition, feature extraction, token
merging, and global semantics. Video-LLaV A (Lin
et al., 2024) maps visual signals to the language fea-
ture space to achieve unified visual representations,
introducing a method for aligning features prior
to projection. VideoAgent (Wang et al., 2024c)
proposes an agent-based system that iteratively ex-
tracts and compiles key information for question
answering, using vision-language models for visual
translation and retrieval. VidRecap (Islam et al.,
2024) proposes a hierarchical caption generation
method that creates CLIP captions, segment de-
scriptions, and video summaries, trained using a
coarse-to-fine approach to learn the structure of
video. LongVU (Shen et al., 2024) preserves frame
information for lengthy videos by compressing to-
kens based on similarity and selecting relevant vi-
sual tokens for text queries. InternVideo2.5 (Wang
et al., 2025c) introduces a length-adaptive token
approach to process videos, integrating visual per-
ception with MLLM for fine-grained analysis.
Documentary
Vlog
LiveFigure 2: The video types and examples of our dataset.
2.3 Multimodal Large Language Model
CLIP (Radford et al., 2021) achieves cross-modal
understanding and unified representation by apply-
ing contrastive learning to unlabeled image-text
pairs, eliminating the need for task-specific annota-
tion. Flamingo (Alayrac et al., 2022) inserts new
gated cross-attention layers into the LLMs to inject
visual features and pre-trains the new layers on bil-
lions of image-text pairs. Emu (Sun et al., 2024)
extends the approach of Flamingo (Alayrac et al.,
2022) by integrating additional modalities to model
generation and the corresponding training corpus.
BLIP-2 (Li et al., 2023b) introduces Q-Former
for visual and linguistic representation learning,
achieving zero-shot image-text generation and
strong performance on visual language tasks with
more efficient parameterization. InternVL (Chen
et al., 2024c) presents the first alignment of a
large-scale vision encoder with LLMs and intro-
duces a progressive image-text alignment strategy,
enabling efficient training of large-scale vision-
language foundation models. InstructBLIP (Dai
et al., 2023) introduces an instruction-aware feature
extraction method for vision-language instruction
tuning, significantly enhancing multimodal model
performance. LLaV A-NeXT (Li et al., 2024) en-
hances visual detail capture via improved input
image resolution and refines its data mix through
adapted visual instructions.
3",4284
http://arxiv.org/abs/2509.20666v1,2509.20666v1,2025-09-25T01:58:46+00:00,Understanding Mode Switching in Human-AI Collaboration: Behavioral Insights and Predictive Modeling,"Human-AI collaboration is typically offered in one of two of user control
levels: guidance, where the AI provides suggestions and the human makes the
final decision, and delegation, where the AI acts autonomously within
user-defined constraints. Systems that integrate both modes, common in robotic
surgery or driving assistance, often overlook shifts in user preferences within
a task in response to factors like evolving trust, decision complexity, and
perceived control. In this work, we investigate how users dynamically switch
between higher and lower levels of control during a sequential decision-making
task. Using a hand-and-brain chess setup, participants either selected a piece
and the AI decided how it moved (brain mode), or the AI selected a piece and
the participant decided how it moved (hand mode). We collected over 400
mode-switching decisions from eight participants, along with gaze, emotional
state, and subtask difficulty data. Statistical analysis revealed significant
differences in gaze patterns and subtask complexity prior to a switch and in
the quality of the subsequent move. Based on these results, we engineered
behavioral and task-specific features to train a lightweight model that
predicted control level switches ($F1 = 0.65$). The model performance suggests
that real-time behavioral signals can serve as a complementary input alongside
system-driven mode-switching mechanisms currently used. We complement our
quantitative results with qualitative factors that influence switching
including perceived AI ability, decision complexity, and level of control,
identified from post-game interview analysis. The combined behavioral and
modeling insights can help inform the design of shared autonomy systems that
need dynamic, subtask-level control switches aligned with user intent and
evolving task demands.","We situate our work at the intersection of mixed-initiative
interaction, user preferences for control, and adaptive au-
tonomy, drawing on research that informs how human–AI
systems manage control sharing over time.
Mixed-Initiative Systems
Mixed-initiative systems aim to balance control between
humans and AI by dynamically adjusting autonomy in re-
sponse to task demands and user behavior (Horvitz 1999;
Bradshaw et al. 2004). They emphasize agent-driven adap-
tation based on internal indicators like task criticality or in-
ferred user ability (Fiore, Clodic, and Alami 2016; Salikut-
luk et al. 2024; Hauptman, Flathmann, and McNeese 2024),
but such strategies may misalign with user preferences, lead-
ing to issues with transparency, predictability, and coordina-
tion (Tambe et al. 2000; Hauptman et al. 2023).
Two common collaboration models reflect this tradeoff. In
machine-in-the-loopsystems, users maintain control while
receiving AI suggestions, an approach commonly adopted in
domains like healthcare, legal decision-making, writing, and
software development (Clark et al. 2018; Green and Chen
2019; Kleinberg et al. 2018; Roemmele and Gordon 2015;
Gero, Liu, and Chilton 2022; Yuan et al. 2022). This mode is
often preferred in contexts requiring fairness, transparency,
or subjective judgment (Lubars and Tan 2019), though out-
comes may be skewed by a user’s algorithm aversion or
overreliance (Bockstedt and Buckman 2025; Jones-Jang and
Park 2023; Klingbeil, Gr ¨utzner, and Schreck 2024).
In contrast,human-in-the-loopsystems delegate task ex-
ecution to the AI, with humans monitoring or refining out-
puts, an approach explored in tasks like image generation
and portfolio optimization (Wu et al. 2018; Evirgen and
Chen 2022; Buckley et al. 2021). While efficient, users of-
ten delegate sub-optimally due to poor mental models, trust
asymmetries, or biases (Pinski, Adam, and Benlian 2023;
F¨ugener et al. 2022; Milewski and Lewis 1997). Survey
studies highlight additional influences such as task difficulty,
risk, and motivation (Lubars and Tan 2019).
Most prior work has focused on static or hypothetical
preferences. Few studies have investigated how control pref-
erences evolve dynamically during real-time human–AI col-
laboration. Our work addresses this gap by modeling control
switching behavior using data from hand-and-brain chess.User Preferences
A growing body of work explores the factors that shape
delegation preferences, often through surveys and hypothet-
ical scenarios (Lubars and Tan 2019; Cvetkovic and Bit-
tner 2022; Jin and Uchida 2024; Svikhnushina et al. 2023).
Trust consistently emerges as a key predictor of delega-
tion behavior, alongside task difficulty, motivation, and per-
ceived risk (Lubars and Tan 2019; Cvetkovic and Bittner
2022). Recent longitudinal studies show increasing willing-
ness to delegate as AI capabilities improve. Jin and Uchida
(2024) report that users are more inclined to delegate dif-
ficult tasks, especially when trust and motivation are high.
Similar patterns are seen in daily interactions with digital
assistants (Svikhnushina et al. 2023). Beyond these factors,
cognitive framing also shapes collaboration behavior. Gur-
ney, Miller, and Pynadath (2023) find that biases such as risk
aversion and automation over-reliance affect user effort and
performance. These findings underscore the need to study
preferences in behaviorally realistic settings.
However, most prior work captures stated preferences,
which may not reflect real-time decision-making in dynamic
contexts (Viney, Lancsar, and Louviere 2002; De Corte,
Cairns, and Grieve 2021). Our study addresses this limita-
tion by observing revealed preferences in a live, sequential
decision-making task.
User Modeling and Autonomy Adaptation
Adaptive autonomy typically involves AI-driven control
modulation based on system-level indicators like confidence
or workload (Amershi et al. 2019; Roehr and Shi 2010;
Hauptman, Flathmann, and McNeese 2024). While effec-
tive for efficiency, such methods often lack explicit models
of user control preferences, and rarely incorporate real-time
behavioral signals.
In parallel, user modeling work has focused on predict-
ing trust or intent in interactive systems (Guo et al. 2022;
Sun et al. 2017; Kraus, Wagner, and Minker 2021), but these
models are generally static and do not address moment-to-
moment shifts in control needs. Recent work by Gurney,
Miller, and Pynadath (2023) highlights how framing effects
shape trust and reliance, further emphasizing the need for
user-centered adaptation. Building on the four-level delega-
bility framework by Lubars and Tan (2019), our setup iso-
lates a decision boundary between object-level and action-
level control, enabling the capture of fine-grained control
preferences within a task. This extends prior work that pri-
marily focused on task-level autonomy choices.",4908
http://arxiv.org/abs/2510.06396v1,2510.06396v1,2025-10-07T19:23:53+00:00,Adaptive Protein Design Protocols and Middleware,"Computational protein design is experiencing a transformation driven by
AI/ML. However, the range of potential protein sequences and structures is
astronomically vast, even for moderately sized proteins. Hence, achieving
convergence between generated and predicted structures demands substantial
computational resources for sampling. The Integrated Machine-learning for
Protein Structures at Scale (IMPRESS) offers methods and advanced computing
systems for coupling AI to high-performance computing tasks, enabling the
ability to evaluate the effectiveness of protein designs as they are developed,
as well as the models and simulations used to generate data and train models.
This paper introduces IMPRESS and demonstrates the development and
implementation of an adaptive protein design protocol and its supporting
computing infrastructure. This leads to increased consistency in the quality of
protein design and enhanced throughput of protein design due to dynamic
resource allocation and asynchronous workload execution.","With the advent of deep learning models that enable high-level insight into protein sequence and structure, several other advances have been made in developing genetic algorithms for design generation.

The recently proposed EvoPro protocol achieves this through iterative runs of sequence generation—using either ProteinMPNN or random mutagenesis—and AlphaFold, which employs single-sequence structure prediction mode to accelerate inference\cite{gnkrk23}. 
EvoPro shows significant advancements in coupling deep learning models for efficient sequence space exploration but faces two limitations. Eliminating MSA generation restricts EvoPro to designing binders of a specific size\cite{apco21}. As the designed domain increases in complexity, AlphaFold2 needs to utilize evolutionary information to ensure an accurately predicted structure\cite{e23}. Furthermore, AlphaFold2 is a powerful classifier, distinguishing between high- and low-quality binders\cite{mdbabb23,dcso24}. Allowing AlphaFold2 to utilize evolutionary information in its constructed MSA improves its predictive abilities and directs IMPRESS toward effectively guiding design exploration for more potent binders.

The newly developed MProt-DPO protocol is another technique that enables efficient exploration of the protein design landscape \cite{dhbo24}. Sequences are generated with a protein LLM, then ranked and sorted into preference pairs to fine-tune the original model \cite{laro23}\cite{rsmo24}.

MProt-DPO achieves remarkable efficiency but is restricted to purely sequence-based models. Therefore, design generation is never directly conditioned on the protein structure, but must rely on downstream MD simulations.

The IMPRESS framework allows any sequence generation method to be plugged into the design pipeline, enabling both LLMs and graph-based models to fully exploit the rich functional information available in protein structures.",1920
http://arxiv.org/abs/2510.09848v1,2510.09848v1,2025-10-10T20:24:20+00:00,Cell Instance Segmentation: The Devil Is in the Boundaries,"State-of-the-art (SOTA) methods for cell instance segmentation are based on
deep learning (DL) semantic segmentation approaches, focusing on distinguishing
foreground pixels from background pixels. In order to identify cell instances
from foreground pixels (e.g., pixel clustering), most methods decompose
instance information into pixel-wise objectives, such as distances to
foreground-background boundaries (distance maps), heat gradients with the
center point as heat source (heat diffusion maps), and distances from the
center point to foreground-background boundaries with fixed angles (star-shaped
polygons). However, pixel-wise objectives may lose significant geometric
properties of the cell instances, such as shape, curvature, and convexity,
which require a collection of pixels to represent. To address this challenge,
we present a novel pixel clustering method, called Ceb (for Cell boundaries),
to leverage cell boundary features and labels to divide foreground pixels into
cell instances. Starting with probability maps generated from semantic
segmentation, Ceb first extracts potential foreground-foreground boundaries
with a revised Watershed algorithm. For each boundary candidate, a boundary
feature representation (called boundary signature) is constructed by sampling
pixels from the current foreground-foreground boundary as well as the
neighboring background-foreground boundaries. Next, a boundary classifier is
used to predict its binary boundary label based on the corresponding boundary
signature. Finally, cell instances are obtained by dividing or merging
neighboring regions based on the predicted boundary labels. Extensive
experiments on six datasets demonstrate that Ceb outperforms existing pixel
clustering methods on semantic segmentation probability maps. Moreover, Ceb
achieves highly competitive performance compared to SOTA cell instance
segmentation methods.","methods [45]–[51], which utilize tree-based structures to solve
instance segmentation problems. These methods generate over-
segmented components, and then perform final instance seg-
mentation by clustering the components. Unlike our method,
such methods often produce tree-like structures without DL
networks. For example, a tree can be generated from super-
pixels [45], [46]; after the “leaf” regions of the initial over-
segmented candidate regions are obtained, a tree structure
is built by iteratively merging similar super-pixels, until a
pre-specified stopping criterion is met. GP-S3Net [52] used
density-based spatial clustering of applications with noise
(DBSCAN) [53] to produce over-segmented candidates, and
a graph neural network (GNN) model was used to predict the
label of each edge. Note that these methods make final deci-
sions based on node features, which make modeling geometric
features of instance regions difficult. In contrast, our method
directly utilizes boundary features for boundary classification,
which are generally easier to identify and classify.
III. METHODOLOGY
In this section, we present our Ceb framework for cell
instance segmentation. Fig. 1 gives an overview of our frame-
work, which consists of five main steps. (1) Seed generation
(Section III-A): We first generate instance seeds from seman-
tic segmentation probability maps. (2) Boundary generation
(Section III-B): Given the generated seeds and probability
maps, we employ a revised Watershed algorithm to generate
possible cell boundaries and the regions enclosed by these
boundaries. Thus, the cell instance segmentation problem
becomes a boundary selection problem. (3) Boundary label
assignment (Section III-C): To obtain boundary labels (true or
false) for the training stage, we build an optimized matching
model between ground truth instance masks and the divided
regions to attain true regions. The corresponding boundaries",1934
http://arxiv.org/abs/2510.07978v1,2510.07978v1,2025-10-09T09:11:38+00:00,VoiceAgentBench: Are Voice Assistants ready for agentic tasks?,"Large-scale Speech Language Models (SpeechLMs) have enabled voice assistants
capable of understanding natural spoken queries and performing complex tasks.
However, existing speech benchmarks primarily focus on isolated capabilities
such as transcription, or question-answering, and do not systematically
evaluate agentic scenarios encompassing multilingual and cultural
understanding, as well as adversarial robustness. To address this, we introduce
VoiceAgentBench, a comprehensive benchmark designed to evaluate SpeechLMs in
realistic spoken agentic settings. It comprises over 5,500 synthetic spoken
queries, including dialogues grounded in Indian context, covering single-tool
invocations, multi-tool workflows, multi-turn interactions, and safety
evaluations. The benchmark supports English, Hindi, and 5 other Indian
languages, reflecting real-world linguistic and cultural diversity. We simulate
speaker variability using a novel sampling algorithm that selects audios for
TTS voice conversion based on its speaker embeddings, maximizing acoustic and
speaker diversity. Our evaluation measures tool selection accuracy, structural
consistency, and the correctness of tool invocations, including adversarial
robustness. Our experiments reveal significant gaps in contextual tool
orchestration tasks, Indic generalization, and adversarial robustness, exposing
critical limitations of current SpeechLMs.","\textbf{LLM Agent Benchmarks.} Interest in evaluating agentic LLMs has grown with advances in their reasoning and decision making capabilities. ToolBench \citep{qin2024toolllm} evaluates models’ ability to invoke external APIs across diverse real-world tasks, while ToolQA \citep{zhuang2023toolqa} assesses LLMs’ use of external tools for question answering via a scalable, automated dataset curation process. Berkeley Function Calling Leaderboard (BFCL) \citep{patil2025the} emphasizes precise API generation across domains and robustness to both single and multiple function calls, and NESTful \citep{basu2025nestfulbenchmarkevaluatingllms} focuses on nested sequences of API calls, where outputs of one call feed into the next. API-Bank and ToolTalk \citep{li2023apibank, farn2023tooltalkevaluatingtoolusageconversational} target multi-turn, dialogue-driven tool-use scenarios, testing sequential API planning and interaction. Tau-bench \citep{yao2025taubench} simulates dynamic conversations with domain-specific tools and policies to evaluate adherence to task rules. AgentHarmBench \citep{andriushchenko2025agentharm} and DoomArena \citep{boisvert2025doomarena} focus on safety and adversarial robustness, testing susceptibility to harmful or unsafe actions. Despite this progress for LLMs, no speech benchmark explicitly evaluates SpeechLMs in such realistic, agentic, and safety-critical settings, underscoring the need for specialized evaluation frameworks.


\textbf{Speech Datasets and Benchmarks.}  Large-scale datasets such as LibriSpeech \citep{7178964}, CommonVoice \citep{ardila-etal-2020-common}, and MuST-C \citep{di-gangi-etal-2019-must} have been foundational for advancing automatic speech recognition (ASR) and speech translation (AST). IndicST \citep{11011192} and Lahaja \citep{javed24_interspeech} extend these tasks to cover diverse Indic speech data. Evaluation suites like SUPERB \citep{yang21c_interspeech} and SLUE \citep{shon-etal-2023-slue} standardize the assessment of tasks such as intent classification, named entity recognition, and slot filling, with IndicSUPERB \citep{javed2022indicsuperbspeechprocessinguniversal} further supporting Indic languages. However, these benchmarks primarily target simpler tasks like transcription, translation, NER and do not assess reasoning or decision-making over spoken content. To address this gap, recent work has begun exploring reasoning in the audio domain. Audio-CoT \citep{ma2025audiocotexploringchainofthoughtreasoning} introduces chain-of-thought prompting for structured multistep inference on speech input, while MMAU \citep{sakshi2025mmau} provides a large-scale benchmark of 10k audio clips covering 27 reasoning skills, such as temporal reasoning and causal inference, in speech, music, and environmental sounds. AIR-Bench \citep{yang-etal-2024-air} and AudioBench \citep{DBLP:journals/corr/abs-2406-16020} extend the scope to open-ended instruction following on various types of audio and speech, whereas VoiceBench \citep{DBLP:journals/corr/abs-2410-17196} emphasizes robustness and generalization by converting text instruction into spoken form with real-world noise and speaker variation. More recently, SpeechR \citep{yang2025speechrbenchmarkspeechreasoning} directly targets high-level reasoning on speech, focusing on logical deduction, and commonsense problem solving.
We also provide an extended discussion of related work on speech models in Appendix \ref{appendix:related_work_speech_models}.",3491
http://arxiv.org/abs/2509.16032v1,2509.16032v1,2025-09-19T14:36:54+00:00,A Matter of Height: The Impact of a Robotic Object on Human Compliance,"Robots come in various forms and have different characteristics that may
shape the interaction with them. In human-human interactions, height is a
characteristic that shapes human dynamics, with taller people typically
perceived as more persuasive. In this work, we aspired to evaluate if the same
impact replicates in a human-robot interaction and specifically with a highly
non-humanoid robotic object. The robot was designed with modules that could be
easily added or removed, allowing us to change its height without altering
other design features. To test the impact of the robot's height, we evaluated
participants' compliance with its request to volunteer to perform a tedious
task. In the experiment, participants performed a cognitive task on a computer,
which was framed as the main experiment. When done, they were informed that the
experiment was completed. While waiting to receive their credits, the robotic
object, designed as a mobile robotic service table, entered the room, carrying
a tablet that invited participants to complete a 300-question questionnaire
voluntarily. We compared participants' compliance in two conditions: A Short
robot composed of two modules and 95cm in height and a Tall robot consisting of
three modules and 132cm in height. Our findings revealed higher compliance with
the Short robot's request, demonstrating an opposite pattern to human dynamics.
We conclude that while height has a substantial social impact on human-robot
interactions, it follows a unique pattern of influence. Our findings suggest
that designers cannot simply adopt and implement elements from human social
dynamics to robots without testing them first.","Relevant previous studies include HRI studies that explored the impact of a robot's height and compliance with robots' requests. 

\subsection{The impact of a robot's height}

Previous studies in HRI have shown that a robot’s height can influence the interaction dynamics and the way the robot is perceived \cite{rae2013influence, hiroi2008bigger, hiroi2016height, walters2009preferences, joosse2021appearance, samarakoon2022review}. While some of these studies indicated effects similar to those observed in human-human interactions \cite{rae2013telepresence, hiroi2016height}, others showed opposite patterns \cite{joosse2021appearance, walters2009preferences}. Among the studies that indicated similar effects to those observed in human-human interactions, Rae et al. (2013) studied compliance to requests presented by operators of telepresence robots. They tested whether the robot's height would impact the perceived authority and persuasiveness of the person operating it. They found that when the telepresence robot was shorter than the participants, its operators' authority was lower, and the participants reported feeling more dominant in the conversation \cite{rae2013telepresence}. Similarly, Walters et al. (2009) found that taller humanoid robots were perceived as more conscientious, while short humanoid robots were viewed as more neurotic and less conscientious \cite{walters2009preferences}, replicating the tendency to attribute positive characteristics to taller individuals. 

Other studies suggested that height may have an opposite effect to that found in human-human interactions, with people showing clear preferences for short robots. For example, Hiroi and Ito (2016) found that participants preferred to converse with a short robot, specifically with a robot that did not reach their eye level \cite{hiroi2016height}. Similarly, Joosse et al. (2021) found that shorter robots were perceived as safer and more comfortable to interact with. This suggests that humans in HRI may prioritize emotional safety and comfort over authority, which contrasts with human interactions where height positively correlates with competence and status \cite{joosse2021appearance}.

These studies indicate that a robot's height can impact the interactions with humans, the robot's perception, and people's preferences. We extend this line of studies by focusing on the impact of a non-humanoid robot's height and its influence on participants' willingness to comply with the robot's request. We tested whether, as in human-human interactions, a taller robot would lead to increased compliance—or whether the specific case of a non-humanoid robot would result in a different pattern. 

\subsection{Compliance with robots' requests} 

Several studies have explored human tendencies to comply with robots' requests \cite{lee2016role, nielsen2022prosocial, erel2024power}. The robot’s emotional behavior \cite{kuhnlenz2018effect, shiomi2017hug}, its ability to align with socially appropriate behaviors \cite{connolly2020prompting}, its social role \cite{saunderson2021persuasive}, its persuasion strategy \cite{lee2019robotic}, and its appearance \cite{kim2014effect} were all shown to encourage compliance with its requests \cite{connolly2020prompting, martin2020investigating, zaga2017gotta}. For example, Moshkina (2012) indicated that when a robot expressed affect, particularly through nonverbal expressions of negative mood such as nervousness and fear, it enhanced participants’ compliance with evacuation requests, leading to earlier and faster responses \cite{moshkina2012improving}. Similarly, Wills et al. (2016) found that participants were more compliant with a donation request when it came from a robot that followed human-like social norms, demonstrating appropriate gaze and facial expression cues, compared to a robot that did not exhibit social responsiveness to its surroundings \cite{wills2016socially}. Another aspect of HRI that impacts people's willingness to comply with a robot's request is its social role. Robots framed as peers were shown to elicit more compliance than those framed as supervisors. For example, Saunderson and Nejat (2021) found that participants were more willing to follow a peer robot's requests in comparison to a robot framed as an authority figure, suggesting that social framing may also shape compliance to robots \cite{saunderson2021persuasive}.

These studies indicate that various factors influence people's tendency to comply with a robot's request. We further explored whether robotic design characteristics can impact social dynamics and enhance participants' compliance. Specifically, we evaluated the impact of height in interactions with a highly non-humanoid robot.",4735
http://arxiv.org/abs/2509.20106v1,2509.20106v1,2025-09-24T13:31:03+00:00,Investigating the Effect of Prior Exposure and Fidelity on Quality and Realism Perception of VR Digital Twins,"This study explores how prior exposure to physical objects influences the
quality and realism perception of Digital Twins (DT) with varying levels of
fidelity in Virtual Reality (VR). In a mixed experimental design, 24
participants were divided into two equal groups: an exposure group, in which
members were shown physical objects before inspecting and rating their replicas
in VR, and a control group without prior knowledge. Three objects were
presented, each under four fidelity conditions with varying texture resolution
and geometric detail. Participants rated perceived quality and realism through
in-VR self-reports. Statistical analysis revealed that texture resolution
significantly affected realism and quality perception, whereas geometric detail
only influenced quality ratings. Investigating the between-factor, no
significant effect of exposure on quality and realism perception was found.
These findings raise important questions about the cognitive relationship
between physical objects and their digital counterparts and how fidelity
influences the perception of DTs in VR.","Concepts.In computer graphics, fidelity, visual realism, and
photorealism are regarded as similar concepts [ 7]. According to
Ferwerda, an image is photorealistic if it produces the same visual
response as looking at a scene in the analog world [ 9]. In the context
of simulation, the term fidelity goes back to Gerathewohl, who
defined it as ""the degree to which a device accurately produces a
specific effect"". [ 11]. In Mixed Reality (MR), Milgram and Kishino
defined the term Reproduction Fidelity as ""the quality with which
the synthesizing display is able to reproduce the actual or intended
images of the objects being displayed"" [ 26]. McMahan later made
a distinction between display fidelity (reproduction of real-world
sensory stimuli) and interaction fidelity (reproduction of real-world
interactions) [ 25]. In a recent publication, Bonfert et al. present
a comprehensive taxonomy differentiating between eight types
of fidelity [ 3]. The term visual realism is often used in a similar
context. Slater et al. distinguish visual realism between geometric
realism (geometric fidelity) and illumination realism (lightning
fidelity) [ 43]. Visual realism, however, is regarded as an objective
descriptor, similar to fidelity, in contrast to perceived realism, which
is regarded as the subjective judgment of realism [ 46]. Another
related subjective concept is the concept of presence in virtual or
simulated environments. Articulated by Slater as a combination
of place illusion (the sense of being there) and plausibility illusion
(the sense that what is happening is real) [ 42]. Despite a conceptual
relationship, higher visual realism does not necessarily lead to a
Investigating the Effect of Prior Exposure and Fidelity on Quality and Realism Perception of VR Digital Twins
greater sense of presence [ 22]. However, perceived realism was
proposed as one part of a two-dimensional construct of presence
together with ""being there"" [ 46]. Finally, Gonçalves et al. proposed
a new theoretical model using the term subjective realism, which
they clearly distinguish from objective realism [ 12]. In their model,
they also draw a connection to presence. Quality or experienced
quality in the context of immersive media is defined as ""a result
of a comparison and judgment process, in which the perceived
quality features, resulting from a perception and reflection process
triggered by a physical signal, are compared to the desired quality
features underlying the user’s expectations""[ 31]. Visual realism and
quality are interconnected, and Fan et al. referred to the quality
evaluation of computer graphics as part of predicting visual realism
[7]. In this research, we treat realism and quality as subjective
factors, referring to them as perceived realism and perceived quality
throughout, while the objective properties of the virtual world are
described in terms of fidelity. The distinction between realism and
quality perception is that high-quality visuals do not necessarily
have to be realistic (e.g., stylized graphics, fantastic worlds).
Measurements.To measure perceived realism, researchers mostly
rely on subjective measurements through self-reports, sometimes
supported by complementary physiological measurements [ 14].
Schmied-Kowartzki et al. proposed the Visual Realism Classifica-
tion Scale (VRCS), a comprehensive scale for measuring the visual
realism of 3D models in MR, distinguishing between the factors of
Lighting, Reflection, Texture, Structure, Form, and Internal Con-
sistency [ 38]. Most commonly, perceived realism is assessed as
part of the Igroup Presence Questionnaire (IPQ), a standardized
questionnaire for measuring presence, as it is one of its sub-scales
[14,18]. Quality ratings have separate methodologies from the field
of computer vision [ 45]. Nehmé et al. compared three subjective
methods from the field of image processing and applied them to
the quality assessment of 3D graphics in VR: Category Rating with
Hidden Reference (ACR-HR), Double Stimulus Impairment Scale
(DSIS), and Subjective Assessment Methodology for Video Quality
(SAMVIQ) [ 29]. Their findings suggest that DSIS, in which partic-
ipants are exposed to two 3D models side by side and asked to
rate the quality degradation, yielded the most promising results.
As fidelity is objective, it can only be measured through objective
assessment. One approach is an add-on for the 3D software Blender,
developed to generate a level of detail (LOD) score for 3D scenes
that distinguishes between geometric and texture fidelity [33].
User Studies.While there is a lack of research on the relation-
ship between fidelity and strictly perceived realism, several studies
have explored the relationship between 3D object fidelity and other
user experience metrics, predominantly presence. Hvass et al. con-
ducted a study using a video game as a stimulus and tested it with
varying degrees of geometric and texture fidelity [ 17]. The results
of self-reports and physiological measurements showed that partic-
ipants rated a higher sense of presence and fear responses in scenes
with higher visual realism. Gutiérrez et al. assessed the quality
metrics for 3D models under different lightning conditions in MR
[16]. The models were presented with varying levels of geometry
and texture and rated through an overall quality estimation. The
findings showed that brighter light is a factor for increased quality
ratings. Brade et al. explored how solid colored ""CAD-style"" objectswould compare to fully texturized models in a VR assembly task [ 4].
While high-fidelity machines created a significantly higher sense
of presence, plain models scored sufficiently well to be considered
as an option for lower development costs. Braga et al. explored
how the visual fidelity of virtual food items influences users’ per-
ceived realism, task motivation, engagement, and interest [ 5]. They
reported that visual fidelity itself did not affect the assessed met-
rics. However, a relationship was found between perceived realism,
task motivation, engagement and interest. Braga et al. argued that,
instead of visual fidelity, the typicality of the experience and plau-
sibility may play prominent roles in shaping the user’s perception.
Skarbez et al. were among the first to investigate how users per-
ceive virtual replicas of physical spaces [ 40]. They conducted three
studies manipulating the scale of the replicated room and furniture,
removing elements from the scene, and altering lighting conditions.
Their findings indicate that the scale of the room and furniture
significantly influenced perceived realism, whereas lighting had a
minimal impact. Lastly, a study by Rzepka et al. explored the effect
of object knowledge, specifically the size of well-known objects
(e.g., Rubik’s Cube), on VR visual perception [ 37]. Participants were
shown multiple virtual objects in varying sizes and distances via a
VR headset. Their findings indicate that participants tended to rely
on familiar object sizes despite what was displayed in VR. Despite
the variety of studies, no previous work has explicitly tested the
present metrics. However, the findings highlight that user percep-
tion is shaped by a combination of objective properties of the VR
and cognitive or contextual factors.",7294
http://arxiv.org/abs/2510.02566v1,2510.02566v1,2025-10-02T21:01:11+00:00,PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,"Reconstructing physically plausible human motion from monocular videos
remains a challenging problem in computer vision and graphics. Existing methods
primarily focus on kinematics-based pose estimation, often leading to
unrealistic results due to the lack of physical constraints. To address such
artifacts, prior methods have typically relied on physics-based post-processing
following the initial kinematics-based motion estimation. However, this
two-stage design introduces error accumulation, ultimately limiting the overall
reconstruction quality. In this paper, we present PhysHMR, a unified framework
that directly learns a visual-to-action policy for humanoid control in a
physics-based simulator, enabling motion reconstruction that is both physically
grounded and visually aligned with the input video. A key component of our
approach is the pixel-as-ray strategy, which lifts 2D keypoints into 3D spatial
rays and transforms them into global space. These rays are incorporated as
policy inputs, providing robust global pose guidance without depending on noisy
3D root predictions. This soft global grounding, combined with local visual
features from a pretrained encoder, allows the policy to reason over both
detailed pose and global positioning. To overcome the sample inefficiency of
reinforcement learning, we further introduce a distillation scheme that
transfers motion knowledge from a mocap-trained expert to the
vision-conditioned policy, which is then refined using physically motivated
reinforcement learning rewards. Extensive experiments demonstrate that PhysHMR
produces high-fidelity, physically plausible motion across diverse scenarios,
outperforming prior approaches in both visual accuracy and physical realism.","2.1 Kinematics-Based Human Mesh Recovery
Parametric human models [Loper et al .2015; Osman et al .2020;
Pavlakos et al .2019; Xu et al .2020] have been widely adopted to re-
construct human motion from monocular video. Early works [Arnab
et al.2019; Bogo et al .2016; Huang et al .2017; Xiang et al .2019]
focus on fitting these models to individual image frames. More
recently, regression-based approaches, which leverage large-scale
datasets, have gained attention for their ability to achieve general-
purpose human mesh recovery [Cai et al .2023; Goel et al .2023;
Yin et al .2025]. To account for dynamic camera movements, data-
driven methods have been extended to estimate per-frame camera
poses [Shin et al .2024; Sun et al .2023; Yuan et al .2022]. Addition-
ally, SLAM (Simultaneous Localization and Mapping) techniques
have proven effective for robust camera motion estimation, further
enhancing human motion recovery in complex scenarios [Wang
et al.2024a]. HuMoR [Rempe et al .2021] learns a generative motion
prior that improves temporal consistency and robustness in pose
estimation. Despite these advances in human mesh recovery, purely
kinematic methods often exhibit artifacts like foot sliding, ground
penetration, and momentum inconsistency.
To address such artifacts, prior works have used physical pri-
ors as an auxiliary supervision to encourage plausible dynamics.
PhysPT [Zhang et al .2024b] proposes a neural module that re-
fines kinematic motion using differentiable Euler-Lagrange losses to
enforce rigid-body dynamics. IPMAN [Tripathi et al .2023] incorpo-
rates intuitive physics cues through loss functions into monocular
pose estimation, but remains a kinematics-based approach with-
out enforcing full physical dynamics. D&D [Li et al. 2022a] refines
kinematic motion by estimating external forces and applying analyt-
ical physical computation to enforce consistency with Newtonian
dynamics. While these methods improve physical realism to some
extent, they operate as post-hoc refinement on kinematic recon-
structions, making it difficult to recover from the ambiguity in the
kinematics-based human mesh recovery stage. Moreover, the physi-
cal consistency is enforced through neural approximations rather
than explicit physical simulation, leaving the overall pipeline fun-
damentally kinematics-based and decoupled from physical control.
2.2 Physics-based Human Motion Imitation
Physics simulation platforms [Makoviychuk et al .2021; Todorov
et al.2012], combined with reinforcement learning, have enabled
physically grounded control of simulated characters, producing
highly realistic human motion [Dou et al .2023; Peng et al .2018a,
2022, 2021a; Tessler et al .2023; Wang et al .2024b]. PPR [Yang et al .
2023] leverages physics priors for plausible video-based reconstruc-
tion, and differentiable dynamics models [Gärtner et al .2022] in-
tegrate physics into end-to-end optimization. By training policies
on large-scale motion capture datasets [Kobayashi et al .2023; Mah-
mood et al .2019; Peng et al .2021b], many works have demonstrated
high-fidelity motion imitation through learned control policies [Luo
et al.2024b, 2023, 2022; Peng et al .2018b; Tessler et al .2024; Wa-
gener et al .2022; Winkler et al .2022a]. PhysCap [Shimada et al .
2020] constrains monocular capture with real-time physical simula-
tion. However, these policies are trained to track clean 3D motionreferences, struggling to generalize when such data is unavailable.
PHC [Luo et al .2023] estimates 3D keypoints from video as motion
references, but its two-stage design decouples control from visual
input, often leading to jitter and unnatural motion.
Moreover, prior methods rely heavily on reinforcement learn-
ing, which typically suffers from low sample efficiency. Hence, they
struggle to fully exploit rich visual information and instead depend
primarily on sparse, deterministic inputs such as 3D keypoints or
kinematics-based representations. simXR [Luo et al .2024a] employs
a distillation-only scheme in a VR setting to train vision-to-action
policies. While this avoids the need for reinforcement learning, it
lacks robustness due to limited data and the absence of exploration.
In contrast, our joint PPO+Distillation training substantially im-
proves stability and generalization, demonstrating clear advantages
over a pure distillation approach.
Learning vision-conditioned policies for human motion recon-
struction that directly aligns with visual evidence remains a largely
underexplored challenge.",4541
http://arxiv.org/abs/2510.04539v1,2510.04539v1,2025-10-06T07:07:14+00:00,C3Editor: Achieving Controllable Consistency in 2D Model for 3D Editing,"Existing 2D-lifting-based 3D editing methods often encounter challenges
related to inconsistency, stemming from the lack of view-consistent 2D editing
models and the difficulty of ensuring consistent editing across multiple views.
To address these issues, we propose C3Editor, a controllable and consistent
2D-lifting-based 3D editing framework. Given an original 3D representation and
a text-based editing prompt, our method selectively establishes a
view-consistent 2D editing model to achieve superior 3D editing results. The
process begins with the controlled selection of a ground truth (GT) view and
its corresponding edited image as the optimization target, allowing for
user-defined manual edits. Next, we fine-tune the 2D editing model within the
GT view and across multiple views to align with the GT-edited image while
ensuring multi-view consistency. To meet the distinct requirements of GT view
fitting and multi-view consistency, we introduce separate LoRA modules for
targeted fine-tuning. Our approach delivers more consistent and controllable 2D
and 3D editing results than existing 2D-lifting-based methods, outperforming
them in both qualitative and quantitative evaluations.","\subsection{Diffusion Model and Fine-tuning}

Diffusion models~\cite{ho2020denoising,rombach2022high} have become powerful tools in generative tasks due to their unique approach of iteratively refining data from noise, allowing for precise control over the generation process. These models learn data distributions through a diffusion process that gradually adds and then reverses noise, effectively modeling complex data patterns in images, audio, and even text. Because of their robust performance, diffusion models are widely applied in tasks~\cite{chen2023fantasia3d,lin2023magic3d, metzer2023latent,wang2024prolificdreamer,xu2024bayesian,srivastava2025lay,zeng2025yolo} such as image synthesis, inpainting, super-resolution, and conditional generation, where they can generate or manipulate visual content based on additional inputs, such as text prompts, segmentation maps, or depth maps. This versatility makes them particularly valuable for tasks requiring high-quality, detailed outputs and subtle adjustments.

Fine-tuning diffusion models is essential for adapting them to specific tasks or datasets. Through targeted fine-tuning, diffusion models can be optimized to perform controlled edits, match stylistic demands, or generalize to new domains beyond their original training data. Techniques such as low-rank adaptation (LoRA)~\cite{hu2021lora} and other parameter-efficient tuning methods~\cite{houlsby2019parameter,li2021prefix,liu2021p} allow for effective customization by focusing on updating key parts of the model while keeping the core structure intact. This approach is especially useful when integrating diffusion models as priors in cross-domain applications, where maintaining high fidelity across varying views is critical. Fine-tuning thus enables diffusion models to meet specialized generative requirements, ensuring they maintain both visual quality and flexibility across diverse tasks.


\subsection{Diffusion-based 2D Editing}

Diffusion-based 2D editing techniques~\cite{avrahami2022blended,hertz2022prompt,kawar2023imagic, meng2021sdedit,ramesh2022hierarchical,brooks2023instructpix2pix} have revolutionized the field of image manipulation by leveraging the denoising diffusion process to transform noise into structured visual representations. In these models, editing is performed iteratively, where each step refines the image by reversing the noise and generating realistic features, allowing for adequate control over the level and type of modifications applied.

The key advantage of diffusion-based 2D editing lies in its ability to use conditional inputs, like text prompts or segmentation maps, to guide the editing process. For example, Instruct-Pix2Pix~\cite{brooks2023instructpix2pix} can interpret prompts to modify colors, add textures, or alter structures while maintaining the coherence of the image. These models can learn data distributions that align with specific editing goals, making them versatile across diverse applications. By fine-tuning or adjusting model parameters, diffusion models can also be specialized for specific editing tasks, allowing them to adapt to particular styles or constraints required by the user. This combination of iterative refinement, conditional control, and adaptability has made diffusion-based 2D editing a powerful tool in modern image generation and editing tasks.

\subsection{2D-lifting-based 3D Editing}

Recent advancements in 3D editing have increasingly integrated diffusion-based 2D editing models, leveraging their established capabilities to enhance 3D workflows~\cite{cao2024mvinpainter,chen2024proedit, chen2024gaussianeditor,haque2023instruct,dong2024vica,chen2024consistdreamer}. These models, originally designed for detailed image modifications, contribute to 3D editing by transferring their proficiency in nuanced, high-quality adjustments to three-dimensional representations. Methods like Neural Radiance Fields (NeRF)~\cite{mildenhall2021nerf} and 3D Gaussian Splatting (3DGS)~\cite{kerbl20233d} incorporate 2D editing models to improve the consistency and detail of 3D content.

By using 2D diffusion models as priors, recent approaches enhance the fidelity and stylistic consistency of 3D edits, especially in maintaining coherence across multiple views. Some works, such as DGE~\cite{chen2024dge}, combine images from different viewpoints into videos for processing. A primary challenge in this domain remains ensuring multi-view consistency, as traditional 2D-based edits applied to 3D models often lead to discrepancies between perspectives. Some methods, such as ConsistDreamer~\cite{chen2024consistdreamer}, model 3D-aware consistency by means of constraints like neural feature alignment or volume-based feature consistency. This has provided inspiration for our work. However, since it is not open-sourced, it is impossible to make a comparison for now. We compare our method with NeRF-based Instruct-NeRF-to-NeRF~\cite{haque2023instruct}, ViCA-NeRF~\cite{dong2024vica}, and GS-based GaussianEditor~\cite{chen2024gaussianeditor}.",5054
http://arxiv.org/abs/2510.02389v1,2510.02389v1,2025-09-30T22:27:18+00:00,From Trace to Line: LLM Agent for Real-World OSS Vulnerability Localization,"Large language models show promise for vulnerability discovery, yet
prevailing methods inspect code in isolation, struggle with long contexts, and
focus on coarse function- or file-level detections - offering limited
actionable guidance to engineers who need precise line-level localization and
targeted patches in real-world software development. We present T2L-Agent
(Trace-to-Line Agent), a project-level, end-to-end framework that plans its own
analysis and progressively narrows scope from modules to exact vulnerable
lines. T2L-Agent couples multi-round feedback with an Agentic Trace Analyzer
(ATA) that fuses runtime evidence - crash points, stack traces, and coverage
deltas - with AST-based code chunking, enabling iterative refinement beyond
single pass predictions and translating symptoms into actionable, line-level
diagnoses. To benchmark line-level vulnerability discovery, we introduce
T2L-ARVO, a diverse, expert-verified 50-case benchmark spanning five crash
families and real-world projects. T2L-ARVO is specifically designed to support
both coarse-grained detection and fine-grained localization, enabling rigorous
evaluation of systems that aim to move beyond file-level predictions. On
T2L-ARVO, T2L-Agent achieves up to 58.0% detection and 54.8% line-level
localization, substantially outperforming baselines. Together, the framework
and benchmark push LLM-based vulnerability detection from coarse identification
toward deployable, robust, precision diagnostics that reduce noise and
accelerate patching in open-source software workflows.","level, but this coarse granularity often fails to provide actionable guid-
ance for developers. GenLoc Asad et al. (2025) identifies potentially
vulnerable files from bug reports and iteratively analyzes them using
code exploration tools. AgentFL Qin et al. (2024) applies a multi-agent
framework for function-level localization, modeling the task as a three-
step pipeline with specialized agents and tools. CoSIL Jiang et al. (2025)
narrows the function-level search space using module call graphs and it-
eratively traverses them for relevant context. Similarly, AutoFL Kang
et al. (2024) prompts LLMs to localize method-level vulnerabilities via
function-call navigation, showing that multi-step reasoning helps over-
come context length limits.
To offer more precise guidance, recent studies have shifted toward
line- or statement-level localization. LineVul Fu & Tantithamthavorn
(2022) uses a Transformer-based classifier for line-level prediction,
while LOV A Li et al. (2024) introduces a self-attention framework to
score and highlight vulnerable lines. MatsVD Weng et al. (2024) en-
hances statement-level localization using dependency-aware attention, and xLoc Yang et al. (2024b)
learns multilingual, task-specific knowledge for bug detection and localization.
3
Building on these efforts, LLM4FL Rafi et al. (2024) proposes a multi-agent framework leverag-
ing graph-based retrieval and navigation to reason about failure causes. MemFL Yeo et al. (2025)
introduces external memory to incorporate project-specific knowledge, improving localization in
complex, repository-scale systems.
Collectively, these works push localization from file to line level and increasingly adopt multi-agent
strategies for subtask coordination. However, most still rely on limited runtime evidence, single-pass
predictions, or benchmarks that lack realistic project settings. OurT2L-Agentaddresses these
limitations with a planner-executor framework that incorporates runtime signals, enables dynamic
feedback loops, and achieves line-level localization in real-world repository environments.",2086
http://arxiv.org/abs/2509.11937v1,2509.11937v1,2025-09-15T13:56:06+00:00,MMORE: Massive Multimodal Open RAG & Extraction,"We introduce MMORE, an open-source pipeline for Massive Multimodal Open
RetrievalAugmented Generation and Extraction, designed to ingest, transform,
and retrieve knowledge from heterogeneous document formats at scale. MMORE
supports more than fifteen file types, including text, tables, images, emails,
audio, and video, and processes them into a unified format to enable downstream
applications for LLMs. The architecture offers modular, distributed processing,
enabling scalable parallelization across CPUs and GPUs. On processing
benchmarks, MMORE demonstrates a 3.8-fold speedup over single-node baselines
and 40% higher accuracy than Docling on scanned PDFs. The pipeline integrates
hybrid dense-sparse retrieval and supports both interactive APIs and batch RAG
endpoints. Evaluated on PubMedQA, MMORE-augmented medical LLMs improve
biomedical QA accuracy with increasing retrieval depth. MMORE provides a
robust, extensible foundation for deploying task-agnostic RAG systems on
diverse, real-world multimodal data. The codebase is available at
https://github.com/swiss-ai/mmore.","Large-scale transformation of unstructured documents into
structured, machine -readable format has attracted substan-
tial attention. We group prior work into two strands:(i)
document ingestion and parsing pipelines, and(ii)RAG
frameworks. To our knowledge, neither line of work si-
multaneously offers the modality coverage and end -to-end
throughput required for industrial -and small -scale multi-
modal assistants that we target withMMORE.
1arXiv:2509.11937v1 [cs.SE]",471
http://arxiv.org/abs/2510.05903v1,2510.05903v1,2025-10-07T13:13:18+00:00,Kaputt: A Large-Scale Dataset for Visual Defect Detection,"We present a novel large-scale dataset for defect detection in a logistics
setting. Recent work on industrial anomaly detection has primarily focused on
manufacturing scenarios with highly controlled poses and a limited number of
object categories. Existing benchmarks like MVTec-AD [6] and VisA [33] have
reached saturation, with state-of-the-art methods achieving up to 99.9% AUROC
scores. In contrast to manufacturing, anomaly detection in retail logistics
faces new challenges, particularly in the diversity and variability of object
pose and appearance. Leading anomaly detection methods fall short when applied
to this new setting. To bridge this gap, we introduce a new benchmark that
overcomes the current limitations of existing datasets. With over 230,000
images (and more than 29,000 defective instances), it is 40 times larger than
MVTec-AD and contains more than 48,000 distinct objects. To validate the
difficulty of the problem, we conduct an extensive evaluation of multiple
state-of-the-art anomaly detection methods, demonstrating that they do not
surpass 56.96% AUROC on our dataset. Further qualitative analysis confirms that
existing methods struggle to leverage normal samples under heavy pose and
appearance variation. With our large-scale dataset, we set a new benchmark and
encourage future research towards solving this challenging problem in retail
logistics anomaly detection. The dataset is available for download under
https://www.kaputt-dataset.com.","Defect detection applications. Defect detection is an im-
portant and widely studied field due to its many commercial
applications, including detecting defective parts in indus-
trial manufacturing [6], inspecting civil infrastructure such
as bridges [24, 27], vehicle damage [32], and medical appli-
cations [15]. However, our use case differs from the stan-
dard industrial manufacturing applications, mainly in terms
Table",425
http://arxiv.org/abs/2510.09379v1,2510.09379v1,2025-10-10T13:35:21+00:00,Task-Level Insights from Eigenvalues across Sequence Models,"Although softmax attention drives state-of-the-art performance for sequence
models, its quadratic complexity limits scalability, motivating linear
alternatives such as state space models (SSMs). While these alternatives
improve efficiency, their fundamental differences in information processing
remain poorly understood. In this work, we leverage the recently proposed
dynamical systems framework to represent softmax, norm and linear attention as
dynamical systems, enabling a structured comparison with SSMs by analyzing
their respective eigenvalue spectra. Since eigenvalues capture essential
aspects of dynamical system behavior, we conduct an extensive empirical
analysis across diverse sequence models and benchmarks. We first show that
eigenvalues influence essential aspects of memory and long-range dependency
modeling, revealing spectral signatures that align with task requirements.
Building on these insights, we then investigate how architectural modifications
in sequence models impact both eigenvalue spectra and task performance. This
correspondence further strengthens the position of eigenvalue analysis as a
principled metric for interpreting, understanding, and ultimately improving the
capabilities of sequence models.","The challenge of modeling long-range dependencies has been a central theme in sequence modeling across recurrent, state space, and attention-based approaches. To better understand the strengths and limitations of each model class from the linear system point of view, researchers have so far employed two primary analytical tools: eigenvalue spectra and memory functions.

\paragraph{Eigenvalue-based analyses.}  Eigenvalue spectra provide a principled way to characterize stability, and the relationship between remembering and forgetting, i.e., memory retention, and information decay in dynamical systems. This perspective has a long history in recurrent neural networks (RNNs), where eigenvalue normalization was introduced to mitigate vanishing gradients while preserving controlled memory decay~\citep{helfrich2019eigenvaluenormalizedrecurrentneural}. Later analyses investigated how different eigenvalue spectra encode solutions to temporal tasks, revealing that seemingly diverse eigenvalue distributions can correspond to functionally equivalent memory behaviors~\citep{Jarne_2022}. More recently, similar ideas have been extended to SSMs, where the eigenvalues of the transition matrix are shown to govern stability and memory.  Early works on SSMs studied how careful initialization, motivated by the eigenvalue placement, can provide models with long-range dependency, enabling them to compete with RNNs and transformers on synthetic benchmarks~\citep{Gu2020,Fu2023}. Beyond initialization, subsequent works focused on structural re-parameterizations. In particular, the eigenvalues of the transition matrix have been shown to play a crucial role in governing the stability and memory length of SSMs~\citep{wang2024stablessmalleviatingcursememory,grazzi2025unlockingstatetrackinglinearrnns}. This perspective directly connects to earlier analyses of RNNs, where eigenvalue spectra determine the decay rates of information. 

\paragraph{Memory function analyses.}A complementary line of work analyzes long-term dependency through the lens of memory functions~\citep{oppenheim1997signals}, which have a strong relation to the eigenvalues of the system. These functions quantify how much influence past inputs have on current outputs, typically formalized via norms of system response functions or spectral measures. For SSMs, memory functions provide a principled way to characterize expressivity and effective memory horizons~\citep{wang2023state}. For RNNs, memory functions are used to prevent rapid decay of state memory~\citep{Su_2019}.

In attention-based architectures, however, analyses have largely focused on mechanisms for extending memory. Early work demonstrated that even simple feed-forward architectures equipped with attention can solve long-memory tasks, outperforming classical recurrent networks~\citep{raffel2016feedforwardnetworksattentionsolve}. Later, augmenting self-attention with persistent memory slots was proposed as a way to extend the memory context~\citep{sukhbaatar2019augmentingselfattentionpersistentmemory}. Other works analyze the statistical properties of the attention score matrices themselves~\citep{bao2024self,bhojanapalli2021eigen}, revealing certain regularities in how attention distributes over inputs. While insightful, these approaches remain tied to the score-matrix formulation and do not naturally translate into a framework that allows systematic comparison with SSMs. By contrast, memory functions have been successfully applied to RNNs and SSMs, but they require architecture-specific design choices and do not generalize easily to linear parameter-varying (LPV) systems, such as attention-based models and Mamba~\citep{mamba2}. The recently proposed DSF addresses this gap by recasting masked attention itself as a dynamical system, thereby making eigenvalue analysis applicable. This unifying perspective enables systematic cross-architectural comparisons of memory behavior, which is leveraged in this work to provide the first comprehensive eigenvalue-based analysis of attention mechanisms, enabling a direct comparison with SSMs. Moreover, casting all these models into the eigenvalue perspective not only unifies their study, but also makes it possible to draw on the vast body of results from linear system theory to analyze and interpret their behavior in order to guide the model design choices.",4371
http://arxiv.org/abs/2509.21917v1,2509.21917v1,2025-09-26T05:57:04+00:00,Taming Flow-based I2V Models for Creative Video Editing,"Although image editing techniques have advanced significantly, video editing,
which aims to manipulate videos according to user intent, remains an emerging
challenge. Most existing image-conditioned video editing methods either require
inversion with model-specific design or need extensive optimization, limiting
their capability of leveraging up-to-date image-to-video (I2V) models to
transfer the editing capability of image editing models to the video domain. To
this end, we propose IF-V2V, an Inversion-Free method that can adapt
off-the-shelf flow-matching-based I2V models for video editing without
significant computational overhead. To circumvent inversion, we devise Vector
Field Rectification with Sample Deviation to incorporate information from the
source video into the denoising process by introducing a deviation term into
the denoising vector field. To further ensure consistency with the source video
in a model-agnostic way, we introduce Structure-and-Motion-Preserving
Initialization to generate motion-aware temporally correlated noise with
structural information embedded. We also present a Deviation Caching mechanism
to minimize the additional computational cost for denoising vector
rectification without significantly impacting editing quality. Evaluations
demonstrate that our method achieves superior editing quality and consistency
over existing approaches, offering a lightweight plug-and-play solution to
realize visual creativity.","\sssection{Image-to-video Generation.} Visual content generation and editing have witnessed significant advancements thanks to the emergence of diffusion models~\citep{ddpm, ddim, ldm}. Recently, DiT~\citep{dit} has become the mainstream architecture of the denoising model with promising generation quality, surpassing U-Net~\citep{unet} with its powerful scaling capability~\citep{kaplan2020scalinglawsneurallanguage} and potential for multimodal interaction~\citep{sd3}. Flow Matching~\citep{flowmatching, rectifiedflow} introduces an improved generative model paradigm that interpolates data and noise linearly in the forward diffusion process, bringing better theoretical properties and conceptual simplicity. Building upon these works, a number of I2V models~\citep{wan, easyanimate, hunyuanvideo, cogvideox, opensora2, vchitect2} have emerged with full 3D attention~\citep{attention2017} instead of decoupled spatiotemporal attention~\citep{guo2024animatediff}, significantly enhancing generation quality and consistency.



\sssection{Training-free Visual Editing.} Training-free visual editing modifies the source image or video according to designated conditions (e.g., text, image, and mask) at test time, using off-the-shelf pretrained models. Existing works can be broadly categorized into two categories: inversion-based and optimization-based methods. Inversion-based methods~\citep{videoshop, dni, wave, yatim2025dynvfxaugmentingrealvideos} adopt the inversion of the diffusion process to map the input back to Gaussian noise, and then perform denoising under given conditions. However, not only is the inversion process time-consuming, but it also inevitably induces error. To overcome the inherent inaccuracy of inversion and ensure consistency with the input, various attention injection strategies~\citep{wave, yatim2025dynvfxaugmentingrealvideos} are utilized to further incorporate source information. Despite their effectiveness, these strategies are model-specific, reducing their universality to different model structures. Optimization-based methods~\citep{dreammotion, ren2025fdsfrequencyawaredenoisingscore, unityindiversity} use SDS~\citep{pooledreamfusion} to directly optimize the input latents towards the desired direction. Nevertheless, the optimization operation introduces considerable computational cost, limiting its availability to common creators. With the prevalence of flow-based models~\citep{flowmatching, rectifiedflow}, there have also been methods~\citep{avrahami2025stableflowvitallayers, dalva2024fluxspacedisentangledsemanticediting, xu2025unveilinversioninvarianceflow} that leverage the properties of the flow matching process to achieve more precise and consistent visual editing. However, few solutions are both lightweight and universal without model-specific design in the video domain, limiting creators to swiftly leverage the most up-to-date I2V base models for video editing within user-friendly resources, such as a single GPU.


There have also been works exploring inversion-free image editing. For instance, InfEdit~\citep{infedit} theoretically depends on the diffusion process, limiting its application to state-of-the-art flow-based models. It also needs attention manipulation, further limiting its universality. FlowEdit~\citep{kulikov2024floweditinversionfreetextbasedediting} leverages flow properties to construct a transport from the source to the target distribution, which is derived from the Euler Discrete Solver~\citep{sd3}. In contrast, our method constructs two parallel ODEs to model the editing process, which does not depend on a specific ODE solver and enables control over editing strength. Our method also introduces SMPI to further enhance video-level spatiotemporal consistency and a flexible caching strategy.",3798
http://arxiv.org/abs/2510.07184v1,2510.07184v1,2025-10-08T16:21:55+00:00,Exploring the Feasibility of Gaze-Based Navigation Across Path Types,"Gaze input, as a modality inherently conveying user intent, offers intuitive
and immersive experiences in extended reality (XR). With eye-tracking now being
a standard feature in modern XR headsets, gaze has been extensively applied to
tasks such as selection, text entry, and object manipulation. However, gaze
based navigation despite being a fundamental interaction task remains largely
underexplored. In particular, little is known about which path types are well
suited for gaze navigation and under what conditions it performs effectively.
To bridge this gap, we conducted a controlled user study evaluating gaze-based
navigation across three representative path types: linear, narrowing, and
circular. Our findings reveal distinct performance characteristics and
parameter ranges for each path type, offering design insights and practical
guidelines for future gaze-driven navigation systems in XR.","With the increasing adoption of eye-tracking technologies, commercial Head-Mounted Devices (HMDs) such as the Apple Vision Pro, Meta Quest Pro, and HTC Vive Focus 3 have begun to integrate gaze tracking as a key input modality. As a maturing technology, eye tracking has been applied across a wide range of domains, including gaming~\cite{pakov2019collaborative, isokoski2009gazeGame}, text entry~\cite{Lui2021iText, Kurauchi2016GazeTextEntry, HedeshyCHI21GazeHumTextEntry}, target selection~\cite{meng2022textselection}, user interface design~\cite{Stellmach2012gazeui,piotrowski2019gaze}, and navigation~\cite{kang2024rayhand, Yushi25TVCGSteeringLatency, Yushi24TVCGSteeringDirection}---either as a standalone input method or as an auxiliary modality combined with other interactions.

Gaze-based interaction offers an intuitive, hands-free alternative to conventional input methods~\cite{Sidenmark2023Comparing, Majaranta2014Tracking, SibertCHI00GazeSelection}. This makes it especially attractive in scenarios where manual input is limited or undesired~\cite{Xueshi2021UISTHandsfree, Xueshi2020IsmarHandsfree}. However, it differs fundamentally from traditional pointer-based control. Scott et al.~\cite{Scott2004motor} attributed the higher re-entry rates in gaze-based input to intrinsic limitations of the gaze system---unlike regular cursor-based input, users cannot rely on continuous visual feedback for correction and must instead make predictive adjustments. Holmqvist et al. further observed that during correction phases, users tend to alternate between short saccades and fixations~\cite{Kenneth2011eyemovement}, ranging from 100 milliseconds to several seconds, stabilizing the cursor and using saccades to compensate for positional discrepancies~\cite{Prablanc1978saccades}.

While prior work has modeled gaze-based selection and steering tasks using adapted versions of Fitts’ Law and the Steering Law~\cite{xuning2025CHILBW, Zhang2010dwell-based}, research on gaze-based performance in more complex trajectory-based navigation tasks remains limited. Key path parameters, such as curvature and path narrowing, may significantly affect user performance in trajectory-based navigation~\cite{accot1997beyond}. Yet their influence on the gaze-based navigation task has not been systematically studied. Given that trajectory-based tasks differ notably from linear steering tasks in both motor behavior and cognitive demands, further investigation is needed to understand how these path parameters affect gaze-based navigation in immersive VR environments.",2568
http://arxiv.org/abs/2510.09171v1,2510.09171v1,2025-10-10T09:14:33+00:00,Instance-Level Generation for Representation Learning,"Instance-level recognition (ILR) focuses on identifying individual objects
rather than broad categories, offering the highest granularity in image
classification. However, this fine-grained nature makes creating large-scale
annotated datasets challenging, limiting ILR's real-world applicability across
domains. To overcome this, we introduce a novel approach that synthetically
generates diverse object instances from multiple domains under varied
conditions and backgrounds, forming a large-scale training set. Unlike prior
work on automatic data synthesis, our method is the first to address
ILR-specific challenges without relying on any real images. Fine-tuning
foundation vision models on the generated data significantly improves retrieval
performance across seven ILR benchmarks spanning multiple domains. Our approach
offers a new, efficient, and effective alternative to extensive data collection
and curation, introducing a new ILR paradigm where the only input is the names
of the target domains, unlocking a wide range of real-world applications.","Instance-level representationsInstance-level recognition requires image representations that capture
fine-grained object details while distinguishing them from numerous semantically similar classes. Generic
models like ResNet (He et al., 2016) and CLIP (Radford et al., 2021) struggle in this setting, as they
prioritize high-level semantics over instance-specific features. A common solution is fine-tuning pre-trained
backbones on domain-specific datasets—such as artwork (Ypsilantis et al., 2021), landmarks (Lee et al.,
2022; Shao et al., 2023; Cao et al., 2020; Suma et al., 2024), or products (Patel et al., 2022; Ramzi et al.,
2022)—to enhance their ability to differentiate individual instances. Recent efforts focus on universal embed-
dings (Ypsilantis et al., 2023) that cover jointly a whole range of domains and tasks. However, models still
require fine-tuning with class-supervised learning to acquire the necessary discriminative properties, making
the scarcity of high-quality labeled datasets a major challenge. Data augmentation techniques (Ypsilantis
2
et al., 2021) help mitigate this issue by generating diverse variations of an instance from limited samples.
The only prior work that also leverages generative models for instance-level tasks (Sundaram et al., 2025)
fine-tunes a separate model for each instance, requiring a few real images as input. In contrast, our approach
trains a single model that generalizes well across objects and domains without relying on any real images.
Training with synthetic imagesSynthetic data has been used in a variety of computer vision problems,
suchasobjectdetection(Pengetal.,2015;Rozantsevetal.,2015;Georgakisetal.,2017), segmentation(Chen
et al., 2019; Ros et al., 2016), autonomous driving (Abu Alhaija et al., 2018), object pose estimation (Cai
et al., 2022; Labbé et al., 2020), 3D-tasks (Chang et al., 2015), and recently for representation learning (Tian
et al., 2024; Wu et al., 2023). An early practice is to cut the real objects and paste them onto backgrounds
to generate synthetic images for instance or object detection (Dwibedi et al., 2017; Georgakis et al., 2017).
However, challenges remain in reducing the boundary artifacts and achieving consistent lighting conditions
between the object and background, as these problems often result in unrealistic composite images. More
recently, themainsourcesofsyntheticimagesarecomputergraphicspipelineorrenderingengines(Mahmood
et al., 2019), generative adversarial networks (GAN) (Besnier et al., 2020; Brock, 2018), and text-to-image
GDM (Fan et al., 2024; Sarıyıldız et al., 2023). Images generated through rendering engines often suffer from
domain gap when compared to real-world test images, requiring domain adaptation techniques to mitigate
the gap during training. In contrast, GAN and GDM produce more realistic images that do not typically
require post-generation domain adaptation (Wang et al., 2020). Text-to-image GDM, in particular, offers
a higher degree of control in the image generation process, for example, changing the background of the
target object using text prompts (Mokady et al., 2023; Raj et al., 2023; Geng et al., 2024; Zhang et al.,
2023). This ability to control image features through text makes GDM particularly valuable for generating
diverse images, which is crucial for representation learning (Tian et al., 2024; Wu et al., 2023). However,
synthesizing images for instance-level task is not trivial, as it requires generating a synthetic object under
various conditions while preserving its structure and texture.
Metric learning for image retrievalGiven a training dataset, the most common approach for training
deep representation networks for image retrieval is supervised learning using categorical labels. As a result, a
large number of methods have proposed classification-based losses (Zhai & Wu, 2018; Deng et al., 2019; Teh
et al., 2020; Qian et al., 2019; Kim et al., 2020). Despite not directly optimizing the pairwise distance metric
that is used at test time, such approaches achieve very good performance, especially when combined with
propagating the representation across examples (Elezi et al., 2020; Seidenschwarz et al., 2021; Kotovenko
et al., 2023). Other methods directly optimize the distance metric with pairwise losses. These most often
rely on hand-crafted loss functions, such as the most popular contrastive (Hadsell et al., 2006), and triplet
loss (Schroff et al., 2015), by postulating a correlation between such a training objective and the test time
objective which is typically an information retrieval metric. Finding informative pairs and triplets (Musgrave
et al., 2020; Roth et al., 2020; Oh Song et al., 2016; Sohn, 2016) appears to be very important. As a natural
follow-up, a few recent methods directly optimized differentiable approximations of retrieval metrics, such
as average precision (Rolínek et al., 2020; He et al., 2018; Revaud et al., 2019; Ramzi et al., 2021; 2022) and
recall (Patel et al., 2022). In this work, we rely on recall@k (Patel et al., 2022) as a loss function which is
demonstrating top results on a variety of benchmarks in the literature and does not require hard negative
mining. Self-supervised (Kim et al., 2022) methods exist as well and are shown effective, but are tested
only on training data from the target distributions, which is not a realistic setup. A recent alternative to
CLIP (Radford et al., 2021), called Unicom (An et al., 2023), trains on LAION 400M (Schuhmann et al.,
2021), treats captions as weak annotations to perform text-based clustering, and reformulates the learning
as a classification task. Their results show improvements in a set of different retrieval datasets, including
instance-level ones. Alternatively, we propose leveraging synthetic data to introduce an extensive collection
of objects with diverse variations into the training dataset.",5906
http://arxiv.org/abs/2510.02934v1,2510.02934v1,2025-10-03T12:25:28+00:00,Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,"Large Language Models (LLMs) have demonstrated impressive capabilities in
code generation and are increasingly integrated into the software development
process. However, ensuring the correctness of LLM-generated code remains a
critical concern. Prior work has shown that the internal representations of
LLMs encode meaningful signals for assessing code correctness. Nevertheless,
the existing methods rely on representations from pre-selected/fixed layers and
token positions, which could limit its generalizability across diverse model
architectures and tasks. In this work, we introduce AUTOPROBE, a novel
model-agnostic approach that dynamically selects the most informative internal
representations for code correctness assessment. AUTOPROBE employs an
attention-based mechanism to learn importance scores for hidden states,
enabling it to focus on the most relevant features. These weighted
representations are then aggregated and passed to a probing classifier to
predict code correctness across multiple dimensions, including compilability,
functionality, and security. To evaluate the performance of AUTOPROBE, we
conduct extensive experiments across multiple benchmarks and code LLMs. Our
experimental results show that AUTOPROBE consistently outperforms the
baselines. For security assessment, AUTOPROBE surpasses the state-of-the-art
white-box approach by 18%. For compilability and functionality assessment,
AUTOPROBE demonstrates its highest robustness to code complexity, with the
performance higher than the other approaches by up to 19% and 111%,
respectively. These findings highlight that dynamically selecting important
internal signals enables AUTOPROBE to serve as a robust and generalizable
solution for assessing the correctness of code generated by various LLMs.","LLM-based code generation.LLMs, known for their
powerful capabilities in contextual understanding and re-
sponse generation, have been widely applied across various
domains,includingnaturallanguageprocessing[39,40,62,
63],vision-languageintegration[64,65],andsoftwareengi-
neering(SE)tasks[4,42,44,3,2].InthecontextofSE,code
LLMs, such as DEEPSEEKCODER[8], CODELLAMA[9],
and MAGICODER[41], which have been pre-trained or fine-
tuned on large-scale code corpora, have demonstrated re-
markable success in a wide range of tasks, e.g. code gen-
eration [2, 3, 66], test generation [44, 67], code summariza-
tion [68], and program repair [69], etc. These models are
increasingly integrated into modern development environ-
mentsviatoolslikeGitHubCopilot[6]andCodeGeeX2[7],
where they assist developers in generating boilerplate code,
completing functions, or writing test cases. Despite their
impressive performance, a key challenge remains, the hal-
lucination problem [45, 70, 11, 71], where LLMs generate
code that is syntactically valid but semantically incorrect,
insecure, or unexecutable. Such hallucinations compromise
the reliability of LLM-generated code and raise concerns
about correctness, security, and trustworthiness. Mitigating
such issues remains an open problem and is essential for
ensuring the reliable and effective adoption of code LLMs
in real-world software projects.
LLM’shallucinationdetection.Variousapproaches[46,
33, 30, 72, 73] have been proposed to detect hallucinations
inLLMs.Theseapproachescanbebroadlycategorizedinto
black-boxandwhite-boxapproaches,dependingonwhether
they rely on the model’s input-output behaviors or leverage
its internal computations.
Black-boxapproaches[72,74,75,76,77]detecthalluci-
nationsbyrelyingsolelyontheinput-outputbehaviorofthe
models.Acommonstrategyisuncertaintyestimation,based
onahypothesisthatresponseswithhighermodelconfidence
are more likely to be correct [72, 74, 78]. For instance,SelfCheckGPT [72] estimates model confidence by sam-
pling multiple responses to the same query and measuring
their semantic consistency. If the responses are consistent
across samples, the output is considered to be reliable;
otherwise, it is likely hallucinated. Another research direc-
tion focuses onfact-checking, where the generated outputs
are validated against factual sources. These sources can be
drawnfromthemodel’sinternalknowledge[75]orexternal
corpora [76, 79, 80]. Additionally, some other black-box
methodsformulatehallucinationdetectionasaclassification
task, training a downstream classifier on features extracted
from the generated outputs to distinguish hallucinated from
faithful outputs [77].
On the other hand,white-boxapproaches leverage the
model’sinternalcomputations,suchashiddenstates,activa-
tions,attentionmaps,todetermineunreliableorhallucinated
responses [30, 31]. For example, Azariaet al.[32] train a
classifier on the hidden activations to estimate the truthful-
nessofthegeneratedstatements.Similarly,INSIDE[33]and
FactoScope [81] analyze internal states to assess semantic
consistency and factual reliability.
Ourworkfollowsthewhile-boxparadigmbyleveraging
internal representations for hallucination detection. How-
ever, unlike prior studies [29, 33, 46, 32] that focus on un-
structuredNaturalLanguageGeneration(NLG)tasks,where
correctness is often subjective by relying on human judg-
ment or based on external factual knowledge, AUTOPROBE
targetcode correctness, where correctness is objectively
definedbysyntax,semantics,andfunctionalbehaviors.Due
to the nature of NLG tasks, previous works [33, 32] mainly
evaluate hallucination through semantic consistency or fac-
tual alignment among model responses. However, these
techniques are not directly applicable to code. In the con-
text of code generation, a task may have multiple correct
solutionsthatdifferinimplementationdetailsoralgorithms.
As a result, inconsistency across generated responses does
not necessarily imply incorrectness. To address this gap,
AUTOPROBEis specifically designed for assessing code
correctness using internal representations.
Code correctness assessment.Ensuring source code
quality is a fundamental objective in the SE process. Tra-
ditional approaches for detecting program issues often rely
heavily on handcrafted features and static analysis tech-
niques [82]. With the emergence of deep learning, pre-
trained models such as CodeBERT [26], GraphBERT [83],
and CodeT5 [27] have been widely adopted due to their
ability to capture rich contextual and semantic information
from source code. These models support automatic feature
extraction from different code representations such as raw
code, program slices, or code property graphs, enabling
bug and vulnerability detection across different levels of
granularity, from coarse-grained levels, i.e., files or meth-
ods, to fine-grained [84, 85, 86, 87, 88], i.e., slices and
statements. For instance, IVDetect [84] employs a graph-
basedneuralnetworktodetectvulnerabilitiesatthefunction
level and uses interpretable models to further localize vul-
nerable statements.LineVul [88] andLineVD [87]leverage
Vuet al.:Preprint submitted to ElsevierPage 17 of 21
AutoProbe
CodeBERTtoencodecoderepresentationsandhavedemon-
strated superior performance over IVDetect in fine-grained
vulnerability detection.
Recently, the quality of code generated by LLMs has
gainedsignificantattentionduetotheincreasingrelianceon
AI-generated code in real-world applications. Several em-
pirical studies have revealed the potential risks of bugs and
securityissuesassociatedwithLLM-generatedcode[89,90,
91,92,93].Multiplestudies[78,94,95]havebeenproposed
for guaranteeing the quality of code generated by LLMs.
Huangetal.[78]proposeatechniqueofuncertaintyanalysis
to measure LLMs’ confidence in generated outputs, which
couldaidinidentifyingpotentiallyunreliablecode.Further-
more, multi-agent frameworks such as AutoSafeCoder [95]
enhance LLM-based code generation by integrating static
analysis and fuzz testing. Similarly, LLMSecGuard [94]
combines the capabilities of static analyzers and LLMs to
improve code security, demonstrating the benefits of hybrid
approaches in mitigating vulnerabilities and strengthening
code robustness.
OPENIA[28] demonstrates that the internal states of
LLMs encode valuable signals related to code correctness
and can be leveraged to detect incorrect code. This work
is closely related to ours, as both adopt a white-box ap-
proach. However, the key difference between AUTOPROBE
andOPENIAliesinthemechanismforselectinginformative
internal representations. OPENIAassumes that the hidden
states of the last generated tokens at the last layer encap-
sulate the most comprehensive information and therefore
uses them directly for correctness assessment. Instead of
selecting a fixed representation that could lead to subopti-
mal performance across models, AUTOPROBEproposes a
model-agnostic approach that dynamically selects the most
informativerepresentationsacrossmultiplelayersandtoken
positions. This flexibility enables AUTOPROBEto better
adapt to different model architectures.",7120
http://arxiv.org/abs/2510.05307v1,2510.05307v1,2025-10-06T19:18:56+00:00,When Should Users Check? A Decision-Theoretic Model of Confirmation Frequency in Multi-Step AI Agent Tasks,"Existing AI agents typically execute multi-step tasks autonomously and only
allow user confirmation at the end. During execution, users have little
control, making the confirm-at-end approach brittle: a single error can cascade
and force a complete restart. Confirming every step avoids such failures, but
imposes tedious overhead. Balancing excessive interruptions against costly
rollbacks remains an open challenge. We address this problem by modeling
confirmation as a minimum time scheduling problem. We conducted a formative
study with eight participants, which revealed a recurring
Confirmation-Diagnosis-Correction-Redo (CDCR) pattern in how users monitor
errors. Based on this pattern, we developed a decision-theoretic model to
determine time-efficient confirmation point placement. We then evaluated our
approach using a within-subjects study where 48 participants monitored AI
agents and repaired their mistakes while executing tasks. Results show that 81
percent of participants preferred our intermediate confirmation approach over
the confirm-at-end approach used by existing systems, and task completion time
was reduced by 13.54 percent.","HCI research has primarily examined how to give users control,
while reliability engineering has modeled when to intervene. We
unify these perspectives by using mathematical models grounded
in user behavior to determine when human control should occur
in long-horizon agentic tasks.
2.1 Human Control in AI Agents
The tension between system autonomy and direct user control has
been a recurring theme in HCI for decades [ 28,45,50,66]. Current
AI agents are highly automated, integrating context, tools, and mem-
ory to solve complex real-world tasks [ 73]. Yet these reasoning and
tool use systems are highly error-prone, yielding only 30% accuracy
on 10-step tasks [ 23,86]. Errors propagate like a snowball—growing
exponentially and causing downstream failures [ 33,96]. Agent self-
correction is inconsistent and does not scale with task complexity
[35]. These challenges highlight the necessity of human control.
To support such control, HCI researchers have proposed a wide
range of tools for human involvement in agent planning and ex-
ecution [ 19,24,25,37,39,63,68,97]. For instance, Cocoa builds
on computational notebook interfaces to support co-planning and
co-execution of research tasks [ 25], AGDebugger enables pausing
and editing agent behaviors in programming [ 24], VAL lets users
instruct agents to execute tasks in user-specified ways [ 39], and
Step-Phasewise decomposes complex analysis workflows for verifi-
cation [ 37]. These systems focus on how to give users control, but
not when that control should be invoked in multi-step tasks.
If we want to study ""when"", we inevitably need mathematical
models. Beyond the agentic AI context, prior work has examined
the timing of user engagement through mathematical models [ 17,
28,74,95]. For instance, Horvitz [28] proposed expected-utility
models for deciding whether to invoke services, and Cockburn et al .
[17] showed that users’ probability-weighting biases shape when
they engage or ignore assistive features. However, these works
focus on single-event decisions and are not applicable to agentic
AI, where tasks involve many interdependent steps. To capture the
long-horizon nature of agent execution, we need to observe users’
sequential behaviors around errors and build mathematical models
When Should Users Check? A Decision-Theoretic Model of Confirmation Frequency in Multi-Step AI Agent Tasks Conference’17, July 2017, Washington, DC, USA
that identify the optimal moments for user confirmation from a
global, system-level perspective.
2.2 Error Prevention and Recovery Models
Outside HCI, reliability engineering offers inspiration. Researchers
have long developed mathematical models to optimize inspection
intervals for complex systems such as railway tracks, drainage
infrastructure, and manufacturing plants [ 32,71,100]. These studies
show how inspection strategies affect whole-system performance,
considering dependencies such as workforce allocation, spare parts,
and interdependent subsystems [ 20,21,79]. The central challenge
is to balance preventive maintenance (early inspections to prevent
failures) with corrective maintenance (recovering after failures
occur), minimizing long-run costs by trading off inspection labor
and detection probabilities against repair and downtime losses
[9,11,70,81]. For AI agents, the analogy is clear: deciding when to
confirm execution steps likewise balances preventive checks with
corrective recovery. But unlike engineering, the relevant cost is
not purely economic—it is shaped by user experience, including
confirmation burden and error recovery effort.
HCI work on errors has largely focused on how to prevent or
repair mistakes. Examples include encouraging users to issue longer
or repeated commands for accuracy [ 38], designing conversations
that make error recovery more resilient [ 8], referring to third-party
apps for additional information during repair [ 44], and studying
whether error detection should be system- or user-initiated [ 41].
These studies enrich our understanding of error handling, but they
do not provide a mathematical account of when intervention should
occur across multi-step tasks.
In summary, while prior HCI work has richly explored how to
give users control and how to prevent or repair errors, it has rarely
asked when such intervention should occur across long-horizon
tasks. Similarly, reliability engineering provides rigorous models of
inspection timing, but they optimize purely for economic cost in
physical systems. Our contribution is to bridge these two traditions:
we introduce a mathematical perspective on thewhenquestion in
agentic AI, grounded not in labor or repair costs, but in the dy-
namics of user experience—balancing confirmation burden against
recovery effort. This shift reframes human–agent interaction as a
problem of scheduling when to switch initiative between agents act-
ing and users verifying and correcting, opening new space for both
theoretical modeling and practical design of adaptive confirmation
strategies.
3Grounding Confirmation Modeling in Agentic
AI Contexts
Before building the model, we conducted a formative study to
address two research questions:
•RQ1: How do users monitor AI agents during task execution?
Is there any consistency in user behavior patterns across
different task types and interface designs?
•RQ2: When should confirmation happen to support users in
effectively monitoring AI agents?
To ensure that the observed user behaviors and the resulting mod-
eling are grounded in realistic usage contexts, we first reviewed
representative task benchmarks and existing AI agents (Section3.1). Based on this review, we then designed a test environment
that combines real-world tasks with deployed agentic systems to
examine RQ1 and RQ2 (Section 3.2).
3.1 Agentic AI Task Types and Existing Systems
3.1.1 Task Types.To understand the landscape of agentic AI tasks
and guide the design of our user study, we reviewed 12 benchmarks
that cover a wide spectrum of real-world scenarios [ 15,34,49,76,82,
85, 88–91, 99]. These benchmarks are grounded in user-generated
platforms such as StackOverflow, Reddit, and other public-facing
systems, where users frequently ask how to complete complex or
ambiguous tasks. Each benchmark contains around 300 pieces of
task data. From these benchmarks, we identified four broad ap-
plication domains—Office, Daily Life, Virtual Environment, and
Mixed Workflow—each encompassing several representative task
types (Figure 2, left). These domains provided the basis for selecting
tasks in our formative study. Despite advances in LLMs, current
models such as GPT-4 or Claude 3.5 still struggle to reliably com-
plete long-horizon tasks. Reported success rates for benchmarks
like OSWorld [ 89], SpreadsheetBench [ 49], and TheAgentCompany
[90] often fall between 30% and 60%, with failure modes including
incorrect tool use, misinterpreted instructions, or skipped steps.
This relatively high error rate underscores the need for human-in-
the-loop oversight, where users monitor and intervene during task
execution.
3.1.2 Existing Systems.We tested tasks from the benchmarks men-
tioned above in nine agentic systems [ 2,3,6,18,26,27,51,54,75].
We also reviewed recent surveys on agent interface design [ 48,87,
98]. Typically, these agents operate within remote desktop environ-
ments, where the agent perceives its environment through periodic
screenshots. The vision-language model (VLM) reasons about the
current state and user instructions, and then performs primitive
actions, such as mouse clicks and keyboard inputs. For instance,
when the agent perceives a hotel booking website, it will enter
the destination city on that website automatically, as shown in the
middle of Figure",7749
http://arxiv.org/abs/2509.26633v2,2509.26633v2,2025-09-30T17:59:02+00:00,OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,"A dominant paradigm for teaching humanoid robots complex skills is to
retarget human motions as kinematic references to train reinforcement learning
(RL) policies. However, existing retargeting pipelines often struggle with the
significant embodiment gap between humans and robots, producing physically
implausible artifacts like foot-skating and penetration. More importantly,
common retargeting methods neglect the rich human-object and human-environment
interactions essential for expressive locomotion and loco-manipulation. To
address this, we introduce OmniRetarget, an interaction-preserving data
generation engine based on an interaction mesh that explicitly models and
preserves the crucial spatial and contact relationships between an agent, the
terrain, and manipulated objects. By minimizing the Laplacian deformation
between the human and robot meshes while enforcing kinematic constraints,
OmniRetarget generates kinematically feasible trajectories. Moreover,
preserving task-relevant interactions enables efficient data augmentation, from
a single demonstration to different robot embodiments, terrains, and object
configurations. We comprehensively evaluate OmniRetarget by retargeting motions
from OMOMO, LAFAN1, and our in-house MoCap datasets, generating over 8-hour
trajectories that achieve better kinematic constraint satisfaction and contact
preservation than widely used baselines. Such high-quality data enables
proprioceptive RL policies to successfully execute long-horizon (up to 30
seconds) parkour and loco-manipulation skills on a Unitree G1 humanoid, trained
with only 5 reward terms and simple domain randomization shared by all tasks,
without any learning curriculum.","\subsection{Motion Retargeting}In computer graphics, transferring motions across characters has been extensively explored. Researchers have employed optimization-based methods to retarget human motions to avatars by preserving distances and orientations between keypoints \cite{cheynel2023sparse}, minimizing deformation energy \cite{Ho2010Spatial, kim2016retargeting}, or scaling the motions to satisfy hard constraints \cite{gleicher1998retargetting}. Others leverage data-driven methods that map diverse skeletons to a canonical representation \cite{aberman2020skeleton}, solve inverse kinematics with neural networks \cite{villegas2018neural}, or use reinforcement learning to preserve an interaction graph \cite{zhang2023simulation}.


Retargeting motions to humanoid robots introduces challenges beyond character animation, particularly the need to enforce physical constraints. For example, PHC \cite{Luo2023PerpetualHC}, a graphics method adopted in robotics~\cite{he2025asap, he2024omnih2o}, uses keypoint matching with unconstrained optimization, often leading to penetration, foot skating, and lack of object or scene awareness.
Similarly, GMR~\cite{ze2025twist} extends keypoint matching to orientations but suffer the same issues. 
VideoMimic~\cite{videomimic} improves realism with soft contact and collision penalties but offers no guarantees and requires careful tuning.

The closest method to ours is Interaction Mesh based Motion Adaptation (IMMA) \cite{Nakaoka2012Interaction}, which also leverages an interaction mesh \cite{Ho2010Spatial} to preserve the spatial relationship between body parts. However, it is not open-sourced and ignores kinematic limits and interactions with the environment or manipulated objects. 
In contrast, \OmniRetarget unifies all hard constraints, including foot sticking, non-penetration, and joint and velocity limits, while explicitly preserving environment and object interactions.

\subsection{Learning-Based Humanoid Whole-Body Control}








Recent learning-based whole-body control has enabled humanoids to traverse dynamic scenes and manipulate objects~\cite{dao2024sim, long2024learning, he2025attention, he2025learning, kuang2025skillblender, zhang2025unleashing, xue2025unified, zhang2406wococo, zhang2025falcon}. These methods typically train with hand-crafted rewards or task interfaces (e.g., velocity tracking, contact schedules, end-effector targets) but depend on extensive reward engineering and mostly fail to yield natural, human-level motions.


Motion imitation offers a promising alternative. In graphics, DeepMimic~\cite{peng2018deepmimic} shows that using human references yields natural, human-like behaviors with agile, dynamic motions. 
However, applying this approach to humanoid robots remains difficult due to the lack of reliable open-source kinematic retargeting pipelines.
With suboptimal reference motions, practitioners are forced to either manually clean the data~\cite{zhang2025hub} or re-introduce extensive reward engineering, such as ad-hoc regularizers for contact, slipping, and air time, to compensate for artifacts~\cite{ze2025twist, he2025asap, li2025reinforcement}. In contrast, trackers with minimal reward formulation like BeyondMimic~\cite{liao2025beyondmimic} achieve state-of-the-art results on hardware with high-fidelity references~\cite{unitree_lafan1_retargeting_dataset}, but those are scarce and robot-only, without interactions.

Beyond single-character motion, human–scene interaction data has proven effective for terrain traversal and loco-manipulation in character animation~\cite{xu2025parc, wu2024human}, but translating this to robotics remains challenging. 
VideoMimic~\cite{videomimic} applies this idea to human–terrain traversal by reconstructing motions and terrains from video, but suffers from artifacts and is limited to static–scene interactions. To bridge this gap, \OmniRetarget enables natural, agile robot-object-scene interactions with high-quality reference from retargeting without manual post-processing or reward engineering.





\subsection{Data Generation for Humanoid Loco-Manipulation}

The demand for whole-body interaction data has motivated many prior works on data generation.
One approach is direct human teleoperation \cite{seo2023deep, fu2024humanplus, he2024omnih2o, ze2025twist, ben2025homie}. While it provides online feedback, teleoperation is difficult to scale: it's labor-intensive, prone to operator fatigue, and limited by the embodiment gap between human and robot kinematics. The lack of rich haptic feedback and difficulty stabilizing extreme motions (e.g., deep squats) further constrain its applicability. To address these scaling challenges, automated data augmentation has been explored, particularly for robotic manipulation. Many works leverage state-of-the-art generative models for visual \cite{zhang2024diffusion, tian2024view, chen2024rovi} and semantic \cite{mandi2022cacti, chen2023genaug, yu2023scaling} augmentations, while others rely on simple open-loop kinematic replay of base trajectories  \cite{mandlekar2023mimicgen, jiang2024dexmimicgen, garrett2024skillmimicgen} or trajectory optimization \cite{yang2025physics} in simulation. 
Despite the advances in manipulation, data augmentation for whole-body loco-manipulation remains largely unexplored. 
The closest prior work~\cite{starke2019neural} interpolates keypoints to augment objects of different shape, but it cannot deal with varied object poses either. 
\OmniRetarget directly addresses this gap.",5534
http://arxiv.org/abs/2509.16518v1,2509.16518v1,2025-09-20T03:48:32+00:00,FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers,"Generating realistic videos with diffusion transformers demands significant
computation, with attention layers the central bottleneck; even producing a
short clip requires running a transformer over a very long sequence of
embeddings, e.g., more than 30K embeddings for a 5-second video, incurring
significant latency. Prior work aims to mitigate this bottleneck by exploiting
sparsity in the attention layers to reduce computation. However, these works
typically rely on block-sparse attention, which skips score computation only
when all entries in a block of attention scores (corresponding to M queries and
M keys, with M = 64 typically) are zero. This coarse-granular skipping of
attention scores does not fully exploit sparsity in the attention map and
leaves room for improvement. In this work, we propose FG-Attn, a sparse
attention mechanism for long-context diffusion transformers that leverages
sparsity at a fine granularity. Unlike block-sparse attention, which skips
entire MxM blocks, our approach skips computations at the granularity of Mx1
slices of the attention map. Each slice is produced by query-key dot products
between a block of query vectors and a single key. To implement our proposed
sparse attention mechanism, we develop a new efficient bulk-load operation
called asynchronous-gather load. This load operation gathers a sparse set of
relevant key-value vectors from memory and arranges them into packed tiles in
the GPU's shared memory. Only a sparse set of keys relevant to those queries
are loaded into shared memory when computing attention for a block of queries,
in contrast to loading full blocks of key tokens in block-sparse attention. Our
fine-grained sparse attention, applied to video diffusion models, achieves an
average 1.55X (up to 1.65X) speedup for 5 second, 480p videos, and an average
1.41X (up to 1.49X) for 5 second, 720p videos on a single H100 GPU.","\textbf{Block sparse attention.} 
Several implementations of block sparse attention~\cite{bsa, flashattn, flexattn, flashinfer, flashmask} propose a coarse-grained sparse attention mechanism that skips entire blocks of attention score computations at granularity of $64\times 64$ or $128\times 128$ at half-precision. Current block-sparse attention mechanisms either prevent further reduction of block size (do not compile) or cause significant hardware underutilization and performance overhead, since they are constrained by the tensor core matrix multiplication width (\cref{sec:motivation_skip_attn_fine_granularity}). Several works in the large language model literature~\cite{minference, xattn, flashdecode, nsa, seerattn} utilize block sparse attention to accelerate attention computation. 


\textbf{Block sparse attention for videoDiTs.} 
Recent works, such as Radial Attention~\cite{radialattn}, X-attention~\cite{xattn}, SparseVideoGen~\cite{sparsevideogen}, and SparseVideoGen2~\cite{sparsevideogen2}, have applied block sparse attention implementations to video diffusion models. These approaches consider a fixed sparsity pattern in the attention map based on empirical observations of significant patterns. Other works, such as Video Sparse Attention~\cite{vsa}, incorporate learned sparse attention patterns by using a parameterized model to derive the attention map mask. Both approaches utilize coarse-grained sparse attention mechanisms. In contrast, our method enables fine-grained skipping of attention blocks, providing more opportunities for skipping computation. We compare \X with SparseVideoGen and Radial Attention in~\cref{sec:results}. 
Moreover, trainable sparse attention methods such as Video Sparse Attention (VSA)~\cite{vsa} can be reformulated to generate sparse masks compatible with \X \textquotesingle s attention kernel. These methods are orthogonal to \X \textquotesingle s kernel implementation and can be used in conjunction as mask-determination strategies for \X.




\textbf{Other techniques to accelerate video diffusion.}
SpargeAttention~\cite{spargeattn}, SageAttention~\cite{sageattn}, and SageAttention2~\cite{sageattn2} propose general attention approximation techniques, such as quantization and token compression mechanisms, that can be applied during inference for both LLM and DiT models. Token compression-based approaches may skip essential tokens relevant to the video, which could lead to inconsistent video generation (pointed out by~\cite{radialattn}). These approaches are orthogonal to our \X.",2556
http://arxiv.org/abs/2510.06871v2,2510.06871v2,2025-10-08T10:39:12+00:00,SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,"Multimodal Large Reasoning Models (MLRMs) demonstrate impressive cross-modal
reasoning but often amplify safety risks under adversarial or unsafe prompts, a
phenomenon we call the \textit{Reasoning Tax}. Existing defenses mainly act at
the output level and do not constrain the reasoning process, leaving models
exposed to implicit risks. In this paper, we propose SaFeR-VLM, a
safety-aligned reinforcement learning framework that embeds safety directly
into multimodal reasoning. The framework integrates four components: (I)
QI-Safe-10K, a curated dataset emphasizing safety-critical and
reasoning-sensitive cases; (II) safety-aware rollout, where unsafe generations
undergo reflection and correction instead of being discarded; (III) structured
reward modeling with multi-dimensional weighted criteria and explicit penalties
for hallucinations and contradictions; and (IV) GRPO optimization, which
reinforces both safe and corrected trajectories. This unified design shifts
safety from a passive safeguard to an active driver of reasoning, enabling
scalable and generalizable safety-aware reasoning. SaFeR-VLM further
demonstrates robustness against both explicit and implicit risks, supporting
dynamic and interpretable safety decisions beyond surface-level filtering.
SaFeR-VLM-3B achieves average performance $70.13$ and $78.97$ on safety and
helpfulness across six benchmarks, surpassing both same-scale and $>10\times$
larger models such as Skywork-R1V3-38B, Qwen2.5VL-72B, and GLM4.5V-106B.
Remarkably, SaFeR-VLM-7B benefits from its increased scale to surpass
GPT-5-mini and Gemini-2.5-Flash by \num{6.47} and \num{16.76} points
respectively on safety metrics, achieving this improvement without any
degradation in helpfulness performance. Our codes are available at
https://github.com/HarveyYi/SaFeR-VLM.","\vspace{-0.4em}
\paragraph{Multimodal Large Reasoning Models (MLRM).} 
MLRMs extend MLLMs~\cite{MLLM-flamingo, MLLM-gpt4o, MLLM-cogvlm, MLLM-qwen2vl, MLLM-qwen2.5vl} by enhancing multimodal reasoning capabilities for complex decision-making tasks. Recent advances, inspired by \llmname{OpenAI's O1}~\cite{MLRM-o1} and \llmname{DeepSeek-R1}~\cite{MLRM-r1}, have integrated reinforcement learning methods like GRPO~\cite{MLRM-grpo} to improve generalization beyond supervised fine-tuning, achieving success in mathematical reasoning~\cite{MLRM-math-lmmr1, MLRM-math-ursa}, spatial understanding~\cite{MLRM-spatial-star}, and visual perception~\cite{MLRM-per-perception, MLRM-per-visionr1, MLRM-per-vrft}. Furthermore, multimodal CoT reasoning~\cite{MLRM-mcot-chain, MLRM-mcot-grit, MLRM-mcot-rex, MLRM-mcot-deepeyes} and self-reflection mechanisms~\cite{MLRM-reflect-mulberry, MLRM-reflect-r3, MLRM-reflect-srpo, MLRM-reflect-vl-rethinker} enable models to integrate visual feedback and revise erroneous reasoning paths. Robustness is additionally enhanced by data augmentation methods~\cite{MLRM-aug-visionmatters, MLRM-aug-shareVL, MLRM-aug-thinknot}, while diverse reward strategies~\cite{MLRM-reward-got-r1, MLRM-reward-pixel, MLRM-reward-sophiavl} improve efficiency and control reasoning quality. Despite these advances, the safety of MLRMs remains underexplored. We introduce \method{}, which embeds reflection and correction~\cite{kumar2024training, safe-safemlrm} into the reasoning process, ensuring that safety shapes reasoning dynamics rather than only outcomes.





\vspace{-0.4em}
\paragraph{Safety of MLLMs.}
Multimodal large language models (MLLMs) have enabled advanced multimodal reasoning but also raise critical safety risks, including adversarial manipulation~\cite{safe-risk-safety, safe-guard-eta, safe-risk-figstep}, harmful content generation~\cite{safe-risk-mllmguard, mmsafetybench, safe-risk-usb}, and representational biases~\cite{safe-risk-aialign, safe-risk-red}. Addressing these challenges requires both \textbf{training-based alignment} and \textbf{inference-time defenses}.  
\textbf{Training-based alignment} incorporates safety during model development, typically guided by the Helpful, Honest, and Harmless principle~\cite{safe-3h}. Representative techniques include supervised fine-tuning with safety-oriented datasets~\cite{vlguard, safe-algin-think}, reinforcement learning from human feedback~\cite{safe-algin-saferlhfv, safe-algin-G-RLHF-V}, and direct preference optimization~\cite{safe-algin-ADPO, safe-algin-SafeVid}. Recent studies further explore generative reward modeling and safe reasoning distillation~\cite{safe-guard-vlmguard} to guide corrective behaviors.  
\textbf{Inference-time defenses} regulate model behavior during deployment without modifying parameters. These include prompt rewriting~\cite{safe-guard-rapguard, safe-guard-vlmguard}, adaptive defense prompting~\cite{safe-guard-adashield}, harm detection modules~\cite{safe-guard-mllmprotector, safe-guard-guardreasoner}, and controlled decoding~\cite{safe-guard-coca, safe-guard-immune}, which mitigate risks while preserving utility.  
However, most existing methods remain outcome-level, constraining outputs without addressing the reasoning dynamics. This gap underscores the need to embed \textbf{safety-aware reasoning} directly into the model’s thought process, making safety an intrinsic driver of reasoning rather than a superficial safeguard.",3466
http://arxiv.org/abs/2509.20762v2,2509.20762v2,2025-09-25T05:38:58+00:00,Identifying Group Anchors in Real-World Group Interactions Under Label Scarcity,"Group interactions occur in various real-world contexts, e.g., co-authorship,
email communication, and online Q&A. In each group, there is often a
particularly significant member, around whom the group is formed. Examples
include the first or last author of a paper, the sender of an email, and the
questioner in a Q&A session. In this work, we discuss the existence of such
individuals in real-world group interactions. We call such individuals group
anchors and study the problem of identifying them. First, we introduce the
concept of group anchors and the identification problem. Then, we discuss our
observations on group anchors in real-world group interactions. Based on our
observations, we develop AnchorRadar, a fast and effective method for group
anchor identification under realistic settings with label scarcity, i.e., when
only a few groups have known anchors. AnchorRadar is a semi-supervised method
using information from groups both with and without known group anchors.
Finally, through extensive experiments on thirteen real-world datasets, we
demonstrate the empirical superiority of AnchorRadar over various baselines
w.r.t. accuracy and efficiency. In most cases, AnchorRadar achieves higher
accuracy in group anchor identification than all the baselines, while using
10.2$\times$ less training time than the fastest baseline and 43.6$\times$
fewer learnable parameters than the most lightweight baseline on average.","IV Concepts and Problem 3
V Observations on Real-World Datasets 3
V-A Observation 1: Informative Yet Limited Topological Features (Table III) . . . . . . . . . . . . . . . . . . . . . . . . . . 4
V-B Observation 2: Stable Yet Contextual Anchorship Across Groups (Tables IV & V) . . . . . . . . . . . . . . . . . . . . 4
VI The Proposed Method: ANCHORRADAR 5
VI-A Stage 1: MLP Training and Topology-based Scores of Node-Hyperedge Pairs . . . . . . . . . . . . . . . . . . . . . . . 6
VI-B Stage 2: Anchor Strength Learning, Global Aggregation, and Final Prediction . . . . . . . . . . . . . . . . . . . . . . . 6
VI-C Complexity Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
VII Experiments 7
VII-A Experimental Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
VII-B Q",918
http://arxiv.org/abs/2510.06504v1,2510.06504v1,2025-10-07T22:41:23+00:00,Text2Interact: High-Fidelity and Diverse Text-to-Two-Person Interaction Generation,"Modeling human-human interactions from text remains challenging because it
requires not only realistic individual dynamics but also precise,
text-consistent spatiotemporal coupling between agents. Currently, progress is
hindered by 1) limited two-person training data, inadequate to capture the
diverse intricacies of two-person interactions; and 2) insufficiently
fine-grained text-to-interaction modeling, where language conditioning
collapses rich, structured prompts into a single sentence embedding. To address
these limitations, we propose our Text2Interact framework, designed to generate
realistic, text-aligned human-human interactions through a scalable
high-fidelity interaction data synthesizer and an effective spatiotemporal
coordination pipeline. First, we present InterCompose, a scalable
synthesis-by-composition pipeline that aligns LLM-generated interaction
descriptions with strong single-person motion priors. Given a prompt and a
motion for an agent, InterCompose retrieves candidate single-person motions,
trains a conditional reaction generator for another agent, and uses a neural
motion evaluator to filter weak or misaligned samples-expanding interaction
coverage without extra capture. Second, we propose InterActor, a
text-to-interaction model with word-level conditioning that preserves
token-level cues (initiation, response, contact ordering) and an adaptive
interaction loss that emphasizes contextually relevant inter-person joint
pairs, improving coupling and physical plausibility for fine-grained
interaction modeling. Extensive experiments show consistent gains in motion
diversity, fidelity, and generalization, including out-of-distribution
scenarios and user studies. We will release code and models to facilitate
reproducibility.","\subsection{Text-to-Human Motion Generation}
\vspace{-0.5mm}


Text-to-motion generation aims to synthesize human motion sequences from natural language descriptions~\citep{fan2024freemotion, tanke2023social, jeong2024multi, jiang2023motiongpt, guo2022generating, zhang2023generating, wan2024tlcontrol, lu2024scamo, guo2024momask}. Early methods such as Text2Action~\citep{ahn2018text2action} and Language2Pose~\citep{ahuja2019language2pose} utilized GANs and sequence-to-sequence architectures to map text to motion, laying foundational work in this area. Subsequent approaches leveraged variational autoencoders (VAEs) for probabilistic generation, including Guo et al.~\citep{guo2022generating} and TEMOS~\citep{petrovich2022temos}, which improved motion diversity and fluency. More recent advancements have focused on powerful generative models. Diffusion-based approaches such as MDM~\citep{tevet2022human} and latent diffusion via MLD~\citep{chen2023executing} significantly improved motion realism and sample efficiency. T2M-GPT~\citep{zhang2023generating} employed autoregressive transformers for fine-grained motion synthesis, while MoMask~\citep{guo2024momask} introduced generative masked transformers to enhance fidelity under the autoregressive paradigm. ReMoDiffuse~\citep{zhang2023remodiffuse} further enhanced generation quality by retrieving reference motions from a motion database. Parallel to improving generation quality, increasing attention has been given to controllable text-to-motion generation. Techniques have explored conditioning on spatial trajectories~\citep{shafir2023human, karunratanakul2023guided, wan2024tlcontrol, xie2023omnicontrol} and linguistic constraints~\citep{wan2024tlcontrol, huang2024controllable} to provide more precise control over generated outputs. Additionally, MotionCLIP~\citep{tevet2022motionclip} aligned motion and language embeddings in a shared space, enabling zero-shot text-to-motion generation. Despite stellar results in single-person motion generation, extending them to two-person interactions introduces additional challenges such as modeling inter-agent coordination and handling semantically richer text descriptions. Our work builds on these foundations by proposing a scalable framework that composes diverse and semantically aligned two-person interactions from single-person motion priors and language models.




\subsection{Human-Human Interaction Generation}
\vspace{-0.5mm}
Although some progress has been achieved in multi-human interaction modeling~\citep{fan2024freemotion, tanke2023social, jeong2024multi}, prior works on human interaction modeling have been mostly focused on the two-person interaction problem. A pioneer work, ComMDM~\citep{shafir2023human}, explores two-person motion generation by using a bridge network to compose the outputs of two single-person motion diffusion models~\citep{tevet2022human}. RIG~\citep{tanaka2023role} and InterGen~\citep{liang2024intergen} first trained dedicated networks to directly model two-person interaction. in2IN~\citep{ruiz2024in2in} explores the simultaneous use of individual and interaction descriptions to enhance textual alignment and generation quality. MoMat-MoGen~\citep{cai2024digital} proposes to enhance generation quality by retrieving from a motion database and a generative framework that models interactive behaviors between agents, considering personality, motivations, and interpersonal relationships. InterMask~\citep{javed2024intermask} utilizes the generative masked transformer architecture and spatial-temporal attention to enhance generation quality and text-motion alignment. TIMotion~\citep{wang2024temporal}, a contemporaneous work, proposes to model the human interaction sequence in a causal sequence, leveraging the temporal and causal properties of human motions. Although these methods have achieved impressive results, there remains significant possibilities of improvement due to their common flaw of limited training corpus and inadequate text modeling granularity. In this paper, we aim to tackle these two key issues with our generative interaction composition framework and fine-grained word-level conditioning module.",4190
http://arxiv.org/abs/2510.04652v1,2510.04652v1,2025-10-06T09:58:05+00:00,Modeling and Managing Temporal Obligations in GUCON Using SPARQL-star and RDF-star,"In the digital age, data frequently crosses organizational and jurisdictional
boundaries, making effective governance essential. Usage control policies have
emerged as a key paradigm for regulating data usage, safeguarding privacy,
protecting intellectual property, and ensuring compliance with regulations. A
central mechanism for usage control is the handling of obligations, which arise
as a side effect of using and sharing data. Effective monitoring of obligations
requires capturing usage traces and accounting for temporal aspects such as
start times and deadlines, as obligations may evolve over times into different
states, such as fulfilled, violated, or expired. While several solutions have
been proposed for obligation monitoring, they often lack formal semantics or
provide limited support for reasoning over obligation states. To address these
limitations, we extend GUCON, a policy framework grounded in the formal
semantics of SPAQRL graph patterns, to explicitly model the temporal aspects of
an obligation. This extension enables the expressing of temporal obligations
and supports continuous monitoring of their evolving states based on usage
traces stored in temporal knowledge graphs. We demonstrate how this extended
model can be represented using RDF-star and SPARQL-star and propose an
Obligation State Manager that monitors obligation states and assess their
compliance with respect to usage traces. Finally, we evaluate both the extended
model and its prototype implementation.","There are several works that focus on usage control in general and obligation monitoring in particular. UCON [34]
is an abstract model that extends access control with the concepts of obligations, decision continuity and attribute
mutability. Although several formalisms have been suggested for UCON, and attempts have been made to include it
in standard representation languages [11, 33], there is currently no established reference or standard policy specifi-
cation and implementation for UCON. As a result, UCON has not gained widespread adoption in industry. Another
language, OSL, formalized in Z [23], is utilized to express conditional prohibitions and obligations. Although one
can express temporal constraints using OSL, the obligations in OSL do not support the notion of obligation states.
Additionally, it is unclear how said obligations should be enforced.
In the Semantic Web community, several general policy languages and frameworks have been proposed, including
Protune [5], Rei [29], Ponder [12], KAoS [49], and the DSA policy framework [44]. Protune focuses on access
control and trust management, but lacks support for expressing obligations. While, Rei, Ponder, KAoS, and the
DSA framework can express obligations, their design lacks the states of obligations, which limits their ability to
monitor obligation lifecycles. Furthermore, it remains unclear how obligations expressed in these languages are
enforced in practice. Additionally, Ponder lacks formal semantics, which limits its applicability in policy reasoning.
In terms of policy standards, ODRL18is a W3C recommendation that provides a model and vocabulary for de-
scribing policies, including obligations. Although ODRL does not have yet an official formal semantics, several
efforts have attempted to formalize it, either by using web ontologies [20] or by defining operational semantics
through rule-based reasoning [17, 46]. Notably, Fornara et al. [18] focused on enriching ODRL by extending the
model with the notions of permission and obligation states. The operational semantics of this extended model are im-
plemented using a production rule system. Our extended GUCON model builds upon the obligation states formally
defined by Fornara et al. [18]. However, we simplify the GUCON model by including only temporal metaproperties
and define the formal semantics of obligation states separately. Currently, a W3C group19is working on defining
the formal semantics of ODRL; which is currently only described informally in plain English, without any formal
specification. Cimmino and Fornara [8] highlight several limitations of ODRL. Although ODRL supports express-
ing various types of constraints; such as temporal and spatial constraints; they lack formal semantics. In particular,
it remains unclear how to reason effectively about obligation start times and deadlines. Furthermore, ODRL does
not incorporate the notion of the state of affairs, which complicates the enforcement and/ or policy re-evaluation
whenever the policy or context changes. In contrast, the state of the affairs, represented as a KB, is a fundamental
component of our GUCON framework. GUCON also leverages graph patterns, enabling straightforward and dy-
namic policy re-evaluation against the current state of the world. SHACL is another standard that has been explored
18ODRL, https://www.w3.org/TR/odrl-model/
19ODRL Formal Semantics, https://w3c.github.io/odrl/formal-semantics/
N. Akaichi et al. / Managing Temporal Obligations in GUCON19
(a) Task 1: Execution Time in Terms of Policy Size
(b) Task 2: Execution Time in Terms of KB Size
Figure",3617
http://arxiv.org/abs/2509.26173v1,2509.26173v1,2025-09-30T12:28:35+00:00,Understanding Collective Social Behavior in OSS Communities: A Co-editing Network Analysis of Activity Cascades,"Understanding the collective social behavior of software developers is
crucial to model and predict the long-term dynamics and sustainability of Open
Source Software (OSS) communities. To this end, we analyze temporal activity
patterns of developers, revealing an inherently ``bursty'' nature of commit
contributions. To investigate the social mechanisms behind this phenomenon, we
adopt a network-based modelling framework that captures developer interactions
through co-editing networks. Our framework models social interactions, where a
developer editing the code of other developers triggers accelerated activity
among collaborators. Using a large data set on 50 major OSS communities, we
further develop a method that identifies activity cascades, i.e. the
propagation of developer activity in the underlying co-editing network. Our
results suggest that activity cascades are a statistically significant
phenomenon in more than half of the studied projects. We further show that our
insights can be used to develop a simple yet practical churn prediction method
that forecasts which developers are likely to leave a project. Our work sheds
light on the emergent collective social dynamics in OSS communities and
highlights the importance of activity cascades to understand developer churn
and retention in collaborative software projects.","Understanding social aspects in software development has
emerged as an important research area in empirical software
engineering, organizational theory, and computational so-
cial science. Our work builds upon several interconnected
research streams that examine temporal collaboration pat-
terns, social networks, and emergent coordination in OSS
communities, which we summarize below.
Collaboration Patterns in Software ProjectsThe appli-
cation of social network analysis to software projects has
provided important insights into how the structure and tem-
poral patterns of collaboration influence project outcomes.
Z¨oller et al. (2020) highlight connections between GitHub
collaboration networks and project success, showing how
network topology affects the resilience and adaptability of
3
Understanding Collective Social Behavior in OSS Communities: A Co-editing Network Analysis of Activity Cascades
projects. In particular, their work shows that the hierar-
chical group structure of collaboration networks influences
long-term sustainability and productivity of Open Source
communities. Studying co-editing networks extracted from
software repositories, Scholtes et al. (2016) studied the in-
terplay between the densification of collaboration structures
and developer productivity. Mapping the social structure of
Free and Open Source Software Communities, Crowston
& Howison (2005) studied interactions in bug fixing pro-
cesses. They find that the centralization of communication
is an important characteristic that distinguishes projects,
where larger communities tend to have more decentralized
communication patterns.
The temporal dimension of collaboration networks has
gained increasing attention in software engineering re-
search. (Gote et al., 2019) developed a method to mine
time-stamped co-editing networks from Git repositories,
which is a foundation for temporal network analysis in OSS
communities. A follow-up work further showed that the co-
editing structure influences the timing of commits, where
developers are faster to edit their own code compared to
code by other developers (Gote et al., 2022). Consider-
ing temporal patterns of commit behavior, Ma et al. (2014)
reveal a power law distribution of file- and project-level
inter-commit times, which is an indication of collective dy-
namics. Similarly, (Sornette et al., 2014) find a heavy-tail
distribution of developer commit contributions, arguing that
it is indicative of a collective dynamical process. Studies of
temporal activity patterns reveal that contribution patterns
are strong predictors of sustained engagement (Xiao et al.,
2023). This finding suggests that responsive behavior early
in a developer’s participation may influence long-term com-
mitment. To understand collaboration dynamics in large
software projects, the timing of interactions may thus be as
important as their content.
Cascade Analysis in Complex NetworksWhile studying
the structure of temporal collaboration networks is impor-
tant, a crucial and less investigated further aspect is that
those networks are the substrate of dynamical processes
that operate on them. An important class of processes are
cascadesby which information, failures, diseases or so-
cial behavior propagates through complex networks. This
phenomenon has been extensively studied in various com-
plex networks, including social media (Goel et al., 2012;
V osoughi et al., 2018; Gomez-Rodriguez et al., 2012), fi-
nancial markets (Elliott et al., 2014; Glasserman & Young,
2016), and biological networks (Pastor-Satorras et al., 2015).
Traditional cascade analysis focuses on information propa-
gation (Kempe et al., 2003) or behavioral contagion (Cen-
tola, 2010), where actions by one agent influence the prob-
ability of similar actions by connected agents. However,
fewer studies have analyzed the propagation of cascadesin the context of collaborative software projects. Being a
natural application of models of failure cascades in tech-
nical systems, some studies have considered the propaga-
tion of changes or failures in software dependency net-
works. Geipel & Schweitzer (2009) studied cascades of
changes propagating through the dependency networks of
Open Source projects. More recently, (Shehata et al., 2025)
investigated how project failures propagate across the de-
pendency network of the Maven Central ecosystem. Neither
of the two works did link this to the developer collaboration
networks of the underlying projects. While Sornette et al.
(2014) studied a “cascading model of productive activity”
in terms of a Hawkes Poisson process, this model deviates
from previously cited works on cascades in the sense that
it did not consider the propagation of cascades in a (social)
network. Recently, Schueller & Wachs (2024) demonstrated
how risks propagate through dependency and maintainer
networks, showing that social factors can amplify technical
vulnerabilities.
Modeling and Predicting Developer TurnoverResearch
on developer motivation and retention has established theo-
retical frameworks for understanding collaborative engage-
ment. Self/Determination Theory (SDT) and Social Identity
Theory have been applied to explain how autonomy, compe-
tence, and social belonging influence developer participation
(Schilling, 2012). While these theories focus on individual
psychological factors, our work examines how social inter-
actions create behavioral cascades that influence collective
activity patterns. The relationship between project charac-
teristics and developer behavior has been explored through
various lenses. Betti et al. (2025) reveal that leadership
changes are common in OSS projects and significantly im-
pact various success metrics. Their findings about leadership
transitions provide context for understanding how individ-
ual departures can trigger broader changes in project dy-
namics—a phenomenon that may be related to the cascade
effects we investigate. Studies of value-based discussions in
OSS projects have shown that conversations about project
values can predict changes in contributor turnover patterns
(Jamieson et al., 2024). This work highlights the importance
of social dynamics beyond pure technical considerations,
supporting our hypothesis that social responsiveness plays
a crucial role in collaborative software development. The
application of survival analysis to understand developer
turnover patterns has provided insights into retention factors
(Lin et al., 2017). Using recent advances in deep graph
learning. Chang et al. (2024) use graph neural networks to
predict which developers are likely to leave a project.
Research GapWhile existing research has established
foundations for understanding social dynamics in software
projects, several research gaps remain. Most studies fo-
cus on network properties or individual-level factors, with
4
Understanding Collective Social Behavior in OSS Communities: A Co-editing Network Analysis of Activity Cascades
limited attention to dynamic social processes that emerge
from direct developer interactions. To the best of our knowl-
edge, no studies examined how developer activity propa-
gates through temporal collaboration networks and how this
process shapes the collective dynamics of developer contri-
butions. Finally, the resulting collective dynamics has not
been considered as a factor that influences which developers
are likely to leave a project. Addressing these gaps, our
work introduces cascade analysis as a tool to understand
collective social dynamics in software development. Unlike
previous works that treat social behavior as exogenous fac-
tor, we position social dynamics as endogeneous driver of
collaborative behavior that can be measured, analyzed, and
leveraged to improve our understanding of large software
projects.",7809
http://arxiv.org/abs/2509.25134v1,2509.25134v1,2025-09-29T17:50:12+00:00,LayerD: Decomposing Raster Graphic Designs into Layers,"Designers craft and edit graphic designs in a layer representation, but
layer-based editing becomes impossible once composited into a raster image. In
this work, we propose LayerD, a method to decompose raster graphic designs into
layers for re-editable creative workflow. LayerD addresses the decomposition
task by iteratively extracting unoccluded foreground layers. We propose a
simple yet effective refinement approach taking advantage of the assumption
that layers often exhibit uniform appearance in graphic designs. As
decomposition is ill-posed and the ground-truth layer structure may not be
reliable, we develop a quality metric that addresses the difficulty. In
experiments, we show that LayerD successfully achieves high-quality
decomposition and outperforms baselines. We also demonstrate the use of LayerD
with state-of-the-art image generators and layer-based editing.","\subsection{Image Layer Decomposition}
Image layer decomposition is a task to decompose an image into a sequence of layers, which are composable with a specific compositing function (e.g., alpha compositing) to reproduce or approximate the original image~\cite{porter1984compositing}.
Color segmentation represents an image with semi-transparent color layers, targeting digital paintings~\cite{tan2016decomposing} and natural images~\cite{tan2018efficient,aksoy2017unmixing,akimoto2020fast}.
Koyama \etal~\cite{koyama2018decomposing} propose to handle non-linear color blending functions, followed by the efficient deep learning-based extension~\cite{horita2022fast}.

There have been many studies on decomposing natural scenes at the object level~\cite{isola2013scene,monnier2021unsupervised,zhan2020self,zheng2021visiting,mulan,liu2024object}. For instance, PCNet~\cite{zhan2020self} decomposes a scene image into object layers by estimating the order of objects and the RGB of occluded parts.
While PCNet assumes the object modal mask is given, Zhang \etal~\cite{zheng2021visiting} create layered data including occluded parts in indoor scenes and decompose the image by training instance segmentation, depth estimation, and background completion.
Text2Layer~\cite{zhang2023text2layer} extracts salient objects from natural images using matting and generates training data for layered image generation.
\TODO{違いを明確に。ただしpreprint}
Recently, MULAN~\cite{mulan} decomposes natural images, including outdoor scenes where obtaining ground truth data is difficult, by combining the latest off-the-shelf open vocabulary object detection models~\cite{yao2023detclipv2}, zero-shot segmentation~\cite{kirillov2023segment}, depth estimation~\cite{ranftl2020towards}, and instance ordering~\cite{lee2022instance} with heuristics.
While the above studies mainly focus on object decomposition, Yang \etal~\cite{yang2024generative} decompose physical object effects (\eg, shadows or reflections) as well.

Compared to the natural image decomposition, graphic design decomposition has to deal with different granularities of \emph{objects}; \eg, a corporate logo in a graphic design consists of an illustration and a text, and whether they should be decomposed into parts depends on the context.
Considering the nature of the task, we propose a simple and effective method and a new quantitative evaluation protocol for inconsistent ground-truth.
A concurrent work~\cite{accordion} tackles the same task as ours with a stacked pipeline approach using a VLM trained on closed data.
\ours{}'s pipeline is overwhelmingly simple and leverages domain knowledge to refine the final quality.
We compare \ours{} with a VLM-based pipeline in our experiment.


\subsection{Image Vectorization}
Related to layer decomposition, image vectorization converts an image or a part into a set of parameters of a specific drawing function, rather than layer images.
Our layer decomposition approach can be useful for vectorization as a pre-processing step to extract part-based raster images.
Du \etal~\cite{du2023image} and Favreau \etal~\cite{favreau2017photo2clipart} obtain a sequence of linear gradient layers that approximate the original image by optimization using alpha blending.
Several works attempt to generate SVG-based representation from raster images~\cite{shen2021clipgen,ma2022towards,song2023clipvg,carlier2020deepsvg,reddy2021im2vec,rodriguez2023starvector},
where they typically assume vector art, cleanly masked images, or clean segmented images as input.
A few specifically focus on typographic representation in graphic design, where they estimate text rendering parameters~\cite{accordion,shimoda2021rendering}.





\subsection{Image Matting and Foreground Extraction}
\TODO{segmentationも?}
Image matting is a task to estimate alpha mattes of objects in an image, and together with other tasks such as background inpainting, forms the layer decomposition task.
Matting approach often assumes user-specified trimap~\cite{chuang2001bayesian,sun2004poisson,xu2017deep,yao2024vitmatte}, and a few trimap-free methods have been reported recently~\cite{birefnet,li2023matting}.
\ours{} mainly uses network architectures used in matting~\cite{birefnet} to extract unoccluded top layers.

While matting estimates alpha mattes, foreground color estimation involves determining the color of the foreground that is mixed with the background.
There are energy-based methods~\cite{levin2007closed,chen2013knn,aksoy2017designing} and their efficient versions~\cite{germer2021fast,forte2021approximate}, as well as deep learning-based methods~\cite{Lutz2021foreground} that estimate the foreground color given the alpha.
Hou \etal and Li \etal simultaneously estimate the alpha map and foreground color given an image and a trimap~\cite{hou2019context,li2025drip}.
The foreground color is deterministic when the background color and foreground alpha are given.
In our setup, we obtain the foreground matte from our trained model and the background color from high-quality background inpainting~\cite{lama}, and then calculate the foreground color.",5125
http://arxiv.org/abs/2510.11133v1,2510.11133v1,2025-10-13T08:22:38+00:00,Test-Time Adaptation by Causal Trimming,"Test-time adaptation aims to improve model robustness under distribution
shifts by adapting models with access to unlabeled target samples. A primary
cause of performance degradation under such shifts is the model's reliance on
features that lack a direct causal relationship with the prediction target. We
introduce Test-time Adaptation by Causal Trimming (TACT), a method that
identifies and removes non-causal components from representations for test
distributions. TACT applies data augmentations that preserve causal features
while varying non-causal ones. By analyzing the changes in the representations
using Principal Component Analysis, TACT identifies the highest variance
directions associated with non-causal features. It trims the representations by
removing their projections on the identified directions, and uses the trimmed
representations for the predictions. During adaptation, TACT continuously
tracks and refines these directions to get a better estimate of non-causal
features. We theoretically analyze the effectiveness of this approach and
empirically validate TACT on real-world out-of-distribution benchmarks. TACT
consistently outperforms state-of-the-art methods by a significant margin.","Existing TTA methods can be broadly categorized into backpropagation-free and backpropagation-
based methods. Backpropagation-free methods modify model outputs or intermediate representations
without gradient-based optimization. These include modifiable prompts [ 36], re-normalized represen-
tations [ 44], updated prototypes [ 19,57], and maximum likelihood estimation [ 4]. Backpropagation-
based methods update the model with the gradient of objective functions such as entropy minimization
[12,37,38,51] and self-training with pseudo-labels [ 17,25,47,52]. Entropy Minimization encour-
ages more confident predictions by reducing the entropy of model predictions during adaptation.
Self-training employs cross entropy [ 5,17,30,47] and knowledge distillation [ 25,52,53] using
model predictions as pseudo-labels. Regularization measures such as information maximization
[30], representation statistics alignment [ 23,58], and consistency regularization [ 35,56] for invariant
prediction under augmentations have been proposed to regularize the adaptation.
A key challenge in test-time adaptation is obtaining reliable pseudo-labels to guide model updates.
One line of work assumes that correct predictions tend to exhibit low entropy, and update the model
using only high-confidence samples with low-entropy predictions [ 19,37,38,57]. However, DeYO
[29] shows that spurious correlations can also result in low entropy predictions and proposes a causal
intervention technique to identify predictions that are more likely based on causal features, using
them selectively for model updates. Another line of work refines pseudo-labels by incorporating
updated prototype and neighborhood information [ 5,17,21,47,53]. AdaContrast [ 5] uses soft voting
among nearest neighbors. TSD [ 53] relies on updated prototypes and spatial local clustering. TAST
[21] employs neighbourhood information in self-training. PROGRAM [ 47] considers both prototype
2
and neighbour-based pseudo-labels to enhance label quality. PASLE [ 17] progressively refines the
pseudo-labels of uncertain predictions using updated prototypes.
All the above methods, except for DeYO, do not consider the effect of non-causal features on model
prediction. Although DeYO finds that non-causal features would make entropy an unreliable metric to
reflect prediction correctness, it does not adjust model predictions. TACT adjusts model predictions by
reducing non-causal features, and our adjusted prediction can be used as a more reliable pseudo-label.",2518
http://arxiv.org/abs/2509.16772v1,2509.16772v1,2025-09-20T18:38:54+00:00,"AI Knows Best? The Paradox of Expertise, AI-Reliance, and Performance in Educational Tutoring Decision-Making Tasks","We present an empirical study of how both experienced tutors and non-tutors
judge the correctness of tutor praise responses under different Artificial
Intelligence (AI)-assisted interfaces, types of explanation (textual
explanations vs. inline highlighting). We first fine-tuned several Large
Language Models (LLMs) to produce binary correctness labels and explanations,
achieving up to 88% accuracy and 0.92 F1 score with GPT-4. We then let the
GPT-4 models assist 95 participants in tutoring decision-making tasks by
offering different types of explanations. Our findings show that although
human-AI collaboration outperforms humans alone in evaluating tutor responses,
it remains less accurate than AI alone. Moreover, we find that non-tutors tend
to follow the AI's advice more consistently, which boosts their overall
accuracy on the task: especially when the AI is correct. In contrast,
experienced tutors often override the AI's correct suggestions and thus miss
out on potential gains from the AI's generally high baseline accuracy. Further
analysis reveals that explanations in text reasoning will increase
over-reliance and reduce underreliance, while inline highlighting does not.
Moreover, neither explanation style actually has a significant effect on
performance and costs participants more time to complete the task, instead of
saving time. Our findings reveal a tension between expertise, explanation
design, and efficiency in AI-assisted decision-making, highlighting the need
for balanced approaches that foster more effective human-AI collaboration.","Prior research on AI-assisted decision-making has produced mixed results, under-
scoring that its benefits depend on contextual factors. On the one hand, studies
indicated that humans are sometimes misled by AI when it makes errors, espe-
cially if explanations are overly complex or fail to clarify uncertainties [18,4,6].
On the other hand, well-designed explanations can bolster users’ ability to judge
the correctness of AI outputs. For instance, [13] found that causal explanations
help participants recognize AI unreliability when the AI is wrong, reducing blind
AI Knows Best? The Paradox of Expertise, AI-Reliance, and Performance 3
reliance. However, if the AI’s decision is correct but the explanation is flawed,
most participants do not immediately doubt the overall judgment; rather, they
attempt to reconcile the erroneous explanation. Notably, visually misleading cues
often carry greater persuasive power than textual errors, and combining incor-
rect visual and textual explanations can create a “double misdirection” effect
[13]. If, however, visual and textual explanations conflict, participants are more
likely to question the AI’s decision.
Despite these challenges, well-designed AI systems can complement human
decision-makers, achieving performance that neither humans nor AI alone can
match [2]. Yet, research also shows that humans frequently either over-rely on
AI even when it is wrong or under-rely on it even when AI is correct, often miss-
ing the “appropriate reliance” that leverages AI’s suggestion when it is correct
and rejecting it when it errs [19,18,4,6]. Even when human-AI teams outperform
humans working by themselves, they often fail to exceed AI-only performance.
One key to boosting human-AI collaboration performance is enhancing AI ex-
plainability. For example, explanation designs that integrate the AI’s reasoning
within the question interface (e.g., inline highlights) reduce cognitive load and
help users better assess correctness [2,18]. Additionally, users’ reliance on AI may
vary according to their task expertise. For instance, [14] find that non-experts
who are less equipped to gauge correctness are more susceptible to over-reliance.
In this study, we aim to explore how different explanation methods, partic-
ularly highlighting and text explanations, can help improve human-AI perfor-
mance in evaluating the effectiveness of tutoring feedback.
2.1 Evaluate Tutor Giving Effective Praise with AI
According to prior work [17], giving effective praise involves genuine and timely
acknowledgment of a student’s specific strengths, avoiding overused expressions
that lose impact. Rather than saying “good job,” effective praise highlights the
student’s accomplishment by connecting it to effort rather than inherent ability.
Clear, truthful, and immediate praise fosters greater motivation, resilience, and
engagement. Prior work has employed AI to classify tutor responses like using
a BERT-based Named Entity Recognition (NER) model to classify the effec-
tive praise [12] and obtained up to 73% accuracy and 0.81 in F1 score. They
found that the model worked well for effort-related praise but it struggled with
outcomes-based praise because of the lack of training data.
We believe that leveraging Large Language Models (LLMs) can effectively
address the challenge of insufficient data. As highlighted in the report of [15],
LLMs have undergone comprehensive training with vast datasets, enabling them
to execute various tasks proficiently. We only need to provide a task description
and a few examples [3], and LLMs can effectively judge whether the tutor’s re-
sponse meets the standards of effective praise. Yet, the random process of text
generation [9] makes verifying LLM outputs difficult. As a result, designing ex-
planation methods that clarify the AI’s decision-making is essential for enabling
educators to interpret—and, when necessary, override—LLM judgments.",3935
http://arxiv.org/abs/2510.06842v1,2510.06842v1,2025-10-08T10:09:47+00:00,Continual Action Quality Assessment via Adaptive Manifold-Aligned Graph Regularization,"Action Quality Assessment (AQA) quantifies human actions in videos,
supporting applications in sports scoring, rehabilitation, and skill
evaluation. A major challenge lies in the non-stationary nature of quality
distributions in real-world scenarios, which limits the generalization ability
of conventional methods. We introduce Continual AQA (CAQA), which equips AQA
with Continual Learning (CL) capabilities to handle evolving distributions
while mitigating catastrophic forgetting. Although parameter-efficient
fine-tuning of pretrained models has shown promise in CL for image
classification, we find it insufficient for CAQA. Our empirical and theoretical
analyses reveal two insights: (i) Full-Parameter Fine-Tuning (FPFT) is
necessary for effective representation learning; yet (ii) uncontrolled FPFT
induces overfitting and feature manifold shift, thereby aggravating forgetting.
To address this, we propose Adaptive Manifold-Aligned Graph Regularization
(MAGR++), which couples backbone fine-tuning that stabilizes shallow layers
while adapting deeper ones with a two-step feature rectification pipeline: a
manifold projector to translate deviated historical features into the current
representation space, and a graph regularizer to align local and global
distributions. We construct four CAQA benchmarks from three datasets with
tailored evaluation protocols and strong baselines, enabling systematic
cross-dataset comparison. Extensive experiments show that MAGR++ achieves
state-of-the-art performance, with average correlation gains of 3.6% offline
and 12.2% online over the strongest baseline, confirming its robustness and
effectiveness. Our code is available at https://github.com/ZhouKanglei/MAGRPP.","\myPara{Action Quality Assessment} 
AQA aims to automatically evaluate the objective execution quality of human actions, spanning numerous applications in sports scoring \cite{xu2022finediving,parmar2019action,pirsiavash2014assessing,xu2025quality,xu2024vision}, rehabilitation \cite{zhou2023video}, and skill assessment \cite{zia2016automated}. 
A major challenge is the scarcity of annotated labels \cite{zhou2024comprehensivesurveyactionquality}, since reliable quality scores demand domain expertise. To address this, most approaches \cite{pan2021adaptive,yu2021group,parmar2019and} leverage PTMs (e.g., I3D \cite{carreira2017quo}) to extract strong visual features and then regress quality scores either via direct regression \cite{zhou2023hierarchical} or contrastive regression \cite{ke2024two}. Ranking-based skill assessment methods \cite{doughty2019pros,doughty2018s} further alleviate annotation costs by comparing relative performance instead of relying on absolute scores. Another core difficulty lies in fine-grained temporal parsing, as PTMs are optimized for coarse action recognition while AQA demands temporal sensitivity. To this end, strategies such as continual pretraining \cite{dadashzadeh2024pecop}, regularization \cite{zhou2025phi,zhou2024cofinal}, and human-centric cues \cite{xu2024fineparser,xu2025human} have been proposed to enhance feature representations. 
Furthermore, non-stationary variations across tasks pose additional challenges for CAQA. While recent works attempt to mitigate this by freezing backbone features \cite{li2024continual}, such designs restrict adaptation capacity and often overlook skill variations within the same action, where subtle distribution shifts make fine-grained evaluation more difficult.
In this work, we address these challenges by designing a framework that both adapts to evolving task distributions and preserves discriminative skill-related cues, enabling AQA in realistic evolving scenarios.

\myPara{Continual Learning}
CL \cite{wang2023incorporating,wang2024comprehensive} enables models to acquire new knowledge from a stream of tasks without forgetting previous ones. This capability is particularly valuable in real-world applications such as robotics, surveillance, and other dynamic vision domains \cite{zhou2025adaptive}.
The main challenge is to avoid catastrophic forgetting of previously learned knowledge. Current efforts can be broadly divided into constraint-based and replay-based methods. Constraint-based methods such as SI~\cite{zenke2017continual}, EWC~\cite{james2017ewc}, and LwF~\cite{li2017learning} impose regularization to preserve old knowledge without storing past data, but often suffer from limited scalability. Replay-based methods, by contrast, achieve stronger retention via exemplar storage (e.g., MER~\cite{riemer2019learning}, DER++~\cite{buzzega2020dark}, TOPIC~\cite{tao2020few}, and GEM~\cite{kukleva2021generalized}), which raises memory and privacy concerns. More recently, feature replay has emerged as a lightweight and privacy-preserving alternative (e.g., SLCA~\cite{zhang2023slca,zhang2024slca++}, NC-FSCIL~\cite{yang2023neural}, FS-Aug~\cite{li2024continual}, and MAGR~\cite{zhou2024magr}). However, applying feature replay in domains like AQA is challenging due to significant domain gaps, and continual adaptation of the backbone often induces manifold shifts, misaligning old and new feature distributions. Motivated by these, our work adopts feature replay as the primary technical route of CAQA while introducing adaptive strategies to alleviate feature misalignment.",3593
http://arxiv.org/abs/2510.10248v1,2510.10248v1,2025-10-11T15:05:45+00:00,Reasoning-Enhanced Large Language Models for Molecular Property Prediction,"Molecular property prediction is crucial for drug discovery and materials
science, yet existing approaches suffer from limited interpretability, poor
cross-task generalization, and lack of chemical reasoning capabilities.
Traditional machine learning models struggle with task transferability, while
specialized molecular language models provide little insight into their
decision-making processes. To address these limitations, we propose
\textbf{MPPReasoner}, a multimodal large language model that incorporates
chemical reasoning for molecular property prediction. Our approach, built upon
Qwen2.5-VL-7B-Instruct, integrates molecular images with SMILES strings to
enable comprehensive molecular understanding. We develop a two-stage training
strategy: supervised fine-tuning (SFT) using 16,000 high-quality reasoning
trajectories generated through expert knowledge and multiple teacher models,
followed by Reinforcement Learning from Principle-Guided Rewards (RLPGR). RLPGR
employs verifiable, rule-based rewards that systematically evaluate chemical
principle application, molecular structure analysis, and logical consistency
through computational verification. Extensive experiments across 8 datasets
demonstrate significant performance improvements, with MPPReasoner
outperforming the best baselines by 7.91\% and 4.53\% on in-distribution and
out-of-distribution tasks respectively. MPPReasoner exhibits exceptional
cross-task generalization and generates chemically sound reasoning paths that
provide valuable insights into molecular property analysis, substantially
enhancing both interpretability and practical utility for chemists. Code is
available at https://anonymous.4open.science/r/MPPReasoner-12687.","This section reviews prior research on machine learning for molecular representations, multimodal language models in chemistry, and reasoning capabilities in LLMs, which are foundational to our proposed framework for training reasoning LLMs tailored to molecular property prediction.

\paragraph{Machine Learning for Molecular Representation.}
GNNs have evolved as a dominant paradigm for molecular graph representation, progressing from early convolutional \citep{moleculargraph,schnet} and message-passing \citep{mpnn} frameworks to sophisticated 3D-aware models \citep{mpnn, rgcl, simsgt,unimol}, enabling robust applications in property prediction, virtual screening, and drug discovery \citep{gnn-chemistry-applications}.
In parallel, specialized molecular language models have reframed molecular structures as textual sequences such as SMILES strings\citep{smiles}, with models like MolecularGPT \citep{moleculargpt} and BioT5-Plus \citep{biot5-plus} supporting few-shot adaptation and multi-task learning for diverse chemical and biological tasks \citep{scilitllm, reactxt, molca}. 

\paragraph{Multimodal Language Models for Chemistry.}
The emerging trend of multimodal LLMs in chemistry integrates diverse data types—such as SMILES strings and molecular graphs to address unimodal limitations, as seen in foundational molecular-text models \citep{mol-llm, chemvlm, molca}, instruction-tuned assistants \citep{instructmol}, and tool-augmented systems \citep{chemcrow}, enhancing robustness in property prediction \citep{molt5}, molecular design \citep{mol-llm}, and synthesis planning \citep{relm, reactxt}. However, these models still lack the capability to provide chemical reasoning for their predictions.

\paragraph{Reasoning in Large Language Models.}
Reasoning capabilities have demonstrated remarkable efficacy in commercial LLMs, particularly through chain-of-thought processes as exemplified in OpenAI's o1 series and other advanced models \citep{cot,openai-o1,gemini,claude}.
Training these abilities leverages RL techniques, from Proximal Policy Optimization \citep{ppo} in RL from Human Feedback (RLHF) \citep{rlhf} for preference alignment, to efficient extensions like Group Relative Policy Optimization (GRPO) \citep{grpo} with outcome-based rewards and Reinforcement Learning with Verifiable Rewards (RLVR) for one-shot verifiable steps, improving generalization on complex tasks \citep{rlvr}.
These RL advancements motivate our adaptation for chemical-specific reasoning in the field of molecular property prediction.",2543
http://arxiv.org/abs/2510.04706v1,2510.04706v1,2025-10-06T11:20:56+00:00,"ID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusion","Human-centric generative models designed for AI-driven storytelling must
bring together two core capabilities: identity consistency and precise control
over human performance. While recent diffusion-based approaches have made
significant progress in maintaining facial identity, achieving fine-grained
expression control without compromising identity remains challenging. In this
work, we present a diffusion-based framework that faithfully reimagines any
subject under any particular facial expression. Building on an ID-consistent
face foundation model, we adopt a compositional design featuring an expression
cross-attention module guided by FLAME blendshape parameters for explicit
control. Trained on a diverse mixture of image and video data rich in
expressive variation, our adapter generalizes beyond basic emotions to subtle
micro-expressions and expressive transitions, overlooked by prior works. In
addition, a pluggable Reference Adapter enables expression editing in real
images by transferring the appearance from a reference frame during synthesis.
Extensive quantitative and qualitative evaluations show that our model
outperforms existing methods in tailored and identity-consistent expression
generation. Code and models can be found at
https://github.com/foivospar/Arc2Face.","\subsection{Facial Expression Generation and Editing}

Early efforts to edit facial expressions in ``in-the-wild'' images primarily employed GAN-based architectures. These approaches typically used conditional GANs for image-to-image translation. For instance, StarGAN \cite{StarGAN} enabled multi-domain facial attribute transfer using categorical emotion labels, while ExprGAN \cite{ding2017exprgan} and Lindt \etal \cite{Lindt2019FacialEE} introduced expression editing along emotion intensity levels. Deviating from hand-crafted emotion representations, GANmut \cite{sdapolito2021GANmut} proposed a framework that implicitly learns a continuous and interpretable emotion space from categorical labels, by associating classification confidence with expression intensity. Despite promising results, these methods are limited by the quality and diversity of their training data, as well as the inherent instability of GAN training. 

At the same time, various works focus on geometry-driven expression editing, addressing the task of face reenactment. Models like ICface \cite{Tripathy_ICface} and GANimation \cite{GANimation_ijcv2019} leverage Action Units (AUs) as an interpretable representation for expression control, whereas Neural Emotion Director \cite{paraperas2022ned} introduces a 3D-based Emotion Manipulator that translates expressions into basic emotions or reference-driven styles using a 3D Morphable Model (3DMM)-conditioned GAN. Another direction explores the latent space of the pre-trained StyleGAN \cite{stylegan} network, for facial editing. InterFaceGAN \cite{shen2020interpreting, shen2020interfacegan} disentangles facial attributes using pre-trained classifiers, enabling semantic control over identity, age, or expression. StyleCLIP \cite{patashnik2021styleclip} builds on this by incorporating CLIP \cite{radford2021learning}, allowing text-driven facial edits within StyleGAN’s latent space, while EmoStyle \cite{EmoStyle_2024_WACV} takes this further by explicitly separating expression from other facial attributes, using Valence-Arousal values to modify expressions while preserving identity and appearance.

\subsection{ID and Attribute Control in Diffusion Models}

Following the advent of diffusion models, research has shifted towards adding controllability to frontier text-to-image models like Stable Diffusion \cite{rombach2022high}. One major area of focus has been identity preservation. A popular approach involves using the CLIP image encoder \cite{radford2021learning} to extract features from reference subjects, which are then injected into pre-trained models via cross-attention layers. Notable examples include FastComposer \cite{xiao2023fastcomposer}, PhotoVerse \cite{chen2023photoverse}, MoA \cite{wang2024moa}, and PhotoMaker \cite{li2023photomaker}. A more robust alternative to CLIP embeddings is the use of face recognition networks, which extract stronger ID-specific features. This approach has been leveraged in works such as Face0 \cite{valevski2023face0}, DreamIdentity \cite{chen2023dreamidentity}, IP-Adapter-FaceID \cite{ye2023ip}, PortraitBooth \cite{peng2023portraitbooth}, and InstantID \cite{wang2024instantid} for both 2D and 3D face generation \cite{gerogiannis2025arc2avatar}. Among these, the pioneering Arc2Face work~\cite{paraperas2024arc2face}, showed how the powerful Stable Diffusion model can be transformed into an ID-consistent face foundation model capable of generating highly realistic and diverse images with compelling identity similarity, by leveraging WebFace \cite{zhu2021webface260m} - the largest public dataset for face recognition. 

In parallel, numerous efforts explore the integration of expression features. These methods vary in the type of representation used, including 2D landmarks \cite{han2024faceadapter, mishima2025facecrafter}, 3DMMs \cite{liang2024caphuman}, Action Units \cite{wei2025magicface, varanka2024fineface}, continuous emotion spaces \cite{emotiondiffusion}, or natural language instructions \cite{wang2024instructavatar, liu2024towards}. Yet, they often fall short in expression fidelity - particularly when handling asymmetric or extreme expressions and typically introduce identity distortion.",4212
http://arxiv.org/abs/2510.10060v1,2510.10060v1,2025-10-11T06:54:10+00:00,Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling,"When modeling a given type of data, we consider it to involve two key
aspects: 1) identifying relevant elements (e.g., image pixels or textual words)
to a central element, as in a convolutional receptive field, or to a query
element, as in self-attention, and 2) encoding these tokens effectively.
Self-attention can adaptively identify these elements but relies on absolute
positional embedding for structural representation learning. In contrast,
convolution encodes elements in a relative manner, yet their fixed kernel size
limits their ability to adaptively select the relevant elements. In this paper,
we introduce Translution, an operation that unifies the adaptive identification
capability of self-attention and the relative encoding advantage of
convolution. However, this integration leads to a substantial increase in the
number of parameters, exceeding most currently available computational
resources. Therefore, we propose a lightweight variant of Translution, named
{\alpha}-Translution. Experiments on computer vision and natural language
processing tasks show that Translution (including {\alpha}-Translution)
achieves superior accuracy compared to self-attention. The code is available at
https://github.com/hehefan/Translution.","Transformer~\citep{DBLP:conf/nips/VaswaniSPUJGKP17,radford2018improving,DBLP:conf/naacl/DevlinCLT19,DBLP:conf/iclr/DosovitskiyB0WZ21,DBLP:conf/iccv/LiuL00W0LG21,DBLP:conf/icml/TouvronCDMSJ21} eschews recurrence (as used in recurrent neural networks) and kernel size (as used in convolutional neural networks), instead employing self‐attention for relevant region identification.  
Because it has no built‐in notion of order, Transformer incorporates explicit absolute positional embeddings into token embeddings, enabling the model to utilize sequence order. 
Subsequent work has explored ``relative attention''~\citep{DBLP:conf/naacl/ShawUV18,DBLP:conf/iclr/HuangVUSHSDHDE19,DBLP:conf/nips/ParmarRVBLS19,DBLP:conf/acl/DaiYYCLS19,DBLP:conf/emnlp/TsaiBYMS19,DBLP:journals/jmlr/RaffelSRLNMZLL20,DBLP:conf/nips/DaiLLT21}, which integrates relative position information into self‐attention. 
They can be categorized into three families: 
\textit{1) Relative positional vector.} Shaw~\etal enhanced Transformer for language modeling by adding learnable relative positional vectors into the key and value computations, respectively~\citep{DBLP:conf/naacl/ShawUV18}. 
BoTNet~\citep{DBLP:conf/cvpr/SrinivasLPSAV21} and HaloNet~\citep{DBLP:conf/cvpr/VaswaniRSPHS21} extended this approach to two dimensions for image processing by adding learnable relative positional vectors into key. 
\textit{2) Relative positional scalar.} Swin Transformer~\citep{DBLP:conf/iccv/LiuL00W0LG21},  CoAtNet~\citep{DBLP:conf/nips/DaiLLT21}, and ConViT~\cite{DBLP:conf/icml/dAscoliTLMBS21} incorporate a learnable relative positional bias (a scalar) into the attention score. 
In these methods, the original self-attention can be regarded as content attention, which measures relationships from the token-feature perspective, while the additional relative positional bias can be regarded as position attention, which measures relationships from the token-position perspective. 
\textit{3) Rotary position embedding.}  RoFormer~\citep{DBLP:journals/ijon/SuALPBL24} introduces a rotary position embedding  mechanism, which encodes relative positional information by applying a rotation operation in the Query and Key representation space. 
Unlike these existing methods, Translution employs a convolution-style approach that uses relative positional matrices for query, key and value computation. 
Section~\ref{sec:position} provides a formal comparison of these methods. 

Convolutional neural networks ~\citep{DBLP:journals/pieee/LeCunBBH98,DBLP:conf/nips/KrizhevskySH12,DBLP:journals/corr/SimonyanZ14a,DBLP:conf/cvpr/SzegedyLJSRAEVR15,DBLP:conf/cvpr/HeZRS16} have been the backbone of deep learning for years. By using small, shared kernels and pooling, convolutional neural networks efficiently capture local patterns.  
Recent architectural developments integrate self-attention with convolution. 
For instance, Conformer~\citep{DBLP:conf/interspeech/GulatiQCPZYHWZW20} combines convolution layers and self-attention layers to capture both local and global dependencies in audio sequences.
Similarly, CeiT~\citep{DBLP:conf/iccv/YuanG0ZYW21} uses convolutions to extract low-level features and self-attention to model long-range dependencies.
Unlike these architectural methods, Translution operates at the basic module or layer level, blending the advantages of self-attention and convolution into a unified fundamental operation.",3405
http://arxiv.org/abs/2509.19669v1,2509.19669v1,2025-09-24T01:02:24+00:00,Games Are Not Equal: Classifying Cloud Gaming Contexts for Effective User Experience Measurement,"To tap into the growing market of cloud gaming, whereby game graphics is
rendered in the cloud and streamed back to the user as a video feed, network
operators are creating monetizable assurance services that dynamically
provision network resources. However, without accurately measuring cloud gaming
user experience, they cannot assess the effectiveness of their provisioning
methods. Basic measures such as bandwidth and frame rate by themselves do not
suffice, and can only be interpreted in the context of the game played and the
player activity within the game. This paper equips the network operator with a
method to obtain a real-time measure of cloud gaming experience by analyzing
network traffic, including contextual factors such as the game title and player
activity stage. Our method is able to classify the game title within the first
five seconds of game launch, and continuously assess the player activity stage
as being active, passive, or idle. We deploy it in an ISP hosting NVIDIA cloud
gaming servers for the region. We provide insights from hundreds of thousands
of cloud game streaming sessions over a three-month period into the dependence
of bandwidth consumption and experience level on the gameplay contexts.","Network traffic analysis of cloud gaming services:As an emerg-
ing type of multimedia streaming service that exhibits high de-
mands on network conditions, cloud gaming services have been an-
alyzed in prior works for their network traffic characteristics, includ-
ing flow anatomy and volumetric profiles. For example, Lyuet al.
[32] discussed the use of network flows during entire user sessions,
from platform administration to server selection and gameplay,
which serve as indicative signatures for network operators to de-
tect cloud gaming sessions played on different types of user setups
by their broadband/mobile subscribers. The works in [ 13,18,34]
analyzed the bandwidth demands of cloud gaming sessions with
different levels of streaming quality settings. Furthermore, objective
quality-of-experience metrics (e.g.,user input lag, graphic resolu-
tion, and streaming frame rates) have also been derived from net-
work quality-of-service (QoS) attributes of game streaming flows
[6,32,44]. For the network observability industry, it has been high-
lighted that purely using objective QoE metrics such as frame rate
cannot effectively measure the quality of game streaming perceived
by players, as gameplay contexts, including game titles and player
activities, can significantly vary the expected/requested QoE and
network QoS levels for good user experience [ 3,14,22,25,27,49,67].
To bridge this gap, we develop a traffic analysis method to classify
the contexts of cloud gameplay, enabling network operators to effec-
tively measure user-perceived streaming quality when correlating
with the objective QoE metrics.
Measuring multimedia services with contexts:Gaining visi-
bility into the coarse categories of multimedia streaming contexts
has become important for the network observability industry to
effectively measure user-perceived streaming quality as delivered
by networks [ 7,12,21,42,54,60,66]. Prior works have developed
methods to classify contexts in popular types of multimedia ser-
vices such as video streaming [ 1,11,64,69], live streaming [ 30,37],
video conferencing [ 35,39,51], voice calling [ 19,26], online gaming
[36,50], and virtual reality applications [ 17,33]. For example, the
works in [ 15,16] focus on the contexts in online gaming that vary
the expected network conditions for a good gameplay experienceincluding game titles, genres, the number of players, peripheral
status and team communication model. In [ 17,33], the authors mea-
sured the network demands for a good VR experience that depend
on the user activity status and the number of surrounding users.
This work is specialized in classifying the contexts (i.e.,game titles,
gameplay activity patterns, and player activity stages) of cloud
game streaming sessions. As demonstrated in a large-scale deploy-
ment at an ISP hosting NVIDIA’s cloud gaming servers, our method,
combined with objective QoE measurement, holds unique value for
network operators to better support this highly demanding service
by effectively measuring the user-perceived streaming quality.",3071
http://arxiv.org/abs/2510.05586v1,2510.05586v1,2025-10-07T05:16:29+00:00,CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval,"Existing Visual Language Models (VLMs) suffer structural limitations where a
few low contribution tokens may excessively capture global semantics,
dominating the information aggregation process and suppressing the
discriminative features in text-driven image retrieval tasks. To address this,
we introduce \textbf{CalibCLIP}, a training-free method designed to calibrate
the suppressive effect of dominant tokens. Specifically, in the visual space,
we propose the Contrastive Visual Enhancer (CVE), which decouples visual
features into target and low information regions. Subsequently, it identifies
dominant tokens and dynamically suppresses their representations.In the textual
space, we introduce the Discriminative Concept Calibrator (DCC), which aims to
differentiate between general and discriminative concepts within the text
query. By mitigating the challenges posed by generic concepts and improving the
representations of discriminative concepts, DCC strengthens the differentiation
among similar samples. Finally, extensive experiments demonstrate consistent
improvements across seven benchmarks spanning three image retrieval tasks,
underscoring the effectiveness of CalibCLIP. Code is available at:
https://github.com/kangbin98/CalibCLIP","Visual Language Models.VLMs[ 10,19,28,62] such as CLIP
excel in achieving cross-modal alignment through contrastive pre-
training but encounter a structural limitation where scattered atten-
tion hinders nuanced feature discrimination. Recent studies [ 29,68]
have highlighted disruptive tokens and proposed various solutions
for fine-grained tasks like open-vocabulary segmentation [ 47] and
object detection[ 20]. However, these methods often require addi-
tional training and currently lack a unified and comprehensive
analysis and solution for image retrieval. In response, we introduce
a straightforward, training-free, dual-space calibration framework
that suppresses dominant token representations without the need
for extra training, thereby enhancing fine-grained perception in
retrieval tasks.
Text-driven image retrieval.Building on the achievements of
large language models (LLMs) [ 2,8], the field of text-driven image
retrieval [ 3,16,34,42,55,59]has made significant strides. However,
existing methods [ 17,49] mainly focus on global alignment between
images and text, often neglecting fine-grained details and strug-
gling with intricate queries. This issue is particularly pronounced intext-based person retrieval tasks [ 40,53,71], which require precise
modeling of subtle attributes and spatial relationships. Yet, current
models primarily concentrate on associations at the object level.
While recent compositional retrieval methods [ 25,44] extend se-
mantic alignment to multi-concept queries, they are hindered by
high computational demands and limitations in data scalability.
This hampers both fine-grained perception and the performance
of compositional queries. To address these challenges, we propose
a training-free approach that tackles the semantic dominance of
abnormal tokens in the shared embedding space, thereby enhancing
fine-grained perception and cross-modal alignment.",1908
http://arxiv.org/abs/2509.11868v1,2509.11868v1,2025-09-15T12:39:55+00:00,Growing Perspectives: Modelling Embodied Perspective Taking and Inner Narrative Development Using Large Language Models,"Language and embodied perspective taking are essential for human
collaboration, yet few computational models address both simultaneously. This
work investigates the PerspAct system [1], which integrates the ReAct (Reason
and Act) paradigm with Large Language Models (LLMs) to simulate developmental
stages of perspective taking, grounded in Selman's theory [2]. Using an
extended director task, we evaluate GPT's ability to generate internal
narratives aligned with specified developmental stages, and assess how these
influence collaborative performance both qualitatively (action selection) and
quantitatively (task efficiency). Results show that GPT reliably produces
developmentally-consistent narratives before task execution but often shifts
towards more advanced stages during interaction, suggesting that language
exchanges help refine internal representations. Higher developmental stages
generally enhance collaborative effectiveness, while earlier stages yield more
variable outcomes in complex contexts. These findings highlight the potential
of integrating embodied perspective taking and language in LLMs to better model
developmental dynamics and stress the importance of evaluating internal speech
during combined linguistic and embodied tasks.","Recent advancements in collaborative AI emphasize the importance of effective perspective taking to improve human-AI interactions. The foundational ReAct framework introduced by Yao et al. \cite{yao2023react} has enabled agents to reason and act efficiently in dynamic collaborative environments. Extending this framework, PerspAct \cite{patania2025perspact} integrates active vision and explicit perspective taking, demonstrating enhanced situated collaboration skills.

Developmental psychology literature addresses the gradual evolution of perspective taking abilities through various models. We choose to refer to Selman's model, where he establishes five stages of perspective taking \cite{selman1980growth, selman1971taking}:

\begin{itemize}
    \item \textbf{Egocentric (Stage 0, undifferentiated, before age 6)}: 
    The child physically distinguishes themselves from others but remains socially egocentric, assuming that other think as they do. 

    \item \textbf{Differentiated, subjective (Stage 1, ages 6–8)}: 
    Children acknowledge that others may hold perspectives different from their own, typically due to having access to different information, but still struggle to accurately judge others' viewpoints.

    \item \textbf{Self-reflective, reciprocal (Stage 2, ages 8–10)}: 
    Children can metaphorically ""step into another person's shoes"", recognizing and validating differing perspectives as appropriate given distinct situational contexts.

    \item \textbf{Mutual, third-person (Stage 3, ages 10–12)}: 
    Children begin viewing interactions from a more neutral, third-person perspective, simultaneously managing multiple viewpoints and integrating them into a shared understanding.

    \item \textbf{Societal (Stage 4, ages 12 and beyond)}: 
    Adolescents develop an understanding of how perspectives are embedded in broader societal frameworks, influenced by institutional roles, norms, and values. They recognize that an individual's perspective is shaped by their social context and position.
\end{itemize}



Recent cognitive science studies suggest perspective taking can be linked to linguistic competencies such as personal pronoun use, highlighting the importance of developmental linguistics in perspective taking \cite{ricard1999personal}. Nematzadeh et al. \cite{nematzadeh2018evaluating} evaluate computational models' capacity for inferring mental states within narrative contexts, underscoring the critical role of linguistic representation in perspective taking tasks. Moreover, Rashkin et al. \cite{rashkin2018modeling} emphasize the importance of empathetic and perspective-aware dialogue generation, highlighting how language models can be tailored to address complex social cognition scenarios.

A growing number of recent studies have also evaluated LLMs' implicit theory of mind capabilities across diverse tasks. For example, Chen et al. \cite{chen2024tombench} and Wang et al. \cite{wang2025rethinking} propose novel benchmarks for probing ToM reasoning in GPT-based models, while Zhou et al. \cite{zhou2023sotopia} investigate LLMs’ ability to manage social beliefs and predict others’ behaviour in interactive settings. These approaches, however, tend to focus on behavioural outcomes rather than grounding LLM reasoning in structured developmental frameworks.

Although wide research has been conducted on perspective taking, not many authors have adopted a developmental framework. In this work, we aim to bridge these developmental insights with contemporary AI by exploring how developmental theories can enhance collaborative capabilities of modern LLM frameworks.",3629
http://arxiv.org/abs/2510.08003v1,2510.08003v1,2025-10-09T09:41:45+00:00,CIR-CoT: Towards Interpretable Composed Image Retrieval via End-to-End Chain-of-Thought Reasoning,"Composed Image Retrieval (CIR), which aims to find a target image from a
reference image and a modification text, presents the core challenge of
performing unified reasoning across visual and semantic modalities. While
current approaches based on Vision-Language Models (VLMs, e.g., CLIP) and more
recent Multimodal Large Language Models (MLLMs, e.g., Qwen-VL) have shown
progress, they predominantly function as ``black boxes."" This inherent opacity
not only prevents users from understanding the retrieval rationale but also
restricts the models' ability to follow complex, fine-grained instructions. To
overcome these limitations, we introduce CIR-CoT, the first end-to-end
retrieval-oriented MLLM designed to integrate explicit Chain-of-Thought (CoT)
reasoning. By compelling the model to first generate an interpretable reasoning
chain, CIR-CoT enhances its ability to capture crucial cross-modal
interactions, leading to more accurate retrieval while making its decision
process transparent. Since existing datasets like FashionIQ and CIRR lack the
necessary reasoning data, a key contribution of our work is the creation of
structured CoT annotations using a three-stage process involving a caption,
reasoning, and conclusion. Our model is then fine-tuned to produce this
structured output before encoding its final retrieval intent into a dedicated
embedding. Comprehensive experiments show that CIR-CoT achieves highly
competitive performance on in-domain datasets (FashionIQ, CIRR) and
demonstrates remarkable generalization on the out-of-domain CIRCO dataset,
establishing a new path toward more effective and trustworthy retrieval
systems.","\subsection{Composed Image Retrieval}

Recent advances in Vision–Language Models (VLMs)~\cite{jia2021scaling,Li2023BLIP2BL} have laid a strong foundation for compositional image retrieval. Building on these models, most contemporary CIR approaches develop various adaptation strategies to tailor them to the retrieval task. Specifically, some methods~\cite{Levy2023DataRA,Liu2023CandidateSR,Anwaar2020CompositionalLO,Chen_2020_CVPR} adopt an early-fusion strategy, where the text and image features are first extracted separately using unimodal encoders and then fused to form a joint query representation, which is subsequently matched against candidate features. The main limitation of such early-fusion approaches lies in their inability to accurately align fine-grained visual details with user intent during feature fusion. To address this issue, another line of work~\cite{bai2023sentence,gal2022image,saito2023pic2word,Tang2023ContextI2WMI} transforms the reference image into a word embedding via textual inversion, concatenates it with the query text to form an enhanced textual feature, and then performs text-to-image retrieval. Despite their effectiveness, the reliance on text encoders limits these methods’ ability to faithfully interpret and retrieve images according to complex user intent. Consequently, a recent work, CIR-LVLM~\cite{sun2025leveraging}, attempts to finetune MLLMs to better capture user intent by directly encoding multimodal inputs and retrieving the target image accordingly. Leveraging the strong comprehension ability of MLLMs, this approach achieves promising results. Unlike prior work, CIR-CoT fully exploits MLLMs by (i) generating explicit, human-readable reasoning that makes retrieval transparent rather than black-box, and (ii) encoding the reasoned user intent as a retrieval representation, yielding stronger performance.




\subsection{Multimodal Large Language Models}
Large Language Models (LLMs)~\cite{chiang2023vicuna,touvron2023llama,zheng2023judging,team2023internlm,openai2023gpt,meta2024introducing,bi2024deepseek,yang2024qwen2} have recently achieved remarkable progress, attracting broad research interest due to their strong reasoning and generation abilities. Building on this success, researchers have extended LLMs to handle visual inputs, which has driven rapid advances in Multimodal Large Language Models (MLLMs)~\cite{liu2023visual,bai2023qwen,zhu2023minigpt,lu2024deepseek,liu2024oryx,li2024mini}. Recent studies have shown that MLLMs excel in diverse vision tasks. Notably, some approaches~\cite{Lai2023LISARS, Lin2025HRSegHV} employ MLLMs for segmentation, marking a departure from the conventional VQA paradigm. However, MLLMs tend to exhibit hallucinations when performing complex tasks and often underutilize visual information.

To address these challenges, some approaches~\cite{Qiao2024PrismAF,Cesista2024MultimodalSG,Chu2023NavigateTE} leverage Chain-of-Thought (CoT) prompting, which decomposes a question into a series of reasoning steps and constructs a chain to guide the model in generating solutions to complex problems. This process significantly enhances the reasoning capabilities of MLLMs. Although direct CoT approaches are effective, later methods~\cite{Xu2024LLaVACoTLV} demonstrated that the proposed structured CoT significantly outperforms direct CoT, further enhancing the reasoning capabilities of MLLMs. 

Building on the developments mentioned above, CIR-CoT is the first approach to apply the structured CoT reasoning capabilities of MLLMs to the CIR task. Its goal is to stimulate fine-grained reasoning in MLLMs over different user inputs and to infer user intent, thereby improving retrieval performance.",3706
http://arxiv.org/abs/2509.20837v1,2509.20837v1,2025-09-25T07:23:30+00:00,Verification Limits Code LLM Training,"Large language models for code generation increasingly rely on synthetic
data, where both problem solutions and verification tests are generated by
models. While this enables scalable data creation, it introduces a previously
unexplored bottleneck: the verification ceiling, in which the quality and
diversity of training data are fundamentally constrained by the capabilities of
synthetic verifiers. In this work, we systematically study how verification
design and strategies influence model performance. We investigate (i) what we
verify by analyzing the impact of test complexity and quantity: richer test
suites improve code generation capabilities (on average +3 pass@1), while
quantity alone yields diminishing returns, (ii) how we verify by exploring
relaxed pass thresholds: rigid 100% pass criteria can be overly restrictive. By
allowing for relaxed thresholds or incorporating LLM-based soft verification,
we can recover valuable training data, leading to a 2-4 point improvement in
pass@1 performance. However, this benefit is contingent upon the strength and
diversity of the test cases used, and (iii) why verification remains necessary
through controlled comparisons of formally correct versus incorrect solutions
and human evaluation: retaining diverse correct solutions per problem yields
consistent generalization gains. Our results show that Verification as
currently practiced is too rigid, filtering out valuable diversity. But it
cannot be discarded, only recalibrated. By combining calibrated verification
with diverse, challenging problem-solution pairs, we outline a path to break
the verification ceiling and unlock stronger code generation models.","The use of synthetic data to train and evaluate code generation models has gained traction as a
scalable alternative to curated datasets. Frameworks like Case2Code [Shao et al., 2025] demonstrate
that synthesizing diverse code samples via LLMs can improve inductive reasoning and code under-
standing. Similarly, Nadˇ aş et al. [2025] surveys LLM-driven synthetic data pipelines, highlighting
techniques such as prompt-based generation and reinforcement learning with execution feedback.
Pipelines for synthetic code generation rely on some form of verification of the generated code as a
proxy for its quality [Luo et al., 2025; Chen et al., 2021]. The most common method by far relies
on unit tests to validate functional correctness, as this also mirrors what is done in popular coding
benchmarks like HumanEval [Balloccu et al., 2024], MBPP [Lyu et al., 2024] or LBPP [Matton
et al., 2024].
However, this approach has potentially several flaws: Unit tests may fail to cover edge cases, overfit
to specific implementations, or discard valid solutions that fail partial tests. Moreover, generating
comprehensive test suites is labor-intensive and brittle for complex problems [Lahiri et al., 2023],
which is only partially alleviated by synthetic methods for unit-test generation [Wang et al., 2025].
Moreover, problem difficulty and diversity have a huge impact on the final model performance.
Tambonetal.[2025]highlightsthatmodelperformancedegradessignificantlyasproblemcomplexity
increases, suggesting that training data must better reflect this diversity. Chen et al. [2024] also
argue that diverse synthetic samples improve generalization, even for smaller models.
14",1674
http://arxiv.org/abs/2510.07459v1,2510.07459v1,2025-10-08T19:04:25+00:00,MoGU: Mixture-of-Gaussians with Uncertainty-based Gating for Time Series Forecasting,"We introduce Mixture-of-Gaussians with Uncertainty-based Gating (MoGU), a
novel Mixture-of-Experts (MoE) framework designed for regression tasks and
applied to time series forecasting. Unlike conventional MoEs that provide only
point estimates, MoGU models each expert's output as a Gaussian distribution.
This allows it to directly quantify both the forecast (the mean) and its
inherent uncertainty (variance). MoGU's core innovation is its
uncertainty-based gating mechanism, which replaces the traditional input-based
gating network by using each expert's estimated variance to determine its
contribution to the final prediction. Evaluated across diverse time series
forecasting benchmarks, MoGU consistently outperforms single-expert models and
traditional MoE setups. It also provides well-quantified, informative
uncertainties that directly correlate with prediction errors, enhancing
forecast reliability. Our code is available from:
https://github.com/yolish/moe_unc_tsf","MoE ModelsThe pursuit of increasingly capable and adaptable artificial intelligence systems has led
to the development of sophisticated architectural paradigms, among which the Mixture-of-Experts
(MoE) stands out. MoE is an architectural concept that adaptively combines predictions from
multiple specialized neural modules, often sharing a common architecture, through a learned gating
mechanism. This paradigm allows for a dynamic allocation of computational resources, enabling
models to specialize on different sub-problems or data modalities. Early implementations of MoE
(Jacobs et al., 1991) focused on ensemble learning (ensemble MoE), where multiple models (experts)
contributed to a final prediction. More recently, MoE layers have been seamlessly integrated within
larger neural architectures, with experts operating in latent domains (latent MoE) (Shazeer et al., 2017;
Fedus et al., 2022). This integration has proven particularly impactful in the realm of large language
models (LLMs), where MoE layers have been instrumental in scaling models to unprecedented sizes
while managing computational costs (Lepikhin et al., 2020; Jiang et al., 2024; Dai et al., 2024). By
selectively activating only a subset of experts for each input token, MoEs enable models with vast
numbers of parameters to achieve high performance without incurring the prohibitive inference costs
of densely activated large models. Despite their contribution and adoption, both ensemble and latent
MoE architectures typically output point estimates, both at the level of the individual expert and at
the level of the overall model. This limits the ability to quantify uncertainty which is important for
decision-making. Few works have explored uncertainty estimation for MoE architectures (see e.g.
Pavlitska et al. (2025); Zhang et al. (2023)). In this work, we focus on ensemble MoE architectures,
as uncertainty quantification is more directly applicable for decision making and interpretability. In
2
our method, we view the experts of the MoE model as an ensemble of models that can be used to
extract both aleatoric and epistemic uncertainties.
Uncertainty Estimation for Regression Tasks.Deep learning regression models are increasingly re-
quired not only to provide accurate point estimates but also to quantify predictive uncertainty. A large
body of research has focused on Bayesian neural networks, which place distributions over weights and
approximate posterior inference using variational methods or Monte Carlo dropout, thereby producing
predictive intervals (Gal & Ghahramani, 2016). Another line of work employs ensembles of neural
networks to capture both aleatoric and epistemic uncertainties, with randomized initialization or
bootstrapped training providing diverse predictions (Lakshminarayanan et al., 2017). More recently,
post-hoc calibration techniques have been proposed, adapting classification-oriented approaches
such as temperature scaling to regression settings, for instance by optimizing proper scoring rules
or variance scaling factors (Kuleshov et al., 2018). Beyond probabilistic calibration, conformal
prediction (CP) methods have gained attention due to their finite-sample coverage guarantees under
minimal distributional assumptions. CP can be applied to regression to produce instance-dependent
prediction intervals with guaranteed coverage, and has been extended to handle asymmetric intervals,
distribution shift, and multi-target regression (V ovk et al., 2005; Romano et al., 2019).
Time Series Forecasting and Uncertainty Estimation.Time series forecasting is a critical discipline
in machine learning and statistics, focusing on predicting future values from a sequence of historical
data points ordered by time. This field has wide-ranging applications, including financial market
analysis, energy consumption forecasting, weather prediction, and medical prognosis. Traditional
statistical methods, such as Autoregressive Integrated Moving Average (ARIMA) and Exponential
Smoothing, have been foundational. However, their effectiveness is often limited by their assumption
of linearity and their inability to capture complex, non-linear dependencies. More recently, deep
learning models, employing Transformers (Nie et al., 2023; Wu et al., 2021; Kitaev et al., 2020),
Multi-Layer Perceptrons (MLPs) (Wang et al., 2024b; Zeng et al., 2023), and Convolutional Neural
Networks (CNNs) (Wu et al., 2023), were shown to be effective in modeling temporal dynamics and
long-range dependencies (Wang et al., 2024a; Lim & Zohren, 2021; Wang et al., 2024c). The ability to
quantify the uncertainty of a forecast, rather than providing just a single point estimate, is of paramount
importance. Uncertainty quantification provides a confidence interval for the prediction, which is
crucial for risk management and informed decision-making. Some recent works have introduced
uncertainty estimation to time series forecasting (see e.g. Cini et al. (2025); Wu et al. (2025)). Given
its wide-ranging applications, the importance of reporting uncertainty, and its challenging nature,
time series forecasting serves as a highly suitable domain to evaluate the performance of MoGU.",5196
