arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2509.19096v2,http://arxiv.org/abs/2509.19096v2,2025-09-23 14:47:33+00:00,Investigating Traffic Accident Detection Using Multimodal Large Language Models,"Traffic safety remains a critical global concern, with timely and accurate
accident detection essential for hazard reduction and rapid emergency response.
Infrastructure-based vision sensors offer scalable and efficient solutions for
continuous real-time monitoring, facilitating automated detection of accidents
directly from captured images. This research investigates the zero-shot
capabilities of multimodal large language models (MLLMs) for detecting and
describing traffic accidents using images from infrastructure cameras, thus
minimizing reliance on extensive labeled datasets. Main contributions include:
(1) Evaluation of MLLMs using the simulated DeepAccident dataset from CARLA,
explicitly addressing the scarcity of diverse, realistic, infrastructure-based
accident data through controlled simulations; (2) Comparative performance
analysis between Gemini 1.5 and 2.0, Gemma 3 and Pixtral models in accident
identification and descriptive capabilities without prior fine-tuning; and (3)
Integration of advanced visual analytics, specifically YOLO for object
detection, Deep SORT for multi-object tracking, and Segment Anything (SAM) for
instance segmentation, into enhanced prompts to improve model accuracy and
explainability. Key numerical results show Pixtral as the top performer with an
F1-score of 0.71 and 83% recall, while Gemini models gained precision with
enhanced prompts (e.g., Gemini 1.5 rose to 90%) but suffered notable F1 and
recall losses. Gemma 3 offered the most balanced performance with minimal
metric fluctuation. These findings demonstrate the substantial potential of
integrating MLLMs with advanced visual analytics techniques, enhancing their
applicability in real-world automated traffic monitoring systems.","\label{sec:rw}

% \todo[color=yellow!40, inline]{Ilhan, Kailin write the literature review.}

Vision-based accident detection methods can be categorized by their model architectures into several categories, including both frame-level and object-centric approaches \cite{fang2023vision}. Frame-level accident detection relies on extracting features from individual frames and requires annotating accident windows in traffic recordings. You and Han \cite{you2020traffic} propose a dataset for traffic understanding, where accident windows are detected by classifying feature embeddings derived from video frames using a 3D-CNN model.
In contrast, methods that emphasize object detection and tracking aim to identify accident participants by detecting and localizing objects in images and monitoring their movement over time. Various approaches have used the YOLO detector \cite{7780460} previously, among which are YOLOv4 \cite{ghahremannezhad2022real} and YOLOv5 \cite{xia2022research}. Karim et al. \cite{karim2024visual} employed YOLOv8 to  detect vehicles and accidents in real time from surveillance video. They integrated Deep-SORT \cite{wojke2017simple} to track vehicle movement across frames, supporting temporal analysis for accident detection. While 3D-CNN based classification and YOLO with Deep-SORT tracking effectively pinpoint accident participants, they sometimes fall short in capturing the broader context needed for a more thorough analysis of accident scenarios.


The growth of artificial intelligence has increased the demand for extensive datasets to train models for specific applications, including accident detection. Consequently, numerous datasets have been developed to support research in this area. Yao et al. \cite{yao2022dota} propose an unsupervised traffic anomaly detection framework that employs future object localization and a novel  spatial-temporal area under curve (STAUC) on the richly annotated DoTA dataset to effectively identify abnormal events in dynamic, egocentric driving videos.
However, since the DoTA dataset primarily comprises dashcam video recordings with highly dynamic perspectives, it may not be ideally suited for applications requiring fixed, infrastructure-based scenes for accident detection.


Lee et al. \cite{lee2017crash} introduced GTACrash, a synthetic dataset derived from GTA V that enables training CNN-based collision prediction algorithms, and demonstrated that synthetic data is a valuable alternative to scarce real-world accident data, although the approach suffered from limited accident diversity and realism.
DeepAccident \cite{wang2024deepaccident} is a large-scale, simulator-generated V2X dataset for autonomous driving, developed using the CARLA simulator \cite{dosovitskiy2017carla}. It offers a wide variety of collision scenarios, accompanied by comprehensive sensor and annotation data from 6 infrastructure cameras, making it a robust resource for accident prediction and accident detection research.


ConnectGPT \cite{tong2024connectgpt} introduces a pipeline that integrates GPT-4 with infrastructure cameras and V2X communication to automatically analyze traffic conditions and generate standardized C-ITS messages, thereby enhancing real-time incident detection and road safety. Authors demonstrated its potential through practical experiments on a small dataset, highlighting improved responsiveness and reduced manual intervention in managing traffic hazards. However, its reliance solely on GPT-4 without evaluating alternative multimodal LLMs may limit its broader applicability.

AccidentGPT \cite{wu2024accidentgpt} presented a framework that combines V2X environmental perception with GPT-based reasoning to enable real-time accident prediction, prevention, and post-accident analysis. By fusing multi-sensor data and using collaborative perception techniques, the system delivers proactive alerts and detailed analyses of accident causation, aiding traffic management and enforcement agencies in improving road safety. 
Although AccidentGPT provides solid scene understanding, its reliance on complex sensor fusion and modular integration may restrict its scalability and limit its practical use in infrastructure-based accident detection. Furthermore, Zhang et al. \cite{zhang2025language} introduced a framework that leverages MLLMs for structured and scalable traffic accident analysis through video classification and visual grounding, incorporating severity-based aggregation, multimodal prompts, and a tailored evaluation metric. Lohner et al. \cite{lohner2024enhancing} introduced the idea of enhancing MLLM inputs with scene graphs.","Vision-based accident detection methods can be categorized by their model architectures into several categories, including both frame-level and object-centric approaches \cite{fang2023vision}. Frame-level accident detection relies on extracting features from individual frames and requires annotating accident windows in traffic recordings. You and Han \cite{you2020traffic} propose a dataset for traffic understanding, where accident windows are detected by classifying feature embeddings derived from video frames using a 3D-CNN model.
In contrast, methods that emphasize object detection and tracking aim to identify accident participants by detecting and localizing objects in images and monitoring their movement over time. Various approaches have used the YOLO detector \cite{7780460} previously, among which are YOLOv4 \cite{ghahremannezhad2022real} and YOLOv5 \cite{xia2022research}. Karim et al. \cite{karim2024visual} employed YOLOv8 to  detect vehicles and accidents in real time from surveillance video. They integrated Deep-SORT \cite{wojke2017simple} to track vehicle movement across frames, supporting temporal analysis for accident detection. While 3D-CNN based classification and YOLO with Deep-SORT tracking effectively pinpoint accident participants, they sometimes fall short in capturing the broader context needed for a more thorough analysis of accident scenarios.


The growth of artificial intelligence has increased the demand for extensive datasets to train models for specific applications, including accident detection. Consequently, numerous datasets have been developed to support research in this area. Yao et al. \cite{yao2022dota} propose an unsupervised traffic anomaly detection framework that employs future object localization and a novel  spatial-temporal area under curve (STAUC) on the richly annotated DoTA dataset to effectively identify abnormal events in dynamic, egocentric driving videos.
However, since the DoTA dataset primarily comprises dashcam video recordings with highly dynamic perspectives, it may not be ideally suited for applications requiring fixed, infrastructure-based scenes for accident detection.


Lee et al. \cite{lee2017crash} introduced GTACrash, a synthetic dataset derived from GTA V that enables training CNN-based collision prediction algorithms, and demonstrated that synthetic data is a valuable alternative to scarce real-world accident data, although the approach suffered from limited accident diversity and realism.
DeepAccident \cite{wang2024deepaccident} is a large-scale, simulator-generated V2X dataset for autonomous driving, developed using the CARLA simulator \cite{dosovitskiy2017carla}. It offers a wide variety of collision scenarios, accompanied by comprehensive sensor and annotation data from 6 infrastructure cameras, making it a robust resource for accident prediction and accident detection research.


ConnectGPT \cite{tong2024connectgpt} introduces a pipeline that integrates GPT-4 with infrastructure cameras and V2X communication to automatically analyze traffic conditions and generate standardized C-ITS messages, thereby enhancing real-time incident detection and road safety. Authors demonstrated its potential through practical experiments on a small dataset, highlighting improved responsiveness and reduced manual intervention in managing traffic hazards. However, its reliance solely on GPT-4 without evaluating alternative multimodal LLMs may limit its broader applicability.

AccidentGPT \cite{wu2024accidentgpt} presented a framework that combines V2X environmental perception with GPT-based reasoning to enable real-time accident prediction, prevention, and post-accident analysis. By fusing multi-sensor data and using collaborative perception techniques, the system delivers proactive alerts and detailed analyses of accident causation, aiding traffic management and enforcement agencies in improving road safety. 
Although AccidentGPT provides solid scene understanding, its reliance on complex sensor fusion and modular integration may restrict its scalability and limit its practical use in infrastructure-based accident detection. Furthermore, Zhang et al. \cite{zhang2025language} introduced a framework that leverages MLLMs for structured and scalable traffic accident analysis through video classification and visual grounding, incorporating severity-based aggregation, multimodal prompts, and a tailored evaluation metric. Lohner et al. \cite{lohner2024enhancing} introduced the idea of enhancing MLLM inputs with scene graphs.",N/A
