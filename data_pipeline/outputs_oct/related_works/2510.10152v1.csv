arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2510.10152v1,http://arxiv.org/abs/2510.10152v1,2025-10-11 10:21:19+00:00,Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,"In this work, we present Color3D, a highly adaptable framework for colorizing
both static and dynamic 3D scenes from monochromatic inputs, delivering
visually diverse and chromatically vibrant reconstructions with flexible
user-guided control. In contrast to existing methods that focus solely on
static scenarios and enforce multi-view consistency by averaging color
variations which inevitably sacrifice both chromatic richness and
controllability, our approach is able to preserve color diversity and
steerability while ensuring cross-view and cross-time consistency. In
particular, the core insight of our method is to colorize only a single key
view and then fine-tune a personalized colorizer to propagate its color to
novel views and time steps. Through personalization, the colorizer learns a
scene-specific deterministic color mapping underlying the reference view,
enabling it to consistently project corresponding colors to the content in
novel views and video frames via its inherent inductive bias. Once trained, the
personalized colorizer can be applied to infer consistent chrominance for all
other images, enabling direct reconstruction of colorful 3D scenes with a
dedicated Lab color space Gaussian splatting representation. The proposed
framework ingeniously recasts complicated 3D colorization as a more tractable
single image paradigm, allowing seamless integration of arbitrary image
colorization models with enhanced flexibility and controllability. Extensive
experiments across diverse static and dynamic 3D colorization benchmarks
substantiate that our method can deliver more consistent and chromatically rich
renderings with precise user control. Project Page
https://yecongwan.github.io/Color3D/.","\vspace{-6pt}
\noindent\textbf{Radiance Fields for Static and Dynamic Scenes.}
Radiance field representations \cite{mildenhall2021nerf,kerbl20233d} have driven a paradigm shift in novel view synthesis. Neural Radiance Fields (NeRF) \cite{mildenhall2021nerf} pioneered implicit volumetric modeling with coordinate-based neural networks, inspiring numerous extensions \cite{barron2021mip,barron2022mip,barron2023zip,verbin2022ref,chen2022tensorf,fridovich2022plenoxels,garbin2021fastnerf,muller2022instant} and subsequent work on dynamic scenes \cite{park2021nerfies,park2021hypernerf,wang2023masked,fang2022fast,liu2023robust,guo2023forward,shao2023tensor4d}. However, NeRF-based methods remain hindered by costly training and slow rendering. To overcome this, 3D Gaussian Splatting (3DGS) \cite{kerbl20233d} introduced an explicit Gaussian representation that enables real-time, photorealistic rendering. Recent advances further extend Gaussian splatting to dynamic reconstruction \cite{duan20244d,li2024spacetime,yang2023real,wu20244d,yang2024deformable}, combining efficiency and fidelity. Motivated by these developments, we employ 3DGS and 4DGS as canonical static and dynamic representations, building a unified framework for controllable 3D colorization with strong spatial-temporal consistency.

\noindent\textbf{From 2D Colorization to 3D.}
Image colorization aims to recover plausible chromatic content from grayscale inputs, with decades of progress improving realism and controllability \cite{zhang2017real,wu2021towards,kim2022bigcolor,kang2023ddcolor,ji2022colorformer}. Beyond automatic approaches, user-guided methods leverage reference images \cite{he2018deep,huang2022unicolor,zhao2021color2embed} or language prompts \cite{weng2023cad,zabari2023diffusing,weng2022code,chang2023coins,liang2024control}. Extending to videos introduces temporal consistency challenges, tackled by matching \cite{yang2024bistnet}, palette transfer \cite{wang2025consistent}, attention \cite{li2024towards}, and memory propagation \cite{yang2024colormnet}. Yet these remain limited to 2D domains, lacking cross-view consistency.
Efforts on 3D scene colorization are still nascent. GBC \cite{liao2024gbc} exploits video models for continuous inputs, while ChromaDistill \cite{dhiman2023corf} and ColorNeRF \cite{cheng2024colorizing} transfer knowledge from pretrained colorizers by averaging inconsistent predictions, often sacrificing controllability and vividness. Moreover, colorizing dynamic 3D scenes while ensuring spatial-temporal consistency remains unaddressed. Our work bridges this gap with a unified framework for both static and dynamic settings, achieving visually rich, consistent, and user-controllable 3D colorization.

\vspace{-6pt}
\vspace{-3pt}","\vspace{-6pt}
\noindent\textbf{Radiance Fields for Static and Dynamic Scenes.}
Radiance field representations \cite{mildenhall2021nerf,kerbl20233d} have driven a paradigm shift in novel view synthesis. Neural Radiance Fields (NeRF) \cite{mildenhall2021nerf} pioneered implicit volumetric modeling with coordinate-based neural networks, inspiring numerous extensions \cite{barron2021mip,barron2022mip,barron2023zip,verbin2022ref,chen2022tensorf,fridovich2022plenoxels,garbin2021fastnerf,muller2022instant} and subsequent work on dynamic scenes \cite{park2021nerfies,park2021hypernerf,wang2023masked,fang2022fast,liu2023robust,guo2023forward,shao2023tensor4d}. However, NeRF-based methods remain hindered by costly training and slow rendering. To overcome this, 3D Gaussian Splatting (3DGS) \cite{kerbl20233d} introduced an explicit Gaussian representation that enables real-time, photorealistic rendering. Recent advances further extend Gaussian splatting to dynamic reconstruction \cite{duan20244d,li2024spacetime,yang2023real,wu20244d,yang2024deformable}, combining efficiency and fidelity. Motivated by these developments, we employ 3DGS and 4DGS as canonical static and dynamic representations, building a unified framework for controllable 3D colorization with strong spatial-temporal consistency.

\noindent\textbf{From 2D Colorization to 3D.}
Image colorization aims to recover plausible chromatic content from grayscale inputs, with decades of progress improving realism and controllability \cite{zhang2017real,wu2021towards,kim2022bigcolor,kang2023ddcolor,ji2022colorformer}. Beyond automatic approaches, user-guided methods leverage reference images \cite{he2018deep,huang2022unicolor,zhao2021color2embed} or language prompts \cite{weng2023cad,zabari2023diffusing,weng2022code,chang2023coins,liang2024control}. Extending to videos introduces temporal consistency challenges, tackled by matching \cite{yang2024bistnet}, palette transfer \cite{wang2025consistent}, attention \cite{li2024towards}, and memory propagation \cite{yang2024colormnet}. Yet these remain limited to 2D domains, lacking cross-view consistency.
Efforts on 3D scene colorization are still nascent. GBC \cite{liao2024gbc} exploits video models for continuous inputs, while ChromaDistill \cite{dhiman2023corf} and ColorNeRF \cite{cheng2024colorizing} transfer knowledge from pretrained colorizers by averaging inconsistent predictions, often sacrificing controllability and vividness. Moreover, colorizing dynamic 3D scenes while ensuring spatial-temporal consistency remains unaddressed. Our work bridges this gap with a unified framework for both static and dynamic settings, achieving visually rich, consistent, and user-controllable 3D colorization.

\vspace{-6pt}
\vspace{-3pt}",N/A
