arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2510.09012v1,http://arxiv.org/abs/2510.09012v1,2025-10-10 05:26:11+00:00,Towards Better & Faster Autoregressive Image Generation: From the Perspective of Entropy,"In this work, we first revisit the sampling issues in current autoregressive
(AR) image generation models and identify that image tokens, unlike text
tokens, exhibit lower information density and non-uniform spatial distribution.
Accordingly, we present an entropy-informed decoding strategy that facilitates
higher autoregressive generation quality with faster synthesis speed.
Specifically, the proposed method introduces two main innovations: 1) dynamic
temperature control guided by spatial entropy of token distributions, enhancing
the balance between content diversity, alignment accuracy, and structural
coherence in both mask-based and scale-wise models, without extra computational
overhead, and 2) entropy-aware acceptance rules in speculative decoding,
achieving near-lossless generation at about 85\% of the inference cost of
conventional acceleration methods. Extensive experiments across multiple
benchmarks using diverse AR image generation models demonstrate the
effectiveness and generalizability of our approach in enhancing both generation
quality and sampling speed.","\label{sec_related_works}
\subsection{Autoregressive image generation}
Early work~\cite{van2016pixelcnn} generates images directly at pixel level. Later approaches adopt a two-stage pipeline: images are first quantized into discrete tokens~\cite{esser2021vqgan,yu2021vit_vqgan}, then generated with Transformers in raster order~\cite{ding2021cogview,ge2023seed,ramesh2021dalle,yu2022parti,he2024mars,wang2024emu3}.
Recent efforts scale this paradign with larger models and stronger conditioning. LlamaGen~\cite{sun2024llamagen} provides class and text-conditioned baselines; Lumina-mGPT~\cite{liu2024lumina_mgpt} and Anole~\cite{chern2024anole} fine-tune Chameleon~\cite{chameleonteam2025chameleon} for improved text-conditioned generation. Unified frameworks further bridge understanding and generation~\cite{wu2024janus,chen2025janus_pro,jiao2025unitoken,unitok} in a single Transformer.
Meanwhile, image tokenizers have evolved for better reconstruction~\cite{yu2024titok,lee2022rqvae,yu2023lfq,zhao2024bsq} or multimodal integration~\cite{zhang2025v2flow,qu2024tokenflow}.

While proven effective, the vanilla autoregressive paradigm suffers from slow and rigid next-token prediction. To improve efficiency, recent studies explore more strategies, including multi-token prediction via random masking~\cite{chang2022maskgit,bai2024meissonic,xie2024show_o}, coarse-to-fine modeling~\cite{tian2024var,ma2024star,tang2024hart,han2024infinity}or hybrid approaches~\cite{he2025nar,yu2024rar}. Nonetheless, vector-quantized models still rely on sampling from token distributions, making generation quality sensitive to the sampling strategy.

\subsection{Sampling strategies in autoregressive models}
Transformers model the probability distribution over tokens, requiring specific sampling strategies to obtain concrete outputs. Common approaches in language modeling include top-\emph{k}~\cite{radford2019gpt2} and top-\emph{p}~\cite{holtzman2019top_p} sampling, which truncate the candidate space by rank or cumulative probability. EDT~\cite{zhang2024edt} dynamically adjusts temperature based on entropy to balance diversity and precision. Other approaches explore repetition penalties~\cite{keskar2019repetition_penalty}, contrastive decoding~\cite{chuang2023dola}, speculative decoding~\cite{leviathan2023speculative_decoding,chen2023accelerating}, and search-based techniques~\cite{meister2020beam_search,snell2024lookahead,guan2025rstar,lightman2023letsverifystepstep,snell2024scalingllmtts} to reduce hallucination or speed up inference.

In visual generation, a higher degree of randomness is often needed to produce more realistic and detailed content. LlamaGen~\cite{sun2024llamagen} and Lumina-mGPT~\cite{liu2024lumina_mgpt} demonstrate that much larger top-\emph{k} values than those used in language models help avoid over-smoothed and low-detail outputs. Recent methods~\cite{teng2024sjd,jang2025lantern} apply speculative~\cite{leviathan2023spec_decode} or parallel decoding~\cite{he2024zipar,wang2024par} to accelerate image synthesis. 
However, they overlook the highly uneven spatial information distribution in images compared to text, and do not tailor decoding for autoregressive image generation.","\subsection{Autoregressive image generation}
Early work~\cite{van2016pixelcnn} generates images directly at pixel level. Later approaches adopt a two-stage pipeline: images are first quantized into discrete tokens~\cite{esser2021vqgan,yu2021vit_vqgan}, then generated with Transformers in raster order~\cite{ding2021cogview,ge2023seed,ramesh2021dalle,yu2022parti,he2024mars,wang2024emu3}.
Recent efforts scale this paradign with larger models and stronger conditioning. LlamaGen~\cite{sun2024llamagen} provides class and text-conditioned baselines; Lumina-mGPT~\cite{liu2024lumina_mgpt} and Anole~\cite{chern2024anole} fine-tune Chameleon~\cite{chameleonteam2025chameleon} for improved text-conditioned generation. Unified frameworks further bridge understanding and generation~\cite{wu2024janus,chen2025janus_pro,jiao2025unitoken,unitok} in a single Transformer.
Meanwhile, image tokenizers have evolved for better reconstruction~\cite{yu2024titok,lee2022rqvae,yu2023lfq,zhao2024bsq} or multimodal integration~\cite{zhang2025v2flow,qu2024tokenflow}.

While proven effective, the vanilla autoregressive paradigm suffers from slow and rigid next-token prediction. To improve efficiency, recent studies explore more strategies, including multi-token prediction via random masking~\cite{chang2022maskgit,bai2024meissonic,xie2024show_o}, coarse-to-fine modeling~\cite{tian2024var,ma2024star,tang2024hart,han2024infinity}or hybrid approaches~\cite{he2025nar,yu2024rar}. Nonetheless, vector-quantized models still rely on sampling from token distributions, making generation quality sensitive to the sampling strategy.

\subsection{Sampling strategies in autoregressive models}
Transformers model the probability distribution over tokens, requiring specific sampling strategies to obtain concrete outputs. Common approaches in language modeling include top-\emph{k}~\cite{radford2019gpt2} and top-\emph{p}~\cite{holtzman2019top_p} sampling, which truncate the candidate space by rank or cumulative probability. EDT~\cite{zhang2024edt} dynamically adjusts temperature based on entropy to balance diversity and precision. Other approaches explore repetition penalties~\cite{keskar2019repetition_penalty}, contrastive decoding~\cite{chuang2023dola}, speculative decoding~\cite{leviathan2023speculative_decoding,chen2023accelerating}, and search-based techniques~\cite{meister2020beam_search,snell2024lookahead,guan2025rstar,lightman2023letsverifystepstep,snell2024scalingllmtts} to reduce hallucination or speed up inference.

In visual generation, a higher degree of randomness is often needed to produce more realistic and detailed content. LlamaGen~\cite{sun2024llamagen} and Lumina-mGPT~\cite{liu2024lumina_mgpt} demonstrate that much larger top-\emph{k} values than those used in language models help avoid over-smoothed and low-detail outputs. Recent methods~\cite{teng2024sjd,jang2025lantern} apply speculative~\cite{leviathan2023spec_decode} or parallel decoding~\cite{he2024zipar,wang2024par} to accelerate image synthesis. 
However, they overlook the highly uneven spatial information distribution in images compared to text, and do not tailor decoding for autoregressive image generation.","2.1 Autoregressive image generation
Early work [ 27] generates images directly at pixel level. Later approaches adopt a two-stage pipeline:
images are first quantized into discrete tokens [ 10,11], then generated with Transformers in raster
order [ 28,29,30,31,32,33]. Recent efforts scale this paradign with larger models and stronger
conditioning. LlamaGen [ 1] provides class and text-conditioned baselines; Lumina-mGPT [ 2] and
2
Top-k
Top-p
Temp.CFG
FIDCLIPCFG
(1) k=16384; CFG=2(2) k=16384; CFG=7.5
(3) k=1; CFG<2
(3) k=1; CFG=7.5
Top-kGreedyRandom
(c)Quantitative Evaluation Across Settings(b)Qualitative Comparison Across SettingsAvg.Spectral of Segments
TextImage
(a) Difference between Text & ImageFrequency (%)Figure 2: (a) Comparison of information density between image and text. Histogram of average
frequency-domain embeddings from LlamaGen [ 1] (image) and Qwen2 [ 6] (text) show the uneven
spatial distribution in images with a large amount of low-frequency components. (b) Qualitative
results under various configurations. High CFG (Classfier-Free Guidance) or low top- Koften harms
fidelity, while lower CFG with higher top- Kimproves fidelity but may reduce text-image consistency.
(c) Quantitative evaluation of LlamaGen under different sampling settings.
Anole [ 34] fine-tune Chameleon [ 12] for improved text-conditioned generation. Unified frameworks
further bridge understanding and generation [ 8,9,35,36] in a single Transformer. Meanwhile, image
tokenizers have evolved for better reconstruction [ 37,38,39,40] or multimodal integration [ 41,42].
While proven effective, the vanilla autoregressive paradigm suffers from slow and rigid next-token pre-
diction. To improve efficiency, recent studies explore more strategies, including multi-token prediction
via random masking [ 14,43,44], coarse-to-fine modeling [ 7,13,45,46]or hybrid approaches [ 47,48].
Nonetheless, vector-quantized models still rely on sampling from token distributions, making genera-
tion quality sensitive to the sampling strategy.
2.2 Sampling strategies in autoregressive models
Transformers model the probability distribution over tokens, requiring specific sampling strategies
to obtain concrete outputs. Common approaches in language modeling include top-k[ 49] and top-
p[18] sampling, which truncate the candidate space by rank or cumulative probability. EDT [ 20]
dynamically adjusts temperature based on entropy to balance diversity and precision. Other ap-
proaches explore repetition penalties [ 50], contrastive decoding [ 22], speculative decoding [ 51,52],
and search-based techniques [53, 54, 25, 55, 56] to reduce hallucination or speed up inference.
In visual generation, a higher degree of randomness is often needed to produce more realistic and
detailed content. LlamaGen [ 1] and Lumina-mGPT [ 2] demonstrate that much larger top-kvalues
than those used in language models help avoid over-smoothed and low-detail outputs. Recent
methods [ 26,57] apply speculative [ 58] or parallel decoding [ 59,60] to accelerate image synthesis.
However, they overlook the highly uneven spatial information distribution in images compared to
text, and do not tailor decoding for autoregressive image generation."
