arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2510.07459v1,http://arxiv.org/abs/2510.07459v1,2025-10-08 19:04:25+00:00,MoGU: Mixture-of-Gaussians with Uncertainty-based Gating for Time Series Forecasting,"We introduce Mixture-of-Gaussians with Uncertainty-based Gating (MoGU), a
novel Mixture-of-Experts (MoE) framework designed for regression tasks and
applied to time series forecasting. Unlike conventional MoEs that provide only
point estimates, MoGU models each expert's output as a Gaussian distribution.
This allows it to directly quantify both the forecast (the mean) and its
inherent uncertainty (variance). MoGU's core innovation is its
uncertainty-based gating mechanism, which replaces the traditional input-based
gating network by using each expert's estimated variance to determine its
contribution to the final prediction. Evaluated across diverse time series
forecasting benchmarks, MoGU consistently outperforms single-expert models and
traditional MoE setups. It also provides well-quantified, informative
uncertainties that directly correlate with prediction errors, enhancing
forecast reliability. Our code is available from:
https://github.com/yolish/moe_unc_tsf","\label{related_work}
\textbf{MoE Models}
The pursuit of increasingly capable and adaptable artificial intelligence systems has led to the development of sophisticated architectural paradigms, among which the Mixture-of-Experts (MoE) stands out. MoE is an architectural concept that adaptively combines predictions from multiple specialized neural modules, often sharing a common architecture, through a learned gating mechanism. This paradigm allows for a dynamic allocation of computational resources, enabling models to specialize on different sub-problems or data modalities. Early implementations of MoE \citep{jacobs1991adaptive} focused on ensemble learning (ensemble MoE), where multiple models (experts) contributed to a final prediction. More recently, MoE layers have been seamlessly integrated within larger neural architectures, with experts operating in latent domains (latent MoE)  \citep{shazeer2017outrageously,fedus2022switch}. This integration has proven particularly impactful in the realm of large language models (LLMs), where MoE layers have been instrumental in scaling models to unprecedented sizes while managing computational costs \citep{lepikhin2020gshard, jiang2024mixtral, dai2024deepseekmoe}. By selectively activating only a subset of experts for each input token, MoEs enable models with vast numbers of parameters to achieve high performance without incurring the prohibitive inference costs of densely activated large models.
Despite  their contribution and adoption, both ensemble and latent MoE architectures typically output point estimates, both at the level of the individual expert and at the level of the overall model. This limits the ability to quantify uncertainty which is important for 
decision-making. 
Few works have explored uncertainty estimation for MoE architectures  
(see e.g. \cite{pavlitska2025moeuncertainty,zhang2023mofme}).
In this work, we focus on ensemble MoE architectures, as uncertainty quantification is more directly applicable for decision making and interpretability. 
In our method, we view the experts of the MoE model as an ensemble of models that can be used to extract both aleatoric and epistemic uncertainties.

\textbf{Uncertainty Estimation for Regression Tasks.} Deep learning regression models are increasingly required not only to provide accurate point estimates but also to quantify predictive uncertainty. A large body of research has focused on Bayesian neural networks, which place distributions over weights and approximate posterior inference using variational methods or Monte Carlo dropout, thereby producing predictive intervals \citep{gal2016dropout}. Another line of work employs ensembles of neural networks to capture both aleatoric and epistemic uncertainties, with randomized initialization or bootstrapped training providing diverse predictions \citep{lakshminarayanan2017simple}. More recently, post-hoc calibration techniques have been proposed, adapting classification-oriented approaches such as temperature scaling to regression settings, for instance by optimizing proper scoring rules or variance scaling factors \citep{kuleshov2018accurate}. Beyond probabilistic calibration, conformal prediction (CP) methods have gained attention due to their finite-sample coverage guarantees under minimal distributional assumptions. CP can be applied to regression to produce instance-dependent prediction intervals with guaranteed coverage, and has been extended to handle asymmetric intervals, distribution shift, and multi-target regression \citep{vovk2005algorithmic,romano2019conformalized}. 

\textbf{Time Series Forecasting and Uncertainty Estimation.} Time series forecasting is a critical discipline in machine learning and statistics, focusing on predicting future values from a sequence of historical data points ordered by time. This field has wide-ranging applications, including financial market analysis, energy consumption forecasting, weather prediction, and medical prognosis. Traditional statistical methods, such as Autoregressive Integrated Moving Average (ARIMA) and Exponential Smoothing, have been foundational. However, their effectiveness is often limited by their assumption of linearity and their inability to capture complex, non-linear dependencies. More recently, deep learning models, employing  Transformers \citep{Yuqietal-2023-PatchTST,autoformer,reformer}, Multi-Layer Perceptrons (MLPs) \citep{wang2024timemixer, dlinear}, and Convolutional Neural Networks (CNNs) \citep{wu2023timesnet}, were shown to be effective in modeling temporal dynamics and long-range dependencies \citep{wang2024deep,time_series_survey,xwang2024deep}. The ability to quantify the uncertainty of a forecast, rather than providing just a single point estimate, is of paramount importance.  Uncertainty quantification provides a confidence interval for the prediction, which is crucial for risk management and informed decision-making. Some recent works have introduced uncertainty estimation to time series forecasting (see e.g. \cite{cini2025corel, wu2025eci}).
Given its wide-ranging applications, the importance of reporting uncertainty, and its challenging nature, time series forecasting serves as a highly suitable domain to evaluate the performance of MoGU.","\textbf{MoE Models}
The pursuit of increasingly capable and adaptable artificial intelligence systems has led to the development of sophisticated architectural paradigms, among which the Mixture-of-Experts (MoE) stands out. MoE is an architectural concept that adaptively combines predictions from multiple specialized neural modules, often sharing a common architecture, through a learned gating mechanism. This paradigm allows for a dynamic allocation of computational resources, enabling models to specialize on different sub-problems or data modalities. Early implementations of MoE \citep{jacobs1991adaptive} focused on ensemble learning (ensemble MoE), where multiple models (experts) contributed to a final prediction. More recently, MoE layers have been seamlessly integrated within larger neural architectures, with experts operating in latent domains (latent MoE)  \citep{shazeer2017outrageously,fedus2022switch}. This integration has proven particularly impactful in the realm of large language models (LLMs), where MoE layers have been instrumental in scaling models to unprecedented sizes while managing computational costs \citep{lepikhin2020gshard, jiang2024mixtral, dai2024deepseekmoe}. By selectively activating only a subset of experts for each input token, MoEs enable models with vast numbers of parameters to achieve high performance without incurring the prohibitive inference costs of densely activated large models.
Despite  their contribution and adoption, both ensemble and latent MoE architectures typically output point estimates, both at the level of the individual expert and at the level of the overall model. This limits the ability to quantify uncertainty which is important for 
decision-making. 
Few works have explored uncertainty estimation for MoE architectures  
(see e.g. \cite{pavlitska2025moeuncertainty,zhang2023mofme}).
In this work, we focus on ensemble MoE architectures, as uncertainty quantification is more directly applicable for decision making and interpretability. 
In our method, we view the experts of the MoE model as an ensemble of models that can be used to extract both aleatoric and epistemic uncertainties.

\textbf{Uncertainty Estimation for Regression Tasks.} Deep learning regression models are increasingly required not only to provide accurate point estimates but also to quantify predictive uncertainty. A large body of research has focused on Bayesian neural networks, which place distributions over weights and approximate posterior inference using variational methods or Monte Carlo dropout, thereby producing predictive intervals \citep{gal2016dropout}. Another line of work employs ensembles of neural networks to capture both aleatoric and epistemic uncertainties, with randomized initialization or bootstrapped training providing diverse predictions \citep{lakshminarayanan2017simple}. More recently, post-hoc calibration techniques have been proposed, adapting classification-oriented approaches such as temperature scaling to regression settings, for instance by optimizing proper scoring rules or variance scaling factors \citep{kuleshov2018accurate}. Beyond probabilistic calibration, conformal prediction (CP) methods have gained attention due to their finite-sample coverage guarantees under minimal distributional assumptions. CP can be applied to regression to produce instance-dependent prediction intervals with guaranteed coverage, and has been extended to handle asymmetric intervals, distribution shift, and multi-target regression \citep{vovk2005algorithmic,romano2019conformalized}. 

\textbf{Time Series Forecasting and Uncertainty Estimation.} Time series forecasting is a critical discipline in machine learning and statistics, focusing on predicting future values from a sequence of historical data points ordered by time. This field has wide-ranging applications, including financial market analysis, energy consumption forecasting, weather prediction, and medical prognosis. Traditional statistical methods, such as Autoregressive Integrated Moving Average (ARIMA) and Exponential Smoothing, have been foundational. However, their effectiveness is often limited by their assumption of linearity and their inability to capture complex, non-linear dependencies. More recently, deep learning models, employing  Transformers \citep{Yuqietal-2023-PatchTST,autoformer,reformer}, Multi-Layer Perceptrons (MLPs) \citep{wang2024timemixer, dlinear}, and Convolutional Neural Networks (CNNs) \citep{wu2023timesnet}, were shown to be effective in modeling temporal dynamics and long-range dependencies \citep{wang2024deep,time_series_survey,xwang2024deep}. The ability to quantify the uncertainty of a forecast, rather than providing just a single point estimate, is of paramount importance.  Uncertainty quantification provides a confidence interval for the prediction, which is crucial for risk management and informed decision-making. Some recent works have introduced uncertainty estimation to time series forecasting (see e.g. \cite{cini2025corel, wu2025eci}).
Given its wide-ranging applications, the importance of reporting uncertainty, and its challenging nature, time series forecasting serves as a highly suitable domain to evaluate the performance of MoGU.","MoE ModelsThe pursuit of increasingly capable and adaptable artificial intelligence systems has led
to the development of sophisticated architectural paradigms, among which the Mixture-of-Experts
(MoE) stands out. MoE is an architectural concept that adaptively combines predictions from
multiple specialized neural modules, often sharing a common architecture, through a learned gating
mechanism. This paradigm allows for a dynamic allocation of computational resources, enabling
models to specialize on different sub-problems or data modalities. Early implementations of MoE
(Jacobs et al., 1991) focused on ensemble learning (ensemble MoE), where multiple models (experts)
contributed to a final prediction. More recently, MoE layers have been seamlessly integrated within
larger neural architectures, with experts operating in latent domains (latent MoE) (Shazeer et al., 2017;
Fedus et al., 2022). This integration has proven particularly impactful in the realm of large language
models (LLMs), where MoE layers have been instrumental in scaling models to unprecedented sizes
while managing computational costs (Lepikhin et al., 2020; Jiang et al., 2024; Dai et al., 2024). By
selectively activating only a subset of experts for each input token, MoEs enable models with vast
numbers of parameters to achieve high performance without incurring the prohibitive inference costs
of densely activated large models. Despite their contribution and adoption, both ensemble and latent
MoE architectures typically output point estimates, both at the level of the individual expert and at
the level of the overall model. This limits the ability to quantify uncertainty which is important for
decision-making. Few works have explored uncertainty estimation for MoE architectures (see e.g.
Pavlitska et al. (2025); Zhang et al. (2023)). In this work, we focus on ensemble MoE architectures,
as uncertainty quantification is more directly applicable for decision making and interpretability. In
2
our method, we view the experts of the MoE model as an ensemble of models that can be used to
extract both aleatoric and epistemic uncertainties.
Uncertainty Estimation for Regression Tasks.Deep learning regression models are increasingly re-
quired not only to provide accurate point estimates but also to quantify predictive uncertainty. A large
body of research has focused on Bayesian neural networks, which place distributions over weights and
approximate posterior inference using variational methods or Monte Carlo dropout, thereby producing
predictive intervals (Gal & Ghahramani, 2016). Another line of work employs ensembles of neural
networks to capture both aleatoric and epistemic uncertainties, with randomized initialization or
bootstrapped training providing diverse predictions (Lakshminarayanan et al., 2017). More recently,
post-hoc calibration techniques have been proposed, adapting classification-oriented approaches
such as temperature scaling to regression settings, for instance by optimizing proper scoring rules
or variance scaling factors (Kuleshov et al., 2018). Beyond probabilistic calibration, conformal
prediction (CP) methods have gained attention due to their finite-sample coverage guarantees under
minimal distributional assumptions. CP can be applied to regression to produce instance-dependent
prediction intervals with guaranteed coverage, and has been extended to handle asymmetric intervals,
distribution shift, and multi-target regression (V ovk et al., 2005; Romano et al., 2019).
Time Series Forecasting and Uncertainty Estimation.Time series forecasting is a critical discipline
in machine learning and statistics, focusing on predicting future values from a sequence of historical
data points ordered by time. This field has wide-ranging applications, including financial market
analysis, energy consumption forecasting, weather prediction, and medical prognosis. Traditional
statistical methods, such as Autoregressive Integrated Moving Average (ARIMA) and Exponential
Smoothing, have been foundational. However, their effectiveness is often limited by their assumption
of linearity and their inability to capture complex, non-linear dependencies. More recently, deep
learning models, employing Transformers (Nie et al., 2023; Wu et al., 2021; Kitaev et al., 2020),
Multi-Layer Perceptrons (MLPs) (Wang et al., 2024b; Zeng et al., 2023), and Convolutional Neural
Networks (CNNs) (Wu et al., 2023), were shown to be effective in modeling temporal dynamics and
long-range dependencies (Wang et al., 2024a; Lim & Zohren, 2021; Wang et al., 2024c). The ability to
quantify the uncertainty of a forecast, rather than providing just a single point estimate, is of paramount
importance. Uncertainty quantification provides a confidence interval for the prediction, which is
crucial for risk management and informed decision-making. Some recent works have introduced
uncertainty estimation to time series forecasting (see e.g. Cini et al. (2025); Wu et al. (2025)). Given
its wide-ranging applications, the importance of reporting uncertainty, and its challenging nature,
time series forecasting serves as a highly suitable domain to evaluate the performance of MoGU."
