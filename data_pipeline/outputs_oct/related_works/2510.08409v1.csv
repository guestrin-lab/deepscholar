arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2510.08409v1,http://arxiv.org/abs/2510.08409v1,2025-10-09 16:28:48+00:00,Optimal Stopping in Latent Diffusion Models,"We identify and analyze a surprising phenomenon of Latent Diffusion Models
(LDMs) where the final steps of the diffusion can degrade sample quality. In
contrast to conventional arguments that justify early stopping for numerical
stability, this phenomenon is intrinsic to the dimensionality reduction in
LDMs. We provide a principled explanation by analyzing the interaction between
latent dimension and stopping time. Under a Gaussian framework with linear
autoencoders, we characterize the conditions under which early stopping is
needed to minimize the distance between generated and target distributions.
More precisely, we show that lower-dimensional representations benefit from
earlier termination, whereas higher-dimensional latent spaces require later
stopping time. We further establish that the latent dimension interplays with
other hyperparameters of the problem such as constraints in the parameters of
score matching. Experiments on synthetic and real datasets illustrate these
properties, underlining that early stopping can improve generative quality.
Together, our results offer a theoretical foundation for understanding how the
latent dimension influences the sample quality, and highlight stopping time as
a key hyperparameter in LDMs.","\label{sec:related-work}
\paragraph{Learning low-dimensional data with diffusion models.} Riemannian Diffusion Models, introduced by \citet{huang2022riemannian, de2022riemannian}, generalize the diffusion process to operate on Riemannian manifolds and preserve a known geometric structure by design. Subsequent theoretical work has analyzed the behavior of standard Denoising Diffusion Probabilistic Models (DDPMs) under the manifold hypothesis, demonstrating that they can implicitly adapt to the data's intrinsic dimension without explicit knowledge of the manifold \citep{tang2024adaptivity, george2025analysis}. 

Further improvements in computational and memory efficiency were introduced by LDMs \citep{rombach2022high} by first training a compression model to transform images into a  lower-dimensional latent space, from which the original data can be reconstructed at high fidelity. In practice, this approach is implemented with a regularized VAE \citep{esser2021taming}. The LDM is then trained in the latent space. Building on this core concept, LDMs have been extended to new domains, such as the generation of high-resolution videos \citep{blattmann2023align}. Furthermore, extensive research has focused on improving LDM's sampling quality, including methods like aligning encoded images with DINOv2 representations \citep{yu2024representation}, and enhancing the robustness of the latent space through explicit or implicit equivariance constraints \citep{kouzelis2025eq, skorokhodov2025improving, zhou2025alias}.
In contrast to standard diffusion models, theoretical properties of LDMs have been little studied; in this work, we investigate the connection of the latent dimension with diffusion stopping time and score matching regularization.

\paragraph{Optimal stopping time of diffusion models.} 
Focusing on a theoretical analysis of this phenomenon, \citet{achilli2025memorization} investigate the optimal stopping time for diffusion models under the assumption that the data is concentrated on a low-dimensional manifold, a concept formalized by the Hidden Manifold Model \citep{goldt2020modeling}. Closer to our contribution is the work of \citet{hurault2025score}. They also investigate the scenario where the true data distribution is Gaussian. Their analysis focuses on learning the score function using SGD, and allows them to determine an optimal stopping time. However, the study of these authors is limited to the diffusion model and did not consider the two-stage architecture of LDMs. Furthermore, the relationship between the data dimension and the derived optimal stopping time remained unexplored in their findings. In contrast, our work directly investigates the influence of the latent dimension on the optimal stopping time by incorporating an autoencoder into the diffusion model framework. We also demonstrate the need of early stopping without discretization of the backward diffusion process.","\paragraph{Learning low-dimensional data with diffusion models.} Riemannian Diffusion Models, introduced by \citet{huang2022riemannian, de2022riemannian}, generalize the diffusion process to operate on Riemannian manifolds and preserve a known geometric structure by design. Subsequent theoretical work has analyzed the behavior of standard Denoising Diffusion Probabilistic Models (DDPMs) under the manifold hypothesis, demonstrating that they can implicitly adapt to the data's intrinsic dimension without explicit knowledge of the manifold \citep{tang2024adaptivity, george2025analysis}. 

Further improvements in computational and memory efficiency were introduced by LDMs \citep{rombach2022high} by first training a compression model to transform images into a  lower-dimensional latent space, from which the original data can be reconstructed at high fidelity. In practice, this approach is implemented with a regularized VAE \citep{esser2021taming}. The LDM is then trained in the latent space. Building on this core concept, LDMs have been extended to new domains, such as the generation of high-resolution videos \citep{blattmann2023align}. Furthermore, extensive research has focused on improving LDM's sampling quality, including methods like aligning encoded images with DINOv2 representations \citep{yu2024representation}, and enhancing the robustness of the latent space through explicit or implicit equivariance constraints \citep{kouzelis2025eq, skorokhodov2025improving, zhou2025alias}.
In contrast to standard diffusion models, theoretical properties of LDMs have been little studied; in this work, we investigate the connection of the latent dimension with diffusion stopping time and score matching regularization.

\paragraph{Optimal stopping time of diffusion models.} 
Focusing on a theoretical analysis of this phenomenon, \citet{achilli2025memorization} investigate the optimal stopping time for diffusion models under the assumption that the data is concentrated on a low-dimensional manifold, a concept formalized by the Hidden Manifold Model \citep{goldt2020modeling}. Closer to our contribution is the work of \citet{hurault2025score}. They also investigate the scenario where the true data distribution is Gaussian. Their analysis focuses on learning the score function using SGD, and allows them to determine an optimal stopping time. However, the study of these authors is limited to the diffusion model and did not consider the two-stage architecture of LDMs. Furthermore, the relationship between the data dimension and the derived optimal stopping time remained unexplored in their findings. In contrast, our work directly investigates the influence of the latent dimension on the optimal stopping time by incorporating an autoencoder into the diffusion model framework. We also demonstrate the need of early stopping without discretization of the backward diffusion process.","Learning low-dimensional data with diffusion models.Riemannian Diffusion Models, introduced
by Huang et al. (2022); De Bortoli et al. (2022), generalize the diffusion process to operate on
Riemannian manifolds and preserve a known geometric structure by design. Subsequent theoretical
work has analyzed the behavior of standard Denoising Diffusion Probabilistic Models (DDPMs)
under the manifold hypothesis, demonstrating that they can implicitly adapt to the data’s intrinsic
dimension without explicit knowledge of the manifold (Tang & Yang, 2024; George et al., 2025).
Further improvements in computational and memory efficiency were introduced by LDMs (Rombach
et al., 2022) by first training a compression model to transform images into a lower-dimensional latent
space, from which the original data can be reconstructed at high fidelity. In practice, this approach is
implemented with a regularized V AE (Esser et al., 2021). The LDM is then trained in the latent space.
Building on this core concept, LDMs have been extended to new domains, such as the generation
of high-resolution videos (Blattmann et al., 2023). Furthermore, extensive research has focused on
improving LDM’s sampling quality, including methods like aligning encoded images with DINOv2
representations (Yu et al., 2024), and enhancing the robustness of the latent space through explicit or
implicit equivariance constraints (Kouzelis et al., 2025; Skorokhodov et al., 2025; Zhou et al., 2025).
In contrast to standard diffusion models, theoretical properties of LDMs have been little studied; in
this work, we investigate the connection of the latent dimension with diffusion stopping time and
score matching regularization.
Optimal stopping time of diffusion models.Focusing on a theoretical analysis of this phenomenon,
Achilli et al. (2025) investigate the optimal stopping time for diffusion models under the assumption
that the data is concentrated on a low-dimensional manifold, a concept formalized by the Hidden
Manifold Model (Goldt et al., 2020). Closer to our contribution is the work of Hurault et al. (2025).
They also investigate the scenario where the true data distribution is Gaussian. Their analysis focuses
on learning the score function using SGD, and allows them to determine an optimal stopping time.
However, the study of these authors is limited to the diffusion model and did not consider the
two-stage architecture of LDMs. Furthermore, the relationship between the data dimension and the
derived optimal stopping time remained unexplored in their findings. In contrast, our work directly
investigates the influence of the latent dimension on the optimal stopping time by incorporating an
autoencoder into the diffusion model framework. We also demonstrate the need of early stopping
without discretization of the backward diffusion process."
