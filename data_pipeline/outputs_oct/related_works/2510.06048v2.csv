arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2510.06048v2,http://arxiv.org/abs/2510.06048v2,2025-10-07 15:42:33+00:00,BLISS: A Lightweight Bilevel Influence Scoring Method for Data Selection in Language Model Pretraining,"Effective data selection is essential for pretraining large language models
(LLMs), enhancing efficiency and improving generalization to downstream tasks.
However, existing approaches often require leveraging external pretrained
models, making it difficult to disentangle the effects of data selection from
those of the external pretrained models. In addition, they often overlook the
long-term impact of selected data if the model is trained to convergence,
primarily due to the prohibitive cost of full-scale LLM pretraining. In this
paper, we introduce BLISS (\textbf{B}ileve\textbf{L} \textbf{I}nfluence
\textbf{S}coring method for data \textbf{S}election): a lightweight data
selection method that operates entirely \emph{from scratch}, without relying on
any external pretrained oracle models, while explicitly accounting for the
long-term impact of selected data. BLISS leverages a small proxy model as a
surrogate for the LLM and employs a score model to estimate the long-term
influence of training samples if the proxy model is trained to convergence. We
formulate data selection as a bilevel optimization problem, where the
upper-level objective optimizes the score model to assign importance weights to
training samples, ensuring that minimizing the lower-level objective (i.e.,
training the proxy model over the weighted training loss until convergence)
leads to best validation performance. Once optimized, the trained score model
predicts influence scores for the dataset, enabling efficient selection of
high-quality samples for LLM pretraining. We validate BLISS by pretraining
410M/1B/2.8B Pythia and LLaMA-0.5B models on selected subsets of the C4
dataset. Notably, under the 1B model setting, BLISS achieves $1.7\times$
speedup in reaching the same performance as the state-of-the-art method,
demonstrating superior performance across multiple downstream tasks.","\vspace*{-0.05in}

\textbf{Data Selection for Language Model Training.} Early approaches to data selection primarily relied on rule-based methods as language filters for training data, employing utility functions tailored to specific datasets~\citep{conneau2019cross,raffel2020exploring,rae2021scaling,penedo2023refinedweb}. Another key category is data deduplication~\citep{lee2021deduplicating,sorscher2022beyond,penedo2023refinedweb,abbas2023semdedup,tirumala2023d4}, which eliminates redundant samples to optimize training efficiency and enhance performance on downstream tasks. A class of methods exist for performing data-quality filtering, which can select data similar to high-quality corpus of data points~\citep{brown2020language,du2022glam,gao2020pile,xie2023data}, with small perplexity~\citep{chowdhery2023palm,wenzek2019ccnet}. More recent methods leverage external pretrained LLMs to evaluate the pretraining data quality~\citep{wettig2024qurating,maini2024rephrasing}. In addition, a similar variant of data selection is domain reweighting for data mixtures~\citep{oren2019distributionally,sagawa2019distributionally,xie2023doremi,fan2023doge,albalak2023efficient,chen2024skill}, which re-scale the contribution of each domain to enhance generalization. Another recently emerged line of research leverages the tool of influence functions~\citep{hampel1974influence,cook1977detection,ling1984residuals,koh2017understanding} to evaluate the impact of individual training samples on a fixed LLM~\citep{park2023trak,engstrom2024dsdm,yu2024mates}. In contrast to these works, our work explicitly considers the long-term impact of selected data if the model is not simply fixed but trained to convergence. In addition, our method can train the model from scratch and does not need any extra information from any external pretrained models, making it a scalable and effective solution.

\vspace*{-0.05in}
\textbf{Bilevel Optimization and Data Selection.} Bilevel optimization provides a powerful framework for modeling optimization problems with a nested structure~\citep{bracken1973mathematical,dempe2002foundations}. Recent research has focused on developing efficient bilevel optimization algorithms with strong theoretical guarantees~\citep{ghadimi2018approximation,hong2023two,ji2021bilevel,kwon2023fully,dagreou2022framework,chen2023bilevel1,grazzi2022bilevel,hao2024bilevel,gong2024a}. This approach has been widely applied in various machine learning tasks, including meta-learning~\citep{finn2017model}, hyperparameter optimization~\citep{franceschi2018bilevel}, and natural language processing~\citep{somayajula2023bi,grangier2023bilevel}. For the application of data selection, bilevel optimization has been utilized for continual learning~\citep{borsos2020coresets,zhou2022probabilistic,hao2023bilevel1} and data reweighting in LLM fine-tuning~\citep{pan2024scalebio,shen2024seal}. Our work is most closely related to SEAL \citep{shen2024seal}, which focuses on selecting high-quality and safe data to fine-tune a pretrained LLM, with the goal of aligning the model with safety and ethical guidelines. However, our approach differs from SEAL in two key aspects:
(1) Problem setting. While SEAL operates in a fine-tuning context, our objective is to select data for \textbf{pretraining} an LLM \textbf{from scratch}, aiming to improve downstream performance \textbf{without relying on any external pretrained models}.
(2) Model update mechanism. SEAL utilizes the LoRA technique~\citep{hu2021lora} to update both the data selector and the LLM during fine-tuning. However, this approach is not directly applicable to our setting due to the following reasons. First, LoRA is only suitable for fine-tuning tasks but insufficient for full model pretraining. Second, their algorithm always updates the original large models directly, which is computationally expensive if all parameters are updated. In contrast, we propose a more efficient framework that introduces lightweight models (a score model and a proxy model) to guide data selection, while allowing full parameter updates within these smaller networks.
To the best of our knowledge, our proposed bilevel influence scoring method is the first to leverage bilevel optimization techniques for data selection in LLM pretraining. %Notably, our method is computationally efficient and does not require any external pretrained models. %, making it a scalable and effective solution.


\vspace*{-0.1in}","\vspace*{-0.05in}

\textbf{Data Selection for Language Model Training.} Early approaches to data selection primarily relied on rule-based methods as language filters for training data, employing utility functions tailored to specific datasets~\citep{conneau2019cross,raffel2020exploring,rae2021scaling,penedo2023refinedweb}. Another key category is data deduplication~\citep{lee2021deduplicating,sorscher2022beyond,penedo2023refinedweb,abbas2023semdedup,tirumala2023d4}, which eliminates redundant samples to optimize training efficiency and enhance performance on downstream tasks. A class of methods exist for performing data-quality filtering, which can select data similar to high-quality corpus of data points~\citep{brown2020language,du2022glam,gao2020pile,xie2023data}, with small perplexity~\citep{chowdhery2023palm,wenzek2019ccnet}. More recent methods leverage external pretrained LLMs to evaluate the pretraining data quality~\citep{wettig2024qurating,maini2024rephrasing}. In addition, a similar variant of data selection is domain reweighting for data mixtures~\citep{oren2019distributionally,sagawa2019distributionally,xie2023doremi,fan2023doge,albalak2023efficient,chen2024skill}, which re-scale the contribution of each domain to enhance generalization. Another recently emerged line of research leverages the tool of influence functions~\citep{hampel1974influence,cook1977detection,ling1984residuals,koh2017understanding} to evaluate the impact of individual training samples on a fixed LLM~\citep{park2023trak,engstrom2024dsdm,yu2024mates}. In contrast to these works, our work explicitly considers the long-term impact of selected data if the model is not simply fixed but trained to convergence. In addition, our method can train the model from scratch and does not need any extra information from any external pretrained models, making it a scalable and effective solution.

\vspace*{-0.05in}
\textbf{Bilevel Optimization and Data Selection.} Bilevel optimization provides a powerful framework for modeling optimization problems with a nested structure~\citep{bracken1973mathematical,dempe2002foundations}. Recent research has focused on developing efficient bilevel optimization algorithms with strong theoretical guarantees~\citep{ghadimi2018approximation,hong2023two,ji2021bilevel,kwon2023fully,dagreou2022framework,chen2023bilevel1,grazzi2022bilevel,hao2024bilevel,gong2024a}. This approach has been widely applied in various machine learning tasks, including meta-learning~\citep{finn2017model}, hyperparameter optimization~\citep{franceschi2018bilevel}, and natural language processing~\citep{somayajula2023bi,grangier2023bilevel}. For the application of data selection, bilevel optimization has been utilized for continual learning~\citep{borsos2020coresets,zhou2022probabilistic,hao2023bilevel1} and data reweighting in LLM fine-tuning~\citep{pan2024scalebio,shen2024seal}. Our work is most closely related to SEAL \citep{shen2024seal}, which focuses on selecting high-quality and safe data to fine-tune a pretrained LLM, with the goal of aligning the model with safety and ethical guidelines. However, our approach differs from SEAL in two key aspects:
(1) Problem setting. While SEAL operates in a fine-tuning context, our objective is to select data for \textbf{pretraining} an LLM \textbf{from scratch}, aiming to improve downstream performance \textbf{without relying on any external pretrained models}.
(2) Model update mechanism. SEAL utilizes the LoRA technique~\citep{hu2021lora} to update both the data selector and the LLM during fine-tuning. However, this approach is not directly applicable to our setting due to the following reasons. First, LoRA is only suitable for fine-tuning tasks but insufficient for full model pretraining. Second, their algorithm always updates the original large models directly, which is computationally expensive if all parameters are updated. In contrast, we propose a more efficient framework that introduces lightweight models (a score model and a proxy model) to guide data selection, while allowing full parameter updates within these smaller networks.
To the best of our knowledge, our proposed bilevel influence scoring method is the first to leverage bilevel optimization techniques for data selection in LLM pretraining. 


\vspace*{-0.1in}",N/A
