arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2510.09379v1,http://arxiv.org/abs/2510.09379v1,2025-10-10 13:35:21+00:00,Task-Level Insights from Eigenvalues across Sequence Models,"Although softmax attention drives state-of-the-art performance for sequence
models, its quadratic complexity limits scalability, motivating linear
alternatives such as state space models (SSMs). While these alternatives
improve efficiency, their fundamental differences in information processing
remain poorly understood. In this work, we leverage the recently proposed
dynamical systems framework to represent softmax, norm and linear attention as
dynamical systems, enabling a structured comparison with SSMs by analyzing
their respective eigenvalue spectra. Since eigenvalues capture essential
aspects of dynamical system behavior, we conduct an extensive empirical
analysis across diverse sequence models and benchmarks. We first show that
eigenvalues influence essential aspects of memory and long-range dependency
modeling, revealing spectral signatures that align with task requirements.
Building on these insights, we then investigate how architectural modifications
in sequence models impact both eigenvalue spectra and task performance. This
correspondence further strengthens the position of eigenvalue analysis as a
principled metric for interpreting, understanding, and ultimately improving the
capabilities of sequence models.","The challenge of modeling long-range dependencies has been a central theme in sequence modeling across recurrent, state space, and attention-based approaches. To better understand the strengths and limitations of each model class from the linear system point of view, researchers have so far employed two primary analytical tools: eigenvalue spectra and memory functions.

\paragraph{Eigenvalue-based analyses.}  Eigenvalue spectra provide a principled way to characterize stability, and the relationship between remembering and forgetting, i.e., memory retention, and information decay in dynamical systems. This perspective has a long history in recurrent neural networks (RNNs), where eigenvalue normalization was introduced to mitigate vanishing gradients while preserving controlled memory decay~\citep{helfrich2019eigenvaluenormalizedrecurrentneural}. Later analyses investigated how different eigenvalue spectra encode solutions to temporal tasks, revealing that seemingly diverse eigenvalue distributions can correspond to functionally equivalent memory behaviors~\citep{Jarne_2022}. More recently, similar ideas have been extended to SSMs, where the eigenvalues of the transition matrix are shown to govern stability and memory.  Early works on SSMs studied how careful initialization, motivated by the eigenvalue placement, can provide models with long-range dependency, enabling them to compete with RNNs and transformers on synthetic benchmarks~\citep{Gu2020,Fu2023}. Beyond initialization, subsequent works focused on structural re-parameterizations. In particular, the eigenvalues of the transition matrix have been shown to play a crucial role in governing the stability and memory length of SSMs~\citep{wang2024stablessmalleviatingcursememory,grazzi2025unlockingstatetrackinglinearrnns}. This perspective directly connects to earlier analyses of RNNs, where eigenvalue spectra determine the decay rates of information. 

\paragraph{Memory function analyses.}A complementary line of work analyzes long-term dependency through the lens of memory functions~\citep{oppenheim1997signals}, which have a strong relation to the eigenvalues of the system. These functions quantify how much influence past inputs have on current outputs, typically formalized via norms of system response functions or spectral measures. For SSMs, memory functions provide a principled way to characterize expressivity and effective memory horizons~\citep{wang2023state}. For RNNs, memory functions are used to prevent rapid decay of state memory~\citep{Su_2019}.

In attention-based architectures, however, analyses have largely focused on mechanisms for extending memory. Early work demonstrated that even simple feed-forward architectures equipped with attention can solve long-memory tasks, outperforming classical recurrent networks~\citep{raffel2016feedforwardnetworksattentionsolve}. Later, augmenting self-attention with persistent memory slots was proposed as a way to extend the memory context~\citep{sukhbaatar2019augmentingselfattentionpersistentmemory}. Other works analyze the statistical properties of the attention score matrices themselves~\citep{bao2024self,bhojanapalli2021eigen}, revealing certain regularities in how attention distributes over inputs. While insightful, these approaches remain tied to the score-matrix formulation and do not naturally translate into a framework that allows systematic comparison with SSMs. By contrast, memory functions have been successfully applied to RNNs and SSMs, but they require architecture-specific design choices and do not generalize easily to linear parameter-varying (LPV) systems, such as attention-based models and Mamba~\citep{mamba2}. The recently proposed DSF addresses this gap by recasting masked attention itself as a dynamical system, thereby making eigenvalue analysis applicable. This unifying perspective enables systematic cross-architectural comparisons of memory behavior, which is leveraged in this work to provide the first comprehensive eigenvalue-based analysis of attention mechanisms, enabling a direct comparison with SSMs. Moreover, casting all these models into the eigenvalue perspective not only unifies their study, but also makes it possible to draw on the vast body of results from linear system theory to analyze and interpret their behavior in order to guide the model design choices.","The challenge of modeling long-range dependencies has been a central theme in sequence modeling across recurrent, state space, and attention-based approaches. To better understand the strengths and limitations of each model class from the linear system point of view, researchers have so far employed two primary analytical tools: eigenvalue spectra and memory functions.

\paragraph{Eigenvalue-based analyses.}  Eigenvalue spectra provide a principled way to characterize stability, and the relationship between remembering and forgetting, i.e., memory retention, and information decay in dynamical systems. This perspective has a long history in recurrent neural networks (RNNs), where eigenvalue normalization was introduced to mitigate vanishing gradients while preserving controlled memory decay~\citep{helfrich2019eigenvaluenormalizedrecurrentneural}. Later analyses investigated how different eigenvalue spectra encode solutions to temporal tasks, revealing that seemingly diverse eigenvalue distributions can correspond to functionally equivalent memory behaviors~\citep{Jarne_2022}. More recently, similar ideas have been extended to SSMs, where the eigenvalues of the transition matrix are shown to govern stability and memory.  Early works on SSMs studied how careful initialization, motivated by the eigenvalue placement, can provide models with long-range dependency, enabling them to compete with RNNs and transformers on synthetic benchmarks~\citep{Gu2020,Fu2023}. Beyond initialization, subsequent works focused on structural re-parameterizations. In particular, the eigenvalues of the transition matrix have been shown to play a crucial role in governing the stability and memory length of SSMs~\citep{wang2024stablessmalleviatingcursememory,grazzi2025unlockingstatetrackinglinearrnns}. This perspective directly connects to earlier analyses of RNNs, where eigenvalue spectra determine the decay rates of information. 

\paragraph{Memory function analyses.}A complementary line of work analyzes long-term dependency through the lens of memory functions~\citep{oppenheim1997signals}, which have a strong relation to the eigenvalues of the system. These functions quantify how much influence past inputs have on current outputs, typically formalized via norms of system response functions or spectral measures. For SSMs, memory functions provide a principled way to characterize expressivity and effective memory horizons~\citep{wang2023state}. For RNNs, memory functions are used to prevent rapid decay of state memory~\citep{Su_2019}.

In attention-based architectures, however, analyses have largely focused on mechanisms for extending memory. Early work demonstrated that even simple feed-forward architectures equipped with attention can solve long-memory tasks, outperforming classical recurrent networks~\citep{raffel2016feedforwardnetworksattentionsolve}. Later, augmenting self-attention with persistent memory slots was proposed as a way to extend the memory context~\citep{sukhbaatar2019augmentingselfattentionpersistentmemory}. Other works analyze the statistical properties of the attention score matrices themselves~\citep{bao2024self,bhojanapalli2021eigen}, revealing certain regularities in how attention distributes over inputs. While insightful, these approaches remain tied to the score-matrix formulation and do not naturally translate into a framework that allows systematic comparison with SSMs. By contrast, memory functions have been successfully applied to RNNs and SSMs, but they require architecture-specific design choices and do not generalize easily to linear parameter-varying (LPV) systems, such as attention-based models and Mamba~\citep{mamba2}. The recently proposed DSF addresses this gap by recasting masked attention itself as a dynamical system, thereby making eigenvalue analysis applicable. This unifying perspective enables systematic cross-architectural comparisons of memory behavior, which is leveraged in this work to provide the first comprehensive eigenvalue-based analysis of attention mechanisms, enabling a direct comparison with SSMs. Moreover, casting all these models into the eigenvalue perspective not only unifies their study, but also makes it possible to draw on the vast body of results from linear system theory to analyze and interpret their behavior in order to guide the model design choices.",N/A
