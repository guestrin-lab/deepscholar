arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2510.06190v1,http://arxiv.org/abs/2510.06190v1,2025-10-07 17:49:30+00:00,"On Powerful Ways to Generate: Autoregression, Diffusion, and Beyond","This paper formally studies generation processes, including auto-regressive
next-token prediction and masked diffusion, that abstract beyond architectural
specifics. At this level of abstraction, we quantify their benefits and
limitations through measurable criteria such as computational hardness and
learnability. In particular, we demonstrate that allowing generation to proceed
beyond autoregression and current masked diffusion, with capabilities to
rewrite and length-variable edit, can bring significant theoretical and
empirical advantages, with important implications for frontier LLMs that aspire
to tackle increasingly hard problems and work universally across domains beyond
natural language, such as coding and science.","\label{appendix:related}

Masked diffusion models~\citep{hoogeboom2021argmax,austin2021structured,lou2024discrete,sahoo2024simple,shi2024simplified} extend continuous diffusion models~\citep{sohl2015deep,ho2020denoising,song2020score} to discrete data. Early work applied these models to specialized domains such as graph generation~\citep{vignac2022digress,sun2023difusco}, protein design~\citep{gruver2023protein}, and drug discovery~\citep{lee2025genmol}, where non-sequential generation provides natural advantages. The field has evolved with recent commercial-scale language models like Gemini Diffusion~\citep{deepmind2025gemini} and Mercury~\citep{inceptionlabs2025mercury}, which demonstrate competitive performance on language generation, reasoning, and coding tasks. This suggests that MDMs can serve as viable alternatives to the autoregressive models that currently dominate LLMs. Against this background, this paper investigates the fundamental computational differences between generation paradigms and explores whether more powerful generation methods exist.

Several works have explored extensions to standard MDM through mechanisms that enable rewriting and editing~\citep{vonrutte2025generalized,wang2025remasking,peng2025path,havasi2025edit,wu2025dreamon,kim2025any}, which relate to our any-process generation framework. \citet{wang2025remasking} introduces random remasking during inference, though this capability is not learned from data. \citet{lou2024discrete,vonrutte2025generalized,sahoo2025diffusion} propose adding uniform noise in the forward process rather than using masks, with models learning to revert them in the backward process, but this approach generally underperforms since modifying tokens directly appears more difficult than unmasking. \citet{peng2025path} introduces path planning to control generation, though the planner is not trained end-to-end with the base model. Current with ours: \citet{havasi2025edit} introduces edit operations to flow matching frameworks but faces similar limitations as uniform noise approaches; \citet{kim2025any} introduces to insert tokens at any 
position while \citet{wu2025dreamon} proposes expansion 
and delete, but these capabilities per se are 
insufficient for handling hard reasoning tasks as 
discussed in \Cref{sec:theory}.","Masked diffusion models~\citep{hoogeboom2021argmax,austin2021structured,lou2024discrete,sahoo2024simple,shi2024simplified} extend continuous diffusion models~\citep{sohl2015deep,ho2020denoising,song2020score} to discrete data. Early work applied these models to specialized domains such as graph generation~\citep{vignac2022digress,sun2023difusco}, protein design~\citep{gruver2023protein}, and drug discovery~\citep{lee2025genmol}, where non-sequential generation provides natural advantages. The field has evolved with recent commercial-scale language models like Gemini Diffusion~\citep{deepmind2025gemini} and Mercury~\citep{inceptionlabs2025mercury}, which demonstrate competitive performance on language generation, reasoning, and coding tasks. This suggests that MDMs can serve as viable alternatives to the autoregressive models that currently dominate LLMs. Against this background, this paper investigates the fundamental computational differences between generation paradigms and explores whether more powerful generation methods exist.

Several works have explored extensions to standard MDM through mechanisms that enable rewriting and editing~\citep{vonrutte2025generalized,wang2025remasking,peng2025path,havasi2025edit,wu2025dreamon,kim2025any}, which relate to our any-process generation framework. \citet{wang2025remasking} introduces random remasking during inference, though this capability is not learned from data. \citet{lou2024discrete,vonrutte2025generalized,sahoo2025diffusion} propose adding uniform noise in the forward process rather than using masks, with models learning to revert them in the backward process, but this approach generally underperforms since modifying tokens directly appears more difficult than unmasking. \citet{peng2025path} introduces path planning to control generation, though the planner is not trained end-to-end with the base model. Current with ours: \citet{havasi2025edit} introduces edit operations to flow matching frameworks but faces similar limitations as uniform noise approaches; \citet{kim2025any} introduces to insert tokens at any 
position while \citet{wu2025dreamon} proposes expansion 
and delete, but these capabilities per se are 
insufficient for handling hard reasoning tasks as 
discussed in \Cref{sec:theory}.",N/A
