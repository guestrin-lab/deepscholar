arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2510.03224v1,http://arxiv.org/abs/2510.03224v1,2025-10-03 17:57:25+00:00,Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,"We propose a test-time defense mechanism against adversarial attacks:
imperceptible image perturbations that significantly alter the predictions of a
model. Unlike existing methods that rely on feature filtering or smoothing,
which can lead to information loss, we propose to ""combat noise with noise"" by
leveraging stochastic resonance to enhance robustness while minimizing
information loss. Our approach introduces small translational perturbations to
the input image, aligns the transformed feature embeddings, and aggregates them
before mapping back to the original reference image. This can be expressed in a
closed-form formula, which can be deployed on diverse existing network
architectures without introducing additional network modules or fine-tuning for
specific attack types. The resulting method is entirely training-free,
architecture-agnostic, and attack-agnostic. Empirical results show
state-of-the-art robustness on image classification and, for the first time,
establish a generic test-time defense for dense prediction tasks, including
stereo matching and optical flow, highlighting the method's versatility and
practicality. Specifically, relative to clean (unperturbed) performance, our
method recovers up to 68.1% of the accuracy loss on image classification, 71.9%
on stereo matching, and 29.2% on optical flow under various types of
adversarial attacks.","\label{sec:related}

The literature on adversarial attack and defense is extensive. We highlight some of the advances. 

\noindent\textbf{Adversarial Training as Defense.}
Adversarial training increases the robustness of the model by training it with adversarially augmented images. The popular attack methods used are Fast Gradient Sign Method (FGSM) \cite{goodfellow2014explaining}, and Projected Gradient Descent (PGD) \cite{madry2017towards}. 
ALP \cite{kannan2018adversarial} Minimizes the difference between the logits of pairs of clean and adversarially augmented images. TRADES \cite{zhang2019theoretically} decomposes prediction error for adversarial examples into natural error and boundary error to improve adversarial robustness at the cost of accuracy. MART \cite{wang2019improving} improves adversarial robustness by considering misclassified natural examples during training. Subsequent work \cite{cai2018curriculum, zhang2020attacks, wang2022self, jin2022enhancing, ghiasvand2024robust} improves adversarial robustness with curriculum learning \cite{bengio2009curriculum}, model ensembling, second order statistics, and gradient tracking. 
% {\color{blue}Adversarial training on feature space}
On the other hand, some methods learn robust feature representation through a modified architecture or feature manipulation. \cite{galloway2019batch, benz2021batch, wang2022removing} investigate the effect of batch normalization on adversarial robustness. \cite{dhillon2018stochastic, madaan2020adversarial} prunes certain activations in the network that are susceptible to adversarial attacks.
\cite{xiao2019enhancing} keep k-features with the largest magnitude and deactivate everything else.
\cite{zoran2020towards} uses an attention mask to highlight robust regions on the feature.
Feature Denoising (FD) \cite{xie2019feature} uses classical denoising techniques to deactivate abnormal activations.
\cite{bai2021improving, yan2021cifs} proposed Channel Activation Suppression (CAS) and Channel-wise Importance-based Feature Selection (CIFS) to deactivate feature channels that are vulnerable to attacks. 
\cite{kim2023feature} improves the robustness with Feature Separation and Recalibration (FSR).
Our method also operates in feature space, but purely during test time. While we choose some of these works as baselines, our method works in conjunction with any aforementioned methods.

% WE ARE NOT AGAINST THEM, SR CAN BE COMBINED WITH ANY OF THEM]

\noindent\textbf{Adversarial Purification as Defense}
Another line of work focuses on purifying or augmenting the images before they are used as input. \cite{tang2024robust, yeh2024test, tsai2023test}  train an FGSM robust classifier, a diffusion model, or a mask auto-encoder, respectively, to purify adversarial examples.
\cite{wang2021fighting} optimizes both the model and the input to minimize the entropy of model predictions to adapt to changing attacks. 
\cite{cohen2024simple} trains a random forest predictor to ensemble outputs from test-time augmented images.
These works involve training a new model or updating the original model, while our method is purely test-time and does not require any training. 
\cite{perez2021enhancing} ensembles model output from different augmentations, which is a special case of our method, as the ensemble is performed solely on the output, while we can ensemble at any layer, which both saves computational cost and achieves higher performance

%, and empirical results demonstrate advantage over output ensemble.

Notably, we recognize that above methods focus solely on classification as a task for adversarial attacks. Through extensive experiments, we demonstrate that our method can not only perform well on classification, but also on dense prediction tasks like optical flow, and stereo matching.


\noindent\textbf{Stochastic Resonance (SR)} was proposed by \cite{benzi1981mechanism} and first applied in climate dynamics \citep{benzi1982stochastic} and later in signal processing \citep{wellens2003stochastic, kosko2001robust, chen2007theory} and acoustics \citep{shu2016application, wang2014adaptive}. Recently, Stochastic Resonance Transformer (SRT) \cite{lao2024sub} uses SR to ``super-resolve'' Vision Transformer (ViT) embeddings. In this work, we instead use SR to mitigate adversarial perturbations. Since SR has been developed specifically to address {\em quantization artifacts}, it has never before been used to mitigate classes of perturbations beyond aliasing. Our novel use of the technique leverages the fact that group transformations and spatial quantization preserve the statistics of natural images, which are heavy-tailed, but do not preserve the statistics of adversarial perturbations.","The literature on adversarial attack and defense is extensive. We highlight some of the advances. 

\noindent\textbf{Adversarial Training as Defense.}
Adversarial training increases the robustness of the model by training it with adversarially augmented images. The popular attack methods used are Fast Gradient Sign Method (FGSM) \cite{goodfellow2014explaining}, and Projected Gradient Descent (PGD) \cite{madry2017towards}. 
ALP \cite{kannan2018adversarial} Minimizes the difference between the logits of pairs of clean and adversarially augmented images. TRADES \cite{zhang2019theoretically} decomposes prediction error for adversarial examples into natural error and boundary error to improve adversarial robustness at the cost of accuracy. MART \cite{wang2019improving} improves adversarial robustness by considering misclassified natural examples during training. Subsequent work \cite{cai2018curriculum, zhang2020attacks, wang2022self, jin2022enhancing, ghiasvand2024robust} improves adversarial robustness with curriculum learning \cite{bengio2009curriculum}, model ensembling, second order statistics, and gradient tracking. 

On the other hand, some methods learn robust feature representation through a modified architecture or feature manipulation. \cite{galloway2019batch, benz2021batch, wang2022removing} investigate the effect of batch normalization on adversarial robustness. \cite{dhillon2018stochastic, madaan2020adversarial} prunes certain activations in the network that are susceptible to adversarial attacks.
\cite{xiao2019enhancing} keep k-features with the largest magnitude and deactivate everything else.
\cite{zoran2020towards} uses an attention mask to highlight robust regions on the feature.
Feature Denoising (FD) \cite{xie2019feature} uses classical denoising techniques to deactivate abnormal activations.
\cite{bai2021improving, yan2021cifs} proposed Channel Activation Suppression (CAS) and Channel-wise Importance-based Feature Selection (CIFS) to deactivate feature channels that are vulnerable to attacks. 
\cite{kim2023feature} improves the robustness with Feature Separation and Recalibration (FSR).
Our method also operates in feature space, but purely during test time. While we choose some of these works as baselines, our method works in conjunction with any aforementioned methods.



\noindent\textbf{Adversarial Purification as Defense}
Another line of work focuses on purifying or augmenting the images before they are used as input. \cite{tang2024robust, yeh2024test, tsai2023test}  train an FGSM robust classifier, a diffusion model, or a mask auto-encoder, respectively, to purify adversarial examples.
\cite{wang2021fighting} optimizes both the model and the input to minimize the entropy of model predictions to adapt to changing attacks. 
\cite{cohen2024simple} trains a random forest predictor to ensemble outputs from test-time augmented images.
These works involve training a new model or updating the original model, while our method is purely test-time and does not require any training. 
\cite{perez2021enhancing} ensembles model output from different augmentations, which is a special case of our method, as the ensemble is performed solely on the output, while we can ensemble at any layer, which both saves computational cost and achieves higher performance



Notably, we recognize that above methods focus solely on classification as a task for adversarial attacks. Through extensive experiments, we demonstrate that our method can not only perform well on classification, but also on dense prediction tasks like optical flow, and stereo matching.


\noindent\textbf{Stochastic Resonance (SR)} was proposed by \cite{benzi1981mechanism} and first applied in climate dynamics \citep{benzi1982stochastic} and later in signal processing \citep{wellens2003stochastic, kosko2001robust, chen2007theory} and acoustics \citep{shu2016application, wang2014adaptive}. Recently, Stochastic Resonance Transformer (SRT) \cite{lao2024sub} uses SR to ``super-resolve'' Vision Transformer (ViT) embeddings. In this work, we instead use SR to mitigate adversarial perturbations. Since SR has been developed specifically to address {\em quantization artifacts}, it has never before been used to mitigate classes of perturbations beyond aliasing. Our novel use of the technique leverages the fact that group transformations and spatial quantization preserve the statistics of natural images, which are heavy-tailed, but do not preserve the statistics of adversarial perturbations.",N/A
