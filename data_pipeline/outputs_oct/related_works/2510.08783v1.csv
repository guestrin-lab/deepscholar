arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2510.08783v1,http://arxiv.org/abs/2510.08783v1,2025-10-09 20:00:41+00:00,MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,"In an ideal design pipeline, user interface (UI) design is intertwined with
user research to validate decisions, yet studies are often resource-constrained
during early exploration. Recent advances in multimodal large language models
(MLLMs) offer a promising opportunity to act as early evaluators, helping
designers narrow options before formal testing. Unlike prior work that
emphasizes user behavior in narrow domains such as e-commerce with metrics like
clicks or conversions, we focus on subjective user evaluations across varied
interfaces. We investigate whether MLLMs can mimic human preferences when
evaluating individual UIs and comparing them. Using data from a crowdsourcing
platform, we benchmark GPT-4o, Claude, and Llama across 30 interfaces and
examine alignment with human judgments on multiple UI factors. Our results show
that MLLMs approximate human preferences on some dimensions but diverge on
others, underscoring both their potential and limitations in supplementing
early UX research.","\label{sec:related-work}


\subsection{LLM-as-a-Judge}
Extensive work has been done in the field of 'LLM as a Judge', and within this paper, we aim to build upon the existing literature. LLM as a judge is a concept that can be applied to a myriad of different domains \citep{gu2024survey} and consists of using LLMs to evaluate the outcomes of complex tasks. They can do so effectively because they can quickly and efficiently process extensive datasets that can then be used to make an informed decision \citep{achiam2023gpt}. LLMs have been used as judges to evaluate everything from other models \citep{li2023generative, wang2023large,zhang2023wider, zheng2023judging}, natural language and information \citep{wang2022self, ramamurthy2022reinforcement, ouyang2022training}, charts \citep{kim2025chart}, and reasoning and thinking \citep{achiam2023gpt, team2023gemini, guo2025deepseek}. Despite this, related literature suggests that using LLMs as judges can offer cost-cutting and a flexible alternative to human evaluation, but warns that they often lack consistency while suffering from issues pertaining to bias and reliability \citep{gu2024survey}. 

Across all studied domains, larger state-of-the art models (e.g., GPT-4o, Claude, etc.) perform better in LLM as a judge tasks than smaller models \citep{thakur2025judgingjudgesevaluatingalignment, zheng2023judging}. Nonetheless, even well-performing models deviate by up to five points on a 10-point scale, and LLM evaluators are often excessively verbose, biased towards the existing positions, and prefer their own outputs \citep{liu2024aligning, panickssery2024llmevaluatorsrecognizefavor}. Given these limitations, LLMs often are capable as approximators, but are far from being human replacements in evaluation tasks. 

We aim to distinguish our work by using a method that specifically explores how MLLMs can be used to evaluate two distinct UIs. In the context of this paper, we use a pairwise MLLM evaluation method \citep{liu2024aligning}, which consists of having the MLLM compare two different options and selecting which is better aligned with a predetermined criterion \citep{gu2024survey, qin2024largelanguagemodelseffective}. This method closely aligns with A/B testing, commonly used in user research to compare two interfaces. We aim to run a pairwise test with human users looking at UIs and a Likert-scale LLM evaluation with the same interfaces to explore how close LLMs are to simulating human results.

    \begin{figure*}[h]
    \centering
    \includegraphics[width=.9\linewidth]{Figures/DomainAllBW.png}
    \caption{%
    {\textbf{User interfaces split by domains:} 
    An overview of a sample of the UIs 
    evaluated by humans and MLLMs can be divided into three domains: landing pages, digital receipts, and catalogs. Here we present low-fidelity versions of the screens that we presented to the users. The users saw high-fidelity, branded, and in-product versions of these screens that have been anonymized.}}
    \label{fig:Domains}
    \end{figure*}


\subsection{MLLMs in User Research and Design}
As in many other domains, AI is becoming increasingly intertwined with user interface design and research \citep{bertao2021artificial, luera2024survey}. Designers and researchers alike are using MLLM to create and explore different interfaces for their respective products.
Specifically in design research, LLMs are being used as assistants that are able to collaborate during UX evaluation sessions \citep{10.1145/3613904.3642168}. LLMs are occasionally being used to create synthetic data that emulates human users \citep{cao2025survey, rosala2024synthetic, 10.1145/3701716.3715452}. For instance, \citet{DBLP:journals/nlpj/JansenJS23} explain how LLMs can use simulated respondents to supplement the creation and analysis of survey research. Further, \citet{10.1145/3544548.3580688} demonstrates that LLMs are capable of simulating user survey responses and creating responses that humans found plausible. Similarly, \citet{li2024frontiers} found that in larger-scale market research surveys, LLM-generated survey responses achieved about a 75\%-85\% agreement with results from actual humans. However, this paper also highlights the importance of these models being trained on human data, as they cannot yet capture all of the nuances of human users. 


There have been some work focused on some aspect of UI evaluations~\citep{duan2024generating, duan2024uicrit, wu2024uiclip,Schoop_2022}. In \citet{duan2024generating}, a Figma plugin was created by assessing GPT-4's alignment with a set of design heuristics. Similarly, \citet{duan2024uicrit} utilized a dataset of design critiques from several experienced designers to improve LLM-generated UI feedback by 55\%. Meanwhile \citet{wu2024uiclip} creates a design tool based on several datasets and shows it performs closely to human ground truth. In addition, \citet{10.1145/3613904.3642481} created an LLM tool used to evaluate smartwatch interfaces using simulated users.
Our work differs from these works as we create a benchmark that compares several different state-of-the-art MLLMs to each other and to the human judgments. Further, the benchmark reflects how MLLMs perform in both pairwise and absolute scoring tasks based on a list of nine UI factors.
Agent A/B \citep{wang2025agenta} asks a similar question and explores whether or not its own model can complete A/B UX tests. They train their agent specifically on UX click-through rate, focusing solely on the filters and menus of e-commerce websites. We aim to focus on a broader question, exploring UI judgments at a higher level while testing an assortment of widely used MLLMs (e.g., GPT-4o, Claude, Gemini) on an assortment of UI screens from different domains.","\subsection{LLM-as-a-Judge}
Extensive work has been done in the field of 'LLM as a Judge', and within this paper, we aim to build upon the existing literature. LLM as a judge is a concept that can be applied to a myriad of different domains \citep{gu2024survey} and consists of using LLMs to evaluate the outcomes of complex tasks. They can do so effectively because they can quickly and efficiently process extensive datasets that can then be used to make an informed decision \citep{achiam2023gpt}. LLMs have been used as judges to evaluate everything from other models \citep{li2023generative, wang2023large,zhang2023wider, zheng2023judging}, natural language and information \citep{wang2022self, ramamurthy2022reinforcement, ouyang2022training}, charts \citep{kim2025chart}, and reasoning and thinking \citep{achiam2023gpt, team2023gemini, guo2025deepseek}. Despite this, related literature suggests that using LLMs as judges can offer cost-cutting and a flexible alternative to human evaluation, but warns that they often lack consistency while suffering from issues pertaining to bias and reliability \citep{gu2024survey}. 

Across all studied domains, larger state-of-the art models (e.g., GPT-4o, Claude, etc.) perform better in LLM as a judge tasks than smaller models \citep{thakur2025judgingjudgesevaluatingalignment, zheng2023judging}. Nonetheless, even well-performing models deviate by up to five points on a 10-point scale, and LLM evaluators are often excessively verbose, biased towards the existing positions, and prefer their own outputs \citep{liu2024aligning, panickssery2024llmevaluatorsrecognizefavor}. Given these limitations, LLMs often are capable as approximators, but are far from being human replacements in evaluation tasks. 

We aim to distinguish our work by using a method that specifically explores how MLLMs can be used to evaluate two distinct UIs. In the context of this paper, we use a pairwise MLLM evaluation method \citep{liu2024aligning}, which consists of having the MLLM compare two different options and selecting which is better aligned with a predetermined criterion \citep{gu2024survey, qin2024largelanguagemodelseffective}. This method closely aligns with A/B testing, commonly used in user research to compare two interfaces. We aim to run a pairwise test with human users looking at UIs and a Likert-scale LLM evaluation with the same interfaces to explore how close LLMs are to simulating human results.

    


\subsection{MLLMs in User Research and Design}
As in many other domains, AI is becoming increasingly intertwined with user interface design and research \citep{bertao2021artificial, luera2024survey}. Designers and researchers alike are using MLLM to create and explore different interfaces for their respective products.
Specifically in design research, LLMs are being used as assistants that are able to collaborate during UX evaluation sessions \citep{10.1145/3613904.3642168}. LLMs are occasionally being used to create synthetic data that emulates human users \citep{cao2025survey, rosala2024synthetic, 10.1145/3701716.3715452}. For instance, \citet{DBLP:journals/nlpj/JansenJS23} explain how LLMs can use simulated respondents to supplement the creation and analysis of survey research. Further, \citet{10.1145/3544548.3580688} demonstrates that LLMs are capable of simulating user survey responses and creating responses that humans found plausible. Similarly, \citet{li2024frontiers} found that in larger-scale market research surveys, LLM-generated survey responses achieved about a 75\


There have been some work focused on some aspect of UI evaluations~\citep{duan2024generating, duan2024uicrit, wu2024uiclip,Schoop_2022}. In \citet{duan2024generating}, a Figma plugin was created by assessing GPT-4's alignment with a set of design heuristics. Similarly, \citet{duan2024uicrit} utilized a dataset of design critiques from several experienced designers to improve LLM-generated UI feedback by 55\
Our work differs from these works as we create a benchmark that compares several different state-of-the-art MLLMs to each other and to the human judgments. Further, the benchmark reflects how MLLMs perform in both pairwise and absolute scoring tasks based on a list of nine UI factors.
Agent A/B \citep{wang2025agenta} asks a similar question and explores whether or not its own model can complete A/B UX tests. They train their agent specifically on UX click-through rate, focusing solely on the filters and menus of e-commerce websites. We aim to focus on a broader question, exploring UI judgments at a higher level while testing an assortment of widely used MLLMs (e.g., GPT-4o, Claude, Gemini) on an assortment of UI screens from different domains.","2.1 LLM-as-a-Judge
Extensive work has been done in the field of ’LLM as a Judge’,
and within this paper, we aim to build upon the existing literature.
LLM as a judge is a concept that can be applied to a myriad of
different domains [ 16] and consists of using LLMs to evaluate the
outcomes of complex tasks. They can do so effectively because they
can quickly and efficiently process extensive datasets that can then
be used to make an informed decision [ 1]. LLMs have been used
as judges to evaluate everything from other models [ 28,55,60,61],
natural language and information [ 37,41,56], charts [ 24], and
reasoning and thinking [ 1,17,50]. Despite this, related literature
suggests that using LLMs as judges can offer cost-cutting and a
flexible alternative to human evaluation, but warns that they often
lack consistency while suffering from issues pertaining to bias and
reliability [16].
Across all studied domains, larger state-of-the art models (e.g.,
GPT-4o, Claude, etc.) perform better in LLM as a judge tasks than
smaller models [ 51,61]. Nonetheless, even well-performing models
deviate by up to five points on a 10-point scale, and LLM evaluators
are often excessively verbose, biased towards the existing positions,
and prefer their own outputs [ 30,39]. Given these limitations, LLMs
often are capable as approximators, but are far from being human
replacements in evaluation tasks.
We aim to distinguish our work by using a method that specifi-
cally explores how MLLMs can be used to evaluate two distinct UIs.
In the context of this paper, we use a pairwise MLLM evaluation
method [ 30], which consists of having the MLLM compare two
different options and selecting which is better aligned with a pre-
determined criterion [ 16,40]. This method closely aligns with A/B
testing, commonly used in user research to compare two interfaces.
We aim to run a pairwise test with human users looking at UIs and
a Likert-scale LLM evaluation with the same interfaces to explore
how close LLMs are to simulating human results.
2.2 MLLMs in User Research and Design
As in many other domains, AI is becoming increasingly intertwined
with user interface design and research [ 5,32]. Designers and re-
searchers alike are using MLLM to create and explore different
interfaces for their respective products. Specifically in design re-
search, LLMs are being used as assistants that are able to collaborate
MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces 2025, , San Jose, CA, USA
Figure 1: User interfaces split by domains: An overview of a sample of the UIs evaluated by humans and MLLMs can be divided
into three domains: landing pages, digital receipts, and catalogs. Here we present low-fidelity versions of the screens that
we presented to the users. The users saw high-fidelity, branded, and in-product versions of these screens that have been
anonymized.
during UX evaluation sessions [ 25]. LLMs are occasionally being
used to create synthetic data that emulates human users [ 10,31,43].
For instance, Jansen et al . [21] explain how LLMs can use simulated
respondents to supplement the creation and analysis of survey re-
search. Further, Hämäläinen et al . [18] demonstrates that LLMs are
capable of simulating user survey responses and creating responses
that humans found plausible. Similarly, Li et al . [29] found that inlarger-scale market research surveys, LLM-generated survey re-
sponses achieved about a 75%-85% agreement with results from
actual humans. However, this paper also highlights the importance
of these models being trained on human data, as they cannot yet
capture all of the nuances of human users.
There have been some work focused on some aspect of UI eval-
uations [ 14,15,44,58]. In Duan et al . [15] , a Figma plugin was
2025, , San Jose, CA, USA Reuben Luera, et al.
created by assessing GPT-4’s alignment with a set of design heuris-
tics. Similarly, Duan et al . [14] utilized a dataset of design critiques
from several experienced designers to improve LLM-generated UI
feedback by 55%. Meanwhile Wu et al . [58] creates a design tool
based on several datasets and shows it performs closely to human
ground truth. In addition, Xiang et al . [59] created an LLM tool used
to evaluate smartwatch interfaces using simulated users. Our work
differs from these works as we create a benchmark that compares
several different state-of-the-art MLLMs to each other and to the
human judgments. Further, the benchmark reflects how MLLMs
perform in both pairwise and absolute scoring tasks based on a
list of nine UI factors. Agent A/B [ 53] asks a similar question and
explores whether or not its own model can complete A/B UX tests.
They train their agent specifically on UX click-through rate, focus-
ing solely on the filters and menus of e-commerce websites. We
aim to focus on a broader question, exploring UI judgments at a
higher level while testing an assortment of widely used MLLMs
(e.g., GPT-4o, Claude, Gemini) on an assortment of UI screens from
different domains."
