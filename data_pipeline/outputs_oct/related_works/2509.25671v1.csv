arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2509.25671v1,http://arxiv.org/abs/2509.25671v1,2025-09-30 02:14:30+00:00,The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,"Benchmarks shape scientific conclusions about model capabilities and steer
model development. This creates a feedback loop: stronger benchmarks drive
better models, and better models demand more discriminative benchmarks.
Ensuring benchmark reliability is therefore essential for trustworthy
evaluation and meaningful progress. In this work, we study benchmark
reliability from a distributional perspective and introduce benchmark harmony,
which measures how uniformly a model's performance is distributed across the
subdomains of a benchmark. We posit that high harmony is a desirable benchmark
property, indicating that the aggregate metric reflects uniform competence
across subdomains. Across 19 multiple-choice benchmarks and five model
families, we map each benchmark onto a mean-variance plane of harmony computed
across models, where high mean and low variance signal more reliable
evaluation. Our analysis shows that less harmonious benchmarks can give
misleading results, since overall accuracy may be disproportionately influenced
by specific subdomains. For instance, ARC-Easy is overwhelmed by questions on
Biological Concepts, overshadowing other critical subdomains such as Geography,
Physics, Chemistry, and Environmental Science. By recommending that harmony
should be reported alongside accuracy, we reframe evaluation from simple
performance averages to a more robust, distributionally reliable measurement of
performance.","% \tianjian{It is also a bit too long, consider moving one paragraph to the appendix.}
% \daniel{+1. ""Efficient Eval"" should be a good candidate to move. The other two paragraphs are long as well. You need to add some structure to make it easier to read. You also need to tie them to your contribution: how does your work distinguish with or improve upon these works? }

\paragraph{Assessing Benchmark Reliability.} Beyond proposing new tasks, a growing body of work interrogates the \emph{reliability} of benchmarks themselves. A line of work targets the robustness of the test sets, focusing on building dynamic benchmarks to replace static benchmarks \citep{kiela2021dynabenchrethinkingbenchmarkingnlp, chiang2024chatbot} and building adversarial perturbations to eliminate spurious cues present in static benchmarks \citep{nie2020adversarialnlinewbenchmark, croce2021robustbenchstandardizedadversarialrobustness}. Closely related are concerns about overfitting to public test sets and contamination from pre-training corpora, which can inflate reported gains \citep{deng2024investigatingdatacontaminationmodern, golchin2024timetravelllmstracing, roberts2023datacontaminationlenstime, dong-etal-2024-generalization}. \citet{dbevalgauntlet} analyzes a collection of benchmarks, showing that some benchmarks (e.g. Hellaswag \citep{zellers2019hellaswagmachinereallyfinish}) scale smoothly with increased scale and compute, while others (e.g. CommonsenseQA \citep{talmor2018commonsenseqa}) do not. 
Another branch of literature audits data reliability and distributional coverage by introducing shifted test sets to probe generalization \citep{recht2019imagenetclassifiersgeneralizeimagenet, taori2020measuringrobustnessnaturaldistribution, teney2020valueoutofdistributiontestingexample} and correcting pervasive label errors in widely used benchmarks \citep{northcutt2021pervasivelabelerrorstest, gema2025mmlu}. Beyond individual datasets, meta-evaluation work proposes frameworks and documentation practices to systematically assess benchmark design, provenance, and intended use \citep{reuel2024betterbenchassessingaibenchmarks, mazumder2023dataperfbenchmarksdatacentricai, gebru2021datasheetsdatasets}. Another important topic is the external validity of benchmarks, such as how well leaderboard gains translate to real-world performance \citep{Ott_2022} and what reported scores actually measure \citep{dehghani2021benchmarklottery, singh2025leaderboardillusion}. Finally, a complementary line of work separates signal from noise in benchmark results by quantifying variance and prescribing protocols that stabilize rankings in order to make comparative conclusions more reliable \citep{madaan2024quantifyingvarianceevaluationbenchmarks, evalarena, heineman2025signalnoiseframeworkreducing}. Advancing this field of work, we contribute a distributional perspective on benchmark reliability. Rather than treating a benchmark evaluation as a single score, we model the benchmark as a mixture over the subdomains of the stated benchmark domain. We then measure how \emph{performance mass} is distributed across these subdomains. This perspective diagnoses whether aggregate metrics reflect a broad competence over the benchmark or are dominated by certain subdomains.


% \textbf{Efficient Evaluation.} 
\textbf{Distributional Frameworks for Efficient Evaluation.} 
Scaling laws of neural language models suggest that performance improves with model size \citep{kaplan2020scalinglawsneurallanguage}, encouraging the development of increasingly larger and costlier models. Consequently, there has been growing interest in developing efficient evaluation methods that reduce computational and financial costs without compromising reliability. \citet{perlitz-etal-2024-efficient} introduce a reliability metric that dynamically adjusts compute by performance tier while preserving rank fidelity. \citet{rodriguez-etal-2021-evaluation} propose Item Response Theory \citep{Tatsuoka1971StatisticalTO} based leaderboards that jointly model difficulty and discrimination to identify examples that best differentiate model performance. Similarly, \citet{polo2024tinybenchmarksevaluatingllmsfewer} propose tinyBenchmarks, an efficient evaluation method that uses IRT to model the discriminative power of benchmark examples, allowing the selection of a small yet representative subset of items that can accurately estimate performance. \citet{vivek2024anchorpointsbenchmarkingmodels} propose anchor point selection to identify small, representative subsets by leveraging cross-model correlations in instance-level predictions. \citet{ethayarajh2022understandingdatasetdifficultymathcalvusable} identify informative data points via \emph{usable information} (how much input a model family can exploit) extending Shannon information to account for model constraints. Notably, these works introduce distinct metrics such as IRT item parameters, cross-model instance correlations, and information-theoretic usable information to characterize the benchmark distribution and guide principled compression of benchmarks. Ultimately, these metrics enable targeted downsampling (e.g., selecting maximally discriminative or most informative items) that preserves rankings and reduces evaluation cost while maintaining coverage. In contrast, we do not seek cheaper evaluations. We instead assess whether a benchmark reliably measures its stated domain and, where it does not, we question the original evaluation rather than preserve it.

Prior work mainly (i) proposes new or dynamic tests, (ii) stabilizes leaderboards through variance control and guidelines, and (iii) compresses evaluation via discriminative selection. We instead audit \emph{existing} benchmarks through a distributional lens, modeling a benchmark as a mixture over subdomains and measuring whether models spread accuracy uniformly. Unlike efficiency work that preserves overall scores while reducing cost, \method{} reveals where aggregate metrics fail to provide a representative understanding of model competency. Our method is post hoc and lightweight, complements robustness and contamination audits, and yields practical guidance: report \method{} with accuracy and rebalance low \method{} benchmarks.

We further discuss additional related work on language model evaluation in Appendix~\ref{sec:add_rel_work}.

\vspace{-0.5em}","\paragraph{Assessing Benchmark Reliability.} Beyond proposing new tasks, a growing body of work interrogates the \emph{reliability} of benchmarks themselves. A line of work targets the robustness of the test sets, focusing on building dynamic benchmarks to replace static benchmarks \citep{kiela2021dynabenchrethinkingbenchmarkingnlp, chiang2024chatbot} and building adversarial perturbations to eliminate spurious cues present in static benchmarks \citep{nie2020adversarialnlinewbenchmark, croce2021robustbenchstandardizedadversarialrobustness}. Closely related are concerns about overfitting to public test sets and contamination from pre-training corpora, which can inflate reported gains \citep{deng2024investigatingdatacontaminationmodern, golchin2024timetravelllmstracing, roberts2023datacontaminationlenstime, dong-etal-2024-generalization}. \citet{dbevalgauntlet} analyzes a collection of benchmarks, showing that some benchmarks (e.g. Hellaswag \citep{zellers2019hellaswagmachinereallyfinish}) scale smoothly with increased scale and compute, while others (e.g. CommonsenseQA \citep{talmor2018commonsenseqa}) do not. 
Another branch of literature audits data reliability and distributional coverage by introducing shifted test sets to probe generalization \citep{recht2019imagenetclassifiersgeneralizeimagenet, taori2020measuringrobustnessnaturaldistribution, teney2020valueoutofdistributiontestingexample} and correcting pervasive label errors in widely used benchmarks \citep{northcutt2021pervasivelabelerrorstest, gema2025mmlu}. Beyond individual datasets, meta-evaluation work proposes frameworks and documentation practices to systematically assess benchmark design, provenance, and intended use \citep{reuel2024betterbenchassessingaibenchmarks, mazumder2023dataperfbenchmarksdatacentricai, gebru2021datasheetsdatasets}. Another important topic is the external validity of benchmarks, such as how well leaderboard gains translate to real-world performance \citep{Ott_2022} and what reported scores actually measure \citep{dehghani2021benchmarklottery, singh2025leaderboardillusion}. Finally, a complementary line of work separates signal from noise in benchmark results by quantifying variance and prescribing protocols that stabilize rankings in order to make comparative conclusions more reliable \citep{madaan2024quantifyingvarianceevaluationbenchmarks, evalarena, heineman2025signalnoiseframeworkreducing}. Advancing this field of work, we contribute a distributional perspective on benchmark reliability. Rather than treating a benchmark evaluation as a single score, we model the benchmark as a mixture over the subdomains of the stated benchmark domain. We then measure how \emph{performance mass} is distributed across these subdomains. This perspective diagnoses whether aggregate metrics reflect a broad competence over the benchmark or are dominated by certain subdomains.



\textbf{Distributional Frameworks for Efficient Evaluation.} 
Scaling laws of neural language models suggest that performance improves with model size \citep{kaplan2020scalinglawsneurallanguage}, encouraging the development of increasingly larger and costlier models. Consequently, there has been growing interest in developing efficient evaluation methods that reduce computational and financial costs without compromising reliability. \citet{perlitz-etal-2024-efficient} introduce a reliability metric that dynamically adjusts compute by performance tier while preserving rank fidelity. \citet{rodriguez-etal-2021-evaluation} propose Item Response Theory \citep{Tatsuoka1971StatisticalTO} based leaderboards that jointly model difficulty and discrimination to identify examples that best differentiate model performance. Similarly, \citet{polo2024tinybenchmarksevaluatingllmsfewer} propose tinyBenchmarks, an efficient evaluation method that uses IRT to model the discriminative power of benchmark examples, allowing the selection of a small yet representative subset of items that can accurately estimate performance. \citet{vivek2024anchorpointsbenchmarkingmodels} propose anchor point selection to identify small, representative subsets by leveraging cross-model correlations in instance-level predictions. \citet{ethayarajh2022understandingdatasetdifficultymathcalvusable} identify informative data points via \emph{usable information} (how much input a model family can exploit) extending Shannon information to account for model constraints. Notably, these works introduce distinct metrics such as IRT item parameters, cross-model instance correlations, and information-theoretic usable information to characterize the benchmark distribution and guide principled compression of benchmarks. Ultimately, these metrics enable targeted downsampling (e.g., selecting maximally discriminative or most informative items) that preserves rankings and reduces evaluation cost while maintaining coverage. In contrast, we do not seek cheaper evaluations. We instead assess whether a benchmark reliably measures its stated domain and, where it does not, we question the original evaluation rather than preserve it.

Prior work mainly (i) proposes new or dynamic tests, (ii) stabilizes leaderboards through variance control and guidelines, and (iii) compresses evaluation via discriminative selection. We instead audit \emph{existing} benchmarks through a distributional lens, modeling a benchmark as a mixture over subdomains and measuring whether models spread accuracy uniformly. Unlike efficiency work that preserves overall scores while reducing cost, \method{} reveals where aggregate metrics fail to provide a representative understanding of model competency. Our method is post hoc and lightweight, complements robustness and contamination audits, and yields practical guidance: report \method{} with accuracy and rebalance low \method{} benchmarks.

We further discuss additional related work on language model evaluation in Appendix~\ref{sec:add_rel_work}.

\vspace{-0.5em}",N/A
