arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2510.06871v2,http://arxiv.org/abs/2510.06871v2,2025-10-08 10:39:12+00:00,SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,"Multimodal Large Reasoning Models (MLRMs) demonstrate impressive cross-modal
reasoning but often amplify safety risks under adversarial or unsafe prompts, a
phenomenon we call the \textit{Reasoning Tax}. Existing defenses mainly act at
the output level and do not constrain the reasoning process, leaving models
exposed to implicit risks. In this paper, we propose SaFeR-VLM, a
safety-aligned reinforcement learning framework that embeds safety directly
into multimodal reasoning. The framework integrates four components: (I)
QI-Safe-10K, a curated dataset emphasizing safety-critical and
reasoning-sensitive cases; (II) safety-aware rollout, where unsafe generations
undergo reflection and correction instead of being discarded; (III) structured
reward modeling with multi-dimensional weighted criteria and explicit penalties
for hallucinations and contradictions; and (IV) GRPO optimization, which
reinforces both safe and corrected trajectories. This unified design shifts
safety from a passive safeguard to an active driver of reasoning, enabling
scalable and generalizable safety-aware reasoning. SaFeR-VLM further
demonstrates robustness against both explicit and implicit risks, supporting
dynamic and interpretable safety decisions beyond surface-level filtering.
SaFeR-VLM-3B achieves average performance $70.13$ and $78.97$ on safety and
helpfulness across six benchmarks, surpassing both same-scale and $>10\times$
larger models such as Skywork-R1V3-38B, Qwen2.5VL-72B, and GLM4.5V-106B.
Remarkably, SaFeR-VLM-7B benefits from its increased scale to surpass
GPT-5-mini and Gemini-2.5-Flash by \num{6.47} and \num{16.76} points
respectively on safety metrics, achieving this improvement without any
degradation in helpfulness performance. Our codes are available at
https://github.com/HarveyYi/SaFeR-VLM.","\vspace{-0.4em}
\paragraph{Multimodal Large Reasoning Models (MLRM).} 
MLRMs extend MLLMs~\cite{MLLM-flamingo, MLLM-gpt4o, MLLM-cogvlm, MLLM-qwen2vl, MLLM-qwen2.5vl} by enhancing multimodal reasoning capabilities for complex decision-making tasks. Recent advances, inspired by \llmname{OpenAI's O1}~\cite{MLRM-o1} and \llmname{DeepSeek-R1}~\cite{MLRM-r1}, have integrated reinforcement learning methods like GRPO~\cite{MLRM-grpo} to improve generalization beyond supervised fine-tuning, achieving success in mathematical reasoning~\cite{MLRM-math-lmmr1, MLRM-math-ursa}, spatial understanding~\cite{MLRM-spatial-star}, and visual perception~\cite{MLRM-per-perception, MLRM-per-visionr1, MLRM-per-vrft}. Furthermore, multimodal CoT reasoning~\cite{MLRM-mcot-chain, MLRM-mcot-grit, MLRM-mcot-rex, MLRM-mcot-deepeyes} and self-reflection mechanisms~\cite{MLRM-reflect-mulberry, MLRM-reflect-r3, MLRM-reflect-srpo, MLRM-reflect-vl-rethinker} enable models to integrate visual feedback and revise erroneous reasoning paths. Robustness is additionally enhanced by data augmentation methods~\cite{MLRM-aug-visionmatters, MLRM-aug-shareVL, MLRM-aug-thinknot}, while diverse reward strategies~\cite{MLRM-reward-got-r1, MLRM-reward-pixel, MLRM-reward-sophiavl} improve efficiency and control reasoning quality. Despite these advances, the safety of MLRMs remains underexplored. We introduce \method{}, which embeds reflection and correction~\cite{kumar2024training, safe-safemlrm} into the reasoning process, ensuring that safety shapes reasoning dynamics rather than only outcomes.





\vspace{-0.4em}
\paragraph{Safety of MLLMs.}
Multimodal large language models (MLLMs) have enabled advanced multimodal reasoning but also raise critical safety risks, including adversarial manipulation~\cite{safe-risk-safety, safe-guard-eta, safe-risk-figstep}, harmful content generation~\cite{safe-risk-mllmguard, mmsafetybench, safe-risk-usb}, and representational biases~\cite{safe-risk-aialign, safe-risk-red}. Addressing these challenges requires both \textbf{training-based alignment} and \textbf{inference-time defenses}.  
\textbf{Training-based alignment} incorporates safety during model development, typically guided by the Helpful, Honest, and Harmless principle~\cite{safe-3h}. Representative techniques include supervised fine-tuning with safety-oriented datasets~\cite{vlguard, safe-algin-think}, reinforcement learning from human feedback~\cite{safe-algin-saferlhfv, safe-algin-G-RLHF-V}, and direct preference optimization~\cite{safe-algin-ADPO, safe-algin-SafeVid}. Recent studies further explore generative reward modeling and safe reasoning distillation~\cite{safe-guard-vlmguard} to guide corrective behaviors.  
\textbf{Inference-time defenses} regulate model behavior during deployment without modifying parameters. These include prompt rewriting~\cite{safe-guard-rapguard, safe-guard-vlmguard}, adaptive defense prompting~\cite{safe-guard-adashield}, harm detection modules~\cite{safe-guard-mllmprotector, safe-guard-guardreasoner}, and controlled decoding~\cite{safe-guard-coca, safe-guard-immune}, which mitigate risks while preserving utility.  
However, most existing methods remain outcome-level, constraining outputs without addressing the reasoning dynamics. This gap underscores the need to embed \textbf{safety-aware reasoning} directly into the model’s thought process, making safety an intrinsic driver of reasoning rather than a superficial safeguard.","\vspace{-0.4em}
\paragraph{Multimodal Large Reasoning Models (MLRM).} 
MLRMs extend MLLMs~\cite{MLLM-flamingo, MLLM-gpt4o, MLLM-cogvlm, MLLM-qwen2vl, MLLM-qwen2.5vl} by enhancing multimodal reasoning capabilities for complex decision-making tasks. Recent advances, inspired by \llmname{OpenAI's O1}~\cite{MLRM-o1} and \llmname{DeepSeek-R1}~\cite{MLRM-r1}, have integrated reinforcement learning methods like GRPO~\cite{MLRM-grpo} to improve generalization beyond supervised fine-tuning, achieving success in mathematical reasoning~\cite{MLRM-math-lmmr1, MLRM-math-ursa}, spatial understanding~\cite{MLRM-spatial-star}, and visual perception~\cite{MLRM-per-perception, MLRM-per-visionr1, MLRM-per-vrft}. Furthermore, multimodal CoT reasoning~\cite{MLRM-mcot-chain, MLRM-mcot-grit, MLRM-mcot-rex, MLRM-mcot-deepeyes} and self-reflection mechanisms~\cite{MLRM-reflect-mulberry, MLRM-reflect-r3, MLRM-reflect-srpo, MLRM-reflect-vl-rethinker} enable models to integrate visual feedback and revise erroneous reasoning paths. Robustness is additionally enhanced by data augmentation methods~\cite{MLRM-aug-visionmatters, MLRM-aug-shareVL, MLRM-aug-thinknot}, while diverse reward strategies~\cite{MLRM-reward-got-r1, MLRM-reward-pixel, MLRM-reward-sophiavl} improve efficiency and control reasoning quality. Despite these advances, the safety of MLRMs remains underexplored. We introduce \method{}, which embeds reflection and correction~\cite{kumar2024training, safe-safemlrm} into the reasoning process, ensuring that safety shapes reasoning dynamics rather than only outcomes.





\vspace{-0.4em}
\paragraph{Safety of MLLMs.}
Multimodal large language models (MLLMs) have enabled advanced multimodal reasoning but also raise critical safety risks, including adversarial manipulation~\cite{safe-risk-safety, safe-guard-eta, safe-risk-figstep}, harmful content generation~\cite{safe-risk-mllmguard, mmsafetybench, safe-risk-usb}, and representational biases~\cite{safe-risk-aialign, safe-risk-red}. Addressing these challenges requires both \textbf{training-based alignment} and \textbf{inference-time defenses}.  
\textbf{Training-based alignment} incorporates safety during model development, typically guided by the Helpful, Honest, and Harmless principle~\cite{safe-3h}. Representative techniques include supervised fine-tuning with safety-oriented datasets~\cite{vlguard, safe-algin-think}, reinforcement learning from human feedback~\cite{safe-algin-saferlhfv, safe-algin-G-RLHF-V}, and direct preference optimization~\cite{safe-algin-ADPO, safe-algin-SafeVid}. Recent studies further explore generative reward modeling and safe reasoning distillation~\cite{safe-guard-vlmguard} to guide corrective behaviors.  
\textbf{Inference-time defenses} regulate model behavior during deployment without modifying parameters. These include prompt rewriting~\cite{safe-guard-rapguard, safe-guard-vlmguard}, adaptive defense prompting~\cite{safe-guard-adashield}, harm detection modules~\cite{safe-guard-mllmprotector, safe-guard-guardreasoner}, and controlled decoding~\cite{safe-guard-coca, safe-guard-immune}, which mitigate risks while preserving utility.  
However, most existing methods remain outcome-level, constraining outputs without addressing the reasoning dynamics. This gap underscores the need to embed \textbf{safety-aware reasoning} directly into the model’s thought process, making safety an intrinsic driver of reasoning rather than a superficial safeguard.",N/A
