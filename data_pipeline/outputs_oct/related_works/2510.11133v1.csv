arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2510.11133v1,http://arxiv.org/abs/2510.11133v1,2025-10-13 08:22:38+00:00,Test-Time Adaptation by Causal Trimming,"Test-time adaptation aims to improve model robustness under distribution
shifts by adapting models with access to unlabeled target samples. A primary
cause of performance degradation under such shifts is the model's reliance on
features that lack a direct causal relationship with the prediction target. We
introduce Test-time Adaptation by Causal Trimming (TACT), a method that
identifies and removes non-causal components from representations for test
distributions. TACT applies data augmentations that preserve causal features
while varying non-causal ones. By analyzing the changes in the representations
using Principal Component Analysis, TACT identifies the highest variance
directions associated with non-causal features. It trims the representations by
removing their projections on the identified directions, and uses the trimmed
representations for the predictions. During adaptation, TACT continuously
tracks and refines these directions to get a better estimate of non-causal
features. We theoretically analyze the effectiveness of this approach and
empirically validate TACT on real-world out-of-distribution benchmarks. TACT
consistently outperforms state-of-the-art methods by a significant margin.","\label{sec:related-work}

Existing TTA methods can be broadly categorized into backpropagation-free and backpropagation-based methods. 
Backpropagation-free methods modify model outputs or intermediate representations without gradient-based optimization. These include 
modifiable prompts \cite{foa}, re-normalized representations \cite{bn_adapt}, updated prototypes \cite{t3a, adanpc}, and maximum likelihood estimation \cite{lame}. 
Backpropagation-based methods
update the model with the gradient of objective functions such as entropy minimization \cite{sotta, eata, sar, tent} and self-training with pseudo-labels \cite{pasle, crkd, program, cotta}.
Entropy Minimization encourages more confident predictions by reducing the entropy of model predictions during adaptation.
Self-training 
employs cross entropy \cite{adacontrast, pasle, shot, program} and knowledge distillation \cite{crkd, cotta, tsd} using  model predictions as pseudo-labels.
Regularization measures such as information maximization \cite{shot}, representation statistics alignment \cite{cafa, AdaRealign}, and consistency regularization \cite{tipi, memo} for invariant prediction under augmentations have been proposed to regularize the adaptation.

A key challenge in test-time adaptation is obtaining reliable pseudo-labels to guide model updates. 
One line of work assumes that correct predictions tend to exhibit low entropy, and update the model 
using only high-confidence samples with low-entropy predictions \cite{t3a, eata, sar, adanpc}.
However, DeYO \cite{deyo} shows that spurious correlations can also result in low entropy predictions and proposes a causal intervention technique to identify predictions that are more likely based on causal features, using them selectively for model updates.
Another line of work  refines pseudo-labels by incorporating updated prototype and neighborhood information \cite{adacontrast, pasle, tast, program, tsd}. 
 AdaContrast \cite{adacontrast} uses soft voting among nearest neighbors. 
 TSD \cite{tsd} relies on updated prototypes and spatial local clustering. 
 TAST \cite{tast} employs neighbourhood information in self-training.
 PROGRAM \cite{program}
 considers both prototype and neighbour-based pseudo-labels to enhance label quality.  
 PASLE \cite{pasle} progressively refines the pseudo-labels of uncertain predictions using updated prototypes. 
 
All the above methods, except for DeYO, do not consider the effect of non-causal features on model prediction.  Although DeYO finds that non-causal features would make entropy an unreliable metric to reflect prediction correctness, it does not adjust model predictions. TACT adjusts model predictions by reducing non-causal features, and our adjusted prediction can be used as a more reliable pseudo-label.","Existing TTA methods can be broadly categorized into backpropagation-free and backpropagation-based methods. 
Backpropagation-free methods modify model outputs or intermediate representations without gradient-based optimization. These include 
modifiable prompts \cite{foa}, re-normalized representations \cite{bn_adapt}, updated prototypes \cite{t3a, adanpc}, and maximum likelihood estimation \cite{lame}. 
Backpropagation-based methods
update the model with the gradient of objective functions such as entropy minimization \cite{sotta, eata, sar, tent} and self-training with pseudo-labels \cite{pasle, crkd, program, cotta}.
Entropy Minimization encourages more confident predictions by reducing the entropy of model predictions during adaptation.
Self-training 
employs cross entropy \cite{adacontrast, pasle, shot, program} and knowledge distillation \cite{crkd, cotta, tsd} using  model predictions as pseudo-labels.
Regularization measures such as information maximization \cite{shot}, representation statistics alignment \cite{cafa, AdaRealign}, and consistency regularization \cite{tipi, memo} for invariant prediction under augmentations have been proposed to regularize the adaptation.

A key challenge in test-time adaptation is obtaining reliable pseudo-labels to guide model updates. 
One line of work assumes that correct predictions tend to exhibit low entropy, and update the model 
using only high-confidence samples with low-entropy predictions \cite{t3a, eata, sar, adanpc}.
However, DeYO \cite{deyo} shows that spurious correlations can also result in low entropy predictions and proposes a causal intervention technique to identify predictions that are more likely based on causal features, using them selectively for model updates.
Another line of work  refines pseudo-labels by incorporating updated prototype and neighborhood information \cite{adacontrast, pasle, tast, program, tsd}. 
 AdaContrast \cite{adacontrast} uses soft voting among nearest neighbors. 
 TSD \cite{tsd} relies on updated prototypes and spatial local clustering. 
 TAST \cite{tast} employs neighbourhood information in self-training.
 PROGRAM \cite{program}
 considers both prototype and neighbour-based pseudo-labels to enhance label quality.  
 PASLE \cite{pasle} progressively refines the pseudo-labels of uncertain predictions using updated prototypes. 
 
All the above methods, except for DeYO, do not consider the effect of non-causal features on model prediction.  Although DeYO finds that non-causal features would make entropy an unreliable metric to reflect prediction correctness, it does not adjust model predictions. TACT adjusts model predictions by reducing non-causal features, and our adjusted prediction can be used as a more reliable pseudo-label.","Existing TTA methods can be broadly categorized into backpropagation-free and backpropagation-
based methods. Backpropagation-free methods modify model outputs or intermediate representations
without gradient-based optimization. These include modifiable prompts [ 36], re-normalized represen-
tations [ 44], updated prototypes [ 19,57], and maximum likelihood estimation [ 4]. Backpropagation-
based methods update the model with the gradient of objective functions such as entropy minimization
[12,37,38,51] and self-training with pseudo-labels [ 17,25,47,52]. Entropy Minimization encour-
ages more confident predictions by reducing the entropy of model predictions during adaptation.
Self-training employs cross entropy [ 5,17,30,47] and knowledge distillation [ 25,52,53] using
model predictions as pseudo-labels. Regularization measures such as information maximization
[30], representation statistics alignment [ 23,58], and consistency regularization [ 35,56] for invariant
prediction under augmentations have been proposed to regularize the adaptation.
A key challenge in test-time adaptation is obtaining reliable pseudo-labels to guide model updates.
One line of work assumes that correct predictions tend to exhibit low entropy, and update the model
using only high-confidence samples with low-entropy predictions [ 19,37,38,57]. However, DeYO
[29] shows that spurious correlations can also result in low entropy predictions and proposes a causal
intervention technique to identify predictions that are more likely based on causal features, using
them selectively for model updates. Another line of work refines pseudo-labels by incorporating
updated prototype and neighborhood information [ 5,17,21,47,53]. AdaContrast [ 5] uses soft voting
among nearest neighbors. TSD [ 53] relies on updated prototypes and spatial local clustering. TAST
[21] employs neighbourhood information in self-training. PROGRAM [ 47] considers both prototype
2
and neighbour-based pseudo-labels to enhance label quality. PASLE [ 17] progressively refines the
pseudo-labels of uncertain predictions using updated prototypes.
All the above methods, except for DeYO, do not consider the effect of non-causal features on model
prediction. Although DeYO finds that non-causal features would make entropy an unreliable metric to
reflect prediction correctness, it does not adjust model predictions. TACT adjusts model predictions by
reducing non-causal features, and our adjusted prediction can be used as a more reliable pseudo-label."
