arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2510.10910v1,http://arxiv.org/abs/2510.10910v1,2025-10-13 02:11:57+00:00,SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model,"With the rapid development of diffusion models, style transfer has made
remarkable progress. However, flexible and localized style editing for scene
text remains an unsolved challenge. Although existing scene text editing
methods have achieved text region editing, they are typically limited to
content replacement and simple styles, which lack the ability of free-style
transfer. In this paper, we introduce SceneTextStylizer, a novel training-free
diffusion-based framework for flexible and high-fidelity style transfer of text
in scene images. Unlike prior approaches that either perform global style
transfer or focus solely on textual content modification, our method enables
prompt-guided style transformation specifically for text regions, while
preserving both text readability and stylistic consistency. To achieve this, we
design a feature injection module that leverages diffusion model inversion and
self-attention to transfer style features effectively. Additionally, a region
control mechanism is introduced by applying a distance-based changing mask at
each denoising step, enabling precise spatial control. To further enhance
visual quality, we incorporate a style enhancement module based on the Fourier
transform to reinforce stylistic richness. Extensive experiments demonstrate
that our method achieves superior performance in scene text style
transformation, outperforming existing state-of-the-art methods in both visual
fidelity and text preservation.","\subsection{Arbitrary Style Transfer}
Early neural style transfer methods focused on applying style from a reference image to a content image. Neural image style transfer~\cite{gatys2016image} was the first neural style transfer method that utilizes pre-trained neural networks to achieve style transfer based on style images. AdaIN~\cite{huang2017arbitrary} enabled arbitrary style transfer by aligning the mean and variance of content images and style images.
More recently, models such as StyleGAN~\cite{karras2019style} and StyTr2~\cite{deng2022stytr2} have explored style generation using GAN~\cite{goodfellow2014generative} and Transformer~\cite{vaswani2017attention} models. These methods require style images and focus on holistic stylization.
To overcome the reliance on style exemplars, recent approaches such as CLIPstyler~\cite{kwon2022clipstyler} have leveraged vision-language models, CLIP~\cite{radford2021learning}, to enable prompt-guided style transfer. 

With the emergence of diffusion models, a new text-guided image synthesis has emerged. Many style transformation methods based on the diffusion model have achieved high-quality results.
StyleDiffusion~\cite{wang2023stylediffusion} proposed a new content-style decoupling framework and introduced a CLIP-based style decoupling loss, which realizes interpretable and controllable style transformations by explicitly extracting content information and implicitly learning supplementary style information.
InST~\cite{zhang2023inversion} proposed an inversion-based style transformation method, in which style pictures are regarded as learnable text descriptions, and style transformation is realized through the attention layer of the diffusion model.
Yang et al.~\cite{yang2023zero} achieved style transformation without the need for fine-tuning and auxiliary networks by comparing the loss of the samples generated by the pre-trained diffusion network with the patches of the original images.
Chung et al.~\cite{chung2024style} also achieved style transformation without training by replacing the keys and values of the self-attention layer of the content image with the corresponding parts of the style image during the generation process.
Diffstyler~\cite{huang2024diffstyler} designed a dual diffusion model structure that utilized text embedding to control the generation of content and style.

While these approaches achieve impressive results in whole-image stylization, they are not designed for region-specific editing, such as selectively transforming only text regions. While also inversion-based, our method targets scene text stylization with precise regional control and zero training.

\subsection{Scene Text Editing}
Scene Text Editing (STE) aims to modify the textual regions of an image while preserving the rest of the scene. Traditional methods~\cite{roy2020stefann, wu2019editing, yang2020swaptext, luo2022siman} usually divide the task into background generation, text style generation, and reintegration modules that have a complicated network structure.
Subsequent works utilize GAN to improve editing fidelity like TextStyleBrush~\cite{krishnan2023textstylebrush} and Mostel~\cite{qu2023exploring}.
Recently, several diffusion-based methods such as DiffSTE~\cite{ji2023improving}, DiffUTE~\cite{chen2024diffute}, GlyphDraw~\cite{ma2023glyphdraw}, GlyphControl~\cite{yang2024glyphcontrol}, TextDiffuser~\cite{chen2024textdiffuser} significantly advanced in scene text generation and editing. However, many of these models still exhibit style inconsistencies.
To address this, TextCtrl~\cite{zeng2024textctrl} incorporates stylistic-structural guidance into the model design as well as the integration of a Glyph-adaptive Mutual Self-attention mechanism, which improves the stylistic consistency of the text.
DARLING~\cite{zhang2024choose} improved multitasking performance for text recognition, removal, and editing by decoupling content and style features and the Multi-task Decoder.
GlyphMastero~\cite{wang2025glyphmastero} targets editing tasks with complex characters, such as Chinese, by combining local character-level features and global text-line structures.
RS-STE~\cite{fang2025recognition} integration of text recognition and editing tasks, eliminating the complexity of modeling a design with a clear separation of background style and text content, enhancing the generation ability in real-world scenarios.

In contrast to the above methods, which either focus on content modification or make limited style changes based on in-image features, our approach targets prompt-guided, free-form style transformation of scene text without altering its content. This enables flexible and diverse stylization beyond the constraints of existing font or color attributes.","\subsection{Arbitrary Style Transfer}
Early neural style transfer methods focused on applying style from a reference image to a content image. Neural image style transfer~\cite{gatys2016image} was the first neural style transfer method that utilizes pre-trained neural networks to achieve style transfer based on style images. AdaIN~\cite{huang2017arbitrary} enabled arbitrary style transfer by aligning the mean and variance of content images and style images.
More recently, models such as StyleGAN~\cite{karras2019style} and StyTr2~\cite{deng2022stytr2} have explored style generation using GAN~\cite{goodfellow2014generative} and Transformer~\cite{vaswani2017attention} models. These methods require style images and focus on holistic stylization.
To overcome the reliance on style exemplars, recent approaches such as CLIPstyler~\cite{kwon2022clipstyler} have leveraged vision-language models, CLIP~\cite{radford2021learning}, to enable prompt-guided style transfer. 

With the emergence of diffusion models, a new text-guided image synthesis has emerged. Many style transformation methods based on the diffusion model have achieved high-quality results.
StyleDiffusion~\cite{wang2023stylediffusion} proposed a new content-style decoupling framework and introduced a CLIP-based style decoupling loss, which realizes interpretable and controllable style transformations by explicitly extracting content information and implicitly learning supplementary style information.
InST~\cite{zhang2023inversion} proposed an inversion-based style transformation method, in which style pictures are regarded as learnable text descriptions, and style transformation is realized through the attention layer of the diffusion model.
Yang et al.~\cite{yang2023zero} achieved style transformation without the need for fine-tuning and auxiliary networks by comparing the loss of the samples generated by the pre-trained diffusion network with the patches of the original images.
Chung et al.~\cite{chung2024style} also achieved style transformation without training by replacing the keys and values of the self-attention layer of the content image with the corresponding parts of the style image during the generation process.
Diffstyler~\cite{huang2024diffstyler} designed a dual diffusion model structure that utilized text embedding to control the generation of content and style.

While these approaches achieve impressive results in whole-image stylization, they are not designed for region-specific editing, such as selectively transforming only text regions. While also inversion-based, our method targets scene text stylization with precise regional control and zero training.

\subsection{Scene Text Editing}
Scene Text Editing (STE) aims to modify the textual regions of an image while preserving the rest of the scene. Traditional methods~\cite{roy2020stefann, wu2019editing, yang2020swaptext, luo2022siman} usually divide the task into background generation, text style generation, and reintegration modules that have a complicated network structure.
Subsequent works utilize GAN to improve editing fidelity like TextStyleBrush~\cite{krishnan2023textstylebrush} and Mostel~\cite{qu2023exploring}.
Recently, several diffusion-based methods such as DiffSTE~\cite{ji2023improving}, DiffUTE~\cite{chen2024diffute}, GlyphDraw~\cite{ma2023glyphdraw}, GlyphControl~\cite{yang2024glyphcontrol}, TextDiffuser~\cite{chen2024textdiffuser} significantly advanced in scene text generation and editing. However, many of these models still exhibit style inconsistencies.
To address this, TextCtrl~\cite{zeng2024textctrl} incorporates stylistic-structural guidance into the model design as well as the integration of a Glyph-adaptive Mutual Self-attention mechanism, which improves the stylistic consistency of the text.
DARLING~\cite{zhang2024choose} improved multitasking performance for text recognition, removal, and editing by decoupling content and style features and the Multi-task Decoder.
GlyphMastero~\cite{wang2025glyphmastero} targets editing tasks with complex characters, such as Chinese, by combining local character-level features and global text-line structures.
RS-STE~\cite{fang2025recognition} integration of text recognition and editing tasks, eliminating the complexity of modeling a design with a clear separation of background style and text content, enhancing the generation ability in real-world scenarios.

In contrast to the above methods, which either focus on content modification or make limited style changes based on in-image features, our approach targets prompt-guided, free-form style transformation of scene text without altering its content. This enables flexible and diverse stylization beyond the constraints of existing font or color attributes.",N/A
