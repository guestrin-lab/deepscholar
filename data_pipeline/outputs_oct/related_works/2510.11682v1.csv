arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2510.11682v1,http://arxiv.org/abs/2510.11682v1,2025-10-13 17:47:39+00:00,Ego-Vision World Model for Humanoid Contact Planning,"Enabling humanoid robots to exploit physical contact, rather than simply
avoid collisions, is crucial for autonomy in unstructured environments.
Traditional optimization-based planners struggle with contact complexity, while
on-policy reinforcement learning (RL) is sample-inefficient and has limited
multi-task ability. We propose a framework combining a learned world model with
sampling-based Model Predictive Control (MPC), trained on a demonstration-free
offline dataset to predict future outcomes in a compressed latent space. To
address sparse contact rewards and sensor noise, the MPC uses a learned
surrogate value function for dense, robust planning. Our single, scalable model
supports contact-aware tasks, including wall support after perturbation,
blocking incoming objects, and traversing height-limited arches, with improved
data efficiency and multi-task capability over on-policy RL. Deployed on a
physical humanoid, our system achieves robust, real-time contact planning from
proprioception and ego-centric depth images. Website:
https://ego-vcp.github.io/","\subsection{Model-Based Contact Planning}

For both locomotion and manipulation, robotics is replete with contact-rich problems, made challenging by the non-smooth dynamics induced by the impact ~\cite{raibert1986legged}. Optimization-based approaches address this by explicitly modeling these physical interactions, like linearizing the complex friction model into a Linear Complementarity Problem (LCP)~\cite{stewart1996implicit}, or relaxing it into a Cone Complementarity Problem (CCP)~\cite{anitescu2006optimization}. These formulations can then be embedded within a trajectory optimization framework~\cite{sleiman2021unified,winkler2018gait}. Another prominent paradigm is Hybrid Zero Dynamics (HZD)~\cite{westervelt2003hybrid, sreenath2011compliant}, which addresses the non-smooth contact dynamics of legged locomotion by enforcing virtual constraints whose associated zero dynamics surface remains invariant through impacts. However, such model-based approaches are often hindered by model inaccuracies and high computational costs~\cite{xue2024full}, which complicate real-time deployment. Furthermore, their reliance on predefined structures, such as periodic gaits~\cite{gong2019feedback} or reference foot-end trajectories~\cite{westervelt2003hybrid}, makes it difficult to scale them to more general, aperiodic whole-body contact scenarios.


\subsection{Learning-Based Contact
Planning}

Learning-based approaches for real-world contact planning have shown remarkable potential, enabling dynamic skills~\cite{li2025reinforcement, cheng2024extreme, jenelten2024dtc, liu2025discrete,margolis2024rapid, margolis2023walk}. However, three significant challenges remain largely unaddressed. First, interaction with both dynamic and static objects is limited. Most existing work on legged locomotion based on simplified 2.5D elevation maps~\cite{roth2025learned}, which cannot represent dynamic or overhanging obstacles such as a moving ball or an archway. Second, sample efficiency remains a bottleneck. Modern approaches rely heavily on synthetic data~\cite{NVIDIA_Isaac_Sim}, but the computational cost of rendering visual input makes on-policy RL algorithms prohibitively expensive~\cite{cheng2024extreme}. The sparse and discontinuous nature of contact events also poses significant challenges for model-free methods, which often require extensive training or carefully designed inductive biases to guide exploration effectively~\cite{liu2025discrete,zhang2024wococo}. Finally, multi-task capability is limited. While current policies can be trained to solve a single task with a specific reward function, they often fail to generalize across interactions with different objects or adapt to variations in task definitions~\cite{schulman2017proximal}.

%  With the learning-based method,  most research used only proprioception or 2.5D terrain elevation maps \cite{cheng2024extreme, liu2024mbc, roth2025learned}, neglecting interaction with other objects. Off-policy and offline policy methods have been demonstrated to be more data-efficient \cite{hafner2023mastering, fujimoto2018addressing, haarnoja2018soft,kumar2020conservative} in general RL domain. 

% Sampling MPC based on learned dynamics offers better generalizability \cite{williams2017information, pan2024model, xue2024full}. The learned dynamic models can be used to achieve adaptive control by defining a cost function on the expected trajectory \cite{xiao2024anycar}. However, defining an expected trajectory is challenging in contact-aware or high-dimensional spaces. Alternatively, control can be achieved by using reinforcement learning to bootstrap a cost function \cite{hansen2022temporal, hafner2019learning}. Despite these developments, using ego-centric perception and a single scalable dynamic model for multi-task remains an open problem.


\subsection{Planning with Robotic World Models}

The world model~\cite{ha2018recurrent} is a learned, internal model of an environment that allows an robot to predict future outcomes based on actions. The emphasis is not on explicitly predicting the future, but rather on predicting its abstract representation. Using a learned world model for model-based planning offers a path toward better generalization and data efficiency, presenting a promising solution for contact planning~\cite{hansen2022temporal,hafner2019learning}.

Early work on world models~\cite{ha2018recurrent} was defined as enabling efficient policy learning in reinforcement learning~\cite{hansen2022temporal, hafner2019learning}. Today, the definition has evolved as a generative AI system~\cite{assran2025v,bruce2024genie} capable of understanding and simulating the physical world's causal relationships, dynamics, and interactions. In control and robotics, a similar path is being explored by learning neural dynamics models to represent complex systems~\cite{o2022neural, zhang2024adaptigraph, li2025offline, li2025robotic}. These learned models could be integrated into frameworks such as sampling-based MPC~\cite{williams2017information, pan2024model, xue2024full} to achieve highly adaptive control~\cite{margolislearning, roth2025learned, xiao2024anycar}.


Nonetheless, enabling robotic world models to fully generalize remains an open problem. This is especially the case for contact planning, as the underlying whole-body contact state is not directly observable and is difficult to infer and predict from partial, noisy sensory data.



% When these two approaches are combined \cite{hansen2022temporal,hafner2019learning}, they can also enhance the overall interpretability and performance of the policy \cite{sferrazza2024humanoidbench}.

% \subsection{Reinforcement Learning}

% Since the introduction of parallelized simulation \cite{makoviychuk2021isaac, NVIDIA_Isaac_Sim, freeman2021brax} combined with PPO \cite{schulman2017proximal}, on-policy training has achieved remarkable success in floating-base robot control \cite{rudin2022learning, mittal2023orbit, hwangbo2019learning, zakka2025mujoco}, including quadruped \cite{lee2020learning,ji2022hierarchical,cheng2024extreme}, bipedal \cite{li2021reinforcement, li2025reinforcement,li2023robust} and humanoid robots \cite{truong2025beyondmimic, radosavovic2024real,he2025attention}. Despite these significant achievements, most research used only proprioceptive information or simplified elevation maps for terrain interaction \cite{liu2024mbc}, neglecting interaction with other objects. The primary reason is the computational burden of visual rendering, leading to inefficient training. Furthermore, these methods, acting as black-box models, suffer from a lack of interpretability. Off-policy and offline policy methods have been demonstrated to be more data-efficient \cite{hafner2023mastering, wu2023daydreamer, fujimoto2018addressing, haarnoja2018soft,kumar2020conservative} in the general RL domain. When these approaches are combined with planning algorithms \cite{hansen2022temporal,hafner2019learning}, they can also enhance the overall interpretability and performance of the policy \cite{sferrazza2024humanoidbench}.



% \subsection{Perceptive planning}

% Model Predictive Control (MPC) has been applied to robot perceptive motion planning, achieving robust performance\cite{grandia2023perceptive,corberes2025perceptive, pankert2020perceptive}. However, MPC with explicit mathematical models highly depend on accurate models and global state estimation, which poses limitations in long-range scenarios and neglects ego-centric information like camera\cite{paden2016survey}.

% Sampling MPC based on learned dynamics offers better generalizability \cite{williams2017information, pan2024model, xue2024full}. The learned dynamic models can be used to achieve adaptive control by defining a cost function on the expected trajectory \cite{xiao2024anycar}. However, defining an expected trajectory is challenging in contact-rich or high-dimensional spaces. Alternatively, control can be achieved by using reinforcement learning to bootstrap a cost function \cite{hansen2022temporal, hafner2019learning}. Despite these developments, using ego-centric perception and a single scalable dynamic model for multi-task remains an open problem. 

% In this paper, we present a novel approach that integrates a scalable world model with the sampling MPC framework. By leveraging a cost function augmented with value estimates, our method enables agile, ego-centric object interaction for humanoid robots.



% \subsection{Scalable World Model}
% \label{sec:wm}","\subsection{Model-Based Contact Planning}

For both locomotion and manipulation, robotics is replete with contact-rich problems, made challenging by the non-smooth dynamics induced by the impact ~\cite{raibert1986legged}. Optimization-based approaches address this by explicitly modeling these physical interactions, like linearizing the complex friction model into a Linear Complementarity Problem (LCP)~\cite{stewart1996implicit}, or relaxing it into a Cone Complementarity Problem (CCP)~\cite{anitescu2006optimization}. These formulations can then be embedded within a trajectory optimization framework~\cite{sleiman2021unified,winkler2018gait}. Another prominent paradigm is Hybrid Zero Dynamics (HZD)~\cite{westervelt2003hybrid, sreenath2011compliant}, which addresses the non-smooth contact dynamics of legged locomotion by enforcing virtual constraints whose associated zero dynamics surface remains invariant through impacts. However, such model-based approaches are often hindered by model inaccuracies and high computational costs~\cite{xue2024full}, which complicate real-time deployment. Furthermore, their reliance on predefined structures, such as periodic gaits~\cite{gong2019feedback} or reference foot-end trajectories~\cite{westervelt2003hybrid}, makes it difficult to scale them to more general, aperiodic whole-body contact scenarios.


\subsection{Learning-Based Contact
Planning}

Learning-based approaches for real-world contact planning have shown remarkable potential, enabling dynamic skills~\cite{li2025reinforcement, cheng2024extreme, jenelten2024dtc, liu2025discrete,margolis2024rapid, margolis2023walk}. However, three significant challenges remain largely unaddressed. First, interaction with both dynamic and static objects is limited. Most existing work on legged locomotion based on simplified 2.5D elevation maps~\cite{roth2025learned}, which cannot represent dynamic or overhanging obstacles such as a moving ball or an archway. Second, sample efficiency remains a bottleneck. Modern approaches rely heavily on synthetic data~\cite{NVIDIA_Isaac_Sim}, but the computational cost of rendering visual input makes on-policy RL algorithms prohibitively expensive~\cite{cheng2024extreme}. The sparse and discontinuous nature of contact events also poses significant challenges for model-free methods, which often require extensive training or carefully designed inductive biases to guide exploration effectively~\cite{liu2025discrete,zhang2024wococo}. Finally, multi-task capability is limited. While current policies can be trained to solve a single task with a specific reward function, they often fail to generalize across interactions with different objects or adapt to variations in task definitions~\cite{schulman2017proximal}.






\subsection{Planning with Robotic World Models}

The world model~\cite{ha2018recurrent} is a learned, internal model of an environment that allows an robot to predict future outcomes based on actions. The emphasis is not on explicitly predicting the future, but rather on predicting its abstract representation. Using a learned world model for model-based planning offers a path toward better generalization and data efficiency, presenting a promising solution for contact planning~\cite{hansen2022temporal,hafner2019learning}.

Early work on world models~\cite{ha2018recurrent} was defined as enabling efficient policy learning in reinforcement learning~\cite{hansen2022temporal, hafner2019learning}. Today, the definition has evolved as a generative AI system~\cite{assran2025v,bruce2024genie} capable of understanding and simulating the physical world's causal relationships, dynamics, and interactions. In control and robotics, a similar path is being explored by learning neural dynamics models to represent complex systems~\cite{o2022neural, zhang2024adaptigraph, li2025offline, li2025robotic}. These learned models could be integrated into frameworks such as sampling-based MPC~\cite{williams2017information, pan2024model, xue2024full} to achieve highly adaptive control~\cite{margolislearning, roth2025learned, xiao2024anycar}.


Nonetheless, enabling robotic world models to fully generalize remains an open problem. This is especially the case for contact planning, as the underlying whole-body contact state is not directly observable and is difficult to infer and predict from partial, noisy sensory data.",N/A
