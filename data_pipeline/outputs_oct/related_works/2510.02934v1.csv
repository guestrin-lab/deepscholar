arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2510.02934v1,http://arxiv.org/abs/2510.02934v1,2025-10-03 12:25:28+00:00,Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,"Large Language Models (LLMs) have demonstrated impressive capabilities in
code generation and are increasingly integrated into the software development
process. However, ensuring the correctness of LLM-generated code remains a
critical concern. Prior work has shown that the internal representations of
LLMs encode meaningful signals for assessing code correctness. Nevertheless,
the existing methods rely on representations from pre-selected/fixed layers and
token positions, which could limit its generalizability across diverse model
architectures and tasks. In this work, we introduce AUTOPROBE, a novel
model-agnostic approach that dynamically selects the most informative internal
representations for code correctness assessment. AUTOPROBE employs an
attention-based mechanism to learn importance scores for hidden states,
enabling it to focus on the most relevant features. These weighted
representations are then aggregated and passed to a probing classifier to
predict code correctness across multiple dimensions, including compilability,
functionality, and security. To evaluate the performance of AUTOPROBE, we
conduct extensive experiments across multiple benchmarks and code LLMs. Our
experimental results show that AUTOPROBE consistently outperforms the
baselines. For security assessment, AUTOPROBE surpasses the state-of-the-art
white-box approach by 18%. For compilability and functionality assessment,
AUTOPROBE demonstrates its highest robustness to code complexity, with the
performance higher than the other approaches by up to 19% and 111%,
respectively. These findings highlight that dynamically selecting important
internal signals enables AUTOPROBE to serve as a robust and generalizable
solution for assessing the correctness of code generated by various LLMs.","\textbf{LLM-based code generation.}
LLMs, known for their powerful capabilities in contextual understanding and response generation, have been widely applied across various domains, including natural language processing~\cite{zhang2025systematic, li2024flexkbqa, wei2023empirical, zhang2025teleclass}, vision-language integration~\cite{zhang2024vision, chen2024vitamin}, and software engineering (SE) tasks~\cite{zheng2025towards, wang2025can, chen2024chatunitest, zhang2023repocoder, rambo}. 
In the context of SE, code LLMs, such as  \deepseek~\cite{deepseek-coder}, \codellama~\cite{codellama}, and \magiccoder~\cite{magicoder}, which have been pre-trained or fine-tuned on large-scale code corpora, have demonstrated remarkable success in a wide range of tasks, e.g. code generation~\cite{rambo, zhang2023repocoder, mu2024clarifygpt}, test generation~\cite{chen2024chatunitest, wang2024hits}, code summarization~\cite{sun2024source}, and program repair~\cite{jin2023inferfix}, etc.
These models are increasingly integrated into modern development environments via tools like GitHub Copilot~\cite{github_copilot} and CodeGeeX2~\cite{codegeex2}, where they assist developers in generating boilerplate code, completing functions, or writing test cases. 
Despite their impressive performance, a key challenge remains, the hallucination problem~\cite{zhang2025llm, liu2024exploring, liu2023your, wang2025towards}, where LLMs generate code that is syntactically valid but semantically incorrect, insecure, or unexecutable. Such hallucinations compromise the reliability of LLM-generated code and raise concerns about correctness, security, and trustworthiness. 
Mitigating such issues remains an open problem and is essential for ensuring the reliable and effective adoption of code LLMs in real-world software projects.  



\textbf{LLM's hallucination detection.} Various approaches~\cite{ llmcheck, inside, inner-working,  manakul2023selfcheckgpt, huang2025survey} have been proposed to detect hallucinations in LLMs. These approaches can be broadly categorized into black-box and white-box approaches, depending on whether they rely on the model's input-output behaviors or leverage its internal computations.

\textbf{\textit{Black-box}} approaches~\cite{manakul2023selfcheckgpt, farquhar2024detecting, zhang2024self, zhang2024knowhalu, zhou2021detecting} detect hallucinations by relying solely on the input-output behavior of the models. A common strategy is \textit{uncertainty estimation}, based on a hypothesis that responses with higher model confidence are more likely to be correct~\cite{manakul2023selfcheckgpt, farquhar2024detecting, huang2023look}.
For instance, SelfCheckGPT~\cite{manakul2023selfcheckgpt} estimates model confidence by sampling multiple responses to the same query and measuring their semantic consistency. If the responses are consistent across samples, the output is considered to be reliable; otherwise, it is likely hallucinated.
Another research direction focuses on \textit{fact-checking}, where the generated outputs are validated against factual sources. These sources can be drawn from the model's internal knowledge~\cite{zhang2024self} or external corpora~\cite{zhang2024knowhalu, augenstein2024factuality, hu2024knowledge}. Additionally, some other black-box methods formulate hallucination detection as a \textit{classification task}, training a downstream classifier on features extracted from the generated outputs to distinguish hallucinated from faithful outputs~\cite{zhou2021detecting}.

On the other hand, \textbf{\textit{white-box}} approaches leverage the model's internal computations, such as \textit{hidden states}, \textit{activations}, \textit{attention maps}, to determine unreliable or hallucinated responses~\cite{inner-working, probing}. 
For example, Azaria~\etal~\cite{internal-state} train a classifier on the hidden activations to estimate the truthfulness of the generated statements. Similarly, INSIDE~\cite{inside} and FactoScope~\cite{factoscope}
analyze internal states to assess semantic consistency and factual reliability.


Our work follows the while-box paradigm by leveraging internal representations for hallucination detection. However, unlike prior studies~\cite{internal-state-2, inside, llmcheck, internal-state} that focus on unstructured Natural Language Generation (NLG) tasks, where correctness is often subjective by relying on human judgment or based on external factual knowledge, \tool target \textit{code correctness}, where correctness is objectively defined by syntax, semantics, and functional behaviors. 
Due to the nature of NLG tasks, previous works~\cite{inside, internal-state} mainly evaluate hallucination through semantic consistency or factual alignment among model responses. 
However, these techniques are not directly applicable to code.
In the context of code generation, a task may have multiple correct solutions that differ in implementation details or algorithms. As a result, inconsistency across generated responses does not necessarily imply incorrectness. 
To address this gap, \tool is specifically designed for assessing code correctness using internal representations.


\textbf{Code correctness assessment.}
Ensuring source code quality is a fundamental objective in the SE process. Traditional approaches for detecting program issues often rely heavily on handcrafted features and static analysis techniques~\cite{croft2021empirical}.
With the emergence of deep learning, pre-trained models such as CodeBERT~\cite{codebert}, GraphBERT~\cite{zhang2020graph}, and CodeT5~\cite{codet5} have been widely adopted due to their ability to capture rich contextual and semantic information from source code. 
These models support automatic feature extraction from different code representations such as raw code, program slices, or code property graphs, enabling bug and vulnerability detection across different levels of granularity, from coarse-grained levels, i.e., files or methods, to fine-grained~\cite{ivdetect, COSTA, velvet, linevd, linevul}, i.e., slices and statements. 
For instance, IVDetect~\cite{ivdetect} employs a graph-based neural network to detect vulnerabilities at the function level and uses interpretable models to further localize vulnerable statements. LineVul~\cite{linevul} and LineVD~\cite{linevd} leverage CodeBERT to encode code representations and have demonstrated superior performance over IVDetect in fine-grained vulnerability detection.


Recently, the quality of code generated by LLMs has gained significant attention due to the increasing reliance on AI-generated code in real-world applications. Several empirical studies have revealed the potential risks of bugs and security issues associated with LLM-generated code~\cite{lost-at-c, empirical-study-2, llm-gen-code-emp-study, calibration, vulnerabilities-copilot}. 
Multiple studies~\cite{huang2023look, llm-security-guard, autosafecoder} have been proposed for guaranteeing the quality of code generated by LLMs.
Huang \etal~\cite{huang2023look} propose a technique of uncertainty analysis to measure LLMs' confidence in generated outputs, which could aid in identifying potentially unreliable code.  
Furthermore, 
multi-agent frameworks such as AutoSafeCoder~\cite{autosafecoder} enhance LLM-based code generation by integrating static analysis and fuzz testing. Similarly, LLMSecGuard~\cite{llm-security-guard} combines the capabilities of static analyzers and LLMs to improve code security, demonstrating the benefits of hybrid approaches in mitigating vulnerabilities and strengthening code robustness.

\openia~\cite{openia} demonstrates that the internal states of LLMs encode valuable signals related to code correctness and can be leveraged to detect incorrect code. This work is closely related to ours, as both adopt a white-box approach. However, the key difference between \tool and \openia lies in the mechanism for selecting informative internal representations. 
\openia assumes that the hidden states of the last generated tokens at the last layer encapsulate the most comprehensive information and therefore uses them directly for correctness assessment. 
Instead of selecting a fixed representation that could lead to suboptimal performance across models, \tool proposes a model-agnostic approach that dynamically selects the most informative representations across multiple layers and token positions. This flexibility enables \tool to better adapt to different model architectures.","\textbf{LLM-based code generation.}
LLMs, known for their powerful capabilities in contextual understanding and response generation, have been widely applied across various domains, including natural language processing~\cite{zhang2025systematic, li2024flexkbqa, wei2023empirical, zhang2025teleclass}, vision-language integration~\cite{zhang2024vision, chen2024vitamin}, and software engineering (SE) tasks~\cite{zheng2025towards, wang2025can, chen2024chatunitest, zhang2023repocoder, rambo}. 
In the context of SE, code LLMs, such as  \deepseek~\cite{deepseek-coder}, \codellama~\cite{codellama}, and \magiccoder~\cite{magicoder}, which have been pre-trained or fine-tuned on large-scale code corpora, have demonstrated remarkable success in a wide range of tasks, e.g. code generation~\cite{rambo, zhang2023repocoder, mu2024clarifygpt}, test generation~\cite{chen2024chatunitest, wang2024hits}, code summarization~\cite{sun2024source}, and program repair~\cite{jin2023inferfix}, etc.
These models are increasingly integrated into modern development environments via tools like GitHub Copilot~\cite{github_copilot} and CodeGeeX2~\cite{codegeex2}, where they assist developers in generating boilerplate code, completing functions, or writing test cases. 
Despite their impressive performance, a key challenge remains, the hallucination problem~\cite{zhang2025llm, liu2024exploring, liu2023your, wang2025towards}, where LLMs generate code that is syntactically valid but semantically incorrect, insecure, or unexecutable. Such hallucinations compromise the reliability of LLM-generated code and raise concerns about correctness, security, and trustworthiness. 
Mitigating such issues remains an open problem and is essential for ensuring the reliable and effective adoption of code LLMs in real-world software projects.  



\textbf{LLM's hallucination detection.} Various approaches~\cite{ llmcheck, inside, inner-working,  manakul2023selfcheckgpt, huang2025survey} have been proposed to detect hallucinations in LLMs. These approaches can be broadly categorized into black-box and white-box approaches, depending on whether they rely on the model's input-output behaviors or leverage its internal computations.

\textbf{\textit{Black-box}} approaches~\cite{manakul2023selfcheckgpt, farquhar2024detecting, zhang2024self, zhang2024knowhalu, zhou2021detecting} detect hallucinations by relying solely on the input-output behavior of the models. A common strategy is \textit{uncertainty estimation}, based on a hypothesis that responses with higher model confidence are more likely to be correct~\cite{manakul2023selfcheckgpt, farquhar2024detecting, huang2023look}.
For instance, SelfCheckGPT~\cite{manakul2023selfcheckgpt} estimates model confidence by sampling multiple responses to the same query and measuring their semantic consistency. If the responses are consistent across samples, the output is considered to be reliable; otherwise, it is likely hallucinated.
Another research direction focuses on \textit{fact-checking}, where the generated outputs are validated against factual sources. These sources can be drawn from the model's internal knowledge~\cite{zhang2024self} or external corpora~\cite{zhang2024knowhalu, augenstein2024factuality, hu2024knowledge}. Additionally, some other black-box methods formulate hallucination detection as a \textit{classification task}, training a downstream classifier on features extracted from the generated outputs to distinguish hallucinated from faithful outputs~\cite{zhou2021detecting}.

On the other hand, \textbf{\textit{white-box}} approaches leverage the model's internal computations, such as \textit{hidden states}, \textit{activations}, \textit{attention maps}, to determine unreliable or hallucinated responses~\cite{inner-working, probing}. 
For example, Azaria~\etal~\cite{internal-state} train a classifier on the hidden activations to estimate the truthfulness of the generated statements. Similarly, INSIDE~\cite{inside} and FactoScope~\cite{factoscope}
analyze internal states to assess semantic consistency and factual reliability.


Our work follows the while-box paradigm by leveraging internal representations for hallucination detection. However, unlike prior studies~\cite{internal-state-2, inside, llmcheck, internal-state} that focus on unstructured Natural Language Generation (NLG) tasks, where correctness is often subjective by relying on human judgment or based on external factual knowledge, \tool target \textit{code correctness}, where correctness is objectively defined by syntax, semantics, and functional behaviors. 
Due to the nature of NLG tasks, previous works~\cite{inside, internal-state} mainly evaluate hallucination through semantic consistency or factual alignment among model responses. 
However, these techniques are not directly applicable to code.
In the context of code generation, a task may have multiple correct solutions that differ in implementation details or algorithms. As a result, inconsistency across generated responses does not necessarily imply incorrectness. 
To address this gap, \tool is specifically designed for assessing code correctness using internal representations.


\textbf{Code correctness assessment.}
Ensuring source code quality is a fundamental objective in the SE process. Traditional approaches for detecting program issues often rely heavily on handcrafted features and static analysis techniques~\cite{croft2021empirical}.
With the emergence of deep learning, pre-trained models such as CodeBERT~\cite{codebert}, GraphBERT~\cite{zhang2020graph}, and CodeT5~\cite{codet5} have been widely adopted due to their ability to capture rich contextual and semantic information from source code. 
These models support automatic feature extraction from different code representations such as raw code, program slices, or code property graphs, enabling bug and vulnerability detection across different levels of granularity, from coarse-grained levels, i.e., files or methods, to fine-grained~\cite{ivdetect, COSTA, velvet, linevd, linevul}, i.e., slices and statements. 
For instance, IVDetect~\cite{ivdetect} employs a graph-based neural network to detect vulnerabilities at the function level and uses interpretable models to further localize vulnerable statements. LineVul~\cite{linevul} and LineVD~\cite{linevd} leverage CodeBERT to encode code representations and have demonstrated superior performance over IVDetect in fine-grained vulnerability detection.


Recently, the quality of code generated by LLMs has gained significant attention due to the increasing reliance on AI-generated code in real-world applications. Several empirical studies have revealed the potential risks of bugs and security issues associated with LLM-generated code~\cite{lost-at-c, empirical-study-2, llm-gen-code-emp-study, calibration, vulnerabilities-copilot}. 
Multiple studies~\cite{huang2023look, llm-security-guard, autosafecoder} have been proposed for guaranteeing the quality of code generated by LLMs.
Huang \etal~\cite{huang2023look} propose a technique of uncertainty analysis to measure LLMs' confidence in generated outputs, which could aid in identifying potentially unreliable code.  
Furthermore, 
multi-agent frameworks such as AutoSafeCoder~\cite{autosafecoder} enhance LLM-based code generation by integrating static analysis and fuzz testing. Similarly, LLMSecGuard~\cite{llm-security-guard} combines the capabilities of static analyzers and LLMs to improve code security, demonstrating the benefits of hybrid approaches in mitigating vulnerabilities and strengthening code robustness.

\openia~\cite{openia} demonstrates that the internal states of LLMs encode valuable signals related to code correctness and can be leveraged to detect incorrect code. This work is closely related to ours, as both adopt a white-box approach. However, the key difference between \tool and \openia lies in the mechanism for selecting informative internal representations. 
\openia assumes that the hidden states of the last generated tokens at the last layer encapsulate the most comprehensive information and therefore uses them directly for correctness assessment. 
Instead of selecting a fixed representation that could lead to suboptimal performance across models, \tool proposes a model-agnostic approach that dynamically selects the most informative representations across multiple layers and token positions. This flexibility enables \tool to better adapt to different model architectures.","LLM-based code generation.LLMs, known for their
powerful capabilities in contextual understanding and re-
sponse generation, have been widely applied across various
domains,includingnaturallanguageprocessing[39,40,62,
63],vision-languageintegration[64,65],andsoftwareengi-
neering(SE)tasks[4,42,44,3,2].InthecontextofSE,code
LLMs, such as DEEPSEEKCODER[8], CODELLAMA[9],
and MAGICODER[41], which have been pre-trained or fine-
tuned on large-scale code corpora, have demonstrated re-
markable success in a wide range of tasks, e.g. code gen-
eration [2, 3, 66], test generation [44, 67], code summariza-
tion [68], and program repair [69], etc. These models are
increasingly integrated into modern development environ-
mentsviatoolslikeGitHubCopilot[6]andCodeGeeX2[7],
where they assist developers in generating boilerplate code,
completing functions, or writing test cases. Despite their
impressive performance, a key challenge remains, the hal-
lucination problem [45, 70, 11, 71], where LLMs generate
code that is syntactically valid but semantically incorrect,
insecure, or unexecutable. Such hallucinations compromise
the reliability of LLM-generated code and raise concerns
about correctness, security, and trustworthiness. Mitigating
such issues remains an open problem and is essential for
ensuring the reliable and effective adoption of code LLMs
in real-world software projects.
LLM’shallucinationdetection.Variousapproaches[46,
33, 30, 72, 73] have been proposed to detect hallucinations
inLLMs.Theseapproachescanbebroadlycategorizedinto
black-boxandwhite-boxapproaches,dependingonwhether
they rely on the model’s input-output behaviors or leverage
its internal computations.
Black-boxapproaches[72,74,75,76,77]detecthalluci-
nationsbyrelyingsolelyontheinput-outputbehaviorofthe
models.Acommonstrategyisuncertaintyestimation,based
onahypothesisthatresponseswithhighermodelconfidence
are more likely to be correct [72, 74, 78]. For instance,SelfCheckGPT [72] estimates model confidence by sam-
pling multiple responses to the same query and measuring
their semantic consistency. If the responses are consistent
across samples, the output is considered to be reliable;
otherwise, it is likely hallucinated. Another research direc-
tion focuses onfact-checking, where the generated outputs
are validated against factual sources. These sources can be
drawnfromthemodel’sinternalknowledge[75]orexternal
corpora [76, 79, 80]. Additionally, some other black-box
methodsformulatehallucinationdetectionasaclassification
task, training a downstream classifier on features extracted
from the generated outputs to distinguish hallucinated from
faithful outputs [77].
On the other hand,white-boxapproaches leverage the
model’sinternalcomputations,suchashiddenstates,activa-
tions,attentionmaps,todetermineunreliableorhallucinated
responses [30, 31]. For example, Azariaet al.[32] train a
classifier on the hidden activations to estimate the truthful-
nessofthegeneratedstatements.Similarly,INSIDE[33]and
FactoScope [81] analyze internal states to assess semantic
consistency and factual reliability.
Ourworkfollowsthewhile-boxparadigmbyleveraging
internal representations for hallucination detection. How-
ever, unlike prior studies [29, 33, 46, 32] that focus on un-
structuredNaturalLanguageGeneration(NLG)tasks,where
correctness is often subjective by relying on human judg-
ment or based on external factual knowledge, AUTOPROBE
targetcode correctness, where correctness is objectively
definedbysyntax,semantics,andfunctionalbehaviors.Due
to the nature of NLG tasks, previous works [33, 32] mainly
evaluate hallucination through semantic consistency or fac-
tual alignment among model responses. However, these
techniques are not directly applicable to code. In the con-
text of code generation, a task may have multiple correct
solutionsthatdifferinimplementationdetailsoralgorithms.
As a result, inconsistency across generated responses does
not necessarily imply incorrectness. To address this gap,
AUTOPROBEis specifically designed for assessing code
correctness using internal representations.
Code correctness assessment.Ensuring source code
quality is a fundamental objective in the SE process. Tra-
ditional approaches for detecting program issues often rely
heavily on handcrafted features and static analysis tech-
niques [82]. With the emergence of deep learning, pre-
trained models such as CodeBERT [26], GraphBERT [83],
and CodeT5 [27] have been widely adopted due to their
ability to capture rich contextual and semantic information
from source code. These models support automatic feature
extraction from different code representations such as raw
code, program slices, or code property graphs, enabling
bug and vulnerability detection across different levels of
granularity, from coarse-grained levels, i.e., files or meth-
ods, to fine-grained [84, 85, 86, 87, 88], i.e., slices and
statements. For instance, IVDetect [84] employs a graph-
basedneuralnetworktodetectvulnerabilitiesatthefunction
level and uses interpretable models to further localize vul-
nerable statements.LineVul [88] andLineVD [87]leverage
Vuet al.:Preprint submitted to ElsevierPage 17 of 21
AutoProbe
CodeBERTtoencodecoderepresentationsandhavedemon-
strated superior performance over IVDetect in fine-grained
vulnerability detection.
Recently, the quality of code generated by LLMs has
gainedsignificantattentionduetotheincreasingrelianceon
AI-generated code in real-world applications. Several em-
pirical studies have revealed the potential risks of bugs and
securityissuesassociatedwithLLM-generatedcode[89,90,
91,92,93].Multiplestudies[78,94,95]havebeenproposed
for guaranteeing the quality of code generated by LLMs.
Huangetal.[78]proposeatechniqueofuncertaintyanalysis
to measure LLMs’ confidence in generated outputs, which
couldaidinidentifyingpotentiallyunreliablecode.Further-
more, multi-agent frameworks such as AutoSafeCoder [95]
enhance LLM-based code generation by integrating static
analysis and fuzz testing. Similarly, LLMSecGuard [94]
combines the capabilities of static analyzers and LLMs to
improve code security, demonstrating the benefits of hybrid
approaches in mitigating vulnerabilities and strengthening
code robustness.
OPENIA[28] demonstrates that the internal states of
LLMs encode valuable signals related to code correctness
and can be leveraged to detect incorrect code. This work
is closely related to ours, as both adopt a white-box ap-
proach. However, the key difference between AUTOPROBE
andOPENIAliesinthemechanismforselectinginformative
internal representations. OPENIAassumes that the hidden
states of the last generated tokens at the last layer encap-
sulate the most comprehensive information and therefore
uses them directly for correctness assessment. Instead of
selecting a fixed representation that could lead to subopti-
mal performance across models, AUTOPROBEproposes a
model-agnostic approach that dynamically selects the most
informativerepresentationsacrossmultiplelayersandtoken
positions. This flexibility enables AUTOPROBEto better
adapt to different model architectures."
