arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2510.00881v1,http://arxiv.org/abs/2510.00881v1,2025-10-01 13:28:26+00:00,Advancing Automated Ethical Profiling in SE: a Zero-Shot Evaluation of LLM Reasoning,"Large Language Models (LLMs) are increasingly integrated into software
engineering (SE) tools for tasks that extend beyond code synthesis, including
judgment under uncertainty and reasoning in ethically significant contexts. We
present a fully automated framework for assessing ethical reasoning
capabilities across 16 LLMs in a zero-shot setting, using 30 real-world
ethically charged scenarios. Each model is prompted to identify the most
applicable ethical theory to an action, assess its moral acceptability, and
explain the reasoning behind their choice. Responses are compared against
expert ethicists' choices using inter-model agreement metrics. Our results show
that LLMs achieve an average Theory Consistency Rate (TCR) of 73.3% and Binary
Agreement Rate (BAR) on moral acceptability of 86.7%, with interpretable
divergences concentrated in ethically ambiguous cases. A qualitative analysis
of free-text explanations reveals strong conceptual convergence across models
despite surface-level lexical diversity. These findings support the potential
viability of LLMs as ethical inference engines within SE pipelines, enabling
scalable, auditable, and adaptive integration of user-aligned ethical
reasoning. Our focus is the Ethical Interpreter component of a broader
profiling pipeline: we evaluate whether current LLMs exhibit sufficient
interpretive stability and theory-consistent reasoning to support automated
profiling.","\label{sec:related}
\noindent\emph{\major{Ethics in autonomous and software-intensive systems.}}
The integration of ethical considerations into autonomous systems has been widely examined in software engineering. Prior work spans design-time approaches that encode ethics via codes of conduct, principles, and rules~\cite{suri2023software,alidoosti2021ethics,alidoosti2022incorporating,DBLP:journals/access/ShahinHNPSGW22,trailer2024ciniselli}, and verification-time approaches that formalize and check system decisions against ethical frameworks~\cite{jedlickova2024ensuring,dennis2016formal,cardoso2021implementing,inverardi2019ethics,de2024engineering,inverardi2022ethical,Machine_Ethics_in_Changing_Contexts:2021,karim2017ethical}. This body of research establishes both the need and the mechanisms for ensuring ethically compliant behavior in autonomous decision pipelines.

%\smallskip
\noindent\emph{\major{LLMs in software engineering and the need for ethical reasoning.}}
Concurrently, LLMs have been adopted across SE tasks such as code generation, bug detection, and documentation~\cite{hou2024large}. As these models are integrated into SE workflows, it becomes important to assess whether they exhibit ethical reasoning and how they might be leveraged in systems with ethical implications~\cite{DBLP:conf/emnlp/RaoKTAC23,han2022aligning}.

%\smallskip
\noindent\emph{\major{Model-centric evaluations of moral reasoning.}}
Han et al.~\cite{han2022aligning} evaluate LLM understanding of moral/ethical reasoning across five domains (justice, deontology, virtue ethics, utilitarianism, and commonsense morality). They fine-tune BERT-base, BERT-large, RoBERTa-large, and ALBERT-xxlarge on ETHICS datasets comprising over 13{,}000 scenarios, and then test the models’ ability to classify scenarios in line with the ethical theories used during fine-tuning. In contrast, the present work does not rely on fine-tuning; it evaluates whether pre-trained LLMs, in a zero-shot setup, display ethical reasoning.

%\smallskip
\noindent\emph{\major{Value identification in scholarly texts vs. ethically charged situations.}}
A complementary thread leverages LLMs to extract values and structure from scientific writing. For example,~\cite{ICSE2025PAPER} studies ChatGPT’s ability to identify human values from titles and abstracts of SE publications using Schwartz’s theory~\cite{schwartz2012overview}, followed by manual verification by humans. Related efforts show that LLMs can assist in extracting insights from scholarly texts, classifying publications, and mining metadata for literature reviews~\cite{hou2024large,alshami2023harnessing}. These studies focus on value detection in academic prose rather than on evaluating ethical concrete reasoning, ethically charged scenarios.

%\smallskip
\noindent\emph{\major{Reasoning with policies and learning moral rewards.}}
Rao et al.~\cite{DBLP:conf/emnlp/RaoKTAC23} argue against hard-coding specific moral values in LLMs and advocate for general ethical reasoning capacities that adapt to diverse contexts. They introduce in-context ethical policies defined at varying abstraction levels and grounded in deontology, virtue ethics, and consequentialism, reporting experiments across GPT-3, ChatGPT, and GPT-4 with GPT-4 showing stronger ethical reasoning. Tennant et al.~\cite{DBLP:conf/iclr/TennantHM25} incorporate intrinsic moral rewards, grounded in deontological and utilitarian theories, into reinforcement learning fine-tuning. Using the Iterated Prisoner’s Dilemma, they demonstrate that LLM agents can learn morally aligned strategies and even unlearn previously selfish behaviors. While these works shape model behavior through policies or moral rewards, the present study compares outputs across multiple models without additional tuning, treating ethical reasoning as a testing ground rather than a direct optimization target.

%\smallskip
\noindent\emph{\major{Positioning of the present study.}}
Prior research establishes design/verification-time mechanisms for ethics in autonomous systems, documents LLM utility in SE, and explores both fine-tuned moral reasoning and in-context ethical policy use. The contribution here is orthogonal: a zero-shot assessment of pre-trained LLMs’ ethical reasoning on ethically charged scenarios, contrasting with fine-tuned or policy-conditioned settings, and distinct from value-mining in academic text.



\begin{comment}","\noindent\emph{\major{Ethics in autonomous and software-intensive systems.}}
The integration of ethical considerations into autonomous systems has been widely examined in software engineering. Prior work spans design-time approaches that encode ethics via codes of conduct, principles, and rules~\cite{suri2023software,alidoosti2021ethics,alidoosti2022incorporating,DBLP:journals/access/ShahinHNPSGW22,trailer2024ciniselli}, and verification-time approaches that formalize and check system decisions against ethical frameworks~\cite{jedlickova2024ensuring,dennis2016formal,cardoso2021implementing,inverardi2019ethics,de2024engineering,inverardi2022ethical,Machine_Ethics_in_Changing_Contexts:2021,karim2017ethical}. This body of research establishes both the need and the mechanisms for ensuring ethically compliant behavior in autonomous decision pipelines.


\noindent\emph{\major{LLMs in software engineering and the need for ethical reasoning.}}
Concurrently, LLMs have been adopted across SE tasks such as code generation, bug detection, and documentation~\cite{hou2024large}. As these models are integrated into SE workflows, it becomes important to assess whether they exhibit ethical reasoning and how they might be leveraged in systems with ethical implications~\cite{DBLP:conf/emnlp/RaoKTAC23,han2022aligning}.


\noindent\emph{\major{Model-centric evaluations of moral reasoning.}}
Han et al.~\cite{han2022aligning} evaluate LLM understanding of moral/ethical reasoning across five domains (justice, deontology, virtue ethics, utilitarianism, and commonsense morality). They fine-tune BERT-base, BERT-large, RoBERTa-large, and ALBERT-xxlarge on ETHICS datasets comprising over 13{,}000 scenarios, and then test the models’ ability to classify scenarios in line with the ethical theories used during fine-tuning. In contrast, the present work does not rely on fine-tuning; it evaluates whether pre-trained LLMs, in a zero-shot setup, display ethical reasoning.


\noindent\emph{\major{Value identification in scholarly texts vs. ethically charged situations.}}
A complementary thread leverages LLMs to extract values and structure from scientific writing. For example,~\cite{ICSE2025PAPER} studies ChatGPT’s ability to identify human values from titles and abstracts of SE publications using Schwartz’s theory~\cite{schwartz2012overview}, followed by manual verification by humans. Related efforts show that LLMs can assist in extracting insights from scholarly texts, classifying publications, and mining metadata for literature reviews~\cite{hou2024large,alshami2023harnessing}. These studies focus on value detection in academic prose rather than on evaluating ethical concrete reasoning, ethically charged scenarios.


\noindent\emph{\major{Reasoning with policies and learning moral rewards.}}
Rao et al.~\cite{DBLP:conf/emnlp/RaoKTAC23} argue against hard-coding specific moral values in LLMs and advocate for general ethical reasoning capacities that adapt to diverse contexts. They introduce in-context ethical policies defined at varying abstraction levels and grounded in deontology, virtue ethics, and consequentialism, reporting experiments across GPT-3, ChatGPT, and GPT-4 with GPT-4 showing stronger ethical reasoning. Tennant et al.~\cite{DBLP:conf/iclr/TennantHM25} incorporate intrinsic moral rewards, grounded in deontological and utilitarian theories, into reinforcement learning fine-tuning. Using the Iterated Prisoner’s Dilemma, they demonstrate that LLM agents can learn morally aligned strategies and even unlearn previously selfish behaviors. While these works shape model behavior through policies or moral rewards, the present study compares outputs across multiple models without additional tuning, treating ethical reasoning as a testing ground rather than a direct optimization target.


\noindent\emph{\major{Positioning of the present study.}}
Prior research establishes design/verification-time mechanisms for ethics in autonomous systems, documents LLM utility in SE, and explores both fine-tuned moral reasoning and in-context ethical policy use. The contribution here is orthogonal: a zero-shot assessment of pre-trained LLMs’ ethical reasoning on ethically charged scenarios, contrasting with fine-tuned or policy-conditioned settings, and distinct from value-mining in academic text.



\begin{comment}",N/A
