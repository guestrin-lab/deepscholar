arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2509.15233v1,http://arxiv.org/abs/2509.15233v1,2025-09-17 02:50:54+00:00,Video2Roleplay: A Multimodal Dataset and Framework for Video-Guided Role-playing Agents,"Role-playing agents (RPAs) have attracted growing interest for their ability
to simulate immersive and interactive characters. However, existing approaches
primarily focus on static role profiles, overlooking the dynamic perceptual
abilities inherent to humans. To bridge this gap, we introduce the concept of
dynamic role profiles by incorporating video modality into RPAs. To support
this, we construct Role-playing-Video60k, a large-scale, high-quality dataset
comprising 60k videos and 700k corresponding dialogues. Based on this dataset,
we develop a comprehensive RPA framework that combines adaptive temporal
sampling with both dynamic and static role profile representations.
Specifically, the dynamic profile is created by adaptively sampling video
frames and feeding them to the LLM in temporal order, while the static profile
consists of (1) character dialogues from training videos during fine-tuning,
and (2) a summary context from the input video during inference. This joint
integration enables RPAs to generate greater responses. Furthermore, we propose
a robust evaluation method covering eight metrics. Experimental results
demonstrate the effectiveness of our framework, highlighting the importance of
dynamic role profiles in developing RPAs.","\subsection{Static Role Playing}
ChatHaruhi~\cite{li2023chatharuhi} provides a dataset of over $54k$ simulated dialogues for $32$ characters spanning Chinese, English, and anime.
CharacterGLM~\cite{zhou2024characterglm} allows for personalizing a diverse range of agent personas and social agents through customizable attributes and behaviors.
CharacterLLM~\cite{characterllm} builds a dataset detailing specific character experiences, then fine-tunes a base model with the dataset to achieve target character portrayal.
RoleLLM~\cite{wang2024rolellm} improves LLM role-playing via a multi-component framework (e.g., role profile construction, role-GPT, role-bench).
Ditto~\cite{lu2024ditto} introduces a self-alignment method to enhance LLM role-playing capabilities through knowledge augmentation and dialogue simulation.
MMrole~\cite{dai2024mmrole}introduces the concept of multimodal role-playing agents and offers a comprehensive framework for their development and evaluation.
RoleMRC~\cite{LUandLI2025RoleMRC} provides a fine-grained composite benchmark for role-playing and instruction-following, revealing activation patterns linked to these distinct abilities.
CoSER~\cite{wang2025coser} provides a dataset comprising $29,798$ authentic conversations and comprehensive data from 771 renowned books and proposes a given-circumstance acting method for training and evaluating role-playing LLMs.

\subsection{Video Understanding}
GPT4Video~\cite{wang2024gpt4video} proposes a unified framework for video understanding and generation via pre-trained model integration and develops a simple text-only fine-tuning method for instruction following and safety alignment.
%MA-LMM~\cite{malmm} proposes a long-term memory bank design to enhance multimodal models for long-term video modeling, reducing GPU memory usage and overcoming LLM context limits via online processing.
LongVLM~\cite{weng2024longvlm} introduces a VideoLLM for long-term video understanding, achieving affordability via segment decomposition, feature extraction, token merging, and global semantics.
Video-LLaVA~\cite{videollava} maps visual signals to the language feature space to achieve unified visual representations, introducing a method for aligning features prior to projection.
VideoAgent~\cite{wang2024videoagent} proposes an agent-based system that iteratively extracts and compiles key information for question answering, using vision-language models for visual translation and retrieval.
VidRecap~\cite{Recap} proposes a hierarchical caption generation method that creates CLIP captions, segment descriptions, and video summaries, trained using a coarse-to-fine approach to learn the structure of video.
LongVU~\cite{shen2024longvu} preserves frame information for lengthy videos by compressing tokens based on similarity and selecting relevant visual tokens for text queries.
InternVideo2.5~\cite{wang2025internvideo2.5} introduces a length-adaptive token approach to process videos, integrating visual perception with MLLM for fine-grained analysis.
\begin{figure}[t]
  \centering \includegraphics[width=\linewidth]{figures/Group_253.pdf}
  \caption{The video types and examples of our dataset.}
  \vspace{-10pt}
  \label{fig:type}
\end{figure}
\subsection{Multimodal Large Language Model}
CLIP~\cite{CLIP} achieves cross-modal understanding and unified representation by applying contrastive learning to unlabeled image-text pairs, eliminating the need for task-specific annotation.
Flamingo~\cite{flamingo} inserts new gated cross-attention layers into the LLMs to inject visual features and pre-trains the new layers on billions of image-text pairs.
Emu~\cite{sun2024emu} extends the approach of Flamingo~\cite{flamingo} by integrating additional modalities to model generation and the corresponding training corpus.
BLIP-2~\cite{li2023blip} introduces Q-Former for visual and linguistic representation learning, achieving zero-shot image-text generation and strong performance on visual language tasks with more efficient parameterization.
InternVL~\cite{chen2024internvl} presents the first alignment of a large-scale vision encoder with LLMs and introduces a progressive image-text alignment strategy, enabling efficient training of large-scale vision-language foundation models.
InstructBLIP~\cite{instructBLIP} introduces an instruction-aware feature extraction method for vision-language instruction tuning, significantly enhancing multimodal model performance.
LLaVA-NeXT~\cite{li2024llavanext-strong} enhances visual detail capture via improved input image resolution and refines its data mix through adapted visual instructions.","\subsection{Static Role Playing}
ChatHaruhi~\cite{li2023chatharuhi} provides a dataset of over $54k$ simulated dialogues for $32$ characters spanning Chinese, English, and anime.
CharacterGLM~\cite{zhou2024characterglm} allows for personalizing a diverse range of agent personas and social agents through customizable attributes and behaviors.
CharacterLLM~\cite{characterllm} builds a dataset detailing specific character experiences, then fine-tunes a base model with the dataset to achieve target character portrayal.
RoleLLM~\cite{wang2024rolellm} improves LLM role-playing via a multi-component framework (e.g., role profile construction, role-GPT, role-bench).
Ditto~\cite{lu2024ditto} introduces a self-alignment method to enhance LLM role-playing capabilities through knowledge augmentation and dialogue simulation.
MMrole~\cite{dai2024mmrole}introduces the concept of multimodal role-playing agents and offers a comprehensive framework for their development and evaluation.
RoleMRC~\cite{LUandLI2025RoleMRC} provides a fine-grained composite benchmark for role-playing and instruction-following, revealing activation patterns linked to these distinct abilities.
CoSER~\cite{wang2025coser} provides a dataset comprising $29,798$ authentic conversations and comprehensive data from 771 renowned books and proposes a given-circumstance acting method for training and evaluating role-playing LLMs.

\subsection{Video Understanding}
GPT4Video~\cite{wang2024gpt4video} proposes a unified framework for video understanding and generation via pre-trained model integration and develops a simple text-only fine-tuning method for instruction following and safety alignment.

LongVLM~\cite{weng2024longvlm} introduces a VideoLLM for long-term video understanding, achieving affordability via segment decomposition, feature extraction, token merging, and global semantics.
Video-LLaVA~\cite{videollava} maps visual signals to the language feature space to achieve unified visual representations, introducing a method for aligning features prior to projection.
VideoAgent~\cite{wang2024videoagent} proposes an agent-based system that iteratively extracts and compiles key information for question answering, using vision-language models for visual translation and retrieval.
VidRecap~\cite{Recap} proposes a hierarchical caption generation method that creates CLIP captions, segment descriptions, and video summaries, trained using a coarse-to-fine approach to learn the structure of video.
LongVU~\cite{shen2024longvu} preserves frame information for lengthy videos by compressing tokens based on similarity and selecting relevant visual tokens for text queries.
InternVideo2.5~\cite{wang2025internvideo2.5} introduces a length-adaptive token approach to process videos, integrating visual perception with MLLM for fine-grained analysis.

\subsection{Multimodal Large Language Model}
CLIP~\cite{CLIP} achieves cross-modal understanding and unified representation by applying contrastive learning to unlabeled image-text pairs, eliminating the need for task-specific annotation.
Flamingo~\cite{flamingo} inserts new gated cross-attention layers into the LLMs to inject visual features and pre-trains the new layers on billions of image-text pairs.
Emu~\cite{sun2024emu} extends the approach of Flamingo~\cite{flamingo} by integrating additional modalities to model generation and the corresponding training corpus.
BLIP-2~\cite{li2023blip} introduces Q-Former for visual and linguistic representation learning, achieving zero-shot image-text generation and strong performance on visual language tasks with more efficient parameterization.
InternVL~\cite{chen2024internvl} presents the first alignment of a large-scale vision encoder with LLMs and introduces a progressive image-text alignment strategy, enabling efficient training of large-scale vision-language foundation models.
InstructBLIP~\cite{instructBLIP} introduces an instruction-aware feature extraction method for vision-language instruction tuning, significantly enhancing multimodal model performance.
LLaVA-NeXT~\cite{li2024llavanext-strong} enhances visual detail capture via improved input image resolution and refines its data mix through adapted visual instructions.","2.1 Static Role Playing
ChatHaruhi (Li et al., 2023a) provides a dataset
of over 54ksimulated dialogues for 32characters
spanning Chinese, English, and anime. Charac-
terGLM (Zhou et al., 2024) allows for personal-
izing a diverse range of agent personas and so-
cial agents through customizable attributes and
behaviors. CharacterLLM (Shao et al., 2023)
builds a dataset detailing specific character ex-
periences, then fine-tunes a base model with
2
the dataset to achieve target character portrayal.
RoleLLM (Wang et al., 2024a) improves LLM role-
playing via a multi-component framework (e.g.,
role profile construction, role-GPT, role-bench).
Ditto (Lu et al., 2024) introduces a self-alignment
method to enhance LLM role-playing capabilities
through knowledge augmentation and dialogue sim-
ulation. MMrole (Dai et al., 2024)introduces the
concept of multimodal role-playing agents and of-
fers a comprehensive framework for their develop-
ment and evaluation. RoleMRC (Lu et al., 2025)
provides a fine-grained composite benchmark for
role-playing and instruction-following, revealing
activation patterns linked to these distinct abilities.
CoSER (Wang et al., 2025b) provides a dataset
comprising 29,798 authentic conversations and
comprehensive data from 771 renowned books and
proposes a given-circumstance acting method for
training and evaluating role-playing LLMs.
2.2 Video Understanding
GPT4Video (Wang et al., 2024e) proposes a unified
framework for video understanding and generation
via pre-trained model integration and develops a
simple text-only fine-tuning method for instruction
following and safety alignment. LongVLM (Weng
et al., 2024) introduces a VideoLLM for long-term
video understanding, achieving affordability via
segment decomposition, feature extraction, token
merging, and global semantics. Video-LLaV A (Lin
et al., 2024) maps visual signals to the language fea-
ture space to achieve unified visual representations,
introducing a method for aligning features prior
to projection. VideoAgent (Wang et al., 2024c)
proposes an agent-based system that iteratively ex-
tracts and compiles key information for question
answering, using vision-language models for visual
translation and retrieval. VidRecap (Islam et al.,
2024) proposes a hierarchical caption generation
method that creates CLIP captions, segment de-
scriptions, and video summaries, trained using a
coarse-to-fine approach to learn the structure of
video. LongVU (Shen et al., 2024) preserves frame
information for lengthy videos by compressing to-
kens based on similarity and selecting relevant vi-
sual tokens for text queries. InternVideo2.5 (Wang
et al., 2025c) introduces a length-adaptive token
approach to process videos, integrating visual per-
ception with MLLM for fine-grained analysis.
Documentary
Vlog
LiveFigure 2: The video types and examples of our dataset.
2.3 Multimodal Large Language Model
CLIP (Radford et al., 2021) achieves cross-modal
understanding and unified representation by apply-
ing contrastive learning to unlabeled image-text
pairs, eliminating the need for task-specific annota-
tion. Flamingo (Alayrac et al., 2022) inserts new
gated cross-attention layers into the LLMs to inject
visual features and pre-trains the new layers on bil-
lions of image-text pairs. Emu (Sun et al., 2024)
extends the approach of Flamingo (Alayrac et al.,
2022) by integrating additional modalities to model
generation and the corresponding training corpus.
BLIP-2 (Li et al., 2023b) introduces Q-Former
for visual and linguistic representation learning,
achieving zero-shot image-text generation and
strong performance on visual language tasks with
more efficient parameterization. InternVL (Chen
et al., 2024c) presents the first alignment of a
large-scale vision encoder with LLMs and intro-
duces a progressive image-text alignment strategy,
enabling efficient training of large-scale vision-
language foundation models. InstructBLIP (Dai
et al., 2023) introduces an instruction-aware feature
extraction method for vision-language instruction
tuning, significantly enhancing multimodal model
performance. LLaV A-NeXT (Li et al., 2024) en-
hances visual detail capture via improved input
image resolution and refines its data mix through
adapted visual instructions.
3"
