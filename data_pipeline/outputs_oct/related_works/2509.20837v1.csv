arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2509.20837v1,http://arxiv.org/abs/2509.20837v1,2025-09-25 07:23:30+00:00,Verification Limits Code LLM Training,"Large language models for code generation increasingly rely on synthetic
data, where both problem solutions and verification tests are generated by
models. While this enables scalable data creation, it introduces a previously
unexplored bottleneck: the verification ceiling, in which the quality and
diversity of training data are fundamentally constrained by the capabilities of
synthetic verifiers. In this work, we systematically study how verification
design and strategies influence model performance. We investigate (i) what we
verify by analyzing the impact of test complexity and quantity: richer test
suites improve code generation capabilities (on average +3 pass@1), while
quantity alone yields diminishing returns, (ii) how we verify by exploring
relaxed pass thresholds: rigid 100% pass criteria can be overly restrictive. By
allowing for relaxed thresholds or incorporating LLM-based soft verification,
we can recover valuable training data, leading to a 2-4 point improvement in
pass@1 performance. However, this benefit is contingent upon the strength and
diversity of the test cases used, and (iii) why verification remains necessary
through controlled comparisons of formally correct versus incorrect solutions
and human evaluation: retaining diverse correct solutions per problem yields
consistent generalization gains. Our results show that Verification as
currently practiced is too rigid, filtering out valuable diversity. But it
cannot be discarded, only recalibrated. By combining calibrated verification
with diverse, challenging problem-solution pairs, we outline a path to break
the verification ceiling and unlock stronger code generation models.","\label{sec:related_work}

The use of synthetic data to train and evaluate code generation models has gained traction as a scalable alternative to curated datasets. Frameworks like Case2Code \citep{shao2025case2codescalablesyntheticdata} demonstrate that synthesizing diverse code samples via LLMs can improve inductive reasoning and code understanding. Similarly, \citet{nadas2024synthetic} surveys LLM-driven synthetic data pipelines, highlighting techniques such as prompt-based generation and reinforcement learning with execution feedback.
Pipelines for synthetic code generation rely on some form of verification of the generated code as a proxy for its quality \citep{luo2025wizardcoderempoweringcodelarge,chen2021evaluatinglargelanguagemodels}. The most common method by far relies on unit tests to validate functional correctness, as this also mirrors what is done in popular coding benchmarks like HumanEval \citep{humeval-2024-human}, MBPP \citep{lyu2024passimprovecodegeneration_mbpp_plus} or LBPP \citep{matton-etal-2024-leakage}.  

However, this approach has potentially several flaws: Unit tests may fail to cover edge cases, overfit to specific implementations, or discard valid solutions that fail partial tests. Moreover, generating comprehensive test suites is labor-intensive and brittle for complex problems \citep{lahiri2023interactivecodegenerationtestdriven}, which is only partially alleviated by synthetic methods for unit-test generation \citep{wang2025automatedunittestcase}. 
Moreover, problem difficulty and diversity have a huge impact on the final model performance.  \citet{tambon2025taskevalassessingdifficultycode} highlights that model performance degrades significantly as problem complexity increases, suggesting that training data must better reflect this diversity. \citet{chen2024diversitysyntheticdataimpact} also argue that diverse synthetic samples improve generalization, even for smaller models.","The use of synthetic data to train and evaluate code generation models has gained traction as a scalable alternative to curated datasets. Frameworks like Case2Code \citep{shao2025case2codescalablesyntheticdata} demonstrate that synthesizing diverse code samples via LLMs can improve inductive reasoning and code understanding. Similarly, \citet{nadas2024synthetic} surveys LLM-driven synthetic data pipelines, highlighting techniques such as prompt-based generation and reinforcement learning with execution feedback.
Pipelines for synthetic code generation rely on some form of verification of the generated code as a proxy for its quality \citep{luo2025wizardcoderempoweringcodelarge,chen2021evaluatinglargelanguagemodels}. The most common method by far relies on unit tests to validate functional correctness, as this also mirrors what is done in popular coding benchmarks like HumanEval \citep{humeval-2024-human}, MBPP \citep{lyu2024passimprovecodegeneration_mbpp_plus} or LBPP \citep{matton-etal-2024-leakage}.  

However, this approach has potentially several flaws: Unit tests may fail to cover edge cases, overfit to specific implementations, or discard valid solutions that fail partial tests. Moreover, generating comprehensive test suites is labor-intensive and brittle for complex problems \citep{lahiri2023interactivecodegenerationtestdriven}, which is only partially alleviated by synthetic methods for unit-test generation \citep{wang2025automatedunittestcase}. 
Moreover, problem difficulty and diversity have a huge impact on the final model performance.  \citet{tambon2025taskevalassessingdifficultycode} highlights that model performance degrades significantly as problem complexity increases, suggesting that training data must better reflect this diversity. \citet{chen2024diversitysyntheticdataimpact} also argue that diverse synthetic samples improve generalization, even for smaller models.","The use of synthetic data to train and evaluate code generation models has gained traction as a
scalable alternative to curated datasets. Frameworks like Case2Code [Shao et al., 2025] demonstrate
that synthesizing diverse code samples via LLMs can improve inductive reasoning and code under-
standing. Similarly, Nadˇ aş et al. [2025] surveys LLM-driven synthetic data pipelines, highlighting
techniques such as prompt-based generation and reinforcement learning with execution feedback.
Pipelines for synthetic code generation rely on some form of verification of the generated code as a
proxy for its quality [Luo et al., 2025; Chen et al., 2021]. The most common method by far relies
on unit tests to validate functional correctness, as this also mirrors what is done in popular coding
benchmarks like HumanEval [Balloccu et al., 2024], MBPP [Lyu et al., 2024] or LBPP [Matton
et al., 2024].
However, this approach has potentially several flaws: Unit tests may fail to cover edge cases, overfit
to specific implementations, or discard valid solutions that fail partial tests. Moreover, generating
comprehensive test suites is labor-intensive and brittle for complex problems [Lahiri et al., 2023],
which is only partially alleviated by synthetic methods for unit-test generation [Wang et al., 2025].
Moreover, problem difficulty and diversity have a huge impact on the final model performance.
Tambonetal.[2025]highlightsthatmodelperformancedegradessignificantlyasproblemcomplexity
increases, suggesting that training data must better reflect this diversity. Chen et al. [2024] also
argue that diverse synthetic samples improve generalization, even for smaller models.
14"
