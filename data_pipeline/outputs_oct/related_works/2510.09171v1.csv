arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2510.09171v1,http://arxiv.org/abs/2510.09171v1,2025-10-10 09:14:33+00:00,Instance-Level Generation for Representation Learning,"Instance-level recognition (ILR) focuses on identifying individual objects
rather than broad categories, offering the highest granularity in image
classification. However, this fine-grained nature makes creating large-scale
annotated datasets challenging, limiting ILR's real-world applicability across
domains. To overcome this, we introduce a novel approach that synthetically
generates diverse object instances from multiple domains under varied
conditions and backgrounds, forming a large-scale training set. Unlike prior
work on automatic data synthesis, our method is the first to address
ILR-specific challenges without relying on any real images. Fine-tuning
foundation vision models on the generated data significantly improves retrieval
performance across seven ILR benchmarks spanning multiple domains. Our approach
offers a new, efficient, and effective alternative to extensive data collection
and curation, introducing a new ILR paradigm where the only input is the names
of the target domains, unlocking a wide range of real-world applications.","\label{sec:relatedwork}

\paragraph{Instance-level representations}
%
Instance-level recognition requires image representations that capture fine-grained object details while distinguishing them from numerous semantically similar classes. Generic models like ResNet~\citep{hzr+16} and CLIP~\citep{rkh+21} struggle in this setting, as they prioritize high-level semantics over instance-specific features. A common solution is fine-tuning pre-trained backbones on domain-specific datasets—such as artwork~\citep{ypsilantis2021met}, landmarks~\citep{lsl+22,sck+23,cas20,ski+24}, or products~\citep{ptm22,ramzi2022hierarchical}—to enhance their ability to differentiate individual instances. Recent efforts focus on universal embeddings~\citep{ypsilantis2023towards} that cover jointly a whole range of domains and tasks. However, models still require fine-tuning with class-supervised learning to acquire the necessary discriminative properties, making the scarcity of high-quality labeled datasets a major challenge. Data augmentation techniques~\citep{ypsilantis2021met} help mitigate this issue by generating diverse variations of an instance from limited samples.
%
The only prior work that also leverages generative models for instance-level tasks~\citep{Sundaram25iclr} fine-tunes a separate model for each instance, requiring a few real images as input. In contrast, our approach trains a single model that generalizes well across objects and domains without relying on any real images.\looseness=-1

\paragraph{Training with synthetic images}
Synthetic data has been used in a variety of computer vision problems, such as object detection~\citep{peng2015learning,rozantsev2015rendering,georgakis2017synthesizing}, segmentation~\citep{chen2019learning,ros2016synthia}, autonomous driving~\citep{abu2018augmented}, object pose estimation~\citep{cai2022ove6d, labbe2020cosypose}, 3D-tasks~\citep{chang2015shapenet}, and recently for representation learning~\citep{tian2024stablerep, wu2023not}. An early practice is to cut the real objects and paste them onto backgrounds to generate synthetic images for instance or object detection~\citep{dwibedi2017cut, georgakis2017synthesizing}. However, challenges remain in reducing the boundary artifacts and achieving consistent lighting conditions between the object and background, as these problems often result in unrealistic composite images.
More recently, the main sources of synthetic images are computer graphics pipeline or rendering engines~\citep{mahmood2019amass}, generative adversarial networks (GAN)~\citep{besnier2020dataset,brock2018large}, and text-to-image GDM~\citep{fan2024scaling,sariyildiz2023fake}. Images generated through rendering engines often suffer from domain gap when compared to real-world test images, requiring domain adaptation techniques to mitigate the gap during training. In contrast, GAN and GDM produce more realistic images that do not typically require post-generation domain adaptation~\citep{wang2020self6d}. Text-to-image GDM, in particular, offers a higher degree of control in the image generation process, for example, changing the background of the target object using text prompts~\citep{mokady2023null,raj2023dreambooth3d,geng2024instructdiffusion,zhang2023adding}. This ability to control image features through text makes GDM particularly valuable for generating diverse images, which is crucial for representation learning~\citep{tian2024stablerep, wu2023not}. However, synthesizing images for instance-level task is not trivial, as it requires generating a synthetic object under various conditions while preserving its structure and texture. 

\paragraph{Metric learning for image retrieval}
%
Given a training dataset, the most common approach for training deep representation networks for image retrieval is supervised learning using categorical labels.
As a result, a large number of methods have proposed classification-based losses~\citep{zw18,dgx+19,tdt20,qss+19,kim2020proxy}.
Despite not directly optimizing the pairwise distance metric that is used at test time, such approaches achieve very good performance, especially when combined with propagating the representation across examples~\citep{evt+20,sel21,kpm+23}. 
Other methods directly optimize the distance metric with pairwise losses. 
These most often rely on hand-crafted loss functions, such as the most popular contrastive~\citep{hcl06}, and triplet loss~\citep{skp+15}, by postulating a correlation between such a training objective and the test time objective which is typically an information retrieval metric. 
Finding informative pairs and triplets~\citep{mbl20,rms+20,sxj+15,sohn16} appears to be very important.
As a natural follow-up, a few recent methods directly optimized differentiable approximations of retrieval metrics, such as average precision~\citep{rmp+20,hls18,rar+19,ramzi2021robust,ramzi2022hierarchical} and recall~\citep{ptm22}. 
In this work, we rely on recall@k~\citep{ptm22} as a loss function which is demonstrating top results on a variety of benchmarks in the literature and does not require hard negative mining.
%
Self-supervised~\citep{kkc+} methods exist as well and are shown effective, but are tested only on training data from the target distributions, which is not a realistic setup. 
%
A recent alternative to CLIP~\citep{rkh+21}, called Unicom~\citep{anunicom}, trains on LAION 400M \citep{schuhmann2021laion}, treats captions as weak annotations to perform text-based clustering, and reformulates the learning as a classification task. Their results show improvements in a set of different retrieval datasets, including instance-level ones.
Alternatively, we propose leveraging synthetic data to introduce an extensive collection of objects with diverse variations into the training dataset.","\paragraph{Instance-level representations}

Instance-level recognition requires image representations that capture fine-grained object details while distinguishing them from numerous semantically similar classes. Generic models like ResNet~\citep{hzr+16} and CLIP~\citep{rkh+21} struggle in this setting, as they prioritize high-level semantics over instance-specific features. A common solution is fine-tuning pre-trained backbones on domain-specific datasets—such as artwork~\citep{ypsilantis2021met}, landmarks~\citep{lsl+22,sck+23,cas20,ski+24}, or products~\citep{ptm22,ramzi2022hierarchical}—to enhance their ability to differentiate individual instances. Recent efforts focus on universal embeddings~\citep{ypsilantis2023towards} that cover jointly a whole range of domains and tasks. However, models still require fine-tuning with class-supervised learning to acquire the necessary discriminative properties, making the scarcity of high-quality labeled datasets a major challenge. Data augmentation techniques~\citep{ypsilantis2021met} help mitigate this issue by generating diverse variations of an instance from limited samples.

The only prior work that also leverages generative models for instance-level tasks~\citep{Sundaram25iclr} fine-tunes a separate model for each instance, requiring a few real images as input. In contrast, our approach trains a single model that generalizes well across objects and domains without relying on any real images.\looseness=-1

\paragraph{Training with synthetic images}
Synthetic data has been used in a variety of computer vision problems, such as object detection~\citep{peng2015learning,rozantsev2015rendering,georgakis2017synthesizing}, segmentation~\citep{chen2019learning,ros2016synthia}, autonomous driving~\citep{abu2018augmented}, object pose estimation~\citep{cai2022ove6d, labbe2020cosypose}, 3D-tasks~\citep{chang2015shapenet}, and recently for representation learning~\citep{tian2024stablerep, wu2023not}. An early practice is to cut the real objects and paste them onto backgrounds to generate synthetic images for instance or object detection~\citep{dwibedi2017cut, georgakis2017synthesizing}. However, challenges remain in reducing the boundary artifacts and achieving consistent lighting conditions between the object and background, as these problems often result in unrealistic composite images.
More recently, the main sources of synthetic images are computer graphics pipeline or rendering engines~\citep{mahmood2019amass}, generative adversarial networks (GAN)~\citep{besnier2020dataset,brock2018large}, and text-to-image GDM~\citep{fan2024scaling,sariyildiz2023fake}. Images generated through rendering engines often suffer from domain gap when compared to real-world test images, requiring domain adaptation techniques to mitigate the gap during training. In contrast, GAN and GDM produce more realistic images that do not typically require post-generation domain adaptation~\citep{wang2020self6d}. Text-to-image GDM, in particular, offers a higher degree of control in the image generation process, for example, changing the background of the target object using text prompts~\citep{mokady2023null,raj2023dreambooth3d,geng2024instructdiffusion,zhang2023adding}. This ability to control image features through text makes GDM particularly valuable for generating diverse images, which is crucial for representation learning~\citep{tian2024stablerep, wu2023not}. However, synthesizing images for instance-level task is not trivial, as it requires generating a synthetic object under various conditions while preserving its structure and texture. 

\paragraph{Metric learning for image retrieval}

Given a training dataset, the most common approach for training deep representation networks for image retrieval is supervised learning using categorical labels.
As a result, a large number of methods have proposed classification-based losses~\citep{zw18,dgx+19,tdt20,qss+19,kim2020proxy}.
Despite not directly optimizing the pairwise distance metric that is used at test time, such approaches achieve very good performance, especially when combined with propagating the representation across examples~\citep{evt+20,sel21,kpm+23}. 
Other methods directly optimize the distance metric with pairwise losses. 
These most often rely on hand-crafted loss functions, such as the most popular contrastive~\citep{hcl06}, and triplet loss~\citep{skp+15}, by postulating a correlation between such a training objective and the test time objective which is typically an information retrieval metric. 
Finding informative pairs and triplets~\citep{mbl20,rms+20,sxj+15,sohn16} appears to be very important.
As a natural follow-up, a few recent methods directly optimized differentiable approximations of retrieval metrics, such as average precision~\citep{rmp+20,hls18,rar+19,ramzi2021robust,ramzi2022hierarchical} and recall~\citep{ptm22}. 
In this work, we rely on recall@k~\citep{ptm22} as a loss function which is demonstrating top results on a variety of benchmarks in the literature and does not require hard negative mining.

Self-supervised~\citep{kkc+} methods exist as well and are shown effective, but are tested only on training data from the target distributions, which is not a realistic setup. 

A recent alternative to CLIP~\citep{rkh+21}, called Unicom~\citep{anunicom}, trains on LAION 400M \citep{schuhmann2021laion}, treats captions as weak annotations to perform text-based clustering, and reformulates the learning as a classification task. Their results show improvements in a set of different retrieval datasets, including instance-level ones.
Alternatively, we propose leveraging synthetic data to introduce an extensive collection of objects with diverse variations into the training dataset.","Instance-level representationsInstance-level recognition requires image representations that capture
fine-grained object details while distinguishing them from numerous semantically similar classes. Generic
models like ResNet (He et al., 2016) and CLIP (Radford et al., 2021) struggle in this setting, as they
prioritize high-level semantics over instance-specific features. A common solution is fine-tuning pre-trained
backbones on domain-specific datasets—such as artwork (Ypsilantis et al., 2021), landmarks (Lee et al.,
2022; Shao et al., 2023; Cao et al., 2020; Suma et al., 2024), or products (Patel et al., 2022; Ramzi et al.,
2022)—to enhance their ability to differentiate individual instances. Recent efforts focus on universal embed-
dings (Ypsilantis et al., 2023) that cover jointly a whole range of domains and tasks. However, models still
require fine-tuning with class-supervised learning to acquire the necessary discriminative properties, making
the scarcity of high-quality labeled datasets a major challenge. Data augmentation techniques (Ypsilantis
2
et al., 2021) help mitigate this issue by generating diverse variations of an instance from limited samples.
The only prior work that also leverages generative models for instance-level tasks (Sundaram et al., 2025)
fine-tunes a separate model for each instance, requiring a few real images as input. In contrast, our approach
trains a single model that generalizes well across objects and domains without relying on any real images.
Training with synthetic imagesSynthetic data has been used in a variety of computer vision problems,
suchasobjectdetection(Pengetal.,2015;Rozantsevetal.,2015;Georgakisetal.,2017), segmentation(Chen
et al., 2019; Ros et al., 2016), autonomous driving (Abu Alhaija et al., 2018), object pose estimation (Cai
et al., 2022; Labbé et al., 2020), 3D-tasks (Chang et al., 2015), and recently for representation learning (Tian
et al., 2024; Wu et al., 2023). An early practice is to cut the real objects and paste them onto backgrounds
to generate synthetic images for instance or object detection (Dwibedi et al., 2017; Georgakis et al., 2017).
However, challenges remain in reducing the boundary artifacts and achieving consistent lighting conditions
between the object and background, as these problems often result in unrealistic composite images. More
recently, themainsourcesofsyntheticimagesarecomputergraphicspipelineorrenderingengines(Mahmood
et al., 2019), generative adversarial networks (GAN) (Besnier et al., 2020; Brock, 2018), and text-to-image
GDM (Fan et al., 2024; Sarıyıldız et al., 2023). Images generated through rendering engines often suffer from
domain gap when compared to real-world test images, requiring domain adaptation techniques to mitigate
the gap during training. In contrast, GAN and GDM produce more realistic images that do not typically
require post-generation domain adaptation (Wang et al., 2020). Text-to-image GDM, in particular, offers
a higher degree of control in the image generation process, for example, changing the background of the
target object using text prompts (Mokady et al., 2023; Raj et al., 2023; Geng et al., 2024; Zhang et al.,
2023). This ability to control image features through text makes GDM particularly valuable for generating
diverse images, which is crucial for representation learning (Tian et al., 2024; Wu et al., 2023). However,
synthesizing images for instance-level task is not trivial, as it requires generating a synthetic object under
various conditions while preserving its structure and texture.
Metric learning for image retrievalGiven a training dataset, the most common approach for training
deep representation networks for image retrieval is supervised learning using categorical labels. As a result, a
large number of methods have proposed classification-based losses (Zhai & Wu, 2018; Deng et al., 2019; Teh
et al., 2020; Qian et al., 2019; Kim et al., 2020). Despite not directly optimizing the pairwise distance metric
that is used at test time, such approaches achieve very good performance, especially when combined with
propagating the representation across examples (Elezi et al., 2020; Seidenschwarz et al., 2021; Kotovenko
et al., 2023). Other methods directly optimize the distance metric with pairwise losses. These most often
rely on hand-crafted loss functions, such as the most popular contrastive (Hadsell et al., 2006), and triplet
loss (Schroff et al., 2015), by postulating a correlation between such a training objective and the test time
objective which is typically an information retrieval metric. Finding informative pairs and triplets (Musgrave
et al., 2020; Roth et al., 2020; Oh Song et al., 2016; Sohn, 2016) appears to be very important. As a natural
follow-up, a few recent methods directly optimized differentiable approximations of retrieval metrics, such
as average precision (Rolínek et al., 2020; He et al., 2018; Revaud et al., 2019; Ramzi et al., 2021; 2022) and
recall (Patel et al., 2022). In this work, we rely on recall@k (Patel et al., 2022) as a loss function which is
demonstrating top results on a variety of benchmarks in the literature and does not require hard negative
mining. Self-supervised (Kim et al., 2022) methods exist as well and are shown effective, but are tested
only on training data from the target distributions, which is not a realistic setup. A recent alternative to
CLIP (Radford et al., 2021), called Unicom (An et al., 2023), trains on LAION 400M (Schuhmann et al.,
2021), treats captions as weak annotations to perform text-based clustering, and reformulates the learning
as a classification task. Their results show improvements in a set of different retrieval datasets, including
instance-level ones. Alternatively, we propose leveraging synthetic data to introduce an extensive collection
of objects with diverse variations into the training dataset."
