arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2510.08669v1,http://arxiv.org/abs/2510.08669v1,2025-10-09 17:22:23+00:00,FreqCa: Accelerating Diffusion Models via Frequency-Aware Caching,"The application of diffusion transformers is suffering from their significant
inference costs. Recently, feature caching has been proposed to solve this
problem by reusing features from previous timesteps, thereby skipping
computation in future timesteps. However, previous feature caching assumes that
features in adjacent timesteps are similar or continuous, which does not always
hold in all settings. To investigate this, this paper begins with an analysis
from the frequency domain, which reveal that different frequency bands in the
features of diffusion models exhibit different dynamics across timesteps.
Concretely, low-frequency components, which decide the structure of images,
exhibit higher similarity but poor continuity. In contrast, the high-frequency
bands, which decode the details of images, show significant continuity but poor
similarity. These interesting observations motivate us to propose
Frequency-aware Caching (FreqCa)
  which directly reuses features of low-frequency components based on their
similarity, while using a second-order Hermite interpolator to predict the
volatile high-frequency ones based on its continuity.
  Besides, we further propose to cache Cumulative Residual Feature (CRF)
instead of the features in all the layers, which reduces the memory footprint
of feature caching by 99%.
  Extensive experiments on FLUX.1-dev, FLUX.1-Kontext-dev, Qwen-Image, and
Qwen-Image-Edit demonstrate its effectiveness in both generation and editing.
Codes are available in the supplementary materials and will be released on
GitHub.","\label{sec:Related Works}
\vspace{-3mm}

Diffusion models have emerged as a cornerstone of modern generative AI, exhibiting state-of-the-art capabilities in synthesizing visual content~\citep{sohl2015deep,ho2020DDPM}. While early models were predominantly built upon U-Net architectures~\citep{ronneberger2015unet}, their scalability limitations paved the way for the Diffusion Transformer (DiT)~\citep{peebles2023dit}. The DiT architecture has since become foundational, catalyzing a wave of powerful models across diverse domains~\citep{opensora,yang2025cogvideox}. Nevertheless, the iterative nature of the diffusion sampling process imposes a significant computational burden during inference, making acceleration a critical area of research~\citep{ho2020DDPM,peebles2023dit}. Current efforts to enhance efficiency are largely focused on two complementary directions: reducing the number of sampling steps and accelerating the denoising network itself.

\vspace{-3mm}
\subsection{Sampling Timestep Reduction}
\vspace{-1mm}

One primary strategy seeks to minimize the number of required sampling iterations while preserving generation quality. Seminal work like DDIM introduced deterministic sampling to reduce step counts without significant fidelity loss~\citep{songDDIM}. This concept was further refined by the DPM-Solver series, which employed high-order ODE solvers to achieve faster convergence~\citep{lu2022dpm,lu2022dpm++,zheng2023dpmsolvervF}. Other notable approaches include knowledge distillation, which trains a student model to emulate multiple denoising steps of a larger teacher model~\citep{salimans2022progressive,meng2022on}, and Rectified Flow, which learns to straighten the generation path between noise and data distributions~\citep{refitiedflow}. More recently, Consistency Models have enabled high-quality synthesis in a single step by directly mapping noise to clean data, circumventing the need for a sequential path~\citep{song2023consistency}.

\vspace{-3mm}
\subsection{Denoising Network Acceleration}
\vspace{-1mm}

An alternative to reducing timesteps is to decrease the computational cost of each forward pass through the denoising network. This is typically achieved via model compression or feature caching.

\vspace{-2mm}
\paragraph{Model Compression-based Acceleration.} 
\vspace{-1mm}

One avenue involves model compression, which includes techniques such as network pruning~\citep{structural_pruning_diffusion, zhu2024dipgo}, quantization~\citep{10377259, shang2023post, kim2025ditto}, and various forms of token reduction that dynamically shorten the input sequence length~\citep{bolya2023tomesd, kim2024tofu, zhang2024tokenpruningcachingbetter, zhang2025sito}. While effective, these methods often necessitate a fine-tuning or retraining stage to mitigate the potential loss of expressive power inherent in model simplification~\citep{li2024snapfusion,10377259}.

\vspace{-2mm}
\paragraph{Feature Caching-based Acceleration.}
\vspace{-2mm}
A compelling training-free alternative is feature caching, which exploits temporal redundancies in the denoising process. Pioneered in U-Net architectures through FasterDiffusion and DeepCache, this paradigm was subsequently adapted to DiTs. Initial efforts focused on a ``cache then reuse'' strategy, while advanced techniques like FORA and $\Delta$-DiT refined this approach. This concept evolved with more sophisticated mechanisms, including dynamic token-level updates (ToCa), adaptive sampling (RAS~\citep{liu2025regionadaptivesamplingdiffusiontransformers}), and explicit error correction frameworks~\citep{qiu2025acceleratingdiffusiontransformererroroptimized, chenIncrementCalibrated2025, chuOmniCache2025}. A pivotal shift was the ``cache then forecast'' paradigm introduced by TaylorSeeer, which was further advanced by more robust numerical methods in FoCa~\citep{zhengFoCa2025}, HiCache~\citep{fengHiCache2025}, and SpeCa~\citep{Liu2025SpeCa}.

However, a crucial flaw underlies these sophisticated paradigms, as hinted at by preliminary frequency-domain analyses. For instance, PAB~\citep{zhao2024PAB} insightfully associated different attention mechanisms with distinct frequency bands but did not delve into token-level frequency dynamics. Similarly, while FasterCache~\citep{lvFasterCacheTrainingFreeVideo2025} examined the frequency-domain differences within Classifier-Free Guidance , its findings were confined to this specific context, not addressing the more universal dynamics of temporal feature evolution and thus showing limited practical acceleration.

In contrast to prior methods that treat features as a monolithic whole, we propose \textbf{FreqCa}, which resolves quality degradation in caching by decomposing features into their stable low-frequency and volatile high-frequency components for differentiated treatment. As an added benefit,  we introduce the Cumulative Residual Feature, collapsing the memory complexity from $\mathcal{O}(L)$ to $\mathcal{O}(1)$ to solve the resource inefficiency of prior ``layer-wise'' architectures.
\vspace{-4mm}","\vspace{-3mm}

Diffusion models have emerged as a cornerstone of modern generative AI, exhibiting state-of-the-art capabilities in synthesizing visual content~\citep{sohl2015deep,ho2020DDPM}. While early models were predominantly built upon U-Net architectures~\citep{ronneberger2015unet}, their scalability limitations paved the way for the Diffusion Transformer (DiT)~\citep{peebles2023dit}. The DiT architecture has since become foundational, catalyzing a wave of powerful models across diverse domains~\citep{opensora,yang2025cogvideox}. Nevertheless, the iterative nature of the diffusion sampling process imposes a significant computational burden during inference, making acceleration a critical area of research~\citep{ho2020DDPM,peebles2023dit}. Current efforts to enhance efficiency are largely focused on two complementary directions: reducing the number of sampling steps and accelerating the denoising network itself.

\vspace{-3mm}
\subsection{Sampling Timestep Reduction}
\vspace{-1mm}

One primary strategy seeks to minimize the number of required sampling iterations while preserving generation quality. Seminal work like DDIM introduced deterministic sampling to reduce step counts without significant fidelity loss~\citep{songDDIM}. This concept was further refined by the DPM-Solver series, which employed high-order ODE solvers to achieve faster convergence~\citep{lu2022dpm,lu2022dpm++,zheng2023dpmsolvervF}. Other notable approaches include knowledge distillation, which trains a student model to emulate multiple denoising steps of a larger teacher model~\citep{salimans2022progressive,meng2022on}, and Rectified Flow, which learns to straighten the generation path between noise and data distributions~\citep{refitiedflow}. More recently, Consistency Models have enabled high-quality synthesis in a single step by directly mapping noise to clean data, circumventing the need for a sequential path~\citep{song2023consistency}.

\vspace{-3mm}
\subsection{Denoising Network Acceleration}
\vspace{-1mm}

An alternative to reducing timesteps is to decrease the computational cost of each forward pass through the denoising network. This is typically achieved via model compression or feature caching.

\vspace{-2mm}
\paragraph{Model Compression-based Acceleration.} 
\vspace{-1mm}

One avenue involves model compression, which includes techniques such as network pruning~\citep{structural_pruning_diffusion, zhu2024dipgo}, quantization~\citep{10377259, shang2023post, kim2025ditto}, and various forms of token reduction that dynamically shorten the input sequence length~\citep{bolya2023tomesd, kim2024tofu, zhang2024tokenpruningcachingbetter, zhang2025sito}. While effective, these methods often necessitate a fine-tuning or retraining stage to mitigate the potential loss of expressive power inherent in model simplification~\citep{li2024snapfusion,10377259}.

\vspace{-2mm}
\paragraph{Feature Caching-based Acceleration.}
\vspace{-2mm}
A compelling training-free alternative is feature caching, which exploits temporal redundancies in the denoising process. Pioneered in U-Net architectures through FasterDiffusion and DeepCache, this paradigm was subsequently adapted to DiTs. Initial efforts focused on a ``cache then reuse'' strategy, while advanced techniques like FORA and $\Delta$-DiT refined this approach. This concept evolved with more sophisticated mechanisms, including dynamic token-level updates (ToCa), adaptive sampling (RAS~\citep{liu2025regionadaptivesamplingdiffusiontransformers}), and explicit error correction frameworks~\citep{qiu2025acceleratingdiffusiontransformererroroptimized, chenIncrementCalibrated2025, chuOmniCache2025}. A pivotal shift was the ``cache then forecast'' paradigm introduced by TaylorSeeer, which was further advanced by more robust numerical methods in FoCa~\citep{zhengFoCa2025}, HiCache~\citep{fengHiCache2025}, and SpeCa~\citep{Liu2025SpeCa}.

However, a crucial flaw underlies these sophisticated paradigms, as hinted at by preliminary frequency-domain analyses. For instance, PAB~\citep{zhao2024PAB} insightfully associated different attention mechanisms with distinct frequency bands but did not delve into token-level frequency dynamics. Similarly, while FasterCache~\citep{lvFasterCacheTrainingFreeVideo2025} examined the frequency-domain differences within Classifier-Free Guidance , its findings were confined to this specific context, not addressing the more universal dynamics of temporal feature evolution and thus showing limited practical acceleration.

In contrast to prior methods that treat features as a monolithic whole, we propose \textbf{FreqCa}, which resolves quality degradation in caching by decomposing features into their stable low-frequency and volatile high-frequency components for differentiated treatment. As an added benefit,  we introduce the Cumulative Residual Feature, collapsing the memory complexity from $\mathcal{O}(L)$ to $\mathcal{O}(1)$ to solve the resource inefficiency of prior ``layer-wise'' architectures.
\vspace{-4mm}",N/A
