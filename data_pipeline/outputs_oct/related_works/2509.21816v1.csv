arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2509.21816v1,http://arxiv.org/abs/2509.21816v1,2025-09-26 03:21:39+00:00,No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials,"Excel is one of the most widely used productivity tools across domains,
offering rich functionality but also overwhelming users with its complexity.
This creates a persistent demand for tutorials to support effective usage.
However, existing tutorials are manually authored by experts, require frequent
updates after each software release, and incur substantial labor costs. Prior
work has not achieved fully automated tutorial generation, since existing
methods still depend on handcrafted operation sequences or example materials.
In this paper, we present the first framework for automatically generating
Excel tutorials directly from natural language task descriptions. Our framework
first instantiates the task. Then a central component of this framework,
Execution Agent, plans and executes the solution in Excel, and collects the
intermediate artifacts required for tutorial construction. These artifacts are
then transformed into both structured Excel documents and video demonstrations.
To build a comprehensive tutorial corpus, we collected 1,559 task descriptions
from real-world scenarios. In addition, we designed a systematic evaluation
framework that integrates assessments from both large language models (LLMs)
and human reviewers. Experimental results show that our framework improves task
execution success rates by 8.5% over state-of-the-art baselines. Moreover, the
generated tutorials demonstrate superior readability and instructional
effectiveness, often approaching or surpassing expert-authored materials.
Importantly, the automated pipeline eliminates manual labor and reduces time
costs to 1/20 of expert authoring, making scalable and high-quality tutorial
generation practical for the first time.","\subsection{Spreadsheet Agent}

With the rapid advancement of large language models (LLMs), recent spreadsheet agents have increasingly leveraged LLM capabilities to address complex tasks involving spreadsheet data. SheetCopilot \cite{li2023sheetcopilot} enables stable interaction between LLMs and spreadsheets by defining atomic actions within a state machine-based task planning framework. SheetAgent \cite{chen2025sheetagent} introduces a modular Planner–Informer–Retriever architecture that effectively addresses long-horizon spreadsheet tasks through iterative reasoning. SheetMind \cite{zhu2025sheetmind} builds a multi-agent system with clearly defined roles, enhancing the robustness and reliability of task execution. Although existing spreadsheet agents have made progress in automating table-level tasks through code execution, their operations remain opaque to end users and fail to provide transparent, interface-based learning paths.

\subsection{Computer-Using Agents (CUAs)}
Recent advances in desktop automation have produced a range of LLM-powered agent systems. UFO \cite{zhang2024ufo} represents one of the earliest multi-agent automation frameworks for Windows, emphasizing GUI interaction through the integration of UI Automation and visual perception. NAVI \cite{bonatti2024windows}, a single-agent baseline from WAA, leverages both screenshots and accessibility metadata to facilitate GUI understanding. OmniAgent \cite{lu2024omniparser} combines OmniParser for visual grounding with GPT-based action planning, enabling robust multimodal reasoning. Agent S \cite{agashe2024agent} employs a multi-agent architecture with experience-driven hierarchical planning, optimized for executing complex, multi-step tasks. Operator \cite{openai2025computer}, a recent high-performance CUA from OpenAI, simulates human-like mouse and keyboard operations based on screenshot inputs. While these systems demonstrate the promise of LLM-driven desktop agents, they often suffer from shallow OS integration and heavy reliance on brittle visual inputs. 

% UFO\^2 \cite{zhang2025ufo2} advances this area by introducing a system-level framework with specialized agents, a hybrid GUI–API action layer, and deep OS awareness, thereby improving robustness and accuracy in task execution across diverse applications. Notably, UFO2 achieves state-of-the-art performance on multiple benchmark evaluations.




\subsection{Automated Tutorial Generation}
Prior work has explored semi-automated tutorial generation using pre-existing materials such as user demonstrations and instructional videos. For instance, MixT \cite{chi2012mixt} produces mixed-media tutorials by combining static instructions with video segments derived from user demonstrations.
In a similar vein, an approach~\cite{truong2021automatic} has been proposed to automatically construct hierarchical tutorials from makeup instructional videos.
Other research \cite{grabler2009generating} captures software operations along with screencast recordings and converts them into document-style tutorials enriched with text descriptions and annotated step images. In the domain of physical tasks, some approaches \cite{denning2011meshflow,grossman2010chronicle} produce  instructional videos through semi-automatic editing of creator-annotated single-take or multi-take demonstrations. 
% \lu{foot note format seems not correct}


In addition to leveraging demonstrations and videos, several studies have explored transforming static textual content (such as user manuals) into richer, multimedia-based tutorials.
HelpViz \cite{zhong2021helpviz} automatically converts text-based mobile instructions into contextual visual tutorials by parsing action sequences, simulating interactions on Android emulators, and synthesizing step-by-step visual assets, leading to higher user preference over plain text.
Other systems such as M2V \cite{liu2024having} and HowToCut \cite{chi2021automatic} focus on generating instructional videos from manuals or markdown tutorials, combining NLP, computer vision, and automatic video editing to produce engaging, easy-to-follow guidance preferred by most users over traditional documentation.

% Unlike these methods, our approach does not rely on pre-existing instructional steps or external resources such as text, images, or videos. Instead, it directly generates the corresponding tutorial in an end-to-end manner based solely on a given query, thereby achieving a higher degree of automation.


%","\subsection{Spreadsheet Agent}

With the rapid advancement of large language models (LLMs), recent spreadsheet agents have increasingly leveraged LLM capabilities to address complex tasks involving spreadsheet data. SheetCopilot \cite{li2023sheetcopilot} enables stable interaction between LLMs and spreadsheets by defining atomic actions within a state machine-based task planning framework. SheetAgent \cite{chen2025sheetagent} introduces a modular Planner–Informer–Retriever architecture that effectively addresses long-horizon spreadsheet tasks through iterative reasoning. SheetMind \cite{zhu2025sheetmind} builds a multi-agent system with clearly defined roles, enhancing the robustness and reliability of task execution. Although existing spreadsheet agents have made progress in automating table-level tasks through code execution, their operations remain opaque to end users and fail to provide transparent, interface-based learning paths.

\subsection{Computer-Using Agents (CUAs)}
Recent advances in desktop automation have produced a range of LLM-powered agent systems. UFO \cite{zhang2024ufo} represents one of the earliest multi-agent automation frameworks for Windows, emphasizing GUI interaction through the integration of UI Automation and visual perception. NAVI \cite{bonatti2024windows}, a single-agent baseline from WAA, leverages both screenshots and accessibility metadata to facilitate GUI understanding. OmniAgent \cite{lu2024omniparser} combines OmniParser for visual grounding with GPT-based action planning, enabling robust multimodal reasoning. Agent S \cite{agashe2024agent} employs a multi-agent architecture with experience-driven hierarchical planning, optimized for executing complex, multi-step tasks. Operator \cite{openai2025computer}, a recent high-performance CUA from OpenAI, simulates human-like mouse and keyboard operations based on screenshot inputs. While these systems demonstrate the promise of LLM-driven desktop agents, they often suffer from shallow OS integration and heavy reliance on brittle visual inputs. 






\subsection{Automated Tutorial Generation}
Prior work has explored semi-automated tutorial generation using pre-existing materials such as user demonstrations and instructional videos. For instance, MixT \cite{chi2012mixt} produces mixed-media tutorials by combining static instructions with video segments derived from user demonstrations.
In a similar vein, an approach~\cite{truong2021automatic} has been proposed to automatically construct hierarchical tutorials from makeup instructional videos.
Other research \cite{grabler2009generating} captures software operations along with screencast recordings and converts them into document-style tutorials enriched with text descriptions and annotated step images. In the domain of physical tasks, some approaches \cite{denning2011meshflow,grossman2010chronicle} produce  instructional videos through semi-automatic editing of creator-annotated single-take or multi-take demonstrations. 



In addition to leveraging demonstrations and videos, several studies have explored transforming static textual content (such as user manuals) into richer, multimedia-based tutorials.
HelpViz \cite{zhong2021helpviz} automatically converts text-based mobile instructions into contextual visual tutorials by parsing action sequences, simulating interactions on Android emulators, and synthesizing step-by-step visual assets, leading to higher user preference over plain text.
Other systems such as M2V \cite{liu2024having} and HowToCut \cite{chi2021automatic} focus on generating instructional videos from manuals or markdown tutorials, combining NLP, computer vision, and automatic video editing to produce engaging, easy-to-follow guidance preferred by most users over traditional documentation.","2.1 Spreadsheet Agent
With the rapid advancement of large language models (LLMs), recent spreadsheet agents have increas-
ingly leveraged LLM capabilities to address complex tasks involving spreadsheet data. SheetCopilot
Li et al. [2023] enables stable interaction between LLMs and spreadsheets by defining atomic actions
within a state machine-based task planning framework. SheetAgent Chen et al. [2025] introduces a
modular Planner–Informer–Retriever architecture that effectively addresses long-horizon spreadsheet
tasks through iterative reasoning. SheetMind Zhu et al. [2025] builds a multi-agent system with
2
clearly defined roles, enhancing the robustness and reliability of task execution. Although existing
spreadsheet agents have made progress in automating table-level tasks through code execution, their
operations remain opaque to end users and fail to provide transparent, interface-based learning paths.
2.2 Computer-Using Agents (CUAs)
Recent advances in desktop automation have produced a range of LLM-powered agent systems. UFO
Zhang et al. [2024b] represents one of the earliest multi-agent automation frameworks for Windows,
emphasizing GUI interaction through the integration of UI Automation and visual perception. NA VI
Bonatti et al. [2024], a single-agent baseline from WAA, leverages both screenshots and accessibility
metadata to facilitate GUI understanding. OmniAgent Lu et al. [2024] combines OmniParser for
visual grounding with GPT-based action planning, enabling robust multimodal reasoning. Agent
S Agashe et al. [2024] employs a multi-agent architecture with experience-driven hierarchical
planning, optimized for executing complex, multi-step tasks. Operator OpenAI [2025c], a recent
high-performance CUA from OpenAI, simulates human-like mouse and keyboard operations based
on screenshot inputs. While these systems demonstrate the promise of LLM-driven desktop agents,
they often suffer from shallow OS integration and heavy reliance on brittle visual inputs.
2.3 Automated Tutorial Generation
Prior work has explored semi-automated tutorial generation using pre-existing materials such as user
demonstrations and instructional videos. For instance, MixT Chi et al. [2012] produces mixed-media
tutorials by combining static instructions with video segments derived from user demonstrations.
In a similar vein, an approach Truong et al. [2021] has been proposed to automatically construct
hierarchical tutorials from makeup instructional videos. Other research Grabler et al. [2009] captures
software operations along with screencast recordings and converts them into document-style tutorials
enriched with text descriptions and annotated step images. In the domain of physical tasks, some
approaches Denning et al. [2011], Grossman et al. [2010] produce instructional videos through
semi-automatic editing of creator-annotated single-take or multi-take demonstrations.
In addition to leveraging demonstrations and videos, several studies have explored transforming
static textual content (such as user manuals) into richer, multimedia-based tutorials. HelpViz Zhong
et al. [2021] automatically converts text-based mobile instructions into contextual visual tutorials by
parsing action sequences, simulating interactions on Android emulators, and synthesizing step-by-step
visual assets, leading to higher user preference over plain text. Other systems such as M2V Liu
et al. [2024] and HowToCut Chi et al. [2021] focus on generating instructional videos from manuals
or markdown tutorials, combining NLP, computer vision, and automatic video editing to produce
engaging, easy-to-follow guidance preferred by most users over traditional documentation."
