arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2509.25155v1,http://arxiv.org/abs/2509.25155v1,2025-09-29 17:55:43+00:00,Context-Driven Performance Modeling for Causal Inference Operators on Neural Processing Units,"The proliferation of large language models (LLMs) has driven demand for long
context inference on resource constrained edge devices. However, deploying
these models on Neural Processing Units (NPUs) presents significant challenges
due to the architectural mismatch: quadratic complexity of standard attention
mechanisms conflicts with memory and compute patterns of edge accelerators.
This paper presents a comprehensive performance analysis of various causal
inference operators on a modern NPU. We benchmark standard quadratic attention
against several sub-quadratic alternatives, including structured state-space
and linear attention models. Our analysis reveals that while sub-quadratic
methods offer superior scalability, they introduce distinct computational
bottlenecks on the NPU's specialized execution units. We identify that
quadratic attention becomes severely memory-bound, suffering from cache
inefficiency and pipeline stalls exceeding 95% at long contexts. In contrast,
sub-quadratic models can become compute-bound on programmable vector cores.
These findings provide critical insights for the co-design of hardware-aware
models and optimization strategies to enable on-device AI inference with
long-contexts.","\subsection{Long-Context Inference on Edge Platforms}
Prior work has explored deploying transformer-based causal large language models (LLMs) on edge platforms \cite{zhang2024edgeshardefficientllminference}, including hardware-specific optimizations for ARM CPUs \cite{10.1145/3700410.3702126} and FPGA-based execution through frameworks like \texttt{llama.cpp} \cite{llamacpp, haris2024designing}. While these efforts target on-device inference, they are not designed for long-context scenarios and do not address the associated memory and compute bottlenecks that emerge in attention-heavy models. Our work addresses this gap by empirically analyzing a range of causal inference mechanisms—including standard transformers and structured state-space models (SSMs)—under long-context settings. This enables us to derive architectural insights that inform the co-design of attention mechanisms for Neural Processing Units (NPUs), supporting more efficient hardware-aware deployment strategies.

\subsection{Acceleration of Sequence Models on NPUs}
Several efforts have investigated transformer acceleration on NPUs \cite{xu2025fast, zhu2025edge}, typically through operator-level scheduling or compiler-level block partitioning. However, these approaches fall short in capturing the fine-grained resource behavior required for efficient long-context inference. Other work has focused on optimizing SSMs for NPUs \cite{das2025xamba, aalishah2025mambalitesr}, leveraging architectural properties such as linear recurrence and memory compression. While successful within their respective domains, these strategies are not directly applicable to transformer-style causal attention. In contrast, our approach uses execution profiling and performance modeling—grounded in structured operator variants—to analyze architectural trade-offs across attention and SSM-style models. By leveraging structured state-space duality (SSD), we characterize how causal operators interact with NPU memory and compute hierarchies, enabling more informed co-design for future inference systems.","\subsection{Long-Context Inference on Edge Platforms}
Prior work has explored deploying transformer-based causal large language models (LLMs) on edge platforms \cite{zhang2024edgeshardefficientllminference}, including hardware-specific optimizations for ARM CPUs \cite{10.1145/3700410.3702126} and FPGA-based execution through frameworks like \texttt{llama.cpp} \cite{llamacpp, haris2024designing}. While these efforts target on-device inference, they are not designed for long-context scenarios and do not address the associated memory and compute bottlenecks that emerge in attention-heavy models. Our work addresses this gap by empirically analyzing a range of causal inference mechanisms—including standard transformers and structured state-space models (SSMs)—under long-context settings. This enables us to derive architectural insights that inform the co-design of attention mechanisms for Neural Processing Units (NPUs), supporting more efficient hardware-aware deployment strategies.

\subsection{Acceleration of Sequence Models on NPUs}
Several efforts have investigated transformer acceleration on NPUs \cite{xu2025fast, zhu2025edge}, typically through operator-level scheduling or compiler-level block partitioning. However, these approaches fall short in capturing the fine-grained resource behavior required for efficient long-context inference. Other work has focused on optimizing SSMs for NPUs \cite{das2025xamba, aalishah2025mambalitesr}, leveraging architectural properties such as linear recurrence and memory compression. While successful within their respective domains, these strategies are not directly applicable to transformer-style causal attention. In contrast, our approach uses execution profiling and performance modeling—grounded in structured operator variants—to analyze architectural trade-offs across attention and SSM-style models. By leveraging structured state-space duality (SSD), we characterize how causal operators interact with NPU memory and compute hierarchies, enabling more informed co-design for future inference systems.",N/A
