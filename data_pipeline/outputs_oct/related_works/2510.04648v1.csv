arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2510.04648v1,http://arxiv.org/abs/2510.04648v1,2025-10-06 09:52:18+00:00,EduPersona: Benchmarking Subjective Ability Boundaries of Virtual Student Agents,"As large language models are increasingly integrated into education, virtual
student agents are becoming vital for classroom simulation and teacher
training. Yet their classroom-oriented subjective abilities remain largely
unassessed, limiting understanding of model boundaries and hindering
trustworthy deployment. We present EduPersona, a large-scale benchmark spanning
two languages, three subjects, and ten persona types based on the Big Five
theory. The dataset contains 1,308 authentic classroom dialogue rounds,
corresponding to 12,814 teacher-student Q&A turns, and is further expanded
through persona stylization into roughly 10 times larger scale (128k turns),
providing a solid foundation for evaluation. Building on this resource, we
decompose hard-to-quantify subjective performance into three progressive tasks:
TASK1 basic coherence (whether behavior, emotion, expression, and voice align
with classroom context), TASK2 student realism, and TASK3 long-term persona
consistency, thereby establishing an evaluation framework grounded in
educational theory and research value. We conduct systematic experiments on
three representative LLMs, comparing their original versions with ten
persona-fine-tuned variants trained on EduPersona. Results show consistent and
significant average improvements across all tasks: TASK1 +33.6%, TASK2 +30.6%,
and TASK3 +14.9%. These improvements highlight the dataset's effectiveness and
research value, while also revealing the heterogeneous difficulty of persona
modeling. In summary, EduPersona delivers the first classroom benchmark
centered on subjective abilities, establishes a decoupled and verifiable
research paradigm, and we will open-source both the dataset and the framework
to support the broader research community in advancing trustworthy and
human-like AI for education.","\label{sec:related-work}

We review prior research through the lens of our three evaluation tasks and highlight why systematic modeling of virtual student agents requires going beyond existing approaches.  
(1) \textbf{Basic Behavioral Coherence:}  
Existing educational datasets (e.g., ScienceQA~\citep{lu2022learn}, C-Eval~\citep{huang2023c}, SocraticQ~\citep{ang2023socratic}, MathQA~\citep{amini2019mathqa}) have advanced knowledge assessment but remain largely single-turn or exam-oriented, lacking modeling of the IRF (Initiation–Response–Feedback) structure central to classrooms. Recent multimodal efforts explore VQA~\citep{lee2025multimodality,xiao2025eduvqa}, emotion recognition~\citep{song2025emotional}, and engagement detection~\citep{xie2025msc}, yet they focus on perception rather than coherence across verbal and non-verbal dimensions. Task~1 (Sec.~\ref{subsec:task1}) addresses this gap.
(2) \textbf{Student Realism:}  
Persona-driven dialogue studies such as PersonaChat~\citep{zhang2018personalizing}, PersonalDialog~\citep{zheng2019personalized}, and MBTI-based generation~\citep{kar2025convergence} illustrate role-conditioned generation, but they rely on simplified tags and are situated in open-domain settings. They cannot answer the classroom-specific question: does a model’s response resemble that of a real student? Task~2 (Sec.~\ref{subsec:task2}) formalizes this evaluation.
(3) \textbf{Persona Consistency:}  
Maintaining stable traits over long interactions remains challenging. Traditional metrics (BLEU, ROUGE) correlate poorly with persona preservation, and alignment methods (RLHF~\citep{ouyang2022training}, Constitutional AI~\citep{bai2022constitutional}) or bias detection~\citep{chen2024persona} provide only partial insights. Systematic evaluation of persona stability in classroom dialogue is still absent, which Task~3 (Sec.~\ref{subsec:task3}) directly operationalizes.
Overall, while prior work has progressed in knowledge testing, role-conditioned generation, and multimodal analytics, it lacks a unified, pedagogically grounded framework for jointly evaluating \textit{basic coherence}, \textit{student realism}, and \textit{persona consistency}. EduPersona is designed to fill this gap. 

\begin{figure*}[t!]
% \vspace{-45pt}
\centering
\includegraphics[width=\textwidth]{image/example.pdf}
% \vspace{-20pt}
\caption{\textbf{Chinese classroom example with persona-conditioned responses.} The top panel shows a real IRF snippet (with English translation), and the bottom presents virtual-student outputs under high/low extraversion with behavior–expression labels. This illustrates the EduPersona pipeline (raw dialogue $\rightarrow$ persona stylization $\rightarrow$ behavior–expression labeling) and highlights how different personas yield linguistic and non-verbal differences within the same teaching context.}
  \label{fig:example}
% \vspace{-12pt}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%","We review prior research through the lens of our three evaluation tasks and highlight why systematic modeling of virtual student agents requires going beyond existing approaches.  
(1) \textbf{Basic Behavioral Coherence:}  
Existing educational datasets (e.g., ScienceQA~\citep{lu2022learn}, C-Eval~\citep{huang2023c}, SocraticQ~\citep{ang2023socratic}, MathQA~\citep{amini2019mathqa}) have advanced knowledge assessment but remain largely single-turn or exam-oriented, lacking modeling of the IRF (Initiation–Response–Feedback) structure central to classrooms. Recent multimodal efforts explore VQA~\citep{lee2025multimodality,xiao2025eduvqa}, emotion recognition~\citep{song2025emotional}, and engagement detection~\citep{xie2025msc}, yet they focus on perception rather than coherence across verbal and non-verbal dimensions. Task~1 (Sec.~\ref{subsec:task1}) addresses this gap.
(2) \textbf{Student Realism:}  
Persona-driven dialogue studies such as PersonaChat~\citep{zhang2018personalizing}, PersonalDialog~\citep{zheng2019personalized}, and MBTI-based generation~\citep{kar2025convergence} illustrate role-conditioned generation, but they rely on simplified tags and are situated in open-domain settings. They cannot answer the classroom-specific question: does a model’s response resemble that of a real student? Task~2 (Sec.~\ref{subsec:task2}) formalizes this evaluation.
(3) \textbf{Persona Consistency:}  
Maintaining stable traits over long interactions remains challenging. Traditional metrics (BLEU, ROUGE) correlate poorly with persona preservation, and alignment methods (RLHF~\citep{ouyang2022training}, Constitutional AI~\citep{bai2022constitutional}) or bias detection~\citep{chen2024persona} provide only partial insights. Systematic evaluation of persona stability in classroom dialogue is still absent, which Task~3 (Sec.~\ref{subsec:task3}) directly operationalizes.
Overall, while prior work has progressed in knowledge testing, role-conditioned generation, and multimodal analytics, it lacks a unified, pedagogically grounded framework for jointly evaluating \textit{basic coherence}, \textit{student realism}, and \textit{persona consistency}. EduPersona is designed to fill this gap.",N/A
