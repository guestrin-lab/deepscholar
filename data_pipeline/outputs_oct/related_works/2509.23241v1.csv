arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2509.23241v1,http://arxiv.org/abs/2509.23241v1,2025-09-27 10:44:38+00:00,Memory Efficient and Staleness Free Pipeline Parallel DNN Training Framework with Improved Convergence Speed,"High resource requirement for Deep Neural Network (DNN) training across
multiple GPUs necessitates development of various parallelism techniques. In
this paper, we introduce two interconnected DNN training frameworks, namely,
V-TiMePReSt and I-TiMePReSt, based on pipeline parallelism, a variant of model
parallelism. V-TiMePReSt is a completely staleness-free system which enables
the DNNs to be trained on the latest updated weights in each stage of all
forward and backward passes. Developing staleness-aware systems at the expense
of weight stashing reduces GPU-memory consumption, however, increases the
number of epochs to converge. Thus, we introduce I-TiMePReSt, which is also a
staleness-aware system, but not at the expense of weight stashing. It does not
rely solely on the stale weights or the latest updated weights. I-TiMePReSt
computes an intermediate weight towards the latter and performs backward pass
on it. Additionally, we formulate the significance of the stale weights
mathematically depending on the degree of staleness. In contrast to
V-TiMePReSt, I-TiMePReSt works based on the assumption that stale weights have
a significant contribution in training, which can be quantified mathematically
based on the degree of staleness, although there are other contributory factors
which should not be ignored. Experimental results show that V-TiMePReSt is
advantageous over existing models in terms of $1)$ the extent of staleness of
the weight parameter values and $2)$ GPU memory efficiency, while I-TiMePReSt
is superior in terms of $1)$ removing staleness of the weight parameters
without removing weight stashing and $2)$ maintaining the trade-off between GPU
memory consumption and convergence speed (number of epochs).","% \label{sec:Related Works}
Two commonly used pipeline parallelism frameworks for distributed large-scale deep learning are PipeDream \cite{narayanan2019pipedream} and GPipe \cite{huang2019gpipe}. In order to achieve improved pipeline throughput, PipeDream integrates intra-batch and inter-batch parallelism. The integration enables the overlap of computation and communication, as well as reduced communication. It retains the same weight versions throughout the forward and backward passes of a mini-batch, as well as all the stages in a pass, leveraging both vertical and horizontal weight stashing. The need to maintain weight consistency increases the memory footprint for each stage of DNN training on a mini-batch. 

A backward pass begins as soon as a forward pass concludes, using the same set of workers in reverse order. This bidirectional training of DNNs is devised in PipeDream. PipeDream introduces a scheduling strategy, named as $1$F$1$B ($1$ Forward $1$ Backward) scheduling, which is capable of avoiding circular waiting between forward and backward passes. PipeDream is an asynchronous approach, sending the output activations of each stage in a forward pass asynchronously to the next stage. Simultaneously, another mini-batch starts being processed during this communication. Similarly, the gradients, computed in each stage of a backward pass, are sent asynchronously to the previous stage, while another mini-batch starts processing. PipeDream leverages both pipeline and data parallelism together. Thus, the system allows multiple mini-batches as input for parallel training. This asynchronous communication of activations and gradients results in significant overlap of computation and communication as they operate on different mini-batches.
% Additionally, PipeDream is extended to automatically find an optimal partition of the DNN across the workers. Figure \ref{fig:PipeDream} is an example of pipeline parallel training on four worker machines using PipeDream.

% PipeDream follows the weight update rule given by%in Equation \ref{eqn:pipedream}.
% \begin{equation}\label{eqn:pipedream}
%     \resizebox{0.44\textwidth}{!}{$\textbf{W}(t+1)=\textbf{W}(t) - \eta \cdot \nabla f\Bigl(\mathbf{W}_1(t-n+1), \mathbf{W}_2(t-n+1), \ldots, \mathbf{W}_n(t-n+1)\Bigr)$}
% \end{equation}
% We denote $\mathbf{W}_l(t)$ as weights of the layers in $l^{th} (l = 1,2,...,n)$ stage after $t$ mini-batches, $\eta$ as the learning rate, $\nabla f\Bigl(\mathbf{W}_1(t), \mathbf{W}_2(t), \ldots, \mathbf{W}_n(t)\Bigr)$ as the gradient computed over all samples in the mini-batch, $f$ as the loss function and $\mathbf{W}(t)$ as the weights of the entire network (rather than a stage) after $t$ mini-batches. Equation \ref{eqn:pipedream} indicates the gradient calculation on stashed weights $\mathbf{W}_l(t-n+1)$, resulting staleness of weights, although the required weight updation is performed on weights different from those used for gradient computation. 


GPipe introduces a novel batch splitting mechanism where each mini-batch is splitted in micro-batches. The underlying pipeline algorithm is utilized to process the micro-batches of a mini-batch in parallel. In contrast to PipeDream, GPipe is synchronous implementing uni-directional training of DNNs, and it performs synchronous mini-batch gradient descent for DNN training, where the computed gradients are gathered across all micro-batches in a mini-batch and the updates are applied on the weights at the end of a mini-batch. 
% More precisely, backward passes begin only when no more mini-batch is yet to finish its forward pass. 
The scheduling policy of GPipe limits its applicability merely to networks that can be expressed as a sequence of layers. Both GPipe and PipeDream exploit pipeline and data parallelism together. In contrast to PipeDream, periodic pipeline flushes are performed in GPipe after each mini-batch, in order to maintain single weight version at a time, making GPipe more memory efficient. There exists a single version of weight parameters for all the micro-batches in a mini-batch. 
% However, it pipelines the executions of micro-batches rather than the mini-batches. 
% The lack of parallelism in mini-batch level, the underlying micro-batch scheduling policy hinder it to exploit the pipeline parallelism in a complete manner, resulting in reduced pipeline throughput. Frequent pipeline flushes also reduce pipeline throughput. Due to the absence of a better partitioning algorithm, GPipe suffers from imbalanced memory requirements and computation flops at different stages of a DNN.

There are two memory-efficient versions of PipeDream, namely PipeDream-2BW \cite{narayanan2021memory} and PipeDream-Flush \cite{narayanan2021memory}. PipeDream-2BW is based on a double buffered weight updates (2BW) technique, which reduces the memory footprint by reducing the number of active weight versions to two - one for already in-flight micro-batches (also known as shadow version) and the other for newly admitted micro-batches. In contrast to GPipe, it increases pipeline throughput by avoiding pipeline flushes. 
% It computes the gradient for each micro-batch, stores it in a 'coalesced' gradient, and performs a single weight update for a set of inputs rather than individual update for all, resulting in a constant weight delay of unity. 
PipeDream-Flush achieves a smaller memory footprint than PipeDream-2BW by performing periodic pipeline flush at the cost of pipeline throughput. It maintains a single weight version at a time, which reduces the memory footprint. Both of them follows 1F1B scheduling, similar to PipeDream.

vPipe \cite{zhao2021v} is a system that introduces dynamic layer partitioning and memory management mechanisms for pipeline parallelism by searching for a plan of near-optimal layer partitioning and memory management. vPipe acts as a virtual layer between pipeline parallel systems such as PipeDream, GPipe, and so forth, and their execution engines such as PyTorch, Tensorflow, and among others. 
% It automatically distributes the layers of a DNN among stages and moves the activations and parameters of a layer to the CPU or GPU memory of its current stage in order to maintain balanced partition of layers so that all stages can achieve approximately equal memory consumption and throughput. 
BPipe \cite{kim2023bpipe} is another approach in literature for achieving memory balance in pipeline parallelism. By using an activation balancing technique, BPipe allows all GPUs to use similar amounts of memory during training by transferring intermediate activations across them. 
% More precisely, BPipe introduces a transfer scheduling algorithm in order to reduce the number of transfers and preserving the computation correctness. It achieves so by designing a pair-adjacent stage assignment to make transfers so that the training time does not get affected. 
Hippie \cite{10.1145/3472456.3472497} is a hybrid parallel training framework integrating pipeline parallelism and data parallelism to increase the throughput and scalability of large DNN training by hiding gradient communication. In addition, it provides a last-stage pipeline scheduling and recomputation mechanism for specific layers. 
% to significantly decrease the memory overhead. It 
% ,and also proposes an index of memory efficiency to represent the tradeoff between throughput and memory overhead. 

PipePar \cite{zhang2023pipepar} is a model partitioning and task placement algorithm for pipeline parallel DNN training in heterogeneous GPU clusters. PipePar is based on dynamic programming with search space pruning that takes into consideration both the heterogeneity of GPUs and network bandwidth. 
% PipePar can profile the DNN model for each type of GPU, and conduct model partitioning and task placement based on given GPUs and network connections, which can optimize pipeline load balancing in heterogeneous environments and thus improve training efficiency. 
 Koala \cite{tangkoala} is an automatic, end-to-end and  globally optimal searching technique for an efficient scheduling strategy with optimal flexibility and training efficiency. 
% For pipeline schedule development it designs a novel domain-specific language and 
It facilitates solving the pipeline schedule development as a Binary-Tree-Traversing optimization problem.
% , and adopts a Dynamic Try-Test Genetic Algorithm to find out the best pipeline scheduling framework. 

Mario \cite{liu2025mario} is a pipeline optimizer technique that uniquely incorporates activation checkpointing into pipelines in order to achieve reduced and balanced memory footprint across devices. Additionally, it also consists of an automatic scheduler which can determine better parameter configurations. WeiPipe \cite{lin2025weipipe}, also known as weight-pipeline parallelism, is a weight-passing pipeline technique, which decreases communication costs across devices,
% by transmitting only weights and their gradients in a pipelined manner. 
 and also ensures scalability 
% by disregarding collective communication primitives.

DualPipe \cite{liu2024deepseek} is a bidirectional pipeline parallelism framework first introduced in DeepSeek V3 Technical Report \cite{liu2024deepseek}. DualPipe eliminates pipeline bubbles through dual-channel processing enabling simultaneous occurrence of forward-backward computation-communication phases and complete synchronization between forward and backward passes. It achieves optimized resource utilization, reduces idle time between processing stages and reduces memory footprint across all the devices. It uses an adaptive task scheduling method based on computational demands. 
% The scheduling strategies include zero-bubble techniques and microbatching with automatic optimal size determination. DualPipe introduces a tensor management system in order to ensure efficient memory usage and data transfer. DualPipe is applicable to large-scale computer vision models as well as LLMs.    

MEPipe \cite{sun2025mepipe} introduces a slice-level scheduling method, named as Sequence Virtual Pipeline Parallelism (SVPP), for sequence pipeline parallelism. This method democratizes the training of LLMs to inexpensive accelerators with low-bandwidth interconnection by reducing memory footprint without increasing communication overhead across all devices. 
% MEPipe employs a fine-grained weight gradient computation to reduce idle time at the end of each iteration and avoid imbalanced computation among different slices. 
Zero Bubble \cite{qi2024zero} pipeline parallelism technique introduces a scheduling strategy that achieves almost zero pipeline bubbles under synchronous training mechanisms. In Zero Bubble, the backward computation is splitted into two parts - gradient computation for the input and that for the parameters. 
% It also enables the system to automatically find an optimal schedule based on specific model configuration and memory limit.     

GraphPipe \cite{jeon2025graphpipe} is another pipeline parallelism technique that enables the system to identify the dependencies between different pipeline stages of a DNN by a directed acyclic graph. In contrast to traditional sequential pipeline parallelism (SPP), GraphPipe enables concurrent execution of computationally independent components of a DNN.
% , which makes it more memory-efficient and helps it to achieve improved GPU performance compared to SPP. 
% It introduces a pipeline stage partitioner that automatically determines the partitioning of the operators and layers of a DNN into a graph of stages, maintaining minimum inter-stage communication. In addition, a static micro-batch scheduler is introduced, which schedules the forward and backward passes of different micro-batches within a mini-batch reducing peak GPU memory requirement. 

TiMePReSt (Time and Memory Efficient Pipeline Parallel DNN Training with Removed Staleness) \cite{dutta2024timeprest} is a memory-efficient pipeline parallelism based DNN training framework that addresses the above issue of stale weights. It has introduced an intra-batch scheduling technique, named as $n$F$1$B scheduling, that makes the framework more time-efficient.","Two commonly used pipeline parallelism frameworks for distributed large-scale deep learning are PipeDream \cite{narayanan2019pipedream} and GPipe \cite{huang2019gpipe}. In order to achieve improved pipeline throughput, PipeDream integrates intra-batch and inter-batch parallelism. The integration enables the overlap of computation and communication, as well as reduced communication. It retains the same weight versions throughout the forward and backward passes of a mini-batch, as well as all the stages in a pass, leveraging both vertical and horizontal weight stashing. The need to maintain weight consistency increases the memory footprint for each stage of DNN training on a mini-batch. 

A backward pass begins as soon as a forward pass concludes, using the same set of workers in reverse order. This bidirectional training of DNNs is devised in PipeDream. PipeDream introduces a scheduling strategy, named as $1$F$1$B ($1$ Forward $1$ Backward) scheduling, which is capable of avoiding circular waiting between forward and backward passes. PipeDream is an asynchronous approach, sending the output activations of each stage in a forward pass asynchronously to the next stage. Simultaneously, another mini-batch starts being processed during this communication. Similarly, the gradients, computed in each stage of a backward pass, are sent asynchronously to the previous stage, while another mini-batch starts processing. PipeDream leverages both pipeline and data parallelism together. Thus, the system allows multiple mini-batches as input for parallel training. This asynchronous communication of activations and gradients results in significant overlap of computation and communication as they operate on different mini-batches.









GPipe introduces a novel batch splitting mechanism where each mini-batch is splitted in micro-batches. The underlying pipeline algorithm is utilized to process the micro-batches of a mini-batch in parallel. In contrast to PipeDream, GPipe is synchronous implementing uni-directional training of DNNs, and it performs synchronous mini-batch gradient descent for DNN training, where the computed gradients are gathered across all micro-batches in a mini-batch and the updates are applied on the weights at the end of a mini-batch. 

The scheduling policy of GPipe limits its applicability merely to networks that can be expressed as a sequence of layers. Both GPipe and PipeDream exploit pipeline and data parallelism together. In contrast to PipeDream, periodic pipeline flushes are performed in GPipe after each mini-batch, in order to maintain single weight version at a time, making GPipe more memory efficient. There exists a single version of weight parameters for all the micro-batches in a mini-batch. 



There are two memory-efficient versions of PipeDream, namely PipeDream-2BW \cite{narayanan2021memory} and PipeDream-Flush \cite{narayanan2021memory}. PipeDream-2BW is based on a double buffered weight updates (2BW) technique, which reduces the memory footprint by reducing the number of active weight versions to two - one for already in-flight micro-batches (also known as shadow version) and the other for newly admitted micro-batches. In contrast to GPipe, it increases pipeline throughput by avoiding pipeline flushes. 

PipeDream-Flush achieves a smaller memory footprint than PipeDream-2BW by performing periodic pipeline flush at the cost of pipeline throughput. It maintains a single weight version at a time, which reduces the memory footprint. Both of them follows 1F1B scheduling, similar to PipeDream.

vPipe \cite{zhao2021v} is a system that introduces dynamic layer partitioning and memory management mechanisms for pipeline parallelism by searching for a plan of near-optimal layer partitioning and memory management. vPipe acts as a virtual layer between pipeline parallel systems such as PipeDream, GPipe, and so forth, and their execution engines such as PyTorch, Tensorflow, and among others. 

BPipe \cite{kim2023bpipe} is another approach in literature for achieving memory balance in pipeline parallelism. By using an activation balancing technique, BPipe allows all GPUs to use similar amounts of memory during training by transferring intermediate activations across them. 

Hippie \cite{10.1145/3472456.3472497} is a hybrid parallel training framework integrating pipeline parallelism and data parallelism to increase the throughput and scalability of large DNN training by hiding gradient communication. In addition, it provides a last-stage pipeline scheduling and recomputation mechanism for specific layers. 



PipePar \cite{zhang2023pipepar} is a model partitioning and task placement algorithm for pipeline parallel DNN training in heterogeneous GPU clusters. PipePar is based on dynamic programming with search space pruning that takes into consideration both the heterogeneity of GPUs and network bandwidth. 

 Koala \cite{tangkoala} is an automatic, end-to-end and  globally optimal searching technique for an efficient scheduling strategy with optimal flexibility and training efficiency. 

It facilitates solving the pipeline schedule development as a Binary-Tree-Traversing optimization problem.


Mario \cite{liu2025mario} is a pipeline optimizer technique that uniquely incorporates activation checkpointing into pipelines in order to achieve reduced and balanced memory footprint across devices. Additionally, it also consists of an automatic scheduler which can determine better parameter configurations. WeiPipe \cite{lin2025weipipe}, also known as weight-pipeline parallelism, is a weight-passing pipeline technique, which decreases communication costs across devices,

 and also ensures scalability 


DualPipe \cite{liu2024deepseek} is a bidirectional pipeline parallelism framework first introduced in DeepSeek V3 Technical Report \cite{liu2024deepseek}. DualPipe eliminates pipeline bubbles through dual-channel processing enabling simultaneous occurrence of forward-backward computation-communication phases and complete synchronization between forward and backward passes. It achieves optimized resource utilization, reduces idle time between processing stages and reduces memory footprint across all the devices. It uses an adaptive task scheduling method based on computational demands. 


MEPipe \cite{sun2025mepipe} introduces a slice-level scheduling method, named as Sequence Virtual Pipeline Parallelism (SVPP), for sequence pipeline parallelism. This method democratizes the training of LLMs to inexpensive accelerators with low-bandwidth interconnection by reducing memory footprint without increasing communication overhead across all devices. 

Zero Bubble \cite{qi2024zero} pipeline parallelism technique introduces a scheduling strategy that achieves almost zero pipeline bubbles under synchronous training mechanisms. In Zero Bubble, the backward computation is splitted into two parts - gradient computation for the input and that for the parameters. 


GraphPipe \cite{jeon2025graphpipe} is another pipeline parallelism technique that enables the system to identify the dependencies between different pipeline stages of a DNN by a directed acyclic graph. In contrast to traditional sequential pipeline parallelism (SPP), GraphPipe enables concurrent execution of computationally independent components of a DNN.



TiMePReSt (Time and Memory Efficient Pipeline Parallel DNN Training with Removed Staleness) \cite{dutta2024timeprest} is a memory-efficient pipeline parallelism based DNN training framework that addresses the above issue of stale weights. It has introduced an intra-batch scheduling technique, named as $n$F$1$B scheduling, that makes the framework more time-efficient.",N/A
