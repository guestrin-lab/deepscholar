arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2509.16518v1,http://arxiv.org/abs/2509.16518v1,2025-09-20 03:48:32+00:00,FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers,"Generating realistic videos with diffusion transformers demands significant
computation, with attention layers the central bottleneck; even producing a
short clip requires running a transformer over a very long sequence of
embeddings, e.g., more than 30K embeddings for a 5-second video, incurring
significant latency. Prior work aims to mitigate this bottleneck by exploiting
sparsity in the attention layers to reduce computation. However, these works
typically rely on block-sparse attention, which skips score computation only
when all entries in a block of attention scores (corresponding to M queries and
M keys, with M = 64 typically) are zero. This coarse-granular skipping of
attention scores does not fully exploit sparsity in the attention map and
leaves room for improvement. In this work, we propose FG-Attn, a sparse
attention mechanism for long-context diffusion transformers that leverages
sparsity at a fine granularity. Unlike block-sparse attention, which skips
entire MxM blocks, our approach skips computations at the granularity of Mx1
slices of the attention map. Each slice is produced by query-key dot products
between a block of query vectors and a single key. To implement our proposed
sparse attention mechanism, we develop a new efficient bulk-load operation
called asynchronous-gather load. This load operation gathers a sparse set of
relevant key-value vectors from memory and arranges them into packed tiles in
the GPU's shared memory. Only a sparse set of keys relevant to those queries
are loaded into shared memory when computing attention for a block of queries,
in contrast to loading full blocks of key tokens in block-sparse attention. Our
fine-grained sparse attention, applied to video diffusion models, achieves an
average 1.55X (up to 1.65X) speedup for 5 second, 480p videos, and an average
1.41X (up to 1.49X) for 5 second, 720p videos on a single H100 GPU.","\label{sec:related_work}


\textbf{Block sparse attention.} 
Several implementations of block sparse attention~\cite{bsa, flashattn, flexattn, flashinfer, flashmask} propose a coarse-grained sparse attention mechanism that skips entire blocks of attention score computations at granularity of $64\times 64$ or $128\times 128$ at half-precision. Current block-sparse attention mechanisms either prevent further reduction of block size (do not compile) or cause significant hardware underutilization and performance overhead, since they are constrained by the tensor core matrix multiplication width (\cref{sec:motivation_skip_attn_fine_granularity}). Several works in the large language model literature~\cite{minference, xattn, flashdecode, nsa, seerattn} utilize block sparse attention to accelerate attention computation. 
% Several works in large language model literature ~\cite{minference, xattn, flashdecode, nsa, seerattn} use block sparse attention to speedup attention computation.

\textbf{Block sparse attention for videoDiTs.} 
Recent works, such as Radial Attention~\cite{radialattn}, X-attention~\cite{xattn}, SparseVideoGen~\cite{sparsevideogen}, and SparseVideoGen2~\cite{sparsevideogen2}, have applied block sparse attention implementations to video diffusion models. These approaches consider a fixed sparsity pattern in the attention map based on empirical observations of significant patterns. Other works, such as Video Sparse Attention~\cite{vsa}, incorporate learned sparse attention patterns by using a parameterized model to derive the attention map mask. Both approaches utilize coarse-grained sparse attention mechanisms. In contrast, our method enables fine-grained skipping of attention blocks, providing more opportunities for skipping computation. We compare \X with SparseVideoGen and Radial Attention in~\cref{sec:results}. 
Moreover, trainable sparse attention methods such as Video Sparse Attention (VSA)~\cite{vsa} can be reformulated to generate sparse masks compatible with \X \textquotesingle s attention kernel. These methods are orthogonal to \X \textquotesingle s kernel implementation and can be used in conjunction as mask-determination strategies for \X.

% While trainable sparse attention methods can outperform training-free approaches, these methods could benefit from \X's fine-grained approach.
% Recent works such as Radial attention~\cite{radialattn}, X-attention~\cite{xattn}, SparseVideoGen~\cite{sparsevideogen}, SparseVideoGen2~\cite{sparsevideogen2} apply block sparse attention implementations for video diffusion models. They consider a fixed sparsity pattern in the attention map based on empirical observation of significant. Other works such as video sparse attention~\cite{vsa} incorporate learnt sparse attention patterns. These works use a parameterized model to derive the mask of the attention maps. Both of these works make use of coarse grain sparse attention mechanisms. Our approach allows fine-grain skipping of attention blocks enabling more opportunity to skip computation. We compare \X with SVG, radial attention in~\cref{sec:results}. Trainable sparse attention methods can make  of coarse grain sparse attention methods, and could potentially benefit from \X. 

\textbf{Other techniques to accelerate video diffusion.}
SpargeAttention~\cite{spargeattn}, SageAttention~\cite{sageattn}, and SageAttention2~\cite{sageattn2} propose general attention approximation techniques, such as quantization and token compression mechanisms, that can be applied during inference for both LLM and DiT models. Token compression-based approaches may skip essential tokens relevant to the video, which could lead to inconsistent video generation (pointed out by~\cite{radialattn}). These approaches are orthogonal to our \X.% SpargeAttention~\cite{spargeattn}, SageAttention~\cite{sageattn}, SageAttention2~\cite{sageattn2}, propose general attention approximation techniques such as quantization and token compression mechanisms that can be applied at inference time to LLM and DiT models. Approaches based on token compression skip essential tokens relevant to the video and may produce inconsistent video (\tofix{add reference here}). Quantization-based techniques are orthogonal to our approach (a lower-precision version of \X can be implemented).","\textbf{Block sparse attention.} 
Several implementations of block sparse attention~\cite{bsa, flashattn, flexattn, flashinfer, flashmask} propose a coarse-grained sparse attention mechanism that skips entire blocks of attention score computations at granularity of $64\times 64$ or $128\times 128$ at half-precision. Current block-sparse attention mechanisms either prevent further reduction of block size (do not compile) or cause significant hardware underutilization and performance overhead, since they are constrained by the tensor core matrix multiplication width (\cref{sec:motivation_skip_attn_fine_granularity}). Several works in the large language model literature~\cite{minference, xattn, flashdecode, nsa, seerattn} utilize block sparse attention to accelerate attention computation. 


\textbf{Block sparse attention for videoDiTs.} 
Recent works, such as Radial Attention~\cite{radialattn}, X-attention~\cite{xattn}, SparseVideoGen~\cite{sparsevideogen}, and SparseVideoGen2~\cite{sparsevideogen2}, have applied block sparse attention implementations to video diffusion models. These approaches consider a fixed sparsity pattern in the attention map based on empirical observations of significant patterns. Other works, such as Video Sparse Attention~\cite{vsa}, incorporate learned sparse attention patterns by using a parameterized model to derive the attention map mask. Both approaches utilize coarse-grained sparse attention mechanisms. In contrast, our method enables fine-grained skipping of attention blocks, providing more opportunities for skipping computation. We compare \X with SparseVideoGen and Radial Attention in~\cref{sec:results}. 
Moreover, trainable sparse attention methods such as Video Sparse Attention (VSA)~\cite{vsa} can be reformulated to generate sparse masks compatible with \X \textquotesingle s attention kernel. These methods are orthogonal to \X \textquotesingle s kernel implementation and can be used in conjunction as mask-determination strategies for \X.




\textbf{Other techniques to accelerate video diffusion.}
SpargeAttention~\cite{spargeattn}, SageAttention~\cite{sageattn}, and SageAttention2~\cite{sageattn2} propose general attention approximation techniques, such as quantization and token compression mechanisms, that can be applied during inference for both LLM and DiT models. Token compression-based approaches may skip essential tokens relevant to the video, which could lead to inconsistent video generation (pointed out by~\cite{radialattn}). These approaches are orthogonal to our \X.",N/A
