arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2510.00956v1,http://arxiv.org/abs/2510.00956v1,2025-10-01 14:29:47+00:00,Bridging the Gap Between Simulated and Real Network Data Using Transfer Learning,"Machine Learning (ML)-based network models provide fast and accurate
predictions for complex network behaviors but require substantial training
data. Collecting such data from real networks is often costly and limited,
especially for critical scenarios like failures. As a result, researchers
commonly rely on simulated data, which reduces accuracy when models are
deployed in real environments. We propose a hybrid approach leveraging transfer
learning to combine simulated and real-world data. Using RouteNet-Fermi, we
show that fine-tuning a pre-trained model with a small real dataset
significantly improves performance. Our experiments with OMNeT++ and a custom
testbed reduce the Mean Absolute Percentage Error (MAPE) in packet delay
prediction by up to 88%. With just 10 real scenarios, MAPE drops by 37%, and
with 50 scenarios, by 48%.","\subsection{Transfer Learning}

Transfer learning enhances performance in a target task (receiver) by reusing knowledge from a related source task (donor) \cite{10.5555/2998687.2998769, pmlr-v15-bengio11b}. % This approach is particularly valuable when the target task has limited data, making it challenging to train complex models from scratch. It is especially effective when the source and target tasks involve similar domains (i.e., input data) or objectives (i.e., output predictions).
% In this paper, we employ transfer learning to improve performance metrics prediction by transferring knowledge from simulated to real-world network scenarios.
% One of the most common applications of transfer learning in neural networks (NNs) is fine-tuning~\cite{10.5555/2969033.2969197}. Fine-tuning involves reusing a pre-trained donor model by transferring some of its learned weights to initialize the receiver model. These weights encode knowledge from the donor's training process. Thus, they provide the receiver model with a strong starting point, potentially reducing training time and improving accuracy, especially when the target dataset is small.
In this paper, we specifically employ fine-tuning~\cite{10.5555/2969033.2969197} to transfer useful knowledge from simulated to real-world network scenarios. Fine-tuning is a particularly popular transfer learning technique for neural networks (NNs). It involves reusing a pre-trained donor model by transferring some of its learned weights to initialize the receiver model. These encode knowledge from the donor's training process, providing the new model with a strong starting point, potentially reducing training time and improving accuracy. The benefits are higher if the target dataset is small.


% In this paper we aim to use fine tuning to improve performance metrics prediction by transferring knowledge from simulated to real-world network scenarios.

% These weights, which encode knowledge from the donor's training process, provide the receiver model with a strong starting point, potentially reducing training time and improving accuracy, especially when the target dataset is small.

Neural network-based models are typically composed of one or more layers that perform progressively complex transformations of the input data. Lower layers often focus on extracting general features from the input (e.g., basic statistical patterns), while higher layers specialize in learning task-specific features or making predictions~\cite{zeiler2013visualizingunderstandingconvolutionalnetworks}. 
% With this in mind, and considering the similarity between the donor and receiver tasks, layers in the receiver model initialize their weights in one of three ways:
Taking into account this, and how similar donor and receiver tasks are, the receiver model initialize its weights in one of three ways:
\begin{enumerate}
    \item Reused and frozen weights: The weights are transferred from the donor model and remain fixed during training. It decreases the training's computational cost.
    \item Reused and adjustable weights: The weights are transferred from the donor model but are allowed to update during training —and thus adapt to better fit the target.
    \item Randomly initialized weights: The layer's weights are trained from scratch if there is no useful knowledge to transfer from the donor (i.e., task-specific layers).
\end{enumerate}

\subsection{RouteNet-Fermi}

RouteNet-Fermi~\cite{ferriolgalmés2022routenetfermi} is a state-of-the-art network performance model. Specifically, it uses a custom representation of the network along with a modified Message-Passing Neural Network architecture \cite{pmlr-v70-gilmer17a} that exploits the interactions between traffic flows and the underlying devices. As a result, it has proven to generate accurate predictions at a fraction of the computational cost of alternatives such as DES. Its design also makes it robust to infer under unseen topologies, including larger than those seen during the model's training. It was also evaluated with both simulated and real-world network data.
% We chose RouteNet-Fermi for its effectiveness as a network model and its data efficiency. Its ability to maintain high accuracy even with minimal training data is especially important given the scarcity of real-world network data.


% \textbf{TODO} Specify:
% - SOA in network modeling
% - Generalization capabilities
% - Also proved on real data
% - Quick training; allows for exhaustive evaluation later on
% Hyperparams uses ones from paper. Conclusions generalizable to other ML models too.","\subsection{Transfer Learning}

Transfer learning enhances performance in a target task (receiver) by reusing knowledge from a related source task (donor) \cite{10.5555/2998687.2998769, pmlr-v15-bengio11b}. 


In this paper, we specifically employ fine-tuning~\cite{10.5555/2969033.2969197} to transfer useful knowledge from simulated to real-world network scenarios. Fine-tuning is a particularly popular transfer learning technique for neural networks (NNs). It involves reusing a pre-trained donor model by transferring some of its learned weights to initialize the receiver model. These encode knowledge from the donor's training process, providing the new model with a strong starting point, potentially reducing training time and improving accuracy. The benefits are higher if the target dataset is small.






Neural network-based models are typically composed of one or more layers that perform progressively complex transformations of the input data. Lower layers often focus on extracting general features from the input (e.g., basic statistical patterns), while higher layers specialize in learning task-specific features or making predictions~\cite{zeiler2013visualizingunderstandingconvolutionalnetworks}. 

Taking into account this, and how similar donor and receiver tasks are, the receiver model initialize its weights in one of three ways:
\begin{enumerate}
    \item Reused and frozen weights: The weights are transferred from the donor model and remain fixed during training. It decreases the training's computational cost.
    \item Reused and adjustable weights: The weights are transferred from the donor model but are allowed to update during training —and thus adapt to better fit the target.
    \item Randomly initialized weights: The layer's weights are trained from scratch if there is no useful knowledge to transfer from the donor (i.e., task-specific layers).
\end{enumerate}

\subsection{RouteNet-Fermi}

RouteNet-Fermi~\cite{ferriolgalmés2022routenetfermi} is a state-of-the-art network performance model. Specifically, it uses a custom representation of the network along with a modified Message-Passing Neural Network architecture \cite{pmlr-v70-gilmer17a} that exploits the interactions between traffic flows and the underlying devices. As a result, it has proven to generate accurate predictions at a fraction of the computational cost of alternatives such as DES. Its design also makes it robust to infer under unseen topologies, including larger than those seen during the model's training. It was also evaluated with both simulated and real-world network data.",N/A
