arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2510.06842v1,http://arxiv.org/abs/2510.06842v1,2025-10-08 10:09:47+00:00,Continual Action Quality Assessment via Adaptive Manifold-Aligned Graph Regularization,"Action Quality Assessment (AQA) quantifies human actions in videos,
supporting applications in sports scoring, rehabilitation, and skill
evaluation. A major challenge lies in the non-stationary nature of quality
distributions in real-world scenarios, which limits the generalization ability
of conventional methods. We introduce Continual AQA (CAQA), which equips AQA
with Continual Learning (CL) capabilities to handle evolving distributions
while mitigating catastrophic forgetting. Although parameter-efficient
fine-tuning of pretrained models has shown promise in CL for image
classification, we find it insufficient for CAQA. Our empirical and theoretical
analyses reveal two insights: (i) Full-Parameter Fine-Tuning (FPFT) is
necessary for effective representation learning; yet (ii) uncontrolled FPFT
induces overfitting and feature manifold shift, thereby aggravating forgetting.
To address this, we propose Adaptive Manifold-Aligned Graph Regularization
(MAGR++), which couples backbone fine-tuning that stabilizes shallow layers
while adapting deeper ones with a two-step feature rectification pipeline: a
manifold projector to translate deviated historical features into the current
representation space, and a graph regularizer to align local and global
distributions. We construct four CAQA benchmarks from three datasets with
tailored evaluation protocols and strong baselines, enabling systematic
cross-dataset comparison. Extensive experiments show that MAGR++ achieves
state-of-the-art performance, with average correlation gains of 3.6% offline
and 12.2% online over the strongest baseline, confirming its robustness and
effectiveness. Our code is available at https://github.com/ZhouKanglei/MAGRPP.","\label{sec:related_work}

\myPara{Action Quality Assessment} 
AQA aims to automatically evaluate the objective execution quality of human actions, spanning numerous applications in sports scoring \cite{xu2022finediving,parmar2019action,pirsiavash2014assessing,xu2025quality,xu2024vision}, rehabilitation \cite{zhou2023video}, and skill assessment \cite{zia2016automated}. 
A major challenge is the scarcity of annotated labels \cite{zhou2024comprehensivesurveyactionquality}, since reliable quality scores demand domain expertise. To address this, most approaches \cite{pan2021adaptive,yu2021group,parmar2019and} leverage PTMs (e.g., I3D \cite{carreira2017quo}) to extract strong visual features and then regress quality scores either via direct regression \cite{zhou2023hierarchical} or contrastive regression \cite{ke2024two}. Ranking-based skill assessment methods \cite{doughty2019pros,doughty2018s} further alleviate annotation costs by comparing relative performance instead of relying on absolute scores. Another core difficulty lies in fine-grained temporal parsing, as PTMs are optimized for coarse action recognition while AQA demands temporal sensitivity. To this end, strategies such as continual pretraining \cite{dadashzadeh2024pecop}, regularization \cite{zhou2025phi,zhou2024cofinal}, and human-centric cues \cite{xu2024fineparser,xu2025human} have been proposed to enhance feature representations. 
Furthermore, non-stationary variations across tasks pose additional challenges for CAQA. While recent works attempt to mitigate this by freezing backbone features \cite{li2024continual}, such designs restrict adaptation capacity and often overlook skill variations within the same action, where subtle distribution shifts make fine-grained evaluation more difficult.
In this work, we address these challenges by designing a framework that both adapts to evolving task distributions and preserves discriminative skill-related cues, enabling AQA in realistic evolving scenarios.

\myPara{Continual Learning}
CL \cite{wang2023incorporating,wang2024comprehensive} enables models to acquire new knowledge from a stream of tasks without forgetting previous ones. This capability is particularly valuable in real-world applications such as robotics, surveillance, and other dynamic vision domains \cite{zhou2025adaptive}.
The main challenge is to avoid catastrophic forgetting of previously learned knowledge. Current efforts can be broadly divided into constraint-based and replay-based methods. Constraint-based methods such as SI~\cite{zenke2017continual}, EWC~\cite{james2017ewc}, and LwF~\cite{li2017learning} impose regularization to preserve old knowledge without storing past data, but often suffer from limited scalability. Replay-based methods, by contrast, achieve stronger retention via exemplar storage (e.g., MER~\cite{riemer2019learning}, DER++~\cite{buzzega2020dark}, TOPIC~\cite{tao2020few}, and GEM~\cite{kukleva2021generalized}), which raises memory and privacy concerns. More recently, feature replay has emerged as a lightweight and privacy-preserving alternative (e.g., SLCA~\cite{zhang2023slca,zhang2024slca++}, NC-FSCIL~\cite{yang2023neural}, FS-Aug~\cite{li2024continual}, and MAGR~\cite{zhou2024magr}). However, applying feature replay in domains like AQA is challenging due to significant domain gaps, and continual adaptation of the backbone often induces manifold shifts, misaligning old and new feature distributions. Motivated by these, our work adopts feature replay as the primary technical route of CAQA while introducing adaptive strategies to alleviate feature misalignment.","\myPara{Action Quality Assessment} 
AQA aims to automatically evaluate the objective execution quality of human actions, spanning numerous applications in sports scoring \cite{xu2022finediving,parmar2019action,pirsiavash2014assessing,xu2025quality,xu2024vision}, rehabilitation \cite{zhou2023video}, and skill assessment \cite{zia2016automated}. 
A major challenge is the scarcity of annotated labels \cite{zhou2024comprehensivesurveyactionquality}, since reliable quality scores demand domain expertise. To address this, most approaches \cite{pan2021adaptive,yu2021group,parmar2019and} leverage PTMs (e.g., I3D \cite{carreira2017quo}) to extract strong visual features and then regress quality scores either via direct regression \cite{zhou2023hierarchical} or contrastive regression \cite{ke2024two}. Ranking-based skill assessment methods \cite{doughty2019pros,doughty2018s} further alleviate annotation costs by comparing relative performance instead of relying on absolute scores. Another core difficulty lies in fine-grained temporal parsing, as PTMs are optimized for coarse action recognition while AQA demands temporal sensitivity. To this end, strategies such as continual pretraining \cite{dadashzadeh2024pecop}, regularization \cite{zhou2025phi,zhou2024cofinal}, and human-centric cues \cite{xu2024fineparser,xu2025human} have been proposed to enhance feature representations. 
Furthermore, non-stationary variations across tasks pose additional challenges for CAQA. While recent works attempt to mitigate this by freezing backbone features \cite{li2024continual}, such designs restrict adaptation capacity and often overlook skill variations within the same action, where subtle distribution shifts make fine-grained evaluation more difficult.
In this work, we address these challenges by designing a framework that both adapts to evolving task distributions and preserves discriminative skill-related cues, enabling AQA in realistic evolving scenarios.

\myPara{Continual Learning}
CL \cite{wang2023incorporating,wang2024comprehensive} enables models to acquire new knowledge from a stream of tasks without forgetting previous ones. This capability is particularly valuable in real-world applications such as robotics, surveillance, and other dynamic vision domains \cite{zhou2025adaptive}.
The main challenge is to avoid catastrophic forgetting of previously learned knowledge. Current efforts can be broadly divided into constraint-based and replay-based methods. Constraint-based methods such as SI~\cite{zenke2017continual}, EWC~\cite{james2017ewc}, and LwF~\cite{li2017learning} impose regularization to preserve old knowledge without storing past data, but often suffer from limited scalability. Replay-based methods, by contrast, achieve stronger retention via exemplar storage (e.g., MER~\cite{riemer2019learning}, DER++~\cite{buzzega2020dark}, TOPIC~\cite{tao2020few}, and GEM~\cite{kukleva2021generalized}), which raises memory and privacy concerns. More recently, feature replay has emerged as a lightweight and privacy-preserving alternative (e.g., SLCA~\cite{zhang2023slca,zhang2024slca++}, NC-FSCIL~\cite{yang2023neural}, FS-Aug~\cite{li2024continual}, and MAGR~\cite{zhou2024magr}). However, applying feature replay in domains like AQA is challenging due to significant domain gaps, and continual adaptation of the backbone often induces manifold shifts, misaligning old and new feature distributions. Motivated by these, our work adopts feature replay as the primary technical route of CAQA while introducing adaptive strategies to alleviate feature misalignment.",N/A
