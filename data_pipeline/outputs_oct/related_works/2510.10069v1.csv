arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2510.10069v1,http://arxiv.org/abs/2510.10069v1,2025-10-11 07:12:44+00:00,SyncLipMAE: Contrastive Masked Pretraining for Audio-Visual Talking-Face Representation,"We introduce SyncLipMAE, a self-supervised pretraining framework for
talking-face video that learns synchronization-aware and transferable facial
dynamics from unlabeled audio-visual streams. Our approach couples masked
visual modeling with cross-modal contrastive alignment and employs three
per-frame prompt tokens that explicitly encode the essential factors of a
talking-face frame - identity, vocal motion (speech-synchronized facial
dynamics), and ambient motion (audio-agnostic movements such as blinks and head
pose). The contrastive objective uses time-aligned vocal-motion and audio
tokens as positives and misaligned pairs as negatives, driving both modalities
into a shared embedding space and yielding token-level audio-visual stream
synchronization. After pretraining, the aligned audio tokens together with the
visual prompt tokens (identity, vocal motion, ambient motion) form a unified
interface for four disparate downstream settings: (i) audio-visual stream
synchronization; (ii) facial emotion and head/face action recognition; (iii)
visual speech recognition; and (iv) visual dubbing, for which we enable
indistinguishable audio- or video-driven control within a single model. Across
four task families that require distinct capabilities, SyncLipMAE achieves
state-of-the-art results, underscoring the effectiveness of
synchronization-aware, factorized self-supervised pretraining.","\paragraph{Audio--visual synchronisation and correspondence.}
A long line of work formulates A/V learning as correspondence or temporal alignment. ~\cite{l3net} learns generic audio–visual correspondence from unlabelled video, while ~\cite{avts} casts synchronisation as in-time vs.\ out-of-time discrimination within the same clip. For faces, ~\cite{syncnet} introduces a two-stream embedding for lip–audio alignment and lag estimation; recent transformers such as ~\cite{vocalist} improve robustness across speech/singing with longer-range context. 

\paragraph{Masked modelling and contrastive learning at scale.}
Contrastive pretraining aligns paired modalities at scale (e.g., CLIP ~\cite{clip}), while masked autoencoders (MAE~\cite{mae}/VideoMAE~\cite{videomae}) show that reconstructing heavily masked inputs yields strong visual/video representations. These paradigms are complementary and now standard building blocks for multimodal pretraining. 


\paragraph{Facial video representation pretraining.} Closer to our setting, MARLIN~\cite{marlin} applies masked autoencoding to facial videos, using facial-region-guided masking to learn a universal face encoder transferable to expression recognition, deepfake detection, etc. Subsequent variants adapt MAE-style pretraining to dynamic facial expression recognition under limited labels~\cite{mae_dfer}. While these works focus on reconstruction-only objectives within the facial domain, we couple MAE with audio–visual contrast to align speech-driven dynamics, and structure the representation to separate identity and motion factors.


\paragraph{Visual speech recognition and audio--visual ASR.}
From early end-to-end lipreading (LipNet) to large-scale self-supervised A/V pretraining (AV-HuBERT) and automated VSR/AVSR recipe design (Auto-AVSR~\cite{autoavsr}), results consistently show that robust visual streams improve recognition and benefit from pretraining on unlabelled A/V data.

\paragraph{Talking-face generation and lip-sync synthesis.}
Audio-driven talking-head synthesis has progressed from GAN-based pipelines with explicit sync critics (e.g., Wav2Lip~\cite{wav2lip}) to diffusion in latent/image space (e.g., LatentSync~\cite{latentsync}) and efficient diffusion heads (MuseTalk~\cite{musetalk}). In parallel, foundation video generators and unified conditional DiT~\cite{dit} frameworks (WAN~\cite{wan}; VACE~\cite{vace}) provide scalable backbones and conditioning interfaces (Video Condition Unit, context adapters) for editing or T2V, which our dubbing setup leverages.

\input{depds/tab_avsync_hallo3}","\paragraph{Audio--visual synchronisation and correspondence.}
A long line of work formulates A/V learning as correspondence or temporal alignment. ~\cite{l3net} learns generic audio–visual correspondence from unlabelled video, while ~\cite{avts} casts synchronisation as in-time vs.\ out-of-time discrimination within the same clip. For faces, ~\cite{syncnet} introduces a two-stream embedding for lip–audio alignment and lag estimation; recent transformers such as ~\cite{vocalist} improve robustness across speech/singing with longer-range context. 

\paragraph{Masked modelling and contrastive learning at scale.}
Contrastive pretraining aligns paired modalities at scale (e.g., CLIP ~\cite{clip}), while masked autoencoders (MAE~\cite{mae}/VideoMAE~\cite{videomae}) show that reconstructing heavily masked inputs yields strong visual/video representations. These paradigms are complementary and now standard building blocks for multimodal pretraining. 


\paragraph{Facial video representation pretraining.} Closer to our setting, MARLIN~\cite{marlin} applies masked autoencoding to facial videos, using facial-region-guided masking to learn a universal face encoder transferable to expression recognition, deepfake detection, etc. Subsequent variants adapt MAE-style pretraining to dynamic facial expression recognition under limited labels~\cite{mae_dfer}. While these works focus on reconstruction-only objectives within the facial domain, we couple MAE with audio–visual contrast to align speech-driven dynamics, and structure the representation to separate identity and motion factors.


\paragraph{Visual speech recognition and audio--visual ASR.}
From early end-to-end lipreading (LipNet) to large-scale self-supervised A/V pretraining (AV-HuBERT) and automated VSR/AVSR recipe design (Auto-AVSR~\cite{autoavsr}), results consistently show that robust visual streams improve recognition and benefit from pretraining on unlabelled A/V data.

\paragraph{Talking-face generation and lip-sync synthesis.}
Audio-driven talking-head synthesis has progressed from GAN-based pipelines with explicit sync critics (e.g., Wav2Lip~\cite{wav2lip}) to diffusion in latent/image space (e.g., LatentSync~\cite{latentsync}) and efficient diffusion heads (MuseTalk~\cite{musetalk}). In parallel, foundation video generators and unified conditional DiT~\cite{dit} frameworks (WAN~\cite{wan}; VACE~\cite{vace}) provide scalable backbones and conditioning interfaces (Video Condition Unit, context adapters) for editing or T2V, which our dubbing setup leverages.

\input{depds/tab_avsync_hallo3}",N/A
