arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2509.23673v1,http://arxiv.org/abs/2509.23673v1,2025-09-28 06:26:11+00:00,RCI: A Score for Evaluating Global and Local Reasoning in Multimodal Benchmarks,"Multimodal Large Language Models (MLLMs) have achieved impressive results on
vision-language benchmarks, yet it remains unclear whether these benchmarks
assess genuine global reasoning or allow success via localized visual cues.
Existing evaluation methods do not explicitly measure this distinction,
hindering effective dataset curation and real-world focused model development.
  We introduce Region Comprehension Index (RCI), the first model-based score to
directly quantify a dataset's reliance on global versus local visual
information. RCI systematically compares reference-model performance on image
patches versus full images, revealing if tasks require holistic image
understanding or can be solved with partial or localized visual cues.
  When applying RCI to 13 widely used multimodal benchmarks, we observed that
most of them favor localized reasoning and exhibit significant spatial biases,
indicating potential risks in real-world applications. RCI equips researchers &
practitioners with an actionable tool for diagnosing & mitigating these biases,
enabling the construction of datasets and benchmarks to foster the development
of robust, enterprise-ready multimodal systems.","\label{sec:related_work}

\subsection{Vision-Language Benchmarks}%[CAMERA READY VERSION] and Dataset Biases}
Vision-language benchmarks such as MS COCO~\cite{chen2015microsoft}, GQA~\cite{hudson2019gqanewdatasetrealworld}, TextVQA \& VizWiz have significantly advanced multimodal model development. However, recent works~\cite{geirhos2020shortcut, guan2024hallusionbenchadvanceddiagnosticsuite,paper3,paper4,whatsup2024} reveal critical limitations: many tasks can be effectively addressed by exploiting localized visual information, creating an \textit{illusion of progress}. For instance, models often leverage minimal contextual clues \& biased spatial distributions to achieve deceptively high benchmark scores~\cite{paper3,whatsup2024}. Benchmarks like SPEC~\cite{spec2024}, AMBER, BLINK, MVTamperBench \cite{agarwal-etal-2025-mvtamperbench}, \& What’s Up~\cite{whatsup2024} explicitly highlight these issues by isolating fine-grained spatial-temporal \& semantic reasoning tasks, uncovering significant model limitations. Such localized shortcuts undermine robustness, interpretability, \& generalization, %particularly 
impacting real-world applications like medical analysis \cite{pattnayak2025clinicalqa20multitask}, document analysis \cite{meghwani-etal-2025-hard,agarwal-etal-2025-fs}, \& autonomous navigation, where comprehensive visual reasoning is essential.
 % [CAMERA READY VERSION]However, these studies do not provide a unified approach for systematically quantifying global contextual reasoning capabilities across diverse tasks, emphasizing the need for a generalized evaluation framework.

\subsection{Spatial Reasoning and Dataset Quality Assessment}
Spatial reasoning remains a challenging yet essential capability for vision-language models~\cite{wu2024vspassessingdualchallenges,whatsup2024}. Recent research demonstrates widespread deficiencies in spatial relation comprehension, even among advanced models~\cite{whatsup2024}. For example, SPEC explicitly diagnoses model comprehension of spatial attributes, demonstrating near-random performance even for state-of-the-art MLLMs ~\cite{spec2024}. Similarly, \citet{zhaoarticle} highlight the importance of dataset quality, revealing substantial annotation issues that exacerbate spatial reasoning deficiencies. To address these shortcomings, some researchers have proposed visual prompting techniques, guiding model's attention explicitly through visual cues~\cite{yu2024attention}. %[CAMERA READY] Although these methods enhance task performance, they do not systematically quantify reasoning dependencies or evaluate spatial biases inherent in benchmarks.

\begin{figure*}[th!]
    \centering
    \includegraphics[width=\textwidth]{latex/images/gci.drawio.png}
    % \caption{Illustration of a sample image from the POPE benchmark, on how we compute the FIP (on the top) and MPP (on the bottom) for a single image. We then aggregate the predictions to compute the performance on the entire benchmark dataset.}
    \caption{Computation of Full Image Performance (FIP) and Maximum Patch Performance (MPP) on a sample from the POPE benchmark. FIP (top) evaluates model performance on the full image, while MPP (bottom) identifies the highest-performing patch. These are aggregated across the dataset to compute RCI.%, quantifying a benchmark’s reliance on global vs. localized reasoning.
}
    % Later we aggregate ( \(\sum_{i}\) ) them over the entire dataset for computation.}
    \label{fig:RCIn_framework}
    \vspace{-1.2em}
\end{figure*}

% \subsection{Evaluation Metrics for Vision-Language Tasks}
\subsection{Metrics for Vision-Language Tasks}
Traditional metrics like CIDEr, BLEU~\cite{papineni2002bleu}, and METEOR~\cite{banerjee2005meteor} primarily assess linguistic properties such as caption similarity and diversity, but fail to explicitly capture deeper reasoning capabilities or spatial dependencies. Metrics like CLIPScore and FID evaluate cross-modal alignment and image realism. % [CAMERA READY] but do not explicitly quantify spatial or reasoning biases inherent in datasets.

Recently, \citet{tao2024probingmultimodallargelanguage} probed multimodal models for global and local semantic representations, highlighting discrepancies in representation across model layers. Although insightful, this work does not quantify how datasets themselves structure or promote spatial reasoning explicitly.

% \subsection{Positioning Our Work}
\paragraph{Our Contribution}
Our research explicitly addresses these critical gaps by introducing RCI. Unlike previous approaches that focus on isolated evaluation dimensions or linguistic alignment, RCI systematically measures and reveals whether a dataset’s tasks fundamentally depend on integrating information across the entire image, or can be addressed using isolated regions. %RCI is a model-based dataset audit that complements standard accuracy-style metrics by exposing the type of visual information a dataset rewards (global vs. local). It is intended to guide benchmark curation and selection, not to declare benchmarks (in)valid.
% OR
%RCI systematically quantifies benchmark's reliance on global contextual reasoning by explicitly comparing reference-model's performance across individual image patches and full images.

% This approach offers a comprehensive evaluation framework, allowing benchmark designers and dataset curators to systematically detect, quantify, and mitigate spatial reasoning biases and promote genuinely robust multimodal reasoning capabilities.



% ACL Industry Track
%","\subsection{Vision-Language Benchmarks}
Vision-language benchmarks such as MS COCO~\cite{chen2015microsoft}, GQA~\cite{hudson2019gqanewdatasetrealworld}, TextVQA \& VizWiz have significantly advanced multimodal model development. However, recent works~\cite{geirhos2020shortcut, guan2024hallusionbenchadvanceddiagnosticsuite,paper3,paper4,whatsup2024} reveal critical limitations: many tasks can be effectively addressed by exploiting localized visual information, creating an \textit{illusion of progress}. For instance, models often leverage minimal contextual clues \& biased spatial distributions to achieve deceptively high benchmark scores~\cite{paper3,whatsup2024}. Benchmarks like SPEC~\cite{spec2024}, AMBER, BLINK, MVTamperBench \cite{agarwal-etal-2025-mvtamperbench}, \& What’s Up~\cite{whatsup2024} explicitly highlight these issues by isolating fine-grained spatial-temporal \& semantic reasoning tasks, uncovering significant model limitations. Such localized shortcuts undermine robustness, interpretability, \& generalization, 
impacting real-world applications like medical analysis \cite{pattnayak2025clinicalqa20multitask}, document analysis \cite{meghwani-etal-2025-hard,agarwal-etal-2025-fs}, \& autonomous navigation, where comprehensive visual reasoning is essential.
 

\subsection{Spatial Reasoning and Dataset Quality Assessment}
Spatial reasoning remains a challenging yet essential capability for vision-language models~\cite{wu2024vspassessingdualchallenges,whatsup2024}. Recent research demonstrates widespread deficiencies in spatial relation comprehension, even among advanced models~\cite{whatsup2024}. For example, SPEC explicitly diagnoses model comprehension of spatial attributes, demonstrating near-random performance even for state-of-the-art MLLMs ~\cite{spec2024}. Similarly, \citet{zhaoarticle} highlight the importance of dataset quality, revealing substantial annotation issues that exacerbate spatial reasoning deficiencies. To address these shortcomings, some researchers have proposed visual prompting techniques, guiding model's attention explicitly through visual cues~\cite{yu2024attention}. 




\subsection{Metrics for Vision-Language Tasks}
Traditional metrics like CIDEr, BLEU~\cite{papineni2002bleu}, and METEOR~\cite{banerjee2005meteor} primarily assess linguistic properties such as caption similarity and diversity, but fail to explicitly capture deeper reasoning capabilities or spatial dependencies. Metrics like CLIPScore and FID evaluate cross-modal alignment and image realism. 

Recently, \citet{tao2024probingmultimodallargelanguage} probed multimodal models for global and local semantic representations, highlighting discrepancies in representation across model layers. Although insightful, this work does not quantify how datasets themselves structure or promote spatial reasoning explicitly.


\paragraph{Our Contribution}
Our research explicitly addresses these critical gaps by introducing RCI. Unlike previous approaches that focus on isolated evaluation dimensions or linguistic alignment, RCI systematically measures and reveals whether a dataset’s tasks fundamentally depend on integrating information across the entire image, or can be addressed using isolated regions.","2.1 Vision-Language Benchmarks
Vision-language benchmarks such as MS
COCO (Chen et al., 2015), GQA (Hudson and
Manning, 2019), TextVQA & VizWiz have signifi-
cantly advanced multimodal model development.
However, recent works (Geirhos et al., 2020; Guan
et al., 2024; Woh et al., 2022; Huang et al., 2024;
Kamath et al., 2023) reveal critical limitations:
many tasks can be effectively addressed by
exploiting localized visual information, creating
anillusion of progress. For instance, models
often leverage minimal contextual clues & biased
spatial distributions to achieve deceptively high
benchmark scores (Woh et al., 2022; Kamath
et al., 2023). Benchmarks like SPEC (Peng
et al., 2024), AMBER, BLINK, MVTamperBench
(Agarwal et al., 2025a), & What’s Up (Kamath
et al., 2023) explicitly highlight these issues by
isolating fine-grained spatial-temporal & semantic
reasoning tasks, uncovering significant model
limitations. Such localized shortcuts undermine
robustness, interpretability, & generalization,
impacting real-world applications like medical
analysis (Pattnayak et al., 2025b), document analy-
sis (Meghwani et al., 2025; Agarwal et al., 2025b),
& autonomous navigation, where comprehensive
visual reasoning is essential.2.2 Spatial Reasoning and Dataset Quality
Assessment
Spatial reasoning remains a challenging yet es-
sential capability for vision-language models (Wu
et al., 2024; Kamath et al., 2023). Recent research
demonstrates widespread deficiencies in spatial re-
lation comprehension, even among advanced mod-
els (Kamath et al., 2023). For example, SPEC
explicitly diagnoses model comprehension of spa-
tial attributes, demonstrating near-random perfor-
mance even for state-of-the-art MLLMs (Peng
et al., 2024). Similarly, Zhao et al. (2023) high-
light the importance of dataset quality, revealing
substantial annotation issues that exacerbate spa-
tial reasoning deficiencies. To address these short-
comings, some researchers have proposed visual
prompting techniques, guiding model’s attention
explicitly through visual cues (Yu et al., 2024).
2.3 Metrics for Vision-Language Tasks
Traditional metrics like CIDEr, BLEU (Papineni
et al., 2002), and METEOR (Banerjee and Lavie,
2005) primarily assess linguistic properties such as
caption similarity and diversity, but fail to explic-
itly capture deeper reasoning capabilities or spatial
dependencies. Metrics like CLIPScore and FID
evaluate cross-modal alignment and image realism.
Recently, Tao et al. (2024) probed multimodal
models for global and local semantic representa-
tions, highlighting discrepancies in representation
across model layers. Although insightful, this work
does not quantify how datasets themselves struc-
ture or promote spatial reasoning explicitly.
Our ContributionOur research explicitly ad-
dresses these critical gaps by introducing RCI. Un-
like previous approaches that focus on isolated eval-
uation dimensions or linguistic alignment, RCI
systematically measures and reveals whether a
dataset’s tasks fundamentally depend on integrat-
ing information across the entire image, or can be
addressed using isolated regions."
