arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2510.06504v1,http://arxiv.org/abs/2510.06504v1,2025-10-07 22:41:23+00:00,Text2Interact: High-Fidelity and Diverse Text-to-Two-Person Interaction Generation,"Modeling human-human interactions from text remains challenging because it
requires not only realistic individual dynamics but also precise,
text-consistent spatiotemporal coupling between agents. Currently, progress is
hindered by 1) limited two-person training data, inadequate to capture the
diverse intricacies of two-person interactions; and 2) insufficiently
fine-grained text-to-interaction modeling, where language conditioning
collapses rich, structured prompts into a single sentence embedding. To address
these limitations, we propose our Text2Interact framework, designed to generate
realistic, text-aligned human-human interactions through a scalable
high-fidelity interaction data synthesizer and an effective spatiotemporal
coordination pipeline. First, we present InterCompose, a scalable
synthesis-by-composition pipeline that aligns LLM-generated interaction
descriptions with strong single-person motion priors. Given a prompt and a
motion for an agent, InterCompose retrieves candidate single-person motions,
trains a conditional reaction generator for another agent, and uses a neural
motion evaluator to filter weak or misaligned samples-expanding interaction
coverage without extra capture. Second, we propose InterActor, a
text-to-interaction model with word-level conditioning that preserves
token-level cues (initiation, response, contact ordering) and an adaptive
interaction loss that emphasizes contextually relevant inter-person joint
pairs, improving coupling and physical plausibility for fine-grained
interaction modeling. Extensive experiments show consistent gains in motion
diversity, fidelity, and generalization, including out-of-distribution
scenarios and user studies. We will release code and models to facilitate
reproducibility.","\subsection{Text-to-Human Motion Generation}
\vspace{-0.5mm}


Text-to-motion generation aims to synthesize human motion sequences from natural language descriptions~\citep{fan2024freemotion, tanke2023social, jeong2024multi, jiang2023motiongpt, guo2022generating, zhang2023generating, wan2024tlcontrol, lu2024scamo, guo2024momask}. Early methods such as Text2Action~\citep{ahn2018text2action} and Language2Pose~\citep{ahuja2019language2pose} utilized GANs and sequence-to-sequence architectures to map text to motion, laying foundational work in this area. Subsequent approaches leveraged variational autoencoders (VAEs) for probabilistic generation, including Guo et al.~\citep{guo2022generating} and TEMOS~\citep{petrovich2022temos}, which improved motion diversity and fluency. More recent advancements have focused on powerful generative models. Diffusion-based approaches such as MDM~\citep{tevet2022human} and latent diffusion via MLD~\citep{chen2023executing} significantly improved motion realism and sample efficiency. T2M-GPT~\citep{zhang2023generating} employed autoregressive transformers for fine-grained motion synthesis, while MoMask~\citep{guo2024momask} introduced generative masked transformers to enhance fidelity under the autoregressive paradigm. ReMoDiffuse~\citep{zhang2023remodiffuse} further enhanced generation quality by retrieving reference motions from a motion database. Parallel to improving generation quality, increasing attention has been given to controllable text-to-motion generation. Techniques have explored conditioning on spatial trajectories~\citep{shafir2023human, karunratanakul2023guided, wan2024tlcontrol, xie2023omnicontrol} and linguistic constraints~\citep{wan2024tlcontrol, huang2024controllable} to provide more precise control over generated outputs. Additionally, MotionCLIP~\citep{tevet2022motionclip} aligned motion and language embeddings in a shared space, enabling zero-shot text-to-motion generation. Despite stellar results in single-person motion generation, extending them to two-person interactions introduces additional challenges such as modeling inter-agent coordination and handling semantically richer text descriptions. Our work builds on these foundations by proposing a scalable framework that composes diverse and semantically aligned two-person interactions from single-person motion priors and language models.




\subsection{Human-Human Interaction Generation}
\vspace{-0.5mm}
Although some progress has been achieved in multi-human interaction modeling~\citep{fan2024freemotion, tanke2023social, jeong2024multi}, prior works on human interaction modeling have been mostly focused on the two-person interaction problem. A pioneer work, ComMDM~\citep{shafir2023human}, explores two-person motion generation by using a bridge network to compose the outputs of two single-person motion diffusion models~\citep{tevet2022human}. RIG~\citep{tanaka2023role} and InterGen~\citep{liang2024intergen} first trained dedicated networks to directly model two-person interaction. in2IN~\citep{ruiz2024in2in} explores the simultaneous use of individual and interaction descriptions to enhance textual alignment and generation quality. MoMat-MoGen~\citep{cai2024digital} proposes to enhance generation quality by retrieving from a motion database and a generative framework that models interactive behaviors between agents, considering personality, motivations, and interpersonal relationships. InterMask~\citep{javed2024intermask} utilizes the generative masked transformer architecture and spatial-temporal attention to enhance generation quality and text-motion alignment. TIMotion~\citep{wang2024temporal}, a contemporaneous work, proposes to model the human interaction sequence in a causal sequence, leveraging the temporal and causal properties of human motions. Although these methods have achieved impressive results, there remains significant possibilities of improvement due to their common flaw of limited training corpus and inadequate text modeling granularity. In this paper, we aim to tackle these two key issues with our generative interaction composition framework and fine-grained word-level conditioning module.","\subsection{Text-to-Human Motion Generation}
\vspace{-0.5mm}


Text-to-motion generation aims to synthesize human motion sequences from natural language descriptions~\citep{fan2024freemotion, tanke2023social, jeong2024multi, jiang2023motiongpt, guo2022generating, zhang2023generating, wan2024tlcontrol, lu2024scamo, guo2024momask}. Early methods such as Text2Action~\citep{ahn2018text2action} and Language2Pose~\citep{ahuja2019language2pose} utilized GANs and sequence-to-sequence architectures to map text to motion, laying foundational work in this area. Subsequent approaches leveraged variational autoencoders (VAEs) for probabilistic generation, including Guo et al.~\citep{guo2022generating} and TEMOS~\citep{petrovich2022temos}, which improved motion diversity and fluency. More recent advancements have focused on powerful generative models. Diffusion-based approaches such as MDM~\citep{tevet2022human} and latent diffusion via MLD~\citep{chen2023executing} significantly improved motion realism and sample efficiency. T2M-GPT~\citep{zhang2023generating} employed autoregressive transformers for fine-grained motion synthesis, while MoMask~\citep{guo2024momask} introduced generative masked transformers to enhance fidelity under the autoregressive paradigm. ReMoDiffuse~\citep{zhang2023remodiffuse} further enhanced generation quality by retrieving reference motions from a motion database. Parallel to improving generation quality, increasing attention has been given to controllable text-to-motion generation. Techniques have explored conditioning on spatial trajectories~\citep{shafir2023human, karunratanakul2023guided, wan2024tlcontrol, xie2023omnicontrol} and linguistic constraints~\citep{wan2024tlcontrol, huang2024controllable} to provide more precise control over generated outputs. Additionally, MotionCLIP~\citep{tevet2022motionclip} aligned motion and language embeddings in a shared space, enabling zero-shot text-to-motion generation. Despite stellar results in single-person motion generation, extending them to two-person interactions introduces additional challenges such as modeling inter-agent coordination and handling semantically richer text descriptions. Our work builds on these foundations by proposing a scalable framework that composes diverse and semantically aligned two-person interactions from single-person motion priors and language models.




\subsection{Human-Human Interaction Generation}
\vspace{-0.5mm}
Although some progress has been achieved in multi-human interaction modeling~\citep{fan2024freemotion, tanke2023social, jeong2024multi}, prior works on human interaction modeling have been mostly focused on the two-person interaction problem. A pioneer work, ComMDM~\citep{shafir2023human}, explores two-person motion generation by using a bridge network to compose the outputs of two single-person motion diffusion models~\citep{tevet2022human}. RIG~\citep{tanaka2023role} and InterGen~\citep{liang2024intergen} first trained dedicated networks to directly model two-person interaction. in2IN~\citep{ruiz2024in2in} explores the simultaneous use of individual and interaction descriptions to enhance textual alignment and generation quality. MoMat-MoGen~\citep{cai2024digital} proposes to enhance generation quality by retrieving from a motion database and a generative framework that models interactive behaviors between agents, considering personality, motivations, and interpersonal relationships. InterMask~\citep{javed2024intermask} utilizes the generative masked transformer architecture and spatial-temporal attention to enhance generation quality and text-motion alignment. TIMotion~\citep{wang2024temporal}, a contemporaneous work, proposes to model the human interaction sequence in a causal sequence, leveraging the temporal and causal properties of human motions. Although these methods have achieved impressive results, there remains significant possibilities of improvement due to their common flaw of limited training corpus and inadequate text modeling granularity. In this paper, we aim to tackle these two key issues with our generative interaction composition framework and fine-grained word-level conditioning module.",N/A
