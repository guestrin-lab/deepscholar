arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2509.17398v1,http://arxiv.org/abs/2509.17398v1,2025-09-22 06:57:46+00:00,Optimizing Split Federated Learning with Unstable Client Participation,"To enable training of large artificial intelligence (AI) models at the
network edge, split federated learning (SFL) has emerged as a promising
approach by distributing computation between edge devices and a server.
However, while unstable network environments pose significant challenges to
SFL, prior schemes often overlook such an effect by assuming perfect client
participation, rendering them impractical for real-world scenarios. In this
work, we develop an optimization framework for SFL with unstable client
participation. We theoretically derive the first convergence upper bound for
SFL with unstable client participation by considering activation uploading
failures, gradient downloading failures, and model aggregation failures. Based
on the theoretical results, we formulate a joint optimization problem for
client sampling and model splitting to minimize the upper bound. We then
develop an efficient solution approach to solve the problem optimally.
Extensive simulations on EMNIST and CIFAR-10 demonstrate the superiority of our
proposed framework compared to existing benchmarks.","\label{Rel_Work}
% \subsection{有没有工作分析过SFL/sl/fed中的不稳定的问题}
% \cite{yang2022robustsplitfederatedlearning}(sfl)
% \subsection{有没有工作分析过SFL/sl/fed中的client sampling问题}
% \cite{10547401}

\textbf{Convergence analysis of FL.}
FedAvg~\cite{mcmahan2017communication} is widely recognized as the first and the most commonly used FL algorithm. Several works have shown the convergence of FedAvg under different settings, e.g., IID setting~\cite{10.5555/3546258.3546471,NEURIPS2018_3ec27c2c}, non-IID setting~\cite{10292582,9261995} even with partial clients participation~\cite{cho2020clientselectionfederatedlearning}.
% Numerous studies in FL have conducted the convergence analysis\cite{10292582,9261995}.
% Unstable client participation presents a critical challenge in FL paradigms. 
% This instability is primarily driven by two factors: first, the communication environment’s instability caused by network congestion, bandwidth fluctuations, and signal attenuation; and second, the intentional dropout of clients for energy conservation or resource management purposes.
% Numerous studies in FL have addressed the unstable communication issue with various techniques, including local updating strategies \cite{9716792}, client sampling \cite{9491936},  sparsification \cite{10.1609/aaai.v37i7.25977}, and quantization\cite{10658402}.
% Soft-DSGD \cite{9716792} is a robust decentralized SGD algorithm that leverages partially received messages and adapts mixing weights based on communication link reliability to overcome the challenges of unstable communications in decentralized FL, while achieving the same asymptotic convergence rate as in perfectly reliable networks
Under IID conditions, Wang et al. \cite{10.5555/3546258.3546471} present a unified framework for communication‑efficient strategies and establish convergence guarantees that both reduce communication overhead and achieve fast error convergence. 
Moreover, Woodworth et al.~\cite{NEURIPS2018_3ec27c2c} introduce a graph‑based oracle model for parallel stochastic optimization and derive the optimal lower bound that depends only on the graph depth and size, thereby clarifying fundamental limits of communication–parallelism trade-offs.
For non-IID settings, Rodio et al.~\cite{10292582} analyze heterogeneous and temporally/spatially correlated client availability, demonstrating that correlation deteriorates FedAvg’s convergence rate if not handled properly.
Dinh et al.~\cite{ 9261995} propose FEDL under strong convexity and smoothness, establish linear convergence by controlling local inexactness and learning rate. They derive closed-form solutions jointly tuning FL hyperparameters to balance wall-clock convergence and device energy costs. Meanwhile, Cho et al. \cite{cho2020clientselectionfederatedlearning} study biased client selection under partial participation and show that prioritizing high-loss clients accelerates FedAvg’s convergence but may introduce a small bias tied to data heterogeneity.
% employ a finite-state Markov chain to analyze the spatiotemporal correlations in client availability, demonstrating that clients in the same time frame or geographical region tend to exhibit similar online and offline patterns, which in turn can slow down the convergence rates if not handled properly. 
% Besides, the work presented in \cite{9261995} analyzes the convergence of FL by enabling heterogeneous clients to update their local models with controlled accuracy. The system achieves a trade-off that is Pareto optimal among convergence speed, communication cost, and local computation accuracy.
% Unlike FL, SFL demonstrates significantly higher sensitivity to unstable client participation due to its continuous client-server communication requirements and high-dimensional data transmission volumes of intermediate representations \cite{10095067,shiranthika2023splitfedresiliencepacketloss}: 
% To overcome this issue, SFL in \cite{10095067} 
% mitigates the adverse effects of noisy communication links on feature and gradient transmission in SFL and ensures robust convergence under severe noise conditions by introducing a dynamic averaging strategy that modulates client weights based on local loss statistics. 
% Similarly, robust frameworks for U-shaped medical image networks adopt dynamic weight correction techniques \cite{yang2022robustsplitfederatedlearning} to counteract model drift caused by disparate data distributions and heterogeneous computing resources, leading to a more stable aggregated model. 
% Moreover, approaches such as Dynamic Federated Split Learning \cite{10547401} leverage resource-aware model partitioning combined with clustering based on neural network layer similarities to optimize training efficiency, reduce energy consumption, and maintain high accuracy in challenging environments. 
% Moreover, Shiranthika et al. \cite{shiranthika2023splitfedresiliencepacketloss}  study the impact of model split points on the packet loss resilience
% of SFL.
% \textcolor{blue}{Li et al. \cite{li2024core} investigate spatio-temporal dependencies and dynamic trends for SFL-based traffic prediction between clients and the server. By analyzing local feature extracted from global model, it minimizes data transmission and mitigates delay and network congestion.}
% demonstrate through empirical studies that deeper model splitting and advanced parameter aggregation strategies can significantly enhance SFL's resilience to packet loss. Totally, these contributions underscore the potential of tailored instability solutions in split federated learning to bridge the gap between unstable and stable training scenarios.
% However, none have provided convergence analysis nor tackled the challenges of strategic model splitting and dynamic client selection under unstable conditions. 
However, despite extensive convergence analyses in FL, existing FL convergence bounds do not directly apply to SFL due to architectural differences, as they often neglect SFL’s frequent communications and the impact of model splitting on convergence.


%Thus, a comprehensive theoretical framework for understanding the impact of insatiability on overall SFL training performance remains an open issue.

\textbf{Client sampling.} Client sampling is a critical design component in distributed machine learning across heterogeneous devices. In the existing FL literature, client sampling strategies primarily include uniform sampling \cite{mcmahan2017communication}, importance-aware sampling~\cite{chen2022optimalclientsamplingfederated,9904868,pmlr-v151-jee-cho22a}, clustering-based sampling~\cite{fraboni2021clustered,NEURIPS2024_7886b9ba}, resource-aware sampling \cite{10.1109/INFOCOM48880.2022.9796935}, and fairness-aware sampling \cite{9810502}. Uniform sampling refers to randomly selecting a fixed proportion of clients from the total pool during each training round. Despite its simplicity, uniform sampling does not account for system or data heterogeneity. Importance sampling mitigates the high variance inherent in stochastic optimization methods like stochastic gradient descent (SGD) by preferentially sampling clients with more ``important'' data. Specifically, these approaches assign higher sampling probabilities to clients whose contributions are quantified using local gradients (e.g., \cite{chen2022optimalclientsamplingfederated,9904868}) or local losses (e.g., \cite{pmlr-v151-jee-cho22a}). 
% \textcolor{blue}{On the other hand, resource-aware optimization-based methods address system-level challenges by focusing on the efficient allocation of resources. These approaches target issues such as CPU frequency allocation (e.g., \cite{8737464}), communication bandwidth distribution (e.g., \cite{9292468,9563947}), straggler-aware client scheduling (e.g., \cite{9355774,9563062}), parameter control (e.g., \cite{8664630}), and task offloading (e.g., \cite{9437529}).} 
Clustering-based sampling first groups clients according to the similarity of their data features or model updates and then selects representative clients from each group~\cite{fraboni2021clustered,NEURIPS2024_7886b9ba}. Resource-aware sampling takes into account the available system resources, such as communication and computational resources, when choosing participating clients  for each training round~\cite{10.1109/INFOCOM48880.2022.9796935,9810502}.  However, the above methods are tailored for traditional FL, which is inapplicable to SFL because SFL has different training stages as alluded earlier.

%Fraboni et al. \cite{fraboni2021clustered} demonstrate that by clustering clients prior to sampling, one can significantly reduce the variance in aggregated updates and improve overall convergence by ensuring that even clients with less ``important'' but unique data are appropriately represented in the global model. HiCS-FL \cite{NEURIPS2024_7886b9ba}, a hierarchical clustered sampling method for non-IID FL, uses output layer updates to estimate client data heterogeneity via an entropy-like measure, and then clusters clients and preferentially selects those with more balanced data, leading to faster convergence and improved model performance with minimal extra overhead.

%For instance, Luo et al.~\cite{10.1109/INFOCOM48880.2022.9796935} derive a convergence bound for resource-aware client sampling, formulate a non-convex optimization problem, and propose an adaptive sampling algorithm that minimizes wall-clock time. Fairness-aware sampling (e.g., Eiffel \cite{9810502}) ensures that clients with different data distributions or lower participation frequencies are adequately represented in the global model update via considering local model loss, data size, computation power, resource demand, and even the age of the last update.


% \textcolor{blue}{The server aggregates every round, while clients perform full local aggregations only every few rounds, resulting in asynchronous updates, which leads to stale information and complicates the accurate measurement of client contributions, ultimately affecting the global model's convergence behavior.
% Moreover, SFL clients can encounter failures in activations uploading, gradient downloading, and model aggregation.}

\textbf{Model splitting.} Model splitting plays a critical role in SFL, especially in scenarios with unstable client participation. The selection of the split point influences both communication/computation latency and overall training performance~\cite{ZW2024ultra-LoLa,zw2025AIoutage}. Several studies have investigated how to determine the optimal model splitting point for SFL \cite{lin2024adaptsfl,10980018,10304624,shiranthika2023splitfedresiliencepacketloss}. 
Shiranthika et al. \cite{shiranthika2023splitfedresiliencepacketloss}  study the impact of model splitting strategies on the packet loss resilience
of SFL. 
Lin et al. \cite{lin2024adaptsfl,10980018} investigate how the model splitting point affects training convergence, revealing that a shallower split point leads to smaller convergence upper bound. Xu et al. \cite{10304624} propose a combined optimization approach to identify the best split point and bandwidth allocation, aiming to reduce total training latency. Unfortunately, these studies overlook how model splitting affects training convergence with unstable client participation. To the best of our knowledge, our research is the first to analyze convergence of SFL under unstable client participation and explore client sampling and model splitting under the context of SFL. 

% We propose a novel client sampling strategy that leverages rigorous convergence analysis to guide its design, ensuring more effective global model convergence even under unstable condition.





% \subsection{有没有工作分析过SFL/sl/fed中的client sampling/unstble/且有理论指导问题}","\textbf{Convergence analysis of FL.}
FedAvg~\cite{mcmahan2017communication} is widely recognized as the first and the most commonly used FL algorithm. Several works have shown the convergence of FedAvg under different settings, e.g., IID setting~\cite{10.5555/3546258.3546471,NEURIPS2018_3ec27c2c}, non-IID setting~\cite{10292582,9261995} even with partial clients participation~\cite{cho2020clientselectionfederatedlearning}.





Under IID conditions, Wang et al. \cite{10.5555/3546258.3546471} present a unified framework for communication‑efficient strategies and establish convergence guarantees that both reduce communication overhead and achieve fast error convergence. 
Moreover, Woodworth et al.~\cite{NEURIPS2018_3ec27c2c} introduce a graph‑based oracle model for parallel stochastic optimization and derive the optimal lower bound that depends only on the graph depth and size, thereby clarifying fundamental limits of communication–parallelism trade-offs.
For non-IID settings, Rodio et al.~\cite{10292582} analyze heterogeneous and temporally/spatially correlated client availability, demonstrating that correlation deteriorates FedAvg’s convergence rate if not handled properly.
Dinh et al.~\cite{ 9261995} propose FEDL under strong convexity and smoothness, establish linear convergence by controlling local inexactness and learning rate. They derive closed-form solutions jointly tuning FL hyperparameters to balance wall-clock convergence and device energy costs. Meanwhile, Cho et al. \cite{cho2020clientselectionfederatedlearning} study biased client selection under partial participation and show that prioritizing high-loss clients accelerates FedAvg’s convergence but may introduce a small bias tied to data heterogeneity.












However, despite extensive convergence analyses in FL, existing FL convergence bounds do not directly apply to SFL due to architectural differences, as they often neglect SFL’s frequent communications and the impact of model splitting on convergence.




\textbf{Client sampling.} Client sampling is a critical design component in distributed machine learning across heterogeneous devices. In the existing FL literature, client sampling strategies primarily include uniform sampling \cite{mcmahan2017communication}, importance-aware sampling~\cite{chen2022optimalclientsamplingfederated,9904868,pmlr-v151-jee-cho22a}, clustering-based sampling~\cite{fraboni2021clustered,NEURIPS2024_7886b9ba}, resource-aware sampling \cite{10.1109/INFOCOM48880.2022.9796935}, and fairness-aware sampling \cite{9810502}. Uniform sampling refers to randomly selecting a fixed proportion of clients from the total pool during each training round. Despite its simplicity, uniform sampling does not account for system or data heterogeneity. Importance sampling mitigates the high variance inherent in stochastic optimization methods like stochastic gradient descent (SGD) by preferentially sampling clients with more ``important'' data. Specifically, these approaches assign higher sampling probabilities to clients whose contributions are quantified using local gradients (e.g., \cite{chen2022optimalclientsamplingfederated,9904868}) or local losses (e.g., \cite{pmlr-v151-jee-cho22a}). 

Clustering-based sampling first groups clients according to the similarity of their data features or model updates and then selects representative clients from each group~\cite{fraboni2021clustered,NEURIPS2024_7886b9ba}. Resource-aware sampling takes into account the available system resources, such as communication and computational resources, when choosing participating clients  for each training round~\cite{10.1109/INFOCOM48880.2022.9796935,9810502}.  However, the above methods are tailored for traditional FL, which is inapplicable to SFL because SFL has different training stages as alluded earlier.









\textbf{Model splitting.} Model splitting plays a critical role in SFL, especially in scenarios with unstable client participation. The selection of the split point influences both communication/computation latency and overall training performance~\cite{ZW2024ultra-LoLa,zw2025AIoutage}. Several studies have investigated how to determine the optimal model splitting point for SFL \cite{lin2024adaptsfl,10980018,10304624,shiranthika2023splitfedresiliencepacketloss}. 
Shiranthika et al. \cite{shiranthika2023splitfedresiliencepacketloss}  study the impact of model splitting strategies on the packet loss resilience
of SFL. 
Lin et al. \cite{lin2024adaptsfl,10980018} investigate how the model splitting point affects training convergence, revealing that a shallower split point leads to smaller convergence upper bound. Xu et al. \cite{10304624} propose a combined optimization approach to identify the best split point and bandwidth allocation, aiming to reduce total training latency. Unfortunately, these studies overlook how model splitting affects training convergence with unstable client participation. To the best of our knowledge, our research is the first to analyze convergence of SFL under unstable client participation and explore client sampling and model splitting under the context of SFL.",N/A
