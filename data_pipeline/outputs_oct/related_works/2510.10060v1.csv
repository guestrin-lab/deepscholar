arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2510.10060v1,http://arxiv.org/abs/2510.10060v1,2025-10-11 06:54:10+00:00,Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling,"When modeling a given type of data, we consider it to involve two key
aspects: 1) identifying relevant elements (e.g., image pixels or textual words)
to a central element, as in a convolutional receptive field, or to a query
element, as in self-attention, and 2) encoding these tokens effectively.
Self-attention can adaptively identify these elements but relies on absolute
positional embedding for structural representation learning. In contrast,
convolution encodes elements in a relative manner, yet their fixed kernel size
limits their ability to adaptively select the relevant elements. In this paper,
we introduce Translution, an operation that unifies the adaptive identification
capability of self-attention and the relative encoding advantage of
convolution. However, this integration leads to a substantial increase in the
number of parameters, exceeding most currently available computational
resources. Therefore, we propose a lightweight variant of Translution, named
{\alpha}-Translution. Experiments on computer vision and natural language
processing tasks show that Translution (including {\alpha}-Translution)
achieves superior accuracy compared to self-attention. The code is available at
https://github.com/hehefan/Translution.","Transformer~\citep{DBLP:conf/nips/VaswaniSPUJGKP17,radford2018improving,DBLP:conf/naacl/DevlinCLT19,DBLP:conf/iclr/DosovitskiyB0WZ21,DBLP:conf/iccv/LiuL00W0LG21,DBLP:conf/icml/TouvronCDMSJ21} eschews recurrence (as used in recurrent neural networks) and kernel size (as used in convolutional neural networks), instead employing self‐attention for relevant region identification.  
Because it has no built‐in notion of order, Transformer incorporates explicit absolute positional embeddings into token embeddings, enabling the model to utilize sequence order. 
Subsequent work has explored ``relative attention''~\citep{DBLP:conf/naacl/ShawUV18,DBLP:conf/iclr/HuangVUSHSDHDE19,DBLP:conf/nips/ParmarRVBLS19,DBLP:conf/acl/DaiYYCLS19,DBLP:conf/emnlp/TsaiBYMS19,DBLP:journals/jmlr/RaffelSRLNMZLL20,DBLP:conf/nips/DaiLLT21}, which integrates relative position information into self‐attention. 
They can be categorized into three families: 
\textit{1) Relative positional vector.} Shaw~\etal enhanced Transformer for language modeling by adding learnable relative positional vectors into the key and value computations, respectively~\citep{DBLP:conf/naacl/ShawUV18}. 
BoTNet~\citep{DBLP:conf/cvpr/SrinivasLPSAV21} and HaloNet~\citep{DBLP:conf/cvpr/VaswaniRSPHS21} extended this approach to two dimensions for image processing by adding learnable relative positional vectors into key. 
\textit{2) Relative positional scalar.} Swin Transformer~\citep{DBLP:conf/iccv/LiuL00W0LG21},  CoAtNet~\citep{DBLP:conf/nips/DaiLLT21}, and ConViT~\cite{DBLP:conf/icml/dAscoliTLMBS21} incorporate a learnable relative positional bias (a scalar) into the attention score. 
In these methods, the original self-attention can be regarded as content attention, which measures relationships from the token-feature perspective, while the additional relative positional bias can be regarded as position attention, which measures relationships from the token-position perspective. 
\textit{3) Rotary position embedding.}  RoFormer~\citep{DBLP:journals/ijon/SuALPBL24} introduces a rotary position embedding  mechanism, which encodes relative positional information by applying a rotation operation in the Query and Key representation space. 
Unlike these existing methods, Translution employs a convolution-style approach that uses relative positional matrices for query, key and value computation. 
Section~\ref{sec:position} provides a formal comparison of these methods. 

Convolutional neural networks ~\citep{DBLP:journals/pieee/LeCunBBH98,DBLP:conf/nips/KrizhevskySH12,DBLP:journals/corr/SimonyanZ14a,DBLP:conf/cvpr/SzegedyLJSRAEVR15,DBLP:conf/cvpr/HeZRS16} have been the backbone of deep learning for years. By using small, shared kernels and pooling, convolutional neural networks efficiently capture local patterns.  
Recent architectural developments integrate self-attention with convolution. 
For instance, Conformer~\citep{DBLP:conf/interspeech/GulatiQCPZYHWZW20} combines convolution layers and self-attention layers to capture both local and global dependencies in audio sequences.
Similarly, CeiT~\citep{DBLP:conf/iccv/YuanG0ZYW21} uses convolutions to extract low-level features and self-attention to model long-range dependencies.
Unlike these architectural methods, Translution operates at the basic module or layer level, blending the advantages of self-attention and convolution into a unified fundamental operation.","Transformer~\citep{DBLP:conf/nips/VaswaniSPUJGKP17,radford2018improving,DBLP:conf/naacl/DevlinCLT19,DBLP:conf/iclr/DosovitskiyB0WZ21,DBLP:conf/iccv/LiuL00W0LG21,DBLP:conf/icml/TouvronCDMSJ21} eschews recurrence (as used in recurrent neural networks) and kernel size (as used in convolutional neural networks), instead employing self‐attention for relevant region identification.  
Because it has no built‐in notion of order, Transformer incorporates explicit absolute positional embeddings into token embeddings, enabling the model to utilize sequence order. 
Subsequent work has explored ``relative attention''~\citep{DBLP:conf/naacl/ShawUV18,DBLP:conf/iclr/HuangVUSHSDHDE19,DBLP:conf/nips/ParmarRVBLS19,DBLP:conf/acl/DaiYYCLS19,DBLP:conf/emnlp/TsaiBYMS19,DBLP:journals/jmlr/RaffelSRLNMZLL20,DBLP:conf/nips/DaiLLT21}, which integrates relative position information into self‐attention. 
They can be categorized into three families: 
\textit{1) Relative positional vector.} Shaw~\etal enhanced Transformer for language modeling by adding learnable relative positional vectors into the key and value computations, respectively~\citep{DBLP:conf/naacl/ShawUV18}. 
BoTNet~\citep{DBLP:conf/cvpr/SrinivasLPSAV21} and HaloNet~\citep{DBLP:conf/cvpr/VaswaniRSPHS21} extended this approach to two dimensions for image processing by adding learnable relative positional vectors into key. 
\textit{2) Relative positional scalar.} Swin Transformer~\citep{DBLP:conf/iccv/LiuL00W0LG21},  CoAtNet~\citep{DBLP:conf/nips/DaiLLT21}, and ConViT~\cite{DBLP:conf/icml/dAscoliTLMBS21} incorporate a learnable relative positional bias (a scalar) into the attention score. 
In these methods, the original self-attention can be regarded as content attention, which measures relationships from the token-feature perspective, while the additional relative positional bias can be regarded as position attention, which measures relationships from the token-position perspective. 
\textit{3) Rotary position embedding.}  RoFormer~\citep{DBLP:journals/ijon/SuALPBL24} introduces a rotary position embedding  mechanism, which encodes relative positional information by applying a rotation operation in the Query and Key representation space. 
Unlike these existing methods, Translution employs a convolution-style approach that uses relative positional matrices for query, key and value computation. 
Section~\ref{sec:position} provides a formal comparison of these methods. 

Convolutional neural networks ~\citep{DBLP:journals/pieee/LeCunBBH98,DBLP:conf/nips/KrizhevskySH12,DBLP:journals/corr/SimonyanZ14a,DBLP:conf/cvpr/SzegedyLJSRAEVR15,DBLP:conf/cvpr/HeZRS16} have been the backbone of deep learning for years. By using small, shared kernels and pooling, convolutional neural networks efficiently capture local patterns.  
Recent architectural developments integrate self-attention with convolution. 
For instance, Conformer~\citep{DBLP:conf/interspeech/GulatiQCPZYHWZW20} combines convolution layers and self-attention layers to capture both local and global dependencies in audio sequences.
Similarly, CeiT~\citep{DBLP:conf/iccv/YuanG0ZYW21} uses convolutions to extract low-level features and self-attention to model long-range dependencies.
Unlike these architectural methods, Translution operates at the basic module or layer level, blending the advantages of self-attention and convolution into a unified fundamental operation.",N/A
