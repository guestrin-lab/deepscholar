arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2510.10248v1,http://arxiv.org/abs/2510.10248v1,2025-10-11 15:05:45+00:00,Reasoning-Enhanced Large Language Models for Molecular Property Prediction,"Molecular property prediction is crucial for drug discovery and materials
science, yet existing approaches suffer from limited interpretability, poor
cross-task generalization, and lack of chemical reasoning capabilities.
Traditional machine learning models struggle with task transferability, while
specialized molecular language models provide little insight into their
decision-making processes. To address these limitations, we propose
\textbf{MPPReasoner}, a multimodal large language model that incorporates
chemical reasoning for molecular property prediction. Our approach, built upon
Qwen2.5-VL-7B-Instruct, integrates molecular images with SMILES strings to
enable comprehensive molecular understanding. We develop a two-stage training
strategy: supervised fine-tuning (SFT) using 16,000 high-quality reasoning
trajectories generated through expert knowledge and multiple teacher models,
followed by Reinforcement Learning from Principle-Guided Rewards (RLPGR). RLPGR
employs verifiable, rule-based rewards that systematically evaluate chemical
principle application, molecular structure analysis, and logical consistency
through computational verification. Extensive experiments across 8 datasets
demonstrate significant performance improvements, with MPPReasoner
outperforming the best baselines by 7.91\% and 4.53\% on in-distribution and
out-of-distribution tasks respectively. MPPReasoner exhibits exceptional
cross-task generalization and generates chemically sound reasoning paths that
provide valuable insights into molecular property analysis, substantially
enhancing both interpretability and practical utility for chemists. Code is
available at https://anonymous.4open.science/r/MPPReasoner-12687.","\label{sec:background}
This section reviews prior research on machine learning for molecular representations, multimodal language models in chemistry, and reasoning capabilities in LLMs, which are foundational to our proposed framework for training reasoning LLMs tailored to molecular property prediction.

\paragraph{Machine Learning for Molecular Representation.}
GNNs have evolved as a dominant paradigm for molecular graph representation, progressing from early convolutional \citep{moleculargraph,schnet} and message-passing \citep{mpnn} frameworks to sophisticated 3D-aware models \citep{mpnn, rgcl, simsgt,unimol}, enabling robust applications in property prediction, virtual screening, and drug discovery \citep{gnn-chemistry-applications}.
In parallel, specialized molecular language models have reframed molecular structures as textual sequences such as SMILES strings\citep{smiles}, with models like MolecularGPT \citep{moleculargpt} and BioT5-Plus \citep{biot5-plus} supporting few-shot adaptation and multi-task learning for diverse chemical and biological tasks \citep{scilitllm, reactxt, molca}. 

\paragraph{Multimodal Language Models for Chemistry.}
The emerging trend of multimodal LLMs in chemistry integrates diverse data types—such as SMILES strings and molecular graphs to address unimodal limitations, as seen in foundational molecular-text models \citep{mol-llm, chemvlm, molca}, instruction-tuned assistants \citep{instructmol}, and tool-augmented systems \citep{chemcrow}, enhancing robustness in property prediction \citep{molt5}, molecular design \citep{mol-llm}, and synthesis planning \citep{relm, reactxt}. However, these models still lack the capability to provide chemical reasoning for their predictions.

\paragraph{Reasoning in Large Language Models.}
Reasoning capabilities have demonstrated remarkable efficacy in commercial LLMs, particularly through chain-of-thought processes as exemplified in OpenAI's o1 series and other advanced models \citep{cot,openai-o1,gemini,claude}.
Training these abilities leverages RL techniques, from Proximal Policy Optimization \citep{ppo} in RL from Human Feedback (RLHF) \citep{rlhf} for preference alignment, to efficient extensions like Group Relative Policy Optimization (GRPO) \citep{grpo} with outcome-based rewards and Reinforcement Learning with Verifiable Rewards (RLVR) for one-shot verifiable steps, improving generalization on complex tasks \citep{rlvr}.
These RL advancements motivate our adaptation for chemical-specific reasoning in the field of molecular property prediction.","This section reviews prior research on machine learning for molecular representations, multimodal language models in chemistry, and reasoning capabilities in LLMs, which are foundational to our proposed framework for training reasoning LLMs tailored to molecular property prediction.

\paragraph{Machine Learning for Molecular Representation.}
GNNs have evolved as a dominant paradigm for molecular graph representation, progressing from early convolutional \citep{moleculargraph,schnet} and message-passing \citep{mpnn} frameworks to sophisticated 3D-aware models \citep{mpnn, rgcl, simsgt,unimol}, enabling robust applications in property prediction, virtual screening, and drug discovery \citep{gnn-chemistry-applications}.
In parallel, specialized molecular language models have reframed molecular structures as textual sequences such as SMILES strings\citep{smiles}, with models like MolecularGPT \citep{moleculargpt} and BioT5-Plus \citep{biot5-plus} supporting few-shot adaptation and multi-task learning for diverse chemical and biological tasks \citep{scilitllm, reactxt, molca}. 

\paragraph{Multimodal Language Models for Chemistry.}
The emerging trend of multimodal LLMs in chemistry integrates diverse data types—such as SMILES strings and molecular graphs to address unimodal limitations, as seen in foundational molecular-text models \citep{mol-llm, chemvlm, molca}, instruction-tuned assistants \citep{instructmol}, and tool-augmented systems \citep{chemcrow}, enhancing robustness in property prediction \citep{molt5}, molecular design \citep{mol-llm}, and synthesis planning \citep{relm, reactxt}. However, these models still lack the capability to provide chemical reasoning for their predictions.

\paragraph{Reasoning in Large Language Models.}
Reasoning capabilities have demonstrated remarkable efficacy in commercial LLMs, particularly through chain-of-thought processes as exemplified in OpenAI's o1 series and other advanced models \citep{cot,openai-o1,gemini,claude}.
Training these abilities leverages RL techniques, from Proximal Policy Optimization \citep{ppo} in RL from Human Feedback (RLHF) \citep{rlhf} for preference alignment, to efficient extensions like Group Relative Policy Optimization (GRPO) \citep{grpo} with outcome-based rewards and Reinforcement Learning with Verifiable Rewards (RLVR) for one-shot verifiable steps, improving generalization on complex tasks \citep{rlvr}.
These RL advancements motivate our adaptation for chemical-specific reasoning in the field of molecular property prediction.",N/A
