arxiv_id,arxiv_link,publication_date,title,abstract,raw_latex_related_works,clean_latex_related_works,pdf_related_works
2510.07978v1,http://arxiv.org/abs/2510.07978v1,2025-10-09 09:11:38+00:00,VoiceAgentBench: Are Voice Assistants ready for agentic tasks?,"Large-scale Speech Language Models (SpeechLMs) have enabled voice assistants
capable of understanding natural spoken queries and performing complex tasks.
However, existing speech benchmarks primarily focus on isolated capabilities
such as transcription, or question-answering, and do not systematically
evaluate agentic scenarios encompassing multilingual and cultural
understanding, as well as adversarial robustness. To address this, we introduce
VoiceAgentBench, a comprehensive benchmark designed to evaluate SpeechLMs in
realistic spoken agentic settings. It comprises over 5,500 synthetic spoken
queries, including dialogues grounded in Indian context, covering single-tool
invocations, multi-tool workflows, multi-turn interactions, and safety
evaluations. The benchmark supports English, Hindi, and 5 other Indian
languages, reflecting real-world linguistic and cultural diversity. We simulate
speaker variability using a novel sampling algorithm that selects audios for
TTS voice conversion based on its speaker embeddings, maximizing acoustic and
speaker diversity. Our evaluation measures tool selection accuracy, structural
consistency, and the correctness of tool invocations, including adversarial
robustness. Our experiments reveal significant gaps in contextual tool
orchestration tasks, Indic generalization, and adversarial robustness, exposing
critical limitations of current SpeechLMs.","\label{sec:related-work}

\textbf{LLM Agent Benchmarks.} Interest in evaluating agentic LLMs has grown with advances in their reasoning and decision making capabilities. ToolBench \citep{qin2024toolllm} evaluates models’ ability to invoke external APIs across diverse real-world tasks, while ToolQA \citep{zhuang2023toolqa} assesses LLMs’ use of external tools for question answering via a scalable, automated dataset curation process. Berkeley Function Calling Leaderboard (BFCL) \citep{patil2025the} emphasizes precise API generation across domains and robustness to both single and multiple function calls, and NESTful \citep{basu2025nestfulbenchmarkevaluatingllms} focuses on nested sequences of API calls, where outputs of one call feed into the next. API-Bank and ToolTalk \citep{li2023apibank, farn2023tooltalkevaluatingtoolusageconversational} target multi-turn, dialogue-driven tool-use scenarios, testing sequential API planning and interaction. Tau-bench \citep{yao2025taubench} simulates dynamic conversations with domain-specific tools and policies to evaluate adherence to task rules. AgentHarmBench \citep{andriushchenko2025agentharm} and DoomArena \citep{boisvert2025doomarena} focus on safety and adversarial robustness, testing susceptibility to harmful or unsafe actions. Despite this progress for LLMs, no speech benchmark explicitly evaluates SpeechLMs in such realistic, agentic, and safety-critical settings, underscoring the need for specialized evaluation frameworks.


\textbf{Speech Datasets and Benchmarks.}  Large-scale datasets such as LibriSpeech \citep{7178964}, CommonVoice \citep{ardila-etal-2020-common}, and MuST-C \citep{di-gangi-etal-2019-must} have been foundational for advancing automatic speech recognition (ASR) and speech translation (AST). IndicST \citep{11011192} and Lahaja \citep{javed24_interspeech} extend these tasks to cover diverse Indic speech data. Evaluation suites like SUPERB \citep{yang21c_interspeech} and SLUE \citep{shon-etal-2023-slue} standardize the assessment of tasks such as intent classification, named entity recognition, and slot filling, with IndicSUPERB \citep{javed2022indicsuperbspeechprocessinguniversal} further supporting Indic languages. However, these benchmarks primarily target simpler tasks like transcription, translation, NER and do not assess reasoning or decision-making over spoken content. To address this gap, recent work has begun exploring reasoning in the audio domain. Audio-CoT \citep{ma2025audiocotexploringchainofthoughtreasoning} introduces chain-of-thought prompting for structured multistep inference on speech input, while MMAU \citep{sakshi2025mmau} provides a large-scale benchmark of 10k audio clips covering 27 reasoning skills, such as temporal reasoning and causal inference, in speech, music, and environmental sounds. AIR-Bench \citep{yang-etal-2024-air} and AudioBench \citep{DBLP:journals/corr/abs-2406-16020} extend the scope to open-ended instruction following on various types of audio and speech, whereas VoiceBench \citep{DBLP:journals/corr/abs-2410-17196} emphasizes robustness and generalization by converting text instruction into spoken form with real-world noise and speaker variation. More recently, SpeechR \citep{yang2025speechrbenchmarkspeechreasoning} directly targets high-level reasoning on speech, focusing on logical deduction, and commonsense problem solving.
We also provide an extended discussion of related work on speech models in Appendix \ref{appendix:related_work_speech_models}.","\textbf{LLM Agent Benchmarks.} Interest in evaluating agentic LLMs has grown with advances in their reasoning and decision making capabilities. ToolBench \citep{qin2024toolllm} evaluates models’ ability to invoke external APIs across diverse real-world tasks, while ToolQA \citep{zhuang2023toolqa} assesses LLMs’ use of external tools for question answering via a scalable, automated dataset curation process. Berkeley Function Calling Leaderboard (BFCL) \citep{patil2025the} emphasizes precise API generation across domains and robustness to both single and multiple function calls, and NESTful \citep{basu2025nestfulbenchmarkevaluatingllms} focuses on nested sequences of API calls, where outputs of one call feed into the next. API-Bank and ToolTalk \citep{li2023apibank, farn2023tooltalkevaluatingtoolusageconversational} target multi-turn, dialogue-driven tool-use scenarios, testing sequential API planning and interaction. Tau-bench \citep{yao2025taubench} simulates dynamic conversations with domain-specific tools and policies to evaluate adherence to task rules. AgentHarmBench \citep{andriushchenko2025agentharm} and DoomArena \citep{boisvert2025doomarena} focus on safety and adversarial robustness, testing susceptibility to harmful or unsafe actions. Despite this progress for LLMs, no speech benchmark explicitly evaluates SpeechLMs in such realistic, agentic, and safety-critical settings, underscoring the need for specialized evaluation frameworks.


\textbf{Speech Datasets and Benchmarks.}  Large-scale datasets such as LibriSpeech \citep{7178964}, CommonVoice \citep{ardila-etal-2020-common}, and MuST-C \citep{di-gangi-etal-2019-must} have been foundational for advancing automatic speech recognition (ASR) and speech translation (AST). IndicST \citep{11011192} and Lahaja \citep{javed24_interspeech} extend these tasks to cover diverse Indic speech data. Evaluation suites like SUPERB \citep{yang21c_interspeech} and SLUE \citep{shon-etal-2023-slue} standardize the assessment of tasks such as intent classification, named entity recognition, and slot filling, with IndicSUPERB \citep{javed2022indicsuperbspeechprocessinguniversal} further supporting Indic languages. However, these benchmarks primarily target simpler tasks like transcription, translation, NER and do not assess reasoning or decision-making over spoken content. To address this gap, recent work has begun exploring reasoning in the audio domain. Audio-CoT \citep{ma2025audiocotexploringchainofthoughtreasoning} introduces chain-of-thought prompting for structured multistep inference on speech input, while MMAU \citep{sakshi2025mmau} provides a large-scale benchmark of 10k audio clips covering 27 reasoning skills, such as temporal reasoning and causal inference, in speech, music, and environmental sounds. AIR-Bench \citep{yang-etal-2024-air} and AudioBench \citep{DBLP:journals/corr/abs-2406-16020} extend the scope to open-ended instruction following on various types of audio and speech, whereas VoiceBench \citep{DBLP:journals/corr/abs-2410-17196} emphasizes robustness and generalization by converting text instruction into spoken form with real-world noise and speaker variation. More recently, SpeechR \citep{yang2025speechrbenchmarkspeechreasoning} directly targets high-level reasoning on speech, focusing on logical deduction, and commonsense problem solving.
We also provide an extended discussion of related work on speech models in Appendix \ref{appendix:related_work_speech_models}.",N/A
