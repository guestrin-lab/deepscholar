,arxiv_id,title,authors,abstract,categories,published_date,updated_date,abs_url,doi,journal_ref,comments
0,2510.03952v1,"Strategy Logic, Imperfect Information, and Hyperproperties","Raven Beutner, Bernd Finkbeiner","Strategy logic (SL) is a powerful temporal logic that enables first-class
reasoning over strategic behavior in multi-agent systems (MAS). In many MASs,
the agents (and their strategies) cannot observe the global state of the
system, leading to many extensions of SL centered around imperfect information,
such as strategy logic with imperfect information (SL$_\mathit{ii}$). Along
orthogonal lines, researchers have studied the combination of strategic
behavior and hyperproperties. Hyperproperties are system properties that relate
multiple executions in a system and commonly arise when specifying security
policies. Hyper Strategy Logic (HyperSL) is a temporal logic that combines
quantification over strategies with the ability to express hyperproperties on
the executions of different strategy profiles. In this paper, we study the
relation between SL$_\mathit{ii}$ and HyperSL. Our main result is that both
logics (restricted to formulas where no state formulas are nested within path
formulas) are equivalent in the sense that we can encode SL$_\mathit{ii}$
instances into HyperSL instances and vice versa. For the former direction, we
build on the well-known observation that imperfect information is a
hyperproperty. For the latter direction, we construct a self-composition of
MASs and show how we can simulate hyperproperties using imperfect information.","cs.LO, cs.AI, cs.MA",2025-10-04T21:37:14+00:00,2025-10-04T21:37:14+00:00,http://arxiv.org/abs/2510.03952v1,,,KR 2025
1,2510.06048v2,BLISS: A Lightweight Bilevel Influence Scoring Method for Data Selection in Language Model Pretraining,"Jie Hao, Rui Yu, Wei Zhang, Huixia Wang, Jie Xu, Mingrui Liu","Effective data selection is essential for pretraining large language models
(LLMs), enhancing efficiency and improving generalization to downstream tasks.
However, existing approaches often require leveraging external pretrained
models, making it difficult to disentangle the effects of data selection from
those of the external pretrained models. In addition, they often overlook the
long-term impact of selected data if the model is trained to convergence,
primarily due to the prohibitive cost of full-scale LLM pretraining. In this
paper, we introduce BLISS (\textbf{B}ileve\textbf{L} \textbf{I}nfluence
\textbf{S}coring method for data \textbf{S}election): a lightweight data
selection method that operates entirely \emph{from scratch}, without relying on
any external pretrained oracle models, while explicitly accounting for the
long-term impact of selected data. BLISS leverages a small proxy model as a
surrogate for the LLM and employs a score model to estimate the long-term
influence of training samples if the proxy model is trained to convergence. We
formulate data selection as a bilevel optimization problem, where the
upper-level objective optimizes the score model to assign importance weights to
training samples, ensuring that minimizing the lower-level objective (i.e.,
training the proxy model over the weighted training loss until convergence)
leads to best validation performance. Once optimized, the trained score model
predicts influence scores for the dataset, enabling efficient selection of
high-quality samples for LLM pretraining. We validate BLISS by pretraining
410M/1B/2.8B Pythia and LLaMA-0.5B models on selected subsets of the C4
dataset. Notably, under the 1B model setting, BLISS achieves $1.7\times$
speedup in reaching the same performance as the state-of-the-art method,
demonstrating superior performance across multiple downstream tasks.",cs.LG,2025-10-07T15:42:33+00:00,2025-10-08T17:49:49+00:00,http://arxiv.org/abs/2510.06048v2,,,
2,2510.01285v1,LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science,"Alireza Salemi, Mihir Parmar, Palash Goyal, Yiwen Song, Jinsung Yoon, Hamed Zamani, Hamid Palangi, Tomas Pfister","The rapid advancement of Large Language Models (LLMs) has opened new
opportunities in data science, yet their practical deployment is often
constrained by the challenge of discovering relevant data within large
heterogeneous data lakes. Existing methods struggle with this: single-agent
systems are quickly overwhelmed by large, heterogeneous files in the large data
lakes, while multi-agent systems designed based on a master-slave paradigm
depend on a rigid central controller for task allocation that requires precise
knowledge of each sub-agent's capabilities. To address these limitations, we
propose a novel multi-agent communication paradigm inspired by the blackboard
architecture for traditional AI models. In this framework, a central agent
posts requests to a shared blackboard, and autonomous subordinate agents --
either responsible for a partition of the data lake or general information
retrieval -- volunteer to respond based on their capabilities. This design
improves scalability and flexibility by eliminating the need for a central
coordinator to have prior knowledge of all sub-agents' expertise. We evaluate
our method on three benchmarks that require explicit data discovery: KramaBench
and modified versions of DS-Bench and DA-Code to incorporate data discovery.
Experimental results demonstrate that the blackboard architecture substantially
outperforms baselines, including RAG and the master-slave multi-agent paradigm,
achieving between 13% to 57% relative improvement in end-to-end task success
and up to a 9% relative gain in F1 score for data discovery over the
best-performing baselines across both proprietary and open-source LLMs. Our
findings establish the blackboard paradigm as a scalable and generalizable
communication framework for multi-agent systems.","cs.MA, cs.AI, cs.CL, cs.IR, cs.LG",2025-09-30T22:34:23+00:00,2025-09-30T22:34:23+00:00,http://arxiv.org/abs/2510.01285v1,,,
3,2510.04539v1,C3Editor: Achieving Controllable Consistency in 2D Model for 3D Editing,"Zeng Tao, Zheng Ding, Zeyuan Chen, Xiang Zhang, Leizhi Li, Zhuowen Tu","Existing 2D-lifting-based 3D editing methods often encounter challenges
related to inconsistency, stemming from the lack of view-consistent 2D editing
models and the difficulty of ensuring consistent editing across multiple views.
To address these issues, we propose C3Editor, a controllable and consistent
2D-lifting-based 3D editing framework. Given an original 3D representation and
a text-based editing prompt, our method selectively establishes a
view-consistent 2D editing model to achieve superior 3D editing results. The
process begins with the controlled selection of a ground truth (GT) view and
its corresponding edited image as the optimization target, allowing for
user-defined manual edits. Next, we fine-tune the 2D editing model within the
GT view and across multiple views to align with the GT-edited image while
ensuring multi-view consistency. To meet the distinct requirements of GT view
fitting and multi-view consistency, we introduce separate LoRA modules for
targeted fine-tuning. Our approach delivers more consistent and controllable 2D
and 3D editing results than existing 2D-lifting-based methods, outperforming
them in both qualitative and quantitative evaluations.","cs.GR, cs.CV",2025-10-06T07:07:14+00:00,2025-10-06T07:07:14+00:00,http://arxiv.org/abs/2510.04539v1,,,
4,2509.15233v1,Video2Roleplay: A Multimodal Dataset and Framework for Video-Guided Role-playing Agents,"Xueqiao Zhang, Chao Zhang, Jingtao Xu, Yifan Zhu, Xin Shi, Yi Yang, Yawei Luo","Role-playing agents (RPAs) have attracted growing interest for their ability
to simulate immersive and interactive characters. However, existing approaches
primarily focus on static role profiles, overlooking the dynamic perceptual
abilities inherent to humans. To bridge this gap, we introduce the concept of
dynamic role profiles by incorporating video modality into RPAs. To support
this, we construct Role-playing-Video60k, a large-scale, high-quality dataset
comprising 60k videos and 700k corresponding dialogues. Based on this dataset,
we develop a comprehensive RPA framework that combines adaptive temporal
sampling with both dynamic and static role profile representations.
Specifically, the dynamic profile is created by adaptively sampling video
frames and feeding them to the LLM in temporal order, while the static profile
consists of (1) character dialogues from training videos during fine-tuning,
and (2) a summary context from the input video during inference. This joint
integration enables RPAs to generate greater responses. Furthermore, we propose
a robust evaluation method covering eight metrics. Experimental results
demonstrate the effectiveness of our framework, highlighting the importance of
dynamic role profiles in developing RPAs.","cs.MM, cs.CL, cs.CV",2025-09-17T02:50:54+00:00,2025-09-17T02:50:54+00:00,http://arxiv.org/abs/2509.15233v1,,,Accepted at EMNLP2025 Main
5,2510.10152v1,Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer,"Yecong Wan, Mingwen Shao, Renlong Wu, Wangmeng Zuo","In this work, we present Color3D, a highly adaptable framework for colorizing
both static and dynamic 3D scenes from monochromatic inputs, delivering
visually diverse and chromatically vibrant reconstructions with flexible
user-guided control. In contrast to existing methods that focus solely on
static scenarios and enforce multi-view consistency by averaging color
variations which inevitably sacrifice both chromatic richness and
controllability, our approach is able to preserve color diversity and
steerability while ensuring cross-view and cross-time consistency. In
particular, the core insight of our method is to colorize only a single key
view and then fine-tune a personalized colorizer to propagate its color to
novel views and time steps. Through personalization, the colorizer learns a
scene-specific deterministic color mapping underlying the reference view,
enabling it to consistently project corresponding colors to the content in
novel views and video frames via its inherent inductive bias. Once trained, the
personalized colorizer can be applied to infer consistent chrominance for all
other images, enabling direct reconstruction of colorful 3D scenes with a
dedicated Lab color space Gaussian splatting representation. The proposed
framework ingeniously recasts complicated 3D colorization as a more tractable
single image paradigm, allowing seamless integration of arbitrary image
colorization models with enhanced flexibility and controllability. Extensive
experiments across diverse static and dynamic 3D colorization benchmarks
substantiate that our method can deliver more consistent and chromatically rich
renderings with precise user control. Project Page
https://yecongwan.github.io/Color3D/.",cs.CV,2025-10-11T10:21:19+00:00,2025-10-11T10:21:19+00:00,http://arxiv.org/abs/2510.10152v1,,,Project Page https://yecongwan.github.io/Color3D/
6,2510.11496v1,AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model,"Zhiwei Jin, Xiaohui Song, Nan Wang, Yafei Liu, Chao Li, Xin Li, Ruichen Wang, Zhihao Li, Qi Qi, Long Cheng, Dongze Hao, Quanlong Zheng, Yanhao Zhang, Haobo Ji, Jian Ma, Zhitong Zheng, Zhenyi Lin, Haolin Deng, Xin Zou, Xiaojie Yin, Ruilin Wang, Liankai Cai, Haijing Liu, Yuqing Qiu, Ke Chen, Zixian Li, Chi Xie, Huafei Li, Chenxing Li, Chuangchuang Wang, Kai Tang, Zhiguang Zhu, Kai Tang, Wenmei Gao, Rui Wang, Jun Wu, Chao Liu, Qin Xie, Chen Chen, Haonan Lu","In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o,
Gemini, and Claude Sonnet have demonstrated outstanding performance with
enormous model sizes reaching hundreds of billions of parameters, they
significantly surpass the limitations in memory, power consumption, and
computing capacity of edge devices such as mobile phones. This paper introduces
AndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on
Qwen3's LLM and various visual encoders. We comprehensively outline the model
architectures, training pipeline, and training data of AndesVL, which achieves
first-tier performance across a wide range of open-source benchmarks, including
fields such as text-rich image understanding, reasoning and math, multi-image
comprehension, general VQA, hallucination mitigation, multilingual
understanding, and GUI-related tasks when compared with state-of-the-art models
of a similar scale. Furthermore, we introduce a 1+N LoR","cs.CV, cs.AI",2025-10-13T15:04:38+00:00,2025-10-13T15:04:38+00:00,http://arxiv.org/abs/2510.11496v1,,,Tech report of OPPO AndesVL Team
7,2509.11868v1,Growing Perspectives: Modelling Embodied Perspective Taking and Inner Narrative Development Using Large Language Models,"Sabrina Patania, Luca Annese, Anna Lambiase, Anita Pellegrini, Tom Foulsham, Azzurra Ruggeri, Silvia Rossi, Silvia Serino, Dimitri Ognibene","Language and embodied perspective taking are essential for human
collaboration, yet few computational models address both simultaneously. This
work investigates the PerspAct system [1], which integrates the ReAct (Reason
and Act) paradigm with Large Language Models (LLMs) to simulate developmental
stages of perspective taking, grounded in Selman's theory [2]. Using an
extended director task, we evaluate GPT's ability to generate internal
narratives aligned with specified developmental stages, and assess how these
influence collaborative performance both qualitatively (action selection) and
quantitatively (task efficiency). Results show that GPT reliably produces
developmentally-consistent narratives before task execution but often shifts
towards more advanced stages during interaction, suggesting that language
exchanges help refine internal representations. Higher developmental stages
generally enhance collaborative effectiveness, while earlier stages yield more
variable outcomes in complex contexts. These findings highlight the potential
of integrating embodied perspective taking and language in LLMs to better model
developmental dynamics and stress the importance of evaluating internal speech
during combined linguistic and embodied tasks.","cs.CL, cs.AI, cs.HC, cs.RO, I.2; I.2.7; I.2.10; J.4",2025-09-15T12:39:55+00:00,2025-09-15T12:39:55+00:00,http://arxiv.org/abs/2509.11868v1,,,Accepted at ICDL https://icdl2025.fel.cvut.cz/
8,2509.20837v1,Verification Limits Code LLM Training,"Srishti Gureja, Elena Tommasone, Jingyi He, Sara Hooker, Matthias Gallé, Marzieh Fadaee","Large language models for code generation increasingly rely on synthetic
data, where both problem solutions and verification tests are generated by
models. While this enables scalable data creation, it introduces a previously
unexplored bottleneck: the verification ceiling, in which the quality and
diversity of training data are fundamentally constrained by the capabilities of
synthetic verifiers. In this work, we systematically study how verification
design and strategies influence model performance. We investigate (i) what we
verify by analyzing the impact of test complexity and quantity: richer test
suites improve code generation capabilities (on average +3 pass@1), while
quantity alone yields diminishing returns, (ii) how we verify by exploring
relaxed pass thresholds: rigid 100% pass criteria can be overly restrictive. By
allowing for relaxed thresholds or incorporating LLM-based soft verification,
we can recover valuable training data, leading to a 2-4 point improvement in
pass@1 performance. However, this benefit is contingent upon the strength and
diversity of the test cases used, and (iii) why verification remains necessary
through controlled comparisons of formally correct versus incorrect solutions
and human evaluation: retaining diverse correct solutions per problem yields
consistent generalization gains. Our results show that Verification as
currently practiced is too rigid, filtering out valuable diversity. But it
cannot be discarded, only recalibrated. By combining calibrated verification
with diverse, challenging problem-solution pairs, we outline a path to break
the verification ceiling and unlock stronger code generation models.","cs.SE, cs.AI, cs.CL",2025-09-25T07:23:30+00:00,2025-09-25T07:23:30+00:00,http://arxiv.org/abs/2509.20837v1,,,
9,2510.01675v1,Geometric Backstepping Control of Omnidirectional Tiltrotors Incorporating Servo-Rotor Dynamics for Robustness against Sudden Disturbances,"Jaewoo Lee, Dongjae Lee, Jinwoo Lee, Hyungyu Lee, Yeonjoon Kim, H. Jin Kim","This work presents a geometric backstepping controller for a variable-tilt
omnidirectional multirotor that explicitly accounts for both servo and rotor
dynamics. Considering actuator dynamics is essential for more effective and
reliable operation, particularly during aggressive flight maneuvers or recovery
from sudden disturbances. While prior studies have investigated actuator-aware
control for conventional and fixed-tilt multirotors, these approaches rely on
linear relationships between actuator input and wrench, which cannot capture
the nonlinearities induced by variable tilt angles. In this work, we exploit
the cascade structure between the rigid-body dynamics of the multirotor and its
nonlinear actuator dynamics to design the proposed backstepping controller and
establish exponential stability of the overall system. Furthermore, we reveal
parametric uncertainty in the actuator model through experiments, and we
demonstrate that the proposed controller remains robust against such
uncertainty. The controller was compared against a baseline that does not
account for actuator dynamics across three experimental scenarios: fast
translational tracking, rapid rotational tracking, and recovery from sudden
disturbance. The proposed method consistently achieved better tracking
performance, and notably, while the baseline diverged and crashed during the
fastest translational trajectory tracking and the recovery experiment, the
proposed controller maintained stability and successfully completed the tasks,
thereby demonstrating its effectiveness.","cs.RO, cs.SY, eess.SY",2025-10-02T05:00:24+00:00,2025-10-02T05:00:24+00:00,http://arxiv.org/abs/2510.01675v1,,,
10,2510.09903v1,An uncertainty-aware framework for data-efficient multi-view animal pose estimation,"Lenny Aharon, Keemin Lee, Karan Sikka, Selmaan Chettih, Cole Hurwitz, Liam Paninski, Matthew R Whiteway","Multi-view pose estimation is essential for quantifying animal behavior in
scientific research, yet current methods struggle to achieve accurate tracking
with limited labeled data and suffer from poor uncertainty estimates. We
address these challenges with a comprehensive framework combining novel
training and post-processing techniques, and a model distillation procedure
that leverages the strengths of these techniques to produce a more efficient
and effective pose estimator. Our multi-view transformer (MVT) utilizes
pretrained backbones and enables simultaneous processing of information across
all views, while a novel patch masking scheme learns robust cross-view
correspondences without camera calibration. For calibrated setups, we
incorporate geometric consistency through 3D augmentation and a triangulation
loss. We extend the existing Ensemble Kalman Smoother (EKS) post-processor to
the nonlinear case and enhance uncertainty quantification via a variance
inflation technique. Finally, to leverage the scaling properties of the MVT, we
design a distillation procedure that exploits improved EKS predictions and
uncertainty estimates to generate high-quality pseudo-labels, thereby reducing
dependence on manual labels. Our framework components consistently outperform
existing methods across three diverse animal species (flies, mice, chickadees),
with each component contributing complementary benefits. The result is a
practical, uncertainty-aware system for reliable pose estimation that enables
downstream behavioral analyses under real-world data constraints.","cs.CV, q-bio.QM",2025-10-10T22:27:13+00:00,2025-10-10T22:27:13+00:00,http://arxiv.org/abs/2510.09903v1,,,
11,2510.09012v1,Towards Better & Faster Autoregressive Image Generation: From the Perspective of Entropy,"Xiaoxiao Ma, Feng Zhao, Pengyang Ling, Haibo Qiu, Zhixiang Wei, Hu Yu, Jie Huang, Zhixiong Zeng, Lin Ma","In this work, we first revisit the sampling issues in current autoregressive
(AR) image generation models and identify that image tokens, unlike text
tokens, exhibit lower information density and non-uniform spatial distribution.
Accordingly, we present an entropy-informed decoding strategy that facilitates
higher autoregressive generation quality with faster synthesis speed.
Specifically, the proposed method introduces two main innovations: 1) dynamic
temperature control guided by spatial entropy of token distributions, enhancing
the balance between content diversity, alignment accuracy, and structural
coherence in both mask-based and scale-wise models, without extra computational
overhead, and 2) entropy-aware acceptance rules in speculative decoding,
achieving near-lossless generation at about 85\% of the inference cost of
conventional acceleration methods. Extensive experiments across multiple
benchmarks using diverse AR image generation models demonstrate the
effectiveness and generalizability of our approach in enhancing both generation
quality and sampling speed.",cs.CV,2025-10-10T05:26:11+00:00,2025-10-10T05:26:11+00:00,http://arxiv.org/abs/2510.09012v1,,,Code is available at https://github.com/krennic999/ARsample
12,2509.20160v1,Characterizing the Performance of Accelerated Jetson Edge Devices for Training Deep Learning Models,"Prashanthi S. K., Sai Anuroop Kesanapalli, Yogesh Simmhan","Deep Neural Networks (DNNs) have had a significant impact on domains like
autonomous vehicles and smart cities through low-latency inferencing on edge
computing devices close to the data source. However, DNN training on the edge
is poorly explored. Techniques like federated learning and the growing capacity
of GPU-accelerated edge devices like NVIDIA Jetson motivate the need for a
holistic characterization of DNN training on the edge. Training DNNs is
resource-intensive and can stress an edge's GPU, CPU, memory and storage
capacities. Edge devices also have different resources compared to workstations
and servers, such as slower shared memory and diverse storage media. Here, we
perform a principled study of DNN training on individual devices of three
contemporary Jetson device types: AGX Xavier, Xavier NX and Nano for three
diverse DNN model--dataset combinations. We vary device and training parameters
such as I/O pipelining and parallelism, storage media, mini-batch sizes and
power modes, and examine their effect on CPU and GPU utilization, fetch stalls,
training time, energy usage, and variability. Our analysis exposes several
resource inter-dependencies and counter-intuitive insights, while also helping
quantify known wisdom. Our rigorous study can help tune the training
performance on the edge, trade-off time and energy usage on constrained
devices, and even select an ideal edge hardware for a DNN workload, and, in
future, extend to federated learning too. As an illustration, we use these
results to build a simple model to predict the training time and energy per
epoch for any given DNN across different power modes, with minimal additional
profiling.",cs.DC,2025-09-24T14:28:37+00:00,2025-09-24T14:28:37+00:00,http://arxiv.org/abs/2509.20160v1,10.1145/3570604,,Preprint of article in ACM SIGMETRICS 2023
13,2510.05586v1,CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval,"Bin Kang, Bin Chen, Junjie Wang, Yulin Li, Junzhi Zhao, Zhuotao Tian","Existing Visual Language Models (VLMs) suffer structural limitations where a
few low contribution tokens may excessively capture global semantics,
dominating the information aggregation process and suppressing the
discriminative features in text-driven image retrieval tasks. To address this,
we introduce \textbf{CalibCLIP}, a training-free method designed to calibrate
the suppressive effect of dominant tokens. Specifically, in the visual space,
we propose the Contrastive Visual Enhancer (CVE), which decouples visual
features into target and low information regions. Subsequently, it identifies
dominant tokens and dynamically suppresses their representations.In the textual
space, we introduce the Discriminative Concept Calibrator (DCC), which aims to
differentiate between general and discriminative concepts within the text
query. By mitigating the challenges posed by generic concepts and improving the
representations of discriminative concepts, DCC strengthens the differentiation
among similar samples. Finally, extensive experiments demonstrate consistent
improvements across seven benchmarks spanning three image retrieval tasks,
underscoring the effectiveness of CalibCLIP. Code is available at:
https://github.com/kangbin98/CalibCLIP",cs.CV,2025-10-07T05:16:29+00:00,2025-10-07T05:16:29+00:00,http://arxiv.org/abs/2510.05586v1,10.1145/3746027.3755765,,ACMMM2025(oral)
14,2509.20666v1,Understanding Mode Switching in Human-AI Collaboration: Behavioral Insights and Predictive Modeling,"Avinash Ajit Nargund, Arthur Caetano, Kevin Yang, Rose Yiwei Liu, Philip Tezaur, Kriteen Shrestha, Qisen Pan, Tobias Höllerer, Misha Sra","Human-AI collaboration is typically offered in one of two of user control
levels: guidance, where the AI provides suggestions and the human makes the
final decision, and delegation, where the AI acts autonomously within
user-defined constraints. Systems that integrate both modes, common in robotic
surgery or driving assistance, often overlook shifts in user preferences within
a task in response to factors like evolving trust, decision complexity, and
perceived control. In this work, we investigate how users dynamically switch
between higher and lower levels of control during a sequential decision-making
task. Using a hand-and-brain chess setup, participants either selected a piece
and the AI decided how it moved (brain mode), or the AI selected a piece and
the participant decided how it moved (hand mode). We collected over 400
mode-switching decisions from eight participants, along with gaze, emotional
state, and subtask difficulty data. Statistical analysis revealed significant
differences in gaze patterns and subtask complexity prior to a switch and in
the quality of the subsequent move. Based on these results, we engineered
behavioral and task-specific features to train a lightweight model that
predicted control level switches ($F1 = 0.65$). The model performance suggests
that real-time behavioral signals can serve as a complementary input alongside
system-driven mode-switching mechanisms currently used. We complement our
quantitative results with qualitative factors that influence switching
including perceived AI ability, decision complexity, and level of control,
identified from post-game interview analysis. The combined behavioral and
modeling insights can help inform the design of shared autonomy systems that
need dynamic, subtask-level control switches aligned with user intent and
evolving task demands.","cs.HC, cs.AI",2025-09-25T01:58:46+00:00,2025-09-25T01:58:46+00:00,http://arxiv.org/abs/2509.20666v1,,,
15,2510.11682v1,Ego-Vision World Model for Humanoid Contact Planning,"Hang Liu, Yuman Gao, Sangli Teng, Yufeng Chi, Yakun Sophia Shao, Zhongyu Li, Maani Ghaffari, Koushil Sreenath","Enabling humanoid robots to exploit physical contact, rather than simply
avoid collisions, is crucial for autonomy in unstructured environments.
Traditional optimization-based planners struggle with contact complexity, while
on-policy reinforcement learning (RL) is sample-inefficient and has limited
multi-task ability. We propose a framework combining a learned world model with
sampling-based Model Predictive Control (MPC), trained on a demonstration-free
offline dataset to predict future outcomes in a compressed latent space. To
address sparse contact rewards and sensor noise, the MPC uses a learned
surrogate value function for dense, robust planning. Our single, scalable model
supports contact-aware tasks, including wall support after perturbation,
blocking incoming objects, and traversing height-limited arches, with improved
data efficiency and multi-task capability over on-policy RL. Deployed on a
physical humanoid, our system achieves robust, real-time contact planning from
proprioception and ego-centric depth images. Website:
https://ego-vcp.github.io/","cs.RO, cs.AI, cs.SY, eess.SY",2025-10-13T17:47:39+00:00,2025-10-13T17:47:39+00:00,http://arxiv.org/abs/2510.11682v1,,,
16,2510.06396v1,Adaptive Protein Design Protocols and Middleware,"Aymen Alsaadi, Jonathan Ash, Mikhail Titov, Matteo Turilli, Andre Merzky, Shantenu Jha, Sagar Khare","Computational protein design is experiencing a transformation driven by
AI/ML. However, the range of potential protein sequences and structures is
astronomically vast, even for moderately sized proteins. Hence, achieving
convergence between generated and predicted structures demands substantial
computational resources for sampling. The Integrated Machine-learning for
Protein Structures at Scale (IMPRESS) offers methods and advanced computing
systems for coupling AI to high-performance computing tasks, enabling the
ability to evaluate the effectiveness of protein designs as they are developed,
as well as the models and simulations used to generate data and train models.
This paper introduces IMPRESS and demonstrates the development and
implementation of an adaptive protein design protocol and its supporting
computing infrastructure. This leads to increased consistency in the quality of
protein design and enhanced throughput of protein design due to dynamic
resource allocation and asynchronous workload execution.","cs.DC, cs.AI, cs.PF, cs.SE",2025-10-07T19:23:53+00:00,2025-10-07T19:23:53+00:00,http://arxiv.org/abs/2510.06396v1,10.1109/IPDPSW66978.2025.00157,,
17,2509.25671v1,The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks,"Arda Uzunoglu, Tianjian Li, Daniel Khashabi","Benchmarks shape scientific conclusions about model capabilities and steer
model development. This creates a feedback loop: stronger benchmarks drive
better models, and better models demand more discriminative benchmarks.
Ensuring benchmark reliability is therefore essential for trustworthy
evaluation and meaningful progress. In this work, we study benchmark
reliability from a distributional perspective and introduce benchmark harmony,
which measures how uniformly a model's performance is distributed across the
subdomains of a benchmark. We posit that high harmony is a desirable benchmark
property, indicating that the aggregate metric reflects uniform competence
across subdomains. Across 19 multiple-choice benchmarks and five model
families, we map each benchmark onto a mean-variance plane of harmony computed
across models, where high mean and low variance signal more reliable
evaluation. Our analysis shows that less harmonious benchmarks can give
misleading results, since overall accuracy may be disproportionately influenced
by specific subdomains. For instance, ARC-Easy is overwhelmed by questions on
Biological Concepts, overshadowing other critical subdomains such as Geography,
Physics, Chemistry, and Environmental Science. By recommending that harmony
should be reported alongside accuracy, we reframe evaluation from simple
performance averages to a more robust, distributionally reliable measurement of
performance.","cs.CL, cs.SE",2025-09-30T02:14:30+00:00,2025-09-30T02:14:30+00:00,http://arxiv.org/abs/2509.25671v1,,,
18,2509.19459v1,Automated Insertion of Flushes and Fences for Persistency,"Yutong Guo, Weiyu Luo, Brian Demsky","CXL shared memory and persistent memory allow the contents of memory to
persist beyond crashes. Stores to persistent or CXL memory are typically not
immediately made persistent; developers must manually flush the corresponding
cache lines to force the data to be written to the underlying storage.
Correctly using flush and fence operations is known to be challenging. While
state-of-the-art tools can find missing flush instructions, they often require
bug-revealing test cases. No existing tools can ensure the absence of missing
flush bugs.
  In this paper, we present PMRobust, a compiler that automatically inserts
flush and fence operations to ensure that code using persistent memory is free
from missing flush and fence bugs. PMRobust employs a novel static analysis
with optimizations that target newly allocated objects. We have evaluated
PMRobust on persistent memory libraries and several persistent memory data
structures and measured a geometric mean overhead of 0.26% relative to the
original benchmarks with hand-placed flush and fence operations.","cs.SE, cs.PL",2025-09-23T18:14:21+00:00,2025-09-23T18:14:21+00:00,http://arxiv.org/abs/2509.19459v1,,,
19,2510.04220v1,MASC: Boosting Autoregressive Image Generation with a Manifold-Aligned Semantic Clustering,"Lixuan He, Shikang Zheng, Linfeng Zhang","Autoregressive (AR) models have shown great promise in image generation, yet
they face a fundamental inefficiency stemming from their core component: a
vast, unstructured vocabulary of visual tokens. This conventional approach
treats tokens as a flat vocabulary, disregarding the intrinsic structure of the
token embedding space where proximity often correlates with semantic
similarity. This oversight results in a highly complex prediction task, which
hinders training efficiency and limits final generation quality. To resolve
this, we propose Manifold-Aligned Semantic Clustering (MASC), a principled
framework that constructs a hierarchical semantic tree directly from the
codebook's intrinsic structure. MASC employs a novel geometry-aware distance
metric and a density-driven agglomerative construction to model the underlying
manifold of the token embeddings. By transforming the flat, high-dimensional
prediction task into a structured, hierarchical one, MASC introduces a
beneficial inductive bias that significantly simplifies the learning problem
for the AR model. MASC is designed as a plug-and-play module, and our extensive
experiments validate its effectiveness: it accelerates training by up to 57%
and significantly improves generation quality, reducing the FID of LlamaGen-XL
from 2.87 to 2.58. MASC elevates existing AR frameworks to be highly
competitive with state-of-the-art methods, establishing that structuring the
prediction space is as crucial as architectural innovation for scalable
generative modeling.","cs.CV, cs.AI, cs.LG",2025-10-05T14:23:51+00:00,2025-10-05T14:23:51+00:00,http://arxiv.org/abs/2510.04220v1,,,
20,2510.11188v1,Protein as a Second Language for LLMs,"Xinhui Chen, Zuchao Li, Mengqi Gao, Yufeng Zhang, Chak Tou Leong, Haoyang Li, Jiaqi Chen","Deciphering the function of unseen protein sequences is a fundamental
challenge with broad scientific impact, yet most existing methods depend on
task-specific adapters or large-scale supervised fine-tuning. We introduce the
""Protein-as-Second-Language"" framework, which reformulates amino-acid sequences
as sentences in a novel symbolic language that large language models can
interpret through contextual exemplars. Our approach adaptively constructs
sequence-question-answer triples that reveal functional cues in a zero-shot
setting, without any further training. To support this process, we curate a
bilingual corpus of 79,926 protein-QA instances spanning attribute prediction,
descriptive understanding, and extended reasoning. Empirically, our method
delivers consistent gains across diverse open-source LLMs and GPT-4, achieving
up to 17.2% ROUGE-L improvement (average +7%) and even surpassing fine-tuned
protein-specific language models. These results highlight that generic LLMs,
when guided with protein-as-language cues, can outperform domain-specialized
models, offering a scalable pathway for protein understanding in foundation
models.","cs.LG, cs.AI, q-bio.BM",2025-10-13T09:21:45+00:00,2025-10-13T09:21:45+00:00,http://arxiv.org/abs/2510.11188v1,,,"Main paper: 9 pages, 6 figures. With references and appendix: 18
  pages, 9 figures total. Submitted to ICLR 2026 (under review)"
21,2510.08783v1,MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces,"Reuben A. Luera, Ryan Rossi, Franck Dernoncourt, Samyadeep Basu, Sungchul Kim, Subhojyoti Mukherjee, Puneet Mathur, Ruiyi Zhang, Jihyung Kil, Nedim Lipka, Seunghyun Yoon, Jiuxiang Gu, Zichao Wang, Cindy Xiong Bearfield, Branislav Kveton","In an ideal design pipeline, user interface (UI) design is intertwined with
user research to validate decisions, yet studies are often resource-constrained
during early exploration. Recent advances in multimodal large language models
(MLLMs) offer a promising opportunity to act as early evaluators, helping
designers narrow options before formal testing. Unlike prior work that
emphasizes user behavior in narrow domains such as e-commerce with metrics like
clicks or conversions, we focus on subjective user evaluations across varied
interfaces. We investigate whether MLLMs can mimic human preferences when
evaluating individual UIs and comparing them. Using data from a crowdsourcing
platform, we benchmark GPT-4o, Claude, and Llama across 30 interfaces and
examine alignment with human judgments on multiple UI factors. Our results show
that MLLMs approximate human preferences on some dimensions but diverge on
others, underscoring both their potential and limitations in supplementing
early UX research.","cs.HC, cs.AI",2025-10-09T20:00:41+00:00,2025-10-09T20:00:41+00:00,http://arxiv.org/abs/2510.08783v1,,,
22,2509.16518v1,FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers,"Sankeerth Durvasula, Kavya Sreedhar, Zain Moustafa, Suraj Kothawade, Ashish Gondimalla, Suvinay Subramanian, Narges Shahidi, Nandita Vijaykumar","Generating realistic videos with diffusion transformers demands significant
computation, with attention layers the central bottleneck; even producing a
short clip requires running a transformer over a very long sequence of
embeddings, e.g., more than 30K embeddings for a 5-second video, incurring
significant latency. Prior work aims to mitigate this bottleneck by exploiting
sparsity in the attention layers to reduce computation. However, these works
typically rely on block-sparse attention, which skips score computation only
when all entries in a block of attention scores (corresponding to M queries and
M keys, with M = 64 typically) are zero. This coarse-granular skipping of
attention scores does not fully exploit sparsity in the attention map and
leaves room for improvement. In this work, we propose FG-Attn, a sparse
attention mechanism for long-context diffusion transformers that leverages
sparsity at a fine granularity. Unlike block-sparse attention, which skips
entire MxM blocks, our approach skips computations at the granularity of Mx1
slices of the attention map. Each slice is produced by query-key dot products
between a block of query vectors and a single key. To implement our proposed
sparse attention mechanism, we develop a new efficient bulk-load operation
called asynchronous-gather load. This load operation gathers a sparse set of
relevant key-value vectors from memory and arranges them into packed tiles in
the GPU's shared memory. Only a sparse set of keys relevant to those queries
are loaded into shared memory when computing attention for a block of queries,
in contrast to loading full blocks of key tokens in block-sparse attention. Our
fine-grained sparse attention, applied to video diffusion models, achieves an
average 1.55X (up to 1.65X) speedup for 5 second, 480p videos, and an average
1.41X (up to 1.49X) for 5 second, 720p videos on a single H100 GPU.","cs.CV, cs.AR",2025-09-20T03:48:32+00:00,2025-09-20T03:48:32+00:00,http://arxiv.org/abs/2509.16518v1,,,
23,2510.06504v1,Text2Interact: High-Fidelity and Diverse Text-to-Two-Person Interaction Generation,"Qingxuan Wu, Zhiyang Dou, Chuan Guo, Yiming Huang, Qiao Feng, Bing Zhou, Jian Wang, Lingjie Liu","Modeling human-human interactions from text remains challenging because it
requires not only realistic individual dynamics but also precise,
text-consistent spatiotemporal coupling between agents. Currently, progress is
hindered by 1) limited two-person training data, inadequate to capture the
diverse intricacies of two-person interactions; and 2) insufficiently
fine-grained text-to-interaction modeling, where language conditioning
collapses rich, structured prompts into a single sentence embedding. To address
these limitations, we propose our Text2Interact framework, designed to generate
realistic, text-aligned human-human interactions through a scalable
high-fidelity interaction data synthesizer and an effective spatiotemporal
coordination pipeline. First, we present InterCompose, a scalable
synthesis-by-composition pipeline that aligns LLM-generated interaction
descriptions with strong single-person motion priors. Given a prompt and a
motion for an agent, InterCompose retrieves candidate single-person motions,
trains a conditional reaction generator for another agent, and uses a neural
motion evaluator to filter weak or misaligned samples-expanding interaction
coverage without extra capture. Second, we propose InterActor, a
text-to-interaction model with word-level conditioning that preserves
token-level cues (initiation, response, contact ordering) and an adaptive
interaction loss that emphasizes contextually relevant inter-person joint
pairs, improving coupling and physical plausibility for fine-grained
interaction modeling. Extensive experiments show consistent gains in motion
diversity, fidelity, and generalization, including out-of-distribution
scenarios and user studies. We will release code and models to facilitate
reproducibility.",cs.CV,2025-10-07T22:41:23+00:00,2025-10-07T22:41:23+00:00,http://arxiv.org/abs/2510.06504v1,,,
24,2510.06919v1,Bayesian Nonparametric Dynamical Clustering of Time Series,"Adrián Pérez-Herrero, Paulo Félix, Jesús Presedo, Carl Henrik Ek","We present a method that models the evolution of an unbounded number of time
series clusters by switching among an unknown number of regimes with linear
dynamics. We develop a Bayesian non-parametric approach using a hierarchical
Dirichlet process as a prior on the parameters of a Switching Linear Dynamical
System and a Gaussian process prior to model the statistical variations in
amplitude and temporal alignment within each cluster. By modeling the evolution
of time series patterns, the method avoids unnecessary proliferation of
clusters in a principled manner. We perform inference by formulating a
variational lower bound for off-line and on-line scenarios, enabling efficient
learning through optimization. We illustrate the versatility and effectiveness
of the approach through several case studies of electrocardiogram analysis
using publicly available databases.","stat.ML, cs.AI, cs.LG, stat.AP, I.5; I.2.1",2025-10-08T11:52:39+00:00,2025-10-08T11:52:39+00:00,http://arxiv.org/abs/2510.06919v1,,,"This work has been submitted to the IEEE for possible publication. 15
  pages. 9 figures"
25,2510.09848v1,Cell Instance Segmentation: The Devil Is in the Boundaries,"Peixian Liang, Yifan Ding, Yizhe Zhang, Jianxu Chen, Hao Zheng, Hongxiao Wang, Yejia Zhang, Guangyu Meng, Tim Weninger, Michael Niemier, X. Sharon Hu, Danny Z Chen","State-of-the-art (SOTA) methods for cell instance segmentation are based on
deep learning (DL) semantic segmentation approaches, focusing on distinguishing
foreground pixels from background pixels. In order to identify cell instances
from foreground pixels (e.g., pixel clustering), most methods decompose
instance information into pixel-wise objectives, such as distances to
foreground-background boundaries (distance maps), heat gradients with the
center point as heat source (heat diffusion maps), and distances from the
center point to foreground-background boundaries with fixed angles (star-shaped
polygons). However, pixel-wise objectives may lose significant geometric
properties of the cell instances, such as shape, curvature, and convexity,
which require a collection of pixels to represent. To address this challenge,
we present a novel pixel clustering method, called Ceb (for Cell boundaries),
to leverage cell boundary features and labels to divide foreground pixels into
cell instances. Starting with probability maps generated from semantic
segmentation, Ceb first extracts potential foreground-foreground boundaries
with a revised Watershed algorithm. For each boundary candidate, a boundary
feature representation (called boundary signature) is constructed by sampling
pixels from the current foreground-foreground boundary as well as the
neighboring background-foreground boundaries. Next, a boundary classifier is
used to predict its binary boundary label based on the corresponding boundary
signature. Finally, cell instances are obtained by dividing or merging
neighboring regions based on the predicted boundary labels. Extensive
experiments on six datasets demonstrate that Ceb outperforms existing pixel
clustering methods on semantic segmentation probability maps. Moreover, Ceb
achieves highly competitive performance compared to SOTA cell instance
segmentation methods.",cs.CV,2025-10-10T20:24:20+00:00,2025-10-10T20:24:20+00:00,http://arxiv.org/abs/2510.09848v1,,,Accepted at IEEE Transactions On Medical Imaging (TMI)
26,2510.10910v1,SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model,"Honghui Yuan, Keiji Yanai","With the rapid development of diffusion models, style transfer has made
remarkable progress. However, flexible and localized style editing for scene
text remains an unsolved challenge. Although existing scene text editing
methods have achieved text region editing, they are typically limited to
content replacement and simple styles, which lack the ability of free-style
transfer. In this paper, we introduce SceneTextStylizer, a novel training-free
diffusion-based framework for flexible and high-fidelity style transfer of text
in scene images. Unlike prior approaches that either perform global style
transfer or focus solely on textual content modification, our method enables
prompt-guided style transformation specifically for text regions, while
preserving both text readability and stylistic consistency. To achieve this, we
design a feature injection module that leverages diffusion model inversion and
self-attention to transfer style features effectively. Additionally, a region
control mechanism is introduced by applying a distance-based changing mask at
each denoising step, enabling precise spatial control. To further enhance
visual quality, we incorporate a style enhancement module based on the Fourier
transform to reinforce stylistic richness. Extensive experiments demonstrate
that our method achieves superior performance in scene text style
transformation, outperforming existing state-of-the-art methods in both visual
fidelity and text preservation.","cs.CV, eess.IV",2025-10-13T02:11:57+00:00,2025-10-13T02:11:57+00:00,http://arxiv.org/abs/2510.10910v1,,,
27,2510.07582v1,"Type, Ability, and Effect Systems: Perspectives on Purity, Semantics, and Expressiveness","Yuyan Bao, Tiark Rompf","Programming benefits from a clear separation between pure, mathematical
computation and impure, effectful interaction with the world. Existing
approaches to enforce this separation include monads, type-and-effect systems,
and capability systems. All share a tension between precision and usability,
and each one has non-obvious strengths and weaknesses.
  This paper aims to raise the bar in assessing such systems. First, we propose
a semantic definition of purity, inspired by contextual equivalence, as a
baseline independent of any specific typing discipline. Second, we propose that
expressiveness should be measured by the degree of completeness, i.e., how many
semantically pure terms can be typed as pure. Using this measure, we focus on
minimal meaningful effect and capability systems and show that they are
incomparable, i.e., neither subsumes the other in terms of expressiveness.
  Based on this result, we propose a synthesis and show that type, ability, and
effect systems combine their respective strengths while avoiding their
weaknesses. As part of our formal model, we provide a logical relation to
facilitate proofs of purity and other properties for a variety of effect typing
disciplines.",cs.PL,2025-10-08T22:01:00+00:00,2025-10-08T22:01:00+00:00,http://arxiv.org/abs/2510.07582v1,,,
28,2509.26173v1,Understanding Collective Social Behavior in OSS Communities: A Co-editing Network Analysis of Activity Cascades,"Lisi Qarkaxhija, Maximilian Carparo, Stefan Menzel, Bernhard Sendhoff, Ingo Scholtes","Understanding the collective social behavior of software developers is
crucial to model and predict the long-term dynamics and sustainability of Open
Source Software (OSS) communities. To this end, we analyze temporal activity
patterns of developers, revealing an inherently ``bursty'' nature of commit
contributions. To investigate the social mechanisms behind this phenomenon, we
adopt a network-based modelling framework that captures developer interactions
through co-editing networks. Our framework models social interactions, where a
developer editing the code of other developers triggers accelerated activity
among collaborators. Using a large data set on 50 major OSS communities, we
further develop a method that identifies activity cascades, i.e. the
propagation of developer activity in the underlying co-editing network. Our
results suggest that activity cascades are a statistically significant
phenomenon in more than half of the studied projects. We further show that our
insights can be used to develop a simple yet practical churn prediction method
that forecasts which developers are likely to leave a project. Our work sheds
light on the emergent collective social dynamics in OSS communities and
highlights the importance of activity cascades to understand developer churn
and retention in collaborative software projects.",cs.SE,2025-09-30T12:28:35+00:00,2025-09-30T12:28:35+00:00,http://arxiv.org/abs/2509.26173v1,,,
29,2509.17914v1,XaaS Containers: Performance-Portable Representation With Source and IR Containers,"Marcin Copik, Eiman Alnuaimi, Alok Kamatar, Valerie Hayot-Sasson, Alberto Madonna, Todd Gamblin, Kyle Chard, Ian Foster, Torsten Hoefler","High-performance computing (HPC) systems and cloud data centers are
converging, and containers are becoming the default method of portable software
deployment. Yet, while containers simplify software management, they face
significant performance challenges in HPC environments as they must sacrifice
hardware-specific optimizations to achieve portability. Although HPC containers
can use runtime hooks to access optimized MPI libraries and GPU devices, they
are limited by application binary interface (ABI) compatibility and cannot
overcome the effects of early-stage compilation decisions. Acceleration as a
Service (XaaS) proposes a vision of performance-portable containers, where a
containerized application should achieve peak performance across all HPC
systems. We present a practical realization of this vision through Source and
Intermediate Representation (IR) containers, where we delay
performance-critical decisions until the target system specification is known.
We analyze specialization mechanisms in HPC software and propose a new
LLM-assisted method for automatic discovery of specializations. By examining
the compilation pipeline, we develop a methodology to build containers
optimized for target architectures at deployment time. Our prototype
demonstrates that new XaaS containers combine the convenience of
containerization with the performance benefits of system-specialized builds.",cs.DC,2025-09-22T15:39:33+00:00,2025-09-22T15:39:33+00:00,http://arxiv.org/abs/2509.17914v1,10.1145/3712285.3759868,,"Accepted at the International Conference for High Performance
  Computing, Networking, Storage and Analysis (SC'25)"
30,2509.11937v1,MMORE: Massive Multimodal Open RAG & Extraction,"Alexandre Sallinen, Stefan Krsteski, Paul Teiletche, Marc-Antoine Allard, Baptiste Lecoeur, Michael Zhang, Fabrice Nemo, David Kalajdzic, Matthias Meyer, Mary-Anne Hartley","We introduce MMORE, an open-source pipeline for Massive Multimodal Open
RetrievalAugmented Generation and Extraction, designed to ingest, transform,
and retrieve knowledge from heterogeneous document formats at scale. MMORE
supports more than fifteen file types, including text, tables, images, emails,
audio, and video, and processes them into a unified format to enable downstream
applications for LLMs. The architecture offers modular, distributed processing,
enabling scalable parallelization across CPUs and GPUs. On processing
benchmarks, MMORE demonstrates a 3.8-fold speedup over single-node baselines
and 40% higher accuracy than Docling on scanned PDFs. The pipeline integrates
hybrid dense-sparse retrieval and supports both interactive APIs and batch RAG
endpoints. Evaluated on PubMedQA, MMORE-augmented medical LLMs improve
biomedical QA accuracy with increasing retrieval depth. MMORE provides a
robust, extensible foundation for deploying task-agnostic RAG systems on
diverse, real-world multimodal data. The codebase is available at
https://github.com/swiss-ai/mmore.","cs.SE, cs.AI, D.2.0; E.m",2025-09-15T13:56:06+00:00,2025-09-15T13:56:06+00:00,http://arxiv.org/abs/2509.11937v1,,,"This paper was originally submitted to the CODEML workshop for ICML
  2025. 9 pages (including references and appendices)"
31,2510.09489v1,Two-Stage Gaussian Splatting Optimization for Outdoor Scene Reconstruction,"Deborah Pintani, Ariel Caputo, Noah Lewis, Marc Stamminger, Fabio Pellacini, Andrea Giachetti","Outdoor scene reconstruction remains challenging due to the stark contrast
between well-textured, nearby regions and distant backgrounds dominated by low
detail, uneven illumination, and sky effects. We introduce a two-stage Gaussian
Splatting framework that explicitly separates and optimizes these regions,
yielding higher-fidelity novel view synthesis. In stage one, background
primitives are initialized within a spherical shell and optimized using a loss
that combines a background-only photometric term with two geometric
regularizers: one constraining Gaussians to remain inside the shell, and
another aligning them with local tangential planes. In stage two, foreground
Gaussians are initialized from a Structure-from-Motion reconstruction, added
and refined using the standard rendering loss, while the background set remains
fixed but contributes to the final image formation. Experiments on diverse
outdoor datasets show that our method reduces background artifacts and improves
perceptual quality compared to state-of-the-art baselines. Moreover, the
explicit background separation enables automatic, object-free environment map
estimation, opening new possibilities for photorealistic outdoor rendering and
mixed-reality applications.",cs.GR,2025-10-10T15:52:23+00:00,2025-10-10T15:52:23+00:00,http://arxiv.org/abs/2510.09489v1,,,
32,2510.02836v1,"VR as a ""Drop-In"" Well-being Tool for Knowledge Workers","Sophia Ppali, Haris Psallidopoulos, Marios Constantinides, Fotis Liarokapis","Virtual Reality (VR) is increasingly being used to support workplace
well-being, but many interventions focus narrowly on a single activity or goal.
Our work explores how VR can meet the diverse physical and mental needs of
knowledge workers. We developed Tranquil Loom, a VR app offering stretching,
guided meditation, and open exploration across four environments. The app
includes an AI assistant that suggests activities based on users' emotional
states. We conducted a two-phase mixed-methods study: (1) interviews with 10
knowledge workers to guide the app's design, and (2) deployment with 35
participants gathering usage data, well-being measures, and interviews. Results
showed increases in mindfulness and reductions in anxiety. Participants enjoyed
both structured and open-ended activities, often using the app playfully. While
AI suggestions were used infrequently, they prompted ideas for future
personalization. Overall, participants viewed VR as a flexible, ``drop-in''
tool, highlighting its value for situational rather than prescriptive
well-being support.",cs.HC,2025-10-03T09:18:36+00:00,2025-10-03T09:18:36+00:00,http://arxiv.org/abs/2510.02836v1,,,"11 pages, 5 figures, 1 table"
33,2510.05740v1,Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect,"Amirtaha Amanzadi, Zahra Dehghanian, Hamid Beigy, Hamid R. Rabiee","The rapid development of generative models has made it increasingly crucial
to develop detectors that can reliably detect synthetic images. Although most
of the work has now focused on cross-generator generalization, we argue that
this viewpoint is too limited. Detecting synthetic images involves another
equally important challenge: generalization across visual domains. To bridge
this gap,we present the OmniGen Benchmark. This comprehensive evaluation
dataset incorporates 12 state-of-the-art generators, providing a more realistic
way of evaluating detector performance under realistic conditions. In addition,
we introduce a new method, FusionDetect, aimed at addressing both vectors of
generalization. FusionDetect draws on the benefits of two frozen foundation
models: CLIP & Dinov2. By deriving features from both complementary models,we
develop a cohesive feature space that naturally adapts to changes in both
thecontent and design of the generator. Our extensive experiments demonstrate
that FusionDetect delivers not only a new state-of-the-art, which is 3.87% more
accurate than its closest competitor and 6.13% more precise on average on
established benchmarks, but also achieves a 4.48% increase in accuracy on
OmniGen,along with exceptional robustness to common image perturbations. We
introduce not only a top-performing detector, but also a new benchmark and
framework for furthering universal AI image detection. The code and dataset are
available at http://github.com/amir-aman/FusionDetect","cs.CV, cs.AI",2025-10-07T10:01:32+00:00,2025-10-07T10:01:32+00:00,http://arxiv.org/abs/2510.05740v1,,,Project code: http://github.com/amir-aman/FusionDetect
34,2510.00956v1,Bridging the Gap Between Simulated and Real Network Data Using Transfer Learning,"Carlos Güemes-Palau, Miquel Ferriol-Galmés, Jordi Paillisse-Vilanova, Albert López-Brescó, Pere Barlet-Ros, Albert Cabellos-Aparicio","Machine Learning (ML)-based network models provide fast and accurate
predictions for complex network behaviors but require substantial training
data. Collecting such data from real networks is often costly and limited,
especially for critical scenarios like failures. As a result, researchers
commonly rely on simulated data, which reduces accuracy when models are
deployed in real environments. We propose a hybrid approach leveraging transfer
learning to combine simulated and real-world data. Using RouteNet-Fermi, we
show that fine-tuning a pre-trained model with a small real dataset
significantly improves performance. Our experiments with OMNeT++ and a custom
testbed reduce the Mean Absolute Percentage Error (MAPE) in packet delay
prediction by up to 88%. With just 10 real scenarios, MAPE drops by 37%, and
with 50 scenarios, by 48%.","cs.NI, cs.AI, cs.LG",2025-10-01T14:29:47+00:00,2025-10-01T14:29:47+00:00,http://arxiv.org/abs/2510.00956v1,,,"This paper was submitted to IEEE ICC 2026. 7 Pages, 5 Figures"
35,2509.14126v1,CrazyMARL: Decentralized Direct Motor Control Policies for Cooperative Aerial Transport of Cable-Suspended Payloads,"Viktor Lorentz, Khaled Wahba, Sayantan Auddy, Marc Toussaint, Wolfgang Hönig","Collaborative transportation of cable-suspended payloads by teams of Unmanned
Aerial Vehicles (UAVs) has the potential to enhance payload capacity, adapt to
different payload shapes, and provide built-in compliance, making it attractive
for applications ranging from disaster relief to precision logistics. However,
multi-UAV coordination under disturbances, nonlinear payload dynamics, and
slack--taut cable modes remains a challenging control problem. To our
knowledge, no prior work has addressed these cable mode transitions in the
multi-UAV context, instead relying on simplifying rigid-link assumptions. We
propose CrazyMARL, a decentralized Reinforcement Learning (RL) framework for
multi-UAV cable-suspended payload transport. Simulation results demonstrate
that the learned policies can outperform classical decentralized controllers in
terms of disturbance rejection and tracking precision, achieving an 80%
recovery rate from harsh conditions compared to 44% for the baseline method. We
also achieve successful zero-shot sim-to-real transfer and demonstrate that our
policies are highly robust under harsh conditions, including wind, random
external disturbances, and transitions between slack and taut cable dynamics.
This work paves the way for autonomous, resilient UAV teams capable of
executing complex payload missions in unstructured environments.","cs.RO, cs.MA",2025-09-17T16:06:59+00:00,2025-09-17T16:06:59+00:00,http://arxiv.org/abs/2509.14126v1,,,This work has been submitted to IEEE for possible publication
36,2510.10069v1,SyncLipMAE: Contrastive Masked Pretraining for Audio-Visual Talking-Face Representation,"Zeyu Ling, Xiaodong Gu, Jiangnan Tang, Changqing Zou","We introduce SyncLipMAE, a self-supervised pretraining framework for
talking-face video that learns synchronization-aware and transferable facial
dynamics from unlabeled audio-visual streams. Our approach couples masked
visual modeling with cross-modal contrastive alignment and employs three
per-frame prompt tokens that explicitly encode the essential factors of a
talking-face frame - identity, vocal motion (speech-synchronized facial
dynamics), and ambient motion (audio-agnostic movements such as blinks and head
pose). The contrastive objective uses time-aligned vocal-motion and audio
tokens as positives and misaligned pairs as negatives, driving both modalities
into a shared embedding space and yielding token-level audio-visual stream
synchronization. After pretraining, the aligned audio tokens together with the
visual prompt tokens (identity, vocal motion, ambient motion) form a unified
interface for four disparate downstream settings: (i) audio-visual stream
synchronization; (ii) facial emotion and head/face action recognition; (iii)
visual speech recognition; and (iv) visual dubbing, for which we enable
indistinguishable audio- or video-driven control within a single model. Across
four task families that require distinct capabilities, SyncLipMAE achieves
state-of-the-art results, underscoring the effectiveness of
synchronization-aware, factorized self-supervised pretraining.","cs.AI, cs.MM",2025-10-11T07:12:44+00:00,2025-10-11T07:12:44+00:00,http://arxiv.org/abs/2510.10069v1,,,
37,2509.23449v1,Beyond Embeddings: Interpretable Feature Extraction for Binary Code Similarity,"Charles E. Gagnon, Steven H. H. Ding, Philippe Charland, Benjamin C. M. Fung","Binary code similarity detection is a core task in reverse engineering. It
supports malware analysis and vulnerability discovery by identifying
semantically similar code in different contexts. Modern methods have progressed
from manually engineered features to vector representations. Hand-crafted
statistics (e.g., operation ratios) are interpretable, but shallow and fail to
generalize. Embedding-based methods overcome this by learning robust
cross-setting representations, but these representations are opaque vectors
that prevent rapid verification. They also face a scalability-accuracy
trade-off, since high-dimensional nearest-neighbor search requires
approximations that reduce precision. Current approaches thus force a
compromise between interpretability, generalizability, and scalability.
  We bridge these gaps using a language model-based agent to conduct structured
reasoning analysis of assembly code and generate features such as input/output
types, side effects, notable constants, and algorithmic intent. Unlike
hand-crafted features, they are richer and adaptive. Unlike embeddings, they
are human-readable, maintainable, and directly searchable with inverted or
relational indexes. Without any matching training, our method respectively
achieves 42% and 62% for recall@1 in cross-architecture and cross-optimization
tasks, comparable to embedding methods with training (39% and 34%). Combined
with embeddings, it significantly outperforms the state-of-the-art,
demonstrating that accuracy, scalability, and interpretability can coexist.","cs.AI, cs.CR, cs.SE",2025-09-27T18:34:32+00:00,2025-09-27T18:34:32+00:00,http://arxiv.org/abs/2509.23449v1,,,"17 pages, 7 figures, submitted to USENIX Security '26"
38,2509.16517v1,Seeing Culture: A Benchmark for Visual Reasoning and Grounding,"Burak Satar, Zhixin Ma, Patrick A. Irawan, Wilfried A. Mulyawan, Jing Jiang, Ee-Peng Lim, Chong-Wah Ngo","Multimodal vision-language models (VLMs) have made substantial progress in
various tasks that require a combined understanding of visual and textual
content, particularly in cultural understanding tasks, with the emergence of
new cultural datasets. However, these datasets frequently fall short of
providing cultural reasoning while underrepresenting many cultures. In this
paper, we introduce the Seeing Culture Benchmark (SCB), focusing on cultural
reasoning with a novel approach that requires VLMs to reason on culturally rich
images in two stages: i) selecting the correct visual option with
multiple-choice visual question answering (VQA), and ii) segmenting the
relevant cultural artifact as evidence of reasoning. Visual options in the
first stage are systematically organized into three types: those originating
from the same country, those from different countries, or a mixed group.
Notably, all options are derived from a singular category for each type.
Progression to the second stage occurs only after a correct visual option is
chosen. The SCB benchmark comprises 1,065 images that capture 138 cultural
artifacts across five categories from seven Southeast Asia countries, whose
diverse cultures are often overlooked, accompanied by 3,178 questions, of which
1,093 are unique and meticulously curated by human annotators. Our evaluation
of various VLMs reveals the complexities involved in cross-modal cultural
reasoning and highlights the disparity between visual reasoning and spatial
grounding in culturally nuanced scenarios. The SCB serves as a crucial
benchmark for identifying these shortcomings, thereby guiding future
developments in the field of cultural reasoning.
https://github.com/buraksatar/SeeingCulture","cs.CV, cs.AI, cs.CL, cs.MM",2025-09-20T03:47:49+00:00,2025-09-20T03:47:49+00:00,http://arxiv.org/abs/2509.16517v1,,,"Accepted to EMNLP 2025 Main Conference,
  https://seeingculture-benchmark.github.io/"
39,2510.04648v1,EduPersona: Benchmarking Subjective Ability Boundaries of Virtual Student Agents,"Buyuan Zhu, Shiyu Hu, Yiping Ma, Yuanming Zhang, Kang Hao Cheong","As large language models are increasingly integrated into education, virtual
student agents are becoming vital for classroom simulation and teacher
training. Yet their classroom-oriented subjective abilities remain largely
unassessed, limiting understanding of model boundaries and hindering
trustworthy deployment. We present EduPersona, a large-scale benchmark spanning
two languages, three subjects, and ten persona types based on the Big Five
theory. The dataset contains 1,308 authentic classroom dialogue rounds,
corresponding to 12,814 teacher-student Q&A turns, and is further expanded
through persona stylization into roughly 10 times larger scale (128k turns),
providing a solid foundation for evaluation. Building on this resource, we
decompose hard-to-quantify subjective performance into three progressive tasks:
TASK1 basic coherence (whether behavior, emotion, expression, and voice align
with classroom context), TASK2 student realism, and TASK3 long-term persona
consistency, thereby establishing an evaluation framework grounded in
educational theory and research value. We conduct systematic experiments on
three representative LLMs, comparing their original versions with ten
persona-fine-tuned variants trained on EduPersona. Results show consistent and
significant average improvements across all tasks: TASK1 +33.6%, TASK2 +30.6%,
and TASK3 +14.9%. These improvements highlight the dataset's effectiveness and
research value, while also revealing the heterogeneous difficulty of persona
modeling. In summary, EduPersona delivers the first classroom benchmark
centered on subjective abilities, establishes a decoupled and verifiable
research paradigm, and we will open-source both the dataset and the framework
to support the broader research community in advancing trustworthy and
human-like AI for education.","cs.CV, cs.CY",2025-10-06T09:52:18+00:00,2025-10-06T09:52:18+00:00,http://arxiv.org/abs/2510.04648v1,,,"Preprint, Under review"
40,2510.08409v1,Optimal Stopping in Latent Diffusion Models,"Yu-Han Wu, Quentin Berthet, Gérard Biau, Claire Boyer, Romuald Elie, Pierre Marion","We identify and analyze a surprising phenomenon of Latent Diffusion Models
(LDMs) where the final steps of the diffusion can degrade sample quality. In
contrast to conventional arguments that justify early stopping for numerical
stability, this phenomenon is intrinsic to the dimensionality reduction in
LDMs. We provide a principled explanation by analyzing the interaction between
latent dimension and stopping time. Under a Gaussian framework with linear
autoencoders, we characterize the conditions under which early stopping is
needed to minimize the distance between generated and target distributions.
More precisely, we show that lower-dimensional representations benefit from
earlier termination, whereas higher-dimensional latent spaces require later
stopping time. We further establish that the latent dimension interplays with
other hyperparameters of the problem such as constraints in the parameters of
score matching. Experiments on synthetic and real datasets illustrate these
properties, underlining that early stopping can improve generative quality.
Together, our results offer a theoretical foundation for understanding how the
latent dimension influences the sample quality, and highlight stopping time as
a key hyperparameter in LDMs.","stat.ML, cs.LG",2025-10-09T16:28:48+00:00,2025-10-09T16:28:48+00:00,http://arxiv.org/abs/2510.08409v1,,,
41,2510.09171v1,Instance-Level Generation for Representation Learning,"Yankun Wu, Zakaria Laskar, Giorgos Kordopatis-Zilos, Noa Garcia, Giorgos Tolias","Instance-level recognition (ILR) focuses on identifying individual objects
rather than broad categories, offering the highest granularity in image
classification. However, this fine-grained nature makes creating large-scale
annotated datasets challenging, limiting ILR's real-world applicability across
domains. To overcome this, we introduce a novel approach that synthetically
generates diverse object instances from multiple domains under varied
conditions and backgrounds, forming a large-scale training set. Unlike prior
work on automatic data synthesis, our method is the first to address
ILR-specific challenges without relying on any real images. Fine-tuning
foundation vision models on the generated data significantly improves retrieval
performance across seven ILR benchmarks spanning multiple domains. Our approach
offers a new, efficient, and effective alternative to extensive data collection
and curation, introducing a new ILR paradigm where the only input is the names
of the target domains, unlocking a wide range of real-world applications.",cs.CV,2025-10-10T09:14:33+00:00,2025-10-10T09:14:33+00:00,http://arxiv.org/abs/2510.09171v1,,,
42,2509.21170v1,Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach,"Yongda Yu, Guohao Shi, Xianwei Wu, Haochuan He, XueMing Gu, Qianqian Zhao, Kui Liu, Qiushi Wang, Zhao Tian, Haifeng Shen, Guoping Rong","Large Language Models (LLMs) have shown great potential in supporting
automated code review due to their impressive capabilities in context
understanding and reasoning. However, these capabilities are still limited
compared to human-level cognition because they are heavily influenced by the
training data. Recent research has demonstrated significantly improved
performance through fine-tuning LLMs with code review data. However, compared
to human reviewers who often simultaneously analyze multiple dimensions of code
review to better identify issues, the full potential of these methods is
hampered by the limited or vague information used to fine-tune the models. This
paper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that
trains LLMs with an impressive reasoning ability to analyze multiple dimensions
of code review by harnessing long COT techniques to provide rich structured
information. To address context loss and reasoning logic loss issues that
frequently occur when LLMs process long COT prompts, we propose a solution that
combines the Maximum Entropy (ME) modeling principle with pre-defined reasoning
pathways in MelcotCR to enable more effective utilization of in-context
knowledge within long COT prompts while strengthening the logical tightness of
the reasoning process. Empirical evaluations on our curated MelcotCR dataset
and the public CodeReviewer dataset reveal that a low-parameter base model,
such as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art
methods in terms of the accuracy of detecting and describing code issues, with
its performance remarkably on par with that of the 671B DeepSeek-R1 model.","cs.SE, cs.AI, D.2.3; I.2.7",2025-09-25T13:51:56+00:00,2025-09-25T13:51:56+00:00,http://arxiv.org/abs/2509.21170v1,,,22 pages
43,2509.19669v1,Games Are Not Equal: Classifying Cloud Gaming Contexts for Effective User Experience Measurement,"Yifan Wang, Minzhao Lyu, Vijay Sivaraman","To tap into the growing market of cloud gaming, whereby game graphics is
rendered in the cloud and streamed back to the user as a video feed, network
operators are creating monetizable assurance services that dynamically
provision network resources. However, without accurately measuring cloud gaming
user experience, they cannot assess the effectiveness of their provisioning
methods. Basic measures such as bandwidth and frame rate by themselves do not
suffice, and can only be interpreted in the context of the game played and the
player activity within the game. This paper equips the network operator with a
method to obtain a real-time measure of cloud gaming experience by analyzing
network traffic, including contextual factors such as the game title and player
activity stage. Our method is able to classify the game title within the first
five seconds of game launch, and continuously assess the player activity stage
as being active, passive, or idle. We deploy it in an ISP hosting NVIDIA cloud
gaming servers for the region. We provide insights from hundreds of thousands
of cloud game streaming sessions over a three-month period into the dependence
of bandwidth consumption and experience level on the gameplay contexts.","cs.NI, cs.AI",2025-09-24T01:02:24+00:00,2025-09-24T01:02:24+00:00,http://arxiv.org/abs/2509.19669v1,10.1145/3730567.3764455,,"This paper is accepted at ACM Internet Measurement Conference (IMC)
  2025. In Proc. ACM IMC, Oct, 2025, Madison, WI, USA"
44,2510.10682v1,Action-Dynamics Modeling and Cross-Temporal Interaction for Online Action Understanding,"Xinyu Yang, Zheheng Jiang, Feixiang Zhou, Yihang Zhu, Na Lv, Nan Xing, Huiyu Zhou","Action understanding, encompassing action detection and anticipation, plays a
crucial role in numerous practical applications. However, untrimmed videos are
often characterized by substantial redundant information and noise. Moreover,
in modeling action understanding, the influence of the agent's intention on the
action is often overlooked. Motivated by these issues, we propose a novel
framework called the State-Specific Model (SSM), designed to unify and enhance
both action detection and anticipation tasks. In the proposed framework, the
Critical State-Based Memory Compression module compresses frame sequences into
critical states, reducing information redundancy. The Action Pattern Learning
module constructs a state-transition graph with multi-dimensional edges to
model action dynamics in complex scenarios, on the basis of which potential
future cues can be generated to represent intention. Furthermore, our
Cross-Temporal Interaction module models the mutual influence between
intentions and past as well as current information through cross-temporal
interactions, thereby refining present and future features and ultimately
realizing simultaneous action detection and anticipation. Extensive experiments
on multiple benchmark datasets -- including EPIC-Kitchens-100, THUMOS'14,
TVSeries, and the introduced Parkinson's Disease Mouse Behaviour (PDMB) dataset
-- demonstrate the superior performance of our proposed framework compared to
other state-of-the-art approaches. These results highlight the importance of
action dynamics learning and cross-temporal interactions, laying a foundation
for future action understanding research.",cs.CV,2025-10-12T16:10:40+00:00,2025-10-12T16:10:40+00:00,http://arxiv.org/abs/2510.10682v1,,,"10 pages, 9 figures"
45,2510.06842v1,Continual Action Quality Assessment via Adaptive Manifold-Aligned Graph Regularization,"Kanglei Zhou, Qingyi Pan, Xingxing Zhang, Hubert P. H. Shum, Frederick W. B. Li, Xiaohui Liang, Liyuan Wang","Action Quality Assessment (AQA) quantifies human actions in videos,
supporting applications in sports scoring, rehabilitation, and skill
evaluation. A major challenge lies in the non-stationary nature of quality
distributions in real-world scenarios, which limits the generalization ability
of conventional methods. We introduce Continual AQA (CAQA), which equips AQA
with Continual Learning (CL) capabilities to handle evolving distributions
while mitigating catastrophic forgetting. Although parameter-efficient
fine-tuning of pretrained models has shown promise in CL for image
classification, we find it insufficient for CAQA. Our empirical and theoretical
analyses reveal two insights: (i) Full-Parameter Fine-Tuning (FPFT) is
necessary for effective representation learning; yet (ii) uncontrolled FPFT
induces overfitting and feature manifold shift, thereby aggravating forgetting.
To address this, we propose Adaptive Manifold-Aligned Graph Regularization
(MAGR++), which couples backbone fine-tuning that stabilizes shallow layers
while adapting deeper ones with a two-step feature rectification pipeline: a
manifold projector to translate deviated historical features into the current
representation space, and a graph regularizer to align local and global
distributions. We construct four CAQA benchmarks from three datasets with
tailored evaluation protocols and strong baselines, enabling systematic
cross-dataset comparison. Extensive experiments show that MAGR++ achieves
state-of-the-art performance, with average correlation gains of 3.6% offline
and 12.2% online over the strongest baseline, confirming its robustness and
effectiveness. Our code is available at https://github.com/ZhouKanglei/MAGRPP.",cs.CV,2025-10-08T10:09:47+00:00,2025-10-08T10:09:47+00:00,http://arxiv.org/abs/2510.06842v1,,,Extended Version of MAGR (ECCV 2024 Oral Presentation)
46,2510.04652v1,Modeling and Managing Temporal Obligations in GUCON Using SPARQL-star and RDF-star,"Ines Akaichi, Giorgos Flouris, Irini Fundulaki, Sabrina Kirrane","In the digital age, data frequently crosses organizational and jurisdictional
boundaries, making effective governance essential. Usage control policies have
emerged as a key paradigm for regulating data usage, safeguarding privacy,
protecting intellectual property, and ensuring compliance with regulations. A
central mechanism for usage control is the handling of obligations, which arise
as a side effect of using and sharing data. Effective monitoring of obligations
requires capturing usage traces and accounting for temporal aspects such as
start times and deadlines, as obligations may evolve over times into different
states, such as fulfilled, violated, or expired. While several solutions have
been proposed for obligation monitoring, they often lack formal semantics or
provide limited support for reasoning over obligation states. To address these
limitations, we extend GUCON, a policy framework grounded in the formal
semantics of SPAQRL graph patterns, to explicitly model the temporal aspects of
an obligation. This extension enables the expressing of temporal obligations
and supports continuous monitoring of their evolving states based on usage
traces stored in temporal knowledge graphs. We demonstrate how this extended
model can be represented using RDF-star and SPARQL-star and propose an
Obligation State Manager that monitors obligation states and assess their
compliance with respect to usage traces. Finally, we evaluate both the extended
model and its prototype implementation.","cs.CR, cs.SY, eess.SY",2025-10-06T09:58:05+00:00,2025-10-06T09:58:05+00:00,http://arxiv.org/abs/2510.04652v1,,,
47,2510.10113v1,ImmerIris: A Large-Scale Dataset and Benchmark for Immersive Iris Recognition in Open Scenes,"Yuxi Mi, Qiuyang Yuan, Zhizhou Zhong, Xuan Zhao, Jiaogen Zhou, Fubao Zhu, Jihong Guan, Shuigeng Zhou","In egocentric applications such as augmented and virtual reality, immersive
iris recognition is emerging as an accurate and seamless way to identify
persons. While classic systems acquire iris images on-axis, i.e., via dedicated
frontal sensors in controlled settings, the immersive setup primarily captures
off-axis irises through tilt-placed headset cameras, with only mild control in
open scenes. This yields unique challenges, including perspective distortion,
intensified quality degradations, and intra-class variations in iris texture.
Datasets capturing these challenges remain scarce. To fill this gap, this paper
introduces ImmerIris, a large-scale dataset collected via VR headsets,
containing 499,791 ocular images from 564 subjects. It is, to the best of
current knowledge, the largest public dataset and among the first dedicated to
off-axis acquisition. Based on ImmerIris, evaluation protocols are constructed
to benchmark recognition methods under different challenging factors. Current
methods, primarily designed for classic on-axis imagery, perform
unsatisfactorily on the immersive setup, mainly due to reliance on fallible
normalization. To this end, this paper further proposes a normalization-free
paradigm that directly learns from ocular images with minimal adjustment.
Despite its simplicity, this approach consistently outperforms
normalization-based counterparts, pointing to a promising direction for robust
immersive recognition.",cs.CV,2025-10-11T08:43:38+00:00,2025-10-11T08:43:38+00:00,http://arxiv.org/abs/2510.10113v1,,,
48,2510.07459v1,MoGU: Mixture-of-Gaussians with Uncertainty-based Gating for Time Series Forecasting,"Yoli Shavit, Jacob Goldberger","We introduce Mixture-of-Gaussians with Uncertainty-based Gating (MoGU), a
novel Mixture-of-Experts (MoE) framework designed for regression tasks and
applied to time series forecasting. Unlike conventional MoEs that provide only
point estimates, MoGU models each expert's output as a Gaussian distribution.
This allows it to directly quantify both the forecast (the mean) and its
inherent uncertainty (variance). MoGU's core innovation is its
uncertainty-based gating mechanism, which replaces the traditional input-based
gating network by using each expert's estimated variance to determine its
contribution to the final prediction. Evaluated across diverse time series
forecasting benchmarks, MoGU consistently outperforms single-expert models and
traditional MoE setups. It also provides well-quantified, informative
uncertainties that directly correlate with prediction errors, enhancing
forecast reliability. Our code is available from:
https://github.com/yolish/moe_unc_tsf","cs.LG, cs.AI",2025-10-08T19:04:25+00:00,2025-10-08T19:04:25+00:00,http://arxiv.org/abs/2510.07459v1,,,
49,2510.02934v1,Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection,"Thanh Trong Vu, Tuan-Dung Bui, Thu-Trang Nguyen, Son Nguyen, Hieu Dinh Vo","Large Language Models (LLMs) have demonstrated impressive capabilities in
code generation and are increasingly integrated into the software development
process. However, ensuring the correctness of LLM-generated code remains a
critical concern. Prior work has shown that the internal representations of
LLMs encode meaningful signals for assessing code correctness. Nevertheless,
the existing methods rely on representations from pre-selected/fixed layers and
token positions, which could limit its generalizability across diverse model
architectures and tasks. In this work, we introduce AUTOPROBE, a novel
model-agnostic approach that dynamically selects the most informative internal
representations for code correctness assessment. AUTOPROBE employs an
attention-based mechanism to learn importance scores for hidden states,
enabling it to focus on the most relevant features. These weighted
representations are then aggregated and passed to a probing classifier to
predict code correctness across multiple dimensions, including compilability,
functionality, and security. To evaluate the performance of AUTOPROBE, we
conduct extensive experiments across multiple benchmarks and code LLMs. Our
experimental results show that AUTOPROBE consistently outperforms the
baselines. For security assessment, AUTOPROBE surpasses the state-of-the-art
white-box approach by 18%. For compilability and functionality assessment,
AUTOPROBE demonstrates its highest robustness to code complexity, with the
performance higher than the other approaches by up to 19% and 111%,
respectively. These findings highlight that dynamically selecting important
internal signals enables AUTOPROBE to serve as a robust and generalizable
solution for assessing the correctness of code generated by various LLMs.",cs.SE,2025-10-03T12:25:28+00:00,2025-10-03T12:25:28+00:00,http://arxiv.org/abs/2510.02934v1,,,
50,2510.07319v1,Temporal Prompting Matters: Rethinking Referring Video Object Segmentation,"Ci-Siang Lin, Min-Hung Chen, I-Jieh Liu, Chien-Yi Wang, Sifei Liu, Yu-Chiang Frank Wang","Referring Video Object Segmentation (RVOS) aims to segment the object
referred to by the query sentence in the video. Most existing methods require
end-to-end training with dense mask annotations, which could be
computation-consuming and less scalable. In this work, we rethink the RVOS
problem and aim to investigate the key to this task. Based on existing
foundation segmentation models, we decompose the RVOS task into referring,
video, and segmentation factors, and propose a Temporal Prompt Generation and
Selection (Tenet) framework to address the referring and video factors while
leaving the segmentation problem to foundation models. To efficiently adapt
image-based foundation segmentation models to referring video object
segmentation, we leverage off-the-shelf object detectors and trackers to
produce temporal prompts associated with the referring sentence. While
high-quality temporal prompts could be produced, they can not be easily
identified from confidence scores. To tackle this issue, we propose Prompt
Preference Learning to evaluate the quality of the produced temporal prompts.
By taking such prompts to instruct image-based foundation segmentation models,
we would be able to produce high-quality masks for the referred object,
enabling efficient model adaptation to referring video object segmentation.
Experiments on RVOS benchmarks demonstrate the effectiveness of the Tenet
framework.",cs.CV,2025-10-08T17:59:57+00:00,2025-10-08T17:59:57+00:00,http://arxiv.org/abs/2510.07319v1,,,
51,2509.26633v2,OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction,"Lujie Yang, Xiaoyu Huang, Zhen Wu, Angjoo Kanazawa, Pieter Abbeel, Carmelo Sferrazza, C. Karen Liu, Rocky Duan, Guanya Shi","A dominant paradigm for teaching humanoid robots complex skills is to
retarget human motions as kinematic references to train reinforcement learning
(RL) policies. However, existing retargeting pipelines often struggle with the
significant embodiment gap between humans and robots, producing physically
implausible artifacts like foot-skating and penetration. More importantly,
common retargeting methods neglect the rich human-object and human-environment
interactions essential for expressive locomotion and loco-manipulation. To
address this, we introduce OmniRetarget, an interaction-preserving data
generation engine based on an interaction mesh that explicitly models and
preserves the crucial spatial and contact relationships between an agent, the
terrain, and manipulated objects. By minimizing the Laplacian deformation
between the human and robot meshes while enforcing kinematic constraints,
OmniRetarget generates kinematically feasible trajectories. Moreover,
preserving task-relevant interactions enables efficient data augmentation, from
a single demonstration to different robot embodiments, terrains, and object
configurations. We comprehensively evaluate OmniRetarget by retargeting motions
from OMOMO, LAFAN1, and our in-house MoCap datasets, generating over 8-hour
trajectories that achieve better kinematic constraint satisfaction and contact
preservation than widely used baselines. Such high-quality data enables
proprioceptive RL policies to successfully execute long-horizon (up to 30
seconds) parkour and loco-manipulation skills on a Unitree G1 humanoid, trained
with only 5 reward terms and simple domain randomization shared by all tasks,
without any learning curriculum.","cs.RO, cs.AI, cs.LG, cs.SY, eess.SY",2025-09-30T17:59:02+00:00,2025-10-08T23:16:20+00:00,http://arxiv.org/abs/2509.26633v2,,,Project website: https://omniretarget.github.io
52,2510.08003v1,CIR-CoT: Towards Interpretable Composed Image Retrieval via End-to-End Chain-of-Thought Reasoning,"Weihuang Lin, Yiwei Ma, Jiayi Ji, Xiaoshuai Sun, Rongrong Ji","Composed Image Retrieval (CIR), which aims to find a target image from a
reference image and a modification text, presents the core challenge of
performing unified reasoning across visual and semantic modalities. While
current approaches based on Vision-Language Models (VLMs, e.g., CLIP) and more
recent Multimodal Large Language Models (MLLMs, e.g., Qwen-VL) have shown
progress, they predominantly function as ``black boxes."" This inherent opacity
not only prevents users from understanding the retrieval rationale but also
restricts the models' ability to follow complex, fine-grained instructions. To
overcome these limitations, we introduce CIR-CoT, the first end-to-end
retrieval-oriented MLLM designed to integrate explicit Chain-of-Thought (CoT)
reasoning. By compelling the model to first generate an interpretable reasoning
chain, CIR-CoT enhances its ability to capture crucial cross-modal
interactions, leading to more accurate retrieval while making its decision
process transparent. Since existing datasets like FashionIQ and CIRR lack the
necessary reasoning data, a key contribution of our work is the creation of
structured CoT annotations using a three-stage process involving a caption,
reasoning, and conclusion. Our model is then fine-tuned to produce this
structured output before encoding its final retrieval intent into a dedicated
embedding. Comprehensive experiments show that CIR-CoT achieves highly
competitive performance on in-domain datasets (FashionIQ, CIRR) and
demonstrates remarkable generalization on the out-of-domain CIRCO dataset,
establishing a new path toward more effective and trustworthy retrieval
systems.",cs.CV,2025-10-09T09:41:45+00:00,2025-10-09T09:41:45+00:00,http://arxiv.org/abs/2510.08003v1,,,
53,2509.19096v2,Investigating Traffic Accident Detection Using Multimodal Large Language Models,"Ilhan Skender, Kailin Tong, Selim Solmaz, Daniel Watzenig","Traffic safety remains a critical global concern, with timely and accurate
accident detection essential for hazard reduction and rapid emergency response.
Infrastructure-based vision sensors offer scalable and efficient solutions for
continuous real-time monitoring, facilitating automated detection of accidents
directly from captured images. This research investigates the zero-shot
capabilities of multimodal large language models (MLLMs) for detecting and
describing traffic accidents using images from infrastructure cameras, thus
minimizing reliance on extensive labeled datasets. Main contributions include:
(1) Evaluation of MLLMs using the simulated DeepAccident dataset from CARLA,
explicitly addressing the scarcity of diverse, realistic, infrastructure-based
accident data through controlled simulations; (2) Comparative performance
analysis between Gemini 1.5 and 2.0, Gemma 3 and Pixtral models in accident
identification and descriptive capabilities without prior fine-tuning; and (3)
Integration of advanced visual analytics, specifically YOLO for object
detection, Deep SORT for multi-object tracking, and Segment Anything (SAM) for
instance segmentation, into enhanced prompts to improve model accuracy and
explainability. Key numerical results show Pixtral as the top performer with an
F1-score of 0.71 and 83% recall, while Gemini models gained precision with
enhanced prompts (e.g., Gemini 1.5 rose to 90%) but suffered notable F1 and
recall losses. Gemma 3 offered the most balanced performance with minimal
metric fluctuation. These findings demonstrate the substantial potential of
integrating MLLMs with advanced visual analytics techniques, enhancing their
applicability in real-world automated traffic monitoring systems.","cs.CV, cs.SE",2025-09-23T14:47:33+00:00,2025-09-24T08:42:59+00:00,http://arxiv.org/abs/2509.19096v2,,,"Accepted for presentation at the 2025 IEEE International Automated
  Vehicle Validation Conference (IAVVC 2025). Final version to appear in IEEE
  Xplore"
54,2510.00881v1,Advancing Automated Ethical Profiling in SE: a Zero-Shot Evaluation of LLM Reasoning,"Patrizio Migliarini, Mashal Afzal Memon, Marco Autili, Paola Inverardi","Large Language Models (LLMs) are increasingly integrated into software
engineering (SE) tools for tasks that extend beyond code synthesis, including
judgment under uncertainty and reasoning in ethically significant contexts. We
present a fully automated framework for assessing ethical reasoning
capabilities across 16 LLMs in a zero-shot setting, using 30 real-world
ethically charged scenarios. Each model is prompted to identify the most
applicable ethical theory to an action, assess its moral acceptability, and
explain the reasoning behind their choice. Responses are compared against
expert ethicists' choices using inter-model agreement metrics. Our results show
that LLMs achieve an average Theory Consistency Rate (TCR) of 73.3% and Binary
Agreement Rate (BAR) on moral acceptability of 86.7%, with interpretable
divergences concentrated in ethically ambiguous cases. A qualitative analysis
of free-text explanations reveals strong conceptual convergence across models
despite surface-level lexical diversity. These findings support the potential
viability of LLMs as ethical inference engines within SE pipelines, enabling
scalable, auditable, and adaptive integration of user-aligned ethical
reasoning. Our focus is the Ethical Interpreter component of a broader
profiling pipeline: we evaluate whether current LLMs exhibit sufficient
interpretive stability and theory-consistent reasoning to support automated
profiling.","cs.SE, cs.AI",2025-10-01T13:28:26+00:00,2025-10-01T13:28:26+00:00,http://arxiv.org/abs/2510.00881v1,,,Accepted at ASE 2025
55,2510.10806v1,Is Implicit Knowledge Enough for LLMs? A RAG Approach for Tree-based Structures,"Mihir Gupte, Paolo Giusto, Ramesh S","Large Language Models (LLMs) are adept at generating responses based on
information within their context. While this ability is useful for interacting
with structured data like code files, another popular method,
Retrieval-Augmented Generation (RAG), retrieves relevant documents to augment
the model's in-context learning. However, it is not well-explored how to best
represent this retrieved knowledge for generating responses on structured data,
particularly hierarchical structures like trees. In this work, we propose a
novel bottom-up method to linearize knowledge from tree-like structures (like a
GitHub repository) by generating implicit, aggregated summaries at each
hierarchical level. This approach enables the knowledge to be stored in a
knowledge base and used directly with RAG. We then compare our method to using
RAG on raw, unstructured code, evaluating the accuracy and quality of the
generated responses. Our results show that while response quality is comparable
across both methods, our approach generates over 68% fewer documents in the
retriever, a significant gain in efficiency. This finding suggests that
leveraging implicit, linearized knowledge may be a highly effective and
scalable strategy for handling complex, hierarchical data structures.","cs.CL, cs.AI, cs.IR, cs.LG",2025-10-12T20:52:43+00:00,2025-10-12T20:52:43+00:00,http://arxiv.org/abs/2510.10806v1,,,Waiting for Conference Response
56,2509.21816v1,No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials,"Yuhang Xie, Jian Mu, Xiaojun Ma, Chaoyun Zhang, Lu Wang, Mengyu Zhou, Mugeng Liu, Si Qin, Qingwei Lin, Saravan Rajmohan, Shi Han, Dongmei Zhang","Excel is one of the most widely used productivity tools across domains,
offering rich functionality but also overwhelming users with its complexity.
This creates a persistent demand for tutorials to support effective usage.
However, existing tutorials are manually authored by experts, require frequent
updates after each software release, and incur substantial labor costs. Prior
work has not achieved fully automated tutorial generation, since existing
methods still depend on handcrafted operation sequences or example materials.
In this paper, we present the first framework for automatically generating
Excel tutorials directly from natural language task descriptions. Our framework
first instantiates the task. Then a central component of this framework,
Execution Agent, plans and executes the solution in Excel, and collects the
intermediate artifacts required for tutorial construction. These artifacts are
then transformed into both structured Excel documents and video demonstrations.
To build a comprehensive tutorial corpus, we collected 1,559 task descriptions
from real-world scenarios. In addition, we designed a systematic evaluation
framework that integrates assessments from both large language models (LLMs)
and human reviewers. Experimental results show that our framework improves task
execution success rates by 8.5% over state-of-the-art baselines. Moreover, the
generated tutorials demonstrate superior readability and instructional
effectiveness, often approaching or surpassing expert-authored materials.
Importantly, the automated pipeline eliminates manual labor and reduces time
costs to 1/20 of expert authoring, making scalable and high-quality tutorial
generation practical for the first time.",cs.SE,2025-09-26T03:21:39+00:00,2025-09-26T03:21:39+00:00,http://arxiv.org/abs/2509.21816v1,,,
57,2510.10248v1,Reasoning-Enhanced Large Language Models for Molecular Property Prediction,"Jiaxi Zhuang, Yaorui Shi, Jue Hou, Yunong He, Mingwei Ye, Mingjun Xu, Yuming Su, Linfeng Zhang, Linfeng Zhang, Guolin Ke, Hengxing Cai","Molecular property prediction is crucial for drug discovery and materials
science, yet existing approaches suffer from limited interpretability, poor
cross-task generalization, and lack of chemical reasoning capabilities.
Traditional machine learning models struggle with task transferability, while
specialized molecular language models provide little insight into their
decision-making processes. To address these limitations, we propose
\textbf{MPPReasoner}, a multimodal large language model that incorporates
chemical reasoning for molecular property prediction. Our approach, built upon
Qwen2.5-VL-7B-Instruct, integrates molecular images with SMILES strings to
enable comprehensive molecular understanding. We develop a two-stage training
strategy: supervised fine-tuning (SFT) using 16,000 high-quality reasoning
trajectories generated through expert knowledge and multiple teacher models,
followed by Reinforcement Learning from Principle-Guided Rewards (RLPGR). RLPGR
employs verifiable, rule-based rewards that systematically evaluate chemical
principle application, molecular structure analysis, and logical consistency
through computational verification. Extensive experiments across 8 datasets
demonstrate significant performance improvements, with MPPReasoner
outperforming the best baselines by 7.91\% and 4.53\% on in-distribution and
out-of-distribution tasks respectively. MPPReasoner exhibits exceptional
cross-task generalization and generates chemically sound reasoning paths that
provide valuable insights into molecular property analysis, substantially
enhancing both interpretability and practical utility for chemists. Code is
available at https://anonymous.4open.science/r/MPPReasoner-12687.","cs.LG, cs.AI",2025-10-11T15:05:45+00:00,2025-10-11T15:05:45+00:00,http://arxiv.org/abs/2510.10248v1,,,
58,2509.21917v1,Taming Flow-based I2V Models for Creative Video Editing,"Xianghao Kong, Hansheng Chen, Yuwei Guo, Lvmin Zhang, Gordon Wetzstein, Maneesh Agrawala, Anyi Rao","Although image editing techniques have advanced significantly, video editing,
which aims to manipulate videos according to user intent, remains an emerging
challenge. Most existing image-conditioned video editing methods either require
inversion with model-specific design or need extensive optimization, limiting
their capability of leveraging up-to-date image-to-video (I2V) models to
transfer the editing capability of image editing models to the video domain. To
this end, we propose IF-V2V, an Inversion-Free method that can adapt
off-the-shelf flow-matching-based I2V models for video editing without
significant computational overhead. To circumvent inversion, we devise Vector
Field Rectification with Sample Deviation to incorporate information from the
source video into the denoising process by introducing a deviation term into
the denoising vector field. To further ensure consistency with the source video
in a model-agnostic way, we introduce Structure-and-Motion-Preserving
Initialization to generate motion-aware temporally correlated noise with
structural information embedded. We also present a Deviation Caching mechanism
to minimize the additional computational cost for denoising vector
rectification without significantly impacting editing quality. Evaluations
demonstrate that our method achieves superior editing quality and consistency
over existing approaches, offering a lightweight plug-and-play solution to
realize visual creativity.","cs.CV, cs.MM",2025-09-26T05:57:04+00:00,2025-09-26T05:57:04+00:00,http://arxiv.org/abs/2509.21917v1,,,
59,2509.16772v1,"AI Knows Best? The Paradox of Expertise, AI-Reliance, and Performance in Educational Tutoring Decision-Making Tasks","Eason Chen, Jeffrey Li, Scarlett Huang, Xinyi Tang, Jionghao Lin, Paulo Carvalho, Kenneth Koedinger","We present an empirical study of how both experienced tutors and non-tutors
judge the correctness of tutor praise responses under different Artificial
Intelligence (AI)-assisted interfaces, types of explanation (textual
explanations vs. inline highlighting). We first fine-tuned several Large
Language Models (LLMs) to produce binary correctness labels and explanations,
achieving up to 88% accuracy and 0.92 F1 score with GPT-4. We then let the
GPT-4 models assist 95 participants in tutoring decision-making tasks by
offering different types of explanations. Our findings show that although
human-AI collaboration outperforms humans alone in evaluating tutor responses,
it remains less accurate than AI alone. Moreover, we find that non-tutors tend
to follow the AI's advice more consistently, which boosts their overall
accuracy on the task: especially when the AI is correct. In contrast,
experienced tutors often override the AI's correct suggestions and thus miss
out on potential gains from the AI's generally high baseline accuracy. Further
analysis reveals that explanations in text reasoning will increase
over-reliance and reduce underreliance, while inline highlighting does not.
Moreover, neither explanation style actually has a significant effect on
performance and costs participants more time to complete the task, instead of
saving time. Our findings reveal a tension between expertise, explanation
design, and efficiency in AI-assisted decision-making, highlighting the need
for balanced approaches that foster more effective human-AI collaboration.",cs.HC,2025-09-20T18:38:54+00:00,2025-09-20T18:38:54+00:00,http://arxiv.org/abs/2509.16772v1,,,
60,2510.09379v1,Task-Level Insights from Eigenvalues across Sequence Models,"Rahel Rickenbach, Jelena Trisovic, Alexandre Didier, Jerome Sieber, Melanie N. Zeilinger","Although softmax attention drives state-of-the-art performance for sequence
models, its quadratic complexity limits scalability, motivating linear
alternatives such as state space models (SSMs). While these alternatives
improve efficiency, their fundamental differences in information processing
remain poorly understood. In this work, we leverage the recently proposed
dynamical systems framework to represent softmax, norm and linear attention as
dynamical systems, enabling a structured comparison with SSMs by analyzing
their respective eigenvalue spectra. Since eigenvalues capture essential
aspects of dynamical system behavior, we conduct an extensive empirical
analysis across diverse sequence models and benchmarks. We first show that
eigenvalues influence essential aspects of memory and long-range dependency
modeling, revealing spectral signatures that align with task requirements.
Building on these insights, we then investigate how architectural modifications
in sequence models impact both eigenvalue spectra and task performance. This
correspondence further strengthens the position of eigenvalue analysis as a
principled metric for interpreting, understanding, and ultimately improving the
capabilities of sequence models.","cs.LG, cs.AI, cs.SY, eess.SY",2025-10-10T13:35:21+00:00,2025-10-10T13:35:21+00:00,http://arxiv.org/abs/2510.09379v1,,,
61,2509.12042v1,FinGEAR: Financial Mapping-Guided Enhanced Answer Retrieval,"Ying Li, Mengyu Wang, Miguel de Carvalho, Sotirios Sabanis, Tiejun Ma","Financial disclosures such as 10-K filings present challenging retrieval
problems due to their length, regulatory section hierarchy, and domain-specific
language, which standard retrieval-augmented generation (RAG) models underuse.
We introduce FinGEAR (Financial Mapping-Guided Enhanced Answer Retrieval), a
retrieval framework tailored to financial documents. FinGEAR combines a finance
lexicon for Item-level guidance (FLAM), dual hierarchical indices for
within-Item search (Summary Tree and Question Tree), and a two-stage
cross-encoder reranker. This design aligns retrieval with disclosure structure
and terminology, enabling fine-grained, query-aware context selection.
Evaluated on full 10-Ks with queries aligned to the FinQA dataset, FinGEAR
delivers consistent gains in precision, recall, F1, and relevancy, improving F1
by up to 56.7% over flat RAG, 12.5% over graph-based RAGs, and 217.6% over
prior tree-based systems, while also increasing downstream answer accuracy with
a fixed reader. By jointly modeling section hierarchy and domain lexicon
signals, FinGEAR improves retrieval fidelity and provides a practical
foundation for high-stakes financial analysis.","cs.CE, cs.CL",2025-09-15T15:25:26+00:00,2025-09-15T15:25:26+00:00,http://arxiv.org/abs/2509.12042v1,,,
62,2510.08669v1,FreqCa: Accelerating Diffusion Models via Frequency-Aware Caching,"Jiacheng Liu, Peiliang Cai, Qinming Zhou, Yuqi Lin, Deyang Kong, Benhao Huang, Yupei Pan, Haowen Xu, Chang Zou, Junshu Tang, Shikang Zheng, Linfeng Zhang","The application of diffusion transformers is suffering from their significant
inference costs. Recently, feature caching has been proposed to solve this
problem by reusing features from previous timesteps, thereby skipping
computation in future timesteps. However, previous feature caching assumes that
features in adjacent timesteps are similar or continuous, which does not always
hold in all settings. To investigate this, this paper begins with an analysis
from the frequency domain, which reveal that different frequency bands in the
features of diffusion models exhibit different dynamics across timesteps.
Concretely, low-frequency components, which decide the structure of images,
exhibit higher similarity but poor continuity. In contrast, the high-frequency
bands, which decode the details of images, show significant continuity but poor
similarity. These interesting observations motivate us to propose
Frequency-aware Caching (FreqCa)
  which directly reuses features of low-frequency components based on their
similarity, while using a second-order Hermite interpolator to predict the
volatile high-frequency ones based on its continuity.
  Besides, we further propose to cache Cumulative Residual Feature (CRF)
instead of the features in all the layers, which reduces the memory footprint
of feature caching by 99%.
  Extensive experiments on FLUX.1-dev, FLUX.1-Kontext-dev, Qwen-Image, and
Qwen-Image-Edit demonstrate its effectiveness in both generation and editing.
Codes are available in the supplementary materials and will be released on
GitHub.","cs.LG, cs.AI, cs.CV",2025-10-09T17:22:23+00:00,2025-10-09T17:22:23+00:00,http://arxiv.org/abs/2510.08669v1,,,"15 pages, 11 figures"
63,2510.03224v1,Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles,"Dong Lao, Yuxiang Zhang, Haniyeh Ehsani Oskouie, Yangchao Wu, Alex Wong, Stefano Soatto","We propose a test-time defense mechanism against adversarial attacks:
imperceptible image perturbations that significantly alter the predictions of a
model. Unlike existing methods that rely on feature filtering or smoothing,
which can lead to information loss, we propose to ""combat noise with noise"" by
leveraging stochastic resonance to enhance robustness while minimizing
information loss. Our approach introduces small translational perturbations to
the input image, aligns the transformed feature embeddings, and aggregates them
before mapping back to the original reference image. This can be expressed in a
closed-form formula, which can be deployed on diverse existing network
architectures without introducing additional network modules or fine-tuning for
specific attack types. The resulting method is entirely training-free,
architecture-agnostic, and attack-agnostic. Empirical results show
state-of-the-art robustness on image classification and, for the first time,
establish a generic test-time defense for dense prediction tasks, including
stereo matching and optical flow, highlighting the method's versatility and
practicality. Specifically, relative to clean (unperturbed) performance, our
method recovers up to 68.1% of the accuracy loss on image classification, 71.9%
on stereo matching, and 29.2% on optical flow under various types of
adversarial attacks.","cs.CV, cs.AI, cs.LG",2025-10-03T17:57:25+00:00,2025-10-03T17:57:25+00:00,http://arxiv.org/abs/2510.03224v1,,,
64,2510.06871v2,SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models,"Huahui Yi, Kun Wang, Qiankun Li, Miao Yu, Liang Lin, Gongli Xi, Hao Wu, Xuming Hu, Kang Li, Yang Liu","Multimodal Large Reasoning Models (MLRMs) demonstrate impressive cross-modal
reasoning but often amplify safety risks under adversarial or unsafe prompts, a
phenomenon we call the \textit{Reasoning Tax}. Existing defenses mainly act at
the output level and do not constrain the reasoning process, leaving models
exposed to implicit risks. In this paper, we propose SaFeR-VLM, a
safety-aligned reinforcement learning framework that embeds safety directly
into multimodal reasoning. The framework integrates four components: (I)
QI-Safe-10K, a curated dataset emphasizing safety-critical and
reasoning-sensitive cases; (II) safety-aware rollout, where unsafe generations
undergo reflection and correction instead of being discarded; (III) structured
reward modeling with multi-dimensional weighted criteria and explicit penalties
for hallucinations and contradictions; and (IV) GRPO optimization, which
reinforces both safe and corrected trajectories. This unified design shifts
safety from a passive safeguard to an active driver of reasoning, enabling
scalable and generalizable safety-aware reasoning. SaFeR-VLM further
demonstrates robustness against both explicit and implicit risks, supporting
dynamic and interpretable safety decisions beyond surface-level filtering.
SaFeR-VLM-3B achieves average performance $70.13$ and $78.97$ on safety and
helpfulness across six benchmarks, surpassing both same-scale and $>10\times$
larger models such as Skywork-R1V3-38B, Qwen2.5VL-72B, and GLM4.5V-106B.
Remarkably, SaFeR-VLM-7B benefits from its increased scale to surpass
GPT-5-mini and Gemini-2.5-Flash by \num{6.47} and \num{16.76} points
respectively on safety metrics, achieving this improvement without any
degradation in helpfulness performance. Our codes are available at
https://github.com/HarveyYi/SaFeR-VLM.","cs.LG, cs.CV",2025-10-08T10:39:12+00:00,2025-10-09T13:46:31+00:00,http://arxiv.org/abs/2510.06871v2,,,
65,2510.07978v1,VoiceAgentBench: Are Voice Assistants ready for agentic tasks?,"Dhruv Jain, Harshit Shukla, Gautam Rajeev, Ashish Kulkarni, Chandra Khatri, Shubham Agarwal","Large-scale Speech Language Models (SpeechLMs) have enabled voice assistants
capable of understanding natural spoken queries and performing complex tasks.
However, existing speech benchmarks primarily focus on isolated capabilities
such as transcription, or question-answering, and do not systematically
evaluate agentic scenarios encompassing multilingual and cultural
understanding, as well as adversarial robustness. To address this, we introduce
VoiceAgentBench, a comprehensive benchmark designed to evaluate SpeechLMs in
realistic spoken agentic settings. It comprises over 5,500 synthetic spoken
queries, including dialogues grounded in Indian context, covering single-tool
invocations, multi-tool workflows, multi-turn interactions, and safety
evaluations. The benchmark supports English, Hindi, and 5 other Indian
languages, reflecting real-world linguistic and cultural diversity. We simulate
speaker variability using a novel sampling algorithm that selects audios for
TTS voice conversion based on its speaker embeddings, maximizing acoustic and
speaker diversity. Our evaluation measures tool selection accuracy, structural
consistency, and the correctness of tool invocations, including adversarial
robustness. Our experiments reveal significant gaps in contextual tool
orchestration tasks, Indic generalization, and adversarial robustness, exposing
critical limitations of current SpeechLMs.","cs.AI, cs.CL, cs.LG",2025-10-09T09:11:38+00:00,2025-10-09T09:11:38+00:00,http://arxiv.org/abs/2510.07978v1,,,
66,2509.25134v1,LayerD: Decomposing Raster Graphic Designs into Layers,"Tomoyuki Suzuki, Kang-Jun Liu, Naoto Inoue, Kota Yamaguchi","Designers craft and edit graphic designs in a layer representation, but
layer-based editing becomes impossible once composited into a raster image. In
this work, we propose LayerD, a method to decompose raster graphic designs into
layers for re-editable creative workflow. LayerD addresses the decomposition
task by iteratively extracting unoccluded foreground layers. We propose a
simple yet effective refinement approach taking advantage of the assumption
that layers often exhibit uniform appearance in graphic designs. As
decomposition is ill-posed and the ground-truth layer structure may not be
reliable, we develop a quality metric that addresses the difficulty. In
experiments, we show that LayerD successfully achieves high-quality
decomposition and outperforms baselines. We also demonstrate the use of LayerD
with state-of-the-art image generators and layer-based editing.","cs.GR, cs.CV",2025-09-29T17:50:12+00:00,2025-09-29T17:50:12+00:00,http://arxiv.org/abs/2509.25134v1,,,"ICCV 2025, Project page: https://cyberagentailab.github.io/LayerD/ ,
  GitHub: https://github.com/CyberAgentAILab/LayerD"
67,2509.24443v1,"A Systematic Review of Digital Twin-Driven Predictive Maintenance in Industrial Engineering: Taxonomy, Architectural Elements, and Future Research Directions","Leila Ismail, Abdelmoneim Abdelmoti, Arkaprabha Basu, Aymen Dia Eddine Berini, Mohammad Naouss","With the increasing complexity of industrial systems, there is a pressing
need for predictive maintenance to avoid costly downtime and disastrous
outcomes that could be life-threatening in certain domains. With the growing
popularity of the Internet of Things, Artificial Intelligence, machine
learning, and real-time big data analytics, there is a unique opportunity for
efficient predictive maintenance to forecast equipment failures for real-time
intervention and optimize maintenance actions, as traditional reactive and
preventive maintenance practices are often inadequate to meet the requirements
for the industry to provide quality-of-services of operations. Central to this
evolution is digital twin technology, an adaptive virtual replica that
continuously monitors and integrates sensor data to simulate and improve asset
performance. Despite remarkable progress in digital twin implementations, such
as considering DT in predictive maintenance for industrial engineering. This
paper aims to address this void. We perform a retrospective analysis of the
temporal evolution of the digital twin in predictive maintenance for industrial
engineering to capture the applications, middleware, and technological
requirements that led to the development of the digital twin from its inception
to the AI-enabled digital twin and its self-learning models. We provide a
layered architecture of the digital twin technology, as well as a taxonomy of
the technology-enabled industrial engineering applications systems, middleware,
and the used Artificial Intelligence algorithms. We provide insights into these
systems for the realization of a trustworthy and efficient smart digital-twin
industrial engineering ecosystem. We discuss future research directions in
digital twin for predictive maintenance in industrial engineering.","cs.AI, cs.ET, cs.SE, 68T01, 68T09, 68M14, 68W10, 68W15, C.2.4; C.4; C.5; D.2.2; D.2.11; I.2.5; I.2.6; I.2.11; J.0; J.7",2025-09-29T08:26:23+00:00,2025-09-29T08:26:23+00:00,http://arxiv.org/abs/2509.24443v1,,,
68,2510.09175v1,Beyond Pairwise Connections: Extracting High-Order Functional Brain Network Structures under Global Constraints,"Ling Zhan, Junjie Huang, Xiaoyao Yu, Wenyu Chen, Tao Jia","Functional brain network (FBN) modeling often relies on local pairwise
interactions, whose limitation in capturing high-order dependencies is
theoretically analyzed in this paper. Meanwhile, the computational burden and
heuristic nature of current hypergraph modeling approaches hinder end-to-end
learning of FBN structures directly from data distributions. To address this,
we propose to extract high-order FBN structures under global constraints, and
implement this as a Global Constraints oriented Multi-resolution (GCM) FBN
structure learning framework. It incorporates 4 types of global constraint
(signal synchronization, subject identity, expected edge numbers, and data
labels) to enable learning FBN structures for 4 distinct levels
(sample/subject/group/project) of modeling resolution. Experimental results
demonstrate that GCM achieves up to a 30.6% improvement in relative accuracy
and a 96.3% reduction in computational time across 5 datasets and 2 task
settings, compared to 9 baselines and 10 state-of-the-art methods. Extensive
experiments validate the contributions of individual components and highlight
the interpretability of GCM. This work offers a novel perspective on FBN
structure learning and provides a foundation for interdisciplinary applications
in cognitive neuroscience. Code is publicly available on
https://github.com/lzhan94swu/GCM.",cs.LG,2025-10-10T09:17:21+00:00,2025-10-10T09:17:21+00:00,http://arxiv.org/abs/2510.09175v1,,,"33 pages, 10 figures, NeurIPS"
69,2509.22711v1,Beyond Western Politics: Cross-Cultural Benchmarks for Evaluating Partisan Associations in LLMs,"Divyanshu Kumar, Ishita Gupta, Nitin Aravind Birur, Tanay Baswa, Sahil Agarwal, Prashanth Harshangi","Partisan bias in LLMs has been evaluated to assess political leanings,
typically through a broad lens and largely in Western contexts. We move beyond
identifying general leanings to examine harmful, adversarial representational
associations around political leaders and parties. To do so, we create datasets
\textit{NeutQA-440} (non-adversarial prompts) and \textit{AdverQA-440}
(adversarial prompts), which probe models for comparative plausibility
judgments across the USA and India. Results show high susceptibility to biased
partisan associations and pronounced asymmetries (e.g., substantially more
favorable associations for U.S. Democrats than Republicans) alongside
mixed-polarity concentration around India's BJP, highlighting systemic risks
and motivating standardized, cross-cultural evaluation.","cs.CY, cs.HC",2025-09-24T03:48:49+00:00,2025-09-24T03:48:49+00:00,http://arxiv.org/abs/2509.22711v1,,,
70,2510.06195v1,Latent Speech-Text Transformer,"Yen-Ju Lu, Yashesh Gaur, Wei Zhou, Benjamin Muller, Jesus Villalba, Najim Dehak, Luke Zettlemoyer, Gargi Ghosh, Mike Lewis, Srinivasan Iyer, Duc Le","Auto-regressive speech-text models are typically pre-trained on a large
number of interleaved sequences of text tokens and raw speech encoded as speech
tokens using vector quantization. These models have demonstrated
state-of-the-art performance in speech-to-speech understanding and generation
benchmarks, together with promising scaling laws, primarily enabled by the
representational alignment between text and speech. Nevertheless, they suffer
from shortcomings, partly owing to the disproportionately longer sequences of
speech tokens in contrast to textual tokens. This results in a large compute
imbalance between modalities during pre-training as well as during inference,
and a potential hindrance to effectively aligning speech and text, ultimately
translating to several orders of magnitude slower scaling laws. We introduce
the Latent Speech-Text Transformer (LST), which makes pre-training speech-text
models more data-efficient by dynamically and inexpensively aggregating speech
tokens into latent speech patches. These patches serve as higher-level units
that can either align with corresponding textual units to aid capability
transfer or even encapsulate common speech sequences like silences to be more
compute-efficient. We show that LST outperforms vanilla approaches on
speech-to-speech as well as text-to-text benchmarks in both data- and
compute-controlled settings, the former indicating more effective
representational alignment and the latter indicating steeper scaling laws for
speech-text models. On HellaSwag story completion, LST achieves 6.5% absolute
gain in speech accuracy under compute-controlled training and 5.3% under
data-controlled training, while also improving text performance. We will
release our models, code, and the evaluation data to facilitate further
research.","cs.CL, cs.AI, cs.LG, eess.AS",2025-10-07T17:52:08+00:00,2025-10-07T17:52:08+00:00,http://arxiv.org/abs/2510.06195v1,,,"16 pages, 13 figures"
71,2509.23673v1,RCI: A Score for Evaluating Global and Local Reasoning in Multimodal Benchmarks,"Amit Agarwal, Hitesh Laxmichand Patel, Srikant Panda, Hansa Meghwani, Jyotika Singh, Karan Dua, Paul Li, Tao Sheng, Sujith Ravi, Dan Roth","Multimodal Large Language Models (MLLMs) have achieved impressive results on
vision-language benchmarks, yet it remains unclear whether these benchmarks
assess genuine global reasoning or allow success via localized visual cues.
Existing evaluation methods do not explicitly measure this distinction,
hindering effective dataset curation and real-world focused model development.
  We introduce Region Comprehension Index (RCI), the first model-based score to
directly quantify a dataset's reliance on global versus local visual
information. RCI systematically compares reference-model performance on image
patches versus full images, revealing if tasks require holistic image
understanding or can be solved with partial or localized visual cues.
  When applying RCI to 13 widely used multimodal benchmarks, we observed that
most of them favor localized reasoning and exhibit significant spatial biases,
indicating potential risks in real-world applications. RCI equips researchers &
practitioners with an actionable tool for diagnosing & mitigating these biases,
enabling the construction of datasets and benchmarks to foster the development
of robust, enterprise-ready multimodal systems.","cs.CV, cs.AI, cs.CL, cs.MM, 68T45, 68T50, I.2.7; I.2.10; I.4.7; I.4.8",2025-09-28T06:26:11+00:00,2025-09-28T06:26:11+00:00,http://arxiv.org/abs/2509.23673v1,,,Accepted in EMNLP 2025
72,2510.08648v1,Inverse-Free Wilson Loops for Transformers: A Practical Diagnostic for Invariance and Order Sensitivity,"Edward Y. Chang, Ethan Y. Chang","Large language models can change answers under harmless edits that matter in
practice: RAG outputs flip when passages are reordered, fine-tuning erodes
invariances learned at pretraining, debate or chain-of-thought prompts take
path-dependent routes, and compiler fusion or reordering perturbs logits near
decision boundaries. These failures violate intended invariances, break
continuous integration, and force teams to trade safety for speed. The effects
are small yet distributed across layers and positions, sensitive to context
length and evaluation order, and costly to repair with retraining or formal
verification. We present WILSON, a minimal post-hoc diagnostic suite that
converts simple loop and reordering checks on internal representations into
system signals. WILSON combines an inverse-free curvature map over positions
and layers, computed with JVPs and Hutchinson probes, with activation-level
commutators that flag reorder risk. Signals are cheap to compute,
model-agnostic for standard Transformers, and exported as thresholds and CSV
artifacts for orchestrators. This enables concrete actions: guard RAG against
order effects, catch fine-tuning regressions, stabilize debate pathways and
long multi-turn contexts, and gate fusions or reorders in deployment. In short,
WILSON helps anticipate failures and approve safe optimizations so reliability
and throughput can improve together without changing model architecture or
training.","cs.LG, cs.AI, F.4.1, I.2.4",2025-10-09T06:41:18+00:00,2025-10-09T06:41:18+00:00,http://arxiv.org/abs/2510.08648v1,,,"24 pages, 10 figures, 2 tables"
73,2510.09484v1,CRPS-LAM: Regional ensemble weather forecasting from matching marginals,"Erik Larsson, Joel Oskarsson, Tomas Landelius, Fredrik Lindsten","Machine learning for weather prediction increasingly relies on ensemble
methods to provide probabilistic forecasts. Diffusion-based models have shown
strong performance in Limited-Area Modeling (LAM) but remain computationally
expensive at sampling time. Building on the success of global weather
forecasting models trained based on Continuous Ranked Probability Score (CRPS),
we introduce CRPS-LAM, a probabilistic LAM forecasting model trained with a
CRPS-based objective. By sampling and injecting a single latent noise vector
into the model, CRPS-LAM generates ensemble members in a single forward pass,
achieving sampling speeds up to 39 times faster than a diffusion-based model.
We evaluate the model on the MEPS regional dataset, where CRPS-LAM matches the
low errors of diffusion models. By retaining also fine-scale forecast details,
the method stands out as an effective approach for probabilistic regional
weather forecasting",cs.LG,2025-10-10T15:48:31+00:00,2025-10-10T15:48:31+00:00,http://arxiv.org/abs/2510.09484v1,,,Preprint
74,2509.20762v2,Identifying Group Anchors in Real-World Group Interactions Under Label Scarcity,"Fanchen Bu, Geon Lee, Minyoung Choe, Kijung Shin","Group interactions occur in various real-world contexts, e.g., co-authorship,
email communication, and online Q&A. In each group, there is often a
particularly significant member, around whom the group is formed. Examples
include the first or last author of a paper, the sender of an email, and the
questioner in a Q&A session. In this work, we discuss the existence of such
individuals in real-world group interactions. We call such individuals group
anchors and study the problem of identifying them. First, we introduce the
concept of group anchors and the identification problem. Then, we discuss our
observations on group anchors in real-world group interactions. Based on our
observations, we develop AnchorRadar, a fast and effective method for group
anchor identification under realistic settings with label scarcity, i.e., when
only a few groups have known anchors. AnchorRadar is a semi-supervised method
using information from groups both with and without known group anchors.
Finally, through extensive experiments on thirteen real-world datasets, we
demonstrate the empirical superiority of AnchorRadar over various baselines
w.r.t. accuracy and efficiency. In most cases, AnchorRadar achieves higher
accuracy in group anchor identification than all the baselines, while using
10.2$\times$ less training time than the fastest baseline and 43.6$\times$
fewer learnable parameters than the most lightweight baseline on average.","cs.SI, cs.LG",2025-09-25T05:38:58+00:00,2025-09-29T01:41:40+00:00,http://arxiv.org/abs/2509.20762v2,,,IEEE International Conference on Data Mining (ICDM) 2025
75,2510.10695v1,Stock Prediction via a Dual Relation Fusion Network incorporating Static and Dynamic Relations,"Long Chen, Huixin Bai, Mingxin Wang, Xiaohua Huang, Ying Liu, Jie Zhao, Ziyu Guan","Accurate modeling of inter-stock relationships is critical for stock price
forecasting. However, existing methods predominantly focus on single-state
relationships, neglecting the essential complementarity between dynamic and
static inter-stock relations. To solve this problem, we propose a Dual Relation
Fusion Network (DRFN) to capture the long-term relative stability of stock
relation structures while retaining the flexibility to respond to sudden market
shifts. Our approach features a novel relative static relation component that
models time-varying long-term patterns and incorporates overnight informational
influences. We capture dynamic inter-stock relationships through distance-aware
mechanisms, while evolving long-term structures via recurrent fusion of dynamic
relations from the prior day with the pre-defined static relations. Experiments
demonstrate that our method significantly outperforms the baselines across
different markets, with high sensitivity to the co-movement of relational
strength and stock price.",cs.LG,2025-10-12T16:48:25+00:00,2025-10-12T16:48:25+00:00,http://arxiv.org/abs/2510.10695v1,,,11 pages
76,2509.23241v1,Memory Efficient and Staleness Free Pipeline Parallel DNN Training Framework with Improved Convergence Speed,"Ankita Dutta, Nabendu Chaki, Rajat K. De","High resource requirement for Deep Neural Network (DNN) training across
multiple GPUs necessitates development of various parallelism techniques. In
this paper, we introduce two interconnected DNN training frameworks, namely,
V-TiMePReSt and I-TiMePReSt, based on pipeline parallelism, a variant of model
parallelism. V-TiMePReSt is a completely staleness-free system which enables
the DNNs to be trained on the latest updated weights in each stage of all
forward and backward passes. Developing staleness-aware systems at the expense
of weight stashing reduces GPU-memory consumption, however, increases the
number of epochs to converge. Thus, we introduce I-TiMePReSt, which is also a
staleness-aware system, but not at the expense of weight stashing. It does not
rely solely on the stale weights or the latest updated weights. I-TiMePReSt
computes an intermediate weight towards the latter and performs backward pass
on it. Additionally, we formulate the significance of the stale weights
mathematically depending on the degree of staleness. In contrast to
V-TiMePReSt, I-TiMePReSt works based on the assumption that stale weights have
a significant contribution in training, which can be quantified mathematically
based on the degree of staleness, although there are other contributory factors
which should not be ignored. Experimental results show that V-TiMePReSt is
advantageous over existing models in terms of $1)$ the extent of staleness of
the weight parameter values and $2)$ GPU memory efficiency, while I-TiMePReSt
is superior in terms of $1)$ removing staleness of the weight parameters
without removing weight stashing and $2)$ maintaining the trade-off between GPU
memory consumption and convergence speed (number of epochs).",cs.DC,2025-09-27T10:44:38+00:00,2025-09-27T10:44:38+00:00,http://arxiv.org/abs/2509.23241v1,,,
77,2510.03122v2,HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided Versatile Diffusion,"Shiyi Zhang, Dong Liang, Hairong Zheng, Yihang Zhou","The reconstruction of visual information from brain activity fosters
interdisciplinary integration between neuroscience and computer vision.
However, existing methods still face challenges in accurately recovering highly
complex visual stimuli. This difficulty stems from the characteristics of
natural scenes: low-level features exhibit heterogeneity, while high-level
features show semantic entanglement due to contextual overlaps. Inspired by the
hierarchical representation theory of the visual cortex, we propose the HAVIR
model, which separates the visual cortex into two hierarchical regions and
extracts distinct features from each. Specifically, the Structural Generator
extracts structural information from spatial processing voxels and converts it
into latent diffusion priors, while the Semantic Extractor converts semantic
processing voxels into CLIP embeddings. These components are integrated via the
Versatile Diffusion model to synthesize the final image. Experimental results
demonstrate that HAVIR enhances both the structural and semantic quality of
reconstructions, even in complex scenes, and outperforms existing models.","cs.CV, cs.AI",2025-10-03T15:50:52+00:00,2025-10-12T15:32:28+00:00,http://arxiv.org/abs/2510.03122v2,,,
78,2509.17398v1,Optimizing Split Federated Learning with Unstable Client Participation,"Wei Wei, Zheng Lin, Xihui Liu, Hongyang Du, Dusit Niyato, Xianhao Chen","To enable training of large artificial intelligence (AI) models at the
network edge, split federated learning (SFL) has emerged as a promising
approach by distributing computation between edge devices and a server.
However, while unstable network environments pose significant challenges to
SFL, prior schemes often overlook such an effect by assuming perfect client
participation, rendering them impractical for real-world scenarios. In this
work, we develop an optimization framework for SFL with unstable client
participation. We theoretically derive the first convergence upper bound for
SFL with unstable client participation by considering activation uploading
failures, gradient downloading failures, and model aggregation failures. Based
on the theoretical results, we formulate a joint optimization problem for
client sampling and model splitting to minimize the upper bound. We then
develop an efficient solution approach to solve the problem optimally.
Extensive simulations on EMNIST and CIFAR-10 demonstrate the superiority of our
proposed framework compared to existing benchmarks.",cs.NI,2025-09-22T06:57:46+00:00,2025-09-22T06:57:46+00:00,http://arxiv.org/abs/2509.17398v1,,,
79,2510.05805v1,Improving Clinical Dataset Condensation with Mode Connectivity-based Trajectory Surrogates,"Pafue Christy Nganjimi, Andrew Soltan, Danielle Belgrave, Lei Clifton, David A. Clifton, Anshul Thakur","Dataset condensation (DC) enables the creation of compact, privacy-preserving
synthetic datasets that can match the utility of real patient records,
supporting democratised access to highly regulated clinical data for developing
downstream clinical models. State-of-the-art DC methods supervise synthetic
data by aligning the training dynamics of models trained on real and those
trained on synthetic data, typically using full stochastic gradient descent
(SGD) trajectories as alignment targets; however, these trajectories are often
noisy, high-curvature, and storage-intensive, leading to unstable gradients,
slow convergence, and substantial memory overhead. We address these limitations
by replacing full SGD trajectories with smooth, low-loss parametric surrogates,
specifically quadratic B\'ezier curves that connect the initial and final model
states from real training trajectories. These mode-connected paths provide
noise-free, low-curvature supervision signals that stabilise gradients,
accelerate convergence, and eliminate the need for dense trajectory storage. We
theoretically justify B\'ezier-mode connections as effective surrogates for SGD
paths and empirically show that the proposed method outperforms
state-of-the-art condensation approaches across five clinical datasets,
yielding condensed datasets that enable clinically effective model development.","cs.LG, cs.CV, cs.DB",2025-10-07T11:22:27+00:00,2025-10-07T11:22:27+00:00,http://arxiv.org/abs/2510.05805v1,,,"20 pages, 4 figures, Submitted to AISTATS 2026"
80,2510.06190v1,"On Powerful Ways to Generate: Autoregression, Diffusion, and Beyond","Chenxiao Yang, Cai Zhou, David Wipf, Zhiyuan Li","This paper formally studies generation processes, including auto-regressive
next-token prediction and masked diffusion, that abstract beyond architectural
specifics. At this level of abstraction, we quantify their benefits and
limitations through measurable criteria such as computational hardness and
learnability. In particular, we demonstrate that allowing generation to proceed
beyond autoregression and current masked diffusion, with capabilities to
rewrite and length-variable edit, can bring significant theoretical and
empirical advantages, with important implications for frontier LLMs that aspire
to tackle increasingly hard problems and work universally across domains beyond
natural language, such as coding and science.",cs.LG,2025-10-07T17:49:30+00:00,2025-10-07T17:49:30+00:00,http://arxiv.org/abs/2510.06190v1,,,
81,2510.05307v1,When Should Users Check? A Decision-Theoretic Model of Confirmation Frequency in Multi-Step AI Agent Tasks,"Jieyu Zhou, Aryan Roy, Sneh Gupta, Daniel Weitekamp, Christopher J. MacLellan","Existing AI agents typically execute multi-step tasks autonomously and only
allow user confirmation at the end. During execution, users have little
control, making the confirm-at-end approach brittle: a single error can cascade
and force a complete restart. Confirming every step avoids such failures, but
imposes tedious overhead. Balancing excessive interruptions against costly
rollbacks remains an open challenge. We address this problem by modeling
confirmation as a minimum time scheduling problem. We conducted a formative
study with eight participants, which revealed a recurring
Confirmation-Diagnosis-Correction-Redo (CDCR) pattern in how users monitor
errors. Based on this pattern, we developed a decision-theoretic model to
determine time-efficient confirmation point placement. We then evaluated our
approach using a within-subjects study where 48 participants monitored AI
agents and repaired their mistakes while executing tasks. Results show that 81
percent of participants preferred our intermediate confirmation approach over
the confirm-at-end approach used by existing systems, and task completion time
was reduced by 13.54 percent.",cs.HC,2025-10-06T19:18:56+00:00,2025-10-06T19:18:56+00:00,http://arxiv.org/abs/2510.05307v1,,,
82,2510.05903v1,Kaputt: A Large-Scale Dataset for Visual Defect Detection,"Sebastian Höfer, Dorian Henning, Artemij Amiranashvili, Douglas Morrison, Mariliza Tzes, Ingmar Posner, Marc Matvienko, Alessandro Rennola, Anton Milan","We present a novel large-scale dataset for defect detection in a logistics
setting. Recent work on industrial anomaly detection has primarily focused on
manufacturing scenarios with highly controlled poses and a limited number of
object categories. Existing benchmarks like MVTec-AD [6] and VisA [33] have
reached saturation, with state-of-the-art methods achieving up to 99.9% AUROC
scores. In contrast to manufacturing, anomaly detection in retail logistics
faces new challenges, particularly in the diversity and variability of object
pose and appearance. Leading anomaly detection methods fall short when applied
to this new setting. To bridge this gap, we introduce a new benchmark that
overcomes the current limitations of existing datasets. With over 230,000
images (and more than 29,000 defective instances), it is 40 times larger than
MVTec-AD and contains more than 48,000 distinct objects. To validate the
difficulty of the problem, we conduct an extensive evaluation of multiple
state-of-the-art anomaly detection methods, demonstrating that they do not
surpass 56.96% AUROC on our dataset. Further qualitative analysis confirms that
existing methods struggle to leverage normal samples under heavy pose and
appearance variation. With our large-scale dataset, we set a new benchmark and
encourage future research towards solving this challenging problem in retail
logistics anomaly detection. The dataset is available for download under
https://www.kaputt-dataset.com.","cs.CV, cs.AI, cs.LG",2025-10-07T13:13:18+00:00,2025-10-07T13:13:18+00:00,http://arxiv.org/abs/2510.05903v1,,,Accepted to ICCV 2025
83,2510.06512v1,LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval,"Avishree Khare, Hideki Okamoto, Bardh Hoxha, Georgios Fainekos, Rajeev Alur","Neural models such as YOLO and HuBERT can be used to detect local properties
such as objects (""car"") and emotions (""angry"") in individual frames of videos
and audio clips respectively. The likelihood of these detections is indicated
by scores in [0, 1]. Lifting these scores to temporal properties over sequences
can be useful for several downstream applications such as query matching (e.g.,
""does the speaker eventually sound happy in this audio clip?""), and ranked
retrieval (e.g., ""retrieve top 5 videos with a 10 second scene where a car is
detected until a pedestrian is detected""). In this work, we formalize this
problem of assigning Scores for TempOral Properties (STOPs) over sequences,
given potentially noisy score predictors for local properties. We then propose
a scoring function called LogSTOP that can efficiently compute these scores for
temporal properties represented in Linear Temporal Logic. Empirically, LogSTOP,
with YOLO and HuBERT, outperforms Large Vision / Audio Language Models and
other Temporal Logic-based baselines by at least 16% on query matching with
temporal properties over objects-in-videos and emotions-in-speech respectively.
Similarly, on ranked retrieval with temporal properties over objects and
actions in videos, LogSTOP with Grounding DINO and SlowR50 reports at least a
19% and 16% increase in mean average precision and recall over zero-shot
text-to-video retrieval baselines respectively.","cs.CV, cs.AI",2025-10-07T23:05:20+00:00,2025-10-07T23:05:20+00:00,http://arxiv.org/abs/2510.06512v1,,,
84,2509.16032v1,A Matter of Height: The Impact of a Robotic Object on Human Compliance,"Michael Faber, Andrey Grishko, Julian Waksberg, David Pardo, Tomer Leivy, Yuval Hazan, Emanuel Talmansky, Benny Megidish, Hadas Erel","Robots come in various forms and have different characteristics that may
shape the interaction with them. In human-human interactions, height is a
characteristic that shapes human dynamics, with taller people typically
perceived as more persuasive. In this work, we aspired to evaluate if the same
impact replicates in a human-robot interaction and specifically with a highly
non-humanoid robotic object. The robot was designed with modules that could be
easily added or removed, allowing us to change its height without altering
other design features. To test the impact of the robot's height, we evaluated
participants' compliance with its request to volunteer to perform a tedious
task. In the experiment, participants performed a cognitive task on a computer,
which was framed as the main experiment. When done, they were informed that the
experiment was completed. While waiting to receive their credits, the robotic
object, designed as a mobile robotic service table, entered the room, carrying
a tablet that invited participants to complete a 300-question questionnaire
voluntarily. We compared participants' compliance in two conditions: A Short
robot composed of two modules and 95cm in height and a Tall robot consisting of
three modules and 132cm in height. Our findings revealed higher compliance with
the Short robot's request, demonstrating an opposite pattern to human dynamics.
We conclude that while height has a substantial social impact on human-robot
interactions, it follows a unique pattern of influence. Our findings suggest
that designers cannot simply adopt and implement elements from human social
dynamics to robots without testing them first.","cs.RO, cs.HC",2025-09-19T14:36:54+00:00,2025-09-19T14:36:54+00:00,http://arxiv.org/abs/2509.16032v1,,,"8 pages, 6 figures, 1 table, submitted to IEEE RO-MAN 2025"
85,2510.11133v1,Test-Time Adaptation by Causal Trimming,"Yingnan Liu, Rui Qiao, Mong Li Lee, Wynne Hsu","Test-time adaptation aims to improve model robustness under distribution
shifts by adapting models with access to unlabeled target samples. A primary
cause of performance degradation under such shifts is the model's reliance on
features that lack a direct causal relationship with the prediction target. We
introduce Test-time Adaptation by Causal Trimming (TACT), a method that
identifies and removes non-causal components from representations for test
distributions. TACT applies data augmentations that preserve causal features
while varying non-causal ones. By analyzing the changes in the representations
using Principal Component Analysis, TACT identifies the highest variance
directions associated with non-causal features. It trims the representations by
removing their projections on the identified directions, and uses the trimmed
representations for the predictions. During adaptation, TACT continuously
tracks and refines these directions to get a better estimate of non-causal
features. We theoretically analyze the effectiveness of this approach and
empirically validate TACT on real-world out-of-distribution benchmarks. TACT
consistently outperforms state-of-the-art methods by a significant margin.",cs.LG,2025-10-13T08:22:38+00:00,2025-10-13T08:22:38+00:00,http://arxiv.org/abs/2510.11133v1,,,"Accepted to the Thirty-Ninth Annual Conference on Neural Information
  Processing Systems (NeurIPS 2025); Code is available at
  https://github.com/NancyQuris/TACT"
86,2510.02566v1,PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction,"Qiao Feng, Yiming Huang, Yufu Wang, Jiatao Gu, Lingjie Liu","Reconstructing physically plausible human motion from monocular videos
remains a challenging problem in computer vision and graphics. Existing methods
primarily focus on kinematics-based pose estimation, often leading to
unrealistic results due to the lack of physical constraints. To address such
artifacts, prior methods have typically relied on physics-based post-processing
following the initial kinematics-based motion estimation. However, this
two-stage design introduces error accumulation, ultimately limiting the overall
reconstruction quality. In this paper, we present PhysHMR, a unified framework
that directly learns a visual-to-action policy for humanoid control in a
physics-based simulator, enabling motion reconstruction that is both physically
grounded and visually aligned with the input video. A key component of our
approach is the pixel-as-ray strategy, which lifts 2D keypoints into 3D spatial
rays and transforms them into global space. These rays are incorporated as
policy inputs, providing robust global pose guidance without depending on noisy
3D root predictions. This soft global grounding, combined with local visual
features from a pretrained encoder, allows the policy to reason over both
detailed pose and global positioning. To overcome the sample inefficiency of
reinforcement learning, we further introduce a distillation scheme that
transfers motion knowledge from a mocap-trained expert to the
vision-conditioned policy, which is then refined using physically motivated
reinforcement learning rewards. Extensive experiments demonstrate that PhysHMR
produces high-fidelity, physically plausible motion across diverse scenarios,
outperforming prior approaches in both visual accuracy and physical realism.",cs.CV,2025-10-02T21:01:11+00:00,2025-10-02T21:01:11+00:00,http://arxiv.org/abs/2510.02566v1,,,
87,2510.05707v1,Stable Robot Motions on Manifolds: Learning Lyapunov-Constrained Neural Manifold ODEs,"David Boetius, Abdelrahman Abdelnaby, Ashok Kumar, Stefan Leue, Abdalla Swikir, Fares J. Abu-Dakka","Learning stable dynamical systems from data is crucial for safe and reliable
robot motion planning and control. However, extending stability guarantees to
trajectories defined on Riemannian manifolds poses significant challenges due
to the manifold's geometric constraints. To address this, we propose a general
framework for learning stable dynamical systems on Riemannian manifolds using
neural ordinary differential equations. Our method guarantees stability by
projecting the neural vector field evolving on the manifold so that it strictly
satisfies the Lyapunov stability criterion, ensuring stability at every system
state. By leveraging a flexible neural parameterisation for both the base
vector field and the Lyapunov function, our framework can accurately represent
complex trajectories while respecting manifold constraints by evolving
solutions directly on the manifold. We provide an efficient training strategy
for applying our framework and demonstrate its utility by solving Riemannian
LASA datasets on the unit quaternion (S^3) and symmetric positive-definite
matrix manifolds, as well as robotic motions evolving on \mathbb{R}^3 \times
S^3. We demonstrate the performance, scalability, and practical applicability
of our approach through extensive simulations and by learning robot motions in
a real-world experiment.","cs.RO, cs.LG, math.OC",2025-10-07T09:16:48+00:00,2025-10-07T09:16:48+00:00,http://arxiv.org/abs/2510.05707v1,,,"12 pages, 6 figures"
88,2510.05650v1,EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario,"Yiping Ma, Shiyu Hu, Buyuan Zhu, Yipei Wang, Yaxuan Kang, Shiqing Liu, Kang Hao Cheong","Reproducing cognitive development, group interaction, and long-term evolution
in virtual classrooms remains a core challenge for educational AI, as real
classrooms integrate open-ended cognition, dynamic social interaction,
affective factors, and multi-session development rarely captured together.
Existing approaches mostly focus on short-term or single-agent settings,
limiting systematic study of classroom complexity and cross-task reuse. We
present EduVerse, the first user-defined multi-agent simulation space that
supports environment, agent, and session customization. A distinctive
human-in-the-loop interface further allows real users to join the space. Built
on a layered CIE (Cognition-Interaction-Evolution) architecture, EduVerse
ensures individual consistency, authentic interaction, and longitudinal
adaptation in cognition, emotion, and behavior-reproducing realistic classroom
dynamics with seamless human-agent integration. We validate EduVerse in
middle-school Chinese classes across three text genres, environments, and
multiple sessions. Results show: (1) Instructional alignment: simulated IRF
rates (0.28-0.64) closely match real classrooms (0.37-0.49), indicating
pedagogical realism; (2) Group interaction and role differentiation: network
density (0.27-0.40) with about one-third of peer links realized, while
human-agent tasks indicate a balance between individual variability and
instructional stability; (3) Cross-session evolution: the positive transition
rate R+ increase by 11.7% on average, capturing longitudinal shifts in
behavior, emotion, and cognition and revealing structured learning
trajectories. Overall, EduVerse balances realism, reproducibility, and
interpretability, providing a scalable platform for educational AI. The system
will be open-sourced to foster cross-disciplinary research.","cs.CV, cs.CY",2025-10-07T07:58:32+00:00,2025-10-07T07:58:32+00:00,http://arxiv.org/abs/2510.05650v1,,,"Preprint, Under review"
89,2509.20106v1,Investigating the Effect of Prior Exposure and Fidelity on Quality and Realism Perception of VR Digital Twins,"Maximilian Warsinke, Maurizio Vergari, Tanja Kojić, Daniel Nikulin, Sebastian Möller","This study explores how prior exposure to physical objects influences the
quality and realism perception of Digital Twins (DT) with varying levels of
fidelity in Virtual Reality (VR). In a mixed experimental design, 24
participants were divided into two equal groups: an exposure group, in which
members were shown physical objects before inspecting and rating their replicas
in VR, and a control group without prior knowledge. Three objects were
presented, each under four fidelity conditions with varying texture resolution
and geometric detail. Participants rated perceived quality and realism through
in-VR self-reports. Statistical analysis revealed that texture resolution
significantly affected realism and quality perception, whereas geometric detail
only influenced quality ratings. Investigating the between-factor, no
significant effect of exposure on quality and realism perception was found.
These findings raise important questions about the cognitive relationship
between physical objects and their digital counterparts and how fidelity
influences the perception of DTs in VR.",cs.HC,2025-09-24T13:31:03+00:00,2025-09-24T13:31:03+00:00,http://arxiv.org/abs/2509.20106v1,,,"Accepted to ACM Symposium on Virtual Reality Software and Technology
  (VRST) 2025"
90,2510.00895v1,Visualizing Quantum Circuits: State Vector Difference Highlighting and the Half-Matrix,"Michael J. McGuffin, Jean-Marc Robert","Existing graphical user interfaces for circuit simulators often show small
visual summaries of the reduced state of each qubit, showing the probability,
phase, purity, and/or Bloch sphere coordinates associated with each qubit.
These necessarily provide an incomplete picture of the quantum state of the
qubits, and can sometimes be confusing for students or newcomers to quantum
computing. We contribute two novel visual approaches to provide more complete
information about small circuits. First, to complement information about each
qubit, we show the complete state vector, and illustrate the way that
amplitudes change from layer-to-layer under the effect of different gates, by
using a small set of colors, arrows, and symbols. We call this ``state vector
difference highlighting'', and show how it elucidates the effect of Hadamard,
X, Y, Z, S, T, Phase, and SWAP gates, where each gate may have an arbitrary
combination of control and anticontrol qubits. Second, we display pairwise
information about qubits (such as concurrence and correlation) in a triangular
``half-matrix'' visualization. Our open source software implementation, called
MuqcsCraft, is available as a live online demonstration that runs in a web
browser without installing any additional software, allowing a user to define a
circuit through drag-and-drop actions, and then simulate and visualize it.","quant-ph, cs.HC",2025-10-01T13:37:21+00:00,2025-10-01T13:37:21+00:00,http://arxiv.org/abs/2510.00895v1,,,
91,2509.25155v1,Context-Driven Performance Modeling for Causal Inference Operators on Neural Processing Units,"Neelesh Gupta, Rakshith Jayanth, Dhruv Parikh, Viktor Prasanna","The proliferation of large language models (LLMs) has driven demand for long
context inference on resource constrained edge devices. However, deploying
these models on Neural Processing Units (NPUs) presents significant challenges
due to the architectural mismatch: quadratic complexity of standard attention
mechanisms conflicts with memory and compute patterns of edge accelerators.
This paper presents a comprehensive performance analysis of various causal
inference operators on a modern NPU. We benchmark standard quadratic attention
against several sub-quadratic alternatives, including structured state-space
and linear attention models. Our analysis reveals that while sub-quadratic
methods offer superior scalability, they introduce distinct computational
bottlenecks on the NPU's specialized execution units. We identify that
quadratic attention becomes severely memory-bound, suffering from cache
inefficiency and pipeline stalls exceeding 95% at long contexts. In contrast,
sub-quadratic models can become compute-bound on programmable vector cores.
These findings provide critical insights for the co-design of hardware-aware
models and optimization strategies to enable on-device AI inference with
long-contexts.","cs.DC, cs.LG",2025-09-29T17:55:43+00:00,2025-09-29T17:55:43+00:00,http://arxiv.org/abs/2509.25155v1,,,IEEE HiPC 2025
92,2510.07184v1,Exploring the Feasibility of Gaze-Based Navigation Across Path Types,"Yichuan Zhang, Liangyuting Zhang, Xuning Hu, Yong Yue, Hai-Ning Liang","Gaze input, as a modality inherently conveying user intent, offers intuitive
and immersive experiences in extended reality (XR). With eye-tracking now being
a standard feature in modern XR headsets, gaze has been extensively applied to
tasks such as selection, text entry, and object manipulation. However, gaze
based navigation despite being a fundamental interaction task remains largely
underexplored. In particular, little is known about which path types are well
suited for gaze navigation and under what conditions it performs effectively.
To bridge this gap, we conducted a controlled user study evaluating gaze-based
navigation across three representative path types: linear, narrowing, and
circular. Our findings reveal distinct performance characteristics and
parameter ranges for each path type, offering design insights and practical
guidelines for future gaze-driven navigation systems in XR.",cs.HC,2025-10-08T16:21:55+00:00,2025-10-08T16:21:55+00:00,http://arxiv.org/abs/2510.07184v1,,,"4 pages, 4 figures. Accepted to ISMAR 2025 GEMINI Workshop"
93,2509.18800v1,Security Evaluation of Android apps in budget African Mobile Devices,"Alioune Diallo, Anta Diop, Abdoul Kader Kabore, Jordan Samhi, Aleksandr Pilgun, Tegawendé F. Bissyande, Jacque Klein","Android's open-source nature facilitates widespread smartphone accessibility,
particularly in price-sensitive markets. System and vendor applications that
come pre-installed on budget Android devices frequently operate with elevated
privileges, yet they receive limited independent examination. To address this
gap, we developed a framework that extracts APKs from physical devices and
applies static analysis to identify privacy and security issues in embedded
software. Our study examined 1,544 APKs collected from seven African
smartphones. The analysis revealed that 145 applications (9%) disclose
sensitive data, 249 (16%) expose critical components without sufficient
safeguards, and many present additional risks: 226 execute privileged or
dangerous commands, 79 interact with SMS messages (read, send, or delete), and
33 perform silent installation operations. We also uncovered a vendor-supplied
package that appears to transmit device identifiers and location details to an
external third party. These results demonstrate that pre-installed applications
on widely distributed low-cost devices represent a significant and
underexplored threat to user security and privacy.","cs.CR, cs.SE",2025-09-23T08:45:07+00:00,2025-09-23T08:45:07+00:00,http://arxiv.org/abs/2509.18800v1,,,"13 pages, 3 figures, submitted (wating for notification)"
94,2510.04706v1,"ID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusion","Foivos Paraperas Papantoniou, Stefanos Zafeiriou","Human-centric generative models designed for AI-driven storytelling must
bring together two core capabilities: identity consistency and precise control
over human performance. While recent diffusion-based approaches have made
significant progress in maintaining facial identity, achieving fine-grained
expression control without compromising identity remains challenging. In this
work, we present a diffusion-based framework that faithfully reimagines any
subject under any particular facial expression. Building on an ID-consistent
face foundation model, we adopt a compositional design featuring an expression
cross-attention module guided by FLAME blendshape parameters for explicit
control. Trained on a diverse mixture of image and video data rich in
expressive variation, our adapter generalizes beyond basic emotions to subtle
micro-expressions and expressive transitions, overlooked by prior works. In
addition, a pluggable Reference Adapter enables expression editing in real
images by transferring the appearance from a reference frame during synthesis.
Extensive quantitative and qualitative evaluations show that our model
outperforms existing methods in tailored and identity-consistent expression
generation. Code and models can be found at
https://github.com/foivospar/Arc2Face.",cs.CV,2025-10-06T11:20:56+00:00,2025-10-06T11:20:56+00:00,http://arxiv.org/abs/2510.04706v1,,,"ICCVW 2025, Code: https://github.com/foivospar/Arc2Face"
95,2510.10060v1,Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling,"Hehe Fan, Yi Yang, Mohan Kankanhalli, Fei Wu","When modeling a given type of data, we consider it to involve two key
aspects: 1) identifying relevant elements (e.g., image pixels or textual words)
to a central element, as in a convolutional receptive field, or to a query
element, as in self-attention, and 2) encoding these tokens effectively.
Self-attention can adaptively identify these elements but relies on absolute
positional embedding for structural representation learning. In contrast,
convolution encodes elements in a relative manner, yet their fixed kernel size
limits their ability to adaptively select the relevant elements. In this paper,
we introduce Translution, an operation that unifies the adaptive identification
capability of self-attention and the relative encoding advantage of
convolution. However, this integration leads to a substantial increase in the
number of parameters, exceeding most currently available computational
resources. Therefore, we propose a lightweight variant of Translution, named
{\alpha}-Translution. Experiments on computer vision and natural language
processing tasks show that Translution (including {\alpha}-Translution)
achieves superior accuracy compared to self-attention. The code is available at
https://github.com/hehefan/Translution.","cs.LG, cs.AI, cs.CL, cs.CV",2025-10-11T06:54:10+00:00,2025-10-11T06:54:10+00:00,http://arxiv.org/abs/2510.10060v1,,,technical report
96,2510.11108v1,A Vision for Access Control in LLM-based Agent Systems,"Xinfeng Li, Dong Huang, Jie Li, Hongyi Cai, Zhenhong Zhou, Wei Dong, XiaoFeng Wang, Yang Liu","The autonomy and contextual complexity of LLM-based agents render traditional
access control (AC) mechanisms insufficient. Static, rule-based systems
designed for predictable environments are fundamentally ill-equipped to manage
the dynamic information flows inherent in agentic interactions. This position
paper argues for a paradigm shift from binary access control to a more
sophisticated model of information governance, positing that the core challenge
is not merely about permission, but about governing the flow of information. We
introduce Agent Access Control (AAC), a novel framework that reframes AC as a
dynamic, context-aware process of information flow governance. AAC operates on
two core modules: (1) multi-dimensional contextual evaluation, which assesses
not just identity but also relationships, scenarios, and norms; and (2)
adaptive response formulation, which moves beyond simple allow/deny decisions
to shape information through redaction, summarization, and paraphrasing. This
vision, powered by a dedicated AC reasoning engine, aims to bridge the gap
between human-like nuanced judgment and scalable Al safety, proposing a new
conceptual lens for future research in trustworthy agent design.","cs.MA, cs.AI, cs.CR",2025-10-13T07:57:09+00:00,2025-10-13T07:57:09+00:00,http://arxiv.org/abs/2510.11108v1,,,"10 pages, 1 figure"
97,2510.02389v1,From Trace to Line: LLM Agent for Real-World OSS Vulnerability Localization,"Haoran Xi, Minghao Shao, Brendan Dolan-Gavitt, Muhammad Shafique, Ramesh Karri","Large language models show promise for vulnerability discovery, yet
prevailing methods inspect code in isolation, struggle with long contexts, and
focus on coarse function- or file-level detections - offering limited
actionable guidance to engineers who need precise line-level localization and
targeted patches in real-world software development. We present T2L-Agent
(Trace-to-Line Agent), a project-level, end-to-end framework that plans its own
analysis and progressively narrows scope from modules to exact vulnerable
lines. T2L-Agent couples multi-round feedback with an Agentic Trace Analyzer
(ATA) that fuses runtime evidence - crash points, stack traces, and coverage
deltas - with AST-based code chunking, enabling iterative refinement beyond
single pass predictions and translating symptoms into actionable, line-level
diagnoses. To benchmark line-level vulnerability discovery, we introduce
T2L-ARVO, a diverse, expert-verified 50-case benchmark spanning five crash
families and real-world projects. T2L-ARVO is specifically designed to support
both coarse-grained detection and fine-grained localization, enabling rigorous
evaluation of systems that aim to move beyond file-level predictions. On
T2L-ARVO, T2L-Agent achieves up to 58.0% detection and 54.8% line-level
localization, substantially outperforming baselines. Together, the framework
and benchmark push LLM-based vulnerability detection from coarse identification
toward deployable, robust, precision diagnostics that reduce noise and
accelerate patching in open-source software workflows.","cs.SE, cs.CR, cs.LG",2025-09-30T22:27:18+00:00,2025-09-30T22:27:18+00:00,http://arxiv.org/abs/2510.02389v1,,,
98,2509.12441v1,Automatic Network Planning with Digital Radio Twin,"Xiaomeng Li, Yuru Zhang, Qiang Liu, Mehmet Can Vuran, Nathan Huynh, Li Zhao, Mizan Rahman, Eren Erman Ozguven","Network planning seeks to determine base station parameters that maximize
coverage and capacity in cellular networks. However, achieving optimal planning
remains challenging due to the diversity of deployment scenarios and the
significant simulation-to-reality discrepancy. In this paper, we propose
\emph{AutoPlan}, a new automatic network planning framework by leveraging
digital radio twin (DRT) techniques. We derive the DRT by finetuning the
parameters of building materials to reduce the sim-to-real discrepancy based on
crowdsource real-world user data. Leveraging the DRT, we design a Bayesian
optimization based algorithm to optimize the deployment parameters of base
stations efficiently. Using the field measurement from Husker-Net, we
extensively evaluate \emph{AutoPlan} under various deployment scenarios, in
terms of both coverage and capacity. The evaluation results show that
\emph{AutoPlan} flexibly adapts to different scenarios and achieves performance
comparable to exhaustive search, while requiring less than 2\% of its
computation time.",cs.NI,2025-09-15T20:48:50+00:00,2025-09-15T20:48:50+00:00,http://arxiv.org/abs/2509.12441v1,,,
99,2510.07626v1,LLM Unlearning Under the Microscope: A Full-Stack View on Methods and Metrics,"Chongyu Fan, Changsheng Wang, Yancheng Huang, Soumyadeep Pal, Sijia Liu","Machine unlearning for large language models (LLMs) aims to remove undesired
data, knowledge, and behaviors (e.g., for safety, privacy, or copyright) while
preserving useful model capabilities. Despite rapid progress over the past two
years, research in LLM unlearning remains fragmented, with limited clarity on
what constitutes effective unlearning and how it should be rigorously
evaluated. In this work, we present a principled taxonomy of twelve recent
stateful unlearning methods, grouped into three methodological families:
divergence-driven optimization, representation misalignment, and
rejection-based targeted unlearning. Building on this taxonomy, we revisit the
evaluation of unlearning effectiveness (UE), utility retention (UT), and
robustness (Rob), focusing on the WMDP benchmark. Our analysis shows that
current evaluations, dominated by multiple-choice question (MCQ) accuracy,
offer only a narrow perspective, often overstating success while overlooking
the model's actual generation behavior. To address this gap, we introduce open
question-answering (Open-QA) metrics that better capture generative performance
and reveal the inherent UE-UT tradeoff across method families. Furthermore, we
demonstrate that robustness requires finer-grained analysis: for example,
vulnerabilities differ substantially between in-domain relearning and
out-of-domain fine-tuning, even though both fall under model-level attacks.
Through this study, we hope to deliver a full-stack revisit of LLM unlearning
and actionable guidance for designing and evaluating future methods.","cs.LG, cs.CL",2025-10-08T23:47:05+00:00,2025-10-08T23:47:05+00:00,http://arxiv.org/abs/2510.07626v1,,,
