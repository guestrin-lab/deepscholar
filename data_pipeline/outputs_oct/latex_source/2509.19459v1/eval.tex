\section{Evaluation \label{sec:eval}}

In this section, we present the evaluation of \tool on several benchmarks in order to answer the following research questions:
\begin{itemize}
    \item RQ1 Applicablility: Does \tool analyze PM programs in a reasonable time?
    \item RQ2 Performance: Do programs transformed by \tool have performance similar to the original programs?
    \item RQ3 Developer Burden: How much developer involvement does \tool require to relax the persistency requirements? 
\end{itemize}
We start by describing our system setup, the settings of \tool, and the benchmarks. Finally, we discuss how our research questions can be answered by our evaluation findings. 

\noindent \textbf{System Setup}: While one  motivation for this
work is CXL shared memory, it is not 
commercially available yet.  Thus, we have evaluated \tool
on Intel Optane PM, which requires the same use of flush and
fence instructions.

\Tool is implemented as a transformation pass in LLVM. All
benchmarks are compiled with clang/clang++ and  LLVM 
with optimization level O3. We used a Ubuntu 22.04.4 machine with a 16
core 2.4 GHz Intel Xeon Silver 4314 processor, 256 GB of RAM, and 256
GB of Optane PM.

\noindent \textbf{\tool Settings:} We evaluate \Tool with three
optimization settings to understand the contribution of
each optimization. 
\textit{PMRobust\textsubscript{base}} uses our alias analysis to insert a flush and a
fence immediately after every store and atomic load to PM.  \textit{PMRobust\textsubscript{opt}} adds our escape and
persistency state analysis to delay insertion of fences, as described
in Section~\ref{sec:trans}. 
\textit{PMRobust\textsubscript{flit}} adds  FliT on
top of \textit{PMRobust\textsubscript{opt}}.

\subsection{Benchmarks}

Our evaluation includes RECIPE~\cite{recipe}, a set of
high-performance concurrent index data structures modified to be crash-consistent for persistent
memory. RECIPE uses PMDK~\cite{pmdk}'s libvmmalloc to convert all
dynamic memory allocations to PM allocations. Of the data structures
in RECIPE, P-HOT, WOART, and LevelHash fail to compile with LLVM,
and we evaluate the remaining six (P-ART, P-BwTree, P-CLHT,
P-Masstree, FAST\&FAIR, CCEH) using the YCSB
benchmark~\cite{ycsb}. During evaluation, we found timing-related
bugs in CCEH and FAST\&FAIR and
discard those runs when they occur. These bugs also occur in the
unmodified programs. By comparison with the transformed version, we also discovered a number of
missing flushes and fences in the functions \code{getChild()} and
\code{getChildren()} in P-ART's node classes, which allow the client to retrieve values that are not yet persisted in the index. If the client then writes to PM
locations based on the retrieved value and a crash happens, the later update could be persisted while its source in the index might not, \ie a crash consistency bug. We added these flushes and
fences to P-ART. 

We also include Memcached~\cite{memcached}, a popular distributed
memory object caching system. Specifically, we choose a version of Memcached
that supports persistent memory via PMDK's libpmem. We use the
memaslap load testing tool from libMemcached~\cite{libmemcached} for
benchmarking.

Lastly, we evaluate our tool on PMDK~\cite{pmdk}, the most widely-used
open-source libraries for programming persistent memory, using a data
store implementation which is provided as one of its example programs. The data
store has a swappable backend that allows choosing from among seven
map data structures: \code{btree}, \code{rbtree}, \code{ctree},
\code{hashmap\_atomic}, \code{hashmap\_tx}, \code{hashmap\_rp}, and
\code{skiplist}.

For each benchmark, we show the average throughput of the original and
the transformed program over 5 runs for RECIPE and over 10 runs for 
memcached and PMDK's data store, and display the standard deviation as
error bars. In the transformed programs, we first removed the existing
flushes and fences from the original programs and then our tool
transforms them by inserting flushes and fences.

\subsection{RQ1: Applicability}
To answer RQ1, we measure the execution time of \tool pass and under the \textit{PMRobust\textsubscript{opt}} setting for each of the benchmarks. We then compare them against the execution time of the PM bug repair tool PMBugAssist~\cite{pmbugassist}, which uses SMT solving to generate targeted fixes for PM bugs contained in the execution trace, but is not able to guarantee the repaired program is free of further PM bugs. Although PMBugAssist belongs to a separate paradigm, there is currently no other PM bug repair tool that takes a purely static approach like \tool does. We use the comparison to demonstrate that \tool is able to run in reasonable time on the same programs as PMBugAssist, while providing stronger correctness guarantees. Thus, in addition to previously mentioned benchmarks, we include measurements on PMDK test programs evaluated by PMBugAssist. The times are presented in Table~\ref{table:Perf} along with the code size of the benchmarks. The time reported for PMBugAssist is the total time it takes on our machine to fix all bugs provided for each benchmark in the original evaluation.

\begin{table}[h!]
\begin{center}
{\scriptsize
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{ |c|c|c|c|c|c|c|}
 \hline
 Benchmark & PMR Time(s)& PBA Time(s) & Code Size (KLOC)\\
 \hline
   data\_store        & 20.8 & N/A     & 95.2$^*$\\
   Memcached          & 199.1& 1861.49 & 23\\
   RECIPE             & 64   & 5.94    & 40.3\\
   obj\_constructor   & 17.9 & 0.1     & 95.4$^*$\\
   obj\_first\_next   & 17.7 & 0.3     & 95.5$^*$\\
   obj\_mem           & 18.1 & 149.5   & 95.3$^*$\\
   obj\_memops        & 28.3 & 0.3     & 95.8$^*$\\
   obj\_toid          & 18.1 & 0.1     & 95.3$^*$\\
   rpmemd\_db         & 12.2 & 0.1     & 25.3$^*$\\
   pmemspoil          & 14.8 & 0.1     & 47.8$^*$\\
   pmem\_memcpy       & 11.9 & 0.3     & 46.7$^*$\\
   pmem\_memmove      & 11.9 & 0.2     & 46.7$^*$\\
   pmem\_memset       & 11.9 & 198.9   & 46.6$^*$\\
   pmreorder\_simple  & 10.3 & 4810.0  & 46.6$^*$\\
   pmreorder\_flushes & 10.4 & 5911.8  & 46.6$^*$\\
   pmreorder\_stack   & 10.4 & 1.2     & 46.6$^*$\\
 \hline
   \multicolumn{4}{l}{\footnotesize $^*$ including sublibraries of PMDK}
\end{tabular}
}
\iffalse
libpmem 46.5 KLOC
libpmemobj + libpmem 95.2 KLOC
\fi
\end{center}
\caption{Runtime of \tool vs. PMBugAssist. PMR stands for \tool, PBA stands for PMBugAssist. N/A means not included in PBA's original evaluation }\vspace{-.2cm}
\label{table:Perf}
\end{table}

Table~\ref{table:Perf} shows that \tool's execution time roughly correlates with the code size, whereas the execution time of PMBugAssist varies widely, ranging from less than a second to close to an hour, depending on the length of execution traces, the number of bugs, and the SMT solving process. These measurements support the claim that \tool is applicable to programs targeted by prior tools, but with more consistent analysis times and stronger guarantees. 

\subsection{RQ2: Performance}\label{ref:rq2}
Here we explore RQ2 by comparing the performance of \tool's transformed programs with the originals. 

\noindent \textbf{RECIPE:}
We follow RECIPE's evaluation procedure~\cite{recipe} by
testing the ordered indexes (P-ART, P-BwTree, P-Masstree, FAST\&FAIR)
on two types of keys---\textbf{randint} (8 byte random integer keys)
and \textbf{string} (24 byte YCSB string keys), all uniformly
distributed, with YCSB workloads A, B, C, and E, and the unordered
index P-CLHT and CCEH on workloads A, B, and C with uniformly
distributed randint keys. In both cases, we set the thread count to 16
and first populate the index with 64M keys using LoadA, and then run
the respective workloads that insert and/or read the keys. 
%Note that fixing the crash consistency bug we discovered caused a decrease in the original P-ART's performance on read-heavy workloads, especially workload E that performs range scans.

We discovered a bottleneck when running YCSB workload E with the transformed P-BwTree that heavily uses an iterator data structure to traverse retrieved values. An iterator and the data it contains is
never reused after a crash, yet its data is allocated in PM. This
unnecessary usage of PM occurs because the RECIPE benchmarks, as
research prototypes, do not use separate memory allocators for PM and
DRAM allocations, but simply use libvmmalloc to perform all dynamic
allocations using PM. In a proper implementation, iterator data
should be allocated in DRAM. Thus, we eliminated the flush insertion on
iterator contexts with 5 annotation pairs, where each 
annotation pair ignores exactly only one program statement. Figure~\ref{fig:recipe_perf} shows the resulting
throughputs.

\begin{figure}[!htb]
\centering
\begin{subfigure}{.45\linewidth}
\includegraphics[width=\linewidth]{figures/ycsb-randint.pdf}
\caption{Integer keys}
\end{subfigure}
~
\begin{subfigure}{.45\linewidth}
\includegraphics[width=\linewidth]{figures/ycsb-string.pdf}
\caption{String keys}
\end{subfigure}
\caption{Throughput of RECIPE indexes.  Larger is better.}
\label{fig:recipe_perf}
\end{figure}

\noindent\textbf{Memcached:}
We evaluate Memcached's throughput with 16 threads and 100K
operations using memaslap. The workload uses 16-byte keys and
1024-bytes values. The proportion of get and set operations is 90/10.
Figure~\ref{fig:memcached_perf} reports the throughputs.

\begin{figure}[!htb]
\vspace{-.3cm}
\begin{center}
\includegraphics[width=0.5\linewidth]{figures/memcached.pdf}
\end{center}
\vspace{-.3cm}
\caption{Throughput of Memcached.  Larger is better.}
\label{fig:memcached_perf}
\end{figure}

\noindent\textbf{PMDK:}
During the data store benchmark, 100K randomly generated 8-byte
integer keys are inserted into the data store and then
removed. Figure~\ref{fig:data_store_perf} presents the results of the
seven map data structures.

\begin{figure}[!htb]
\vspace{-.4cm}
\begin{center}
\includegraphics[width=.8\linewidth]{figures/data_store.pdf}
\end{center}
\vspace{-.2cm}
\caption{Throughput of PMDK's data store.  Larger is better.\label{fig:data_store_perf}}
\end{figure}

\vspace{.2cm}
First, we see from the evaluation results that the base
transformation achieves performance on-par with the original programs
on a number of benchmarks, including P-CLHT, P-Masstree, Fast\&Fair,
CCEH, and Memcached. This suggests that in these benchmarks, extra
flushes and fences inserted by the base approach do not appear
on performance-critical paths. For the other benchmarks that have a
noticeable gap between the performance of
\textit{PMRobust\textsubscript{opt}} and the original program,
\textit{PMRobust\textsubscript{opt}} is able to either reduce, or in
some cases completely eliminate the performance gaps. The FliT
transformation of \textit{PMRobust\textsubscript{flit}} further
reduces the gap on many workloads for P-ART and P-BwTree due to their
frequent atomic loads. The improvement is most notable for P-ART,
which performs many atomic loads during its tree traversal on each key
insertion. \textit{PMRobust\textsubscript{flit}} outperforms the
original program significantly on workload E as it eliminates most of
the overhead from the originally missing flushes mentioned in
Section~\ref{ref:rq2}.  It also outperforms the original
Memcached by approximately 20\%, which can be explained by the fact
that \tool inlines flush instructions whereas the original benchmark
called a flush function.

Across all benchmarks, the geometric mean overhead over the original
programs is 11.21\% for \textit{PMRobust\textsubscript{base}}, and
6.41\% for \textit{PMRobust\textsubscript{opt}}, and only \overhead
for \textit{PMRobust\textsubscript{flit}}.

Overall, the performance results show that \tool's automatic flush and
fences insertion is able to match the performance of the original
programs on most benchmarks using our dataflow analysis and FliT, while only using a few user annotations. This answers the research question in the affirmative---\tool is able to produce programs with performance close to the originals,

\subsection{RQ3: Developer Burden}\label{ref:rq3}
In this section we answer RQ3 regarding the extent users of \tool need to be involved to relax the persistency requirement using manual annotations. As we noted in \ref{ref:rq2}, there was only one instance of significant bottleneck that we removed with an annotation during the performance evaluation. We identified the source of the bottleneck by profiling. 

Recall that we noted that this one example was unusual, and due to the fact that RECIPE does not implement a reasonable memory allocation strategy.  We would not expect to need annotations for this same reason in non-research software.
Based on our experience, \tool does not impose too much burden on developers as bottlenecks should be rare, and fixing them only requires familiarity with profilers, which would be reasonable to expect from developers working on low-level software such as PM programs.

\subsection{Threats to Validity}

\Tool currently does not implement support for function pointers and may produce imprecise results for them.  This can be addressed by ensuring that all objects are clean before making a call using a function pointer and ensuring that if a function has its address taken, that all objects must be clean at exit.  This could also potentially be handled by pointer analysis.

Our current implementation uses whole-program analysis.  As a result, we perform
our analysis and transformation passes after all translation units are
linked together.
This
requirement can be removed by treating interactions with code outside of
the current compilation unit conservatively. 
