\section{Underlying Hardware and Failure Model}
Persistency semantics are important for a range of new memory
hardware. A well-studied example is Px86~\cite{intelx86}, the semantics of persistent memory for x86 processors, which adds a global \emph{persistent buffer} that holds stores after they exited the thread-local store buffers. Stores in the persistent buffer are written back to memory in arbitrary order at cache line granularity, and those not written back are lost upon a crash. To avoid inconsistent state caused by crashes under persistency semantics, fence and flushes instructions are often needed to impose persistency ordering between stores. 

While the problem has received much attention for
persistent memory, the same problem exists for CXL shared
 memory.  Figure~\ref{fig:memorysystem} presents a
graphical overview of CXL shared disaggregated memory.  In this
setting, processing nodes and memory nodes are connected via CXL
networking.  Memory can be coherently shared between multiple hosts
--- memory nodes contain directories that track which cache lines have
been requested by processing nodes.  CXL on x86 is intended to provide
TSO ordering guarantees across machines.

Failures of compute nodes pose a consistency problem.  Compute nodes
cache their writes to CXL shared memory and the contents of these
caches can be written back in an arbitrary order.  The CXL standard
provides for global persistent flush (GPF).  When GPF is supported, it uses
energy reserves to write the contents of the CPU cache back to the underlying storage upon a crash or system shutdown. However, GPF does not cover a wide
range of failure modes, \eg failed cables, failed CXL transceivers,
failed CPUs, failed motherboards, etc, and therefore does not provide a complete solution to the consistency problem.

If even a single machine that is updating a given CXL memory region
fails, the data in that machine's cache is corrupted for all machines
that access the CXL memory region.  In our email discussions with
Intel engineers, it is not yet decided how CXL will expose such
failures to the software layer.  One option is to report those
cache lines as poisoned and throw exceptions on access and another
is to allow software to access the copy of the data that
resides in the CXL memory node and let software manage flushing data
with flush and fence operations in a manner similar to 
PM systems.

\begin{figure}[!htbp]
\begin{center}
  \includegraphics[scale=0.26]{figures/cxl}
\end{center}
\vspace{-.2cm}
  \caption{CXL Memory System\label{fig:memorysystem}}
\end{figure}

\textbf{PM can be viewed as a special case of the CXL shared memory
model with the differences that: (1) there is only one processing node and
 (2) after a crash, the node can return having lost the state of its
local cache.}

The x86 architecture provides the following instructions to force the cache
to write data back to persistent storage: (1) the  \code{clflush} and \code{clflushopt} instructions that
flush (and evict) a cache line and (2) the 
\code{clwb} instruction that writes back a cache line potentially without eviction.  Each of these instructions takes as input the address to flush.  The \code{clflush} instruction flushes a cache line immediately while the \code{clflushopt} and \code{clwb} instructions are not guaranteed to flush or write back a cache line until the thread executes a fence instruction.

