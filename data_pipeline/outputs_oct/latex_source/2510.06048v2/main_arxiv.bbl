\begin{thebibliography}{69}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbas et~al.(2023)Abbas, Tirumala, Simig, Ganguli, and
  Morcos]{abbas2023semdedup}
Amro Abbas, Kushal Tirumala, D{\'a}niel Simig, Surya Ganguli, and Ari~S Morcos.
\newblock Semdedup: Data-efficient learning at web-scale through semantic
  deduplication.
\newblock \emph{arXiv preprint arXiv:2303.09540}, 2023.

\bibitem[Albalak et~al.(2023)Albalak, Pan, Raffel, and
  Wang]{albalak2023efficient}
Alon Albalak, Liangming Pan, Colin Raffel, and William~Yang Wang.
\newblock Efficient online data mixing for language model pre-training.
\newblock In \emph{R0-FoMo: Robustness of Few-shot and Zero-shot Learning in
  Large Foundation Models}, 2023.

\bibitem[Albalak et~al.(2024)Albalak, Elazar, Xie, Longpre, Lambert, Wang,
  Muennighoff, Hou, Pan, Jeong, et~al.]{albalak2024survey}
Alon Albalak, Yanai Elazar, Sang~Michael Xie, Shayne Longpre, Nathan Lambert,
  Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong,
  et~al.
\newblock A survey on data selection for language models.
\newblock \emph{arXiv preprint arXiv:2402.16827}, 2024.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Gao, Choi, et~al.]{bisk2020piqa}
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et~al.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~34, pp.\  7432--7439, 2020.

\bibitem[Borsos et~al.(2020)Borsos, Mutny, and Krause]{borsos2020coresets}
Zal{\'a}n Borsos, Mojmir Mutny, and Andreas Krause.
\newblock Coresets via bilevel optimization for continual learning and
  streaming.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 14879--14890, 2020.

\bibitem[Bracken \& McGill(1973)Bracken and McGill]{bracken1973mathematical}
Jerome Bracken and James~T McGill.
\newblock Mathematical programs with optimization problems in the constraints.
\newblock \emph{Operations Research}, 21\penalty0 (1):\penalty0 37--44, 1973.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
  Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter,
  Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
  Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
  Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Chen et~al.(2023)Chen, Xu, and Zhang]{chen2023bilevel1}
Lesi Chen, Jing Xu, and Jingzhao Zhang.
\newblock On bilevel optimization without lower-level strong convexity.
\newblock \emph{arXiv preprint arXiv:2301.00712}, 2023.

\bibitem[Chen et~al.(2024)Chen, Roberts, Bhatia, Wang, Zhang, Sala, and
  R{\'e}]{chen2024skill}
Mayee Chen, Nicholas Roberts, Kush Bhatia, Jue Wang, Ce~Zhang, Frederic Sala,
  and Christopher R{\'e}.
\newblock Skill-it! a data-driven skills framework for understanding and
  training language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Chowdhery et~al.(2023)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2023palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0
  (240):\penalty0 1--113, 2023.

\bibitem[Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and
  Toutanova]{clark2019boolq}
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
  Collins, and Kristina Toutanova.
\newblock Boolq: Exploring the surprising difficulty of natural yes/no
  questions.
\newblock \emph{arXiv preprint arXiv:1905.10044}, 2019.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick,
  and Tafjord]{clark2018think}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa
  Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning
  challenge.
\newblock \emph{arXiv preprint arXiv:1803.05457}, 2018.

\bibitem[Conneau \& Lample(2019)Conneau and Lample]{conneau2019cross}
Alexis Conneau and Guillaume Lample.
\newblock Cross-lingual language model pretraining.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Cook(1977)]{cook1977detection}
R~Dennis Cook.
\newblock Detection of influential observation in linear regression.
\newblock \emph{Technometrics}, 19\penalty0 (1):\penalty0 15--18, 1977.

\bibitem[Dagr{\'e}ou et~al.(2022)Dagr{\'e}ou, Ablin, Vaiter, and
  Moreau]{dagreou2022framework}
Mathieu Dagr{\'e}ou, Pierre Ablin, Samuel Vaiter, and Thomas Moreau.
\newblock A framework for bilevel optimization that enables stochastic and
  global variance reduction algorithms.
\newblock \emph{arXiv preprint arXiv:2201.13409}, 2022.

\bibitem[Dempe(2002)]{dempe2002foundations}
Stephan Dempe.
\newblock \emph{Foundations of bilevel programming}.
\newblock Springer Science \& Business Media, 2002.

\bibitem[Du et~al.(2022)Du, Huang, Dai, Tong, Lepikhin, Xu, Krikun, Zhou, Yu,
  Firat, et~al.]{du2022glam}
Nan Du, Yanping Huang, Andrew~M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu,
  Maxim Krikun, Yanqi Zhou, Adams~Wei Yu, Orhan Firat, et~al.
\newblock Glam: Efficient scaling of language models with mixture-of-experts.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5547--5569. PMLR, 2022.

\bibitem[Elazar et~al.(2023)Elazar, Bhagia, Magnusson, Ravichander, Schwenk,
  Suhr, Walsh, Groeneveld, Soldaini, Singh, et~al.]{elazar2023s}
Yanai Elazar, Akshita Bhagia, Ian Magnusson, Abhilasha Ravichander, Dustin
  Schwenk, Alane Suhr, Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer
  Singh, et~al.
\newblock What's in my big data?
\newblock \emph{arXiv preprint arXiv:2310.20707}, 2023.

\bibitem[Engstrom et~al.(2024)Engstrom, Feldmann, and Madry]{engstrom2024dsdm}
Logan Engstrom, Axel Feldmann, and Aleksander Madry.
\newblock Dsdm: Model-aware dataset selection with datamodels.
\newblock \emph{arXiv preprint arXiv:2401.12926}, 2024.

\bibitem[Fan et~al.(2023)Fan, Pagliardini, and Jaggi]{fan2023doge}
Simin Fan, Matteo Pagliardini, and Martin Jaggi.
\newblock Doge: Domain reweighting with generalization estimation.
\newblock \emph{arXiv preprint arXiv:2310.15393}, 2023.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{International conference on machine learning}, pp.\
  1126--1135. PMLR, 2017.

\bibitem[Franceschi et~al.(2018)Franceschi, Frasconi, Salzo, Grazzi, and
  Pontil]{franceschi2018bilevel}
Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and
  Massimiliano Pontil.
\newblock Bilevel programming for hyperparameter optimization and
  meta-learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1568--1577. PMLR, 2018.

\bibitem[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang,
  He, Thite, Nabeshima, Presser, and Leahy]{gao2020pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
  Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser,
  and Connor Leahy.
\newblock The pile: An 800gb dataset of diverse text for language modeling.
\newblock \emph{arXiv preprint arXiv:2101.00027}, 2020.

\bibitem[Gao et~al.(2021)Gao, Tow, Biderman, Black, DiPofi, Foster, Golding,
  Hsu, McDonell, Muennighoff, et~al.]{gao2021framework}
Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles
  Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
  et~al.
\newblock A framework for few-shot language model evaluation.
\newblock \emph{Version v0. 0.1. Sept}, 10:\penalty0 8--9, 2021.

\bibitem[Ghadimi \& Wang(2018)Ghadimi and Wang]{ghadimi2018approximation}
Saeed Ghadimi and Mengdi Wang.
\newblock Approximation methods for bilevel programming.
\newblock \emph{arXiv preprint arXiv:1802.02246}, 2018.

\bibitem[Gong et~al.(2024)Gong, Hao, and Liu]{gong2024a}
Xiaochuan Gong, Jie Hao, and Mingrui Liu.
\newblock A nearly optimal single loop algorithm for stochastic bilevel
  optimization under unbounded smoothness.
\newblock In \emph{Forty-first International Conference on Machine Learning},
  2024.

\bibitem[{Google}(2024)]{google2024geminiterms}
{Google}.
\newblock {Gemini API Additional Terms of Service}, 2024.
\newblock URL \url{https://ai.google.dev/gemini-api/terms}.
\newblock Accessed: January 30, 2025.

\bibitem[Grangier et~al.(2023)Grangier, Ablin, and Hannun]{grangier2023bilevel}
David Grangier, Pierre Ablin, and Awni Hannun.
\newblock Bilevel optimization to learn training distributions for language
  modeling under domain shift.
\newblock In \emph{NeurIPS 2023 Workshop on Distribution Shifts: New Frontiers
  with Foundation Models}, 2023.

\bibitem[Grazzi et~al.(2022)Grazzi, Pontil, and Salzo]{grazzi2022bilevel}
Riccardo Grazzi, Massimiliano Pontil, and Saverio Salzo.
\newblock Bilevel optimization with a lower-level contraction: Optimal sample
  complexity without warm-start.
\newblock \emph{arXiv preprint arXiv:2202.03397}, 2022.

\bibitem[Hampel(1974)]{hampel1974influence}
Frank~R Hampel.
\newblock The influence curve and its role in robust estimation.
\newblock \emph{Journal of the american statistical association}, 69\penalty0
  (346):\penalty0 383--393, 1974.

\bibitem[Hao et~al.(2023)Hao, Ji, and Liu]{hao2023bilevel1}
Jie Hao, Kaiyi Ji, and Mingrui Liu.
\newblock Bilevel coreset selection in continual learning: A new formulation
  and algorithm.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem[Hao et~al.(2024)Hao, Gong, and Liu]{hao2024bilevel}
Jie Hao, Xiaochuan Gong, and Mingrui Liu.
\newblock Bilevel optimization under unbounded smoothness: A new algorithm and
  convergence analysis.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}, 2024.

\bibitem[Hong et~al.(2023)Hong, Wai, Wang, and Yang]{hong2023two}
Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang.
\newblock A two-timescale stochastic algorithm framework for bilevel
  optimization: Complexity analysis and application to actor-critic.
\newblock \emph{SIAM Journal on Optimization}, 33\penalty0 (1):\penalty0
  147--180, 2023.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and
  Chen]{hu2021lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
  Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{arXiv preprint arXiv:2106.09685}, 2021.

\bibitem[Ji et~al.(2021)Ji, Yang, and Liang]{ji2021bilevel}
Kaiyi Ji, Junjie Yang, and Yingbin Liang.
\newblock Bilevel optimization: Convergence analysis and enhanced design.
\newblock In \emph{International conference on machine learning}, pp.\
  4882--4892. PMLR, 2021.

\bibitem[Koh \& Liang(2017)Koh and Liang]{koh2017understanding}
Pang~Wei Koh and Percy Liang.
\newblock Understanding black-box predictions via influence functions.
\newblock In \emph{International conference on machine learning}, pp.\
  1885--1894. PMLR, 2017.

\bibitem[Kwon et~al.(2023)Kwon, Kwon, Wright, and Nowak]{kwon2023fully}
Jeongyeol Kwon, Dohyun Kwon, Stephen Wright, and Robert~D Nowak.
\newblock A fully first-order method for stochastic bilevel optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  18083--18113. PMLR, 2023.

\bibitem[Lauren{\c{c}}on et~al.(2022)Lauren{\c{c}}on, Saulnier, Wang, Akiki,
  Villanova~del Moral, Le~Scao, Von~Werra, Mou, Gonz{\'a}lez~Ponferrada,
  Nguyen, et~al.]{laurenccon2022bigscience}
Hugo Lauren{\c{c}}on, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert
  Villanova~del Moral, Teven Le~Scao, Leandro Von~Werra, Chenghao Mou, Eduardo
  Gonz{\'a}lez~Ponferrada, Huu Nguyen, et~al.
\newblock The bigscience roots corpus: A 1.6 tb composite multilingual dataset.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 31809--31826, 2022.

\bibitem[Lee et~al.(2021)Lee, Ippolito, Nystrom, Zhang, Eck, Callison-Burch,
  and Carlini]{lee2021deduplicating}
Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck,
  Chris Callison-Burch, and Nicholas Carlini.
\newblock Deduplicating training data makes language models better.
\newblock \emph{arXiv preprint arXiv:2107.06499}, 2021.

\bibitem[Li et~al.(2024)Li, Fang, Smyrnis, Ivgi, Jordan, Gadre, Bansal, Guha,
  Keh, Arora, et~al.]{li2024datacomp}
Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre,
  Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, et~al.
\newblock Datacomp-lm: In search of the next generation of training sets for
  language models.
\newblock \emph{arXiv preprint arXiv:2406.11794}, 2024.

\bibitem[Ling(1984)]{ling1984residuals}
Robert~F Ling.
\newblock Residuals and influence in regression, 1984.

\bibitem[Liu et~al.(2020)Liu, Cui, Liu, Huang, Wang, and Zhang]{liu2020logiqa}
Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang.
\newblock Logiqa: A challenge dataset for machine reading comprehension with
  logical reasoning.
\newblock \emph{arXiv preprint arXiv:2007.08124}, 2020.

\bibitem[Maini et~al.(2024)Maini, Seto, Bai, Grangier, Zhang, and
  Jaitly]{maini2024rephrasing}
Pratyush Maini, Skyler Seto, He~Bai, David Grangier, Yizhe Zhang, and Navdeep
  Jaitly.
\newblock Rephrasing the web: A recipe for compute and data-efficient language
  modeling.
\newblock \emph{arXiv preprint arXiv:2401.16380}, 2024.

\bibitem[Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and
  Sabharwal]{mihaylov2018can}
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.
\newblock Can a suit of armor conduct electricity? a new dataset for open book
  question answering.
\newblock \emph{arXiv preprint arXiv:1809.02789}, 2018.

\bibitem[{OpenAI}(2024)]{openai2024terms}
{OpenAI}.
\newblock {OpenAI Terms of Service}, 2024.
\newblock URL \url{https://openai.com/terms}.
\newblock Accessed: Jan 30, 2025.

\bibitem[Oren et~al.(2019)Oren, Sagawa, Hashimoto, and
  Liang]{oren2019distributionally}
Yonatan Oren, Shiori Sagawa, Tatsunori~B Hashimoto, and Percy Liang.
\newblock Distributionally robust language modeling.
\newblock \emph{arXiv preprint arXiv:1909.02060}, 2019.

\bibitem[Pan et~al.(2024)Pan, Zhang, Pan, Pi, Wang, and Zhang]{pan2024scalebio}
Rui Pan, Jipeng Zhang, Xingyuan Pan, Renjie Pi, Xiaoyu Wang, and Tong Zhang.
\newblock Scalebio: Scalable bilevel optimization for llm data reweighting.
\newblock \emph{arXiv preprint arXiv:2406.19976}, 2024.

\bibitem[Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi,
  Pezzelle, Baroni, Boleda, and Fern{\'a}ndez]{paperno2016lambada}
Denis Paperno, Germ{\'a}n Kruszewski, Angeliki Lazaridou, Quan~Ngoc Pham,
  Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel
  Fern{\'a}ndez.
\newblock The lambada dataset: Word prediction requiring a broad discourse
  context.
\newblock \emph{arXiv preprint arXiv:1606.06031}, 2016.

\bibitem[Park et~al.(2023)Park, Georgiev, Ilyas, Leclerc, and
  Madry]{park2023trak}
Sung~Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and
  Aleksander Madry.
\newblock Trak: Attributing model behavior at scale.
\newblock \emph{arXiv preprint arXiv:2303.14186}, 2023.

\bibitem[Penedo et~al.(2023)Penedo, Malartic, Hesslow, Cojocaru, Cappelli,
  Alobeidli, Pannier, Almazrouei, and Launay]{penedo2023refinedweb}
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,
  Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,
  and Julien Launay.
\newblock The refinedweb dataset for falcon llm: outperforming curated corpora
  with web data, and web data only.
\newblock \emph{arXiv preprint arXiv:2306.01116}, 2023.

\bibitem[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song,
  Aslanides, Henderson, Ring, Young, et~al.]{rae2021scaling}
Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,
  Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,
  et~al.
\newblock Scaling language models: Methods, analysis \& insights from training
  gopher.
\newblock \emph{arXiv preprint arXiv:2112.11446}, 2021.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{Journal of machine learning research}, 21\penalty0
  (140):\penalty0 1--67, 2020.

\bibitem[Rajpurkar(2016)]{rajpurkar2016squad}
P~Rajpurkar.
\newblock Squad: 100,000+ questions for machine comprehension of text.
\newblock \emph{arXiv preprint arXiv:1606.05250}, 2016.

\bibitem[Sagawa et~al.(2019)Sagawa, Koh, Hashimoto, and
  Liang]{sagawa2019distributionally}
Shiori Sagawa, Pang~Wei Koh, Tatsunori~B Hashimoto, and Percy Liang.
\newblock Distributionally robust neural networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Sakaguchi et~al.(2021)Sakaguchi, Bras, Bhagavatula, and
  Choi]{sakaguchi2021winogrande}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock \emph{Communications of the ACM}, 64\penalty0 (9):\penalty0 99--106,
  2021.

\bibitem[Shen et~al.(2024)Shen, Chen, Das, and Chen]{shen2024seal}
Han Shen, Pin-Yu Chen, Payel Das, and Tianyi Chen.
\newblock Seal: Safety-enhanced aligned llm fine-tuning via bilevel data
  selection.
\newblock \emph{arXiv preprint arXiv:2410.07471}, 2024.

\bibitem[Somayajula et~al.(2023)Somayajula, Jin, Song, Mi, and
  Yu]{somayajula2023bi}
Sai~Ashish Somayajula, Lifeng Jin, Linfeng Song, Haitao Mi, and Dong Yu.
\newblock Bi-level finetuning with task-dependent similarity structure for
  low-resource training.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  ACL 2023}, pp.\  8569--8588, 2023.

\bibitem[Sorscher et~al.(2022)Sorscher, Geirhos, Shekhar, Ganguli, and
  Morcos]{sorscher2022beyond}
Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos.
\newblock Beyond neural scaling laws: beating power law scaling via data
  pruning.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 19523--19536, 2022.

\bibitem[Tirumala et~al.(2023)Tirumala, Simig, Aghajanyan, and
  Morcos]{tirumala2023d4}
Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari Morcos.
\newblock D4: Improving llm pretraining via document de-duplication and
  diversification.
\newblock \emph{Advances in Neural Information Processing Systems},
  36:\penalty0 53983--53995, 2023.

\bibitem[Welbl et~al.(2017)Welbl, Liu, and Gardner]{welbl2017crowdsourcing}
Johannes Welbl, Nelson~F Liu, and Matt Gardner.
\newblock Crowdsourcing multiple choice science questions.
\newblock \emph{arXiv preprint arXiv:1707.06209}, 2017.

\bibitem[Wenzek et~al.(2019)Wenzek, Lachaux, Conneau, Chaudhary, Guzm{\'a}n,
  Joulin, and Grave]{wenzek2019ccnet}
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary,
  Francisco Guzm{\'a}n, Armand Joulin, and Edouard Grave.
\newblock Ccnet: Extracting high quality monolingual datasets from web crawl
  data.
\newblock \emph{arXiv preprint arXiv:1911.00359}, 2019.

\bibitem[Wettig et~al.(2024)Wettig, Gupta, Malik, and Chen]{wettig2024qurating}
Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen.
\newblock Qurating: Selecting high-quality data for training language models.
\newblock \emph{arXiv preprint arXiv:2402.09739}, 2024.

\bibitem[Xia et~al.(2023)Xia, Gao, Zeng, and Chen]{xia2023sheared}
Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen.
\newblock Sheared llama: Accelerating language model pre-training via
  structured pruning.
\newblock \emph{arXiv preprint arXiv:2310.06694}, 2023.

\bibitem[Xia et~al.(2024)Xia, Malladi, Gururangan, Arora, and
  Chen]{xia2024less}
Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi
  Chen.
\newblock Less: Selecting influential data for targeted instruction tuning.
\newblock \emph{arXiv preprint arXiv:2402.04333}, 2024.

\bibitem[Xie et~al.(2023{\natexlab{a}})Xie, Pham, Dong, Du, Liu, Lu, Liang, Le,
  Ma, and Yu]{xie2023doremi}
Sang~Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy
  Liang, Quoc~V Le, Tengyu Ma, and Adams~Wei Yu.
\newblock Doremi: Optimizing data mixtures speeds up language model
  pretraining.
\newblock \emph{Advances in Neural Information Processing Systems},
  2023{\natexlab{a}}.

\bibitem[Xie et~al.(2023{\natexlab{b}})Xie, Santurkar, Ma, and
  Liang]{xie2023data}
Sang~Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy~S Liang.
\newblock Data selection for language models via importance resampling.
\newblock \emph{Advances in Neural Information Processing Systems},
  36:\penalty0 34201--34227, 2023{\natexlab{b}}.

\bibitem[Yu et~al.(2024)Yu, Das, and Xiong]{yu2024mates}
Zichun Yu, Spandan Das, and Chenyan Xiong.
\newblock Mates: Model-aware data selection for efficient pretraining with data
  influence models.
\newblock \emph{arXiv preprint arXiv:2406.06046}, 2024.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and
  Choi]{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock \emph{arXiv preprint arXiv:1905.07830}, 2019.

\bibitem[Zhou et~al.(2022)Zhou, Pi, Zhang, Lin, Chen, and
  Zhang]{zhou2022probabilistic}
Xiao Zhou, Renjie Pi, Weizhong Zhang, Yong Lin, Zonghao Chen, and Tong Zhang.
\newblock Probabilistic bilevel coreset selection.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  27287--27302. PMLR, 2022.

\end{thebibliography}
