\section{Conclusion}
We presented PhysHMR, a unified framework for reconstructing physically plausible human motion from monocular videos by directly mapping visual inputs to humanoid control actions. Unlike prior methods, PhysHMR learns a visual-to-action policy that integrates physical dynamics during inference. To improve efficiency and robustness, we introduce motion distillation from a mocap-trained expert and a novel pixel-as-ray strategy that provides soft global guidance without relying on noisy 3D root predictions. 

\noindent \textbf{Limitation and Future Work.}
While PhysHMR generates high-fidelity motion, a real-to-sim gap persists due to differences in body mechanics and contact properties, which can sometimes lead to visible artifacts.  Future work will incorporate personalized physical parameters to better reflect real-world dynamics. Additionally, motion reconstruction from a single monocular video is underconstrained due to ambiguity and occlusion; using a conditional generative model instead of a deterministic policy may better capture diverse and physically plausible motions. Our current framework does not explicitly support humanâ€“scene interactions (e.g., sitting or leaning against surfaces), which we plan to address through environment reconstruction and interaction-aware control in our future works.
