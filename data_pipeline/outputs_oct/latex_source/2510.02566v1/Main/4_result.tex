
\section{Experiments}
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{Figs/main.pdf}
\caption{
Comparison against two physics-based methods. The black line indicates the ground. PhysPT (row 2) uses neural networks to approximate physics, but still suffers from ground penetration. PHC+ (row 3) amplifies motion reconstruction errors during tracking, leading to unstable results. Both methods cannot correct upstream errors. In contrast, our visual-to-action approach produces motion that is both physically plausible and visually aligned.
}
  \Description{Comparison}
    \label{results}
\end{figure*}

\begin{table*}[t]
\centering
\caption{Comparison of our motion reconstruction variants on AIST++ and EMDB2 under kinematic and physical plausibility metrics. Lower is better.}
\label{tab:main}
\small
\begin{tabular}{l l 
                c c c c c c c 
                c c c c c c c}
\toprule
\multirow{2}{*}{{Phys. Type}} & \multirow{2}{*}{{Method}} 
& \multicolumn{7}{c}{{EMDB2}} 
& \multicolumn{7}{c}{{AIST++}} \\
\cmidrule(lr){3-9} \cmidrule(lr){10-16}
 &  & PA & WA & MPJ & FS & HV & ACC & VEL
     & PA & WA & MPJ & FS & HV & ACC & VEL \\
\midrule
\multirow{2}{*}{Kin.} 
    & TRAM               &  \textbf{35.51}  &  \textbf{148.05}  & \underline{56.74}  &  11.76 & 22.97  &  \textbf{4.77}  & \textbf{8.77 }  
                          & \textbf{50.18}  & 189.44 &  \underline{76.30} &  23.18 &  7.93 &  9.31 &   21.85 \\
    & GVHMR             &  40.95 &  228.67  &  65.21 &  \underline{5.65}  & 26.42   &  5.40 &  \underline{10.19}
                        &  53.43 &  \textbf{175.64}  &  79.05 &  11.54  &  4.64   &  10.19 & \underline{14.40} \\
\midrule
\multirow{3}{*}{Neural} 
    & PhysPT (CLIFF)        & 48.40  &  762.78   &  77.00  &  11.02 &  6.54 & 6.72  &  19.92 
                          & 70.72  &  260.07  &  108.57  &  13.68  &  3.71  &  9.21  &  17.05 \\
    & TRAM $\times$ PhysPT       &   39.90  & 704.57  &  61.42  &  8.49  &  7.02  &   5.38 &  17.71
                          &   52.79  &  250.30   &  83.93  &  \underline{10.94} &  3.55  & \underline{8.59}  &  16.00  \\
    &  GVHMR $\times$ PhysPT    &  41.34  &   682.03  &  66.08   &  10.71 &  8.46   &  \underline{5.35 }  & 17.24 
                          & 55.29   & 235.66   &  83.93  &  11.24 &  \underline{3.46}  &  8.90 &  15.36  \\
\midrule
\multirow{2}{*}{Track.} 
    & TRAM $\times$ PHC+           & 52.94 & \underline{158.58} & 74.34  & 23.41 & 7.64 & 9.56  & 14.00 
                         & 71.21 &  212.28 & 101.70  & 36.57  & 4.95  & 12.55   &  22.74     \\
    & GVHMR $\times$ PHC+          &  46.24 & 193.01 &  72.50  & 12.71 &  7.71 & 7.43  &  12.21
                          & 67.38 & 193.23 & 109.77  & 24.79  &  6.05 & 10.05  & 17.10   \\
\midrule

V2A 

    & \textbf{Ours} & \underline{39.34}  &   189.26  &  \textbf{55.48}  &   \textbf{4.60}  & \textbf{5.04} & {5.49}   &  10.53     
                  &   \underline{50.40}  &  \underline{187.42}  &  \textbf{63.94}  &   \textbf{9.14} &  \textbf{3.10} &  \textbf{6.58}  & \textbf{12.13}   \\
\bottomrule
\end{tabular}
\end{table*}







\subsection{Implementation Details}
\label{sec:implementation}

We use Isaac Gym~\cite{makoviychuk2021isaac} as the physics simulator and train our model on a single NVIDIA L40 GPU. The physics simulation runs at 60 Hz, while control actions are issued at 30 Hz. All videos and motion sequences are sampled at 30 FPS for consistency. The physical model parameters (e.g., masses, joint torque limits, friction coefficients) all follow the settings in PHC~\cite{Luo2023PerpetualHC}.
% 2D keypoints are estimated with Vitpose \cite{xu2022vitpose}. 

We parallelize training with 1,536 environments to improve sample efficiency. The main policy network is implemented as an MLP with hidden layer dimensions of [2048, 1536, 1024, 1024, 512, 512] and SiLU as the activation function. Reinforcement learning is conducted with Proximal Policy Optimization (PPO), using a clip coefficient of 0.2. PHC+\cite{luo2024universal}
% PHC~\cite{Luo2023PerpetualHC} 
is used as the teacher policy. The distillation loss is jointly optimized with the PPO objective. 
We apply gradient clipping with a threshold of 50 to ensure stability. Early termination is enabled to reduce ineffective exploration on failed episodes and accelerate convergence. The model typically converges after approximately three days of training. 

The current pipeline depends on the pretrained GVHMR image encoder, which is not real-time, and therefore the entire system operates offline. Extending this framework with causal attention and efficient encoders could make online deployment feasible in future work.

%\noindent \textit{Dealing with Shape Variance.}
Our approach does not rely on explicit shape information. We estimate the human shape parameters of the SMPL model with an off-the-shelf tool and compute the scale difference relative to a zero-shape SMPL model. This scales the simulation space to match real-world units, such that the humanoid retains canonical zero-shape. %Specifically, we divide the translation component of \(T_t^{\text{c2w}}\) by the estimated scale factor, as reflected in Equation~\eqref{eq:xxx}.
%The scale parameter is used to adjust \(T^{c2w}_t\)
% , ensuring that the humanoid always maintains a zero-shape appearance within the simulator.

\subsection{Datasets}
We utilize Human3.6M \cite{h36m_pami},  AIST++ \cite{li2021learn}, EMDB2 \cite{kaufmann2023emdb}, and AMASS \cite{AMASS:ICCV:2019} for our experiments.
Human3.6M contains 3.6M 3D human poses from 11 actors across 4 viewpoints, with accurate 3D keypoints but no SMPL ground truth. We exclude sequences involving chairs to avoid simulator inconsistencies. SMPL parameters are estimated via GVHMR and refined using LBFGS by aligning SMPL joints with the 3D keypoints. Note that since the full Human3.6M dataset is used in training of both TRAM and GVHMR, we only use it to conduct ablation studies.
We use a zero-shape SMPL model and introduce an additional scale parameter to account for individual body proportions~\ref{sec:implementation}.
%AIST++ is a large-scale dance motion dataset containing 1,408 dance sequences performed by 30 dancers and captured from 9 camera views. 
AIST++ provides 1,408 dance sequences from 30 subjects across 9 views, featuring dynamic and diverse movements that are challenging for physics-based motion learning. AIST++ provides only a scale parameter without explicit shape information. AMASS is a large-scale, image-free motion capture dataset. EMDB2 contains long-range, moving-camera sequences. We remove sequences like skateboarding and stair climbing that are incompatible with simulation. We train PhysHMR on the combined training splits of Human3.6M, AIST++, and AMASS (image-free). Only AMASS is used for the AMP reward. EMDB2 is used for evaluation only.

\subsection{Metrics}  
To evaluate the accuracy and physical plausibility of the reconstructed motion, we use the following metrics:
(1) \textbf{MPJ} (Mean Per Joint Position Error, MPJPE, mm): Measures the average Euclidean distance between predicted and ground-truth 3D joint positions after aligning the root joint. %, reflecting overall reconstruction accuracy in a root-relative coordinate system.
(2) \textbf{WA} (World-aware MPJPE, WA-MPJPE, mm): Similar to MPJPE, but computed in the global coordinate system, capturing errors in both pose and global translation.
(3) \textbf{PA} (Procrustes Aligned MPJPE, PA-MPJPE, mm): Computes MPJPE after applying rigid alignment (scale, rotation, translation) to isolate pose errors independent of global position.
(4) \textbf{VEL} (Velocity Error, mm/s) and (5) \textbf{ACC} (Acceleration Error, mm/s\(^2\)): Measure the temporal consistency of joint movement across frames.

To assess physical realism, we introduce a new metric: (6) \textbf{HV} (Foot Height Variance, mm): In every frame, we record the vertical position of the lowest foot joint. We select the lowest 25 \% across all frames and compute their variance; smaller HV indicates more stable, physically realistic contact. This kinematic proxy evaluates contact consistency without requiring an explicit ground plane.
Additionally, we use (7) \textbf{FS} (Foot Sliding, mm) to measure undesired foot movement when the foot is expected to be in contact with the ground.
Together, these metrics provide a comprehensive evaluation of both motion accuracy and physical plausibility.


Physics-based methods are less stable than kinematic ones: once a failure occurs, the humanoid usually falls and remains collapsed, causing large errors to dominate the averages. To mitigate this, we split all test sequences into 100-frame clips and evaluate them individually. This protocol, also common in kinematics-based methods, ensures fairness. Following PHC+, we compute metrics only on successful clips (discarding those with PA-MPJPE > 100), which avoids excessive sequence removal while keeping the results representative.
%Physics-based methods are generally less stable than purely kinematic methods. Once a failure occurs, the humanoid typically falls and remains in a collapsed state for the remainder of the sequence, which leads to arbitrarily large errors dominating the averages. To address this, we split all test sequences into 100-frame clips and evaluate them individually. Note that this protocol is also commonly adopted by kinematics-based methods, and therefore does not introduce unfairness or bias into the comparison. Following PHC+, we compute metrics only on successfully executed sequences, discarding those with large errors (e.g., PA-MPJPE > 100). Splitting into smaller segments avoids excessive sequence removal while ensuring that the final metrics remain representative.





%To evaluate the accuracy and physical plausibility of the reconstructed motion, we use the following metrics:  
%\textbf{MPJPE} (Mean Per Joint Position Error): Measures the Euclidean distance between the predicted and ground-truth 3D joint positions in millimeters, evaluating overall reconstruction accuracy.  
%\textbf{WA-MPJPE} (World-aware MPJPE): Similar to MPJPE but computed in the global coordinate system, reflecting errors in both pose and global translation.  
%\textbf{PA-MPJPE} (Procrustes Aligned MPJPE): Computes the MPJPE after aligning the predicted joints to the ground-truth via a rigid transformation (scale, rotation, translation), isolating pose errors independent of global position.
%\textbf{Velocity Error} and \textbf{Acceleration Error}: Evaluate temporal consistency of joint movements.
%To assess the physical realism of the generated motion, we consider: \textbf{Foot Sliding}: Measures undesired foot movement when contact with the ground is expected.  
%\textbf{Foot Height variance}: \FQ{proposed by us}
%These metrics provide a comprehensive evaluation, balancing motion accuracy and physical plausibility.  




\subsection{Comparisons}  

We compare our method with both kinematic and physics-based state-of-the-art approaches. Kinematic methods, TRAM~\cite{TRAM} and GVHMR~\cite{shen2024gvhmr}, estimate human motion from videos without enforcing physical constraints. In contrast, PhysPT~\cite{zhang2024physpt} introduces a physics-based approach by first estimating SMPL parameters using CLIFF~\cite{li2022cliff} and then refining the motion with a transformer-based model to improve physical plausibility. We also provide results for GVHMR $\times$ PhysPT and TRAM $\times$ PhysPT, where the SMPL estimation backbone of PhysPT is replaced with TRAM and GVHMR, respectively, to ensure a fair comparison. 
Additionally, we evaluate tracking-based methods, TRAM $\times$ PHC+ and GVHMR $\times$ PHC+, where the tracking policy PHC+~\cite{luo2024universal} is applied to track the outputs of TRAM and GVHMR, respectively, providing a direct comparison between motion reconstruction via traditional tracking policies and our visual-to-action policy.


For fair comparison, all baselines rely on global human trajectories: GVHMR and TRAM each estimate their own, and variants (e.g., GVHMR × PHC+, TRAM × PHC+) follow them. Our method instead leverages the camera trajectory and 2D keypoints to form the pixel-as-ray input. Since TRAM estimates extrinsics while GVHMR does not, we use TRAM’s camera trajectory, rigidly aligned to GVHMR’s first-frame coordinates, to provide consistent camera input. 

We use the GVHMR encoder for image features, ensuring fairness on the vision side. For physics, policies trained on high-dynamic datasets (e.g., AIST++) are overly sensitive to noisy estimates, so we adopt PHC+ as the tracker baseline.


\subsubsection{Quantitative Results}
As shown in Tab. \ref{tab:main}, kinematic-based methods generally achieve lower errors on MPJPE, PA-MPJPE, and WA-MPJPE, as they are directly optimized to minimize 3D keypoint discrepancies and ignore physical constraints. In contrast, physics-based approaches trade off keypoint accuracy for physical plausibility. For example, PhysPT, TRAM $\times$ PhysPT, and GVHMR $\times$ PhysPT all achieve better physical metrics due to PhysPT's physics-aware design. However, its global trajectory relies on foot-ground contact prediction, which can be inaccurate and result in high WA-MPJPE, especially for long-range motions in EMDB2. %While its accuracy drop is less pronounced in in-place motions like dancing, it becomes significant with translational movement.

Traditional tracking-based methods, TRAM $\times$ PHC+ and GVHMR $\times$ PHC+, exhibit stable performance when the kinematic estimates are accurate, but their quality degrades severely when those estimates are poor, as seen on AIST++, where the challenging motions lead to high PA-MPJPE. Moreover, such methods fails to capitalize on physical simulation, with subpar FS and HV scores. This is due to excessive movements of the limbs during balance recovery, which degrades physical metrics.

In contrast, our method achieves state-of-the-art performance on FS and HV, demonstrating superior physical realism. It also remains competitive across MPJPE metrics. By learning policies directly from visual features, our approach can produce 3D human motion that is both physically plausible and visually aligned with the input video.


\subsubsection{User Study}
To further evaluate perceptual quality beyond quantitative error metrics, we conducted a user study comparing PhysHMR against PhysPT and GVHMR~$\times$~PHC+. 
Participants (26 in total) were presented with 5 groups of side-by-side videos and asked to select the result they perceived as more visually aligned with the input video and physically plausible. Overall, 66.3\% of participants preferred PhysHMR, compared to 19.9\% for PhysPT and 13.8\% for GVHMR~$\times$~PHC+ (see Table~\ref{tab:user-study}), indicating that our method not only improves numerical accuracy but also provides higher perceptual fidelity. 

\begin{table}[h]
\centering
\caption{User study preference results. Values indicate the percentage of times each method was preferred.}
\label{tab:user-study}
\begin{tabular}{lccc}
\toprule
Method & PhysHMR  & PhysPT & GVHMR~$\times$~PHC+ \\
\midrule
Preference (\%) & \textbf{66.3} & 19.9 & 13.8 \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Ablation Study}  
We conduct ablation experiments on H36M to validate the effectiveness of our proposed \textit{pixel-as-ray} formulation and the combined training strategy based on distillation and reinforcement learning.

\noindent \textbf{Effect of Pixel-as-Ray.}  
Table~\ref{tab:ablation-obs} evaluates the impact of different global instruction strategies. Removing the global instruction entirely (ImgFeat) yields good PA-MPJPE and MPJPE, but significantly worse WA-MPJPE, indicating that the humanoid mimics local motions well but fails to track global trajectories. Replacing pixel-as-ray with global supervision from explicit root-relative displacements estimated with GVHMR (+ 3D root) results in degraded performance across all metrics, as errors in root estimation introduce misleading guidance that conflicts with local motion. In contrast, using 2D keypoints via pixel-as-ray (+ pixelray) provides more robust and relaxed global instruction, achieving comparable PA-MPJPE to the no-global setting while substantially improving WA-MPJPE.

\noindent \textbf{Effect of Distillation.}  
Table~\ref{tab:ablation-policy} and Figure~\ref{fig:training-curves} compare different training strategies. We also report success rates, defined as the percentage of sequences where PA-MPJPE remains below 50 mm for all frames. Combining PPO with distillation achieves the highest success rate, showing that PPO substantially improves long-term stability.
Using PPO Only leads to slow convergence and suboptimal final performance. %Behavior cloning 
The distillation-only setting enables faster early-stage learning but lacks exploration, resulting in limited reward improvements. Note that in the Distillation Only setting, the reward is computed only for evaluation and not used during training. Our joint training strategy combines the strengths of both: it accelerates convergence and achieves higher final rewards, while also delivering better generalization on test sequences. 










\begin{table}[t]
  \centering
  \caption{Ablation on global instruction strategies. }
  \label{tab:ablation-obs}
  \begin{tabular}{lccc}
    \toprule
    Obs. Type   & PA ↓  & WA ↓ &   MPJ ↓  \\
    \midrule
    ImgFeat     &   \textbf{35.85}     &   142.17            &    47.78   \\
    + 3D root      &   38.70              &   195.59            &    65.03   \\
    + pixelray (Ours)     &   36.05              &   \textbf{112.60}   &    \textbf{47.01}   \\
    \bottomrule
  \end{tabular}
\end{table}



\begin{table}[t]
  \centering
  \caption{Comparison of policy learning strategies. Combining reinforcement learning (PPO) and distillation yields the best performance.}
  \label{tab:ablation-policy}
  \begin{tabular}{lcccc}
    \toprule
    Strategy & PA ↓ & WA ↓ & MPJ ↓ &SR ↑ \\
    \midrule
    PPO Only       &    42.18    &  117.33  &    58.25 &  65.5\%       \\
    Distill. Only       &    39.62    &  114.69  &    52.41  & 72.0\%   \\
    PPO + Distill.       &  \textbf{36.05}  &  \textbf{112.60}  &  \textbf{47.01}  & \textbf{88.4\%}\\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{Figs/Figure_4.pdf}
\caption{
Mean reward curves during training. PPO Only converges slowly and underperforms. Distillation Only converges quickly but plateaus early. Our approach (PPO + Distillation) achieves both faster convergence and higher final rewards.
}
  \Description{reward curves}
    \label{fig:training-curves}
\end{figure}