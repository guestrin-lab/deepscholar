\section{Preliminaries}

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.95\linewidth]{Figs/pipeline_final.pdf}
% \caption{
% Overview of our pipeline. A visual-to-action policy reconstructs physically plausible motion from monocular video. Training efficiency is improved by combining reinforcement learning and knowledge distillation. Global motion is guided using a \textit{pixel-as-ray} module that lifts 2D keypoints into 3D rays.
% }
%     \label{fig:pipeline}
% \end{figure*}
We formulate physically plausible human motion reconstruction as a goal-conditioned, physics-based motion imitation problem. Specifically, we use deep reinforcement learning (DRL) to train a policy that drives a simulated humanoid \cite{Luo2023PerpetualHC} to imitate motion sequences within a physical environment, with the goal signals as guidance. The policy, $\pi$, is modeled as a Markov Decision Process (MDP), defined by the tuple
\(\mathcal{M} = \langle S, A, T, R, \gamma \rangle\), where \( S \), \( A \), \( T \), \( R \), and \( \gamma \) denote the state space, action space, transition dynamics, reward function, and discount factor, respectively.

At each timestep \(t\), the state \(s_t\) consists of proprioceptive information \(s_t^p\) and goal information \(s_t^g\). Here, \(s_t^p\) includes the local 3D pose \(q_t\) and velocity \(\dot{q}_t\). In traditional motion imitation tasks, the goal \(s_t^g\) is typically defined by a reference trajectory \((\theta_t, \Gamma_t, \tau_t)\), encoding the local pose, global translation, and global rotation. In our method, the goal information is extracted from the input video, including frame-level visual features
% , target root rotation, 
and global spatial guidance computed by a pixel-as-ray strategy. This design enables the policy to directly leverage visual observations for motion imitation (see Sec.~\ref{sec:method}).

The action, \( a_t \), specifies target joint rotations, which are provided as control targets to a proportional-derivative 
(PD) controller to generate physically valid motion. 
 At each timestep \( t \), the humanoid agent samples an action \( a_t \in A\) from the policy \( \pi(a_t | s_t)\), 
where \( s_t \in S\) is the current state of the humanoid. The action is then executed in a physics simulator, 
producing the next state \( s_{t+1} = T(s_t, a_t)\) and the reward $r_t = R(s_t, a_t)$ for this action. We optimize the policy using Proximal Policy Optimization (PPO), with the objective of maximizing the expected discounted return:
\(\mathbb{E} \left[ \sum_{t=1}^{N} \gamma^{t-1} r_t \right].
\)
% \subsection{Physics-based Human Motion Control.}
% To ensure the physical plausibility of human motion, we learn a physics-based humanoid control policy that drives a simulated humanoid in a physics environment, where the humanoid's states serve as the human pose representation. 
% This task is formulated as a Markov Decision Process (MDP), defined by the tuple
% \(\mathcal{M} = \langle S, A, T, R, \gamma \rangle\), where \( S \), \( A \), \( T \), \( R \), and \( \gamma \) denote the state space, action space, transition dynamics, reward function, and discount factor, respectively. The control policy is typically achieved by using deep reinforcement learning, where at each timestep \( t \), the agent samples an action \( a_t \in A\) from the policy \( \pi(a_t | s_t)\), 
% where \( s_t \in S\) is the current state of the humanoid. The action is then executed in a physics simulator, 
% producing the next state \( s_{t+1} = T(s_t, a_t)\) and the reward $r_t = R(s_t, a_t)$ for this action. The overall objective is to maximize the expected discounted return:
% \(\mathbb{E} \left[ \sum_{t=1}^{N} \gamma^{t-1} r_t \right].
% \)

% \subsection{Goal Conditioned RL for Motion Imitation.}
% The humanoid state \( s_t \) consists of proprioceptive information \( s_t^p \) and goal information \( g_t \), which together condition the control policy.
% The proprioceptive state includes the local 3D body pose \( q_t \) and velocity \( \dot{q}_t \), while the goal state can be task-dependent.  
% In motion imitation, the goal \( g_t \) is typically defined by a reference motion \((\theta_t, \Gamma_t, \tau_t) \), where \(\theta_t\) represents the local pose, \(\Gamma_t\) denotes the global translation, and \(\tau_t\) corresponds to the global rotation. The policy is trained to replicate this motion.
% Actions \( a_t \in \mathbb{R}^{23 \times 3} \) specify target joint rotations, which are executed via a proportional-derivative 
% (PD) controller to generate physically valid motion. The policy is trained using Proximal Policy Optimization (PPO) to learn control strategies from high-structured reference signals.

% In our method, the goal information is redefined to include frame-level visual features extracted from the input video, the target root rotation, and global spatial guidance computed by a pixel-as-ray strategy. This design enables visual observations to directly influence action generation. The training strategy is also adapted accordingly, as detailed in Section~\ref{sec:method}.








%Method:
%1. Vision-Conditioned Policy，pretrained Encoder
%2. distillation. data augmentation
%3. Pixel-as-ray global motion instruction.





% \subsection{Local Reference from Visual Observations}
%  Visual Feature Extraction
%  Root Rotation Encoding
%
% \subsection{Global Guidance via Pixel-as-Ray}
%  Keypoint Lifting and Ray Construction
%  Canonical Projection
% Spatial Guidance Encoding
%
% \subsection{Policy Learning with Reinforcement and Distillation}
% Deal with shape variance
% Expert Distillation
% Reward Design



% Implement Details

\section{Method}
\label{sec:method}

Fig. \ref{fig:pipeline} provides an overview of our method.
Given a monocular video with $N$ frames $\{I_t\}_{t=1}^{N}$, our goal is to reconstruct a physically plausible human motion sequence, which consists of local poses $\{\theta_t \in \mathbb{R}^{23\times 3} \}_{t=1}^{N}$, global translation $\{\Gamma_t \in \mathbb{R}^3 \}_{t=1}^{N}$, and orientation $\{\tau_t \in \mathbb{R}^3 \}_{t=1}^{N}$ in the world. 

% Unlike existing methods that regress kinematic pose sequences, we reconstruct motion by learning control actions that drive a simulated humanoid in a physics-based environment. To improve training efficiency and stability, visual features are extracted from each frame using a pre-trained human mesh recovery model. These features are used as inputs to a visual-to-action policy network, which predicts control actions for the humanoid (Sec.~\ref{4.1},~\ref{4.2}). The resulting motion, executed within the simulator, naturally satisfies physical constraints such as ground contact and momentum conservation.
% To provide spatial grounding under dynamic cameras, we propose a pixel-as-ray strategy that lifts 2D keypoints into 3D rays and transforms them into global space using SLAM-estimated camera trajectories~\cite{TRAM}. These global rays provide soft guidance to the humanoid policy without enforcing strict positional constraints (Sec.~\ref{4.2}).
% Finally, to further enhance policy robustness and training efficiency, the policy network is optimize using Proximal Policy Optimization (PPO) jointly with knowledge distillation from a pre-trained motion imitation expert (Sec.~\ref{4.3}).

Our visual-to-action policy, PhysHMR, starts by extracting local visual features for each video frame using a pre-trained HMR model~\citep{shen2024gvhmr}. These features are then used as input to the policy network, which predicts control actions for the humanoid (Sec. \ref{4.1}). % , \ref{4.2}
% These actions are executed in a physics simulator, ensuring physical plausibility through constraints like contact and momentum conservation. 
To provide spatial grounding, we propose a pixel-as-ray strategy that lifts 2D keypoints into 3D rays and transforms them into global space using camera poses.
%SLAM-estimated  ~\cite{TRAM}
These global rays provide soft global guidance to the policy without enforcing strict positional constraints (Sec. \ref{4.2}). 
Finally, we enhance the training of our visual-to-action policy through knowledge distillation from a pre-trained motion imitation expert, leading to improved sample efficiency and policy robustness (Sec. \ref{4.3}).

%To account for dynamic cameras and provide spatial grounding, we estimate camera trajectories $\{T_t^{\text{c2w}}\}$ using a SLAM-based method~\cite{TRAM} and lift 2D keypoints into pixel-aligned 3D rays, which serve as global guidance signals for the humanoid (Sec.~\ref{4.2}).
%Instead of estimating kinematics-based per-frame human poses from the input images, our method outputs physically plausible body motion by employing a simulated humanoid in a physics-based environment and learning control signals (i.e., actions) from the images~(Sec. \ref{4.1}).
%To address the drifting issue and enable moving cameras, pixel-as-rays and SLAM-based methods are used to obtain camera trajectories \(\{T_t^{c2w}\}\) and compute the position \(\{\Gamma_t, \tau_t\}\) in world space for simulation.
%To improve training efficiency, we use Proximal Policy Optimization (PPO) while distilling knowledge from a pre-trained motion imitation policy network. 




% \subsection{Local Reference from Visual Observations}
%  Visual Feature Extraction
%  Root Rotation Encoding









\subsection{Local Reference from Visual Observations}
\label{4.1}
%\FQ{Sec 4.1 is to be revised. }
%Traditional methods first recover motion sequences from videos and then train a policy to imitate these motions~\cite{yuan2021simpoe}. However, in this process, the policy only has access to the reconstructed motion, which collapses multiple plausible solutions into a single estimate due to monocular ambiguity. As a result, the original visual uncertainty is lost, and the recovered motion may deviate from the true motion. Although the learned policy can enforce physical consistency, the resulting motion often fails to align well with the input video \YM{due to error accumulation}.

%To this end, we propose to learn a visual-to-action policy that directly predicts control actions from image features. This design preserves local structural cues from the original visual input and avoids compounding errors from prior kinematic estimation. However, designing such a visual policy presents two key challenges.
Prior works~\cite{shen2024gvhmr,ye2023slahmr} show that local motion features that capture relative joint articulation while being invariant to global root transformations are critical for effective motion learning. 
% To support effective policy learning, \YM{we need to first extract local motion features from the monocular video frames that are suitable for guiding motion control in the humanoid agent's local coordinate frame. 
%Specifically, such features should capture relative joint articulation while being invariant to global root transformations, which 
Such features are difficult to infer explicitly from images due to camera motion and depth ambiguity. 
Hence, we propose to use the pretrained video encoder from GVHMR~\cite{shen2024gvhmr}, which is trained to predict SMPL joint rotations relative to their parent joints. This naturally yields root-invariant visual features that serve as structured and physically meaningful input for local control. Unlike explicit pose reconstructions that commit to a single, potentially inaccurate estimate, these visual features retain rich pose-related information without collapsing to a deterministic pose.

Given video frames $\{I_t\}_{t=1}^{N}$, we first preprocess each frame $I_{t}$ to extract image features~\cite{goel2023humans}, bounding boxes~\cite{yolov8, li2022cliff}, 2D keypoints~\cite{xu2022vitpose}, and relative camera rotations~\cite{NEURIPS2023_7ac484b0}, denoted as $f^{\text{feat}}_t,f^{\text{bbox}}_t,f^{\text{kp2d}}_t,f^{\text{cam}}_t$, respectively. These per-frame features are then fed into the video encoder, which aggregates information across frames:
\[
\{F_t\}_{t=1}^{N} = \text{Enc}_{\text{GVHMR}}(\{f^{\text{feat}}_t,f^{\text{bbox}}_t,f^{\text{kp2d}}_t,f^{\text{cam}}_t\}_{t=1}^{N}) \in \mathbb{R}^{N \times D},
\]
where \(D\) is the feature dimension. The cross-frame fusion process not only enhances stability under occlusion but also supports flexible feature masking, allowing the model to operate with partial inputs. This flexibility enables the use of motion-only datasets like AMASS~\cite{AMASS:ICCV:2019} during training, even without paired RGB images, i.e., $f^{\text{feat}}_t$ is dropped.

Although local features bolster reconstruction robustness, accurate physics learning still demands global guidance, because simulations must be carried out in the world coordinate frame.
Thus, we enhance the local observation by leveraging GVHMR’s multitask MLP head to explicitly regress the future root orientation \(\bar{\tau}_{t+1}\) from the visual features \(F_t\). This auxiliary prediction provides a forward-looking estimate of the global root orientation in the camera coordinate system. We transform \(\bar{\tau}_{t+1}\) into the world frame and compute its relative difference from the current root pose \(\tau_t\) of the humanoid agent as:
\[
\Delta \tau_t = \tau_t^{-1} \bar{\tau}_{t+1}.
\]
This signal provides an explicit orientation cue that guides the agent’s future heading. We include both the visual feature \(F_t\) and the relative root orientation \(\Delta \tau_t\) in the observation passed to the policy at each timestep $t$.

% we decompose the observation into two components: local instruction and global instruction (see Sec.~\ref{4.2}). The local instruction aims to capture relative pose cues that are invariant to global root transformations, which are more suitable for control in the agent's local frame. 
% Traditional methods are typically conditioned on reconstructed motion, where relative targets can be explicitly computed (e.g., the relative translation target can be computed as \( \Delta \Gamma_{\text{rel}} = \tau_{\text{cur}}^{-1} (\Gamma_{\text{tar}} - \Gamma_{\text{cur}}) \)). However, visual features are expressed in the camera coordinate system, making it nontrivial to extract root-invariant motion cues directly from raw images. Moreover, reinforcement learning is sample-inefficient, and training a generalizable visual encoder from scratch is particularly challenging~\cite{schwarzer2021data, Luo_2024_CVPR}.

%To address this, We adopt the video encoder from GVHMR~\cite{shen2024gvhmr}, which is trained to predict SMPL joint rotations relative to their parent joints. These joint-angle representations naturally satisfy the root-invariant requirement and provide a structured input for local control. We use the resulting features as local observation \(\{F_t\}_{t=0}^N\) for the policy.
%To improve stability under occlusion and support temporal reasoning, we leverage GVHMR’s temporal fusion module, which aggregates neighboring frames and 2D keypoints. This design also enables us to construct pseudo-visual features from motion-only datasets like AMASS~\cite{AMASS:ICCV:2019}, by projecting 3D joints to 2D and forming temporally coherent visual inputs. This allows the policy to benefit from large-scale motion data, improving generalization and robustness.

% To address these challenges, we adopt the video encoder from GVHMR~\cite{shen2024gvhmr}, which is trained to predict SMPL joint rotations relative to their parent joints. These local joint-angle representations are naturally root-invariant and well-suited for local control. We use the resulting features as the local observation \(\{F_t\}_{t=0}^N\) for our policy. Notably, the features \(\{F_t\}_{t=0}^N\) also encode the root orientation in the camera coordinate system. By extracting the root joint orientation $\bar{\tau}_{t+1}$ from the visual feature and transforming it into the world coordinate system, we compute the orientation difference \(\Delta \tau_t = \tau_t^{-1} \bar{\tau}_{t+1}\) between the estimated and current root pose. This signal provides additional guidance for local control.



% In addition to providing root-invariant structure, GVHMR incorporates a cross-frame encoder that aggregates information from neighboring frames and 2D keypoints, improving stability under occlusion and enabling temporal reasoning. This design also allows us to construct pseudo-visual features from motion-only datasets such as AMASS~\cite{AMASS:ICCV:2019}, by projecting 3D joints to 2D and forming temporally coherent visual inputs. This allows the policy to benefit from large-scale motion data, improving generalization and robustness.



%First, task observations are most effective when defined in the agent’s local coordinate frame, since the control task aims to achieve a relative target rather than matching absolute global poses.
%When conditioned on reconstructed motion, relative targets can be explicitly computed (e.g., the relative translation target can be computed as \( T_{\text{rel}} = R_{\text{tar}} R_{\text{cur}}^{-1} (T_{\text{tar}} - T_{\text{cur}}) \)). In contrast, image features are inherently expressed in the camera coordinate system, and it is nontrivial to extract local, root-invariant motion cues from them. Second, reinforcement learning is sample-inefficient, and training a high-capacity, generalizable visual encoder from scratch is particularly challenging under the sparse supervision of reinforcement learning~\cite{schwarzer2021data, Luo_2024_CVPR}.

%To address both challenges, we factor the observation into two components: local instruction and global guidance. Local instruction aims to capture relative pose cues that are invariant to global root transformation, while global guidance provides absolute spatial grounding (see Sec.~\ref{4.2}). For the local part, we adopt the image encoder from \cite{shen2024gvhmr}, which is trained to predict joint rotations from images. Importantly, these pose predictions are defined in local joint coordinates relative to their parent nodes, which aligns well with the requirements of our local instruction. Moreover, the encoder is trained on large-scale image datasets for human mesh recovery, providing strong visual representations that mitigate the sample inefficiency of reinforcement learning.

%We further leverage the encoder’s temporal fusion mechanism, which aggregates information from neighboring frames and 2D keypoints to produce stable feature representations $\{F_t\}_{t=0}^N$ even under partial occlusions. This design also enables the use of motion-only datasets such as AMASS~\cite{AMASS:ICCV:2019}, by constructing pseudo-visual features through 2D projections and neighboring pose context. As a result, our policy can benefit from large-scale motion capture data during training, improving its generalization and robustness.
























\subsection{Global Guidance via Pixel-as-Ray}
\label{4.2}

Accurate global positioning is critical for physically plausible motion reconstruction, especially when camera motion is involved. However, directly predicting 3D trajectories from a monocular video is often unreliable due to depth ambiguity and motion noise. These trajectory errors can significantly degrade the performance of tracking-based control policies, leading to unstable motion. To circumvent this issue, we propose a pixel-as-ray strategy that encodes global guidance without enforcing explicit positional targets.


\subsubsection{Keypoint Lifting to 3D Rays}
Given extracted 2D keypoints, \(f_{t}^{\text{kp2d}} = \{(u^i_t, v^i_t)\}_{i=1}^{J}\), that represent the image-space locations of each joint \(i\) of the simulated humanoid in frame \(t\), and the camera intrinsics matrix \(\mathbf{K}\), we back-project each keypoint to obtain a 3D ray in the camera coordinate system:
\begin{equation}
    \text{ray}^i_t(s) = \mathbf{o}_t + s \cdot \mathbf{r}^i_t, \quad s > 0,
\end{equation}
\begin{equation}
o_t = \begin{bmatrix}
    0\\
    0\\
    0
\end{bmatrix}, \quad
    \mathbf{r}^i_t = \mathbf{K}^{-1}
\begin{bmatrix}
u^i_t \\
v^i_t \\
1
\end{bmatrix},
\end{equation}
where $\mathbf{o}_t$ is the camera origin in frame $t$, and $\mathbf{r}^i_t$ is the viewing direction for keypoint $i$.
This ray represents all possible 3D positions of joint $i$ along the corresponding viewing direction.

To align the ray with the simulation world, we further transform it using the camera-to-world transformation  $\mathbf{T}_t^{\text{c2w}}$, which is estimated via off-the-shelf methods~\cite{shen2024gvhmr,TRAM}:

\begin{equation}
\hat{\mathbf{o}}_t = \mathbf{T}_t^{\text{c2w}} \cdot h(\mathbf{o}_t), \quad 
\hat{\mathbf{r}}^i_{t} = \mathbf{T}_t^{\text{c2w}} \cdot h(\mathbf{r}^i_t), 
\end{equation}
where $\mathbf{T}_t^{\text{c2w}}\in \mathbb{R}^{3\times4}$ is a transformation matrix consisting of rotation and translation, $h(\cdot)$ is the homogeneous lifting function that augments a 3D vector with a $1$, $\hat{\mathbf{o}}_t$ denotes the ray origin in world coordinates, and $\hat{\mathbf{r}}^i_{t}$ is the transformed ray direction for keypoint $i$.



\subsubsection{Computing Ray Displacement Vectors}
For each humanoid joint $i$ at time $t$, we compute the shortest vector from the joint position $\mathbf{j}^i_t$ to the corresponding ray defined by origin $\hat{\mathbf{o}}_t$ and direction $\hat{\mathbf{r}}^i_t$, yielding a displacement vector $\mathbf{d}^i_t$:
\begin{equation}
    \mathbf{d}^i_t = \text{proj}_{\perp}(\mathbf{j}^i_t, \hat{\mathbf{o}}_t, \hat{\mathbf{r}}^i_t),
\end{equation}

\noindent where $\text{proj}_{\perp}(\cdot)$ denotes the perpendicular offset from the point $\mathbf{j}^i_t$ to the ray \(\hat{\mathbf{o}}_t + s \cdot \hat{\mathbf{r}}^i_t\), and $\mathbf{j}^i_t$ is obtained from the simulated humanoid proprioception $\mathbf{s}_t^p$.
These displacements $\{\mathbf{d}^i_t\}_{i=1}^J$ are concatenated and passed to the policy network as global spatial observations. Compared to using noisy 3D joint positions, this formulation enables more flexible and robust spatial grounding for humanoid control.

Unlike reprojection error, which is typically used as a training loss, our pixel-as-ray formulation is explicitly used as part of the policy input, allowing the network to exploit these signals during inference for robust global alignment.

To account for potentially unreliable 2D keypoint estimates, 
% However, 2D keypoint estimates can be unreliable or missing due to occlusions or motion blur. To address this, 
we append the keypoint confidence scores predicted by the 2D keypoint estimator to the displacement vectors, enabling the policy to adaptively modulate its reliance on uncertain inputs. Additionally, inspired by \cite{goel2023humans}, we introduce random masking and perturbation of keypoint inputs during training to improve robustness under in-the-wild conditions.









\subsection{Policy Learning with Reinforcement and Distillation}
\label{4.3}

\subsubsection{Distillation} Although we've used a pretrained visual encoder to extract informative features from monocular images, directly training a control policy from these features using reinforcement learning remains highly sample-inefficient. To address this, we introduce a knowledge distillation framework that transfers motion expertise from a pretrained teacher policy, \(\pi_{\text{teach}}(a_t\mid s_t^p, \theta_t)\), which is trained to perform standard motion imitation using ground-truth pose supervision from the AMASS dataset~\cite{AMASS:ICCV:2019}. The teacher policy takes as input the agent's proprioceptive state \(s_t^p\) and the target kinematic pose \(\theta_t\), and outputs physically valid actions that track the reference motion.

Given paired supervision data \((I_t, \theta_t)\), where \(I_t\) is the input image and \(\theta_t\) is the corresponding target kinematic pose, we use the teacher's action as a supervision signal to guide the training of our visual-to-action policy, \(\pi_{\text{PhysHMR}}(a_t \mid s_t^p, F_t, \Delta \tau_t, \{\mathbf{d}_t^i\}_{i=1}^{J})\). This policy takes as input the agent’s proprioceptive state \(s_t^p\), frame-level visual features \(F_t\), relative root orientation \(\Delta \tau_t\), and pixel-to-ray spatial displacements \(\{\mathbf{d}_t^i\}_{i=1}^{J}\), and is trained to imitate the teacher’s actions without using ground-truth pose targets as explicit input. The training objective minimizes a distillation loss between the actions predicted by the student and teacher policies:
\[
L_{\text{distill}} = \left\| \pi_{\text{PhysHMR}}(a_t \mid s_t^p, F_t, \Delta \tau_t, \{\mathbf{d}_t^i\}_i) - \pi_{\text{teach}}(a_t \mid s_t^p, \theta_t) \right\|^2
\]

\noindent Note that both policies operate on the same proprioceptive state \(s_t^p\) obtained from simulation, but differ in how goal information is provided: the teacher policy is conditioned on the explicit ground-truth pose \(\theta_t\), while the PhysHMR policy relies on features extracted from the input image.
This objective encourages our PhysHMR policy to learn a control strategy capable of effectively imitating motion depicted in the input video without explicit pose signals.

\subsubsection{Overall Loss}
While distillation enables efficient learning from a strong teacher, it is inherently limited by the accuracy and diversity of the teacher’s actions. To enable more accurate and adaptable motion control that goes beyond the fixed supervision provided by the teacher, we complement supervised knowledge distillation with reinforcement learning, allowing the PhysHMR policy to refine its behavior through dynamic interaction with the environment. Specifically, we utilize a composite reward function:
\begin{equation}
R(s_t^{p}, \theta_t) = \alpha_1 R_{\text{pose}} + \alpha_2 R_{\text{amp}} + \alpha_3 R_{\text{energy}},
\label{eq:reward}
\end{equation}
where \(\theta_t\) is the target reference pose. \(R_{\text{pose}}\) is an imitation reward that promotes alignment with the reference pose, \(\theta_t\) \cite{Luo2023PerpetualHC}. \(R_{\text{amp}}\) is a style-based reward via Adversarial Motion Priors (AMP)~\cite{AMP} 
% A discriminator is trained to distinguish real motion sequences from those generated by the policy, providing a scalar reward 
that encourages the generation of realistic, human-like motion aligned with the motion prior. \(R_{\text{energy}}\) is an energy reward that penalizes excessive joint accelerations to improve smoothness and reduce jitter~\cite{10.1145/3550469.3555411}.  Additional details about the reward formulation are provided in the suppl. document.

The overall training objective combines both supervised and reinforcement signals:
\begin{equation}
L = L_{\text{distill}} + L_{\text{PPO}},
\end{equation}
where \(L_{\text{PPO}}\) is the loss term calculated by PPO using the reward in Eq.~\ref{eq:reward}. By jointly training with distillation and reinforcement learning, our visual-to-action policy benefits from both sample-efficient supervision and environment-driven refinement, resulting in not only accelerated convergence but also enhanced policy performance.
