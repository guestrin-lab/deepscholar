\section{Introduction}
Faithfully reconstructing human body dynamics from monocular videos, also known as Human Mesh Recovery (HMR), is a fundamental problem in computer vision, graphics, and robotics. Recent advances in human motion reconstruction~\cite{TRAM, WHAM, ye2023slahmr, shen2024gvhmr, PHALP, yuan2022glamr, TRACE} have achieved high accuracy in estimating body pose and shape. However, most existing methods overlook physically plausible body dynamics, leading to various artifacts such as foot sliding, ground penetration, and inconsistent contact behavior (see Fig.~\ref{fig:teaser}(b)). Achieving physically plausible human motion reconstruction remains an open and challenging problem.

Prior works have attempted to introduce physical constraints through a post-hoc correction stage. Some approaches incorporate analytical priors derived from rigid body dynamics, such as the Euler-Lagrange equation~\cite{zhang2024physpt,jiang2023drop,zhang2024incorporating}, while others leverage reinforcement learning to train humanoid controllers that track pre-reconstructed motion~\cite{yuan2021simpoe}. Although these methods improve physical realism to some extent, they share a \emph{common limitation}: motion is first reconstructed from visual cues alone and then refined by a separate physics module. This decoupled design overlooks the ambiguity inherent in monocular videos, where multiple plausible motions can explain the same visual observation. Once a single solution is selected in the reconstruction stage, the downstream physics module can no longer access the full observational context, leading to suboptimal corrections and limited consistency with the visual evidence (see Fig.~\ref{fig:teaser}c).
%Although encouraging results were demonstrated, they still fall short of achieving \textit{tracking accuracy} and \textit{physical plausibility} simultaneously, as shown in Fig.~\ref{fig:teaser} (c).
%Ensuring alignment with visual input while strictly adhering to physical constraints remains a key challenge.

In light of these limitations, we argue that a more effective approach is to {\bf unify motion estimation and physical reasoning within a single framework}, allowing visual cues and physical constraints to inform the same decision process. To this end, we propose PhysHMR, a novel framework that directly learns a visual-to-action policy to control a simulated humanoid directly from monocular video observations, resulting in reconstructed motion that is both visually consistent and physically plausible. Unlike prior two-stage approaches, PhysHMR unifies the two stages through a single policy network that jointly reasons over visual observations and physical dynamics. By executing motion within a physics-based simulator~\cite{makoviychuk2021isaac}, it naturally enforces physical constraints such as ground contact, joint limits, and momentum conservation. By conditioning the policy directly on image features, we can exploit rich visual context beyond skeletal pose estimations, enabling the humanoid to produce motion that faithfully aligns with the input video while adhering to physical laws.

Training high-dimensional visual-control policies purely with reinforcement learning is often sample-inefficient and unstable \cite{Luo_2024_CVPR}. To address these issues, PhysHMR proposes a distillation strategy that transfers knowledge from a mocap-trained imitation expert, thereby facilitating the training of the visual-to-action policy. Specifically, a pretrained visual encoder\cite{shen2024gvhmr} extracts features from each video frame, which serve as local pose references for the control policy. These features retain rich pose information without committing to potentially inaccurate 3D reconstructions. The expert controller, trained on high-quality motion capture data, provides action supervision that imparts strong human motion priors, which significantly accelerates convergence and stabilizes learning. The policy is further refined with reinforcement learning, using a composite reward that balances motion imitation, realism through adversarial motion priors, and physical smoothness.

Since physical plausibility must be assessed in the global pose space rather than the local pose space, it is necessary to estimate global pose information (i.e., the root joint position) in addition to local pose references from images. However, predicting the 3D root joint position from monocular video is often noisy, which significantly compromises the robustness of policy generalization. This is because inconsistencies between local pose estimates and erroneous 3D root predictions can lead to unnatural global motionsâ€”for example, the local pose may indicate forward movement, while the noisy root prediction pulls the motion backward, resulting in jittery or unstable behavior. Such mismatches make it difficult for the policy to produce physically consistent dynamics in global space. To address this, instead of relying on explicit 3D root prediction, we lift multiple detected 2D keypoints into 3D rays, which serve as a soft global pose reference. These spatial rays condition the policy to predict actions that transform the humanoid into globally consistent poses without requiring strict absolute 3D root input. This approach provides gentle global information, improves the robustness of policy execution, and enables physically plausible human motion reconstruction. 

We evaluate PhysHMR on challenging motion datasets, including Human3.6M, AIST++, and EMDB2, showing comparable motion accuracy to state-of-the-art kinematics-based methods while significantly improving physical plausibility. Our approach reduces common non-physical artifacts (e.g. foot sliding, ground penetration), improving the suitability of reconstructed motion for downstream applications such as simulation, animation, and robotics. 

In summary, our contributions are three-fold:
\begin{itemize}[leftmargin=*]
\item We present PhysHMR, the {\bf first} unified framework for jointly performing human motion perception and control, enabling high-quality and physically plausible human motion reconstruction from monocular videos. 

\item We introduce a distillation approach to distill a visual-to-action policy from a pretrained mocap imitation policy, which accelerates convergence and stabilizes policy learning. 

\item We propose a soft global grounding strategy by lifting 2D keypoints into 3D spatial rays, avoiding the need for noisy 3D root predictions and enabling robust policy learning of physically plausible motion in global space.
\end{itemize}


