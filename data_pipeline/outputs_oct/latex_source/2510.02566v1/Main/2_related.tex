
\section{Related Works}
\begin{figure*}[t]
    \centering
\includegraphics[width=0.9\linewidth]{Figs/pipeline_final.pdf}
\caption{
Overview of our pipeline. A visual-to-action policy reconstructs physically plausible motion from monocular videos. Training efficiency is improved by combining reinforcement learning and knowledge distillation. Global motion is guided using a \textit{pixel-as-ray} module that lifts 2D keypoints into 3D rays.
}
  \Description{pipeline}
    \label{fig:pipeline}
\end{figure*}
\subsection{Kinematics-Based Human Mesh Recovery}
Parametric human models~\cite{SMPL:2015, STAR:2020, SMPL-X:2019, 50649} have been widely adopted to reconstruct human motion from monocular video. 
% These models represent the human body using low-dimensional pose and shape parameters, allowing motion reconstruction to be formulated as estimating a sequence of such parameters across video frames. 
Early works~\cite{10.1007/978-3-319-46454-1_34, Arnab_CVPR_2019, MuVS:3DV:2017, xiang2019monocular} focus on fitting these models to individual image frames. 
More recently, regression-based approaches, which leverage large-scale datasets, have gained attention for their ability to achieve general-purpose human mesh recovery~\cite{goel2023humans, cai2023smplerx, yin2025smplest}. 
To account for dynamic camera movements, data-driven methods have been extended to estimate per-frame camera poses~\cite{TRACE, WHAM, yuan2022glamr}. Additionally, SLAM (Simultaneous Localization and Mapping) techniques have proven effective for robust camera motion estimation, further enhancing human motion recovery in complex scenarios~\cite{TRAM}.
HuMoR~\cite{rempe2021humor} learns a generative motion prior that improves temporal consistency and robustness in pose estimation.
Despite these advances in human mesh recovery, purely kinematic methods often exhibit artifacts like foot sliding, ground penetration, and momentum inconsistency.
% Despite advances in human mesh recovery both per-frame and in global coordinates, these methods often suffer from artifacts such as foot sliding, ground penetration, and momentum inconsistency due to their purely kinematic formulation.

To address such artifacts, prior works have used physical priors as an auxiliary supervision to encourage plausible dynamics. PhysPT~\cite{zhang2024physpt} proposes a neural module that refines kinematic motion using differentiable Euler-Lagrange losses to enforce rigid-body dynamics. IPMAN~\cite{tripathi2023intuitivephysics} incorporates intuitive physics cues through loss functions into monocular pose estimation, but remains a kinematics-based approach without enforcing full physical dynamics.
% PhysPT \cite{zhang2024physpt} proposes a neural refinement module that takes a kinematically reconstructed motion sequence as input and outputs a physically corrected version. The module is trained using differentiable loss terms derived from the Euler-Lagrange equations to enforce rigid-body dynamics.
D\&D~\cite{li2022dnd} refines kinematic motion by estimating external forces and applying analytical physical computation to enforce consistency with Newtonian dynamics. 
While these methods improve physical realism to some extent, they operate as post-hoc refinement on kinematic reconstructions, making it difficult to recover from the ambiguity in the kinematics-based human mesh recovery stage. Moreover, the physical consistency is enforced through neural approximations rather than explicit physical simulation, leaving the overall pipeline fundamentally kinematics-based and decoupled from physical control. 

\subsection{Physics-based Human Motion Imitation}
Physics simulation platforms~\cite{makoviychuk2021isaac, todorov2012mujoco}, combined with reinforcement learning, have enabled physically grounded control of simulated characters, producing highly realistic human motion~\cite{2018-TOG-deepMimic, wang2024skillmimic, AMP, 2022-TOG-ASE, dou2022case, tessler2023calm}. 
PPR~\cite{yang2023ppr} leverages physics priors for plausible video-based reconstruction, and differentiable dynamics models~\cite{gartner2022diffphy} integrate physics into end-to-end optimization. By training policies on large-scale motion capture datasets~\cite{AMASS:ICCV:2019, peng2021neural, kobayashi2023motion}, many works have demonstrated high-fidelity motion imitation through learned control policies~\cite{wagener2022mocapact, Luo2022EmbodiedSH, luo2024universal, Luo2023PerpetualHC, tessler2024maskedmimic, peng2018sfv, winkler2022questsim}. PhysCap~\cite{shimada2020physcap} constrains monocular capture with real-time physical simulation. However, these policies are trained to track clean 3D motion references, struggling to generalize when such data is unavailable. PHC~\cite{Luo2023PerpetualHC} estimates 3D keypoints from video as motion references, but its two-stage design decouples control from visual input, often leading to jitter and unnatural motion. 


Moreover, prior methods rely heavily on reinforcement learning, which typically suffers from low sample efficiency. Hence, they struggle to fully exploit rich visual information and instead depend primarily on sparse, deterministic inputs such as 3D keypoints or kinematics-based representations. 
simXR~\cite{Luo_2024_CVPR} employs a distillation-only scheme in a VR setting to train vision-to-action policies. While this avoids the need for reinforcement learning, it lacks robustness due to limited data and the absence of exploration. In contrast, our joint PPO+Distillation training substantially improves stability and generalization, demonstrating clear advantages over a pure distillation approach.

Learning vision-conditioned policies for human motion reconstruction that directly aligns with visual evidence remains a largely underexplored challenge.

