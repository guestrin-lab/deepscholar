\appendix



\section{About camera-to-world Transformation}

The key to linking the simulated world with the image domain is the $T^{c2w}_t$ transformation. There are multiple ways to compute it, and our method does not depend on a specific solution.

In the EMDB2 experiments, we combine TRAM and GVHMR. TRAM provides accurate SLAM-based camera trajectories, but its result may differ from the global frame by a rigid transformation. GVHMR, on the other hand, does not model the camera explicitly but produces outputs in gravity-aligned space, where the gravity direction is downward.

For moving camera,
we ignore translation and compute a global rotation by aligning the joint positions of GVHMR and TRAM (e.g., using Procrustes alignment). Once the rotation is known, we determine the translation by aligning the first-frame SMPL root position. We define the floor based on the foot position of the GVHMR result in the first frame, which is also used to initialize the humanoid in the simulator, similar to the PHC series. Small alignment errors are acceptable in practice. While the estimated ground height may contain small errors, it is acceptable in practice.

For static cameras, the process is simpler: we directly align the GVHMR first-frame gravity space result with the camera space.

\section{Dealing with Shape Variance}

In practice, the shape of a real human SMPL model can be approximated as a scaled version of the zero-shape SMPL. However, the humanoid used in the simulator always follows the zero-shape size. To resolve this mismatch, we apply a scale correction to the transformation:
\[
\hat{T}_{t}^{c2w} = T_{t}^{c2w}  \cdot \frac{1}{\text{scale}}
\]

\section{Reward Definition}

Following PHC+, we use a SMPL-based humanoid agent, which consists of 24 rigid bodies, 23 of which are actuated. The proprioceptive state $ s^{p}_{t} $ is defined as:
\begin{equation}
    s^{p}_{t} := (r_{t}, p_{t}, v_{t}, \omega_{t})
\end{equation}
where $ r_{t} $, $ p_{t} $, $ v_{t} $, and $ \omega_{t} $ are the simulated joint rotations, positions, velocities, and angular velocities, respectively. The reference state $\theta_{t} $ is defined as:
\begin{equation}
    \theta_{t} := (\hat{r}_{t+1} \ominus r_t, \hat{p}_{t+1} - p_t, \hat{v}_{t+1} - v_t, \hat{\omega}_{t+1} - \omega_t, \hat{r}_{t+1}, \hat{p}_{t+1})
\end{equation}
where $ \ominus $ denotes rotation difference. $ \hat{r}_{t+1} $, $ \hat{p}_{t+1} $, $ \hat{v}_{t+1} $, and $ \hat{\omega}_{t+1} $ represent the reference joint rotations, positions, velocities, and angular velocities, respectively. The imitation reward $R_{\text{pose}}$ is defined as:
\begin{equation}
    \begin{aligned}
            R_{\text{pose}} \ := w_{p}e^{-\lambda_{p}\|p_{t} - \hat{p}_{t}\|} + w_{r}e^{-\lambda_{r}\|r_{t} - \hat{r}_{t}\|} \\
    \ + w_{v} e^{-\lambda_{v}\|v_{t} - \hat{v}_{t}\|} + w_{\omega}e^{-\lambda_{\omega}\|\omega_{t} - \hat{\omega}_{t}\|}
    \end{aligned}
\end{equation}
where $ w_{\{ \cdot \}} $, $ \lambda_{\{ \cdot \}} $ denote the corresponding weights. We utilize the same weight settings as PHC+.

To obtain the style reward, $R_{\text{amp}}$, we train a discriminator $D(s^{p}_{t-10:t})$ jointly with the policy network to distinguish real motion sequences from those generated by the policy. The discriminator produces a scalar value based on the proprioception of the humanoid, encouraging the generation of realistic, human-like motion aligned with the motion prior.

\section{Test Data}

We exclude AIST++ sequences with high heels and ballet shoes, resulting in 326 videos from 4 camera views (totaling 1304 videos). From EMDB2, we remove sequences involving skateboards or stairs, which cannot be replicated in the simulator. The remaining sequences are:

\texttt{
P0\_09,  
P2\_19,  
P2\_20,  
P2\_24,  
P3\_27,  
P3\_28,  
P4\_35,  
P4\_36,  
P4\_37,  
P5\_40,  
P7\_55,  
P7\_61,  
P8\_65,  
P9\_79,  
P9\_80
}

In \texttt{P2\_24}, we removed frames 1650--1800 due to a step-up motion. The sequence was split into \texttt{P2\_24\_0} and \texttt{P2\_24\_1}. For \texttt{P4\_36}, we removed the first 300 frames, and for \texttt{P7\_61}, the first 600 frames (sitting or lying).

%\section{Evaluation Protocol}

%Physics-based methods are generally less stable. Once failure occurs, large errors dominate, making the overall average error meaningless. To address this, we split all test sequences into 100-frame clips and evaluate them individually. Following PHC+, we compute metrics only on successfully executed sequences, discarding those with large errors (e.g., PA-MPJPE > 100). Splitting into smaller segments avoids excessive sequence removal, while ensuring that the final metrics remain representative.