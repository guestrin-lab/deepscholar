
\documentclass{article} % For LaTeX2e
% \usepackage{arxiv}
\usepackage{arxiv_iclr,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}   % for \toprule, \midrule, \bottomrule
\usepackage{caption}    % for better caption control (optional)
\usepackage{array}      % for better table alignment (optional)
\usepackage{tabularx}   % for flexible column width
\usepackage{longtable}  % for tables that span multiple pages
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{arydshln}
\usepackage{threeparttable}


\title{Latent Speech-Text Transformer} 
% \\ Conference Submissions}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{\textbf{Yen-Ju Lu$^{1\diamond}$, Yashesh Gaur$^{2}$, Wei Zhou$^{2}$, Benjamin Muller$^{2}$, Jesus Villalba$^1$, Najim Dehak$^1$,} \\ 
\textbf{Luke Zettlemoyer$^{2}$, Gargi Ghosh$^{2}$, Mike Lewis$^{2}$, Srinivasan Iyer$^{2\dagger}$, Duc Le$^{2\dagger}$} \\ 
\\
$^1$Center for Language and Speech Processing, Johns Hopkins University \\
$^2$Meta Superintelligence Labs \\
$^\dagger$Joint last author \\
% , $^\diamond$Work done at Meta \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\modelfull}{{\textsc Latent Speech-Text Transformer}}
\newcommand{\model}{{\textsc LST}}
\newcommand{\modelbf}{\textbf{LST}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle
\renewcommand{\thefootnote}{}
\footnotetext{$^\diamond$Work done during internship at Meta.}
\renewcommand{\thefootnote}{\arabic{footnote}}


\begin{abstract}
Auto-regressive speech-text models are typically pre-trained on a large number of interleaved sequences of text tokens and raw speech encoded as speech tokens using vector quantization. These models have demonstrated state-of-the-art performance in speech-to-speech understanding and generation benchmarks, together with promising scaling laws, primarily enabled by the representational alignment between text and speech. Nevertheless, they suffer from shortcomings, partly owing to the disproportionately longer sequences of speech tokens in contrast to textual tokens. This results in a large compute imbalance between modalities during pre-training as well as during inference, and a potential hindrance to effectively aligning speech and text, ultimately translating to several orders of magnitude slower scaling laws. We introduce the \modelfull{} (\model{}), which makes pre-training speech-text models more data-efficient by dynamically and inexpensively aggregating speech tokens into latent speech patches. These patches serve as higher-level units that can either align with corresponding textual units to aid capability transfer or even encapsulate common speech sequences like silences to be more compute-efficient. We show that \model{} outperforms vanilla approaches on speech-to-speech as well as text-to-text benchmarks in both data- and compute-controlled settings, the former indicating more effective representational alignment and the latter indicating steeper scaling laws for speech-text models. On HellaSwag story completion, \model{} achieves 6.5\% absolute gain in speech accuracy under compute-controlled training and 5.3\% under data-controlled training, while also improving text performance. We will release our models, code, and the evaluation data to facilitate further research.
\end{abstract}

\section{Introduction}

Inspired by the strong zero- and few-shot understanding and generation capabilities of large auto-regressive textual language models with billions of parameters that are pre-trained on trillions of tokens, \cite{lakhotia2021generative} introduce the task of Generative Spoken Language Modeling (GSLM) a.k.a Textless NLP, where raw speech is encoded as a sequence of discrete tokens based on a dictionary of quantized speech features, and an auto-regressive language model (LM) is trained on these tokens with Next Token Prediction (NTP). While initially successful, \cite{cuervo2024scaling} estimate that this approach would require up to three orders of magnitude more data to obtain equivalent capabilities as textual LLMs, largely owing to the same information requiring a significantly larger number of speech tokens to represent compared to text. This increased sequence length also means that these models utilize considerably more compute during inference to process the same amount of semantic content compared to text.

To improve scaling properties of large speech models by taking advantage of the comparatively larger corpus of web text compared to speech, recent efforts have leveraged transfer learning from textual modalities in the form of warm initialization from large pre-trained text models \citep{hassid2023textually}, pre-training with interleaved speech-text data \citep{nguyen2025spirit}, and modeling speech and text in multiple streams to leverage the textual chain of thought or ``inner monologue" \citep{defossez2024moshi}. All these works attempted to some extent to achieve \emph{representational alignment} between text and speech, where a perfect alignment means the model can treat the two modalities interchangeably without any performance difference. Despite this, there remains a large gap between text-to-text and speech-to-speech performance on the same benchmarks, highlighting the incompleteness of the alignment. We hypothesize that the severe mismatch in information densities between the speech and text tokens is one of the primary factors hindering speech-text alignment.

To overcome the aforementioned challenges, we introduce the \modelfull{} (\model{}) based on the byte-latent transformer (BLT) architecture \citep{pagnoni2024byte}, comprising an encoder that dynamically groups sequences of speech tokens into higher-level speech patches, a global speech-transformer that auto-regressively models interleaved sequences of textual tokens and speech patches, and a light-weight transformer decoder \citep{vaswani2017attention} that maps patches back into speech tokens of dynamic sizes. Working in terms of speech patches allows the model to encode more content given the same training cost, makes inference more efficient.
% , and yields considerable compute-savings which steepens the scaling curves for speech-text models. 
These speech patches can represent higher-level speech concepts or prolonged silences, and serve to level the information density between speech and text, thus making them easier to align (see Figure \ref{fig:training-ratio}). 


\begin{figure}[t]
\centering
\begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{iclr2026/figs/HellaSwag_accuracy_fixed_iters.pdf}
    \caption{\emph{Compute-controlled}: fixed training iterations}
    \label{fig:token-ratio}
\end{subfigure}\hfill
\begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{iclr2026/figs/HellaSwag_accuracy_fixed_tokens.pdf}
    \caption{\emph{Data-controlled}: fixed data budget}
    \label{fig:weight-ratio}
\end{subfigure}
\caption{Comparison of LST and Baseline on HellaSwag story completion under two experimental setups, (a) \emph{compute-controlled}: same number of training iterations and (b) \emph{data-controlled}: same amount of training data.}
\label{fig:training-ratio}
\vspace{-2mm}
\end{figure}

% \{ TODO for Yen-ju: Insert results figure here \}


We first demonstrate that \model{} models with fixed-size speech patching schemes similar to what \cite{yu2023megabyte} did with text, are able to significantly outperform their non-patching counterparts. Such models are aware of the internals of patches without expending much compute in the process, in contrast with methods that expand the speech token vocabulary by applying subword tokenization, which yield poor downstream performance \citep{cuervo2024scaling}. We further improve the performance by introducing speech-patching based on textual alignment at the word/subword levels, which crucially also includes patching large sequences of silences. Since this approach requires text-speech alignment timestamps during training and inference, we also introduce a curriculum-based method to eliminate the need for such alignments during inference.

To summarize, this paper makes the following contributions:

(1) We show improved performance of \model{} models in both data- and compute-controlled settings compared to vanilla interleaved speech-text models like SpiritLM \citep{nguyen2025spirit}, as well as models that use subword tokenization, on speech versions of popular text understanding benchmarks such as HellaSwag \citep{zellers2019hellaswag}. \model{}-based models save considerable training and inference compute and improve speech-text representational alignment (see Figure \ref{fig:training-ratio}).

% TODO(Duc): rewrite this bullet
(2) We introduce different variations of speech patching schemes, including fixed-size static patching and alignment-based patching, and analyze their effectiveness.

(3) We demonstrate that \model{} continues outperforming the baseline when scaling up the model size from 1B to 7B parameters, which highlights the scalability of our method.

\section{Background} 
\label{sec:background}
Generalized spoken language models \citep{lakhotia2021generative} typically comprise three components: (1) a speech tokenizer model that maps a raw speech waverform $s$ to a sequence of speech tokens $\{s_0, \dots, s_{n} \}$, (2) a decoder-only transformer model \citep{vaswani2017attention} with parameters $\theta$ that models the distribution of the next speech token given the previous context i.e. $p_\theta(s_{i}|s_{<i})$, and (3) a vocoder model that maps speech token sequences back to a speech waveform, such as HiFi-GAN \citep{kong2020hifi}.

\textbf{Speech tokenization} Approaches for speech tokenization include semantic tokens represented by cluster-ids obtained by k-means clustering of frame representations as in Hubert \citep{hsu2021hubert}, acoustic tokens obtained as discretized embeddings from residual-vector quantization bottlenecks from self-supervised neural codec models \citep{zeghidour2021soundstream,defossez2024moshi}, as well as additional tokens for expressivity and also, combinations of different token categories. In this paper, we follow \cite{hassid2023textually,nguyen2025spirit} and use Hubert tokens using a codebook of 501 speech tokens at 25Hz. Unlike \cite{nguyen2025spirit} we do not need to deduplicate Hubert tokens as this is organically handled by the \model{} architecture.

\textbf{Sequence Modeling} Similar to LLMs for text, speech token modeling is typically done using a large transformer decoder model using causal self-attention, to maximize the likelihood of sequences from a large speech pre-training corpus ($\mathcal{D}$) in an auto-regressive fashion:
\begin{align}
\mathcal{L(\mathcal{D}; \theta)} = \sum_{s \in \mathcal{D}} \sum_i \log p_\theta(s_i|s_{<i})
\end{align}
\textbf{Interleaved Data} Since speech sequences are longer and less compact that their corresponding text sequences, such models can require several orders of magnitude more data in order to achieve performance comparable to text models \citep{cuervo2024scaling}. In order to bridge the gap, \cite{nguyen2025spirit} find that training on interleaved sequences of text and speech data directly correlates with improved performance. 
% 
For a subset of the pre-training dataset that contains the textual sequence $\{t_0, \dots, t_m\}$, where text tokens are obtained using a tokenizer (we use the Llama 2 tokenizer \citep{touvron2023llama} in this paper) and each text token can correspond to a span of speech tokens, the model is trained on an interleaved sequence obtained by replacing arbitrary spans of speech tokens in the sequence sequence with text tokens separated by special modality tokens. This allows the same model to be used for S$\rightarrow$S, S$\rightarrow$T, T$\rightarrow$S and T$\rightarrow$T tasks. We discuss the process of producing interleaved data from parallel text-speech data in Appendix~\ref{app:interleaved}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{iclr2026/figs/Latent_Speech-Text_Transformer_v4.pdf}
    \caption{\textbf{Latent Speech-Text Transformer (LST).} 
    % The model takes as input both BPE text tokens and HuBERT speech tokens. 
    The model encodes BPE text tokens and HuBERT speech tokens into a shared latent space. 
    A \emph{Patch Encoder} compresses local speech segments into patch representations, which are jointly processed with text tokens. % to form a shared latent space. 
    A \emph{Patch Decoder} predicts future speech tokens from latent representations, enabling alignment and transfer across modalities.}
    \label{fig:lst-arch}
    \vspace{-2mm}
\end{figure}


 
\section{Latent Speech-Text Transformers} 

The core idea of the \model{} architecture is to auto-regressively model latent patches of tokens (using a global transformer), rather than individual tokens, similar in spirit to BLT \citep{pagnoni2024byte} which models dynamic-sized patches of bytes. The transformation of speech/text spans to patches and vice-versa takes place with the help of a light-weight local encoder and local decoder, and the entire model is trained end-to-end using the same token-level likelihood as before. Figure \ref{fig:lst-arch} illustrates this architecture specialized to the task of speech-text modeling. The majority of the compute expended in terms of FLOPs is in the global transformer, which yields savings by operating on information-dense speech patches instead of granular speech tokens.

\textbf{Local Encoder.} Similar to BLT, the local encoder uses a series of sliding window self-attention and cross-attention layers to aggregate token representations into patch representations. In \model{}, we only patch spans of speech tokens using strategies described in Section \ref{sec:patching}. Note that a simple alternative to patching is to use subword tokenization methods like Byte Pair Encoding (BPE) on the speech tokens. This was also explored by \cite{cuervo2024scaling} and similar to them, failed to improve performance in our experiments (ablations in Section~\ref{sec:exp_main}). Unlike BLT, we do not use hash embeddings, as they did not provide improvements in our experiments.

\textbf{Local Decoder.} A light-weight transformer is used as a decoder and trained with NTP loss, with cross-attention layers inserted between every transformer layer. Each token attends to both the previously generated speech patches and text tokens to incorporate patch-level information (using cross-attention) as well as a sliding window of the past 512 tokens (using self-attention).



\subsection{Patching}
\label{sec:patching}
Let $\mathbf{X}=[x_0,\dots,x_T]\in\mathbb{R}^{T\times d}$ be speech token embeddings obtained using a learned embedding matrix applied to speech tokens $\{s_0, \dots, s_T \}$ .
% TODO(Duc): is the notation for $S_i$ correct?
The process of patching maps $\mathbf{X}$ to a shorter sequence of patch embeddings
$\mathbf{Z}=[z_0,\dots,z_{T'}]\in\mathbb{R}^{T'\times d}$ by aggregating local frame segments. For a frame-index set $\mathcal{P}_i\subseteq\{0,\dots,T\}$, a patch embedding is formed via the local encoder:
%  with cross-attention
\[
z_i = \mathrm{LocalEnc}\!\left(X_{\mathcal{P}_i}\right),
\]

% where  $\mathrm{LocalEnc}(\cdot)$ is a lightweight transformer block that 
integrating the frames indexed by $\mathcal{P}_i$ into a single patch embedding. Different patching strategies correspond to different segmentation $\{\mathcal{P}_i\}$.
% choices of the 

\begin{figure}[t]
\centering
\begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{iclr2026/figs/Patching.pdf}
    \caption{Static patching segments speech into fixed-size patches, while aligned patching uses Wav2Vec2+CTC boundaries. (\texttt{sw} = spoken word and \texttt{sil} = silence.)}
    \label{fig:patching-basic}
\end{subfigure}
\hfill
\begin{subfigure}{0.45\linewidth}
    \centering
    \includegraphics[width=\linewidth]{iclr2026/figs/Alignment.pdf}
    \caption{Example of alignment from Wav2Vec2 + CTC, where purple markers indicate word boundaries, aligning the audio signal with corresponding text.}
    \label{fig:alignment}
\end{subfigure}
\caption{Illustrations of alignment and patching methods.}
\label{fig:alignment-patching}
\vspace{-2mm}
\end{figure}


\textbf{Static Patching.}
Speech sequence is split into non-overlapping segments of a fixed length $p$ (patch size). Each patch token is obtained by the local encoder from the embeddings in the patch:
\[
\mathcal{P}_i = \{ip,\dots,\min((i+1)p -1,T)\}.
\]

For $p=3$ and input embeddings $\mathbf{X}=[x_0, x_1, x_2, x_3, x_4, x_5, x_6, \dots]$, the first patch is $\{x_0, x_1, x_2\}$, the second $\{x_3, x_4, x_5\}$, and so on. Each segment is encoded into a single patch embedding $z_i$ by the local encoder. 
% 
This provides a uniform compression ratio independent of alignment information.

\textbf{Alignment Patching.}
To better synchronize speech and text at the semantic level, alignment patching leverages forced alignment timestamps between speech frames and textual units (e.g. words or BPE tokens). Let $\mathcal{A} = \{(b_k, e_k)\}_{k=1}^K$ denotes the aligned frame ranges, where $[b_k, e_k]$ spans the $k$-th textual unit.
The corresponding patch is
\[
\mathcal{P}_k = \{b_k,\dots,e_k\}.
\]
Frames outside text spans (e.g., silence) are grouped into separate patches (Fig.~\ref{fig:patching-basic}).
For instance, if one word aligns to $[2,4]$ and the next to $[6,7]$, patches are $\{x_2, x_3, x_4\}$ and $\{x_6, x_7\}$, 
with silence forming $\{x_0, x_1\}$ and $\{x_5\}$. 
% Each text unit thus corresponds to a speech patch, and silent segments are also grouped, 
% ensuring temporal correspondence across modalities.

We obtain alignments with Wav2Vec2+CTC \citep{baevski2020wav2vec}, yielding one patch per text unit and silence segment (Fig.~\ref{fig:alignment}). While this enforces cross-modal correspondence, it requires an auxiliary model at inference, introducing possible errors. \emph{Curriculum patching} (Sec.~\ref{sec:curr_patching}) mitigates this by gradually shifting from aligned to static patching during training.


\begin{figure}[t]
    \centering
    \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\linewidth]{iclr2026/figs/Pronouns.png}
        \caption{Personal Pronouns}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\linewidth]{iclr2026/figs/Sharp_Tools.png}
        \caption{Sharp Tool Terms}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\linewidth]{iclr2026/figs/Water_Movement.png}
        \caption{Water-related Terms} % (Water/Movement)}
    \end{subfigure}

\caption{Visualization of word-level speech patch embeddings from alignment patching models on HellaSwag speech, grouped by different linguistic categories. Clusters of the same word are tight and well-separated from others.}

\label{fig:word_embeddings}
\end{figure}

\textbf{Curriculum Patching.}
\label{sec:curr_patching}
Curriculum patching interpolates between alignment-based and static patching during training. Let $P(u)\in[0,1]$ denote the probability of using alignment at training step $u$: 
\[
P(u) =
\begin{cases}
1, & u < \tau_1, \\
1-\frac{u-\tau_1}{\tau_2-\tau_1}, & \tau_1 \leq u < \tau_2, \\
0, & u \geq \tau_2.
\end{cases}
\]
At step $u$, we choose alignment patches with probability $P(u)$ and static patches otherwise.  
This retains alignment benefits during early training while enabling simple static-only inference.
% This allows the model to both benefit from alignment-based training while retaining the simpler inference of static patching.



\section{Experimental Setup}
We next describe the datasets, models, and evaluation protocols used in our experiments.
% \subsection{Datasets}
% 
\subsection{Training and Evaluation Datasets}
Our pre-training data comprises a mixture of text and interleaved speech datasets. 


\textbf{Text.} Our text training data consists of extensive web and academic corpora, sourced from a selected portion of the Llama 2 pre-training collection \citep{touvron2023llama}, totaling 1.8T tokens. We follow the LLaMA~2 setup and apply its SentencePiece~\citep{kudo2018sentencepiece} BPE tokenizer with a 32K vocabulary.

\textbf{Speech.} 
Our speech training data includes speech which is discretized into HuBERT tokens (501-entry codebook at 25Hz) together with paired text transcriptions. We use LibriLight (60k hours), People's Speech (30k hours), Multilingual LibriSpeech (50k hours), and Spotify (60k hours), detailed in Table~\ref{tab:speech-datasets}. All corpora are aligned using the Wav2Vec2 + CTC framework to provide token-level correspondence between speech and text (Figure~\ref{fig:alignment}). 
% We illustrate the alignment process in Figure~\ref{fig:alignment}, which shows token-level boundaries produced by the aligner.

% VoxPopuli (100k and 300k hours)  \citep{wang2021voxpopuli},


\begin{table}[h]
\centering
\caption{Speech training datasets with total speech hours and the amount of Hubert tokens.}
\begin{tabular}{lcc}
\toprule
\textbf{Dataset}  & \textbf{Hours} & \textbf{Hubert Tokens (B)} \\
\midrule
LibriLight \citep{kahn2020libri}    & 44,174  & 3.7 \\
People Speech \citep{galvez2021people}   & 14,699  & 1.2 \\
% VP100k  & Speech      & 96,263  & 8.1 \\
% VP300k  & Speech      & 271,462 & 22.8 \\
Multilingual LibriSpeech  \citep{pratap2020mls}   & 50,601  & 4.2 \\
Spotify  \citep{clifton2020spotify}  & 55,309  & 4.6 \\
\bottomrule
\end{tabular}
\label{tab:speech-datasets}
\end{table}



% \begin{figure}[h]
% \centering
% \includegraphics[width=0.8\linewidth]{iclr2026/figs/Patching.pdf}
% \caption{Static patching segments speech into fixed-size patches, while aligned patching uses Wav2Vec2+CTC boundaries. Here, \texttt{sw} = spoken word and \texttt{sil} = silence.}

% \label{fig:patching-basic}
% \end{figure}


% \begin{figure}[h]
% \centering
% \includegraphics[width=0.9\linewidth]{iclr2026/figs/Alignment.pdf}
% \caption{Example of speech-text alignment with Wav2Vec2 + CTC. Token boundaries are identified to provide precise correspondence between the audio signal and transcriptions.}
% \label{fig:alignment}
% \end{figure}



% \begin{table}[h]
% \centering
% \caption{Speech training datasets. We report total hours of speech and equivalent Llama 2 BPE tokens.}
% \begin{tabular}{lccc}
% \toprule
% \textbf{Dataset Name} & \textbf{Modality} & \textbf{\# Speech Hours} & \textbf{\# B Tokens (B)} \\
% \midrule
% LL60k & Interleaved & 55,309 & 4.6 \\
% PPL30k & Interleaved & 16,404 & 1.4 \\
% VP100k & Speech & 96,263 & 8.1 \\
% VP300k & Speech & 271,462 & 22.8 \\
% MLS & Interleaved & 50,601 & 4.2 \\
% SP60k & Interleaved & 55,309 & 4.6 \\
% \bottomrule
% \end{tabular}
% \end{table}



% We evaluate the model on three multiple-choice benchmarks, where each dataset provides a narrative context and a set of candidate endings, and the model selects the most plausible continuation. 
% Together, they test narrative understanding, commonsense reasoning, and topic coherence across three formats: 1-in-4 (HellaSwag) and 1-in-2 (StoryCloze, Topic StoryCloze). 
% We evaluate in both speech-to-speech (S$\rightarrow$S) and text-to-text (T$\rightarrow$T) modes, using Kokoro TTS~\citep{hexgrad2025kokoro} to generate all spoken data for evaluation.



% \textbf{HellaSwag-Speech (HS).} 
% We create a speech version of HellaSwag~\citep{zellers2019hellaswag}, which is a 1-in-4 multiple-choice task designed to probe everyday commonsense reasoning.
% Speech prompts and candidate responses are generated independently and concatenated afterwards, ensuring that all responses are compared against the same prompt.

% \textbf{StoryCloze and Topic StoryCloze (SC/TSC).}
% SC~\citep{mostafazadeh2016corpus} and its topic-based extension TSC~\citep{hassid2023textually} are both 1-in-2 multiple-choice benchmarks: SC focuses on narrative coherence, while TSC emphasizes topic-sensitive consistency. They are widely used in prior multimodal work (e.g., \citealt{nguyen2025spirit}). We resynthesize both with Kokoro TTS to provide higher-quality speech inputs.

We evaluate the model on three benchmarks, where each dataset provides a narrative context and candidate endings, and the model selects the most plausible continuation. Together, they test narrative understanding, commonsense reasoning, and topic coherence. We evaluate the model in both speech-to-speech (S$\rightarrow$S) and text-to-text (T$\rightarrow$T) modes. For the speech mode, we apply Kokoro TTS model~\citep{hexgrad2025kokoro} to generate the speech for evaluation. 

\textbf{sHellaSWAG (HS).} We create a speech version of HellaSwag~\citep{zellers2019hellaswag} with Kokoro TTS. This benchmark evaluates everyday commonsense reasoning with spoken inputs and outputs. To ensure fairness, we generate the speech for prompts and responses independently and concatenate them afterwards, so that all responses are evaluated against the same speech prompt. 

\textbf{StoryCloze and Topic StoryCloze (SC/TSC).} SC~\citep{mostafazadeh2016corpus} and its topic-based extension TSC~\citep{hassid2023textually} are widely used in prior multimodal work (e.g., \citealt{nguyen2025spirit}) to test coherence and topic-sensitive reasoning. We resynthesize both datasets with Kokoro TTS for higher-quality speech inputs.

\begin{table}[h]
\centering
\caption{Evaluation datasets for story completion (MC = Multiple Choice).}
\begin{tabular}{lccc}
\toprule
\textbf{Dataset}  & \textbf{Format} & \textbf{Focus} \\
\midrule
HellaSwag \citep{zellers2019hellaswag}       & 1-in-4 MC & Commonsense reasoning \\
StoryCloze \citep{mostafazadeh2016corpus}      &  1-in-2 MC & Narrative coherence \\
TopicStoryCloze \citep{hassid2023textually} &   1-in-2 MC & Topic consistency \\
\bottomrule
\end{tabular}
\end{table}


We omit sWUGGY and sBLiMP~\citep{nguyen2020zero}, as they target lexical and syntactic judgments on very short speech segments. Such settings are less aligned with our focus on narrative reasoning, where story-level coherence and commonsense understanding are required.




\subsection{LST Models and Baselines}
% \paragraph{LST Models}
\textbf{LST Models.} We explore four patching strategies for speech tokens:
\begin{itemize}
\item \textbf{Static Patching.} Fixed-length patches (4 HuBERT tokens) as in \cite{yu2023megabyte}, independent of alignment and consistent across training/inference.

\item \textbf{Aligned Patching.} Uses Wav2Vec2+CTC boundaries (Fig.~\ref{fig:alignment}). For each text span \([b_k,e_k]\), we form patch set \(\mathcal{P}_k=\{b_k,\dots,e_k\}\), synchronizing speech and text tokens (Fig.~\ref{fig:patching-basic}).

\item \textbf{Mixed Patching.} Randomly applies static or aligned patching per sequence, combining the robustness of static patching with the fine-grained sync of aligned.

\item \textbf{Curriculum Patching.} Training shifts from aligned (first third) to mixed (middle) to static (final), leveraging early alignment while ensuring robustness to static-only inference.

\end{itemize}


\textbf{Baselines.}
We include two speechLLM systems as baselines:

\begin{itemize}
\item \textbf{Base SpeechLLM.} Processes speech tokens directly with text tokens, without patching, similar to SpiritLM \citep{nguyen2025spirit}.

\item \textbf{BPE SpeechLLM.} Maps speech tokens into 1k BPE units using a SentencePiece tokenizer \citep{kudo2018sentencepiece} trained on 100k random speech sequences, replacing speech tokens with BPE-derived units\footnote{We use the 1k configuration as our BPE baseline, as larger vocabularies (5k, 10k) showed no benefit.}. 

\end{itemize}



\subsection{Training Settings}
To balance modalities, we set speech tokens to account for about one third (33\%) of the total training data, while the rest (67\%) is text-only.  This ensures that the model benefits from large-scale text pretraining while still maintaining substantial exposure to speech for effective multimodal alignment.
% 
For comparison, SpiritLM \citep{nguyen2025spirit} adopts a different composition: 33\% pure speech, 33\% interleaved, and 33\% text tokens. Since SpiritLM starts from a text-pretrained model, the relatively smaller text fraction is sufficient. In contrast, when training from scratch, we find that using 33\% interleaved and 66\% text tokens yields better performance (see Appendix~\ref{sec:speech_proportion}).





\section{Results}
% \subsection{Main Comparison with Baselines}
\label{sec:exp_main}

% \begin{itemize}
%     \item \textbf{Compute-controlled.} We fix the number of training iterations and match the per-step sequence budget so that the total number of processed units is identical across methods (baseline tokens $=$ LST patches). This setting highlights that LST achieves stronger performance under the same compute cost.
%     \item \textbf{Data-controlled.} We fix the data budget with the same amounts of speech and text tokens. Since LST compresses sequences into patches, it processes fewer patch tokens than the baselines. Under this setting, LST is not only more efficient but also achieves higher accuracy, demonstrating that patching improves speech--text alignment and reduces the performance gap between modalities.
% \end{itemize}


\begin{table}[t]
\centering
\caption{Main comparison of LST models and baselines under the \textbf{same computation budget} scheme. Each dataset reports both S$\rightarrow$S and T$\rightarrow$T.}
\label{tab:main-ratio}
\begin{tabular}{lcc|cc|cc|cc}
\toprule
\textbf{Model} & \multicolumn{2}{c|}{\textbf{Tokens (B)}} & \multicolumn{2}{c|}{HellaSwag} & \multicolumn{2}{c|}{StoryCloze} & \multicolumn{2}{c}{TopicStoryCloze} \\
&  Int. & Text & S$\rightarrow$S & T$\rightarrow$T & S$\rightarrow$S & T$\rightarrow$T & S$\rightarrow$S & T$\rightarrow$T  \\
\midrule
Base SpeechLLM & 87 & 175 & 39.0 & 47.0 & 59.1 & 67.8 & 87.5 & 95.7 \\
BPE SpeechLLM  & 95 & 190 & 38.0 & 47.5 & 58.0 & 66.4 & 87.0 & 93.5 \\
\midrule
LST (Static)   & 108 & 217 & \underline{44.3} & 51.1 & 60.5 & 70.3 & 87.7 & \textbf{96.2} \\
LST (Aligned)  & 108 & 217& 42.7 & 51.7 & 60.4 & 70.4 & 86.6 & 95.7 \\
LST (Mixed)    & 108 & 217& \underline{44.3} & \underline{51.9} & \textbf{61.4} & \underline{70.8} & \textbf{88.0} & 95.9 \\
LST (Curriculum) & 108 & 217 & \textbf{45.5} & \textbf{52.2} & \underline{61.2} & \textbf{71.6} & \underline{87.9} & \underline{96.1} \\
\bottomrule
\end{tabular}
\vspace{-2mm}
\end{table}

\textbf{Compute-controlled.}  
We fix the number of training iterations and per-step sequence budget so that all methods process the same number of units (baseline tokens $=$ LST patches).   
Table~\ref{tab:main-ratio} shows three trends on HellaSwag.  
% Across all three datasets, LST variants consistently outperform the baselines, with Curriculum Patching achieving the best average performance.  
% On HellaSwag, three key observations emerge.
First, patching increases the effective token budget, benefiting both modalities: Curriculum Patching improves T$\rightarrow$T by +5.2 (47.0→52.2) and S$\rightarrow$S by +6.5 (39.0→45.5).  
Second, Aligned Patching is less effective at evaluation, since variable word spans often yield longer patches, reducing the test-time compute.  
Finally, Mixed and Curriculum patching combine the advantages of shorter evaluation patches with alignment information, consistently outperforming Static and Aligned across datasets.

\textbf{Data-controlled.}
Here we fix the data budget with the same amounts of speech and text tokens. Since LST compresses sequences into patches, it processes fewer patch tokens than the baselines, leading to higher efficiency.  
Table~\ref{tab:main-weight} shows that the BPE baseline fails to surpass vanilla SpeechLLM, whereas LST continues to achieve consistent gains. On HellaSwag, Curriculum Patching improves T$\rightarrow$T accuracy from 49.6 to 52.2 despite reduced computation, while boosting S$\rightarrow$S from 40.2 to 45.5.
Similar improvements are observed on StoryCloze and TopicStoryCloze.
Overall, LST with Curriculum Patching reduces the speech–text performance gap from 9.4 to 6.7, demonstrating that alignment through patching benefits both modalities while offering meaningful compute savings.

% Different application scenarios may require different balances between speech and text:
% speech-oriented tasks benefit from higher speech ratios, while text-heavy tasks favor lower ratios.
% Rather than targeting one universal setting, this experiment highlights the robustness of our approach across ratio choices.
% 
% Across all tested ratios, LST consistently outperforms the baseline SpeechLLM in both speech and text performance.
% This indicates that patching strategies remain effective regardless of the speech-to-text distribution,
% allowing practitioners to adjust ratios according to task requirements without losing the benefits of our method.
% Notably, with a 50\% speech ratio, LST improves HellaSwag (S$\rightarrow$S) from 35.5 to 40.2 and StoryCloze (S$\rightarrow$S) from 57.5 to 59.2, while at a lower 10--20\% ratio, it still boosts text-side metrics such as TSC (T$\rightarrow$T) from 95.2--96.6 up to 95.8.
% These results illustrate that LST not only achieves the best performance at balanced ratios but also maintains strong gains under speech-scarce conditions.

% \begin{figure}[h]
% \centering
% \includegraphics[width=0.7\linewidth]{iclr2026/figs/HellaSwag_accuracy_ratio.pdf}
% \caption{Effect of speech-to-text ratio in training across different speech to text token ratio at 200k iterations. 
% Results are reported on HellaSwag under both S$\rightarrow$S and T$\rightarrow$T.}
% \label{fig:speech-ratio}
% \end{figure}


% \begin{figure}[h]
% \centering
% \includegraphics[width=0.7\linewidth]{iclr2026/figs/HellaSwag_accuracy_fixed_iters_7B.pdf}
% \caption{Comparison of LST and baseline on 7B model. Results are reported on HellaSwag under both S$\rightarrow$S and T$\rightarrow$T.}
% \label{fig:speech-ratio}
% \end{figure}



\begin{table}[t]
\centering
\caption{Main comparison of LST models and baselines under the \textbf{same speech/text tokens} scheme. Each dataset reports both S$\rightarrow$S and T$\rightarrow$T.}
\label{tab:main-weight}
\begin{tabular}{lccc|cc|cc}
\toprule
\textbf{Model} & \textbf{Compute Savings} & \multicolumn{2}{c|}{HellaSwag} & \multicolumn{2}{c|}{StoryCloze} & \multicolumn{2}{c}{TopicStoryCloze} \\
 & (\%) & S$\rightarrow$S & T$\rightarrow$T & S$\rightarrow$S & T$\rightarrow$T & S$\rightarrow$S & T$\rightarrow$T  \\
\midrule
% Base SpeechLLM    & 35.8 & 48.0 & 57.3 & 69.5 & 86.0 & 95.9 \\
 Base SpeechLLM & - & 40.2 & 49.6 &	60.2 &	69.1 &	87.5 &	95.2 \\
 BPE SpeechLLM  & 8.2\%  & 39.4 & 48.4 & 58.3 & 66.3 & 86.5 & 93.9 \\
\midrule
LST (Static)  &   19.3\%  & {44.3} & 51.1 & 60.5 & 70.3 & 87.7 & \textbf{96.2} \\
% LST (Aligned)     & 41.4 &  49.5 &	60.4 &	69.2 &	87.2 &	\textbf{96.4} \\
% LST (Mixed)       & \textbf{43.2} &  \textbf{49.8} &	60.1 &	\textbf{69.4} &	86.9 &	95.6  \\
LST (Curriculum) & 19.7\% &  \textbf{45.5} & \textbf{52.2} & \textbf{61.2} & \textbf{71.6} & \textbf{87.9} & {96.1} \\

\bottomrule
\end{tabular}
\vspace{-2mm}
\end{table}


\textbf{Visualization of Word-Level Speech Patch Embeddings}
We use t-SNE~\citep{van2008visualizing} to project embeddings of representative word groups from the aligned-patching LST model (Fig.~\ref{fig:word_embeddings}).
Across different categories, embeddings of the same word consistently form tight clusters, while different words remain well separated.
Each word forms its own cluster (e.g., he, she, they in pronouns; knife, scissors, sharpener in tools; boat, canoe, surfing in water-related terms). Related variants such as sail–sailing show stability under inflection, while semantically similar pairs like scissors–shears also appear nearby despite being distinct words.
These qualitative patterns match quantitative results: within-word similarity is high ($\sim$0.87), between-word similarity is much lower ($\sim$0.43), and silhouette scores (0.65–0.68)~\citep{rousseeuw1987silhouettes} confirm well-separated clusters.


\textbf{Scaling Trends.}  
Table~\ref{tab:scaling} summarizes results at both 1B and 7B scales, while Figure~\ref{fig:speech-7B} provides the training curve of the 7B model on HellaSwag.
Scaling consistently improves performance across all datasets. 
At 1B, LST already outperforms the baseline (e.g., 41.3 vs.\ 36.8 on S$\rightarrow$S, and 49.2 vs.\ 47.1 on T$\rightarrow$T). 
At 7B, the improvements persist: LST reaches 44.2/55.3 compared to the baseline’s 42.0/54.8. 
The figure further shows that LST exhibits a steeper growth curve over iterations, indicating more efficient utilization of larger capacity. Importantly, the 7B model remains far from convergence under the same processed token budget but with much fewer iterations, suggesting that extended training would likely amplify the advantage of LST and further widen the gap.
% The figure further shows that LST exhibits a steeper growth curve over iterations, indicating more efficient utilization of larger capacity. 
% Notably, under the same number of processed tokens with much fewer steps, the 7B model remains further from convergence than the 1B model, suggesting that additional training would likely widen the performance gap.



\begin{figure}[t]
\centering
\begin{minipage}{0.515\linewidth}
    \centering
    \captionof{table}{Scaling trends of baseline SpeechLLM and LST models at 1B and 7B parameter scales. Each dataset reports both S$\rightarrow$S and T$\rightarrow$T.}
    \label{tab:scaling}
    \vspace{2mm}
    \begin{tabular}{lcccc}
    \toprule
    \textbf{Model} & \textbf{Batch} & \textbf{Iters} & \multicolumn{2}{c}{HellaSwag} \\
     &  (M) & (k) & S$\rightarrow$S & T$\rightarrow$T \\
    \midrule
    Baseline (1B) & 0.5 & 200 & 36.8 & 47.1 \\
    LST (1B) & 0.5 & 200 & \textbf{41.3} & \textbf{49.2} \\
    \midrule
    Baseline (7B) & 4.0 & 25  & 42.0 & 54.8 \\
    LST (7B) & 4.0 & 25  & \textbf{44.2} & \textbf{55.3} \\
    \bottomrule
    \end{tabular}
    
\end{minipage}
    % \label{fig:word_embeddings}
% \end{minipage}
\hfill
\begin{minipage}{0.45\linewidth}
    \centering
    \includegraphics[width=\linewidth]{iclr2026/figs/HellaSwag_accuracy_fixed_iters_7B.pdf}
    \caption{Comparison of LST and baseline on 7B model at 25k iterations. Results are reported on HellaSwag S$\rightarrow$S and T$\rightarrow$T.}
    \label{fig:speech-7B}
\end{minipage}
\vspace{-3mm}
\end{figure}




\textbf{Ablation on Patching Strategies}
Table~\ref{tab:patching} compares static and aligned patching. 
Aligned patching uses word boundaries from alignment, producing semantically coherent patches.  
We consider two variants: Align (sil sep.), keeping silence spans as separate patches, and Align (sil merged), merging them with adjacent words. 
Both outperform static patching at similar patch sizes—for instance, Align (sil sep.) reaches 60.3 on StoryCloze S$\rightarrow$S vs.\ 58.7 for static size~6, and Align (sil merged) scores 38.5 on HellaSwag S$\rightarrow$S vs.\ 37.2 for static size~9.  
\emph{Curriculum} starts with \emph{Align (sil sep.)} and gradually shifts to \emph{Static} during training, retaining alignment benefits while matching the shorter-patch evaluation regime; it yields the strongest and most consistent results (e.g., 41.3 on HellaSwag S$\rightarrow$S).
Overall, aligned patching better preserves semantics than static, and curriculum combines alignment supervision with static-style evaluation for the best performance.
For completeness, we also report {BPE-aligned} patching experiments in Appendix~\ref{sec:bpe_align}.





    % \hfill


    % \begin{subfigure}{0.23\linewidth}
    %     \includegraphics[width=\linewidth]{iclr2026/figs/Water Movement.png}
    %     \caption{Phonetic \& semantic}
    % \end{subfigure}
    % \hfill


    % \begin{subfigure}{0.23\linewidth}
    %     \includegraphics[width=\linewidth]{iclr2026/figs/Function Words.png}
    %     \caption{Function words}
    % \end{subfigure}
    
    % \begin{subfigure}{0.23\linewidth}
    %     \includegraphics[width=\linewidth]{iclr2026/figs/Body Parts.png}
    %     \caption{Body parts}
    % \end{subfigure}
    % \hfill
    % \begin{subfigure}{0.23\linewidth}
    %     \includegraphics[width=\linewidth]{iclr2026/figs/SomethingElse.png}
    %     \caption{Category 7}
    % \end{subfigure}
    % \hfill
    % \begin{subfigure}{0.23\linewidth}
    %     \includegraphics[width=\linewidth]{iclr2026/figs/Another.png}
    %     \caption{Category 8}
    % \end{subfigure}

% \begin{figure}[t]
%     \centering
%     \begin{subfigure}{0.3\linewidth}
%         \includegraphics[width=\linewidth]{iclr2026/figs/Pronouns.png}
%         \caption{Pronouns}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.3\linewidth}
%         \includegraphics[width=\linewidth]{iclr2026/figs/Sharp Tools.png}
%         \caption{Nouns (Sharp Tools)}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.3\linewidth}
%         \includegraphics[width=\linewidth]{iclr2026/figs/Function Words.png}
%         \caption{Function words}
%     \end{subfigure}

%     \vspace{0.3cm}

%     \begin{subfigure}{0.3\linewidth}
%         \includegraphics[width=\linewidth]{iclr2026/figs/V vs Ving.png}
%         \caption{Verb vs. V-ing}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.3\linewidth}
%         \includegraphics[width=\linewidth]{iclr2026/figs/Water Movement.png}
%         \caption{Phonetic \& semantic} % (Water/Movement)}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.3\linewidth}
%         \includegraphics[width=\linewidth]{iclr2026/figs/Body Parts.png}
%         \caption{Body parts}
%     \end{subfigure}
% \caption{Visualization of word-level speech patch embeddings on HellaSwag across different linguistic categories. 
% (a) \textbf{Pronouns}: personal pronouns form distinct clusters. 
% (b) \textbf{Nouns (Sharp Tools)}: semantically related nouns are clearly separated. 
% (c) \textbf{Function words}: despite weak semantics, function words show coherent clustering. 
% (d) \textbf{Verb vs. V-ing}: most verb–ing pairs (e.g., \textit{walk–walking}) cluster closely, while context-sensitive verbs (e.g., \textit{run–running}) show larger centroid shifts. 
% (e) \textbf{Phonetic \& semantic (Water/Movement)}: phonetically and semantically related words (e.g., \textit{boat–floats}) are closer than unrelated words but remain distinguishable. 
% (f) \textbf{Body parts}: singular/plural and related forms (e.g., \textit{hand–hands}) exhibit tight intra-class grouping. 
% Quantitative metrics (e.g., within-word vs. between-word cosine similarity and silhouette scores) confirm these observations.}

% \label{fig:word_embeddings}
% \end{figure}


% 

% \begin{table}[h]
% \centering
% \caption{Ablation on patching strategies. Results are reported for HS, SC, and TSC under both S$\rightarrow$S and T$\rightarrow$T under 600k iterations. All the experiments are trained with interleaved and text data.}
% \label{tab:patching}
% \begin{tabular}{lcc|cc|cc}
% \toprule
% \textbf{Model} & \multicolumn{2}{c|}{HellaSwag} & \multicolumn{2}{c|}{StoryCloze} & \multicolumn{2}{c}{TopicStoryCloze} \\
%  & S$\rightarrow$S & T$\rightarrow$T & S$\rightarrow$S & T$\rightarrow$T & S$\rightarrow$S & T$\rightarrow$T  \\
% \midrule
% Static (patch=3)  & 41.4 & 49.2 & 59.6 & 66.9 & 86.6 & 95.5  \\
% Static (patch=4)  & 42.0 & 46.9 & 59.2 & 64.5 & 86.3 & 91.9  \\
% Static (patch=5)  & 41.2 & 47.1 & 58.3 & 66.5 & 85.9 & 95.0  \\
% \midrule
% Aligned           & 39.4 & 50.6 & 59.4 & 69.3 & 85.5 & 95.7  \\
% % Mixed       &  \\
% % Curriculum (patch=3)   &   \\
% % Curriculum (patch=4)   &   \\
% % Curriculum (patch=5)   &   \\
% Curriculum    & 41.7 & 50.3 & 58.7 & 68.7 & 86.4 & 96.2  \\
% \bottomrule
% \end{tabular}
% \end{table}

% \subsection{Knowledge Transfer Analysis}
% Table~\ref{tab:transfer} examines knowledge transfer under different training configurations: text-only, speech+text, and interleaved+text (aligned and curriculum). 
% Interleaved training consistently outperforms speech+text, confirming the benefit of alignment for cross-modal knowledge transfer.

% \begin{table}[h]
% \centering
% \caption{Knowledge transfer analysis: speech+text vs. interleaved+text. Each dataset reports both S$\rightarrow$S and T$\rightarrow$T.}
% \label{tab:transfer}
% \begin{tabular}{lcc|cc|cc}
% \toprule
% \textbf{Model} & \multicolumn{2}{c|}{HellaSwag} & \multicolumn{2}{c|}{StoryCloze} & \multicolumn{2}{c}{TopicStoryCloze} \\
%  & S$\rightarrow$S & T$\rightarrow$T & S$\rightarrow$S & T$\rightarrow$T & S$\rightarrow$S & T$\rightarrow$T  \\
% \midrule
% Text-only baseline        & -- & -- & -- & -- & -- & -- \\
% Speech + Text (Static)    & -- & -- & -- & -- & -- & -- \\
% Interleaved + Text (Align)& -- & -- & -- & -- & -- & -- \\
% Interleaved + Text (Curr.)& -- & -- & -- & -- & -- & -- \\
% \bottomrule
% \end{tabular}
% \end{table}



% \begin{tabular}{lcccc|cc|cc}
% \toprule
% \textbf{Model} & Batch Size & Iterations & \multicolumn{2}{c|}{HellaSwag} & \multicolumn{2}{c|}{StoryCloze} & \multicolumn{2}{c}{TopicStoryCloze} \\
%  & (M units) & (k) & S$\rightarrow$S & T$\rightarrow$T & S$\rightarrow$S & T$\rightarrow$T & S$\rightarrow$S & T$\rightarrow$T  \\
% \midrule
% % Baseline (1B) & 0.5M & 100k & 33.6 & 40.4 & 57.5 & 64.0 & 84.7 & 92.9  \\
% % LST (1B)      & 0.5M & 100k & 37.0 & 43.9 & 56.7 & 67.2 & 85.7 & 94.8 \\  % TO UPDATE
% Baseline (1B) & 0.5M & 200k & 36.8 & 44.7 & 58.0 & 63.5 & 85.6 & 90.9 \\
% LST (1B)      & 0.5M & 200k & \textbf{41.3}&	\textbf{49.2}&	\textbf{58.6} &	\textbf{67.8} &	\textbf{86.6}&	\textbf{95.4} \\ 
% \midrule
% % Baseline (7B) &  4M & 15k &  39.2 &	51.2 &	60.1 &	70.8 &	86.4 & 	96.3 \\
% % LST (7B)      & 4M & 15k &   40.7 &	52.7 &	59.7 &	70.8 &	85.8 & 	96.3 \\
% % Baseline (7B) &  4M & 20k &  41.3 &	54.1 &	60.5 &	72.5 &	87.1 & 	96.4 \\
% % LST (7B)      & 4M & 20k &   43.3 &	54.5 &	61.1 &	72.0 &	88.1 & 	96.0\\
% Baseline (7B) & 4M & 25k &  42.0 & 54.8 & 61.1& 71.8& 87.3 & \textbf{96.0} \\ 
% LST (7B)      & 4M & 25k & \textbf{44.2} & \textbf{55.3} & \textbf{61.4} & \textbf{72.3} & \textbf{88.3} & \textbf{96.0} \\
% % \hline
% % Baseline (1B) & 0.5M & 500k& 39.0 & 47.0 & 59.1 & 67.8 & 87.5 & 95.7 \\
% % LST (1B)      & 0.5M & 500k & \textbf{45.5} & \textbf{52.2} & \textbf{61.2} & \textbf{71.6} & \textbf{87.9} & \textbf{96.1} \\
% % Baseline (7B) & 43.9 & 52.8 & 60.7 & 60.9 & 87.7 & 87.3 \\ % TO UPDATE
% % LST (7B)      & \textbf{50.5} & \textbf{55.2} & \textbf{65.2} & \textbf{73.6} & \textbf{90.9} & \textbf{98.1} \\ % TO UPDATE 
% \bottomrule
% \end{tabular}



\section{Related Work}

\textbf{LLMs using speech tokens.} Early neural audio generation methods included direct auto-regressive generation of the speech waveform \citep{van2016wavenet}, or using adversarial approaches \citep{kong2020hifi}. Following this, \textit{textless NLP} work \citep{lakhotia2021generative} showed that by using discrete speech tokens obtained from self-supervised speech encoders (CPC, wav2vec 2.0, HuBERT) as targets for language modeling, can enable fully spoken LLMs.   AudioLM \citep{borsos2023audiolm} further uses hierarchical generation, first predicts semantic tokens, and subsequent stages predict fine-grained acoustic tokens from SoundStream \citep{zeghidour2021soundstream}, to achieve both high audio quality as well as long-term consistency. In addition to augmenting semantic speech tokens with pitch and style tokens to explicitly model expressivity, SpiritLM \citep{nguyen2025spirit} also introduced interleaving speech modeling with text-tokens. More recently, Moshi \citep{defossez2024moshi} propose a hierarchical \textit{inner monologue} method, that jointly predicts time-aligned text and acoustic tokens (with distilled semantic information), together with modeling multiple-stream audio for handling full-duplex audio dialogues. Finally, similar to scaling laws for text LLMs \citep{hoffmann2022training}, \cite{cuervo2024scaling} fit scaling law curves to predict the performance of spoken LLMs, and find that they scale upto three order of magnitude more slowly than text LLMs.



\textbf{Transferring textual knowledge into speech LMs.} Slower scaling trends, together with a disproportionately lower amount of data, lead to a knowledge and reasoning gap between speech and text LLMs. To bridge this, AudioPaLM and TWIST \citep{rubenstein2023audiopalm,hassid2023textually} initialize a spoken LLM from a strong text model (PaLM-2, LLaMA), improving both speech understanding/generation and cross-lingual transfer. SpiritLM demonstrates that interleaved speech–text training significantly improves inter-modality knowledge transfer. Spectron \citep{nachmanispoken} uses a “Chain-of-Modality” pipeline to first produce text and then speech conditioned on the text, trading latency for stronger textual control, while Moshi \citep{defossez2024moshi} uses a similar approach but generates interleaved text and speech as an inner monologue. To improve latency, LLaMA-Omni \citep{fang2024llama} style systems decode text and speech simultaneously, by upsampling textual LLM hidden states to decode speech units, before proceeding to decode the next text token.



\begin{table}[t]
\centering
\begin{threeparttable}
\caption{Comparison of patching strategies with approximately matched patch sizes.  Static uses fixed patch lengths. Align (sil sep.) treats silence as separate patches, Align (sil merged) merges silence into words, and Curriculum starts with Align (sil sep.) and gradually shifts to Static during training.}


% \caption{Ablation on patching strategies. Results are reported for HS, SC, and TSC under both S$\rightarrow$S and T$\rightarrow$T under 200k iterations. All the experiments are trained with interleaved and text data.}
\label{tab:patching}
\begin{tabular}{lccc|cc|cc}
\toprule
\textbf{Model} & \textbf{Ave Patch Size} & \multicolumn{2}{c|}{HellaSwag} & \multicolumn{2}{c|}{StoryCloze} & \multicolumn{2}{c}{TopicStoryCloze} \\
 & (tokens) & S$\rightarrow$S & T$\rightarrow$T & S$\rightarrow$S & T$\rightarrow$T & S$\rightarrow$S & T$\rightarrow$T  \\
\midrule
% LST (Static)  & 3 &   40.6&	48.2&	\textbf{60.1}&	68.3&	86.5&	95.5 \\
LST (Static)  & 4 &   40.5&	48.8&	58.2&	\textbf{69.4}&	86.2&	95.1 \\
LST (\textbf{Curriculum})  & 5.8\tnote{*} $\rightarrow$ 4 &   \textbf{41.3}&	\textbf{49.2}&	\textbf{58.6} &	67.8&	\textbf{86.6}&	\textbf{95.4} \\
\midrule
% LST (Static)  & 5 &   \textbf{39.9} &	48.9&	58.4&	68.9&	85.2&	\textbf{95.7} \\
% /3.7 (sil)
LST (\textbf{Align, sil sep.})    & 5.8\tnote{*}  &    \textbf{39.9} &	\textbf{49.3}&	\textbf{60.3}&	\textbf{69.9}&	\textbf{85.7}&	\textbf{95.3} \\
LST (Static)  & 6 &  39.4&	49.2&	58.7&	69.6&	84.9&	94.9  \\
\midrule
% LST (Static)  & 9 &  34.7&	\textbf{44.7}&	56.7&	66.3&	83.1&	94.7 \\ % TO UPDATE
% LST (\textbf{Align, sil merged})   & 9.4 &  \textbf{35.0} &	44.4	& \textbf{57.9}	& \textbf{67.7} &	\textbf{85.2}	& \textbf{95.4} \\ % TO UPDATE
% LST (Static) & 10 &  33.9&	44.4&	56.5&	67.1&	81.9&	94.3 \\ % TO UPDATE
LST (Static)  & 9 & 37.2 & \textbf{49.4} & 57.5 & \textbf{69.7} & 84.7 & 95.9 \\ % TO UPDATE
LST (\textbf{Align, sil merged}) & 9.4  &  \textbf{38.5} & 49.0 & \textbf{58.8} & \textbf{69.7} & \textbf{86.9} & \textbf{96.0}  \\ % TO UPDATE
% LST (Static) & 10 & 36.3 & \textbf{49.5} & 57.1 & 69.4 & 84.1 & 95.7   \\ % TO UPDATE

\bottomrule
\end{tabular}
\begin{tablenotes}[para,flushleft]
\footnotesize
\item[*] The average patch length is 5.8 for words in Align (sil sep.), while silence has an average of 3.7.
\end{tablenotes}
\end{threeparttable}
\vspace{-2mm}
\end{table}


\textbf{Speech model efficiency.} Compared to text, speech yields much longer token sequences, owing to higher frequency audio codecs, that consume many times additional compute to pre-train and generate. Efforts to mitigate this include methods to produce coarser speech units \citep{baadesyllablelm,tseng2025taste}, hierarchical generation \citep{borsos2023audiolm}, and producing residual tokens using parallel streams \citep{copet2023simple}. Attempts at text-inspired approaches to compress token sequences such as BPE \citep{ren2022speech,li2024effectiveness} achieved limited success. In this paper, we take inspiration from recent dynamic patching approaches that have yielded improvements in other modalities such as text \citep{pagnoni2024byte,yu2023megabyte,videau2025bytes} and vision \citep{pang2024next,beyer2023flexivit}, and extend these methods to speech-text LLMs.

\textbf{Speech Understanding Benchmarks.} Going beyond measuring only acoustic and phonetic capabilities of speech models using scores such as ABX \citep{kahn2020libri}, \cite{nguyen2020zero} established the Zero Resource Speech Benchmark 2021, comprising datasets/metrics to evaluate   lexical (sWUGGY), syntactic (sBLIMP) and lexical-semantic (sSIMI) capabilities of spoken LLMs. Since these benchmarks contrast between very short speech segments, we found that dynamic compute  approaches such as ours, do not yield significant improvements. However, subsequently, \cite{hassid2023textually} introduced the sStoryCloze and TopicStoryCloze datasets, which are story completion benchmarks in the speech modality measuring commonsense/understanding abilities of Spoken LLMs. We use these benchmarks in this paper, together with a speech version of the popular HellaSWAG textual benchmark, also measuring commonsense reasoning capabilities.

%Voxeval 

\section{Limitations}
Our study has several limitations. First, we focus on half-duplex speech–text modeling, where speech and text alternate in turns, and do not yet address full-duplex interaction required for real-time dialogue such as  Moshi~\citep{defossez2024moshi}. Second, our analysis is restricted to the pre-training stage, without exploring instruction fine-tuning or downstream adaptation, which we leave for future work. Third, some of our patching strategies, such as alignment and curriculum, rely on forced alignments during pre-training; although curriculum patching reduces this dependency at inference, fully alignment-free approaches remain an open challenge. Finally, our experiments are limited to the speech–text modality, and we have not yet extended LST to additional modalities such as image or video, which represent a promising next direction.
% Full-duplex

\section{Conclusion}
We presented the Latent Speech-Text Transformer (LST), a patching-based framework that compresses speech tokens into latent units for efficient and balanced multimodal training. Our experiments demonstrate that LST consistently outperforms baseline SpeechLLMs, with curriculum patching delivering the most robust gains across diverse datasets. By reducing speech–text imbalance, LST improves speech understanding while maintaining strong text performance, and its advantages grow further with model scaling. These findings highlight LST as a practical and scalable approach to bridging speech and text, offering improved efficiency, stronger cross-modal transfer, and greater robustness under varying training conditions.

\section*{Ethics Statement}
This work adheres to the ICLR Code of Ethics. %\footnote{\url{https://iclr.cc/public/CodeOfEthics}} 
We use only publicly available datasets (e.g., LibriLight, People’s Speech, Multilingual LibriSpeech, Spotify) and did not collect any new human-subject data. No personally identifiable information is included. Our methods aim to improve efficiency and generalization in speech–text modeling, with potential positive impacts on accessibility and multilingual applications. As with all large language models, there remain general risks of misuse, and we encourage responsible use of our work. 
% \subsubsection*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

% \subsubsection*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.


\bibliography{iclr2026/arxiv_iclr}
\bibliographystyle{iclr2026_conference}

\appendix
\section{Appendix}
% You may include other additional sections here.
\subsection{Interleaved Data Construction}
\label{app:interleaved}

To generate interleaved sequences from parallel speech–text data, we proceed as follows:
\begin{enumerate}
    \item \textbf{Alignment.} We obtain alignment information by using Wav2Vec2 + CTC to determine the boundaries linking text tokens to their corresponding spans of speech tokens. 
    \item \textbf{Span selection.} For each training example, we randomly select a contiguous span of words. The selected span is replaced by text tokens, while the following span of approximately half that length is kept as speech tokens. 
    \item \textbf{Modality markers.} We insert special tokens \texttt{<t>} and \texttt{<s>} to indicate the start of text and speech segments, respectively. This ensures the model can disambiguate between modalities. 
    \item \textbf{Dynamic sampling.} Interleaved sequences are generated dynamically at training time, so each epoch exposes the model to different interleaving patterns for better robustness.
\end{enumerate}

This process yields diverse interleaved training examples while preserving alignment between speech and text, allowing the same model to be applied uniformly to S$\rightarrow$S, S$\rightarrow$T, T$\rightarrow$S, and T$\rightarrow$T tasks.


\subsection{Hyperparameters}
\label{app:hyper}
\subsubsection{Optimization and Training Configuration}
We trained the model using the AdamW optimizer ($\beta_1=0.9$, $\beta_2=0.95$, weight decay = 0.1). The learning rate was initialized at $4\times10^{-4}$ and scheduled with cosine decay, including a warmup period of 2{,}000 steps and a minimum ratio of 0.01 at the final step. For the 1B model, training was performed on 32 H100 GPUs with a per-GPU batch size of 4 sequences (sequence length = 4{,}096 units), leading to a total batch size of 0.5M units. Mixed-precision training with bfloat16 was used for efficiency. Gradient clipping was applied at 1.0, and gradient accumulation was set to 1. Model parallelism used a single partition, and Fully Sharded Data Parallel (FSDP) was enabled for memory efficiency. No dropout was applied.
The 1B model was trained for 200k steps, corresponding to approximately 1 trillion units, and required around 17 hours to complete on 32 H100 GPUs.

\subsubsection{Model Architecture}
Table~\ref{tab:model-arch} summarizes the hierarchical architecture used in our experiments. 
The model consists of a shallow local encoder, a deep global transformer, and a moderately deep local decoder. 
The local modules operate with restricted attention windows to capture fine-grained context, while the global transformer 
uses block-causal attention with rotary position embeddings (RoPE) to handle long-range dependencies efficiently. 
This design balances local detail preservation with scalable long-context modeling.

\begin{table}[h]
\centering
\caption{Model architecture configuration. Each module is shown with its depth, hidden dimension, number of attention heads, and other relevant settings.}
\label{tab:model-arch}
\begin{tabular}{lcccc}
\toprule
\textbf{Module} & \textbf{Layers} & \textbf{Dim.} & \textbf{Heads} & \textbf{Notes} \\
\midrule
Local Encoder   & 1   & 1024  & 16 & Local window = 512 \\
Global Transformer & 25  & 2048  & 16 & Block-causal; RoPE ($\theta=5\!\times\!10^5$) \\
Local Decoder   & 9   & 1024  & 16 & Local window = 512 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Compute-Optimal Scaling}
\label{sec:hellaswag_scaling}

Figure~\ref{fig:hellaswag} presents HellaSwag accuracy under compute-optimal training across model sizes ranging from 420M to 1.8B. For text, we follow the scaling rule of \citet{hoffmann2022training}, allocating 20× the model size in training tokens. Speech uses half of the text tokens, preserving a 2:1 ratio of text to speech tokens.
At the smallest scale (420M), LST already outperforms the baseline, reaching 29.2\% vs. 28.4\% for speech and 31.6\% vs. 31.1\% for text. These improvements compound with scale: at 1.8B, LST (Speech) attains 39.0\% compared to 35.3\%, while LST (Text) achieves 46.3\% over 45.7\%. Overall, LST provides consistent gains in both modalities, with advantages apparent from the earliest scale and amplified as model capacity increases.

We further analyze model preference using the 
\emph{NLL difference} in Fig.~\ref{fig:hellaswag_nll_diff}, defined as the average gap between the negative log-likelihood of the correct option and that of the incorrect ones. 
More negative values indicate stronger separation, i.e., the model assigns 
lower NLL to the correct choice relative to distractors. 
LST consistently yields 
larger-magnitude gaps than the baseline across both text and speech. 
For speech (Fig.~\ref{fig:nll_speech}), the gap expands from about 
$-0.53$ to $-1.80$, while the baseline only improves from $-0.45$ to $-1.39$. 
For text (Fig.~\ref{fig:nll_text}), the difference deepens from $-5.8$ at 420M to $-15.6$ at 1.8B, compared to the baseline’s $-5.3$ to $-15.0$. 
Since NLL difference serves as a more stable indicator of model preference than accuracy alone, these results provide clearer evidence of the improvements achieved by LST as scale increases.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{iclr2026/figs/HellaSwag_accuracy_compute_optimal.pdf}
    \caption{HellaSwag accuracy under compute-optimal training (420M–1.8B). LST consistently outperforms the baseline for both text and speech, with gains increasing at larger scales.}
    \label{fig:hellaswag}
\end{figure}



\begin{figure}[h]
  \centering
  \begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{iclr2026/figs/HellaSwag_accuracy_compute_optimal_nll_speech.pdf}% replace with your file
    \caption{Negative log-likelihood difference on HellaSwag Speech.}
    \label{fig:nll_speech}
  \end{subfigure}\hfill
  \begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{iclr2026/figs/HellaSwag_accuracy_compute_optimal_nll_text.pdf}
    \caption{Negative log-likelihood difference on HellaSwag Text.}
    \label{fig:nll_text}
  \end{subfigure}
  \caption{{HellaSwag NLL difference (correct minus incorrect) across scales.}
  More negative is better (lower NLL for the correct option).
  LST separates correct vs.\ incorrect choices more strongly than the baseline for both modalities, and the gap widens with model size (420M\,$\rightarrow$\,1.8B).}
  \label{fig:hellaswag_nll_diff}
\end{figure}



\subsection{Effect of Speech Proportion}
\label{sec:speech_proportion}

Figure~\ref{fig:speech-ratio} illustrates the effect of varying the training speech--to--text token ratio on HellaSwag.
Across all settings, LST consistently outperforms the baseline, and both methods exhibit the best speech–text trade-off at the $1{:}2$ ratio.
Moving from $1{:}3$ to $1{:}2$ improves LST (S$\rightarrow$S) from $40.2$ to $41.3$ while keeping LST (T$\rightarrow$T) high at $49.7$; pushing further to $1{:}1$ does not provide speech gain ($41.2$) but a large text drop ($47.2$, $-2.5$).
The baseline shows the same pattern: at $1{:}2$ it reaches $36.8$ (S$\rightarrow$S) and $47.1$ (T$\rightarrow$T), whereas $1{:}1$ gives only $37.0$ on speech (+0.2) but lowers text to $45.4$ ($-1.7$).
Averaging speech and text accuracies, the macro score peaks at $1{:}2$ for both LST and the baseline.
These results indicate that allocating about one-third of tokens to speech ($1{:}2$) offers a fair and robust operating point for both models to avoid the substantial text-side degradation seen at $1{:}1$ while securing clear gains over lower speech ratios.


\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{iclr2026/figs/HellaSwag_accuracy_ratio.pdf}
\caption{Effect of speech-to-text token ratio at 200k iterations on HellaSwag. Results are reported on HellaSwag under both S$\rightarrow$S and T$\rightarrow$T.}
\label{fig:speech-ratio}
\end{figure}

\subsection{BPE-Aligned Patching}
\label{sec:bpe_align}
In addition to word-aligned patching, we also explored {BPE-aligned} patching, where speech patches are constructed according to BPE segmentation of the text. To ensure comparability, we applied the same forced-alignment procedure at the character level and then mapped aligned spans to their corresponding BPE units. While this provides finer granularity, the resulting boundaries are less precise and the subword pieces do not always correspond to meaningful acoustic events. 
% 
As shown in Table~\ref{tab:bpe_patching}, word alignment generally outperforms BPE alignment in S$\rightarrow$S (e.g., 59.4 vs.\ 55.6 on StoryCloze and 84.8 vs.\ 79.6 on TopicStoryCloze), reflecting the more reliable word-level boundaries. On the other hand, BPE achieves slightly better T$\rightarrow$T results, likely because its patching 
is directly aligned with the underlying text BPE tokens. Finally, curriculum training further boosts HellaSwag S$\rightarrow$S performance, improving from 40.0/39.2 to 41.5/41.3 for Word and BPE, respectively.



\begin{table}[h]
\centering
\begin{threeparttable}
\caption{Comparison of aligned patching strategies under a speech-to-text token ratio of 1:4. 
\emph{Word Align} uses word-level forced alignment, 
\emph{BPE Align} uses BPE segmentation, 
and \emph{Curriculum} gradually shifts from alignment-based to static patching. }
\label{tab:bpe_patching}
\begin{tabular}{lccc|cc|cc}
\toprule
\textbf{Model} & \textbf{Ave Patch Size} & \multicolumn{2}{c|}{HellaSwag} & \multicolumn{2}{c|}{StoryCloze} & \multicolumn{2}{c}{TopicStoryCloze} \\
 & (tokens) & S$\rightarrow$S & T$\rightarrow$T & S$\rightarrow$S & T$\rightarrow$T & S$\rightarrow$S & T$\rightarrow$T  \\
\midrule
% LST (Static)        & 4   &   39.3 & 50.1 & 58.7 & 69.1 & 84.7 & \textbf{96.0} \\
LST (Word Align)    & 5.8\tnote{*} &  \textbf{40.0} & 49.9 & \textbf{59.4} & 68.6 & \textbf{84.8} & 94.6 \\
LST (BPE Align)     & 5.0\tnote{*} &  39.2 & \textbf{50.1} & 55.6 & \textbf{69.1} & 79.6 & \textbf{95.6} \\ \midrule
LST (Word Curr.)    & 5.8$\rightarrow$4 &  \textbf{41.5}   &   \textbf{49.5}   & 57.9 & \textbf{68.9} & \textbf{86.8} & 95.1 \\
LST (BPE Curr.)     & 5.0$\rightarrow$4 &  41.3   &    48.6  & \textbf{59.1} & 67.1 & 86.5 & \textbf{95.4} \\
\bottomrule
\end{tabular}
\begin{tablenotes}[para,flushleft]
\footnotesize
\item[*] The average patch length is 5.8 for words, 5.0 for BPEs, and 3.7 for silence spans.
\end{tablenotes}
\end{threeparttable}
\vspace{-2mm}
\end{table}

\subsection{Stability Analysis Across Tasks}
\label{sec:stability}
We further examine the robustness of patching strategies by repeating each experiment three times and reporting the average (Ave) and standard deviation (Std). Table~\ref{tab:stability} summarizes results for HellaSwag (HS), StoryCloze (SC), and TopicStoryCloze (TSC) under both speech-to-speech (S$\rightarrow$S) and text-to-text (T$\rightarrow$T) directions. HellaSwag results are generally more stable than the other tasks: both Curriculum and the Baseline show near-zero std (0.13 and 0.22 for S$\rightarrow$S), while Static is relatively less stable with larger fluctuations (0.67). By contrast, StoryCloze and TopicStoryCloze exhibit considerably higher deviations, occasionally exceeding 1.5, which indicates greater instability. Overall, Curriculum improves the average accuracy across all tasks while delivering highly consistent results on HellaSwag, underscoring its effectiveness in stabilizing training.


\begin{table}[h]
\centering
\begin{threeparttable}
\caption{Average (Ave) and standard deviation (Std) across three runs. Each task is reported with both S$\rightarrow$S and T$\rightarrow$T directions.}
\label{tab:stability}
\begin{tabular}{lccc|cc|cc}
\toprule
\textbf{Model} & \textbf{Evaluation} & \multicolumn{2}{c|}{HellaSwag} & \multicolumn{2}{c|}{StoryCloze} & \multicolumn{2}{c}{TopicStoryCloze} \\
 & & Ave & Std & Ave & Std & Ave & Std \\
\midrule
Curriculum & S$\rightarrow$S & \textbf{41.4} & 0.13 & \textbf{59.2} & 0.68 & \textbf{87.1} & 0.45 \\
           & T$\rightarrow$T & \textbf{49.1} & 0.06 & \textbf{69.5} & 1.56 & \textbf{95.6} & 0.45 \\
\midrule
Static (4)     & S$\rightarrow$S & 40.9 & 0.67 & 58.5 & 0.50 & 86.6 & 0.52 \\
           & T$\rightarrow$T & 48.5 & 0.37 & 69.4 & 0.11 & 95.1 & 0.19 \\
\midrule
Baseline   & S$\rightarrow$S & 36.5 & 0.22 & 58.3 & 0.21 & 86.3 & 0.52 \\
           & T$\rightarrow$T & 46.3 & 0.78 & 66.6 & 1.56 & 93.9 & 1.44 \\
\bottomrule
\end{tabular}
\end{threeparttable}
\vspace{-2mm}
\end{table}






% \subsection{LLM Usage.} We used large language models solely for polishing some sentences in this paper.  


\end{document}
