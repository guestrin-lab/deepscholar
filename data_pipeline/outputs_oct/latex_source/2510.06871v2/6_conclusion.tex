\vspace{-0.7em}
\section{Conclusion}
\vspace{-0.7em}



In this paper, we introduce \textbf{\method{}}, a safety-aligned reinforcement learning framework that embeds safety as an active driver of multimodal reasoning. By integrating the curated QI-Safe-10K dataset, safety-aware rollouts with reflection and correction, and structured reward modeling, \method{} shifts safety from a passive safeguard to a core component of inference. Extensive evaluations show that it achieves SOTA safety and competitive helpfulness, surpasses same-scale and larger open-source models, and performs on par with leading proprietary systems. Robustness and ablation studies confirm that the improvements stem from injecting safety-awareness into the reasoning process. Together, these results establish \method{} as a reliable paradigm for multimodal safety alignment and a foundation for building future safe and interpretable AI systems.




