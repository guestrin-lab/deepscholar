\begin{thebibliography}{70}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc, Mensch, Millican, Reynolds, et~al.]{MLLM-flamingo}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 23716--23736, 2022.

\bibitem[Askell et~al.(2021)Askell, Bai, Chen, Drain, Ganguli, Henighan, Jones, Joseph, Mann, DasSarma, et~al.]{safe-3h}
Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et~al.
\newblock A general language assistant as a laboratory for alignment.
\newblock \emph{arXiv preprint arXiv:2112.00861}, 2021.

\bibitem[Bai et~al.(2025)Bai, Chen, Liu, Wang, Ge, Song, Dang, Wang, Wang, Tang, et~al.]{MLLM-qwen2.5vl}
Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et~al.
\newblock Qwen2. 5-vl technical report.
\newblock \emph{arXiv preprint arXiv:2502.13923}, 2025.

\bibitem[Bensal et~al.(2025)Bensal, Jamil, Bryant, Russak, Kamble, Mozolevskyi, Ali, and AlShikh]{MLRM-reflect-r3}
Shelly Bensal, Umar Jamil, Christopher Bryant, Melisa Russak, Kiran Kamble, Dmytro Mozolevskyi, Muayad Ali, and Waseem AlShikh.
\newblock Reflect, retry, reward: Self-improving llms via reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2505.24726}, 2025.

\bibitem[Chen et~al.(2025)Chen, Pang, Dong, Wang, Du, and Chen]{safe-guard-vlmguard}
Menglan Chen, Xianghe Pang, Jingjing Dong, WenHao Wang, Yaxin Du, and Siheng Chen.
\newblock Vlmguard-r1: Proactive safety alignment for vlms via reasoning-driven prompt optimization.
\newblock \emph{arXiv preprint arXiv:2504.12661}, 2025.

\bibitem[Comanici et~al.(2025)Comanici, Bieber, Schaekermann, Pasupat, Sachdeva, Dhillon, Blistein, Ram, Zhang, Rosen, et~al.]{comanici2025gemini}
Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et~al.
\newblock Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities.
\newblock \emph{arXiv preprint arXiv:2507.06261}, 2025.

\bibitem[Ding et~al.(2024)Ding, Li, and Zhang]{safe-guard-eta}
Yi~Ding, Bolian Li, and Ruqi Zhang.
\newblock Eta: Evaluating then aligning safety of vision language models at inference time.
\newblock \emph{arXiv preprint arXiv:2410.06625}, 2024.

\bibitem[Duan et~al.(2025)Duan, Fang, Wang, Wang, Huang, Zeng, Li, and Liu]{MLRM-reward-got-r1}
Chengqi Duan, Rongyao Fang, Yuqing Wang, Kun Wang, Linjiang Huang, Xingyu Zeng, Hongsheng Li, and Xihui Liu.
\newblock Got-r1: Unleashing reasoning capability of mllm for visual generation with reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2505.17022}, 2025.

\bibitem[Fan et~al.(2025{\natexlab{a}})Fan, Feng, Lyu, Zhou, and Yue]{MLRM-reward-sophiavl}
Kaixuan Fan, Kaituo Feng, Haoming Lyu, Dongzhan Zhou, and Xiangyu Yue.
\newblock Sophiavl-r1: Reinforcing mllms reasoning with thinking reward.
\newblock \emph{arXiv preprint arXiv:2505.17018}, 2025{\natexlab{a}}.

\bibitem[Fan et~al.(2025{\natexlab{b}})Fan, He, Yang, Zheng, Kuo, Zheng, Narayanaraju, Guan, and Wang]{MLRM-mcot-grit}
Yue Fan, Xuehai He, Diji Yang, Kaizhi Zheng, Ching-Chen Kuo, Yuting Zheng, Sravana~Jyothi Narayanaraju, Xinze Guan, and Xin~Eric Wang.
\newblock Grit: Teaching mllms to think with images.
\newblock \emph{arXiv preprint arXiv:2505.15879}, 2025{\natexlab{b}}.

\bibitem[Fang et~al.(2025{\natexlab{a}})Fang, Wang, Wang, Yao, Wang, Zhang, Wang, and Chua]{safe-safemlrm}
Junfeng Fang, Yukai Wang, Ruipeng Wang, Zijun Yao, Kun Wang, An~Zhang, Xiang Wang, and Tat-Seng Chua.
\newblock Safemlrm: Demystifying safety in multi-modal large reasoning models.
\newblock \emph{arXiv preprint arXiv:2504.08813}, 2025{\natexlab{a}}.

\bibitem[Fang et~al.(2025{\natexlab{b}})Fang, Jin, Xiong, Jin, Zhong, Ouyang, Zhang, Han, and Lu]{cello1}
Yin Fang, Qiao Jin, Guangzhi Xiong, Bowen Jin, Xianrui Zhong, Siru Ouyang, Aidong Zhang, Jiawei Han, and Zhiyong Lu.
\newblock Cell-o1: Training llms to solve single-cell reasoning puzzles with reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2506.02911}, 2025{\natexlab{b}}.

\bibitem[Gao et~al.(2024)Gao, Pi, Han, Wu, Hong, Kong, Jiang, and Li]{safe-guard-coca}
Jiahui Gao, Renjie Pi, Tianyang Han, Han Wu, Lanqing Hong, Lingpeng Kong, Xin Jiang, and Zhenguo Li.
\newblock Coca: Regaining safety-awareness of multimodal large language models with constitutional calibration.
\newblock \emph{arXiv preprint arXiv:2409.11365}, 2024.

\bibitem[Ghosal et~al.(2025)Ghosal, Chakraborty, Singh, Guan, Wang, Beirami, Huang, Velasquez, Manocha, and Bedi]{safe-guard-immune}
Soumya~Suvra Ghosal, Souradip Chakraborty, Vaibhav Singh, Tianrui Guan, Mengdi Wang, Ahmad Beirami, Furong Huang, Alvaro Velasquez, Dinesh Manocha, and Amrit~Singh Bedi.
\newblock Immune: Improving safety against jailbreaks in multi-modal llms via inference-time alignment.
\newblock In \emph{Proceedings of the Computer Vision and Pattern Recognition Conference}, pp.\  25038--25049, 2025.

\bibitem[Gong et~al.(2025)Gong, Ran, Liu, Wang, Cong, Wang, Duan, and Wang]{safe-risk-figstep}
Yichen Gong, Delong Ran, Jinyuan Liu, Conglei Wang, Tianshuo Cong, Anyu Wang, Sisi Duan, and Xiaoyun Wang.
\newblock Figstep: Jailbreaking large vision-language models via typographic visual prompts.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~39, pp.\  23951--23959, 2025.

\bibitem[Gou et~al.(2024)Gou, Chen, Liu, Hong, Xu, Li, Yeung, Kwok, and Zhang]{safe-guard-esco}
Yunhao Gou, Kai Chen, Zhili Liu, Lanqing Hong, Hang Xu, Zhenguo Li, Dit-Yan Yeung, James~T Kwok, and Yu~Zhang.
\newblock Eyes closed, safety on: Protecting multimodal llms via image-to-text transformation.
\newblock In \emph{European Conference on Computer Vision}, pp.\  388--404. Springer, 2024.

\bibitem[Gu et~al.(2024)Gu, Zhou, Huang, Dandan, Wang, Zhao, Yao, Yang, Teng, Qiao, et~al.]{safe-risk-mllmguard}
Tianle Gu, Zeyang Zhou, Kexin Huang, Liang Dandan, Yixu Wang, Haiquan Zhao, Yuanqi Yao, Yujiu Yang, Yan Teng, Yu~Qiao, et~al.
\newblock Mllmguard: A multi-dimensional safety evaluation suite for multimodal large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 37:\penalty0 7256--7295, 2024.

\bibitem[Guo et~al.(2025)Guo, Yang, Zhang, Song, Wang, Zhu, Xu, Zhang, Ma, Bi, et~al.]{MLRM-r1}
Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et~al.
\newblock Deepseek-r1 incentivizes reasoning in llms through reinforcement learning.
\newblock \emph{Nature}, 645\penalty0 (8081):\penalty0 633--638, 2025.

\bibitem[Hong et~al.(2025)Hong, Yu, Gu, Wang, Gan, Tang, Cheng, Qi, Ji, Pan, et~al.]{MLRM-model-glm4.5}
Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji~Qi, Junhui Ji, Lihang Pan, et~al.
\newblock Glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning.
\newblock \emph{arXiv e-prints}, pp.\  arXiv--2507, 2025.

\bibitem[Huang et~al.(2025)Huang, Jia, Zhai, Cao, Ye, Zhao, Xu, Hu, and Lin]{MLRM-per-visionr1}
Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin.
\newblock Vision-r1: Incentivizing reasoning capability in multimodal large language models.
\newblock \emph{arXiv preprint arXiv:2503.06749}, 2025.

\bibitem[Hurst et~al.(2024)Hurst, Lerer, Goucher, Perelman, Ramesh, Clark, Ostrow, Welihinda, Hayes, Radford, et~al.]{MLLM-gpt4o}
Aaron Hurst, Adam Lerer, Adam~P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ~Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et~al.
\newblock Gpt-4o system card.
\newblock \emph{arXiv preprint arXiv:2410.21276}, 2024.

\bibitem[Jaech et~al.(2024)Jaech, Kalai, Lerer, Richardson, El-Kishky, Low, Helyar, Madry, Beutel, Carney, et~al.]{MLRM-o1}
Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et~al.
\newblock Openai o1 system card.
\newblock \emph{arXiv preprint arXiv:2412.16720}, 2024.

\bibitem[Ji et~al.(2023)Ji, Qiu, Chen, Zhang, Lou, Wang, Duan, He, Zhou, Zhang, et~al.]{safe-risk-aialign}
Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, et~al.
\newblock Ai alignment: A comprehensive survey.
\newblock \emph{arXiv preprint arXiv:2310.19852}, 2023.

\bibitem[Ji et~al.(2024)Ji, Zhou, Lou, Chen, Hong, Wang, Chen, Wang, Pan, Li, et~al.]{Data-align}
Jiaming Ji, Jiayi Zhou, Hantao Lou, Boyuan Chen, Donghai Hong, Xuyao Wang, Wenqi Chen, Kaile Wang, Rui Pan, Jiahao Li, et~al.
\newblock Align anything: Training all-modality models to follow instructions with language feedback.
\newblock \emph{arXiv preprint arXiv:2412.15838}, 2024.

\bibitem[Ji et~al.(2025)Ji, Chen, Pan, Zhang, Zhu, Li, Hong, Chen, Zhou, Wang, et~al.]{safe-algin-saferlhfv}
Jiaming Ji, Xinyu Chen, Rui Pan, Conghui Zhang, Han Zhu, Jiahao Li, Donghai Hong, Boyuan Chen, Jiayi Zhou, Kaile Wang, et~al.
\newblock Safe rlhf-v: Safe reinforcement learning from multi-modal human feedback.
\newblock \emph{arXiv preprint arXiv:2503.17682}, 2025.

\bibitem[Jiang et~al.(2025)Jiang, Chen, Zeng, Yu, and Zhang]{MLRM-mcot-rex}
Qing Jiang, Xingyu Chen, Zhaoyang Zeng, Junzhi Yu, and Lei Zhang.
\newblock Rex-thinker: Grounded object referring via chain-of-thought reasoning.
\newblock \emph{arXiv preprint arXiv:2506.04034}, 2025.

\bibitem[Jiang et~al.(2024)Jiang, Tan, and Yue]{safe-guard-rapguard}
Yilei Jiang, Yingshui Tan, and Xiangyu Yue.
\newblock Rapguard: Safeguarding multimodal large language models via rationale-aware defensive prompting.
\newblock \emph{arXiv preprint arXiv:2412.18826}, 2024.

\bibitem[Jin et~al.(2025)Jin, Qi, Chen, Guo, and Wang]{mdit}
Bohan Jin, Shuhan Qi, Kehai Chen, Xinyi Guo, and Xuan Wang.
\newblock Mdit-bench: Evaluating the dual-implicit toxicity in large multimodal models.
\newblock \emph{arXiv preprint arXiv:2505.17144}, 2025.

\bibitem[Kumar et~al.(2024)Kumar, Zhuang, Agarwal, Su, Co-Reyes, Singh, Baumli, Iqbal, Bishop, Roelofs, et~al.]{kumar2024training}
Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi~Su, John~D Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, et~al.
\newblock Training language models to self-correct via reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2409.12917}, 2024.

\bibitem[Li et~al.(2024)Li, Li, Yin, Ahmed, Liu, and Liu]{safe-risk-red}
Mukai Li, Lei Li, Yuwei Yin, Masood Ahmed, Zhenguang Liu, and Qi~Liu.
\newblock Red teaming visual language models.
\newblock \emph{arXiv preprint arXiv:2401.12915}, 2024.

\bibitem[Li et~al.(2025{\natexlab{a}})Li, Wei, Zheng, Huang, Kong, Sun, and Huang]{MLRM-aug-visionmatters}
Yuting Li, Lai Wei, Kaipeng Zheng, Jingyuan Huang, Linghe Kong, Lichao Sun, and Weiran Huang.
\newblock Vision matters: Simple visual perturbations can boost multimodal math reasoning.
\newblock \emph{arXiv preprint arXiv:2506.09736}, 2025{\natexlab{a}}.

\bibitem[Li et~al.(2025{\natexlab{b}})Li, Ma, Li, Li, Rong, Xu, Zhang, Zhao, and Huang]{MLRM-spatial-star}
Zongzhao Li, Zongyang Ma, Mingze Li, Songyou Li, Yu~Rong, Tingyang Xu, Ziqi Zhang, Deli Zhao, and Wenbing Huang.
\newblock Star-r1: Spatial transformation reasoning by reinforcing multimodal llms.
\newblock \emph{arXiv preprint arXiv:2505.15804}, 2025{\natexlab{b}}.

\bibitem[Liu et~al.(2024{\natexlab{a}})Liu, Zhu, Gu, Lan, Yang, and Qiao]{mmsafetybench}
Xin Liu, Yichen Zhu, Jindong Gu, Yunshi Lan, Chao Yang, and Yu~Qiao.
\newblock Mm-safetybench: A benchmark for safety evaluation of multimodal large language models.
\newblock In \emph{European Conference on Computer Vision}, pp.\  386--403. Springer, 2024{\natexlab{a}}.

\bibitem[Liu et~al.(2024{\natexlab{b}})Liu, Zhu, Lan, Yang, and Qiao]{liu2024safety}
Xin Liu, Yichen Zhu, Yunshi Lan, Chao Yang, and Yu~Qiao.
\newblock Safety of multimodal large language models on images and texts.
\newblock \emph{arXiv preprint arXiv:2402.00357}, 2024{\natexlab{b}}.

\bibitem[Liu et~al.(2025{\natexlab{a}})Liu, Zhai, Du, Chen, Cao, Gao, Wang, Li, Wang, Fang, et~al.]{safe-guard-guardreasoner}
Yue Liu, Shengfang Zhai, Mingzhe Du, Yulin Chen, Tri Cao, Hongcheng Gao, Cheng Wang, Xinfeng Li, Kun Wang, Junfeng Fang, et~al.
\newblock Guardreasoner-vl: Safeguarding vlms via reinforced reasoning.
\newblock \emph{arXiv preprint arXiv:2505.11049}, 2025{\natexlab{a}}.

\bibitem[Liu et~al.(2025{\natexlab{b}})Liu, Sun, Zang, Dong, Cao, Duan, Lin, and Wang]{MLRM-per-vrft}
Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang.
\newblock Visual-rft: Visual reinforcement fine-tuning.
\newblock \emph{arXiv preprint arXiv:2503.01785}, 2025{\natexlab{b}}.

\bibitem[Lou et~al.(2025)Lou, Li, Xu, Shi, Chen, and Huang]{safe-algin-think}
Xinyue Lou, You Li, Jinan Xu, Xiangyu Shi, Chi Chen, and Kaiyu Huang.
\newblock Think in safety: Unveiling and mitigating safety alignment collapse in multimodal large reasoning model.
\newblock \emph{arXiv preprint arXiv:2505.06538}, 2025.

\bibitem[Luo et~al.(2025)Luo, Zheng, Wang, Ni, Lin, Jiang, Yu, Shi, Chu, Zeng, et~al.]{MLRM-math-ursa}
Ruilin Luo, Zhuofan Zheng, Yifan Wang, Xinzhe Ni, Zicheng Lin, Songtao Jiang, Yiyao Yu, Chufan Shi, Ruihang Chu, Jin Zeng, et~al.
\newblock Ursa: Understanding and verifying chain-of-thought reasoning in multimodal mathematics.
\newblock \emph{arXiv preprint arXiv:2501.04686}, 2025.

\bibitem[Na et~al.(2025)Na, Jeong, and Lee]{safe-guard-sia}
Youngjin Na, Sangheon Jeong, and Youngwan Lee.
\newblock Sia: Enhancing safety via intent awareness for vision-language models.
\newblock \emph{arXiv preprint arXiv:2507.16856}, 2025.

\bibitem[OpenAI(2025{\natexlab{a}})]{gpt4.1}
OpenAI.
\newblock Gpt-4.1 model card.
\newblock \url{https://platform.openai.com/docs/models/gpt-4.1}, 2025{\natexlab{a}}.
\newblock [Accessed 31-08-2025].

\bibitem[OpenAI(2025{\natexlab{b}})]{gpt5}
OpenAI.
\newblock Gpt-5 system card.
\newblock \url{https://openai.com/index/gpt-5-system-card/}, 2025{\natexlab{b}}.
\newblock Accessed 13-08-2025.

\bibitem[Peng et~al.(2025)Peng, Zhang, Zhang, You, Liu, Zhu, Yang, Xu, Geng, and Yang]{MLRM-math-lmmr1}
Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu~Yang.
\newblock Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl.
\newblock \emph{arXiv preprint arXiv:2503.07536}, 2025.

\bibitem[Pi et~al.(2024)Pi, Han, Zhang, Xie, Pan, Lian, Dong, Zhang, and Zhang]{safe-guard-mllmprotector}
Renjie Pi, Tianyang Han, Jianshu Zhang, Yueqi Xie, Rui Pan, Qing Lian, Hanze Dong, Jipeng Zhang, and Tong Zhang.
\newblock Mllm-protector: Ensuring mllm's safety without hurting performance.
\newblock \emph{arXiv preprint arXiv:2401.02906}, 2024.

\bibitem[Qi et~al.(2024)Qi, Panda, Lyu, Ma, Roy, Beirami, Mittal, and Henderson]{safe-risk-safety}
Xiangyu Qi, Ashwinee Panda, Kaifeng Lyu, Xiao Ma, Subhrajit Roy, Ahmad Beirami, Prateek Mittal, and Peter Henderson.
\newblock Safety alignment should be made more than just a few tokens deep.
\newblock \emph{arXiv preprint arXiv:2406.05946}, 2024.

\bibitem[Shao et~al.(2024)Shao, Wang, Zhu, Xu, Song, Bi, Zhang, Zhang, Li, Wu, et~al.]{MLRM-grpo}
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK~Li, Y~Wu, et~al.
\newblock Deepseekmath: Pushing the limits of mathematical reasoning in open language models.
\newblock \emph{arXiv preprint arXiv:2402.03300}, 2024.

\bibitem[Shen et~al.(2025)Shen, Pei, Peng, Song, Liu, Peng, Sun, Hao, Wang, Zhang, et~al.]{MLRM-model-skywork}
Wei Shen, Jiangbo Pei, Yi~Peng, Xuchen Song, Yang Liu, Jian Peng, Haofeng Sun, Yunzhuo Hao, Peiyu Wang, Jianhao Zhang, et~al.
\newblock Skywork-r1v3 technical report.
\newblock \emph{arXiv preprint arXiv:2507.06167}, 2025.

\bibitem[Su et~al.(2025)Su, Wang, Ren, Lin, and Chen]{MLRM-reward-pixel}
Alex Su, Haozhe Wang, Weimin Ren, Fangzhen Lin, and Wenhu Chen.
\newblock Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2505.15966}, 2025.

\bibitem[Team et~al.(2025)Team, Du, Yin, Xing, Qu, Wang, Chen, Zhang, Du, Wei, et~al.]{MLRM-model-2025kimi}
Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, et~al.
\newblock Kimi-vl technical report.
\newblock \emph{arXiv preprint arXiv:2504.07491}, 2025.

\bibitem[Wan et~al.(2025)Wan, Dou, Liu, Zhang, Cui, Zhao, Shen, Xiong, Xin, Jiang, et~al.]{MLRM-reflect-srpo}
Zhongwei Wan, Zhihao Dou, Che Liu, Yu~Zhang, Dongfei Cui, Qinjian Zhao, Hui Shen, Jing Xiong, Yi~Xin, Yifan Jiang, et~al.
\newblock Srpo: Enhancing multimodal llm reasoning via reflection-aware reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2506.01713}, 2025.

\bibitem[Wang et~al.(2025{\natexlab{a}})Wang, Qu, Huang, Chu, Lin, and Chen]{MLRM-reflect-vl-rethinker}
Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen.
\newblock Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2504.08837}, 2025{\natexlab{a}}.

\bibitem[Wang et~al.(2025{\natexlab{b}})Wang, Lin, Cheng, and Shou]{MLRM-aug-thinknot}
Jiaqi Wang, Kevin~Qinghong Lin, James Cheng, and Mike~Zheng Shou.
\newblock Think or not? selective reasoning via reinforcement learning for vision-language models.
\newblock \emph{arXiv preprint arXiv:2505.16854}, 2025{\natexlab{b}}.

\bibitem[Wang et~al.(2024{\natexlab{a}})Wang, Bai, Tan, Wang, Fan, Bai, Chen, Liu, Wang, Ge, et~al.]{MLLM-qwen2vl}
Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et~al.
\newblock Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution.
\newblock \emph{arXiv preprint arXiv:2409.12191}, 2024{\natexlab{a}}.

\bibitem[Wang et~al.(2024{\natexlab{b}})Wang, Ye, Cheng, Duan, Li, Fu, Qiu, and Huang]{siuo}
Siyin Wang, Xingsong Ye, Qinyuan Cheng, Junwen Duan, Shimin Li, Jinlan Fu, Xipeng Qiu, and Xuanjing Huang.
\newblock Safe inputs but unsafe output: Benchmarking cross-modality safety alignment of large vision-language model.
\newblock \emph{arXiv preprint arXiv:2406.15279}, 2024{\natexlab{b}}.

\bibitem[Wang et~al.(2024{\natexlab{c}})Wang, Lv, Yu, Hong, Qi, Wang, Ji, Yang, Zhao, XiXuan, et~al.]{MLLM-cogvlm}
Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji~Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Song XiXuan, et~al.
\newblock Cogvlm: Visual expert for pretrained language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 37:\penalty0 121475--121499, 2024{\natexlab{c}}.

\bibitem[Wang et~al.(2025{\natexlab{c}})Wang, Liu, Gao, Huang, Yuan, He, Wang, and Tu]{MMSafeAware}
Wenxuan Wang, Xiaoyuan Liu, Kuiyi Gao, Jen-tse Huang, Youliang Yuan, Pinjia He, Shuai Wang, and Zhaopeng Tu.
\newblock Can't see the forest for the trees: Benchmarking multimodal safety awareness for multimodal llms.
\newblock \emph{arXiv preprint arXiv:2502.11184}, 2025{\natexlab{c}}.

\bibitem[Wang et~al.(2025{\natexlab{d}})Wang, Song, Gao, Wang, Yao, Teng, Ma, Wang, and Jiang]{safe-algin-SafeVid}
Yixu Wang, Jiaxin Song, Yifeng Gao, Xin Wang, Yang Yao, Yan Teng, Xingjun Ma, Yingchun Wang, and Yu-Gang Jiang.
\newblock Safevid: Toward safety aligned video large multimodal models.
\newblock \emph{arXiv preprint arXiv:2505.11926}, 2025{\natexlab{d}}.

\bibitem[Wang et~al.(2024{\natexlab{d}})Wang, Liu, Li, Chen, and Xiao]{safe-guard-adashield}
Yu~Wang, Xiaogeng Liu, Yu~Li, Muhao Chen, and Chaowei Xiao.
\newblock Adashield: Safeguarding multimodal large language models from structure-based attack via adaptive shield prompting.
\newblock In \emph{European Conference on Computer Vision}, pp.\  77--94. Springer, 2024{\natexlab{d}}.

\bibitem[Weng et~al.(2025)Weng, Lou, Feng, Huang, and Wang]{safe-algin-ADPO}
Fenghua Weng, Jian Lou, Jun Feng, Minlie Huang, and Wenjie Wang.
\newblock Adversary-aware dpo: Enhancing safety alignment in vision language models via adversarial training.
\newblock \emph{arXiv preprint arXiv:2502.11455}, 2025.

\bibitem[Xu et~al.(2025)Xu, Chan, Li, Aljunied, Yuan, Wang, Xiao, Chen, Liu, Li, et~al.]{lingshu}
Weiwen Xu, Hou~Pong Chan, Long Li, Mahani Aljunied, Ruifeng Yuan, Jianyu Wang, Chenghao Xiao, Guizhen Chen, Chaoqun Liu, Zhaodonghui Li, et~al.
\newblock Lingshu: A generalist foundation model for unified multimodal medical understanding and reasoning.
\newblock \emph{arXiv preprint arXiv:2506.07044}, 2025.

\bibitem[Yao et~al.(2024)Yao, Huang, Wu, Zhang, Wang, Liu, Wang, Song, Feng, Shen, et~al.]{MLRM-reflect-mulberry}
Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li~Shen, et~al.
\newblock Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search.
\newblock \emph{arXiv preprint arXiv:2412.18319}, 2024.

\bibitem[Yao et~al.(2025)Yao, Yin, Zhang, Yang, Wang, Wu, Su, Shen, Qiu, Tao, et~al.]{MLRM-aug-shareVL}
Huanjin Yao, Qixiang Yin, Jingyi Zhang, Min Yang, Yibo Wang, Wenhao Wu, Fei Su, Li~Shen, Minghui Qiu, Dacheng Tao, et~al.
\newblock R1-sharevl: Incentivizing reasoning capability of multimodal large language models via share-grpo.
\newblock \emph{arXiv preprint arXiv:2505.16673}, 2025.

\bibitem[Ye et~al.(2025)Ye, Rong, Huang, Du, Yu, and Tao]{ye2025survey}
Mang Ye, Xuankun Rong, Wenke Huang, Bo~Du, Nenghai Yu, and Dacheng Tao.
\newblock A survey of safety on large vision-language models: Attacks, defenses and evaluations.
\newblock \emph{arXiv preprint arXiv:2502.14881}, 2025.

\bibitem[Yu et~al.(2025)Yu, Lin, Zhao, Yin, Wei, Peng, Wei, Sun, Han, Ge, et~al.]{MLRM-per-perception}
En~Yu, Kangheng Lin, Liang Zhao, Jisheng Yin, Yana Wei, Yuang Peng, Haoran Wei, Jianjian Sun, Chunrui Han, Zheng Ge, et~al.
\newblock Perception-r1: Pioneering perception policy with reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2504.07954}, 2025.

\bibitem[Zhang et~al.(2025{\natexlab{a}})Zhang, Gao, Zhang, Li, Zhang, Liu, Yuan, Wu, Jia, Zhu, et~al.]{MLRM-mcot-chain}
Xintong Zhang, Zhi Gao, Bofei Zhang, Pengxiang Li, Xiaowen Zhang, Yang Liu, Tao Yuan, Yuwei Wu, Yunde Jia, Song-Chun Zhu, et~al.
\newblock Chain-of-focus: Adaptive visual search and zooming for multimodal reasoning via rl.
\newblock \emph{arXiv preprint arXiv:2505.15436}, 2025{\natexlab{a}}.

\bibitem[Zhang et~al.(2025{\natexlab{b}})Zhang, Chen, Zheng, Gao, Zheng, Fu, Yin, Jin, Qiao, Huang, et~al.]{spavl}
Yongting Zhang, Lu~Chen, Guodong Zheng, Yifeng Gao, Rui Zheng, Jinlan Fu, Zhenfei Yin, Senjie Jin, Yu~Qiao, Xuanjing Huang, et~al.
\newblock Spa-vl: A comprehensive safety preference alignment dataset for vision language models.
\newblock In \emph{Proceedings of the Computer Vision and Pattern Recognition Conference}, pp.\  19867--19878, 2025{\natexlab{b}}.

\bibitem[Zheng et~al.(2025{\natexlab{a}})Zheng, Chen, Zhong, Teng, Tan, Liu, Wang, Liu, Yang, Jing, et~al.]{safe-risk-usb}
Baolin Zheng, Guanlin Chen, Hongqiong Zhong, Qingyang Teng, Yingshui Tan, Zhendong Liu, Weixun Wang, Jiaheng Liu, Jian Yang, Huiyun Jing, et~al.
\newblock Usb: A comprehensive and unified safety evaluation benchmark for multimodal large language models.
\newblock \emph{arXiv preprint arXiv:2505.23793}, 2025{\natexlab{a}}.

\bibitem[Zheng et~al.(2025{\natexlab{b}})Zheng, Yang, Hong, Zhao, Xu, Yang, Shen, and Yu]{MLRM-mcot-deepeyes}
Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le~Yang, Chao Shen, and Xing Yu.
\newblock Deepeyes: Incentivizing" thinking with images" via reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2505.14362}, 2025{\natexlab{b}}.

\bibitem[Zhou et~al.(2025)Zhou, Ji, Chen, Sun, Chen, Hong, Han, Guo, and Yang]{safe-algin-G-RLHF-V}
Jiayi Zhou, Jiaming Ji, Boyuan Chen, Jiapeng Sun, Wenqi Chen, Donghai Hong, Sirui Han, Yike Guo, and Yaodong Yang.
\newblock Generative rlhf-v: Learning principles from multi-modal human preference.
\newblock \emph{arXiv preprint arXiv:2505.18531}, 2025.

\bibitem[Zhou et~al.(2024)Zhou, Liu, Zhao, Compalas, Song, and Wang]{mssbench}
Kaiwen Zhou, Chengzhi Liu, Xuandong Zhao, Anderson Compalas, Dawn Song, and Xin~Eric Wang.
\newblock Multimodal situational safety.
\newblock \emph{arXiv preprint arXiv:2410.06172}, 2024.

\bibitem[Zong et~al.(2024)Zong, Bohdal, Yu, Yang, and Hospedales]{vlguard}
Yongshuo Zong, Ondrej Bohdal, Tingyang Yu, Yongxin Yang, and Timothy Hospedales.
\newblock Safety fine-tuning at (almost) no cost: A baseline for vision large language models.
\newblock \emph{arXiv preprint arXiv:2402.02207}, 2024.

\end{thebibliography}
