@String { ACMMM        = {Proceedings of the ACM International Conference on Multimedia} }
@String { ARXIV        = {arXiv} }
@String { BMVC         = {Proceedings of the British Machine Vision Conference} }
@String { CVPR         = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition} }
@String { ECCV         = {Proceedings of the European Conference on Computer Vision.} }
@String { ICASSP       = {Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing} }
@String { ICCP         = {Proceedings of the IEEE International Conference on Computational Photography} }
@String { ICCV         = {Proceedings of the IEEE/CVF International Conference on Computer Vision} }
@String { ICIP         = {Proceedings of the IEEE International Conference on Image Processing} }
@String { ICLR         = {Proceedings of the International Conference on Learning Representations} }
@String { ICME         = {Proceedings of the IEEE International Conference on Multimedia and Expo} }
@String { ICML         = {Proceedings of the International Conference on Machine Learning} }
@String { ICPR         = {International Conference on Pattern Recognition} } 
@String { IJCV         = {Int. J. Comput. Vis.} }
@String { NIPS         = {Proceedings of the Conference on Neural Information Processing Systems} }
@String { NIPSW        = {Proceedings of the International Conference on Neural Information Processing Systems Workshop} }
@String { TPAMI         = {IEEE Transactions on Pattern Analysis and Machine Intelligence} }
@String { TNNLS        = {IEEE Transactions on Neural Networks and Learning Systems}}
@String { PR           = {Pattern Recognition} }
@String { TIP          = {IEEE Transactions on Image Processing} }
@String { TMM          = {IEEE Transactions on Multimedia} }
@String { TIT          = {IEEE Transactions on Information Theory} }
@String { WACV         = {Proceedings of the IEEE Winter Conference on Applications of Computer Vision}}
@String { SPL         = {IEEE Signal Processing Letters}}
@String { AAAI         = {Proceedings of the AAAI Conference on Artificial Intelligence}}
@String { IJCAI         = {Proceedings of the International Joint Conference on Artificial Intelligence}}
@String { WWW        = {Proceedings of the Web Conference}}
@String { TOIS        = {ACM Transactions on Information Systems}}
@String { TKDD        = {ACM Transactions on Knowledge Discovery from Data}}
@String { TKDE        = {IEEE Transactions on Knowledge and Data Engineering}}
@String { ICDE        = {Proceedings of the IEEE Conference on Data Engineering}}
@String { CIKM        = {Proceedings of the International Conference on Information and Knowledge Management}}
@String { TOMM        = {ACM Transactions on Multimedia Computing, Communications, and Applications}}
@String { SIGIR        = {Proceedings of the International ACM SIGIR Conference on Research \& Development in Information Retrieval}}
@String { KDD        = {Proceedings of the International ACM SIGKDD Conference on Knowledge Discovery \& Data Mining}}
@String { VLDB       = {Proceedings of the International Conference on Very Large Data Bases}}
@String { TCVST       = {IEEE Transactions on Circuits and Systems for Video Technology}}
@String { EMNLP       = {Proceedings of the Conference on Empirical Methods in Natural Language Processing }}
@String { JMLR       = {Journal of machine learning research}}
@String { SPLINE       = {International Workshop on Sensing, Processing and Learning for Intelligent Machines}}
%%%%%%  Base   %%%%%
@article{noukhovitch2023language,
  title={Language model alignment with elastic reset},
  author={Noukhovitch, Michael and Lavoie, Samuel and Strub, Florian and Courville, Aaron C},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={3439--3461},
  year={2023}
}

@article{borah2025alignment,
  title={Alignment Quality Index (AQI): Beyond Refusals: AQI as an Intrinsic Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise Pooled Representations},
  author={Borah, Abhilekh and Sharma, Chhavi and Khanna, Danush and Bhatt, Utkarsh and Singh, Gurpreet and Abdullah, Hasnat Md and Ravi, Raghav Kaushik and Jain, Vinija and Patel, Jyoti and Singh, Shubham and others},
  journal={arXiv preprint arXiv:2506.13901},
  year={2025}
}


@article{Base-cot,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}


@inproceedings{andreas2017modular,
  title={Modular multitask reinforcement learning with policy sketches},
  author={Andreas, Jacob and Klein, Dan and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={166--175},
  year={2017},
  organization={PMLR}
}

@article{wang2025safety,
  title={Safety in large reasoning models: A survey},
  author={Wang, Cheng and Liu, Yue and Bi, Baolong and Zhang, Duzhen and Li, Zhong-Zhi and Ma, Yingwei and He, Yufei and Yu, Shengju and Li, Xinfeng and Fang, Junfeng and others},
  journal={arXiv preprint arXiv:2504.17704},
  year={2025}
}

%%%%%%  MLLM   %%%%%
@article{MLLM-flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}


@article{MLLM-otter,
  title={Otter: A multi-modal model with in-context instruction tuning},
  author={Li, Bo and Zhang, Yuanhan and Chen, Liangyu and Wang, Jinghao and Pu, Fanyi and Cahyono, Joshua Adrian and Yang, Jingkang and Li, Chunyuan and Liu, Ziwei},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2025},
  publisher={IEEE}
}

@article{MLLM-gpt4o,
  title={Gpt-4o system card},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}


@article{MLLM-llama-ada,
  title={Llama-adapter v2: Parameter-efficient visual instruction model},
  author={Gao, Peng and Han, Jiaming and Zhang, Renrui and Lin, Ziyi and Geng, Shijie and Zhou, Aojun and Zhang, Wei and Lu, Pan and He, Conghui and Yue, Xiangyu and others},
  journal={arXiv preprint arXiv:2304.15010},
  year={2023}
}

@article{MLLM-cogvlm,
  title={Cogvlm: Visual expert for pretrained language models},
  author={Wang, Weihan and Lv, Qingsong and Yu, Wenmeng and Hong, Wenyi and Qi, Ji and Wang, Yan and Ji, Junhui and Yang, Zhuoyi and Zhao, Lei and XiXuan, Song and others},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={121475--121499},
  year={2024}
}




@article{MLLM-qwen2vl,
  title={Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}



@article{MLLM-qwen2.5vl,
  title={Qwen2. 5-vl technical report},
  author={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and others},
  journal={arXiv preprint arXiv:2502.13923},
  year={2025}
}


%%%%%%% MLRM %%%%%%%
@article{MLRM-o1,
  title={Openai o1 system card},
  author={Jaech, Aaron and Kalai, Adam and Lerer, Adam and Richardson, Adam and El-Kishky, Ahmed and Low, Aiden and Helyar, Alec and Madry, Aleksander and Beutel, Alex and Carney, Alex and others},
  journal={arXiv preprint arXiv:2412.16720},
  year={2024}
}



@article{MLRM-r1,
  title={DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Zhang, Ruoyu and Ma, Shirong and Bi, Xiao and others},
  journal={Nature},
  volume={645},
  number={8081},
  pages={633--638},
  year={2025},
  publisher={Nature Publishing Group UK London}
}


@article{MLRM-grpo,
  title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Y and others},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024}
}


@article{MLRM-math-lmmr1,
  title={Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl},
  author={Peng, Yingzhe and Zhang, Gongrui and Zhang, Miaosen and You, Zhiyuan and Liu, Jie and Zhu, Qipeng and Yang, Kai and Xu, Xingzhong and Geng, Xin and Yang, Xu},
  journal={arXiv preprint arXiv:2503.07536},
  year={2025}
}


@article{MLRM-math-eureka,
  title={Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning},
  author={Meng, Fanqing and Du, Lingxiao and Liu, Zongkai and Zhou, Zhixiang and Lu, Quanfeng and Fu, Daocheng and Shi, Botian and Wang, Wenhai and He, Junjun and Zhang, Kaipeng and others},
  journal={CoRR},
  year={2025}
}


@article{MLRM-math-ursa,
  title={Ursa: Understanding and verifying chain-of-thought reasoning in multimodal mathematics},
  author={Luo, Ruilin and Zheng, Zhuofan and Wang, Yifan and Ni, Xinzhe and Lin, Zicheng and Jiang, Songtao and Yu, Yiyao and Shi, Chufan and Chu, Ruihang and Zeng, Jin and others},
  journal={arXiv preprint arXiv:2501.04686},
  year={2025}
}

@article{MLRM-spatial-star,
  title={STAR-R1: Spatial TrAnsformation Reasoning by Reinforcing Multimodal LLMs},
  author={Li, Zongzhao and Ma, Zongyang and Li, Mingze and Li, Songyou and Rong, Yu and Xu, Tingyang and Zhang, Ziqi and Zhao, Deli and Huang, Wenbing},
  journal={arXiv preprint arXiv:2505.15804},
  year={2025}
}

@article{MLRM-per-perception,
  title={Perception-r1: Pioneering perception policy with reinforcement learning},
  author={Yu, En and Lin, Kangheng and Zhao, Liang and Yin, Jisheng and Wei, Yana and Peng, Yuang and Wei, Haoran and Sun, Jianjian and Han, Chunrui and Ge, Zheng and others},
  journal={arXiv preprint arXiv:2504.07954},
  year={2025}
}



@article{MLRM-per-vrft,
  title={Visual-rft: Visual reinforcement fine-tuning},
  author={Liu, Ziyu and Sun, Zeyi and Zang, Yuhang and Dong, Xiaoyi and Cao, Yuhang and Duan, Haodong and Lin, Dahua and Wang, Jiaqi},
  journal={arXiv preprint arXiv:2503.01785},
  year={2025}
}

@article{MLRM-per-visionr1,
  title={Vision-r1: Incentivizing reasoning capability in multimodal large language models},
  author={Huang, Wenxuan and Jia, Bohan and Zhai, Zijie and Cao, Shaosheng and Ye, Zheyu and Zhao, Fei and Xu, Zhe and Hu, Yao and Lin, Shaohui},
  journal={arXiv preprint arXiv:2503.06749},
  year={2025}
}

@article{MLRM-mcot-grit,
  title={GRIT: Teaching MLLMs to Think with Images},
  author={Fan, Yue and He, Xuehai and Yang, Diji and Zheng, Kaizhi and Kuo, Ching-Chen and Zheng, Yuting and Narayanaraju, Sravana Jyothi and Guan, Xinze and Wang, Xin Eric},
  journal={arXiv preprint arXiv:2505.15879},
  year={2025}
}

@article{MLRM-mcot-rex,
  title={Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning},
  author={Jiang, Qing and Chen, Xingyu and Zeng, Zhaoyang and Yu, Junzhi and Zhang, Lei},
  journal={arXiv preprint arXiv:2506.04034},
  year={2025}
}


@article{MLRM-mcot-deepeyes,
  title={DeepEyes: Incentivizing" Thinking with Images" via Reinforcement Learning},
  author={Zheng, Ziwei and Yang, Michael and Hong, Jack and Zhao, Chenxiao and Xu, Guohai and Yang, Le and Shen, Chao and Yu, Xing},
  journal={arXiv preprint arXiv:2505.14362},
  year={2025}
}


@article{MLRM-mcot-chain,
  title={Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL},
  author={Zhang, Xintong and Gao, Zhi and Zhang, Bofei and Li, Pengxiang and Zhang, Xiaowen and Liu, Yang and Yuan, Tao and Wu, Yuwei and Jia, Yunde and Zhu, Song-Chun and others},
  journal={arXiv preprint arXiv:2505.15436},
  year={2025}
}


@article{MLRM-reflect-srpo,
  title={Srpo: Enhancing multimodal llm reasoning via reflection-aware reinforcement learning},
  author={Wan, Zhongwei and Dou, Zhihao and Liu, Che and Zhang, Yu and Cui, Dongfei and Zhao, Qinjian and Shen, Hui and Xiong, Jing and Xin, Yi and Jiang, Yifan and others},
  journal={arXiv preprint arXiv:2506.01713},
  year={2025}
}

@article{MLRM-reflect-mulberry,
  title={Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search},
  author={Yao, Huanjin and Huang, Jiaxing and Wu, Wenhao and Zhang, Jingyi and Wang, Yibo and Liu, Shunyu and Wang, Yingjie and Song, Yuxin and Feng, Haocheng and Shen, Li and others},
  journal={arXiv preprint arXiv:2412.18319},
  year={2024}
}
@article{MLRM-reflect-virgo,
  title={Virgo: A Preliminary Exploration on Reproducing o1-like MLLM},
  author={Du, Yifan and Liu, Zikang and Li, Yifan and Zhao, Wayne Xin and Huo, Yuqi and Wang, Bingning and Chen, Weipeng and Liu, Zheng and Wang, Zhongyuan and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2501.01904},
  year={2025}
}

@article{MLRM-reflect-r3,
  title={Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning},
  author={Bensal, Shelly and Jamil, Umar and Bryant, Christopher and Russak, Melisa and Kamble, Kiran and Mozolevskyi, Dmytro and Ali, Muayad and AlShikh, Waseem},
  journal={arXiv preprint arXiv:2505.24726},
  year={2025}
}

@article{MLRM-reflect-vl-rethinker,
  title={Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning},
  author={Wang, Haozhe and Qu, Chao and Huang, Zuming and Chu, Wei and Lin, Fangzhen and Chen, Wenhu},
  journal={arXiv preprint arXiv:2504.08837},
  year={2025}
}


@article{MLRM-aug-visionmatters,
  title={Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math Reasoning},
  author={Li, Yuting and Wei, Lai and Zheng, Kaipeng and Huang, Jingyuan and Kong, Linghe and Sun, Lichao and Huang, Weiran},
  journal={arXiv preprint arXiv:2506.09736},
  year={2025}
}

@article{MLRM-aug-shareVL,
  title={R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO},
  author={Yao, Huanjin and Yin, Qixiang and Zhang, Jingyi and Yang, Min and Wang, Yibo and Wu, Wenhao and Su, Fei and Shen, Li and Qiu, Minghui and Tao, Dacheng and others},
  journal={arXiv preprint arXiv:2505.16673},
  year={2025}
}

@article{MLRM-aug-thinknot,
  title={Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models},
  author={Wang, Jiaqi and Lin, Kevin Qinghong and Cheng, James and Shou, Mike Zheng},
  journal={arXiv preprint arXiv:2505.16854},
  year={2025}
}

@article{MLRM-reward-got-r1,
  title={Got-r1: Unleashing reasoning capability of mllm for visual generation with reinforcement learning},
  author={Duan, Chengqi and Fang, Rongyao and Wang, Yuqing and Wang, Kun and Huang, Linjiang and Zeng, Xingyu and Li, Hongsheng and Liu, Xihui},
  journal={arXiv preprint arXiv:2505.17022},
  year={2025}
}

@article{MLRM-reward-sophiavl,
  title={SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward},
  author={Fan, Kaixuan and Feng, Kaituo and Lyu, Haoming and Zhou, Dongzhan and Yue, Xiangyu},
  journal={arXiv preprint arXiv:2505.17018},
  year={2025}
}

@article{MLRM-reward-pixel,
  title={Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning},
  author={Su, Alex and Wang, Haozhe and Ren, Weimin and Lin, Fangzhen and Chen, Wenhu},
  journal={arXiv preprint arXiv:2505.15966},
  year={2025}
}




@article{MLRM-model-skywork,
  title={Skywork-r1v3 technical report},
  author={Shen, Wei and Pei, Jiangbo and Peng, Yi and Song, Xuchen and Liu, Yang and Peng, Jian and Sun, Haofeng and Hao, Yunzhuo and Wang, Peiyu and Zhang, Jianhao and others},
  journal={arXiv preprint arXiv:2507.06167},
  year={2025}
}


@article{MLRM-model-2025kimi,
  title={Kimi-vl technical report},
  author={Team, Kimi and Du, Angang and Yin, Bohong and Xing, Bowei and Qu, Bowen and Wang, Bowen and Chen, Cheng and Zhang, Chenlin and Du, Chenzhuang and Wei, Chu and others},
  journal={arXiv preprint arXiv:2504.07491},
  year={2025}
}

@article{MLRM-model-glm4.5,
  title={Glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning},
  author={Hong, Wenyi and Yu, Wenmeng and Gu, Xiaotao and Wang, Guo and Gan, Guobing and Tang, Haomiao and Cheng, Jiale and Qi, Ji and Ji, Junhui and Pan, Lihang and others},
  journal={arXiv e-prints},
  pages={arXiv--2507},
  year={2025}
}

@misc{gpt4.1,
	author = {OpenAI},
	title = {GPT-4.1 Model Card},
	howpublished = {\url{https://platform.openai.com/docs/models/gpt-4.1}},
	year = {2025},
	note = {[Accessed 31-08-2025]},
}

@misc{gpt5,
  title        = {GPT-5 System Card},
  author       = {OpenAI},
  year         = {2025},
  howpublished = {\url{https://openai.com/index/gpt-5-system-card/}},
  note         = {Accessed 13-08-2025}
}


@article{comanici2025gemini,
  title={Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities},
  author={Comanici, Gheorghe and Bieber, Eric and Schaekermann, Mike and Pasupat, Ice and Sachdeva, Noveen and Dhillon, Inderjit and Blistein, Marcel and Ram, Ori and Zhang, Dan and Rosen, Evan and others},
  journal={arXiv preprint arXiv:2507.06261},
  year={2025}
}




@article{Data-align,
  title={Align anything: Training all-modality models to follow instructions with language feedback},
  author={Ji, Jiaming and Zhou, Jiayi and Lou, Hantao and Chen, Boyuan and Hong, Donghai and Wang, Xuyao and Chen, Wenqi and Wang, Kaile and Pan, Rui and Li, Jiahao and others},
  journal={arXiv preprint arXiv:2412.15838},
  year={2024}
}
%%%%%%%%%%%%%% Safe %%%%%%%%%%%%%%%%%%%
@article{safe-3h,
  title={A general language assistant as a laboratory for alignment},
  author={Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and others},
  journal={arXiv preprint arXiv:2112.00861},
  year={2021}
}

@article{safe-safemlrm,
  title={Safemlrm: Demystifying safety in multi-modal large reasoning models},
  author={Fang, Junfeng and Wang, Yukai and Wang, Ruipeng and Yao, Zijun and Wang, Kun and Zhang, An and Wang, Xiang and Chua, Tat-Seng},
  journal={arXiv preprint arXiv:2504.08813},
  year={2025}
}

@article{safe-risk-safety,
  title={Safety alignment should be made more than just a few tokens deep},
  author={Qi, Xiangyu and Panda, Ashwinee and Lyu, Kaifeng and Ma, Xiao and Roy, Subhrajit and Beirami, Ahmad and Mittal, Prateek and Henderson, Peter},
  journal={arXiv preprint arXiv:2406.05946},
  year={2024}
}

@article{safe-risk-aialign,
  title={Ai alignment: A comprehensive survey},
  author={Ji, Jiaming and Qiu, Tianyi and Chen, Boyuan and Zhang, Borong and Lou, Hantao and Wang, Kaile and Duan, Yawen and He, Zhonghao and Zhou, Jiayi and Zhang, Zhaowei and others},
  journal={arXiv preprint arXiv:2310.19852},
  year={2023}
}

@article{safe-risk-mllmguard,
  title={Mllmguard: A multi-dimensional safety evaluation suite for multimodal large language models},
  author={Gu, Tianle and Zhou, Zeyang and Huang, Kexin and Dandan, Liang and Wang, Yixu and Zhao, Haiquan and Yao, Yuanqi and Yang, Yujiu and Teng, Yan and Qiao, Yu and others},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={7256--7295},
  year={2024}
}



@article{safe-risk-red,
  title={Red teaming visual language models},
  author={Li, Mukai and Li, Lei and Yin, Yuwei and Ahmed, Masood and Liu, Zhenguang and Liu, Qi},
  journal={arXiv preprint arXiv:2401.12915},
  year={2024}
}



@inproceedings{safe-risk-figstep,
  title={Figstep: Jailbreaking large vision-language models via typographic visual prompts},
  author={Gong, Yichen and Ran, Delong and Liu, Jinyuan and Wang, Conglei and Cong, Tianshuo and Wang, Anyu and Duan, Sisi and Wang, Xiaoyun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={22},
  pages={23951--23959},
  year={2025}
}



@article{safe-risk-usb,
  title={USB: A Comprehensive and Unified Safety Evaluation Benchmark for Multimodal Large Language Models},
  author={Zheng, Baolin and Chen, Guanlin and Zhong, Hongqiong and Teng, Qingyang and Tan, Yingshui and Liu, Zhendong and Wang, Weixun and Liu, Jiaheng and Yang, Jian and Jing, Huiyun and others},
  journal={arXiv preprint arXiv:2505.23793},
  year={2025}
}


@article{safe-guard-vlmguard,
  title={VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven Prompt Optimization},
  author={Chen, Menglan and Pang, Xianghe and Dong, Jingjing and Wang, WenHao and Du, Yaxin and Chen, Siheng},
  journal={arXiv preprint arXiv:2504.12661},
  year={2025}
}

@article{safe-guard-rapguard,
  title={RapGuard: Safeguarding Multimodal Large Language Models via Rationale-aware Defensive Prompting},
  author={Jiang, Yilei and Tan, Yingshui and Yue, Xiangyu},
  journal={arXiv preprint arXiv:2412.18826},
  year={2024}
}

@inproceedings{safe-guard-adashield,
  title={Adashield: Safeguarding multimodal large language models from structure-based attack via adaptive shield prompting},
  author={Wang, Yu and Liu, Xiaogeng and Li, Yu and Chen, Muhao and Xiao, Chaowei},
  booktitle={European Conference on Computer Vision},
  pages={77--94},
  year={2024},
  organization={Springer}
}

@article{safe-guard-mllmprotector,
  title={Mllm-protector: Ensuring mllm's safety without hurting performance},
  author={Pi, Renjie and Han, Tianyang and Zhang, Jianshu and Xie, Yueqi and Pan, Rui and Lian, Qing and Dong, Hanze and Zhang, Jipeng and Zhang, Tong},
  journal={arXiv preprint arXiv:2401.02906},
  year={2024}
}

@inproceedings{safe-guard-immune,
  title={Immune: Improving safety against jailbreaks in multi-modal llms via inference-time alignment},
  author={Ghosal, Soumya Suvra and Chakraborty, Souradip and Singh, Vaibhav and Guan, Tianrui and Wang, Mengdi and Beirami, Ahmad and Huang, Furong and Velasquez, Alvaro and Manocha, Dinesh and Bedi, Amrit Singh},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={25038--25049},
  year={2025}
}

@article{safe-guard-coca,
  title={Coca: Regaining safety-awareness of multimodal large language models with constitutional calibration},
  author={Gao, Jiahui and Pi, Renjie and Han, Tianyang and Wu, Han and Hong, Lanqing and Kong, Lingpeng and Jiang, Xin and Li, Zhenguo},
  journal={arXiv preprint arXiv:2409.11365},
  year={2024}
}

@article{safe-guard-guardreasoner,
  title={Guardreasoner-vl: Safeguarding vlms via reinforced reasoning},
  author={Liu, Yue and Zhai, Shengfang and Du, Mingzhe and Chen, Yulin and Cao, Tri and Gao, Hongcheng and Wang, Cheng and Li, Xinfeng and Wang, Kun and Fang, Junfeng and others},
  journal={arXiv preprint arXiv:2505.11049},
  year={2025}
}

@inproceedings{safe-guard-esco,
  title={Eyes closed, safety on: Protecting multimodal llms via image-to-text transformation},
  author={Gou, Yunhao and Chen, Kai and Liu, Zhili and Hong, Lanqing and Xu, Hang and Li, Zhenguo and Yeung, Dit-Yan and Kwok, James T and Zhang, Yu},
  booktitle={European Conference on Computer Vision},
  pages={388--404},
  year={2024},
  organization={Springer}
}


@article{safe-guard-sia,
  title={SIA: Enhancing Safety via Intent Awareness for Vision-Language Models},
  author={Na, Youngjin and Jeong, Sangheon and Lee, Youngwan},
  journal={arXiv preprint arXiv:2507.16856},
  year={2025}
}

@article{safe-guard-eta,
  title={Eta: Evaluating then aligning safety of vision language models at inference time},
  author={Ding, Yi and Li, Bolian and Zhang, Ruqi},
  journal={arXiv preprint arXiv:2410.06625},
  year={2024}
}




@article{safe-algin-think,
  title={Think in Safety: Unveiling and Mitigating Safety Alignment Collapse in Multimodal Large Reasoning Model},
  author={Lou, Xinyue and Li, You and Xu, Jinan and Shi, Xiangyu and Chen, Chi and Huang, Kaiyu},
  journal={arXiv preprint arXiv:2505.06538},
  year={2025}
}

@article{safe-algin-saferlhfv,
  title={Safe RLHF-V: Safe Reinforcement Learning from Multi-modal Human Feedback},
  author={Ji, Jiaming and Chen, Xinyu and Pan, Rui and Zhang, Conghui and Zhu, Han and Li, Jiahao and Hong, Donghai and Chen, Boyuan and Zhou, Jiayi and Wang, Kaile and others},
  journal={arXiv preprint arXiv:2503.17682},
  year={2025}
}

@article{safe-algin-G-RLHF-V,
  title={Generative RLHF-V: Learning Principles from Multi-modal Human Preference},
  author={Zhou, Jiayi and Ji, Jiaming and Chen, Boyuan and Sun, Jiapeng and Chen, Wenqi and Hong, Donghai and Han, Sirui and Guo, Yike and Yang, Yaodong},
  journal={arXiv preprint arXiv:2505.18531},
  year={2025}
}

@article{safe-algin-ADPO,
  title={Adversary-Aware DPO: Enhancing Safety Alignment in Vision Language Models via Adversarial Training},
  author={Weng, Fenghua and Lou, Jian and Feng, Jun and Huang, Minlie and Wang, Wenjie},
  journal={arXiv preprint arXiv:2502.11455},
  year={2025}
}

@article{safe-algin-SafeVid,
  title={SafeVid: Toward Safety Aligned Video Large Multimodal Models},
  author={Wang, Yixu and Song, Jiaxin and Gao, Yifeng and Wang, Xin and Yao, Yang and Teng, Yan and Ma, Xingjun and Wang, Yingchun and Jiang, Yu-Gang},
  journal={arXiv preprint arXiv:2505.11926},
  year={2025}
}

@article{li2025system,
  title={From system 1 to system 2: A survey of reasoning large language models},
  author={Li, Zhong-Zhi and Zhang, Duzhen and Zhang, Ming-Liang and Zhang, Jiaxin and Liu, Zengyan and Yao, Yuxuan and Xu, Haotian and Zheng, Junhao and Wang, Pei-Jie and Chen, Xiuyi and others},
  journal={arXiv preprint arXiv:2502.17419},
  year={2025}
}

@article{huang2025vision,
  title={Vision-r1: Incentivizing reasoning capability in multimodal large language models},
  author={Huang, Wenxuan and Jia, Bohan and Zhai, Zijie and Cao, Shaosheng and Ye, Zheyu and Zhao, Fei and Xu, Zhe and Hu, Yao and Lin, Shaohui},
  journal={arXiv preprint arXiv:2503.06749},
  year={2025}
}


###################



#################### Intro
@article{liu2024safety,
  title={Safety of multimodal large language models on images and texts},
  author={Liu, Xin and Zhu, Yichen and Lan, Yunshi and Yang, Chao and Qiao, Yu},
  journal={arXiv preprint arXiv:2402.00357},
  year={2024}
}

@article{ye2025survey,
  title={A survey of safety on large vision-language models: Attacks, defenses and evaluations},
  author={Ye, Mang and Rong, Xuankun and Huang, Wenke and Du, Bo and Yu, Nenghai and Tao, Dacheng},
  journal={arXiv preprint arXiv:2502.14881},
  year={2025}
}





@inproceedings{mmsafetybench,
  title={Mm-safetybench: A benchmark for safety evaluation of multimodal large language models},
  author={Liu, Xin and Zhu, Yichen and Gu, Jindong and Lan, Yunshi and Yang, Chao and Qiao, Yu},
  booktitle={European Conference on Computer Vision},
  pages={386--403},
  year={2024},
  organization={Springer}
}

@inproceedings{spavl,
  title={SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Models},
  author={Zhang, Yongting and Chen, Lu and Zheng, Guodong and Gao, Yifeng and Zheng, Rui and Fu, Jinlan and Yin, Zhenfei and Jin, Senjie and Qiao, Yu and Huang, Xuanjing and others},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={19867--19878},
  year={2025}
}

@article{vlguard,
  title={Safety fine-tuning at (almost) no cost: A baseline for vision large language models},
  author={Zong, Yongshuo and Bohdal, Ondrej and Yu, Tingyang and Yang, Yongxin and Hospedales, Timothy},
  journal={arXiv preprint arXiv:2402.02207},
  year={2024}
}


@article{siuo,
  title={Safe Inputs but Unsafe Output: Benchmarking Cross-modality Safety Alignment of Large Vision-Language Model},
  author={Wang, Siyin and Ye, Xingsong and Cheng, Qinyuan and Duan, Junwen and Li, Shimin and Fu, Jinlan and Qiu, Xipeng and Huang, Xuanjing},
  journal={arXiv preprint arXiv:2406.15279},
  year={2024}
}

@article{mssbench,
  title={Multimodal situational safety},
  author={Zhou, Kaiwen and Liu, Chengzhi and Zhao, Xuandong and Compalas, Anderson and Song, Dawn and Wang, Xin Eric},
  journal={arXiv preprint arXiv:2410.06172},
  year={2024}
}

@article{mdit,
  title={MDIT-Bench: Evaluating the Dual-Implicit Toxicity in Large Multimodal Models},
  author={Jin, Bohan and Qi, Shuhan and Chen, Kehai and Guo, Xinyi and Wang, Xuan},
  journal={arXiv preprint arXiv:2505.17144},
  year={2025}
}

@article{MMSafeAware,
  title={Can't See the Forest for the Trees: Benchmarking Multimodal Safety Awareness for Multimodal LLMs},
  author={Wang, Wenxuan and Liu, Xiaoyuan and Gao, Kuiyi and Huang, Jen-tse and Yuan, Youliang and He, Pinjia and Wang, Shuai and Tu, Zhaopeng},
  journal={arXiv preprint arXiv:2502.11184},
  year={2025}
}


@article{lingshu,
  title={Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning},
  author={Xu, Weiwen and Chan, Hou Pong and Li, Long and Aljunied, Mahani and Yuan, Ruifeng and Wang, Jianyu and Xiao, Chenghao and Chen, Guizhen and Liu, Chaoqun and Li, Zhaodonghui and others},
  journal={arXiv preprint arXiv:2506.07044},
  year={2025}
}

@article{cello1,
  title={Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning},
  author={Fang, Yin and Jin, Qiao and Xiong, Guangzhi and Jin, Bowen and Zhong, Xianrui and Ouyang, Siru and Zhang, Aidong and Han, Jiawei and Lu, Zhiyong},
  journal={arXiv preprint arXiv:2506.02911},
  year={2025}
}


@article{kumar2024training,
  title={Training language models to self-correct via reinforcement learning},
  author={Kumar, Aviral and Zhuang, Vincent and Agarwal, Rishabh and Su, Yi and Co-Reyes, John D and Singh, Avi and Baumli, Kate and Iqbal, Shariq and Bishop, Colton and Roelofs, Rebecca and others},
  journal={arXiv preprint arXiv:2409.12917},
  year={2024}
}