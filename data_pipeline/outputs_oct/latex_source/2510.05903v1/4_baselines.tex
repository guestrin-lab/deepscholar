\section{Benchmark}
\label{sec:benchmark}

To demonstrate the challenge posed by our dataset and establish baseline performance, we evaluate various state-of-the-art models. We define four distinct evaluation scenarios, based on whether an approach uses the training data, the reference images, none or both. We choose methods in such a way to cover a wide variety of relevant approaches, favoring established and widely adopted methods over their latest variants.

\subsection{Evaluation Metrics}
\label{subsec:metrics}
To compare the different baselines, we formulate the task as a binary classification problem between \emph{no defect} and \emph{any defect} (\ie defect severity minor or major). Formally, we evaluate a classifier
$f(
    \textbf{x}_\textrm{q}^\textrm{ID},
    \{
        \textbf{x}_\textrm{ref}^{\textrm{ID}(1)},
        \ldots,
        \textbf{x}_\textrm{ref}^{\textrm{ID}(G_\textrm{ID})}
    \}
)
= \hat{y}$
given a (labeled) test query image $(\textbf{x}_\textrm{q}^\textrm{ID}, y) \in \mathcal{D}_\textrm{q}^\textrm{test}$ with item identifier ID and binary defect label $y \in \{0, 1\}$ (0 = non-defective, 1 = defective), and a set of $G_\textrm{ID}$ corresponding (unlabeled) test reference images with the same item identifier from $\mathcal{D}_\textrm{ref}^\textrm{test}$. Note that both the number of query and reference images per item identifier varies, and also that models (\eg the methods listed in Section~\ref{subsec:with-training-no-reference}) may ignore reference images altogether.

Apart from the query and reference test sets, our dataset also comprises equivalent subset pairs for model training $\mathcal{D}_\textrm{q}^\textrm{train}$, $\mathcal{D}_\textrm{ref}^\textrm{train}$ and validation $\mathcal{D}_\textrm{q}^\textrm{valid}$, $\mathcal{D}_\textrm{ref}^\textrm{valid}$. The training datasets are used by the supervised and combined methods, while the validation datasets are used for hyperparameter tuning and decision threshold selection.

To compare different models $f$, we use Average Precision (AP) on \textbf{any} (minor or major) defect (\textbf{AP\textsubscript{any}}), as our key performance metric, and additional auxiliary metrics:
\begin{itemize}
    \item Average Precision (AP) on major defect (\textbf{AP\textsubscript{major}}), computed only on the subset of either non-defective or items with major defects,
    \item Area under Receiver Operator Characteristic (\textbf{AUROC}),
    \item Recall at 50\% Precision (\textbf{R@50\%P}), and
    \item Recall at 1\% False Positive Rate (\textbf{R@1\%FPR}).
\end{itemize}
Some methods perform better on full images and others on item-cropped images. For the sake of brevity, we report numbers only for the best-performing variant of each method, indicating what type of image is used in Table~\ref{tab:num_results}.

\subsection{Scenarios}
\label{subsec:scenarios}

\begin{table*}[!ht]
\centering
\begin{tabular}{rcccccc}
    \textbf{Baseline} & \textbf{Tray/Item} & \textbf{AP\textsubscript{any}} [\%] & \textbf{AP\textsubscript{major}} [\%] & \textbf{AUROC} & \textbf{R@50\%P} [\%] & \textbf{R@1\%FPR} [\%] \\ \hline \hline 
%---------
\texttt{\textit{Random}} & - & 31.84 & 14.00 & 50.00 & 0.00 & 1.08 \\
\hline
\multicolumn{7}{c}{\textbf{No training, no references} (zero-shot, few-shot)}\\ 
\hline
\texttt{CLIP} & item & 36.20 & 17.15 & 56.05 & \textbf{0.56} & \textbf{1.53} \\
\texttt{POMP} & item  & 32.98 & 18.17 & 50.44 & 0.00 & 1.28 \\
\texttt{WinCLIP-zero} & item & 33.87 & 19.11 & 52.30 & 0.03 & 1.37 \\ 
\texttt{Claude-icl} & tray & \textbf{36.57} & \textbf{24.76} & \textbf{56.96} & 0.00 & 0.31 \\
\texttt{Pixtral-zero} & tray & 32.75 & 16.42 & 50.93 & 0.00 & 0.81 \\
\texttt{Pixtral-icl} & tray & 32.18 & 15.83 & 50.86 & 0.00 & 0.69 \\
\hline
\multicolumn{7}{c}{\textbf{No training, \emph{with} references} (few-shot, non-parametric, in-context learning)} \\ 
\hline
\texttt{PatchCore50} & item & \textbf{35.86} & 17.80 & \textbf{54.69} & \textbf{2.46} & \textbf{2.18} \\ 
\texttt{WinCLIP-few} & item & 34.05 & \textbf{19.29} & 52.41 & 0.66 & 1.56 \\
\hline
\multicolumn{7}{c}{\textbf{\emph{With} training, no references} (supervised/instruction fine-tuning)}\\ 
\hline
\texttt{ResNet50} & tray & 81.06 & 74.93 & 88.36 & 91.98 & 30.01 \\ 
\texttt{ViT-S}  & tray  & \textbf{90.67} & \textbf{91.45} & \textbf{94.27} & \textbf{97.69} & \textbf{59.36} \\
\texttt{Pixtral-ft} & tray & 33.43 & 17.19 & 51.44 & 3.62 & 3.62 \\ 
\texttt{AutoGluonMM}    & item & 87.77 & 86.10 & 92.47 & 96.76 & 46.26 \\ 
\hline
\multicolumn{7}{c}{\textbf{\emph{With} training, \emph{with} references} (supervised with references, non-parametric with fine-tuning)}\\ 
\hline
\texttt{PatchCore50-ft} & item & 40.18 & 20.98 & 60.14 & 6.52 & 2.37 \\
\texttt{AutoGluonMM-ref}    & item & \textbf{71.21} & \textbf{61.45} & \textbf{84.29} & \textbf{89.83} & \textbf{13.32} \\ 
\hline \hline
\end{tabular}
% }
\caption{Results of the evaluated baseline methods on the test set split with 10067 total samples (minor defect: 2089, major defect: 1117).
}
\label{tab:num_results}
\end{table*}

We present four distinct evaluation scenarios that each explore unique aspects of our dataset.

(1) \emph{No training and no reference images}. Such approaches are commonly referred to as zero shot, leveraging strong general-purpose vision-language models, here
CLIP~\citep{pmlr-v139-radford21a}, Claude~\citep{claude-model-card}, and Pixtral~\citep{agrawal2024pixtral12b}.

(2) \emph{No training and with reference images}. Here, models have no access to the labeled training set but they leverage (unlabeled) reference images at test time.
In the anomaly detection (AD) context, this approach is commonly referred to as few-shot AD~\citep{yi2020patch} or few-normal-shot AD~\citep{jeong2023winclip}.

(3) \emph{With training and without reference images}. These are purely supervised methods~\citep{resnet2016,dosovitskiy_vit_2020} that leverage the annotated training data to train a binary classifier, ignoring the reference datasets.

(4) \emph{With training and with reference images}. These methods combine approaches (2) and (3) by both training a model (backbone) and leveraging reference images.

Next, we detail the methods we evaluate as part of each of the four scenarios. All model and training details required to replicate the results are provided in the Supplementary Material~\ref{sec:model_training_details}.
Note that some methods can be applied in multiple scenarios, as we point out in the following.

\subsubsection{No Training and No Reference Images}
\label{subsec:no-training-no-reference}

In this scenario, we test whether and to which extent strong general-purpose image understanding capabilities translate to zero-shot defect detection performance.

\myparagraph{CLIP.} We test the vanilla CLIP model~\cite{pmlr-v139-radford21a} (\texttt{CLIP}), and CLIP with fine-tuned prompts~\citep{ren2023prompt} (\texttt{POMP}). For vanilla CLIP, we perform manual prompt optimization on the validation set, and ended up with the following prompts for classification: \texttt{Image of an item without problems} and \texttt{Image of an item with problems}, for non-defective and defective samples, respectively. For POMP, we use the labels \texttt{undamaged} and \texttt{damaged} for the respective classes.

\myparagraph{WinCLIP.} WinCLIP \cite{jeong2023winclip} extends the original CLIP model for anomaly detection, by (1) providing a diverse set of text prompts representing defective and healthy samples, and (2) using multi-scale image feature extraction and comparison. In the zero-shot setting, WinCLIP only uses query image and text prompts (\texttt{WinCLIP-zero}).

\myparagraph{Claude.} We evaluate Anthropic's Claude 3.5 Sonnet~\citep{claude-model-card}, a public Vision-Language Model (VLM), in the zero-shot setting in two ways.
\texttt{Claude-zero} evaluates the model when provided with a text prompt and the query image, and ask the model to inspect the image for defects using Chain-of-Thought and finally grade the defect severity on a scale from 0 to 10. We tested different prompts on a small held-out set, and apply the best-performing prompt to the entire dataset (see Supplementary Material \ref{sec:vlm_prompts}). In the second setting (\texttt{Claude-icl}), we apply the few-shot in-context learning (ICL) scenario, where we additionally provide five samples as positive \emph{defective} classes with respective example answers. Due to computational constraints, prompt images are rescaled to $512 \times 512$.

\myparagraph{Pixtral.} In addition to Claude, we evaluate the recent open-source Pixtral-12B model \cite{agrawal2024pixtral12b} as another VLM on our dataset. Similarly, we evaluate both a zero-shot (\texttt{Pixtral-zero}), and an in-context learning (\texttt{Pixtral-icl}) setting.
The best-performing prompts can be found in the Supplementary Material \ref{sec:vlm_prompts}.

\subsubsection{No Training and With Reference Images}
\label{subsec:no-training-with-reference}

We evaluate two AD approaches that leverage reference images of known item categories at test time.

\myparagraph{PatchCore}~\cite{roth2022towards} is a state-of-the-art anomaly detection method that leverages patch-level features from an image, comparing each patch's feature to a memory bank of normal patches and identifying anomalous samples through patch-level feature distance. The image-level anomalous score is computed as the maximum of the patch-level anomaly scores across all patches in the image. We use the reference test set to create the individual memory banks of features for different items and to compute the anomaly score. As a backbone for feature extraction, we test ResNet50 pretrained on ImageNet (\texttt{PatchCore50}).

\myparagraph{WinCLIP.} We test WinCLIP in the few-shot setting, by enabling it to perform visual feature comparison with reference images (\texttt{WinCLIP-few}).

\subsubsection{With Training and No Reference Images}
\label{subsec:with-training-no-reference}

Here, we focus on common supervised methods that leverage the annotated training data to learn how to recognize the appearance of visual defects. To do so, we fine-tune two common types image backbones for defect classification using a binary cross entropy (BCE) loss, feeding only the query images as input.

\myparagraph{Convolutional Networks}. We fine-tune a ResNet50 model~\citep{resnet2016} backbone, pretrained on ImageNet \cite{imagenet2009}, on our training data  (\texttt{ResNet50}).
Preliminary experiments demonstrated that a high resolution is necessary to prevent subtle or small defects from being obscured or lost due to downsampling, and we thus use a resolution of $1024 \times 1024$ pixels. Training is conducted for 20 epochs with an initial learning rate of $5 \times 10^{-5}$ and batch size 48.

\myparagraph{Vision Transformers}. We fine-tune a ViT-small pretrained on DINOv2 \cite{oquab2024dinov} with patch size $14 \times 14$ px \cite{dosovitskiy2021imageworth16x16words} at $1024 \times 1024$ px for 30 epochs, with an initial learning rate of $5 \times 10^{-6}$ and batch size 8 (\texttt{ViT-S}). Additionally, we test an AutoML approach using the AutoGluon MultiModal framework \cite{tang2024autogluon} (\texttt{AutoGluonMM}).

\myparagraph{Pixtral fine-tuned}. In addition to the zero-shot and few-shot variants of Pixtral, we \emph{instruct-finetuned} the model on question-answer pairs from our dataset in order to adapt the model to our domain (\texttt{Pixtral-ft}). We run LoRA fine-tuning for one epoch on 10,000 samples from the training set with a fixed learning rate of $3 \times 10^{-5}$.

\subsubsection{With Training and With Reference Images}
\label{subsec:with-training-with-reference}

Finally, we study whether access to both training data and reference images improves performance.

\myparagraph{PatchCore with a fine-tuned backbone}. We test PatchCore as explained in Section~\ref{subsec:no-training-with-reference}, but replace the ResNet50 backbone fine-tuned on ImageNet with a ResNet50 backbone fine-tuned on our training dataset from Section~\ref{subsec:with-training-no-reference} (\texttt{PatchCore50-ft}) similar to~\cite{koshil2024apc}.

\myparagraph{AutoGluonMM}. To handle both query and reference images of the same item at train and test time, we use AutoGluon MultiModal~\cite{tang2024autogluon} by passing  all images (query \emph{and} references) for each sample through the same image backbone and averaging their respective embeddings to obtain the final representation  (\texttt{AutoGluon-ref}).

\subsection{Results}

We summarize the results of all baseline methods in
Tables~\ref{tab:num_results} and~\ref{tab:reduce_results}, with a detailed error analysis provided in the Supplementary Material~\ref{sec:error_analysis}. Our experiments aim to answer the following questions. (1) How well do methods with access to (all) defective instances at training time perform? (2) How does performance deteriorate when fewer defective instances are available for training? (3) How well do unsupervised and anomaly detection methods without access to defective instances for training perform?

\myparagraph{(1) Upper bound with access to a large number of defective instances at training time}.
The supervised baselines perform well, with \texttt{ViT-S} reaching 90.67\% \textbf{AP\textsubscript{any}}. While models effectively detect major defects like deconstructions, penetrations, and deformations, they struggle with subtle anomalies, rare defect types (spillage), and reference-dependent defects (missing unit). False positives primarily occur with oddly-shaped items and ``adversarial'' items featuring damage-like designs. Notably, methods using both training data and references (\texttt{PatchCore50-ft} and \texttt{AutoGluonMM-ref}) underperform compared to reference-free approaches. This suggests that naive reference usage actually hinders model performance, likely due to feature averaging across input images complicating the learning task. This hypothesis is supported by inspecting their training set performance (96\% \textbf{AP\textsubscript{any}} without references versus 87\% with).

\myparagraph{(2) Reduced access to defective instances at training time}.
Table~\ref{tab:reduce_results} shows how supervised baselines perform in a more realistic scenario with limited number of defective training samples, with only 1\% defective rate in the training set. Unsurprisingly, performance drops significantly from 90.67\% \textbf{AP\textsubscript{any}} to 57.7\% \textbf{AP\textsubscript{any}} for fully supervised methods (\texttt{Query only}). As before, the model is not able to leverage the non-defective samples (\texttt{Query + ref}).

\myparagraph{(3) No defective instances at training time}. No method without training surpasses 36.57\% \textbf{AP\textsubscript{any}} (\texttt{Claude-icl}), with both zero-shot/in-context learning models (\texttt{CLIP}/\texttt{POMP}, \texttt{Pixtral-*}) and anomaly detection models (\texttt{PatchCore50}, \texttt{WinCLIP-*}) performing only slightly above chance.
Out of the CLIP-based approaches the original CLIP model performs best. We find that the model seems to occasionally read the text on the items and wrongly associates it with defect predictions.
VLMs provide a reasonable overall description and can catch egregious defects like gross deconstruction, but fail to capture the intricacy and variety of minor defects concerning deformable items, stickers/dirt on trays, and subtle anomalies, corroborating previous findings by \cite{jiang2025mmad}. Anomaly detection methods latch on to non-defect related visual differences, such as novel poses/viewpoints, background noise, and packaging variations.

Interestingly, PatchCore with a fine-tuned ResNet50 backbone (\texttt{PatchCore50-ft}) shows 4.32 ppts improvement compared to the ImageNet-based model \texttt{PatchCore50}, indicating the usefulness of leveraging defective instances for representation learning in anomaly detection~\cite{koshil2024apc}. However, it still struggles in detecting minor actuation and deconstruction, particularly when items are slightly displaced of their packaging. Moreover, some false negatives stem from faulty reference images incorrectly assumed to be non-defective, highlighting the extra pre-caution required in using unlabeled reference data in anomaly detection. False positives mostly arise from visual disparities between test and reference images, including variations in pose and product appearance.
These results highlight the need for improved anomaly detection methods with a more thorough understanding of defects and more sophisticated ways for using references for visual comparison.

\begin{table}[tb]
\centering

\begin{tabular}{rccc}
    \textbf{Input} & \textbf{AP\textsubscript{any}} [\%] & \textbf{AP\textsubscript{major}} [\%] & \textbf{AUROC}  \\ \hline

    \texttt{Query only} & 57.7 & 40.5 & 74.4 \\
    \texttt{Query + ref} & 40.4 & 14.9 & 63.2 \\
    \hline
\end{tabular}

\caption{Classification performance on a reduced training set, with a defect rate of only 1\%. We compare a ViT using only query images and a late-fusion ViT using both query \emph{and} reference images.}
\vspace{-.75em}
\label{tab:reduce_results}
\end{table}

In summary, supervised methods perform best when given access to large amounts of defective instances during training, but still struggle with edge cases such as deformable and adversarial items. Adding reference images naively degrades rather than improves performance. Unsupervised and anomaly detection methods fall short by a significant margin, but improve with access to training data.