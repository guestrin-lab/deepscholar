@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(ICML = {Int. Conf. Mach. Learn.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(ICML  = {ICML})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})


@inproceedings{zhong2020random,
  title={Random erasing data augmentation},
  author={Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={07},
  pages={13001--13008},
  year={2020}
}


@inproceedings{cubuk2020randaugment,
  title={Randaugment: Practical automated data augmentation with a reduced search space},
  author={Cubuk, Ekin D and Zoph, Barret and Shlens, Jonathon and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops},
  pages={702--703},
  year={2020}
}


@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@inproceedings{yun2019cutmix,
  title={Cutmix: Regularization strategy to train strong classifiers with localizable features},
  author={Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={6023--6032},
  year={2019}
}

@inproceedings{touvron2021training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International conference on machine learning},
  pages={10347--10357},
  year={2021},
  organization={PMLR}
}

@inproceedings{caron2021emerging,
  title={Emerging properties in self-supervised vision transformers},
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={9650--9660},
  year={2021}
}

@inproceedings{he2020momentum,
  title={Momentum contrast for unsupervised visual representation learning},
  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9729--9738},
  year={2020}
}

@article{chen2020improved,
  title={Improved baselines with momentum contrastive learning},
  author={Chen, Xinlei and Fan, Haoqi and Girshick, Ross and He, Kaiming},
  journal={arXiv preprint arXiv:2003.04297},
  year={2020}
}


@inproceedings{hoefer_towards_2022,
	title = {Towards {A}utomated {V}isual {P}roduct {D}amage {D}etection at {A}mazon's {F}ulfillment {C}enters},
	author = {H\"ofer, Sebastian and Sorgi, Lorenzo and Henning, Dorian  and Meyer, Leslie and Lapin, Maksim and Amiranashvili, Artemij},
	month = aug,
	year = {2022},
}

@inproceedings{milan_product_2020,
	title = {{P}roduct {I}dentification {B}eyond {B}arcodes},
	journal = { AMLC Workshop on ASIN Contents Embeddings, Clusters and Applications},
	author = {Milan, Anton and Lapin, Maksim and Sorgi, Lorenzo and Antonakos, Nontas},
	year = {2020},
}

@inproceedings{he_deep_2016,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  pages={770-778},
  doi={10.1109/CVPR.2016.90}
}


@article{hughes_package_2023,
title = {Package Damage DSI: Uncertainty, Systematics, and Recommendations},
author={Craker, Mallory and Megler, Veronika},
}


@article{hora_aleatory_1996,
title = {Aleatory and epistemic uncertainty in probability elicitation with an example from hazardous waste management},
journal = {Reliability Engineering and System Safety},
volume = {54},
number = {2},
pages = {217-223},
year = {1996},
note = {Treatment of Aleatory and Epistemic Uncertainty},
issn = {0951-8320},
doi = {https://doi.org/10.1016/S0951-8320(96)00077-4},
url = {https://www.sciencedirect.com/science/article/pii/S0951832096000774},
author = {Stephen C. Hora},
abstract = {The quantification of a risk assessment model often requires the elicitation of expert judgments about quantities that cannot be precisely measured. The aims of the model being quantified provide important guidance as to the types of questions that should be asked of the experts. The uncertainties underlying a quantity may be classified as aleatory or epistemic according to the goals of the risk process. This paper discusses the nature of such a classification and how it affects the probability elicitation process and implementation of the resulting judgments. Examples from various areas of risk assessment are used to show the practical implications of how uncertainties are treated. An extended example from hazardous waste disposal is given.}
}

@article{zhuang_convnet_2022,
  author       = {Zhuang Liu and
                  Hanzi Mao and
                  Chao{-}Yuan Wu and
                  Christoph Feichtenhofer and
                  Trevor Darrell and
                  Saining Xie},
  title        = {A ConvNet for the 2020s},
  journal      = {CoRR},
  volume       = {abs/2201.03545},
  year         = {2022},
  url          = {https://arxiv.org/abs/2201.03545},
  eprinttype    = {arXiv},
  eprint       = {2201.03545},
  timestamp    = {Thu, 20 Jan 2022 14:21:35 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2201-03545.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{shi_improvement_2021,
	title = {Improvement of {Damage} {Segmentation} {Based} on {Pixel}-{Level} {Data} {Balance} {Using} {VGG}-{Unet}},
	volume = {11},
	doi = {10.3390/app11020518},
	abstract = {In this research, 200 corrosion images of steel and 500 crack images of rubber bearing are collected and manually labeled to build the data set. Then the two data sets are respectively adopted to train VGG-Unet models in two methods, aiming to conduct Damage Segmentation by inputting different size of data set. One method is Squashing Segmentation to input squashed images from high resolution directly into VGG-Unet model while Cropping Segmentation uses cropped image with size 224 × 224 as input images. Because the proportion of damage pixels in the data set is different, the results produced by the two data sets are quite different. For large size damage (such as corrosion) segmentation, Cropping Segmentation has a better result while for minor damage (such as crack) segmentation, the result is opposite. The main reason is the gap in the concentration of valid data from the data set. To improve the capability of crack segmentation based on Cropping Segmentation, Background Data Drop Rate (BDDR) is adopted to reduce the quantity of background images to control the proportion of damage pixels from the data set in pixel-level. The ratio of damage pixels from the data set can be decided by different value of BDDR. By testing, the accuracy of Cropping Segmentation becomes relatively higher under BDDR being 0.8.},
	journal = {Applied Sciences},
	author = {Shi, Jiyuan and Dang, Ji and Cui, Mida and Zuo, Rongzhi and Shimizu, Kazuhiro and Tsunoda, Akira and Suzuki, Yasuhiro},
	month = jan,
	year = {2021},
	pages = {pp.518.1--17},
	file = {Shi et al. - 2021 - Improvement of Damage Segmentation Based on Pixel-.pdf:/Users/hoefersh/Zotero/storage/7XKCZ5PE/Shi et al. - 2021 - Improvement of Damage Segmentation Based on Pixel-.pdf:application/pdf},
}

@article{rubio_multi-class_2019,
	title = {Multi-class structural damage segmentation using fully convolutional networks},
	volume = {112},
	issn = {0166-3615},
	url = {https://www.sciencedirect.com/science/article/pii/S0166361518308911},
	doi = {10.1016/j.compind.2019.08.002},
	abstract = {Structural Health Monitoring (SHM) has benefited from computer vision and more recently, Deep Learning approaches, to accurately estimate the state of deterioration of infrastructure. In our work, we test Fully Convolutional Networks (FCNs) with a dataset of deck areas of bridges for damage segmentation. We create a dataset for delamination and rebar exposure that has been collected from inspection records of bridges in Niigata Prefecture, Japan. The dataset consists of 734 images with three labels per image, which makes it the largest dataset of images of bridge deck damage. This data allows us to estimate the performance of our method based on regions of agreement, which emulates the uncertainty of in-field inspections. We demonstrate the practicality of FCNs to perform automated semantic segmentation of surface damages. Our model achieves a mean accuracy of 89.7\% for delamination and 78.4\% for rebar exposure, and a weighted F1 score of 81.9\%.},
	language = {en},
	urldate = {2021-06-24},
	journal = {Computers in Industry},
	author = {Rubio, Juan Jose and Kashiwa, Takahiro and Laiteerapong, Teera and Deng, Wenlong and Nagai, Kohei and Escalera, Sergio and Nakayama, Kotaro and Matsuo, Yutaka and Prendinger, Helmut},
	month = nov,
	year = {2019},
	keywords = {Deep learning, Bridge damage detection, Semantic segmentation},
	pages = {103121},
	file = {Rubio et al. - 2019 - Multi-class structural damage segmentation using f.pdf:/Users/hoefersh/Zotero/storage/GTLT29SG/Rubio et al. - 2019 - Multi-class structural damage segmentation using f.pdf:application/pdf;ScienceDirect Snapshot:/Users/hoefersh/Zotero/storage/MZM7RD7I/S0166361518308911.html:text/html},
}

@article{cohen_transformer-based_2020,
	title = {Transformer-{Based} {Anomaly} {Segmentation}},
	url = {http://arxiv.org/abs/2005.02357},
	abstract = {The recent improvement in anomaly detection methods has prompted research into anomaly segmentation i.e. finding the pixels of the image that contain anomalies. In this paper, we investigate novel methods for unleashing the full power of pretrained features for anomaly segmentation. We first present a simple baseline that uses a pyramid of deep convolutional features and show that it significantly improves over the state-of-the-art methods, which are much more complex. One issue with the baseline approach is that it is unable to use the global context of the image effectively. We show that global attention-based methods are better able to utilize the global context. Specifically, we present an approach based on a multi-scale transformer architecture and show that it further improves performance. By analysing the attention maps, we find that they often detect anomalous image regions in a zero-shot fashion, providing some insight into the result. A qualitative evaluation of our method shows significant gains.},
	urldate = {2021-01-06},
	journal = {arXiv:2005.02357 [cs]},
	author = {Cohen, Niv and Hoshen, Yedid},
	month = dec,
	year = {2020},
	note = {arXiv: 2005.02357},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/hoefersh/Zotero/storage/TD5IKCR3/2005.html:text/html;Cohen_Hoshen_2020_Transformer-Based Anomaly Segmentation.pdf:/Users/hoefersh/Zotero/storage/KFWVJAWJ/Cohen_Hoshen_2020_Transformer-Based Anomaly Segmentation.pdf:application/pdf},
}

@article{zhang_vehicle-damage-detection_2020,
	title = {Vehicle-{Damage}-{Detection} {Segmentation} {Algorithm} {Based} on {Improved} {Mask} {RCNN}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.2964055},
	abstract = {Traffic congestion due to vehicular accidents seriously affects normal travel, and accurate and effective mitigating measures and methods must be studied. To resolve traffic accident compensation problems quickly, a vehicle-damage-detection segmentation algorithm based on transfer learning and an improved mask regional convolutional neural network (Mask RCNN) is proposed in this paper. The experiment first collects car damage pictures for preprocessing and uses Labelme to make data set labels, which are divided into training sets and test sets. The residual network (ResNet) is optimized, and feature extraction is performed in combination with Feature Pyramid Network (FPN). Then, the proportion and threshold of the Anchor in the region proposal network (RPN) are adjusted. The spatial information of the feature map is preserved by bilinear interpolation in ROIAlign, and different weights are introduced in the loss function for different-scale targets. Finally, the results of self-made dedicated dataset training and testing show that the improved Mask RCNN has better Average Precision (AP) value, detection accuracy and masking accuracy, and improves the efficiency of solving traffic accident compensation problems.},
	journal = {IEEE Access},
	author = {Zhang, Qinghui and Chang, Xianing and Bian, Shanfeng Bian},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {Accidents, Automobiles, detection accuracy, Feature extraction, Image segmentation, loss function, Mask RCNN, Object detection, Prediction algorithms, Training, vehicle-damage-detection},
	pages = {6997--7004},
	file = {IEEE Xplore Abstract Record:/Users/hoefersh/Zotero/storage/2HIF2FYJ/8950115.html:text/html;Zhang et al. - 2020 - Vehicle-Damage-Detection Segmentation Algorithm Ba.pdf:/Users/hoefersh/Zotero/storage/G33HZVW7/Zhang et al. - 2020 - Vehicle-Damage-Detection Segmentation Algorithm Ba.pdf:application/pdf},
}

@article{bergmann_mvtec_2021,
	title = {The {MVTec} {Anomaly} {Detection} {Dataset}: {A} {Comprehensive} {Real}-{World} {Dataset} for {Unsupervised} {Anomaly} {Detection}},
	volume = {129},
	issn = {0920-5691, 1573-1405},
	shorttitle = {The {MVTec} {Anomaly} {Detection} {Dataset}},
	url = {http://link.springer.com/10.1007/s11263-020-01400-4},
	doi = {10.1007/s11263-020-01400-4},
	abstract = {The detection of anomalous structures in natural image data is of utmost importance for numerous tasks in the ﬁeld of computer vision. The development of methods for unsupervised anomaly detection requires data on which to train and evaluate new approaches and ideas. We introduce the MVTec anomaly detection dataset containing 5354 high-resolution color images of different object and texture categories. It contains normal, i.e., defect-free images intended for training and images with anomalies intended for testing. The anomalies manifest themselves in the form of over 70 different types of defects such as scratches, dents, contaminations, and various structural changes. In addition, we provide pixel-precise ground truth annotations for all anomalies. We conduct a thorough evaluation of current state-of-the-art unsupervised anomaly detection methods based on deep architectures such as convolutional autoencoders, generative adversarial networks, and feature descriptors using pretrained convolutional neural networks, as well as classical computer vision methods. We highlight the advantages and disadvantages of multiple performance metrics as well as threshold estimation techniques. This benchmark indicates that methods that leverage descriptors of pretrained networks outperform all other approaches and deep-learning-based generative models show considerable room for improvement.},
	language = {en},
	number = {4},
	urldate = {2021-05-25},
	journal = {International Journal of Computer Vision},
	author = {Bergmann, Paul and Batzner, Kilian and Fauser, Michael and Sattlegger, David and Steger, Carsten},
	month = apr,
	year = {2021},
	pages = {1038--1059},
	file = {Bergmann et al. - 2021 - The MVTec Anomaly Detection Dataset A Comprehensi.pdf:/Users/hoefersh/Zotero/storage/PNQV8HSC/Bergmann et al. - 2021 - The MVTec Anomaly Detection Dataset A Comprehensi.pdf:application/pdf},
}

@article{roth_towards_2021,
	title = {Towards {Total} {Recall} in {Industrial} {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/2106.08265},
	abstract = {Being able to spot defective parts is a critical component in large-scale industrial manufacturing. A particular challenge that we address in this work is the cold-start problem: ﬁt a model using nominal (non-defective) example images only. While handcrafted solutions per class are possible, the goal is to build systems that work well simultaneously on many different tasks automatically. The best peforming approaches combine embeddings from ImageNet models with an outlier detection model. In this paper, we extend on this line of work and propose PatchCore, which uses a maximally representative memory bank of nominal patchfeatures. PatchCore offers competitive inference times while achieving state-of-the-art performance for both detection and localization. On the standard dataset MVTec AD PatchCore achieves an image-level anomaly detection AUROC score of 99.1\%, more than halving the error compared to the next best competitor. We further report competitive results on two additional datasets and also ﬁnd competitive results in the few samples regime.},
	language = {en},
	urldate = {2022-03-01},
	journal = {arXiv:2106.08265 [cs]},
	author = {Roth, Karsten and Pemula, Latha and Zepeda, Joaquin and Schölkopf, Bernhard and Brox, Thomas and Gehler, Peter},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.08265},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Roth et al. - 2021 - Towards Total Recall in Industrial Anomaly Detecti.pdf:/Users/hoefersh/Zotero/storage/5NM23PAF/Roth et al. - 2021 - Towards Total Recall in Industrial Anomaly Detecti.pdf:application/pdf},
}

@article{yang_generalized_2021,
	title = {Generalized {Out}-of-{Distribution} {Detection}: {A} {Survey}},
	shorttitle = {Generalized {Out}-of-{Distribution} {Detection}},
	url = {http://arxiv.org/abs/2110.11334},
	abstract = {Out-of-distribution (OOD) detection is critical to ensuring the reliability and safety of machine learning systems. For instance, in autonomous driving, we would like the driving system to issue an alert and hand over the control to humans when it detects unusual scenes or objects that it has never seen before and cannot make a safe decision. This problem first emerged in 2017 and since then has received increasing attention from the research community, leading to a plethora of methods developed, ranging from classification-based to density-based to distance-based ones. Meanwhile, several other problems are closely related to OOD detection in terms of motivation and methodology. These include anomaly detection (AD), novelty detection (ND), open set recognition (OSR), and outlier detection (OD). Despite having different definitions and problem settings, these problems often confuse readers and practitioners, and as a result, some existing studies misuse terms. In this survey, we first present a generic framework called generalized OOD detection, which encompasses the five aforementioned problems, i.e., AD, ND, OSR, OOD detection, and OD. Under our framework, these five problems can be seen as special cases or sub-tasks, and are easier to distinguish. Then, we conduct a thorough review of each of the five areas by summarizing their recent technical developments. We conclude this survey with open challenges and potential research directions.},
	urldate = {2021-11-03},
	journal = {arXiv:2110.11334 [cs]},
	author = {Yang, Jingkang and Zhou, Kaiyang and Li, Yixuan and Liu, Ziwei},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.11334},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/hoefersh/Zotero/storage/GGAESUN9/Yang et al. - 2021 - Generalized Out-of-Distribution Detection A Surve.pdf:application/pdf;arXiv.org Snapshot:/Users/hoefersh/Zotero/storage/TBISJIYW/2110.html:text/html},
}

@article{ehret_image_2019,
	title = {Image {Anomalies}: {A} {Review} and {Synthesis} of {Detection} {Methods}},
	volume = {61},
	issn = {1573-7683},
	shorttitle = {Image {Anomalies}},
	url = {https://doi.org/10.1007/s10851-019-00885-0},
	doi = {10.1007/s10851-019-00885-0},
	abstract = {We review the broad variety of methods that have been proposed for anomaly detection in images. Most methods found in the literature have in mind a particular application. Yet we focus on a classification of the methods based on the structural assumption they make on the “normal” image, assumed to obey a “background model.” Five different structural assumptions emerge for the background model. Our analysis leads us to reformulate the best representative algorithms in each class by attaching to them an a-contrario detection that controls the number of false positives and thus deriving a uniform detection scheme for all. By combining the most general structural assumptions expressing the background’s normality with the proposed generic statistical detection tool, we end up proposing several generic algorithms that seem to generalize or reconcile most methods. We compare the six best representatives of our proposed classes of algorithms on anomalous images taken from classic papers on the subject, and on a synthetic database. Our conclusion hints that it is possible to perform automatic anomaly detection on a single image.},
	language = {en},
	number = {5},
	urldate = {2022-04-20},
	journal = {Journal of Mathematical Imaging and Vision},
	author = {Ehret, Thibaud and Davy, Axel and Morel, Jean-Michel and Delbracio, Mauricio},
	month = jun,
	year = {2019},
	pages = {710--743},
	file = {Submitted Version:/Users/hoefersh/Zotero/storage/EUHEJYEV/Ehret et al. - 2019 - Image Anomalies A Review and Synthesis of Detecti.pdf:application/pdf},
}

@article{omeiza_smooth_2019,
	title = {Smooth {Grad}-{CAM}++: {An} {Enhanced} {Inference} {Level} {Visualization} {Technique} for {Deep} {Convolutional} {Neural} {Network} {Models}},
	shorttitle = {Smooth {Grad}-{CAM}++},
	url = {http://arxiv.org/abs/1908.01224},
	abstract = {Gaining insight into how deep convolutional neural network models perform image classiﬁcation and how to explain their outputs have been a concern to computer vision researchers and decision makers. These deep models are often referred to as black box due to low comprehension of their internal workings. As an eﬀort to developing explainable deep learning models, several methods have been proposed such as ﬁnding gradients of class output with respect to input image (sensitivity maps), class activation map (CAM), and Gradient based Class Activation Maps (Grad-CAM). These methods under perform when localizing multiple occurrences of the same class and do not work for all CNNs. In addition, Grad-CAM does not capture the entire object in completeness when used on single object images, this aﬀect performance on recognition tasks. With the intention to create an enhanced visual explanation in terms of visual sharpness, object localization and explaining multiple occurrences of objects in a single image, we present Smooth Grad-CAM++ 1, a technique that combines methods from two other recent techniques—SMOOTHGRAD and Grad-CAM++. Our Smooth Grad-CAM++ technique provides the capability of either visualizing a layer, subset of feature maps, or subset of neurons within a feature map at each instance at the inference level (model prediction process). After experimenting with few images, Smooth Grad-CAM++ produced more visually sharp maps with better localization of objects in the given input images when compared with other methods.},
	language = {en},
	urldate = {2022-04-20},
	journal = {arXiv:1908.01224 [cs]},
	author = {Omeiza, Daniel and Speakman, Skyler and Cintas, Celia and Weldermariam, Komminist},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.01224},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Omeiza et al. - 2019 - Smooth Grad-CAM++ An Enhanced Inference Level Vis.pdf:/Users/hoefersh/Zotero/storage/AXT3IHBD/Omeiza et al. - 2019 - Smooth Grad-CAM++ An Enhanced Inference Level Vis.pdf:application/pdf},
}

@article{li_cutpaste_2021,
	title = {{CutPaste}: {Self}-{Supervised} {Learning} for {Anomaly} {Detection} and {Localization}},
	shorttitle = {{CutPaste}},
	url = {http://arxiv.org/abs/2104.04015},
	abstract = {We aim at constructing a high performance model for defect detection that detects unknown anomalous patterns of an image without anomalous data. To this end, we propose a two-stage framework for building anomaly detectors using normal training data only. We ﬁrst learn self-supervised deep representations and then build a generative one-class classiﬁer on learned representations. We learn representations by classifying normal data from the CutPaste, a simple data augmentation strategy that cuts an image patch and pastes at a random location of a large image. Our empirical study on MVTec anomaly detection dataset demonstrates the proposed algorithm is general to be able to detect various types of real-world defects. We bring the improvement upon previous arts by 3.1 AUCs when learning representations from scratch. By transfer learning on pretrained representations on ImageNet, we achieve a new state-of-theart 96.6 AUC. Lastly, we extend the framework to learn and extract representations from patches to allow localizing defective areas without annotations during training.},
	language = {en},
	urldate = {2021-09-06},
	journal = {arXiv:2104.04015 [cs]},
	author = {Li, Chun-Liang and Sohn, Kihyuk and Yoon, Jinsung and Pfister, Tomas},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.04015},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Li et al. - 2021 - CutPaste Self-Supervised Learning for Anomaly Det.pdf:/Users/hoefersh/Zotero/storage/K7NXR4IM/Li et al. - 2021 - CutPaste Self-Supervised Learning for Anomaly Det.pdf:application/pdf},
}

@article{northcutt_pervasive_2021,
	title = {Pervasive {Label} {Errors} in {Test} {Sets} {Destabilize} {Machine} {Learning} {Benchmarks}},
	url = {http://arxiv.org/abs/2103.14749},
	abstract = {We identify label errors in the test sets of 10 of the most commonly-used computer vision, natural language, and audio datasets, and subsequently study the potential for these label errors to affect benchmark results. Errors in test sets are numerous and widespread: we estimate an average of at least 3.3\% errors across the 10 datasets, where for example label errors comprise at least 6\% of the ImageNet validation set. Putative label errors are identified using confident learning algorithms and then human-validated via crowdsourcing (51\% of the algorithmically-flagged candidates are indeed erroneously labeled, on average across the datasets). Traditionally, machine learning practitioners choose which model to deploy based on test accuracy - our findings advise caution here, proposing that judging models over correctly labeled test sets may be more useful, especially for noisy real-world datasets. Surprisingly, we find that lower capacity models may be practically more useful than higher capacity models in real-world datasets with high proportions of erroneously labeled data. For example, on ImageNet with corrected labels: ResNet-18 outperforms ResNet-50 if the prevalence of originally mislabeled test examples increases by just 6\%. On CIFAR-10 with corrected labels: VGG-11 outperforms VGG-19 if the prevalence of originally mislabeled test examples increases by just 5\%. Test set errors across the 10 datasets can be viewed at https://labelerrors.com and all label errors can be reproduced by https://github.com/cleanlab/label-errors.},
	urldate = {2021-12-07},
	journal = {arXiv:2103.14749 [cs, stat]},
	author = {Northcutt, Curtis G. and Athalye, Anish and Mueller, Jonas},
	month = nov,
	year = {2021},
	note = {arXiv: 2103.14749},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/hoefersh/Zotero/storage/QX7UJR6Y/Northcutt et al. - 2021 - Pervasive Label Errors in Test Sets Destabilize Ma.pdf:application/pdf;arXiv.org Snapshot:/Users/hoefersh/Zotero/storage/I8T7R8KK/2103.html:text/html},
}

@article{kong_multi-task_2021,
	title = {Multi-{Task} {Classification} and {Segmentation} for {Explicable} {Capsule} {Endoscopy} {Diagnostics}},
	volume = {8},
	issn = {2296-889X},
	url = {https://www.frontiersin.org/article/10.3389/fmolb.2021.614277},
	abstract = {Capsule endoscopy is a leading diagnostic tool for small bowel lesions which faces certain challenges such as time-consuming interpretation and harsh optical environment inside the small intestine. Specialists unavoidably waste lots of time on searching for a high clearness degree image for accurate diagnostics. However, current clearness degree classification methods are based on either traditional attributes or an unexplainable deep neural network. In this paper, we propose a multi-task framework, called the multi-task classification and segmentation network (MTCSN), to achieve joint learning of clearness degree (CD) and tissue semantic segmentation (TSS) for the first time. In the MTCSN, the CD helps to generate better refined TSS, while TSS provides an explicable semantic map to better classify the CD. In addition, we present a new benchmark, named the Capsule-Endoscopy Crohn’s Disease dataset, which introduces the challenges faced in the real world including motion blur, excreta occlusion, reflection, and various complex alimentary scenes that are widely acknowledged in endoscopy examination. Extensive experiments and ablation studies report the significant performance gains of the MTCSN over state-of-the-art methods.},
	urldate = {2022-02-10},
	journal = {Frontiers in Molecular Biosciences},
	author = {Kong, Zishang and He, Min and Luo, Qianjiang and Huang, Xiansong and Wei, Pengxu and Cheng, Yalu and Chen, Luyang and Liang, Yongsheng and Lu, Yanchang and Li, Xi and Chen, Jie},
	year = {2021},
	file = {Full Text PDF:/Users/hoefersh/Zotero/storage/595X28GP/Kong et al. - 2021 - Multi-Task Classification and Segmentation for Exp.pdf:application/pdf},
}

@misc{hu_damage_2021,
	title = {Damage {Detection} in {Robotic} {Induct} {Stations}},
	author = {Hu, Siyao and Gallenstein, Brian and Dhillon, Sachal and Stallman, Tim and Preiswerk, Frank},
	year = {2021},
}

@article{caruana_multitask_1997,
	title = {Multitask {Learning}},
	volume = {28},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1007379606734},
	doi = {10.1023/A:1007379606734},
	abstract = {Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.},
	language = {en},
	number = {1},
	urldate = {2022-04-25},
	journal = {Machine Learning},
	author = {Caruana, Rich},
	month = jul,
	year = {1997},
	keywords = {backpropagation, generalization, inductive transfer, k-nearest neighbor, kernel regression, multitask learning, parallel transfer, supervised learning},
	pages = {41--75},
	file = {Full Text PDF:/Users/hoefersh/Zotero/storage/5SVV8YKI/Caruana - 1997 - Multitask Learning.pdf:application/pdf},
}

@inproceedings{ronneberger_u-net_2015,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	isbn = {978-3-319-24574-4},
	shorttitle = {U-{Net}},
	doi = {10.1007/978-3-319-24574-4_28},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2015},
	publisher = {Springer International Publishing},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
	year = {2015},
	keywords = {Convolutional Layer, Data Augmentation, Deep Network, Ground Truth Segmentation, Training Image},
	pages = {234--241},
	file = {Full Text PDF:/Users/hoefersh/Zotero/storage/H44VQF56/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:application/pdf},
}

@article{badrinarayanan_segnet_2016,
	title = {{SegNet}: {A} {Deep} {Convolutional} {Encoder}-{Decoder} {Architecture} for {Image} {Segmentation}},
	shorttitle = {{SegNet}},
	url = {http://arxiv.org/abs/1511.00561},
	abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN and also with the well known DeepLab-LargeFOV, DeconvNet architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. We show that SegNet provides good performance with competitive inference time and more efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
	urldate = {2022-04-25},
	journal = {arXiv:1511.00561 [cs]},
	author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
	month = oct,
	year = {2016},
	note = {arXiv: 1511.00561
version: 3},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/hoefersh/Zotero/storage/TG4ZMD9R/Badrinarayanan et al. - 2016 - SegNet A Deep Convolutional Encoder-Decoder Archi.pdf:application/pdf;arXiv.org Snapshot:/Users/hoefersh/Zotero/storage/KFWV5V3F/1511.html:text/html},
}

@article{buslaev_albumentations_2020,
	title = {Albumentations: {Fast} and {Flexible} {Image} {Augmentations}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2078-2489},
	shorttitle = {Albumentations},
	url = {https://www.mdpi.com/2078-2489/11/2/125},
	doi = {10.3390/info11020125},
	abstract = {Data augmentation is a commonly used technique for increasing both the size and the diversity of labeled training sets by leveraging input transformations that preserve corresponding output labels. In computer vision, image augmentations have become a common implicit regularization technique to combat overfitting in deep learning models and are ubiquitously used to improve performance. While most deep learning frameworks implement basic image transformations, the list is typically limited to some variations of flipping, rotating, scaling, and cropping. Moreover, image processing speed varies in existing image augmentation libraries. We present Albumentations, a fast and flexible open source library for image augmentation with many various image transform operations available that is also an easy-to-use wrapper around other augmentation libraries. We discuss the design principles that drove the implementation of Albumentations and give an overview of the key features and distinct capabilities. Finally, we provide examples of image augmentations for different computer vision tasks and demonstrate that Albumentations is faster than other commonly used image augmentation tools on most image transform operations.},
	language = {en},
	number = {2},
	urldate = {2022-04-25},
	journal = {Information},
	author = {Buslaev, Alexander and Iglovikov, Vladimir I. and Khvedchenya, Eugene and Parinov, Alex and Druzhinin, Mikhail and Kalinin, Alexandr A.},
	month = feb,
	year = {2020},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {computer vision, data augmentation, deep learning},
	pages = {125},
	file = {Full Text PDF:/Users/hoefersh/Zotero/storage/BVK8SHNA/Buslaev et al. - 2020 - Albumentations Fast and Flexible Image Augmentati.pdf:application/pdf;Snapshot:/Users/hoefersh/Zotero/storage/6LFMPKBC/125.html:text/html},
}

@inproceedings{dosovitskiy_vit_2020,
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  booktitle = {ICLR},
  year = {2021}
 }

@inproceedings{jeong2023winclip,
  title={Winclip: Zero-/few-shot anomaly classification and segmentation},
  author={Jeong, Jongheon and Zou, Yang and Kim, Taewan and Zhang, Dongqing and Ravichandran, Avinash and Dabeer, Onkar},
  booktitle={CVPR},
  pages={19606--19616},
  year={2023}
}

@article{pang2021deep,
  title={{Deep Learning for Anomaly Detection: A Review}},
  author={Pang, Guansong and Shen, Chunhua and Cao, Longbing and Hengel, Anton Van Den},
  journal={ACM computing surveys (CSUR)},
  volume={54},
  number={2},
  pages={1--38},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{han2022adbench,
  title={Adbench: Anomaly detection benchmark},
  author={Han, Songqiao and Hu, Xiyang and Huang, Hailiang and Jiang, Minqi and Zhao, Yue},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={32142--32159},
  year={2022}
}

@inproceedings{yi2020patch,
  title={Patch svdd: Patch-level svdd for anomaly detection and segmentation},
  author={Yi, Jihun and Yoon, Sungroh},
  booktitle={Proceedings of the Asian Conference on Computer Vision},
  year={2020}
}

@inproceedings{ruff2018deep,
  title={Deep one-class classification},
  author={Ruff, Lukas and Vandermeulen, Robert and Goernitz, Nico and Deecke, Lucas and Siddiqui, Shoaib Ahmed and Binder, Alexander and M{\"u}ller, Emmanuel and Kloft, Marius},
  booktitle={International conference on machine learning},
  pages={4393--4402},
  year={2018},
  organization={PMLR}
}

@article{bergmann2018improving,
  title={Improving unsupervised defect segmentation by applying structural similarity to autoencoders},
  author={Bergmann, Paul and L{\"o}we, Sindy and Fauser, Michael and Sattlegger, David and Steger, Carsten},
  journal={arXiv preprint arXiv:1807.02011},
  year={2018}
}



@misc{balestriero_cookbook_2023,
	title = {A {Cookbook} of {Self}-{Supervised} {Learning}},
	url = {http://arxiv.org/abs/2304.12210},
	doi = {10.48550/arXiv.2304.12210},
	abstract = {Self-supervised learning, dubbed the dark matter of intelligence, is a promising path to advance machine learning. Yet, much like cooking, training SSL methods is a delicate art with a high barrier to entry. While many components are familiar, successfully training a SSL method involves a dizzying set of choices from the pretext tasks to training hyper-parameters. Our goal is to lower the barrier to entry into SSL research by laying the foundations and latest SSL recipes in the style of a cookbook. We hope to empower the curious researcher to navigate the terrain of methods, understand the role of the various knobs, and gain the know-how required to explore how delicious SSL can be.},
	urldate = {2023-05-02},
	publisher = {arXiv},
	author = {Balestriero, Randall and Ibrahim, Mark and Sobal, Vlad and Morcos, Ari and Shekhar, Shashank and Goldstein, Tom and Bordes, Florian and Bardes, Adrien and Mialon, Gregoire and Tian, Yuandong and Schwarzschild, Avi and Wilson, Andrew Gordon and Geiping, Jonas and Garrido, Quentin and Fernandez, Pierre and Bar, Amir and Pirsiavash, Hamed and LeCun, Yann and Goldblum, Micah},
	month = apr,
	year = {2023},
	note = {arXiv:2304.12210 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/hoefersh/Zotero/storage/QHIE75ZF/Balestriero et al. - 2023 - A Cookbook of Self-Supervised Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/hoefersh/Zotero/storage/39KUDE8S/2304.html:text/html},
}


@inproceedings{akcay2019ganomaly,
  title={Ganomaly: Semi-supervised anomaly detection via adversarial training},
  author={Akcay, Samet and Atapour-Abarghouei, Amir and Breckon, Toby P},
  booktitle={Computer Vision--ACCV 2018: 14th Asian Conference on Computer Vision, Perth, Australia, December 2--6, 2018, Revised Selected Papers, Part III 14},
  pages={622--637},
  year={2019},
  organization={Springer}
}

@article{schlegl2019f,
  title={f-AnoGAN: Fast unsupervised anomaly detection with generative adversarial networks},
  author={Schlegl, Thomas and Seeb{\"o}ck, Philipp and Waldstein, Sebastian M and Langs, Georg and Schmidt-Erfurth, Ursula},
  journal={Medical image analysis},
  volume={54},
  pages={30--44},
  year={2019},
  publisher={Elsevier}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{deng2022anomaly,
  title={Anomaly detection via reverse distillation from one-class embedding},
  author={Deng, Hanqiu and Li, Xingyu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9737--9746},
  year={2022}
}

@inproceedings{roth2022towards,
  title={Towards total recall in industrial anomaly detection},
  author={Roth, Karsten and Pemula, Latha and Zepeda, Joaquin and Sch{\"o}lkopf, Bernhard and Brox, Thomas and Gehler, Peter},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14318--14328},
  year={2022}
}

@inproceedings{zou2022spot,
  title={SPot-the-Difference Self-Supervised Pre-training for Anomaly Detection and Segmentation},
  author={Zou, Yang and Jeong, Jongheon and Pemula, Latha and Zhang, Dongqing and Dabeer, Onkar},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{mitash2023armbench,
    title={{ARMBench: An object-centric benchmark dataset for robotic manipulation}},
    author={Mitash, Chaitanya and Wang, Fan and Lu, Shiyang and Terhuja, Vikedo and Garaas, Tyler and Polido, Felipe and Nambi, Manikantan},
    booktitle={ICRA},
    year={2023}
}

@article{Tabernik2019JIM,
  author = {Tabernik, Domen and {\v{S}}ela, Samo and Skvar{\v{c}}, Jure and 
  Sko{\v{c}}aj, Danijel},
  journal = {Journal of Intelligent Manufacturing},
  title = {{Segmentation-Based Deep-Learning Approach for Surface-Defect Detection}},
  year = {2019},
  month = {May},
  day = {15},
  issn={1572-8145},
  doi={10.1007/s10845-019-01476-x}
}

@INPROCEEDINGS{jezek_2021,
  author={Jezek, Stepan and Jonak, Martin and Burget, Radim and Dvorak, Pavel and Skotak, Milos},
  booktitle={2021 13th International Congress on Ultra Modern Telecommunications and Control Systems and Workshops (ICUMT)}, 
  title={Deep learning-based defect detection of metal parts: evaluating current methods in complex conditions}, 
  year={2021},
  volume={},
  number={},
  pages={66-71},
  doi={10.1109/ICUMT54235.2021.9631567}
}

@inproceedings{mishra21-vt-adl,
    author = {Mishra, Pankaj and Verk, Riccardo and Fornasier, Daniele and Piciarelli, Claudio and Foresti, Gian Luca},
    title = {{VT-ADL}: A Vision Transformer Network for Image Anomaly Detection and Localization},
    booktitle = {30th IEEE/IES International Symposium on Industrial Electronics (ISIE)},
    year = {2021},
    month = {June},
    location = {Kyoto, Japan}
}

@article{ren2023prompt,
    title={Prompt pre-training with twenty-thousand classes for open-vocabulary visual recognition},
    author={Ren, Shuhuai and Zhang, Aston and Zhu, Yi and Zhang, Shuai and Zheng, Shuai and Li, Mu and Smola, Alexander J and Sun, Xu},
    journal={Advances in Neural Information Processing Systems},
    volume={36},
    pages={12569--12588},
    year={2023}
}

@inproceedings{khattak2023maplemultimodalpromptlearning,
      title={MaPLe: Multi-modal Prompt Learning}, 
      author={Muhammad Uzair Khattak and Hanoona Rasheed and Muhammad Maaz and Salman Khan and Fahad Shahbaz Khan},
      year={2023},
      eprint={2210.03117},
      booktitle = CVPR,
      url={https://arxiv.org/abs/2210.03117}, 
}

@inproceedings{dosovitskiy2021imageworth16x16words,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2021},
      eprint={2010.11929},
      booktitle = ICLR,
      url={https://arxiv.org/abs/2010.11929}, 
}

@article{oquab2024dinov,
    title={{DINO}v2: Learning Robust Visual Features without Supervision},
    author={Maxime Oquab and Timoth{\'e}e Darcet and Th{\'e}o Moutakanni and Huy V. Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel HAZIZA and Francisco Massa and Alaaeldin El-Nouby and Mido Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Herve Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},
    journal={Transactions on Machine Learning Research},
    issn={2835-8856},
    year={2024},
    url={https://openreview.net/forum?id=a68SUt6zFt},
    note={}
}

@INPROCEEDINGS{imagenet2009,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle=CVPR, 
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  volume={},
  number={},
  pages={248-255},
  keywords={Large-scale systems;Image databases;Explosions;Internet;Robustness;Information retrieval;Image retrieval;Multimedia databases;Ontologies;Spine},
  doi={10.1109/CVPR.2009.5206848}
}

@INPROCEEDINGS{resnet2016,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle=CVPR, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  keywords={Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation},
  doi={10.1109/CVPR.2016.90}
}


@InProceedings{pmlr-v139-radford21a,
  title = 	 {Learning Transferable Visual Models From Natural Language Supervision},
  author =       {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = 	 ICML,
  pages = 	 {8748--8763},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/radford21a/radford21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/radford21a.html},
  abstract = 	 {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.}
}

@misc{agrawal2024pixtral12b,
      title={Pixtral 12B}, 
      author={Pravesh Agrawal and Szymon Antoniak and Emma Bou Hanna and Baptiste Bout and Devendra Chaplot and Jessica Chudnovsky and Diogo Costa and Baudouin De Monicault and Saurabh Garg and Theophile Gervet and Soham Ghosh and Amélie Héliou and Paul Jacob and Albert Q. Jiang and Kartik Khandelwal and Timothée Lacroix and Guillaume Lample and Diego Las Casas and Thibaut Lavril and Teven Le Scao and Andy Lo and William Marshall and Louis Martin and Arthur Mensch and Pavankumar Muddireddy and Valera Nemychnikova and Marie Pellat and Patrick Von Platen and Nikhil Raghuraman and Baptiste Rozière and Alexandre Sablayrolles and Lucile Saulnier and Romain Sauvestre and Wendy Shang and Roman Soletskyi and Lawrence Stewart and Pierre Stock and Joachim Studnia and Sandeep Subramanian and Sagar Vaze and Thomas Wang and Sophia Yang},
      year={2024},
      eprint={2410.07073},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.07073}, 
}

@misc{claude-model-card,
      title={The Claude 3 Model Family: Opus, Sonnet, Haiku}, 
      author={Anthropic},
      year={2024},
      url={https://assets.anthropic.com/m/61e7d27f8c8f5919/original/Claude-3-Model-Card.pdf}, 
}

@inproceedings{akcay2022anomalib,
  title={Anomalib: A deep learning library for anomaly detection},
  author={Akcay, Samet and Ameln, Dick and Vaidya, Ashwin and Lakshmanan, Barath and Ahuja, Nilesh and Genc, Utku},
  booktitle=ICIP,
  pages={1706--1710},
  year={2022},
  organization={IEEE}
}

@article{tang2024autogluon,
  title={AutoGluon-Multimodal (AutoMM): Supercharging Multimodal AutoML with Foundation Models},
  author={Tang, Zhiqiang and Fang, Haoyang and Zhou, Su and Yang, Taojiannan and Zhong, Zihan and Hu, Tony and Kirchhoff, Katrin and Karypis, George},
  journal={arXiv preprint arXiv:2404.16233},
  year={2024}
}
@misc{yao2024gladbetterreconstructionglobal,
      title={GLAD: Towards Better Reconstruction with Global and Local Adaptive Diffusion Models for Unsupervised Anomaly Detection}, 
      author={Hang Yao and Ming Liu and Haolin Wang and Zhicun Yin and Zifei Yan and Xiaopeng Hong and Wangmeng Zuo},
      year={2024},
      eprint={2406.07487},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.07487}, 
}
@article{chen2024unified,
  title={A Unified Anomaly Synthesis Strategy with Gradient Ascent for Industrial Anomaly Detection and Localization},
  author={Chen, Qiyu and Luo, Huiyuan and Lv, Chengkan and Zhang, Zhengtao},
  journal={arXiv preprint arXiv:2407.09359},
  year={2024}
}

@misc{jiang2025mmad,
      title={{MMAD}: {A} {C}omprehensive {B}enchmark for {M}ultimodal {L}arge {L}anguage {M}odels in {I}ndustrial {A}nomaly {D}etection}, 
      author={Xi Jiang and Jian Li and Hanqiu Deng and Yong Liu and Bin-Bin Gao and Yifeng Zhou and Jialin Li and Chengjie Wang and Feng Zheng},
      year={2025},
      eprint={2410.09453},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2410.09453}, 
}

@misc{koshil2024apc,
      title={AnomalousPatchCore: Exploring the Use of Anomalous Samples in Industrial Anomaly Detection}, 
      author={Mykhailo Koshil and Tilman Wegener and Detlef Mentrup and Simone Frintrop and Christian Wilms},
      year={2024},
      eprint={2408.15113},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2408.15113}, 
}