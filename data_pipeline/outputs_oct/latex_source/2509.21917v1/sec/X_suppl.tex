\section*{Appendix Overview}

The appendix includes extra experimental results, corresponding analyses, and further discussions of IF-V2V. The appendix is organized as follows:
\begin{itemize}
    \item \sref{sec:diagnostic_lambda} analyzes the effect of the rectification scale in \sref{sec:vfr-sd}.
    \item \sref{sec:abl_smpi} further ablates the components of SMPI.
    \item \sref{sec:comp_solver} provides comparisons on adopting different ODE solvers.
    \item \sref{sec:ext_flow} demonstrates quantitative and qualitative results of extending IF-V2V to other flow-based I2V models for editing.
    \item \sref{sec:t2v_edit} presents qualitative results of extending IF-V2V for text-guided video editing.
    \item \sref{sec:image_edit} adapts an inversion-free image editing method FlowEdit~\citep{kulikov2024floweditinversionfreetextbasedediting} to the video domain for quantitative comparison.
    \item \sref{sec:vis_extra} shows more qualitative results of IF-V2V.
    \item \sref{sec:discussions} further discusses the theoretical justifications, hyperparameter selection criteria, limitations, and societal impacts of IF-V2V.
\end{itemize}

\section{Effect of the Rectification Scale}
\label{sec:diagnostic_lambda}

\begin{figure}[ht]
  \centering
  \includegraphics[width=\linewidth]{figs/vis_lambda.pdf}
  \caption{Illustration of the effect of the rectification scale $\lambda$ (\sref{sec:diagnostic_lambda}). The source video shows a boy in a \textbf{white} T-shirt cycling along the road, and the edited first frame changes the boy's T-shirt to \textbf{red}. The value of $\lambda$ can be tuned in an appropriate range to control the strength of the source video prior. A small $\lambda$ ($\leq 0.50$) brings a weak prior from the source video, resulting in inconsistency with the source video that the boy cycles along a road with an endless wall. When $\lambda$ is too large ($\geq 1.25$), artifacts like blurs and oversaturation also emerge. Please zoom in for details.}
  \label{fig:lambda}
\end{figure}

The rectification scale $\lambda$ in \sref{sec:vfr-sd} determines the strength that VFR-SD incorporates the source video's deviation from expectation during the target ODE solving process. To provide an intuitive understanding of the impact of $\lambda$ on the edited video, we present a visualization of using different values of $\lambda$ to edit a video sample in \cref{fig:lambda}. The video originally captures a boy in a white T-shirt cycling along the road, and the edited frame turns the boy's T-shirt red. When $\lambda$ is small ($\leq 0.5$), the deviation from the source video is relatively weak, and the denoising process resembles the straightforward I2V process. In this case, the sample deviation is inadequate to direct the denoising process, resulting in the boy cycling along the road with an \textit{endless} wall. In contrast, an overly large $\lambda$ value ($\geq 1.25$) also induces artifacts like blurs and oversaturation because the rectification term pushes the generated sample too far from the original distribution.

\section{Extra Ablations on SMPI}
\label{sec:abl_smpi}

We conduct extra ablations to provide a better understanding of SMPI. The results are displayed in \cref{tab:abl_smpi}, where \textit{w/o} MPI stands for without motion-preserving initialization. Comparing \#1 and \#2, we can observe that structure-preserving initialization better maintains the consistency with original videos (OVC). From \#2 and \#3, it can be concluded that motion-preserving initialization further enhances temporal consistency.

\begin{table}[ht]
  \caption{Quantitative ablations on SMPI (\sref{sec:abl_smpi}). Results in \textbf{bold} are the best.
  }
  \label{tab:abl_smpi}
  \centering
  \begin{tabular}{l|ccccc|c}
    \toprule
    Setting & AS & TC & EFC & OVC & AEC & Time \\
    \midrule
    \textit{w/o} SMPI & 4.78 & 98.19 & 92.67 & 75.45 & 84.06 & 622.38 \\
    \textit{w/o} MPI & 4.81 & 98.06 & 92.51 & 76.30 & 84.41 & \textbf{615.17} \\
    IF-V2V & \textbf{4.88} & \textbf{98.71} & \textbf{92.79} & \textbf{76.44} & \textbf{84.62} & 616.60 \\
    \bottomrule
  \end{tabular}
\end{table}

\section{Comparisons on ODE Solvers}
\label{sec:comp_solver}

To validate IF-V2V's compatibility with different ODE solvers, we evaluate IF-V2V's performance with Euler Discrete Scheduler~\citep{sd3} and UniPC Scheduler~\citep{unipc}. Quantitative results are displayed in \cref{tab:comp_solver} with the same settings as \sref{sec:diagnostic}. We also present qualitative results in \cref{fig:vis_solver}. From the above results, we can conclude that IF-V2V also achieves satisfactory performance with UniPC Scheduler~\citep{unipc}, demonstrating the universality of our method.

\begin{table}
  \caption{Comparisons on adopting different ODE solvers in IF-V2V (\sref{sec:comp_solver}).
  % Results in \textbf{bold} are the best.
  }
  \label{tab:comp_solver}
  \centering
  \begin{tabular}{l|ccccc}
    \toprule
    Setting & AS & TC & EFC & OVC & AEC \\
    \midrule
    UniPC~\citep{unipc} & 4.86 & 98.70 & 92.01 & 76.48 & 84.25  \\
    Euler~\citep{sd3} & 4.88 & 98.71 & 92.79 & 76.44 & 84.62 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[ht]
  \centering
  \includegraphics[width=\linewidth]{figs_supp/vis_solver.pdf}
  \caption{Visualizations of using different ODE solvers in IF-V2V (\sref{sec:comp_solver}). The edited first frame is in the bottom-right corner of the edited video. IF-V2V is compatible with multiple ODE solvers to produce high-quality editing results.}
  \label{fig:vis_solver}
\end{figure}

\section{Extension to Other Flow-based I2V Models}
\label{sec:ext_flow}

\begin{table}
  \caption{Comparisons on using different I2V models in IF-V2V (\sref{sec:ext_flow}).
  % Results in \textbf{bold} are the best.
  }
  \label{tab:comp_flow}
  \centering
  \begin{tabular}{l|ccccc}
    \toprule
    Setting & AS & TC & EFC & OVC & AEC \\
    \midrule
    % \makecell[l]{HunyuanVideo\\\citep{hunyuanvideo}} & 4.75 & 98.60 & 92.92 & 75.36 & 84.14  \\
    HunyuanVideo~\citep{hunyuanvideo} & 4.75 & 98.60 & 92.92 & 75.36 & 84.14  \\
    Wan2.1~\citep{wan} & 4.88 & 98.71 & 92.79 & 76.44 & 84.62 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figs_supp/vis_hunyuan.pdf}
  \caption{Editing samples of using different flow-based I2V models in IF-V2V (\sref{sec:ext_flow}). The edited first frame is in the bottom-right corner of the source video. IF-V2V can be applied to various flow-based I2V models for high-quality video editing.}
  \label{fig:vis_hunyuan}
\end{figure}

To further demonstrate the universality of IF-V2V, we select HunyuanVideo~\citep{hunyuanvideo} as another base I2V model to apply our method. We present the quantitative results in \cref{tab:comp_flow}, from which we can find that HunyuanVideo~\citep{hunyuanvideo} also achieves a satisfying result that surpasses prior arts. \cref{fig:vis_hunyuan} shows the visualization of some editing cases, in which we can observe that both HunyuanVideo~\citep{hunyuanvideo} and Wan2.1~\citep{wan} achieve excellent consistency with both the edited frame and the original video with the help of IF-V2V.

\section{Extension to Text-guided Editing}
\label{sec:t2v_edit}

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figs_supp/vis_t2v.pdf}
  \caption{Text-guided editing results of IF-V2V (\sref{sec:t2v_edit}). We provide the simplified editing caption under each sample. Compared to \textit{I2V + Init}, our method preserves the details more faithfully, like shells and splashes in (a), and aligns better with the editing instruction, such as the guitar in (b).}
  \label{fig:vis_t2v}
\end{figure}

IF-V2V can also be used for text-guided video editing by removing the condition embedding in Structure-Preserving Initialization. We present some qualitative results using Wan2.1~\citep{wan} as the text-to-video model in \cref{fig:vis_t2v}, in which we can observe that IF-V2V also achieves excellent consistency and editing quality. In \cref{fig:vis_t2v} (a), IF-V2V keeps the details more faithfully, such as shells on the beach and splashes in the sea, compared to directly blending Gaussian noise and source video latents as the initial condition for the I2V model (I2V + Init). In \cref{fig:vis_t2v} (b), IF-V2V also aligns better with the editing prompt that alters the electric guitar into a normal one.

\section{Quantitative Comparisons with FlowEdit}
\label{sec:image_edit}

To demonstrate the superiority of IF-V2V over directly adopting image-based inversion-free editing methods for videos, we adapt FlowEdit~\citep{kulikov2024floweditinversionfreetextbasedediting}, a flow-based inversion-free image editing method, for video editing on Wan2.1~\citep{wan}. The performance is displayed in \cref{tab:comp_flowedit}, from which we can observe that both its editing quality and inference speed remain inferior to IF-V2V. This further validates the effectiveness of our new perspective on the ODE solving process, video-specific designs, and flexible caching strategy.

\begin{table}
  \caption{Quantitative comparisons with FlowEdit (\sref{sec:image_edit}). Results in \textbf{bold} are the best.}
  \label{tab:comp_flowedit}
  \centering
  \begin{tabular}{l|ccc|c}
    \toprule
    Setting & AS & TC & EFC & Time \\
    \midrule
    FlowEdit~\citep{kulikov2024floweditinversionfreetextbasedediting} & 4.76 & 98.01 & 92.32 & 802.42 \\
    IF-V2V (Ours) & \textbf{4.88} & \textbf{98.71} & \textbf{92.79} & \textbf{616.60}\\
    \bottomrule
  \end{tabular}
\end{table}

\section{More Visualizations}
\label{sec:vis_extra}

We illustrate more editing results of IF-V2V in \cref{fig:vis_extra}, which include object addition (a), object removal (b), and attribute modification (c). As observed, IF-V2V consistently achieves satisfactory performance on various video editing tasks. Please refer to the supplementary video for dynamic versions of editing samples.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figs/vis_extra.pdf}
  \caption{More visualizations of editing results of IF-V2V (\sref{sec:vis_extra}). The edited first frame is in the bottom-right corner of the source video. The cases include object addition (a), object removal (b), and attribute modification (c). Our method propagates the edited frame to the whole video with excellent quality and consistency.
  }
  \label{fig:vis_extra}
\end{figure}

\section{More Discussions}
\label{sec:discussions}

\subsection{Theoretical Justifications}
\label{sec:theoretical}
IF-V2V shares a similar theoretical basis as inversion-based editing methods: Optimal Transport (OT) mapping between the source and target distribution. The theoretical difference is that inversion-based methods conduct a mapping on the marginal distribution $p(z_0|z_{t_{max}})$, while IF-V2V performs mappings on transition distributions $p(z_{t-\Delta t} |z_t)$. When \(\Delta t\) is small enough, both the source and target transition distributions can be viewed as Gaussians with the same variance~\citep{ddpm, gmflow}. In this case, IF-V2V with $\lambda=1$ performs the exact OT mapping on the transitions.

\subsection{Hyperparameter Selection}
\label{sec:hyperparam}

The editing results suffer from over-saturation and distortion when the rectification scale $\lambda$ in \sref{sec:vfr-sd} is overly large. When the edited frame is not aligned with the original frame, the rectification sometimes causes unintended drifts due to the conflict. The scale of structure-preserving initialization $\beta$ in \sref{sec:spi} should be small enough when the edited region is large. Otherwise, IF-V2V mostly preserves the original video. An overly large flow-guided initialization factor $\alpha$ in \sref{sec:mpi} breaks the temporal Gaussianity of the noise and fails the generation.

It has been discovered that initial steps are more crucial for editing, and the vector difference in these steps is also larger. The caching threshold $\delta$ in \sref{sec:dcache} is selected around the vector difference in early steps to avoid caching these steps. Caching in later steps reduces computational cost with less impact on editing quality.

\subsection{Limitations}
\label{sec:limitations}

The editing capability of our method is inherently bounded by the selected image editing model and the I2V model. Failure in either stage will result in unsatisfactory results. Moreover, since existing I2V models only predict the expectation of the distribution without covariance information, IF-V2V cannot exploit the covariance to achieve more precise mapping from the source sample to the target sample in a training-free way. 

\subsubsection{Image Editing Methods}
\label{sec:limitations_imageedit}

IF-V2V's editing results rely on the first frame edited by image editing methods. However, current state-of-the-art methods~\citep{gpt4oimg, stepedit} still suffer from inconsistencies and trial-and-error. For instance, the image edited by GPT-4o~\citep{gpt4oimg} often misaligns with the original image, especially when changing the original image into a significantly different style (\eg, Ghibli cartoonish style). Such misalignment may cause undesired alterations in the edited videos. In addition, Step1X-Edit~\citep{stepedit} sometimes needs several tries to achieve a satisfactory editing result. We expect that the future advancements of image editing methods will ease the process of obtaining a satisfactory first frame and further boost the performance of IF-V2V.

\subsubsection{I2V Models}
\label{sec:limitations_i2v}

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figs_supp/vis_failure.pdf}
  \caption{Failure cases of IF-V2V (\sref{sec:limitations_i2v}). The edited first frame is in the bottom-right corner of the source video. IF-V2V struggles to handle editing samples with overly complex (a) or fast (b) motions due to the limited capability of I2V models.}
  \label{fig:failure}
\end{figure}

IF-V2V fails to produce satisfactory results when motion in the source video is overly complex or fast. As \cref{fig:failure} (a) displays, when there are complicated motions in the source video like simultaneous multiple subject movement with changing occlusions, IF-V2V cannot genuinely reproduce such motion in the edited video. In \cref{fig:failure} (b), IF-V2V generates unsatisfactory results when dealing with breakdance, which contains rapid human body movements. These phenomena stem from state-of-the-art I2V models' limited ability to generate rapid or sophisticated motions. This problem may be resolved by more powerful I2V models in the future which are capable of handling such complex motions.

Furthermore, mainstream flow-based I2V models~\citep{wan, easyanimate, hunyuanvideo, cogvideox, opensora2, vchitect2} only predict the \textit{expectation} of the target distribution without further information like covariance, making it hard to conduct more fine-grained operations to map the source sample to the target distribution in a training-free way. This may limit the method's ability to maintain the consistency of fine-grained details in edited videos.

\subsection{Societal Impacts}
\label{sec:impacts}

IF-V2V can achieve high-quality video editing by combining off-the-shelf image editing and I2V methods without training, enabling practitioners to flexibly leverage the most up-to-date models to implement their creativity. For individual creators, the lightweight nature of our method enables them to introduce AI-assisted video content creation into their workflow, democratizing the application of advanced AIGC tools. This shift can also expand storytelling beyond traditional media institutions to include diverse voices and perspectives. For commercial teams, our method provides them with a new chance to flexibly combine their internal results or models with the progress of the open-source community, boosting the quality of the produced videos with minor extra cost.

On the other hand, with IF-V2V's powerful capability of manipulating objects and attributes in the video, it can produce fabricated videos that appear highly realistic, posing significant challenges for verifying the authenticity of visual media. Such content can distort public perception and raise privacy concerns when fake contents featuring an individual are generated in an unauthorized way.
