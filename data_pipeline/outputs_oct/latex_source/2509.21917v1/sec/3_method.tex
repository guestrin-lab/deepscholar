\section{Methodology}
\label{sec:method}

\subsection{Task Formulation}
\label{sec:formulation}

Given a source video $x^{src}=\{x_i^{src}\}_{i=1}^L$ with $L$ frames and the edited first frame $x_1^{edit}$, image-conditioned video editing aims to propagate the modifications along the temporal dimension while maintaining overall structure and motion consistency with the source video, resulting in an edited target video $x^{tar}=\{x_i^{tar}\}_{i=1}^L$.

\subsection{Preliminaries}
\label{sec:preliminaries}

Flow-based generative models~\citep{flowmatching, rectifiedflow} formulate a probability flow ODE over timestep $t\in [0,1]$ to establish the transport map between the data distribution $p(x)$ and a standard Gaussian distribution $\mathcal{N}\sim(0,I)$:
\begin{equation}
    \mathrm{d}z_t=v(z_t,t)\mathrm{d}t,
    \label{eq:flow}
\end{equation}
where $z_t$ stands for intermediate variables and $v$ is a time-dependent vector field usually parameterized by a neural network model. For the boundary condition, $z_1$ is the noise from $\mathcal{N}\sim(0,I)$, and $z_0$ is the data from $p(x)$. To generate a sample in $p(x)$, we initialize the ODE at $t=1$ with a Gaussian sample $z_1$ and numerically solve the ODE backwards to obtain a sample $z_0$ that follows the distribution $p(x)$.

In practice, the ODE is solved numerically, with the timestep $t$ discretized into a sequence. Numerical ODE solvers are subject to discretization errors under curved ODE trajectories. Therefore, to encourage the trajectories to be \textit{straight}, flow matching models typically learn the vector field with a linear interpolation between the noise and data, using the flow matching loss function:
\begin{equation}
    \mathcal{L_\theta} = \mathbb{E}_{t,z_0\sim p(x),z_1\sim \mathcal{N}(0,I)}\left[\left\|v_\theta(z_t, t) - (z_1 - z_0)\right\|^2\right],
\end{equation}
where $\theta$ denotes the network parameters, and $z_t$ is a linear interpolation between $z_0$ and $z_1$:
\begin{equation}
    z_t=(1-t)z_0+t z_1.
\label{eq:rectifiedflow_interp}
\end{equation}

For the I2V task, the model takes an extra condition input $c$ to predict the vector field for the conditional distribution. $c$ includes the first frame of the generated video and the corresponding text prompt. For simplicity, we omit the text prompt and global conditions in $c$ in the following part.

\subsection{Vector Field Rectification with Sample Deviation (VFR-SD)}
\label{sec:vfr-sd}

Given an image as the first frame condition, the I2V model can transform Gaussian noise into a video sample by solving an ODE according to the predicted vector field of the conditional distribution. When it comes to editing, we numerically solve the following ODE:
\begin{equation}
    \mathrm{d}z_t^{tar}=v(z_t^{tar},t,c^{tar})\mathrm{d}t,
    \label{eq:ode_tar}
\end{equation}
so that the generated video $x^{tar}=z_0^{tar}$ is not only faithful to the edited frame $x_1^{edit}$ encoded in the target condition $c^{tar}$, but also consistent with the temporal evolution of original video $x^{src}$. However, the model only predicts a vector towards the \textit{expectation} of the target distribution~\citep{flowmatching, gao2025diffusionmeetsflow, gmflow}, which hinders the preservation of sample-specific properties. As a prevailing method, inversion serves as a solution to incorporate sample-specific information by mapping $x^{src}$ to the initial Gaussian noise as the boundary condition $z_1^{tar}$. Nevertheless, this process is highly inaccurate and is often accompanied by model-specific designs to further inject information from the source video $x^{src}$ to ensure consistency.

\begin{figure}
  \centering
  \begin{subfigure}{0.44\linewidth}
      \includegraphics[width=\linewidth]{figs/vfrsp_v4.pdf}
      \caption{VFR-SD constructs two parallel ODEs to map the characteristics of the source sample to the target distribution using a deviation term. The term is defined as the difference between the ground truth denoising vector $v^{gt}$ and the predicted source denoising vector $v_t^{src}$, and is used to rectify the target denoising vector $v_t^{tar}$.}
      \label{fig:vfrsd}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.54\linewidth}
      \centering
      \includegraphics[width=\linewidth]{figs/smpi.pdf}
      \caption{(i) Structural information of the source video is embedded through boundary condition initialization and flow model conditioning. (ii) Motion of the source video is incorporated through temporally correlated noise generated by warping with optical flow.}
      \label{fig:smpi}
    \end{subfigure}
  \caption{Illustration of VFR-SD (\sref{sec:vfr-sd}) and SMPI (\sref{sec:smpi}).}
\end{figure}

To overcome the sample consistency challenge, VFR-SD exploits the probabilistic properties of flow-based models~\citep{albergo2023building, gmflow} by adding a sample-specific deviation to the target denoising vector. Specifically, while solving the ODE of the target video (\cref{eq:ode_tar}), VFR-SD also constructs a parallel ODE solving process for the source video:
\begin{equation}
    \mathrm{d}z_t^{src}=v(z_t^{src},t,c^{src})\mathrm{d}t.
    \label{eq:ode_src}
\end{equation}
As the source video is already given, we know the solution and the sample-specific ground truth vector field of \cref{eq:ode_src}. By leveraging the information along the ground truth denoising path of the source video, a target sample $x^{tar}$ can be produced while respecting the source sample without model-specific designs. 

The core idea of VFR-SD is presented in \cref{fig:vfrsd}. Given the initial noise $z_{t_{max}}^{tar}$, we construct two parallel ODEs for the source and the target video distribution, respectively. The source latent variable $z_t^{src}$ moves along the ground truth denoising path, while the target latent variable $z_t^{tar}$ is updated by the rectified denoising vector $v_t$. At each timestep $t$, the flow model predicts $v_t^{src}$ based on $z_t^{src}$, which points towards the expectation of the conditional source video distribution $\mathbb{E}[x^{src}|z_t^{src}]$. Then, we compute the difference between the ground truth denoising vector $v^{gt}$ and the model prediction $v_t^{src}$, which represents the sample-specific properties that deviate from the mean of the conditional distribution. Finally, we rectify the model-predicted target denoising vector $v_t^{tar}$ using the sample-specific deviation term above:
\begin{equation}
    v_t=v_t^{tar}+\lambda (v^{gt}-v_t^{src}),
    \label{eq:vfrsd}
\end{equation}
where $\lambda$ determines the rectification scale. The rectification term maps the deviation from expectation from the source distribution to the target distribution, thus preserving characteristics of the source video. Before the next iteration, the rectified vector $v_t$ is used to update the target latent $z_t^{tar}$ through an ODE solver. Meanwhile, the source latent variable $z_t^{src}$ is also updated using the ground truth denoising vector $v^{gt}$. We detail the complete algorithm in \cref{alg:vfr-sd}, in which the target condition $c^{tar}$ is the edited first frame $x_1^{edit}$ and the source condition $c^{src}$ is the original first frame $x_1^{src}$.

\begin{algorithm}[t]
\SetAlgoNoLine
\KwIn{Source video $x^{src}$, source condition $c^{src}$, target condition $c^{tar}$, flow model $v_\theta$, initial timestep $t_{max}$, rectification scale $\lambda$.}
\KwOut{Edited video $x^{tar}$.}

% \Comment{\textcolor{mycommentcolor}{Initialization.}}

$\epsilon\sim\mathcal{N}(0,I)$

$z_{t_{max}}^{tar},z_{t_{max}}^{src}\leftarrow (1 - t_{max}) x^{src} + t_{max} \epsilon$~~~~\Comment{\textcolor{mycommentcolor}{Latents initialization.}}

% $z_{t_{max}}^{src}\leftarrow z_{t_{max}}^{tar}$

$v^{gt}\leftarrow \epsilon - x^{src}$

\Comment{\textcolor{mycommentcolor}{Numerically solve the parallel ODEs.}}
\For{$t\leftarrow t_{max}$ downto $0$}{
    $v_t^{tar}\leftarrow v_\theta(z_t^{tar},t,\bm{c}^{tar})$~~~~\Comment{\textcolor{mycommentcolor}{Predict the target denoising vector.}}

    % $z_t^{src}\leftarrow (1-\sigma _t)\bm{x}^{src}+\sigma _t\bm{N}$
    $v_t^{src}\leftarrow v_\theta(z_t^{src},t,\bm{c}^{src})$~~~~\Comment{\textcolor{mycommentcolor}{Predict the source denoising vector.}}
    
    $v_t\leftarrow v_t^{tar}+\lambda (v^{gt}-v_t^{src})$~~~~\Comment{\textcolor{mycommentcolor}{Rectification.}}

    % $z_{t-1}^{tar}\leftarrow z_t^{tar}+(\sigma _{t-1}-\sigma _t)v_t$
    $z_{t-\Delta t}^{tar}\leftarrow\texttt{solver}_{t\rightarrow t-\Delta t}(z_t^{tar},v_t)$~~~~\Comment{\textcolor{mycommentcolor}{Update target latents accordingly.}}

    $z_{t-\Delta t}^{src}\leftarrow\texttt{solver}_{t\rightarrow t-\Delta t}(z_t^{src},v_t^{gt})$~~~~\Comment{\textcolor{mycommentcolor}{Update source latents with GT vector.}}
}

\KwRet{$x^{tar}\leftarrow z_0^{tar}$}

\caption{Vector Field Rectification with Sample Deviation (VFR-
SD, \sref{sec:vfr-sd})}
\label{alg:vfr-sd}
\end{algorithm}

\subsection{Structure-and-Motion-Preserving Initialization (SMPI)}
\label{sec:smpi}

To further preserve the structure and motion of the source video without modifying internal layers of the model, we designed SMPI (\cref{fig:smpi}) to incorporate such information into the boundary condition of ODE $z_{t_{max}}^{tar}$ and the target condition $c^{tar}$.

\subsubsection{Structure-Preserving Initialization}
\label{sec:spi}
Previous research~\citep{meng2022sdedit,wang2024diffusionmodelsgenerateimages, omsdpm, hertzprompt} suggests that visual outlines are generated in the early stages of diffusion sampling and details at later timesteps.
% We embed the structural information from the source video $\bm{x}^{src}$ into both the initialization of ODE $z_{t_{max}}^{tar}$ and the target condition $\bm{c}^{tar}$. 
Consequently, to enhance the structural information from the source video $x^{src}$, we select an initial timestep $t_{max}$ that is slightly smaller than the pure noise timestep $t=1$, and initialize the boundary condition $z_{t_{max}}^{tar}$ as follows:
\begin{equation}
    z_{t_{max}}^{tar}=(1-t_{max})x^{src}+t_{max}\epsilon,
    \label{eq:smpi_init}
\end{equation}
where $\epsilon$ is a sample from the standard Gaussian distribution. By exploiting the ground-truth denoising path of the source video in early steps, this strategy ensures a better consistency of the general layout between the source and the edited video.

For mainstream I2V models, the condition $c$ consists of the concatenation of the first frame and zero paddings to align with the video length $L$. We propose to leverage these unused paddings to encode information from the source video. Specifically, we compose the target condition $c^{tar}$ as follows:
\begin{equation}
    c^{tar}=\texttt{concat}(x_1^{edit},\beta\{x_i^{src}\}_{i=2}^L),
    \label{eq:smpi_condition}
\end{equation}
where $\beta$ is the embedding scale, which should be set to a small value to align with the training setting of the model. This approach allows further reference information from the source video.

\subsubsection{Motion-Preserving Initialization}
\label{sec:mpi}
There have been methods~\citep{howiwarpedyournoise, gowiththeflow} that replace the temporal Gaussianity with warped noise derived from optical flow to achieve motion control. However, these methods require extensive training. 
To ensure motion consistency with the source video in a training-free way, we devise a noise initialization strategy to encode the motion by temporal correlation while maintaining the general Gaussianity of the noise sample. Specifically, we first extract the optical flow of the source video $\{o_i^{src}\}_{i=2}^L$, and then modulate the independent Gaussian noise sequence $\epsilon=\{\epsilon_i\}_{i=1}^L$ with the motion cue as follows:
\begin{equation}
    \begin{split}
        &\epsilon_1^m=\epsilon_1,\\
        &\epsilon_i^m=\frac{1}{\sqrt{(1-\alpha)^2+\alpha^2}}((1-\alpha)\cdot\texttt{warp}(\epsilon_{i-1},o_i^{src})+\alpha \epsilon_i),
    \end{split}
    \label{eq:smpi_motion}
\end{equation}
where \texttt{warp} stands for the 2D warping operation according to the optical flow, $\alpha$ is the blending factor to control the degree of temporal correlation, and the scaling factor is to preserve the unit covariance of the Gaussian distribution. The generated temporally correlated noise sequence $\epsilon^m=\{\epsilon_i^m\}_{i=1}^L$ is used to substitute the original i.i.d. noise sequence $\epsilon$ in \cref{alg:vfr-sd} for enhanced motion prior.

\subsection{Deviation Caching (D-Cache)}
\label{sec:dcache}

Calculating the rectification term $v^{gt}-v_t^{src}$ requires an additional pass through the flow-based model $v$, which almost doubles the computation. Inspired by recent work that explores using cached states to bypass some computation~\citep{teacache, pab}, we designed the D-Cache mechanism to reduce the cost of denoising vector rectification to an acceptable scale.

D-Cache reuses the previously calculated deviation term when the variation is small. Given that $v^{gt}$ is constant throughout the denoising process, it is $v_t^{src}$ that accounts for the variation. However, it is impossible to know the extent of its change before we compute $v_t^{src}$ through the model $v$. To estimate its change at the current timestep, we adopt the variation of the target denoising vector $v_t^{tar}$ as they are predicted by the same flow model $v$ to solve parallel ODEs at the same timestep $t$, and $v_t^{tar}$ can be obtained before predicting $v_t^{src}$. To be specific, we define the cumulative variation between $t_a$ and $t_b$ ($t_a < t_b$) as follows:
\begin{equation}
    d(t_a,t_b)=\sum_{t=t_a}^{t_b - \Delta t}\|v_t^{tar}-v_{t+\Delta t}^{tar}\|_1,
    \label{eq:dcache}
\end{equation}
where $\Delta t$ is the step size. At the current timestep $t$, we calculate the cumulative variation starting from a previous timestep $t_p$. If $d(t,t_p)\leq\delta$, we use the cached source denoising vector $v_{t_p}^{src}$ as $v_t^{src}$ instead of predicting through the model. $\delta$ is designated as the caching threshold. By reusing the deviation term when the variation is minor, we achieve more efficient vector rectification without significantly compromising editing quality.
