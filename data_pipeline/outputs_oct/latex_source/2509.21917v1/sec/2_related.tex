\section{Related Work}
\label{sec:related}

\sssection{Image-to-video Generation.} Visual content generation and editing have witnessed significant advancements thanks to the emergence of diffusion models~\citep{ddpm, ddim, ldm}. Recently, DiT~\citep{dit} has become the mainstream architecture of the denoising model with promising generation quality, surpassing U-Net~\citep{unet} with its powerful scaling capability~\citep{kaplan2020scalinglawsneurallanguage} and potential for multimodal interaction~\citep{sd3}. Flow Matching~\citep{flowmatching, rectifiedflow} introduces an improved generative model paradigm that interpolates data and noise linearly in the forward diffusion process, bringing better theoretical properties and conceptual simplicity. Building upon these works, a number of I2V models~\citep{wan, easyanimate, hunyuanvideo, cogvideox, opensora2, vchitect2} have emerged with full 3D attention~\citep{attention2017} instead of decoupled spatiotemporal attention~\citep{guo2024animatediff}, significantly enhancing generation quality and consistency.

% Image-conditioned video editing methods leverage the temporal prior of I2V models to propagate the edited keyframe along the temporal dimension while preserving structure and motion consistency with the source video. Videoshop~\citep{videoshop} introduces noise extrapolation to enhance the inversion process. DreamMotion~\citep{dreammotion} utilizes score distillation sampling (SDS)~\citep{pooledreamfusion} to optimize the source video latents towards the condition image, during which space-time self-similarities constraints are applied to better match the source video. I2VEdit~\citep{i2vedit} first trains a sample-specific motion LoRA~\citep{lora} and then performs attention matching between the EDM~\citep{edm} inversion and denoising process. AnyV2V~\citep{kuanyv2v} performs DDIM~\citep{ddim} inversion and exploits an attention injection paradigm to ensure consistency with the source video. VideoRepainter~\citep{videorepainter} repurposes an I2V model for editing by fine-tuning it with a symmetric condition mechanism to avoid mask ambiguity caused by downsampling. VACE~\citep{vace} provides an all-in-one solution for video editing by introducing a ControlNet-style~\citep{controlnet} Context Adapter structure, which requires extensive training. These approaches include either model-specific designs or costly optimization, limiting their ability to keep up with the rapid advancement of I2V models.

\sssection{Training-free Visual Editing.} Training-free visual editing modifies the source image or video according to designated conditions (e.g., text, image, and mask) at test time, using off-the-shelf pretrained models. Existing works can be broadly categorized into two categories: inversion-based and optimization-based methods. Inversion-based methods~\citep{videoshop, dni, wave, yatim2025dynvfxaugmentingrealvideos} adopt the inversion of the diffusion process to map the input back to Gaussian noise, and then perform denoising under given conditions. However, not only is the inversion process time-consuming, but it also inevitably induces error. To overcome the inherent inaccuracy of inversion and ensure consistency with the input, various attention injection strategies~\citep{wave, yatim2025dynvfxaugmentingrealvideos} are utilized to further incorporate source information. Despite their effectiveness, these strategies are model-specific, reducing their universality to different model structures. Optimization-based methods~\citep{dreammotion, ren2025fdsfrequencyawaredenoisingscore, unityindiversity} use SDS~\citep{pooledreamfusion} to directly optimize the input latents towards the desired direction. Nevertheless, the optimization operation introduces considerable computational cost, limiting its availability to common creators. With the prevalence of flow-based models~\citep{flowmatching, rectifiedflow}, there have also been methods~\citep{avrahami2025stableflowvitallayers, dalva2024fluxspacedisentangledsemanticediting, xu2025unveilinversioninvarianceflow} that leverage the properties of the flow matching process to achieve more precise and consistent visual editing. However, few solutions are both lightweight and universal without model-specific design in the video domain, limiting creators to swiftly leverage the most up-to-date I2V base models for video editing within user-friendly resources, such as a single GPU.
% to take the temporal dimension into account.

There have also been works exploring inversion-free image editing. For instance, InfEdit~\citep{infedit} theoretically depends on the diffusion process, limiting its application to state-of-the-art flow-based models. It also needs attention manipulation, further limiting its universality. FlowEdit~\citep{kulikov2024floweditinversionfreetextbasedediting} leverages flow properties to construct a transport from the source to the target distribution, which is derived from the Euler Discrete Solver~\citep{sd3}. In contrast, our method constructs two parallel ODEs to model the editing process, which does not depend on a specific ODE solver and enables control over editing strength. Our method also introduces SMPI to further enhance video-level spatiotemporal consistency and a flexible caching strategy.
