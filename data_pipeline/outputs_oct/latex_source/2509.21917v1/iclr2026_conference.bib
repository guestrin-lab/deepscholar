@misc{filmsurvey,
      title={Generative AI for Film Creation: A Survey of Recent Advances}, 
      author={Ruihan Zhang and Borou Yu and Jiajian Min and Yetong Xin and Zheng Wei and Juncheng Nemo Shi and Mingzhen Huang and Xianghao Kong and Nix Liu Xin and Shanshan Jiang and Praagya Bahuguna and Mark Chan and Khushi Hora and Lijian Yang and Yongqi Liang and Runhe Bian and Yunlei Liu and Isabela Campillo Valencia and Patricia Morales Tredinick and Ilia Kozlov and Sijia Jiang and Peiwen Huang and Na Chen and Xuanxuan Liu and Anyi Rao},
      year={2025},
      eprint={2504.08296},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2504.08296}, 
}

@misc{stepedit,
      title={Step1X-Edit: A Practical Framework for General Image Editing}, 
      author={Shiyu Liu and Yucheng Han and Peng Xing and Fukun Yin and Rui Wang and Wei Cheng and Jiaqi Liao and Yingming Wang and Honghao Fu and Chunrui Han and Guopeng Li and Yuang Peng and Quan Sun and Jingwei Wu and Yan Cai and Zheng Ge and Ranchen Ming and Lei Xia and Xianfang Zeng and Yibo Zhu and Binxing Jiao and Xiangyu Zhang and Gang Yu and Daxin Jiang},
      year={2025},
      eprint={2504.17761},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2504.17761}, 
}

@misc{seededit,
      title={SeedEdit: Align Image Re-Generation to Image Editing}, 
      author={Yichun Shi and Peng Wang and Weilin Huang},
      year={2024},
      eprint={2411.06686},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2411.06686}, 
}

@misc{gpt4oimg,
      title={Introducing 4o Image Generation}, 
      author={OpenAI},
      year={2025},
      url={https://openai.com/index/introducing-4o-image-generation}, 
}

@misc{omnigen,
      title={OmniGen: Unified Image Generation}, 
      author={Shitao Xiao and Yueze Wang and Junjie Zhou and Huaying Yuan and Xingrun Xing and Ruiran Yan and Chaofan Li and Shuting Wang and Tiejun Huang and Zheng Liu},
      year={2024},
      eprint={2409.11340},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.11340}, 
}

@misc{ace,
      title={ACE: All-round Creator and Editor Following Instructions via Diffusion Transformer}, 
      author={Zhen Han and Zeyinzi Jiang and Yulin Pan and Jingfeng Zhang and Chaojie Mao and Chenwei Xie and Yu Liu and Jingren Zhou},
      year={2024},
      eprint={2410.00086},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.00086}, 
}

@misc{acepp,
      title={ACE++: Instruction-Based Image Creation and Editing via Context-Aware Content Filling}, 
      author={Chaojie Mao and Jingfeng Zhang and Yulin Pan and Zeyinzi Jiang and Zhen Han and Yu Liu and Jingren Zhou},
      year={2025},
      eprint={2501.02487},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2501.02487}, 
}

@INPROCEEDINGS{smartedit,
  author={Huang, Yuzhou and Xie, Liangbin and Wang, Xintao and Yuan, Ziyang and Cun, Xiaodong and Ge, Yixiao and Zhou, Jiantao and Dong, Chao and Huang, Rui and Zhang, Ruimao and Shan, Ying},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={SmartEdit: Exploring Complex Instruction-Based Image Editing with Multimodal Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={8362-8371},
  keywords={Training;Visualization;Computer vision;Large language models;Diffusion models;Cognition;Pattern recognition;Instruction-based Image Editing;Multimodal Large Language Models},
  doi={10.1109/CVPR52733.2024.00799}}

@misc{anyedit,
      title={AnyEdit: Mastering Unified High-Quality Image Editing for Any Idea}, 
      author={Qifan Yu and Wei Chow and Zhongqi Yue and Kaihang Pan and Yang Wu and Xiaoyang Wan and Juncheng Li and Siliang Tang and Hanwang Zhang and Yueting Zhuang},
      year={2025},
      eprint={2411.15738},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2411.15738}, 
}

@inproceedings{ultraedit,
 author = {Zhao, Haozhe and Ma, Xiaojian and Chen, Liang and Si, Shuzheng and Wu, Rujie and An, Kaikai and Yu, Peiyu and Zhang, Minjia and Li, Qing and Chang, Baobao},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {3058--3093},
 publisher = {Curran Associates, Inc.},
 title = {UltraEdit: Instruction-based Fine-Grained Image Editing at Scale},
 volume = {37},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/05a30a0fc9e6bacdd3abd4ca8508a9e6-Paper-Datasets_and_Benchmarks_Track.pdf},
 year = {2024}
}
 % 

@misc{videoeditingsurvey,
      title={Diffusion Model-Based Video Editing: A Survey}, 
      author={Wenhao Sun and Rong-Cheng Tu and Jingyi Liao and Dacheng Tao},
      year={2024},
      eprint={2407.07111},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2407.07111}, 
}

@article{edm,
  title={Elucidating the design space of diffusion-based generative models},
  author={Karras, Tero and Aittala, Miika and Aila, Timo and Laine, Samuli},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={26565--26577},
  year={2022}
}

@inproceedings{i2vedit,
author = {Ouyang, Wenqi and Dong, Yi and Yang, Lei and Si, Jianlou and Pan, Xingang},
title = {I2VEdit: First-Frame-Guided Video Editing via Image-to-Video Diffusion Models},
year = {2024},
isbn = {9798400711312},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3680528.3687656},
doi = {10.1145/3680528.3687656},
abstract = {The remarkable generative capabilities of diffusion models have motivated extensive research in both image and video editing. Compared to video editing which faces additional challenges in the time dimension, image editing has witnessed the development of more diverse, high-quality approaches and more capable software like Photoshop. In light of this gap, we introduce a novel and generic solution that extends the applicability of image editing tools to videos by propagating edits from a single frame to the entire video using a pre-trained image-to-video model. Our method, dubbed I2VEdit, adaptively preserves the visual and motion integrity of the source video depending on the extent of the edits, effectively handling global edits, local edits, and moderate shape changes, which existing methods cannot fully achieve. At the core of our method are two main processes: Coarse Motion Extraction to align basic motion patterns with the original video, and Appearance Refinement for precise adjustments using fine-grained attention matching. We also incorporate a skip-interval strategy to mitigate quality degradation from auto-regressive generation across multiple video clips. Experimental results demonstrate our framework’s superior performance in fine-grained video editing, proving its capability to produce high-quality, temporally consistent outputs.},
booktitle = {SIGGRAPH Asia 2024 Conference Papers},
articleno = {95},
numpages = {11},
keywords = {Video Editing, Image Editing, Stable Video Diffusion},
location = {Tokyo, Japan},
series = {SA '24}
}

@misc{magicprop,
      title={MagicProp: Diffusion-based Video Editing via Motion-aware Appearance Propagation}, 
      author={Hanshu Yan and Jun Hao Liew and Long Mai and Shanchuan Lin and Jiashi Feng},
      year={2023},
      eprint={2309.00908},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2309.00908}, 
}

@inproceedings{motioni2v,
author = {Shi, Xiaoyu and Huang, Zhaoyang and Wang, Fu-Yun and Bian, Weikang and Li, Dasong and Zhang, Yi and Zhang, Manyuan and Cheung, Ka Chun and See, Simon and Qin, Hongwei and Dai, Jifeng and Li, Hongsheng},
title = {Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling},
year = {2024},
isbn = {9798400705250},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641519.3657497},
doi = {10.1145/3641519.3657497},
abstract = {We introduce Motion-I2V, a novel framework for consistent and controllable text-guided image-to-video generation (I2V). In contrast to previous methods that directly learn the complicated image-to-video mapping, Motion-I2V factorizes I2V into two stages with explicit motion modeling. For the first stage, we propose a diffusion-based motion field predictor, which focuses on deducing the trajectories of the reference image’s pixels. For the second stage, we propose motion-augmented temporal attention to enhance the limited 1-D temporal attention in video latent diffusion models. This module can effectively propagate reference image features to synthesized frames with the guidance of predicted trajectories from the first stage. Compared with existing methods, Motion-I2V can generate more consistent videos even in the presence of large motion and viewpoint variation. By training a sparse trajectory ControlNet for the first stage, Motion-I2V can support users to precisely control motion trajectories and motion regions with sparse trajectory and region. This offers more controllability of the I2V process than solely relying on textual instructions. Additionally, Motion-I2V’s second stage naturally supports zero-shot video-to-video translation. Both qualitative and quantitative comparisons demonstrate the advantages of Motion-I2V over prior approaches in consistent and controllable image-to-video generation. Please see our project page at https://xiaoyushi97.github.io/Motion-I2V/.},
booktitle = {ACM SIGGRAPH 2024 Conference Papers},
articleno = {111},
numpages = {11},
keywords = {Diffusion models, Image animation},
location = {Denver, CO, USA},
series = {SIGGRAPH '24}
}

@InProceedings{videoshop,
author="Fan, Xiang
and Bhattad, Anand
and Krishna, Ranjay",
editor="Leonardis, Ale{\v{s}}
and Ricci, Elisa
and Roth, Stefan
and Russakovsky, Olga
and Sattler, Torsten
and Varol, G{\"u}l",
title="VIDEOSHOP: Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion",
booktitle="Computer Vision -- ECCV 2024",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="232--250",
abstract="We introduce Videoshop, a training-free video editing algorithm for localized semantic edits. Videoshop allows users to use any editing software, including Photoshop and generative inpainting, to modify the first frame; it automatically propagates those changes, with semantic, spatial, and temporally consistent motion, to the remaining frames. Unlike existing methods that enable edits only through imprecise textual instructions, Videoshop allows users to add or remove objects, semantically change objects, insert stock photos into videos, etc. with fine-grained control over locations and appearance. We achieve this through image-based video editing by inverting latents with noise extrapolation, from which we generate videos conditioned on the edited image. Videoshop produces higher quality edits against 6 baselines on 2 editing benchmarks using 10 evaluation metrics.",
isbn="978-3-031-73254-6"
}

@misc{moca,
      title={Motion-Conditioned Image Animation for Video Editing}, 
      author={Wilson Yan and Andrew Brown and Pieter Abbeel and Rohit Girdhar and Samaneh Azadi},
      year={2023},
      eprint={2311.18827},
      archivePrefix={arXiv},
      primaryClass={cs.GR},
      url={https://arxiv.org/abs/2311.18827}, 
}

@misc{vace,
      title={VACE: All-in-One Video Creation and Editing}, 
      author={Zeyinzi Jiang and Zhen Han and Chaojie Mao and Jingfeng Zhang and Yulin Pan and Yu Liu},
      year={2025},
      eprint={2503.07598},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2503.07598}, 
}

@InProceedings{save,
author="Song, Yeji
and Shin, Wonsik
and Lee, Junsoo
and Kim, Jeesoo
and Kwak, Nojun",
editor="Leonardis, Ale{\v{s}}
and Ricci, Elisa
and Roth, Stefan
and Russakovsky, Olga
and Sattler, Torsten
and Varol, G{\"u}l",
title="SAVE: Protagonist Diversification with Structure Agnostic Video Editing",
booktitle="Computer Vision -- ECCV 2024",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="41--57",
abstract="Driven by the upsurge progress in text-to-image (T2I) generation models, text-to-video (T2V) generation has experienced a significant advance as well. Accordingly, tasks such as modifying the object or changing the style in a video have been possible. However, previous works usually work well on trivial and consistent shapes, and easily collapse on a difficult target that has a largely different body shape from the original one. In this paper, we spot the bias problem in the existing video editing method that restricts the range of choices for the new protagonist and attempt to address this issue using the conventional image-level personalization method. We adopt motion personalization that isolates the motion from a single source video and then modifies the protagonist accordingly. To deal with the natural discrepancy between image and video, we propose a motion word with an inflated textual embedding to properly represent the motion in a source video. We also regulate the motion word to attend to proper motion-related areas by introducing a novel pseudo optical flow, efficiently computed from the pre-calculated attention maps. Finally, we decouple the motion from the appearance of the source video with an additional pseudo word. Extensive experiments demonstrate the editing capability of our method, taking a step toward more diverse and extensive video editing. Our project page: https://ldynx.github.io/SAVE/",
isbn="978-3-031-72989-8"
}

@INPROCEEDINGS{dit,
  author={Peebles, William and Xie, Saining},
  booktitle={2023 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Scalable Diffusion Models with Transformers}, 
  year={2023},
  volume={},
  number={},
  pages={4172-4182},
  keywords={Computer vision;Computational modeling;Scalability;Computer architecture;Benchmark testing;Transformers;Complexity theory},
  doi={10.1109/ICCV51070.2023.00387}}

@inproceedings{flowmatching,
  title={Flow Matching for Generative Modeling},
  author={Lipman, Yaron and Chen, Ricky TQ and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matthew},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@misc{wan,
      title={Wan: Open and Advanced Large-Scale Video Generative Models}, 
      author={Team Wan and Ang Wang and Baole Ai and Bin Wen and Chaojie Mao and Chen-Wei Xie and Di Chen and Feiwu Yu and Haiming Zhao and Jianxiao Yang and Jianyuan Zeng and Jiayu Wang and Jingfeng Zhang and Jingren Zhou and Jinkai Wang and Jixuan Chen and Kai Zhu and Kang Zhao and Keyu Yan and Lianghua Huang and Mengyang Feng and Ningyi Zhang and Pandeng Li and Pingyu Wu and Ruihang Chu and Ruili Feng and Shiwei Zhang and Siyang Sun and Tao Fang and Tianxing Wang and Tianyi Gui and Tingyu Weng and Tong Shen and Wei Lin and Wei Wang and Wei Wang and Wenmeng Zhou and Wente Wang and Wenting Shen and Wenyuan Yu and Xianzhong Shi and Xiaoming Huang and Xin Xu and Yan Kou and Yangyu Lv and Yifei Li and Yijing Liu and Yiming Wang and Yingya Zhang and Yitong Huang and Yong Li and You Wu and Yu Liu and Yulin Pan and Yun Zheng and Yuntao Hong and Yupeng Shi and Yutong Feng and Zeyinzi Jiang and Zhen Han and Zhi-Fan Wu and Ziyu Liu},
      year={2025},
      eprint={2503.20314},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2503.20314}, 
}

@misc{easyanimate,
      title={EasyAnimate: A High-Performance Long Video Generation Method based on Transformer Architecture}, 
      author={Jiaqi Xu and Xinyi Zou and Kunzhe Huang and Yunkuo Chen and Bo Liu and MengLi Cheng and Xing Shi and Jun Huang},
      year={2024},
      eprint={2405.18991},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2405.18991}, 
}

@misc{hunyuanvideo,
      title={HunyuanVideo: A Systematic Framework For Large Video Generative Models}, 
      author={Weijie Kong and Qi Tian and Zijian Zhang and Rox Min and Zuozhuo Dai and Jin Zhou and Jiangfeng Xiong and Xin Li and Bo Wu and Jianwei Zhang and Kathrina Wu and Qin Lin and Junkun Yuan and Yanxin Long and Aladdin Wang and Andong Wang and Changlin Li and Duojun Huang and Fang Yang and Hao Tan and Hongmei Wang and Jacob Song and Jiawang Bai and Jianbing Wu and Jinbao Xue and Joey Wang and Kai Wang and Mengyang Liu and Pengyu Li and Shuai Li and Weiyan Wang and Wenqing Yu and Xinchi Deng and Yang Li and Yi Chen and Yutao Cui and Yuanbo Peng and Zhentao Yu and Zhiyu He and Zhiyong Xu and Zixiang Zhou and Zunnan Xu and Yangyu Tao and Qinglin Lu and Songtao Liu and Dax Zhou and Hongfa Wang and Yong Yang and Di Wang and Yuhong Liu and Jie Jiang and Caesar Zhong},
      year={2025},
      eprint={2412.03603},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2412.03603}, 
}

@misc{cogvideox,
      title={CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer}, 
      author={Zhuoyi Yang and Jiayan Teng and Wendi Zheng and Ming Ding and Shiyu Huang and Jiazheng Xu and Yuanming Yang and Wenyi Hong and Xiaohan Zhang and Guanyu Feng and Da Yin and Yuxuan Zhang and Weihan Wang and Yean Cheng and Bin Xu and Xiaotao Gu and Yuxiao Dong and Jie Tang},
      year={2025},
      eprint={2408.06072},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2408.06072}, 
}

@misc{opensora2,
      title={Open-Sora 2.0: Training a Commercial-Level Video Generation Model in \$200k}, 
      author={Xiangyu Peng and Zangwei Zheng and Chenhui Shen and Tom Young and Xinying Guo and Binluo Wang and Hang Xu and Hongxin Liu and Mingyan Jiang and Wenjun Li and Yuhui Wang and Anbang Ye and Gang Ren and Qianran Ma and Wanying Liang and Xiang Lian and Xiwen Wu and Yuting Zhong and Zhuangyan Li and Chaoyu Gong and Guojun Lei and Leijun Cheng and Limin Zhang and Minghao Li and Ruijie Zhang and Silan Hu and Shijie Huang and Xiaokang Wang and Yuanheng Zhao and Yuqi Wang and Ziang Wei and Yang You},
      year={2025},
      eprint={2503.09642},
      archivePrefix={arXiv},
      primaryClass={cs.GR},
      url={https://arxiv.org/abs/2503.09642}, 
}

@misc{vchitect2,
      title={Vchitect-2.0: Parallel Transformer for Scaling Up Video Diffusion Models}, 
      author={Weichen Fan and Chenyang Si and Junhao Song and Zhenyu Yang and Yinan He and Long Zhuo and Ziqi Huang and Ziyue Dong and Jingwen He and Dongwei Pan and Yi Wang and Yuming Jiang and Yaohui Wang and Peng Gao and Xinyuan Chen and Hengjie Li and Dahua Lin and Yu Qiao and Ziwei Liu},
      year={2025},
      eprint={2501.08453},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2501.08453}, 
}

@misc{pab,
      title={Real-Time Video Generation with Pyramid Attention Broadcast}, 
      author={Xuanlei Zhao and Xiaolong Jin and Kai Wang and Yang You},
      year={2025},
      eprint={2408.12588},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2408.12588}, 
}

@InProceedings{dreammotion,
author="Jeong, Hyeonho
and Chang, Jinho
and Park, Geon Yeong
and Ye, Jong Chul",
editor="Leonardis, Ale{\v{s}}
and Ricci, Elisa
and Roth, Stefan
and Russakovsky, Olga
and Sattler, Torsten
and Varol, G{\"u}l",
title="DreamMotion: Space-Time Self-similar Score Distillation for Zero-Shot Video Editing",
booktitle="Computer Vision -- ECCV 2024",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="358--376",
abstract="Text-driven diffusion-based video editing presents a unique challenge not encountered in image editing literature: establishing real-world motion. Unlike existing video editing approaches, here we focus on score distillation sampling to circumvent the standard reverse diffusion process and initiate optimization from videos that already exhibit natural motion. Our analysis reveals that while video score distillation can effectively introduce new content indicated by target text, it can also cause significant structure and motion deviation. To counteract this, we propose to match the space-time self-similarities of the original video and the edited video during the score distillation. Thanks to the use of score distillation, our approach is model-agnostic, which can be applied for both cascaded and non-cascaded video diffusion frameworks. Through extensive comparisons with leading methods, our approach demonstrates its superiority in altering appearances while accurately preserving the original structure and motion.",
isbn="978-3-031-73404-5"
}

@inproceedings{ddpm,
 author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {6840--6851},
 publisher = {Curran Associates, Inc.},
 title = {Denoising Diffusion Probabilistic Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{ddim,
  title={Denoising Diffusion Implicit Models},
  author={Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@INPROCEEDINGS{ldm,
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
  year={2022},
  volume={},
  number={},
  pages={10674-10685},
  keywords={Training;Visualization;Image synthesis;Computational modeling;Noise reduction;Superresolution;Process control;Image and video synthesis and generation},
  doi={10.1109/CVPR52688.2022.01042}}

@misc{kaplan2020scalinglawsneurallanguage,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.08361}, 
}

@inproceedings{attention2017,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 volume = {30},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 year = {2017}
}
 % 

@InProceedings{unet,
author="Ronneberger, Olaf
and Fischer, Philipp
and Brox, Thomas",
editor="Navab, Nassir
and Hornegger, Joachim
and Wells, William M.
and Frangi, Alejandro F.",
title="U-Net: Convolutional Networks for Biomedical Image Segmentation",
booktitle="Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="234--241",
abstract="There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.",
isbn="978-3-319-24574-4"
}


@InProceedings{sd3,
  title = 	 {Scaling Rectified Flow Transformers for High-Resolution Image Synthesis},
  author =       {Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and M\"{u}ller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and Podell, Dustin and Dockhorn, Tim and English, Zion and Rombach, Robin},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {12606--12633},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/esser24a/esser24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/esser24a.html},
  abstract = 	 {Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models. Stability AI is considering making experimental data, code, and model weights publicly available.}
}

@inproceedings{guo2024animatediff,
  title={AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning},
  author={Guo, Yuwei and Yang, Ceyuan and Rao, Anyi and Liang, Zhengyang and Wang, Yaohui and Qiao, Yu and Agrawala, Maneesh and Lin, Dahua and Dai, Bo},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{lora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@INPROCEEDINGS{videorepainter,
  author={Guo, Yuwei and Yang, Ceyuan and Rao Anyi and Meng Chenlin and Bar-Tal Omer and Ding Shuangrui and Agrawala, Maneesh and Lin, Dahua and Dai, Bo},
  booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Keyframe-Guided Creative Video Inpainting}, 
  year={2025},
}

@inproceedings{pooledreamfusion,
  title={DreamFusion: Text-to-3D using 2D Diffusion},
  author={Poole, Ben and Jain, Ajay and Barron, Jonathan T and Mildenhall, Ben},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@INPROCEEDINGS{controlnet,
  author={Zhang, Lvmin and Rao, Anyi and Agrawala, Maneesh},
  booktitle={2023 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Adding Conditional Control to Text-to-Image Diffusion Models}, 
  year={2023},
  volume={},
  number={},
  pages={3813-3824},
  keywords={Training;Image segmentation;Computer vision;Image coding;Image edge detection;Neural networks;Computer architecture},
  doi={10.1109/ICCV51070.2023.00355}}

@article{kuanyv2v,
  title={AnyV2V: A Tuning-Free Framework For Any Video-to-Video Editing Tasks},
  author={Ku, Max and Wei, Cong and Ren, Weiming and Yang, Huan and Chen, Wenhu},
  journal={Transactions on Machine Learning Research},
  year={2024}
}

@InProceedings{dni,
author="Yoon, Sunjae
and Koo, Gwanhyeong
and Hong, Ji Woo
and Yoo, Chang D.",
editor="Leonardis, Ale{\v{s}}
and Ricci, Elisa
and Roth, Stefan
and Russakovsky, Olga
and Sattler, Torsten
and Varol, G{\"u}l",
title="DNI: Dilutional Noise Initialization for Diffusion Video Editing",
booktitle="Computer Vision -- ECCV 2024",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="180--195",
abstract="Text-based diffusion video editing systems have been successful in performing edits with high fidelity and textual alignment. However, this success is limited to rigid-type editing such as style transfer and object overlay, while preserving the original structure of the input video. This limitation stems from an initial latent noise employed in diffusion video editing systems. The diffusion video editing systems prepare initial latent noise to edit by gradually infusing Gaussian noise onto the input video. However, we observed that the visual structure of the input video still persists within this initial latent noise, thereby restricting non-rigid editing such as motion change necessitating structural modifications. To this end, this paper proposes Dilutional Noise Initialization (DNI) framework which enables editing systems to perform precise and dynamic modification including non-rigid editing. DNI introduces a concept of `noise dilution' which adds further noise to the latent noise in the region to be edited to soften the structural rigidity imposed by input video, resulting in more effective edits closer to the target prompt. Extensive experiments demonstrate the effectiveness of the DNI framework.",
isbn="978-3-031-73195-2"
}

@InProceedings{wave,
author="Feng, Yutang
and Gao, Sicheng
and Bao, Yuxiang
and Wang, Xiaodi
and Han, Shumin
and Zhang, Juan
and Zhang, Baochang
and Yao, Angela",
editor="Leonardis, Ale{\v{s}}
and Ricci, Elisa
and Roth, Stefan
and Russakovsky, Olga
and Sattler, Torsten
and Varol, G{\"u}l",
title="WAVE: Warping DDIM Inversion Features for Zero-Shot Text-to-Video Editing",
booktitle="Computer Vision -- ECCV 2024",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="38--55",
abstract="Text-driven video editing has emerged as a prominent application based on the breakthroughs of image diffusion models. Existing state-of-the-art methods focus on zero-shot frameworks due to limited training data and computing resources. To preserve structure consistency, previous frameworks usually employ Denoising Diffusion Implicit Model (DDIM) inversion to provide inverted noise latents as guidance. The key challenge lies in limiting errors caused by the randomness and inaccuracy in each step of the na{\"i}ve DDIM inversion process, which can lead to temporal inconsistency in video editing tasks. Our observation indicates that incorporating temporal keyframe information can alleviate the accumulated error during inversion. In this paper, we propose an effective warping strategy in the feature domain to obtain high-quality DDIM inverted noise latents. Specifically, we shuffle the editing frames randomly in each timestep and use optical flow extracted from the source video to propagate the latent features of the first keyframe to subsequent keyframes. Moreover, we develop a comprehensive zero-shot framework that adapts to this strategy in both the inversion and denoising processes, thereby facilitating the generation of consistent edited videos. We compare our method with state-of-the-art text-driven editing methods on various real-world videos with different forms of motion. The project page is available at https://ree1s.github.io/wave/.",
isbn="978-3-031-73116-7"
}

@misc{yatim2025dynvfxaugmentingrealvideos,
      title={DynVFX: Augmenting Real Videos with Dynamic Content}, 
      author={Danah Yatim and Rafail Fridman and Omer Bar-Tal and Tali Dekel},
      year={2025},
      eprint={2502.03621},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2502.03621}, 
}

@misc{avrahami2025stableflowvitallayers,
      title={Stable Flow: Vital Layers for Training-Free Image Editing}, 
      author={Omri Avrahami and Or Patashnik and Ohad Fried and Egor Nemchinov and Kfir Aberman and Dani Lischinski and Daniel Cohen-Or},
      year={2025},
      eprint={2411.14430},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2411.14430}, 
}

@misc{ren2025fdsfrequencyawaredenoisingscore,
      title={FDS: Frequency-Aware Denoising Score for Text-Guided Latent Diffusion Image Editing}, 
      author={Yufan Ren and Zicong Jiang and Tong Zhang and Søren Forchhammer and Sabine Süsstrunk},
      year={2025},
      eprint={2503.19191},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2503.19191}, 
}

@INPROCEEDINGS{unityindiversity,
  author={Gao, Junyu and Yang, Kunlin and Yao, Xuan and Hu, Yufan},
  booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Unity in Diversity: Video Editing via Gradient-Latent Purification}, 
  year={2025},
}

@misc{kulikov2024floweditinversionfreetextbasedediting,
      title={FlowEdit: Inversion-Free Text-Based Editing Using Pre-Trained Flow Models}, 
      author={Vladimir Kulikov and Matan Kleiner and Inbar Huberman-Spiegelglas and Tomer Michaeli},
      year={2024},
      eprint={2412.08629},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2412.08629}, 
}

@misc{dalva2024fluxspacedisentangledsemanticediting,
      title={FluxSpace: Disentangled Semantic Editing in Rectified Flow Transformers}, 
      author={Yusuf Dalva and Kavana Venkatesh and Pinar Yanardag},
      year={2024},
      eprint={2412.09611},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2412.09611}, 
}

@misc{xu2025unveilinversioninvarianceflow,
      title={Unveil Inversion and Invariance in Flow Transformer for Versatile Image Editing}, 
      author={Pengcheng Xu and Boyuan Jiang and Xiaobin Hu and Donghao Luo and Qingdong He and Jiangning Zhang and Chengjie Wang and Yunsheng Wu and Charles Ling and Boyu Wang},
      year={2025},
      eprint={2411.15843},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2411.15843}, 
}

@misc{rectifiedflow,
      title={Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow}, 
      author={Xingchao Liu and Chengyue Gong and Qiang Liu},
      year={2022},
      eprint={2209.03003},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2209.03003}, 
}

@misc{wang2024diffusionmodelsgenerateimages,
      title={Diffusion Models Generate Images Like Painters: an Analytical Theory of Outline First, Details Later}, 
      author={Binxu Wang and John J. Vastola},
      year={2024},
      eprint={2303.02490},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2303.02490}, 
}


@InProceedings{omsdpm,
  title = 	 {{OMS}-{DPM}: Optimizing the Model Schedule for Diffusion Probabilistic Models},
  author =       {Liu, Enshu and Ning, Xuefei and Lin, Zinan and Yang, Huazhong and Wang, Yu},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {21915--21936},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/liu23ab/liu23ab.pdf},
  url = 	 {https://proceedings.mlr.press/v202/liu23ab.html},
  abstract = 	 {Diffusion probabilistic models (DPMs) are a new class of generative models that have achieved state-of-the-art generation quality in various domains. Despite the promise, one major drawback of DPMs is the slow generation speed due to the large number of neural network evaluations required in the generation process. In this paper, we reveal an overlooked dimension—model schedule—for optimizing the trade-off between generation quality and speed. More specifically, we observe that small models, though having worse generation quality when used alone, could outperform large models in certain generation steps. Therefore, unlike the traditional way of using a single model, using different models in different generation steps in a carefully designed model schedule could potentially improve generation quality and speed simultaneously. We design OMS-DPM, a predictor-based search algorithm, to determine the optimal model schedule given an arbitrary generation time budget and a set of pre-trained models. We demonstrate that OMS-DPM can find model schedules that improve generation quality and speed than prior state-of-the-art methods across CIFAR-10, CelebA, ImageNet, and LSUN datasets. When applied to the public checkpoints of the Stable Diffusion model, we are able to accelerate the sampling by 2x while maintaining the generation quality.}
}

@inproceedings{hertzprompt,
  title={Prompt-to-Prompt Image Editing with Cross-Attention Control},
  author={Hertz, Amir and Mokady, Ron and Tenenbaum, Jay and Aberman, Kfir and Pritch, Yael and Cohen-or, Daniel},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@misc{teacache,
      title={Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model}, 
      author={Feng Liu and Shiwei Zhang and Xiaofeng Wang and Yujie Wei and Haonan Qiu and Yuzhong Zhao and Yingya Zhang and Qixiang Ye and Fang Wan},
      year={2025},
      eprint={2411.19108},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2411.19108}, 
}

@inproceedings{cfg,
  title={Classifier-Free Diffusion Guidance},
  author={Ho, Jonathan and Salimans, Tim},
  booktitle={NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications},
  year={2021}
}

@INPROCEEDINGS{davis,
  author={Perazzi, F. and Pont-Tuset, J. and McWilliams, B. and Van Gool, L. and Gross, M. and Sorkine-Hornung, A.},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation}, 
  year={2016},
  volume={},
  number={},
  pages={724-732},
  keywords={Object segmentation;Motion segmentation;Computer vision;Benchmark testing;Image segmentation;Algorithm design and analysis;Manuals},
  doi={10.1109/CVPR.2016.85}}

@inproceedings{laion5b,
 author = {Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and Schramowski, Patrick and Kundurthy, Srivatsa and Crowson, Katherine and Schmidt, Ludwig and Kaczmarczyk, Robert and Jitsev, Jenia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {25278--25294},
 publisher = {Curran Associates, Inc.},
 title = {LAION-5B: An open large-scale dataset for training next generation image-text models},
 volume = {35},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/a1859debfb3b59d094f3504d5ebb6c25-Paper-Datasets_and_Benchmarks.pdf},
 year = {2022}
}
 % 

@InProceedings{clip,
  title = 	 {Learning Transferable Visual Models From Natural Language Supervision},
  author =       {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8748--8763},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/radford21a/radford21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/radford21a.html},
  abstract = 	 {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.}
}

@article{do2025lineartimetransport,
  title={Linear-Time Transport with Rectified Flows},
  author={Do, Khoa and Coeurjolly, David and Memari, Pooran and Bonneel, Nicolas},
  journal={ACM Trans. Graph.},
  volume={44},
  month={Aug},
  numpages={13},
  year={2025}
}

@inproceedings{unipc,
 author = {Zhao, Wenliang and Bai, Lujia and Rao, Yongming and Zhou, Jie and Lu, Jiwen},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {49842--49869},
 publisher = {Curran Associates, Inc.},
 title = {UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/9c2aa1e456ea543997f6927295196381-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@misc{gao2025diffusionmeetsflow,
  author = {Gao, Ruiqi and Hoogeboom, Emiel and Heek, Jonathan and Bortoli, Valentin De and Murphy, Kevin P. and Salimans, Tim},
  title = {Diffusion Meets Flow Matching: Two Sides of the Same Coin},
  year = {2024},
  url  = {https://diffusionflow.github.io/}
}

@inproceedings{gmflow,
  title={Gaussian Mixture Flow Matching Models},
  author={Hansheng Chen and Kai Zhang and Hao Tan and Zexiang Xu and Fujun Luan and Leonidas Guibas and Gordon Wetzstein and Sai Bi},
  booktitle={ICML},
  year={2025},
}

@inproceedings{
albergo2023building,
title={Building Normalizing Flows with Stochastic Interpolants},
author={Michael Samuel Albergo and Eric Vanden-Eijnden},
booktitle={ICLR},
year={2023},
}

@inproceedings{
      meng2022sdedit,
      title={{SDE}dit: Guided Image Synthesis and Editing with Stochastic Differential Equations},
      author={Chenlin Meng and Yutong He and Yang Song and Jiaming Song and Jiajun Wu and Jun-Yan Zhu and Stefano Ermon},
      booktitle={ICLR},
      year={2022},
}

@INPROCEEDINGS{infedit,
  author={Xu, Sihan and Huang, Yidong and Pan, Jiayi and Ma, Ziqiao and Chai, Joyce},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Inversion-Free Image Editing with Language-Guided Diffusion Models}, 
  year={2024},
  volume={},
  number={},
  pages={9454-9461},
  keywords={Schedules;Computer vision;Accuracy;Noise reduction;Semantics;Diffusion models;Sampling methods;Diffusion Models;Image Editing;Inversion;Attention Control;Consistency Models},
  doi={10.1109/CVPR52733.2024.00903}
}

@InProceedings{gowiththeflow,
    author    = {Burgert, Ryan and Xu, Yuancheng and Xian, Wenqi and Pilarski, Oliver and Clausen, Pascal and He, Mingming and Ma, Li and Deng, Yitong and Li, Lingxiao and Mousavi, Mohsen and Ryoo, Michael and Debevec, Paul and Yu, Ning},
    title     = {Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2025},
    pages     = {13-23}
}

@inproceedings{howiwarpedyournoise,
  title={How I Warped Your Noise: a Temporally-Correlated Noise Prior for Diffusion Models},
  author={Chang, Pascal and Tang, Jingwei and Gross, Markus and Azevedo, Vinicius C},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024},
}
