\section{Analysis}
\label{sec:motivation}

\subsection{Latency of Video DiT Models}
\label{sec:motivation_videodit_modelperformance}

\input{tables/breakdown}


As discussed in~\cref{sec:background_diffusiontransformer}, video-DiT models first encode a set of video frames as latent embedding vectors using a vector-quantized variational autoencoder (VQVAE). The VQVAE compresses each spatiotemporal patch of $H \times W$ pixels across $F$ consecutive frames into a single latent vector. For instance, setting $H=W=8$ and $F=4$ maps every $8\times 8$ pixel block over $4$ frames into one latent embedding vector. Even a short video, such as a five-second video at $480\times 832$ resolution, gets encoded as approximately $32000$ embedding vectors. Attention layers over such a large number of embeddings require substantial computation, as each layer must process an massive number of floating-point operations to generate even a short video.

Table~\ref{tab:attention_breakdown} shows the amount of time required to produce a 5s video at 720p using the Wan 2.1 1.3B and 14B models~\cite{wan} on a single H100 GPU chip. We see that a significant amount of time, about 10 minutes using Wan 14B model, is required to produce even a short video.
\begin{figure}[!htb]
    \centering
    \includegraphics[width=\linewidth]{figs_1/attnbreakdown}
    \caption{Breakdown of time spent (in seconds) by different operations during inference of Wan 2.1 1.3B~\cite{wan} (eager mode).}
    \label{fig:attnbreakdown}
\end{figure}
In Fig.~\ref{fig:attnbreakdown}, we depict a breakdown of the time required by each operator to produce the video (note that ``others'' here indicate the operators to encode the text tokens and the initial noisy video frames using the VQVAE). 
We observe that the majority of computation time is spent evaluating the transformer model, with the attention layer accounting for most of this cost. Specifically, when processing long sequences of embeddings (of length $N$), the attention operation scales as $O(N^2)$, in contrast to components such as the feed-forward network, which scale as $O(N)$. Consequently, for video-DiT models, longer sequences—arising from higher-resolution or longer videos—lead to an even greater fraction of time being dominated by attention. For example, in Wan 2.1 1.3B~\cite{wan}, nearly $91\%$ of the runtime is spent computing attention when generating 81 frames at 720p, compared to the already-high $76\%$ for 49 frames at 480p.

\subsection{Attention Sparsity}
\label{sec:motivation_attentionsparsity}
\label{sec:motivation_blocksparseattention}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\linewidth]{figs2/fig4.pdf}
    \caption{Sparsity in attention computation: attention scores are highly sparse, and locations of negligible attention scores are irregularly distributed.}
    \label{fig:distribution_of_attnvalues}
\end{figure}

Fig.~\ref{fig:distribution_of_attnvalues} shows heatmap of the attention scores in one attention heads of the Wan 2.1 1.3B vDIT model~\cite{wan}. 
We observe that the vast majority of attention scores are close to zero. Since many $q,k$ pairs produce negligible attention scores, attention computation can be significantly accelerated by bypassing these score computations.

Prior sparse attention mechanisms such as FlexAttention~\cite{flexattn}, block-sparse attention~\cite{bsa} and FlashAttention~\cite{flashattn} skip attention score computations to enable faster execution. In practice, these methods avoid loading and computing pairwise $q-k$ dot-products between contiguous sets (blocks) of $q,k$ vectors (typically 64 queries or 64 keys). The block size $M$, or the number of queries/keys to skip computing attention score over is determined by the matrix multiplication operation dimensions in the tensor core of the accelerator (e.g., $64 \times 64$ on H100 GPUs). This allows $M \times M$ query-dot product computation to be skipped, corresponding to attention scores computed from $M$ queries and $M$ keys. However, in order to maintain accuracy, computing a block of attention scores can be skipped only when \emph{all} query–key pairs within the block produce negligible attention scores.

\input{tables/blocksparse}
\textbf{Sparsity in attention scores is fine-grained.} 
\label{sec:motivation_skip_attn_fine_granularity}
Table~\ref{tab:attn_block_sparsity} shows the sparsity of the attention map at different block sizes of attention scores. 
We find that skipping finer-grained blocks ($16 \times 16$) yields about 70\% sparsity, whereas coarser blocks ($64 \times 64$) achieve only 22\%. This suggests that operating at finer granularity provides a greater opportunity for speedup. However, existing block-sparse attention implementations typically skip computations only at coarse block sizes. Current block sparse attention mechanisms~\cite{bsa, flexattn} are unable to leverage finer-grain sparsity (below $64\times 64$). In this work, we aim to exploit the higher sparsity available in finer-grained blocks to design a more efficient block-sparse attention mechanism that substantially reduces FLOPs without sacrificing accuracy.
