\section{Introduction}
\label{sec:introduction}

Media generation models in deep learning are highly effective at capturing complex data distributions, such as videos~\cite{wan, hunyuanvideo}, audio~\cite{diffwave}, 3D models~\cite{hunyuan3d}, and images~\cite{flux, sd3, pixart}. When trained on real-world data, these models can generate realistic content, representing a major advancement in synthetic media generation. They enable a wide range of applications, such as advanced video editing, intuitive 3D modeling, and the creation of immersive 3D environments.


Video generation is a popular media generation task powered by diffusion models, a family of deep learning approaches that iteratively refine random-noise inputs to produce a sample of a data distribution. Diffusion models learn the \emph{score function} (\cref{sec:background_diffusiontransformer}) of a complex data distribution using a parameterized function. In video generation, the data distribution is the set of real-world videos, and the parameterized function is typically a vision transformer (ViT)~\cite{vit}, referred to as a diffusion transformer (DiT)~\cite{dit}.

Videos are generated by iteratively refining (or \emph{denoising}) the latent space representation of the video. This is done in two steps: First, the latent-space representation of video frames, represented as a sequence of embedding vectors, are initialized with random noise. Each embedding vector typically corresponds to a latent representation of $(H, W)$ patches of pixels across $F$ frames of the video. Second, these embeddings are iteratively refined by the transformer. At each step, the transformer takes as input the current embedding sequence, and produces a cleaner sequence of vectors that becomes the input for the next iteration. Through this repeated denoising process, the embeddings gradually converge to the latent representation of realistic video frames. Finally, the refined embedding sequence is decoded back into the pixel space to synthesize the video.

Even a short, low-resolution video requires a very large number of embedding vectors. For example, state-of-the-art video DiT models such as Wan 2.1~\cite{wan} encodes a 5-second video at 720p resolution and 16 fps using $74000$ embeddings. On a single H100 GPU, the Wan 2.1 1.3B model requires 5 minutes of runtime for transformer evaluation, and the Wan 2.1 14B model requires over 25 minutes.

A significant portion of this latency is attributed to computing the attention layers of the transformer ($91\%$ of the runtime for this example with Wan 2.1).
Since the attention computation scales quadratically with embedding count, longer or higher resolution videos incur significantly larger processing latencies. For instance, generating a 10-second video at 720p and 16 fps requires roughly twice as many embeddings, resulting in about four times the runtime. Thus, as the embeddings increase, the runtime becomes increasingly dominated by computation in the attention layer. In Wan 2.1~\cite{wan} 1.3B, attention layers take $76\%$ of the runtime to produce 49 frames, and $91\%$ for 81 frames of video.

The inputs to attention layers, however, contain significant redundancies. As shown in Fig.\ref{fig:attnheads_zero}, computing only 20\% of the attention scores per head still produces a valid video with no noticeable loss in quality. Note that the generated video frames are slightly different because the denoising process is sensitive to small perturbations in early denoising iterations, and the diffusion model with sparse attention scores still captures the same video distribution.

Several works~\cite{xattn, radialattn, sparsevideogen2, sparsevideogen, vsa} exploit this redundancy to accelerate video generation.
Recent methods such as RadialAttention~\cite{radialattn}, X-attention~\cite{xattn}, and SparseVideoGen~\cite{sparsevideogen} observe that the most important attention scores are typically concentrated in static regions of the attention map, particularly around the diagonal (\cref{sec:background_attn}) across all attention layers. They propose using static \emph{masks} for attention layers, requiring computing only a fraction of the scores. This makes attention computation more efficient and speeds up video generation. However, because these methods skip a fixed subset of attention scores at each head, they may also omit essential scores. To address this limitation, another line of work, e.g., Video Sparse Attention (VSA)~\cite{vsa}, proposes to infer the attention mask dynamically at runtime, and use this mask in block sparse attention. They introduce additional parameters to the model, enabling it to learn the attention mask (i.e., the set of attention scores to compute).

Both the static and learned mask approaches described above typically rely on block sparse attention (\cref{sec:motivation_blocksparseattention}) as the underlying sparse attention mechanism. Block sparse attention implementations in literature~\cite{bsa, flashinfer,flashdecode,flashattn,flashmask,flexattn} skip computing attention scores between all pairs of $M$ query tokens and $M$ key tokens, where $M$ is a hardware-dependent parameter, at each attention head. In practice, this often means skipping entire $64\times64$ blocks of attention scores, where $M=64$ to match underlying GPU hardware parameters for efficient GPU implementations. While this approach provides some acceleration, it is only effective if all scores in the skipped block are guaranteed to be near zero.

We instead propose to leverage sparsity at a finer granularity which would offer much greater opportunity for reducing computation. For instance, skipping $16\times16$ blocks of attention can reduce the FLOP count by up to $70\%$, compared to only about $15\%$ when using $64\times64$ blocks, without noticeably affecting video generation quality (see~\cref{sec:motivation_skip_attn_fine_granularity}). Similarly, skipping $128\times1$ slices of query-key dot products (at bfloat16 precision) can reduce FLOPs by as much as $55\%$.
These findings suggest that exploiting fine-grained sparsity in attention computation can yield substantially greater speedups than current coarse-grained block sparse implementations. % The same benefit applies to trainable methods that generate attention masks. This fact is highlighted by VSA~\cite{vsa}.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\linewidth]{figs2/fig1.pdf}
    \caption{Left: A video frame from Wan 2.1~\cite{wan} for the prompt “horse bending and drinking water from a lake”. Right: The same model generates a similar video using only 20\% of attention scores per head, achieving comparable results with a fraction of the FLOPs.}
    \label{fig:attnheads_zero}
\end{figure}
% \end{wrapfigure}

In this work, we aim to develop a fine-grained sparse attention mechanism to accelerate diffusion model inference.
Our goals are to:
(1) design an efficient sparse attention mechanism that skips computing attention scores for faster runtimes \emph{at a finer granularity}, i.e, skip computing smaller sets of attention scores at a time, compared to block sparse attention, and
(2) propose a mechanism to identify which sets of attention scores need to be computed or skipped, i.e, determine a sparse attention mask for our attention mechanism.

To this end, we introduce \X, an efficient fine-grained sparse attention mechanism for diffusion transformer models. The key idea is to skip computing scores corresponding to \emph{slices} of size $M \times 1$ of the attention map, instead of the $M \times M$ blocks as in current sparse attention implementations. This results in skipping computation for attention scores produced by one key and a group of $M$ contiguous queries. Such fine-grained skipping allows for discarding a larger number of insignificant attention score computations. The key challenges in achieving this are:
(i) Mapping our sliced sparse attention strategy onto modern GPUs while incurring negligible overhead and maintaining high device utilization. This is difficult because naive implementations of fine-grained sparsity may incur irregular memory access and data movement, which can drastically reduce GPU hardware utilization.
(ii) Computing a fine-grained attention mask that reliably identifies slices of attention scores which can be safely skipped. 

For (i), efficient GPU attention implementations (FlashAttention~\cite{flashattn}) typically load a block (usually a multiple of $64 \times 64$) of queries and keys from high-bandwidth memory (HBM) into on-chip SRAM before using tensor cores to compute pairwise query–key dot products. To efficiently skip insignificant slices of attention scores, however, we must load only the sparse set of relevant keys and values for a group of queries from HBM into SRAM. To make effective use of tensor cores, these loaded keys and values must be packed into SRAM in an appropriate format expected by the tensor core. To enable this, we construct a new load primitive, which we call the asynchronous gather-load, that loads a sparse set of key/value vectors into an SRAM in the format required by the tensor core (as a tile, with a swizzled layout). Since modern GPUs lack hardware support (e.g., Tensor Memory Accelerator, TMA) for accelerated address generation to load a sparse set of vectors, we emulate the asynchronous gather-load using existing asynchronous load instructions on the H100 GPU.

For (ii), we propose two training-free strategies to identify the set of keys to load for a given group of queries. Our first strategy takes inspiration from prior works that apply caching techniques to accelerate diffusion model inference. We observe that, within each attention head, the query–key pairs yielding significant attention scores remain largely stable across denoising iterations. Leveraging this, in the first denoising iteration, we compute the full attention map, identify the significant slices of attention scores via thresholding, and then reuse this information in subsequent denoising iterations in our sparse attention mechanism.

A limitation of this approach is the HBM overhead required to store cached masks. To mitigate this overhead, we also introduce a second lightweight altenative strategy. For a group of queries $q_1, q_2, \dots, q_M$, we only compute dot products with a key if the group’s mean query, $q_{\text{mean}}$, is likely to yield a significant attention score. Thus, instead of evaluating all keys, we load only the top-$k$ keys determined by their score against $q_{\text{mean}}$ (\cref{sec:method_maskdetermination}). This heuristic is motivated by the observation that nearby embeddings tend to produce similar query distributions, allowing us to safely approximate with far fewer key computations.


\noindent We demonstrate that \X enables faster video generation without sacrificing the output quality of the video.
On state-of-art text-to-video generation models, we show that \X speeds up the video generation time by up to $1.65\times$ ($1.48\times$ on average). Our contributions are:
\begin{itemize}
\item We demonstrate that video diffusion models contain a significant amount of fine-grain sparsity in their attention maps that are not leveraged by existing block sparse attention methods.
% \item We provide a comprehensive performance characterization of state-of-the-art, open-source video diffusion models and demonstrate the potential inference speedups enabled by sparse attention mechanisms.
\item We introduce the first slice-based sparse attention mechanism that can practically exploit fine-grained sparsity on modern GPUs. To support this, we design a novel asynchronous gather-load primitive, which efficiently assembles sparse key/value vectors into tensor-core-compatible tiles, overcoming the overheads of irregular memory access.%We propose a low-overhead attention implementation that effectively leverages fine-grain sparsity in the attention using a novel gather primitive to efficient pack 
%We construct a new loading primitive called the \emph{gather-load} primitive that can efficiently load a sparse set of key/value vectors from memory. 
%We show that this loading primitive can be used implement a low-overhead version of our sliced attention mechanism on modern GPUs.
% \item We implement an low-overhead version of our sliced attention mechanism, a form of fine-grained sparse attention, on modern H100 GPUs.
\item We demonstrate that sparsity patterns remain stable across denoising iterations, enabling a cache-based thresholding strategy that avoids recomputation while preserving accuracy.
\item We propose two lightweight strategies for sparse mask generation that operate entirely without retraining. This ensures \X is directly applicable to existing state-of-the-art video DiTs.
%We propose novel approaches for generating sparse attention masks designed to be used with \X in video diffusion models.
\item We show that our sliced attention mechanism can fully supersede existing block-sparse attention methods, achieving performance equivalent to or better than all prior coarse-grained approaches with negligible accuracy loss.
\end{itemize}


% \begin{itemize}
% \item We provide a comprehensive performance characterization of state-of-the-art, open-source video diffusion models and demonstrate the potential inference speedups enabled by sparse attention mechanisms.
% \item We implement an low-overhead version of our sliced attention mechanism—a form of fine-grained sparse attention—on modern H100 GPUs.
% \item We propose two training-free strategies for fine-grained mask determination in video diffusion models, designed to be used with our fine-grained masked attention mechanism.
% \item We show that our sliced attention mechanism can fully supersede existing block-sparse attention methods, achieving performance equivalent to or better than all prior coarse-grained approaches.
% \end{itemize}



