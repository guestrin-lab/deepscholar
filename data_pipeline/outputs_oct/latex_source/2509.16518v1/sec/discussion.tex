\section{Discussion}
\label{sec:discussion}


While this work primarily evaluates \X on video diffusion transformers (DiTs), the fine-grained sparse attention mechanism is broadly applicable to other long-context diffusion transformer models, such as 3D generative models, large language diffusion models, audio diffusion models, and 3D generative models, all of which exhibit redundancy in their attention maps.
The slice-based skipping introduced in \X can be incorporated into these settings, making it a general drop-in replacement for block-sparse attention. 

Our current design of \X is implemented in NVIDIA H100 GPU. However, the asynchronous gather-load primitive can be re-implemented to align with other accelerators’ memory hierarchies and matrix multiplication units. For instance, TPUs with sparse core architectures can be readily used to implement our gather load primitive. Next-generation GPUs such as NVIDIA’s B100 introduce hardware support for gathering tiles/vectors, which \X can readily take advantage of.

A limitation of \X is that the low overheads of the gather-load operation may be difficult to obtain when attention computation is not compute-bound, such as LLM decoding, as it may not be possible to fully overlap the latency of address generation and data loading. % This is because the efficiency of \X relies on the gather-load primitive’s latency being hidden behind attention computation, a property that may not hold in memory-bound workloads like autoregressive decoding. 
%Second, \X is tightly coupled to NVIDIA’s tensor core architecture and its support for asynchronous load instructions, which enable pipelined memory–compute overlap. Porting to other accelerators (e.g., AMD GPUs or Google TPUs) would require re-implementing the gather-load primitive to align with their distinct memory hierarchies and matrix multiplication units. For instance, TPUs employ systolic arrays with different tiling constraints, which may necessitate alternative sparse indexing strategies. Next-generation GPUs such as NVIDIA Blackwell B100 introduce hardware-assisted sparse memory access (e.g., gather/scatter support), which could further improve \X’s efficiency.% Exploring these adaptations is a promising direction for future work.

