\section{Related Work}
\label{sec:related_work}


\textbf{Block sparse attention.} 
Several implementations of block sparse attention~\cite{bsa, flashattn, flexattn, flashinfer, flashmask} propose a coarse-grained sparse attention mechanism that skips entire blocks of attention score computations at granularity of $64\times 64$ or $128\times 128$ at half-precision. Current block-sparse attention mechanisms either prevent further reduction of block size (do not compile) or cause significant hardware underutilization and performance overhead, since they are constrained by the tensor core matrix multiplication width (\cref{sec:motivation_skip_attn_fine_granularity}). Several works in the large language model literature~\cite{minference, xattn, flashdecode, nsa, seerattn} utilize block sparse attention to accelerate attention computation. 
% Several works in large language model literature ~\cite{minference, xattn, flashdecode, nsa, seerattn} use block sparse attention to speedup attention computation.

\textbf{Block sparse attention for videoDiTs.} 
Recent works, such as Radial Attention~\cite{radialattn}, X-attention~\cite{xattn}, SparseVideoGen~\cite{sparsevideogen}, and SparseVideoGen2~\cite{sparsevideogen2}, have applied block sparse attention implementations to video diffusion models. These approaches consider a fixed sparsity pattern in the attention map based on empirical observations of significant patterns. Other works, such as Video Sparse Attention~\cite{vsa}, incorporate learned sparse attention patterns by using a parameterized model to derive the attention map mask. Both approaches utilize coarse-grained sparse attention mechanisms. In contrast, our method enables fine-grained skipping of attention blocks, providing more opportunities for skipping computation. We compare \X with SparseVideoGen and Radial Attention in~\cref{sec:results}. 
Moreover, trainable sparse attention methods such as Video Sparse Attention (VSA)~\cite{vsa} can be reformulated to generate sparse masks compatible with \X \textquotesingle s attention kernel. These methods are orthogonal to \X \textquotesingle s kernel implementation and can be used in conjunction as mask-determination strategies for \X.

% While trainable sparse attention methods can outperform training-free approaches, these methods could benefit from \X's fine-grained approach.
% Recent works such as Radial attention~\cite{radialattn}, X-attention~\cite{xattn}, SparseVideoGen~\cite{sparsevideogen}, SparseVideoGen2~\cite{sparsevideogen2} apply block sparse attention implementations for video diffusion models. They consider a fixed sparsity pattern in the attention map based on empirical observation of significant. Other works such as video sparse attention~\cite{vsa} incorporate learnt sparse attention patterns. These works use a parameterized model to derive the mask of the attention maps. Both of these works make use of coarse grain sparse attention mechanisms. Our approach allows fine-grain skipping of attention blocks enabling more opportunity to skip computation. We compare \X with SVG, radial attention in~\cref{sec:results}. Trainable sparse attention methods can make  of coarse grain sparse attention methods, and could potentially benefit from \X. 

\textbf{Other techniques to accelerate video diffusion.}
SpargeAttention~\cite{spargeattn}, SageAttention~\cite{sageattn}, and SageAttention2~\cite{sageattn2} propose general attention approximation techniques, such as quantization and token compression mechanisms, that can be applied during inference for both LLM and DiT models. Token compression-based approaches may skip essential tokens relevant to the video, which could lead to inconsistent video generation (pointed out by~\cite{radialattn}). These approaches are orthogonal to our \X.% SpargeAttention~\cite{spargeattn}, SageAttention~\cite{sageattn}, SageAttention2~\cite{sageattn2}, propose general attention approximation techniques such as quantization and token compression mechanisms that can be applied at inference time to LLM and DiT models. Approaches based on token compression skip essential tokens relevant to the video and may produce inconsistent video (\tofix{add reference here}). Quantization-based techniques are orthogonal to our approach (a lower-precision version of \X can be implemented). 




