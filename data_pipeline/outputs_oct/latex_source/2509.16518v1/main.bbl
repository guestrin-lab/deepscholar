\begin{thebibliography}{31}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Chen et~al.(2023)Chen, Yu, Ge, Yao, Xie, Wu, Wang, Kwok, Luo, Lu, et~al.]{pixart}
Chen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang, Z., Kwok, J., Luo, P., Lu, H., et~al.
\newblock Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis.
\newblock \emph{arXiv preprint arXiv:2310.00426}, 2023.

\bibitem[Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and R{\'e}]{flashattn}
Dao, T., Fu, D., Ermon, S., Rudra, A., and R{\'e}, C.
\newblock Flashattention: Fast and memory-efficient exact attention with io-awareness.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 16344--16359, 2022.

\bibitem[Dong et~al.(2024)Dong, Feng, Guessous, Liang, and He]{flexattn}
Dong, J., Feng, B., Guessous, D., Liang, Y., and He, H.
\newblock Flex attention: A programming model for generating optimized attention kernels.
\newblock \emph{arXiv preprint arXiv:2412.05496}, 2024.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{vit}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Esser et~al.(2024{\natexlab{a}})Esser, Kulal, Blattmann, Entezari, M{\"u}ller, Saini, Levi, Lorenz, Sauer, Boesel, et~al.]{sd3}
Esser, P., Kulal, S., Blattmann, A., Entezari, R., M{\"u}ller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et~al.
\newblock Scaling rectified flow transformers for high-resolution image synthesis.
\newblock In \emph{Forty-first international conference on machine learning}, 2024{\natexlab{a}}.

\bibitem[Esser et~al.(2024{\natexlab{b}})Esser, Townsend, Kulal, Dockhorn, Müller, Alterovych, Dehaerne, Lu, Hazirbas, Rampas, Rombach, D'Asaro, Watson, Voinea, Puzon, Boureau, and Mentzer]{flux}
Esser, P., Townsend, M. S.~M., Kulal, S., Dockhorn, T., Müller, J., Alterovych, A., Dehaerne, D., Lu, P. T.~H., Hazirbas, C., Rampas, D., Rombach, R., D'Asaro, J., Watson, D., Voinea, D., Puzon, L., Boureau, Y.-L., and Mentzer, F.
\newblock Flux: A unified approach to pixel-based and latent-space diffusion models, 2024{\natexlab{b}}.

\bibitem[Gao et~al.(2024)Gao, Zeng, Du, Cao, Zhou, Qi, Lai, So, Cao, Yang, et~al.]{seerattn}
Gao, Y., Zeng, Z., Du, D., Cao, S., Zhou, P., Qi, J., Lai, J., So, H. K.-H., Cao, T., Yang, F., et~al.
\newblock Seerattention: Learning intrinsic sparse attention in your llms.
\newblock \emph{arXiv preprint arXiv:2410.13276}, 2024.

\bibitem[Guo et~al.(2024)Guo, Tang, Yang, Zhang, Liu, and Han]{bsa}
Guo, J., Tang, H., Yang, S., Zhang, Z., Liu, Z., and Han, S.
\newblock {Block Sparse Attention}.
\newblock \url{https://github.com/mit-han-lab/Block-Sparse-Attention}, 2024.

\bibitem[Hong et~al.(2023)Hong, Dai, Xu, Mao, Li, Liu, Chen, Dong, and Wang]{flashdecode}
Hong, K., Dai, G., Xu, J., Mao, Q., Li, X., Liu, J., Chen, K., Dong, Y., and Wang, Y.
\newblock Flashdecoding++: Faster large language model inference on gpus.
\newblock \emph{arXiv preprint arXiv:2311.01282}, 2023.

\bibitem[Hu et~al.(2025)Hu, Meng, Akhauri, Abdelfattah, Seo, Zhang, and Gupta]{freecache}
Hu, Z., Meng, J., Akhauri, Y., Abdelfattah, M.~S., Seo, J.-s., Zhang, Z., and Gupta, U.
\newblock Accelerating diffusion language model inference via efficient kv caching and guided diffusion.
\newblock \emph{arXiv preprint arXiv:2505.21467}, 2025.

\bibitem[Huang et~al.(2024)Huang, He, Yu, Zhang, Si, Jiang, Zhang, Wu, Jin, Chanpaisit, et~al.]{vbench}
Huang, Z., He, Y., Yu, J., Zhang, F., Si, C., Jiang, Y., Zhang, Y., Wu, T., Jin, Q., Chanpaisit, N., et~al.
\newblock Vbench: Comprehensive benchmark suite for video generative models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  21807--21818, 2024.

\bibitem[Jiang et~al.(2024)Jiang, Li, Zhang, Wu, Luo, Ahn, Han, Abdi, Li, Lin, et~al.]{minference}
Jiang, H., Li, Y., Zhang, C., Wu, Q., Luo, X., Ahn, S., Han, Z., Abdi, A.~H., Li, D., Lin, C.-Y., et~al.
\newblock Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention.
\newblock \emph{Advances in Neural Information Processing Systems}, 37:\penalty0 52481--52515, 2024.

\bibitem[Kong et~al.(2024)Kong, Tian, Zhang, Min, Dai, Zhou, Xiong, Li, Wu, Zhang, et~al.]{hunyuanvideo}
Kong, W., Tian, Q., Zhang, Z., Min, R., Dai, Z., Zhou, J., Xiong, J., Li, X., Wu, B., Zhang, J., et~al.
\newblock Hunyuanvideo: A systematic framework for large video generative models.
\newblock \emph{arXiv preprint arXiv:2412.03603}, 2024.

\bibitem[Kong et~al.(2020)Kong, Ping, Huang, Zhao, and Catanzaro]{diffwave}
Kong, Z., Ping, W., Huang, J., Zhao, K., and Catanzaro, B.
\newblock Diffwave: A versatile diffusion model for audio synthesis.
\newblock \emph{arXiv preprint arXiv:2009.09761}, 2020.

\bibitem[Li et~al.(2025)Li, Li, Cai, Xi, Yang, Lin, Zhang, Yang, Hu, Peng, et~al.]{radialattn}
Li, X., Li, M., Cai, T., Xi, H., Yang, S., Lin, Y., Zhang, L., Yang, S., Hu, J., Peng, K., et~al.
\newblock Radial attention: $ o (nlog n) $ sparse attention with energy decay for long video generation.
\newblock \emph{arXiv preprint arXiv:2506.19852}, 2025.

\bibitem[Lu et~al.(2022)Lu, Zhou, Bao, Chen, Li, and Zhu]{dpm}
Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J.
\newblock Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 5775--5787, 2022.

\bibitem[Ma et~al.(2025)Ma, Yu, Fang, and Wang]{d_kv}
Ma, X., Yu, R., Fang, G., and Wang, X.
\newblock dkv-cache: The cache for diffusion language models.
\newblock \emph{arXiv preprint arXiv:2505.15781}, 2025.

\bibitem[Peebles \& Xie(2023)Peebles and Xie]{dit}
Peebles, W. and Xie, S.
\newblock Scalable diffusion models with transformers.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on computer vision}, pp.\  4195--4205, 2023.

\bibitem[Spector et~al.(2024)Spector, Arora, Singhal, Fu, and R{\'e}]{thunderkittens}
Spector, B.~F., Arora, S., Singhal, A., Fu, D.~Y., and R{\'e}, C.
\newblock Thunderkittens: Simple, fast, and adorable ai kernels.
\newblock \emph{arXiv preprint arXiv:2410.20399}, 2024.

\bibitem[Wan et~al.(2025)Wan, Wang, Ai, Wen, Mao, Xie, Chen, Yu, Zhao, Yang, et~al.]{wan}
Wan, T., Wang, A., Ai, B., Wen, B., Mao, C., Xie, C.-W., Chen, D., Yu, F., Zhao, H., Yang, J., et~al.
\newblock Wan: Open and advanced large-scale video generative models.
\newblock \emph{arXiv preprint arXiv:2503.20314}, 2025.

\bibitem[Wang et~al.(2024)Wang, Zeng, Xiao, Wu, Yang, Zheng, Chen, Bian, Yu, and Wang]{flashmask}
Wang, G., Zeng, J., Xiao, X., Wu, S., Yang, J., Zheng, L., Chen, Z., Bian, J., Yu, D., and Wang, H.
\newblock Flashmask: Efficient and rich mask extension of flashattention.
\newblock \emph{arXiv preprint arXiv:2410.01359}, 2024.

\bibitem[Xi et~al.(2025)Xi, Yang, Zhao, Xu, Li, Li, Lin, Cai, Zhang, Li, et~al.]{sparsevideogen}
Xi, H., Yang, S., Zhao, Y., Xu, C., Li, M., Li, X., Lin, Y., Cai, H., Zhang, J., Li, D., et~al.
\newblock Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity.
\newblock \emph{arXiv preprint arXiv:2502.01776}, 2025.

\bibitem[Xu et~al.(2025)Xu, Xiao, Huang, Guo, and Han]{xattn}
Xu, R., Xiao, G., Huang, H., Guo, J., and Han, S.
\newblock Xattention: Block sparse attention with antidiagonal scoring.
\newblock \emph{arXiv preprint arXiv:2503.16428}, 2025.

\bibitem[Yang et~al.(2025)Yang, Xi, Zhao, Li, Zhang, Cai, Lin, Li, Xu, Peng, et~al.]{sparsevideogen2}
Yang, S., Xi, H., Zhao, Y., Li, M., Zhang, J., Cai, H., Lin, Y., Li, X., Xu, C., Peng, K., et~al.
\newblock Sparse videogen2: Accelerate video generation with sparse attention via semantic-aware permutation.
\newblock \emph{arXiv preprint arXiv:2505.18875}, 2025.

\bibitem[Ye et~al.(2025)Ye, Chen, Lai, Lin, Zhang, Wang, Chen, Kasikci, Grover, Krishnamurthy, et~al.]{flashinfer}
Ye, Z., Chen, L., Lai, R., Lin, W., Zhang, Y., Wang, S., Chen, T., Kasikci, B., Grover, V., Krishnamurthy, A., et~al.
\newblock Flashinfer: Efficient and customizable attention engine for llm inference serving.
\newblock \emph{arXiv preprint arXiv:2501.01005}, 2025.

\bibitem[Yuan et~al.(2025)Yuan, Gao, Dai, Luo, Zhao, Zhang, Xie, Wei, Wang, Xiao, et~al.]{nsa}
Yuan, J., Gao, H., Dai, D., Luo, J., Zhao, L., Zhang, Z., Xie, Z., Wei, Y., Wang, L., Xiao, Z., et~al.
\newblock Native sparse attention: Hardware-aligned and natively trainable sparse attention.
\newblock \emph{arXiv preprint arXiv:2502.11089}, 2025.

\bibitem[Zhang et~al.(2024{\natexlab{a}})Zhang, Huang, Zhang, Wei, Zhu, and Chen]{sageattn2}
Zhang, J., Huang, H., Zhang, P., Wei, J., Zhu, J., and Chen, J.
\newblock Sageattention2: Efficient attention with thorough outlier smoothing and per-thread int4 quantization.
\newblock \emph{arXiv preprint arXiv:2411.10958}, 2024{\natexlab{a}}.

\bibitem[Zhang et~al.(2024{\natexlab{b}})Zhang, Wei, Huang, Zhang, Zhu, and Chen]{sageattn}
Zhang, J., Wei, J., Huang, H., Zhang, P., Zhu, J., and Chen, J.
\newblock Sageattention: Accurate 8-bit attention for plug-and-play inference acceleration.
\newblock \emph{arXiv preprint arXiv:2410.02367}, 2024{\natexlab{b}}.

\bibitem[Zhang et~al.(2025{\natexlab{a}})Zhang, Xiang, Huang, Xi, Zhu, Chen, et~al.]{spargeattn}
Zhang, J., Xiang, C., Huang, H., Xi, H., Zhu, J., Chen, J., et~al.
\newblock Spargeattention: Accurate and training-free sparse attention accelerating any model inference.
\newblock In \emph{Forty-second International Conference on Machine Learning}, 2025{\natexlab{a}}.

\bibitem[Zhang et~al.(2025{\natexlab{b}})Zhang, Huang, Chen, Lin, Liu, Stoica, Xing, and Zhang]{vsa}
Zhang, P., Huang, H., Chen, Y., Lin, W., Liu, Z., Stoica, I., Xing, E., and Zhang, H.
\newblock Vsa: Faster video diffusion with trainable sparse attention.
\newblock \emph{arXiv preprint arXiv:2505.13389}, 2025{\natexlab{b}}.

\bibitem[Zhao et~al.(2025)Zhao, Lai, Lin, Zhao, Liu, Yang, Feng, Yang, Zhang, Yang, et~al.]{hunyuan3d}
Zhao, Z., Lai, Z., Lin, Q., Zhao, Y., Liu, H., Yang, S., Feng, Y., Yang, M., Zhang, S., Yang, X., et~al.
\newblock Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation.
\newblock \emph{arXiv preprint arXiv:2501.12202}, 2025.

\end{thebibliography}
