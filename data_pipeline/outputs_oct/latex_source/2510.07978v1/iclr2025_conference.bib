@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@misc{gao2024retrievalaugmentedgenerationlargelanguage,
      title={Retrieval-Augmented Generation for Large Language Models: A Survey}, 
      author={Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Meng Wang and Haofen Wang},
      year={2024},
      eprint={2312.10997},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.10997}, 
}


@article{DBLP:journals/corr/abs-2406-11931,
  publtype={informal},
  author={DeepSeek-AI and Qihao Zhu and Daya Guo and Zhihong Shao and Dejian Yang and Peiyi Wang and Runxin Xu and Y. Wu and Yukun Li and Huazuo Gao and Shirong Ma and Wangding Zeng and Xiao Bi and Zihui Gu and Hanwei Xu and Damai Dai and Kai Dong and Liyue Zhang and Yishi Piao and Zhibin Gou and Zhenda Xie and Zhewen Hao and Bingxuan Wang and Junxiao Song and Deli Chen and Xin Xie and Kang Guan and Yuxiang You and Aixin Liu and Qiushi Du and Wenjun Gao and Xuan Lu and Qinyu Chen and Yaohui Wang and Chengqi Deng and Jiashi Li and Chenggang Zhao and Chong Ruan and Fuli Luo and Wenfeng Liang},
  title={DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence},
  year={2024},
  cdate={1704067200000},
  journal={CoRR},
  volume={abs/2406.11931},
  url={https://doi.org/10.48550/arXiv.2406.11931}
}

@misc{rozière2024codellamaopenfoundation,
      title={Code Llama: Open Foundation Models for Code}, 
      author={Baptiste Rozière and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Romain Sauvestre and Tal Remez and Jérémy Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre Défossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},
      year={2024},
      eprint={2308.12950},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.12950}, 
}

@inproceedings{
qin2024toolllm,
title={Tool{LLM}: Facilitating Large Language Models to Master 16000+ Real-world {API}s},
author={Yujia Qin and Shihao Liang and Yining Ye and Kunlun Zhu and Lan Yan and Yaxi Lu and Yankai Lin and Xin Cong and Xiangru Tang and Bill Qian and Sihan Zhao and Lauren Hong and Runchu Tian and Ruobing Xie and Jie Zhou and Mark Gerstein and dahai li and Zhiyuan Liu and Maosong Sun},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=dHng2O0Jjr}
}

@inproceedings{
yao2023react,
title={ReAct: Synergizing Reasoning and Acting in Language Models},
author={Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik R Narasimhan and Yuan Cao},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=WE_vluYUL-X}
}

@inproceedings{
wei2022chain,
title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and brian ichter and Fei Xia and Ed H. Chi and Quoc V Le and Denny Zhou},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=_VjQlMeSB_J}
}

@inproceedings{
patil2024gorilla,
title={Gorilla: Large Language Model Connected with Massive {API}s},
author={Shishir G Patil and Tianjun Zhang and Xin Wang and Joseph E. Gonzalez},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=tBRNC6YemY}
}

@inproceedings{
patil2025the,
title={The Berkeley Function Calling Leaderboard ({BFCL}): From Tool Use to Agentic Evaluation of Large Language Models},
author={Shishir G Patil and Huanzhi Mao and Fanjia Yan and Charlie Cheng-Jie Ji and Vishnu Suresh and Ion Stoica and Joseph E. Gonzalez},
booktitle={Forty-second International Conference on Machine Learning},
year={2025},
url={https://openreview.net/forum?id=2GmDdhBdDk}
}

@inproceedings{
li2023apibank,
title={{API}-Bank: A Comprehensive Benchmark for Tool-Augmented {LLM}s},
author={Minghao Li and Yingxiu Zhao and Bowen Yu and Feifan Song and Hangyu Li and Haiyang Yu and Zhoujun Li and Fei Huang and Yongbin Li},
booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
year={2023},
url={https://openreview.net/forum?id=o2HBfgY20b}
}

@inproceedings{
boisvert2025doomarena,
title={DoomArena: A framework for Testing {AI} Agents Against Evolving Security Threats},
author={L{\'e}o Boisvert and Abhay Puri and Gabriel Huang and Mihir Bansal and Chandra Kiran Reddy Evuru and Avinandan Bose and Maryam Fazel and Quentin Cappart and Alexandre Lacoste and Alexandre Drouin and Krishnamurthy Dj Dvijotham},
booktitle={Second Conference on Language Modeling},
year={2025},
url={https://openreview.net/forum?id=GanmYQ0RpE}
}

@inproceedings{
andriushchenko2025agentharm,
title={AgentHarm: A Benchmark for Measuring Harmfulness of {LLM} Agents},
author={Maksym Andriushchenko and Alexandra Souly and Mateusz Dziemian and Derek Duenas and Maxwell Lin and Justin Wang and Dan Hendrycks and Andy Zou and J Zico Kolter and Matt Fredrikson and Yarin Gal and Xander Davies},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=AC5n7xHuR1}
}

@misc{xu2025qwen25omnitechnicalreport,
      title={Qwen2.5-Omni Technical Report}, 
      author={Jin Xu and Zhifang Guo and Jinzheng He and Hangrui Hu and Ting He and Shuai Bai and Keqin Chen and Jialin Wang and Yang Fan and Kai Dang and Bin Zhang and Xiong Wang and Yunfei Chu and Junyang Lin},
      year={2025},
      eprint={2503.20215},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2503.20215}, 
}

@misc{yang2025qwen3technicalreport,
      title={Qwen3 Technical Report}, 
      author={An Yang and Anfeng Li and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Gao and Chengen Huang and Chenxu Lv and Chujie Zheng and Dayiheng Liu and others},
      year={2025},
      eprint={2505.09388},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.09388}, 
}

@misc{kimiteam2025kimiaudiotechnicalreport,
      title={Kimi-Audio Technical Report}, 
      author={KimiTeam and Ding Ding and Zeqian Ju and Yichong Leng and Songxiang Liu and Tong Liu and Zeyu Shang and Kai Shen and Wei Song and Xu Tan and Heyi Tang and Zhengtao Wang and others},
      year={2025},
      eprint={2504.18425},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2504.18425}, 
}

@misc{gemmateam2025gemma3technicalreport,
      title={Gemma 3 Technical Report}, 
      author={Gemma Team and Aishwarya Kamath and Johan Ferret and Shreya Pathak and Nino Vieillard and Ramona Merhej and Sarah Perrin and Tatiana Matejovicova and Alexandre Ramé and Morgane Rivière and Louis Rouillard and Thomas Mesnard and Geoffrey Cideron and Jean-bastien Grill and Sabela Ramos and others},
      year={2025},
      eprint={2503.19786},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2503.19786}, 
}

@inproceedings{
ghosh2025audio,
title={Audio Flamingo 3: Advancing Audio Intelligence with Fully Open Large Audio Language Models},
author={Sreyan Ghosh and Ramani Duraiswami},
booktitle={TTIC Summer Workshop on Foundations of Speech and Audio Foundation Models 2025},
year={2025},
url={https://openreview.net/forum?id=6QVkUdLJFK}
}

@article{DBLP:journals/corr/abs-2410-17196,
  publtype={informal},
  author={Yiming Chen and Xianghu Yue and Chen Zhang and Xiaoxue Gao and Robby T. Tan and Haizhou Li},
  title={VoiceBench: Benchmarking LLM-Based Voice Assistants},
  year={2024},
  cdate={1704067200000},
  journal={CoRR},
  volume={abs/2410.17196},
  url={https://doi.org/10.48550/arXiv.2410.17196}
}

@article{DBLP:journals/corr/abs-2406-16020,
  publtype={informal},
  author={Bin Wang and Xunlong Zou and Geyu Lin and Shuo Sun and Zhuohan Liu and Wenyu Zhang and Zhengyuan Liu and AiTi Aw and Nancy F. Chen},
  title={AudioBench: A Universal Benchmark for Audio Large Language Models},
  year={2024},
  cdate={1704067200000},
  journal={CoRR},
  volume={abs/2406.16020},
  url={https://doi.org/10.48550/arXiv.2406.16020}
}

@inproceedings{
sakshi2025mmau,
title={{MMAU}: A Massive Multi-Task Audio Understanding and Reasoning Benchmark},
author={S Sakshi and Utkarsh Tyagi and Sonal Kumar and Ashish Seth and Ramaneswaran Selvakumar and Oriol Nieto and Ramani Duraiswami and Sreyan Ghosh and Dinesh Manocha},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=TeVAZXr3yv}
}

@misc{yang2025speechrbenchmarkspeechreasoning,
      title={SpeechR: A Benchmark for Speech Reasoning in Large Audio-Language Models}, 
      author={Wanqi Yang and Yanda Li and Yunchao Wei and Meng Fang and Ling Chen},
      year={2025},
      eprint={2508.02018},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2508.02018}, 
}

@article{DBLP:journals/corr/abs-2410-18050,
  publtype={informal},
  author={Qingfei Zhao and Ruobing Wang and Yukuo Cen and Daren Zha and Shicheng Tan and Yuxiao Dong and Jie Tang},
  title={LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering},
  year={2024},
  cdate={1704067200000},
  journal={CoRR},
  volume={abs/2410.18050},
  url={https://doi.org/10.48550/arXiv.2410.18050}
}

@inproceedings{
zhuang2023toolqa,
title={Tool{QA}: A Dataset for {LLM} Question Answering with External Tools},
author={Yuchen Zhuang and Yue Yu and Kuan Wang and Haotian Sun and Chao Zhang},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2023},
url={https://openreview.net/forum?id=pV1xV2RK6I}
}

@inproceedings{
yao2025taubench,
title={\{\${\textbackslash}tau\$\}-bench: A Benchmark for {\textbackslash}underline\{T\}ool-{\textbackslash}underline\{A\}gent-{\textbackslash}underline\{U\}ser Interaction in Real-World Domains},
author={Shunyu Yao and Noah Shinn and Pedram Razavi and Karthik R Narasimhan},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=roNSXZpUDN}
}

@misc{basu2025nestfulbenchmarkevaluatingllms,
      title={NESTFUL: A Benchmark for Evaluating LLMs on Nested Sequences of API Calls}, 
      author={Kinjal Basu and Ibrahim Abdelaziz and Kiran Kate and Mayank Agarwal and Maxwell Crouse and Yara Rizk and Kelsey Bradford and Asim Munawar and Sadhana Kumaravel and Saurabh Goyal and Xin Wang and Luis A. Lastras and Pavan Kapanipathi},
      year={2025},
      eprint={2409.03797},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2409.03797}, 
}

@misc{farn2023tooltalkevaluatingtoolusageconversational,
      title={ToolTalk: Evaluating Tool-Usage in a Conversational Setting}, 
      author={Nicholas Farn and Richard Shin},
      year={2023},
      eprint={2311.10775},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.10775}, 
}

@inproceedings{9747631,
  author={Guzhov, Andrey and Raue, Federico and Hees, Jörn and Dengel, Andreas},
  booktitle={ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Audioclip: Extending Clip to Image, Text and Audio}, 
  year={2022},
}

@INPROCEEDINGS{10095889,
  author={Elizalde, Benjamin and Deshmukh, Soham and Ismail, Mahmoud Al and Wang, Huaming},
  booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={CLAP Learning Audio Concepts from Natural Language Supervision}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  keywords={Training;Codes;Natural languages;Supervised learning;Buildings;Focusing;Predictive models;contrastive learning;general purpose audio representation;zero-shot;sound event classification},
  doi={10.1109/ICASSP49357.2023.10095889}}

@misc{radford2022robustspeechrecognitionlargescale,
      title={Robust Speech Recognition via Large-Scale Weak Supervision}, 
      author={Alec Radford and Jong Wook Kim and Tao Xu and Greg Brockman and Christine McLeavey and Ilya Sutskever},
      year={2022},
      eprint={2212.04356},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2212.04356}, 
}
@inproceedings{chen2024salm,
  author={Zhehuai Chen and He Huang and Andrei Andrusenko and Oleksii Hrinchuk and Krishna C. Puvvada and Jason Li and Subhankar Ghosh and Jagadeesh Balam and Boris Ginsburg},
  title={SALM: Speech-Augmented Language Model with in-Context Learning for Speech Recognition and Translation},
  year={2024},
  cdate={1704067200000},
  pages={13521-13525},
  url={https://doi.org/10.1109/ICASSP48485.2024.10447553},
  booktitle={ICASSP}
}

@misc{rubenstein2023audiopalmlargelanguagemodel,
      title={AudioPaLM: A Large Language Model That Can Speak and Listen}, 
      author={Paul K. Rubenstein and Chulayuth Asawaroengchai and Duc Dung Nguyen and Ankur Bapna and Zalán Borsos and Félix de Chaumont Quitry and Peter Chen and Dalia El Badawy and Wei Han and Eugene Kharitonov and Hannah Muckenhirn and Dirk Padfield and James Qin and Danny Rozenberg and Tara Sainath and Johan Schalkwyk and Matt Sharifi and Michelle Tadmor Ramanovich and Marco Tagliasacchi and Alexandru Tudor and Mihajlo Velimirović and Damien Vincent and Jiahui Yu and Yongqiang Wang and Vicky Zayats and Neil Zeghidour and Yu Zhang and Zhishuai Zhang and Lukas Zilka and Christian Frank},
      year={2023},
      eprint={2306.12925},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.12925}, 
}

@inproceedings{DBLP:conf/aaai/HuangLYSCYWHHLR24,
  author={Rongjie Huang and Mingze Li and Dongchao Yang and Jiatong Shi and Xuankai Chang and Zhenhui Ye and Yuning Wu and Zhiqing Hong and Jiawei Huang and Jinglin Liu and Yi Ren and Yuexian Zou and Zhou Zhao and Shinji Watanabe},
  title={AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head},
  year={2024},
  cdate={1704067200000},
  pages={23802-23804},
  url={https://doi.org/10.1609/aaai.v38i21.30570},
  booktitle={AAAI},
}

@article{DBLP:journals/corr/abs-2404-00656,
  publtype={informal},
  author={Shujie Hu and Long Zhou and Shujie Liu and Sanyuan Chen and Hongkun Hao and Jing Pan and Xunying Liu and Jinyu Li and Sunit Sivasankaran and Linquan Liu and Furu Wei},
  title={WavLLM: Towards Robust and Adaptive Speech Large Language Model},
  year={2024},
  cdate={1704067200000},
  journal={CoRR},
  volume={abs/2404.00656},
  url={https://doi.org/10.48550/arXiv.2404.00656}
}

@misc{chu2024qwen2audiotechnicalreport,
      title={Qwen2-Audio Technical Report}, 
      author={Yunfei Chu and Jin Xu and Qian Yang and Haojie Wei and Xipin Wei and Zhifang Guo and Yichong Leng and Yuanjun Lv and Jinzheng He and Junyang Lin and Chang Zhou and Jingren Zhou},
      year={2024},
      eprint={2407.10759},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2407.10759}, 
}
@inproceedings{
tang2024salmonn,
title={{SALMONN}: Towards Generic Hearing Abilities for Large Language Models},
author={Changli Tang and Wenyi Yu and Guangzhi Sun and Xianzhao Chen and Tian Tan and Wei Li and Lu Lu and Zejun MA and Chao Zhang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=14rn7HpKVk}
}
@inproceedings{
deshmukh2023pengi,
title={Pengi: An Audio Language Model for Audio Tasks},
author={Soham Deshmukh and Benjamin Elizalde and Rita Singh and Huaming Wang},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=gJLAfO4KUq}
}

@inproceedings{
gong2024listen,
title={Listen, Think, and Understand},
author={Yuan Gong and Hongyin Luo and Alexander H. Liu and Leonid Karlinsky and James R. Glass},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=nBZBPXdJlC}
}

@INPROCEEDINGS{7178964,
  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle={2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Librispeech: An ASR corpus based on public domain audio books}, 
  year={2015},
  volume={},
  number={},
  pages={5206-5210},
  keywords={Resource description framework;Genomics;Bioinformatics;Blogs;Information services;Electronic publishing;Speech Recognition;Corpus;LibriVox},
  doi={10.1109/ICASSP.2015.7178964}}

@inproceedings{ardila-etal-2020-common,
    title = "Common Voice: A Massively-Multilingual Speech Corpus",
    author = "Ardila, Rosana  and
      Branson, Megan  and
      Davis, Kelly  and
      Kohler, Michael  and
      Meyer, Josh  and
      Henretty, Michael  and
      Morais, Reuben  and
      Saunders, Lindsay  and
      Tyers, Francis  and
      Weber, Gregor",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.520/",
    pages = "4218--4222",
    language = "eng",
    ISBN = "979-10-95546-34-4",
    abstract = "The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identification). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla{'}s DeepSpeech Speech-to-Text toolkit. By applying transfer learning from a source English model, we find an average Character Error Rate improvement of 5.99 {\ensuremath{\pm}} 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the first ever published results on end-to-end Automatic Speech Recognition."
}
@inproceedings{di-gangi-etal-2019-must,
    title = "{M}u{ST}-{C}: a {M}ultilingual {S}peech {T}ranslation {C}orpus",
    author = "Di Gangi, Mattia A.  and
      Cattoni, Roldano  and
      Bentivogli, Luisa  and
      Negri, Matteo  and
      Turchi, Marco",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1202/",
    doi = "10.18653/v1/N19-1202",
    pages = "2012--2017",
    abstract = "Current research on spoken language translation (SLT) has to confront with the scarcity of sizeable and publicly available training corpora. This problem hinders the adoption of neural end-to-end approaches, which represent the state of the art in the two parent tasks of SLT: automatic speech recognition and machine translation. To fill this gap, we created MuST-C, a multilingual speech translation corpus whose size and quality will facilitate the training of end-to-end systems for SLT from English into 8 languages. For each target language, MuST-C comprises at least 385 hours of audio recordings from English TED Talks, which are automatically aligned at the sentence level with their manual transcriptions and translations. Together with a description of the corpus creation methodology (scalable to add new data and cover new languages), we provide an empirical verification of its quality and SLT results computed with a state-of-the-art approach on each language direction."
}
@INPROCEEDINGS{11011192,
  author={Shah, Sanket and Saxena, Kavya Ranjan and Bharadwaj, Kancharana Manideep and Adavanne, Sharath and Adiga, Nagaraj},
  booktitle={2025 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)}, 
  title={IndicST: Indian Multilingual Translation Corpus For Evaluating Speech Large Language Models}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  keywords={Training;Translation;Large language models;Training data;Signal processing;Data collection;Multilingual;Speech processing;Automatic speech recognition;Synthetic data;Speech LLMs;Speech encoders;Automatic Speech Translation;Automatic Speech Recognition},
  doi={10.1109/ICASSPW65056.2025.11011192}}

@misc{javed2024lahajarobustmultiaccentbenchmark,
      title={LAHAJA: A Robust Multi-accent Benchmark for Evaluating Hindi ASR Systems}, 
      author={Tahir Javed and Janki Nawale and Sakshi Joshi and Eldho George and Kaushal Bhogale and Deovrat Mehendale and Mitesh M. Khapra},
      year={2024},
      eprint={2408.11440},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.11440}, 
}

@inproceedings{yang21c_interspeech,
  title     = {SUPERB: Speech Processing Universal PERformance Benchmark},
  author    = {Shu-wen Yang and Po-Han Chi and Yung-Sung Chuang and Cheng-I Jeff Lai and Kushal Lakhotia and Yist Y. Lin and Andy T. Liu and Jiatong Shi and Xuankai Chang and Guan-Ting Lin and Tzu-Hsien Huang and Wei-Cheng Tseng and Ko-tik Lee and Da-Rong Liu and Zili Huang and Shuyan Dong and Shang-Wen Li and Shinji Watanabe and Abdelrahman Mohamed and Hung-yi Lee},
  year      = {2021},
  booktitle = {Interspeech 2021},
  pages     = {1194--1198},
  doi       = {10.21437/Interspeech.2021-1775},
  issn      = {2958-1796},
}

@inproceedings{javed24_interspeech,
  title     = {{LAHAJA: A Robust Multi-accent Benchmark for Evaluating Hindi ASR Systems}},
  author    = {Tahir Javed and Janki Nawale and Sakshi Joshi and Eldho George and Kaushal Bhogale and Deovrat Mehendale and Mitesh M. Khapra},
  year      = {2024},
  booktitle = {{Interspeech 2024}},
  pages     = {2320--2324},
  doi       = {10.21437/Interspeech.2024-2376},
  issn      = {2958-1796},
}

@inproceedings{shon-etal-2023-slue,
    title = "{SLUE} Phase-2: A Benchmark Suite of Diverse Spoken Language Understanding Tasks",
    author = "Shon, Suwon  and
      Arora, Siddhant  and
      Lin, Chyi-Jiunn  and
      Pasad, Ankita  and
      Wu, Felix  and
      Sharma, Roshan  and
      Wu, Wei-Lun  and
      Lee, Hung-yi  and
      Livescu, Karen  and
      Watanabe, Shinji",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.496/",
    doi = "10.18653/v1/2023.acl-long.496",
    pages = "8906--8937",
    abstract = "Spoken language understanding (SLU) tasks have been studied for many decades in the speech research community, but have not received as much attention as lower-level tasks like speech and speaker recognition. In this work, we introduce several new annotated SLU benchmark tasks based on freely available speech data, which complement existing benchmarks and address gaps in the SLU evaluation landscape. We contribute four tasks: question answering and summarization involve inference over longer speech sequences; named entity localization addresses the speech-specific task of locating the targeted content in the signal; dialog act classification identifies the function of a given speech utterance. In order to facilitate the development of SLU models that leverage the success of pre-trained speech representations, we will release a new benchmark suite, including for each task (i) curated annotations for a relatively small fine-tuning set, (ii) reproducible pipeline (speech recognizer + text model) and end-to-end baseline models and evaluation metrics, (iii) baseline model performance in various types of systems for easy comparisons. We present the details of data collection and annotation and the performance of the baseline models. We also analyze the sensitivity of pipeline models' performance to the speech recognition accuracy, using more than 20 publicly availablespeech recognition models."
}
@misc{javed2022indicsuperbspeechprocessinguniversal,
      title={IndicSUPERB: A Speech Processing Universal Performance Benchmark for Indian languages}, 
      author={Tahir Javed and Kaushal Santosh Bhogale and Abhigyan Raman and Anoop Kunchukuttan and Pratyush Kumar and Mitesh M. Khapra},
      year={2022},
      eprint={2208.11761},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2208.11761}, 
}
@misc{ma2025audiocotexploringchainofthoughtreasoning,
      title={Audio-CoT: Exploring Chain-of-Thought Reasoning in Large Audio Language Model}, 
      author={Ziyang Ma and Zhuo Chen and Yuping Wang and Eng Siong Chng and Xie Chen},
      year={2025},
      eprint={2501.07246},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2501.07246}, 
}
@inproceedings{yang-etal-2024-air,
    title = "{AIR}-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension",
    author = "Yang, Qian  and
      Xu, Jin  and
      Liu, Wenrui  and
      Chu, Yunfei  and
      Jiang, Ziyue  and
      Zhou, Xiaohuan  and
      Leng, Yichong  and
      Lv, Yuanjun  and
      Zhao, Zhou  and
      Zhou, Chang  and
      Zhou, Jingren",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.109/",
    doi = "10.18653/v1/2024.acl-long.109",
    pages = "1979--1998",
    abstract = "Recently, instruction-following audio-language models have received broad attention for human-audio interaction. However, the absence of benchmarks capable of evaluating audio-centric interaction capabilities has impeded advancements in this field. Previous models primarily focus on assessing different fundamental tasks, such as automatic speech recognition, and lack an assessment of the open-ended generative capabilities centered around audio. Thus, it is challenging to track the progression in the Large Audio-Language Models (LALMs) domain and to provide guidance for future improvement.In this paper, we introduce AIR-Bench (Audio InstRuction Benchmark), the first benchmark designed to evaluate the ability of LALMs to understand various types of audio signals (including human speech, natural sounds, and music), and furthermore, to interact with humans in the textual format. AIR-Bench encompasses two dimensions: foundation and chat benchmarks. The former consists of 19 tasks with approximately 19k single-choice questions, intending to inspect the basic single-task ability of LALMs. The latter one contains 2k instances of open-ended question-and-answer data, directly assessing the comprehension of the model on complex audio and its capacity to follow instructions. Both benchmarks require the model to generate hypotheses directly. We design a unified framework that leverages advanced language models, such as GPT-4, to evaluate the scores of generated hypotheses given the meta-information of the audio. Experimental results demonstrate a high level of consistency between GPT-4-based evaluation and human evaluation. By revealing the limitations of existing LALMs through evaluation results, AIR-Bench can provide insights into the direction of future research. Dataset and evaluation code are available at https://github.com/OFA-Sys/AIR-Bench."
}
@inproceedings{sharma-etal-2025-indicsynth,
    title = "{I}ndic{S}ynth: A Large-Scale Multilingual Synthetic Speech Dataset for Low-Resource {I}ndian Languages",
    author = "Sharma, Divya V  and
      Ekbote, Vijval  and
      Gupta, Anubha",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-long.1070/",
}

%% Intro - LLMs

@article{touvron2023llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{grattafiori2024llama,
  title={The llama 3 herd of models},
  author={Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{abdin2025phi,
  title={Phi-4-reasoning technical report},
  author={Abdin, Marah and Agarwal, Sahaj and Awadallah, Ahmed and Balachandran, Vidhisha and Behl, Harkirat and Chen, Lingjiao and de Rosa, Gustavo and Gunasekar, Suriya and Javaheripi, Mojan and Joshi, Neel and others},
  journal={arXiv preprint arXiv:2504.21318},
  year={2025}
}
@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@article{yang2025qwen3,
  title={Qwen3 technical report},
  author={Yang, An and Li, Anfeng and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Gao, Chang and Huang, Chengen and Lv, Chenxu and others},
  journal={arXiv preprint arXiv:2505.09388},
  year={2025}
}

@article{team2025gemma,
  title={Gemma 3 technical report},
  author={Team, Gemma and Kamath, Aishwarya and Ferret, Johan and Pathak, Shreya and Vieillard, Nino and Merhej, Ramona and Perrin, Sarah and Matejovicova, Tatiana and Ram{\'e}, Alexandre and Rivi{\`e}re, Morgane and others},
  journal={arXiv preprint arXiv:2503.19786},
  year={2025}
}

@inproceedings{PointNet++,
 author = {Qi, Charles Ruizhongtai and Yi, Li and Su, Hao and Guibas, Leonidas J},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d8bf84be3800d12f74d8b05e9b89836f-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{
wang2025diversity,
title={Diversity Measurement and Subset Selection for Instruction Tuning Datasets},
author={Peiqi Wang and Yikang Shen and Zhen Guo and Matthew Stallone and Yoon Kim and Polina Golland and Rameswar Panda},
booktitle={ICLR 2025 Workshop on Navigating and Addressing Data Problems for Foundation Models},
year={2025},
url={https://openreview.net/forum?id=cV9OF45hBb}
}

@inproceedings{coreset,
 author = {Mahabadi, Sepideh and Trajanovski, Stojan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {78987--79011},
 publisher = {Curran Associates, Inc.},
 title = {Core-sets for Fair and Diverse Data Summarization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/f980ba94f513168f2b292f58aef929ec-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@article{hurst2024gpt,
  title={Gpt-4o system card},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}


@inproceedings{Desplanques_2020,
   title={ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in {TDNN} Based Speaker Verification},
   author={Desplanques, Brecht and Thienpondt, Jenthe and Demuynck, Kris},
   booktitle={Interspeech 2020},
   publisher={ISCA},
   year={2020},
   month={October},
   doi={10.21437/Interspeech.2020-2650}
}


@misc{yang2025measuringdatadiversityinstruction,
      title={Measuring Data Diversity for Instruction Tuning: A Systematic Analysis and A Reliable Metric}, 
      author={Yuming Yang and Yang Nan and Junjie Ye and Shihan Dou and Xiao Wang and Shuo Li and Huijie Lv and Mingqi Wu and Tao Gui and Qi Zhang and Xuanjing Huang},
      year={2025},
      eprint={2502.17184},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.17184}
}



@article{2025bhasha,
  title={BhashaKritika: Building Synthetic Pretraining Data at Scale for Indic Languages},
  author={Authors Anonymous},
  journal={Under submission},
  year={2025}
}

@misc{dinkel2025midashenglmefficientaudiounderstanding,
      title={MiDashengLM: Efficient Audio Understanding with General Audio Captions}, 
      author={Heinrich Dinkel and Gang Li and Jizhong Liu and Jian Luan and Yadong Niu and Xingwei Sun and Tianzi Wang and Qiyang Xiao and Junbo Zhang and Jiahao Zhou},
      year={2025},
      eprint={2508.03983},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2508.03983}, 
}

@misc{macháček2023turningwhisperrealtimetranscription,
      title={Turning Whisper into Real-Time Transcription System}, 
      author={Dominik Macháček and Raj Dabre and Ondřej Bojar},
      year={2023},
      eprint={2307.14743},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.14743}, 
}