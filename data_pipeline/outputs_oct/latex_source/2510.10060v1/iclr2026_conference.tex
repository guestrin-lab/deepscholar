
\documentclass{article} % For LaTeX2e
\usepackage{iclr2026_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage{subcaption}

\usepackage{wrapfig}
\usepackage[table,xcdraw]{xcolor}

\usepackage{dsfont}
\usepackage{pifont}
\usepackage{cancel}
\usepackage{array}
\definecolor{my_green}{HTML}{2a9d8f}
\definecolor{my_red}{HTML}{e76f51}
\definecolor{my_yellow}{HTML}{e9c46a} 


\usepackage{mathtools}
\usepackage{multirow}
\usepackage{multicol}
\newcommand{\etal}{\emph{et al}.}
\newcommand{\ie}{\emph{i.e}.}
\newcommand{\eg}{\emph{e.g}.}
\newcommand{\etc}{\emph{etc}}
\newcommand{\wrt}{\emph{w.r.t~}}

% \title{Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling}

\title{Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling}

\iclrfinalcopy 
% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

% \thanks{ Use footnote for providing further information about author (webpage, alternative address)---\emph{not} for acknowledging funding agencies.  Funding acknowledgements go at the end of the paper.} 




\author{
\hspace{-0.52em}
Hehe Fan \\
Zhejiang University\\
\texttt{hehefan@zju.edu.cn} \\
\And
Yi Yang \\
Zhejiang University\\
\texttt{yangyics@zju.edu.cn} 
\AND
Mohan Kankanhalli \\
National University of Singapore \\
\texttt{mohan@comp.nus.edu.sg} \\
\And
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~Fei Wu \\
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~Zhejiang University \\
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\texttt{wufei@zju.edu.cn}
}

% \begin{tabular}{cc}
% Hehe Fan & Yi Yang \\
% Zhejiang University & Zhejiang University \\
% \end{tabular}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
When modeling a given type of data, we consider it to involve two key aspects: ~~1) identifying relevant  elements (\eg, image pixels or textual words) to a central element, as in a convolutional receptive field, or to a query element, as in self-attention, and 2) encoding these tokens effectively. 
Self-attention can adaptively identify these elements but relies on absolute positional embedding for structural representation learning. 
In contrast, convolution encodes elements in a relative manner, yet their fixed kernel size limits their ability to adaptively select the relevant elements. 
In this paper, we introduce Translution, an operation that unifies the adaptive identification capability of self-attention and the relative encoding advantage of convolution. 
However, this integration leads to a substantial increase in the number of parameters, exceeding most currently available computational resources. 
Therefore, we propose a lightweight variant of Translution, named $\alpha$-Translution. 
Experiments on computer vision and natural language processing tasks show that Translution (including $\alpha$-Translution) achieves superior accuracy compared to self-attention. The code is available at \url{https://github.com/hehefan/Translution}.
\end{abstract}

% , demonstrating its potential to build the next generation of deep neural networks. 
\section{Introduction}

Recent evidence suggests that directly scaling up deep neural networks, particularly Transformers~\citep{DBLP:conf/nips/VaswaniSPUJGKP17,radford2018improving,DBLP:conf/naacl/DevlinCLT19,DBLP:conf/iclr/DosovitskiyB0WZ21}, with additional data and parameters is encountering diminishing returns. Leading Artificial Intelligence (AI) labs have similarly noted slower-than-anticipated improvements in next-generation models, despite extensive training efforts. Given the saturation of available data and limitations imposed by current scaling laws, it is crucial now to reflect on past successes and pursue the design of innovative neural networks to sustain future progress in deep learning.   

When employing deep neural networks to model a specific type of data, the process can be decomposed into two key aspects: 1) identifying relevant data elements and 2) encoding these elements into effective representations.  
When using convolutional neural networks~\citep{DBLP:journals/pieee/LeCunBBH98,DBLP:conf/nips/KrizhevskySH12,DBLP:journals/corr/SimonyanZ14a,DBLP:conf/cvpr/SzegedyLJSRAEVR15,DBLP:conf/cvpr/HeZRS16} to process images, the basic element is pixel. When using Transformers, the element is  word for natural language processing and  patch for visual tasks. 



\subsection{Identification of Relevant Elements}

In convolution, as shown in Figure~\ref{fig:1}~(a), the relevant element identification step is handled by convolutional filters (kernels) with a fixed local receptive field.
This fixed kernel defines a neighborhood that is considered relevant to the center. 
For visual data like images, such local focus is often effective because spatially adjacent pixels  tend to be related (\eg, forming parts of the same object). 
However, the rigid nature of a fixed-size kernel makes convolution inevitably cover irrelevant pixels, especially near object boundaries or in background areas that fall inside the window. 

In contrast, as shown in Figure~\ref{fig:1}~(b), self-attention~\citep{DBLP:conf/nips/VaswaniSPUJGKP17} can adaptively identify relevant regions. Instead of being limited to a predetermined locality, it allows the model to dynamically attend to relevant regions. This means that self-attention can focus on important features regardless of their physical distance. This capability provides greater flexibility compared to the convolution's fixed receptive field.
% , making self-attention  particularly well-suited for processing lengthy text.   

% \vspace{-0.5em}
\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{imgs/intro-attn.pdf}
    \vspace{-2em}
    \caption{Difference between convolution and self-attention in identifying relevant elements (blue patches) for the kernel center or query element (yellow patch). Here, convolution is assumed to operate on image patches. 1) Convolution utilizes a fixed kernel size to define a neighborhood of elements considered relevant, inevitably including some irrelevant regions, particularly near object boundaries or within background areas inside the window.     
    The fixed receptive field in convolution can be interpreted as a special case of attention, where the attention score is set to 1 within the receptive field and 0 outside it.
    2) Self-attention adaptively identifies relevant elements by assigning greater attention scores to areas with higher relevance, thereby mitigating the inclusion of noisy or irrelevant information.} 
    \label{fig:1}
    % \vspace{-1em}
\end{figure} 

\subsection{Encoding of Relevant Elements}

When it comes to encoding the structure from these relevant elements, convolution and self-attention employ different strategies. As shown in Figure~\ref{fig:2}~(a), a convolutional kernel learns distinct parameters $\{\mW_{\delta_x, \delta_y}\}$ for each relative  direction and distance within its receptive field. 
In other words, the filter has separate parameters $\mW_{\delta_x, \delta_y}$ for each offset $\delta_x, \delta_y$  from the center. This design enables convolution to encode local structure relatively — capturing orientation and distance relationships.  


In contrast, as shown in Figure~\ref{fig:2}~(b), self-attention uses three shared sets of parameters $\mW^q$, $\mW^k$ and $\mW^v$ to process inputs for all positions. 
Consequently, the query, key and value of self-attention do not encode whether one patch is to the left or right of another. 
To introduce positional information, Transformer incorporates absolute positional embeddings into the input features at the outset. 
Although these embeddings enable Transformer to infer order or spatial relationships, they introduce noise into each token's representation. The absolute position information becomes part of the input features. Consequently, when the same object moves to a different location, Transformer may struggle to recognize it. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\linewidth]{imgs/intro-enco.pdf}
    \vspace{-2em}
    \caption{Difference between convolution and self-attention in encoding relevant elements: consider the scenario where convolution and self-attention are capturing the structure of a circle. 
    1) Convolution learns separate parameters $\{\mW_{\delta_x, \delta_y}\}$ for each offset, where $\delta_x, \delta_y \in [-1,1]$, from the kernel center, allowing it to effectively encode relative local structures. Thus, when the circle appears in a different location, it is still readily recognized due to this relative awareness.
    2) Self-attention incorporates absolute position into each token's representation and uses position-irrelevant parameters $\mW \in \{\mW^q, \mW^k, \mW^v\}$ across all tokens for computing query, key and value, respectively. While this method facilitates general processing, the inclusion of absolute positional  embeddings makes it more challenging to recognize the circle when it is moved to a different location.} 
    \label{fig:2}
    % \vspace{-1em}
\end{figure} 



\subsection{Unification of Convolution and Transformer}


In summary, convolution encodes structure through fixed local filters with position-specific weights, whereas self-attention relies on adaptive global attention and requires absolute positional encoding to capture order or spatial structures. 

In this paper, we introduce Translution, a new type of operation that unifies the adaptive identification capability of self-attention with the relative encoding advantage of convolution.
Specifically, Translution employs a convolution-style approach that assigns separate parameters (matrices) to each distance and direction when computing the query, key and value. This design enables Translution to effectively encode relative structures. 

% However, this unification leads to a significant increase in the number of parameters. Specifically, for an input with $H \times W$ image patches, self-attention requires only 1 set of shared parameters to calculate query, key or value, whereas Translution consumes $(2H-1) \times (2W-1)$ sets of parameters. 
% This exceeds most currently available computational resources. 
% Therefore, we propose a lightweight variant of Translution, named $\alpha$-Translution, which significantly reduces the number of  parameters. 
% This variant achieves lower accuracy than  the ``ideal'' (original) Translution but better accuracy than self-attention.


However, this unification leads to a significant increase in the number of parameters and exceeds most currently available computational resources. 
Therefore, we propose a lightweight variant of Translution, named $\alpha$-Translution, which significantly reduces the number of parameters. 
This variant achieves lower accuracy than  the ``ideal'' (original) Translution but better accuracy than self-attention.
%, demonstrating its potential to build the next generation of deep neural networks. 

As a fundamental operation, we investigate whether Translution can outperform self-attention. We conduct experiments on two widely-used Transformer architectures: Vision Transformer (ViT)~\citep{DBLP:conf/iclr/DosovitskiyB0WZ21} for computer vision tasks and Generative Pre-trained Transformer (GPT)~\citep{radford2018improving,radford2019language,DBLP:conf/nips/BrownMRSKDNSSAA20} for natural language processing tasks. Experiments demonstrate that Translution and $\alpha$-Translution surpass self-attention in terms of accuracy.  


\section{Related Work} 


Transformer~\citep{DBLP:conf/nips/VaswaniSPUJGKP17,radford2018improving,DBLP:conf/naacl/DevlinCLT19,DBLP:conf/iclr/DosovitskiyB0WZ21,DBLP:conf/iccv/LiuL00W0LG21,DBLP:conf/icml/TouvronCDMSJ21} eschews recurrence (as used in recurrent neural networks) and kernel size (as used in convolutional neural networks), instead employing self‐attention for relevant region identification.  
Because it has no built‐in notion of order, Transformer incorporates explicit absolute positional embeddings into token embeddings, enabling the model to utilize sequence order. 
Subsequent work has explored ``relative attention''~\citep{DBLP:conf/naacl/ShawUV18,DBLP:conf/iclr/HuangVUSHSDHDE19,DBLP:conf/nips/ParmarRVBLS19,DBLP:conf/acl/DaiYYCLS19,DBLP:conf/emnlp/TsaiBYMS19,DBLP:journals/jmlr/RaffelSRLNMZLL20,DBLP:conf/nips/DaiLLT21}, which integrates relative position information into self‐attention. 
They can be categorized into three families: 
\textit{1) Relative positional vector.} Shaw~\etal enhanced Transformer for language modeling by adding learnable relative positional vectors into the key and value computations, respectively~\citep{DBLP:conf/naacl/ShawUV18}. 
BoTNet~\citep{DBLP:conf/cvpr/SrinivasLPSAV21} and HaloNet~\citep{DBLP:conf/cvpr/VaswaniRSPHS21} extended this approach to two dimensions for image processing by adding learnable relative positional vectors into key. 
\textit{2) Relative positional scalar.} Swin Transformer~\citep{DBLP:conf/iccv/LiuL00W0LG21},  CoAtNet~\citep{DBLP:conf/nips/DaiLLT21}, and ConViT~\cite{DBLP:conf/icml/dAscoliTLMBS21} incorporate a learnable relative positional bias (a scalar) into the attention score. 
In these methods, the original self-attention can be regarded as content attention, which measures relationships from the token-feature perspective, while the additional relative positional bias can be regarded as position attention, which measures relationships from the token-position perspective. 
\textit{3) Rotary position embedding.}  RoFormer~\citep{DBLP:journals/ijon/SuALPBL24} introduces a rotary position embedding  mechanism, which encodes relative positional information by applying a rotation operation in the Query and Key representation space. 
Unlike these existing methods, Translution employs a convolution-style approach that uses relative positional matrices for query, key and value computation. 
Section~\ref{sec:position} provides a formal comparison of these methods. 

Convolutional neural networks ~\citep{DBLP:journals/pieee/LeCunBBH98,DBLP:conf/nips/KrizhevskySH12,DBLP:journals/corr/SimonyanZ14a,DBLP:conf/cvpr/SzegedyLJSRAEVR15,DBLP:conf/cvpr/HeZRS16} have been the backbone of deep learning for years. By using small, shared kernels and pooling, convolutional neural networks efficiently capture local patterns.  
Recent architectural developments integrate self-attention with convolution. 
For instance, Conformer~\citep{DBLP:conf/interspeech/GulatiQCPZYHWZW20} combines convolution layers and self-attention layers to capture both local and global dependencies in audio sequences.
Similarly, CeiT~\citep{DBLP:conf/iccv/YuanG0ZYW21} uses convolutions to extract low-level features and self-attention to model long-range dependencies.
Unlike these architectural methods, Translution operates at the basic module or layer level, blending the advantages of self-attention and convolution into a unified fundamental operation. 

\section{Preliminary: Convolution and Self-attention}

\subsection{Convolution}

Suppose $\vf_{x,y} \in \R^{1 \times C}$ denotes the feature or representation at location $(x,y)$ in an image of height $H$ and width $W$, where $C$ is the number of the input feature channels. 
Convolution is designed to capture the local structure centered at $(x,y)$ with a fixed kernel size $h \times w$,   
\begin{equation*}
    \vf'_{x,y} = \sum_{\delta_x =-\lfloor h/2 \rfloor}^{\lfloor h/2 \rfloor} ~ \sum_{\delta_y =-\lfloor w/2 \rfloor}^{\lfloor w/2 \rfloor} \vf_{x+\delta_x,y+\delta_y} \cdot \mW_{\delta_x, \delta_y},
\end{equation*} 
where $\mW_{\delta_x, \delta_y} \in  \R^{C \times C'}$ denotes the learnable parameters corresponding to the displacement $(\delta_x, \delta_y)$, $C'$ indicates the output feature dimension, and $\cdot$ denotes matrix multiplication. 
By assigning a set of parameters for each offset within the receptive field, convolution is able to discern direction and distance, and capture the local structure relatively. This means that when the absolute location of an object changes, it can still capture the same structure. 
However, convolution employs a rigid method to identify relevant regions, \ie, using a fixed-size window, making it inevitably include irrelevant pixels or regions — particularly near object boundaries or in background areas within the window. 

\subsection{Self-attention} 
Suppose $\vx_i \in \R^{1 \times C}$ represents the feature or representation of the $i$-th patch at location $(x_i, y_i)$.     
Transformer~\citep{DBLP:conf/nips/VaswaniSPUJGKP17} first incorporates the embedding of absolute position into the input $\vx_i$, as follows, 
\begin{equation*} 
    \mathrm{input~~positional~~embedding}\!: ~~ \vf_i = \vx_i + \mathrm{Embed}(x_i, y_i).   
\end{equation*} 
Then, self-attention performs two separate linear projections on the feature to generate query $\vq_i \in \R^{1 \times C'}$ and key $\vk_j \in \R^{1 \times C'}$,  where $C'$ is the dimension for query or key,
\begin{equation*}
\begin{aligned}
    & \mathrm{query~~encoding}\!: &  \vq_i = \vf_i \cdot \mW^q, \\
    & \mathrm{key~~encoding}\!: & \vk_j = \vf_j \cdot \mW^k,
\end{aligned}
\end{equation*}
where $\mW^q/\mW^k \in \R^{C \times C'}$.  
Subsequently, scaled dot-product attention is computed for each query, and a softmax function is applied to normalize the attention weights for a query across all positions, 
\begin{equation*}
      \mathrm{attention}\!: ~~  a_{i,j} = \frac{\vq_i \cdot \vk_j^T}{\sqrt{C'}},   ~~~ \alpha_{i,j} = \frac{e^{a_{i,j}}}{\sum_{n=1}^Ne^{a_{i,n}}},  
\vspace{-0.3em} 
\end{equation*}
where $N = H \times W$. 
Next, self-attention conducts another linear projection on the input feature  to generate value  $\vv_i \in \R^{1 \times C'}$, as follows, 
\begin{equation*}
   \mathrm{value~~encoding}\!: ~~ \vv_j =  \vf_j \cdot \mW^v, 
\end{equation*} 
where $\mW_v \in \R^{C \times C'}$. Finally, the output is computed as a weighted sum of the values, \ie, 
\begin{equation*}
    \mathrm{weighted~~sum}\!: ~~ \vf'_i = \sum_{j=1}^N \alpha_{i,j} \times  \vv_j, 
\end{equation*} 
where $\vf_i' \in \R^{1 \times C'}$.  
In this way, self-attention can adaptively search for related regions, providing greater flexibility than methods that use local fixed-size windows. However, unlike convolution, which learns a feature encoding for every direction and distance, self-attention does not encode the structure in a relative manner.  


\subsection{Translution} 
\label{sec:sec:lution}

Translution is designed to integrate the adaptive related region identification capabilities of self-attention with the relative encoding strengths of convolution.  Specifically, as shown in Figure~\ref{fig:3}, Translution employs a convolution-style formulation by assigning different  parameters to compute query, key, and value, respectively, as follows: 
\begin{equation}
\label{eq:lution}
\mathrm{Translution\!} ~ \left\{
\begin{aligned}
    & \mathrm{relative~~query~~encoding}\!: ~  \vq_{i,j} = \vf_i \cdot \mW^q_{\delta_x, \delta_y}, ~ \delta_x = x_i - x_j, ~ \delta_y = y_i - y_j, \\
    & \mathrm{relative~~key~~encoding}\!: ~ \vk_{j,i} = \vf_j \cdot \mW^k_{-\delta_x, -\delta_y}, \\
    & \mathrm{relative~~attention}\!:  ~ a_{i,j} = \frac{\vq_{i,j} \cdot \vk_{j,i}^T}{\sqrt{C'}},   ~ \alpha_{i,j} = \frac{e^{a_{i,j}}}{\sum_{n=1}^Ne^{a_{i,n}}},   \\ 
    &\mathrm{relative~~value~~encoding}\!: ~  \vv_{i,j} = \vf_j \cdot \mW^v_{\delta_x, \delta_y},    \\
    & \mathrm{weighted~~sum}\!:  ~ \vf'_i = \sum_{j=1}^N \alpha_{i,j} \times  \vv_{i,j}, 
\end{aligned}
\right. 
\end{equation}
where $\mW^q_{\delta_x, \delta_y}/\mW^k_{\delta_x, \delta_y}/\mW^v_{\delta_x, \delta_y} \in \R^{C \times C'}$,  represent the learnable parameter matrices for the query, key, and value corresponding to the displacement $(\delta_x, \delta_y)$. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{imgs/2translution.pdf} 
    \vspace{-1.5em}
    \caption{Comparison of self-attention and Translution. 1) Self-attention  employs three shared sets of weights, \ie, $\mW^q$, $\mW^k$, and $\mW^v$, across all patches to compute query, key, and value, respectively.  2) Translution uses separate parameters for each offset (direction and distance), \ie, $\{\mW^q_{\delta_x, \delta_y}\}$, $\{\mW^k_{\delta_x, \delta_y}\}$ and $\{\mW^v_{\delta_x, \delta_y}\}$, to encode relative structures.
    % Suppose there are $H \times W$ patches, Translution consumes $(2H-1) \times (2W-1)$ times more parameters compared to Transformer. This may be one of the major issues with Translution. However, with the upgrading of computational  resources and the increase in GPU memory, this may not be a problem in the future.
    } 
    \vspace{-0.5em}
    \label{fig:3}
\end{figure}

\textit{Translution unifies convolution and self-attention.} 

The fixed receptive field in convolution can be interpreted as a special case of attention, where the attention score is set to 1 within the receptive field and 0 outside it, as shown in Figure~\ref{fig:2}. The weights $\mW^q$, $\mW^k$, and $\mW^v$ in self-attention serve as shared linear projections that are uniformly applied across all spatial directions and distances.
 Consequently, Translution integrates the functionalities of convolution and self-attention, as follows, 
\begin{equation*} 
\begin{array}{lll}
\mathrm{Convolution\!:} & \vf'_i = \sum_{j=1}^N \alpha_{i, j}  \times \vf_j \cdot  \mW_{\delta_x, \delta_y}, & ~~ \mathrm{where} ~~  \alpha_{i, j} = \left\{ \begin{array}{ll}
     1,    & (\delta_x, \delta_y) \in \mathrm{kernel},   \\
     0,    &  \mathrm{otherwise}. 
    \end{array}  \right. \\ 
\mathrm{Self\!-\!attention\!:} &  \vf'_i = \sum_{j=1}^N \alpha_{i, j}  \times \vf_j \cdot \mW^v, & ~~ \mathrm{where} ~~ a_{i,j} = \frac{\vq_i \cdot \vk_j^T}{\sqrt{C'}}, ~~~~~ \alpha_{i,j} = \frac{e^{a_{i,j}}}{\sum_{n=1}^Ne^{a_{i,n}}}.  \\ 
\mathrm{Translution\!:} &  \vf'_i = \sum_{j=1}^N \alpha_{i, j}  \times \vf_j \cdot  \mW^v_{\delta_x, \delta_y}, & ~~ \mathrm{where} ~~ a_{i,j} = \frac{\vq_{i,j} \cdot \vk_{j,i}^T}{\sqrt{C'}}, ~~ \alpha_{i,j} = \frac{e^{a_{i,j}}}{\sum_{n=1}^Ne^{a_{i,n}}}. 
\end{array}
\end{equation*} 
In other words, convolution and self-attention can be viewed as specific instances of Translution, where convolution simplifies the attention mechanism and self-attention omits the encoding of direction and distance. 







\subsection{$\alpha$-Translution}

Suppose there are $H \times W$ input image patches. The relative encoding method in Translution requires $(2H - 1) \times (2W - 1) \times C \times C'$ parameters.
Specifically, it requires one parameter matrix $\mW^q_{\delta_x,\delta_y}$, $\mW^k_{\delta_x,\delta_y}$ or $\mW^v_{\delta_x,\delta_y} \in \R^{C \times C'}$ for each relative position $(\delta_x,\delta_y)$, where $\delta_x \in \{-(H-1), \cdots, 0, \cdots, H-1\}$ and $\delta_y \in \{-(W-1), \cdots, 0, \cdots, W-1\}$.  
This approach leads to excessive parameter demands, making it impractical for most computational devices currently. For instance, in the ViT/16 architecture~\citep{DBLP:conf/iclr/DosovitskiyB0WZ21}  with input resolution $224\times224$, we have $H = W = \frac{224}{16}=14$, resulting in $(2H - 1) \times (2W - 1)  = 729$ distinct weight matrices for query, key or value. 
To reduce the number of parameters, we propose a variant of Translution, \ie, $\alpha$-Translution, which decreases both the input dimension $C$ and the output dimension $C’$ of each $\mW^q_{\delta_x,\delta_y}$, $\mW^k_{\delta_x,\delta_y}$, and $\mW^v_{\delta_x,\delta_y}$, as follows: 
\vspace{0.5em}
\begin{equation*}
\mW^q_{\delta_x,\delta_y} \Rightarrow \mW^q_1 \cdot \mW^q_{\delta_x,\delta_y}, ~~~~~~ \mW^k_{\delta_x,\delta_y} \Rightarrow \mW^k_1 \cdot \mW^k_{\delta_x,\delta_y}, ~~~~~~ \mW^v_{\delta_x,\delta_y} \Rightarrow \mW^v_1 \cdot \mW^v_{\delta_x,\delta_y} \cdot \mW^v_2,
\vspace{0.3em}
\end{equation*} 
where $\mW^q_1/\mW^k_1/\mW^v_1 \in \R^{C \times C^1}$, $\mW^q_{\delta_x, \delta_y}/\mW^k_{\delta_x, \delta_y}/\mW^v_{\delta_x, \delta_y}  \in \R^{C^1 \times C^2} $, $\mW^v_2 \in \R^{C^2 \times C'}$, and $C^1 \ll C$, $C^2 \ll C'$. 
Smaller values of $C^1$ and $C^2$ will significantly reduce the number of parameters. 

However, setting $C^1$ and $C^2$ too small may overly compress the query, key and value information, negatively impacting performance. 
To preserve the information, we incorporate the query, key and value computation mechanism of self-attention into $\alpha$-Translution. Specifically, the updated computation is defined as follows: 
\begin{equation}
\label{eq:alution}
\alpha\mathrm{\!-\!Translution\!} ~ \left\{
\begin{aligned}
    ~ & \mathrm{query~~encoding}\!: ~~~  \vq_{i,j} = \vf_i \cdot \mW^q_1 \cdot \mW^q_{\delta_x, \delta_y}, ~~~ \vq_i = \vf_i \cdot \mW^q,  \\
    ~ & \mathrm{key~~encoding}\!: ~~~ \vk_{j,i} = \vf_j \cdot \mW^k_1 \cdot \mW^k_{-\delta_x, -\delta_y}, ~~~ \vk_j = \vf_j \cdot \mW^k,  \\
    ~ & \mathrm{attention}\!:  ~~~ a_{i,j} = \frac{\vq_{i,j} \cdot \vk_{j,i}^T + \vq_i \cdot \vk_j^T}{\sqrt{C'}},   ~~~ \alpha_{i,j} = \frac{e^{a_{i,j}}}{\sum_{n=1}^Ne^{a_{i,n}}},   \\ 
    ~ &\mathrm{value~~encoding}\!: ~~~  \vv_{i,j} = \vf_j \cdot (\mW^v_1 \cdot \mW^v_{\delta_x, \delta_y} \cdot \mW^v_2 + \mW^v),    \\
    ~ & \mathrm{weighted~~sum}\!:  ~~~ \vf'_i = \sum_{j=1}^N \alpha_{i,j} \times  \vv_{i,j}. 
\end{aligned} 
\right.
\end{equation}
In this way, $\alpha$-Translution not only possesses relative modeling capability but also reduces the number of parameters. 


\section{Experiment}

In this section, as a fundamental operation, our primary objective is to compare Translution with self-attention, rather than to achieve state-of-the-art performance through specialized network architectures or extensive training techniques.
To this end, we conduct experiments using two widely adopted Transformer architectures: 
\begin{itemize}
\vspace{-0.5em}
\setlength{\leftmargin}{0pt}
\setlength\itemsep{-0.25em}
    \item Vision Transformer (ViT)~\citep{DBLP:conf/iclr/DosovitskiyB0WZ21} for computer vision tasks. 
    \item Generative Pre-trained Transformer (GPT)~\citep{radford2018improving,radford2019language,DBLP:conf/nips/BrownMRSKDNSSAA20} for natural language processing tasks. Section~\ref{sec:1translution} demonstrates how to apply Translution to text modeling. 
\vspace{-0.75em}
\end{itemize}  

Table~\ref{tab:vit} provides an overview of various architecture configures. We substitute self-attention in ViT and GPT with Translution, while maintaining the remaining architecture unchanged. 

\begin{table}[h]
\small
    \vspace{-1em}
    \centering
    \caption{Specifics of architecture configures used in this paper.}
    \vspace{-1em}
    \begin{tabular}{c|cccc}
    \hline
    \rowcolor[HTML]{EDEDED} Architecture & Depth (\#Layers) & Embedding Dim (Hidden size) & \#Heads & MLP Dim (Feedforward) \\ \hline
    A & 6 & 192 & 3 & 768 \\
    B & 12 & 192 & 3 & 768 \\
    C & 12 & 384 & 6 & 1,536 \\
    \hline
    \end{tabular}
    \label{tab:vit}
% \vspace{-1.75em}
\vspace{-1em}
\end{table} 

Due to limited computational resources, our evaluation is primarily conducted on small- and medium-scale architectures. Large-scale evaluation can be performed when single-GPU memory capacities approach approximately $2\sim3$ TB. All training starts from scratch.  The default compression dimensions for the relative encoding in  $\alpha$-Translution are set as $C^1 = C^2 = 8$.  

\subsection{Image Classification with ViT}

\subsubsection{Dynamic MNIST}

To evaluate the capability of modeling relative structure, we synthesize a dynamic MNIST dataset~\citep{DBLP:conf/icml/SrivastavaMS15,DBLP:journals/corr/abs-1910-08287}, where digits (originally sized $28 \times 28$ pixels) move within a $84 \times 84$ pixel area, as illustrated in Figure~\ref{fig:mm}. 
 For comparison, we also create a static MNIST dataset of the same size, where digits remain fixed at the center of each image.  
 \vspace{-0.5em} 
\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{imgs/mnist.png}
    \vspace{-1.5em} 
    \caption{Examples of static and dynamic MNIST. Static MNIST digits are fixed at the center of images, whereas dynamic MNIST digits are randomly positioned within the images.}
    \label{fig:mm}
    \vspace{-1.0em}
\end{figure}

 \begin{table}[h]
\small
     \vspace{-0.5em}
    \centering
    \setlength{\tabcolsep}{1.4pt} 
    \caption{Top-1 accuracy (\%) on different MNIST settings with the ViT-A architecture. $\gA \rightarrow \gB$ denotes that models are trained on dataset $\gA$ and evaluated on dataset $\gB$.}
    \vspace{-1em}
    \begin{tabular}{c|c|r|c|c|c}
    \hline
    \rowcolor[HTML]{EDEDED} Arch. & Method & \#Params &  Static$\rightarrow$Static & Dynamic$\rightarrow$Dynamic & Static$\rightarrow$Dynamic \\ \hline 
    \multirow{3}{*}{ViT-A/12} & Self-attention~\citep{DBLP:conf/nips/VaswaniSPUJGKP17}     & 2.7 M   & 98.48 & 92.64 & 18.18 \\ 
    & $\alpha$-Translution (relative dim = 8)              & 4.6 M   & 98.48 & 97.31 & 34.90 \\
    & Translution                                              & 116.2 M    & \textbf{98.60} & \textbf{97.35} & \textbf{36.40}   \\ \hline 
    \multirow{3}{*}{ViT-A/7} & Self-attention~\citep{DBLP:conf/nips/VaswaniSPUJGKP17}     & 2.7 M   & 98.52 & 93.90 & 19.94 \\ 
    & $\alpha$-Translution (relative dim = 8)              & 8.3 M   & 98.81 & 98.57 & 40.05 \\
    & Translution                                              & 355.0 M    & \textbf{98.91} & \textbf{98.60} & \textbf{48.07}  \\ \hline
    \end{tabular}
     \vspace{-1.0em}
    \label{tab:mm}
\end{table}

As shown in Table~\ref{tab:mm}, all models achieve high accuracy when trained and evaluated on static MNIST. However, when digit locations vary, the self-attention's accuracy significantly decreases, whereas Translution (including $\alpha$-Translution) still maintains high accuracy. 
This is because absolute positional embedding makes digit locations part of its representation. Consequently, when digits shift positions, networks may become confused and fail to recognize digits accurately. 
% Specifically, when trained on static MNIST and tested on dynamic MNIST, Transformer achieves only 18.18\% accuracy with ViT-A/12, indicating near-total failure. 
In contrast, Translution employs relative encoding, effectively capturing digit structures independently of their absolute locations. This significantly reduces sensitivity to location variability, demonstrating Translution's superior capability in modeling relative structures. 
However, when training on static MNIST, the uniformly black image background causes some $\mW_{\delta_x, \delta_y}$ not to be well trained. As a result, when evaluated on dynamic MNIST, Translution fails to achieve very high accuracy. 



\begin{table}[h]
\small
    \centering
    \setlength{\tabcolsep}{13.4pt} 
    % \vspace{-1.5em}
    \caption{Accuracy (\%) on the ImageNet-1K dataset with patch sizes of 56 and 32. Training is conducted from scratch without pretraining on external datasets, with a batch size of 256.}
    \vspace{-1em}
    \begin{tabular}{c|l|r|c|c} 
    \hline
\rowcolor[HTML]{EDEDED}    Architecture  & ~~~~~~~~~~~~~~~~~~~~~~~~~Method& \#Parameters  &   Top-1 & Top-5  \\  \hline
\multirow{3}{*}{ViT-A/56}  &  Self-attention~\citep{DBLP:conf/nips/VaswaniSPUJGKP17} & 4.7 M &   46.28  & 71.17  \\ 
                              &  $\alpha$-Translution (relative enc dim = 8)          & 5.3 M &   48.36  & 73.31  \\    
                              &  Translution   & 38.5 M    &    \textbf{52.41} & \textbf{76.50}  \\  \hline 
                              


\multirow{3}{*}{ViT-B/56}     &  Self-attention~\citep{DBLP:conf/nips/VaswaniSPUJGKP17} & 7.4 M          &  53.75          & 77.59  \\  
                              &  $\alpha$-Translution (relative enc dim = 8)         & 8.7 M    &      55.87 &          79.16     \\ 
                              &  Translution                                         & 75.0 M &  \textbf{59.51} & \textbf{81.97}  \\  \hline  

\multirow{3}{*}{ViT-C/56} &  Self-attention~\citep{DBLP:conf/nips/VaswaniSPUJGKP17}& 25.3 M     &  64.15 &  84.95  \\   
                              &  $\alpha$-Translution (relative enc dim = 8)  &  30.5 M    &  66.54        &  86.49   \\  
                              &  Translution      &  296.0 M                   & \textbf{68.05}  & \textbf{88.62}    \\  \hline  % haha


  &  Self-attention~\citep{DBLP:conf/nips/VaswaniSPUJGKP17} & 3.5 M &   57.63 & 80.96  \\  
  ViT-A/32  &  $\alpha$-Translution (relative enc dim = 8)  & 5.3 M    &  60.26 & 83.07 \\  
  &  Translution          & 116.9 M                  &  \textbf{66.03} & \textbf{86.01}  \\   \hline 

  &  Self-attention~\citep{DBLP:conf/nips/VaswaniSPUJGKP17}&  6.1 M   &      66.13 &  86.87  \\   
  ViT-B/32  &  $\alpha$-Translution (relative enc dim = 8)  & 9.9 M    &  67.63   &  87.96   \\ 
  &  Translution     & 223.1 M      &  \textbf{70.63}  & \textbf{90.10}  \\  \hline % haha

   \rowcolor[HTML]{EDEDED} \multicolumn{5}{c}{Translution runs out of memory under the following architectures.} \\ \hline 
                              
   \multirow{2}{*}{ViT-C/32}    &  Self-attention~\citep{DBLP:conf/nips/VaswaniSPUJGKP17} &   22.9 M    &         73.62 &  91.12  \\   
  &  $\alpha$-Translution (relative enc dim = 8)  &  38.0 M   &  \textbf{74.19} &   \textbf{91.52}  \\  \hline 
 
       % &  Translution     &    930.6 M      &  OOM  &  OOM \\  \hline 

\multirow{2}{*}{ViT-A/16}  &  Self-attention~\citep{DBLP:conf/nips/VaswaniSPUJGKP17} & 3.0 M &   64.71	  & 86.25  \\ 
                              &  $\alpha$-Translution (relative enc dim = 8)          & 10.7 M &   \textbf{69.28}  & \textbf{89.24}  \\ \hline    
                              % &  Translution   & 38.5 M    &    \textbf{52.41} & \textbf{76.50}  \\  \hline 

\multirow{2}{*}{ViT-B/16}  &  Self-attention~\citep{DBLP:conf/nips/VaswaniSPUJGKP17} & 5.7 M &   73.51	  & 91.89  \\ 
                              &  $\alpha$-Translution (relative enc dim = 8)          & 21.1 M &   \textbf{76.20}  & \textbf{93.04}  \\ \hline   

\multirow{2}{*}{ViT-C/16}  &  Self-attention~\citep{DBLP:conf/nips/VaswaniSPUJGKP17} & 22.0 M &   78.91		  & 94.10  \\ 
                              &  $\alpha$-Translution (relative enc dim = 8)          & 85.4 M &   \textbf{79.70}  & \textbf{94.52}  \\ \hline   	
       
    \end{tabular}
    \label{tab:imagenet-1}
    % \vspace{-0.5em}
\end{table}



\subsubsection{ImageNet}

ImageNet-1K~\cite{DBLP:conf/cvpr/DengDSLL009} is a widely used dataset for computer vision research, particularly in the area of image classification. 
It  contains 1,000 object categories (classes), each with approximately 1,300 training images and 50 validation images, amounting to about 1.28 million training images and 50,000 validation images in total. Images are resized to $224\times224$. 
As shown in Table~\ref{tab:imagenet-1}, compared to self-attention~\citep{DBLP:conf/nips/VaswaniSPUJGKP17}, Translution and $\alpha$-Translution effectively improve ImageNet classification.  



We compare Translution with existing positional encoding strategies, which typically represent positional information by introducing additional positional biases,  as scalars~\cite{DBLP:conf/iccv/LiuL00W0LG21,DBLP:conf/icml/dAscoliTLMBS21} or vectors~\citep{DBLP:conf/nips/VaswaniSPUJGKP17,DBLP:conf/naacl/ShawUV18}.
The formal differences between these approaches are detailed in Section~\ref{sec:position}. 
As shown in Table~\ref{tab:relative-main}, compared to existing relative encoding methods, Translution achieves a notable improvement in accuracy. 

\begin{table}[h]
    \centering
    \small
    \setlength{\tabcolsep}{16.5pt} 
    % \vspace{-0.5em}
    \caption{Comparison of different positional encoding strategies. Results are reported on ImageNet-1K with ViT-A/56, trained from scratch (no external pretraining) using a batch size of 256.}
    \vspace{-0.5em}
    \begin{tabular}{l|r|c|c}
    \hline
     \rowcolor[HTML]{EDEDED}   Method & \#Parameters & Top-1 & Top-5  \\ \hline
     Self-attention w/o Pos Emb    &  4.69 M & 42.49  & 67.39 \\  \hline
     Self-attention w/ Pos Emb~\citep{DBLP:conf/nips/VaswaniSPUJGKP17}     &   4.69 M & 46.28  & 71.17 \\ 
     Relative key vector~\citep{DBLP:conf/naacl/ShawUV18}   & 4.74 M & 46.39 & 71.25 \\ 
     Relative value vector~\citep{DBLP:conf/naacl/ShawUV18}  & 4.74 M & 46.35 & 71.04 \\ 
     Swin Transformer~\citep{DBLP:conf/iccv/LiuL00W0LG21} & 4.69 M & 46.36 & 71.31 \\ 
     ConViT~\citep{DBLP:conf/icml/dAscoliTLMBS21}  & 4.69 M & 46.39 & 71.44 \\  
     RoFormer~\citep{DBLP:journals/ijon/SuALPBL24} & 4.69 M & 46.65 & 71.51  \\  \hline     
     $\alpha$-Translution & 5.33 M & 48.36  & 73.31 \\ 
     Translution & 38.53 M & 52.41  & 76.50 \\  \hline
    \end{tabular}
    \label{tab:relative-main}
    \vspace{-1em}
\end{table}



\subsubsection{Ablation Study}

\noindent
\textit{1) Is the improvement of Translution (including $\alpha$-Translution) caused by the introduction of additional parameters or the proposed modeling approach based on relative encoding?}

Compared to self-attention, which employs three parameter matrices $\mW^q$, $\mW^k$, $\mW^v$ to compute query, key and value, Translution uses three groups of parameter matrices $\{\mW^q_{\delta_x, \delta_y}\}$, $\{\mW^k_{\delta_x, \delta_y}\}$, $\{\mW^v_{\delta_x, \delta_y}\}$ for relative encoding, thus introducing  more parameters. 

To investigate whether the improvement arises from the increased parameter count or from the relative encoding method itself, we conducted the following experiment: 
\vspace{-0.3em}
\begin{equation*}
\mathrm{relative~~encoding}\!:  \mW^q_{\delta_x, \delta_y}, ~~\mW^k_{\delta_x, \delta_y}, ~~ \mW^v_{\delta_x, \delta_y} ~~\Rightarrow~~ \mathrm{absolute~~encoding}\!: \mW^q_{i, j}, ~~\mW^k_{i, j}, ~~ \mW^v_{i, j},   
\vspace{-0.25em}
\end{equation*}
 where $\delta_x \in \{-(H-1), \cdots, 0, \cdots, H-1\}$, $\delta_y \in \{-(W-1), \cdots, 0, \cdots, W-1\}$, and indices $ i \in [1, H \times W]$ and $ y \in [1, H \times W]$.   Specifically, for each pair of patches $(i, j)$, a distinct parameter matrix is employed to calculate query, key or value, rather than using the shared offset-based matrices. 
Under this modification, Translution transitions to absolute modeling. 
Moreover, this adjustment significantly increases the number of parameter matrices from $(2H-1)\times(2W-1)$ to $(H\times W)^2$. 
\vspace{-0.25em}
\begin{table}[h!]
\small
    \centering
    \setlength{\tabcolsep}{6.5pt} 
    \caption{Investigation of whether the improvement of Translution arises from the additional parameters or the proposed relative encoding method ($\mW^q_{\delta_x, \delta_y}$, $\mW^k_{\delta_x, \delta_y}$, $\mW^v_{\delta_x, \delta_y}$). Because the absolute encoding method ($\mW^q_{i, j}$, $\mW^k_{i, j}$, $\mW^v_{i, j}$) consumes a large number of parameters, Translation with ViT-A/7 encounters the out-of-memory issue. Therefore, experiments are conducted using ViT-A/12.}
    \vspace{-1em}
    \begin{tabular}{c|c|r|c|c|c}
    \hline
    \rowcolor[HTML]{EDEDED} Method         & Encoding   & \#Parameters &  Static$\rightarrow$Static & Dynamic$\rightarrow$Dynamic & Static$\rightarrow$Dynamic \\ \hline 
    \multirow{2}{*}{$\alpha$-Translution}  & relative       & 4.6 M   & \textbf{98.48} & \textbf{97.31} & \textbf{34.90}  \\
                                           & absolute           & 28.7 M   & 98.42 & 96.18 & 25.37 \\ \hline
    \multirow{2}{*}{Translution}           & relative            &  116.2 M    & \textbf{98.60} & \textbf{97.35} & \textbf{36.24}   \\
                  & absolute    & 1660.9 M    & 98.55 & 53.79 & 11.23  \\
    \hline
    \end{tabular}
    \label{tab:mm2}
    \vspace{-1em}
\end{table}

As shown in Table~\ref{tab:mm2}, although  absolute encoding involves significantly more parameters, it achieves lower accuracy than  relative encoding. Therefore, simply increasing the number of parameters does not lead to performance improvements.

% Also, as shown in Table~\ref{tab:imagenet-1}, the Transformer with ViT-C/56 consumes 25.3~M parameters and achieves 64.15\% Top-1 and 84.95\% Top-5 accuracy, whereas $\alpha$-Translution with ViT-A/56 requires only 9.9~M parameters yet achieves 67.63\% Top-1 and 87.96\% Top-5 accuracy. 
% This further confirms that the improvement of Translution is attributed to the relative encoding approach, rather than merely increasing parameters.   


\textit{2) Impact of relative encoding dimension on the performance of $\alpha$-Translution.}

To reduce parameter usage, $\alpha$-Translution employs smaller input ($C^1$) and output ($C^2$) dimensions for $\{\mW^q_{\delta_x, \delta_y}\}$,  $\{\mW^k_{\delta_x, \delta_y}\}$ and  $\{\mW^v_{\delta_x, \delta_y}\}$. In our experiments, we set the relative encoding dimensions as $C^1 = C^2 = 8$. This section investigates the impact of varying $C^1$ and $C^2$ on performance. 
As shown in Table~\ref{tab:imagenet-r}, increasing the relative encoding dimension improves accuracy but results in more parameters. 
Therefore, the relative encoding dimension presents a trade-off between efficiency and effectiveness for $\alpha$-Translution. (When $C^1 = C^2 = 0$, it reduces to self-attention without positional embedding.)
\begin{table}[t]
\small
    \centering
    \setlength{\tabcolsep}{7.2pt} 
    % \vspace{-0.5em}
    \caption{Impact of relative encoding dimension on the performance of $\alpha$-Translution with ViT-A/56. }
    \vspace{-1em}
    \begin{tabular}{l|r|c|c||l|r|c|c} 
    \hline
\rowcolor[HTML]{EDEDED}    Relative Enc Dim  &  \#Params  &   Top-1 & Top-5 & Relative Enc Dim  &  \#Params  &   Top-1 & Top-5  \\  \hline
$C^1 = C^2 = 0$  & 4.7 M & 42.49 & 67.39 & $C^1 = C^2 = 8$  & 5.3 M & 48.36 & 73.31 \\ 
$C^1 = C^2 = 2$  & 4.8 M & 46.10 & 71.29 & $C^1 = C^2 = 16$  & 7.0 M & 48.91 & 73.65\\  
$C^1 = C^2 = 4$  & 4.9 M & 47.61 & 72.18 & $C^1 = C^2 = 32$  & 13.8 M & 50.07 & 74.84 \\ \hline 
    \end{tabular}
    \vspace{-2.0em}
    \label{tab:imagenet-r}
\end{table}




\subsection{Natural Language Modeling with GPT}

To compare Translution and Transformer for natural language processing, we conduct experiments using the OpenWebText dataset~\citep{pile}, an openly available reproduction of OpenAI's proprietary WebText dataset used for GPT-2~\citep{radford2019language}. OpenWebText contains 9 billion training tokens and 4 million validation tokens, with a vocabulary size of 50K. We use perplexity, defined as the exponentiation of the cross-entropy loss, as the evaluation metric, where a lower perplexity indicates stronger language modeling performance. 
Since the most powerful GPU  available to us has 80GB memeory, Translution can handle at most a text sequence of length 160 with the GPT-A architecture. 
Therefore, we conduct the Translution experiment with sequences of length 160. 
% To align with GPT-1, which uses a sequence length of 512, we evaluate $\alpha$-Translution on sequences of length 512. 
As shown in Table~\ref{tab:gpt}, Translution achieves lower perplexity compared to Transformer, demonstrating its effectiveness in natural language modeling.

\begin{table}[h!]
\small
    \centering
    \vspace{-0.5em}
    \setlength{\tabcolsep}{16.4pt} 
    \caption{Perplexity on OpenWebText using a batch size of 8 and a sequence length of 160.}
    \vspace{-1em}
    \begin{tabular}{c|l|r|c}
    \hline
\rowcolor[HTML]{EDEDED} Architecture & ~~~~~~~~~~~~~~~~~~~~~~~~~Method& \#Parameters    &  Perplexity $\downarrow$  \\  \hline
\multirow{3}{*}{GPT-A-160}    &  Self-attention~\citep{DBLP:conf/nips/VaswaniSPUJGKP17}&22.0 M &  60.40   \\ %4.101 
                             &  $\alpha$-Translution (relative enc dim = 8)  & 23.7 M     &  57.97  \\    % 4.060
                             &  Translution &  127.5 M   &    \textbf{56.26}   \\  \hline % \textbf{4.030}
\rowcolor[HTML]{EDEDED} \multicolumn{4}{c}{Translution runs out of memory under the following architectures.} \\ \hline 
\multirow{2}{*}{GPT-B-160} &  Self-attention~\citep{DBLP:conf/nips/VaswaniSPUJGKP17}& 24.7 M               &  54.82  \\ % 4.004 
 &  $\alpha$-Translution (relative enc dim = 8)  & 28.2 M       &  \textbf{52.72}   \\  \hline  % \textbf{3.965}
\multirow{2}{*}{GPT-C-160}   &  Self-attention~\citep{DBLP:conf/nips/VaswaniSPUJGKP17}& 60.0 M                 &  39.88   \\  % 3.686
                             &  $\alpha$-Translution (relative enc dim = 8)  & 74.0 M                 &  \textbf{39.25}   \\  \hline   % \textbf{3.670}

% \multirow{2}{*}{GPT-A-512} &  Self-attention& 22.2 M         &  3.808   \\ 
%  &  $\alpha$-Translution (relative enc dim = 8)  & - M       &  \textbf{-}   \\  \hline 

% \multirow{2}{*}{GPT-B-512} &  Self-attention& 22.2 M         &  3.808   \\ 
%  &  $\alpha$-Translution (relative enc dim = 8)  & - M       &  \textbf{-}   \\  \hline 

% \rowcolor[HTML]{EDEDED} \multicolumn{4}{c}{$\alpha$-Translution runs out of memory with the GPT-C-512 architecture.} \\ \hline 
 
    \end{tabular}
    \label{tab:gpt}
\end{table}


\section{Conclusion}

In this paper, we introduce Translution, a new operation  that unifies self-attention and convolution for adaptive and relative modeling.  Experiments on computer vision and natural language processing tasks demonstrate the effectiveness of Translution. 

However, due to current limited computational resources, the validation  in this paper is preliminary. 
We encourage the community to further evaluate Translution using larger-scale frameworks and datasets in diverse scenarios to verify its broader applicability, particularly when single GPUs equipped with over $2\sim3$ TB of memory are available.   

Given Translution's substantial parameter consumption, it is worthwhile to explore optimized variants, such as $\alpha$-Translution. 
For instance, certain relative positions may share the same parameter, especially when the distance between elements is too long. 
At the same time, extending Translution to 3D, video, molecule, and other modalities of processing holds significant promise. 

As a fundamental operation, Translution can be employed beyond the ViT and GPT architectures. More effective and efficient architectures for Translution merit further exploration in future.  


 



\bibliography{iclr2026_conference}
\bibliographystyle{iclr2026_conference}

\newpage

\appendix

\section{Default Notation}

\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{0.5in}p{2in}p{0.5in}p{1.5in}}
$\displaystyle a, A$ & A scalar & $\displaystyle \va$ & A vector\\
$\displaystyle \mA$ & A matrix & $\displaystyle \tA$ & A tensor \\
$\displaystyle \times$ & Scalar multiplication & $\displaystyle ~\cdot$ & Matrix multiplication \\
\end{tabular}
\egroup
\vspace{0.25cm}

\section{General Translution} 

The calculation of the query, key and value in Translution, \ie, Eq.~(\ref{eq:lution}), assumes that element positions  (\eg, image patches or textual words) are discrete. 
In this setting, it is feasible to assign a different set of parameters for each direction and distance. 
However, if the positions are continuous variables, \eg, in point clouds, it becomes impractical to assign individual weights for each direction and distance, as there are infinitely many possible variations in continuous space. 
In this case, it may be necessary to design new functions for the relative encoding. 


Suppose $\vp_i$ denotes the position of the $i$-th element. 
For language, $\vp_i$ can represent the index of the $i$-th word in the text. 
For images, $\vp_i$ corresponds to the row and column indices of the $i$-th patch. 
For point clouds, $\vp_i$ refers to the 3D coordinates of the $i$-th point. 
A more general version of Translution can be formulated as follows, 
\begin{equation*}
\mathrm{General~~Translution\!:} ~~~ \vf'_i = \sum_{j=1}^N \alpha(\vp_i - \vp_j, \vf_i, \vf_j, )  \times  v(\vp_i - \vp_j, \vf_j), 
\end{equation*}
where $\alpha \in [0, 1]$ denotes the attention score measuring the relevance of the $j$-th element to the $i$-th element, and $v: \R^{d + C} \rightarrow \R^{C'}$ is a function that encodes relative positional information into the element features ($d$ denotes the dimensionality of the position, $C$ is the number of input feature channels, and $C'$ is the number of output feature channels). 
When applying Translution to a new type of data, the key is to develop  effective  $\alpha$ and  $v$ functions. 

\section{1D Translution for Natural Language Processing}
\label{sec:1translution}

In the main text, we demonstrate how to apply Translution for image modeling. That Translution can be viewed as a 2D operation  because the relative encoding involves two spatial directions. 
However, in natural language, relative encoding operates along a single dimension, which makes Translution a one-dimensional model when applied to text.


\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\linewidth]{imgs/1translution.pdf} 
    \caption{When modeling text, Translution operates in a 1D setting. For a sequence of length $N$, it employs separate parameters for each positional offset (considering both direction and distance), \ie, $\{\mW^q_{-(N-1)}, \cdots, \mW^q_{0}, \cdots, \mW^q_{N-1}\}$, $\{\mW^k_{-(N-1)}, \cdots, \mW^k_{0}, \cdots, \mW^k_{N-1}\}$ and $\{\mW^v_{-(N-1)}, \cdots, \mW^v_{0}, \cdots, \mW^v_{N-1}\}$, to encode relative language structure.} 
    \label{fig:1translution}
\end{figure}

Suppose $\vf_i \in \R^{1 \times C}$ denotes the embedding (or representation) of the $i$-th token within a text sequence of length $N$, where $C$ represents the embedding dimension. 
As shown in Figure~\ref{fig:1translution}, 1D Translution is designed to integrate adaptive identification of related tokens with relative structural encoding for language modeling. Specifically, Translution retains the self-attention mechanism of the Transformer but employs distinct parameters for computing the Query, Key and Value representations, as follows, 
\begin{equation*}
\label{eq:1dlution}
\mathrm{1D~Translution\!} ~ \left\{
\begin{aligned}
    & \mathrm{relative~~query~~encoding}\!: ~  \vq_{i,j} = \vf_i \cdot \mW^q_{\delta}, ~ \delta = i - j, \\
    & \mathrm{relative~~key~~encoding}\!: ~ \vk_{j,i} = \vf_j \cdot \mW^k_{-\delta}, \\
    & \mathrm{relative~~attention}\!:  ~ a_{i,j} = \frac{\vq_{i,j} \cdot \vk_{j,i}^T}{\sqrt{C'}},   ~ \alpha_{i,j} = \frac{e^{a_{i,j}}}{\sum_{n=1}^Ne^{a_{i,n}}},   \\ 
    &\mathrm{relative~~value~~encoding}\!: ~  \vv_{i,j} = \vf_j \cdot \mW^v_{\delta},    \\
    & \mathrm{weighted~~sum}\!:  ~ \vf'_i = \sum_{j=1}^N \alpha_{i,j} \times  \vv_{i,j}, 
\end{aligned}
\right. 
\end{equation*}
where $\mW^q_{\delta}/\mW^k_{\delta}/\mW^v_{\delta}  \in  \R^{C \times C'}$ denotes the learnable parameters for displacement $\delta$. 

\newpage

\textbf{Causal 1D Translution}

For autoregressive tasks, such as language modeling in GPT, a causal variant is typically required to ensure future tokens remain unseen during inference. In causal 1D Translution, each token attends only to itself and preceding tokens, guaranteeing that predictions rely exclusively on past context, as follows, 
\begin{equation*}
\label{eq:causal1dlution}
\mathrm{Causal~1D~Translution\!} ~ \left\{
\begin{aligned}
    & \mathrm{relative~~query~~encoding}\!: ~  \vq_{i,j} = \vf_i \cdot \mW^q_{\delta}, ~ \delta = i - j, \\
    & \mathrm{relative~~key~~encoding}\!: ~ \vk_{j,i} = \vf_j \cdot \mW^k_{-\delta}, \\
    & \mathrm{relative~~attention}\!:  ~ a_{i,j} = \frac{\vq_{i,j} \cdot \vk_{j,i}^T}{\sqrt{C'}}, \\ 
    & \mathrm{causal~~attention}\!:  ~ a'_{i, j} = \left\{ \begin{array}{ll}
     a_{i, j},    & i \geq j,   \\
     -\infty,    &  \mathrm{otherwise}, 
    \end{array}  \right. ~ \alpha_{i,j} = \frac{e^{a'_{i,j}}}{\sum_{n=1}^Ne^{a'_{i,n}}}, \\ 
    &\mathrm{relative~~value~~encoding}\!: ~  \vv_{i,j} = \left\{ \begin{array}{ll}
     \vf_j \cdot \mW_{\delta},  &  \delta = i - j \geq 0,  \\
     \forall,    &  \mathrm{otherwise},  
    \end{array}  \right. \\        
    & \mathrm{weighted~~sum}\!:  ~ \vf'_i = \sum_{j=1}^N \alpha_{i,j} \times  \vv_{i,j}.  
\end{aligned}
\right. 
\end{equation*}

\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\linewidth]{imgs/causal.pdf} 
    \caption{Illustration of causal 1D Translution. For a sequence of length $N$, it employs $N$ parameter matrices  to encode relative language structure. Compared to the original 1D Translution, the causal variant reduces the number of parameters required to compute Query, key and Value by half.} 
    \label{fig:causal}
\end{figure}

As shown in Figure~\ref{fig:causal}, compared to the original variant, causal 1D Translution reduces by half the number of parameters needed to compute the query, key and value representations. 

\newpage

\section{Memory-Efficient Implementation of $\alpha$-Translution: Optimizing Runtime Memory Usage}
\label{sec:position}


Recall that $\alpha$-Transformer is defined as follows, 
\begin{equation*}
\alpha\mathrm{\!-\!Translution\!:}~~~\vf'_i  = \sum_{j=1}^N \alpha_{i, j}  \times \vf_j \cdot (\mW^v + \mW^{v1} \cdot \mW^v_{\delta_x, \delta_y} \cdot \mW^{v2}),
\end{equation*} 
where $\mW^v \in \R^{C \times C'}$, $\mW^{v1} \in \R^{C \times C^1}$, $\mW^v_{\delta_x, \delta_y}  \in \R^{C^1 \times C^2} $, $\mW^{v2} \in \R^{C^2 \times C'}$, and $C^1 \ll C$, $C^2 \ll C'$.  
 Although this variant significantly reduces the number of parameters, it still demands considerable runtime memory. 
Specifically, as shown in Figure~\ref{fig:3},  the resulting value tensor of Translution is $\tV \in \R^{N\times N\times C'}$, which is considerably larger than the Transformer's value matrix $\mV \in \R^{N\times C'}$. 
To address this issue, we implement $\alpha$-Translution as follows, 
\begin{equation*}
\vf'_i  = \sum_{j=1}^N \alpha_{i, j}  \times \vf_j \cdot \mW^v + \Big (\sum_{j=1}^N \alpha_{i, j}  \times \vf_j \cdot (\mW^{v1} \cdot \mW^v_{\delta_x, \delta_y} ) \Big) \cdot \mW^{v2}. 
\end{equation*} 
This reformulation reduces the peak runtime memory usage from $N\times N\times C'$ to $N\times C' + N \times N \times C^2$, where $C^2 \ll C'$, thus significantly alleviating memory demands during computation.

\newpage

\section{Comparison with Existing Position Modeling Methods}

Existing methods typically encode positional information by introducing additional positional biases (either scalars or vectors). 
In this paper, inspired by convolution, we propose an alternative approach that employs offset-based matrices for relative encoding. 
In this section, we provide a detailed comparison between these approaches. Suppose $\vx_i \in \R^{1 \times C}$  represents the feature or representation of the $i$-th patch, located at $(x_i, y_i)$ in an image composed of $N = H \times W$ patches.  
 

\textit{1. Baseline (Self-attention w/o Positional Embedding)} 

We consider the self-attention without position embedding as the baseline, formulated as follows: 
\begin{equation*} 
    \begin{aligned}
        & \mathrm{w/o~~input~~position~~embedding}\!: ~~ \vf_i = \vx_i, \\
        & \mathrm{query~~encoding}\!: ~~  \vq_i = \vf_i \cdot \mW_q, \\
        & \mathrm{key~~encoding}\!: ~~ \vk_j = \vf_j \cdot \mW_k, \\
        & \mathrm{attention}\!: ~~ a_{i,j} = \frac{\vq_i \cdot \vk_j^T}{\sqrt{C'}},   ~~~ \alpha_{i,j} = \frac{e^{a_{i,j}}}{\sum_{n=1}^Ne^{a_{i,n}}},  \\ 
        &\mathrm{value~~encoding}\!: ~~ \vv_j = \vf_j \cdot \mW_v,    \\
        & \mathrm{weighted~~sum}\!:  ~~  \vf'_i = \sum_{j=1}^N \alpha_{i,j} \times  \vv_j. 
    \end{aligned}
\end{equation*}
\textit{2. Transformer (Self-attention with Positional Embedding)} 

Most Transformers,  including the original Transformer~\citep{DBLP:conf/nips/VaswaniSPUJGKP17},  employ position embedding to incorporate positional information. Specifically, they integrate absolute positions into element representations, formulated as follows: 
\begin{equation*}
    \begin{aligned}
        & \mathrm{w/~~input~~position~~embedding}\!: \vf_i  = \vx_i \textcolor{my_red}{ ~ + ~\mathrm{Embed}(x_i, y_i)}, \\
        & \mathrm{query~~encoding}\!: ~~  \vq_i = \vf_i \cdot \mW_q, \\
        & \mathrm{key~~encoding}\!: ~~ \vk_j = \vf_j \cdot \mW_k, \\
        & \mathrm{attention}\!: ~~ a_{i,j} = \frac{\vq_i \cdot \vk_j^T}{\sqrt{C'}},   ~~ \alpha_{i,j} = \frac{e^{a_{i,j}}}{\sum_{n=1}^Ne^{a_{i,n}}},  \\ 
        &\mathrm{value~~encoding}\!: ~~ \vv_j = \vf_j \cdot \mW_v,    \\
        & \mathrm{weighted~~sum}\!:  ~~  \vf'_i = \sum_{j=1}^N \alpha_{i,j} \times  \vv_j. 
    \end{aligned}
\end{equation*}
\textit{3. Relative Key Vector}

\cite{DBLP:conf/naacl/ShawUV18} enhanced Transformer for language modeling by adding learnable relative positional vectors into the key computations. 
BoTNet~\citep{DBLP:conf/cvpr/SrinivasLPSAV21} and HaloNet~\citep{DBLP:conf/cvpr/VaswaniRSPHS21} extended this approach to two dimensions for image processing by adding  learnable relative positional vectors into the key computation. 
This can be formulated as follows, 
\begin{equation*}
    \begin{aligned}
        & \mathrm{w/o~~input~~position~~embedding}\!: ~~ \vf_i = \vx_i, \\
        & \mathrm{query~~encoding}\!: ~~  \vq_i = \vf_i \cdot \mW_q, \\
        & \mathrm{key~~encoding}\!: ~~ \vk_j = \vf_j \cdot \mW_k \textcolor{my_red}{ ~ + ~ \vr_{\delta_x, \delta_y}}, \\
        & \mathrm{attention}\!: ~~ a_{i,j} = \frac{\vq_i \cdot \vk_j^T }{\sqrt{C'}},   ~~ \alpha_{i,j} = \frac{e^{a_{i,j}}}{\sum_{n=1}^Ne^{a_{i,n}}},  \\ 
        &\mathrm{value~~encoding}\!: ~~ \vv_j = \vf_j \cdot \mW_v,    \\
        & \mathrm{weighted~~sum}\!:  ~~  \vf'_i = \sum_{j=1}^N \alpha_{i,j} \times  \vv_j,  
    \end{aligned}
\end{equation*} 
where $\vr_{\delta_x, \delta_y} \in \R^{1 \times C'}$. 

\vspace{1em}
\textit{4. Relative Value Vector}

\cite{DBLP:conf/naacl/ShawUV18} also extended the above relative vector method to the value computations, as follows:
\begin{equation*}
    \begin{aligned}
        & \mathrm{w/o~~input~~position~~embedding}\!: ~~ \vf_i = \vx_i, \\
        & \mathrm{query~~encoding}\!: ~~  \vq_i = \vf_i \cdot \mW_q, \\
        & \mathrm{key~~encoding}\!: ~~ \vk_j = \vf_j \cdot \mW_k, \\
        & \mathrm{self\!-\!attention}\!: ~~ a_{i,j} = \frac{\vq_i \cdot \vk_j^T }{\sqrt{C'}},   ~~ \alpha_{i,j} = \frac{e^{a_{i,j}}}{\sum_{n=1}^Ne^{a_{i,n}}},  \\ 
        &\mathrm{value~~encoding}\!: ~~ \vv_j = \vf_j \cdot \mW_v \textcolor{my_red}{ ~ + ~ \vr_{\delta_x, \delta_y}},    \\
        & \mathrm{weighted~~sum}\!:  ~~  \vf'_i = \sum_{j=1}^N \alpha_{i,j} \times  \vv_j.   
    \end{aligned}
\end{equation*}
\textit{5. Relative Positional Scalar}

Swin Transformer~\citep{DBLP:conf/iccv/LiuL00W0LG21} and CoAtNet~\citep{DBLP:conf/nips/DaiLLT21} incorporate a learnable relative positional bias (a scalar) into the attention score. 
In these methods, the original self-attention can be regarded as content attention, which measures relationships from the token-feature perspective, while the additional relative positional bias can be regarded as position attention, which measures relationships from the token-position perspective. Formally, this can be expressed as follows: 
\begin{equation*}
    \begin{aligned}
        & \mathrm{w/o~~input~~position~~embedding}\!: ~~ \vf_i = \vx_i, \\
        & \mathrm{query~~encoding}\!: ~~  \vq_i = \vf_i \cdot \mW_q, \\
        & \mathrm{key~~encoding}\!: ~~ \vk_j = \vf_j \cdot \mW_k, \\
        & \mathrm{attention}\!: ~~ a_{i,j} = \frac{\vq_i \cdot \vk_j^T }{\sqrt{C'}} \textcolor{my_red}{~ + ~ b_{\delta_x, \delta_y}},   ~~ \alpha_{i,j} = \frac{e^{a_{i,j}}}{\sum_{n=1}^Ne^{a_{i,n}}},  \\ 
        &\mathrm{value~~encoding}\!: ~~ \vv_j = \vf_j \cdot \mW_v,    \\
        & \mathrm{weighted~~sum}\!:  ~~  \vf'_i = \sum_{j=1}^N \alpha_{i,j} \times  \vv_j,
    \end{aligned}
\end{equation*} 
where $b_{\delta_x, \delta_y} \in \R$. ConViT~\citep{DBLP:conf/icml/dAscoliTLMBS21} introduces Gated Positional Self-Attention (GPSA), a variant of self-attention that incorporates a positional inductive bias. Moreover, a learnable gating parameter in each attention head controls the balance between positional and content-based attention, as follows, 
\begin{equation*}
    \begin{aligned}
        & \mathrm{w/o~~input~~position~~embedding}\!: ~~ \vf_i = \vx_i, \\
        & \mathrm{query~~encoding}\!: ~~  \vq_i = \vf_i \cdot \mW_q, \\
        & \mathrm{key~~encoding}\!: ~~ \vk_j = \vf_j \cdot \mW_k, \\
        & \mathrm{patch~~attention}\!: ~~ a_{i,j} = \frac{\vq_i \cdot \vk_j^T }{\sqrt{C'}},   ~~ \alpha_{i,j} = \frac{e^{a_{i,j}}}{\sum_{n=1}^Ne^{a_{i,n}}},  \\ 
        & \mathrm{position~~attention}\!: \textcolor{my_red}{~~ b_{i,j} = \vw \cdot\vr_{\|\delta\|},   ~~ \beta_{i,j} = \frac{e^{b_{i,j}}}{\sum_{n=1}^Ne^{b_{i,n}}}, }  \\ 
        & \mathrm{gated~~attention}\!: \textcolor{my_red}{~~ c_{i,j} = \big(1 - \sigma (\lambda)\big) ~\times~} \alpha_{i,j} \textcolor{my_red}{ ~+~ \sigma (\lambda) \times \beta_{i,j},  ~~ \xi_{i,j} = \frac{c_{i,j}}{\sum_{n=1}^N c_{i,n}},} \\
        &\mathrm{value~~encoding}\!: ~~ \vv_j = \vf_j \cdot \mW_v,    \\
        & \mathrm{weighted~~sum}\!:  ~~  \vf'_i = \sum_{j=1}^N \xi_{i,j} \times  \vv_j,
    \end{aligned}
\end{equation*} 
where $\vw$ is a trainable vector for embedding, $\vr_{\|\delta\|}$ is the relative positional encoding, $\lambda$ is a learnable gate and $\sigma$ is the Sigmoid function. 

\newpage

\textit{6. Rotary Position Embedding}

Unlike the above vector- and scalar-based methods, RoFormer~\citep{DBLP:journals/ijon/SuALPBL24} proposes a rotation-based positional encoding  method that is applied directly to queries and keys. As a result, attention scores depend solely on relative distances, eliminating the need to explicitly store a positional vector or scalar, as follows, 
\begin{equation*}
    \begin{aligned}
        & \mathrm{w/o~~input~~position~~embedding}\!: ~~ \vf_i = \vx_i, \\
        & \mathrm{query~~encoding}\!: ~~  \vq_i = \vf_i \cdot \mW_q, \\
        & \mathrm{key~~encoding}\!: ~~ \vk_j = \vf_j \cdot \mW_k, \\
        & \mathrm{attention}\!: ~~ \textcolor{my_red}{\vq'_i, ~ \vk'_j = \mathrm{rotary} (\vq_i, ~ \vk_j)}, ~~ a_{i,j} = \frac{\vq'_i \cdot {\vk'_j}^T }{\sqrt{C'}},   ~~ \alpha_{i,j} = \frac{e^{a_{i,j}}}{\sum_{n=1}^Ne^{a_{i,n}}},  \\ 
        &\mathrm{value~~encoding}\!: ~~ \vv_j = \vf_j \cdot \mW_v,    \\
        & \mathrm{weighted~~sum}\!:  ~~  \vf'_i = \sum_{j=1}^N \alpha_{i,j} \times  \vv_j,    
    \end{aligned}
\end{equation*} 
where $\mathrm{rotary}(\cdot)$ is a rotary position embedding function. 

\textit{7. Relative Positional Matrix (Translution)}

Inspired by convolution, we propose Translution that performs matrix multiplication to produce a vector output that encodes displacement or offset information, defined as follows: 
\begin{equation*}
    \begin{aligned} 
        & \mathrm{w/o~~input~~position~~embedding}\!: ~~ \vf_i = \vx_i, \\
        & \mathrm{relative~~query~~encoding}\!:  ~~ \vq_{i,j} = \vf_i \cdot \textcolor{my_red}{\mW^q_{\delta_x, \delta_y}}, \\   
        & \mathrm{relative~~key~~encoding}\!: ~~ \vk_{j,i} = \vf_j \cdot \textcolor{my_red}{\mW^k_{-\delta_x, -\delta_y}}, \\   
        & \mathrm{relative~~attention}\!:   ~~ a_{i,j} = \frac{\vq_{i,j} \cdot \vk_{j,i}^T}{\sqrt{C'}},   ~~~ \alpha_{i,j} = \frac{e^{a_{i,j}}}{\sum_{n=1}^Ne^{a_{i,n}}},  \\ 
        &\mathrm{relative~~value~~encoding}\!:  ~~ \vv_{i,j} = \vf_j \cdot  \textcolor{my_red}{\mW^v_{\delta_x, \delta_y}},    \\
        & \mathrm{weighted~~sum}\!:  ~~  \vf'_i = \sum_{j=1}^N \alpha_{i,j} \times  \vv_{i,j}. 
    \end{aligned}
\end{equation*} 
Table~\ref{tab:relative} provides a summary of various positional encoding strategies. 

% We compare these relative modeling methods with the proposed Translution using the ViT-A/56 architecture on the ImageNet-1K dataset.
% As shown in Table~\ref{tab:relative}, the proposed Convolution-style relative encoding method effectively captures relative structure and outperforms existing approaches.




\begin{table}[h]
    \centering
    \setlength{\tabcolsep}{3.6pt} 
    \caption{Summary of different position encoding strategies.}
    \vspace{-0.5em}
    \begin{tabular}{>{\centering\arraybackslash}m{4cm}|>{\centering\arraybackslash}m{4cm}|p{5.2cm}}
    \hline
     \rowcolor[HTML]{EDEDED}   \multicolumn{3}{c}{Method}    \\ \hline
     w/o Pos Emb    &  $\vf_i = \vx_i$  & Baseline  \\  \hline 
     w/ Pos Emb    &  $\vf_i = \vx_i + \mathrm{Embed}(x_i, y_i)$ & Transformer~\citep{DBLP:conf/nips/VaswaniSPUJGKP17}  \\ \hline 
     \multirow{2}{*}{Relative Positional Vector} &  Key &  \cite{DBLP:conf/naacl/ShawUV18}, BoTNet~\citep{DBLP:conf/cvpr/SrinivasLPSAV21},  HaloNet~\citep{DBLP:conf/cvpr/VaswaniRSPHS21}, \etc    \\ \cline{2-3} 
     &  Value & \cite{DBLP:conf/naacl/ShawUV18}  \\ \hline 
     \multirow{2}{*}{Relative Positional Scalar} & w/o gating  & Swin Transformer~\citep{DBLP:conf/iccv/LiuL00W0LG21},  CoAtNet~\citep{DBLP:conf/nips/DaiLLT21}, \etc  \\  \cline{2-3}
     & w/~~ gating & ConViT~\citep{DBLP:conf/icml/dAscoliTLMBS21}  \\  \hline 
     Rotary Position Embedding & \multicolumn{2}{c}{ RoFormer~\citep{DBLP:journals/ijon/SuALPBL24}}   \\  \hline     
     \multirow{2}{*}{Relative Positional Matrix} & \multicolumn{2}{c}{ $\alpha$-Translution}  \\  \cline{2-3} 
     &   \multicolumn{2}{c}{Translution}  \\  \hline
    \end{tabular}
    \label{tab:relative}
\end{table}

\section{Translution with Input Positional Embedding}
In this section, we examine whether incorporating the input positional embedding method from Transformer can further improve Translution. To this end, we implement Translution as follows: 
\begin{equation*}
    \begin{aligned} 
        & \mathrm{w/~~input~~position~~embedding}\!: ~~ \vf_i = \vx_i \textcolor{my_red}{ ~ + ~\mathrm{Embed}(x_i, y_i)}, \\
        & \mathrm{relative~~query~~encoding}\!:  ~~ \vq_{i,j} = \vf_i \cdot \textcolor{my_red}{\mW^q_{\delta_x, \delta_y}}, \\   
        & \mathrm{relative~~key~~encoding}\!: ~~ \vk_{j,i} = \vf_j \cdot \textcolor{my_red}{\mW^k_{-\delta_x, -\delta_y}}, \\   
        & \mathrm{relative~~attention}\!:   ~~ a_{i,j} = \frac{\vq_{i,j} \cdot \vk_{j,i}^T}{\sqrt{C'}},   ~~~ \alpha_{i,j} = \frac{e^{a_{i,j}}}{\sum_{n=1}^Ne^{a_{i,n}}},  \\ 
        &\mathrm{relative~~value~~encoding}\!:  ~~ \vv_{i,j} = \vf_j \cdot  \textcolor{my_red}{\mW^v_{\delta_x, \delta_y}},    \\
        & \mathrm{weighted~~sum}\!:  ~~  \vf'_i = \sum_{j=1}^N \alpha_{i,j} \times  \vv_{i,j}. 
    \end{aligned}
\end{equation*} 


As shown in Table~\ref{tab:mm3}, incorporating the Transformer's absolute positional embedding does not yield a clear performance gain for Translution in the static-to-static setting, leads to a slight drop in the dynamic-to-dynamic setting, and results in a substantial drop in the static-to-dynamic setting.

\begin{table}[h!]
\small
    \centering
    \setlength{\tabcolsep}{4.6pt} 
    \caption{Accuracy (\%) of Translution w/o and w/ the  absolute positional embedding method from Transformer. Results are reported on Static and Dynamic MNIST with ViT-A/12.}
    \vspace{-1em}
    \begin{tabular}{c|c|r|c|c|c}
    \hline
    \rowcolor[HTML]{EDEDED} Method         & $\mathrm{Embed}(x_i, y_i)$   & \#Parameters &  Static$\rightarrow$Static & Dynamic$\rightarrow$Dynamic & Static$\rightarrow$Dynamic \\ \hline 
    \multirow{2}{*}{$\alpha$-Translution}  & \ding{55}       & 4.6 M   & 98.48 & \textbf{97.31} & \textbf{34.90}  \\
                                           & \ding{51}       & 4.6 M   & \textbf{98.72} & 96.81 & 17.20 \\ \hline
    \multirow{2}{*}{Translution}           & \ding{55}       &  116.2 M    & \textbf{98.60} & \textbf{97.35} & \textbf{36.24}   \\
                  & \ding{51}    & 116.2 M    & 98.47 & 96.31 & 16.50  \\
    \hline
    \end{tabular}
    \label{tab:mm3}
    \vspace{-1em}
\end{table}



\section{ Impact of  $\bold{\textit{W}}^q$, $\bold{\textit{W}}^k$ and $\bold{\textit{W}}^v$ on  $\alpha$-Translution}

Recall that: 
\textit{
To reduce the number of parameters, we propose $\alpha$-Translution, which decreases both the input dimension $C^1$ and the output dimension $C^2$ of each $\mW^q_{\delta_x,\delta_y}$, $\mW^k_{\delta_x,\delta_y}$, and $\mW^v_{\delta_x,\delta_y}$. 
However, setting $C^1$ and $C^2$ too small can overly compress the query, key, and value representations, thereby degrading performance. 
To address this issue, we integrate the $\mW^q$, $\mW^k$, and $\mW^v$ of Transformer into $\alpha$-Translution to better preserve essential information.
}

In this section, we  analyze the impact of $\mW^q$, $\mW^k$, and $\mW^v$ by systematically removing them from Eq.~(\ref{eq:alution}) as follows: 
\begin{equation*}
\alpha\mathrm{\!-\!Translution\!} ~ \left\{
\begin{aligned}
    ~ & \mathrm{query~~encoding}\!: ~~~  \vq_{i,j} = \vf_i \cdot \mW^q_1 \cdot \mW^q_{\delta_x, \delta_y}, ~~~ \cancel{\vq_i = \vf_i \cdot \mW^q,}  \\
    ~ & \mathrm{key~~encoding}\!: ~~~ \vk_{j,i} = \vf_j \cdot \mW^k_1 \cdot \mW^k_{-\delta_x, -\delta_y}, ~~~ \cancel{\vk_j = \vf_j \cdot \mW^k,}  \\
    ~ & \mathrm{self\!-\!attention}\!:  ~~~ a_{i,j} = \frac{\vq_{i,j} \cdot \vk_{j,i}^T + \cancel{\vq_i \cdot \vk_j^T}}{\sqrt{C'}},   ~~~ \alpha_{i,j} = \frac{e^{a_{i,j}}}{\sum_{n=1}^Ne^{a_{i,n}}},   \\ 
    ~ &\mathrm{value~~encoding}\!: ~~~  \vv_{i,j} = \vf_j \cdot (\mW^v_1 \cdot \mW^v_{\delta_x, \delta_y} \cdot \mW^v_2 + \cancel{\mW^v}),    \\
    ~ & \mathrm{weighted~~sum}\!:  ~~~ \vf'_i = \sum_{j=1}^N \alpha_{i,j} \times  \vv_{i,j}. 
\end{aligned} 
\right.
\end{equation*}

As shown in Table~\ref{tab:alution-former}, incorporating $\mW^q$, $\mW^k$, and $\mW^v$ significantly enhances the performance of $\alpha$-Translution, particularly when $C^1$ and $C^2$ are small. 
As $C^1$ and $C^2$ grow larger, the improvement decreases because the information is no longer overly compressed. In this case, $\mW^q$, $\mW^k$, and $\mW^v$ become less critical. 


\begin{table}[h]
\small
    \centering
    \setlength{\tabcolsep}{14.5pt} 
    \caption{Impact of $\mW^q$, $\mW^k$ and $\mW^v$ on $\alpha$-Transformer. Results are reported on ImageNet-1K with ViT-A/56, trained from scratch (no external pretraining) using a batch size of 256.} 
    \begin{tabular}{c|c|c|c|c}
    \hline 
    \rowcolor[HTML]{EDEDED} Relative Encoding Dimension  & $\mW^q$, $\mW^k$, $\mW^v$ & \#Parameters & Top-1 & Top-5 \\ \hline
    $C^1 = C^2 =0$                  & \ding{51}        & 4.68 M       & 42.49 & 67.39 \\ \hline
    \multirow{2}{*}{$C^1 = C^2 =2$} & \ding{55}        & 4.08 M       & 31.77 & 56.66 \\
                                    & \ding{51}        & 4.75 M       & 46.10 & 71.29 \\ \hline
    \multirow{2}{*}{$C^1 = C^2 =4$} & \ding{55}        & 4.21 M       & 37.46 & 62.72 \\
                                    & \ding{51}        & 4.89 M       & 47.61 & 72.18 \\  \hline 
    \multirow{2}{*}{$C^1 = C^2 =8$} & \ding{55}        & 4.67 M       & 41.81 & 67.23 \\
                                    & \ding{51}        & 5.33 M       & 48.36 & 73.31 \\ \hline
    \multirow{2}{*}{$C^1 = C^2 =16$}& \ding{55}        & 6.40 M       & 44.87 & 69.91 \\ 
                                    & \ding{51}        & 7.06 M       & 48.91 & 73.65 \\ \hline 
    \multirow{2}{*}{$C^1 = C^2 =32$}& \ding{55}        & 13.09 M      & 47.27 & 72.20 \\ 
                                    & \ding{51}        & 13.75 M      & 50.07 & 74.84 \\ \hline     
    \end{tabular}
    \label{tab:alution-former}
\end{table}


\section{Relative CLS Token}

For classification tasks, besides the image tokens, there is an additional $\mathrm{CLS}$ token (classification token)  that serves as a global representation of the input image.  
Usually, the $\mathrm{CLS}$ token is a learnable embedding appended at the beginning of the input token sequence fed into Transformer. 
To apply the strategy of relative encoding to the $\mathrm{CLS}$ token, we introduce additional parameters: $\mW^q_{CLS\_in}$, $\mW^q_{CLS}$, $\mW^q_{CLS\_out}$, $\mW^k_{CLS\_in}$, $\mW^k_{CLS}$, $\mW^k_{CLS\_out}$, and $\mW^v_{CLS\_in}$, $\mW^v_{CLS}$, $\mW^v_{CLS\_out}$, corresponding to the query, key, and value, respectively. 


\begin{figure}[h]
    \centering
    \vspace{-0.5em}
    \includegraphics[width=0.75\textwidth]{imgs/cls-token.pdf}
    \vspace{-0.5em}
    \caption{Illustration of relative encoding for the $\mathrm{CLS}$ token. For $\mathrm{CLS}$, there are three encoding directions: in, in-place, and out. Correspondingly, three sets of weights, \ie, $\mW_{CLS\_in}$, $\mW_{CLS}$, and $\mW_{CLS\_out}$,  are introduced for relative encoding in each respective direction.}
    \label{fig:cls}
\end{figure}

As shown in Figure~\ref{fig:cls}, $\mW_{CLS\_in}$ is utilized when gathering information from the image  tokens to update the $\mathrm{CLS}$ token; $\mW_{CLS}$ is applied when updating the $\mathrm{CLS}$ token based on its own information; and $\mW_{CLS\_out}$ is employed when image  tokens gather information from the $\mathrm{CLS}$ token to update themselves.   

% \section{Training Details}

% \subsection{Dynamic MNIST}
% The training is conducted with an input image size of 84 for 10 epochs using a batch size of 24. Optimization employs the AdamW optimizer with an initial learning rate of $3\times 10^{-4}$ and a weight decay of 0.05. The learning rate is scheduled with CosineAnnealingLR over the training epochs. The input data is preprocessed using a transformation pipeline consisting of Normalize(0.1307,0.3081).

% \subsection{ImageNet}

% The model is trained over 300 epochs, using an input image size of 224 and a batch size of 256. Optimization is carried out using the AdamW optimizer, with an initial learning rate of $3\times 10^{-4}$ and a weight decay of 0.05. The learning rate follows a CosineAnnealingLR schedule, which gradually decreases it according to a smooth cosine curve throughout training to aid in stable convergence. Data augmentation includes RandomResizedCrop (224), RandomHorizontalFlip, and RandAugment (with two random operations and magnitude 9), providing automated, robust augmentation policies that enhance generalization. 
% The input data is preprocessed using a transformation pipeline consisting of Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]). 
% Additionally, CutMix ($\alpha=1.0$) and MixUp ($\alpha=0.8$) are applied in a batch-level ``random choice'' manner for strong regularization—CutMix pastes patches between images while proportionally mixing labels; MixUp blends images and labels to improve generalization and training stability. 


% \subsection{OpenWebText}

% The training runs for 9.6 million iterations with a mini-batch size of 8. Optimization is performed using the AdamW optimizer with an initial learning rate of $3\times 10^{-4}$ and a weight decay of 0.05. The learning rate is scheduled by CosineAnnealingLR over every 3.2k iterations. Gradient clipping is applied with a maximum norm of 1.0. 





% \begin{table}[h!]
% \small
%     \centering
%     \setlength{\tabcolsep}{20pt} 
%     \caption{Accuracy (\%) of Translution w/o and w/ the  absolute positional embedding method from Transformer. Results are reported on ImageNet-1K with ViT-A/56, trained from scratch (no external pretraining) using a batch size of 256.}
%     \vspace{-1em}
%     \begin{tabular}{c|c|r|c|c}
%     \hline
%     \rowcolor[HTML]{EDEDED} Method         & $\mathrm{Embed}(x_i, y_i)$  & \#Parameters &  Top-1 &  Top-2 \\ \hline 
%     \multirow{2}{*}{$\alpha$-Translution}  & \ding{55}       & 5.33 M   & \textbf{48.36} & \textbf{73.31}   \\
%                                            & \ding{51}           & 5.35 M   & 48.14 & 72.96  \\ \hline
%     \multirow{2}{*}{Translution}           & \ding{55}            &  38.53 M    & \textbf{52.41} & \textbf{76.50}  \\
%                   & \ding{51}    & 38.55 M    & 52.57 & 76.59   \\
%     \hline
%     \end{tabular}
%     \label{tab:abs-lution}
%     \vspace{-1em}
% \end{table}

% \begin{table}[h]
%     \centering
%     \setlength{\tabcolsep}{5.0pt} 
%     \begin{tabular}{c|c|c|c|c|c|r|c|c}
%     \hline
%     \multicolumn{2}{c|}{Query} & \multicolumn{2}{c|}{Key} & \multicolumn{2}{c|}{Value} & \multirow{2}{*}{\#Params} & \multirow{2}{*}{Top-1} & \multirow{2}{*}{Top-2} \\ \cline{1-6}
%     Abs. Emb. & $\mW^q_{\delta_x, \delta_y}$ &  Abs. Emb. & $\mW^k_{\delta_x, \delta_y}$ & Abs. Emb. & $\mW^v_{\delta_x, \delta_y}$  &    &  & \\ \hline 
%     Baseline &  &  &  &  &  & 4.68 M & 42.49 & 67.39 \\ \hline  \hline 
%      \ding{51} & &   & &   & &  M &  &  \\ \hline  
%       & &  \ding{51} & &   & &  M &  &  \\ \hline 
%        & &   & &  \ding{51}  & &  M &  &  \\ \hline 
%     \ding{51} & &  \ding{51}  & &    & &  M &  &  \\ \hline 
%     \ding{51} & &    & &   \ding{51} & &  M &  &  \\ \hline 
%      & &  \ding{51}  & &   \ding{51} & &  M &  &  \\ \hline 
%     \ding{51} & Transformer & \ding{51}  & & \ding{51}  & & 4.69 M & 46.28 & 71.17 \\ \hline \hline 
%     & \ding{51} & &   & &  &  M &  &  \\ \hline 
%     &  & & \ding{51}  & &  &  M &  &  \\ \hline 
%     $\gamma$-Translution &  & &   & & \ding{51} &  16.0 M & 50.96 & 75.16 \\ \hline 
%      & \ding{51} & & \ding{51}  & &  &  M &  &  \\ \hline 
%     & \ding{51} & &   & & \ding{51} &  M &  &  \\ \hline 
%     $\beta$-Translution&  & & \ding{51}  & & \ding{51} &  M &  &  \\ \hline 
%     Translution & \ding{51} & & \ding{51}  & & \ding{51} & 38.53 M & 52.41 & 76.50 \\ \hline \hline
%      \ding{51} & \ding{51} & \ding{51} & \ding{51}  & \ding{51} & \ding{51} & M &  &  \\ \hline 
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:placeholder}
% \end{table}


% \begin{equation}
% \label{eq:blution}
% \mathrm{\beta\!-\!Translution\!} ~ \left\{
% \begin{aligned}
%     & \mathrm{query~~encoding}\!: ~  \vq_i = \vf_i \cdot \mW^q, \\
%     & \mathrm{relative~~key~~encoding}\!: ~ \vk_{i,j} = \vf_j \cdot \mW^k_{\delta_x, \delta_y}, \\
%     & \mathrm{relative~~self\!-\!attention}\!:  ~ a_{i,j} = \frac{\vq_i \cdot \vk_{i,j}^T}{\sqrt{C'}},   ~ \alpha_{i,j} = \frac{e^{a_{i,j}}}{\sum_{n=1}^Ne^{a_{i,n}}},   \\ 
%     &\mathrm{relative~~value~~encoding}\!: ~  \vv_{i,j} = \vf_j \cdot \mW^v_{\delta_x, \delta_y},    \\
%     & \mathrm{weighted~~sum}\!:  ~ \vf'_i = \sum_{j=1}^N \alpha_{i,j} \times  \vv_{i,j}. 
% \end{aligned}
% \right. 
% \end{equation}


% \begin{equation}
% \label{eq:rlution}
% \mathrm{\gamma\!-\!Translution\!} ~ \left\{
% \begin{aligned}
%     & \mathrm{query~~encoding}\!: ~  \vq_i = \vf_i \cdot \mW^q, \\
%     & \mathrm{key~~encoding}\!: ~ \vk_j = \vf_j \cdot \mW^k,  \\
%     & \mathrm{self\!-\!attention}\!:  ~ a_{i,j} = \frac{\vq_i \cdot \vk_j^T}{\sqrt{C'}},   ~ \alpha_{i,j} = \frac{e^{a_{i,j}}}{\sum_{n=1}^Ne^{a_{i,n}}},   \\ 
%     &\mathrm{relative~~value~~encoding}\!: ~  \vv_{i,j} = \vf_j \cdot \mW^v_{\delta_x, \delta_y},  \\
%     & \mathrm{weighted~~sum}\!:  ~ \vf'_i = \sum_{j=1}^N \alpha_{i,j} \times  \vv_{i,j}. 
% \end{aligned}
% \right. 
% \end{equation}

 

\end{document}
