
\documentclass{article} % For LaTeX2e
\usepackage{iclr2026_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{iclr2026/math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{multirow} 
\usepackage[table]{xcolor}
\usepackage{pifont}
\usepackage{enumitem}

\bibpunct{(}{)}{,}{n}{,}{,}  
\let\cite\citep              

\usepackage{tikz}
\newcommand{\blackcircle}[1]{%
    \begin{tikzpicture}[baseline=-0.9ex]
        \node[circle,fill=black,text=white,inner sep=0.2pt,minimum size=1pt] {#1};
    \end{tikzpicture}%
}

\newcommand{\zt}[1]{{\color{cyan}{[#1 -zt]}}}

\title{Protein as a Second Language for LLMs}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Xinhui Chen\textsuperscript{\rm 1}\quad 
%  
Zuchao Li\textsuperscript{\rm 1\thanks{Zuchao Li (zcli-charlie@whu.edu) is the corresponding author with School of Artificial Intelligence, Wuhan University.}}\quad 
Mengqi Gao\textsuperscript{\rm 1}\quad
Yufeng Zhang\textsuperscript{\rm 1}\quad
Chak Tou Leong\textsuperscript{\rm 2}\quad \\
\textbf{Haoyang Li}\textsuperscript{\rm 3}\quad 
\textbf{Jiaqi Chen}\textsuperscript{\rm 3,4} \\
\textsuperscript{\rm 1}Wuhan University\quad
\textsuperscript{\rm 2}Hong Kong Polytechnic University\quad
\textsuperscript{\rm 3}Stanford University\quad
\textsuperscript{\rm 4}Topify AI
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
\input{file/0-abstract}
\end{abstract}

\input{file/1-intro}
\input{file/2-related}
\input{file/3-method}
\input{file/4-exp}
\input{file/5-conclusion}
\input{file/reproducibility_statement}

% \bibliography{iclr2026_conference}
\bibliographystyle{iclr2026_conference}
\begin{thebibliography}{60}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdine et~al.(2024)Abdine, Chatzianastasis, Bouyioukos, and Vazirgiannis]{abdine2024prot2text}
Hadi Abdine, Michail Chatzianastasis, Costas Bouyioukos, and Michalis Vazirgiannis.
\newblock Prot2text: Multimodal protein's function generation with gnns and transformers.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~38, pp.\  10757--10765, 2024.

\bibitem[Bairoch \& Apweiler(2000)Bairoch and Apweiler]{bairoch2000swiss}
Amos Bairoch and Rolf Apweiler.
\newblock The swiss-prot protein sequence database and its supplement trembl in 2000.
\newblock \emph{Nucleic acids research}, 28\penalty0 (1):\penalty0 45--48, 2000.

\bibitem[Brandes et~al.(2022)Brandes, Ofer, Peleg, Rappoport, and Linial]{brandes2022proteinbert}
Nadav Brandes, Dan Ofer, Yam Peleg, Nadav Rappoport, and Michal Linial.
\newblock Proteinbert: a universal deep-learning model of protein sequence and function.
\newblock \emph{Bioinformatics}, 38\penalty0 (8):\penalty0 2102--2110, 2022.

\bibitem[Cao \& Shen(2021)Cao and Shen]{cao2021tale}
Yue Cao and Yang Shen.
\newblock Tale: Transformer-based protein function annotation with joint sequence--label embedding.
\newblock \emph{Bioinformatics}, 37\penalty0 (18):\penalty0 2825--2833, 2021.

\bibitem[Chaplot(2023)]{chaplot2023albert}
Devendra~Singh Chaplot.
\newblock Albert q. jiang, alexandre sablayrolles, arthur mensch, chris bamford, devendra singh chaplot, diego de las casas, florian bressand, gianna lengyel, guillaume lample, lucile saulnier, l{\'e}lio renard lavaud, marie-anne lachaux, pierre stock, teven le scao, thibaut lavril, thomas wang, timoth{\'e}e lacroix, william el sayed.
\newblock \emph{arXiv preprint arXiv:2310.06825}, 3, 2023.

\bibitem[Chen et~al.(2024{\natexlab{a}})Chen, Cheng, Li, Geng, Gong, Li, Bei, Tan, Wang, Zeng, et~al.]{chen2024xtrimopglm}
Bo~Chen, Xingyi Cheng, Pan Li, Yangli-ao Geng, Jing Gong, Shen Li, Zhilei Bei, Xu~Tan, Boyan Wang, Xin Zeng, et~al.
\newblock xtrimopglm: unified 100b-scale pre-trained transformer for deciphering the language of protein.
\newblock \emph{arXiv preprint arXiv:2401.06199}, 2024{\natexlab{a}}.

\bibitem[Chen et~al.(2024{\natexlab{b}})Chen, Chen, Xie, Xue, Zhang, Zhou, and Fang]{chen2024unifying}
Zhiyuan Chen, Tianhao Chen, Chenggang Xie, Yang Xue, Xiaonan Zhang, Jingbo Zhou, and Xiaomin Fang.
\newblock Unifying sequences, structures, and descriptions for any-to-any protein generation with the large multimodal model helixprotx.
\newblock \emph{arXiv preprint arXiv:2407.09274}, 2024{\natexlab{b}}.

\bibitem[Clark \& Radivojac(2011)Clark and Radivojac]{clark2011analysis}
Wyatt~T Clark and Predrag Radivojac.
\newblock Analysis of protein function and its prediction from amino acid sequence.
\newblock \emph{Proteins: Structure, Function, and Bioinformatics}, 79\penalty0 (7):\penalty0 2086--2096, 2011.

\bibitem[Devos \& Valencia(2000)Devos and Valencia]{devos2000practical}
Damien Devos and Alfonso Valencia.
\newblock Practical limits of function prediction.
\newblock \emph{Proteins: Structure, Function, and Bioinformatics}, 41\penalty0 (1):\penalty0 98--107, 2000.

\bibitem[Elnaggar et~al.(2021)Elnaggar, Heinzinger, Dallago, Rehawi, Wang, Jones, Gibbs, Feher, Angerer, Steinegger, et~al.]{elnaggar2021prottrans}
Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Yu~Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, et~al.
\newblock Prottrans: Toward understanding the language of life through self-supervised learning.
\newblock \emph{IEEE transactions on pattern analysis and machine intelligence}, 44\penalty0 (10):\penalty0 7112--7127, 2021.

\bibitem[Fang et~al.(2024)Fang, Liang, Zhang, Liu, Huang, Chen, Fan, and Chen]{fang2023mol}
Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, and Huajun Chen.
\newblock Mol-instructions: {A} large-scale biomolecular instruction dataset for large language models.
\newblock In \emph{The Twelfth International Conference on Learning Representations}. OpenReview.net, 2024.
\newblock URL \url{https://openreview.net/pdf?id=Tlsdsb6l9n}.

\bibitem[Ferruz et~al.(2022)Ferruz, Schmidt, and H{\"o}cker]{ferruz2022protgpt2}
Noelia Ferruz, Steffen Schmidt, and Birte H{\"o}cker.
\newblock Protgpt2 is a deep unsupervised language model for protein design.
\newblock \emph{Nature communications}, 13\penalty0 (1):\penalty0 4348, 2022.

\bibitem[Gass et~al.(2020)Gass, Behney, and Plonsky]{gass2020second}
Susan~M Gass, Jennifer Behney, and Luke Plonsky.
\newblock \emph{Second language acquisition: An introductory course}.
\newblock Routledge, 2020.

\bibitem[Guo et~al.(2025)Guo, Yang, Zhang, Song, Zhang, Xu, Zhu, Ma, Wang, Bi, et~al.]{guo2025deepseek}
Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et~al.
\newblock Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2501.12948}, 2025.

\bibitem[Guo et~al.(2023)Guo, Huo, Zhang, and Xie]{guo2023proteinchat}
Han Guo, Mingjia Huo, Ruiyi Zhang, and Pengtao Xie.
\newblock Proteinchat: Towards achieving chatgpt-like functionalities on protein 3d structures.
\newblock \emph{Authorea Preprints}, 2023.

\bibitem[Hayes et~al.(2025)Hayes, Rao, Akin, Sofroniew, Oktay, Lin, Verkuil, Tran, Deaton, Wiggert, et~al.]{hayes2025simulating}
Thomas Hayes, Roshan Rao, Halil Akin, Nicholas~J Sofroniew, Deniz Oktay, Zeming Lin, Robert Verkuil, Vincent~Q Tran, Jonathan Deaton, Marius Wiggert, et~al.
\newblock Simulating 500 million years of evolution with a language model.
\newblock \emph{Science}, pp.\  eads0018, 2025.

\bibitem[Hu et~al.(2024)Hu, Tan, Xu, Gao, Xia, Wu, and Li]{ProtGO}
Bozhen Hu, Cheng Tan, Yongjie Xu, Zhangyang Gao, Jun Xia, Lirong Wu, and Stan~Z. Li.
\newblock Protgo: Function-guided protein modeling for unified representation learning.
\newblock In A.~Globerson, L.~Mackey, D.~Belgrave, A.~Fan, U.~Paquet, J.~Tomczak, and C.~Zhang (eds.), \emph{Advances in Neural Information Processing Systems}, volume~37, pp.\  88581--88604. Curran Associates, Inc., 2024.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2024/file/a1722a6bd1023c026a3d6a570fb3af75-Paper-Conference.pdf}.

\bibitem[Huckin \& Coady(1999)Huckin and Coady]{huckin1999incidental}
Thomas Huckin and James Coady.
\newblock Incidental vocabulary acquisition in a second language: A review.
\newblock \emph{Studies in second language acquisition}, 21\penalty0 (2):\penalty0 181--193, 1999.

\bibitem[Jararweh et~al.(2025)Jararweh, Macaulay, Arredondo, Hu, Tafoya, Virupakshappa, and Sahu]{Protein2Text2025}
Ala Jararweh, Oladimeji Macaulay, David Arredondo, Yue Hu, Luis Tafoya, Kushal Virupakshappa, and Avinash Sahu.
\newblock Protein2text: Resampling mechanism to translate protein sequences into human-interpretable text.
\newblock In \emph{NAACL 2025 - Industry Track}, 2025.

\bibitem[Jarvis \& Pavlenko(2008)Jarvis and Pavlenko]{jarvis2008crosslinguistic}
Scott Jarvis and Aneta Pavlenko.
\newblock \emph{Crosslinguistic influence in language and cognition}.
\newblock Routledge, 2008.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness, Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska, et~al.]{kirkpatrick2017overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei~A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the national academy of sciences}, 114\penalty0 (13):\penalty0 3521--3526, 2017.

\bibitem[Kitadai \& Maruyama(2018)Kitadai and Maruyama]{kitadai2018origins}
Norio Kitadai and Shigenori Maruyama.
\newblock Origins of building blocks of life: A review.
\newblock \emph{Geoscience Frontiers}, 9\penalty0 (4):\penalty0 1117--1153, 2018.

\bibitem[Koonin \& Galperin(2002)Koonin and Galperin]{koonin2002sequence}
Eugene Koonin and Michael~Y Galperin.
\newblock \emph{Sequence—Evolution—Function: Computational Approaches in Comparative Genomics}.
\newblock Springer Science \& Business Media, 2002.

\bibitem[Lin \& Hovy(2002)Lin and Hovy]{lin2002manual}
Chin-Yew Lin and Eduard Hovy.
\newblock Manual and automatic evaluation of summaries.
\newblock In \emph{Proceedings of the ACL-02 workshop on automatic summarization}, pp.\  45--51, 2002.

\bibitem[Lin et~al.(2023)Lin, Akin, Rao, Hie, Zhu, Lu, Smetanin, Verkuil, Kabeli, Shmueli, et~al.]{lin2023evolutionary}
Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, et~al.
\newblock Evolutionary-scale prediction of atomic-level protein structure with a language model.
\newblock \emph{Science}, 379\penalty0 (6637):\penalty0 1123--1130, 2023.

\bibitem[Liu et~al.(2024{\natexlab{a}})Liu, Sun, Ji, Tian, Tang, Wu, and Lan]{liu2024evollama}
Nuowei Liu, Changzhi Sun, Tao Ji, Junfeng Tian, Jianxin Tang, Yuanbin Wu, and Man Lan.
\newblock Evollama: Enhancing llms' understanding of proteins via multimodal structure and sequence representations.
\newblock \emph{arXiv preprint arXiv:2412.11618}, 2024{\natexlab{a}}.

\bibitem[Liu et~al.(2024{\natexlab{b}})Liu, Zhang, Fei, Zhang, Wang, Kawaguchi, and Chua]{liu-etal-2024-prott3}
Zhiyuan Liu, An~Zhang, Hao Fei, Enzhi Zhang, Xiang Wang, Kenji Kawaguchi, and Tat-Seng Chua.
\newblock {P}rot{T}3: Protein-to-text generation for text-based protein understanding.
\newblock In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  5949--5966, Bangkok, Thailand, 2024{\natexlab{b}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2024.acl-long.324}.
\newblock URL \url{https://aclanthology.org/2024.acl-long.324/}.

\bibitem[Luo et~al.(2024)Luo, Zhang, Fan, Yang, Hong, Wu, Qiao, and Nie]{luo2024biomedgpt}
Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Massimo Hong, Yushuai Wu, Mu~Qiao, and Zaiqing Nie.
\newblock Biomedgpt: An open multimodal large language model for biomedicine.
\newblock \emph{IEEE Journal of Biomedical and Health Informatics}, 2024.

\bibitem[Lv et~al.(2024)Lv, Lin, Li, Liu, Cui, Yu-Chian~Chen, Yuan, and Tian]{lv2024prollama}
Liuzhenghao Lv, Zongying Lin, Hao Li, Yuyang Liu, Jiaxi Cui, Calvin Yu-Chian~Chen, Li~Yuan, and Yonghong Tian.
\newblock Prollama: A protein large language model for multi-task protein language processing.
\newblock \emph{arXiv e-prints}, pp.\  arXiv--2402, 2024.

\bibitem[Lv et~al.(2025)Lv, Lin, Li, Liu, Cui, Chen, Yuan, and Tian]{lv2025prollama}
Liuzhenghao Lv, Zongying Lin, Hao Li, Yuyang Liu, Jiaxi Cui, Calvin Yu-Chian Chen, Li~Yuan, and Yonghong Tian.
\newblock Prollama: A protein large language model for multi-task protein language processing.
\newblock \emph{IEEE Transactions on Artificial Intelligence}, 2025.

\bibitem[Ma et~al.(2025)Ma, Fan, Wang, Chen, Lin, Li, Feng, Zhang, Cao, and Gao]{ma2025prottex}
Zicheng Ma, Chuanliu Fan, Zhicong Wang, Zhenyu Chen, Xiaohan Lin, Yanheng Li, Shihao Feng, Jun Zhang, Ziqiang Cao, and Yi~Qin Gao.
\newblock Prottex: Structure-in-context reasoning and editing of proteins with large language models.
\newblock \emph{arXiv preprint arXiv:2503.08179}, 2025.

\bibitem[Madani et~al.(2023)Madani, Krause, Greene, Subramanian, Mohr, Holton, Olmos, Xiong, Sun, Socher, et~al.]{madani2023large}
Ali Madani, Ben Krause, Eric~R Greene, Subu Subramanian, Benjamin~P Mohr, James~M Holton, Jose~Luis Olmos, Caiming Xiong, Zachary~Z Sun, Richard Socher, et~al.
\newblock Large language models generate functional protein sequences across diverse families.
\newblock \emph{Nature echnology}, 41\penalty0 (8):\penalty0 1099--1106, 2023.

\bibitem[Mondon(1984)]{mondon1984classification}
Camille Mondon.
\newblock Classification and regression trees, 1984.

\bibitem[Nijkamp et~al.(2023)Nijkamp, Ruffolo, Weinstein, Naik, and Madani]{nijkamp2023progen2}
Erik Nijkamp, Jeffrey~A Ruffolo, Eli~N Weinstein, Nikhil Naik, and Ali Madani.
\newblock Progen2: exploring the boundaries of protein language models.
\newblock \emph{Cell systems}, 14\penalty0 (11):\penalty0 968--978, 2023.

\bibitem[OpenAI et~al.(2024)]{openai2024gpt4}
OpenAI et~al.
\newblock Gpt-4 technical report, 2024.

\bibitem[Pei et~al.(2023{\natexlab{a}})Pei, Zhang, Zhu, Wu, Gao, Wu, Xia, and Yan]{BioT5}
Qizhi Pei, Wei Zhang, Jinhua Zhu, Kehan Wu, Kaiyuan Gao, Lijun Wu, Yingce Xia, and Rui Yan.
\newblock {B}io{T}5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pp.\  1102--1123, Singapore, December 2023{\natexlab{a}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.emnlp-main.70}.
\newblock URL \url{https://aclanthology.org/2023.emnlp-main.70/}.

\bibitem[Pei et~al.(2023{\natexlab{b}})Pei, Zhang, Zhu, Wu, Gao, Wu, Xia, and Yan]{pei2023biot5}
Qizhi Pei, Wei Zhang, Jinhua Zhu, Kehan Wu, Kaiyuan Gao, Lijun Wu, Yingce Xia, and Rui Yan.
\newblock Biot5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations.
\newblock \emph{arXiv preprint arXiv:2310.07276}, 2023{\natexlab{b}}.

\bibitem[Pesquita et~al.(2008)Pesquita, Faria, Bastos, Ferreira, Falc{\~a}o, and Couto]{pesquita2008metrics}
Catia Pesquita, Daniel Faria, Hugo Bastos, Ant{\'o}nio~EN Ferreira, Andr{\'e}~O Falc{\~a}o, and Francisco~M Couto.
\newblock Metrics for go based protein semantic similarity: a systematic evaluation.
\newblock \emph{BMC bioinformatics}, 9\penalty0 (Suppl 5):\penalty0 S4, 2008.

\bibitem[Rost et~al.(1998)]{rost1998protein}
Burkhard Rost et~al.
\newblock Protein structure prediction in 1d, 2d, and 3d.
\newblock \emph{Encyclopedia of Computational Chemistry}, pp.\  2242--2255, 1998.

\bibitem[Shen et~al.(2024)Shen, Chen, Mamalakis, He, Xia, Li, Su, He, and Wang]{shen2024fine}
Yiqing Shen, Zan Chen, Michail Mamalakis, Luhan He, Haiyang Xia, Tianbin Li, Yanzhou Su, Junjun He, and Yu~Guang Wang.
\newblock A fine-tuning dataset and benchmark for large language models for protein understanding.
\newblock In \emph{2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, pp.\  2390--2395. IEEE, 2024.

\bibitem[Steinegger \& S{\"o}ding(2017)Steinegger and S{\"o}ding]{steinegger2017mmseqs2}
Martin Steinegger and Johannes S{\"o}ding.
\newblock Mmseqs2 enables sensitive protein sequence searching for the analysis of massive data sets.
\newblock \emph{Nature biotechnology}, 35\penalty0 (11):\penalty0 1026--1028, 2017.

\bibitem[Su et~al.(2023)Su, Han, Zhou, Shan, Zhou, and Yuan]{su2023saprot}
Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, and Fajie Yuan.
\newblock Saprot: Protein language modeling with structure-aware vocabulary.
\newblock \emph{BioRxiv}, pp.\  2023--10, 2023.

\bibitem[Tan et~al.(2025)Tan, Gou, Zhong, Hong, Yu, and Zhou]{tan2025venusxunlockingfinegrainedfunctional}
Yang Tan, Wenrui Gou, Bozitao Zhong, Liang Hong, Huiqun Yu, and Bingxin Zhou.
\newblock Venusx: Unlocking fine-grained functional understanding of proteins, 2025.
\newblock URL \url{https://arxiv.org/abs/2505.11812}.

\bibitem[Taylor et~al.(2022)Taylor, Kardas, Cucurull, Scialom, Hartshorn, Saravia, Poulton, Kerkez, and Stojnic]{taylor2022galactica}
Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic.
\newblock Galactica: A large language model for science.
\newblock \emph{ArXiv preprint}, abs/2211.09085, 2022.
\newblock URL \url{https://arxiv.org/abs/2211.09085}.

\bibitem[Team et~al.(2025)Team, Bai, Bao, Chen, Chen, Chen, Chen, Chen, Chen, Chen, et~al.]{team2025kimi}
Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et~al.
\newblock Kimi k2: Open agentic intelligence.
\newblock \emph{arXiv preprint arXiv:2507.20534}, 2025.

\bibitem[Team(2024)]{qwen2.5}
Qwen Team.
\newblock Qwen2.5: A party of foundation models, September 2024.
\newblock URL \url{https://qwenlm.github.io/blog/qwen2.5/}.

\bibitem[Team(2025)]{qwen3technicalreport}
Qwen Team.
\newblock Qwen3 technical report, 2025.
\newblock URL \url{https://arxiv.org/abs/2505.09388}.

\bibitem[Wang et~al.(2024)Wang, Fan, Quan, and Yang]{wang2024protchatgpt}
Chao Wang, Hehe Fan, Ruijie Quan, and Yi~Yang.
\newblock Protchatgpt: Towards understanding proteins with large language models.
\newblock \emph{arXiv preprint arXiv:2402.09649}, 2024.

\bibitem[Wang \& Pollock(2005)Wang and Pollock]{wang2005context}
Zhengyuan~O Wang and David~D Pollock.
\newblock Context dependence and coevolution among amino acid residues in proteins.
\newblock In \emph{Methods in enzymology}, volume 395, pp.\  779--790. Elsevier, 2005.

\bibitem[Wu et~al.(2025)Wu, Liu, Cao, Li, Feng, Shu, Yu, Yuan, and Li]{wu2025rethinking}
Juntong Wu, Zijing Liu, He~Cao, Hao Li, Bin Feng, Zishan Shu, Ke~Yu, Li~Yuan, and Yu~Li.
\newblock Rethinking text-based protein understanding: Retrieval or llm?
\newblock \emph{arXiv preprint arXiv:2505.20354}, 2025.

\bibitem[Wu et~al.(2024{\natexlab{a}})Wu, Chang, and Zou]{wu2024proteinclip}
Kevin~E Wu, Howard Chang, and James Zou.
\newblock Proteinclip: enhancing protein language models with natural language.
\newblock \emph{bioRxiv}, pp.\  2024--05, 2024{\natexlab{a}}.

\bibitem[Wu et~al.(2024{\natexlab{b}})Wu, Yost, Daniel, Belk, Xia, Egawa, Satpathy, Chang, and Zou]{wu2024tcr}
Kevin~E Wu, Kathryn Yost, Bence Daniel, Julia Belk, Yu~Xia, Takeshi Egawa, Ansuman Satpathy, Howard Chang, and James Zou.
\newblock Tcr-bert: learning the grammar of t-cell receptors for flexible antigen-binding analyses.
\newblock In \emph{Machine Learning in Computational Biology}, pp.\  194--229. PMLR, 2024{\natexlab{b}}.

\bibitem[Xiang et~al.(2024)Xiang, Xiong, Chen, Xiong, Zhang, Fu, Zheng, Liu, and Shi]{xiang2024fapm}
Wenkai Xiang, Zhaoping Xiong, Huan Chen, Jiacheng Xiong, Wei Zhang, Zunyun Fu, Mingyue Zheng, Bing Liu, and Qian Shi.
\newblock Fapm: functional annotation of proteins using multimodal models beyond structural modeling.
\newblock \emph{Bioinformatics}, 40\penalty0 (12):\penalty0 btae680, 2024.

\bibitem[Xiao et~al.(2024)Xiao, Sun, Jin, Wang, and Wang]{xiao2024proteingpt}
Yijia Xiao, Edward Sun, Yiqiao Jin, Qifan Wang, and Wei Wang.
\newblock Proteingpt: Multimodal llm for protein property prediction and structure understanding.
\newblock \emph{arXiv preprint arXiv:2408.11363}, 2024.

\bibitem[Xiao et~al.(2025)Xiao, Zhao, Zhang, Jin, Zhang, Ren, Sun, Wang, Wan, Lu, et~al.]{xiao2025protein}
Yijia Xiao, Wanjia Zhao, Junkai Zhang, Yiqiao Jin, Han Zhang, Zhicheng Ren, Renliang Sun, Haixin Wang, Guancheng Wan, Pan Lu, et~al.
\newblock Protein large language models: A comprehensive survey.
\newblock \emph{arXiv preprint arXiv:2502.17504}, 2025.

\bibitem[Xu \& Wang(2022)Xu and Wang]{xu2022protranslator}
Hanwen Xu and Sheng Wang.
\newblock Protranslator: zero-shot protein function prediction using textual description.
\newblock In \emph{International conference on research in computational molecular biology}, pp.\  279--294. Springer, 2022.

\bibitem[Xu et~al.(2023{\natexlab{a}})Xu, Yuan, Miret, and Tang]{protst}
Minghao Xu, Xinyu Yuan, Santiago Miret, and Jian Tang.
\newblock Protst: Multi-modality learning of protein sequences and biomedical texts.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), \emph{International Conference on Machine Learning, {ICML} 2023, 23-29 July 2023, Honolulu, Hawaii, {USA}}, volume 202 of \emph{Proceedings of Machine Learning Research}, pp.\  38749--38767. {PMLR}, 2023{\natexlab{a}}.
\newblock URL \url{https://proceedings.mlr.press/v202/xu23t.html}.

\bibitem[Xu et~al.(2023{\natexlab{b}})Xu, Yuan, Miret, and Tang]{xu2023protst}
Minghao Xu, Xinyu Yuan, Santiago Miret, and Jian Tang.
\newblock Protst: Multi-modality learning of protein sequences and biomedical texts.
\newblock In \emph{International Conference on Machine Learning}, pp.\  38749--38767. PMLR, 2023{\natexlab{b}}.

\bibitem[Zhang et~al.(2022)Zhang, Bi, Liang, Cheng, Hong, Deng, Zhang, Lian, and Chen]{zhang2022ontoprotein}
Ningyu Zhang, Zhen Bi, Xiaozhuan Liang, Siyuan Cheng, Haosen Hong, Shumin Deng, Qiang Zhang, Jiazhang Lian, and Huajun Chen.
\newblock Ontoprotein: Protein pretraining with gene ontology embedding.
\newblock In \emph{The Tenth International Conference on Learning Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}. OpenReview.net, 2022.
\newblock URL \url{https://openreview.net/forum?id=yfe1VMYAXa4}.

\bibitem[Zhuo et~al.(2024)Zhuo, Chi, Xu, Huang, Zhao, Zheng, He, Mao, and Zhang]{zhuo2024protllm}
Le~Zhuo, Zewen Chi, Minghao Xu, Heyan Huang, Jianan Zhao, Heqi Zheng, Conghui He, Xian-Ling Mao, and Wentao Zhang.
\newblock {P}rot{LLM}: An interleaved protein-language {LLM} with protein-as-word pre-training.
\newblock In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  8950--8963, Bangkok, Thailand, August 2024. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2024.acl-long.484}.
\newblock URL \url{https://aclanthology.org/2024.acl-long.484/}.

\end{thebibliography}


\appendix
% You may include other additional sections here.
\input{file/6-appendix}


\end{document}
