% Deciphering the cellular function of an uncharacterized protein sequence remains a long-standing challenge in biology. Existing approaches either rely on additional adapters or demand large-scale data with task-specific fine-tuning. We propose the \textbf{\textit{``Protein-as-Second-Language''}} framework, which treats amino-acid strings as sentences in a novel language that can be learned efficiently with appropriate context exposure. By adaptively constructing sequence–question–answer exemplars, our approach enables large language models to perform zero-shot protein understanding. To support this process, we curate a ``bilingual'' dataset of 79,926 protein–question–answer triples covering attribute, description, and extended-information tasks. Without any additional training, our method improves the average ROUGE-L by 7\% across diverse open-source models and GPT-4o, with gains up to 17.2\%. Our results show that LLMs can accomplish efficient protein-to-function learning when supported by contextual cues, yielding systematic performance improvements.

Deciphering the function of unseen protein sequences is a fundamental challenge with broad scientific impact, yet most existing methods depend on task-specific adapters or large-scale supervised fine-tuning. We introduce the ``\textbf{\textit{Protein-as-Second-Language}}'' framework, which reformulates amino-acid sequences as sentences in a novel symbolic language that large language models can interpret through contextual exemplars. Our approach adaptively constructs sequence–question–answer triples that reveal functional cues in a zero-shot setting, without any further training. To support this process we curate a bilingual corpus of 79,926 protein–QA instances spanning attribute prediction, descriptive understanding, and extended reasoning. Empirically, our method delivers consistent gains across diverse open-source LLMs and GPT-4o, achieving up to 17.2\% ROUGE-L improvement (average +7\%) and even surpassing fine-tuned protein-specific language models. These results highlight that generic LLMs, when guided with protein-as-language cues, can outperform domain-specialized models, offering a scalable pathway for protein understanding in foundation models.