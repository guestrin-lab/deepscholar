\section{The Proposed Method}
\label{sec:baseline}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/fig_paradigm.pdf}
    \caption{Paradigm comparison between SOTAs and the proposed method. Given an ocular image, (a) SOTAs rely on normalization to obtain a normalized iris texture, from which hand-crafted filters or recognition models extract iris codes or templates representing identity. However, normalization is non-intuitive and prone to failure under distortion or degradation, leading to degraded identity features. (b) The proposed method waives normalization and directly uses ocular images with minimal modification as input, achieving higher performance in immersive iris recognition.}
    \label{fig:paradigm}
    % \vspace{-2mm}
\end{figure}

Ever since the seminal work by Daugman~\cite{daugman2009iris}, iris recognition systems have employed a two-stage paradigm as the \textit{de facto} standard, as shown in~\cref{fig:paradigm}(a). First, a normalization stage preprocesses ocular images by segmenting the iris region, fitting it with circular or elliptical rings, and transforming the ring into a unified rectangular shape in polar coordinates, known as the normalized iris. Second, a feature extraction stage derives identity templates from the normalized iris using either hand-crafted filters or DNNs, and calculates their similarity.

Though achieved great success, this paradigm is non-intuitive and was primarily a compromise in the early years, when ocular images were captured under controlled setups and feature extraction techniques were relatively primitive. In such cases, normalization provided a robust and invariant representation that facilitated recognition. However, in emerging immersive scenarios, ocular images are captured with variable headset positions and user behaviors that can introduce off-axis shape distortion, quality degradation, and changes in illumination and gaze point. These factors collectively deteriorate normalization and often produce ineffectively normalized irises. Meanwhile, feature extraction techniques to date have demonstrated advanced utility in handling intra-class variations, largely taking over the role that normalization once played. In this sense, normalization is becoming a technical debt. % 需要ineffectively normalized irises示例图吗？

To improve performance in immersive scenarios, we reframe an end-to-end iris recognition paradigm, that waives the fallible normalization and directly uses ocular images with minimal changes for feature extraction. We find that this simplest and most natural approach works surprisingly well with the support of up-to-date feature extraction techniques, as experimentally demonstrated later in~\cref{sec:benchmark}.

Specifically, we draw inspiration from modern face recognition (FR) systems. For decades, FR has been tailored to be robust in in-the-wild environments. Its success is not attributed to dedicated preprocessing, but to the combined power of large-scale training data, robust feature extractors, and discriminative similarity metrics. To leverage these advantages, for feature extraction, we simply inherit the proven practice of a standard FR pipeline. We employ a ResNet model as the feature extractor. We experimentally find that even a lightweight model suffices to achieve high recognition performance while maintaining efficiency comparable to SOTAs. The model is trained under common angular-margin-based objectives in FR, \textit{wlog.}, ArcFace, to encourage learning identity-discriminative features. We deliberately adopt this unpretentious setting to highlight the stand-alone contribution of our reframed paradigm.

Instead of normalizing the iris by dedicated design, our key paradigm shift lies in directly feeding ocular images into the feature extractor. Specifically, we minimally adjust the images by cropping the iris region with a square bounding box. This bounding box can be robustly annotated by existing iris detection methods even for open-scene ocular images, and can also be efficiently obtained on the fly. We then extend the bounding box by a factor of 1.2 to include adjacent ocular regions. This provides contextual information that broadens the receptive field of the extractor and prevents overfitting to local features. Areas of the bounding box outside the original image are padded with zeros. We then rescale the cropped bounding box region to a fixed size as the model input. \Cref{fig:paradigm}(b) illustrates our proposed overall pipeline. We refer to this concrete method of our paradigm as IR-BBox and regard it as a baseline in later benchmarks. We hope it will encourage the community to explore more dedicated designs following this direction.%  We hope it can foster more dedicated designs following this paradigm within the community.