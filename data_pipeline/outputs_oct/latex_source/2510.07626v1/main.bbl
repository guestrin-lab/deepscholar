\begin{thebibliography}{70}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amini et~al.(2019)Amini, Gabriel, Lin, Koncel-Kedziorski, Choi, and Hajishirzi]{amini2019mathqa}
Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi.
\newblock Mathqa: Towards interpretable math word problem solving with operation-based formalisms.
\newblock \emph{arXiv preprint arXiv:1905.13319}, 2019.

\bibitem[Arjovsky et~al.(2019)Arjovsky, Bottou, Gulrajani, and Lopez-Paz]{arjovsky2019invariant}
Martin Arjovsky, L{\'e}on Bottou, Ishaan Gulrajani, and David Lopez-Paz.
\newblock Invariant risk minimization.
\newblock \emph{arXiv preprint arXiv:1907.02893}, 2019.

\bibitem[Barez et~al.(2025)Barez, Fu, Prabhu, Casper, Sanyal, Bibi, O'Gara, Kirk, Bucknall, Fist, et~al.]{barez2025open}
Fazl Barez, Tingchen Fu, Ameya Prabhu, Stephen Casper, Amartya Sanyal, Adel Bibi, Aidan O'Gara, Robert Kirk, Ben Bucknall, Tim Fist, et~al.
\newblock Open problems in machine unlearning for ai safety.
\newblock \emph{arXiv preprint arXiv:2501.04952}, 2025.

\bibitem[Chao et~al.(2025)Chao, Robey, Dobriban, Hassani, Pappas, and Wong]{chao2025jailbreaking}
Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George~J Pappas, and Eric Wong.
\newblock Jailbreaking black box large language models in twenty queries.
\newblock In \emph{2025 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)}, pp.\  23--42. IEEE, 2025.

\bibitem[Che et~al.(2025)Che, Casper, Kirk, Satheesh, Slocum, McKinney, Gandikota, Ewart, Rosati, Wu, et~al.]{che2025model}
Zora Che, Stephen Casper, Robert Kirk, Anirudh Satheesh, Stewart Slocum, Lev~E McKinney, Rohit Gandikota, Aidan Ewart, Domenic Rosati, Zichu Wu, et~al.
\newblock Model tampering attacks enable more rigorous evaluations of llm capabilities.
\newblock \emph{arXiv preprint arXiv:2502.05209}, 2025.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[Deeb \& Roger(2024)Deeb and Roger]{deeb2024unlearning}
Aghyad Deeb and Fabien Roger.
\newblock Do unlearning methods remove information from language model weights?
\newblock \emph{arXiv preprint arXiv:2410.08827}, 2024.

\bibitem[Dubey et~al.(2024)Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman, Mathur, Schelten, Yang, Fan, et~al.]{dubey2024llama}
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et~al.
\newblock The llama 3 herd of models.
\newblock \emph{arXiv e-prints}, pp.\  arXiv--2407, 2024.

\bibitem[Eldan \& Russinovich(2023)Eldan and Russinovich]{eldan2023whos}
Ronen Eldan and Mark Russinovich.
\newblock Who's harry potter? approximate unlearning in llms, 2023.

\bibitem[Engstrom et~al.(2018)Engstrom, Ilyas, and Athalye]{engstrom2018evaluating}
Logan Engstrom, Andrew Ilyas, and Anish Athalye.
\newblock Evaluating and understanding the robustness of adversarial logit pairing.
\newblock \emph{arXiv preprint arXiv:1807.10272}, 2018.

\bibitem[Fan et~al.(2024{\natexlab{a}})Fan, Liu, Lin, Jia, Zhang, Mei, and Liu]{fan2024simplicity}
Chongyu Fan, Jiancheng Liu, Licong Lin, Jinghan Jia, Ruiqi Zhang, Song Mei, and Sijia Liu.
\newblock Simplicity prevails: Rethinking negative preference optimization for llm unlearning.
\newblock \emph{arXiv preprint arXiv:2410.07163}, 2024{\natexlab{a}}.

\bibitem[Fan et~al.(2024{\natexlab{b}})Fan, Liu, Zhang, Wei, Wong, and Liu]{fan2023salun}
Chongyu Fan, Jiancheng Liu, Yihua Zhang, Dennis Wei, Eric Wong, and Sijia Liu.
\newblock Salun: Empowering machine unlearning via gradient-based weight saliency in both image classification and generation.
\newblock In \emph{International Conference on Learning Representations}, 2024{\natexlab{b}}.

\bibitem[Fan et~al.(2025)Fan, Jia, Zhang, Ramakrishna, Hong, and Liu]{fan2025towards}
Chongyu Fan, Jinghan Jia, Yihua Zhang, Anil Ramakrishna, Mingyi Hong, and Sijia Liu.
\newblock Towards llm unlearning resilient to relearning attacks: A sharpness-aware minimization perspective and beyond.
\newblock \emph{arXiv preprint arXiv:2502.05374}, 2025.

\bibitem[Feng et~al.(2025)Feng, Xu, Robey, Kirk, Davies, Gal, Schwarzschild, and Kolter]{feng2025existing}
Zhili Feng, Yixuan~Even Xu, Alexander Robey, Robert Kirk, Xander Davies, Yarin Gal, Avi Schwarzschild, and J~Zico Kolter.
\newblock Existing large language model unlearning evaluations are inconclusive.
\newblock \emph{arXiv preprint arXiv:2506.00688}, 2025.

\bibitem[Foret et~al.(2021)Foret, Kleiner, Mobahi, and Neyshabur]{foret2021sharpnessaware}
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur.
\newblock Sharpness-aware minimization for efficiently improving generalization.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Gandikota et~al.(2024)Gandikota, Feucht, Marks, and Bau]{gandikota2024erasing}
Rohit Gandikota, Sheridan Feucht, Samuel Marks, and David Bau.
\newblock Erasing conceptual knowledge from language models.
\newblock \emph{arXiv preprint arXiv:2410.02760}, 2024.

\bibitem[Golatkar et~al.(2020)Golatkar, Achille, and Soatto]{golatkar2020eternal}
Aditya Golatkar, Alessandro Achille, and Stefano Soatto.
\newblock Eternal sunshine of the spotless net: Selective forgetting in deep networks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  9304--9312, 2020.

\bibitem[Halawi et~al.(2024)Halawi, Wei, Wallace, Wang, Haghtalab, and Steinhardt]{halawi2024covert}
Danny Halawi, Alexander Wei, Eric Wallace, Tony~T Wang, Nika Haghtalab, and Jacob Steinhardt.
\newblock Covert malicious finetuning: Challenges in safeguarding llm adaptation.
\newblock \emph{arXiv preprint arXiv:2406.20053}, 2024.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock \emph{arXiv preprint arXiv:2009.03300}, 2020.

\bibitem[Hu et~al.(2024)Hu, Fu, Wu, and Smith]{hu2024jogging}
Shengyuan Hu, Yiwei Fu, Zhiwei~Steven Wu, and Virginia Smith.
\newblock Jogging the memory of unlearned model through targeted relearning attack.
\newblock \emph{arXiv preprint arXiv:2406.13356}, 2024.

\bibitem[Hu et~al.(2025)Hu, Kale, Thaker, Fu, Wu, and Smith]{hu2025blur}
Shengyuan Hu, Neil Kale, Pratiksha Thaker, Yiwei Fu, Steven Wu, and Virginia Smith.
\newblock Blur: A benchmark for llm unlearning robust to forget-retain overlap.
\newblock \emph{arXiv preprint arXiv:2506.15699}, 2025.

\bibitem[Huang et~al.(2024)Huang, Hu, Ilhan, Tekin, and Liu]{huang2024harmful}
Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim~Furkan Tekin, and Ling Liu.
\newblock Harmful fine-tuning attacks and defenses for large language models: A survey.
\newblock \emph{arXiv preprint arXiv:2409.18169}, 2024.

\bibitem[Jain et~al.(2023)Jain, Kirk, Lubana, Dick, Tanaka, Grefenstette, Rockt{\"a}schel, and Krueger]{jain2023mechanistically}
Samyak Jain, Robert Kirk, Ekdeep~Singh Lubana, Robert~P Dick, Hidenori Tanaka, Edward Grefenstette, Tim Rockt{\"a}schel, and David~Scott Krueger.
\newblock Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks.
\newblock \emph{arXiv preprint arXiv:2311.12786}, 2023.

\bibitem[Ji et~al.(2024)Ji, Hong, Zhang, Chen, Dai, Zheng, Qiu, Zhou, Wang, Li, et~al.]{ji2024pku}
Jiaming Ji, Donghai Hong, Borong Zhang, Boyuan Chen, Juntao Dai, Boren Zheng, Tianyi Qiu, Jiayi Zhou, Kaile Wang, Boxuan Li, et~al.
\newblock Pku-saferlhf: Towards multi-level safety alignment for llms with human preference.
\newblock \emph{arXiv preprint arXiv:2406.15513}, 2024.

\bibitem[Jin et~al.(2024)Jin, Cao, Wang, He, Yuan, Li, Chen, Liu, and Zhao]{jin2024rwku}
Zhuoran Jin, Pengfei Cao, Chenhao Wang, Zhitao He, Hongbang Yuan, Jiachun Li, Yubo Chen, Kang Liu, and Jun Zhao.
\newblock Rwku: Benchmarking real-world knowledge unlearning for large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 37:\penalty0 98213--98263, 2024.

\bibitem[Kannan et~al.(2018)Kannan, Kurakin, and Goodfellow]{kannan2018adversarial}
Harini Kannan, Alexey Kurakin, and Ian Goodfellow.
\newblock Adversarial logit pairing.
\newblock \emph{arXiv preprint arXiv:1803.06373}, 2018.

\bibitem[Lee et~al.(2025)Lee, Foote, Infanger, Shor, Kamath, Goldman-Wetzler, Woodworth, Cloud, and Turner]{lee2025distillation}
Bruce~W Lee, Addie Foote, Alex Infanger, Leni Shor, Harish Kamath, Jacob Goldman-Wetzler, Bryce Woodworth, Alex Cloud, and Alexander~Matt Turner.
\newblock Distillation robustifies unlearning.
\newblock \emph{arXiv preprint arXiv:2506.06278}, 2025.

\bibitem[Lee et~al.(2018)Lee, Ajanthan, and Torr]{lee2018snip}
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip~HS Torr.
\newblock Snip: Single-shot network pruning based on connection sensitivity.
\newblock \emph{arXiv preprint arXiv:1810.02340}, 2018.

\bibitem[Lermen et~al.(2023)Lermen, Rogers-Smith, and Ladish]{lermen2023lora}
Simon Lermen, Charlie Rogers-Smith, and Jeffrey Ladish.
\newblock Lora fine-tuning efficiently undoes safety training in llama 2-chat 70b.
\newblock \emph{arXiv preprint arXiv:2310.20624}, 2023.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and Goldstein]{li2018visualizing}
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Li et~al.(2024)Li, Pan, Gopal, Yue, Berrios, Gatti, Li, Dombrowski, Goel, Mukobi, Helm-Burger, Lababidi, Justen, Liu, Chen, Barrass, Zhang, Zhu, Tamirisa, Bharathi, Herbert-Voss, Breuer, Zou, Mazeika, Wang, Oswal, Lin, Hunt, Tienken-Harder, Shih, Talley, Guan, Steneker, Campbell, Jokubaitis, Basart, Fitz, Kumaraguru, Karmakar, Tupakula, Varadharajan, Shoshitaishvili, Ba, Esvelt, Wang, and Hendrycks]{li2024wmdp}
Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin~D. Li, Ann-Kathrin Dombrowski, Shashwat Goel, Gabriel Mukobi, Nathan Helm-Burger, Rassin Lababidi, Lennart Justen, Andrew~Bo Liu, Michael Chen, Isabelle Barrass, Oliver Zhang, Xiaoyuan Zhu, Rishub Tamirisa, Bhrugu Bharathi, Ariel Herbert-Voss, Cort~B Breuer, Andy Zou, Mantas Mazeika, Zifan Wang, Palash Oswal, Weiran Lin, Adam~Alfred Hunt, Justin Tienken-Harder, Kevin~Y. Shih, Kemper Talley, John Guan, Ian Steneker, David Campbell, Brad Jokubaitis, Steven Basart, Stephen Fitz, Ponnurangam Kumaraguru, Kallol~Krishna Karmakar, Uday Tupakula, Vijay Varadharajan, Yan Shoshitaishvili, Jimmy Ba, Kevin~M. Esvelt, Alexandr Wang, and Dan Hendrycks.
\newblock The {WMDP} benchmark: Measuring and reducing malicious use with unlearning.
\newblock In \emph{Proceedings of the 41st International Conference on Machine Learning}, pp.\  28525--28550, 2024.

\bibitem[Lin et~al.(2021)Lin, Hilton, and Evans]{lin2021truthfulqa}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock Truthfulqa: Measuring how models mimic human falsehoods.
\newblock \emph{arXiv preprint arXiv:2109.07958}, 2021.

\bibitem[Liu et~al.(2024{\natexlab{a}})Liu, Wang, Flanigan, and Liu]{liu2024large}
Chris~Yuhao Liu, Yaxuan Wang, Jeffrey Flanigan, and Yang Liu.
\newblock Large language model unlearning via embedding-corrupted prompts.
\newblock \emph{arXiv preprint arXiv:2406.07933}, 2024{\natexlab{a}}.

\bibitem[Liu et~al.(2024{\natexlab{b}})Liu, Yao, Jia, Casper, Baracaldo, Hase, Yao, Liu, Xu, Li, Varshney, Bansal, Koyejo, and Liu]{liu2024rethinking}
Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Yuguang Yao, Chris~Yuhao Liu, Xiaojun Xu, Hang Li, Kush~R. Varshney, Mohit Bansal, Sanmi Koyejo, and Yang Liu.
\newblock Rethinking machine unlearning for large language models.
\newblock \emph{arXiv preprint arXiv:2402.08787}, 2024{\natexlab{b}}.

\bibitem[Liu et~al.(2024{\natexlab{c}})Liu, Zhu, Tan, and Chen]{liu2024learning}
Zhenhua Liu, Tong Zhu, Chuanyuan Tan, and Wenliang Chen.
\newblock Learning to refuse: Towards mitigating privacy risks in llms.
\newblock \emph{arXiv preprint arXiv:2407.10058}, 2024{\natexlab{c}}.

\bibitem[Liu et~al.(2024{\natexlab{d}})Liu, Dou, Tan, Tian, and Jiang]{liu2024towards}
Zheyuan Liu, Guangyao Dou, Zhaoxuan Tan, Yijun Tian, and Meng Jiang.
\newblock Towards safer large language models through machine unlearning.
\newblock In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), \emph{Findings of the Association for Computational Linguistics: ACL 2024}, pp.\  1817--1829, August 2024{\natexlab{d}}.

\bibitem[Lu et~al.(2022)Lu, Welleck, Hessel, Jiang, Qin, West, Ammanabrolu, and Choi]{lu2022quark}
Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, and Yejin Choi.
\newblock Quark: Controllable text generation with reinforced unlearning.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 27591--27609, 2022.

\bibitem[{\L}ucki et~al.(2024){\L}ucki, Wei, Huang, Henderson, Tram{\`e}r, and Rando]{lucki2024adversarial}
Jakub {\L}ucki, Boyi Wei, Yangsibo Huang, Peter Henderson, Florian Tram{\`e}r, and Javier Rando.
\newblock An adversarial perspective on machine unlearning for ai safety.
\newblock \emph{arXiv preprint arXiv:2409.18025}, 2024.

\bibitem[Lynch et~al.(2024)Lynch, Guo, Ewart, Casper, and Hadfield-Menell]{lynch2024eight}
Aengus Lynch, Phillip Guo, Aidan Ewart, Stephen Casper, and Dylan Hadfield-Menell.
\newblock Eight methods to evaluate robust unlearning in llms.
\newblock \emph{arXiv preprint arXiv:2402.16835}, 2024.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and Vladu]{madry2018towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=rJzIBfZAb}.

\bibitem[Maini et~al.(2024)Maini, Feng, Schwarzschild, Lipton, and Kolter]{maini2024tofu}
Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary~Chase Lipton, and J~Zico Kolter.
\newblock {TOFU}: A task of fictitious unlearning for {LLM}s.
\newblock In \emph{First Conference on Language Modeling}, 2024.

\bibitem[Merity et~al.(2016)Merity, Xiong, Bradbury, and Socher]{merity2016pointer}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models, 2016.

\bibitem[Patil et~al.(2024)Patil, Hase, and Bansal]{patil2023can}
Vaidehi Patil, Peter Hase, and Mohit Bansal.
\newblock Can sensitive information be deleted from llms? objectives for defending against extraction attacks.
\newblock \emph{ICLR}, 2024.

\bibitem[Poliak(2020)]{poliak2020survey}
Adam Poliak.
\newblock A survey on recognizing textual entailment as an nlp evaluation.
\newblock \emph{arXiv preprint arXiv:2010.03061}, 2020.

\bibitem[Qi et~al.(2023)Qi, Zeng, Xie, Chen, Jia, Mittal, and Henderson]{qi2023fine}
Xiangyu Qi, Yi~Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson.
\newblock Fine-tuning aligned language models compromises safety, even when users do not intend to!
\newblock \emph{arXiv preprint arXiv:2310.03693}, 2023.

\bibitem[Rafailov et~al.(2024)Rafailov, Sharma, Mitchell, Manning, Ermon, and Finn]{rafailov2024direct}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher~D Manning, Stefano Ermon, and Chelsea Finn.
\newblock Direct preference optimization: Your language model is secretly a reward model.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Sadasivan et~al.(2024)Sadasivan, Saha, Sriramanan, Kattakinda, Chegini, and Feizi]{sadasivan2024fast}
Vinu~Sankar Sadasivan, Shoumik Saha, Gaurang Sriramanan, Priyatham Kattakinda, Atoosa Chegini, and Soheil Feizi.
\newblock Fast adversarial attacks on language models in one gpu minute.
\newblock \emph{arXiv preprint arXiv:2402.15570}, 2024.

\bibitem[Schwinn et~al.(2024)Schwinn, Dobre, Xhonneux, Gidel, and G{\"u}nnemann]{schwinn2024soft}
Leo Schwinn, David Dobre, Sophie Xhonneux, Gauthier Gidel, and Stephan G{\"u}nnemann.
\newblock Soft prompt threats: Attacking safety alignment and unlearning in open-source llms through the embedding space.
\newblock \emph{Advances in Neural Information Processing Systems}, 37:\penalty0 9086--9116, 2024.

\bibitem[Sheshadri et~al.(2024)Sheshadri, Ewart, Guo, Lynch, Wu, Hebbar, Sleight, Stickland, Perez, Hadfield-Menell, et~al.]{sheshadri2024latent}
Abhay Sheshadri, Aidan Ewart, Phillip Guo, Aengus Lynch, Cindy Wu, Vivek Hebbar, Henry Sleight, Asa~Cooper Stickland, Ethan Perez, Dylan Hadfield-Menell, et~al.
\newblock Latent adversarial training improves robustness to persistent harmful behaviors in llms.
\newblock \emph{arXiv preprint arXiv:2407.15549}, 2024.

\bibitem[Shi et~al.(2024)Shi, Lee, Huang, Malladi, Zhao, Holtzman, Liu, Zettlemoyer, Smith, and Zhang]{shi2024muse}
Weijia Shi, Jaechan Lee, Yangsibo Huang, Sadhika Malladi, Jieyu Zhao, Ari Holtzman, Daogao Liu, Luke Zettlemoyer, Noah~A Smith, and Chiyuan Zhang.
\newblock Muse: Machine unlearning six-way evaluation for language models.
\newblock \emph{arXiv preprint arXiv:2407.06460}, 2024.

\bibitem[Shin et~al.(2020)Shin, Razeghi, Logan~IV, Wallace, and Singh]{shin2020autoprompt}
Taylor Shin, Yasaman Razeghi, Robert~L Logan~IV, Eric Wallace, and Sameer Singh.
\newblock Autoprompt: Eliciting knowledge from language models with automatically generated prompts.
\newblock \emph{arXiv preprint arXiv:2010.15980}, 2020.

\bibitem[Sileo(2023)]{sileo2023tasksource}
Damien Sileo.
\newblock tasksource: Structured dataset preprocessing annotations for frictionless extreme multi-task learning and evaluation.
\newblock \emph{arXiv preprint arXiv:2301.05948}, 2023.

\bibitem[Singh et~al.(2025)Singh, M{\"u}ller, Croce, and Hein]{singh2025unlearning}
Naman~Deep Singh, Maximilian M{\"u}ller, Francesco Croce, and Matthias Hein.
\newblock Unlearning that lasts: Utility-preserving, robust, and almost irreversible forgetting in llms.
\newblock \emph{arXiv preprint arXiv:2509.02820}, 2025.

\bibitem[Sondej et~al.(2025)Sondej, Yang, Kniejski, Windys, et~al.]{sondej2025robust}
Filip Sondej, Yushi Yang, Miko{\'L} Kniejski, Marcel Windys, et~al.
\newblock Robust llm unlearning with mudman: Meta-unlearning with disruption masking and normalization.
\newblock \emph{arXiv preprint arXiv:2506.12484}, 2025.

\bibitem[Tamirisa et~al.(2024)Tamirisa, Bharathi, Phan, Zhou, Gatti, Suresh, Lin, Wang, Wang, Arel, et~al.]{tamirisa2024tamper}
Rishub Tamirisa, Bhrugu Bharathi, Long Phan, Andy Zhou, Alice Gatti, Tarun Suresh, Maxwell Lin, Justin Wang, Rowan Wang, Ron Arel, et~al.
\newblock Tamper-resistant safeguards for open-weight llms.
\newblock \emph{arXiv preprint arXiv:2408.00761}, 2024.

\bibitem[Thudi et~al.(2022)Thudi, Jia, Shumailov, and Papernot]{thudi2022necessity}
Anvith Thudi, Hengrui Jia, Ilia Shumailov, and Nicolas Papernot.
\newblock On the necessity of auditable algorithmic definitions for machine unlearning.
\newblock In \emph{31st USENIX Security Symposium (USENIX Security 22)}, pp.\  4007--4022, 2022.

\bibitem[Wang et~al.(2025{\natexlab{a}})Wang, Fan, Zhang, Jia, Wei, Ram, Baracaldo, and Liu]{wang2025reasoning}
Changsheng Wang, Chongyu Fan, Yihua Zhang, Jinghan Jia, Dennis Wei, Parikshit Ram, Nathalie Baracaldo, and Sijia Liu.
\newblock Reasoning model unlearning: Forgetting traces, not just answers, while preserving reasoning skills.
\newblock \emph{arXiv preprint arXiv:2506.12963}, 2025{\natexlab{a}}.

\bibitem[Wang et~al.(2025{\natexlab{b}})Wang, Zhang, Jia, Ram, Wei, Yao, Pal, Baracaldo, and Liu]{wang2025invariance}
Changsheng Wang, Yihua Zhang, Jinghan Jia, Parikshit Ram, Dennis Wei, Yuguang Yao, Soumyadeep Pal, Nathalie Baracaldo, and Sijia Liu.
\newblock Invariance makes llm unlearning resilient even to unanticipated downstream fine-tuning.
\newblock \emph{arXiv preprint arXiv:2506.01339}, 2025{\natexlab{b}}.

\bibitem[Wei et~al.(2024)Wei, Huang, Huang, Xie, Qi, Xia, Mittal, Wang, and Henderson]{wei2024assessing}
Boyi Wei, Kaixuan Huang, Yangsibo Huang, Tinghao Xie, Xiangyu Qi, Mengzhou Xia, Prateek Mittal, Mengdi Wang, and Peter Henderson.
\newblock Assessing the brittleness of safety alignment via pruning and low-rank modifications.
\newblock \emph{arXiv preprint arXiv:2402.05162}, 2024.

\bibitem[Wu et~al.(2023)Wu, Li, Xu, Dong, Wu, Bian, and Xiong]{wu2023depn}
Xinwei Wu, Junzhuo Li, Minghui Xu, Weilong Dong, Shuangzhi Wu, Chao Bian, and Deyi Xiong.
\newblock Depn: Detecting and editing privacy neurons in pretrained language models.
\newblock \emph{arXiv preprint arXiv:2310.20138}, 2023.

\bibitem[Yao \& Barbosa(2024)Yao and Barbosa]{yao2024accurate}
Peiran Yao and Denilson Barbosa.
\newblock Accurate and nuanced open-qa evaluation through textual entailment.
\newblock \emph{arXiv preprint arXiv:2405.16702}, 2024.

\bibitem[Yao et~al.(2023)Yao, Xu, and Liu]{yao2023large}
Yuanshun Yao, Xiaojun Xu, and Yang Liu.
\newblock Large language model unlearning.
\newblock \emph{arXiv preprint arXiv:2310.10683}, 2023.

\bibitem[Yao et~al.(2024)Yao, Xu, and Liu]{yao2024large}
Yuanshun Yao, Xiaojun Xu, and Yang Liu.
\newblock Large language model unlearning.
\newblock In \emph{The Thirty-eighth Annual Conference on Neural Information Processing Systems}, 2024.

\bibitem[Yuan et~al.(2024)Yuan, Pang, Du, Chen, Zhang, and Lin]{yuan2024closer}
Xiaojian Yuan, Tianyu Pang, Chao Du, Kejiang Chen, Weiming Zhang, and Min Lin.
\newblock A closer look at machine unlearning for large language models.
\newblock \emph{arXiv preprint arXiv:2410.08109}, 2024.

\bibitem[Zhang et~al.(2024{\natexlab{a}})Zhang, Lin, Bai, and Mei]{zhang2024negative}
Ruiqi Zhang, Licong Lin, Yu~Bai, and Song Mei.
\newblock Negative preference optimization: From catastrophic collapse to effective unlearning.
\newblock In \emph{First Conference on Language Modeling}, 2024{\natexlab{a}}.

\bibitem[Zhang et~al.(2024{\natexlab{b}})Zhang, Wang, Li, Wu, Tang, Liu, He, Yin, and Wang]{zhang2024catastrophic}
Zhiwei Zhang, Fali Wang, Xiaomin Li, Zongyu Wu, Xianfeng Tang, Hui Liu, Qi~He, Wenpeng Yin, and Suhang Wang.
\newblock Catastrophic failure of llm unlearning via quantization.
\newblock \emph{arXiv preprint arXiv:2410.16454}, 2024{\natexlab{b}}.

\bibitem[Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing, et~al.]{zheng2023judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, et~al.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
\newblock \emph{Advances in neural information processing systems}, 36:\penalty0 46595--46623, 2023.

\bibitem[Zhou et~al.(2023)Zhou, Lu, Mishra, Brahma, Basu, Luan, Zhou, and Hou]{zhou2023instruction}
Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi~Luan, Denny Zhou, and Le~Hou.
\newblock Instruction-following evaluation for large language models.
\newblock \emph{arXiv preprint arXiv:2311.07911}, 2023.

\bibitem[Zou et~al.(2023)Zou, Wang, Kolter, and Fredrikson]{zou2023universal}
Andy Zou, Zifan Wang, J~Zico Kolter, and Matt Fredrikson.
\newblock Universal and transferable adversarial attacks on aligned language models.
\newblock \emph{arXiv preprint arXiv:2307.15043}, 2023.

\bibitem[Zou et~al.(2024)Zou, Phan, Wang, Duenas, Lin, Andriushchenko, Kolter, Fredrikson, and Hendrycks]{zou2024improving}
Andy Zou, Long Phan, Justin Wang, Derek Duenas, Maxwell Lin, Maksym Andriushchenko, J~Zico Kolter, Matt Fredrikson, and Dan Hendrycks.
\newblock Improving alignment and robustness with circuit breakers.
\newblock \emph{Advances in Neural Information Processing Systems}, 37:\penalty0 83345--83373, 2024.

\end{thebibliography}
