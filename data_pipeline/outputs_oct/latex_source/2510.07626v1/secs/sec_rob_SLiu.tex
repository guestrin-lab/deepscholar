\section{Multi-Faceted Robustness Assessments for Unlearning}
\label{sec:eval_Rob}

\begin{tcolorbox}[enhanced,attach boxed title to top center={yshift=-3mm,yshifttext=-1mm},
  colback=white,colframe=green!35!black,colbacktitle=green!5!white,
  title={\parbox{0.7\linewidth}{\centering Summary of insights into  unlearning robustness}}, coltitle=black, boxrule=0.8pt, 
  boxed title style={size=small,colframe=green!35!black} ]

% \textbf{(1)} From \textit{weight tampering} perspective, robustness evaluation via (post-unlearning) relearning over in-forget domain data (termed as ``in-domain relearning'') and  that via additional fine-tuning over out-of-forget domain tasks (termed as ``out-of-domain fine-tuning'') are different and give different robustness insights into unlearning methods: For example, {\MDiv} methods are generally more robust under in-domain relearning, whereas {\MRep} methods exhibit stronger robustness under out-of-domain fine-tuning. 


% \textbf{(2)} From \textit{weight quantization} perspective, robustness of unlearned models against weight compression should be carefully measured taking into account the utility influence after quantization, and their influence in knowledge unlearning (\textit{e.g.}, WMDP focusing on hazardous knowledge removal) and data-wise unleanring (\textit{e.g.}, MUSE focusing on specific textual piece of content ) can be different. 


% \textbf{(3)} From  the overall robustness viewpoing including \textit{both weight and input-level perturbations}, additional robust designs (\textit{e.g.}, SAM \cite{fan2025towards}, IRM \cite{wang2025invariance}, and TAR \cite{tamirisa2024tamper}) integrated into standard LLM unlearning methods are generally useful, yielding better robustness than non-robustified unlearning methods even if the robust design itself was not designed considering the full spectrum of vulnerabilities.  

% \textbf{(1)} From a \textit{weight tampering} perspective, robustness evaluations via post-unlearning relearning on in-forget domain data (\textit{i.e.}, ``in-domain relearning'') differ from those via fine-tuning on out-of-forget domain tasks (\textit{i.e.}, ``out-of-domain finetuning''), offering complementary insights. For example, {\MDiv} methods are generally more robust under in-domain relearning, whereas {\MRep} methods show stronger robustness under out-of-domain finetuning.  

% \textbf{(2)} From a \textit{weight quantization} perspective, robustness must be assessed jointly with utility degradation after compression. Moreover, quantization may affect knowledge unlearning (\textit{e.g.}, WMDP for hazardous knowledge removal) and data-specific unlearning (\textit{e.g.}, MUSE for specific content deletion) differently, highlighting the attention on quantization for task-aware evaluation.  

% \textbf{(3)} From an \textit{overall robustness} perspective, covering both weight- and input-level perturbations, integrating additional robust designs (\textit{e.g.}, SAM \cite{fan2025towards}, IRM \cite{wang2025invariance}, TAR \cite{tamirisa2024tamper}) into standard unlearning pipelines generally improves robustness. Notably, these gains hold even when such designs were not tailored to the full spectrum of unlearning vulnerabilities.  

\textbf{(1)} From a \textit{model tampering} perspective, robustness against in-domain relearning differs from out-of-domain fine-tuning.  For instance, {\MDiv} methods are typically more resilient to in-domain relearning, while {\MRep} methods better withstand out-of-domain fine-tuning.

\textbf{(2)} From a \textit{model quantization} perspective, robustness should be assessed alongside the utility loss caused by compression, to avoid misinterpreting performance gains that simply stem from model incapacity under quantization.

\textbf{(3)} From an \textit{input-level} perspective, {\MRep} methods are generally vulnerable to jailbreaking attacks, and robustness to such attacks aligns more closely with in-domain relearning than with out-of-domain fine-tuning.

\textbf{(4)} From an \textit{overall robustness} perspective, including both weight- and input-level perturbations, augmenting unlearning with robust designs, \textit{e.g.}, SAM \citep{fan2025towards}, IRM \citep{wang2025invariance}, and TAR \citep{tamirisa2024tamper}, improves resilience, even when these techniques were not explicitly developed to address all unlearning vulnerabilities.  

\end{tcolorbox}


% \SL{[General comment: Novelty is not clear.]}

In this section, we investigate the robustness of LLM unlearning methods as categorized in Table\,\ref{tab:taxonomy}. Our analysis spans the full spectrum of vulnerabilities, from model-level (\textit{e.g.}, in-domain relearning, out-of-domain fine-tuning, and quantization) to input-level jailbreaking attacks. %We place robustness under the microscope, examining which dimensions each evaluation targets, whether they provide complementary perspectives, and what important differences in LLM unlearning robustness may have been overlooked.  


% Robustness evaluation of unlearning methods can be organized by the level at which adversarial attacks are applied \citep{che2025model}. 
% We distinguish between \textit{model-level attacks}, including in-domain relearning \citep{hu2024jogging,fan2025towards}, out-of-domain fine-tuning \citep{wang2025invariance}, and quantization, and \textit{input-level attacks}, such as jailbreaking \citep{lucki2024adversarial}. 
% In what follows, we analyze the robustness of existing unlearning methods across these attack categories.

%\begin{wrapfigure}{r}{0.4\textwidth}
% \begin{figure}[htb]
%     \centering
%     %\vspace{-4mm}
%     \includegraphics[width=0.38\textwidth]{example-image-a}
%     \caption{\SL{TBD: Fig.\,\ref{fig:robust_wmdp}-(a). Think about the best presentations by checking my texts.}}
%     \label{fig:rob_weight_perturbations}
% %\end{wrapfigure}
% \end{figure}

% \begin{wrapfigure}{r}{0.4\textwidth}
%     \centering
%     \vspace{-4mm}
%     \includegraphics[width=0.38\textwidth]{figs/heatmap_robustness.pdf}
%     \caption{WMDP evaluation accuracy of twelve unlearning methods on WMDP Bio with Llama-3 8B Instruct before and after in-domain relearning (IDR), out-of-domain fine-tuning (ODF on GSM8K), quantization (Quan), and input-level jailbreaking attacks (Jail). An asterisk (*) denotes the presence of a robust design.
%     \SL{[can we show to indicate correlation betwene model-level and input-level robustness? Please be careful of notations.]}
%     }
%     \label{fig:rob_weight_perturbations}
% \end{wrapfigure}

\noindent \textbf{Robustness against in-domain relearning and out-of-domain fine-tuning.}  
Prior work \citep{hu2024jogging,fan2025towards,wang2025invariance,deeb2024unlearning,hu2025blur,che2025model} has highlighted the vulnerability of unlearned models to model-level attack after unlearning. 
However, most studies examine only one type: either \textit{in-domain relearning}  (on data aligned with the forget set $\Df$, \textit{e.g.}, subsets of $\Df$) or \textit{out-of-domain fine-tuning} (adapting to unrelated downstream tasks such as GSM8K for math reasoning). We argue that these correspond to two distinct categories of perturbations, analogous to adversarial robustness versus out-of-distribution robustness. Thus, we propose studying them jointly to obtain a more complete understanding of unlearning robustness.  



To this end, we evaluate unlearning performance, measured by answer selection accuracy ($\mathrm{UE}_\text{MCQ}$) and ES-based generation assessment ($\mathrm{UE}_\text{Open-QA}$) as shown in Fig.\,\ref{fig:evaluatio_wmdp}-(a), for the methods in Table\,\ref{tab:taxonomy}, both before and after in-domain relearning and out-of-domain fine-tuning.



To conduct in-domain relearning, we update the unlearned model on samples from the forget set, following \citep{fan2025towards}. For out-of-domain fine-tuning, we adapt the model to unrelated downstream tasks (including GSM8K, SST2, MNLI), following \citep{wang2025invariance}. See \textbf{Appendix\,\ref{appx:exp_setup}} for more details. \textbf{Fig.\,\ref{fig:rob_weight_perturbations}} illustrates the robustness of in-domain relearning ($\mathrm{Rob}_\text{ReL}$) and out-of-domain fine-tuning ($\mathrm{Rob}_\text{FT}$) for various LLM unlearning methods on WMDP dataset, applied to Llama-3 8B Instruct.  
 % \SL{[Did we focus on both WMDP and Cyber? In earlier sections, we never mentioned it clearly!]}
In the figure, lower accuracy (lighter color) indicates stronger robustness, and methods with explicit robust designs are marked with an asterisk (*). Several key insights can be drawn from Fig.\,\ref{fig:rob_weight_perturbations}.


% \begin{wrapfigure}{r}{0.6\textwidth} % r=右侧, 宽度约一半版面
\begin{figure}[htbp]
\centering
% \vspace*{-3mm}

\begin{tabular}{cc}
%\hspace*{-3mm}
\includegraphics[width=0.33\textwidth,height=!]{figs/heatmap_rob_ft_acc.pdf} 
&
%\hspace*{-6mm}
\includegraphics[width=0.33\textwidth,height=!]{figs/heatmap_rob_ft_es.pdf}
\vspace*{-1mm}
\\
\hspace*{3mm} \small{(a) $\mathrm{UE}_\text{MCQ}$ (Accuracy $\downarrow$)} &  
%\hspace*{-3mm} 
\small{(b) $\mathrm{UE}_\text{Open-QA}$ (ES $\downarrow$)}\\
\end{tabular}
% \vspace{-4mm}
\caption{\small{
Robustness of in-domain relearning ($\mathrm{Rob}_\text{ReL}$) and out-of-domain fine-tuning ($\mathrm{Rob}_\text{FT}$) for 12 unlearning methods on WMDP with Llama-3 8B Instruct evaluated by (a) $\mathrm{UE}_\text{MCQ}$ (Accuracy) and (b) $\mathrm{UE}_\text{Open-QA}$ (ES). Out-of-domain fine-tuning uses GSM8K, SST2, and MNLI. Methods with * include robust designs, and the first column (``unlearned'') shows results before attack. 
}}
\label{fig:rob_weight_perturbations}
% \vspace*{-5mm}
\end{figure}
% \end{wrapfigure}


\textit{First}, relative to the unlearning performance before attack (\textit{i.e.}, ``unlearned'' column), both in-domain relearning ($\mathrm{Rob}_\text{ReL}$) and out-of-domain fine-tuning ($\mathrm{Rob}_\text{FT}$) degrade unlearning effectiveness, as reflected by higher values in UE. Moreover, in-domain relearning acts as the worst-case testing, yielding lower robustness than out-of-domain fine-tuning.  

\textit{Second}, focusing on methods without explicit robust designs, {\MDiv} approaches (including GradDiff, NPO, SimNPO) generally exhibit stronger robustness to in-domain relearning  ($\mathrm{Rob}_\text{ReL}$) than {\MRep} (including RMU, RR) and {\MRej} (including ELM, DPO). \textit{However}, this trend can reverse under out-of-domain fine-tuning ($\mathrm{Rob}_\text{FT}$), where {\MDiv} methods become less robust than {\MRep}, as seen with NPO vs. RMU under $\mathrm{UE}_\text{MCQ}$.  
For {\MRej} methods, DPO is the most robust under $\mathrm{Rob}_\text{FT}$; however, this advantage does not consistently hold under $\mathrm{Rob}_\text{ReL}$. Hence, both robustness dimensions ($\mathrm{Rob}_\text{ReL}$ and $\mathrm{Rob}_\text{FT}$) should be jointly considered for a comprehensive assessment.  


% \textit{Third}, 
% % for methods without a robust design, under $\mathrm{UE}_\text{MCQ}$, {\MDiv} methods (\textit{e.g.}, GradDiff, NPO, SimNPO) generally show stronger robustness to in-domain relearning than {\MRep} methods (\textit{e.g.}, RMU, RR).  
% % \textit{Interestingly}, this trend reverses under out-of-domain fine-tuning, where {\MDiv} becomes less robust than {\MRep}. This suggests that the over-forgetting induced by {\MDiv} in the forget domain does not necessarily yield greater robustness in out-of-domain settings. Thus, both robustness evaluations should be considered jointly for a comprehensive assessment. 
% for {\MRej} methods, DPO emerges as the most robust, while ELM is the least. Under $\mathrm{UE}_\text{Open-QA}$, {\MRep}-based methods show clear improvements in robustness for both in-domain relearning and out-of-domain fine-tuning, sometimes even surpassing {\MDiv}. Notably, DPO, despite lacking a robust design, demonstrates remarkable robustness in out-of-domain fine-tuning.  

% \begin{wrapfigure}{r}{0.5\textwidth} % r=右侧, 宽度约一半版面
% \vspace*{-2mm}
% \begin{tabular}{cc}
% \hspace*{-3mm}
% \includegraphics[width=0.24\textwidth,height=!]{figs/landscape_tar.pdf} 
% &
% \hspace*{-5mm}
% \includegraphics[width=0.24\textwidth,height=!]{figs/landscape_rmulat.pdf}
% \vspace*{-1mm}
% \\
% \hspace*{-3mm} 
% \small{(a) TAR} &  
% \hspace*{-5mm} 
% \small{(b) RMU+LAT}\\
% \end{tabular}
%  \vspace*{-2mm}
% \caption{\small{The prediction loss landscape of the TAR and RMU+LAT-unlearned model on the forget set using the visualization tool in \citep{li2018visualizing}.
% %where higher values around $x = y = 0$ indicate more effective unlearning. The 3D loss landscape is defined as $z = \ell(\btheta + x \cdot \mathbf{r}_1 + y \cdot \mathbf{r}_2)$, with $\btheta$ representing the unlearned model.
% }}
% \label{fig:landscape}
% % \vspace*{-5mm}
% \end{wrapfigure}



\textit{Third}, incorporating robustness-oriented designs consistently improves robustness against both in-domain relearning and out-of-domain fine-tuning, regardless of whether they build on {\MDivB} (\textit{e.g.}, NPO+SAM, NPO+IRM) or {\MRepB} (\textit{e.g.}, TAR). An exception is RMU+LAT, which fails to show consistent improvements over RMU. Similar to adversarial logit pairing \citep{kannan2018adversarial}, it offers only \textit{local robustness}, leading to a less smooth loss landscape \citep{engstrom2018evaluating}. \textbf{Fig.\,\ref{fig:landscape}} of \textbf{Appendix\,\ref{appx:add_exp}} confirms this limitation, as TAR exhibits a much smoother loss surface than RMU+LAT.


% An exception is RMU+LAT, which does not yield consistent or superior gains over its vanilla counterpart RMU. This echoes a known debate \citep{engstrom2018evaluating} contrasting standard adversarial training \citep{madry2018towards} with adversarial logit pairing \citep{kannan2018adversarial}. The latter, though leveraging logits (or other latent information), was argued to provide only \textit{local robustness}, reflected in a less smooth loss landscape. By analogy, in the context of LLM unlearning, RMU+LAT also appears insufficient to confer broad robustness due to its locality constraint. \textbf{Fig.\,\ref{fig:landscape}} validates the limitation of RMU+LAT compared to TAR through visualization of the forget loss landscape \citep{li2018visualizing}. Indeed, the landscape for TAR is noticeably smoother than that of RMU+LAT.  



% \textit{Third}, for a fixed unlearning method, in-domain relearning often serves as a stronger robustness test than out-of-domain fine-tuning. Not surprisingly, it gives a worst-case fine-tuning scenario, though it does not fully capture robustness under out-of-domain shifts.  


% \begin{figure}[htbp]
% \centering
% \begin{tabular}{ccc}
% \hspace{-5mm}
% \includegraphics[width=0.5\linewidth]{figs/2d_ue_ut_legend.pdf} & 
% \hspace*{1mm}
% \includegraphics[width=0.2\linewidth]{figs/line_wmdp_legend.pdf} &
% \hspace*{5mm}
% \includegraphics[width=0.18\linewidth]{figs/line_muse_legend.pdf} \\
% \end{tabular}
% \begin{tabular}{cccc}
% \hspace*{-3mm}
% \includegraphics[width=0.21\textwidth]{figs/2d_ue_mcq_vs_ut_mcq.pdf} &
% \hspace*{-6mm}
% \includegraphics[width=0.21\textwidth]{figs/2d_ue_openqa_vs_ut_openqa.pdf} &
% \hspace*{-8mm}
% \includegraphics[width=0.285\textwidth]{figs/line_wmdp.pdf} &
% \hspace*{-6mm}
% \includegraphics[width=0.26\textwidth]{figs/line_muse.pdf}
% \\
% \small{(a) $\mathrm{UE}_\text{MCQ}$ vs. $\mathrm{UT}_\text{MCQ}$} &
% \small{(b) $\mathrm{UE}_\text{Open-QA}$ vs. $\mathrm{UT}_\text{Open-QA}$} &
% \small{(c) $\mathrm{Rob}_\text{QT}$ on WMDP} &
% \small{(d) $\mathrm{Rob}_\text{QT}$ on MUSE}
% \end{tabular}
% \vspace{-2mm}
% \caption{\small{Robustness of quantization ($\mathrm{Rob}_\text{QT}$) of twelve unlearning methods on WMDP Bio under Llama-3 8B Instruct, evaluated by (a) $\mathrm{UE}_\text{MCQ}$ (Accuracy) vs.  $\mathrm{UE}_\text{MCQ}$ (MMLU) and (b)   $\mathrm{UE}_\text{Open-QA}$ (ES) vs.  $\mathrm{UT}_\text{Open-QA}$ (GSM8K). Solid lines link each method before and after \SL{[xxx-bit weight]} quantization; markers with black diagonal hatching denote quantized models. 
% \SL{[legend position, and missing the legend for after QT.]}
% \SL{[The following plots are not clear. How about making as a table, e.g., Before QT vs. After QT, UE vs. UT. You need to specify what UE and UT are used]}
% (c) $\mathrm{Rob}_\text{QT}$ of NPO and RMU on WMDP Bio with Llama-3 8B Instruct, evaluated by WMDP Bio and GSM8K accuracy. (d) $\mathrm{Rob}_\text{QT}$ of NPO and RMU on MUSE Books with Llama-2 7B, evaluated by KnowMem on $\Df$ (KMF) and KnowMem on $\Dr$ (KMR).}}
% \label{fig:rob_weight_quantization}
% \vspace{-4mm}
% \end{figure}





% \begin{wrapfigure}{r}{0.5\textwidth} % r=右侧, 宽度约一半版面
\begin{figure}[htbp]
\vspace*{-2mm}
\centering
% \hspace{-5mm}
\includegraphics[width=0.7\textwidth]{figs/2d_ue_ut_legend_quan.pdf} \\

\begin{tabular}{cc}
%\hspace*{-3mm}
\includegraphics[width=0.28\textwidth]{figs/2d_ue_mcq_vs_ut_mcq.pdf} &
%\hspace*{-6mm}
\hspace*{3mm}
\includegraphics[width=0.28\textwidth]{figs/2d_ue_openqa_vs_ut_openqa.pdf}
\\
%\hspace*{3mm}
\small{(a) $\mathrm{UE}_\text{MCQ}$ vs. $\mathrm{UT}_\text{MCQ}$} &
\hspace*{3mm}
\small{(b) $\mathrm{UE}_\text{Open-QA}$ vs. $\mathrm{UT}_\text{Open-QA}$}
\end{tabular}
\vspace*{-1mm}
\caption{\small{
Robustness of quantization ($\mathrm{Rob}_\text{QT}$) for 12 unlearning methods on WMDP with Llama-3 8B Instruct, evaluated by (a) $\mathrm{UE}_\text{MCQ}$ (Accuracy) vs. $\mathrm{UT}_\text{MCQ}$ (MMLU) and (b) $\mathrm{UE}_\text{Open-QA}$ (ES) vs. $\mathrm{UT}_\text{Open-QA}$ (GSM8K). Lines link models pre- and post-4bit quantization; hatched markers indicate quantized models.
% Robustness of quantization ($\mathrm{Rob}_\text{QT}$) of twelve unlearning methods on WMDP Bio under Llama-3 8B Instruct, evaluated by (a) $\mathrm{UE}_\text{MCQ}$ (Accuracy) vs.  $\mathrm{UE}_\text{MCQ}$ (MMLU) and (b)   $\mathrm{UE}_\text{Open-QA}$ (ES) vs.  $\mathrm{UT}_\text{Open-QA}$ (GSM8K). Solid lines link each method before and after 4-bit weight quantization; markers with black diagonal hatching denote quantized models.
}}
\label{fig:rob_weight_quantization}
\vspace*{-2mm}
% \end{wrapfigure}
\end{figure}




\noindent \textbf{Robustness against quantization.}  Quantization is another form of model-level attack after unlearning. Unlike in-domain relearning or out-of-domain fine-tuning, it does not introduce new knowledge but still alters model parameters and can affect robustness \citep{zhang2024catastrophic}. Overly aggressive compression (\textit{e.g.}, with very few quantization bits) may degrade the unlearned model’s overall capability, making it unable to answer forget queries. This can create the \textit{illusion} of improved unlearning performance, a false robustness gain that simply reflects model incapacity. Thus, as a first principle, robustness under quantization should therefore be evaluated with the full UE–UT tradeoff. 




To this end, \textbf{Fig.\,\ref{fig:rob_weight_quantization}(a)} shows $\mathrm{UE}_\text{MCQ}$ (Accuracy) versus $\mathrm{UT}_\text{MCQ}$ (MMLU) before and after quantization. For {\MRep} and {\MRej} methods, the quantized models (markers with black diagonal hatching) exhibit declines in both $\mathrm{UT}_\text{MCQ}$ and $\mathrm{UE}_\text{MCQ}$, with the drop in $\mathrm{UT}_\text{MCQ}$ being substantially larger. In contrast, {\MDiv} methods remain largely unaffected by quantization. \textbf{Fig.\,\ref{fig:rob_weight_quantization}(b)} presents $\mathrm{UE}_\text{Open-QA}$ (ES) versus $\mathrm{UT}_\text{Open-QA}$ (GSM8K) before and after quantization, showing trends consistent with Fig.\,\ref{fig:rob_weight_quantization}(a). In addition, another interesting finding is that knowledge removal (\textit{e.g.}, WMDP) is generally more robust to post-unlearning quantization than data-centric unlearning (\textit{e.g.}, MUSE for content removal). See Table\,\ref{tab:quantization_results} in \textbf{Appendix\,\ref{appx:add_exp}} for details.


\iffalse 
In addition, another interesting finding is that knowledge removal (\textit{e.g.}, on WMDP) is generally more robust to post-unlearning quantization than data-centric  unlearning (\textit{e.g.}, MUSE for content deletion). 
\SL{In WMDP, knowledge removal is evaluated using forget queries that are broad and not tied to exact training samples.} \SL{[correct?]} \CF{correct} By contrast, data-wise unlearning in MUSE targets specific textual information memorized during fine-tuning on MUSE (before unlearning), making it more sensitive to quantization.  
\textbf{Fig.\,\ref{fig:rob_weight_quantization} (c) and (d)} compares quantization robustness between WMDP and MUSE. For WMDP, we use GSM8K to measure $\mathrm{UT}$ and WMDP Bio accuracy to measure $\mathrm{UE}$. For MUSE, we use KnowMem on $\Dr$ for $\mathrm{UT}$ and KnowMem on $\Df$ for $\mathrm{UE}$. As we can see, quantization leads to a drop in utility for both WMDP and MUSE, especially under 4-bit settings. However, the change in WMDP Bio accuracy is much smaller than that of KnowMem on $\Df$ in MUSE, indicating that knowledge removal is generally more robust to post-unlearning quantization than data-centric removal.
\fi 

% Quantization reduces the storage and computational costs of large language models by mapping high-precision parameters into a discrete range without modifying the model architecture \citep{hong2025stablequant,zheng2025empirical}. 
% \citet{zhang2024catastrophic} were the first to investigate the effect of quantization on unlearned models in the context of the MUSE benchmark, showing that quantization can be regarded as a form of model-level attack. By lowering numerical precision, quantization may potentially cause an unlearned model to recover parts of the knowledge that was intended to be forgotten.

% We further evaluate the robustness of unlearned models under 4-bit quantization, using four metrics: $\mathrm{UE}_\text{MCQ}$ (accuracy), $\mathrm{UE}_\text{Open-QA}$ (entailment score), $\mathrm{UT}_\text{MCQ}$ (MMLU), and $\mathrm{UT}_\text{Open-QA}$ (GSM8K). As shown in \textbf{Fig.~\ref{fig:robust_wmdp}(b) and (c)}, quantization generally improves unlearning effectiveness (lower UE values) but degrades utility. The impact is more pronounced for {\MRep} methods than for {\MDiv} methods. Notably, methods such as RR are not robust to quantization, as their UE values actually increase after quantization.


% 1. On the WMDP benchmark, quantization has little impact on evaluation outcomes, as all unlearning methods remain robust regardless of whether performance is measured using MCQ or Open-QA tasks. 2. In contrast, on the MUSE benchmark, quantization is effective against only a subset of methods, such as NPO and GradDiff. 3. To further explain this phenomenon, we analyze the overlap in the distribution of the top-50 tokens, which highlights how quantization shifts prediction distributions in a way that selectively affects certain approaches.

% \begin{wrapfigure}{r}{0.4\textwidth}
%     \centering
%     \vspace{-4mm}
%     \includegraphics[width=0.38\textwidth]{figs/heatmap_robustness.pdf}
%     \caption{WMDP evaluation accuracy of twelve unlearning methods on WMDP Bio with Llama-3 8B Instruct before and after in-domain relearning (IDR), out-of-domain fine-tuning (ODF on GSM8K), quantization (Quan), and input-level jailbreaking attacks (Jail). An asterisk (*) denotes the presence of a robust design.
%     \SL{[can we show to indicate correlation betwene model-level and input-level robustness? Please be careful of notations.]}
%     }
%     \label{fig:rob_input}
% \end{wrapfigure}


% \begin{wrapfigure}{r}{0.6\textwidth} % r=靠右，宽度可调
% \vspace*{-6mm}
% \centering
% \begin{tabular}{cc}
% \includegraphics[width=0.32\textwidth]{figs/heatmap_robustness.pdf} &
% \hspace*{-4mm}
% \includegraphics[width=0.27\textwidth]{figs/line_idr_odf_pair.pdf} \\
% \small{(a) $\mathrm{UE}_\text{MCQ}$ (Accuracy $\downarrow$)} &
% \small{(b) Correlation of $\mathrm{Rob}_\text{JA}$} 
% \end{tabular}


\begin{figure}[htb]
%\begin{wrapfigure}{r}{0.5\textwidth} % r=靠右，宽度可调
\vspace*{-1mm}
\centering
\begin{tabular}{cc}
\includegraphics[width=0.3\textwidth]{figs/heatmap_robustness.pdf} &
%\hspace*{-4mm}
\includegraphics[width=0.26\textwidth]{figs/line_idr_odf_pair.pdf} \\
\small{(a) $\mathrm{UE}_\text{MCQ}$ (Accuracy $\downarrow$)} &
\small{(b) Correlation of $\mathrm{Rob}_\text{JA}$} 
\end{tabular}
%\vspace{-2mm}
\caption{\small{
(a) Overall robustness of 12 unlearning methods on WMDP with Llama-3 8B Instruct, including in-domain relearning ($\mathrm{Rob}_\text{ReL}$), out-of-domain fine-tuning ($\mathrm{Rob}_\text{FT}$), quantization ($\mathrm{Rob}_\text{QT}$), and jailbreaking ($\mathrm{Rob}_\text{JA}$) evaluated by  $\mathrm{UE}_\text{MCQ}$ (Accuracy) (b) Correlations between $\mathrm{Rob}_\text{JA}$ and $\mathrm{Rob}_\text{ReL}$ / $\mathrm{Rob}_\text{FT}$.
% Overall robustness: in-domain relearning ($\mathrm{Rob}_\text{ReL}$), out-of-domain fine-tuning ($\mathrm{Rob}_\text{FT}$), quantization ($\mathrm{Rob}_\text{QT}$), and input-level jailbreaking attacks ($\mathrm{Rob}_\text{JA}$) for unlearning methods on WMDP under Llama-3 8B Instruct. Evaluation is based on $\mathrm{UE}_\text{MCQ}$ (Accuracy) in sub-figure (a), the correlation between $\mathrm{Rob}_\text{JA}$ and $\mathrm{Rob}_\text{ReL}$ / $\mathrm{Rob}_\text{FT}$ in (b).
}}
\label{fig:robustness_all}
\vspace*{-1mm}
%\end{wrapfigure}
\end{figure}

\noindent \textbf{Robustness against jailbreaking and its interaction with model-level robustness.
}  
Beyond model-level perturbations, unlearned models are also vulnerable to input-level \textit{jailbreaking attacks} \citep{lucki2024adversarial, lynch2024eight, patil2023can}, which manipulate prompts to bypass unlearning. Next, we examine whether current unlearning methods provide comparable robustness to both model-level and input-level attacks, and how these two robustness dimensions interact. Robustness against jailbreaking attacks is denoted as $\mathrm{Rob}_\text{JA}$, with adversarial prompts generated using  the enhanced GCG \citep{lucki2024adversarial}. 

%\SL{[I stop here.]}


As illustrated in \textbf{Fig.\,\ref{fig:robustness_all} (a)},  without explicit robust design, {\MDiv} methods (GradDiff, NPO, SimNPO) generally demonstrate stronger $\mathrm{Rob}_\text{JA}$ compared with {\MRep} methods. This may be attributed to the degraded generative capacity of {\MDivB} methods, which inadvertently hinders their ability to reveal sensitive knowledge. For {\MRej} methods, $\mathrm{Rob}_\text{JA}$ varies considerably: ELM shows almost no robustness, whereas IDK+AP remains  robust with nearly no degradation. When robust design is incorporated, most methods (except RMU+LAT) exhibit significantly enhanced resilience against jailbreaking attacks. 

Moreover, \textbf{Fig.\,\ref{fig:robustness_all}(b)} jointly presents the relationship between input-level robustness ($\mathrm{Rob}_\text{JA}$, learned input perturbations) and model-level robustness ($\mathrm{Rob}_\text{ReL}$ and $\mathrm{Rob}_\text{FT}$, learned weight perturbations). The results indicate that $\mathrm{Rob}_\text{JA}$ patterns align more closely with $\mathrm{Rob}_\text{ReL}$ than with $\mathrm{Rob}_\text{FT}$. This positive correlation arises because both $\mathrm{Rob}_\text{JA}$ and $\mathrm{Rob}_\text{ReL}$ correspond to worst-case adversarial testing, with attack primarily activated by forget data in the unlearned domain.  


%In contrast, $\mathrm{Rob}_\text{FT}$ also depends on the generalization of retained representations, making it less correlated with $\mathrm{Rob}_\text{JA}$.


% \begin{wrapfigure}{r}{0.7\textwidth} % r=靠右，宽度可调
% \vspace*{-2mm}
% \centering
% \begin{tabular}{ccc}
% \includegraphics[width=0.25\textwidth]{figs/heatmap_robustness.pdf} &
% \hspace*{-4mm}
% \includegraphics[width=0.22\textwidth]{figs/line_idr.pdf} &
% \hspace*{-4mm}
% \includegraphics[width=0.22\textwidth]{figs/line_odf.pdf} \\
% \small{(a) Robustness} &
% \small{(b) $\mathrm{Rob}_\text{JA}$ vs. $\mathrm{Rob}_\text{ReL}$} &
% \small{(c) $\mathrm{Rob}_\text{JA}$ vs. $\mathrm{Rob}_\text{FT}$} \\
% \end{tabular}

% \vspace{-2mm}
% \caption{\small{
% Full spectrum of unlearning robustness: in-domain relearning ($\mathrm{Rob}_\text{ReL}$), out-of-domain fine-tuning ($\mathrm{Rob}_\text{FT}$), quantization ($\mathrm{Rob}_\text{QT}$), and input-level jailbreaking attacks ($\mathrm{Rob}_\text{JA}$) for unlearning methods on WMDP under Llama-3 8B Instruct. Evaluation is based on $\mathrm{UE}_\text{MCQ}$ (Accuracy) in sub-figure (a), the relationship between $\mathrm{Rob}_\text{JA}$ and $\mathrm{Rob}_\text{ReL}$ in (b), and between $\mathrm{Rob}_\text{JA}$ and $\mathrm{Rob}_\text{FT}$ in (c).
% }}
% \label{fig:robustness_all}
% \vspace*{-4mm}
% \end{wrapfigure}



% \begin{figure}[htb]
% \centering
% \begin{tabular}{cccc}
% \includegraphics[width=0.27\textwidth]{figs/heatmap_robustness.pdf} &
% \hspace*{-4mm}
% \includegraphics[width=0.23\textwidth]{figs/line_idr.pdf} &
% \hspace*{-4mm}
% \includegraphics[width=0.23\textwidth]{figs/line_odf.pdf} &
% \hspace*{-4mm}
% \includegraphics[width=0.23\textwidth]{figs/line_quan.pdf} \\
% \small{(a) Robustness} &
% \small{(b) $\mathrm{Rob}_\text{JA}$ vs. $\mathrm{Rob}_\text{ReL}$} &
% \small{(c) $\mathrm{Rob}_\text{JA}$ vs. $\mathrm{Rob}_\text{FT}$} &
% \small{(d) $\mathrm{Rob}_\text{JA}$ vs. $\mathrm{Rob}_\text{QT}$} \\
% \end{tabular}

% \vspace{-2mm}
% \caption{\small{
% Full spectrum of unlearning robustness: in-domain relearning ($\mathrm{Rob}_\text{ReL}$), out-of-domain fine-tuning ($\mathrm{Rob}_\text{FT}$), quantization ($\mathrm{Rob}_\text{QT}$), and input-level jailbreaking attacks ($\mathrm{Rob}_\text{JA}$) for unlearning methods on WMDP under Llama-3 8B Instruct. Evaluation is based on $\mathrm{UE}_\text{MCQ}$ (Accuracy) in sub-figure (a), the relationship between $\mathrm{Rob}_\text{JA}$ and $\mathrm{Rob}_\text{ReL}$ in (b), and between $\mathrm{Rob}_\text{JA}$ and $\mathrm{Rob}_\text{FT}$ in (c).  
% \SL{[remove (d) vs. QT]} \SL{[You can also consider to split this figure into two figures (a) vs. (b)-(c) and use wrapfigures.]}
% }}
% \label{fig:robustness_all}
% % \vspace*{-4mm}
% \end{figure}


% \begin{wrapfigure}{r}{0.6\textwidth} % r=右侧, 宽度约 70% 版面
% \vspace*{-2mm}

% \begin{tabular}{ccc}
% \hspace*{-3mm}
% \includegraphics[width=0.19\textwidth,height=!]{figs/line_idr.pdf} 
% &
% \hspace*{-6mm}
% \includegraphics[width=0.19\textwidth,height=!]{figs/line_odf.pdf}
% &
% \hspace*{-6mm}
% \includegraphics[width=0.19\textwidth,height=!]{figs/line_quan.pdf}
% \vspace*{-1mm}
% \\
% \small{(a) $\mathrm{Rob}_\text{JA}$ vs. $\mathrm{Rob}_\text{ReL}$} &
% \small{(b) $\mathrm{Rob}_\text{JA}$ vs. $\mathrm{Rob}_\text{FT}$} &
% \small{(c) $\mathrm{Rob}_\text{JA}$ vs. $\mathrm{Rob}_\text{QT}$} \\
% \end{tabular}

% \caption{\small{Relationship between $\mathrm{Rob}_\text{JA}$ and other robustness metrics.}}
% \label{fig:robustness}
% \vspace*{-5mm}
% \end{wrapfigure}


% \begin{wrapfigure}{r}{0.4\textwidth}
%     \centering
%     \vspace{-4mm}
%     \includegraphics[width=0.38\textwidth]{figs/heatmap_robustness.pdf}
%     \caption{WMDP evaluation accuracy of twelve unlearning methods on WMDP Bio with Llama-3 8B Instruct before and after in-domain relearning (IDR), out-of-domain fine-tuning (ODF on GSM8K), quantization (Quan), and input-level jailbreaking attacks (Jail). An asterisk (*) denotes the presence of a robust design.
%     \SL{[can we show to indicate correlation betwene model-level and input-level robustness? Please be careful of notations.]}
%     }
%     \label{fig:}
% \end{wrapfigure}



% \noindent \textbf{Interactions between model-level and input-level attacks.} 
% Model-level and input-level attacks are complementary in how they probe vulnerabilities of unlearned models. 
% To visualize their relationship, use heatmaps comparing the outcomes of model-level attacks with those of input-level attacks.


% Input-level attack:
% 1. Jailbreaking attack 
% 2. In-context learning

% Tadeoff of robustnedd and UE

% The relationship between model-level and inpu-level attack: heatmap
