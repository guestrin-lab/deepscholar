\section{Beyond Answer Selection: Rethinking Unlearning Evaluation for Effectiveness and Utility}
\label{sec:eval_UE_UT}


\begin{tcolorbox}[enhanced,attach boxed title to top center={yshift=-3mm,yshifttext=-1mm},
  colback=white,colframe=green!35!black,colbacktitle=green!5!white,
  title={\parbox{0.7\linewidth}{\centering Summary of insights into  unlearning effectiveness and utility retention}}, coltitle=black, boxrule=0.8pt, 
  boxed title style={size=small,colframe=green!35!black} ]
  
%  \textbf{(1)} Unlearning evaluation should \textit{extend beyond answer selection} (\textit{i.e.}, choosing the option with highest probability in WMDP) to also examine the \textit{generated content} (\textit{i.e.}, answer generation) of the unlearned model across both UE and UT dimensions.  

% \textbf{(2)} Divergence-driven optimization methods (Table\,\ref{tab:taxonomy}) often \textit{over-forget}, as the resulting unlearned model tends to break its generation ability on forget queries rather than issuing ``refusals'' or ``random answers''. In contrast, representation-misalignment methods generally better preserve generation capability when assessed on both UE and UT, as revealed by prediction logit distributions.   

% \textbf{(3)} A fundamental tradeoff exists between UE and UT, but it may be obscured when relying solely on answer selection-based MCQ evaluations. Over-forgetting, in particular, undermines utility, especially on Open-QA tasks that depend on generated content.  

\textbf{(1)} Unlearning evaluation should \textit{go beyond answer selection} (\textit{i.e.}, highest-probability choice in WMDP) to assess actual generation content in both UE and UT.  

\textbf{(2)} {\textcolor{CDiv}{\textit{Divergence-driven optimization}}} methods often \textit{over-forget}, breaking generation on forget queries. In contrast, {\MRep} methods better preserve generation.  

\textbf{(3)} For {\MRej}, evaluating beyond answer selection is essential to capture UE. Moreover, aggressive fine-tuning on rejection targets can harm UT.

\textbf{(4)} UE-UT \textit{tradeoffs} are fundamental but often hidden by MCQ-based evaluation. Over-forgetting especially degrades utility on Open-QA tasks that rely on generated content.  

% \SL{(4) Contrast with rejection?}
\end{tcolorbox}

In this section, we revisit unlearning evaluation across the UE (unlearning effectiveness) and UT (utility retention) dimensions for the methodological families in Table\,\ref{tab:taxonomy}. We contrast answer selection-based evaluations (\textit{i.e.}, MCQ tasks) with content-based evaluations (\textit{i.e.}, Open-QA tasks). Recall that in MCQ settings, the model selects the option with the highest predicted score, whereas Open-QA requires free-form generation. 
%Unless noted otherwise, we adopt WMDP as the default benchmark. Our analysis will highlight the importance of generation-based evaluation for UE and UT, the over-forgetting issue and its causes in divergence-driven optimization, the inherent UE–UT tradeoff, and comparisons with rejection-based unlearning methods. 
At the start of this section, we summarized the key insights.


% %\begin{wrapfigure}{r}{0.4\textwidth}
% \begin{figure}[htb]
%     \centering
%     %\vspace{-4mm}
%     \includegraphics[width=0.5\textwidth]{example-image-a}
%     \caption{\SL{TBD: answer selection vs. generation examples; Please read my texts. Or consider using wrapfigure if needed.} \CF{TODO: Add NPO and Original on WMDP and GSM8K.}}
%     \label{fig:selection_generation}
% %\end{wrapfigure}
% \end{figure}

% \input{tabs/example}

\noindent \textbf{Open-QA as a crucial lens for UE and UT evaluation.}
In LLM unlearning on WMDP, UE is typically measured by \textit{accuracy} on the WMDP evaluation set, assessed through MCQ where success is judged by selecting the correct option (\textit{e.g.}, A/B/C/D). Likewise, most unlearning benchmarks (including WMDP) evaluate UT using \textit{MMLU} tasks, which also follow the MCQ format. The main limitation of relying solely on MCQ-based evaluations is that they fail to capture the model’s actual generated capabilities after unlearning, leading to a false sense of unlearning success and obscuring the true rationale and quality of unlearning.

In \textbf{Table\,\ref{tab:selection_generation}} of \textbf{Appendix\,\ref{appx:add_exp}}, we illustrate the distinction between answer selection and answer generation using NPO/RMU-unlearned Llama-3 8B Instruct on WMDP, evaluated against a forget-relevant query. From the answer selection perspective, the unlearned model (whether NPO or RMU) selects an \textit{incorrect} option (\textit{i.e.}, option $D$, differing from the original model’s choice), indicating successful unlearning on WMDP. However, from an answer generation perspective, the model produces \textit{nonsensical text} instead of valid answer choices, revealing that it has internally disrupted its generation ability for forget queries. This also raises concerns of \textit{over-forgetting}, as the degradation may also impair generation on non-forget inputs, which would not be captured by MMLU.

To capture the perspective of answer generation, we propose using Open-QA for evaluating both UE and UT. For UE, we adopt the \textit{entailment score} (\textbf{ES}) \citep{yuan2024closer, yao2024accurate, poliak2020survey}, which measures the factual consistency of model outputs against the original pre-unlearned model’s response (\textit{i.e.}, the correct answer). A higher ES indicates that the unlearned model can still infer the correct information when queried with forget data. To improve ES compatibility with the unlearned model’s output format, we integrate few-shot examples (with the desired answer style) into the forget data prompts to guide generation toward the correct format during evaluation; see \textbf{Appendix\,\ref{appx:exp_setup}} for details.
For UT, we recommend incorporating Open-QA tasks, such as \textbf{IFEval} \citep{zhou2023instruction}, \textbf{GSM8K} \citep{cobbe2021training}, alongside MCQ tasks including \textbf{MMLU} \citep{hendrycks2020measuring}, \textbf{MathQA} \citep{amini2019mathqa}, \textbf{TruthfulQA} \citep{lin2021truthfulqa}. Adding these benchmarks enables a more complete evaluation of utility. In particular, IFEval captures instruct-following ability, and GSM8K measures quantitative reasoning. 
%MathQA focuses on symbolic and arithmetic reasoning, and TruthfulQA tests resistance to producing false or misleading statements. 
Together they provide a balanced view of how unlearning affects model utility.

\begin{figure}[htbp]
\vspace*{-1mm}
\center
% \hspace*{50mm}
\includegraphics[width=0.8\linewidth]{figs/2d_ue_ut_legend.pdf}\\
\begin{tabular}{ccc}
\includegraphics[width=0.32\linewidth,height=!]{figs/2d_ue.pdf} 
&
\includegraphics[width=0.30\linewidth,height=!]{figs/heatmap_ut.pdf}
&
% \hspace*{-6mm}
\includegraphics[width=0.31\linewidth,height=!]{figs/2d_ue_ut.pdf}
\vspace*{-1mm}
\\
\hspace*{3mm}

% \hspace*{-3mm}
{\small (a) $\mathrm{UE}_\text{MCQ}$ vs. $\mathrm{UE}_\text{Open-QA}$} &
{\small (b) $\mathrm{UT}_\text{MCQ}$ and $\mathrm{UT}_\text{Open-QA}$} ($\uparrow$) &
% \hspace*{-3mm}
{\small (c) $\mathrm{UE}_\text{Avg}$ vs. $\mathrm{UT}_\text{Avg}$} \\
\end{tabular}
\vspace*{-2mm}
\caption{\small{    
Unlearning effectiveness (UE) and utility retention (UT) evaluation of unlearning methods on WMDP with Llama-3 8B
Instruction. 
(a) $\mathrm{UE}_\text{MCQ}$ denotes accuracy on the WMDP evaluation set, and $\mathrm{UE}_\text{Open-QA}$ denotes ES on the WMDP evaluation set. The arrow direction along each axis indicates the direction of better performance. (b) $\mathrm{UT}_\text{MCQ}$ includes \textcolor{MCQ}{MMLU, TruthfulQA, and MathQA}, while $\mathrm{UT}_\text{Open-QA}$ includes \textcolor{Open-QA}{IFEval and GSM8K}. 
(c) $\mathrm{UT}_\text{Avg}$ is defined as the mean of $\mathrm{UT}_\text{MCQ}$ and $\mathrm{UT}_\text{Open-QA}$, and 
$\mathrm{UE}_\text{Avg}$ is defined analogously.
}
}
\label{fig:evaluatio_wmdp}
\vspace*{-1mm}
\end{figure}

In \textbf{Fig.\,\ref{fig:evaluatio_wmdp}-(a)}, we present the $\mathrm{UE}_\text{MCQ}$ and $\mathrm{UE}_\text{Open-QA}$ of 12 unlearning methods along with the original model. As shown, for RMU and NPO, even when the unlearned models achieve the same $\mathrm{UE}_\text{MCQ}$, their $\mathrm{UE}_\text{Open-QA}$ can differ significantly. This highlights the necessity of jointly measuring both $\mathrm{UE}_\text{MCQ}$ and $\mathrm{UE}_\text{Open-QA}$. Moreover, we observe that {\MDiv} generally achieves better $\mathrm{UE}_\text{MCQ}$ and $\mathrm{UE}_\text{Open-QA}$ compared with other families.


% In \textbf{Fig.\,\ref{fig:evaluatio_wmdp}-(b)}, we present the $\mathrm{UT}_\text{MCQ}$ and $\mathrm{UT}_\text{Open-QA}$ of 12 unlearning methods along with the original model. As we can see, the $\mathrm{UT}_\text{MCQ}$ on MMLU, TruthfulQA, and MathQA is substantially lower than the $\mathrm{UT}_\text{Open-QA}$ on IFEval and GSM8K, indicating that using only MMLU to measure UT in WMDP is far from sufficient. Moreover, {\MRep} significantly outperforms {\MDiv} on UT, indicating that the latter suffers from over-forgetting which leads to substantial utility loss. Furthermore, compared with RMU, TAR shows a marked decline in $\mathrm{UT}_\text{Open-QA}$, indicating that adding robustness to RMU comes at the expense of utility in Open-QA. In \textbf{Fig.\,\ref{fig:bar_logits}} of \textbf{Appendix\,\ref{appx:add_exp}}, analysis of generation logits shows that NPO achieves unlearning by collapsing logits and inducing over-forgetting, which ultimately impairs generation ability. 

In \textbf{Fig.\,\ref{fig:evaluatio_wmdp}-(b)}, we present the $\mathrm{UT}_\text{MCQ}$ and $\mathrm{UT}_\text{Open-QA}$ of 12 unlearning methods along with the original model. As shown, although {\MDiv} (e.g., NPO) achieves a similar $\mathrm{UT}_\text{MCQ}$ as {\MRep} (e.g., RMU), its $\mathrm{UT}_\text{Open-QA}$ is much lower, indicating that NPO over-forgets and thereby reduces its generation capability. Furthermore, compared with RMU, TAR shows a marked decline in $\mathrm{UT}_\text{Open-QA}$, indicating that adding robustness to RMU comes at the expense of utility in Open-QA. In \textbf{Fig.\,\ref{fig:bar_logits}} of \textbf{Appendix\,\ref{appx:add_exp}}, analysis of logits shows that NPO achieves unlearning by collapsing logits and inducing over-forgetting, which ultimately impairs generation ability. 

In \textbf{Fig.\,\ref{fig:evaluatio_wmdp}-(c)}, we report $\mathrm{UE}_\text{Avg}$ and $\mathrm{UT}_\text{Avg}$. Here, $\mathrm{UT}_\text{Avg}$ is computed as the average of $\mathrm{UT}_\text{MCQ}$ and $\mathrm{UT}_\text{Open-QA}$, and $\mathrm{UE}_\text{Avg}$ is defined analogously. A higher $\mathrm{UT}_\text{Avg}$ indicates better utility, while a lower $\mathrm{UE}_\text{Avg}$ reflects stronger unlearning effectiveness. When considering UE and UT jointly, {\MRep} generally outperforms {\MRej}, which in turn outperforms {\MDiv}. Within these families, RMU is the strongest under {\MRepB}, DPO under {\MRejB}, and SimNPO under {\MDivB}.  


% \begin{wrapfigure}{r}{0.3\linewidth}
%     \centering
%     \hspace{4mm}
%     \includegraphics[width=0.7\linewidth]{figs/bar_logits_legend.pdf}
%     \includegraphics[width=1.0\linewidth]{figs/bar_logits_freq.pdf}
%     \caption{ABCD and top4 logits of the original (Llama-3 8B Instruct), NPO unlearned and RMU unlearned model on the WMDP evaluation set.}
%     \label{fig:bar_logits}
%     %\vspace{-3mm}
% \end{wrapfigure}


% \noindent \textbf{From logits to behavior: Over-forgetting in divergence-driven unlearning.}
% As indicated by Fig.\,\ref{fig:evaluatio_wmdp}-(c), {\MDivB} is prone to over-forgetting on Open-QA tasks. To further investigate this limitation, we compare the prediction logit distributions of the unlearned models (obtained by NPO and RMU) with the original pre-unlearned model over the answer options (A/B/C/D) and their top-4 predictions.  
% \textbf{Fig.\,\ref{fig:bar_logits}} illustrates how prediction logit distributions differ across unlearning methods on WMDP, comparing the answer options with each model’s top-4 predicted tokens.  

% From the perspective of ABCD logits, NPO drives all four options close to zero and nearly identical, achieving $\mathrm{UE}_\text{MCQ}$ by uniformly suppressing candidate scores. In other words, ABCD are not true top-token candidates under NPO, as revealed by its top-4 prediction logits on ABCD. By contrast, RMU maintains the distribution of the original model’s logits but reshapes their relative distribution, attaining $\mathrm{UE}_\text{MCQ}$ by reordering rather than erasing signals. For the top-4 logits, NPO assigns much higher values than RMU or the original model, but these correspond to meaningless tokens (Table\,\ref{tab:selection_generation}). This shows that NPO achieves $\mathrm{UE}_\text{Open-QA}$ by severely disrupting generative capacity, explaining its substantially lower $\mathrm{UT}_\text{Open-QA}$ relative to RMU and the original model.  



% To further investigate the unlearning mechanism of {\MDivB}, in \textbf{Fig.\,\ref{fig:bar_logits}} we plot the ABCD logits and the top-4 logits of the original model, the NPO unlearned model, and the RMU unlearned model on the WMDP evaluation set.  

% For the ABCD logits, we observe that in NPO the logits of all four options are close to zero and nearly identical, indicating that NPO achieves $\mathrm{UE}_\text{MCQ}$ by uniformly suppressing the logits of all candidates. In contrast, the RMU logits for ABCD preserve the magnitude of the original model but alter their relative distribution, suggesting that RMU attains $\mathrm{UE}_\text{MCQ}$ by reshaping the internal ranking among the options rather than eliminating their signal. For the top-4 logits, NPO produces values much larger than those of the original and RMU, yet these correspond to meaningless tokens (e.g., \texttt{@nate}, \texttt{@js}, \texttt{styleType}, \texttt{\_vlog}). This indicates that NPO attains $\mathrm{UE}_\text{Open-QA}$ by severely disrupting the model’s generative capacity, which explains why its $\mathrm{UT}_\text{Open-QA}$ is substantially lower than that of both RMU and the original model.



% \noindent \textbf{Rethinking rejection-based methods.} Compared to {\MDivB} and {\MRepB}, the {\MRej} family is less common in LLM unlearning, though its idea is simple: outputting rejections for forget-relevant queries. We highlight three insights. \textit{First}, as shown in Fig.\,\ref{fig:evaluatio_wmdp}-(a), rejection-based methods (ELM, DPO, IDK+AP) yield much higher $\mathrm{UE}_\text{MCQ}$, 说明他们并不能很好实现 unlearn. Yet under $\mathrm{UE}_\text{Open-QA}$ their performance diverges, with DPO achieving the best results. \textit{Second}, Fig.\,\ref{fig:evaluatio_wmdp}-(b) shows DPO’s MMLU utility is not clearly better than {\MDivB}, reinforcing the belief it has no UT advantage. However, DPO’s $\mathrm{UT}_\text{Open-QA}$ (IFEval, GSM8k) remains close to the original model, showing its potential. \textit{Third}, although IDK+AP and DPO both aim for rejection responses, Fig.\,\ref{fig:evaluatio_wmdp}-(c) shows that IDK+AP yields worse UT, owing to its stricter log-likelihood loss design. In \textbf{Fig.\,\ref{fig:idk_ap_dpo}} of \textbf{Appendix\,\ref{appx:add_exp}}, we validate this insight and propose a mitigation for IDK+AP by leveraging the strengths of DPO.


% \noindent \textbf{Rethinking rejection-based methods.} Compared to {\MDivB} and {\MRepB}, the {\MRej} family is less common in LLM unlearning, though its idea is simple: outputting rejections for forget-relevant queries. We highlight three insights. \textit{First}, as shown in Fig.\,\ref{fig:evaluatio_wmdp}-(a), rejection-based methods (ELM, DPO, IDK+AP) yield much higher $\mathrm{UE}_\text{MCQ}$, explaining their limited popularity. Yet under $\mathrm{UE}_\text{Open-QA}$ their performance diverges, with DPO achieving the best results. \textit{Second}, Fig.\,\ref{fig:evaluatio_wmdp}-(b) shows DPO’s MMLU utility is not clearly better than {\MDivB}, reinforcing the belief it has no UT advantage. However, DPO’s $\mathrm{UT}_\text{Open-QA}$ (IFEval, GSM8k) remains close to the original model, showing its potential. \textit{Third}, although IDK+AP and DPO both aim for rejection responses, Fig.\,\ref{fig:evaluatio_wmdp}-(c) shows that IDK+AP yields worse UT, owing to its stricter log-likelihood loss design. In \textbf{Fig.\,\ref{fig:idk_ap_dpo}} of \textbf{Appendix\,\ref{appx:add_exp}}, we validate this insight and propose a mitigation for IDK+AP by leveraging the strengths of DPO.

\noindent \textbf{Rethinking rejection-based methods.} Compared to {\MDivB} and {\MRepB}, the {\MRej} family is less commonly used in LLM unlearning. Below, we highlight several overlooked insights. \textit{First}, as shown in Fig.\,\ref{fig:evaluatio_wmdp}-(a), {\MRejB} exhibit significantly lower $\mathrm{UE}_\text{MCQ}$ compared to others. This performance gap is one of the primary reasons for their limited popularity in LLM unlearning \citep{li2024wmdp,shi2024muse}. However, this view may be myopic, as under $\mathrm{UE}_\text{Open-QA}$ the performance of rejection-based methods varies much more substantially, with DPO achieving the best $\mathrm{UE}_\text{Open-QA}$ among them. \textit{Second}, if we only consider the conventional UT assessment under MMLU in Fig.\,\ref{fig:evaluatio_wmdp}-(b), DPO does not exhibit clearly better MMLU accuracy than {\MDivB} methods. Consequently, the prevailing understanding in the literature has been that DPO offers \textit{no advantage} in terms of utility \citep{maini2024tofu,zhang2024negative}. However, this view is also myopic, as DPO's $\mathrm{UT}_\text{Open-QA}$ metrics (IFEval and GSM8k) remain comparable to those of the original model. \textit{Third}, while IDK+AP and DPO share the objective of eliciting rejection responses (\textit{e.g.}, ``I don't know'') to forget-relevant queries, IDK+AP shows markedly worse UT, as in Fig.\,\ref{fig:evaluatio_wmdp}-(c). We attribute this degradation to its stricter log-likelihood loss, which overly enforces rejection probabilities. In \textbf{Fig.\,\ref{fig:idk_ap_dpo}} of \textbf{Appendix\,\ref{appx:add_exp}}, we propose a mitigation for IDK+AP by leveraging the strengths of DPO.


\iffalse 
We revisit our earlier experiments and highlight key insights and observations.
First, the unlearning effectiveness of rejection-based methods can solely be captured by the $\mathrm{UE}_\text{Open-QA}$ metric. This is evidenced by Fig.\,\ref{fig:evaluatio_wmdp}-(a), where all three  rejection-based methods DPO, IDK+AP and ELM attain similar $\mathrm{UE}_\text{MCQ}$, but differ largely in $\mathrm{UE}_\text{Open-QA}$. Second, effective utility preservation in rejection-based methods is also revealed largely by $\mathrm{UT}_\text{Open-QA}$ metrics. For example, in Fig.\,\ref{fig:evaluatio_wmdp}-(b), while DPO achieves a MMLU accuracy closer to the divergence-driven optimization methods, $\mathrm{UT}_\text{Open-QA}$ metrics (IFEval and GSM8k) stays as good as the original model. Third, DPO and IDK+AP have similar targeted unlearning objectives, \textit{i.e.} both push the model to have similar rejection responses like `I don't know' to forget-relevant questions. However, IDK+AP achieves much worse unlearning performance than DPO (see Fig.\,\ref{fig:evaluatio_wmdp}-(c)). This motivates us to rethink this category of unlearning, which we present below. 
\fi 







% \SL{[@Soumyadeep, improve the following and make it direct and associated with evidence.]}
% %First, we re-emphasize the importance of using Open-QA for evaluation as evidenced from the rejection based unlearning methods. 
% As we observe in Fig.\,\ref{fig:evaluatio_wmdp}-(a), all three  rejection-based methods DPO, IDK+AP and ELM attain similar $\mathrm{UE}_\text{MCQ}$, whereas the effectiveness of unlearning is revealed  by the $\mathrm{UE}_\text{Open-QA}$ metric. 
% Consistent with this, we observe a similar phenomenon in utility preservation. While DPO achieves a MMLU accuracy closer to the divergence-driven optimization methods (\SL{see Fig.\,xxx?}), we observe the model's effectiveness in $\mathrm{UT}_\text{Open-QA}$ metrics (IFEval and GSM8k), where the model stays as good as the original model. 
% % In both Fig.\,\ref{fig:evaluatio_wmdp}-(a) and Fig.\,\ref{fig:evaluatio_wmdp}-(c), we observe that the unlearning method categories usually achieve 

% \begin{wrapfigure}{r}{0.4\linewidth}
%     \centering
%     %\vspace{-3mm}
%     \includegraphics[width=1.0\linewidth]{figs/idk_ap_dpo.pdf}
%     \caption{\SP{Not yet complete}}
%     \label{fig:idk_ap_dpo}
%     %\vspace{-3mm}
% \end{wrapfigure}


% Furthermore, despite having similar rejection-based targets like the rejection responses akin to `I don't know', we observe from  Fig.\,\ref{fig:evaluatio_wmdp}-(a) and Fig.\,\ref{fig:evaluatio_wmdp}-(c) that DPO and IDK+AP achieve very different unlearning results. More specifically, DPO retains a lot of the original model's utility, which we attribute to the presence of a positive preference. However, the degradation of utility observed in IDK+AP may be because of the stricter log-likelihood loss function which increases the likelihood of rejection responses, given forget-relevant questions. To this end, we unlearn our original model by providing a `warm-start' using DPO for a few epochs and then unlearning using IDK+AP. As seen in \textbf{Fig.\,\ref{fig:idk_ap_dpo}}, this strategy (called IDK+AP w/ DPO) can achieve \SP{tbd}. 
% We note this is in contrast to the  setting of post-training of LLMs \citep{dubey2024llama} where SFT is followed by DPO. We think that in unlearning, the rejection responses provide a strong distribution shift for IDK+AP which is managed by warm-starting with DPO.


% % \noindent \textbf{\SL{From logits to behavior: Over-forgetting in divergence-driven unlearning.}}
% % \SL{[Corresponding to insight 2]}
% % \CF{TODO: Use logits frequency and top-4 distribution. Put RMU example in the appendix.}



%\noindent \textbf{Redefine the trade-off between unlearning effectiveness and utility, and provide a ranking.}


% \SL{[Corresponding to insight 3]} 
% \CF{TODO: Add radius.}


% \noindent \textbf{\SL{[where is rejection?]} @Soumyadeep}


% \input{tabs/distance}

% To more systematically compare unlearning methods, we jointly consider four groups of metrics: 
% $\mathrm{UT}_\text{MCQ}$, $\mathrm{UT}_\text{Open-QA}$, $\mathrm{UE}_\text{MCQ}$, and $\mathrm{UE}_\text{Open-QA}$.  
% Inspired by the TOPSIS framework, we define an \textit{ideal unlearned model} as one whose $\mathrm{UT}_\text{MCQ}$ and $\mathrm{UT}_\text{Open-QA}$ remains identical to the original model, 
% while its $\mathrm{UE}_\text{MCQ}$ and $\mathrm{UE}_\text{Open-QA}$ align with the random baseline.  
% We then compute group-level averages to obtain $\mathrm{UT}_\text{MCQ}$, $\mathrm{UT}_\text{Open-QA}$, $\mathrm{UE}_\text{MCQ}$, and $\mathrm{UE}_\text{Open-QA}$, 
% and further calculate $\mathrm{UT}_\text{Avg}$ and $\mathrm{UE}_\text{Avg}$ as the mean of the two corresponding group-level scores. Finally, the Euclidean distance between each method and the ideal unlearned model provides a comprehensive criterion for evaluation.  

% As shown in \textbf{Fig.~\ref{fig:evaluatio_wmdp}(c)}, unlearning methods that lie closer to the ideal unlearned model achieve a more desirable trade-off between UE and UT. \textbf{Table~\ref{tab:distance_ranking}} reports the quantitative distances to the ideal unlearned model.  
% As expected, the original model lies farthest away, which is consistent with the definition of unlearning.  
% Among all approaches, RMU achieves the closest alignment, while methods under \RepM without robust design generally outperform those in \DivO without robust design. Within the \DivO\ family, SimNPO delivers the strongest results.


% \noindent \textbf{\SL{Rejection vs. non-rejection based unlearning.}}

% \input{tabs/example}


% \begin{table}[h!]
%   \centering
%   \caption{Comparison of answer prediction processes in MCQ (WMDP) and Open-QA (GSM8K) tasks.}
%   \label{tab:mcq_openqa_examples}
%   \renewcommand{\arraystretch}{1.3} % 增加行高
%   \resizebox{0.95\linewidth}{!}{%
%     \begin{tabular}{>{\centering\arraybackslash}p{0.15\linewidth} 
%                     >{\arraybackslash}p{0.42\linewidth} 
%                     >{\arraybackslash}p{0.42\linewidth}}
%       \toprule[1pt]
%       \toprule
%        & \textbf{MCQ (WMDP)} & \textbf{Open-QA (GSM8K)} \\
%       \midrule
%       \textbf{Question} &
%       In RNA virus genome synthesis, what potential risk is associated with obtaining virions solely from sequence information?  ... B.~Re-creating and engineering dangerous pathogens; ... D.~Accelerated biological research with minimal risks &
%       James decides to run 3 sprints 3 times a week. He runs 60 meters each sprint.  
%       How many total meters does he run a week? \\
%       \midrule
%       % \textbf{Answer generation process} &
%       % The model generates a single token and selects the option (A/B/C/D) with the highest predicted logit as the answer. &
%       % The model produces a complete free-form response, from which the final answer is extracted using rule-based criteria. \\
%       % \midrule
%       \rowcolor{gray!10} \textbf{Original Model} &
%       B &
%       James runs 3 sprints 3 times a week. So he runs $3 \times 3 = 9$ sprints in total.  
%       Each sprint is 60 meters, yielding $9 \times 60 = 540$ meters per week. \\
%       \textbf{NPO-Unlearned Model} &
%       D &
%       @nate@nate 3 sprints @-@ 3 times a week = 9 sprints a week the the the @@@@@@ \\
%       \bottomrule
%       \bottomrule[1pt]
%     \end{tabular}
%   }
% \end{table}



% \begin{table}[h!]
%   \centering
%   \caption{Comparison of answer prediction processes in MCQ (WMDP) and Open-QA (GSM8K) tasks.}
%   \label{tab:mcq_openqa_examples_transposed}
%   \renewcommand{\arraystretch}{1.3} % 增加行高
%   \resizebox{0.95\linewidth}{!}{%
%     \begin{tabular}{>{\centering\arraybackslash}m{0.18\linewidth} 
%                     >{\arraybackslash}m{0.38\linewidth} 
%                     >{\arraybackslash}m{0.38\linewidth}}
%       \toprule[1pt]
%       \toprule
%       & \textbf{MCQ (WMDP)} & \textbf{Open-QA (GSM8K)} \\
%       \midrule
%       \textbf{Question} &
%       In RNA virus genome synthesis, what potential risk is associated with obtaining virions solely from sequence information?  
%       ... B.~Re-creating and engineering dangerous pathogens; ... D.~Accelerated biological research with minimal risks &
%       James decides to run 3 sprints 3 times a week. He runs 60 meters each sprint.  
%       How many total meters does he run a week? \\
%       \midrule
%       \rowcolor{gray!10} \textbf{Original Model} &
%       B. Re-creating and engineering dangerous pathogens &
%       James runs 3 sprints 3 times a week. So he runs $3 \times 3 = 9$ sprints in total.  
%       Each sprint is 60 meters, yielding $9 \times 60 = 540$ meters per week. \\
%       \textbf{NPO-Unlearned Model} &
%       D. Accelerated biological research with minimal risks &
%       @nate@nate 3 sprints @-@ 3 times a week = 9 sprints a week the the the @@@@@@ \\
%       \bottomrule
%       \bottomrule[1pt]
%     \end{tabular}
%   }
% \end{table}


% \begin{table}[h!]
%   \centering
%   \caption{Comparison of answer prediction processes in MCQ (WMDP) and Open-QA (GSM8K).}
%   \label{tab:mcq_openqa_2x3}
%   \renewcommand{\arraystretch}{1.25}
%   \resizebox{0.95\linewidth}{!}{%
%     \begin{tabular}{>{\centering\arraybackslash}m{0.12\linewidth} 
%                     >{\arraybackslash}m{0.40\linewidth} 
%                     >{\arraybackslash}m{0.40\linewidth}}
%       \toprule[1pt]
%       \toprule
%       & \textbf{MCQ (WMDP)} & \textbf{Open-QA (GSM8K)} \\
%       \midrule
%       \textbf{Question} &
%       In RNA virus genome synthesis, what potential risk is associated with obtaining virions solely from sequence information? \newline
%       B.~Re-creating and engineering dangerous pathogens; \newline
%       D.~Accelerated biological research with minimal risks
%       &
%       James decides to run 3 sprints 3 times a week. He runs 60 meters each sprint. How many total meters does he run a week? \\
%       \midrule
%       \textbf{Original Model} &
%       \textbf{B.} Re-creating and engineering dangerous pathogens
%       &
%       James runs 3 sprints 3 times a week. So he runs $3 \times 3 = 9$ sprints in total. Each sprint is 60 meters, yielding $9 \times 60 = 540$ meters per week. \\
%       \rowcolor{gray!10} \textbf{NPO-Unlearned Model} &
%       \textbf{D.} Accelerated biological research with minimal risks
%       &
%       @nate@nate 3 sprints @-@ 3 times a week = 9 sprints a week the the the @@@@@@ \\
%       \bottomrule
%       \bottomrule[1pt]
%     \end{tabular}
%   }
% \end{table}


% \begin{table}[h!]
%   \centering
%   \caption{Comparison of MCQ (WMDP) and Open-QA (GSM8K) tasks, showing ground-truth answers and model predictions.}
%   \label{tab:mcq_openqa_with_gt}
%   \renewcommand{\arraystretch}{1.3}
%   \resizebox{0.95\linewidth}{!}{%
%     \begin{tabular}{>{\centering\arraybackslash}m{0.12\linewidth} 
%                     >{\arraybackslash}m{0.33\linewidth} 
%                     >{\centering\arraybackslash}m{0.1\linewidth} 
%                     >{\arraybackslash}m{0.40\linewidth}}
%       \toprule[1pt]
%       \toprule
%        & \textbf{Question} & \textbf{Ground Truth} & \textbf{Model Output} \\
%       \midrule
%       \textbf{MCQ (WMDP)} &
%       In RNA virus genome synthesis, what potential risk is associated with obtaining virions solely from sequence information?  
%       ... B.~Re-creating and engineering dangerous pathogens; ...  
%       D.~Accelerated biological research with minimal risks &
%       \textbf{B} &
%       \textbf{Original Model:} B. Re-creating and engineering dangerous pathogens \newline
%       \textbf{NPO-Unlearned Model:} D. Accelerated biological research with minimal risks \\
%       \midrule
%       \textbf{Open-QA (GSM8K)} &
%       James decides to run 3 sprints 3 times a week. He runs 60 meters each sprint.  
%       How many total meters does he run a week? &
%       \textbf{540} &
%       \textbf{Original Model:} James runs 3 sprints 3 times a week. So he runs $3 \times 3 = 9$ sprints in total.  
%       Each sprint is 60 meters, yielding $9 \times 60 = 540$ meters per week. \newline
%       \textbf{NPO-Unlearned Model:} @nate@nate 3 sprints @-@ 3 times a week = 9 sprints a week the the the @@@@@@ \\
%       \bottomrule
%       \bottomrule[1pt]
%     \end{tabular}
%   }
% \end{table}





% \begin{table}[h!]
%   \centering
%   \caption{Examples of answers from different methods on WMDP and GSM8K questions. \SL{Check Table 1 in \url{https://arxiv.org/pdf/2410.08109}}}
%   \label{tab:wmdp_gsm8k_examples}
%   \resizebox{0.95\linewidth}{!}{%
%     \begin{tabular}{>{\centering\arraybackslash}m{0.08\linewidth} 
%                     >{\centering\arraybackslash}m{0.45\linewidth} 
%                     >{\centering\arraybackslash}m{0.45\linewidth}}
%       \toprule[1pt]
%       \toprule
%        & \textbf{WMDP Question} & \textbf{GSM8K Question} \\
%       \midrule
%       \textbf{Question} &
%       In RNA virus genome synthesis, what potential risk is associated with obtaining virions solely from sequence information?  
%       ...  
%       B. Re-creating and engineering dangerous pathogens;  
%       ...
%       D. Accelerated biological research with minimal risks &
%       James decides to run 3 sprints 3 times a week. He runs 60 meters each sprint. How many total meters does he run a week? \\
%       \midrule
%       \rowcolor{gray!10} \textbf{Original} &
%       B. Re-creating and engineering dangerous pathogens &
%       James runs 3 sprints 3 times a week. So he runs $3 \times 3 = 9$ sprints a week.  
%       Each sprint is 60 meters, so he runs $9 \times 60 = 540$ meters a week. \\
%       \textbf{NPO} &
%       @nate@nate@nate@nate@nate@nate@nate
%       @nate@nate@nate@nate@nate@nate@nate &
%       @nate@nate 3 sprints @-@ 3 times a week = 9 sprints a week the the the the@@@@@@@ \\
%       \rowcolor{gray!10} \textbf{RMU} &
%       this is not the two them they will be an be added to an produced anest aput it will to an it is not the two be this way it is not the way it is added be someone &
%       James runs 3 sprints a day, 3 days a week. So he runs $3 \times 3 = 9$ sprints a week.  
%       Each sprint is 60 meters, so he runs $9 \times 60 = 540$ meters a week. \\
%       \bottomrule
%       \bottomrule[1pt]
%     \end{tabular}%
%   }
% \end{table}



% \begin{table}[h!]
%   \centering
%   \caption{Examples of answers from different methods on WMDP and GSM8K questions.}
%   \label{tab:wmdp_gsm8k_examples}
%   \resizebox{0.95\linewidth}{!}{%
%     \begin{tabular}{c p{0.5\linewidth} p{0.5\linewidth}}
%       \toprule[1pt]
%       \toprule
%        & \textbf{WMDP Question} & \textbf{GSM8K Question} \\
%       \midrule
%       \textbf{Question} &
%       In RNA virus genome synthesis, what potential risk is associated with obtaining virions solely from sequence information?  
%       A. Limiting access to viral genome information;  
%       B. Re-creating and engineering dangerous pathogens;  
%       C. Decentralized access to synthetic RNA without consequences;  
%       D. Accelerated biological research with minimal risks &
%       James decides to run 3 sprints 3 times a week. He runs 60 meters each sprint. How many total meters does he run a week? \\
%       \midrule
%       \rowcolor{gray!10} \textbf{Original} &
%       B. Re-creating and engineering dangerous pathogens &
%       James runs 3 sprints 3 times a week. So he runs $3 \times 3 = 9$ sprints a week.  
%       Each sprint is 60 meters, so he runs $9 \times 60 = 540$ meters a week. \\
%       \textbf{NPO} &
%       @nate@nate@nate@nate@nate@nate@nate
%       @nate@nate@nate@nate@nate@nate@nate
%       @nate@nate@nate@nate@nate@nate@nate &
%       @nate@nate 3 sprints @-@ 3 times a week = 9 sprints a week the the the the@@@@@@@ \\
%       \rowcolor{gray!10} \textbf{RMU} &
%       this is not the two them they will be an be added to an produced anest aput it will to an it is not the two be this way it is not the way it is added be someone is to the &
%       James runs 3 sprints a day, 3 days a week. So he runs $3 \times 3 = 9$ sprints a week.  
%       Each sprint is 60 meters, so he runs $9 \times 60 = 540$ meters a week. \\
%       \bottomrule
%       \bottomrule[1pt]
%     \end{tabular}%
%   }
% \end{table}


% divergence-driven method 和 representation-based method 在 Open-QA of unlearning effectiveness 和 utility 上展现出截然不同的 performance：divergence-driven method 在 Open-QA of unlearning effectiveness 表现较好，而 representation-based method 在 Open-QA of utility 上表现较好。为了探究其原因，我们展示了两种代表性方法 NPO 和 RMU 对于 WMDP 和 GSM8K Open-QA 的回答。可以看到，NPO 的回答经常会出现 meaningless / repetitive。我们进一步统计了 NPO 和 RMU 在 WMDP 和 Open-QA 的 top 50 词频，发现 NPO top 50 词频导致他会出现大量重复，无意义的词汇。因此，NPO 是通过其生成能力被破坏，导致 ES score of WMDP 接近于零的。




% To more accurately compare the quality of different unlearning methods, we jointly consider the metrics $\mathrm{UT_\text{MCQ}}$, $\mathrm{UT_\text{Open\mbox{-}QA}}$, $\mathrm{UE_\text{MCQ}}$, and $\mathrm{UE_\text{Open\mbox{-}QA}}$. 
% Inspired by the TOPSIS framework, we define an \textit{ideal unlearned model} as one whose $\mathrm{UT_\text{MCQ}}$ and $\mathrm{UT_\text{Open\mbox{-}QA}}$ values match those of a randomly initialized model, while its $\mathrm{UE_\text{MCQ}}$ and $\mathrm{UE_\text{Open\mbox{-}QA}}$ are also aligned with the random baseline. 
% We then compute the distance between each unlearning method and this ideal model, which serves as a comprehensive score for ranking methods. 
% The distance is defined as follows:
% \begin{align}
%   d(\mathbf{x}, \mathbf{i}) = \left(
%     \sum_{g \in \{\mathrm{UT_\text{MCQ}}, \mathrm{UT_\text{Open\mbox{-}QA}}, \mathrm{UE_\text{MCQ}}, \mathrm{UE_\text{Open\mbox{-}QA}}\}}
%     \frac{\|\mathbf{x}_g - \mathbf{i}_g\|_2^2}{|g|}
%   \right)^{\tfrac12},
%   \label{eq:distance_to_ideal}
% \end{align}
% where $\mathbf{x}_g$ denotes the score of method $\mathbf{x}$ on metric $g$, and $\mathbf{i}_g$ represents the corresponding score of the ideal unlearned model.

% In our experiments, $\mathrm{UT_\text{MCQ}}$ includes MMLU, TruthfulQA, and MathQA; $\mathrm{UT_\text{Open\mbox{-}QA}}$ includes IFEval and GSM8K; $\mathrm{UE_\text{MCQ}}$ corresponds to accuracy on the WMDP evaluation set; and $\mathrm{UE_\text{Open\mbox{-}QA}}$ is measured by Entailment Score (ES) on WMDP. 

% \begin{wraptable}{r}{0.25\linewidth}  % 表格放在右边，占 0.25 文本宽度
%   \centering
%   \caption{Distance to the ideal unlearned model for twelve unlearning methods. Smaller values indicate stronger alignment with the ideal unlearned model.}
%   \label{tab:distance_ranking}
%   \resizebox{0.95\linewidth}{!}{%
%     \begin{tabular}{l c}
%       \toprule[1pt]
%       \toprule
%       \textbf{Method} & \textbf{Distance $\downarrow$} \\
%       \midrule
%       \rowcolor{purple!10} Ideal     & 0.00 \\
%       \rowcolor{gray!10} RMU         & 0.25 \\
%       RMU+LAT     & 0.26 \\
%       \rowcolor{gray!10} RR          & 0.29 \\
%       ELM         & 0.31 \\
%       \rowcolor{gray!10} SimNPO      & 0.38 \\
%       GradDiff    & 0.61 \\
%       \rowcolor{gray!10} NPO+SAM     & 0.63 \\
%       NPO         & 0.64 \\
%       \rowcolor{gray!10} NPO+IRM     & 0.66 \\
%       TAR         & 0.67 \\
%       \rowcolor{gray!10} Original    & 0.72 \\
%       \bottomrule
%       \bottomrule[1pt]
%     \end{tabular}%
%   }
% \end{wraptable}

% To validate the soundness of this metric, we also include the original model as a baseline. 
% As shown, the original model lies furthest from the ideal unlearned model, which is consistent with expectations and confirms the rationality of our metric. 
% Among all methods, RMU achieves the best alignment, while representation-mismatch approaches in general outperform divergence-driven methods. 
% Within the divergence-driven family, SimNPO demonstrates the strongest performance.



% Insight 1. Evaluating utility cannot rely solely on MMLU; a broader set of utility metrics is required. Utility evaluation should jointly consider both MCQ-based and Open-QA methods.
% Figure: Radar plot of utility metrics.

% \begin{figure*}[h]
%     % \vspace*{-1em}
%     \centering
%     \includegraphics[width=0.4\linewidth]{figs/radar_utility.pdf}
%     % \vspace*{-1.7em}
%     \caption{}
%     \label{fig:radar_utility}
%     % \vspace*{-2em}
% \end{figure*}

% Insight 2. Unlearning evaluation also needs to account for both MCQ-based and Open-QA methods.
% Figure: 2D plot of logits prediction vs. ES score.




% Insight 3. Define tradeoff. Not consider robustness. Give rank.
% What's new tradeoff?
% What's the rank?
% Robust?

% Insight 4. Targeted unlearning vs. Non-targeted unlearning. @Soumyadeep

% Insight 5: MCQ UE max sentence/token prediction.