\section{Broader Impacts}
\label{appx:impact}
Improving unlearning is an essential step toward safer and more trustworthy language models. 
By clarifying methodologies, metrics, and robustness, our study provides a foundation for designing more effective approaches. 
Such progress has the potential to mitigate privacy risks, prevent the reproduction of harmful or copyrighted content, and support compliance with emerging regulations. 
At the same time, unlearning methods must be deployed with care, since excessive forgetting can degrade useful capabilities and adversarial adaptation may expose new vulnerabilities. 
We hope this work encourages the community to pursue principled, transparent, and responsible unlearning practices that balance safety, utility, and robustness in large language models.