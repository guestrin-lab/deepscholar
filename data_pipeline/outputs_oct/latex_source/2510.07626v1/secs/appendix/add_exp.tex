\section{Additional Experiment Results}
\label{appx:add_exp}

\input{tabs/example}

\paragraph{Answer selection vs. answer generation.} 
\textbf{Table\,\ref{tab:selection_generation}} highlights the contrast between answer selection and answer generation for an NPO-unlearned Llama-3 8B Instruct on WMDP. Under the MCQ setting, both NPO and RMU successfully alter the model’s prediction to an incorrect choice, suggesting effective unlearning of forget-relevant knowledge. However, in the Open-QA setting, the same models produce incoherent or nonsensical text rather than valid answers, indicating that their generative ability on forget queries is internally disrupted. This mismatch reveals a critical limitation of relying solely on MCQ-based evaluations, as they can obscure issues of over-forgetting that may also degrade performance on non-forget inputs.


\begin{figure}[htbp]
    \centering
    \hspace{8mm}
    \includegraphics[width=0.25\linewidth]{figs/bar_logits_legend.pdf} \\
    \includegraphics[width=0.4\linewidth]{figs/bar_logits_freq.pdf}
    \caption{ABCD and top-4 token logits of the original (Llama-3 8B Instruct), NPO unlearned and RMU unlearned model on the WMDP evaluation set.}
    \label{fig:bar_logits}
    %\vspace{-3mm}
\end{figure}


\paragraph{From logits to behavior: over-forgetting in divergence-driven unlearning.}
As indicated by Fig.\,\ref{fig:evaluatio_wmdp}-(c), {\MDivB} is prone to over-forgetting on Open-QA tasks. To further investigate this limitation, we compare the prediction logit distributions of the unlearned models (NPO and RMU) with the original pre-unlearned model over the answer options (A/B/C/D) and their top-4 predictions.  
\textbf{Fig.\,\ref{fig:bar_logits}} illustrates how prediction logit distributions differ across unlearning methods on WMDP, comparing the options with each model’s top-4 predicted tokens.  

From the perspective of ABCD logits, NPO drives all four options close to zero and nearly identical, achieving $\mathrm{UE}_\text{MCQ}$ by uniformly suppressing candidate scores. In other words, ABCD are not true top-token candidates under NPO, as revealed by its top-4 prediction logits on ABCD. By contrast, RMU maintains the distribution of the original model’s logits but reshapes their relative distribution, attaining $\mathrm{UE}_\text{MCQ}$ by reordering rather than erasing signals. For the top-4 logits, NPO assigns much higher values than RMU or the original model, but these correspond to meaningless tokens (Table\,\ref{tab:selection_generation}). This shows that NPO achieves $\mathrm{UE}_\text{Open-QA}$ by severely disrupting generative capacity, explaining its substantially lower $\mathrm{UT}_\text{Open-QA}$ relative to RMU and the original model.  

\begin{figure}[htbp]
    \centering
    %\vspace{-3mm}
    \includegraphics[width=0.4\linewidth]{figs/idk_ap_dpo.pdf}
    \caption{Effective unlearning with utility preservation of IDK+AP when warm-started with DPO (called IDK+AP w/ DPO), given by $\mathrm{UT}_\text{Avg}$ and $\mathrm{UE}_\text{Avg}$ (as defined in Fig.\,\ref{fig:evaluatio_wmdp}.}
    \label{fig:idk_ap_dpo}
    %\vspace{-3mm}
\end{figure}

\paragraph{Mitigating utility loss in IDK+AP via DPO warm-start.}
As seen in Fig.\,\ref{fig:evaluatio_wmdp}-(c), DPO retains a significant portion of the original model’s utility, which we attribute to the presence of a positive preference signal that guides the model to prefer the targeted answers in response to the forget queries. We hypothesize that the pronounced utility degradation of IDK+AP arises from its stricter log-likelihood loss, which aggressively increases the probability of rejection responses for forget-relevant questions. To mitigate this and to verify our hypothesis, we propose to unlearn using IDK+AP after a `warm-start' with DPO for a few epochs.
As seen in \textbf{Fig.\,\ref{fig:idk_ap_dpo}}, this strategy (called IDK+AP w/ DPO) can infact increase preserve the utility, while achieving effective unlearning.
We note this is in contrast to the  setting of post-training of LLMs \citep{dubey2024llama} where SFT is followed by DPO. We think that in unlearning, the rejection responses provide a strong distribution shift for IDK+AP which is managed by warm-starting with DPO.

\paragraph{Robustness of knowledge vs. data-centric unlearning under quantization.} 

\input{tabs/quan_muse}

As presented in Table\,\ref{tab:quantization_results} of \textbf{Appendix\,\ref{appx:add_exp}}, we observe that quantization affects both unlearning effectiveness and utility. For data-centric unlearning (MUSE), 4-bit quantization leads to a sharp decline in performance, with NPO showing a significant increase in KnowMem on $\Df$ and RMU suffering large drops in KnowMem on $\Dr$. In contrast, for knowledge removal (WMDP), both NPO and RMU maintain consistent UE across full precision, 8-bit, and 4-bit settings, while UT degrades only slightly. These results suggest that knowledge removal is generally more robust to post-unlearning quantization than content-based unlearning.



\begin{figure}[htbp] 
\centering
\vspace*{-2mm}
\begin{tabular}{cc}
\hspace*{-3mm}
\includegraphics[width=0.24\textwidth,height=!]{figs/landscape_tar.pdf} 
&
\hspace*{-5mm}
\includegraphics[width=0.24\textwidth,height=!]{figs/landscape_rmulat.pdf}
\vspace*{-1mm}
\\
\hspace*{-3mm} 
\small{(a) TAR} &  
\hspace*{-5mm} 
\small{(b) RMU+LAT}\\
\end{tabular}
 \vspace*{-2mm}
\caption{\small{The prediction loss landscape of the TAR and RMU+LAT-unlearned model on the forget set using the visualization tool in \citep{li2018visualizing}.
%where higher values around $x = y = 0$ indicate more effective unlearning. The 3D loss landscape is defined as $z = \ell(\btheta + x \cdot \mathbf{r}_1 + y \cdot \mathbf{r}_2)$, with $\btheta$ representing the unlearned model.
}}
\label{fig:landscape}
% \vspace*{-5mm}
\end{figure}

\paragraph{Loss landscape of TAR and RMU+LAT.} 
\textbf{Fig.\,\ref{fig:landscape}} visualizes the forget loss landscape of TAR and RMU+LAT following \citep{li2018visualizing}. The landscape of TAR is noticeably smoother, while RMU+LAT exhibits irregularities, indicating only \textit{local robustness}. This echoes the debate \citep{engstrom2018evaluating} contrasting standard adversarial training \citep{madry2018towards} with adversarial logit pairing \citep{kannan2018adversarial}, where leveraging logits (or other latent information) was argued to yield limited robustness. By analogy, in LLM unlearning, RMU+LAT also fails to achieve broad robustness due to its locality constraint.

