\section{Limitations}
\label{appx:limit}
While this work offers a comprehensive full-stack investigation of LLM unlearning, we acknowledge several limitations. 
First, our taxonomy covers twelve representative methods, but additional approaches outside this scope may reveal further insights. 
Second, our robustness evaluations of input-level attacks focus mainly on jailbreak prompts. Future work should extend to other adversarial scenarios, such as in-context demonstrations or more advanced prompting techniques, to obtain a fuller picture. 
Third, our evaluation relies heavily on automatic metrics. Although these metrics enable systematic comparisons, they may overlook subtle aspects of model behavior. Human evaluations would provide complementary perspectives on unlearning success and user trust. 
