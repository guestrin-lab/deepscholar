\SL{[The current outline is more likely reviewing existing approaches; rather than a story/Insight telling. I strongly suggest to revise it further.]}

\SL{[To re-do the story-telling outline, I strongly suggest learning from the following papers: \url{https://arxiv.org/pdf/2002.08347} (check their Table 1), \url{https://arxiv.org/pdf/1802.00420} (check case study section 5 and Table 1), and \url{https://arxiv.org/pdf/2010.00467}]} (check Table 1)
\begin{itemize}
\item Key question to answer: \SL{[Summarize the key questions that your work attempts to answer; Please list them below. You can first finish novelties list then go back to this part.]}
    \item Novelties: \SL{[You need to consider lay out the story based on novelties]}
    \begin{itemize}
        \item Method/evaluation categorization
        \begin{enumerate}
            \item Methods: Optimization divergence-driven, Random feature/label-driven, Robustness-enhanced, targeted unlearning vs. untargeted unlearning
            \item Robustness measurements: Weight-level (in-domain relearning (relearning attacks), out-of-domain fine-tuning (irrelevant fine-tuning), weight quantization), input-level (jailbreaking and worst-case in-context example)
            \item  Utility metrics: beyond MMLU (MCQ), it includes \SL{instruction-following generation tasks}
            \item Unlearn effectiveness metrics: E.g., in WMDP, max letter vs. max text sequence 
            \item Task categorization: E.g., in WMDP, answer selection unlearning (MCQ) vs. answer generation unlearning (ES)
        \end{enumerate}
        \item Insights gained (key Insights)  by revisiting \SL{ten?} \SL{unlearning methods recently published at ICLR, ICML and NeurIPS} \SL{[You can organize your insights and Insights based on above method/evaluation categorization]}
        \SL{[Please list them below and think about how to tell the whole story]}
        \SL{[The insights can be within a task/metric/method, as well as across multiple tasks/metrics/methods]}
    \end{itemize}
\end{itemize}


% \begin{itemize}
%     \item General utility evaluation needs to consider generation-based metrics: random feature/label–driven is much higher than optimization divergence–driven.

%     \item Unlearning evaluation cannot only focus on logits prediction on options. (max sentence + ES)

%     \item Robustness is obtained by permuting general utility (RMU vs. RMU-LAT vs. TAR). tradeoff.

%     \item Differences among different robustness designs. (robust variant)

%     \item In-domain relearning attacks cannot represent the robustness of out-of-domain fine-tuning.

%     \item Quantization attacks have no effect on MCQ-style evaluation. relearning attack vs quantization

%     \item Input-level attacks, such as jailbreaking attacks: optimization divergence–driven is more robust. correlation between relearning attack and input
% \end{itemize}


\section{Introduction}

\SL{Research question: How can we design a systematic, in-depth, full-stack investigation that accommodates diverse unlearning methods, incorporates overlooked dimensions such as task differences (e.g., answer selection vs. generation) and robustness types (e.g., in-forget-domain weight relearning vs. out-of-forget-domain irrelevant fine-tuning), and establishes a principled taxonomy for assessing utility, effectiveness, and robustness?}


\SL{Overview of your main results across methods focusing on WMDP.}

\CF{Main Table on WMDP, Heatmap: Method vs. UE/Utility/Robustness.}


\begin{table}[h!]
\centering
\caption{Overview of the performance of 12 unlearning methods on WMDP Bio with Llama-3 8B Instruct. The unlearning methods are categorized into three groups: \textit{divergence-driven}, \textit{representation-mismatch}, and \textit{rejection-based}. Evaluation is conducted from four perspectives: (1) \textbf{Retain performance}, measured by MMLU accuracy; (2) \textbf{General utility}, measured by TruthfulQA, MathQA, IFEval, and GSM8K accuracy; (3) \textbf{Unlearning effectiveness}, measured by WMDP Bio accuracy (WMDP Acc) and entailment score (WMDP ES); and (4) \textbf{Robustness evaluation}, assessed by WMDP Acc after in-domain relearning (IDR), out-of-domain fine-tuning (ODF), jailbreaking attacks (Jail), and quantization attacks (Quan). Metrics based on multiple-choice questions are highlighted in \textcolor{green}{green}, while metrics based on generation tasks are highlighted in \textcolor{red}{red}.
\CF{TODO: Add Citation.}}
\renewcommand{\arraystretch}{1.5} % 扩大行距
\vspace{2mm}
\resizebox{\textwidth}{!}{%
% --- Restore column definitions to color the data rows ---
\begin{tabular}{c|c|c|>{\columncolor{lightgreen}}c|>{\columncolor{lightgreen}}c>{\columncolor{lightgreen}}c|>{\columncolor{lightred}}c>{\columncolor{lightred}}c|>{\columncolor{lightgreen}}c|>{\columncolor{lightred}}c|*{4}{>{\centering\arraybackslash}p{1.4cm}}}
\toprule[1pt]
\toprule
% --- Header Level 1 (with cells manually set to white) ---
\multirow{2}{*}{\centering\textbf{Method}} & \multirow{2}{*}{\textbf{Reference}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Robust\\Design\end{tabular}}} & \multicolumn{1}{c|}{\cellcolor{white}\textbf{Retain}} & \multicolumn{4}{c|}{\cellcolor{white}\textbf{General Utility}} & \multicolumn{2}{c|}{\cellcolor{white}\textbf{Unlearning Effectiveness}} & \multicolumn{4}{c}{\textbf{Robustness Evaluation}} \\
\cline{4-14}
% --- Header Level 2 (with cells manually set to white) ---
& & & \multicolumn{1}{c|}{\cellcolor{white}\textbf{MMLU}} & \multicolumn{1}{c|}{\cellcolor{white}\textbf{Truthful}} & \multicolumn{1}{c|}{\cellcolor{white}\textbf{MathQA}} & \multicolumn{1}{c|}{\cellcolor{white}\textbf{IFEval}} & \multicolumn{1}{c|}{\cellcolor{white}\textbf{GSM8K}} & \multicolumn{1}{c|}{\cellcolor{white}\textbf{WMDP Acc}} & \multicolumn{1}{c|}{\cellcolor{white}\textbf{WMDP ES}} & \textbf{IDR} & \textbf{ODF} & \textbf{Jail} & \textbf{Quan} \\
\midrule
% --- Data Rows (will now have correct column colors) ---
\textbf{Original Model} & NA & NA & & & & & & & & & & & \\
\midrule
\rowcolor{LightCyan!50}
\multicolumn{14}{c}{\textbf{\textit{Divergence-driven}}} \\
\midrule
\textbf{GradDiff} & & \xmark & & & & & & & & & & & \\
\textbf{NPO} & \citet{zhang2024negative} & \xmark & & & & & & & & & & & \\
\textbf{SimNPO} & \citet{fan2024simplicity} & \xmark & & & & & & & & & & & \\
\textbf{NPO+SAM} & & \cmark & & & & & & & & & & & \\
\textbf{NPO+IRM} & & \cmark & & & & & & & & & & & \\
\midrule
\rowcolor{LightCyan!50}
\multicolumn{14}{c}{\textbf{\textit{Representation-mismatch}}} \\
\midrule
\textbf{RMU} & & \xmark & & & & & & & & & & & \\
\textbf{RR} & & \xmark & & & & & & & & & & & \\
\textbf{ELM} & & \xmark & & & & & & & & & & & \\
\textbf{RMU-LAT} & & \cmark & & & & & & & & & & & \\
\textbf{TAR} & & \cmark & & & & & & & & & & & \\
\midrule
\rowcolor{LightCyan!50}
\multicolumn{14}{c}{\textbf{\textit{Rejection-based}}} \\
\midrule
\textbf{IDK+AP} & & \xmark & & & & & & & & & & & \\
\textbf{JensUn} & & \xmark & & & & & & & & & & & \\
\bottomrule
\bottomrule[1pt]
\end{tabular}
}
\end{table}


% \begin{table}[h!]
% \centering
% \caption{Benchmark Results for Unlearning Methods}
% \renewcommand{\arraystretch}{1.5} % 扩大行距
% \resizebox{0.9\textwidth}{!}{%
% % \begin{tabular}{>{\centering\arraybackslash}p{1.8cm}|>{\centering\arraybackslash}p{1.3cm}|>{\centering\arraybackslash}p{1.0cm}|>{\centering\arraybackslash}p{1.1cm}>{\centering\arraybackslash}p{1.1cm}|>{\centering\arraybackslash}p{1.1cm}>{\centering\arraybackslash}p{1.1cm}|>{\centering\arraybackslash}p{1.6cm}|>{\centering\arraybackslash}p{1.1cm}|*{4}{>{\centering\arraybackslash}p{1.4cm}}}
% \begin{tabular}{c|c|c|cc|cc|c|c|*{4}{>{\centering\arraybackslash}p{1.4cm}}}
% \toprule[1pt]
% \toprule
% % --- Header Level 1 ---
% \multirow{3}{*}{\centering\textbf{Method}} & \multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Robust\\Design\end{tabular}}} & \textbf{Retain} & \multicolumn{4}{c|}{\textbf{General Utility}} & \multicolumn{2}{c|}{\textbf{Unlearning Effectiveness}} & \multicolumn{4}{c}{\textbf{Robustness Evaluation}} \\
% \cline{3-4} \cline{4-7} \cline{8-9} \cline{10-13}
% % --- Header Level 2 ---
% & & \multirow{2}{*}{\textbf{MMLU}} & \multicolumn{2}{c|}{\textbf{MCQ}} & \multicolumn{2}{c|}{\textbf{Gen}} & \textbf{MCQ} & \textbf{Gen} & \multirow{2}{*}{\textbf{IDR}} & \multirow{2}{*}{\textbf{ODF}} & \multirow{2}{*}{\textbf{Jail}} & \multirow{2}{*}{\textbf{Quan}} \\
% \cline{4-5} \cline{6-7} \cline{8-8} \cline{9-9}
% % --- Header Level 3 ---
% & & & \textbf{Truthful} & \textbf{MathQA} & \textbf{Ifeval} & \textbf{GSM8K} & \textbf{WMDP Acc} & \textbf{WMDP ES} & & & & \\
% \midrule
% % --- Data Rows ---
% \textbf{Original} & NA & & & & & & & & & & & \\
% \midrule
% \rowcolor{LightCyan!50}
% \multicolumn{13}{c}{\textbf{\textit{Divergence-driven}}} \\
% \midrule
% \textbf{GradDiff} & \xmark & & & & & & & & & & & \\
% \textbf{NPO} & \xmark & & & & & & & & & & & \\
% \textbf{SimNPO} & \xmark & & & & & & & & & & & \\
% \textbf{NPO+SAM} & \cmark & & & & & & & & & & & \\
% \textbf{NPO+IRM} & \cmark & & & & & & & & & & & \\
% \midrule
% \rowcolor{LightCyan!50}
% \multicolumn{13}{c}{\textbf{\textit{Representation-mismatch}}} \\
% \midrule
% \textbf{RMU} & \xmark & & & & & & & & & & & \\
% \textbf{RR} & \xmark & & & & & & & & & & & \\
% \textbf{ELM} & \xmark & & & & & & & & & & & \\
% \textbf{RMU-LAT} & \cmark & & & & & & & & & & & \\
% \textbf{TAR} & \cmark & & & & & & & & & & & \\
% \midrule
% \rowcolor{LightCyan!50}
% \multicolumn{13}{c}{\textbf{\textit{Rejection-based}}} \\
% \midrule
% \textbf{IDK+AP} & \cmark & & & & & & & & & & & \\
% \textbf{JensUn} & \cmark & & & & & & & & & & & \\
% \bottomrule
% \bottomrule[1pt]
% \end{tabular}
% }
% \end{table}

% \begin{table}[h!]
% \centering
% \caption{Benchmark Results for Unlearning Methods}
% \resizebox{\linewidth}{!}{%
% \begin{tabular}{>{\centering\arraybackslash}p{1.5cm}|>{\centering\arraybackslash}p{1.3cm}|>{\centering\arraybackslash}p{1.0cm}|>{\centering\arraybackslash}p{1.1cm}>{\centering\arraybackslash}p{1.1cm}|>{\centering\arraybackslash}p{1.1cm}>{\centering\arraybackslash}p{1.1cm}|*{2}{>{\centering\arraybackslash}p{1.1cm}|}*{4}{>{\centering\arraybackslash}p{1.4cm}}}
% \toprule[1pt]
% \toprule
% % --- Header Level 1 ---
% \multirow{3}{*}{\centering\textbf{Method}} & \multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Robust\\design\end{tabular}}} & \textbf{Retain} & \multicolumn{4}{c|}{\textbf{General Utility}} & \multicolumn{2}{c|}{\textbf{Unlearning effectiveness}} & \multicolumn{4}{c}{\textbf{Robustness}} \\
% \cline{4-7} \cline{8-9} \cline{10-13}
% % --- Header Level 2 ---
% & & \multirow{2}{*}{\textbf{MMLU}} & \multicolumn{2}{c}{\textbf{MCQ}} & \multicolumn{2}{c|}{\textbf{Gen}} & \textbf{MCQ} & \textbf{Gen} & \multirow{2}{*}{\textbf{IDR}} & \multirow{2}{*}{\textbf{ODF}} & \multirow{2}{*}{\textbf{Jail}} & \multirow{2}{*}{\textbf{Quan}} \\
% \cline{4-5} \cline{6-7} \cline{8-8} \cline{9-9}
% % --- Header Level 3 ---
% & & & \textbf{Truthful} & \textbf{MathQA} & \textbf{Ifeval} & \textbf{GSM8K} & \textbf{WMDP} & \textbf{ES} & & & & \\
% \midrule
% % --- Data Rows ---
% \textbf{Original} & NA & & & & & & & & & & & \\
% \midrule
% \multicolumn{13}{c}{\textit{Divergence-driven}} \\
% \midrule
% \textbf{GradDiff} & \xmark & & & & & & & & & & & \\
% \textbf{NPO} & \xmark & & & & & & & & & & & \\
% \textbf{SimNPO} & \xmark & & & & & & & & & & & \\
% \textbf{NPO+SAM} & \cmark & & & & & & & & & & & \\
% \textbf{NPO+IRM} & \cmark & & & & & & & & & & & \\
% \midrule
% \multicolumn{13}{c}{\textit{Representation-mismatch}} \\
% \midrule
% \textbf{RMU} & \xmark & & & & & & & & & & & \\
% \textbf{RR} & \xmark & & & & & & & & & & & \\
% \textbf{ELM} & \xmark & & & & & & & & & & & \\
% \textbf{RMU-LAT} & \cmark & & & & & & & & & & & \\
% \textbf{TAR} & \cmark & & & & & & & & & & & \\
% \midrule
% \multicolumn{13}{c}{\textit{Rejection-based}} \\
% \midrule
% \textbf{IDK+AP} & \cmark & & & & & & & & & & & \\
% \textbf{JensUn} & \cmark & & & & & & & & & & & \\
% \bottomrule
% \bottomrule[1pt]
% \end{tabular}
% }
% \end{table}

% \begin{table}[h!]
% \centering
% \caption{Benchmark Results for Unlearning Methods}
% \resizebox{\linewidth}{!}{%
% \begin{tabular}{c|c|c|*{4}{>{\centering\arraybackslash}p{0.65cm}|}*{2}{>{\centering\arraybackslash}p{0.65cm}|}*{4}{>{\centering\arraybackslash}p{0.65cm}}}
% \toprule[1pt]
% \toprule
% % --- Header ---
% \multirow{3}{*}{\textbf{Method}} & \multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Robust \\ design\end{tabular}}} & \textbf{Retain} & \multicolumn{4}{c|}{\textbf{General Utility}} & \multicolumn{2}{c|}{\textbf{Unlearning effectiveness}} & \multicolumn{4}{c}{\textbf{Robustness}} \\
% \cline{3-3} \cline{4-7} \cline{8-9} \cline{10-13}
% & & \multirow{2}{*}{\textbf{MMLU}} & \multicolumn{2}{c|}{\textbf{MCQ}} & \multicolumn{2}{c|}{\textbf{Gen}} & \textbf{MCQ} & \textbf{Gen} & \multirow{2}{*}{\rot{\textbf{In-domain}}} & \multirow{2}{*}{\rot{\textbf{Out-domain}}} & \multirow{2}{*}{\rot{\textbf{Jailbreak}}} & \multirow{2}{*}{\rot{\textbf{Quantization}}} \\
% \cline{4-5} \cline{6-7} \cline{8-8} \cline{9-9}
% & & & \textbf{Truth} & \textbf{Math} & \textbf{Ifeval} & \textbf{GSM8K} & \textbf{WMDP} & \textbf{ES} & & & & \\
% \midrule
% % --- Data Rows ---
% \textbf{Original} & NA & & & & & & & & & & & \\
% \midrule
% \multicolumn{13}{c}{\textit{Divergence-driven}} \\
% \midrule
% \textbf{GradDiff} & \xmark & & & & & & & & & & & \\
% \textbf{NPO} & \xmark & & & & & & & & & & & \\
% \textbf{SimNPO} & \xmark & & & & & & & & & & & \\
% \textbf{NPO+SAM} & \cmark & & & & & & & & & & & \\
% \textbf{NPO+IRM} & \cmark & & & & & & & & & & & \\
% \midrule
% \multicolumn{13}{c}{\textit{Representation-mismatch}} \\
% \midrule
% \textbf{RMU} & \xmark & & & & & & & & & & & \\
% \textbf{RR} & \xmark & & & & & & & & & & & \\
% \textbf{ELM} & \xmark & & & & & & & & & & & \\
% \textbf{RMU-LAT} & \cmark & & & & & & & & & & & \\
% \textbf{TAR} & \cmark & & & & & & & & & & & \\
% \midrule
% \multicolumn{13}{c}{\textit{Rejection-based}} \\
% \midrule
% \textbf{IDK+AP} & \cmark & & & & & & & & & & & \\
% \textbf{JensUn} & \cmark & & & & & & & & & & & \\
% \bottomrule
% \bottomrule[1pt]
% \end{tabular}
% }
% \end{table}

% The formulation of LLM unlearning is defined as:
% \begin{align}
% \begin{array}{ll}
%  \displaystyle \min_{\boldsymbol{\theta}}    
%  &  \underbrace{ \ell_{\mathrm{f}}(\boldsymbol{\theta} \mid \mathcal{D}_\mathrm{f}) }_\text{Forget} 
%    + \lambda \underbrace{ \ell_\mathrm{r}( \boldsymbol{\theta} \mid \mathcal{D}_{\mathrm{r}} ) }_\text{Retain},
%  \vspace*{-2mm}
% \end{array}
% \label{eq:prob_LLM_MU}
% \end{align}

\section{Related Work}

\SL{[LLM unlearning: optimization-driven,.]}
 % 目标是介绍 LLM unlearning 的三种 类别 算法 {\MRep} {\MDiv} {\MRej}
 %  但是每一个类别下 不止我们选择的算法，还有额外的 算法 (如果介绍了 那么是否要说明理由，为什么我们选择 文中的几种算法？)
 % 并且这里需要介绍出 有无 robustness design [我们的paper不需要强调 robustness design?]

\textbf{Machine unlearning in LLMs.} Recent studies on machine unlearning in large language models (LLMs) have shown encouraging progress in alleviating risks~\citep{liu2024rethinking, maini2024tofu, liu2024large, yao2023large} such as copyright infringement~\citep{eldan2023whos}, privacy leakage~\citep{hu2024jogging,wu2023depn}, and harmful content generation~\citep{li2024wmdp, lu2022quark}. Current approaches to unlearning, aiming to avoid the prohibitive cost of full retraining, can be broadly divided into three families. The first family builds on objectives that drive the unlearned model’s predictions away from those of the original or a reference model, a strategy denoted as {\MDiv}~\citep{zhang2024negative, fan2024simplicity}. Complementary to this, {\MRep} refers to methods that intervene the representation space by projecting the latent embeddings of unlearned targets toward random or orthogonal directions, thereby preventing the model from correctly answering queries related to the corresponding content~\citep{zou2024improving, li2024wmdp, sheshadri2024latent}. Finally, \MRej guides the model to align its outputs with a predefined library of refusal statements whenever prompts tied to the unlearned content are encountered~\citep{rafailov2024direct, yuan2024closer, singh2025unlearning, gandikota2024erasing}. Together, these approaches demonstrate the versatility of current LLM unlearning research in balancing the dual objectives of reliable forgetting and utility preservation.


\SL{[Unlearning benchmarking studies: datasets/tasks/evaluations/robust benchmark]}
\textbf{LLM Unlearning benchmarking.} Furthermore, with the rapid emergence of various LLM unlearning methods, a series of recent benchmarking efforts have also been proposed, providing structured frameworks for evaluating the effectiveness of unlearning algorithms~\citep{li2024wmdp, maini2024tofu, jin2024rwku, shi2024muse, eldan2023whos}. Based on whether the original model needs to be fine-tuned on the forget corpus before unlearning, existing benchmarks can be roughly categorized into two groups. The first group requires fine-tuning the model on a domain-specific forget corpus to ensure the presence of unlearning targets.

The "Who is Harry Potter" (WHP) benchmark~\citep{eldan2023whos} first fine-tunes models on the Harry Potter series to ensure the presence of domain-specific knowledge. It then defines 300 prompts from this universe as the forget set. The unlearning effectiveness is evaluated by inspecting the model’s completion probabilities on the forget prompts, whereas general utility is assessed using standard benchmarks such as Winogrande~\citep{sakaguchi2021winogrande} and Hellaswag~\citep{zellers2019hellaswag} to measure language understanding capabilities. Similarly, the TOFU benchmark (Task of Fictitious Unlearning)~\citep{maini2024tofu} is designed to unlearn with a dataset of 200 diverse synthetic author profiles based on question-answer pairs format, which enables precise control over the knowledge source to be unlearned. A designated subset of profiles constitutes the forget set, while the remaining profiles serve as the retain set. Evaluation spans multiple distributions—the forget set, the retain set, real authors, and world-fact questions—and quantifies unlearning by comparing model outputs to ground-truth answers using ROUGE-based score. The MUSE benchmark~\citep{shi2024muse} further extends this paradigm by providing separate corpora: a Harry Potter books dataset paired with FanWiki~\cite{} content (retain set), and a BBC News dataset randomly split into disjoint forget and retain sets. Evaluations include QA-based tests of verbatim and knowledge memorization, as well as a proposed privacy leakage metric for over-unlearning detection. 

However, all benchmarks in this category face a critical limitation: once the model is fine-tuned on a narrow domain, it tends to lose general capabilities such as mathematical reasoning and logical generation, making it difficult to assess the broader utility trade-offs of unlearning methods~\citep{jin2024rwku}.

The second group of benchmarks does not require fine-tuning, thereby aligning more closely with real-world unlearning scenarios. A representative example is the Weapons of Mass Destruction Proxy Benchmark (WMDP)~\citep{li2024wmdp}, which constructs a forget corpus by collecting a large set of articles related to biosecurity, cybersecurity, and chemical security. The unlearning effect is then evaluated on 3,668 multiple-choice questions derived from these domains, serving as proxies for hazardous knowledge. In addition, model utility is assessed through general-purpose benchmarks such as MMLU~\citep{} and MT-Bench~\citep{zheng2023judging}. Another example is RWKU (Real-World Knowledge Unlearning)~\cite{jin2024rwku}, which focuses on concept-level unlearning rather than domain-specific corpora. RWKU targets knowledge about 200 real-world famous people, and evaluates models via knowledge memorization on the forget set, while assessing retain performance across general ability, reasoning, truthfulness, factuality, and fluency. 

Finally, some existing safety-related benchmarks can also be leveraged in the unlearning context, such as PKU-SafeRLHF \citep{}, LKF for factual consistency \citep{lkf}, and Circuit Breaker for toxic content removal \citep{}. However, these benchmarks primarily focus on safe alignment rather than unlearning itself, and thus provide limited insights into the unique characteristics of unlearning algorithms.

% Why we select WMDP as our main benchmark? 
% What are the diff between the current robust benchmark and our work? 
% CV unlearning benchmark / VLLM unlearning benchmark


\SL{[Adversarial robustness of unlearning: Attack and defense.]}

Most importantly, the robustness of unlearning has become a central concern, as prior work has repeatedly shown that unlearned models remain highly vulnerable to attacks. More recent research has provided empirical evidence highlighting the severity of this issue. For example, \citep{lynch2024eight} demonstrated that standardized evaluation protocols often overlook residual knowledge persisting in unlearned models, while Łucki et al. (2024) \citep{lucki2024adversarial} showed that fine-tuning on as few as ten unrelated examples can already restore most of the forgotten information. In the same vein, Hu et al. (2024) \citep{hu2024jogging} introduced relearning attacks, where a small amount of unlearning-related data suffices to recover forgotten content via fine-tuning. Beyond these strategies, other attack vectors have also been explored. The Logit Lens method \citep{} probes each layer’s residual stream by projecting activations onto the vocabulary and tracking A/B/C/D logits in multiple-choice tasks; pruning has been applied to reverse the effects of unlearning \citep{}; and a more systematic study \citep{} classified unlearning attacks into three categories: input-space attacks (adversarial prompt optimization), latent-space attacks (embedding and activation perturbations), and weight-space attacks (fine-tuning with full parameters, LoRA adapters, or pruning). Collectively, these works highlight that current unlearning algorithms are fragile under diverse attacks. However, most of these evaluations remain constrained by the WMDP benchmark, which emphasizes multiple-choice settings and thus risks introducing bias by relying on a single evaluation metric.

As attack methods against unlearning continue to evolve, several works have introduced robust unlearning designs. For example, \citep{} apply meta-learning–based adversarial training to resist tampering attacks, while \citep{TAR} leverage sharpness-aware minimization (SAM) to promote local flatness and improve resistance to relearning. Inspired by invariant risk minimization, \citep{} propose invariant LLM unlearning (ILU), a regularization framework that enforces invariance for robustness. Distillation-based approaches such as UNDO \citep{} enhance robustness by distilling an unlearned model into a partially noised copy of itself. More recently, OBLIVIATE \citep{} introduces a structured loss combining masking, distillation, and world-fact preservation to robustly remove targeted data while maintaining utility.

% Overall these methods 都被 constraint 在使用的 benchmark 中，只依赖于 benchmark 提供的相较单一的 评估指标，导致了设计的 robust 方法具有一定的局限性

% Attack ? 把之前的所有attck 方法都写出来？ 我们后面有提到 为什么选哪几个attack 方法吗？ 
% defense? 指的 专门的具有robust design 的 unlearning 方法？

% Do we still need this part? 


%\section{[Contribution 1] A Taxonomy of Unlearning Methods: From Standard Approaches to Robust Enhancements}

\section{\SL{A Review of Ten Stateful Unlearning Methods: Methodologies and Key Insights}}

1. A Tabular Review of LLM Unlearning Methods: Summarizes evaluation approaches used in prior work: whether robustness evaluation is included, and which benchmarks are employed.

2. Formulations of Several Methods

\begin{table}[h!]
\centering
\caption{}
\renewcommand{\arraystretch}{1.25}
\vspace{2mm}
\resizebox{0.55\textwidth}{!}{% % Increased resizebox width slightly to accommodate the new column
\begin{tabular}{c|c|c|cc|cc|cc|c}
\toprule[1pt]
\toprule
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Reference}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Robust\\Design\end{tabular}}} & \multicolumn{2}{c|}{\textbf{UE}} & \multicolumn{2}{c|}{\textbf{GU}} & \multicolumn{2}{c|}{\textbf{RE}} & \multirow{2}{*}{\textbf{Benchmark}} \\
\cline{4-9}
& & & \textbf{MCQ} & \textbf{Gen} & \textbf{MCQ} & \textbf{Gen} & \textbf{Model} & \textbf{Input} & \\
\midrule
\rowcolor{LightCyan!50}
\multicolumn{10}{c}{\textbf{\textit{Divergence-driven}}} \\
\midrule
\rowcolor{gray!20}
\textbf{GradDiff} & \citet{liu2022continual} & \xmark & $\bullet$ & & & $\bullet$ & & $\bullet$ & WMDP \\
\textbf{NPO} & \citet{zhang2024negative} & \xmark & & $\bullet$ & $\bullet$ & & & & TOFU \\
\rowcolor{gray!20}
\textbf{SimNPO} & \citet{fan2024simplicity} & \xmark & $\bullet$ & & & & $\bullet$ & & WMDP/TOFU/MUSE \\
\textbf{NPO+SAM} & \citet{fan2025towards} & \cmark & & $\bullet$ & & $\bullet$ & & $\bullet$ & WMDP/MUSE \\
\rowcolor{gray!20}
\textbf{NPO+IRM} & \citet{wang2025invariance} & \cmark & $\bullet$ & & $\bullet$ & & $\bullet$ & & WMDP/MUSE \\
\midrule
\rowcolor{LightCyan!50}
\multicolumn{10}{c}{\textbf{\textit{Representation-mismatch}}} \\
\midrule
\textbf{RMU} & & \xmark & & $\bullet$ & & $\bullet$ & & & WMDP \\
\rowcolor{gray!20}
\textbf{RR} & & \xmark & $\bullet$ & & $\bullet$ & & & $\bullet$ & WMDP \\
\textbf{ELM} & & \xmark & & & & $\bullet$ & $\bullet$ & & WMDP \\
\rowcolor{gray!20}
\textbf{RMU+LAT} & & \cmark & $\bullet$ & $\bullet$ & & & & $\bullet$ & WMDP \\
\textbf{TAR} & & \cmark & & & $\bullet$ & $\bullet$ & $\bullet$ & & WMDP \\
\midrule
\rowcolor{LightCyan!50}
\multicolumn{10}{c}{\textbf{\textit{Rejection-based}}} \\
\midrule
\rowcolor{gray!20}
\textbf{IDK+AP} & & \xmark & & $\bullet$ & $\bullet$ & & & $\bullet$ & TOFU \\
\textbf{JensUn} & & \xmark & $\bullet$ & & & & $\bullet$ & & WMDP \\
\bottomrule
\bottomrule[1pt]
\end{tabular}
}
\label{tab:unlearn_method}
\end{table}

% \begin{table}[h!]
% \centering
% \begin{tabular}{c|c|c|cc|cc|cc|c}
% \toprule[1pt]
% \toprule
% % --- Header ---
% \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Principle}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Robust \\ design\end{tabular}}} & \multicolumn{2}{c|}{\textbf{Unlearning}} & \multicolumn{2}{c|}{\textbf{Utility}} & \multicolumn{2}{c|}{\textbf{Robustness}} & \multirow{2}{*}{\textbf{Benchmark}} \\ 
% \cline{4-5} \cline{6-7} \cline{8-9} % Use cmidrule for partial lines
% & & & \textbf{MCQ} & \textbf{Gen} & \textbf{MCQ} & \textbf{Gen} & \textbf{Weight} & \textbf{Input} & \\ \midrule
% % --- Data Rows (with abbreviated Principle) ---
% \textbf{GradDiff} & DD & \xmark & 0 & 0 & 0 & 0 & 0 & 0 & WMDP \\
% \rowcolor{gray!20}
% \textbf{NPO} & DD & \xmark & 0 & 0 & 0 & 0 & 0 & 0 & WMDP \\
% \textbf{SimNPO} & DD & \xmark & 0 & 0 & 0 & 0 & 0 & 0 & WMDP \\
% \rowcolor{gray!20}
% \textbf{NPO+SAM} & DD & \cmark & 0 & 0 & 0 & 0 & 0 & 0 & WMDP \\
% \textbf{NPO+IRM} & DD & \cmark & 0 & 0 & 0 & 0 & 0 & 0 & WMDP \\
% \rowcolor{gray!20}
% \textbf{RMU} & RM & \xmark & 0 & 0 & 0 & 0 & 0 & 0 & WMDP \\
% \textbf{RR} & RM & \xmark & 0 & 0 & 0 & 0 & 0 & 0 & WMDP \\
% \rowcolor{gray!20}
% \textbf{ELM} & RM & \xmark & 0 & 0 & 0 & 0 & 0 & 0 & WMDP \\
% \textbf{RMU-LAT} & RM & \cmark & 0 & 0 & 0 & 0 & 0 & 0 & WMDP \\
% \rowcolor{gray!20}
% \textbf{TAR} & RM & \cmark & 0 & 0 & 0 & 0 & 0 & 0 & WMDP \\
% \textbf{IDK+AP} & RB & \xmark & 0 & 0 & 0 & 0 & 0 & 0 & WMDP \\
% \rowcolor{gray!20}
% \textbf{JensUn} & RB & \xmark & 0 & 0 & 0 & 0 & 0 & 0 & WMDP \\
% \bottomrule
% \bottomrule[1pt]
% \end{tabular}
% \end{table}


\section{\SL{Unlearning Should Be Evaluated Beyond Answer Selection Tasks for Both Effectiveness and Utility}}


Insight 1. Evaluating utility cannot rely solely on MMLU; a broader set of utility metrics is required. Utility evaluation should jointly consider both MCQ-based and generation-based methods.
Figure: Radar plot of utility metrics.

\begin{figure*}[h]
    % \vspace*{-1em}
    \centering
    \includegraphics[width=0.4\linewidth]{figs/radar_utility.pdf}
    % \vspace*{-1.7em}
    \caption{}
    \label{fig:radar_utility}
    % \vspace*{-2em}
\end{figure*}

Insight 2. Unlearning evaluation also needs to account for both MCQ-based and generation-based methods.
Figure: 2D plot of logits prediction vs. ES score.

\CW{Some Writing as reference}

\CW{Point 1: Why Max Letter Probability Is Not Sufficient for the WMDP Benchmark?
The max letter probability metric relies solely on the greedy prediction of the first token (i.e., A/B/C/D). However, large language models do not necessarily follow the strict convention of beginning their answers with a single choice letter. For instance, when an LLM intends to answer “Answer: C”, the first predicted token might be decoded as “A”, leading to an incorrect evaluation. As a result, WMDP evaluation based on max letter probability is highly sensitive to prompt formatting and can introduce significant bias.}

\CW{Point 2: Max Sentence Probability
Building on the limitations of max letter probability in the WMDP benchmark, cite{Existing Large Language Model Unlearning Evaluations Are Inconclusive} introduced max sentence probability. Unlike max letter probability, which depends only on the greedy prediction of the first choice token, this method computes the average log-probability across the entire candidate choice. By doing so, it substantially reduces the sensitivity to prompt formatting and provides a more reliable measure of unlearning performance. As shown in Table X, max sentence probability is consistently lower than max letter probability across different methods. For some unlearning approaches, such as NPO and TAR, the results are close to 0.25—the lower bound corresponding to random guessing in multiple-choice settings.
}

\CW{Point 3: Why We Need Entailment Score for the WMDP Benchmark
Both max letter probability and max sentence probability rely on predefined candidate choices (letters or choice sentences). These metrics can only assess whether the model suppresses unsafe options under constrained multiple-choice settings, but they cannot capture the case where, during free-form generation, the model might still produce the correct answer. Another limitation is that probability-based evaluations have a uniform lower bound of 0.25 (random guessing for four-way multiple choice), which makes it difficult to distinguish the relative effectiveness of different unlearning methods.}

\CW{In contrast, prior benchmarks in the unlearning community, such as MSUE and RWKU, adopt generation-based metrics to evaluate unlearning effectiveness. Motivated by this, cite{A CLOSER LOOK AT MACHINE UNLEARNING FOR LARGE LANGUAGE MODELS} introduced the Entailment Score (ES), which measures the factual consistency of model outputs against ground-truth answers. This metric has a lower bound of 0, indicating that the model fails to generate any useful response. Crucially, ES enables finer-grained differentiation between approaches: for example, it reveals a clear performance gap between divergence-based unlearning methods and representation-based unlearning methods.
}






\begin{figure*}[h]
    % \vspace*{-1em}
    \centering
    \includegraphics[width=0.2\linewidth]{figs/2d.png}
    % \vspace*{-1.7em}
    \caption{}
    \label{fig:2d_unlearn}
    % \vspace*{-2em}
\end{figure*}


Insight 3. Define tradeoff. Not consider robustness. Give rank.
What's new tradeoff?
What's the rank?
Robust?

Insight 4. Targeted unlearning vs. Non-targeted unlearning. @Soumyadeep

Insight 5: MCQ UE max sentence/token prediction.


\section{\SL{Multi-Faceted Robustness Assessments for Unlearning: Needed and Complementary}}

Insight 4. In-domain relearning cannot substitute for out-of-domain fine-tuning.
Figure: 2D plot comparing in-domain relearning and out-of-domain fine-tuning.

\begin{figure*}[h]
    % \vspace*{-1em}
    \centering
    \includegraphics[width=0.2\linewidth]{figs/2d.png}
    % \vspace*{-1.7em}
    \caption{}
    \label{fig:2d_relearn}
    % \vspace*{-2em}
\end{figure*}

Insight 5. Quantization attacks show no effect under MCQ evaluation, but their impact becomes evident under generation-based evaluation.
Figure: Bar plot before and after attack.

\CW{ cite{quantization} introduced the finding that quantization applied to unlearned models may revoke forgotten knowledge. However, their claims were mainly demonstrated on the MUSE dataset. We consider this phenomenon a serious threat to the robustness of unlearning, since quantization is a widely adopted approach for model inference. Therefore, we also evaluate quantization under the WMDP benchmark. During training we use fp16 precision, while at inference time we apply 4-bit quantization to the unlearned models. Observation 1: Based on WMDP’s prediction-based evaluation, quantization does not revoke unlearned knowledge compared to its effect on MUSE. Nearly all scores remain the same before and after the quantization attack. To explain this, we analyze the logits of the A/B/C/D tokens relative to the top-k predicted tokens. As shown in Figure X, for unlearned models such as those produced by NPO, the logits of the choice tokens are far from the top ranks. Since quantization mainly affects the relative positions of the top-k tokens, the distant A/B/C/D tokens are unaffected. This explains why, even after quantization, divergence-based methods do not exhibit knowledge revocation. }

% \CW{ From the prediction-based evaluation perspective, RMU shows some distributional changes under quantization, but they are not very significant. This explains why some representation-based methods display limited knowledge revocation, though the effect is much smaller compared to what has been observed on the MUSE dataset. }

\CW{ For divergence-based methods, robustness holds under both prediction- and generation-based evaluations, since the logits of the top-k tokens and the A/B/C/D choice tokens are extremely separated. }

\CW{ For representation-based methods, we can explain why A/B/C/D predictions sometimes fail under quantization: although the logits of these tokens are generally away from the top-k region, there exists partial overlap in some cases, which makes them more vulnerable. }

\CW{ But how about the generation-based evaluation for representation-based methods? }

\begin{figure*}[h]
    % \vspace*{-1em}
    \centering
    \includegraphics[width=0.2\linewidth]{figs/bar.png}
    % \vspace*{-1.7em}
    \caption{}
    \label{fig:bar_quan}
    % \vspace*{-2em}
\end{figure*}


Insight 6. Model-level robustness does not necessarily capture input-level robustness.
Figure: Radar plot comparing robustness under model-level vs. input-level attacks.
[https://arxiv.org/pdf/2206.14486]

\begin{figure*}[h]
    \vspace*{-1em}
    \centering
    \includegraphics[width=0.2\linewidth]{figs/radar.png}
    % \vspace*{-1.7em}
    \caption{}
    \label{fig:radar_attack}
    % \vspace*{-2em}
\end{figure*}

Insight 7: Tradeoff of robustness and UE.

Insight 8: MCQ UE max sentence/token prediction. @changsheng

MUSE
Assignment
Incontext

\section{Discussion}
\section{Conclusion and Limitations}



% \section{LLM Unlearning Method}


% Based on the underlying principles, existing LLM unlearning methods can be broadly categorized into two families:
% \begin{itemize}
%     \item \textbf{Divergence-driven methods:} These approaches enforce unlearning by driving the logits predicted by the unlearned model to progressively diverge from those of the original model. Representative methods include \emph{GradDiff}, \emph{NPO}, and \emph{SimNPO}.
%     \item \textbf{Representation-mismatch methods:} These methods achieve unlearning by mapping the information to be forgotten into random vectors, thereby eliminating its representational structure. Representative approaches include \emph{RMU} and \emph{RR}.
% \end{itemize}

% \section{Adversarial Attacks in LLM Unlearning}

% LLM unlearning is not inherently robust and is vulnerable to a variety of attack strategies that compromise its performance and resurface the forgotten knowledge. Attacks can be categorized according to where they are applied:

% \begin{itemize}
%     \item \textbf{Weight-level attacks:} These attacks require direct modifications to the parameters of the unlearned model. They can be further classified into:
%     \begin{enumerate}
%         \item \textbf{Relearning attack:} Fine-tuning the unlearned model on a small subset of the forgetting samples.
%         \item \textbf{Irrelevant fine-tuning:} Fine-tuning on downstream datasets that are unrelated to the unlearning objective.
%         \item \textbf{Quantization attack:} Applying quantization techniques to reduce the bit-width of the unlearned model, thereby reactivating forgotten knowledge.
%     \end{enumerate}
%     \item \textbf{Input-level attacks:} These attacks do not alter model weights but instead craft prompts such that the unlearned content reappears.
% \end{itemize}

% \section{Towards Robust LLM Unlearning}

% To enhance robustness against adversarial attacks, several strategies have been explored:

% \begin{itemize}
%     \item \textbf{Perturbation-based defenses:} 
%     \begin{itemize}
%         \item \emph{SAM (Sharpness-Aware Minimization):} Introduces perturbations at the weight level to enforce smoothness and improve robustness.
%         \item \emph{RMU-LAT:} Combines adversarial training principles with RMU loss, applying perturbations at the activation level.
%     \end{itemize}
%     \item \textbf{Invariant learning:} Incorporating principles from Invariant Risk Minimization (IRM) to prevent relearning of forgotten content.
%     \item \textbf{Meta-learning approaches:} For example, \emph{TAR}, which adapts meta-learning paradigms for robust unlearning.
% \end{itemize}

% \section{Experimental Results}

% \subsection{Main Results}

% Main table.

% \begin{itemize}
%     \item \textbf{Insight 1:} Representation-mismatch methods demonstrate strong performance in preserving generation utility.
%     \item \textbf{Insight 2:} Robust designs further enhance general utility.
%     \item \textbf{Insight 3:} ...
% \end{itemize}
