\section{Introduction}

% \input{tabs/result_table}

With the rapid advances of large language models (LLMs), their tendency to memorize and regurgitate training data has raised serious concerns about privacy, safety, and intellectual property \citep{liu2024towards,che2025model,barez2025open}. More broadly, from the perspective of generative behavior, sensitive information, harmful content, and copyrighted material can be unintentionally reproduced, motivating the urgent need for \emph{machine unlearning}. The goal of unlearning is to selectively remove undesired data, knowledge, or behaviors from a trained model while preserving its general utility for normal tasks \citep{yao2024large,maini2024tofu,liu2024rethinking,wang2025reasoning}.



% The surge of interest in LLM unlearning has led to the development of a wide range of algorithms. In this work, we examine twelve representative unlearning methods and categorize them into three groups based on their underlying principles: {\MDivB}, {\MRepB}, and {\MRejB}. Divergence-driven optimization methods drive the model away from the reference while still requiring convergence of the optimization process \citep{yao2024large,fan2025towards,wang2025invariance}, such as negative preference optimization (NPO) \citep{zhang2024negative} and simple NPO (SimNPO) \citep{fan2024simplicity}. Representation misalignment methods disrupt the embeddings of forget data vs. their original representations in the reference model \citep{zou2024improving,tamirisa2024tamper,sheshadri2024latent}, with representation misdirection for unlearning (RMU) \citep{li2024wmdp} as one example. Rejection-based methods enforce targeted unlearning by producing explicit rejection responses to forget queries \citep{yuan2024closer,singh2025unlearning}, with I Don’t Know (IDK) \citep{maini2024tofu} serving as a representative case. These examples illustrate the diversity within each category, and are not meant to be exhaustive.

The growing interest in LLM unlearning has spurred the development of a wide range of algorithms. In this work, we examine \textbf{twelve} representative methods and categorize them into three families: {\MDivB}, {\MRepB}, and {\MRejB}. Divergence-driven optimization methods drive the model away from a reference distribution \citep{yao2024large,fan2025towards,wang2025invariance}, with negative preference optimization (NPO) \citep{zhang2024negative} and simple NPO (SimNPO) \citep{fan2024simplicity} as examples. Representation-misalignment methods disrupt the embeddings of forget data relative to their reference model representations \citep{zou2024improving,tamirisa2024tamper,sheshadri2024latent}, such as representation misdirection for unlearning (RMU) \citep{li2024wmdp}. Rejection-based methods enforce unlearning by producing explicit rejection responses to forget queries \citep{yuan2024closer,singh2025unlearning}, with I Don’t Know (IDK) \citep{maini2024tofu} as a representative case. 
%These examples highlight the diversity of approaches within each family, though they are not exhaustive.

These diverse LLM unlearning approaches are typically evaluated on off-the-shelf benchmarks. A common choice is the Weapons of Mass Destruction Proxy (WMDP) benchmark \citep{li2024wmdp}, valued for its practical focus on erasing harmful generation behaviors and for not requiring prior fine-tuning on the forget set. In WMDP, unlearning performance is assessed along two dimensions: \emph{unlearning effectiveness} (UE) and \emph{utility retention} (UT). However, these evaluations primarily rely on multiple-choice questions (MCQ), where the model selects from predefined options. The risk is that such evaluations may \textit{obscure} the actual free-form generation behavior of LLMs post-unlearning, limiting the assessment of UE and UT on forget-relevant and forget-irrelevant queries beyond predefined answer options. In addition, even under the MCQ evaluation metric, insights and comparative analyses across different unlearning method families remain limited.
Beyond UE and UT, unlearning robustness (Rob) is also critical, as forgotten knowledge can re-emerge once the unlearned model is ``attacked'' \citep{hu2024jogging,lucki2024adversarial,tamirisa2024tamper,che2025model,lynch2024eight}. Attacks take various forms, ranging from model-level \citep{hu2024jogging,tamirisa2024tamper,fan2025towards,wang2025invariance,zhang2024catastrophic} to input-level \citep{lucki2024adversarial,lynch2024eight}. However, systematic studies of unlearning robustness and its relationship to these different attacks also remain lacking.

Given the diversity of unlearning methods and the incompleteness of current evaluations, the key research question we aim to address is:
\begin{tcolorbox}[before skip=2mm, after skip=0.0cm, boxsep=0.0cm, middle=0.0cm, top=0.05cm, bottom=0.05cm, boxrule=0.6pt]
\begin{center}
\textit{\textbf{(Q)} Can we conduct a full-stack investigation of LLM unlearning that yields methodology-wise insights across all key metrics (UE, UT, and Rob)?}
     \end{center}
\end{tcolorbox} 
\vspace*{2mm}

To tackle (Q), we first draw methodological insights from our proposed categorization: {\MDiv}, {\MRep}, and {\MRej}. We then revisit the conventional unlearning assessments, UE and UT, and argue that they should also be evaluated through open question answering (Open-QA), where the model generates free-form responses, beyond MCQ. Relying solely on MCQ can lead to a myopic view of UE and UT across different unlearning methods, while Open-QA provides an important complementary perspective. For instance, MCQ-based MMLU assessments of UT \textit{cannot} fully capture the over-forgetting issue in {\MDivB} methods such as NPO \citep{zhang2024negative}, nor can MCQ-based UE evaluations fully reflect the performance of {\MRejB}. Furthermore, in the dimension of Rob, we find that robustness in LLM unlearning should be examined at a finer granularity, including (i) in-domain relearning \citep{hu2024jogging,fan2025towards}, where the model is fine-tuned on a subset of forget data, and (ii) out-of-domain fine-tuning \citep{wang2025invariance}, where the model is adapted to unrelated downstream tasks. Divergence-driven optimization  methods are generally more resilient to in-domain relearning, whereas {\MRepB} methods show stronger resistance to out-of-domain fine-tuning. In addition, robustness against input-level attacks such as jailbreaking is more closely aligned with in-domain relearning.


%For example, in addition to using MCQ accuracy on the WMDP evaluation set (termed $\mathrm{UE}_\text{MCQ}$) and using MMLU as utility measurement along the MCQ dimension  (termed $\mathrm{UT}_\text{MCQ}$) \citep{li2024wmdp},  we further employ the few-shot entailment score (ES) \citep{yuan2024closer, yao2024accurate, poliak2020survey} as the Open-QA-fashion UE evaluation (termed $\mathrm{UE}_\text{Open-QA}$).


% Another open challenge lies in evaluation. We adopt the widely used WMDP benchmark \citep{li2024wmdp} for our experiments and measure performance along two dimensions: \emph{unlearning effectiveness} (UE) and \emph{utility retention} (UT). Our findings indicate that evaluation solely through multiple-choice questions (MCQ), where the model selects from predefined options, is insufficient. It is also necessary to include open question answering (Open-QA), where the model generates free-form answers. Therefore, in addition to using accuracy on the WMDP evaluation set as $\mathrm{UE}_\text{MCQ}$ and MMLU as $\mathrm{UT}_\text{MCQ}$ \citep{li2024wmdp}, we further employ the few-shot entailment score (ES) as $\mathrm{UE}_\text{Open-QA}$ \citep{yuan2024closer, yao2024accurate, poliak2020survey}, and GSM8K \citep{cobbe2021training} together with IFEval \citep{zhou2023instruction} as $\mathrm{UT}_\text{Open-QA}$. Our results reveal that the trade-offs between UE and UT are fundamental, although they are often hidden when relying only on MCQ-based evaluation. {\MDivB} methods frequently suffer from over-forgetting, which substantially damages generative ability and results in significantly lower $\mathrm{UT}_\text{Open-QA}$ compared with {\MRepB}.


% Equally important is robustness. Recent studies have demonstrated that LLM unlearning methods are not inherently robust, since forgotten knowledge can re-emerge once the model is attacked \citep{hu2024jogging,lucki2024adversarial,tamirisa2024tamper,che2025model}. 

% We categorize attacks into two groups according to how they interact with the model: model-level attacks and input-level attacks. Model-level attacks include in-domain relearning \citep{hu2024jogging,fan2025towards}, where the model is fine-tuned on a small subset of forget data, out-of-domain fine-tuning \citep{wang2025invariance}, where the model is adapted to unrelated downstream tasks such as GSM8K for mathematical reasoning, and quantization \citep{zhang2024catastrophic}. Input-level attacks involve jailbreaking \citep{lucki2024adversarial}, where adversarial prompts can induce the model to produce content it was intended to forget. Our experiments reveal that robustness against relearning on forget-domain data (``in-domain relearning'') differs from robustness under fine-tuning on unrelated tasks (``out-of-domain fine-tuning''). For instance, {\MDivB} methods are typically more resilient to in-domain relearning, whereas {\MRepB} methods exhibit stronger resistance to out-of-domain fine-tuning. Moreover, robustness against input-level attacks such as jailbreaking is more closely aligned with in-domain relearning.


Prior work has made initial attempts to study LLM unlearning \textit{systematically}, focusing on robustness \citep{che2025model,hu2025blur} and evaluation \citep{feng2025existing}. Our work advances these efforts with three key novelties: (i) a methodological categorization that guides a rethinking of LLM unlearning, (ii) evaluation of UE and UT across both answer selection and free-form generation, highlighting their interaction with different methods, and (iii) an analysis of unlearning robustness that examines diverse forms of model-level weight perturbations and their connections to input-level jailbreaking attacks. We summarize \textbf{our contributions} below.


%Although benchmarks for evaluating algorithmic robustness have recently emerged~\citep{che2025model,hu2025blur}, \citet{che2025model} relies solely on multiple-choice accuracy to measure unlearning effectiveness, and \citet{hu2025blur} focuses only on a single type of relearning attack, neither providing a comprehensive assessment of robustness.

% In this paper, we provide a \textbf{full-stack investigation of LLM unlearning}, spanning methods,
% evaluation, and robustness. Our contributions are threefold:

$\bullet$ We establish a principled taxonomy of 12 recent unlearning methods, categorizing them into {\MDivB}, {\MRepB} and {\MRejB}.

$\bullet$ We revisit evaluation practices by moving beyond MCQ to incorporate Open-QA metrics, which better capture generative performance and reveal fundamental characteristics of different unlearning method families across UT and UE.

$\bullet$ We revisit the robustness of LLM unlearning by analyzing vulnerabilities under model-level attacks (in-domain relearning, out-of-domain fine-tuning, and quantization) and input-level jailbreak attacks, demonstrating how these robustness dimensions are connected.

% $\bullet$ We propose a principled taxonomy of 12 unlearning methods, grouped into {\MDivB}, {\MRepB}, and {\MRejB}.  

% $\bullet$ We extend evaluation beyond MCQ to Open-QA, better capturing generative performance and revealing key differences across UT and UE.  

% $\bullet$ We analyze robustness under model-level attacks (in-domain relearning, out-of-domain fine-tuning, quantization) and input-level jailbreaks, showing their connections and method-specific vulnerabilities.  
