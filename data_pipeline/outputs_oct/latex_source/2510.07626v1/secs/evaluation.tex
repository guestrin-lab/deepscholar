\section{Unlearning Should Be Evaluated Beyond Answer Selection Tasks for Both Effectiveness and Utility}

The evaluation of unlearning methods can broadly be divided into two categories: multiple-choice question (MCQ) tasks and open-ended question answering (Open-QA) tasks. 
In MCQ settings, the model assigns probabilities to candidate options and selects the choice with the highest predicted score. 
By contrast, Open-QA evaluation requires the model to generate free-form responses, from which the final answers are extracted using predefined rule-based procedures.

For large language model (LLM) unlearning, two complementary aspects are typically examined: \textit{unlearning effectiveness} and \textit{utility}. 
Unlearning effectiveness reflects how reliably the targeted knowledge or behaviors are eliminated, whereas utility measures how well the model retains its ability to perform on standard downstream benchmarks. 
For instance, in the WMDP benchmark, accuracy on the WMDP evaluation set is often used as a measure of unlearning effectiveness, while accuracy on MMLU is commonly employed to assess utility. 
It is important to highlight that both of these metrics rely exclusively on MCQ-style evaluation. 
However, restricting evaluation to MCQ settings can overlook critical aspects of model behavior, and thus does not provide a complete picture of unlearning performance.

\CF{TODO: add examples of MCQ and Open-QA.}

\paragraph{Utility evaluation requires both MCQ and Open-QA tasks.} 

% \begin{wrapfigure}{r}{0.4\textwidth}
%     \centering
%     \vspace{-4mm}
%     \includegraphics[width=0.38\textwidth]{figs/heatmap_ut.pdf}
%     \caption{Utility evaluation on MMLU, MathQA, TruthfulQA, GSM8K, and IFEval for twelve unlearning methods on WMDP Bio with Llama-3 8B Instruct. Higher values indicate stronger utility.}
%     \label{fig:radar_utility}
% \end{wrapfigure}

For WMDP, utility is typically assessed using MMLU \citep{li2024wmdp,tamirisa2024tamper}. 
MMLU belongs to the MCQ evaluation category and tests the breadth of knowledge and reasoning ability across 57 diverse disciplines, including history, medicine, and law. 
However, evaluating LLM utility requires going beyond such knowledge-based MCQ benchmarks. 
In addition to knowledge coverage, utility evaluation should also examine reasoning ability, instruction-following capacity, and robustness against hallucination. 
Therefore, a more comprehensive evaluation framework must integrate both MCQ and Open-QA metrics.

To this end, beyond MMLU we consider a broader set of utility evaluations, including MathQA (MCQ reasoning), TruthfulQA (MCQ hallucination detection), GSM8K (Open-QA reasoning), and IFEval (Open-QA instruction-following). 
In \textbf{Fig.~\ref{fig:evaluatio_wmdp} (a)}, we present the results of twelve unlearning methods on these benchmarks. 
All metrics are normalized with respect to the best-performing method. 
The results show that divergence-driven methods tend to achieve slightly lower scores than representation-mismatch methods on MCQ-based evaluations, but perform significantly worse on Open-QA benchmarks. 
This finding further emphasizes the necessity of incorporating both MCQ and Open-QA tasks for a comprehensive utility evaluation.


\begin{figure}[htb]
%\vspace*{-2mm}
\center
\hspace*{50mm}
\includegraphics[width=0.6\linewidth]{figs/2d_ue_ut_legend.pdf}\\
% \vspace{-1mm}

\begin{tabular}{ccc}
% \hspace*{-3mm}
% \hspace*{-6mm}
\includegraphics[width=0.32\textwidth,height=!]{figs/heatmap_ut.pdf}
&
\includegraphics[width=0.30\textwidth,height=!]{figs/2d_ue.pdf} 
&
% \hspace*{-6mm}
\includegraphics[width=0.29\textwidth,height=!]{figs/2d_ue_ut.pdf}
\vspace*{-1mm}
\\
\hspace*{3mm}{\small (a) $\mathrm{UT}_\text{MCQ}$ and $\mathrm{UT}_\text{Open-QA}$} &
% \hspace*{-3mm}
{\small (b) $\mathrm{UE}_\text{MCQ}$ vs. $\mathrm{UE}_\text{Open-QA}$} & 
% \hspace*{-3mm}
{\small (c) $\mathrm{UE}_\text{Avg}$ vs. $\mathrm{UT}_\text{Avg}$} \\
\end{tabular}
\vspace{-4mm}
\caption{\small{    
Utility (UT) and unlearning effectiveness (UE) evaluation of unlearning methods on WMDP with LLaMA-3 8B. 
(a) $\mathrm{UT}_\text{MCQ}$ includes MMLU, TruthfulQA, and MathQA, while $\mathrm{UT}_\text{Open-QA}$ includes IFEval and GSM8K. 
(b) $\mathrm{UE}_\text{MCQ}$ denotes Accuracy on the WMDP evaluation set, and $\mathrm{UE}_\text{Open-QA}$ denotes entailment scores (ES). 
(c) $\mathrm{UT}_\text{Avg}$ is defined as the mean of $\mathrm{UT}_\text{MCQ}$ and $\mathrm{UT}_\text{Open-QA}$, 
where each term is itself the average over its corresponding group of benchmarks; 
$\mathrm{UE}_\text{Avg}$ is defined analogously.
}
}
\label{fig:evaluatio_wmdp}
% \vspace*{-5mm}
\end{figure}



\paragraph{Unlearning effectiveness evaluation requires both MCQ and Open-QA tasks.}

% \begin{wrapfigure}{r}{0.44\textwidth}
%     \center
%     % \hspace*{6mm}
%     \includegraphics[width=0.45\textwidth]{figs/2d_ue_legend.pdf}\\
%     \vspace{1mm}
%     \includegraphics[width=0.35\textwidth]{figs/2d_ue.pdf}
%     \caption{Comparison of MCQ accuracy and Open-QA Entailment Score (ES) for twelve unlearning methods on WMDP Bio with Llama-3 8B Instruct. Lower values indicate stronger unlearning effectiveness.}
%     \label{fig:2d_unlearn}
% \end{wrapfigure}

An ideal unlearned model should not only fail to answer MCQ tasks related to the forgotten knowledge but should also avoid generating outputs that reveal such knowledge in free-form responses. 
Nevertheless, \citet{wang2025reasoning} demonstrate that unlearned models may still expose sensitive or undesired information during generation, underscoring the necessity of Open-QA evaluation.

One representative Open-QA metric is the Entailment Score (ES), which measures the factual consistency of model outputs against ground truth answers \citep{yao2024accurate, poliak2020survey, yuan2024closer}. 
In \textbf{Fig.~\ref{fig:evaluatio_wmdp} (b)}, we present the results of twelve unlearning methods evaluated on the WMDP benchmark, reporting both MCQ accuracy and ES. 
For both metrics, lower values indicate stronger unlearning effectiveness. 
The results reveal that divergence-driven methods often achieve more effective unlearning. 
Notably, models with comparable MCQ accuracy may still diverge significantly in ES scores (e.g., RMU vs.\ NPO), highlighting the importance of incorporating generation-based evaluation alongside MCQ tasks.


\paragraph{Redefine the trade-off between unlearning effectiveness and utility, and provide a ranking.}

\input{tabs/distance}

To more systematically compare unlearning methods, we jointly consider four groups of metrics: 
$\mathrm{UT}_\text{MCQ}$, $\mathrm{UT}_\text{Open-QA}$, $\mathrm{UE}_\text{MCQ}$, and $\mathrm{UE}_\text{Open-QA}$.  
Inspired by the TOPSIS framework, we define an \textit{ideal unlearned model} as one whose utility on MCQ and Open-QA benchmarks 
($\mathrm{UT}_\text{MCQ}$ and $\mathrm{UT}_\text{Open-QA}$) remains identical to the original model, 
while its unlearning effectiveness scores ($\mathrm{UE}_\text{MCQ}$ and $\mathrm{UE}_\text{Open-QA}$) align with the random baseline.  
We then compute group-level averages to obtain $\mathrm{UT}_\text{MCQ}$, $\mathrm{UT}_\text{Open-QA}$, $\mathrm{UE}_\text{MCQ}$, and $\mathrm{UE}_\text{Open-QA}$, 
and further calculate $\mathrm{UT}_\text{Avg}$ and $\mathrm{UE}_\text{Avg}$ as the mean of the two corresponding group-level scores.  
Finally, the Euclidean distance between each method and the ideal model provides a comprehensive criterion for evaluation.  

As shown in \textbf{Fig.~\ref{fig:evaluatio_wmdp}(c)}, unlearning methods that lie closer to the ideal point achieve a more desirable trade-off between unlearning effectiveness and utility. \textbf{Table~\ref{tab:distance_ranking}} reports the quantitative distances to the ideal model.  
As expected, the original model lies farthest away, which is consistent with the definition of unlearning.  
Among all approaches, RMU achieves the closest alignment, while methods under \RepM\ generally outperform those in \DivO.  
Within the \DivO\ family, SimNPO delivers the strongest results.




\paragraph{Explain why divergence-based methods lead to a decline in generation ability.}

% divergence-driven method 和 representation-based method 在 Open-QA of unlearning effectiveness 和 utility 上展现出截然不同的 performance：divergence-driven method 在 Open-QA of unlearning effectiveness 表现较好，而 representation-based method 在 Open-QA of utility 上表现较好。为了探究其原因，我们展示了两种代表性方法 NPO 和 RMU 对于 WMDP 和 GSM8K Open-QA 的回答。可以看到，NPO 的回答经常会出现 meaningless / repetitive。我们进一步统计了 NPO 和 RMU 在 WMDP 和 Open-QA 的 top 50 词频，发现 NPO top 50 词频导致他会出现大量重复，无意义的词汇。因此，NPO 是通过其生成能力被破坏，导致 ES score of WMDP 接近于零的。

\paragraph{Rejection-based method}



% To more accurately compare the quality of different unlearning methods, we jointly consider the metrics $\mathrm{UT_\text{MCQ}}$, $\mathrm{UT_\text{Open\mbox{-}QA}}$, $\mathrm{UE_\text{MCQ}}$, and $\mathrm{UE_\text{Open\mbox{-}QA}}$. 
% Inspired by the TOPSIS framework, we define an \textit{ideal unlearned model} as one whose $\mathrm{UT_\text{MCQ}}$ and $\mathrm{UT_\text{Open\mbox{-}QA}}$ values match those of a randomly initialized model, while its $\mathrm{UE_\text{MCQ}}$ and $\mathrm{UE_\text{Open\mbox{-}QA}}$ are also aligned with the random baseline. 
% We then compute the distance between each unlearning method and this ideal model, which serves as a comprehensive score for ranking methods. 
% The distance is defined as follows:
% \begin{align}
%   d(\mathbf{x}, \mathbf{i}) = \left(
%     \sum_{g \in \{\mathrm{UT_\text{MCQ}}, \mathrm{UT_\text{Open\mbox{-}QA}}, \mathrm{UE_\text{MCQ}}, \mathrm{UE_\text{Open\mbox{-}QA}}\}}
%     \frac{\|\mathbf{x}_g - \mathbf{i}_g\|_2^2}{|g|}
%   \right)^{\tfrac12},
%   \label{eq:distance_to_ideal}
% \end{align}
% where $\mathbf{x}_g$ denotes the score of method $\mathbf{x}$ on metric $g$, and $\mathbf{i}_g$ represents the corresponding score of the ideal unlearned model.

% In our experiments, $\mathrm{UT_\text{MCQ}}$ includes MMLU, TruthfulQA, and MathQA; $\mathrm{UT_\text{Open\mbox{-}QA}}$ includes IFEval and GSM8K; $\mathrm{UE_\text{MCQ}}$ corresponds to accuracy on the WMDP evaluation set; and $\mathrm{UE_\text{Open\mbox{-}QA}}$ is measured by Entailment Score (ES) on WMDP. 

% \begin{wraptable}{r}{0.25\linewidth}  % 表格放在右边，占 0.25 文本宽度
%   \centering
%   \caption{Distance to the ideal unlearned model for twelve unlearning methods. Smaller values indicate stronger alignment with the ideal unlearned model.}
%   \label{tab:distance_ranking}
%   \resizebox{0.95\linewidth}{!}{%
%     \begin{tabular}{l c}
%       \toprule[1pt]
%       \toprule
%       \textbf{Method} & \textbf{Distance $\downarrow$} \\
%       \midrule
%       \rowcolor{purple!10} Ideal     & 0.00 \\
%       \rowcolor{gray!10} RMU         & 0.25 \\
%       RMU+LAT     & 0.26 \\
%       \rowcolor{gray!10} RR          & 0.29 \\
%       ELM         & 0.31 \\
%       \rowcolor{gray!10} SimNPO      & 0.38 \\
%       GradDiff    & 0.61 \\
%       \rowcolor{gray!10} NPO+SAM     & 0.63 \\
%       NPO         & 0.64 \\
%       \rowcolor{gray!10} NPO+IRM     & 0.66 \\
%       TAR         & 0.67 \\
%       \rowcolor{gray!10} Original    & 0.72 \\
%       \bottomrule
%       \bottomrule[1pt]
%     \end{tabular}%
%   }
% \end{wraptable}

% To validate the soundness of this metric, we also include the original model as a baseline. 
% As shown, the original model lies furthest from the ideal unlearned model, which is consistent with expectations and confirms the rationality of our metric. 
% Among all methods, RMU achieves the best alignment, while representation-mismatch approaches in general outperform divergence-driven methods. 
% Within the divergence-driven family, SimNPO demonstrates the strongest performance.



% Insight 1. Evaluating utility cannot rely solely on MMLU; a broader set of utility metrics is required. Utility evaluation should jointly consider both MCQ-based and Open-QA methods.
% Figure: Radar plot of utility metrics.

% \begin{figure*}[h]
%     % \vspace*{-1em}
%     \centering
%     \includegraphics[width=0.4\linewidth]{figs/radar_utility.pdf}
%     % \vspace*{-1.7em}
%     \caption{}
%     \label{fig:radar_utility}
%     % \vspace*{-2em}
% \end{figure*}

% Insight 2. Unlearning evaluation also needs to account for both MCQ-based and Open-QA methods.
% Figure: 2D plot of logits prediction vs. ES score.




% Insight 3. Define tradeoff. Not consider robustness. Give rank.
% What's new tradeoff?
% What's the rank?
% Robust?

% Insight 4. Targeted unlearning vs. Non-targeted unlearning. @Soumyadeep

% Insight 5: MCQ UE max sentence/token prediction.