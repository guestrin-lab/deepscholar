\begin{abstract}
\iffalse 
With the rapid deployment of large language models, a pressing challenge is how to effectively and reliably unlearn undesirable knowledge while preserving general utility. This paper establishes a principled taxonomy to comprehensively examine existing unlearning methods. We categorize twelve stateful approaches into three families: \DivO, \RepM, and \RejU. These methods are systematically evaluated on widely used benchmarks such as WMDP and MUSE, with a focus on the trade-offs between unlearning effectiveness, utility, and robustness. Our analysis shows that \DivO (\textit{e.g.}, NPO) refine gradient ascent with bounded objectives but remain highly sensitive under generation-based evaluations. \RepM (\textit{e.g.}, RMU) achieve stronger alignment with the ideal unlearned model and exhibit greater robustness to out-of-domain fine-tuning. \RejU enforce explicit refusal behaviors but introduce notable utility degradation. Beyond method design, we demonstrate the limitations of current evaluation practices that rely primarily on MCQ tasks, arguing that Open-QA tasks is essential to capture the full spectrum of unlearning effectiveness and utility. Finally, we introduce a novel distance-to-ideal metric that redefines the trade-off between effectiveness and utility, providing a principled ranking of unlearning methods. Overall, this work delivers a comprehensive benchmark and actionable insights to guide the design and evaluation of unlearning techniques for large language models.
\fi 
Machine unlearning for large language models (LLMs) aims to remove \textit{undesired} data, knowledge, and behaviors (\textit{e.g.}, for safety, privacy, or copyright) while preserving useful model capabilities. Despite rapid progress over the past two years, research in LLM unlearning remains fragmented, with limited clarity on what constitutes effective unlearning and how it should be rigorously evaluated. In this work, we present a principled taxonomy of \textit{twelve} recent stateful unlearning methods, grouped into three methodological families: {\MDiv}, {\MRep}, and {\MRej}. Building on this taxonomy, we revisit the evaluation of unlearning effectiveness (UE), utility retention (UT), and robustness (Rob), focusing on the WMDP benchmark. Our analysis shows that current evaluations, dominated by multiple-choice question (MCQ) accuracy, offer only a narrow perspective, often overstating success while overlooking the model’s actual generation behavior. To address this gap, we introduce open question-answering (Open-QA) metrics that better capture generative performance and reveal the inherent UE–UT tradeoff across method families. Furthermore, we demonstrate that robustness requires finer-grained analysis: For example, vulnerabilities differ substantially between \textit{in-domain relearning} and \textit{out-of-domain fine-tuning}, even though both fall under model-level attacks. Through this study, we hope to deliver a full-stack revisit of LLM unlearning and actionable guidance for designing and evaluating future methods.
\end{abstract}