\section{Related Work}

% \textbf{Machine unlearning in LLMs.} Recent studies on machine unlearning in large language models (LLMs) have shown encouraging progress in alleviating risks~\citep{liu2024rethinking, maini2024tofu, liu2024large, yao2023large} such as copyright infringement~\citep{eldan2023whos}, privacy leakage~\citep{hu2024jogging,wu2023depn}, and harmful content generation~\citep{li2024wmdp, lu2022quark}. Current approaches to unlearning can be broadly divided into three families. The first family builds on objectives that drive the unlearned model’s predictions away from those of the original or a reference model, a strategy denoted as {\MDivB}~\citep{zhang2024negative, fan2024simplicity}. Complementary to this, {\MRepB} refers to methods that intervene the representation space by projecting the latent embeddings of unlearned targets toward random or orthogonal directions, thereby preventing the model from correctly answering queries related to the corresponding content~\citep{zou2024improving, li2024wmdp, sheshadri2024latent}. Finally, {\MRejB} guides the model to align its outputs with a predefined library of refusal statements whenever prompts tied to the unlearned content are encountered~\citep{rafailov2024direct, yuan2024closer, singh2025unlearning, gandikota2024erasing}. Together, these approaches demonstrate the versatility of current LLM unlearning research in balancing the dual objectives of reliable forgetting and utility preservation.

\noindent \textbf{LLM unlearning and benchmarking studies.} Recent work on LLM unlearning~\citep{liu2024rethinking,maini2024tofu,liu2024large,yao2023large} has made progress in mitigating risks such as copyright infringement~\citep{eldan2023whos}, privacy leakage~\citep{hu2024jogging,wu2023depn}, and harmful content generation~\citep{li2024wmdp,lu2022quark}. For detailed introductions to the diverse families of unlearning methods, we refer readers to Sec.\,\ref{sec:toxonomy}.

Given the growing interest in LLM unlearning, several benchmarks have been proposed to systematically evaluate its effectiveness~\citep{li2024wmdp,maini2024tofu,jin2024rwku,shi2024muse,eldan2023whos}. These benchmarks can be grouped into two categories based on whether models are fine-tuned on the forget corpus. The first category fine-tunes models on domain-specific corpora to introduce unlearning targets. WHP~\citep{eldan2023whos} uses the Harry Potter series, TOFU~\citep{maini2024tofu} constructs synthetic author profiles, and MUSE~\citep{shi2024muse} provides Harry Potter and news corpora with privacy-leakage evaluation. A drawback is that domain-specific fine-tuning can degrade general abilities such as reasoning, complicating fair utility assessment~\citep{jin2024rwku}. The second category avoids fine-tuning, aligning more closely with real-world use cases.
% The first category fine-tunes models on domain-specific corpora to ensure the presence of unlearning targets. For example, WHP~\citep{eldan2023whos} fine-tunes on the Harry Potter series and evaluates forgetting via completion probabilities on forget prompts, with utility tested on Winogrande~\citep{sakaguchi2021winogrande} and Hellaswag~\citep{zellers2019hellaswag}. TOFU~\citep{maini2024tofu} builds synthetic author profiles and measures unlearning using ROUGE-based comparisons across forget, retain, real-author, and world-fact sets. MUSE~\citep{shi2024muse} provides Harry Potter and news corpora, evaluates verbatim and knowledge memorization on forget/retain sets, and introduces a privacy-leakage metric. A key drawback of this category is that domain-specific fine-tuning often degrades general abilities such as reasoning, complicating fair utility assessment~\citep{jin2024rwku}. The second category avoids fine-tuning, aligning more closely with real-world scenarios. 
WMDP~\citep{li2024wmdp} constructs forget corpora in biology, chemistry, and cybersecurity, and evaluates unlearning via multiple-choice QA proxies for hazardous knowledge, alongside utility on MMLU~\citep{hendrycks2020measuring} and MT-Bench~\citep{zheng2023judging}. RWKU~\citep{jin2024rwku} focuses on real-world entities, evaluating memorization on the forget set and reasoning, truthfulness, factuality, and fluency on retain tests. 
%A common limitation across benchmarks is their narrow scope, as most restrict evaluation to a single format (e.g., multiple-choice or open-ended QA), limiting the comprehensiveness of unlearning assessment.

% Given the popularity of LLM unlearning, several benchmarking efforts have been proposed to systematically evaluate their effectiveness~\citep{li2024wmdp, maini2024tofu, jin2024rwku, shi2024muse, eldan2023whos}. Existing benchmarks can be roughly divided into two categories depending on whether models are needed to be fine-tuned on the forget corpus. The first category fine-tunes models on a domain corpus to guarantee the presence of unlearning targets. WHP~\citep{eldan2023whos} fine-tunes models on the Harry Potter series and evaluates unlearning by measuring completion probabilities on forget prompts, while assessing utility on general benchmarks such as Winogrande~\citep{sakaguchi2021winogrande} and Hellaswag~\citep{zellers2019hellaswag}. TOFU~\citep{maini2024tofu} constructs synthetic author profiles and evaluates unlearning by comparing model outputs with ground truth across the forget set, retain set, real authors, and world-fact questions using ROUGE-based scores. MUSE~\citep{shi2024muse} provides both Harry Potter books and news corpora, and evaluates verbatim and knowledge memorization on forget and retain sets, while also introducing a privacy-leakage metric to measure the extent of unlearning. A limitation of this category is that domain-specific fine-tuning often degrades general abilities such as reasoning, making it difficult to fairly assess broader utility~\citep{jin2024rwku}. The second category avoids fine-tuning, thereby aligning more closely with real-world unlearning scenarios. WMDP\citep{li2024wmdp} constructs forget corpora in the biology, chemistry, and cybersecurity domains, and evaluates unlearning through multiple-choice QA tasks as proxies for hazardous knowledge, complemented by utility assessments on MMLU~\citep{hendrycks2020measuring} and MT-Bench~\citep{zheng2023judging}. RWKU~\citep{jin2024rwku} focuses on knowledge of real-world entities and evaluates by probing memorization on the forget set, while retain tests cover reasoning, truthfulness, factuality, and fluency. Across these benchmarks, a common limitation is their narrow evaluation scope, most provide restrict evaluation to a single format such as multiple-choice or open-ended QA. This limited design constrains the comprehensiveness of unlearning assessment. 


%Existing approaches can be broadly categorized into three families. {\MDivB} methods optimize objectives that push the unlearned model’s predictions away from those of the original or a reference model~\citep{zhang2024negative,fan2024simplicity}. {\MRepB} methods intervene in the representation space by projecting the latent embeddings of forget targets toward random or orthogonal directions, thereby disrupting the model’s ability to correctly answer related queries~\citep{zou2024improving,li2024wmdp,sheshadri2024latent}. {\MRejB} methods enforce explicit refusals by aligning outputs with a predefined library of rejection statements whenever prompts tied to the unlearned content are encountered~\citep{rafailov2024direct,yuan2024closer,singh2025unlearning,gandikota2024erasing}. 
%Collectively, these approaches illustrate the methodological diversity of LLM unlearning and highlight the central challenge of balancing reliable forgetting with utility preservation.


% 强调每一个benchmark 提供的 evaluation， 最后说一下局限性

% Full version
% \textbf{LLM unlearning benchmarking.} Furthermore, with the rapid emergence of various LLM unlearning methods, a series of recent benchmarking efforts have also been proposed, providing structured frameworks for evaluating the effectiveness of unlearning algorithms~\citep{li2024wmdp, maini2024tofu, jin2024rwku, shi2024muse, eldan2023whos}. Based on whether the original model needs to be fine-tuned on the forget corpus before unlearning, existing benchmarks can be roughly categorized into two groups. The first group requires fine-tuning the model on a domain-specific forget corpus to ensure the presence of unlearning targets. The "Who is Harry Potter" (WHP) benchmark~\citep{eldan2023whos} first fine-tunes models on the Harry Potter series to ensure the presence of domain-specific knowledge. It then defines 300 prompts from this universe as the forget set. The unlearning effectiveness is evaluated by inspecting the model’s completion probabilities on the forget prompts, whereas general utility is assessed using standard benchmarks such as Winogrande~\citep{sakaguchi2021winogrande} and Hellaswag~\citep{zellers2019hellaswag} to measure language understanding capabilities. Similarly, the TOFU benchmark~\citep{maini2024tofu} is designed to unlearn with a dataset of 200 diverse synthetic author profiles based on question-answer pairs format, which enables precise control over the knowledge source to be unlearned. A designated subset of profiles constitutes the forget set, while the remaining profiles serve as the retain set. Evaluation spans multiple distributions—the forget set, the retain set, real authors, and world-fact questions—and quantifies unlearning by comparing model outputs to ground-truth answers using ROUGE-based score. The MUSE benchmark~\citep{shi2024muse} further extends this paradigm by providing separate corpora: a Harry Potter books dataset paired with FanWiki~\cite{} content (retain set), and a BBC News dataset randomly split into disjoint forget and retain sets. Evaluations include QA-based tests of verbatim and knowledge memorization, as well as a proposed privacy leakage metric for over-unlearning detection. 

% However, all benchmarks in this category face a critical limitation: once the model is fine-tuned on a narrow domain, it tends to lose general capabilities such as mathematical reasoning and logical generation, making it difficult to assess the broader utility trade-offs of unlearning methods~\citep{jin2024rwku}.

% The second group of benchmarks does not require fine-tuning, thereby aligning more closely with real-world unlearning scenarios. A representative example is the Weapons of Mass Destruction Proxy Benchmark (WMDP)~\citep{li2024wmdp}, which constructs a forget corpus by collecting a large set of articles related to biosecurity, cybersecurity, and chemical security. The unlearning effect is then evaluated on 3,668 multiple-choice questions derived from these domains, serving as proxies for hazardous knowledge. In addition, model utility is assessed through general-purpose benchmarks such as MMLU~\citep{hendrycks2020measuring} and MT-Bench~\citep{zheng2023judging}. Another example is RWKU (Real-World Knowledge Unlearning)~\cite{jin2024rwku}, which focuses on concept-level unlearning rather than domain-specific corpora. RWKU targets knowledge about 200 real-world famous people, and evaluates models via knowledge memorization on the forget set, while assessing retain performance across general ability, reasoning, truthfulness, factuality, and fluency. 

% \textbf{LLM unlearning benchmarking.}
% With the rapid emergence of LLM unlearning methods, several benchmarking efforts have been proposed to systematically evaluate their effectiveness~\citep{li2024wmdp, maini2024tofu, jin2024rwku, shi2024muse, eldan2023whos}. Existing benchmarks can be roughly divided into two categories depending on whether models are needed to be fine-tuned on the forget corpus. The first category fine-tunes models on a domain corpus to guarantee the presence of unlearning targets. WHP~\citep{eldan2023whos} fine-tunes models on the Harry Potter series and evaluates unlearning by measuring completion probabilities on forget prompts, while assessing utility on general benchmarks such as Winogrande~\citep{sakaguchi2021winogrande} and Hellaswag~\citep{zellers2019hellaswag}. TOFU~\citep{maini2024tofu} constructs synthetic author profiles and evaluates unlearning by comparing model outputs with ground truth across the forget set, retain set, real authors, and world-fact questions using ROUGE-based scores. MUSE~\citep{shi2024muse} provides both Harry Potter books and news corpora, and evaluates verbatim and knowledge memorization on forget and retain sets, while also introducing a privacy-leakage metric to measure the extent of unlearning. A limitation of this category is that domain-specific fine-tuning often degrades general abilities such as reasoning, making it difficult to fairly assess broader utility~\citep{jin2024rwku}. The second category avoids fine-tuning, thereby aligning more closely with real-world unlearning scenarios. WMDP\citep{li2024wmdp} constructs forget corpora in the biology, chemistry, and cybersecurity domains, and evaluates unlearning through multiple-choice QA tasks as proxies for hazardous knowledge, complemented by utility assessments on MMLU~\citep{hendrycks2020measuring} and MT-Bench~\citep{zheng2023judging}. RWKU~\citep{jin2024rwku} focuses on knowledge of real-world entities and evaluates by probing memorization on the forget set, while retain tests cover reasoning, truthfulness, factuality, and fluency. Across these benchmarks, a common limitation is their narrow evaluation scope, most provide restrict evaluation to a single format such as multiple-choice or open-ended QA. This limited design constrains the comprehensiveness of unlearning assessment. 




% \textbf{Adversarial robustness of unlearning: Attack and defense.} A central challenge in machine unlearning is robustness, as numerous studies have shown that unlearned models remain vulnerable to a wide range of attacks. At the parameter level, adversaries can directly alter model weights through fine-tuning or structural modifications: for example, \citet{lucki2024adversarial} showed that training on as few as ten unrelated examples can restore most of the forgotten knowledge, while \citet{hu2024jogging} introduced relearning attacks where small amounts of related data rapidly recover unlearned content. Other works further demonstrated that pruning can reverse forgetting and that unlearning methods can be brittle to quantization approaches, which re-expose forgotten knowledge by compressing model parameters. At the representation level, attacks manipulate latent states by perturbing embeddings or hidden activations, effectively reactivating forgotten traces; recent work on activation engineering demonstrates how simple transformations to hidden representations can significantly influence model behavior. At the input level, adversaries operate through adversarial prompting or query optimization, crafting inputs that elicit memorized content from supposedly unlearned models; typical approaches include gradient-guided methods such as Greedy Coordinate Gradient as well as perplexity-guided beam search strategies. While these attack vectors highlight the fragility of current unlearning algorithms, most evaluations have relied exclusively on the WMDP benchmark, which focuses on multiple-choice QA as the sole evaluation format, thereby introducing bias and limiting the comprehensiveness of robustness assessment.

% \SL{Is this a relevant work ``Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs''.}
% \CW{This approach focuses on filtering out biosecurity-related content during the pretraining stage, representing a retrain-oriented strategy for addressing harmful contexts. It does not seem necessary for us to include it in our work.}

\noindent \textbf{Adversarial robustness of LLM unlearning.} Robustness has emerged as a central challenge for unlearning, as unlearned models remain vulnerable to diverse attacks~\citep{lucki2024adversarial,che2025model,hu2024jogging}. Parameter-level attacks can restore forgotten content through light fine-tuning~\citep{lucki2024adversarial,hu2024jogging,qi2023fine,halawi2024covert,lermen2023lora,huang2024harmful}, pruning~\citep{jain2023mechanistically,wei2024assessing,lee2018snip}, or quantization~\citep{zhang2024catastrophic}, which may re-expose knowledge by compressing weights. Representation-level attacks perturb embeddings or hidden activations to revive residual traces~\citep{schwinn2024soft,sheshadri2024latent}, while input-level attacks exploit adversarial prompting or query optimization~\citep{chao2025jailbreaking,shin2020autoprompt}, ranging from gradient-guided search~\citep{zou2023universal} to perplexity-based strategies~\citep{sadasivan2024fast}. In response, several defenses have been proposed. Adversarial training strengthens robustness against such attacks~\citep{sheshadri2024latent}, and meta-learning strategies further enhance defense~\citep{tamirisa2024tamper,sondej2025robust}. Sharpness-aware minimization encourages flat minima to reduce susceptibility to relearning~\citep{fan2025towards}. Invariance-based regularization introduces robustness through invariant risk principles~\citep{wang2025invariance}, while distillation-based methods transfer knowledge into partially noised student models~\citep{lee2025distillation}.  Although robustness benchmarks are emerging~\citep{che2025model,hu2025blur}, they remain limited: \citet{che2025model} evaluates only with MCQ accuracy, and \citet{hu2025blur} considers only in-domain relearning, leaving comprehensive assessment open.  


% \textbf{Adversarial robustness of LLM unlearning.} Robustness has emerged as an important challenge for unlearning, as unlearned models remain vulnerable to diverse attacks~\citep{lucki2024adversarial, che2025model, hu2024jogging}. Parameter-level attacks restore forgotten content via light fine-tuning~\citep{lucki2024adversarial,hu2024jogging, qi2023fine,halawi2024covert, lermen2023lora, huang2024harmful}, pruning~\citep{jain2023mechanistically,wei2024assessing,lee2018snip}, or even quantization~\citep{zhang2024catastrophic}, which can re-expose knowledge by compressing weights. Representation-level attacks perturb embeddings or hidden activations to revive residual traces~\citep{schwinn2024soft, sheshadri2024latent}, while input-level attacks rely on adversarial prompting or query optimization~\citep{chao2025jailbreaking,shin2020autoprompt}, ranging from gradient-guided search~\citep{zou2023universal} to perplexity-based strategies~\citep{sadasivan2024fast}. As attack methods evolve, a line of work has proposed more robust unlearning designs. Adversarial training has been adopted to strengthen unlearning robustness against attacks~\citep{sheshadri2024latent}, and also extends with meta-learning strategies to further improve defense~\citep{tamirisa2024tamper, sondej2025robust}. In parallel, other studies leverage sharpness-aware minimization to encourage flat minima, thereby reducing the model’s susceptibility to relearning~\citep{fan2025towards}. Invariant LLM unlearning introduces invariance-based regularization~\citep{wang2025invariance}, and distillation-based approaches enhance robustness by transferring knowledge into partially noised student models~\citep{lee2025distillation}. Although benchmarks for evaluating algorithmic robustness have recently emerged~\citep{che2025model,hu2025blur}, \citet{che2025model} relies solely on multiple-choice accuracy to measure unlearning effectiveness, and \citet{hu2025blur} focuses only on a single type of relearning attack, neither providing a comprehensive assessment of robustness.



