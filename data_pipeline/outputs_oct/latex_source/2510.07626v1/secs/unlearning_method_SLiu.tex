\section{A Taxonomy of Stateful Unlearning Methods: Methodologies, Design Principles, and Insights}
\label{sec:toxonomy}

%\SL{[Please always have a summary paragraph in the beginning to lay our the flow of each section.]}
% In this section, we present a taxonomy of LLM unlearning methods by reviewing \textit{12} stateful approaches grouped into three families: \textit{(i) {\MDiv}}, \textit{(ii) {\MRep}}, and \textit{(iii) {\MRej}}. These methods were selected for their broad coverage of existing unlearning paradigms and relevance across established benchmarks. Our taxonomy highlights key design choices unique to unlearning, providing a foundation for principled cross-family comparisons and for understanding when and why each method is (in)effective in the remainder of the paper.  



\noindent \textbf{Problem setup for LLM unlearning.}
The unlearning problem is defined with respect to a subset of data instances that must be erased, denoted as the \textit{forget set} ($\Df$). To preserve model utility, one also specifies a complementary \textit{retain set} ($\Dr$) whose knowledge should remain unaffected. These two sets capture the dual objectives of unlearning: removing information from $\Df$ while maintaining the model's general utility as reflected by $\Dr$. This trade-off can be formulated as a regularized optimization problem \citep{liu2024rethinking}:
\begin{align}
%\hspace*{-3mm}
\begin{array}{ll}
 \displaystyle \minimize_{\boldsymbol{\theta}}   
&  \ellf (\boldsymbol{\theta}; {\Df})  
 + \lambda  \ellr(\boldsymbol{\theta}; {\Dr})  ,
\end{array}
%\hspace*{-3mm}
\label{eq:prob_LLM_MU}
\end{align}
where $\btheta$ are the model parameters to be updated, $\lambda \geq 0$ is a regularization weight balancing forgetting and retention, and $\ellf$ and $\ellr$ denote loss terms for the forget and retain objectives, respectively.

A large body of work has investigated different design choices for instantiating~\eqref{eq:prob_LLM_MU} \citep{yao2023large, zhang2024negative, fan2024simplicity, fan2025towards, wang2025invariance, li2024wmdp, zou2024improving, gandikota2024erasing, sheshadri2024latent, tamirisa2024tamper, yuan2024closer, singh2025unlearning}. Based on their methodological principles, we categorize existing approaches into three families: {\MDiv}, {\MRep}, and {\MRej}. See detailed insights below.


% A considerable body of work has focused on designing and analyzing suitable forget and retain loss functions to address problem \eqref{eq:prob_LLM_MU} \citep{yao2023large, zhang2024negative, fan2024simplicity, fan2025towards, wang2025invariance, li2024wmdp, zou2024improving, gandikota2024erasing, sheshadri2024latent, tamirisa2024tamper, yuan2024closer, singh2025unlearning}. 
% According to the underlying principles of LLM unlearning, existing approaches can be broadly categorized into {\MDiv}, {\MRep}, and {\MRej}. These methods have been used and validated in different existing unlearning benchmark tasks. The commonly-considered ones include WMDP for hazardous knowledge removal \cite{li2024wmdp}, MUSE for copyrighted information removal \cite{shi2024muse}, TOFU for fictions data removal \cite{maini2024tofu}, WHP for Harry Potter book series information removal \cite{eldan2023whos}, PKU-SafeRLHF for ..., LKF for ..., RWKU for ..., and Circuit Breaker for ....

\noindent \textbf{Divergence-driven optimization for unlearning.} 
The first family of methods designs the forget loss $\ellf$ in~\eqref{eq:prob_LLM_MU} to maximize the ``divergence'' between the prediction logits of the unlearned model and the reference (original) model on the forget set $\Df$. We refer to this class as {\MDiv}, since it explicitly drives the model away from the reference model.  

The most basic instance is gradient ascent (GA) \citep{thudi2022necessity}, which directly increases the prediction loss on $\Df$. However,  GA often pushes the model too far from the reference, leading to collapse \citep{zhang2024negative}. To address this, several GA-type variants have been proposed. Gradient difference (\textbf{GradDiff}) \citep{yao2023large} balances objectives by applying gradient ascent on $\Df$ while using gradient descent on $\Dr$, thereby controlling divergence from the reference.  
Negative preference optimization (\textbf{NPO}) \citep{zhang2024negative} interprets forget samples as negative preferences within the DPO (direct preference optimization) \citep{rafailov2024direct}. This  specifies $\ellf$ with 
\begin{align}
\ell_{\mathrm{NPO}}(\btheta) =  
\mathbb E_{(x,y) \in \Df} \left[
- \tfrac{2}{\beta} \log \sigma \Big( - \beta \log \tfrac{\pi_{\btheta}(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)} \Big)
\right], \label{eq:NPO_loss}
\end{align}
where $\pi_{\btheta}(y \mid x)$ represents the prediction probability of the model $\btheta$ given
the input-response pair $(x,y)$, and  $\pi_{\mathrm{ref}}$ refers to the reference model.
Simple NPO (\textbf{SimNPO}) \citep{fan2024simplicity} further mitigates reference-model bias in NPO by modifying $\ellf$ to
$- \tfrac{2}{\beta} \log \sigma \!\left( - \tfrac{\beta}{|y|}\log \pi_{\btheta}(y \mid x) \right)$.

Building on NPO, recent works have further addressed robustness gaps in LLM unlearning, particularly sensitivity to model-level attack after unlearning. Examples include \textbf{NPO+SAM} \citep{fan2025towards}, which leverages sharpness-aware minimization (SAM) \citep{foret2021sharpnessaware} to flatten the forget loss landscape, and \textbf{NPO+IRM} \citep{wang2025invariance}, which applies invariant risk minimization (IRM) \citep{arjovsky2019invariant} to encourage robustness across distributional variations.

% To emphasize, {\MDivB} methods inherit the simplicity of GA but refine it with regularization or bounded losses to prevent collapse and achieve more stable forgetting. Building on NPO, recent works have further addressed robustness gaps in LLM unlearning, particularly sensitivity to weight perturbations after unlearning. Examples include \textbf{NPO+SAM} \citep{fan2025towards}, which leverages sharpness-aware minimization (SAM) \citep{foret2021sharpnessaware} to flatten the forget loss landscape, and \textbf{NPO+IRM} \citep{wang2025invariance}, which applies invariant risk minimization (IRM) \citep{arjovsky2019invariant} to encourage robustness across distributional variations.


\noindent \textbf{Representation misalignment for unlearning.}
% In contrast to {\MDivB}, which enforces divergence at the output level, another
Another class of methods operates on internal representations, seeking to misalign the representations of $\Df$ with their original representations in the reference model. We term this family {\MRep}. The principle originates from early methods \citep{golatkar2020eternal,fan2023salun} that inject randomness into forget data to disrupt memorization and reduce alignment on $\Df$.
% The underlying principle traces back to early unlearning techniques such as random feature perturbation or random relabeling \cite{golatkar2020eternal,fan2023salun}, which inject randomness into the learned representations of forget data to disrupt memorization and weaken alignment on $\Df$.
The most popular approach wihtin this family is 
representation misdirection for unlearning (\textbf{RMU}) \citep{li2024wmdp}, where the hidden states of the unlearned model are mapped to a random vector. This formulates the forget loss $\ellf$ as 
\begin{align}
 % & \hspace*{-2mm} 
  \ell_{\mathrm{RMU}}(\boldsymbol{\theta}) = 
  \mathbb E_{ x \in \Df} \left[
    || {M_{\boldsymbol{\theta}}(x) - c \cdot {\mathbf u}||}_2^2 
  \right],
  \label{eq:RMU_loss}
\end{align}
where $M_{\boldsymbol{\theta}}$ represents certain intermediate-layer representations of $\btheta$, $c > 0$ is a hyperparameter that controls activation scaling, and $\mathbf{u}$ is a random vector drawn from a standard uniform distribution.

% \SL{[The following paragraph is not clear. See how I revised for NPO part. Highlight and introduce the method name you will cover.]} \CF{Revised.} 
% In addition, representation Rerouting (\textbf{RR}) \citep{zou2024improving} replaces the $l2$ norm in $\ell_f$ of RMU with the cosine similarity between the unlearned model $M_{\boldsymbol{\theta}}$ and the reference model. Other works, Tampering Attack Resistance (\textbf{TAR}) \citet{tamirisa2024tamper} and Latent Adversarial Training (\textbf{LAT}) \citet{sheshadri2024latent}, combine RMU with meta-learning or adversarial training to enhance robustness.

In addition, representation rerouting (\textbf{RR}) \citep{zou2024improving} modifies RMU by replacing the $\ell_2$ norm in $\ell_f$ with cosine similarity between the unlearned model $M_{\boldsymbol{\theta}}$ and the reference model. Other approaches, such as tampering attack resistance (\textbf{TAR}) \citep{tamirisa2024tamper} and latent adversarial training (\textbf{LAT}) \citep{sheshadri2024latent}, build on RMU with meta-learning or adversarial training in the latent space to improve unlearning robustness.  


\noindent \textbf{Rejection-based unlearning.}
Unlike divergence-driven or representation-misalignment approaches, which perform \textit{untargeted} unlearning by discouraging alignment with the forget set, the {\MRej} family enforces \textit{targeted} unlearning through explicit rejection responses to forget queries.  
%We denote this family as {\MRej}, where the model is encouraged to respond with a rejection token or distribution instead of producing knowledge from $\Df$.  
%
A representative method is the I Don’t Know (\textbf{IDK}) strategy \citep{maini2024tofu}, which defines the forget loss $\ellf$ as the prediction loss over rejection-labeled forget data:
\begin{align}
 \ell_{\mathrm{IDK}}(\btheta) =  
 \mathbb E_{(x \in \Df, y \in \mathcal{D}_\text{IDK})} 
 \left [ - \log \pi_{\btheta}(y \mid x) \right ] ,
 \label{eq:IDK_loss}
\end{align}
where $\mathcal{D}_\text{IDK}$ denotes the set of rejection labels expressed in different formats.

Beyond the simplest IDK formulation, \textbf{DPO} \citep{rafailov2024direct,zhang2024negative} can also be adapted for unlearning by treating rejection as the positive response for forget data. Other extensions include \textbf{IDK+AP} \citep{yuan2024closer}, which, similar in spirit to DPO, introduces an answer preservation (AP) loss that regards the normal response as positive on retain data and the rejection response as positive on forget data, thereby augmenting IDK with an additional alignment objective.
Similarly, erasing via language modeling (\textbf{ELM}) \citep{gandikota2024erasing} aligns the unlearned model’s outputs with those of a prompted reference model, using a predefined prefix (\textit{e.g.}, \textit{``As a novice in bioweapons''}) to steer responses toward refusal-like outputs.

% \input{tabs/unlearn_method}

\noindent \textbf{Benchmarks and evaluations.} The aforementioned methods have been validated under different unlearning benchmarks, such as WMDP for hazardous knowledge removal \citep{li2024wmdp}, MUSE for copyrighted content removal \citep{shi2024muse}, TOFU for fictional data removal \citep{maini2024tofu}, WHP for ``Harry Potter'' book series knowledge \citep{eldan2023whos}, PKU-SafeRLHF for harmful content removal \citep{ji2024pku} and Circuit Breaker for toxic content removal \citep{zou2024improving}.

\input{tabs/unlearn_method}
% Although there is no consensus on the \textit{most appropriate} benchmarks for evaluating unlearning, we adopt \textbf{WMDP} \citep{li2024wmdp} in this work because (i) it emphasizes practical capability removal by erasing harmful knowledge generation, and (ii) it does not require additional fine-tuning on the forget set to first memorize the knowledge to be forgotten. Given the widespread use of the WMDP-Bio dataset (which contains biological information for harm reduction) in the LLM unlearning literature, we refer to WMDP as WMDP-Bio for ease of presentation. We may also cover \textbf{MUSE} \citep{shi2024muse} for supplementary dataset validation.

Although no consensus exists on the \textit{most appropriate} benchmarks for unlearning, we adopt \textbf{WMDP} \citep{li2024wmdp} for its focus on erasing harmful knowledge without requiring extra fine-tuning on the $\Df$. Given the common use of  WMDP-Bio (which contains biological knowledge for harm reduction), we refer to WMDP as WMDP-Bio. For supplementary validation, we may also consider \textbf{MUSE} \citep{shi2024muse}.


Unlearning performance is most commonly evaluated in terms of unlearning effectiveness (\textbf{UE}) and utility retention (\textbf{UT}).  
The UE and UT evaluation metrics in existing benchmarks can be generally classified into two types: \textit{(i) multiple-choice questions (\textbf{MCQ})}, where the model selects from predefined options, and \textit{(ii) open question-answering (\textbf{Open-QA})}, where the model generates free-form answers. Therefore, we denote $\mathrm{UE}_{\mathrm{MCQ}}$ (or $\mathrm{UT}_{\mathrm{MCQ}}$) and $\mathrm{UE}_{\mathrm{Open\text{-}QA}}$ (or $\mathrm{UT}_{\mathrm{Open\text{-}QA}}$) as the corresponding UE (or UT) metrics, respectively. In addition to UE and UT, robustness (\textbf{Rob}) has emerged as another critical dimension in evaluating LLM unlearning. We distinguish two types of robustness assessments: \textit{model-level attacks}, such as in-domain relearning \citep{hu2024jogging, fan2025towards} and out-of domain fine-tuning \citep{wang2025invariance}; and \textit{input-level attacks}, such as  jailbreaking attack \citep{lucki2024adversarial}.
% \CW{No quantization?} 

\textbf{Table\,\ref{tab:taxonomy}} summarizes 12 unlearning approaches with their benchmark applications and evaluation metrics, where benchmark abbreviations denote both methods and datasets. \textbf{Appendix\,\ref{appx:exp_setup}} outlines key implementation details. In the remainder, we revisit these approaches to assess unlearning evaluation (Sec.\,\ref{sec:eval_UE_UT}) and robustness (Sec.\,\ref{sec:eval_Rob}), uncovering overlooked insights into LLM unlearning.


% \textbf{Table\,\ref{tab:taxonomy}} summarizes the taxonomy of 12 unlearning approaches considered in this work, along with their benchmark applications and evaluation metrics. Benchmark abbreviations are used to denote both the evaluation methods and their corresponding datasets.  
% \textbf{Appendix\,\ref{appx:exp_setup}} provides key implementation details of the unlearning methods.
% In the remainder of the paper, we revisit these approaches to examine unlearning effectiveness and utility (Sec.\,\ref{sec:eval_UE_UT}) and robustness (Sec.\,\ref{sec:eval_Rob}) across different methodological families in Table\,\ref{tab:taxonomy}, uncovering overlooked insights into LLM unlearning.  


% \SL{[Add argument why prefers WMDP to others? No need of additional fine-tuning to overshoot to dataset before unlearning? more precisely reflect model behavior ...?]}
% \SL{In the remainder of the paper, we revisit these approaches, examining unlearning effectiveness and model utility (Sec.\,\ref{sec:eval_UE_UT}), and robustness (Sec.\,\ref{sec:eval_Rob}) across the methodological families in Table\,\ref{tab:taxonomy} to uncover overlooked insights into LLM unlearning.}

% \CF{TODO: Add unlearning setting.}


% In \textbf{Table\,\ref{tab:unlearn_method}}, we provide a summary of twelve representative unlearning methods and indicate whether they incorporate a robust design. 
% The evaluation is organized along three key dimensions: unlearning effectiveness (UE), utility (UT), and robustness evaluation (RE). 
% For UE and UT, the reported metrics are further divided according to the evaluation protocol into multiple-choice question (MCQ) and open question-answer (Open-QA) settings. 
% For RE, we distinguish between model-level attacks (e.g., in-domain relearning \citep{hu2024jogging,fan2025towards} and out-of-domain fine-tuning \citep{wang2025invariance}) and input-level attacks (e.g., jailbreaking \citep{lucki2024adversarial}). 
% The letters denote the benchmarks used in the corresponding studies, such as W for WMDP \citep{li2024wmdp}. 
% As shown in the table, WMDP emerges as one of the most frequently adopted benchmarks, and therefore our subsequent experiments will primarily focus on WMDP.



% \begin{table}[h!]
% \centering
% \caption{}
% \renewcommand{\arraystretch}{1.25}
% \vspace{2mm}
% \resizebox{0.55\textwidth}{!}{% % Increased resizebox width slightly to accommodate the new column
% \begin{tabular}{c|c|c|cc|cc|cc|c}
% \toprule[1pt]
% \toprule
% \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Reference}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Robust\\Design\end{tabular}}} & \multicolumn{2}{c|}{\textbf{UE}} & \multicolumn{2}{c|}{\textbf{UT}} & \multicolumn{2}{c|}{\textbf{RE}} & \multirow{2}{*}{\textbf{Benchmark}} \\
% \cline{4-9}
% & & & \textbf{MCQ} & \textbf{Gen} & \textbf{MCQ} & \textbf{Gen} & \textbf{Model} & \textbf{Input} & \\
% \midrule
% \rowcolor{LightCyan!50}
% \multicolumn{10}{c}{\textbf{\textit{Divergence-driven}}} \\
% \midrule
% \rowcolor{gray!20}
% \textbf{GradDiff} & \citet{yao2023large} & \xmark & & P & & P & & & PKU-SafeRLHF \\
% \textbf{NPO} & \citet{zhang2024negative} & \xmark & & T & & T & & & TOFU \\
% \rowcolor{gray!20}
% \textbf{SimNPO} & \citet{fan2024simplicity} & \xmark & W & T/M & W & T/M & T & & WMDP/TOFU/MUSE \\
% \textbf{NPO+SAM} & \citet{fan2025towards} & \cmark & W & M & W & M & W/M & W & WMDP/MUSE \\
% \rowcolor{gray!20}
% \textbf{NPO+IRM} & \citet{wang2025invariance} & \cmark & W & M & W & M & W/M & & WMDP/MUSE \\
% \midrule
% \rowcolor{LightCyan!50}
% \multicolumn{10}{c}{\textbf{\textit{Representation-mismatch}}} \\
% \midrule
% \textbf{RMU} & \citet{li2024wmdp} & \xmark & W & & W & & & W & WMDP \\
% \rowcolor{gray!20}
% \textbf{RR} & \citet{zou2024improving} & \xmark & C & & C & C & & C & Circuit Breaker \\
% \textbf{ELM} & \citet{gandikota2024erasing} & \xmark & W/H & & W/H & W/H & & W & WMDP/WHP \\
% \rowcolor{gray!20}
% \textbf{RMU+LAT} & \citet{sheshadri2024latent} & \cmark & W/H & & W/H & W & W & H & WMDP/WHP \\
% \textbf{TAR} & \citet{tamirisa2024tamper} & \cmark & W & & W & & W & & WMDP \\
% \midrule
% \rowcolor{LightCyan!50}
% \multicolumn{10}{c}{\textbf{\textit{Rejection-based}}} \\
% \midrule
% \rowcolor{gray!20}
% \textbf{IDK+AP} & \citet{yuan2024closer} & \xmark & & T & T & T & & & TOFU \\
% \textbf{JensUn} & \citet{singh2025unlearning} & \xmark & L/R & L/R & L/R & R & L & & LKF/RWKU \\
% \bottomrule
% \bottomrule[1pt]
% \end{tabular}
% }
% \end{table}