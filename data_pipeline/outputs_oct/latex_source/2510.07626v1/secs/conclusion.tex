\section{Conclusion}

In this work, we presented a full-stack investigation of LLM unlearning, encompassing methodology, evaluation, and robustness. We established a principled taxonomy that organizes twelve representative unlearning methods into three families: {\MDiv}, {\MRep}, and {\MRej}, providing a systematic lens to understand their underlying mechanisms. Our analysis revealed that conventional multiple-choice questioning (MCQ) evaluations of unlearning effectiveness (UE) and utility retention (UT) offer an incomplete picture, and we introduced open question answering (Open-QA) as a complementary paradigm to better capture generative behaviors and expose the strengths and limitations of different methods. Furthermore, we provide a comprehensive robustness assessment across model-level and input-level attacks, revealing nuanced relationships among in-domain relearning, out-of-domain fine-tuning, quantization, and jailbreak attacks. These findings clarify the trade-offs of current unlearning algorithms and guide the design of future methods that are both effective and robust. The use of LLM, limitation and broader impact are further discussed in \textbf{Appendix\,\ref{appx:llm_usage}}, \textbf{Appendix\,\ref{appx:limit}} and \textbf{Appendix\,\ref{appx:impact}}.
