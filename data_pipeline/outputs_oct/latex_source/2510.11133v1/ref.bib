% add in post rebuttal
@inproceedings{
wang2022sound,
title={Sound and Complete Causal Identification with Latent Variables Given Local Background Knowledge},
author={Tian-Zuo Wang and Tian Qin and Zhi-Hua Zhou},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=S8-duMv77W3}
}

@article{scholkopf2021toward,
  title={Toward causal representation learning},
  author={Sch{\"o}lkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
  journal={Proceedings of the IEEE},
  volume={109},
  number={5},
  pages={612--634},
  year={2021},
  publisher={IEEE}
}

@inproceedings{
geirhos2018imagenettrained,
title={ImageNet-trained {CNN}s are biased towards texture; increasing shape bias improves accuracy and robustness.},
author={Robert Geirhos and Patricia Rubisch and Claudio Michaelis and Matthias Bethge and Felix A. Wichmann and Wieland Brendel},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bygh9j09KX},
}

% related work -TTA 
@inproceedings{tent,
title={Tent: Fully Test-Time Adaptation by Entropy Minimization},
author={Dequan Wang and Evan Shelhamer and Shaoteng Liu and Bruno Olshausen and Trevor Darrell},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=uXl3bZLkr3c}
}

@inproceedings{t3a,
 author = {Iwasawa, Yusuke and Matsuo, Yutaka},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {2427--2440},
 title = {Test-Time Classifier Adjustment Module for Model-Agnostic Domain Generalization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/1415fe9fea0fa1e45dddcff5682239a0-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{lame,
  title={Parameter-free online test-time adaptation},
  author={Boudiaf, Malik and Mueller, Romain and Ben Ayed, Ismail and Bertinetto, Luca},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8344--8353},
  year={2022}
}

@inproceedings{memo,
  title={Memo: Test time robustness via adaptation and augmentation},
  author={Zhang, Marvin and Levine, Sergey and Finn, Chelsea},
  booktitle={Advances in Neural Information Processing Systems},
  volume={35},
  pages={38629--38642},
  year={2022}
}

@inproceedings{
sar,
title={Towards Stable Test-time Adaptation in Dynamic Wild World},
author={Shuaicheng Niu and Jiaxiang Wu and Yifan Zhang and Zhiquan Wen and Yaofo Chen and Peilin Zhao and Mingkui Tan},
booktitle={International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=g2YraF75Tj}
}

@inproceedings{tast,
title={Test-Time Adaptation via Self-Training with Nearest Neighbor Information},
author={Minguk Jang and Sae-Young Chung and Hye Won Chung},
booktitle={International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=EzLtB4M1SbM}
}

@inproceedings{deyo,
title={Entropy is not Enough for Test-Time Adaptation: From the Perspective of Disentangled Factors},
author={Jonghyun Lee and Dahuin Jung and Saehyung Lee and Junsung Park and Juhyeon Shin and Uiwon Hwang and Sungroh Yoon},
booktitle={International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=9w3iw8wDuE}
}

@inproceedings{pl,
  title={Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks},
  author={Lee, Dong-Hyun and others},
  booktitle={Workshop on challenges in representation learning, ICML},
  volume={3},
  number={2},
  pages={896},
  year={2013},
  organization={Atlanta}
}

@InProceedings{shot,
  title = 	 {Do We Really Need to Access the Source Data? {S}ource Hypothesis Transfer for Unsupervised Domain Adaptation},
  author =       {Liang, Jian and Hu, Dapeng and Feng, Jiashi},
  booktitle = 	 {International Conference on Machine Learning},
  pages = 	 {6028--6039},
  year = 	 {2020},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/liang20a/liang20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/liang20a.html},
  abstract = 	 {Unsupervised domain adaptation (UDA) aims to leverage the knowledge learned from a labeled source dataset to solve similar tasks in a new unlabeled domain. Prior UDA methods typically require to access the source data when learning to adapt the model, making them risky and inefficient for decentralized private data. This work tackles a practical setting where only a trained source model is available and investigates how we can effectively utilize such a model without source data to solve UDA problems. We propose a simple yet generic representation learning framework, named \emph{Source HypOthesis Transfer} (SHOT). SHOT freezes the classifier module (hypothesis) of the source model and learns the target-specific feature extraction module by exploiting both information maximization and self-supervised pseudo-labeling to implicitly align representations from the target domains to the source hypothesis. To verify its versatility, we evaluate SHOT in a variety of adaptation cases including closed-set, partial-set, and open-set domain adaptation. Experiments indicate that SHOT yields state-of-the-art results among multiple domain adaptation benchmarks.}
}


@inproceedings{
pasle,
title={Selective Label Enhancement Learning for Test-Time Adaptation},
author={Yihao Hu and Congyu Qiao and Xin Geng and Ning Xu},
booktitle={International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=3Z2flzXzBY}
}

@InProceedings{TIPI,
    author    = {Nguyen, A. Tuan and Nguyen-Tang, Thanh and Lim, Ser-Nam and Torr, Philip H.S.},
    title     = {TIPI: Test Time Adaptation With Transformation Invariance},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    month     = {June},
    year      = {2023},
    pages     = {24162-24171}
}

@inproceedings{tsd,
  title={Feature alignment and uniformity for test time adaptation},
  author={Wang, Shuai and Zhang, Daoan and Yan, Zipei and Zhang, Jianguo and Li, Rui},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={20050--20060},
  year={2023}
}

@inproceedings{program,
title={{PROGRAM}: {PRO}totype {GRA}ph Model based Pseudo-Label Learning for Test-Time Adaptation},
author={Haopeng Sun and Lumin Xu and Sheng Jin and Ping Luo and Chen Qian and Wentao Liu},
booktitle={International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=x5LvBK43wg}
}

% hypothesis 
@inproceedings{mahajan2021domain,
  title={Domain generalization using causal matching},
  author={Mahajan, Divyat and Tople, Shruti and Sharma, Amit},
  booktitle={International Conference on Machine Learning},
  pages={7313--7324},
  year={2021},
  organization={PMLR}
}

@inproceedings{
kaur2023modeling,
title={Modeling the Data-Generating Process is Necessary for Out-of-Distribution Generalization},
author={Jivat Neet Kaur and Emre Kiciman and Amit Sharma},
booktitle={International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=uyqks-LILZX}
}

@inproceedings{
wiles2022a,
title={A Fine-Grained Analysis on Distribution Shift},
author={Olivia Wiles and Sven Gowal and Florian Stimberg and Sylvestre-Alvise Rebuffi and Ira Ktena and Krishnamurthy Dj Dvijotham and Ali Taylan Cemgil},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=Dl4LetuLdyK}
}

@inproceedings{ye2022ood,
  title={Ood-bench: Quantifying and understanding two dimensions of out-of-distribution generalization},
  author={Ye, Nanyang and Li, Kaican and Bai, Haoyue and Yu, Runpeng and Hong, Lanqing and Zhou, Fengwei and Li, Zhenguo and Zhu, Jun},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7947--7958},
  year={2022}
}

@inproceedings{domainbed,
title={In Search of Lost Domain Generalization},
author={Ishaan Gulrajani and David Lopez-Paz},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=lQdXeXDoWtI}
}

@inproceedings{cotta,
  title={Continual test-time domain adaptation},
  author={Wang, Qin and Fink, Olga and Van Gool, Luc and Dai, Dengxin},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7201--7211},
  year={2022}
}

@inproceedings{foa,
title={Test-Time Model Adaptation with Only Forward Passes},
author={Shuaicheng Niu and Chunyan Miao and Guohao Chen and Pengcheng Wu and Peilin Zhao},
booktitle={International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=qz1Vx1v9iK}
}

@inproceedings{atta,
title={Active Test-Time Adaptation: Theoretical Analyses and An Algorithm},
author={Shurui Gui and Xiner Li and Shuiwang Ji},
booktitle={International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=YHUGlwTzFB}
}

@inproceedings{cafa,
  title={Cafa: Class-aware feature alignment for test-time adaptation},
  author={Jung, Sanghun and Lee, Jungsoo and Kim, Nanhee and Shaban, Amirreza and Boots, Byron and Choo, Jaegul},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={19060--19071},
  year={2023}
}


@InProceedings{crkd,
  title = 	 {Leveraging Proxy of Training Data for Test-Time Adaptation},
  author =       {Kang, Juwon and Kim, Nayeong and Kwon, Donghyeon and Ok, Jungseul and Kwak, Suha},
  booktitle = 	 {International Conference on Machine Learning},
  pages = 	 {15737--15752},
  year = 	 {2023},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/kang23a/kang23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/kang23a.html},
  abstract = 	 {We consider test-time adaptation (TTA), the task of adapting a trained model to an arbitrary test domain using unlabeled input data on-the-fly during testing. A common practice of TTA is to disregard data used in training due to large memory demand and privacy leakage. However, the training data are the only source of supervision. This motivates us to investigate a proper way of using them while minimizing the side effects. To this end, we propose two lightweight yet informative proxies of the training data and a TTA method fully exploiting them. One of the proxies is composed of a small number of images synthesized (hence, less privacy-sensitive) by data condensation which minimizes their domain-specificity to capture a general underlying structure over a wide spectrum of domains. Then, in TTA, they are translated into labeled test data by stylizing them to match styles of unlabeled test samples. This enables virtually supervised test-time training. The other proxy is inter-class relations of training data, which are transferred to target model during TTA. On four public benchmarks, our method outperforms the state-of-the-art ones at remarkably less computation and memory.}
}


@inproceedings{bn_adapt,
  title={Improving robustness against common corruptions by covariate shift adaptation},
  author={Schneider, Steffen and Rusak, Evgenia and Eck, Luisa and Bringmann, Oliver and Brendel, Wieland and Bethge, Matthias},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={11539--11551},
  year={2020}
}

@inproceedings{AdaRealign,
 author = {Zhang, Zhen-Yu and Xie, Zhiyu and Yao, Huaxiu and Sugiyama, Masashi},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {94607--94632},
 title = {Test-time Adaptation in Non-stationary Environments via Adaptive Representation Alignment},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/abe31a12e83111fdf2cfd54deed5a2ce-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}


@inproceedings{adanpc,
  title={Adanpc: Exploring non-parametric classifier for test-time adaptation},
  author={Zhang, Yifan and Wang, Xue and Jin, Kexin and Yuan, Kun and Zhang, Zhang and Wang, Liang and Jin, Rong and Tan, Tieniu},
  booktitle={International Conference on Machine Learning},
  pages={41647--41676},
  year={2023},
  organization={PMLR}
}

@inproceedings{sotta,
title={So{TTA}: Robust Test-Time Adaptation on Noisy Data Streams},
author={Taesik Gong and Yewon Kim and Taeckyung Lee and Sorn Chottananurak and Sung-Ju Lee},
booktitle={Advances in Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=3bdXag2rUd}
}

@inproceedings{conjugatepl,
title={Test Time Adaptation via Conjugate Pseudo-labels},
author={Sachin Goyal and Mingjie Sun and Aditi Raghunathan and J Zico Kolter},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=2yvUYc-YNUH}
}

@inproceedings{adacontrast,
  title={Contrastive test-time adaptation},
  author={Chen, Dian and Wang, Dequan and Darrell, Trevor and Ebrahimi, Sayna},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={295--305},
  year={2022}
}

@InProceedings{eata,
  title = 	 {Efficient Test-Time Model Adaptation without Forgetting},
  author =       {Niu, Shuaicheng and Wu, Jiaxiang and Zhang, Yifan and Chen, Yaofo and Zheng, Shijian and Zhao, Peilin and Tan, Mingkui},
  booktitle = 	 {International Conference on Machine Learning},
  pages = 	 {16888--16905},
  year = 	 {2022},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/niu22a/niu22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/niu22a.html},
  abstract = 	 {Test-time adaptation provides an effective means of tackling the potential distribution shift between model training and inference, by dynamically updating the model at test time. This area has seen fast progress recently, at the effectiveness of handling test shifts. Nonetheless, prior methods still suffer two key limitations: 1) these methods rely on performing backward computation for each test sample, which takes a considerable amount of time; and 2) these methods focus on improving the performance on out-of-distribution test samples and ignore that the adaptation on test data may result in a catastrophic forgetting issue, \ie, the performance on in-distribution test samples may degrade. To address these issues, we propose an efficient anti-forgetting test-time adaptation (EATA) method. Specifically, we devise a sample-efficient entropy minimization loss to exclude uninformative samples out of backward computation, which improves the overall efficiency and meanwhile boosts the out-of-distribution accuracy. Afterward, we introduce a regularization loss to ensure that critical model weights tend to be preserved during adaptation, thereby alleviating the forgetting issue. Extensive experiments on CIFAR-10-C, ImageNet-C, and ImageNet-R verify the effectiveness and superiority of our EATA.}
}


@InProceedings{ttab,
  title = 	 {On Pitfalls of Test-Time Adaptation},
  author =       {Zhao, Hao and Liu, Yuejiang and Alahi, Alexandre and Lin, Tao},
  booktitle = 	 {International Conference on Machine Learning},
  pages = 	 {42058--42080},
  year = 	 {2023},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/zhao23d/zhao23d.pdf},
  url = 	 {https://proceedings.mlr.press/v202/zhao23d.html},
  abstract = 	 {Test-Time Adaptation (TTA) has recently gained significant attention as a new paradigm for tackling distribution shifts. Despite the sheer number of existing methods, the inconsistent experimental conditions and lack of standardization in prior literature make it difficult to measure their actual efficacies and progress. To address this issue, we present a large-scale open-sourced Test-Time Adaptation Benchmark, dubbed TTAB, which includes nine state-of-the-art algorithms, a diverse array of distribution shifts, and two comprehensive evaluation protocols. Through extensive experiments, we identify three common pitfalls in prior efforts: (i) choosing appropriate hyper-parameter, especially for model selection, is exceedingly difficult due to online batch dependency; (ii) the effectiveness of TTA varies greatly depending on the quality of the model being adapted; (iii) even under optimal algorithmic conditions, existing methods still systematically struggle with certain types of distribution shifts. Our findings suggest that future research in the field should be more transparent about their experimental conditions, ensure rigorous evaluations on a broader set of models and shifts, and re-examine the assumptions underlying the potential success of TTA for practical applications.}
}


@inproceedings{dfr,
title={Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations},
author={Polina Kirichenko and Pavel Izmailov and Andrew Gordon Wilson},
booktitle={International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=Zb6c8A-Fghk}
}

@inproceedings{pre_dfr,
title={On Feature Learning in the Presence of Spurious Correlations},
author={Pavel Izmailov and Polina Kirichenko and Nate Gruver and Andrew Gordon Wilson},
booktitle={Advances in Neural Information Processing Systems},
year={2022},
url={https://openreview.net/forum?id=wKhUPzqVap6}
}

@inproceedings{linear_interpolation,
  title={Deep feature interpolation for image content changes},
  author={Upchurch, Paul and Gardner, Jacob and Pleiss, Geoff and Pless, Robert and Snavely, Noah and Bala, Kavita and Weinberger, Kilian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7064--7073},
  year={2017}
}

@inproceedings{clip,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International Conference on Machine Learning},
  pages={8748--8763},
  year={2021},
  organization={PmLR}
}

@article{wordembedding,
  title={A latent variable model approach to pmi-based word embeddings},
  author={Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
  journal={Transactions of the Association for Computational Linguistics},
  volume={4},
  pages={385--399},
  year={2016},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}

@inproceedings{linear_representation_gan,
  title={Interpreting the latent space of gans for semantic face editing},
  author={Shen, Yujun and Gu, Jinjin and Tang, Xiaoou and Zhou, Bolei},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9243--9252},
  year={2020}
}

@InProceedings{linear_representation,
  title = 	 {The Linear Representation Hypothesis and the Geometry of Large Language Models},
  author =       {Park, Kiho and Choe, Yo Joong and Veitch, Victor},
  booktitle = 	 {International Conference on Machine Learning},
  pages = 	 {39643--39666},
  year = 	 {2024},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/park24c/park24c.pdf},
  url = 	 {https://proceedings.mlr.press/v235/park24c.html},
  abstract = 	 {Informally, the "linear representation hypothesis" is the idea that high-level concepts are represented linearly as directions in some representation space. In this paper, we address two closely related questions: What does "linear representation" actually mean? And, how do we make sense of geometric notions (e.g., cosine similarity and projection) in the representation space? To answer these, we use the language of counterfactuals to give two formalizations of linear representation, one in the output (word) representation space, and one in the input (context) space. We then prove that these connect to linear probing and model steering, respectively. To make sense of geometric notions, we use the formalization to identify a particular (non-Euclidean) inner product that respects language structure in a sense we make precise. Using this <em>causal inner product</em>, we show how to unify all notions of linear representation. In particular, this allows the construction of probes and steering vectors using counterfactual pairs. Experiments with LLaMA-2 demonstrate the existence of linear representations of concepts, the connection to interpretation and control, and the fundamental role of the choice of inner product.}
}

@inproceedings{dgp,
 author = {Sun, Xinwei and Wu, Botong and Zheng, Xiangyu and Liu, Chang and Chen, Wei and Qin, Tao and Liu, Tie-Yan},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {16846--16859},
 title = {Recovering Latent Causal Factor for Generalization to Distributional Shifts},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/8c6744c9d42ec2cb9e8885b54ff744d0-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{dgp2,
title={Harnessing Out-Of-Distribution Examples via Augmenting Content and Style},
author={Zhuo Huang and Xiaobo Xia and Li Shen and Bo Han and Mingming Gong and Chen Gong and Tongliang Liu},
booktitle={International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=boNyg20-JDm}
}

@inproceedings{
goel2021model,
title={Model Patching: Closing the Subgroup Performance Gap with Data Augmentation},
author={Karan Goel and Albert Gu and Yixuan Li and Christopher Re},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=9YlaeLfuhJF}
}

@inproceedings{lv2022causality,
  title={Causality inspired representation learning for domain generalization},
  author={Lv, Fangrui and Liang, Jian and Li, Shuang and Zang, Bin and Liu, Chi Harold and Wang, Ziteng and Liu, Di},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={8046--8056},
  year={2022}
}

@inproceedings{gao2023out,
  title={Out-of-domain robustness via targeted augmentations},
  author={Gao, Irena and Sagawa, Shiori and Koh, Pang Wei and Hashimoto, Tatsunori and Liang, Percy},
  booktitle={International Conference on Machine Learning},
  pages={10800--10834},
  year={2023},
  organization={PMLR}
}

% datasets 
@inproceedings{WILDS,
  title={Wilds: A benchmark of in-the-wild distribution shifts},
  author={Koh, Pang Wei and Sagawa, Shiori and Marklund, Henrik and Xie, Sang Michael and Zhang, Marvin and Balsubramani, Akshay and Hu, Weihua and Yasunaga, Michihiro and Phillips, Richard Lanas and Gao, Irena and others},
  booktitle={International Conference on Machine Learning},
  pages={5637--5664},
  year={2021},
  organization={PMLR}
}
@article{bandi2018detection,
  title={From detection of individual metastases to classification of lymph node status at the patient level: the CAMELYON17 challenge},
  author={Bandi, Peter and Geessink, Oscar and Manson, Quirine and Van Dijk, Marcory and Balkenhol, Maschenka and Hermsen, Meyke and Bejnordi, Babak Ehteshami and Lee, Byungjae and Paeng, Kyunghyun and Zhong, Aoxiao and others},
  journal={IEEE Transactions on Medical Imaging},
  year={2018},
  publisher={IEEE}
}

@book{jolliffe2002principal,
  title={Principal Component Analysis},
  author={Jolliffe, I.T.},
  isbn={9780387954424},
  lccn={2002019560},
  series={Springer Series in Statistics},
  url={https://books.google.com.sg/books?id=_olByCrhjwIC},
  year={2002},
  publisher={Springer}
}

@inproceedings{mikolov2013linguistic,
  title={Linguistic regularities in continuous space word representations},
  author={Mikolov, Tom{\'a}{\v{s}} and Yih, Wen-tau and Zweig, Geoffrey},
  booktitle={Proceedings of the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies},
  pages={746--751},
  year={2013}
}

@book{horn2012matrix,
  title={Matrix analysis},
  author={Horn, Roger A and Johnson, Charles R},
  year={2012},
  publisher={Cambridge university press}
}

#### new bib
@inproceedings{borkan2019nuanced,
  title={Nuanced metrics for measuring unintended bias with real data for text classification},
  author={Borkan, Daniel and Dixon, Lucas and Sorensen, Jeffrey and Thain, Nithum and Vasserman, Lucy},
  booktitle={Companion Proceedings of The 2019 World Wide Web Conference},  
  year={2019}
}


@misc{birdcalls_1, 
    title={A collection of fully-annotated soundscape recordings from the southwestern Amazon Basin}, 
    Howpublished={URL https://zenodo.org/records/7079124}, 
    journal={Zenodo}, 
    publisher={Zenodo}, 
    author={Hopping, W. Alexander and Kahl, Stefan and Klinck, Holger}, year={2022}
}

@misc{birdcalls_2, 
    title={A collection of fully-annotated soundscape recordings from the Northeastern United States}, 
    Howpublished={URL https://zenodo.org/records/7018484}, 
    journal={Zenodo}, 
    publisher={Zenodo}, 
    author={Kahl, Stefan and Charif, Russell and Klinck, Holger}, 
    year={2022}
}

@misc{birdcalls_3, 
    title={A collection of fully-annotated soundscape recordings from the Island of Hawai'i}, 
    Howpublished={URL https://doi.org/10.5281/zenodo.7078499}, 
    journal={Zenodo}, 
    publisher={Zenodo}, 
    author={Navine, Amanda and Kahl, Stefan and Tanimoto-Johnson, Ann and Klinck, Holger and Hart, Patrick}, 
    year={2022}
}

@inproceedings{imagenet_r,
  title={The many faces of robustness: A critical analysis of out-of-distribution generalization},
  author={Hendrycks, Dan and Basart, Steven and Mu, Norman and Kadavath, Saurav and Wang, Frank and Dorundo, Evan and Desai, Rahul and Zhu, Tyler and Parajuli, Samyak and Guo, Mike and others},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={8340--8349},
  year={2021}
}

@article{ILSVRC15,
Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
Title = {{ImageNet Large Scale Visual Recognition Challenge}},
Year = {2015},
journal   = {International Journal of Computer Vision (IJCV)},
doi = {10.1007/s11263-015-0816-y},
volume={115},
number={3},
pages={211-252}
}

@inproceedings{imagenet_v2,
  title={Do imagenet classifiers generalize to imagenet?},
  author={Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
  booktitle={International conference on machine learning},
  pages={5389--5400},
  year={2019},
  organization={PMLR}
}

@article{tellez2018whole,
  title={Whole-slide mitosis detection in H\&E breast histology using PHH3 as a reference to train distilled stain-invariant convolutional networks},
  author={Tellez, David and Balkenhol, Maschenka and Otte-H{\"o}ller, Irene and Van De Loo, Rob and Vogels, Rob and Bult, Peter and Wauters, Carla and Vreuls, Willem and Mol, Suzanne and Karssemeijer, Nico and others},
  journal={IEEE transactions on medical imaging},
  volume={37},
  number={9},
  pages={2126--2136},
  year={2018},
  publisher={IEEE}
}

@inproceedings{cubuk2020randaugment,
  title={Randaugment: Practical automated data augmentation with a reduced search space},
  author={Cubuk, Ekin D and Zoph, Barret and Shlens, Jonathon and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops},
  pages={702--703},
  year={2020}
}

@inproceedings{cubuk2019autoaugment,
  title={Autoaugment: Learning augmentation strategies from data},
  author={Cubuk, Ekin D and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={113--123},
  year={2019}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@inproceedings{selvaraju2017grad,
  title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={618--626},
  year={2017}
}