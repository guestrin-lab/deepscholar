\documentclass{article}

\usepackage[final]{neurips_2025}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[hidelinks]{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage[pdftex]{graphicx}
\usepackage{wrapfig}
\usepackage[labelformat=simple]{subcaption}
\usepackage{amsmath, amsthm}
\usepackage{tcolorbox}
\usepackage{enumitem}
\usepackage{array}
\usepackage{selectp}
\usepackage{mathtools} 
\usepackage{cases}
\usepackage{tocloft}
\usepackage{tablefootnote}
\tcbuselibrary{breakable, skins}

\newtheorem{proposition}{Proposition}
\renewcommand{\thesubfigure}{(\alph{subfigure})}



\newcommand{\squishlisttwo}{
 \begin{list}{\tiny$\bullet$}
  { \setlength{\itemsep}{1pt}
     \setlength{\parsep}{0pt}
    \setlength{\topsep}{0pt}
    \setlength{\partopsep}{0pt}
    \setlength{\leftmargin}{2.5em}
    \setlength{\labelwidth}{2.5em}
    \setlength{\labelsep}{1em} 
    } }

\newcommand{\squishend}{
  \end{list}  }

\title{Test-Time Adaptation by Causal Trimming}

\author{
Yingnan Liu\textsuperscript{1,2} \quad\: Rui Qiao\textsuperscript{1,3} \quad\: Mong Li Lee\textsuperscript{1,2} \quad\: Wynne Hsu\textsuperscript{1,2} \\
\textsuperscript{1}School of Computing, National University of Singapore \\
\textsuperscript{2}Institute of Data Science, National University of Singapore \\
\textsuperscript{3}Singapore-MIT Alliance for Research and Technology \\
\texttt{\{liu.yingnan, rui.qiao\}@u.nus.edu}, \: \texttt{\{dcsleeml, dcshsuw\}@nus.edu.sg}
}


\begin{document}
\maketitle
\begin{abstract}
Test-time adaptation aims to improve model robustness under distribution shifts by adapting models with access to unlabeled target samples. A primary cause of performance degradation under such shifts is the model’s reliance on features that lack a direct causal relationship with the prediction target.
We introduce Test-time Adaptation by Causal Trimming (TACT), a method that identifies and removes non-causal components from representations for test distributions. TACT applies data augmentations that preserve causal features while varying non-causal ones.
By analyzing the changes in the representations using Principal Component Analysis, TACT identifies the highest variance directions associated with non-causal features. It trims the representations by removing their projections on the identified directions, and uses the trimmed representations for the predictions. During adaptation, TACT continuously tracks and refines these directions to get a better estimate of non-causal features. We theoretically analyze the effectiveness of this approach and empirically validate TACT on real-world out-of-distribution benchmarks. TACT consistently outperforms state-of-the-art methods by a significant margin. Our code is available at \url{https://github.com/NancyQuris/TACT}.

\end{abstract}


\section{Introduction} \label{sec:intro}
% Introduce TTA



Machine learning models often exhibit significant performance degradation when evaluated on data drawn from a distribution that differs from their training data distribution \cite{domainbed}. To address this challenge, test-time adaptation (TTA) has emerged as a promising approach. TTA methods adapt a pretrained model to the test distribution dynamically, using the incoming test data to enhance predictive performance without requiring access to the original training data \cite{t3a, tent, memo}.
Despite recent advances, many existing TTA methods rely heavily on predicted labels generated by the model itself to guide the adaptation process \cite{sotta, eata, sar, tent}. However, the effectiveness of these methods hinges critically on the quality of the predictions. When the model's predictions are influenced by non-causal features that do not have a direct causal relationship with the prediction target \cite{kaur2023modeling, wiles2022a}, the predicted label may be unreliable, leading to sub-optimal adaptation outcomes \cite{deyo, program}.

 Unlike causal features that have stable associations with the semantic structure of the prediction task \cite{dfr}, 
non-causal features exhibit inconsistent  or spurious correlations with the prediction target across training and test distributions \cite{ye2022ood}. Over-reliance on non-causal features is a key factor in model performance degradation under distribution shift.
While DeYO \cite{deyo} recognizes this issue, it does not explicitly mitigate reliance on non-causal features. Instead, it updates  the model using predictions that leverage causal features only, relying on gradual adaptation to reinforce causal features over time. Consequently, early predictions may still be influenced by non-causal signals, requiring many adaptation steps to suppress their effects.

\begin{wrapfigure}{R}{0.3\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/figure_1.png}
  \caption{Batch accuracy on Camelyon17 dataset of the first 100 adaptation steps.}
  \label{fig:acc_com}
\end{wrapfigure}

Given the above limitations, we propose to actively reduce non-causal features.
Prior studies have shown that feature representations learned through standard training encode a mixture of causal and non-causal features and that  the causal part is often learned sufficiently well for accurate prediction \cite{pre_dfr, dfr}.
Motivated by this, we propose a Test-time Adaptation by Causal Trimming (TACT) framework that seeks to improve adaptation performance by isolating and removing non-causal components from the representations of samples from test distributions. Our framework aims to achieve more reliable predictions in the presence of distribution shift by reducing the model’s dependence on unstable, non-causal features. 
% proposed method 
To identify non-causal features in representations, we analyze how these representations change when we apply targeted perturbations to the input data. Specifically, we perform input augmentations that preserve the underlying causal contents while introducing variability in other, non-causal aspects of the input \cite{gao2023out, goel2021model, dgp2, lv2022causality, mahajan2021domain}. 
These augmentations produce multiple test-time samples that share the same causal semantics but differ in spurious or incidental attributes. By examining how the feature representations of these samples vary, we can disentangle causal and non-causal components.

We operationalize this by applying Principal Component Analysis (PCA) to the set of augmented representations and identify the direction of greatest variance. We interpret this dominant direction as being aligned with the non-causal features, under the assumption that causal content remains stable across augmentations, while non-causal attributes vary. This approach is inspired by prior work showing that high-level semantic factors are often linearly encoded in the learned representation space \cite{wordembedding, linear_representation, linear_representation_gan}.
Building on the insight that linear manipulations in representation space can produce meaningful changes in semantic content \cite{clip, linear_interpolation}, we propose to reduce the influence of non-causal features by subtracting the projection of a test sample’s representation along the identified non-causal direction. 
Since the prototypes used for prediction, defined as template representations corresponding to the weights for each class in the linear classifier, are influenced by non-causal features, we apply the same operation to them using the identified non-causal direction. During adaptation, we maintain a moving average of the updated prototypes to mitigate noise effects.
Compared to DeYO, TACT can immediately produce predictions that are less affected by non-causal features, eliminating the need for iterative updates to achieve reliable results (see Figure~\ref{fig:acc_com}). 
We provide a theoretical analysis to establish the conditions under which TACT can improve prediction accuracy under distribution shift.
Empirically, we evaluate TACT on five real-world out-of-distribution datasets, demonstrating its effectiveness and superiority over state-of-the-art TTA methods. 


\section{Related Work} \label{sec:related-work}

Existing TTA methods can be broadly categorized into backpropagation-free and backpropagation-based methods. 
Backpropagation-free methods modify model outputs or intermediate representations without gradient-based optimization. These include 
modifiable prompts \cite{foa}, re-normalized representations \cite{bn_adapt}, updated prototypes \cite{t3a, adanpc}, and maximum likelihood estimation \cite{lame}. 
Backpropagation-based methods
update the model with the gradient of objective functions such as entropy minimization \cite{sotta, eata, sar, tent} and self-training with pseudo-labels \cite{pasle, crkd, program, cotta}.
Entropy Minimization encourages more confident predictions by reducing the entropy of model predictions during adaptation.
Self-training 
employs cross entropy \cite{adacontrast, pasle, shot, program} and knowledge distillation \cite{crkd, cotta, tsd} using  model predictions as pseudo-labels.
Regularization measures such as information maximization \cite{shot}, representation statistics alignment \cite{cafa, AdaRealign}, and consistency regularization \cite{tipi, memo} for invariant prediction under augmentations have been proposed to regularize the adaptation.

A key challenge in test-time adaptation is obtaining reliable pseudo-labels to guide model updates. 
One line of work assumes that correct predictions tend to exhibit low entropy, and update the model 
using only high-confidence samples with low-entropy predictions \cite{t3a, eata, sar, adanpc}.
However, DeYO \cite{deyo} shows that spurious correlations can also result in low entropy predictions and proposes a causal intervention technique to identify predictions that are more likely based on causal features, using them selectively for model updates.
Another line of work  refines pseudo-labels by incorporating updated prototype and neighborhood information \cite{adacontrast, pasle, tast, program, tsd}. 
 AdaContrast \cite{adacontrast} uses soft voting among nearest neighbors. 
 TSD \cite{tsd} relies on updated prototypes and spatial local clustering. 
 TAST \cite{tast} employs neighbourhood information in self-training.
 PROGRAM \cite{program}
 considers both prototype and neighbour-based pseudo-labels to enhance label quality.  
 PASLE \cite{pasle} progressively refines the pseudo-labels of uncertain predictions using updated prototypes. 
 
All the above methods, except for DeYO, do not consider the effect of non-causal features on model prediction.  Although DeYO finds that non-causal features would make entropy an unreliable metric to reflect prediction correctness, it does not adjust model predictions. TACT adjusts model predictions by reducing non-causal features, and our adjusted prediction can be used as a more reliable pseudo-label.
\section{Preliminaries} \label{sec:preliminary}
\begin{wrapfigure}{R}{0.25\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{figures/dgp.png}
  % \caption{Causal model}
  \caption{Structural causal model of the data-generating process.}
  \label{fig:dgp}
\end{wrapfigure}
We consider the problem of adapting a well-trained model to test-time distributions that differ from the training distribution. Our goal is to improve the model’s performance on these shifted distributions with unlabeled test samples.
Following prior work \cite{dgp2, dgp}, we model the distribution shift using a structural causal model that captures the underlying data-generating process, as illustrated in Figure~\ref{fig:dgp}.

% introduce the DGP
We model the observed sample $X$ and its label 
$Y$ as being generated from causal factors $X_C$ and non-causal factors $X_{NC}$. 
Only $X_C$ is causally related to $Y$, while $X$ is related to both $X_C$ and $X_{NC}$. 
The correlation between $X_C$ and $Y$ is stable, i.e., the conditional distribution $P(Y|X_C)$ remains unchanged at test time.
We also assume that the distribution of causal factors $P(X_C)$ remains invariant across the training and test datasets, whereas distribution shifts arise from changes in 
$P(X_{NC})$.
The model would have stable performance across distributions if the prediction is based solely on features related to the causal factors $X_C$. In contrast, reliance on non-causal factors $X_{NC}$ can lead to unreliable performance under distribution shifts.

We consider a $c$-class classification task, where the model $f \coloneq h \circ g$ used for adaptation is composed of a feature extractor $g$ and a classifier $h$. The feature extractor
$g$ maps an input sample to a $d$-dimensional vector $z\in\mathbb{R}^d$ as the representation. The classifier 
$h$ maintains a set of class prototypes $\{q_1, \dots,q_c\} \in\mathbb{R}^d$, where each prototype $q_i$ serves as a template representation for class $i$. Predictions are made by computing the similarity between the input representation $z$ and each class prototype using the dot product $z \cdot q_i$, referred to as the logit of class $i$. 
A softmax function is then applied to the logits to obtain the probability distribution over the classes.
\section{Proposed Method} \label{sec:method}
The works in \cite{pre_dfr, dfr} show that models are often capable of learning causal features, even when their predictions are predominantly driven by non-causal features with spurious correlations. However, the predictive influence of these causal features is frequently obscured or suppressed due to the heavily weighted non-causal components in the learned representations.
Based on this observation, we propose TACT, a method that identifies non-causal features and reduces their influence by causally trimming the learned representations.
We hypothesize that non-causal features are embedded in representations along a specific direction. 
Such direction is of the maximum variance when the non-causal features change. 
To suppress their influence, we subtract the projection of both the input representation and class prototypes onto this identified direction. This operation attenuates the non-causal information present in both elements.
Since class prototypes serve as canonical representations for each class, and the non-causal direction estimated from a single test sample may be noisy, 
we maintain a moving average of the updated prototypes throughout test-time adaptation.
At inference, predictions are made by measuring the similarity between the adapted representation and the moving average of the updated prototypes, thereby reducing influence of non-causal features.



\subsection{Non-Causal Feature Identification}
Given a sample $x$ at test time, if we have access to additional samples generated with the same causal factors but different non-causal factors, we can compare their representations to infer the influence of non-causal factors. 
Changes in the representations across these samples can be attributed to variations in these non-causal factors. By systematically analyzing these representational differences, we can isolate and identify the components of the representation that correspond to non-causal features.

To simulate variations in the data-generating process, we apply data augmentation to target non-causal features \cite{gao2023out, goel2021model, dgp2, lv2022causality, mahajan2021domain}. 
For a test sample $x$, we generate  $n$ augmented samples $\{\tilde{x}_i\}_{i=1}^n$ that preserve the causal feature while varying the non-causal factors. 
We collect the representations of these samples in a matrix $\mathbf{Z}=[z, \tilde{z}_1, ..., \tilde{z}_n]^\top$, where $z$ is the representation of the original sample and $\tilde{z}_i$ are those of the augmented samples. 

We interpret non-causal features as corresponding to specific, disentangled directions in the representation space, consistent with prior work that indicates high-level semantic concepts are linearly encoded as vector directions in learned representations~\cite{wordembedding, linear_representation, linear_representation_gan}. 
For instance, the vector difference between ``woman'' and ``man'' would resemble that between ``queen'' and ``king'' \cite{mikolov2013linguistic}, both aligning to the direction representing gender. Along this direction, specific instances of the gender concept, such as ``male'' and ``female'', take different magnitudes. 

Given representations of samples that differ only in their non-causal factors, the direction along which the representations change the most is expected to capture the non-causal features.  
This dominant direction can be identified via Principal Component Analysis (PCA) which analyzes the covariance matrix of the representations to extract the principal components.
Principal components are vectors along which the representations' projections exhibit maximum variances.
We first compute the mean of the representations as:
$\bar{z} = \frac{1}{n+1}z+\frac{n}{n+1}\sum_{i=1}^n \tilde{z}_i$. 
Using this mean, we construct a matrix $\bar{\mathbf{Z}} = [\bar{z}, \bar{z}, .., \bar{z}]^\top$ that has the same size as the representation matrix $\mathbf{Z}$.
The covariance matrix of the representations is then given by
 $\mathbf{\Sigma_Z} = (\mathbf{Z}-\bar{\mathbf{Z}})^\top(\mathbf{Z}-\bar{\mathbf{Z}})$.
The eigenvectors of $\mathbf{\Sigma_Z}$ correspond to the principal components,  and their eigenvalues quantify the variance along these components \cite{jolliffe2002principal}. 
Since $\mathbf{\Sigma_Z}$ is a real symmetric matrix, its eigenvectors form an orthonormal basis in $\mathbb{R}^d$ \cite{horn2012matrix}. 
Using spectral decomposition, we express the covariance matrix as:
$\mathbf{\Sigma_Z}=\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^\top$,
where $\mathbf{Q}=[e_1, e_2, ..., e_d]$ 
is an orthogonal matrix whose 
 columns are the orthonormal eigenvectors,  and 
 $\mathbf{\Lambda}$ is a diagonal matrix containing the 
eigenvalues of $\mathbf{\Sigma_Z}$. 
Here, $e_i$ denotes the direction along which the variance of the projected representations is the $i^{th}$ largest. 


\subsection{Causal Trimming to Reduce Non-Causal Feature}

Prior work has demonstrated that applying linear transformations to representations can manipulate the semantics they encode
\cite{clip, linear_interpolation}.
Since the principal components $\{e_i\}_{i=1}^d$ form an orthonormal basis in $\mathbb{R}^d$, any representation $z$ can be expressed as a linear combination of these components.
To reduce the influence of non-causal features, we propose to trim the representation by removing its components along the top-$m$ principal components:
% as shown in Equation \eqref{eq:trim}.
\begin{equation}
% \textstyle
    \hat{z} = z-\sum_{i=1}^m (z\cdot e_i)e_i\ 
    \label{eq:trim}
\end{equation}
Since each $e_i$ is a  vector of unit length, the term $(z\cdot e_i)e_i$ is the projection of $z$  onto $e_i$ whose magnitude is given by the dot product
$(z\cdot e_i)$.
By subtracting these terms, we obtain an updated representation
 $\hat{z}$ which is composed of components only formed by  
  $\{e_i\}_{i=m+1}^d$.
  If causal features are invariant under data augmentations and their corresponding semantic directions are orthogonal to those of the removed directions, causal information present in $z$ is preserved in the trimmed representation 
 $\hat{z}$.


\subsection{Model Adaptation}

In a prototype-based classifier, each class prototype $q_j$ 
serves as a template representation learned by the classifier $h$,  summarizing the representations of samples belonging to class $j$. However, if the learned representations encode non-causal features, the prototypes will be influenced by these  features. 
To mitigate this issue, we apply the same causal trimming to the class prototypes. Specifically, let $q_j$ be the prototype of class $j$.
Given the top-$m$ principal components $\{e_i\}_{i=1}^m$ that are used to trim the test sample representation $z$, we obtain the trimmed prototype $\hat{q}_j$ for each class $ j \in \{1, 2, \dots, c\}$ as:
\begin{equation}
% \textstyle
    \hat{q}_j = q_j-\sum_{i=1}^m (q_j\cdot e_i)e_i\ 
    \label{eq:trim_p}
\end{equation}
Since the identified non-causal directions may vary across samples due to noise or context-specific factors, we compute a batch-wise average of trimmed prototypes to obtain a more stable estimate.
To track the estimate across batches during adaptation,
we maintain a moving average of the trimmed prototypes during test-time adaptation. 
Suppose we obtain trimmed prototypes $\hat{q}^{(i)}_j$ for each class $j$ at batch $i$, the moving-average $\bar{\hat{q}}_j$ is updated by 
$\bar{\hat{q}}_j= \frac{i-1}{i} \bar{\hat{q}}_j + \frac{1}{i} \hat{q}^{(i)}_j$.
This moving average serves as a more robust estimate of the causally refined prototypes, effectively smoothing out sample-specific variance. 
The prediction made by the average of the trimmed prototypes is the same as that of an ensemble over the logits produced by individual trimmed prototypes, resulting in more stable predictions.
At test time, for a given input sample  $x$, we compute the causally trimmed representation $\hat{z}$ and compare it to the moving-averaged trimmed prototypes $\bar{\hat{q}}_j$. The 
 logit for class $j$ is given by the dot product  $\hat{z} \cdot \bar{\hat{q}}_j$, and the  final predicted label $y$ is given by: 
\begin{equation}
    y =  \underset{j}{\arg\max} \frac{\exp{(\hat{z} \cdot \bar{\hat{q}}_j})}{\sum\limits_{i=1}^c \exp{(\hat{z} \cdot \bar{\hat{q}}_i})}
    % y =  \underset{j}{\arg\max} \hat{z} \cdot \bar{\hat{q}}_j
    \label{eq:pred}
\end{equation}

\section{Theoretical Analysis} \label{sec:math-analysis}
\label{sc:theory}
We present the conditions under which TACT would correct a wrong prediction and maintain a correct prediction.
We consider binary classification $Y\in\{+1, -1\}$.
The two prototypes learned by the binary classifier $h$ are represented as $\{q_{+1}, q_{-1}\}$. We drop the bias term for simplicity.
Meanwhile, we assume the existence of causal prototypes  $\{p_{+1}, p_{-1}\}$, which always make correct predictions on the learned representations and do not leverage non-causal features.
To simplify the analysis, we consider the decision boundary vectors $\Delta q=q_{+1}-q_{-1}$ and $\Delta p=p_{+1}-p_{-1}$. 
We analyze the representation $z$ of an instance with label $y$.
Given the principal components $\{e_i\}_{i=1}^d$ computed from $z$ and its augmented variants, we write $z$ as $\sum_{i=1}^d \alpha_i e_i$, where $\alpha_i$ is the magnitude of $z$'s projection on $e_i$.
Similarly, we define the learned decision boundary $\Delta q$'s projection magnitude as $\{\gamma_i\}_{i=1}^d$. 
We write the projection magnitude of causal decision boundary $\Delta p$ as $\{\eta_i\gamma_i\}_{i=1}^d$, 
to view $\Delta p$ as a transformation from $\Delta q$ by a projection magnitude $\eta_i$ on the direction of each principal component.

We can obtain $\hat{z}$ by trimming the top-$m$ principal components (PCs) for $z$. 
Proposition \ref{pp:improve} shows the conditions under which TACT can correct a wrong prediction.  

\smallskip
\begin{proposition} \label{pp:improve}
For any $z$ that is misclassified by the learned decision boundary $\Delta q$, the misclassification can be corrected by using the representation obtained after removing the top-$m$ principal components, if both of the following two conditions are satisfied: 
\begin{equation}
% \textstyle
    y\sum_{i=1}^m\alpha_i\gamma_i <0 \quad \text{and}
    \quad 
    y\sum_{i=m+1}^d\alpha_i\gamma_i >0
    \label{eq:improve_1}
\end{equation}
\begin{equation} \label{eq:improve_2}
% \textstyle
\left|\sum_{i=1}^m\alpha_i\gamma_i\right| >\left|\sum_{i=m+1}^d\alpha_i\gamma_i\right|
\end{equation}
\end{proposition}
Appendix \ref{app:prop_correct} provides the formal proof. Equation \eqref{eq:improve_1} captures the case in which a prediction based solely on the top-$m$ PCs leads to an incorrect outcome, whereas a prediction based on the remaining PCs yields the correct result. 
Equation \eqref{eq:improve_2} 
requires the absolute value of the prediction score derived from the top-$m$ PCs must be greater than that from the remaining PCs.
Together, these conditions in
Proposition \ref{pp:improve} suggests that a wrong prediction can be corrected by TACT when the top-$m$ PCs are solely responsible for the wrong prediction, and the prediction made by the top-$m$ PCs weighs more than the prediction made by the remaining PCs. 

In Proposition~\ref{pp:correct_causal}, we establish the conditions under which the trimmed representation $\hat{z}$ retains sufficient causal information to preserve the correct prediction by the causal decision boundary $\Delta p$.

\begin{proposition}
[Causal Preservation]
\label{pp:correct_causal}
For any original representation $z$, the trimmed representation $\hat{z}$ preserves the correct prediction under the causal decision boundary $\Delta p$ 
if any one of the following conditions holds:
    \begin{equation}
        \begin{cases}
        y\sum\limits_{i=1}^m\eta_i\alpha_i\gamma_i = 0 \\
        y\sum\limits_{i=1}^m\eta_i\alpha_i\gamma_i < 0 \\
        0<y\sum\limits_{i=1}^m\eta_i\alpha_i\gamma_i < y\sum\limits_{i=1}^d\eta_i\alpha_i\gamma_i
    \end{cases}
    \label{eq:correct_causal}
    \end{equation}
\end{proposition}
The proof is provided in Appendix \ref{app:prop_rep}. Equation \eqref{eq:correct_causal} characterizes three cases: (a) the top-$m$ PCs have no contribution to the causal prediction; (b)  the top-$m$ PCs has a negative influence  on the causal prediction and thus their removal is beneficial; (c) the top-$m$ PCs has a positive contribution, but the representation forms by all PCs contribute even more strongly.
When the top-$m$ PCs have no contribution to the causal predictions, they are considered non-causal features. In other words, the removed component $z-\hat{z}$ does not contain causal information. 
When the top-$m$ PCs contain causal information, $m$ should be selected such that the top-$m$ PCs contribute less to the prediction compared to all the PCs, ensuring that the trimmed representation $\hat{z}$ remains causally informative.
In other words, sufficient causal features need to be preserved after causal trimming. 

 

Finally, in Proposition \ref{pp:rep_space_rm_new}, we identify the conditions under which causal trimming would have no negative impact on the prediction of samples that are already correctly classified. 

\begin{proposition} \label{pp:rep_space_rm_new}
Suppose  $z$ is correctly classified by the learned decision boundary $\Delta q$. The trimmed representation $\hat{z}$ obtained via TACT will still be classified correctly if either of the conditions holds: 
\begin{enumerate}
    \item $y(z-\hat{z})\Delta q \leq 0$, or
    \item $y(z-\hat{z})\Delta q > 0$, and Equation \eqref{eq:learned_correct} holds, assuming $\hat{z}$ already satisfies the Causal Preservation condition (Proposition~\ref{pp:correct_causal}).
    \begin{equation}
    % \textstyle
        \mathrm{sign}\left(\sum_{i=m+1}^d \eta_i\alpha_i\gamma_i\right) = \mathrm{sign} \left(\sum_{i=m+1}^d \alpha_i\gamma_i\right)
        \label{eq:learned_correct}
    \end{equation}
\end{enumerate}
\end{proposition}
The proof can be found in Appendix \ref{app:prop_trimmed_miss}. Equation \eqref{eq:learned_correct} indicates that when classification relies only on the representations formed by the remaining PCs, the learned decision boundary makes the same prediction as the causal decision boundary. 
Proposition \ref{pp:rep_space_rm_new} also shows that if a correct prediction is made by the learned decision boundary, TACT will preserve this correstness as long as the removed part $z-\hat{z}$ contributes negatively or does not contribute to the prediction. 
On the other hand, when the trimmed representation $\hat{z}$ contains sufficient causal information as established in Proposition \ref{pp:correct_causal}, the learned decision boundary is required to align directionally with the causal decision boundary defined by the remaining PCs.

\section{Performance Study} \label{sec:experiments}
We study the test-time adaptation performance under real-world distribution shifts, using datasets from multiple modalities, including image, audio, and text. 
Compared to prior works that primarily benchmark on image data, our comprehensive experiments offer broader insights into the generalizability of TACT and other TTA methods.


\textbf{Datasets.} We summarize the datasets used in our experiments below:
\squishlisttwo
    \item Birdcalls \cite{birdcalls_1,birdcalls_2,birdcalls_3}, curated by \cite{gao2023out}, is an audio classification dataset to identify bird species from clips recorded in diverse environments. 
    Each clip is converted into a Mel spectrogram for classification. Distribution shifts stem from variations in microphone gain settings, habitat acoustics (e.g. 
    other animal sounds), and bird population. 
    The test set includes 724 audio clips.
    
    
    \item Camelyon17 \cite{bandi2018detection}, sourced from from the Wilds benchmark \cite{WILDS}, is a medical imaging dataset for binary classification of tumor versus normal tissue images.
    The distribution shift arises from variations in slide staining protocols, patient demographics, and scanner equipment. The test set consists of 85,054 images.
    
    \item CivilComments \cite{borkan2019nuanced}, from the Wilds benchmark \cite{WILDS}, is a natural language dataset comprising user-submitted text comments. The task is to classify whether a comment is toxic or non-toxic. The toxicity is spuriously associated with the mention of certain demographics in the training data. The test set contains 133,782 comments.
    
    
    \item ImageNet-R \cite{imagenet_r}  contains 30,000 images of objects from 200 ImageNet \cite{ILSVRC15} classes. The images consist of various renditions,  
    resulting in visual domain shifts from the original dataset.
    
    
    \item ImageNet-V2 \cite{imagenet_v2} is collected years after the original ImageNet using the same methodology, and includes 10,000 images across 1,000 original classes. It represents a natural temporal shift.
    
\squishend

\textbf{Non-causal feature identification for TACT.} We applied the following data augmentations to identify non-causal features in each dataset:
For Birdcalls, we follow \cite{gao2023out} that investigates augmentations that randomize features independent of labels but dependent on distributions. Here, 
random color jitter is applied to the Mel spectrograms to simulate changes in microphone gain settings.
For Camelyon17, we use 
stain color jitter \cite{tellez2018whole} as suggested in \cite{gao2023out} to mimics variations in histopathological slide staining.
For CivilComments, we randomly prepend or append short demographic-referencing sentences to the original text. 
The full list of sentences is provided in Appendix \ref{app:aug}.
For ImageNet-R and ImageNet-V2, where the sources of distribution shift are unknown, we experiment with general-purpose image augmentations. 
Specifically, we apply AutoAugment \cite{cubuk2019autoaugment} with ImageNet policy and RandomAugment \cite{cubuk2020randaugment}. 
Both methods apply a series of transformations to the images. 
A detailed discussion on augmentation design and selection in practice is presented in Appendix~\ref{app:aug_design}.


\textbf{Baselines.}
Since TACT is a backpropagation-free approach, we 
compare TACT with the following state-of-the-art (SOTA) TTA backpropagation-free algorithms: 
\squishlisttwo
\item T3A \cite{t3a} adapts the classifier by updating class prototypes using confident test-time representations.

\item LAME \cite{lame} adjusts model output probabilities via Laplacian-adjusted maximum likelihood estimation.

\item FOA \cite{foa} introduces an adaptable prompt at model input to match the representation statistics of test and train data.
\squishend


We also implement a variant called TACT-adapt, where predictions from TACT are used to guide gradient-based model updates with cross entropy loss $\mathcal{L}_{CE}$. We employ the information maximization loss $\mathcal{L}_{IM}$ proposed in SHOT \cite{shot} as regularization. 
We optimize the model using the objective:
$ \mathcal{L} = \mathcal{L}_{CE}\left(\hat{y}, y_\text{TACT}\right) + \lambda \mathcal{L}_{IM}\left(\hat{y}\right)$.
$\hat{y}$ is the model's prediction, and $y_\text{TACT}$ is TACT's prediction. $\lambda$ is the hyperparameter balancing the two terms.

We compare TACT-adapt with the following SOTA backpropagation-based methods:
\squishlisttwo
\item SHOT \cite{shot} adapts the feature extractor using information maximization and cross entropy loss on confident prediction.

\item Tent \cite{tent} performs entropy minimization to update the affine parameters of normalization layers at test time.

\item SAR \cite{sar} builds upon Tent by incorporating sharpness-aware minimization and  model reset to mitigate overfitting to noisy samples.

\item DeYO \cite{deyo} identify confident samples that leverage causal features only by image augmentations that destroy shapes and using confidence-reweighted entropy minimization to update the affine parameters.

\item TAST \cite{tast} adapts a trainable module on top of the trained feature extractor via self-training with nearest neighbor information.

\item TSD \cite{tsd} enhances feature representations through self-distillation and local clustering, ensuring alignment and uniformity while filtering noisy labels.

\item PASLE \cite{pasle} refines uncertain pseudo-labels progressively using selective label enhancement with candidate label sets and classifier-consistent loss.
\squishend

\textbf{Model architecture.}
We study TACT on transformer-based architectures, which are increasingly used in practice but remain relatively underexplored in TTA.
Specifically, we use ViT-B/32 \cite{dosovitskiy2020image} as the backbone for Birdcalls, Camelyon17, ImageNet-R, and ImageNet-V2, and DistilBERT \cite{sanh2019distilbert} for CivilComments.
Appendix \ref{app:pretrain} provides more details on model studied. 

\textbf{Hyperparameters and model selection.}
We use a test batch size of 64 \cite{deyo, foa}. 
There are two hyperparameters in TACT, the number of augmentation $n$ and the number of removed principal components $m$. 
We search $n \in \{2^1, 2^2, \dots, 2^8\}$, $m \in [1, 16]$ and $m$ is an integer.
For TACT-adapt, we search $\lambda\in \{1, 5\}\times\{0.1, 1, 10, 100\}$. The rest hyperparameters follow the search space of SHOT.
For all baseline methods, we perform hyperparameter tuning within the search spaces specified in their respective papers. The detailed configurations and search procedures are provided in  Appendix \ref{app:hyper}.
Following the protocol recommended in \cite{ttab}, we employ oracle selection to choose the best-performing hyperparameters, ensuring a fair and consistent evaluation across all methods.

\subsection{Test-time Adaptation Performance}
Following the evaluation protocol of each dataset, we use macro F1 for Birdcalls, accuracy for Camelyon17, ImageNet-R and ImageNet-V2, and worst-group accuracy for CivilComment, whose data are grouped by demographic attributes and toxicity.
Due to the high variability observed in Birdcalls, each experiment is repeated ten times, whereas experiments on the remaining datasets are conducted three times. The mean and standard deviation are summarized in Table~\ref{tb:tta}.

We see that 
TACT consistently outperforms existing backpropagation-free methods on all the datasets, with substantial gains  of 4\% on Birdcalls, 15\% on CivilComments, and 1.7\% on ImageNet-R.
Further, TACT-adapt achieves the best overall performance across all datasets, outperforming both backpropagation-free and backpropagation-based baselines.
These results suggest that non-causal features are a major source of performance degradation under distribution shift, and that removing them improves predictive reliability.
It also confirms the value of TACT not only as a standalone method but also as a reliable supervisory signal for test-time learning.

We  note that TACT performs well when causal features are approximately invariant under augmentation.
For ImageNet-R and ImageNet-V2, AutoAugment \cite{cubuk2019autoaugment} and RandomAugment \cite{cubuk2020randaugment}  maintain the key causal features, which are object structure and shape \cite{geirhos2018imagenettrained, deyo}. 
Other causal features that could be helpful in inferring objects, such as color when inferring strawberries, are altered. 
In addition, the models we perform adaptation on do not have their representation space explicitly constrained such that causal and non-causal features are linearly encoded, disentangled, or orthogonal. 
Yet, the approximate separation of causal and non-causal features by PCA yields consistent performance gains, suggesting the robustness of TACT. 

\begin{table}
\caption{Test-time adaptation performance (\%). We group the methods into backpropagation-free (BP-free) and backpropagation-based (BP-based). The best performance of each dataset is in bold.}
    \centering
  \small  
    \begin{tabular}{ll|ccccc}
    \toprule
    
       & Method & Birdcalls & Camelyon17 & CivilComments & ImageNet-R & ImageNet-V2 \\ 
        \midrule
      &  No TTA & 22.74 & 62.31 & 55.38 & 41.83 & 62.97 \\
        \midrule 
       & T3A  & 26.16$\pm$1.33 & 69.96$\pm$1.98 & 56.43$\pm$0.00 & 41.78$\pm$0.12 & 62.93$\pm$0.02 \\
      BP- & LAME & 23.66$\pm$1.01 & 62.38$\pm$0.03 & 56.24$\pm$0.10 & 41.77$\pm$0.01 & 63.00$\pm$0.02 \\
      free & FOA  & 26.95$\pm$1.81 & 58.36$\pm$0.77 & - & 41.46$\pm$0.16 & 62.76$\pm$0.08 \\
      &  \textbf{TACT} & 31.14$\pm$1.69 & 70.17$\pm$0.05 & 71.80$\pm$0.35 & 43.59$\pm$0.02 & 63.33$\pm$0.10 \\
        \midrule
       & SHOT  & 26.82$\pm$5.14 & 80.28$\pm$5.61 & 13.93$\pm$0.97 & 48.79$\pm$0.08 & 63.32$\pm$0.09 \\
       & Tent  & 23.16$\pm$0.42 & 62.29$\pm$0.01 & 55.38$\pm$0.00 & 42.08$\pm$0.05 & 63.09$\pm$0.03 \\
      &  SAR   & 23.16$\pm$0.42 & 62.30$\pm$0.00 & 55.38$\pm$0.00 & 42.58$\pm$0.11 & 62.97$\pm$0.01 \\
    BP-   & DeYO  & 23.29$\pm$0.39 & 69.64$\pm$1.47 & - & 46.87$\pm$0.08 & 62.96$\pm$0.01\\
    based   & TAST  & 26.08$\pm$1.11 & 83.01$\pm$1.42 & 56.56$\pm$0.20 & 41.09$\pm$0.08 & 62.84$\pm$0.07 \\
       & TSD   & 27.33$\pm$1.75 & 67.33$\pm$4.74 & 55.38$\pm$0.00 & 41.76$\pm$0.01 & 62.98$\pm$0.01 \\
       & PASLE & 27.35$\pm$1.79 & 60.66$\pm$0.04 & 55.77$\pm$0.15 & 46.08$\pm$0.09 & 63.15$\pm$0.04\\
       & \textbf{TACT-adapt} & \textbf{31.25$\pm$3.59} & \textbf{83.70$\pm$1.10} & \textbf{71.98$\pm$0.19} &  \textbf{48.81$\pm$0.05 }&  \textbf{63.44$\pm$0.07} \\
   
    \bottomrule
    \end{tabular}
    \label{tb:tta}
\end{table} 

\subsection{Visualization of Predictions after Causal Trimming}
To gain insight into the predictions made after causal trimming, we employ GradCAM \cite{selvaraju2017grad} to visualize the focus of the original predictions and those made by TACT on samples from ImageNet-R. GradCAM identifies which parts of an input image contribute most to a prediction by computing the gradients of the predicted class score with respect to the embeddings of the image patches. The resulting heatmaps are overlaid on the input images, where brighter regions indicate higher importance for the prediction.

The visualization results are presented in Figure \ref{fig:gradcam}. 
Compared to the original predictions, TACT places less emphasis on non-causal information, such as background elements. For instance, in the snowbird sample, TACT disregards irrelevant features like the surrounding branches.
Similarly, in the white shark example, TACT restricts the focus to the object itself, unlike the original prediction that diffuses significant attention across the background.
The sea background in the scuba diver example, and the dot texture in the background of the strawberry example, are likely to be spuriously correlated with certain prediction classes. These features are de-emphasized by TACT, contributing to a more accurate prediction.

Furthermore, TACT enhances attention on core causal features, leading to a sharper focus on an object's defining characteristics. This is clearly demonstrated in the lorikeet example, where the beak becomes the key focus, and the meerkat example, where attention is concentrated on the banded pattern and body. 
Moreover, in cases where the original prediction neglects causal features, as shown in the space shuttle and pug-dog example, TACT can redirect the emphasis to the actual salient features, such as the nose cone of the space shuttle and the face of the pug-dog, resulting in improved prediction performance.

\begin{figure}
    \centering
    \begin{subfigure}{0.497\linewidth}
      \centering
        \hspace{1.5em} {\scriptsize Input} \hspace{3.5em} {\scriptsize GradCAM} \hspace{2.5em} {\scriptsize TACT-GradCAM}\\
        \includegraphics[width=0.325\linewidth]{figures/examples/correct_bird.png}
        \includegraphics[width=0.325\linewidth]{figures/examples/correct_bird_cam.png}
        \includegraphics[width=0.325\linewidth]{figures/examples/correct_bird_tcam.png}
        {\scriptsize ground truth: snowbird\\}
        \includegraphics[width=0.325\linewidth]{figures/examples/correct_shark.png}
        \includegraphics[width=0.325\linewidth]{figures/examples/correct_shark_cam.png}
        \includegraphics[width=0.325\linewidth]{figures/examples/correct_shark_tcam.png}
        {\scriptsize ground truth: white shark\\}
        \includegraphics[width=0.325\linewidth]{figures/examples/correct_lorikeet.png}
        \includegraphics[width=0.325\linewidth]{figures/examples/correct_lorikeet_cam.png}
        \includegraphics[width=0.325\linewidth]{figures/examples/correct_lorikeet_tcam.png}
        {\scriptsize ground truth: lorikeet\\}     
        \includegraphics[width=0.325\linewidth]{figures/examples/correct_meerkat.png}
        \includegraphics[width=0.325\linewidth]{figures/examples/correct_meerkat_cam.png}
        \includegraphics[width=0.325\linewidth]{figures/examples/correct_meerkat_tcam.png}
        {\scriptsize ground truth: meerkat\\}
        \caption{correct predictions}
        \label{subfig:correct}
    \end{subfigure}
    \begin{subfigure}{0.497\linewidth}
        \centering
        \hspace{1.5em} {\scriptsize Input} \hspace{3.5em} {\scriptsize GradCAM} \hspace{2.5em} {\scriptsize TACT-GradCAM}\\
        \includegraphics[width=0.325\linewidth]{figures/examples/wrong_scuba.png}
        \includegraphics[width=0.325\linewidth]{figures/examples/wrong_scuba_cam.png}
        \includegraphics[width=0.325\linewidth]{figures/examples/wrong_scuba_tcam.png}
        {\scriptsize ground truth: scuba diver; prediction: grey whale\\}
        \includegraphics[width=0.325\linewidth]{figures/examples/wrong_strawberry.png}
        \includegraphics[width=0.325\linewidth]{figures/examples/wrong_strawberry_cam.png}
        \includegraphics[width=0.325\linewidth]{figures/examples/wrong_strawberry_tcam.png}
        {\scriptsize ground truth: strawberry; prediction: bathtub\\}
        \includegraphics[width=0.325\linewidth]{figures/examples/wrong_space_shuttle.png}
        \includegraphics[width=0.325\linewidth]{figures/examples/wrong_space_shuttle_cam.png}
        \includegraphics[width=0.325\linewidth]{figures/examples/wrong_space_shuttle_tcam.png}
        {\scriptsize ground truth: space shuttle; prediction: harp\\}
        \includegraphics[width=0.325\linewidth]{figures/examples/wrong_pug.png}
        \includegraphics[width=0.325\linewidth]{figures/examples/wrong_pug_cam.png}
        \includegraphics[width=0.325\linewidth]{figures/examples/wrong_pug_tcam.png}
        {\scriptsize ground truth: pug-dog; prediction: hatchet\\}
        \caption{wrong predictions corrected by TACT}
        \label{subfig:wrong}
    \end{subfigure}
    \caption{GradCAM visualizations of the original predictions and TACT's predictions.}
    \label{fig:gradcam}
\end{figure}

\subsection{Effect of Hyperparameters}
\label{sec:hparam}
The performance of TACT depends on two key hyperparameters: the number of augmentations $n$ and the number of removed principal components $m$. These parameters govern the accuracy of non-causal direction estimation and the extent of causal trimming, respectively. Figure~\ref{fig:hparam} shows the performance under different numbers of augmentations and removed principal components for the Camelyon17, CivilComments and ImageNet-R datasets. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/hparams.png}
    \caption{Performance across number of augmentation and number of removed principal components.}
    \label{fig:hparam}
\end{figure}

Since representations from augmented samples are used to compute the covariance matrix from which the directions of maximum variances are identified, a larger number of augmentations $n$ generally leads to more stable and accurate identification of non-causal directions. Empirically, we find that values of $n\in\{128, 256, 512\}$ provides sufficient performance,
while small values of $n$ often fail to adequately capture the variance needed for accurate principal component estimation.
The number of removed principal components $m$,  
should be carefully selected 
to ensure effective reduction of non-causal features while retaining  sufficient causal features as suggested by our theoretical analysis.
In practice, 
removing the top principal component, which typically captures the dominant non-causal variation, often suffices. However, for datasets with more complex or layered distribution shifts, such as ImageNet-R, removing more principal components would further boost the performance. 


\subsection{Ablation Study}
We conduct two sets of ablation experiments. 
In the first experiment,  
we isolate the impact of representation trimming, without prototype averaging.
In the second experiment, 
we assess whether prototype averaging alone can sufficiently filter out non-causal features.

Table \ref{tab:ablation} shows the results. 
We observe that trimming the representation $z$ yields better performance than no adaptation, confirming that removing components aligned with non-causal directions in representations is beneficial.
 While using only the averaged trimmed prototypes 
$\hat{q}$
  also improves performance over no adaptation, the gains are generally less significant than when trimmed representations are employed. 
This suggests that relying solely on the averaged trimmed prototypes is insufficient for effectively reducing non-causal features.
The best performance is achieved when both the trimmed representation and the averaged trimmed prototypes are used in conjunction, indicating that mitigating non-causal features in both representations and prototypes is crucial.

\begin{table}[ht]
    \caption{Results of ablation study.}
    \small
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{ccc|ccccc}
    \toprule
         trim $z$ & trim $q$ & average $\hat{q}$ & Birdcalls  & Camelyon17 & CivilComments & ImageNet-R & ImageNet-V2\\
        \midrule 
      \multicolumn{3}{c|}{No TTA}  &22.74 & 62.31 & 55.38 & 41.83 & 62.97 \\
      \midrule
        $\checkmark$ & & &25.91$\pm$1.67 & 69.43$\pm$0.01 & 67.84$\pm$0.37 & 43.21$\pm$0.03  & 63.24$\pm$0.10\\
                     & $\checkmark$ & $\checkmark$ & 27.36$\pm$0.23 & 64.74$\pm$0.05 & 62.41$\pm$0.08 & 42.24$\pm$0.00 & 63.03$\pm$0.01\\
        $\checkmark$ & $ \checkmark$ & $\checkmark$ & \textbf{31.14$\pm$1.69} & \textbf{70.17$\pm$0.05} & \textbf{71.80$\pm$0.35} & \textbf{43.59$\pm$0.02} & \textbf{63.33$\pm$0.10} \\
     \bottomrule
    \end{tabular}
    }
    \label{tab:ablation}
\end{table}

\section{Conclusion and Future Work} \label{sec:conclusion}
We present TACT, a test-time adaptation method that reduces model reliance on non-causal features for test representations. TACT identifies non-causal components in the representation space by analyzing samples with identical causal features but varying non-causal features. The directions of maximum variance among the representations are treated as the non-causal directions.
To adapt the model, we subtract the projection of the representation and class prototypes onto this non-causal direction.
We keep track of the identified directions and utilize the average of the trimmed class prototypes for improved prediction.
We analyze the theoretical conditions for TACT to enhance predictive performance.
Extensive experiments on five real-world out-of-distribution datasets
demonstrate the effectiveness and generalizability of our approach.
While TACT demonstrates strong performance, it requires prior knowledge of the data to select augmentations that vary non-causal features without altering causal ones.
Future work should explore identifying non-causal features when such knowledge is unavailable, and better methods to find non-causal features beyond PCA's orthogonality constraint.


\section*{Acknowledgement} 
We thank Dr Fusheng Liu for the helpful discussions. 
We appreciate the anonymous reviewers and AC for the constructive and valuable feedback.



\bibliographystyle{plain}
\bibliography{ref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section*{NeurIPS Paper Checklist}
\begin{enumerate}

\item {\bf Claims}
    \item[] Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \item[] Answer: \answerYes{}
    \item[] Justification: We clearly state the claims and contributions in the abstract and introduction, which are further justified in Sections~\ref{sec:method},~\ref{sec:math-analysis}, and~\ref{sec:experiments}.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the abstract and introduction do not include the claims made in the paper.
        \item The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. 
        \item The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. 
        \item It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 
    \end{itemize}

\item {\bf Limitations}
    \item[] Question: Does the paper discuss the limitations of the work performed by the authors?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: we discuss the limitations in Section~\ref{sec:conclusion}.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. 
        \item The authors are encouraged to create a separate "Limitations" section in their paper.
        \item The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
        \item The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
        \item The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
        \item The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
        \item If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
        \item While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
    \end{itemize}

\item {\bf Theory assumptions and proofs}
    \item[] Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: We discuss the assumptions of the Propositions in Section~\ref{sec:math-analysis}, and provide the proof in Appendix~\ref{app:all-proofs}.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include theoretical results. 
        \item All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
        \item All assumptions should be clearly stated or referenced in the statement of any theorems.
        \item The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. 
        \item Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
        \item Theorems and Lemmas that the proof relies upon should be properly referenced. 
    \end{itemize}

    \item {\bf Experimental result reproducibility}
    \item[] Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?
    \item[] Answer: \answerYes{} % 
    \item[] Justification: We have provided experimental configurations in Section~\ref{sec:experiments} and Appendix~\ref{app:exp-details} for reproducibility. 
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
        \item If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. 
        \item Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
        \item While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example
        \begin{enumerate}
            \item If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.
            \item If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.
            \item If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).
            \item We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
        \end{enumerate}
    \end{itemize}


\item {\bf Open access to data and code}
    \item[] Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: We provide the code and the used data in the supplemental materials. 
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that paper does not include experiments requiring code.
        \item Please see the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
        \item While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
        \item The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
        \item The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
        \item The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
        \item At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
        \item Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
    \end{itemize}


\item {\bf Experimental setting/details}
    \item[] Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?
    \item[] Answer: \answerYes{}{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: We have provided experimental configurations in Section~\ref{sec:experiments} and Appendix~\ref{app:exp-details}. 
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
        \item The full details can be provided either with the code, in appendix, or as supplemental material.
    \end{itemize}

\item {\bf Experiment statistical significance}
    \item[] Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: We have included error bars for both the main experiments and the ablation study in Section~\ref{sec:experiments}.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
        \item The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
        \item The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
        \item The assumptions made should be given (e.g., Normally distributed errors).
        \item It should be clear whether the error bar is the standard deviation or the standard error of the mean.
        \item It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96\% CI, if the hypothesis of Normality of errors is not verified.
        \item For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
        \item If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
    \end{itemize}

\item {\bf Experiments compute resources}
    \item[] Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: We provide the computer resources used in the Appendix ~\ref{app:exp-details}.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
        \item The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. 
        \item The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). 
    \end{itemize}
    
\item {\bf Code of ethics}
    \item[] Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics \url{https://neurips.cc/public/EthicsGuidelines}?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
        \item If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
        \item The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
    \end{itemize}


\item {\bf Broader impacts}
    \item[] Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?
    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that there is no societal impact of the work performed.
        \item If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
        \item Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
        \item The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
        \item The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
        \item If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
    \end{itemize}
    
\item {\bf Safeguards}
    \item[] Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?
    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: Our paper uses existing public datasets and develops a method trained on these datasets.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper poses no such risks.
        \item Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. 
        \item Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
        \item We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
    \end{itemize}

\item {\bf Licenses for existing assets}
    \item[] Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: 
    We have cited the datasets in Section~\ref{sec:experiments} and baseline related works in Section~\ref{sec:related-work} and Section~\ref{sec:experiments}.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not use existing assets.
        \item The authors should cite the original paper that produced the code package or dataset.
        \item The authors should state which version of the asset is used and, if possible, include a URL.
        \item The name of the license (e.g., CC-BY 4.0) should be included for each asset.
        \item For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
        \item If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, \url{paperswithcode.com/datasets} has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
        \item For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
        \item If this information is not available online, the authors are encouraged to reach out to the asset's creators.
    \end{itemize}

\item {\bf New assets}
    \item[] Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: We provide the code and datasets used in the supplemental materials. 
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not release new assets.
        \item Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. 
        \item The paper should discuss whether and how consent was obtained from people whose asset is used.
        \item At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
    \end{itemize}

\item {\bf Crowdsourcing and research with human subjects}
    \item[] Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? 
    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: The paper does not involve crowdsourcing nor research with human subjects
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
        \item Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. 
        \item According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 
    \end{itemize}

\item {\bf Institutional review board (IRB) approvals or equivalent for research with human subjects}
    \item[] Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?
    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: The paper does not involve crowdsourcing nor research with human subjects.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
        \item Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. 
        \item We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. 
        \item For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.
    \end{itemize}

\item {\bf Declaration of LLM usage}
    \item[] Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required.
    %this research? 
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: The core method development in this research does not involve LLMs as any important, original, or non-standard components, except that we use ChatGPT to generate the augmentations for the textual data in CivilComments dataset, as described in Appendix~\ref{app:aug}.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components.
        \item Please refer to our LLM policy (\url{https://neurips.cc/Conferences/2025/LLM}) for what should or should not be described.
    \end{itemize}

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage 
\input{supp}


\end{document}