\newpage

% 重新设置图和表的计数器，使其前缀与当前章节（附录）关联
\renewcommand{\thefigure}{\Alph{section}\arabic{figure}} % 例如: Figure A1
\renewcommand{\thetable}{\Alph{section}\arabic{table}}   % 例如: Table A1
% 重置图和表的计数器为1
\setcounter{figure}{0}
\setcounter{table}{0}
\section*{APPENDIX}

\section{Use of Large Language Models}

%本研究未使用大型语言模型。所有研究想法、算法设计、实验方法、数据分析和论文写作均完全由作者独立完成。
No Large Language Models were used in this research. All research ideas, algorithmic designs, experimental methodologies, data analysis, and manuscript writing were entirely completed by the authors independently.

% \section{Model Implementation Details}

% \subsection{Experimental Setup}
% % 写实验硬件配置、分辨率设置、数据集使用情况
% For the text-to-image generation evaluation, all latency and FLOPs measurements are obtained by generating 200 images from the DrawBench prompt set. FLUX.1-dev, FLUX.1-schnell, and Qwen-Image-Lightning were measured on NVIDIA A100 GPUs at 1024 $\times$ 1024 resolution, while Qwen-Image and Qwen-Image-Lightning were additionally measured on NVIDIA H20 GPUs at 1328 $\times$ 1328 and 1024 $\times$ 1024 resolutions, respectively.
% For image editing evaluation, all latency and FLOPs measurements for FLUX.1-Kontext-dev are obtained on NVIDIA A100 GPUs using the GEdit-Bench dataset, and those for Qwen-Image-Edit are acquired on NVIDIA H20 GPUs under the same dataset.




\section{Detailed Experimental Setup}
\label{appendix:experiment_setup}

This section provides comprehensive technical details for all experimental configurations mentioned in Section 4.1.

\subsection{Model and Task Specifications}
\paragraph{$\text{FLUX}$.1-dev and Qwen-Image:}% distilled model?
The images generated by the FLUX.1-dev, FLUX.1-schnell, and Qwen-Image-Lightning models are obtained at 1024x1024 resolution using 200 high-quality prompts sourced from the DrawBench benchmark, while those generated by Qwen-Image were obtained at 1328 $\times$ 1328 resolutions. Quality assessment is performed using ImageReward, a robust perceptual metric for text-to-image alignment.


\paragraph{$\text{FLUX}$.1-Kontext-dev and Qwen-Image-Edit:}
We employ the $\text{FLUX}$.1-Kontext-dev and Qwen-Image-Edit model for image editing synthesis. Images editing and quality assessment is performed using GEdit benchmark, which grounded in real-world usages is developed to support more authentic and comprehensive evaluation of image editing models.


\subsection{Hardware and Computational Resources}

All experiments are conducted on enterprise-grade GPU infrastructure:
\begin{itemize}[leftmargin=10pt,topsep=-2pt]
    \item $\text{FLUX}$.1-dev experiments: NVIDIA A100 GPU
    \item $\text{FLUX}$.1-Kontext-dev experiments: NVIDIA A100 GPU
    \item $\text{Qwen}$-Image experiments: NVIDIA H20 GPU  
    \item $\text{Qwen}$-Image-Edit experiments: NVIDIA H20 GPU  

\end{itemize}

\subsection{FreqCa Implementation Parameters}
% Key parameters including frequency decomposition method, prediction order, and caching interval
\begin{itemize}[leftmargin=10pt,topsep=-2pt]
    \item $\text{FLUX}$.1-dev experiments: DCT-based frequency decomposition was adopted.
    \item $\text{FLUX}$.1-Kontext-dev experiments: DCT-based frequency decomposition was adopted.
    \item $\text{Qwen}$-Image experiments: FFT-based frequency decomposition was adopted.
    \item $\text{Qwen}$-Image-Edit experiments: FFT-based frequency decomposition was adopted.
\end{itemize}



\section{Decomposition and Order of Prediction Ablation Study}
% 写FFT、DCT、无分解三种方法的技术细节和选择原理


% 这里我们对比了经典的频率分解方法 FFT和DCT，以及不采用频率分解方法，我们实验发现采用频率分解方法能显著提升效果，没有使用分解方法会导致模型效果sharp drop，然后使用频率分解方法能保持稳定，抑制下降趋势
% 同时，我们在各个方法上进行研究高低频采取不同的策略的结构，我们这里发现错误的预测方法会导致效果下降，低频0阶高频2阶的效果普遍比较好

As shown in Figure~\ref{fig:flux-order-combination}, we systematically compared classical frequency decomposition methods (FFT and DCT) with a baseline that does not perform any frequency decomposition. The results clearly demonstrate that frequency decomposition plays a critical role in stabilizing model performance: omitting decomposition leads to a sharp drop in ImageReward scores, whereas both FFT and DCT significantly mitigate this degradation and maintain stable quality across timesteps. Furthermore, we investigated the impact of different prediction orders for low- and high-frequency components. We observe that inappropriate prediction strategies can easily introduce errors and harm generation quality. Among all tested configurations, the combination of zeroth-order prediction (direct reuse) for low-frequency components and second-order prediction for high-frequency components achieves consistently superior results, validating our hypothesis that low-frequency features should be reused directly while high-frequency components benefit from higher-order modeling. These findings confirm the necessity of frequency-aware design and provide empirical guidance for selecting optimal prediction strategies.

\subsection{Prediction Order Combinations}
% 写不同预测阶数组合的实验设计和结果分析


\begin{figure}[htp]
    \centering
    \includegraphics[width=1\linewidth]{figures/imagereward_subplots.pdf}
    \caption{The ImageReward scores of FLUX.1-dev’s decomposition strategies (FFT, DCT, no-decomposition) paired with different frequency prediction approaches are presented here. This content includes the optimal prediction method for each decomposition strategy where low-frequency reuse and high-frequency prediction apply to FFT and DCT, and direct reuse applies to the no-decomposition (None) strategy.}
    \label{fig:flux-order-combination}
\end{figure}


% 写消融研究的主要发现和结论，解释为什么某些组合效果更好
% 我们的消融研究揭示了频率感知缓存策略的关键洞察。频率分解对稳定性能至关重要，没有分解会导致生成质量急剧下降，而FFT和DCT都能有效缓解这个问题。在预测策略方面，最优配置是低频直接复用配合高频二阶预测，这验证了我们关于不同频率成分时间特性的核心假设。我们进一步发现错误的预测阶数组合会导致显著的性能下降，而FFT和DCT在不同模型上表现相当，表明频率感知设计原理比具体分解技术更关键。

%Our ablation study reveals critical insights about frequency-aware caching strategies. Frequency decomposition proves essential for maintaining stable performance, as strategies without decomposition suffer from sharp quality degradation under higher acceleration ratios, while both FFT and DCT effectively mitigate this issue by enabling more targeted prediction approaches. Regarding prediction strategies, the optimal configuration consistently employs zeroth-order prediction for low-frequency components paired with second-order prediction for high-frequency components, which validates our core hypothesis about the distinct temporal characteristics of different frequency components. We further observe that mismatched prediction orders lead to substantial performance drops, with inappropriate combinations resulting in quality degradation. Across different models and acceleration ratios, both FFT and DCT demonstrate comparable performance with only minor variations, suggesting that the underlying frequency-aware design principle matters more than the specific decomposition technique employed.

% \subsection{Additional Experimental Results}

% \subsection{Model-Specific Performance Analysis}
% % 写不同模型（FLUX.1-dev, Qwen-Image等）上的详细性能数据和分析

% \subsection{Acceleration Ratio vs Quality Trade-offs}
% % 写不同加速比下的质量权衡分析，包括具体数值对比

% \subsection{Memory and Computational Efficiency Analysis}
% % 写内存使用量、计算复杂度的具体分析，与baseline的详细对比






% All latency and FLOPs measurements are obtained by generating 200 images from the DrawBench prompt set. FLUX.1-dev, FLUX.1-schnell, and Qwen-Image-Lightning were measured on NVIDIA A100 GPUs at 1024 $\times$ 1024 resolution, while Qwen-Image and Qwen-Image-Lightning were additionally measured on NVIDIA H20 GPUs at 1328 $\times$ 1328 and 1024 $\times$ 1024 resolutions, respectively.
% Latency and FLOPs measurements for FLUX.1-Kontext-dev are obtained on NVIDIA A100 GPUs using the GEdit-Bench dataset, and those for Qwen-Image-Edit are acquired on NVIDIA H20 GPUs under the same dataset.