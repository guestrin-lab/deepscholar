\section{Related Works}\label{sec:Related Works}
\vspace{-3mm}

Diffusion models have emerged as a cornerstone of modern generative AI, exhibiting state-of-the-art capabilities in synthesizing visual content~\citep{sohl2015deep,ho2020DDPM}. While early models were predominantly built upon U-Net architectures~\citep{ronneberger2015unet}, their scalability limitations paved the way for the Diffusion Transformer (DiT)~\citep{peebles2023dit}. The DiT architecture has since become foundational, catalyzing a wave of powerful models across diverse domains~\citep{opensora,yang2025cogvideox}. Nevertheless, the iterative nature of the diffusion sampling process imposes a significant computational burden during inference, making acceleration a critical area of research~\citep{ho2020DDPM,peebles2023dit}. Current efforts to enhance efficiency are largely focused on two complementary directions: reducing the number of sampling steps and accelerating the denoising network itself.

\vspace{-3mm}
\subsection{Sampling Timestep Reduction}
\vspace{-1mm}

One primary strategy seeks to minimize the number of required sampling iterations while preserving generation quality. Seminal work like DDIM introduced deterministic sampling to reduce step counts without significant fidelity loss~\citep{songDDIM}. This concept was further refined by the DPM-Solver series, which employed high-order ODE solvers to achieve faster convergence~\citep{lu2022dpm,lu2022dpm++,zheng2023dpmsolvervF}. Other notable approaches include knowledge distillation, which trains a student model to emulate multiple denoising steps of a larger teacher model~\citep{salimans2022progressive,meng2022on}, and Rectified Flow, which learns to straighten the generation path between noise and data distributions~\citep{refitiedflow}. More recently, Consistency Models have enabled high-quality synthesis in a single step by directly mapping noise to clean data, circumventing the need for a sequential path~\citep{song2023consistency}.

\vspace{-3mm}
\subsection{Denoising Network Acceleration}
\vspace{-1mm}

An alternative to reducing timesteps is to decrease the computational cost of each forward pass through the denoising network. This is typically achieved via model compression or feature caching.

\vspace{-2mm}
\paragraph{Model Compression-based Acceleration.} 
\vspace{-1mm}

One avenue involves model compression, which includes techniques such as network pruning~\citep{structural_pruning_diffusion, zhu2024dipgo}, quantization~\citep{10377259, shang2023post, kim2025ditto}, and various forms of token reduction that dynamically shorten the input sequence length~\citep{bolya2023tomesd, kim2024tofu, zhang2024tokenpruningcachingbetter, zhang2025sito}. While effective, these methods often necessitate a fine-tuning or retraining stage to mitigate the potential loss of expressive power inherent in model simplification~\citep{li2024snapfusion,10377259}.

\vspace{-2mm}
\paragraph{Feature Caching-based Acceleration.}
\vspace{-2mm}
A compelling training-free alternative is feature caching, which exploits temporal redundancies in the denoising process. Pioneered in U-Net architectures through FasterDiffusion and DeepCache, this paradigm was subsequently adapted to DiTs. Initial efforts focused on a ``cache then reuse'' strategy, while advanced techniques like FORA and $\Delta$-DiT refined this approach. This concept evolved with more sophisticated mechanisms, including dynamic token-level updates (ToCa), adaptive sampling (RAS~\citep{liu2025regionadaptivesamplingdiffusiontransformers}), and explicit error correction frameworks~\citep{qiu2025acceleratingdiffusiontransformererroroptimized, chenIncrementCalibrated2025, chuOmniCache2025}. A pivotal shift was the ``cache then forecast'' paradigm introduced by TaylorSeeer, which was further advanced by more robust numerical methods in FoCa~\citep{zhengFoCa2025}, HiCache~\citep{fengHiCache2025}, and SpeCa~\citep{Liu2025SpeCa}.

However, a crucial flaw underlies these sophisticated paradigms, as hinted at by preliminary frequency-domain analyses. For instance, PAB~\citep{zhao2024PAB} insightfully associated different attention mechanisms with distinct frequency bands but did not delve into token-level frequency dynamics. Similarly, while FasterCache~\citep{lvFasterCacheTrainingFreeVideo2025} examined the frequency-domain differences within Classifier-Free Guidance , its findings were confined to this specific context, not addressing the more universal dynamics of temporal feature evolution and thus showing limited practical acceleration.

In contrast to prior methods that treat features as a monolithic whole, we propose \textbf{FreqCa}, which resolves quality degradation in caching by decomposing features into their stable low-frequency and volatile high-frequency components for differentiated treatment. As an added benefit,  we introduce the Cumulative Residual Feature, collapsing the memory complexity from $\mathcal{O}(L)$ to $\mathcal{O}(1)$ to solve the resource inefficiency of prior ``layer-wise'' architectures.
\vspace{-4mm}
