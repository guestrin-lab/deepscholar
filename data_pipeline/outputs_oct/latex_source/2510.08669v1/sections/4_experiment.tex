\section {Experiment}
\vspace{-1.5mm}

\subsection{Experiment Settings}
\vspace{-1.5mm}


\paragraph{Model Configurations.}
The experiments are conducted on four state-of-the-art visual generative models—\textbf{FLUX.1-dev}~\citep{flux2024}, \textbf{Qwen-Image}~\citep{liu2023rectified}, \textbf{
FLUX.1-Kontext-dev}~\citep{kontext2025}, and \textbf{Qwen-Image-Edit}~\citep{salimans2022progressive}. 


\vspace{-1.5mm}
\paragraph{Evaluation and Metrics.} 
For the text-to-image generation evaluation, we adopt the DrawBench~\citep{saharia2022drawbench} benchmark. The generated samples are systematically evaluated using ImageReward~\citep{xu2023imagereward} and CLIP Score~\citep{hessel2021clipscore}, which jointly measure image quality and text–image semantic alignment. To assess visual fidelity, we further employ PSNR, SSIM~\citep{wang2004imagequality} and LPIPS~\citep{zhangUnreasonableEffectivenessDeep2018}, thereby capturing both pixel-level similarity and perceptual consistency. Additionally, we evaluate general-purpose image editing using the GEdit benchmark~\citep{gedit2024}, which systematically assesses instruction-driven editing fidelity and alignment to target modifications under textual and visual guidance. 

% All latency and FLOPs measurements are obtained by generating 200 images from the DrawBench prompt set. FLUX.1-dev, FLUX.1-schnell, and Qwen-Image-Lightning were measured on NVIDIA A100 GPUs at 1024 $\times$ 1024 resolution, while Qwen-Image and Qwen-Image-Lightning were additionally measured on NVIDIA H20 GPUs at 1328 $\times$ 1328 and 1024 $\times$ 1024 resolutions, respectively.
% Additionally, we evaluate general-purpose image editing using the GEdit benchmark~\citep{gedit2024}, which systematically assesses instruction-driven editing fidelity and alignment to target modifications under textual and visual guidance. Latency and FLOPs measurements for 
% FLUX.1-Kontext-dev are obtained on NVIDIA A100 GPUs using the GEdit-Bench dataset, and those for Qwen-Image-Edit are acquired on NVIDIA H20 GPUs under the same dataset.


% qwenimage 1328*1328
% For the evaluation of text-to-image generation, we adopt the DrawBench~\citep{saharia2022drawbench} benchmark, generating images for 200 prompts at a resolution of 1024×1024 for FLUX-Dev, FLUX-Schnell, and Qwen-Image-Lightning, and at 1328×1328 for Qwen-Image. The generated samples are systematically evaluated using ImageReward~\citep{xu2023imagereward} and CLIP Score~\citep{hessel2021clipscore}, which collectively measure image quality and text-image semantic alignment. To further assess visual fidelity, we employ PSNR, SSIM~\citep{wang2004imagequality}, and LPIPS~\citep{zhangUnreasonableEffectivenessDeep2018}, capturing both pixel-level similarity and perceptual consistency. All latency and FLOPs measurements for FLUX-Dev, FLUX-Schnell, and Qwen-Image-Lightning are obtained using NVIDIA A100 GPUs by generating 200 images at 1024×1024 resolution from the DrawBench prompt set, while those for Qwen-Image are measured on NVIDIA H20 GPUs at 1328×1328 resolution.

% Additionally, we evaluate general-purpose image editing using the GEdit benchmark~\citep{gedit2024}, which systematically assesses instruction-following editing fidelity and alignment with target modifications under both textual and visual guidance. Latency and FLOPs measurements for FLUX-Kontext are obtained on NVIDIA A100 GPUs using the GEdit-Bench dataset, and those for Qwen-Image-Edit are acquired on NVIDIA H20 GPUs under the same dataset.


\vspace{-1.5mm}
\subsection{Text-to-Image Generation}
\vspace{-1.5mm}

\subsubsection{FLUX.1-dev}
\vspace{-1.5mm}
\input{tables/1_flux}
%On FLUX.1-dev, we compare \textit{FreqCa} with several state-of-the-art acceleration methods. At \textbf{2.63$\times$} speedup, \textit{FreqCa} achieves an \textbf{ImageReward of 1.00}, clearly outperforming FORA and TeaCache. At \textbf{4.99$\times$} speedup, it consistently maintains lossless quality. Even under the extreme setting of \textbf{6.24$\times$} speedup, \textit{FreqCa} delivers an \textbf{{81.7\%}} reduction in latency with only a \textbf{2\%} drop in ImageReward (0.97), whereas the best competing method, TaylorSeer, suffers a degradation as large as \textbf{13.1\%}. Overall, across all acceleration levels, \textit{FreqCa} not only sustains superior quality metrics but also achieves substantially better perceptual fidelity  than all competing approaches. \textit{FreqCa} also achieves \textbf{1.68$\times$} speedup on distilled FLUX-schnell while improving ImageReward from 0.93 to 0.95.
On FLUX.1-dev, \textit{FreqCa} consistently outperforms state-of-the-art acceleration methods across different speedup levels. At \textbf{2.63$\times$} speedup, \textit{FreqCa} achieves an \textbf{ImageReward of 1.00}, clearly outperforming FORA and TeaCache. At \textbf{4.99$\times$} speedup, it maintains lossless quality. Even under \textbf{6.24$\times$} speedup, \textit{FreqCa} achieves only a \textbf{2\%} drop in ImageReward (0.97), while TaylorSeer suffers a degradation of \textbf{13.1\%}. \textit{FreqCa} also achieves \textbf{2.00$\times$} speedup on distilled FLUX.1-schnell while improving ImageReward from 0.93 to 0.95.







% \subsubsection{HiDream}
% \input{tables/3_hidream}
% \paragraph{Quantitative Study.}
% \paragraph{Qualitative Study.}
\vspace{-1.5mm}
\subsubsection{Qwen-Image}
\vspace{-1.5mm}
\input{tables/4_qwen_image}



%On Qwen-Image, we compare \textit{FreqCa} with several state-of-the-art acceleration methods. At \textbf{3.49$\times$} speedup, \textit{FreqCa} achieves an \textbf{ImageReward of 1.19}, clearly outperforming TaylorSeer (1.06). At \textbf{4.39$\times$} speedup with \textbf{7.14$\times$} FLOPs reduction, \textit{FreqCa} delivers a \textbf{77.2\%} reduction in latency with only a \textbf{16.8\%} drop in ImageReward (1.04), whereas TaylorSeer shows a much larger \textbf{40.8\%} quality loss under similar acceleration conditions. The perceptual quality metrics (PSNR, SSIM, LPIPS) consistently favor \textit{FreqCa} across all speedup levels.
%On Qwen-Image, At \textbf{4.28$\times$} speedup, \textit{FreqCa} achieves an ImageReward of \textbf{1.20}, clearly outperforming TaylorSeer (1.01). At \textbf{5.68$\times$} speedup with \textbf{7.14$\times$} FLOPs reduction, \textit{FreqCa} delivers a \textbf{82.4\%} reduction in latency with only a \textbf{18.4\%} drop in ImageReward (1.02), whereas TaylorSeer shows a much larger \textbf{41.6\%} quality loss (0.73) under similar acceleration conditions. The perceptual quality metrics (PSNR, SSIM, LPIPS) consistently favor \textit{FreqCa} across all speedup levels. \textit{FreqCa} achieves \textbf{2.21$\times$} speedup on distilled Qwen-Image-Lightning with minimal quality degradation.
On Qwen-Image, \textit{FreqCa} demonstrates superior performance across different acceleration levels. At \textbf{5.00$\times$} speedup, \textit{FreqCa} achieves an ImageReward of \textbf{1.20}, outperforming TaylorSeer (1.01). At \textbf{7.14$\times$} speedup, \textit{FreqCa} shows only a \textbf{18.4\%} drop in ImageReward (1.02), while TaylorSeer suffers a \textbf{41.6\%} quality loss (0.73). \textit{FreqCa} achieves \textbf{4.00$\times$} speedup on distilled Qwen-Image-Lightning with minimal quality degradation.



\vspace{-1.5mm}
\subsection{Image Editing}
\vspace{-1.5mm}
\subsubsection{FLUX.1-Kontext-dev}
\vspace{-1.5mm}


%On FLUX-Kontext, we compare \textit{FreqCa} with several SOTA acceleration methods. At \textbf{4.66$\times$} speedup with \textbf{5.00$\times$} FLOPs reduction, \textit{FreqCa} achieves a Q\_O score of \textbf{6.195}, outperforming ToCa (6.125). Even under the extreme setting of \textbf{6.24$\times$} FLOPs reduction, \textit{FreqCa} delivers an \textbf{82.6\%} reduction in latency with only a \textbf{0.4\%} drop in Q\_O score (6.190), whereas the best competing method TaylorSeer shows a degradation of \textbf{2.2\%} (6.074). \textit{FreqCa} maintains strong quality metrics and demonstrates better perceptual fidelity than competing approaches.
On FLUX.1-Kontext-dev, \textit{FreqCa} outperforms other acceleration methods. At \textbf{5.00$\times$} speedup, \textit{FreqCa} achieves a Q\_O score of \textbf{6.195}, outperforming ToCa (6.125). At \textbf{6.24$\times$} speedup, \textit{FreqCa} shows only a \textbf{0.4\%} drop in Q\_O score, demonstrating better perceptual fidelity.
\input{tables/2_flux_kontext}

\subsubsection{Qwen-Image-Edit}
\vspace{-3mm}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]
    {figures/qwenimage_edit_acceleration_methods_comparison.pdf}
    \captionof{figure}{On 
    Qwen-Image-Edit, \textit{FreqCa} delivers higher speedup with near-original editing quality }

    \label{fig:qwen-image-edit}
    \vspace{-2mm}
\end{figure}

\input{tables/5_qwen_image_edit}


On Qwen-Image-Edit, \textit{FreqCa} demonstrates superior performance in bilingual editing tasks. At \textbf{5.00$\times$} speedup, \textit{FreqCa} achieves Q\_O scores of {7.49} on GEdit-CN and {7.52} on GEdit-EN, outperforming TaylorSeer (6.92 and 6.89). At \textbf{6.24$\times$} speedup, \textit{FreqCa} shows quality drops of only \textbf{1.9\%} and \textbf{4.3\%}, while TaylorSeer degrades by {14.8\%} and {16.3\%}.

%As shown in \textbf{Figure \ref{fig:qwen-image-edit}}, qualitative evaluation reveals \textit{FreqCa}'s superior visual quality preservation. At comparable acceleration levels, FORA ($\times$5.45) exhibits significant color artifacts, while TaylorSeer ($\times$5.28) fails to preserve structural integrity, resulting in distorted outputs. In contrast, \textit{FreqCa} ($\times$5.57) maintains consistent visual quality across the evaluated editing tasks, effectively preserving both semantic accuracy and perceptual fidelity under acceleration. As shown in \textbf{Figure \ref{fig:quality}},  FreqCa ($\times$6.24) significantly accelerates processing while maintaining image-editing quality that is comparable to, or even surpasses, that of the original model.

As shown in \textbf{Figures \ref{fig:quality} and \ref{fig:qwen-image-edit} }, qualitative evaluation confirms \textit{FreqCa}'s superior visual quality preservation. While FORA (6.24$\times$), Duca(5.46$\times$) and TaylorSeer (6.24$\times$) exhibit significant artifacts, \textit{FreqCa} (6.24$\times$) maintains consistent visual quality comparable to the original model.








\vspace{-1.5mm}


\subsection{Ablation Studies}
\vspace{-1.5mm}

\subsubsection{Cache Memory and Computational Efficiency}
\vspace{-1.5mm}


Conventional layer-wise caching methods store both attention and MLP outputs per layer ($N=2$) and retain $m+1$ historical states for $m$-th order prediction, yielding memory cost $\mathcal{K}_{\text{layer}} = 2(m+1)L$. For FLUX.1-dev ($L=57$) with second-order prediction ($m=2$), this requires 342 cache units.

In contrast, \textit{FreqCa} caches only the CRF, adopting a frequency-decoupled strategy: low-frequency components are reused (1 unit), while high-frequency components employ second-order Hermite interpolation (3 units). The total cost is constant:
\[
\mathcal{K}_{\text{FreqCa}} =1+3= 4, \quad R = \frac{\mathcal{K}_{\text{FreqCa}}}{\mathcal{K}_{\text{layer}}} = \frac{4}{(m+1)\cdot N \cdot L} \approx 1.17\% \quad (m=2,\ L=57,\,N=2),
\]
reducing memory complexity from $\mathcal{O}(L)$ to $\mathcal{O}(1)$. Computationally, prediction steps  incur negligible cost $C_{\text{pred}} \ll C_{\text{full}}$. Executing a full forward pass every $S$ steps yields average cost:
\[
\bar{\mathcal{C}} = \tfrac{1}{S} C_{\text{full}} + (1-\tfrac{1}{S}) C_{\text{pred}} \quad \Rightarrow \quad \text{Speedup} \approx S \quad \text{as } C_{\text{pred}} \to 0.
\]

\textit{FreqCa} achieves near-S$\times$ acceleration with only \textbf{~1\%} additional memory overhead, establishing the first constant-memory, high-throughput inference acceleration framework for diffusion models. 

\begin{figure}[htp]
    \centering
    \begin{minipage}[t]{0.63\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/order_combination_comparison.pdf}
         \vspace{-4mm}
        \caption{QwenImage ablation results showing image quality under different frequency prediction configurations and acceleration ratios. $(x,y) = (low, high)$ prediction orders.}
        \label{fig:qwen-image-ablation}
        \vspace{-2mm}
    \end{minipage}%
    \hfill
    \begin{minipage}[t]{0.35\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Bubble.pdf}
        \vspace{-4mm}
        \caption{Imagereward versus speedup ratio across methods. Bubble size indicates cache memory.}
    \label{fig:Bubble}
    \vspace{-2mm}
\end{minipage}
\end{figure}


\input{tables/6_ablation_cache_memory}

\subsubsection{Decomposition and Order of Prediction Ablation Study}
\vspace{-1.5mm}
%We perform an ablation study on FLUX to identify the optimal frequency decomposition method and prediction order. The study compares three decomposition strategies including FFT, DCT, and a baseline without decomposition, each paired with various prediction approaches for frequency components. Figure~\ref{fig:flux-order-combination} in the Appendix reports the ImageReward scores for all combinations, allowing us to determine the best prediction order per decomposition method. Figure~\ref{fig:flux-ablation-single} compares these optimized configurations. The results indicate that the DCT-based approach, particularly with low-frequency reuse and high-frequency prediction, achieves consistently high ImageReward across acceleration ratios and demonstrates marked superiority at larger intervals ($N > 8$). This robustness under high acceleration factors confirms the rationale for our choice. A separate ablation on Qwen-Image revealed that the FFT-based method with the same prediction strategy performed best. As shown in Figure~\ref{fig:qwen-image-ablation}, other configurations result in significant quality degradation. Although we performed similar ablations for all models presented in this paper to determine their optimal settings, we omit detailed results for brevity.
We perform an ablation study on FLUX.1-dev to identify the optimal frequency decomposition method and prediction order. The study compares three decomposition strategies including FFT, DCT, and a baseline without decomposition, each paired with various prediction approaches for frequency components. Figure~\ref{fig:flux-ablation-single} compares these optimized configurations. The results indicate that the DCT-based approach, particularly with low-frequency reuse and high-frequency prediction, achieves consistently high ImageReward across acceleration ratios and demonstrates marked superiority at larger intervals ($N > 8$). This robustness under high acceleration factors confirms the rationale for our choice. A separate ablation on Qwen-Image revealed that the FFT-based method with the same prediction strategy performed best. As shown in Figure~\ref{fig:qwen-image-ablation}, other configurations result in significant quality degradation when compared to our optimal settings.










\begin{figure}[htbp]
    \centering
    \vspace{-3mm}
    \begin{minipage}[t]{0.57\linewidth}
         \centering
        \raisebox{4mm}{\includegraphics[width=\linewidth]{figures/flux_edit_comparison.pdf}}
        \vspace{-6mm}
        \caption{On FLUX.1-Fill-dev, \textit{FreqCa} achieves 6.24$\times$ acceleration while preserving image inpainting quality indistinguishable from the original.}
        \label{fig:flux-fill}
        \vspace{-5mm}
    \end{minipage}
    \hfill
    % 右：占位图
    \vspace{-3mm}
    \begin{minipage}[t]{0.4\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/single_comparison_plot.pdf}
        \vspace{-6mm}
        \caption{Comparing optimal predictors under varied frequency decompositions on FLUX across speedup ratios.}
        \label{fig:flux-ablation-single}
        \vspace{-5mm}
    \end{minipage}
    % cn
\end{figure}


