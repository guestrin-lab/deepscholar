\section{Method}
\vspace{-1.5mm}
\subsection{Preliminary}
% Diffusion models~\citep{ho2020denoising,sohl2015deep,song2020score} are a class of generative models that operate by learning to reverse a fixed, forward noising process. This reversal is discretized into a finite number of timesteps, where the model's objective at each step $t$ is to approximate the posterior distribution $p(x_{t-1} | x_t)$ with a learned Gaussian transition. This transition is parameterized as:
% \begin{equation}
% p_\theta(x_{t-1} | x_t) = \mathcal{N}\!\left(x_{t-1}; \frac{1}{\sqrt{\alpha_t}}\Big(x_t - \tfrac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \,\tau_\theta(x_t, t)\Big), \, \beta_t \mathbf{I} \right),
% \end{equation}
% where $\tau_\theta(x_t, t)$ is a neural network trained to predict the noise component in $x_t$, and $\alpha_t$, $\beta_t$ are hyperparameters derived from a pre-defined noise schedule. The full generative process starts with a sample $x_T$ from a standard Gaussian distribution and iteratively denoises it by sampling from $p_\theta(x_{t-1} | x_t)$ until a clean sample $x_0$ is produced.



\vspace{-1.5mm}
\subsubsection{Diffusion Transformer Architecture.}  
\vspace{-1.5mm}
The Diffusion Transformer (DiT)~\citep{DiT} employs a hierarchical structure 
$\mathcal{G} = g_1 \circ g_2 \circ \cdots \circ g_L$, 
where each module $g_l = \mathcal{F}_{\text{SA}}^l \circ \mathcal{F}_{\text{CA}}^l \circ \mathcal{F}_{\text{MLP}}^l$ 
is composed of self-attention (SA), cross-attention (CA), and multilayer perceptron (MLP) components. 
In DiT, these components are dynamically adapted over time to handle different noise levels during the image generation process. 
The input $\mathbf{x}_t = \{x_i\}_{i=1}^{H \times W}$ is represented as a sequence of tokens corresponding to image patches. 
Each module integrates information through residual connections of the form 
$\mathcal{F}(\mathbf{x}) = \mathbf{x} + \text{AdaLN} \circ f(\mathbf{x})$, 
where AdaLN denotes adaptive layer normalization, which stabilizes training and improves learning effectiveness.


\vspace{-1.5mm}
\subsubsection{Frequency Decomposition Methods }
\vspace{-1mm}
Frequency decomposition, through methods like the Fast Fourier Transform (FFT) and Discrete Cosine Transform (DCT), is a powerful technique for decoupling signals into distinct components. This process separates a signal into its \textbf{low-frequency} components, which typically represent global structure and smooth layouts, and its \textbf{high-frequency} components, which correspond to fine-grained details and sharp edges. In the context of diffusion models, this decoupling allows us to differentiate between stable, foundational structures and volatile, transient details along the generative trajectory. 





\vspace{-2mm}
\subsection{Frequency-Aware Cache Acceleration Framework}
\vspace{-2mm}
In this section, we introduce the \textbf{FreqCa (Frequency-aware Feature Caching)} framework, which is built upon three key components: (i) performing frequency decomposition on the feature to be cached and applying separate strategies for its low- and high-frequency components;  (ii) employing a nonlinear Hermite-polynomial-based predictor for the high-frequency part to improve prediction accuracy; and (iii) identifying the Cumulative Residual Feature (CRF) as a novel, highly efficient single-tensor caching target that encapsulates the entire transformation history of the model.

\begin{figure}[htb]
    \centering
    \includegraphics[width=1\linewidth]{figures/mmm3.pdf}
    \vspace{-7mm}
    \caption{\textbf{Overview of the FreqCa framework.}\textbf{ (a) CRF Caching }: Instead of caching features at every layer, we cache only the single Cumulative Residual Feature (CRF) at the end. \textbf{(b) Frequency-aware Caching}: The cached features are separated into low- and high-frequency bands using frequency decomposition techniques such as FFT or DCT. \textbf{(c) Low-Frequency Strategy}: Low-frequency component is directly reused from the prior step. \textbf{(d) High-Frequency Strategy}:  High-frequency component is forecasted using a Hermite predictor fitted on the last two activated steps.}
    \label{fig:method}
    \vspace{-2mm}
\end{figure}

\vspace{-2.5mm}
\paragraph{1. Frequency-Decomposed Caching and Prediction Strategy.} 

Our differentiated caching strategy is motivated by the distinct temporal dynamics of frequency components. Low-frequency components exhibit high similarity but low continuity, making them stable but difficult to predict. Conversely, high-frequency components are less similar but more continuous, making them volatile yet predictable along a trajectory. This key difference means that a one-size-fits-all approach is not the best and that a differentiated strategy is required.


To implement this, we first decompose the feature $\mathbf{z}_t$ into its constituent parts using a generic frequency transform $\mathcal{D}(\cdot)$:
\[
\mathbf{z}_t = \mathbf{z}_t^{\text{low}} + \mathbf{z}_t^{\text{high}}, \quad \text{where} \quad \mathbf{z}_t^{\text{low/high}} = \mathcal{P}_{\text{low/high}}\!\big(\mathcal{D}(\mathbf{z}_t)\big).
\]
Here, $\mathcal{P}_{\text{low/high}}$ are complementary projection operators. The low-frequency part governs global structure, while the high-frequency part encodes fine details.




Based on their dynamics, we apply tailored strategies. For \textbf{the stable low-frequency component} $\mathbf{z}_t^{\text{low}}$, we apply a direct reuse strategy to maintain global consistency with negligible cost:
$
    \widehat{\mathbf{z}}_t^{\text{low}} = \mathbf{z}_{t-1}^{\text{low}}.
$

For the \textbf{predictable high-frequency component} $\mathbf{z}_t^{\text{high}}$, we employ a nonlinear predictor based on Hermite polynomials to accurately forecast its trajectory. Each high-frequency coefficient $\widehat{h}_i$ at a normalized time $s_t \in [-1,1]$ is modeled as:
$
    \widehat{h}_i(s_t) \;=\; \sum_{k=0}^{m} c_{i,k}\,\mathrm{He}_k(s_t),
$
where the coefficients $c_{i,k}$ are estimated via least-squares regression from the $K$ most recent cached steps. This yields the precisely reconstructed high-frequency component $\widehat{\mathbf{z}}_t^{\text{high}}$.

Finally, the two components are recombined  to yield the final predicted feature, $\widehat{\mathbf{z}}t = \widehat{\mathbf{z}}_t^{\text{low}} + \widehat{\mathbf{z}}_t^{\text{high}}$.

\vspace{-2mm}
\paragraph{2.Cumulative Residual Feature (CRF)} 
At its core, a Diffusion Transformer (DiT) is a deep stack of $L$ residual blocks. The transformation at each block $l$ is not a replacement but an incremental update, as described by the standard residual connection:
$
    \mathbf{h}^{(l+1)} \;=\; \mathbf{h}^{(l)} \;+\; \mathcal{F}^{(l)}\!\big(\mathbf{h}^{(l)},\, t\big),
$
where $\mathcal{F}^{(l)}(\cdot,t)$ denotes the transformation module at layer $l$ (including Attention, and MLP), which is dynamically modulated by the diffusion timestep $t$ (e.g., through AdaLN).

The structure of the DiT's final output is thus revealed: $\phi_L(\mathbf{x}_t) \;=\; \mathbf{h}^{(0)} \;+\; \sum_{l=0}^{L-1} \mathcal{F}^{(l)}\!\big(\mathbf{h}^{(l)},\, t\big)$.
This formulation shows that the final output is not just another intermediate feature, but the \textbf{accumulation of the initial input and all subsequent residual updates}. We define this special output $\mathbf{z}_t \;\triangleq\; \phi_L(\mathbf{x}_t)$ and name it the \textbf{Cumulative Residual Feature (CRF)}, reflecting its composite nature.

This insight leads to a more memory-efficient strategy. While conventional layer-wise caching must store all intermediate features $\{\mathbf{h}^{(l)}\}_{l=0}^{L-1}$, our approach uses the fact that the CRF already contains the entire transformation history. We use this single, globally fused tensor as a highly efficient replacement for the full feature set. As shown in Figure~\ref{fig:lite_error_analysis}, caching only the CRF achieves nearly identical reconstruction fidelity to full layer-wise caching, incurring only a 4\% higher MSE on average, which confirms that the CRF acts as a near-lossless compression of the entire computational path. This makes it an ideal lightweight caching target, enabling a revolutionary reduction in memory complexity from $\mathcal{O}(L)$ to $\mathcal{O}(1)$ without a meaningful sacrifice in quality.

\vspace{-8mm}
\begin{figure}[htbp]
  \centering
  \begin{minipage}[t]{0.65\textwidth}
    \centering
       \raisebox{5mm}{\includegraphics[width=1.09\linewidth]{figures/timestep_boxplot_filled_with_titles.pdf}}
       \vspace{-9mm}
    \caption{Box plots of Mean Squared Error (MSE) between ground-truth and predicted features per timestep. (a) layer-wise feature caching and (b) cumulative residual feature (CRF) caching.}
    \label{fig:lite_error_analysis}
    \vspace{-8mm}
  \end{minipage}\hfill%
  \begin{minipage}[t]{0.32\textwidth}
    \centering
    \includegraphics[width=1.095\linewidth]{figures/qwenimage_edit_comparision_radar.pdf}
    \vspace{-9mm}
    \captionof{figure}{Gedit Benchmark on Qwen-Image-Edit, {\textit{FreqCa}} outperforms most baselines.}
    \label{fig:quality}
    \vspace{-8mm}
  \end{minipage}
\end{figure}

\vspace{-1.5mm}
