\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Blattmann et~al.(2023)Blattmann, Dockhorn, Kulal, Mendelevitch, Kilian, Lorenz, Levi, English, Voleti, Letts, et~al.]{blattmann2023SVD}
Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et~al.
\newblock Stable video diffusion: Scaling latent video diffusion models to large datasets.
\newblock \emph{arXiv preprint arXiv:2311.15127}, 2023.

\bibitem[Bolya \& Hoffman(2023)Bolya and Hoffman]{bolya2023tomesd}
Daniel Bolya and Judy Hoffman.
\newblock Token merging for fast stable diffusion.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  4599--4603, 2023.

\bibitem[Chen et~al.(2024)Chen, Shen, Ye, Cao, Tu, Bouganis, Zhao, and Chen]{chen2024delta-dit}
Pengtao Chen, Mingzhu Shen, Peng Ye, Jianjian Cao, Chongjun Tu, Christos-Savvas Bouganis, Yiren Zhao, and Tao Chen.
\newblock $\delta$-dit: A training-free acceleration method tailored for diffusion transformers.
\newblock \emph{arXiv preprint arXiv:2406.01125}, 2024.

\bibitem[Chen et~al.(2025)Chen, Li, Jia, Ye, and Ma]{chenIncrementCalibrated2025}
Zhiyuan Chen, Keyi Li, Yifan Jia, Le~Ye, and Yufei Ma.
\newblock Accelerating diffusion transformer via increment-calibrated caching with channel-aware singular value decomposition, 2025.

\bibitem[Chu et~al.(2025)Chu, Wu, Fen, and Zhang]{chuOmniCache2025}
Huanpeng Chu, Wei Wu, Guanyu Fen, and Yutao Zhang.
\newblock Omnicache: A trajectory-oriented global perspective on training-free cache reuse for diffusion transformer models, 2025.

\bibitem[Fang et~al.(2023)Fang, Ma, and Wang]{structural_pruning_diffusion}
Gongfan Fang, Xinyin Ma, and Xinchao Wang.
\newblock Structural pruning for diffusion models.
\newblock \emph{arXiv preprint arXiv:2305.10924}, 2023.

\bibitem[Feng et~al.(2025)Feng, Zheng, Liu, Lin, Zhou, Cai, Wang, Chen, Zou, Ma, and Zhang]{fengHiCache2025}
Liang Feng, Shikang Zheng, Jiacheng Liu, Yuqi Lin, Qinming Zhou, Peiliang Cai, Xinyu Wang, Junjie Chen, Chang Zou, Yue Ma, and Linfeng Zhang.
\newblock Hicache: Training-free acceleration of diffusion models via hermite polynomial-based feature caching, 2025.

\bibitem[Hessel et~al.(2021)Hessel, Holtzman, Forbes, Bras, and Choi]{hessel2021clipscore}
Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan~Le Bras, and Yejin Choi.
\newblock Clipscore: A reference-free evaluation metric for image captioning.
\newblock \emph{arXiv preprint arXiv:2104.08718}, 2021.

\bibitem[Ho et~al.(2020{\natexlab{a}})Ho, Jain, and Abbeel]{DM}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising {Diffusion} {Probabilistic} {Models}, December 2020{\natexlab{a}}.
\newblock URL \url{http://arxiv.org/abs/2006.11239}.
\newblock arXiv:2006.11239 [cs].

\bibitem[Ho et~al.(2020{\natexlab{b}})Ho, Jain, and Abbeel]{ho2020DDPM}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 6840--6851, 2020{\natexlab{b}}.

\bibitem[Kim et~al.(2024)Kim, Gao, Hsu, Shen, and Jin]{kim2024tofu}
Minchul Kim, Shangqian Gao, Yen-Chang Hsu, Yilin Shen, and Hongxia Jin.
\newblock Token fusion: Bridging the gap between token pruning and token merging.
\newblock In \emph{Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}, pp.\  1383--1392, 2024.

\bibitem[Kim et~al.(2025)Kim, Lee, Cho, Park, and Ro]{kim2025ditto}
Sungbin Kim, Hyunwuk Lee, Wonho Cho, Mincheol Park, and Won~Woo Ro.
\newblock Ditto: Accelerating diffusion model via temporal value similarity.
\newblock In \emph{Proceedings of the 2025 IEEE International Symposium on High-Performance Computer Architecture (HPCA)}. IEEE, 2025.

\bibitem[Labs(2024)]{flux2024}
Black~Forest Labs.
\newblock Flux.
\newblock \url{https://github.com/black-forest-labs/flux}, 2024.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Hu, Khan, Li, Yang, Wang, Cheng, and Yang]{li2023FasterDiffusion}
Senmao Li, Taihang Hu, Fahad~Shahbaz Khan, Linxuan Li, Shiqi Yang, Yaxing Wang, Ming-Ming Cheng, and Jian Yang.
\newblock Faster diffusion: Rethinking the role of unet encoder in diffusion models.
\newblock \emph{arXiv preprint arXiv:2312.09608}, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Liu, Lian, Yang, Dong, Kang, Zhang, and Keutzer]{10377259}
Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer.
\newblock Q-diffusion: Quantizing diffusion models.
\newblock In \emph{2023 IEEE/CVF International Conference on Computer Vision (ICCV)}, pp.\  17489--17499, 2023{\natexlab{b}}.
\newblock \doi{10.1109/ICCV51070.2023.01608}.

\bibitem[Li et~al.(2024)Li, Wang, Jin, Hu, Chemerys, Fu, Wang, Tulyakov, and Ren]{li2024snapfusion}
Yanyu Li, Huan Wang, Qing Jin, Ju~Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren.
\newblock Snapfusion: Text-to-image diffusion model on mobile devices within two seconds.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Liu et~al.(2025{\natexlab{a}})Liu, Zou, Lyu, Chen, and Zhang]{liuTaylorSeer2025}
Jiacheng Liu, Chang Zou, Yuanhuiyi Lyu, Junjie Chen, and Linfeng Zhang.
\newblock From reusing to forecasting: Accelerating diffusion models with taylorseers, 2025{\natexlab{a}}.

\bibitem[Liu et~al.(2025{\natexlab{b}})Liu, Zou, Lyu, Li, Wang, and Zhang]{Liu2025SpeCa}
Jiacheng Liu, Chang Zou, Yuanhuiyi Lyu, Kaixin Li, Shaobo Wang, and Linfeng Zhang.
\newblock Speca: Accelerating diffusion transformers with speculative feature caching.
\newblock In \emph{Proceedings of the 33rd ACM International Conference on Multimedia (MM '25)}, pp.\  to appear, Dublin, Ireland, October 2025{\natexlab{b}}. ACM.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Gong, and Liu]{liu2023rectified}
Xingchao Liu, Chengyue Gong, and Qiang Liu.
\newblock Rectified flow: A general and versatile method for generative modeling, 2023{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2210.11493}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Gong, et~al.]{refitiedflow}
Xingchao Liu, Chengyue Gong, et~al.
\newblock Flow straight and fast: Learning to generate and transfer data with rectified flow.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023{\natexlab{b}}.

\bibitem[Liu et~al.(2025{\natexlab{c}})Liu, Yang, Zhang, Zhang, Qiu, You, and Yang]{liu2025regionadaptivesamplingdiffusiontransformers}
Ziming Liu, Yifan Yang, Chengruidong Zhang, Yiqi Zhang, Lili Qiu, Yang You, and Yuqing Yang.
\newblock Region-adaptive sampling for diffusion transformers, 2025{\natexlab{c}}.
\newblock URL \url{https://arxiv.org/abs/2502.10389}.

\bibitem[Lu et~al.(2022{\natexlab{a}})Lu, Zhou, Bao, Chen, Li, and Zhu]{lu2022dpm}
Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.
\newblock Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 5775--5787, 2022{\natexlab{a}}.

\bibitem[Lu et~al.(2022{\natexlab{b}})Lu, Zhou, Bao, Chen, Li, and Zhu]{lu2022dpm++}
Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.
\newblock Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models.
\newblock \emph{arXiv preprint arXiv:2211.01095}, 2022{\natexlab{b}}.

\bibitem[Lv et~al.(2025)Lv, Si, Song, Yang, Qiao, Liu, and Wong]{lvFasterCacheTrainingFreeVideo2025}
Zhengyao Lv, Chenyang Si, Junhao Song, Zhenyu Yang, Yu~Qiao, Ziwei Liu, and Kwan-Yee~K. Wong.
\newblock {{FasterCache}}: {{Training-Free Video Diffusion Model Acceleration}} with {{High Quality}}, 2025.

\bibitem[Ma et~al.(2024)Ma, Fang, and Wang]{ma2024deepcache}
Xinyin Ma, Gongfan Fang, and Xinchao Wang.
\newblock Deepcache: Accelerating diffusion models for free.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  15762--15772, 2024.

\bibitem[Meng et~al.(2022)Meng, Gao, Kingma, Ermon, Ho, and Salimans]{meng2022on}
Chenlin Meng, Ruiqi Gao, Diederik~P Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans.
\newblock On distillation of guided diffusion models.
\newblock In \emph{NeurIPS 2022 Workshop on Score-Based Methods}, 2022.
\newblock URL \url{https://openreview.net/forum?id=6QHpSQt6VR-}.

\bibitem[Peebles \& Xie(2023{\natexlab{a}})Peebles and Xie]{DiT}
William Peebles and Saining Xie.
\newblock Scalable {Diffusion} {Models} with {Transformers}, March 2023{\natexlab{a}}.
\newblock URL \url{http://arxiv.org/abs/2212.09748}.
\newblock arXiv:2212.09748 [cs].

\bibitem[Peebles \& Xie(2023{\natexlab{b}})Peebles and Xie]{peebles2023dit}
William Peebles and Saining Xie.
\newblock Scalable diffusion models with transformers.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pp.\  4195--4205, 2023{\natexlab{b}}.

\bibitem[Qiu et~al.(2025)Qiu, Wang, Lu, Liu, Jiang, and Hao]{qiu2025acceleratingdiffusiontransformererroroptimized}
Junxiang Qiu, Shuo Wang, Jinda Lu, Lin Liu, Houcheng Jiang, and Yanbin Hao.
\newblock Accelerating diffusion transformer via error-optimized cache, 2025.
\newblock URL \url{https://arxiv.org/abs/2501.19243}.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and Ommer]{StableDiffusion}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer.
\newblock High-{Resolution} {Image} {Synthesis} with {Latent} {Diffusion} {Models}, April 2022.
\newblock URL \url{http://arxiv.org/abs/2112.10752}.
\newblock arXiv:2112.10752 [cs].

\bibitem[Ronneberger et~al.(2015)Ronneberger, Fischer, and Brox]{ronneberger2015unet}
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In \emph{Medical image computing and computer-assisted intervention--MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18}, pp.\  234--241. Springer, 2015.

\bibitem[Saharia et~al.(2022)Saharia, Chan, Saxena, Li, Whang, Denton, Ghasemipour, Ayan, Mahdavi, Lopes, Salimans, Ho, Fleet, and Norouzi]{saharia2022drawbench}
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar~Seyed Ghasemipour, Burcu~Karagol Ayan, S.~Sara Mahdavi, Rapha~Gontijo Lopes, Tim Salimans, Jonathan Ho, David~J. Fleet, and Mohammad Norouzi.
\newblock Photorealistic text-to-image diffusion models with deep language understanding.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  24219--24238, 2022.

\bibitem[Salimans \& Ho(2022)Salimans and Ho]{salimans2022progressive}
Tim Salimans and Jonathan Ho.
\newblock Progressive distillation for fast sampling of diffusion models.
\newblock \emph{arXiv preprint arXiv:2202.00512}, 2022.

\bibitem[Selvaraju et~al.(2024)Selvaraju, Ding, Chen, Zharkov, and Liang]{selvaraju2024fora}
Pratheba Selvaraju, Tianyu Ding, Tianyi Chen, Ilya Zharkov, and Luming Liang.
\newblock Fora: Fast-forward caching in diffusion transformer acceleration.
\newblock \emph{arXiv preprint arXiv:2407.01425}, 2024.

\bibitem[Shang et~al.(2023)Shang, Yuan, Xie, Wu, and Yan]{shang2023post}
Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan.
\newblock Post-training quantization on diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  1972--1981, 2023.

\bibitem[Sohl-Dickstein et~al.(2015)Sohl-Dickstein, Weiss, Maheswaranathan, and Ganguli]{sohl2015deep}
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In \emph{International conference on machine learning}, pp.\  2256--2265. PMLR, 2015.

\bibitem[Song et~al.(2021)Song, Meng, and Ermon]{songDDIM}
Jiaming Song, Chenlin Meng, and Stefano Ermon.
\newblock Denoising diffusion implicit models.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Song et~al.(2023)Song, Dhariwal, Chen, and Sutskever]{song2023consistency}
Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever.
\newblock Consistency models.
\newblock In \emph{International Conference on Machine Learning}, pp.\  32211--32252. PMLR, 2023.

\bibitem[Veit et~al.(2016)Veit, Wilber, and Belongie]{veit2016residual}
Andreas Veit, Michael~J Wilber, and Serge Belongie.
\newblock Residual networks behave like ensembles of relatively shallow networks.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Wang et~al.(2024)Wang, Chen, Zhang, Ma, and Yan]{gedit2024}
Wen Wang, Qifeng Chen, Lvmin Zhang, Yue Ma, and Zexuan Yan.
\newblock Gedit: A unified metric for evaluating instruction-based image editing.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  12345--12354, 2024.

\bibitem[Wang et~al.(2004)Wang, Bovik, Sheikh, and Simoncelli]{wang2004imagequality}
Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli.
\newblock Image quality assessment: From error visibility to structural similarity.
\newblock \emph{IEEE Transactions on Image Processing}, 13\penalty0 (4):\penalty0 600--612, 2004.
\newblock \doi{10.1109/TIP.2003.819861}.

\bibitem[Xu et~al.(2023)Xu, Liu, Wu, Tong, Li, Ding, Tang, and Dong]{xu2023imagereward}
Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong.
\newblock Imagereward: Learning and evaluating human preferences for text-to-image generation, 2023.
\newblock URL \url{https://arxiv.org/abs/2304.05977}.

\bibitem[Yang et~al.(2025)Yang, Teng, Zheng, Ding, Huang, Xu, Yang, Hong, Zhang, Feng, Yin, Gu, Yuxuan.Zhang, Wang, Cheng, Xu, Dong, and Tang]{yang2025cogvideox}
Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da~Yin, Xiaotao Gu, Yuxuan.Zhang, Weihan Wang, Yean Cheng, Bin Xu, Yuxiao Dong, and Jie Tang.
\newblock Cogvideox: Text-to-video diffusion models with an expert transformer.
\newblock In \emph{The Thirteenth International Conference on Learning Representations}, 2025.
\newblock URL \url{https://openreview.net/forum?id=LQzN6TRFg9}.

\bibitem[Zhang et~al.(2024)Zhang, Xiao, Tang, Ma, Zou, Ning, Hu, and Zhang]{zhang2024tokenpruningcachingbetter}
Evelyn Zhang, Bang Xiao, Jiayi Tang, Qianli Ma, Chang Zou, Xuefei Ning, Xuming Hu, and Linfeng Zhang.
\newblock Token pruning for caching better: 9 times acceleration on stable diffusion for free, 2024.
\newblock URL \url{https://arxiv.org/abs/2501.00375}.

\bibitem[Zhang et~al.(2025)Zhang, Tang, Ning, and Zhang]{zhang2025sito}
Evelyn Zhang, Jiayi Tang, Xuefei Ning, and Linfeng Zhang.
\newblock Training-free and hardware-friendly acceleration for diffusion models via similarity-based token pruning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, 2025.

\bibitem[Zhang \& Agrawala(2025)Zhang and Agrawala]{kontext2025}
Lvmin Zhang and Maneesh Agrawala.
\newblock Flux.1 kontext: Flow matching for in-context image generation and editing, 2025.
\newblock URL \url{https://arxiv.org/abs/2506.15742}.

\bibitem[Zhang et~al.(2018)Zhang, Isola, Efros, Shechtman, and Wang]{zhangUnreasonableEffectivenessDeep2018}
Richard Zhang, Phillip Isola, Alexei~A. Efros, Eli Shechtman, and Oliver Wang.
\newblock The {{Unreasonable Effectiveness}} of {{Deep Features}} as a {{Perceptual Metric}}, 2018.

\bibitem[Zhao et~al.(2024)Zhao, Jin, Wang, and You]{zhao2024PAB}
Xuanlei Zhao, Xiaolong Jin, Kai Wang, and Yang You.
\newblock Real-time video generation with pyramid attention broadcast.
\newblock \emph{arXiv preprint arXiv:2408.12588}, 2024.

\bibitem[Zheng et~al.(2023)Zheng, Lu, Chen, and Zhu]{zheng2023dpmsolvervF}
Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu.
\newblock {DPM}-solver-v3: Improved diffusion {ODE} solver with empirical model statistics.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=9fWKExmKa0}.

\bibitem[Zheng et~al.(2025)Zheng, Feng, Wang, Zhou, Cai, Zou, Liu, Lin, Chen, Ma, and Zhang]{zhengFoCa2025}
Shikang Zheng, Liang Feng, Xinyu Wang, Qinming Zhou, Peiliang Cai, Chang Zou, Jiacheng Liu, Yuqi Lin, Junjie Chen, Yue Ma, and Linfeng Zhang.
\newblock Forecast then calibrate: Feature caching as ode for efficient diffusion transformers, 2025.

\bibitem[Zheng et~al.(2024)Zheng, Peng, Yang, Shen, Li, Liu, Zhou, Li, and You]{opensora}
Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You.
\newblock Open-sora: Democratizing efficient video production for all, March 2024.
\newblock URL \url{https://github.com/hpcaitech/Open-Sora}.

\bibitem[Zhu et~al.(2024)Zhu, Tang, Liu, Lu, Zheng, Peng, Li, Wang, Jiang, Tian, Tiwari, Sirasao, Yong, Wang, and Barsoum]{zhu2024dipgo}
Haowei Zhu, Dehua Tang, Ji~Liu, Mingjie Lu, Jintu Zheng, Jinzhang Peng, Dong Li, Yu~Wang, Fan Jiang, Lu~Tian, Spandan Tiwari, Ashish Sirasao, Jun-Hai Yong, Bin Wang, and Emad Barsoum.
\newblock Dip-go: A diffusion pruner via few-step gradient optimization, 2024.

\bibitem[Zou et~al.(2024)Zou, Zhang, Guo, Xu, He, Hu, and Zhang]{zou2024DuCa}
Chang Zou, Evelyn Zhang, Runlin Guo, Haohang Xu, Conghui He, Xuming Hu, and Linfeng Zhang.
\newblock Accelerating diffusion transformers with dual feature caching, 2024.
\newblock URL \url{https://arxiv.org/abs/2412.18911}.

\bibitem[Zou et~al.(2025)Zou, Liu, Liu, Huang, and Zhang]{zou2024accelerating}
Chang Zou, Xuyang Liu, Ting Liu, Siteng Huang, and Linfeng Zhang.
\newblock Accelerating diffusion transformers with token-wise feature caching.
\newblock In \emph{Proceedings of the 13th International Conference on Learning Representations (ICLR 2025)}. ICLR, 2025.
\newblock URL \url{https://openreview.net/forum?id=yYZbZGo4ei}.
\newblock accepted to ICLR 2025.

\end{thebibliography}
