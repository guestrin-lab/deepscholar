% =========================
%   THEORY APPENDIX
% =========================
\section{Theory Appendix}
\label{app:theory}

\subsection{Scope and what is new}
Results below formalize properties used throughout the paper. 
\emph{What is classical:} orthogonal similarity preserves the Frobenius norm and Rademacher/Gaussian probe distributions \citep[][Sec.~2]{hornjohnson2013matrixanalysis}; Hutchinson-type trace/energy estimators are unbiased with well-studied concentration \citep{hutchinson1989stochastic,avron2011randomized,hsu2012tail,rudelson2013hansonwright}; the equivalence between a zero loop commutator and identity holonomy for \emph{invertible} transports is standard in gauge theory \citep{ambrose1953holonomy,wilson1974confinement}. 
\emph{What is new here:} the \textbf{inverse-free, JVP-only curvature} $\kappa_{\mathrm{inv}}$ tied to \emph{Transformer residual streams} and the \emph{diagnostic role} connecting curvature/commutators to invariance breaks and order sensitivity.

\subsection{Notation and setup}
Let \(B=\{1,\dots,T\}\times\{0,\dots,L\}\) be the product base (Position\(\times\)Layer).
At node \((i,\ell)\) the residual stream is a vector in \(\mathbb{R}^d\).
Vertical transports \(T^{\mathrm{layer}}_{i,\ell}:\mathbb{R}^d\!\to\!\mathbb{R}^d\) map \((i,\ell)\!\to\!(i,\ell{+}1)\);
horizontal transports \(T^{\mathrm{attn}}_{i\leftarrow j,\ell}:\mathbb{R}^d\!\to\!\mathbb{R}^d\) map \((j,\ell)\!\to\!(i,\ell)\).
Define
\[
A_{i,j,\ell}:=T^{\mathrm{layer}}_{i,\ell}T^{\mathrm{attn}}_{i\leftarrow j,\ell}
- T^{\mathrm{attn}}_{i\leftarrow j,\ell+1}T^{\mathrm{layer}}_{j,\ell}\,:\ \mathbb{R}^d\to\mathbb{R}^d.
\]
The inverse-free curvature (Def.~\eqref{eq:invfree-kappa}) is
\[
\kappa_{\mathrm{inv}}(i,j,\ell)^2 \;=\; \mathbb{E}_{v}\,\big\|A_{i,j,\ell}\,v\big\|_2^2
\;=\; \mathrm{tr}\!\big(A_{i,j,\ell}^\top A_{i,j,\ell}\big)
\;=\;\|A_{i,j,\ell}\|_F^2,
\]
with expectation over Rademacher or standard Gaussian probes \(v\in\mathbb{R}^d\)
(\(\mathbb{E}[vv^\top]=I\)).

\subsection{Hutchinson/JVP estimator: unbiasedness and concentration}
\label{app:hutchinson}
Let \(A:=A_{i,j,\ell}\), \(\theta:=\|A\|_F^2\), and \(v_s\overset{\text{i.i.d.}}{\sim}\{\pm1\}^d\).
Define
\[
\widehat{\theta}_r \;=\; \frac{1}{r}\sum_{s=1}^r \|A v_s\|_2^2
= \frac{1}{r}\sum_{s=1}^r v_s^\top (A^\top A)\,v_s.
\]

\begin{lemma}[Unbiasedness \citep{hutchinson1989stochastic,avron2011randomized}]
\label{lem:unbiased}
\(\mathbb{E}\,\widehat{\theta}_r = \theta\).
\end{lemma}

\begin{proof}
\(\mathbb{E}\|Av\|_2^2=\mathbb{E}\mathrm{tr}(v^\top A^\top A v)=\mathrm{tr}(A^\top A)=\theta\), using \(\mathbb{E}[vv^\top]=I\).
\end{proof}

\begin{theorem}[Concentration via Hanson--Wright \citep{hsu2012tail,rudelson2013hansonwright}]
\label{thm:concentration}
Let \(\mathrm{sr}(A):=\|A\|_F^2/\|A\|_2^2\) be the stable rank. For any \(\varepsilon\in(0,1)\),
\[
\Pr\!\left(\left|\widehat{\theta}_r-\theta\right|\ge \varepsilon\,\theta\right)
\;\le\; 2\exp\!\left(-c_1\,r\,\min\big\{\varepsilon^2,\;\varepsilon\,\mathrm{sr}(A)\big\}\right),
\]
for universal constant \(c_1>0\). Thus it suffices that
\(r \gtrsim \min\{\varepsilon^{-2},(\varepsilon\,\mathrm{sr}(A))^{-1}\}\log(2/\delta)\)
to get error \(\le \varepsilon\theta\) with prob.\ \(\ge 1-\delta\).
\end{theorem}

\begin{proof}[Proof sketch]
Each \(Z_s=\|Av_s\|_2^2=v_s^\top (A^\top A)v_s\) is a sub-Gaussian quadratic form; Hanson--Wright controls tails. Averaging \(r\) i.i.d.\ copies gives the stated rate; use \(\|A^\top A\|_F^2=\|A\|_F^4\), \(\|A^\top A\|_2=\|A\|_2^2\) to expose \(\mathrm{sr}(A)\).
\end{proof}

\begin{corollary}[RMS rate]
\label{cor:mse}
\(\mathrm{MSE}(\widehat{\theta}_r)=\mathcal{O}(\|A\|_F^4/r)\); by the delta method, \(\mathrm{SE}(\kappa_{\mathrm{inv}})\approx \tfrac{1}{2}\theta^{-1/2}\sqrt{\mathrm{Var}[\widehat{\theta}_r]}\).
\end{corollary}

\subsection{Work and memory complexity under \texorpdfstring{\((r,k,m)\)}{(r,k,m)} sampling}
\label{app:complexity}
Per layer, sample \(k\) targets \(i\) and top-\(m\) neighbors \(j\) (by attention mass). 
\textbf{Scan (frozen-softmax):}
\(\text{work}=\mathcal{O}(Lkm\cdot H d_h^2)\).
\textbf{Confirm (JVP):} two JVP paths per probe \(\Rightarrow\) \(4rLkm\) JVPs total:
\(\text{work}=\mathcal{O}(rLkm\cdot \mathrm{cost}_{\mathrm{JVP}})\).
Only hotspots (few\%) need confirmation in practice.

\subsection{Connections and limiting cases}
\label{app:connections}
\paragraph{Linear/tied case.}
If sublayers are linear and \(T^{\mathrm{attn}}_{\ell+1}\!=\!T^{\mathrm{attn}}_{\ell}\), then
\(A=[T^{\mathrm{layer}},T^{\mathrm{attn}}]\) and \(\kappa_{\mathrm{inv}}^2=\|[T^{\mathrm{layer}},T^{\mathrm{attn}}]\|_F^2\).

\subsection{Assumptions and caveats}
\label{app:caveats}
(1) Transports are Jacobian blocks (or frozen-softmax approximations) on the residual stream; non-invertibilities motivate inverse-free loops. (2) Concentration uses sub-Gaussian probes (Rademacher/Gaussian). (3) The stable-rank dependence captures conditioning; ill-conditioned loops require larger \(r\).