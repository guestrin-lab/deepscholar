\section{Related Work}
\label{sec:related}

\paragraph{Missing invariance tests, fragile behavior, and reorder risk.}
Behavioral test suites show that modern NLP systems often fail simple invariances and controlled perturbations, motivating \emph{diagnostic} evaluation beyond accuracy (e.g., CheckList) \citep{ribeiro2020checklist}. Robustness Gym systematizes stress tests and highlights brittleness under template-preserving edits \citep{goel2021robustgym}. For code LLMs, semantics-preserving program transformations (notably identifier or variable renaming) can spuriously change predictions, motivating invariance-oriented evaluation and augmentation \citep{ankner2021varrename, wang2022recoderobustnessevaluationcode}. At the systems layer, ML compilers (TVM, XLA) aggressively fuse and reorder operators to improve throughput, but floating point non-associativity means reordering can alter numerics \citep{chen2018tvm, higham2002accuracy}. Without guards, such optimizations risk correctness regressions. For long-context LLMs, input order effects are well documented: models can underuse middle context and flip answers when premise or evidence order changes \citep{liu2023lostmiddle, chen2024premiseorder}. In retrieval-augmented pipelines, document order can interact with position bias; recent studies report mixed magnitudes \citep{cuconasu2025rags, zhang2024compensateposbias}.

\paragraph{Attempts to address the problems.}
Architectural symmetry has a long history: group-equivariant models \citep{cohen2016groupcnn, cohen2017steerable, bronstein2021geometric} \emph{bake in} invariances via layer design. Our approach is \emph{post hoc} and model-agnostic: we diagnose order sensitivity and invariance breaks at inference time. For long-context stability, positional schemes such as ALiBi, positional interpolation for RoPE, LongRoPE, and YaRN reduce phase or length drift but modify training or attention parameterization rather than \emph{diagnose} order sensitivity online \citep{press2021alibi, chen2023positioninterp, ding2024longrope, peng2024yarn}. In code robustness, RECode and related work use semantics-preserving transforms and augmentation to improve invariance but again focus on training-time fixes \citep{wang2022recoderobustnessevaluationcode}.

\paragraph{Evaluation frameworks.}
Beyond accuracy, comprehensive evaluations emphasize robustness, calibration, and efficiency, aligning with our diagnostic goals. HELM codifies multi-metric, scenario-based evaluation for LLMs \citep{liang2022helm}. Dynabench advances dynamic, human-in-the-loop stress testing \citep{kiela2021dynabench}. These motivate systematic invariance and order-sensitivity checks alongside standard metrics.

\paragraph{Alternatives and complements: representation alignment and probing.}
Representation-similarity measures (SVCCA, CKA) and orthogonal Procrustes alignment compare layers or seeds while acknowledging gauge freedom, motivating our $O(d)$ analysis gauge and gauge-invariant summaries \citep{raghu2017svcca, kornblith2019cka}. Probing critiques emphasize instability and confounds, arguing for controls and reproducibility, gaps our gauge-fixed logging aims to reduce \citep{hewitt2019designing}. Mechanistic interpretability develops circuit-level tools such as activation patching and progress measures; our commutator and holonomy signals provide a complementary, automatable geometry that flags \emph{order sensitivity} without manual circuit discovery \citep{elhage2021mathematical, nanda2023progress, zhang2024towards}.

\paragraph{Geometry and curvature in learning.}
Discrete curvature notions such as Ollivier and Forman curvature have characterized graphs and GNN dynamics, where curvature reflects structure and flow \citep{samal2018forman, ni2015ricci}. We differ by defining \emph{inverse-free, data-dependent holonomy on Transformer representations} using JVPs, turning curvature into a practical, per-layer and per-position diagnostic tied to invariance and operator order. Unlike graph-level discrete Ricci approaches \citep{samal2018forman, ollivier2007markov}, we compute inverse-free, data-dependent holonomy on internal representations via JVPs to yield granular diagnostics.

\paragraph{ML systems: fusion, scheduling, and safety.}
Compiler stacks (TVM, XLA) and inference schedulers search fusions or reorderings for latency and throughput \citep{chen2018tvm}. Floating point reordering risk and nondeterminism motivate \emph{gates} that decide when such transformations are safe, which is where our curvature and commutator signals act as low-overhead guards. Deterministic summation methods show how fixed accumulation orders can reduce variance across runs and hardware \citep{ahrens2020reprod}. Our signals also align with orchestration frameworks (e.g., the UCCT line of work on semantic anchoring and tool coordination \cite{chang2025UCCT}), where curvature can prioritize \emph{parallel} low-risk regions and enforce invariants during \emph{sequential} high-risk steps.

\paragraph{Physics foundations: groups, gauges, bundles, holonomy.}
Our geometric lens borrows classical ideas from mathematical and theoretical physics. Group symmetries and Noetherâ€™s theorem connect invariants to conserved quantities \citep{weyl1952symmetry, noether1918invariante}. Gauge theory formalizes local symmetry with connections and parallel transport on fiber bundles \citep{yang1954gauge, kobayashi1963foundations, nakahara2003gtp, frankel2011geometry}. Holonomy and Wilson loops quantify curvature via loop transports \citep{ambrose1953holonomy, wilson1974confinement}; geometric phases offer an operational view in quantum mechanics \citep{simon1983holonomy, berry1984phase}. Unlike gauge or group equivariant architectures that \emph{instantiate} symmetry in layers, we apply these notions \emph{post hoc} to produce model-agnostic, gauge-stable diagnostics for Transformers.
