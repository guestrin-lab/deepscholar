% =========================
%   CODE APPENDIX
% =========================
%\appendix
\section{Code Appendix}
\label{app:code}

% Use the user's exact listing attributes for all code
\lstdefinestyle{apxpy}{
  language=Python,
  basicstyle=\ttfamily\footnotesize,
  breaklines=true,
  breakatwhitespace=true,
  columns=fullflexible,
  keepspaces=true,
  postbreak=\mbox{\textcolor{gray}{$\hookrightarrow$}\space}
}
\lstset{style=apxpy}
\UseRawInputEncoding
\begin{lstlisting}[caption={Inverse-free curvature with JVPs (matrix-free). Requires PyTorch 2.0+ (`torch.func.jvp`).}]
import torch
from torch.func import jvp

# Each fn_* maps residual stream -> residual stream (same shape).
# attn_l:   f(h) = within-layer attention at layer ℓ (output at position i)
# attn_lp1: f(h) = within-layer attention at layer ℓ+1 (output at position i)
# layer_i:  f(h) = vertical transport (i,ℓ) -> (i,ℓ+1)
# layer_j:  f(h) = vertical transport (j,ℓ) -> (j,ℓ+1)
# h_l:      residual stream at layer ℓ (batch or single vector at position i/j)
def kappa_inv_once(attn_l, attn_lp1, layer_i, layer_j, h_l, v):
    # Path A: attn@ℓ then layer@i
    _, j_attn_v = jvp(attn_l, (h_l,), (v,))
    _, pathA    = jvp(layer_i, (h_l,), (j_attn_v,))
    # Path B: layer@j then attn@ℓ+1
    _, j_layer_v = jvp(layer_j, (h_l,), (v,))
    _, pathB     = jvp(attn_lp1, (h_l,), (j_layer_v,))
    return torch.norm(pathA - pathB)

def rademacher_like(x):
    # Returns ±1 with same dtype/device; supports float/bfloat16.
    return (torch.randint_like(x, low=0, high=2).mul_(2).sub_(1)) .to(dtype=x.dtype)

def kappa_inv(attn_l, attn_lp1, layer_i, layer_j, h_l, r=6):
    # Hutchinson average of squared norms, then sqrt.
    d = h_l.shape[-1]
    acc = h_l.new_zeros(())
    for _ in range(r):
        v = rademacher_like(h_l[..., :d])  # shape-compatible probe
        acc = acc + kappa_inv_once(attn_l, attn_lp1, layer_i, layer_j, h_l, v)**2
    return (acc / r).sqrt()
\end{lstlisting}

\begin{lstlisting}[caption={Frozen-softmax horizontal transport (per-head) as a fast scan surrogate.}]
# Builds a block-diagonal approximation ∑_h α_ij^(h) W_V^(h) W_O^(h)
# Use for scanning; confirm hotspots with JVP-based kappa_inv().
def frozen_attn_transport(alpha_ij_per_head, W_V_list, W_O_list):
    # alpha_ij_per_head: list[H] of scalars (on a calibration batch)
    # W_V_list, W_O_list: list[H] of (d_h x d_h) tensors
    H = len(W_V_list); d_h = W_V_list[0].shape[0]
    T = W_V_list[0].new_zeros((H*d_h, H*d_h))
    off = 0
    for h in range(H):
        T[off:off+d_h, off:off+d_h] = alpha_ij_per_head[h] * (W_V_list[h] @ W_O_list[h])
        off += d_h
    return T
\end{lstlisting}

\begin{lstlisting}[caption={Commutator map for two submodules A,B on a calibration batch.}]
# Computes Δ_{A,B} = || A(B(X)) - B(A(X)) ||_F over a batch X.
@torch.no_grad()
def commutator_norm(A, B, X):
    AB = A(B(X))
    BA = B(A(X))
    return torch.linalg.vector_norm(AB - BA)
\end{lstlisting}

\begin{lstlisting}[caption={Gauge-fix: whitening + orthogonal Procrustes alignment.}]
# Whitening: H Σ^{-1/2}, where Σ is covariance over tokens/batch dims.
def whiten(H, eps=1e-5):
    # H: (N, d) matrix of features (stack tokens/batch as N)
    mu = H.mean(dim=0, keepdim=True)
    X  = H - mu
    # Covariance and eigen
    C = (X.T @ X) / max(1, H.shape[0] - 1)
    evals, evecs = torch.linalg.eigh(C)
    Dm12 = torch.diag(torch.clamp(evals, min=eps).rsqrt())
    W = evecs @ Dm12 @ evecs.T
    return (X @ W), (mu, W)

# Orthogonal Procrustes: align H2 to H1 (minimize ||H1 - H2 R||_F over R in O(d)).
def procrustes_R(H1, H2):
    # assumes H1,H2 are whitened; returns R (d x d)
    U, _, Vh = torch.linalg.svd(H2.T @ H1, full_matrices=False)
    return U @ Vh

def gauge_fix(H1, H2):
    H1w, _ = whiten(H1)
    H2w, _ = whiten(H2)
    R = procrustes_R(H1w, H2w)
    return H1w, (H2w @ R), R
\end{lstlisting}

\begin{lstlisting}[caption={CSV schema helpers and writers for holonomy/commutator outputs.}]
import csv

def write_holonomy_csv(path, rows):
    # rows: iterable of dicts with keys ["position","layer","kappa"]
    with open(path, "w", newline="") as f:
        w = csv.DictWriter(f, fieldnames=["position","layer","kappa"])
        w.writeheader()
        for r in rows: w.writerow(r)

def write_commutator_csv(path, rows):
    # rows: iterable of dicts with keys ["i","j","value"]
    with open(path, "w", newline="") as f:
        w = csv.DictWriter(f, fieldnames=["i","j","value"])
        w.writeheader()
        for r in rows: w.writerow(r)
\end{lstlisting}

\begin{lstlisting}[caption={Policy: SafeFuse gate using commutator and drift surrogate.}]
def plan_step(graph, kappa_map, invariants, tau_kappa=0.12):
    # Partition frontier by curvature threshold
    par = [u for u in graph.frontier() if kappa_map[u] <= tau_kappa]
    seq = [u for u in graph.frontier() if kappa_map[u] >  tau_kappa]
    # Low-risk: parallel with invariant guards
    exec_parallel([guarded(task, invariants) for task in par])
    # High-risk: sequential + extra verification
    for task in seq:
        run_with_checks(task, invariants, extra_verifiers=True)
\end{lstlisting}

\begin{lstlisting}[caption={Colab harness sketch for E1--E7 (seed, knobs, figure/CSV export).}]
def run_suite(model, tokenizer, cfg):
    torch.manual_seed(cfg.seed)
    # E1: alpha-renaming invariance
    ir = run_alpha_renaming(model, tokenizer, cfg.alpha_suite)
    # E2/E7: gauge-fix stability
    stab = run_gauge_stability(model, cfg.gauge_suite)
    # E3: commutator heatmap
    comm = run_commutator_maps(model, cfg.comm_suite)
    write_commutator_csv(cfg.out_dir /"commutator.csv", comm.rows)
    # E4: RoPE drift
    drift = run_rope_drift(model, cfg.rope_suite)
    # E5/E6: curvature maps + prediction
    holo = run_holonomy_maps(model, cfg.holo_suite)   # writes holonomy.csv
    roc  = run_curvature_predicts_fail(model, cfg.pred_suite)
    # Export figures/tables
    save_figs(cfg.out_dir); save_tables(cfg.out_dir)
    return {"IR": ir, "stability": stab, "drift": drift, "roc": roc}
\end{lstlisting}
