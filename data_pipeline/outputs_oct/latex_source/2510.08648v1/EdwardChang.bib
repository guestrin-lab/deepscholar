@inproceedings{SocraHealth2023,
author = {Jocelyn J. Chang and et al.},
title = {{SocraHealth: Enhancing Medical Diagnosis and Correcting Historical Records}},
month = {December},
year = {2023},
booktitle={The $10^{th}$ International Conf. on Computational Science and Computational Intelligence}, 
}


@inproceedings{DikeErisChang2025,
  author={Edward Y Chang},
  booktitle={{ICML}}, 
  title={{A Checks-and-Balances Framework for Context-Aware Ethical AI Alignment}}, 
  month={July},
  year={2025},
}

@misc{OVAL2025noora,
  author    = {Lynn Koegel and Monica Lam},
  title     = {AI Social Coach Noora for Autism},
  year      = {2025},
  month     = {August},
  url       = {https://news.stanford.edu/stories/2025/08/ai-social-coach-noora-autism},
  note      = {Stanford University News},
  urldate   = {2025-09-11},
}



@article{CRIT-Extended,
  title={{CRIT}: {An Inquisitive Prompt Template for Critical Reading} (Extended Version) },
  author={Edward Y.Chang},
  journal={Stanford InfoLab Technical Report},
  month={February},
  year={2023},
}

@ARTICLE{SocraPediaDecember2023,
  author={Edward Y Chang},
  journal={Stanford University InfoLab Technical Report}, 
  title={{SocraPedia: A Wikipedia Generated by SocraSynth with Collaborative Large Language Models}}, 
  year={2023},
  month={November},
  url ={\url{www.socrapedia.com}},
}

@inproceedings{chang2024RFDL,
  author={Edward Y Chang},
  booktitle={arXiv:2408.13464}, 
  title={{Uncovering Biases with Reflective Large Language Models}}, 
  month={August},
  year={2024},
}


@misc{chang2024RFDLOld,
      title={{Uncovering Biases with Reflective Large Language Models}}, 
      author={Edward Y. Chang},
      year={2024},
      eprint={2408.13464},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2408.13464}, 
}

@article{SocraticIEEECCWC2023,
  title={{CRIT: Prompting Large Language Models With the Socratic Method}},
  author={Edward Y. Chang},
  journal={IEEE $13^{th}$ Annual Computing and Communication Workshop and Conference},
  month={March},
  year={2023},
  url = {\url{https://arxiv.org/abs/2303.08769}},
}

@inproceedings{Chang2023CRIT,
  title       = {CRIT: Prompting Large Language Models with the Socratic Method},
  author      = {Chang, Edward Y.},
  booktitle   = {IEEE Computing and Communication Workshop and Conference (CCWC)},
  year        = {2023},
  note        = {arXiv:2303.08769},
  url         = {https://arxiv.org/abs/2303.08769}
}


@misc{AnonymousCRIT2023,
  title        = {CRIT: Prompting Large Language Models with the Socratic Method},
  author       = {Anonymous},
  year         = {2023},
  note         = {Withheld for anonymous review; protocol identical to camera-ready version}
}


@inproceedings{Ano23a,
  author = {Anonymous},
  title  = {{CRIT: Prompting Large Language Models with the Socratic Method}},
  booktitle = {Withheld for anonymous review},
  year   = {2023},
  note   = {Protocol identical to camera-ready version}
}

@inproceedings{Ano23b,
  author = {Anonymous},
  title  = {{Examining GPT-4's Capabilities and Enhancement with SocraSynth}},
  booktitle = {Withheld for anonymous review},
  year   = {2023},
  note   = {Details withheld for anonymous review}
}

@book{Ano24,
  author    = {Anonymous},
  title     = {{Multi-LLM Agent Collaborative Intelligence: The Path to Artificial General Intelligence}},
  publisher = {Withheld for anonymous review},
  year      = {2024},
  note      = {Full details to be provided post-review}
}


@book{AnonymousPathAGI2024,
  author    = {Anonymous},
  title     = {{Withheld for anonymous review}},
  publisher = {Withheld for anonymous review},
  year      = {2024},
  note      = {Full details to be provided post-review}
}

@inproceedings{SocraSynthChangCSCI2023,
  author={Edward Y Chang},
  booktitle={{The $10^{th}$ International Conf. on Comp. Science and Comp. Intelligence}}, 
  title={{Examining GPT-4's Capabilities and Enhancement with SocraSynth}}, 
  month={December},
  year={2023},
}

@inproceedings{AnonymousSocraSynth2023,
  title      = {Examining {GPT-4}'s Capabilities and Enhancement with {SocraSynth}},
  author     = {Anonymous},
  booktitle  = {Proceedings of the 10th International Conference on Computational Science and Computational Intelligence (CSCI)},
  year       = {2023},
  month      = dec,
  note       = {Withheld for anonymous review}
}



@inproceedings{EVINCEChang2024,
  author={Edward Y Chang},
  booktitle={{arXiv:2408.14575}}, 
  title={{EVINCE: Optimizing Adversarial LLM Dialogues via Conditional Statistics and Information Theory}}, 
  month={August},
  year={2024},
}

@inproceedings{anon2024itdc,
  title     = {An Information-Theoretic Controller for Multi-Agent Debates},
  author    = {Anonymous},
  booktitle = {arXiv (number concealed)},
    month={August},
  year      = {2024}
}

@book{PathAGIChang2024,
    author = {Edward Y. Chang},
    title = {{Multi-LLM Agent Collaborative Intelligence: The Path to Artificial General Intelligence}},
    publisher = {SocraSynth, March 2024; ACM Books},
    year = {2025},
}

@inproceedings{chang2025sagallm,
      title={{SagaLLM: Context Management, Validation, and Transaction Guarantees for Multi-Agent LLM Planning}}, 
      author={Edward Y. Chang and Longling Geng},
      booktitle = {Proceedings of VLDB},
      year={2025},
}

@misc{chang2024integrating,
      title={{Integrating Emotional and Linguistic Models for Ethical Compliance in Large Language Models}}, 
      author={Edward Y. Chang},
      year={2024},
      eprint={2405.07076},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={\url{https://arxiv.org/abs/2405.07076}}, 
}

@inproceedings{chang2024NeurIPS,
      title={{A Three-Branch Checks-and-Balances Framework for Context-Aware Ethical Alignment of Large Language Models}}, 
      author={Edward Y. Chang},
      year={2024},
      booktitle={Advances in Neural Information Processing Systems, SafeGenAI Workshop},
}


@misc{chang2020consciousness,
  title={{Consciousness Modeling Lecture Series}},
  author={Chang, Edward Y},
  year={2020-22},
  url ={\url{http://infolab.stanford.edu/~echang/cs372/cs372-syllabus.html}}
}

@misc{ChangTricorder2022,
  doi = {10.48550/ARXIV.2212.13591},
  url ={\url{https://arxiv.org/abs/2212.13591}},
  author = {Chang, Edward Y.},
  keywords = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.7; K.3.2},
  title = {Knowledge-Guided Data-Centric AI in Healthcare: Progress, Shortcomings, and Future Directions},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@book{chang2011foundations,
  title={{Foundations of Large-Scale Multimedia Information Management and Retrieval: Mathematics of Perception}},
  author={Chang, Edward Y.},
  year={2011},
  publisher={Springer}
}

@misc{CS372-Lecture18,
author = {Edward Y. Chang},
title = {Stanford CS372 Lecture-18, Intelligence Series Part 3: Consciousness, Mind, Will, and Ethics},
year = {2020-22},
publisher = {Stanford University},
url = {https://www.youtube.com/watch?v=wkLVgRj9Dd0}
}

@inproceedings{AIA-GAI-Consciousness,
  title = {Model Generative Artificial Intelligence with Consciousness (closing keynote)},
  booktitle = {Taiwan Artificial Intelligence Academy (AIA) Annual Conference},
  year = {2022},
  month = {11},
  author = {Edward Y. Chang},
  howpublished = {\url{https://drive.google.com/file/d/1-2A687VrPVymrHprI7tXXD3-g9O8XO_V/view?usp=share_link}},
}


@article{KGGAN2019,
  author    = {Che{-}Han Chang and
               Chun{-}Hsien Yu and
               Szu{-}Ying Chen and
               Edward Y. Chang},
  title     = {{KG-GAN:} Knowledge-Guided Generative Adversarial Networks},
  journal   = {arXiv},
  volume    = {abs/1905.12261},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.12261},
  eprinttype = {arXiv},
  eprint    = {1905.12261},
  timestamp = {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-12261.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{chang2023cocomo,
      title={{CoCoMo}: {Computational Consciousness Modeling for Generative and Ethical AI}}, 
      author={Edward Y. Chang},
      year={2023},
      booktitle={arXiv: 2304.02438},
      url={\url{https://arxiv.org/abs/2304.02438}},
}



@inproceedings{NIPS2007_ddb30680,
 author = {Zhu, Kaihua and Wang, Hao and Bai, Hongjie and Li, Jian and Qiu, Zhihuan and Cui, Hang and Chang, Edward},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Parallelizing Support Vector Machines on Distributed Computers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ddb30680a691d157187ee1cf9e896d03-Paper.pdf},
 volume = {20},
 year = {2007}
}

@inproceedings{MirrorChang2024,
   author={Edward Y. Chang},
   title={It Takes a Mirror to Find Flaws: Uncovering Biases with Reflective Large Language Models},
   booktitle={Stanford University InfoLab Technical Report},
   year={2024} }



@inproceedings{IEEEInfra-AGI-Consciousness,
  title = {Towards Artificial General Intelligence via Consciousness Modeling (invited talk)},
  booktitle = {IEEE Infrastructure Conference},
  year = {2022},
  month = {September},
  city = {Carmel},
  author = {Edward Y. Chang},
}

@misc{ChangTricorder2022,
  doi = {10.48550/ARXIV.2212.13591},
  url ={\url{https://arxiv.org/abs/2212.13591}},
  author = {Chang, Edward Y.},
  keywords = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.7; K.3.2},
  title = {Knowledge-Guided Data-Centric AI in Healthcare: Progress, Shortcomings, and Future Directions},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{10.1145/1961189.1961198,
author = {Liu, Zhiyuan and Zhang, Yuzhou and Chang, Edward Y. and Sun, Maosong},
title = {PLDA+: Parallel Latent Dirichlet Allocation with Data Placement and Pipeline Processing},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/1961189.1961198},
doi = {10.1145/1961189.1961198},
abstract = {Previous methods of distributed Gibbs sampling for LDA run into either memory or communication bottlenecks. To improve scalability, we propose four strategies: data placement, pipeline processing, word bundling, and priority-based scheduling. Experiments show that our strategies significantly reduce the unparallelizable communication bottleneck and achieve good load balancing, and hence improve scalability of LDA.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {5},
articleno = {26},
numpages = {18},
keywords = {latent Dirichlet allocation, Topic models, Gibbs sampling, distributed parallel computations}
}

@article{SocreticMethodChang2023,
  title={Prompting Large Language Models
With the Socratic Method},
  author={Edward Y. Chang},
  journal={IEEE 13th Computing and Communication Workshop and Conference (CCWC)},
  year={2023},
}

@inproceedings{DiscoveringUnknownChang-July2023,
    author = {Edward Y. Chang and Emily J. Chang},
    title = {{Discovering Insights Beyond the Known}: {A Dialogue Between GPT-4 Agents from Adam and Eve to the Nexus of Ecology, AI, and the Brain}},
    booktitle = {Stanford InfoLab Technical Report},
    year = {2023}
}


@article{MiddleEast-October2023,
    author = {Edward Y. Chang},
    title = {{LLM Debate on the Middle East Conflict: Is It Resolvable?}},
    journal = {Stanford University InfoLab Technical Report},
    volume = {},
    number = {},
    year = {2023},
    month = {10},
    doi = {},
}

@article{li2003discovery,
  title={Discovery of a perceptual distance function for measuring image similarity},
  author={Li, Beitao and Chang, Edward and Wu, Yi},
  journal={Multimedia Systems},
  volume={8},
  pages={512--522},
  year={2003},
  publisher={Springer},
  doi={10.1007/s00530-002-0069-9}
}


@INPROCEEDINGS{ParallelFactorization2007,
  author={Zhu, Kaihua and Cui, Hang and Wang, Hao and Bai, Hongjie and Xu, Hui and Li, Jian and Chang, Edward Y. and Qiu, Zhihuan},
  booktitle={2007 IEEE International Conference on Multimedia and Expo}, 
  title={Parallel Approximate Matrix Factorization for Kernel Methods}, 
  year={2007},
  volume={},
  number={},
  pages={1275-1278},
}


@inproceedings{PSVM2007,
author = {Chang, Edward Y. and Zhu, Kaihua and Wang, Hao and Bai, Hongjie and Li, Jian and Qiu, Zhihuan and Cui, Hang},
title = {PSVM: Parallelizing Support Vector Machines on Distributed Computers},
year = {2007},
isbn = {9781605603520},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Support Vector Machines (SVMs) suffer from a widely recognized scalability problem in both memory use and computational time. To improve scalability, we have developed a parallel SVM algorithm (PSVM), which reduces memory use through performing a row-based, approximate matrix factorization, and which loads only essential data to each machine to perform parallel computation. Let n denote the number of training instances, p the reduced matrix dimension after factorization (p is significantly smaller than n), and m the number of machines. PSVM reduces the memory requirement from O(n2) to O(np/m), and improves computation time to O(np2/m). Empirical study shows PSVM to be effective. PSVM Open Source is available for download at http://code.google.com/p/psvm/.},
booktitle = {Proceedings of the 20th International Conference on Neural Information Processing Systems},
pages = {257–264},
series = {NIPS'07}
}

@inproceedings{PFP2008,
author = {Li, Haoyuan and Wang, Yi and Zhang, Dong and Zhang, Ming and Chang, Edward Y.},
title = {{PFP: Parallel FP-Growth for Query Recommendation}},
year = {2008},
publisher = {ACM},
numpages = {8},
keywords = {parallel fp-growth, frequent itemset mining, data mining},
location = {Lausanne, Switzerland},
booktitle = {ACM RecSys '08}
}

@ARTICLE{PSC2011,
  author={Chen, Wen-Yen and Song, Yangqiu and Bai, Hongjie and Lin, Chih-Jen and Chang, Edward Y.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={{Parallel Spectral Clustering in Distributed Systems}}, 
  year={2011},
  volume={33},
  number={3},
  pages={568-586},
  keywords={Sparse matrices;Clustering algorithms;Nearest neighbor searches;Distributed computing;Concurrent computing;Laplace equations;Computer science;USA Councils;Scalability;Parallel algorithms;Parallel spectral clustering;distributed computing;normalized cuts;nearest neighbors;Nyström approximation.},
  doi={10.1109/TPAMI.2010.88}}

@InProceedings{PLDA2009,
author="Wang, Yi
and Bai, Hongjie
and Stanton, Matt
and Chen, Wen-Yen
and Chang, Edward Y.",
editor="Goldberg, Andrew V.
and Zhou, Yunhong",
title="PLDA: Parallel Latent Dirichlet Allocation for Large-Scale Applications",
booktitle="Algorithmic Aspects in Information and Management",
year="2009",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="301--314",
}

@inproceedings{chang2024uncoveringbiases,
    author = {Edward Y. Chang},
    title = {{Uncovering Biases with Reflective Large Language Models}},
    booktitle = {arXiv:2408.13464},
    year = {2024}
}

@misc{chang2024uncoveringbiases2,
      title={{Uncovering Biases with Reflective Large Language Models}}, 
      author={Edward Y. Chang},
      year={2024},
      eprint={2408.13464},
      archivePrefix={arXiv},
      booklet={arXiv:2408.13464},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2408.13464}, 
}

@inproceedings{chang2024MIPR,
      title={{Behavioral Emotion Analysis Model for Large Language Models (invited paper)}}, 
      author={Edward Y. Chang},
      month = {August},
      year={2024},
      booktitle={Proceedings of the $7^{th}$ IEEE MIPR Conference} 
}


@misc{SocraPedia2024,
      title={{Uncovering Biases with Reflective Large Language Models}}, 
      author={Edward Y. Chang},
      year={2024},
      eprint={2408.13464},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url ={\url{https://arxiv.org/abs/2408.13464}}, 
}

@InProceedings{WebScalePCM2028,
author="Liu, Jiakai
and Hu, Rong
and Wang, Meihong
and Wang, Yi
and Chang, Edward Y.",
title={{Web-Scale Image Annotation}},
booktitle="Advances in Multimedia Information Processing - PCM 2008",
year="2008",
publisher="Springer Berlin Heidelberg",
pages="663--674",
abstract="In this paper, we describe our experiments using Latent Dirichlet Allocation (LDA) to model images containing both perceptual features and words. To build a large-scale image tagging system, we distribute the computation of LDA parameters using MapReduce. Empirical study shows that our scalable LDA supports image annotation both effectively and efficiently.",
isbn="978-3-540-89796-5"
}


@article{PLDA2011,
author = {Liu, Zhiyuan and Zhang, Yuzhou and Chang, Edward Y. and Sun, Maosong},
title = {{PLDA+: Parallel Latent Dirichlet Allocation with Data Placement and Pipeline Processing}},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/1961189.1961198},
doi = {10.1145/1961189.1961198},
abstract = {Previous methods of distributed Gibbs sampling for LDA run into either memory or communication bottlenecks. To improve scalability, we propose four strategies: data placement, pipeline processing, word bundling, and priority-based scheduling. Experiments show that our strategies significantly reduce the unparallelizable communication bottleneck and achieve good load balancing, and hence improve scalability of LDA.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {5},
articleno = {26},
numpages = {18},
keywords = {Gibbs sampling, Topic models, distributed parallel computations, latent Dirichlet allocation}
}

@proceedings{Kronecker2005,
author = {Gang Wu and Zhihua Zhang and Edward Chang},
title = {Kronecker Factorization for Speeding up Kernel Machines},
booktitle = {Proceedings of the 2005 SIAM International Conference on Data Mining (SDM)},
chapter = {},
pages = {611-615},
doi = {10.1137/1.9781611972757.71},
year = {2005},
}


@inproceedings{DataDrivePioneer2010,
author = {Wang, Zhiyu and Xia, Dingyin and Chang, Edward Y.},
title = {A deep-learning model-based and data-driven hybrid architecture for image annotation},
year = {2010},
isbn = {9781450301664},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1878137.1878141},
doi = {10.1145/1878137.1878141},
abstract = {Does adding more training data always help improve the effectiveness of a machine-learning or pattern-recognition task? Recent evidences in machine translation and speech recognition seem to suggest that the data-driven approach outperforms the traditional model-based approach. Instead of carefully modeling rules and their exceptions, the data-driven approach relies on identifying similar patterns in massive datasets and then uses the similar patterns to predict the labels (or other outcomes) of unseen instances. In this work, we compare representative data-driven and model-based schemes on an image annotation task. We enumerate pros and cons of these two approaches, and propose a hybrid approach, which can harness the strengths of the two.},
booktitle = {Proceedings of the International Workshop on Very-Large-Scale Multimedia Corpus, Mining and Retrieval},
pages = {13–18},
numpages = {6},
keywords = {data-driven, deep learning, image annotation, model-based},
location = {Firenze, Italy},
series = {VLS-MCMR '10}
}


@article{DPF2003,
  title={{Discovery of A Perceptual Distance Function for Measuring Image Similarity}},
  author={Beitao Li and Edward Y. Chang and Yi Wu},
  journal={Multimedia Systems},
  year={2003},
  volume={8},
  pages={512–522}
}

@INPROCEEDINGS{DPF2002,
  author={Baitao Li and Chang, E. and Ching-Tung Wu},
  booktitle={Proceedings. International Conference on Image Processing}, 
  title={{DPF - A Perceptual Distance Function for Image Retrieval}}, 
  year={2002},
  volume={2},
  number={},
  pages={II-II},
  keywords={Image retrieval;Content based retrieval;Data mining;Area measurement;Psychology;Information retrieval;Testing;Digital images;Earth;Fuzzy logic},
  doi={10.1109/ICIP.2002.1040021}}

@article{PLDA2011,
author = {Liu, Zhiyuan and Zhang, Yuzhou and Chang, Edward Y. and Sun, Maosong},
title = {{PLDA+: Parallel Latent Dirichlet Allocation with Data Placement and Pipeline Processing}},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/1961189.1961198},
doi = {10.1145/1961189.1961198},
abstract = {Previous methods of distributed Gibbs sampling for LDA run into either memory or communication bottlenecks. To improve scalability, we propose four strategies: data placement, pipeline processing, word bundling, and priority-based scheduling. Experiments show that our strategies significantly reduce the unparallelizable communication bottleneck and achieve good load balancing, and hence improve scalability of LDA.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {may},
articleno = {26},
numpages = {18},
keywords = {Gibbs sampling, Topic models, distributed parallel computations, latent Dirichlet allocation}
}


@book{MMBookChang2011,
  author = {Chang, Edward Y.},
  title = {{Foundations of Large-Scale Multimedia Information Management and Retrieval: Mathematics of Perception}},
  publisher = {Springer-Verlag},
  year = {2011},
}

@inproceedings{REFUEL2017,
author = {Peng, Yu-Shao and Tang, Kai-Fu and Lin, Hsuan-Tien and more},
title = {REFUEL: exploring sparse features in deep reinforcement learning for fast disease diagnosis},
year = {2018},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {7333–7342},
series = {NIPS'18}
}

@misc{UCCTchang2025,
      title={The Unified Cognitive Consciousness Theory for Language Models: Anchoring Semantics, Thresholds of Activation, and Emergent Reasoning}, 
      author={Edward Y. Chang},
      year={2025},
      eprint={2506.02139},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2506.02139}, 
}

@misc{anonymous2025supplement,
  author       = {Anonymous},
  title        = {Supplementary Materials},
  year         = {2025},
  howpublished = {\url{https://drive.google.com/file/d/1kw6fg8OxX8TBd2ijlmaMJ_9YRxKaLWGJ/view?usp=sharing}},
}

@misc{chang2025MACIDD,
      title={Multi-Agent Collaborative Intelligence: Dual-Dial Control for Reliable LLM Reasoning}, 
      author={Edward Y. Chang and Ethan Y. Chang},
      year={2025},
      eprint={2510.04488},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2510.04488}, 
}