\section{Introduction}
\label{sec:intro}

Modern LLM deployments need diagnostics that (i) anticipate when model behavior will violate intended \emph{invariances} (e.g., code $\alpha$-renaming; algebraic equivalence), and (ii) tell systems when it is safe to \emph{fuse, reorder, or parallelize} operators without changing outputs. We introduce $\WS$, a minimal post-hoc diagnostic suite that converts simple loop and reordering checks on internal representations into system signals. We call the suite $\WS$ in reference to Wilson loops that measure holonomy; the name is a mnemonic rather than an acronym.

\paragraph{Why this matters now.}
LLMs are embedded in pipelines where \emph{behavioral stability} is a hard requirement: builds must pass CI, compilers seek fusions or reorders for latency, and products promise consistent outcomes across benign edits. In practice we see: (1) \textbf{invariance breaks} under semantics-preserving transforms such as identifier renaming, algebraic equivalence, and template-preserving paraphrase (Fig.~\ref{fig:alpha-rename}); (2) \textbf{order sensitivity}, where seemingly safe operator changes alter outputs, exacerbated by floating-point non-associativity and long-context phase drift; and (3) \textbf{seed instability} that complicates interpretability and audits. These issues surface in real applications: RAG answers can flip when retrieved passages are reordered, fine-tuning can erode invariances learned at pretraining, and debate or chain-of-thought prompts can take path-dependent routes to different conclusions. Such failures raise costs (canaries, rollbacks), obscure root causes, and block safe speedups.

\paragraph{Why not just train it away?}
Architectural equivariance and invariance-aware training help, but they (a) constrain model classes, (b) require retraining, and (c) still leave open \emph{online} decisions about where fusion or reordering is safe for the \emph{deployed} checkpoint. Saliency or probing can be informative but are gauge-unstable across seeds. Mechanistic case studies are insightful but hard to scale. Formal verification is too heavy for routine inference. We instead seek \emph{post-hoc, cheap, gauge-stable} signals that travel with any Transformer and slot into existing systems.

% =========================
% MOTIVATING EXAMPLES (Intro, after "Why not just train it away?")
% =========================
\subsection*{Two motivating examples (why a diagnostic is needed)}
\label{sec:examples}

\paragraph{Example 1: $\alpha$-renaming should not change behavior (but often does).}
The two programs below are \emph{semantically identical}. A robust code LLM should produce the same decision (e.g., \texttt{pass@k}) and similar next-token probabilities under either identifier scheme. An \textbf{invariance failure} occurs when the invariance ratio $\mathrm{IR}(x) < 1 - \delta$ (Sec.~\ref{sec:metrics}).

\begin{lstlisting}[
  language=Python, label={lst:alpha-rename},
  basicstyle=\ttfamily\footnotesize,
  breaklines=true, breakatwhitespace=true,
  columns=fullflexible, keepspaces=true,
  postbreak=\mbox{\textcolor{gray}{$\hookrightarrow$}\space},
  caption={Alpha-renaming: semantics preserved, identifiers changed.}
]
# (A) Original
def is_balanced(s: str) -> bool:
    stk = []
    for ch in s:
        if ch == '(':
            stk.append(ch)
        elif ch == ')':
            if not stk: return False
            stk.pop()
    return not stk

# (B) Renamed (alpha-equivalent)
def is_balanced(u: str) -> bool:
    Z = []
    for c in u:
        if c == '(':
            Z.append(c)
        elif c == ')':
            if not Z: return False
            Z.pop()
    return not Z
\end{lstlisting}

\emph{What goes wrong.} Identifier statistics from pretraining can shift tokenization and local logits enough to change \texttt{pass@k} or a critical beam step. Our diagnostic goal is to predict such failures \emph{before} they happen. High curvature $\kappa_{\mathrm{inv}}$ near the relevant positions and layers flags risk, and elevated commutators $\Delta_{A,B}$ identify order-sensitive submodules that amplify the effect.

\paragraph{Example 2: ``Safe'' operator changes can flip the next token (order sensitivity).}
Even mathematically equivalent rewrites can differ numerically in finite precision. Consider attention logits $z=\tfrac{QK^\top}{\sqrt d}+M$. Two fused kernels compute $z$ with different accumulation orders:
\begin{footnotesize}
\[
\mathrm{Path\ A:}\;(\sum_i q_i k_i)/\sqrt d + m \quad \text{vs.} \quad
\mathrm{Path\ B:}\;\sum_i (q_i k_i/\sqrt d) + m,
\]
\end{footnotesize}
equal in $\mathbb{R}$ but \emph{not} in FP16 or BF16 due to rounding. A perturbation on the order of $10^{-3}$ can flip the top token.

\begin{lstlisting}[
  language=Python, label={lst:tie-flip},
  basicstyle=\ttfamily\footnotesize,
  breaklines=true, breakatwhitespace=true,
  columns=fullflexible, keepspaces=true,
  postbreak=\mbox{\textcolor{gray}{$\hookrightarrow$}\space},
  caption={Toy numeric: tiny logit changes at a tie boundary can flip the argmax.}
]
import numpy as np

# Near-tie logits at step t
zA = np.array([0.000,  0.001], dtype=np.float32)   # Path A
zB = np.array([-0.001, 0.001], dtype=np.float32)   # Path B

pA = np.exp(zA) / np.exp(zA).sum()
pB = np.exp(zB) / np.exp(zB).sum()

# Nudge by 1e-3 (within fused-kernel rounding)
zB_pert = zB + np.array([+0.0015, -0.0005], dtype=np.float32)
pB_pert = np.exp(zB_pert) / np.exp(zB_pert).sum()
# The argmax can flip when margins are razor thin.
\end{lstlisting}

\emph{What to measure.} We define \textbf{drift} $\delta(x)=\|y_{AB}-y_{BA}\|$ between outputs under two safe reorder or fuse options and use the \emph{commutator} $\Delta_{A,B}=\|A\!\circ\!B - B\!\circ\!A\|_F$ as a predictor of this drift. In parallel, the \emph{inverse-free curvature} $\kappa_{\mathrm{inv}}$ (Eq.~\eqref{eq:invfree-kappa}) localizes \emph{where} order sensitivity is likely (high-curvature loops), allowing systems to avoid reorders or fusions there or to add checks.

\paragraph{Our route in one line.}
Treat the model as a discrete bundle over (position, layer); form small plaquette loops from vertical layer transports and horizontal attention transports; estimate \emph{inverse-free} holonomy $\kappa_{\mathrm{inv}}$ with $r$ Hutchinson JVP probes per loop to localize order sensitivity; compute activation commutators $\Delta_{A,B}$ on a calibration batch to predict reorder risk for candidate submodules; apply an $O(d)$ orthogonal gauge fix (whitening plus Procrustes) to 
align logs across seeds;
compute $\kappa_{\mathrm{inv}}$ in the native basis; and emit curvature
and commutator scores as CSV with calibrated thresholds that gate fusions, reorders, and parallel routes in CI.

\textit{Terminology note.} We borrow terms such as holonomy and commutator for intuition, but we use discrete, data-dependent path differences on Transformer representations as engineering diagnostics rather than claims about continuous field theories.

\paragraph{LLM-specific applications.}
While $\WS$ is model-agnostic, several high-confidence use cases emerge for LLM systems: prompt robustness under paraphrasing, RAG passage reordering sensitivity, fine-tuning regression detection, multi-turn drift monitoring, and chain-of-thought pathway stability (\S6.8). These applications require only diagnostic signals with no retraining. We also identify potential synergies with semantic anchoring frameworks such as $\UCCT$. Empirical validation of these connections remains future work (\S8.2).

\paragraph{How this helps semantic anchoring ($\UCCT$).}
Semantic anchoring \cite{chang2025UCCT} benefits from geometry-derived signals. Prefer low-$\kappa_{\mathrm{inv}}$ regions for anchor placement (flat loops), gate anchors with high local commutators, and maintain anchor health via gauge-fixed logs. We treat this as an application of our diagnostics (details in \S\ref{sec:systems}); nothing in our method depends on $\UCCT$.

\paragraph{Design goals.}
We seek diagnostics that are (G1) \textbf{predictive} of failures before they happen, (G2) \textbf{cheap} (JVP-only; matrix-free), (G3) \textbf{gauge-stable} across seeds and runs, (G4) \textbf{model-agnostic} for Transformers with standard residual blocks and RoPE or relative positions, and (G5) \textbf{systems-usable} with clear thresholds and CSV artifacts that feed orchestrators and planners.

\paragraph{Idea in brief.}
We view the network as a \emph{discrete bundle} over \emph{(position, layer)}. Vertical edges carry layer transports. Horizontal edges carry attention transports. We define an \textbf{inverse-free Wilson loop} that measures \emph{holonomy} using only JVPs with Hutchinson probes, yielding a curvature score $\kappa_{\mathrm{inv}}$ (Eq.~\eqref{eq:invfree-kappa}). Large curvature indicates \emph{non-commuting transports} and flags order-sensitive regions. Small curvature suggests safe fusions or reorders. In parallel, we compute \textbf{activation commutators} $\Delta_{A,B}$ between submodules to map order-sensitive pairs and relate them to output drift (Fig.~\ref{fig:diagnostic-suite}(b)). A light \textbf{orthogonal gauge fix} (whitening plus Procrustes; Fig.~\ref{fig:gauge-pipeline}) stabilizes features and logging across seeds in CI.

\paragraph{Contributions.}
\begin{itemize}[leftmargin=1.2em,itemsep=2pt]
\item \textbf{Minimal formalism.} Discrete transports on (position, layer) with $O(d)$ gauge. An \emph{inverse-free} curvature $\kappa_{\mathrm{inv}}$ computable by JVPs. Estimator error derivations and an empirical study of gauge behavior (Appx.).
\item \textbf{Actionable diagnostics.} Curvature scores for predicting invariance failures (ROC and AP; Fig.~\ref{fig:diagnostic-suite}(a)). Commutators for predicting drift under reorder or fuse (Fig.~\ref{fig:diagnostic-suite}(b)). Both expose safe \emph{parallel} regions and risky \emph{sequential} ones.
\item \textbf{Gauge-stable logging.} Whitening plus Procrustes reduces probe variance and stabilizes saliency ranks across seeds, improving CI reproducibility (Fig.~\ref{fig:gauge-pipeline}).
\item \textbf{Systems hooks.} CSV artifacts and thresholds integrate with an \emph{LLM-driven TestBench} and a \emph{CI or release gate} to prioritize metamorphic tests (e.g., $\alpha$-renaming orbits), schedule reorder or fuse canaries, and enforce acceptance thresholds (\S\ref{sec:systems}).
\item \textbf{Reproducible suite.} E1--E7 diagnostics with schemas, defaults, ablations, and a Colab recipe. Outputs are gauge-stable and append-only for forensic replay.
\end{itemize}

\paragraph{Impacts we target.}
(1) \textbf{Reliability}: detect invariance breaks early. (2) \textbf{Throughput}: safe fusion, reordering, and parallelization in low-curvature regions. (3) \textbf{Reproducibility}: gauge-stable logs for CI and audits. (4) \textbf{Cost}: diagnostics with at most 10--20\% overhead and concrete budget knobs.

\paragraph{Metrics (how we judge success).}
We evaluate six axes (\S\ref{sec:metrics}): (M1) \emph{IR} (invariance ratio) on orbits (Fig.~\ref{fig:alpha-rename}); (M2) \emph{AUC or AP} for curvature predicting failures (Fig.~\ref{fig:diagnostic-suite}(a)); (M3) \emph{commutator to drift correlation} (Fig.~\ref{fig:diagnostic-suite}(b)); (M4) \emph{gauge-stable CI} (variance reduction and rank stability); (M5) \emph{RoPE phase drift} versus depth and context length (Fig.~\ref{fig:rope-rotation}); (M6) \emph{overhead} (Fig.~\ref{fig:diagnostic-suite}(c)).

\paragraph{Methods (one-paragraph sketch).}
Vertical transports $T^{\text{layer}}_{i,\ell}\!\approx\!\partial h_{i,\ell+1}/\partial h_{i,\ell}$ and edge-wise horizontal transports $T^{\text{attn}}_{i\leftarrow j,\ell}\!\approx\!\partial h^{\text{out}}_{i,\ell}/\partial h^{\text{in}}_{j,\ell}$ define small loops on the grid. We avoid inverses by comparing two paths that end in the same fiber (Eq.~\eqref{eq:invfree-kappa}), estimating norms with $r$ Rademacher probes via JVPs. A frozen-softmax scan proposes hotspots. We confirm them with full JVPs. Activation commutators measure $\|A\!\circ\!B - B\!\circ\!A\|_F$ on a calibration batch and correlate with output drift under safe reorder or fuse. Gauge fix (whiten plus Procrustes) precedes logging, not curvature.

\paragraph{Experimental setup (overview).}
We will test small or medium open models (7B--13B Transformers with RoPE or relative positions) on code orbits (HumanEval or MBPP style $\alpha$-renaming), algebraic rewrites, synthetic RoPE phase shifts, and calibration batches for commutators. Hardware: A100-class GPUs, BF16 or FP16 with FP32 pins for LN and softmax JVPs. Seeds fixed. Deterministic kernels where available. $(r,k,m)$ defaults $(6,8,6)$. Top-$m$ neighbors by attention mass plus light random exploration. Full procedures appear in \S\ref{sec:setup}. Metrics in \S\ref{sec:metrics}.

\paragraph{Evaluation plan (pre-registered; no results in this version).}
We will report: (i) IR on orbits, (ii) ROC and PR AUC for curvature predicting invariance failures, (iii) Spearman $\rho$ and Pearson $r$ between commutator norms and reorder drift, (iv) gauge-stability deltas (variance ratios and Kendall-$\tau$), (v) RoPE phase-drift areas versus depth and context length, and (vi) end-to-end overhead with attribution (scan versus confirm). Null baselines (randomized curvature maps and random-init networks) and bootstrap CIs are included. Empirical values are intentionally omitted in this version and will appear in a subsequent update.

\paragraph{Scope and non-claims.}
This is a \emph{code-first} note on Transformer residual streams. We do not claim a continuous principal bundle for full models, nor that embeddings realize formal grammar groups. We position curvature and commutator signals as \emph{engineering diagnostics}, leaving broader theory and non-Transformer architectures to future work (\S\ref{sec:conclusion}, Appx.).
