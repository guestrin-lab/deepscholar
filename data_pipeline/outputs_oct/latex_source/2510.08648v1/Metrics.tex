% ===============================
%   EVALUATION METRICS (INLINE)
% ===============================
\section{Evaluation Metrics}
\label{sec:metrics}

% choose how far to drop the subfigure labels
\newcommand{\subcapdrop}{8mm}
\begin{figure*}[t]
\centering
\begin{tikzpicture}
\begin{groupplot}[
  group style={group size=3 by 1, horizontal sep=15mm},
  scale only axis=true,
  width=0.25\textwidth,
  height=0.20\textwidth,
  title style={font=\footnotesize},
  label style={font=\footnotesize},
  tick label style={font=\scriptsize}
]

% (a) ROC curvature
\nextgroupplot[
  xlabel=False Positive Rate,
  ylabel=True Positive Rate,
  title={Curvature $\kappa$ predicts invariance failures},
  xmin=0, xmax=1, ymin=0, ymax=1, grid=both
]
\addplot+[thick] coordinates {(0,0) (0.1,0.5) (0.2,0.7) (0.4,0.85) (0.7,0.95) (1,1)};
\addplot[dashed] coordinates {(0,0) (1,1)};
\label{fig:roc-curvature}

% (b) Commutator vs drift
\nextgroupplot[
  xlabel={Commutator norm $\Delta_{A,B}$},
  ylabel={Drift $\|y_{AB}-y_{BA}\|$},
  title={Commutators correlate with drift},
]
\addplot+[only marks] coordinates {
(0.02,0.01) (0.08,0.04) (0.31,0.18) (0.35,0.22) (0.07,0.03) (0.29,0.17)
};
\label{fig:comm-drift}

% (c) Overhead bars
\nextgroupplot[
  ybar,
  ylabel={Relative overhead (\%)},
  symbolic x coords={Scan,Confirm,Total},
  xtick=data,
  nodes near coords,
  ymin=0, ymax=20,
  title={Runtime overhead at $(r,k,m)=(6,8,6)$},
  bar width=25pt, bar shift=1pt
]
\addplot coordinates {(Scan,7) (Confirm,9) (Total,16)};
\label{fig:overhead-bars}

\end{groupplot}

% Subfigure labels (a), (b), (c)
\node[anchor=north] at ($(group c1r1.south)+(0,-\subcapdrop)$) {(a) ROC (illustrative)};
\node[anchor=north] at ($(group c2r1.south)+(0,-\subcapdrop)$) {(b) $\Delta$ vs.\ drift};
\node[anchor=north] at ($(group c3r1.south)+(0,-\subcapdrop)$) {(c) Overhead breakdown};
\end{tikzpicture}
\vspace{-.3in}
\caption{Diagnostic suite: (a) curvature ROC; (b) commutator–drift correlation; (c) scan/confirm/total overhead. Replace illustrative values with measured results.}
\label{fig:diagnostic-suite}
\vspace{-.1in}
\end{figure*}

We score the proposed diagnostics along six axes: strict invariance under clean transforms (IR), curvature–failure predictiveness (ROC/AP), output sensitivity to operator reordering (drift), reproducibility under gauge fixing, RoPE phase-drift stability, and runtime overhead. Each metric is defined at the \emph{per-input} level and as \emph{dataset aggregates}. Diagnostic/topology maps appear in \S\ref{sec:diagnostics}; this section defines how we turn those signals into reportable numbers. Unless stated otherwise, we use the defaults in Table~\ref{tab:exp-setup}.

\paragraph{Invariance ratio (E1; ties to Fig.~\ref{fig:alpha-rename}).}
For an input $x$ with an orbit $\mathcal{O}(x)$ (e.g., $\alpha$-renamings; Fig.~\ref{fig:alpha-rename}), define
\begin{footnotesize}
\[
\mathrm{IR}(x)=\frac{1}{|\mathcal{O}(x)|}\sum_{x'\in\mathcal{O}(x)} \mathbf{1}\{f(x')=f(x)\},
\]
\end{footnotesize}
where $f(\cdot)$ is a task decision (e.g., pass@k for code, argmax token for classification). Assign a failure label $y{=}1$ if $\mathrm{IR}(x) < 1 - \delta$, where $\delta$ is a tolerance calibrated on a benign validation set.  
\text{Aggregates:} macro-average
\begin{footnotesize}
\[
\overline{\mathrm{IR}}_{\text{macro}}=\frac{1}{N}\sum_{x} \mathrm{IR}(x),
\]
\[
\overline{\mathrm{IR}}_{\text{micro}}=\frac{\sum_{x}\sum_{x'\in\mathcal{O}(x)} \mathbf{1}\{f(x')=f(x)\}}{\sum_{x} |\mathcal{O}(x)|},
\]
\end{footnotesize}
with stratified bootstrap 95\% CIs.

\paragraph{Curvature summary (E5; ties to Figs.~\ref{fig:fiber-grid}, \ref{fig:holonomy-scatter}).}
Given loops $\mathcal{L}(x)$ associated with $x$, aggregate inverse-free curvature (\S\ref{sec:invfree}) as
\[
\kappa_{\max}(x)=\max_{(i,j,\ell)\in\mathcal{L}(x)} \kappa_{\mathrm{inv}}(i,j,\ell),
\]
\[
\kappa_{p95}(x)=\mathrm{quantile}_{0.95}\!\left(\{\kappa_{\mathrm{inv}}(i,j,\ell)\}_{(i,j,\ell)\in\mathcal{L}(x)}\right).
\]
We visualize position/layer curvature in \S\ref{sec:diagnostics} (Fig.~\ref{fig:holonomy-scatter}) and use these aggregates as predictors below.

\paragraph{Predictive power for invariance breaks (E6).}
Treat a curvature aggregate $s(x)\!\in\!\{\kappa_{\max}(x),\kappa_{p95}(x)\}$ as a score for predicting $y$. Sweep thresholds to obtain ROC and PR curves; report ROC AUC, Average Precision (AP), Brier score, and Expected Calibration Error (ECE; 10 bins). Use stratified bootstrap for 95\% CIs. Include null baselines: (i) input-wise random permutation of curvature maps; (ii) random-initialized networks matched by width/depth.

\paragraph{Order sensitivity and drift (E3; ties to Figs.~\ref{fig:comm-heatmap}, \ref{fig:diagnostic-suite}(b)).}
Compute the commutator norm $\Delta_{A,B}=\|A\!\circ\!B - B\!\circ\!A\|_F$ on a calibration batch and measure \emph{output drift} under safe reorder/fuse, $\delta(x)=\|y_{AB}-y_{BA}\|$.  
\textbf{Aggregates:} report Spearman $\rho(\Delta,\delta)$, Pearson $r$, and a robust slope via Theil–Sen, all with bootstrap CIs. When relevant, report partial correlations controlling for sequence length or output entropy.

% Requires: \usepackage{subcaption}, \usepackage{tikz}, \usepackage{pgfplots}
% Also load: \usepgfplotslibrary{groupplots}


\paragraph{Gauge-stable logging (E2/E7; ties to Fig.~\ref{fig:gauge-pipeline}).}
Across random seeds and identical data slices, compare: (i) probe-accuracy variance (lower is better), (ii) saliency-rank stability via Kendall-$\tau$ (higher is better), and (iii) cross-seed cosine distance of layerwise means (lower is better). Report pre/post gauge-fix (whiten\,$+$\,Procrustes) deltas with CIs. In CI, set acceptance thresholds (e.g., variance ratio $\le 0.6$) that fail builds when reproducibility regresses.

\paragraph{RoPE phase-drift stability (E4; ties to Fig.~\ref{fig:rope-rotation}).}
Apply controlled phase offsets $\delta\theta$ to RoPE frequencies; measure Wasserstein (or symmetric KL) between attention-score distributions at depth $\ell$ \emph{with} vs.\ \emph{without} the offset. Summarize by area-under-drift vs.\ depth/context length; larger areas indicate long-context brittleness.

\paragraph{Runtime overhead (all experiments).}
Measure end-to-end overhead of diagnostics at defaults $(r,k,m)$ and at reduced settings. Attribute cost to scan vs.\ confirm (frozen-softmax multiplies vs.\ JVPs). Target $\le 10$–$20\%$ overhead at defaults; additionally report throughput gains unlocked by curvature/commutator-informed fusion (\S\ref{sec:diagnostics}).

\vspace{1ex}
\noindent\textbf{Reporting checklist.} Always include: $N$ inputs, loops/input, seeds, model sizes; tolerances $\delta$; bootstrap details (stratification, reps); nulls; confidence intervals; ablation knobs (loop size, gauge-fix mode, transport mode); and wall-clock environment (GPU/TPU type, batch size).
