
\documentclass{article} % For LaTeX2e
\usepackage{iclr2026_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage[table]{xcolor}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{caption} % for \captionof
\usepackage{comment} 
\usepackage{wrapfig}

\title{Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Dong Lao\textsuperscript{1,2} \quad Yuxiang Zhang\textsuperscript{2} \quad Haniyeh Ehsani Oskouie\textsuperscript{2} \quad Yangchao Wu\textsuperscript{2} \\ \textbf{Alex Wong\textsuperscript{3} \quad Stefano Soatto\textsuperscript{2}} \\ 
\textsuperscript{1}LSU Vision Lab  \quad\quad \textsuperscript{2}UCLA Vision Lab \quad\quad \textsuperscript{3}Yale Vision Lab \\
\textsuperscript{1}\texttt{dong.lao@lsu.edu} \\ \textsuperscript{2}\texttt{\{zhangbrandon102,haniyehehsani,wuyangchao1997,soatto\}@ucla.edu} \\ \textsuperscript{3}\texttt{alex.wong@yale.edu}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
We propose a test-time defense mechanism against adversarial attacks: imperceptible image perturbations that significantly alter the predictions of a model. Unlike existing methods that rely on feature filtering or smoothing, which can lead to information loss, we propose to ``combat noise with noise'' by leveraging stochastic resonance to enhance robustness while minimizing information loss. Our approach introduces small translational perturbations to the input image, aligns the transformed feature embeddings, and aggregates them before mapping back to the original reference image. This can be expressed in a closed-form formula, which can be deployed on diverse existing network architectures without introducing additional network modules or fine-tuning for specific attack types. The resulting method is entirely training-free, architecture-agnostic, and attack-agnostic. Empirical results show state-of-the-art robustness on image classification and, for the first time, establish a generic test-time defense for dense prediction tasks, including stereo matching and optical flow, highlighting the method’s versatility and practicality. Specifically, relative to clean (unperturbed) performance, our method recovers up to 68.1\% of the accuracy loss on image classification, 71.9\% on stereo matching, and 29.2\% on optical flow under various types of adversarial attacks.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

Most deep neural networks in use today are deterministic maps from a fixed-size input to a fixed-size feature vector. In auto-regressive Transformer models, that vector encodes the next element (token) in the input sequence. Similarly, in convolutional architectures, that vector may encode the input data. In either case, the output vector is often highly sensitive to perturbations of the input, and one can intentionally choose these imperceptible perturbations {\em adversarially} so as to maximize the change in the output \cite{goodfellow2014explaining}. In some cases, the same perturbation can even be disruptive for a large number of possible inputs \cite{moosavi2017universal}, exploiting the convoluted geometry of the decision boundary imposed by such trained models \cite{tramer2017space}. This spurious sensitivity could be exploited adversarially to disrupt the operation of a model. 

From a classical perspective of signal processing, adversarial perturbations of images appear as small high-frequency ``noise'' resembling {\em aliasing} artifacts. These are imperceptible since the human visual system easily discounts them on account of their poor fit to the `ecological statistics' of natural images \cite{gibson2014ecological}.
Classical sampling theory prescribes anti-aliasing by low-pass filtering the data, removing information along with the artifacts. Low-pass filtering consists of spatial averaging of the perturbed data with respect to a chosen kernel, typically a Gaussian or a constant (``pillbox''). Alternatively, one could ``denoise'' the embeddings by averaging output vectors, also a lossy operation. The choice of the kernel should match the statistics of the perturbations, which sets up a cat-and-mouse game where the adversary can easily modify the perturbations to bypass the anti-aliasing filter, namely {\em adaptive} attacks \cite{tramer2020adaptive}, and the model needs to constantly be fine-tuned to ``anti-alias'' new forms of adversarial perturbations. In the context of deep neural networks, existing defense methods that manipulate feature representations \cite{xie2019feature,bai2021improving,yan2021cifs,kim2023feature} fundamentally adhere to this paradigm in principle. Despite substantial engineering efforts, these methods remain inherently vulnerable to adaptive attacks because they rely on pre-defined filtering strategies that are fixed at inference time.

\textbf{Desiderata:} 
To break this cycle, we advocate a method to mitigate the effect of adversarial perturbation that (i) operates at test time, without the need to update the model weights, and (ii) does not entail information loss associated with direct or indirect spatial filtering. In addition, it would be ideal if this method could (iii) be applied to existing network architectures without modifications, and (iv) be agnostic to the specific type of adversarial perturbation.  


\textbf{Stochastic resonance} is a technique that resolves a quantized signal below the quantization level, by quantizing and ensembling perturbed versions of the signal \cite{benzi1981mechanism}. It has been used extensively in cochlear implants, where power constraints limit the resolution of the digital circuitry \cite{stocks2002application}. It has also been used to `super-resolve' Vision Transformer embeddings, where entire patches are encoded into a vector, which is computed at a coarsely subsampled grid \cite{lao2024sub}. In this paper, instead, we use Stochastic Resonance for the opposite purpose, {\em not} to super-resolve the quantized signal, but to perform latent ensembling to remove the effects of adversarial perturbations in the embedding.

%\textbf{Key idea:} 

Rather than averaging the data, or averaging their embedding as in classical denoising, {\em we average transformed embeddings} in latent space. This averaging is performed over small transformations sampled at random or deterministically from the group of planar translations, by computing the encoding of the transformed image, and then mapping the encoding back through the push-forward of the inverse transformation. This process can be expressed as a single formula in \eqref{eq:SR}. Since the embedding is typically computed on a coarse grid, but the transformations are sampled on the native lattice of the image, the resulting embedding is free of spatial averaging artifacts. As with other uses of Stochastic Resonance, the effect is seemingly paradoxical as {\em we combat noise with noise}: We apply purposeful perturbations to eliminate the effect of adversarial perturbations. 


%\textbf{Interpretation.} 
Our method can be thought of as marginalizing the translation group in latent space with respect to a chosen prior, which is the only design choice in our method. We choose the simplest, which is the constant prior. The purposeful perturbations alter the spatial sampling, and the implicit ensembling in latent space averages out the effect of sampling artifacts, thwarting the effect of adversarial perturbations. It is as if we were given multiple images with different `noise', except that the noise in question is not the adversarial perturbation, but the splinters of adversarial perturbations obtained by different spatially quantized versions of the perturbed image, due to the translational perturbations, which are then averaged out by the latent ensembling.

\textbf{Outcomes:} Our method fulfills the desiderata (i)-(iv) laid out earlier: (i) It does not require training or fine-tuning; (ii) it minimizes information loss by latent ensembling of perturbed embeddings; (iii) it can be applied to different network architectures and tasks, \emph{including} networks already equipped with different defense techniques like adversarial training, and (iv) is agnostic to the specific perturbation. To measure the effectiveness of our method in mitigating the effects of adversarial perturbations, we test it on three vastly different tasks, including image classification, and two other dense prediction tasks: stereo matching and optical flow.  
% compare it with other methods on [DESCRIBE DATASAETS AND BENCHMARKS]
where we are the first to show a significant and consistent improvement in robustness to various adversarial attacks. 


%\noindent\textbf{End-to-end (worst-case) adversary:} 
One could argue that there is still a cat-and-mouse game in our setting, if the adversary knows our technique and tailors the adversarial perturbations to bypass it. To assess this risk, we conduct ``worst-case'' adaptive tests to measure the performance of our method under adaptive attacks when the attacker knows the exact defense strategy, thus the adversarial perturbation is designed to maximally disrupt the result end-to-end, {\em including} our stochastic resonance. Our results show that the method is resistant to breaking even when the adversary optimizes adaptively through it end-to-end.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{Figures/diagram.pdf}
    \caption{\sl\small {\bf Defense against adversarial attacks via stochastic resonance.} Neural networks are highly sensitive to small perturbations in the input space, which adversarial attacks exploit to manipulate network outputs. Conventional defense strategies primarily focus on filtering out unreliable features or denoising either the input or the features. Instead of removing noise, we propose a novel defense by introducing noise. Based on stochastic resonance, controlled transformations are introduced to the input. Features are then aggregated after inverting these transformations. The resulting method can be applied exclusively at inference time, requires no training, and is compatible with diverse network architectures. Notably, it not only improves robustness against adversarial attacks but also increases the difficulty of crafting successful adversarial examples, even when the attacker is fully aware of whether and how stochastic resonance is being used (i.e. adaptive attacks).
}
    \label{fig:schematic}
    %\vspace{-2mm}
\end{figure*}

\section{Related Work}
\label{sec:related}

The literature on adversarial attack and defense is extensive. We highlight some of the advances. 

\noindent\textbf{Adversarial Training as Defense.}
Adversarial training increases the robustness of the model by training it with adversarially augmented images. The popular attack methods used are Fast Gradient Sign Method (FGSM) \cite{goodfellow2014explaining}, and Projected Gradient Descent (PGD) \cite{madry2017towards}. 
ALP \cite{kannan2018adversarial} Minimizes the difference between the logits of pairs of clean and adversarially augmented images. TRADES \cite{zhang2019theoretically} decomposes prediction error for adversarial examples into natural error and boundary error to improve adversarial robustness at the cost of accuracy. MART \cite{wang2019improving} improves adversarial robustness by considering misclassified natural examples during training. Subsequent work \cite{cai2018curriculum, zhang2020attacks, wang2022self, jin2022enhancing, ghiasvand2024robust} improves adversarial robustness with curriculum learning \cite{bengio2009curriculum}, model ensembling, second order statistics, and gradient tracking. 
% {\color{blue}Adversarial training on feature space}
On the other hand, some methods learn robust feature representation through a modified architecture or feature manipulation. \cite{galloway2019batch, benz2021batch, wang2022removing} investigate the effect of batch normalization on adversarial robustness. \cite{dhillon2018stochastic, madaan2020adversarial} prunes certain activations in the network that are susceptible to adversarial attacks.
\cite{xiao2019enhancing} keep k-features with the largest magnitude and deactivate everything else.
\cite{zoran2020towards} uses an attention mask to highlight robust regions on the feature.
Feature Denoising (FD) \cite{xie2019feature} uses classical denoising techniques to deactivate abnormal activations.
\cite{bai2021improving, yan2021cifs} proposed Channel Activation Suppression (CAS) and Channel-wise Importance-based Feature Selection (CIFS) to deactivate feature channels that are vulnerable to attacks. 
\cite{kim2023feature} improves the robustness with Feature Separation and Recalibration (FSR).
Our method also operates in feature space, but purely during test time. While we choose some of these works as baselines, our method works in conjunction with any aforementioned methods.

% WE ARE NOT AGAINST THEM, SR CAN BE COMBINED WITH ANY OF THEM]

\noindent\textbf{Adversarial Purification as Defense}
Another line of work focuses on purifying or augmenting the images before they are used as input. \cite{tang2024robust, yeh2024test, tsai2023test}  train an FGSM robust classifier, a diffusion model, or a mask auto-encoder, respectively, to purify adversarial examples.
\cite{wang2021fighting} optimizes both the model and the input to minimize the entropy of model predictions to adapt to changing attacks. 
\cite{cohen2024simple} trains a random forest predictor to ensemble outputs from test-time augmented images.
These works involve training a new model or updating the original model, while our method is purely test-time and does not require any training. 
\cite{perez2021enhancing} ensembles model output from different augmentations, which is a special case of our method, as the ensemble is performed solely on the output, while we can ensemble at any layer, which both saves computational cost and achieves higher performance

%, and empirical results demonstrate advantage over output ensemble.

Notably, we recognize that above methods focus solely on classification as a task for adversarial attacks. Through extensive experiments, we demonstrate that our method can not only perform well on classification, but also on dense prediction tasks like optical flow, and stereo matching.


\noindent\textbf{Stochastic Resonance (SR)} was proposed by \cite{benzi1981mechanism} and first applied in climate dynamics \citep{benzi1982stochastic} and later in signal processing \citep{wellens2003stochastic, kosko2001robust, chen2007theory} and acoustics \citep{shu2016application, wang2014adaptive}. Recently, Stochastic Resonance Transformer (SRT) \cite{lao2024sub} uses SR to ``super-resolve'' Vision Transformer (ViT) embeddings. In this work, we instead use SR to mitigate adversarial perturbations. Since SR has been developed specifically to address {\em quantization artifacts}, it has never before been used to mitigate classes of perturbations beyond aliasing. Our novel use of the technique leverages the fact that group transformations and spatial quantization preserve the statistics of natural images, which are heavy-tailed, but do not preserve the statistics of adversarial perturbations.

\section{Method}
\label{sec:method}

\noindent\textbf{Notation.}
A digital image $x \in [0, \ L-1]^{W\times H}$ can be described as a map from a discrete planar lattice $\Lambda \subset {\mathbb R}^2$ with $H$ rows and $W$ columns to $L$ discrete levels, $x:\Lambda \rightarrow [0, \ L-1]$; a `feature' or `embedding' of an image $x$ is the output of an encoder $\phi$ that maps it to a vector space with $K$ channels, typically through a parametric trained model:
\[\phi: x \mapsto \phi(x) \in {\mathbb R}^K.\]
We represent a group transformation of the image through an operator $g: {\mathbb R}^2 \rightarrow {\mathbb R}^2$, which can be restricted to the lattice $\Lambda$ through padding, sampling and quantization at the expense of invertibility: 
\[
g:  x \mapsto g (x) \in \Lambda \subset {\mathbb R}^2.
\]
For example, a translation by an integer pixel can be represented by an upper diagonal matrix $G$, $g(x) = Gx$ with ones above the diagonal.  The group $g$ operating on $x$ induces an operation on $\phi$ via
\[
g_* \phi(x) \doteq \phi(g(x)).
\]
We call the composition of $\phi$ and $g$ the encoding of the transformed image 
\[
\psi(x) \doteq \phi(g(x)) = \phi\circ g(x).
\]
The main object of interest in our method is:
\[
g_*^{-1} \psi(x) = g_*^{-1} \circ \phi \circ g (x).
\]
This is obtained, reading right-to-left, by first transforming the image, then passing it through an encoder, and then transforming back the feature map through the push-forward action $g^{-1}_*$.

\noindent\textbf{Perturbations.}
We consider two types of perturbations, extraneous and purposeful. The extraneous one could be an additive perturbation to an image, $\tilde x = x + n$, designed to maximally change the value of the embedding (adversarial perturbation) $\phi(\tilde x)$:
\[
\tilde x = x + n(x) \quad | \quad n(x) = \arg\max d(\phi(x), \phi(\tilde x)),  \   |n|< \epsilon
\]
for some small $\epsilon$  designed so the perturbation is, ideally, imperceptible by humans. %For digital images, these perturbations turn out to resemble high-frequency aliasing easily discounted as artifacts by the human visual system on account of their poor fit with the statistics of natural images. 

The purposeful perturbations are small group actions $g_i, \ i = 1, \dots, N$, which could be sampled deterministically or at random according to some chosen distribution $g_i \sim P_g$, either way yielding a set $
\{g_1(x), \dots, g_N(x)\}$. 
Our goal is to use these purposeful perturbations to combat the effects of extraneous adversarial perturbations.

\noindent\textbf{Averaging, smoothing, and stochastic resonance.}
The resemblance between adversarial perturbations and aliasing artifacts has motivated the use of anti-aliasing, or smoothing, techniques to mitigate them. These consist of {\em spatial averaging} of the data prior to computing  the map $\phi$. If we call $B^\sigma_{ij}$ a neighborhood of size $\sigma>0$ around $(i,j)\in \Lambda$, 
\[
B^\sigma_{ij} = \{(i', j') \in \Lambda \ | \ d((i,j),(i',j')) \le \sigma\} 
\]
then the simplest form of smoothing is simply averaging in a neighborhood, 
\[
\bar x_{i,j} = \frac{1}{\sigma^2} \sum_{(i',j') \in B^\sigma_{i,j}}x_{i'j'}
\]
which we write in terms of translations $g(x_{i,j}) = x_{i+u, j+v}$ within the same neighborhood $B^\sigma$,
\[
\bar x = \frac{1}{N} \sum_{i=1}^N g_i(x).
\]
One can then obtain an encoding by %embedding the smoothed image, $\phi(\bar x)$, or 
smoothing the embedding
\[
\bar \phi(x) = \frac{1}{N}\sum_{i = 1}^N \phi(g_i(x)).
\]
This can be interpreted as marginalizing the translation group with a prior $P_g$ when computing $\phi$. Notice that the average can be computed on a coarser lattice $\tilde \Lambda$, but its value still depends on data in the finer grid $\Lambda$. 
Classical Sampling Theory teaches that smoothing mitigates the effect of high-frequency aliasing $n$ at the cost of information loss on $x$.

Stochastic resonance also marginalizes the translation group, but by {\em averaging transformed data in latent space:}
\begin{equation}
\hat \phi(x) = \frac{1}{N} \sum_{i=1}^N {g_i}_*^{-1}\circ \phi \circ g_i(x).
\label{eq:SR}
\end{equation}
More general groups, and more general averaging kernels, can be considered although we find that the simplest case described here already suffices. 

Stochastic resonance is {\em not} smoothing, as it averages transformed versions of the image without blurring it, thanks to the inverse push-forward. It is also {\em not} super-resolution, where fine-granularity details are hallucinated based on side information or priors, although it does allow resolving features computed on a coarse grid $\tilde \Lambda$ within a finer grid $\Lambda$. Stochastic resonance uses the averaging of perturbations in latent space to ensemble populations of embeddings, rather than averaging or interpolation of the same embedding. 

\noindent\textbf{Purposeful Perturbation.} The only design choice in the method is the set of purposeful perturbations. While that can be optimized for performance, we optimize for simplicity, restricting our attention to translation by integer pixels. 
We know that, for adversarial perturbations, $d(\phi(\tilde x), \phi(x))$ is large, where $d(\cdot)$ defines the distance between features. Ideally, for stochastic resonance, we want $d(\hat \phi(\tilde x), \hat \phi(x)) = 0$ %. That is easy to achieve with a constant function. We also want
while keeping $\hat \phi$ as information-preserving as possible. 
The theory of Stochastic Resonance shows that, if we sub-sample a signal from its native granularity $\Lambda$ to a coarser grid $\tilde \Lambda \subset \Lambda$, and choose the purposeful perturbations to act on the finer grid $\Lambda$, under certain conditions one can recover the original signal at the finer granularity \cite{benzi1981mechanism}. 
In this paper, we focus on testing whether $\hat \phi$ is insensitive to adversarial perturbations. We do so empirically in Sect. \ref{sec:experiments}.

\def\figd{Figures/Cifar}
\def\fWidD{0.2\textwidth}
\begin{figure*}[t]

\centering
\hspace*{-4mm}
{\scriptsize
\begin{tabular}[0mm]{c@{\hskip 0.01in}c@{\hskip 0.01in}c@{\hskip 0.01in}c@{\hskip 0.01in}c}
\includegraphics[width=\fWidD]{\figd/1}&
\includegraphics[width=\fWidD]{\figd/2}&
\includegraphics[width=\fWidD]{\figd/3}&
\includegraphics[width=\fWidD]{\figd/4}&
\includegraphics[width=\fWidD]{\figd/5}
\end{tabular}
}
\caption{\sl\small {\bf Results on CIFAR-10 under varying levels of stochastic resonance.} Increasing the stochastic resonance level consistently enhances robustness across all settings, yielding clear gains over the baseline method (FSR). Notably, our approach achieves superior performance even under adaptive adversarial attacks (Ours-WorstCase), despite the baseline being evaluated only in the non-adaptive case.}

%Although higher levels of stochastic resonance demand greater computational resources, this overhead can be alleviated through parallelization. This tunable parameter thus enables a controlled trade-off between enhanced robustness and computational burden, all achieved without additional training.

\label{fig:cifar}
\end{figure*}






\section{Experiments}
\label{sec:experiments}
While $g$ can be sampled from any invertible group transformation (e.g., rotation, scaling), we implement stochastic resonance using integer-pixel translations, denoted as $\{g_i\}=\{(x, y) | x \in [-d_x, d_x], j \in [-d_y, d_y]\}$, following the approach in SRT \cite{lao2024sub}, which avoids interpolation artifacts. While the networks' inherent sensitivity to pixel-level shifts is typically regarded as detrimental due to the ``flickering problem'' \cite{azulay2019deep, sundaramoorthi2019translation}, our approach, on the contrary, leverages it to defend against adversarial perturbations.
% as an advantage in enhancing feature robustness.
Given these perturbations $\{g_i\}$, ensembling can be performed at any chosen layer of the network. Features are aggregated as described in Eq. \ref{eq:SR} and then passed to downstream network components. 

We first validate this approach on image classification (Sect.~\ref{sec:classification}) following standard benchmarks, and also provide ablation studies on levels/layers of latent ensemble and rotation as augmentation. Subsequently, we defend against adversarial attacks on dense prediction tasks, including stereo matching (Sec.~\ref{sec:stereo}) and optical flow (Sec.~\ref{sec:flow}). This is achieved given that our method is agnostic to attack mechanisms, network pre-training, and largely independent of architecture, requiring only latent ensembling, resulting in a purely test-time, training-free approach with no auxiliary modules.



\subsection{Image Classification}
\label{sec:classification}


\begin{table*}[t]
\footnotesize
\setlength{\tabcolsep}{0.8pt} 
  \centering
  \begin{tabular}{l|l|>{\centering}p{14mm}|>{\centering}p{14mm}|>{\centering}p{14mm}|>{\centering}p{14mm}|>{\centering}p{14mm}||c}
Method&Architecture& Natural & FGSM& PGD-20& PGD-100& C\&W& Ensemble \\\hline\hline
AT&\multirow{8}{*}{ResNet-18}&85.02 & 56.21 & 48.22 & 46.37 & 47.38 & 45.90 \\
+ FD&&85.14&56.81&48.54&46.70&47.72&45.82 \\
+ CAS&&\textbf{85.78}&55.57&50.42&49.91&53.47&46.46 \\
+ CIFS&&79.87&56.53&49.80&48.17&49.89&47.26 \\
+ FSR&& 81.46 & 58.07 & 52.47 & 51.02 & 49.44 & 48.34 \\
+ TTE&& 85.25 & 59.20 & 53.00 &51.65 &52.45&50.60\\
\rowcolor{gray!20}+ Ours&& 84.93&\textbf{61.02}&\textbf{56.08}&	\textbf{55.17}&	\textbf{55.53}&	\textbf{53.68}\\
\rowcolor{gray!20}+ Ours-WorstCase&&84.93& 58.81 & 53.58 & 52.39 & 52.73 & 50.95 \\
%+ SR-Out&&84.99&60.92&\textbf{56.27}&&52.71&51.19\\
%+ SR-Out-WorstCase&&84.99&57.59&52.40&51.25&52.71&51.19\\
\hline
TRADES&&84.92&60.87&56.13&55.16&54.02&53.38 \\
\rowcolor{gray!20}+Ours &\multirow{-2}{*}{WideResNet-34}& \textbf{85.03}&\textbf{62.43}&\textbf{58.64}&\textbf{57.87}&\textbf{57.18}&\textbf{56.28}\\\hline
MART&& \textbf{83.07}&60.21&54.14&52.90&49.62&48.95\\
\rowcolor{gray!20}+ Ours &\multirow{-2}{*}{ResNet-18}& 82.70&\textbf{62.62}&\textbf{59.03}&\textbf{58.13}&\textbf{55.51}&\textbf{54.61}
\end{tabular} %\vspace{-1mm}
  \caption{\sl\small{\bf Defense against adversarial attacks on classification task (CIFAR-10).} Compared to baselines that filter or manipulate features, ours does not modify network architecture or weights. Instead, ours performs an ensemble in the feature space. On CIFAR-10, ours achieves state-of-the-art robustness without requiring any additional training. Moreover, even in a worst-case adaptive adversary setting where the attacker is fully aware of the defense and how stochastic resonance is applied, the effectiveness of adversarial attacks is still notably reduced, while the computational cost for executing such attacks is significantly increased.}
     \label{tab:cifar10}
     %\vspace{-2mm}
\end{table*}

\begin{table}[t]
\scriptsize
\parbox{.49\linewidth}{
\centering
\setlength{\tabcolsep}{0.8pt} 
% ---------------- LEFT COLUMN ----------------
  \begin{tabular}{l|c|c|c|c|c|c|c|c}
Att. Strength ($\epsilon$)&\multicolumn{2}{c|}{8/255}&\multicolumn{2}{c|}{4/255}&\multicolumn{2}{c|}{2/255}&\multicolumn{2}{c}{1/255}\\
\hline
Metric&Top-1&Top-5&Top-1&Top-5&Top-1&Top-5&Top-1&Top-5\\\hline
No Defense&4.51&19.55&12.25&42.15&29.79&65.09&48.09&78.13\\
Ours ($d=1$)&11.66&46.27&27.85&65.80&45.94&77.63&57.48&83.43\\
Ours ($d=2$)&18.78&58.58&36.42&72.86&51.58&80.85&60.41&84.77\\
Ours ($d=3$)&\textbf{25.77}&\textbf{65.88}&\textbf{42.52}&\textbf{76.75}&\textbf{54.94}&\textbf{82.64}&\textbf{62.08}&\textbf{85.50}
    \end{tabular} %\vspace{-1mm}
  \captionof{table}{\small{\bf ImageNet with ViT-Small.}  Increasing the level of stochastic resonance consistently improves both Top-1 and Top-5 accuracy under adversarial attacks. Relative to the clean baseline (72.9 Top-1, 92.91 Top-5), our method recovers up to 55.8\% of the Top-1 accuracy loss and 68.1\% of the Top-5 accuracy loss.}
  \label{tab:ViT}
}
\hfill
\parbox{.49\linewidth}{
\centering
\setlength{\tabcolsep}{0.8pt} 
  \begin{tabular}{l|c|c|c|c|c|c|c|c}
Att. Strength ($\epsilon$)&\multicolumn{2}{c|}{8/255}&\multicolumn{2}{c|}{4/255}&\multicolumn{2}{c|}{2/255}&\multicolumn{2}{c}{1/255}\\
\hline
Metric&Top-1&Top-5&Top-1&Top-5&Top-1&Top-5&Top-1&Top-5\\\hline
No Defense&8.51&26.19&9.45&27.92&11.10&30.63&15.21&36.76\\
Initial Conv.&16.91&34.66&17.36&35.28&18.12&36.30&19.59&38.25\\
Res. Block 1&22.44&44.95&23.39&46.03&24.84&47.64&27.57&50.80\\
Res. Block 2&\textbf{24.88}&\textbf{48.54}&\textbf{25.90}&\textbf{49.66}&\textbf{27.23}&\textbf{51.19}&\textbf{30.11}&\textbf{54.15}\\
Res. Block 3&21.76&44.28&22.73&45.47&24.14&47.14&27.18&50.20
  \end{tabular} %\vspace{-1mm}
  \captionof{table}{\small{\bf Layer-wise ablation on ResNet-50.} Adversarial perturbations resemble high-frequency noise. Applying stochastic resonance through shallow layers is sufficient to defend against adversarial attacks, substantially reducing the overall computational cost.}
  \label{tab:layers}
  } %\vspace{-2mm}
\end{table}

\noindent\textbf{CIFAR-10. }
We evaluate our method on CIFAR-10 \cite{krizhevsky2009learning}, building upon the standard and publicly available code base of FSR \cite{kim2023feature} %\footnote{https://github.com/wkim97/FSR} 
and accompanying evaluation protocol. We apply stochastic resonance to networks pre-trained with AT \cite{madry2017towards}, TRADES \cite{zhang2019theoretically}, and MART \cite{wang2019improving}, using publicly released weights without any modification. Our method operates with these methods purely at test time without any training. We conduct experiments on ResNet-18 \cite{he2016deep} and WideResNet-34 \cite{zagoruyko2016wide}, depending on the availability of author-released pre-trained weights. In all cases, feature ensembling is performed before the final linear layer.
Furthermore, we consider a worst-case adaptive adversary setting, where the attacker has full knowledge of the model weights, and knows every stochastic resonance transformation by accessing \emph{every single} forward pass and its gradients. 

Fig.~\ref{fig:cifar} shows the results varying different levels of stochastic resonance. Increasing the resonance level consistently enhances robustness in all attack settings, leading to substantial improvements over the baseline method (FSR). Importantly, our approach surpasses the baseline even under adaptive adversarial attacks. 
Additionally, we compare our method against multiple baselines, including feature-level manipulation methods (FD \cite{xie2019feature}, CAS \cite{bai2021improving}, CIFS \cite{yan2021cifs}) and ensemble-based approach TTE \cite{perez2021enhancing}. The results, summarized in Tab.~\ref{tab:cifar10}, demonstrate that stochastic resonance consistently outperforms all baselines across different attacks. Even in the worst-case adaptive attack scenario, where the adversary accounts for all stochastic resonance forward passes, the model remains significantly more robust than the baseline methods. In addition, stochastic resonance increases attack complexity under the adaptive settings, making adversarial noise generation more challenging for the attacker. As a result, the computational cost for generating adaptive adversarial perturbations increases substantially. For example, with stochastic resonance, 8x more wall-clock time is required to create adversarial examples with PGD-100.


\noindent\textbf{ImageNet.}
We further evaluate our approach on the ImageNet \cite{deng2009imagenet} classification dataset using standard segmentation backbones, including ResNet-50 \cite{he2016deep} and Vision Transformer \cite{dosovitskiy2020image}, without adversarial training. As in the CIFAR experiments, we vary the level of stochastic resonance and conduct ablation studies by testing against PGD attacks of different strengths. Tab.~\ref{tab:ViT} reports the results for ViT-Small. Consistent with the CIFAR-10 findings, increasing the resonance level leads to consistent improvements in robustness, as measured by both Top-1 and Top-5 accuracy. Notably, the vanilla ViT-Small model without attack achieves 72.9 (Top-1) and 92.91 (Top-5), which means our method recovers the accuracy drop under adversarial attacks by up to a relative 55.8\% (Top-1, when $\epsilon=4/255$) and 68.1\% (Top-5, when $\epsilon=2/255$). We further evaluate our method on ResNet-50 and observe a consistent trend, as shown in Tab.~\ref{tab:Rotation}. 



\begin{wraptable}{r}{0.5\textwidth}
\vspace{-4mm}
\setlength{\tabcolsep}{0.8pt} 
\scriptsize
  \centering
  \begin{tabular}{l|c|c|c|c|c|c|c|c}
Att. Strength ($\epsilon$)&\multicolumn{2}{c|}{8/255}&\multicolumn{2}{c|}{4/255}&\multicolumn{2}{c|}{2/255}&\multicolumn{2}{c}{1/255}\\
\hline
Metric&Top-1&Top-5&Top-1&Top-5&Top-1&Top-5&Top-1&Top-5\\\hline
No Defense&8.51&26.19&9.45&27.92&11.10&30.63&15.21&36.76\\
\rowcolor{gray!20}Ours ($d=1$)&18.40&40.36&19.54&42.65&21.02&43.86&24.66&47.81\\
\rowcolor{gray!20}w/ Rotation&20.01&41.32&20.84&42.39&22.10&43.89&24.72&46.89\\
Ours ($d=2$)&20.01&42.22&21.02&43.45&22.44&45.32&25.80&48.97\\
w/ Rotation&18.86&38.67&19.55&39.5&20.36&40.48&22.01&42.50\\
\rowcolor{gray!20}Ours ($d=3$)&\textbf{21.17}&\textbf{43.58}&\textbf{22.04}&\textbf{44.78}&\textbf{23.44}&\textbf{46.53}&\textbf{26.58}&\textbf{49.73}\\
\rowcolor{gray!20}w/ Rotation&15.15&32.89&15.61&33.46&16.17&34.19&17.33&35.59

\end{tabular}
  \caption{\small{\bf Stochastic resonance using translation v.s. rotation on ResNet-50.} While rotations provide similar gains at low resonance levels, performance degrades as the resonance level increases, likely due to the lack of rotational invariance in convolutional filters.}
     \label{tab:Rotation}\vspace{-2mm}
\end{wraptable}

We also explored group transformations other than translation, e.g. rotations in Tab.~\ref{tab:Rotation}. For a fair comparison, we use the same number of augmentations as in the translation experiments. Rotations behave similarly to translations at low levels of stochastic resonance, but performance degrades as the resonance level increases. We hypothesize that since convolutional filters are inherently translation-invariant but not rotation-invariant, aligning features under different rotations reduces feature quality. Moreover, rotations are approximately 30\% slower due to interpolation overhead.

Since adversarial perturbations often manifest as high-frequency noise, having a low-pass filter in early layers may form an effective defense. As our method applies to arbitrarily chosen layers, we vary the termination layer of stochastic resonance. As shown in Tab.~\ref{tab:layers}, applying it only through the first residual block already achieves strong adversarial robustness, while extending it to the second block yields the strongest result. This finding is significant: running stochastic resonance through shallow layers can be sufficient as a defense strategy, which reduces overall computational cost.

\begin{table*}[t]
\setlength{\tabcolsep}{0.8pt} 
\footnotesize
  \centering
  \begin{tabular}{c|l|c|c|c|c|c|c|c|c|c|c|c|c}
  &Attack Strength ($\epsilon$)&\multicolumn{3}{c|}{0.02}&\multicolumn{3}{c|}{0.01}&\multicolumn{3}{c|}{0.005}&\multicolumn{3}{c}{0.002}\\\cline{2-14}
&Metric&\,MAE\,&RMSE&D1-err&\,MAE\,&RMSE&D1-err&\,MAE\,&RMSE&D1-err&\,MAE\,&RMSE&D1-err\\\hline
{\multirow{5}{*}{\rotatebox[origin=c]{90}{FGSM}}} &No Defense&14.83&24.10&97.33&8.49&14.53&90.49&5.05&7.70&74.71&3.01&3.49&38.12 \\
&Latent Smoothing&13.42&22.09&96.61&8.12&13.69&89.25&4.89&7.05&73.01&2.89&3.32&36.25
\\
&Ours ($d=1$)&10.12&15.81&95.80&6.46&9.92&86.30&4.39&5.93&68.55&2.78&3.18&31.99 \\
&\cellcolor{gray!20}Error Reduced (\%)& \cellcolor{gray!20}31.76&\cellcolor{gray!20}34.40&\cellcolor{gray!20}1.57&\cellcolor{gray!20}23.91&\cellcolor{gray!20}31.73&\cellcolor{gray!20}4.63&\cellcolor{gray!20}13.07&\cellcolor{gray!20}22.99&\cellcolor{gray!20}7.58&\cellcolor{gray!20}7.64&\cellcolor{gray!20}8.88&\cellcolor{gray!20}16.08\\
&Ours ($d=2$)&\textbf{9.22}&\textbf{13.88}&\textbf{94.61}&\textbf{6.13}&\textbf{8.87}&\textbf{84.49}&\textbf{4.19}&\textbf{5.43}&\textbf{66.74}&\textbf{2.73}&\textbf{3.12}&\textbf{31.18}\\
&\cellcolor{gray!20}Error Reduced (\%)&\cellcolor{gray!20} 37.82&\cellcolor{gray!20}42.40&\cellcolor{gray!20}2.79&\cellcolor{gray!20}27.78&\cellcolor{gray!20}38.95&\cellcolor{gray!20}6.63&\cellcolor{gray!20}16.92&\cellcolor{gray!20}29.41&\cellcolor{gray!20}10.66&\cellcolor{gray!20}9.21&\cellcolor{gray!20}10.65&\cellcolor{gray!20}18.21 \\\hline
{\multirow{5}{*}{\rotatebox[origin=c]{90}{PGD}}} &No Defense&161.70&162.61&99.99&131.66&140.55&98.64&63.97&88.55&85.31&6.83&17.03&39.24 \\
&Latent Smoothing&161.79&162.69&99.99&131.46&140.41&98.57&63.86&88.21&85.39&7.28&17.44&39.81\\
&Ours ($d=1$)&107.86&125.14&98.29&69.97&84.72&91.79&20.66&42.29&73.21&4.17&8.32&29.09\\
&\cellcolor{gray!20}Error Reduced (\%)&\cellcolor{gray!20}33.30 &\cellcolor{gray!20} 23.04 &\cellcolor{gray!20} 1.70 &\cellcolor{gray!20} 46.86 &\cellcolor{gray!20} 39.72 &\cellcolor{gray!20} 6.94 &\cellcolor{gray!20} 67.70 &\cellcolor{gray!20} 52.24 &\cellcolor{gray!20} 14.18 &\cellcolor{gray!20} 38.95 &\cellcolor{gray!20} 51.15 &\cellcolor{gray!20} 25.87\\
&Ours ($d=2$)&\textbf{77.59}&\textbf{100.14}&\textbf{96.14}&\textbf{44.77}&\textbf{66.23}&\textbf{89.24}&\textbf{17.98}&\textbf{32.80}&\textbf{71.09}&\textbf{3.76}&\textbf{6.73}&\textbf{28.44}\\
&\cellcolor{gray!20}Error Reduced (\%)&\cellcolor{gray!20} 52.01 &\cellcolor{gray!20} 38.41 &\cellcolor{gray!20} 3.85 &\cellcolor{gray!20} 66.02 &\cellcolor{gray!20} 52.87 &\cellcolor{gray!20} 9.53 &\cellcolor{gray!20} 71.89 &\cellcolor{gray!20} 62.94 &\cellcolor{gray!20} 16.66 &\cellcolor{gray!20} 44.99 &\cellcolor{gray!20} 60.52 &\cellcolor{gray!20} 27.53
\end{tabular}
  \caption{\sl\small{\bf Stochastic Resonance Enhances Stereo Matching Robustness.} Incorporating stochastic resonance significantly reduces prediction errors induced by adversarial attacks across all evaluation metrics, reducing error by up to 71.89\% (MAE, when attacked by PGD with $\epsilon=0.005$). Notably, this defense mechanism operates entirely at test time without requiring any model re-training, which sets it apart from existing methods.}
     \label{tab:stereo}%\vspace{-2mm}
\end{table*}



\subsection{Stereo Matching}\label{sec:stereo}
Stereopagnosia \cite{wong2021stereopagnosia} first introduced adversarial attacks to stereo matching , yet no test-time method has demonstrated an effective defense. The primary challenge arises from the infeasibility of data augmentations, as they risk altering the physics of the input, leading to incorrect estimation. In contrast, our feature-level ensemble is suitable for this task, as transformations introduced by stochastic resonance are ``undone" in the latent space, ensuring that features remain aligned precisely with the original input. This process mirrors AugUndo \cite{wu2024augundo} conceptually.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{Figures/Stereo.pdf}%\vspace{-2mm}
    \caption{\sl\small {\bf Stereo matching robustness via stochastic resonance.} We present visual results on stereo matching under various adversarial attack scenarios, including PGD and FGSM at different perturbation levels. These attacks significantly degrade the network's predictions, leading to substantial errors. By incorporating stochastic resonance, we demonstrate a significant reduction in prediction errors. This technique holds significant potential for improving robustness in safety-critical real-world applications, such as autonomous driving, where stereo vision must remain reliable under diverse environmental conditions and adversarial threats.
}
    \label{fig:stereo}%\vspace{-2mm}
\end{figure*}

We evaluate our method on the standard benchmark used in \emph{Stereopagnosia}, derived from KITTI~\cite{geiger2012we}. Experiments are conducted using FGSM and PGD attacks against a pre-trained PSMNet~\cite{chang2018pyramid}. Since no existing test-time defense is available, we adopt latent-space smoothing as a baseline. As shown in Tab.~\ref{tab:stereo}, both attacks corrupt network predictions, with PGD proving substantially more effective due to its iterative nature. Nevertheless, stochastic resonance consistently improves robustness across attack strengths. In particular, under PGD with $\epsilon=0.005$, our method reduces errors by up to 71.89\% in terms of MAE. Crucially, these gains are achieved entirely at test time, without additional training or prior knowledge of the attack.


Fig.~\ref{fig:stereo} provides qualitative results, featuring different attack strengths and methods. The adversarial perturbations introduce significant distortions, as indicated by bright regions in the visualized predictions. Stochastic resonance effectively mitigates these distortions, drastically reducing prediction errors. This experiment is particularly relevant for safety-critical applications such as autonomous driving, where adversarial disturbances can arise not only from malicious attacks but also from environmental factors such as adverse weather conditions, varying illumination, or sensor degradation. While some  defenses \cite{zhang2023weatherstream, cheng2021towards, cheng2022revisiting, berger2022stereoscopic} have been proposed to train a more robust network under adverse conditions, test-time defenses remain largely unexplored. Our method is the first to provide a viable solution in this setting.






\subsection{Optical Flow}
\label{sec:flow}



We further evaluate our method on optical flow, which computes the dense motion field between two images. The accuracy of optical flow is measured using the End-Point Error (EPE). We note that multiple adversarial attacks exist for optical flow \cite{schrodi2022towards,Ranjan_2019_ICCV, schmalfuss2022perturbation}. Among these attacks, PGD is stronger than patch-based attacks as adversarial patches have localized effects. Therefore, in our experiments, we employ the RAFT \cite{teed2020raft} %\footnote{https://github.com/princeton-vl/RAFT} 
optical flow model and focus on global adversarial perturbations generated by PGD and FGSM.  We test our method on the DAVIS \cite{pont20172017} dataset under both attacks.

%As in the first row of Fig.~\ref{fig:flow-quality} (with amplified noise for better visibility), the attacks introduce fine-grained, structured perturbations that effectively mislead the network, particularly in regions where the color intensity is consistent, making it difficult to establish correct correspondences. This leads to strong errors in the predicted flow fields, as seen in the attacked flow predictions.

To defend against adversarial attacks, we apply stochastic resonance to the convolutional feature extractor of RAFT. Since the perturbation is applied only at the feature extraction stage, no additional overhead is introduced in the computationally intensive matching module. Quantitative results (Fig.~\ref{fig:flow-quantitative}) show that increasing Stochastic Resonance reduces EPE, which aligns with our findings in classification.
As in Fig.~\ref{fig:flow-quality}, our approach effectively removes errors caused by adversarial noise.

\def\figd{Figures/RAFT}
\def\fWidD{0.27\textwidth}
\begin{wrapfigure}{r}{0.5\textwidth}
%\vspace{-4mm}
\centering
{
\footnotesize
\begin{tabular}{c@{\hspace{0.01in}}c}
\hspace*{-5mm}
\includegraphics[width=\fWidD]{\figd/RAFT_PGD.png}
&\includegraphics[width=\fWidD]{\figd/RAFT_FGSM.png} 
\end{tabular}
\caption{\sl\small {\bf Enhanced optical flow robustness with stochastic resonance.} Under PGD and FGSM, stochastic resonance significantly reduces endpoint error in optical flow estimation. Notably, our method performs ensembling in the latent feature space rather than the output space, providing greater flexibility. While ensembling in the output space offers minor performance gains, our approach consistently achieves superior robustness across all levels of stochastic resonance.}
\label{fig:flow-quantitative}
}

\end{wrapfigure}



We further compare our method to an alternative ensembling approach that aggregates predictions in the output space, conceptually similar to TTE \cite{perez2021enhancing}. In this variant, we apply the same stochastic transformations but instead ensemble at the output level rather than in the feature space. While this method provides marginal improvements, it remains less effective than our approach. This finding highlights the advantage of having the freedom to choose from any stage of the model to perform ensemble. In this particular experiment, we demonstrate that ensembling solely at the image encoding sub-module, while leaving the rest of the RAFT network unchanged, yields substantial improvements in robustness, thanks to the flexibility of our method.



\def\figd{Figures/Flow}
\def\fWidD{0.15\textwidth}
\begin{figure*}[t]
%\vspace{-2mm}
\renewcommand{\arraystretch}{0.2} % Default value: 1
\centering
\hspace*{-4mm}
{\scriptsize
\begin{tabular}[0mm]{c@{\hskip 0.01in}c@{\hskip 0.01in}c@{\hskip 0.01in}c|c@{\hskip 0.01in}c@{\hskip 0.01in}c}
& \multicolumn{3}{c}{\normalsize PGD}& \multicolumn{3}{c}{\normalsize FGSM}\\\\
\smash{\rotatebox{90}{\quad Image}}&
\includegraphics[width=\fWidD]{\figd/pgd1}&
\includegraphics[width=\fWidD]{\figd/pgd2}&
\includegraphics[width=\fWidD]{\figd/pgd3}&
\includegraphics[width=\fWidD]{\figd/fgsm1}&
\includegraphics[width=\fWidD]{\figd/fgsm4}&
\includegraphics[width=\fWidD]{\figd/fgsm3}\\
\smash{\rotatebox{90}{Original}}&
\includegraphics[width=\fWidD]{\figd/pgd_org1}&
\includegraphics[width=\fWidD]{\figd/pgd_org2}&
\includegraphics[width=\fWidD]{\figd/pgd_org3}&
\includegraphics[width=\fWidD]{\figd/fgsm_org1}&
\includegraphics[width=\fWidD]{\figd/fgsm_org4}&
\includegraphics[width=\fWidD]{\figd/fgsm_org3}\\
\smash{\rotatebox{90}{Attacked}}&
\includegraphics[width=\fWidD]{\figd/pgd_attacked1}&
\includegraphics[width=\fWidD]{\figd/pgd_attacked2}&
\includegraphics[width=\fWidD]{\figd/pgd_attacked3}&
\includegraphics[width=\fWidD]{\figd/fgsm_attacked1}&
\includegraphics[width=\fWidD]{\figd/fgsm_attacked4}&
\includegraphics[width=\fWidD]{\figd/fgsm_attacked3}\\
\smash{\rotatebox{90}{\quad Ours}}&
\includegraphics[width=\fWidD]{\figd/pgd_ours1}&
\includegraphics[width=\fWidD]{\figd/pgd_ours2}&
\includegraphics[width=\fWidD]{\figd/pgd_ours3}&
\includegraphics[width=\fWidD]{\figd/fgsm_ours1}&
\includegraphics[width=\fWidD]{\figd/fgsm_ours4}&
\includegraphics[width=\fWidD]{\figd/fgsm_ours3}\\
\end{tabular}
}
\caption{\sl\small{\bf Optical flow robustness via stochastic resonance.} 
Qualitative results (visualized with a color wheel) show that our method substantially mitigates the degradation caused by both PGD and FGSM attacks. This robustness is particularly relevant for visual perception systems that rely on accurate motion estimation.}
\label{fig:flow-quality}%\vspace{-2mm}
\end{figure*}

\section{Discussion and Conclusion}\label{sec:discussion}

\noindent\textbf{Speed.} 
Stochastic resonance incurs low computational overhead when executed in parallel: raising the stochastic resonance level to 3 with ResNet-50 adds only 0.06 seconds to the inference time on an NVIDIA 1080Ti GPU. Most of the overhead arises from creating perturbations implemented via Python loops with \texttt{torchvision}; we expect further speedups with efficient CUDA implementations. Even when executed \emph{sequentially}, the computational overhead is 0.095 seconds.  
Moreover, strong robustness can be attained by applying stochastic resonance only to shallow layers, offering substantially greater efficiency than existing ensemble-based defenses (e.g. \cite{perez2021enhancing}) that require multiple passes through the entire network. Moreover, our method is fully plug-and-play. In contrast, attack-specific adversarial training is over 6x slower than a vanilla training pipeline. As such, the computation of our method is well justified by its robustness gains and training-free nature.

\noindent\textbf{On-demand scaling.}
One of the key strengths of our approach is its flexibility: providing a trade-off between robustness and computational cost. We offer a tunable “knob” that allows practitioners to adjust the level of resilience based on available resources on the fly: when the system has more computational capacity, add a higher level of stochastic resonance, vice versa. Note that, such a design does not \emph{rely} on additional computation, yet more computation can bring \emph{extra} performance. Moreover, our experiments show that the method generalizes across a wide range of tasks and architectures that include an encoder. This on-demand scaling mirrors inference-time scaling in language models, where performance can be improved without modifying the underlying pre-trained model.

\noindent\textbf{Limitations. }
Despite its strengths, our method has some limitations. First, while we offer parallel computation as a remedy, the computational overhead introduced by stochastic resonance may not be negligible for scenarios with memory and power constraints. Also, our current study focuses on integer-pixel translations. While this choice avoids interpolation artifacts and preserves spatial consistency, more generic transformations, including learned transformations, could be explored.

\noindent\textbf{Conclusion. }
In this work, we present a signal-processing perspective for defending against adversarial attacks, motivated by the connection between adversarial perturbations and aliasing artifacts. Accordingly, we propose a ``combat noise with noise'' approach by introducing stochastic resonance as a defense mechanism. We formalize the problem and implement stochastic resonance using pixel-level translations paired with their inverse transformation in the feature space. The resulting method is training-free, agnostic to both tasks and attack types, and independent of network architectures.

We evaluate our method across various tasks. Empirical results on image classification demonstrate that our stochastic resonance approach achieves state-of-the-art robustness against diverse attack types, offering a clear advantage over feature-level denoising and filtering. Even in the adaptive adversary scenario, where an attacker is aware of the use of stochastic resonance, our method maintains strong robustness. Furthermore, we are the first to introduce test-time defense to dense prediction tasks. Specifically, we apply this method to stereo matching and optical flow, achieving up to a 71\% reduction in prediction error. More importantly, these findings highlight the practical potential of stochastic resonance as a universal defense in real-world adversarial scenarios.

\clearpage

\section*{Reproducibility Statement}
We provide sufficient technical details in the paper to ensure reproducibility. Specifically, we describe the augmentations used for stochastic resonance, including augmentations (e.g. translation, rotation) and their corresponding inverse transformations, as well as the model architectures, datasets, and the network layers where our method is applied. Attack settings and evaluation protocols are drawn directly from standard benchmark datasets and publicly available code base, ensuring comparability with prior work. All implementation details necessary to reproduce our experiments, including parameters and ablation settings, are provided in the main paper and further expanded in the Appendix. Our experiments can be reproduced on a single desktop-level GPU without requiring large-scale computational resources. We will release the complete source code and pre-computed adversarial data upon publication.

\section*{LLM Statement}
All technical content of this work, including literature review, methodology, experiments, and analyses, was developed entirely by the authors. Large Language Models (LLMs) were employed as a tool for proofreading, without contributing to the scientific or technical substance of the manuscript.


\bibliography{iclr2026_conference}
\bibliographystyle{iclr2026_conference}
\clearpage
\appendix
\section{Implementation for Classification}

For our classification experiments, we built our implementation on top of standard network architectures, implementing SR on two main architectures derived from the FSR codebase \cite{kim2023feature}:
\begin{itemize}
    \item ResNet-18 \cite{he2016deep}: A standard residual network with 18 layers organized in four main blocks with increasing channel dimensions (64, 128, 256, 512).
    \item WideResNet-34 \cite{zagoruyko2016wide}: A wider variant of ResNet with depth 34 and width factor 10, resulting in higher representational capacity with three main blocks with channel dimensions scaled by the width factor (160, 320, 640).
\end{itemize}
For both architectures, we apply SR at the bottleneck layer (after the final convolutional block).

Given an input image batch $x \in \mathbb{R}^{b \times 3 \times h \times w}$ (where $b$ is the batch size), our SR approach operates as follows. First, we create a set of $(2d_x+1) \times (2d_y+1)$ perturbed versions of the input by applying pixel-level translations within the range $[-d_x, d_x] \times [-d_y, d_y]$ pixels:
\begin{equation}
    X_{\text{perturbed}} = \{g_{i,j}(X) \mid i \in [-d_y, d_y], j \in [-d_x, d_x]\}
\end{equation}
where $g_{i,j}$ translates the image by $(i,j)$ pixels. These transformations are applied using PyTorch's ``transforms.functional.affine'' function with translation parameters while preserving the original image properties.

All images are concatenated into one batch and processed through the network in parallel up to the bottleneck layer:
\begin{equation}
    F = \phi(X_{\text{perturbed}})
\end{equation}
where $\phi$ represents the network up to the bottleneck layer. This batch processing approach significantly improves computational efficiency compared to individual forward passes.

After obtaining feature maps for all perturbed inputs, we aggregate them to create a single enhanced feature map:
\begin{equation}
    F_{\text{ensembled}} = \frac{1}{n}\sum_{i,j} T_{-i,-j}(F_{i,j})
\end{equation}
where $T_{-i,-j}$ represents the inverse spatial shift operation that realigns the feature map and $n$ the number of augmentations.

Our implementation requires $(2d_x+1) \times (2d_y+1)$ forward passes through the network up to the bottleneck layer.


For evaluation, we tested our approach against standard adversarial attacks (FGSM \cite{goodfellow2014explaining}, PGD-20 and PGD-100 \cite{madry2017towards}, and C\&W \cite{carlini2017towards}), all bounded within $\epsilon = 8/255$ under $\ell_\infty$-norm. We also report an Ensemble metric that measures the worst-case performance across all attacks for each test example, providing a comprehensive robustness assessment.

\section{Implementation for Stereo Matching}

For our stereo matching experiments, we built our implementation on top of standard stereo network architectures to ensure our approach remains model-agnostic and requires no training or fine-tuning. We integrated SR with PSMNet \cite{chang2018pyramid}, a pyramid stereo matching network with a stacked hourglass architecture that uses 3D convolutions on a cost volume constructed by concatenating features. We apply SR at the feature extraction stage, before cost volume construction, where stereo correspondences are first established.

Given a pair of input stereo images $x_L, x_R \in \mathbb{R}^{b \times 3 \times h \times w}$ (where $b$ is the batch size), our SR approach operates as follows. First, we create a set of $(2d_x+1) \times (2d_y+1)$ perturbed versions of each input image by applying translations within the range $[-d_x, d_x] \times [-d_y, d_y]$ pixels:
\begin{equation}
    X_{L,\text{perturbed}} = \{g_{i,j}(x_L) \mid i \in [-d_y, d_y], j \in [-d_x, d_x]\}
\end{equation}
\begin{equation}
    X_{R,\text{perturbed}} = \{g_{i,j}(x_R) \mid i \in [-d_y, d_y], j \in [-d_x, d_x]\}.
\end{equation}

All images are concatenated into batches and processed through the feature extraction component of the network:
\begin{equation}
    F_L = \phi(X_{L,\text{perturbed}})
\end{equation}
\begin{equation}
    F_R = \phi(X_{R,\text{perturbed}})
\end{equation}
where $\phi$ represents the feature extraction component of the stereo network. This batch processing approach significantly improves computational efficiency compared to individual forward passes.

After obtaining feature maps for all perturbed inputs, we aggregate them to create a single enhanced feature map.

Our implementation requires $(2d_x+1) \times (2d_y+1)$ forward passes through the feature extraction component of the network for each stereo image. 

For evaluation, we tested our approach against adversarial attacks generated using FGSM \cite{goodfellow2014explaining} and I-FGSM \cite{kurakin2018adversarial} (a special case of PGD), bounded within various $\epsilon$ values ($\{0.002, 0.005, 0.01, 0.02\}$) under $\ell_\infty$-norm. We measured performance using three standard stereo matching metrics: Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and D1-error (percentage of pixels with disparity error greater than 3 pixels or 5\% of the ground truth) \cite{luo2018towards}.


\section{Details about Attack on Optical Flow}
To find an adversarial attack for optical flow estimated by a given neural network $f$, we utilize a similar approach to \cite{oskouie2024attack} that aims to find a perturbation $\delta$ for given frames $F_1$ and $F_2$, maximizing the discrepancy between predicted and ground-truth optical flow $OF$. If the ground-truth optical flow is unavailable, we use the predicted optical flow from the unattacked frame as our surrogate ground-truth. Our method applies $\delta$ to the first input frame, then uses a deep neural network to estimate optical flow from the perturbed frames. The objective is to maximize the average end-point error (EPE) between the predicted and ground-truth optical flow, calculated as the mean Euclidean distance between corresponding 2D flow vectors. In other words, the $\epsilon$-norm bounded adversary $\delta$ for optical-flow is calculated by optimizing the following
\begin{equation}
    \max_{\delta: \Vert\delta\Vert \le \epsilon} \text{EPE} \bigl(OF, f(F_1 + \delta, F_2) \bigr).  
\end{equation}

One $l_\infty$-bounded adversary $A$ for the aforementioned optimization problem is Fast Gradient Sign Method (FGSM) \cite{goodfellow2014explaining} which can be obtained by 
\begin{align}
\text{L} = \text{EPE}(OF, f(F_1, F_2)) , \nonumber \\
A = F_1 + \epsilon \cdot \text{sign}\bigl(\nabla_{F_1} \text{L} \bigr).   
\end{align}

Projected gradient descent (PGD) \cite{madry2017towards} represents an enhanced and more complex version of FGSM. This attack method generates adversarial examples through an iterative process and the formulation for this attack is as following
\begin{equation}
F_1^{(t + 1)} = \Pi_{F_1 + \mathcal{S}} \bigl( F_1^{(t)} + \alpha \cdot \text{sign}(\nabla_{F_1} \text{L} ) \bigr).   
\end{equation}

Note that in PGD, since the perturbations are considered to be too minimal to significantly alter the flow dynamics, the ground-truth optical flow is not updated by intermediate perturbations applied to the input data.

For our experimental setup, we chose to set the norm value $\epsilon$ at $\frac{10}{256}$. Furthermore, we configured the PGD algorithm to run for 10 iterations. The step size $\alpha$ was determined by dividing $2.5 \cdot \epsilon$ by the total number of iterations, ensuring a balanced progression throughout the optimization process.

\section{Additional Visualizations}
Here we provide additional visualizations from our experiments comparing SR under both FGSM and PGD attack. We also provide a visual showing the results of FGSM and PGD pertubation on various images.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{Figures/fgsm2e2.pdf}
    \caption{Visual results on stereo matching against FGSM attack, without and with SR, $\epsilon=0.02$}
    \label{fig:fgsm2e2_stereo}
\end{figure*}
\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{Figures/fgsm1e2.pdf}
    \caption{Visual results on stereo matching against FGSM attack, without and with SR, $\epsilon=0.01$}
    \label{fig:fgsm1e2_stereo}
\end{figure*}
\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{Figures/fgsm5e3.pdf}
    \caption{Visual results on stereo matching against FGSM attack, without and with SR, $\epsilon=0.005$}
    \label{fig:fgsm5e3_stereo}
\end{figure*}
\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{Figures/PGD1e2.pdf}
    \caption{Visual results on stereo matching against PGD attack, without and with SR, $\epsilon=0.01$}
    \label{fig:pgd1e2_stereo}
\end{figure*}
\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{Figures/PGD5e3.pdf}
    \caption{Visual results on stereo matching against PGD attack, without and with SR, $\epsilon=0.005$}
    \label{fig:pgd5e3_stereo}
\end{figure*}
\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{Figures/PGD2e3.pdf}
    \caption{Visual results on stereo matching against PGD attack, without and with SR, $\epsilon=0.002$}
    \label{fig:pgd2e3_stereo}
\end{figure*}

\begin{figure}[h]
\setlength{\tabcolsep}{2pt}
    \centering
    \begin{tabular}{ccccc}
        \includegraphics[width=0.18\textwidth]{Figures/perturbations/00004.jpg} &
        \includegraphics[width=0.18\textwidth]{Figures/perturbations/FGSM/attacked_img4.png} &
        \includegraphics[width=0.18\textwidth]{Figures/perturbations/FGSM/noise4.png} &
        \includegraphics[width=0.18\textwidth]{Figures/perturbations/PGD/attacked_img4.png} &
        \includegraphics[width=0.18\textwidth]{Figures/perturbations/PGD/noise4.png} \\
        \includegraphics[width=0.18\textwidth]{Figures/perturbations/00005.jpg} &
        \includegraphics[width=0.18\textwidth]{Figures/perturbations/FGSM/attacked_img5.png} &
        \includegraphics[width=0.18\textwidth]{Figures/perturbations/FGSM/noise5.png} &
        \includegraphics[width=0.18\textwidth]{Figures/perturbations/PGD/attacked_img5.png} &
        \includegraphics[width=0.18\textwidth]{Figures/perturbations/PGD/noise5.png} \\
        \includegraphics[width=0.18\textwidth]{Figures/perturbations/00049.jpg} &
        \includegraphics[width=0.18\textwidth]{Figures/perturbations/FGSM/attacked_img49.png} &
        \includegraphics[width=0.18\textwidth]{Figures/perturbations/FGSM/noise49.png} &
        \includegraphics[width=0.18\textwidth]{Figures/perturbations/PGD/attacked_img49.png} &
        \includegraphics[width=0.18\textwidth]{Figures/perturbations/PGD/noise49.png} \\
        \includegraphics[width=0.18\textwidth]{Figures/perturbations/00062.jpg} &
        \includegraphics[width=0.18\textwidth]{Figures/perturbations/FGSM/attacked_img62.png} &
        \includegraphics[width=0.18\textwidth]{Figures/perturbations/FGSM/noise62.png} &
        \includegraphics[width=0.18\textwidth]{Figures/perturbations/PGD/attacked_img62.png} &
        \includegraphics[width=0.18\textwidth]{Figures/perturbations/PGD/noise62.png} \\
        \textbf{Image} & \textbf{FGSM Attacked} & \textbf{FGSM Noise} & \textbf{PGD Attacked} & \textbf{PGD Noise} \\
    \end{tabular}
    \caption{Original images and their corresponding attacked images and perturbations using FGSM and PGD methods on optical flow. The attacks mostly target the main object observed in the image.}
    \label{Figure:attack}
\end{figure}



\end{document}
