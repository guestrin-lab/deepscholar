% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%

\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{
% Are people over-reliant on AI? Different background knowledge changes over-reliance in education grading task decision-making.
% AI Knows Best? The Paradox of Expertise, AI-Reliance, and Performance in Decision-Making
% AI Knows Best? The Paradox of Expertise, AI-Reliance, and Performance in Educational Decision Making
AI Knows Best? The Paradox of Expertise, AI-Reliance, and Performance in Educational Tutoring Decision-Making Tasks
% AI Knows Best? The Paradox of Expertise and AI-Reliance in Education
}
%
\titlerunning{AI Knows Best? The Paradox of Expertise, AI-Reliance, and Performance}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
% \author{Anonymous Authors}
\author{
Eason Chen\inst{1}
% \orcidID{0000-1111-2222-3333} 
\and
Jeffrey Li\inst{1}
\and
Scarlett Huang\inst{1}
\and
Xinyi Tang\inst{1}
\and
Jionghao Lin\inst{2,1}
\and
Paulo Carvalho\inst{1}
\and
Kenneth Koedinger\inst{1}
% \orcidID{0000-1111-2222-3333}
}
% Second Author\inst{2,3}\orcidID{1111-2222-3333-4444} \and
% Third Author\inst{3}\orcidID{2222--3333-4444-5555}}
%
\authorrunning{Eason Chen et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Carnegie Mellon University
\\ \and
The University of Hong Kong
% \email{EasonC13@cmu.edu}
% \\
% \url{https://hcii.cmu.edu/people/i-sheng-eason-chen}
}

%
\maketitle              % typeset the header of the contribution
%

%
%
%


\begin{abstract}
We present an empirical study of how both experienced tutors and non-tutors judge the correctness of tutor praise responses under different Artificial Intelligence (AI)-assisted interfaces types of explanation (textual explanations vs. inline highlighting). 
We first fine-tuned several Large Language Models (LLMs) to produce binary correctness labels and explanations, achieving up to 88\% accuracy and 0.92 F1 score with GPT-4. 
We then let the GPT-4 models assist 95 participants in tutoring decision-making tasks by offering different types of explanations. 
Our findings show that although human-AI collaboration outperforms humans alone in evaluating tutor responses, it remains less accurate than AI alone. 
Moreover, we find that non-tutors tend to follow the AI’s advice more consistently, which boosts their overall accuracy on the task—especially when the AI is correct. In contrast, experienced tutors often override the AI’s correct suggestions and thus miss out on potential gains from the AI’s generally high baseline accuracy. 
Further analysis reveals that explanations in text reasoning will increase over-reliance and reduce underreliance, while inline highlighting does not. Moreover, neither explanation style actually has a significant effect on performance and costs participants more time to complete the task instead of saving time. Our findings reveal a tension between expertise, explanation design, and efficiency in AI-assisted decision-making, highlighting the need for balanced approaches that foster more effective human-AI collaboration.
\end{abstract}


\keywords{Artificial Intelligence, Human-AI Collaboration, Decision Making}
\section{Introduction}


% <Point 1, Many education tasks involve the educators to make the evaluation such as grading, assessment>
% <P2, these evaluation is consdiered a decision made by educators>
% <P3, given the widely use of LLMs, the decision from educators gradually moving from educator-dominant to relying AI for making such decision in the evaluation tasks in educaiton>

% <P4, it might raise the concerns of overly relying on the LLM prediciton and lack of educators' self-judgment>
% <P5, our study aims to investigate the extent to which this concern exist from the educators perspective>


Many educational tasks---such as grading~\cite{jerrim2020teacher,kreuzfeld2022teachers}, assessment~\cite{anderson2003classroom}, and providing feedback~\cite{carless2019feedback,hattie2007power,lin2023using}---require educators to make critical judgments about student performance and progress. These tasks are inherently \emph{decision-making} processes: teachers assess student outputs against learning objectives, deciding how best to score, comment on, or otherwise respond to student work. As educational settings become more complex and class sizes grow, educators often face increasing time and cognitive demands to maintain fair and consistent evaluations \cite{jerrim2020teacher,kreuzfeld2022teachers,hojo2021association}.

In the present era, Artificial Intelligence (AI) significantly aids human decision-making. Studies show that AI recommendations can improve speed and performance as well as introduce reliance in tasks such as question-answering~\cite{vasconcelos2023explanations}, logical reasoning~\cite{bansal2021does,si2023large}, sentiment analysis~\cite{bansal2021does}, and image annotation~\cite{morrison2023evaluating}. Nevertheless, prior work predominantly employed traditional machine-learning methods for non-educational tasks. Little research has examined how the latest Large Language Models (LLMs) could support decision-making in educational contexts, specifically in tutoring. Unlike these tasks, tutoring requires accountability and specialized pedagogical knowledge (e.g., how to give constructive feedback). Given the context-dependent nature of AI’s impact, applying findings from non-educational domains directly to tutoring settings with LLMs poses inherent risks.

% Meanwhile, tutoring is widely recognized as one of the most effective ways to enhance students’ learning outcomes~\cite{nickow2020impressive}. Nevertheless, a shortage of professionally trained tutors makes it challenging to scale high-quality tutoring services for all students~\cite{kraft2021blueprint}. Recent work has therefore explored onboarding individuals with minimal teaching backgrounds to serve as tutors~\cite{kraft2021blueprint,lin2023using,thomas2023tutor}, often using focused training modules on key principles of effective feedback~\cite{thomas2023tutor}. Although novice tutors can learn to perform well after sufficient practice, the practical shortage of tutors persists at scale~\cite{kraft2021blueprint}.

These circumstances create a strong incentive to incorporate AI tools that reduce educators' cognitive load and help them make effective decisions or feedback~\cite{lin2023using}. For instance, AI-driven feedback could help educators quickly assess how well they are motivating or guiding students. Yet, as discussed above, research on human-AI decision-making in education, especially in tutoring scenarios, remains limited. This paper seeks to close this gap by investigating how LLMs can support decision-making in judging effective tutor responses, focusing on deciding whether certain praise or feedback is appropriate. Specifically, we address the following \textbf{R}esearch \textbf{Q}uestions:

\noindent \textbf{RQ1}: How accurately can an LLM evaluate the appropriateness of tutor-praising responses, and how does its performance compare to (a) humans working alone and (b) human-AI collaboration?

\noindent \textbf{RQ2}: How do differing levels of tutor expertise and various AI explanation styles (e.g., textual reasoning vs. inline highlighting) influence human performance—specifically in terms of accuracy, time efficiency, and reliance on AI?

\subsubsection{Our Contribution.}
In response to those research questions, we find that an LLM can achieve up to 88\% accuracy and 0.92 F1 score in evaluating tutor-praising responses, surpassing both humans alone and human-AI collaboration. We also developed two explanation interfaces and observed that textual reasoning fosters greater reliance on AI; further, humans spend more time when collaborating with AI. Finally, while novices boost their overall accuracy by trusting the AI’s generally correct suggestions (yet over-rely when it is wrong), experienced tutors often override correct AI advice, resulting in under-reliance.

% In response to these research questions, we find that an LLM can achieve up to 88\% accuracy in evaluating tutor-praising responses, outperforming humans alone.
% However, novices tend to over-rely on correct AI outputs while experts under-rely, missing potential benefits from AI’s high baseline accuracy.
% Moreover, textual explanations decrease under-reliance but increase over-reliance, underscoring key design trade-offs for efficient AI-assisted tutoring systems.
% \subsubsection{Our Contribution.}
% In response to our research questions, we (1) demonstrate that large language models (LLMs) like GPT-4 can effectively judge tutor-praise correctness, achieving up to 88\% accuracy; (2) reveal how novices, compared to experienced tutors, tend to over-rely on AI—resulting in better average performance but larger errors when AI is wrong—while experts often under-rely on the AI; (3) show that textual explanations increase over-reliance (yet reduce under-reliance), whereas inline highlighting exerts no significant impact on performance or reliance; and (4) discuss implications for future AI-assisted tutoring systems, emphasizing the need for more nuanced, context-aware explanation strategies that balance efficiency, trust, and accountability.
% \subsubsection{Our Contribution} XXXX

\section{Related Work}


% \subsection{AI-Assisted Decision Making}

Prior research on AI-assisted decision-making has produced mixed results, underscoring that its benefits depend on contextual factors. On the one hand, studies indicated that humans are sometimes misled by AI when it makes errors, especially if explanations are overly complex or fail to clarify uncertainties \cite{vasconcelos2023explanations,buccinca2021trust,dietvorst2015algorithm}. On the other hand, well-designed explanations can bolster users’ ability to judge the correctness of AI outputs. For instance, \cite{morrison2023evaluating} found that causal explanations help participants recognize AI unreliability when the AI is wrong, reducing blind reliance. However, if the AI’s decision is correct but the explanation is flawed, most participants do not immediately doubt the overall judgment; rather, they attempt to reconcile the erroneous explanation. Notably, visually misleading cues often carry greater persuasive power than textual errors, and combining incorrect visual and textual explanations can create a “double misdirection” effect \cite{morrison2023evaluating}. If, however, visual and textual explanations conflict, participants are more likely to question the AI’s decision.

Despite these challenges, well-designed AI systems can complement human decision-makers, achieving performance that neither humans nor AI alone can match \cite{bansal2021does}. Yet, research also shows that humans frequently either over-rely on AI even when it is wrong or under-rely on it even when AI is correct, often missing the “appropriate reliance” that leverages AI's suggestion when it is correct and rejecting it when it errs \cite{vereschak2021evaluate,vasconcelos2023explanations,buccinca2021trust,dietvorst2015algorithm}. Even when human-AI teams outperform humans working by themselves, they often fail to exceed AI-only performance. 
% One key to mitigating these issues is enhancing AI explainability.
One key to boosting human-AI collaboration performance is enhancing AI explainability.
For example, explanation designs that integrate the AI’s reasoning within the question interface (e.g., inline highlights) reduce cognitive load and help users better assess correctness \cite{bansal2021does,vasconcelos2023explanations}. Additionally, users’ reliance on AI may vary according to their task expertise. For instance, \cite{morrison2023impact} find that non-experts who are less equipped to gauge correctness are more susceptible to over-reliance.

In this study, we aim to explore how different explanation methods, particularly highlighting and text explanations, can help improve human-AI performance in evaluating the effectiveness of tutoring feedback.


\subsection{Evaluate Tutor Giving Effective Praise with AI}

According to prior work~\cite{thomas2023tutor}, giving effective praise involves genuine and timely acknowledgment of a student’s specific strengths, avoiding overused expressions that lose impact. Rather than saying “good job,” effective praise highlights the student’s accomplishment by connecting it to effort rather than inherent ability. Clear, truthful, and immediate praise fosters greater motivation, resilience, and engagement. Prior work has employed AI to classify tutor responses like using a BERT-based Named Entity Recognition (NER) model to classify the effective praise~\cite{lin2023using} and obtained up to 73\% accuracy and 0.81 in F1 score. They found that the model worked well for effort-related praise but it struggled with outcomes-based praise because of the lack of training data.

We believe that leveraging Large Language Models (LLMs) can effectively address the challenge of insufficient data. As highlighted in the report of \cite{openai2023gpt4}, LLMs have undergone comprehensive training with vast datasets, enabling them to execute various tasks proficiently. We only need to provide a task description and a few examples \cite{brown2020language}, and LLMs can effectively judge whether the tutor's response meets the standards of effective praise. Yet, the random process of text generation \cite{holtzman2019curious} makes verifying LLM outputs difficult. As a result, designing explanation methods that clarify the AI’s decision-making is essential for enabling educators to interpret—and, when necessary, override—LLM judgments.
% modern Large Language Models (LLMs) \cite{openai2023gpt4} leverage vast pre-trained knowledge and can perform similar classification tasks with minimal examples \cite{brown2020language}. Yet, the unpredictability of text generation \cite{holtzman2019curious} makes verifying LLM outputs difficult. As a result, designing explanation methods that clarify the AI’s decision-making is essential for enabling educators to interpret—and, when necessary, override—LLM judgments.

% Previous studies have made significant efforts to use AI to annotate tutor responses regarding giving effective praise. 
% For example, a recent study \cite{lin2023using} employed a BERT-based Named Entity Recognition (NER) model to differentiate whether a tutor’s praise focused on effort or outcome. They manually annotated hundreds of tutor responses for training and evaluation, finding that BERT achieved promising accuracy in detecting effort-based praise but struggled with outcome-oriented praise. The authors suggest that augmenting the training dataset could address this performance gap, indicating the potential for further improvements in NER for this domain \cite{lin2023using}.
% However, we believe that leveraging Large Language Models (LLMs) can effectively address the challenge of insufficient data. As highlighted in the report of \cite{openai2023gpt4}, LLMs have undergone comprehensive training with vast datasets, enabling them to execute various tasks proficiently. We only need to provide a task description and a few examples \cite{brown2020language}, and LLMs can effectively judge whether the tutor's response meets the standards of Effective Praise.
% However, the drawback of using LLMs is the difficulty in automating verification because this new task lacks existing datasets, and generating Natural Language Generation outputs is somehow random \cite{holtzman2019curious}, making it challenging to ensure the correctness every time. Therefore, we need to enhance the explainability of LLMs' output so that even if LLMs produce incorrect decisions, users can still identify them instead of blindly following AI's actions.

\section{Method}



\subsection{Data Preparation}

Through a tutor training system, we collected 216 responses from a simulated scenario where tutors complimented students after solving a math problem. Two senior and experienced tutors and a PhD learning scientist were invited to label the correctness of these responses. The kappa score between the two experienced tutors was 0.76, indicating a substantial consistency, and then the learning scientist resolved the disagreements. This process established our ground truth data. Of the total responses, 155 were labeled as correct (effective praise), and 61 were labeled as incorrect (not effective praise). Among these, four responses were used as a few-shot training data in the prompt, while the rest were used for prediction.

\subsection{Prompt Engineering}


\begin{table}[b!]
    \caption{Performance Metrics for Different Models}
    \centering
    \begin{tabular}{p{3.7cm}p{1.9cm}p{1.9cm}p{1.9cm}p{1.9cm}}
        \hline
        \textbf{Model} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} \\
        \hline
        \texttt{gpt-4o-2024-11-20} & 0.847 & 0.930 & 0.852 & 0.889 \\
        \texttt{gpt-4o-2024-08-06} & 0.847 & 0.918 & 0.865 & 0.890 \\
        \texttt{gpt-4o-mini-2024-07-18} & 0.815 & 0.908 & 0.826 & 0.865 \\
        \texttt{gpt-4-1106-preview} & 0.870 & 0.885 & 0.942 & 0.912 \\
        \texttt{gpt-4-0125-preview} & 0.861 & 0.879 & 0.935 & 0.906 \\
        \textbf{\texttt{gpt-4-0613}} & \textbf{0.884} & \textbf{0.892} & \textbf{0.955} & \textbf{0.922} \\
        \texttt{gpt-3.5-turbo} & 0.792 & 0.917 & 0.781 & 0.843 \\
        \hline
    \end{tabular}
    
    \label{tab:models_performance}
\end{table}

\begin{table}[b!]
    \centering
    \caption{Ablation analysis result of the prompt with \textit{gpt-4o-2024-08-06}}
    \begin{tabular}{c|c|cccc}
        \hline
        Few-shot & Instruction & Accuracy & Precision & Recall & F1 Score \\
        \hline
        X & O & 0.653 & 0.976 & 0.529 & 0.686 \\
        O & X & 0.824 & 0.820 & 0.968 & 0.888 \\
        O & O & 0.847 & 0.912 & 0.871 & 0.891 \\
        \hline
    \end{tabular}
    \label{tab:ablate}
\end{table}

% \begin{table}[h]
%     \centering
%     \begin{tabular}{lcccc}
%         \hline
%         Models & Accuracy & Precision & Recall & F1 Score \\
%         \hline
%         gpt-4o-2024-11-20 & & & & \\
%         gpt-4-0613 & 0.889 & 0.907 & 0.942 & 0.924 \\
%         gpt-4-1106-preview & & & & \\
%         \hline
%         gpt-4o-2024-11-20 & & & & \\
%         gpt-4o-2024-08-06 & & & & \\
%         gpt-4-0613 & 0.884 & 0.906 & 0.935 & 0.921 \\
%         gpt-4-1106-preview & & & & \\
%         gpt-4-0125-preview & & & & \\
%         \hline
%         gpt-4-0613 & & & & \\
%         gpt-4-1106-preview & 0.875 & 0.886 & 0.948 & 0.916 \\
%         gpt-4-0125-preview & & & & \\
%         \hline
%     \end{tabular}
%     \caption{Performance Metrics for Majority Vote Base Models}
%     \label{tab:majority_vote}
% \end{table}


We aim to use LLM to assist users in determining if a tutor's response is effective praise. By providing guidelines for Giving Effective Praise and some selected examples as the prompt, we enable LLM to evaluate whether the tutor's response meets the criteria for effective praise. Additionally, we ask LLM to generate two explanations for its decision. The first type involves direct text reasoning for the decision. The second type outputs the original input, incorporating HTML tags to Highlight the sections it deems to meet the standards of effective praise. Due to the page limit, the full prompt and example output is at \footnote{\url{https://gist.github.com/anonymousStars/4a56d304515046d806eee4bbfb4d7e2b}}.

We used this prompt to run through all models in Table \ref{tab:models_performance}, the best accuracy of 0.884 is achieved by \textit{gpt-4-0613}, with an F1 Score of 0.918. Moreover, we further performed a majority vote ensemble on the above models and found that combining the following three models from —\textit{"gpt-4o-2024-11-20"}, \textit{"gpt-4-0613"}, and \textit{"gpt-4-1106-preview"}— for majority voting further improved the Accuracy to 0.889 and the F1 Score to 0.924.

We further performed an ablation analysis of the prompt using the \textit{gpt-4o-2024-08-06} model. The results, shown in Table \ref{tab:ablate}, indicate that few-shot learning plays a more significant role in this context compared to instructions. However, instructions still contribute by providing additional knowledge that helps enhance the model's performance and help it create better reasoning and highlight explanations when we observed the output data.


\subsection{Interface Design} \label{interface-design}

\begin{figure}[b!]
    \centering
    \includegraphics[width=0.95\linewidth]{image/AI_BIAS_Conditions.png}
    \caption{The five different interfaces we designed for AI-assisted decision-making.}
    \label{fig:AI_BIAS_Conditions}
\end{figure}

In order to investigate user reliance on AI in different explanation methods, we designed five different interfaces as shown in Figure \ref{fig:AI_BIAS_Conditions}. The AI-generated label, Reasoning, and Highlightings are from \textit{gpt-4-0613}. These five interfaces are:

\begin{enumerate}
    \item N: Task content without AI (\textbf{N}o AI control group).
    \item C: Task content with AI-generated label (\textbf{C}ontrol group with AI)
    \item R: Task content with AI-generated label and text \textbf{R}easoning.
    \item H: Task content with AI-generated labels and inline \textbf{H}ighlightings.
    \item HR: Task content with AI-generated labels, \textbf{H}ighlightings, and \textbf{R}easoning.
\end{enumerate}

We use the No AI control group as a baseline to compare users' performance with and without AI. Moreover, we use the AI label-only control group to compare users' performance and reliance with different types of AI explanations.



\subsection{Participants}

We recruited a total of 95 participants by convenience sampling, including 47 professionally trained tutors and 48 participants from Prolific. The age distribution varied between the two groups: Prolific participants ranged from 23 to 73 years old, with a mean age of 39.2 years (SD = 14.5), while tutors were between 18 and 29 years old, with a mean age of 22.3 years (SD = 2.6). In terms of gender distribution, the Prolific group consisted of 28 male and 20 female participants, whereas the tutor group comprised 17 male and 30 female participants.


\subsection{Procedure and Design}

Our experiment was approved by the Institutional Review Board (IRB). At the experiment platform, participants started by going through a training module on giving effective praise, which consisted of guidelines and examples explaining why certain tutor's responses are or are not effective praise. They then proceeded to engage in a task involving five rounds of judging the correctness of Tutor Responses. We employed a counterbalanced design, in which each participant was randomly assigned to one of the possible permutations of the four conditions in Section \ref{interface-design}, ensuring that each condition occurred in each position equally often across participants. Each condition included 20 tasks.
% , but the order was randomized. Each design included 20 tasks.
% After participants complete all tasks, they will enter a post-test to evaluate their Trust Level of AI and provide qualitative feedback.

In addition, the tasks encountered by the participants were also randomized, but we kept these tasks' distribution and accuracy rates aligned with the original data. 
That is, 
in the 20 tasks of each design condition, participants encountered 13 correct tutor responses and 7 incorrect tutor responses. Moreover, 
to reflect the distribution of real data, we set the AI's accuracy rate to 0.8, and participants encountered 3 false negatives and 1 false positive at each design condition. 
Finally, to prevent participants from losing confidence in the AI due to errors, we ensured that participants received correct suggestions from the AI for the first five tasks across all conditions.

\subsection{Data Collection and Analyze}

We first conducted a 2 x 2 Analysis of Variance (ANOVA) to compare how different groups of participants perform with and without AI. Moreover, we check the participant's reliance by comparing the reliance rate in No Label condition with other conditions in a one-way ANOVA and post hoc tests. Then, we used repeated measures ANOVA with the presence of Highlighting and Text Reasoning in a 2 x 2 ANOVA to compare participants' performance and reliance on different types of explanations. Finally, we compared the difference between the  experienced Tutor and Prolific participants using an independent sample t-test for the following measures:


\subsubsection{Human Performance}
We evaluated the participants' performance based on their accuracy rate when they performed data labeling decision-making tasks in a specific condition.

\subsubsection{Time Spent}
We evaluated the average time participants spent on each task by calculating the time interval between their task submissions. When analyzing the time spent, we noted that since the experiment was conducted online, a few participants disconnected midway, engaged in other activities, and then reconnected later, resulting in some extreme completion times. To address this, we applied the \textbf{Interquartile Range (IQR) method to remove outliers} before conducting our analysis, as recommended by \cite{vinutha2018detection}.

\subsubsection{Reliance on AI in Different Conditions}

Building on prior research \cite{vasconcelos2023explanations,buccinca2021trust,dietvorst2015algorithm,morrison2023evaluating}, we categorize two forms of reliance on AI as follows according to how participants perform in testing scenarios where the AI provides labels and explanations:

\begin{itemize}
    \item \textbf{Over-Reliance}: When the AI provides an incorrect suggestion, and the user follows the suggestion, resulting in an incorrect decision. 
    % The counterpart of over-reliance is \textbf{Misguided-Rejection}, which occurs when the AI provides incorrect advice, but the user rejects it and makes a correct decision.
    \item \textbf{Under-Reliance}: When the AI provides a correct suggestion, but the user ignores it and makes an incorrect decision.
    % The counterpart of under-reliance is \textbf{Appropriate-Reliance}, which occurs when the AI provides correct advice, and the user accepts it, leading to a correct decision.
\end{itemize}

\section{Results}

% Ken:
% - Overall participants are more correct when they have AI advice (M = XX) than when they do not (M = XX; T(X) = X, p < 0.0XX, effectsize).
% - Look at participant performance with AI vs. not cross with AI-hard questions (those AI are wrong) vs. AI-easy-questions : Is there

% AI-hard questions are also harder for participants (M=.45??) than AI-easy questions (M=.75??).  There is a significant interaction such that with AI advice participants do better at the AI-correct than particiapnts without AI and do worse on the AI-incorrect questions than particiapnts without AI.  So, in general participants are relying on the AI.

% Are they more likely to disagree when the AI is correct (mistrust) or agree when the AI is wrong (over-rely)?  Under-reliance (Disagreement) is 1-accuracy for AI correct problems with AI advice and is M=.12?? and

% Participants are generally more trusting of AI -- more likely to agree when AI is wrong (M=.70) than disagree when AI is right (M=.12).

% But still worse than AI.

\subsection{Decision-Making Performance across groups}


\begin{figure}[b!]
    \centering
    \includegraphics[width=1\linewidth]{image/correct_percentage.png}
    \caption{Correct Percentage across different conditions and groups. Participants from Prolific and working with AI performed better. The error bar is Standard Error.}
    \label{fig:correct_percentage}
\end{figure}




\begin{table}[b!]
\scriptsize
    \centering
    \caption{Decision-Making Performance Comparison Between Tutor and Prolific Participants. T-test Results. }
    \begin{tabular}{c c c c l c}
        \hline
        Condition & Tutor Mean (SD) & Prolific Mean (SD) & t-statistic & p-value & Effect Size \\
        \hline
        No Label & 0.696 (0.133) & 0.714 (0.107) & -0.757 & 0.4512 & 0.154 \\
        AI Label Only & 0.703 (0.132) & 0.742 (0.091) & -1.676 & 0.0970\textsuperscript{+} & 0.342 \\
        Reasoning & 0.726 (0.122) & 0.748 (0.109) & -0.949 & 0.3450 & 0.194 \\
        Highlighting & 0.707 (0.108) & 0.762 (0.081) & -2.818 & 0.0059** & 0.575 \\
        Highlighting + Reasoning & 0.729 (0.105) & 0.768 (0.076) & -2.129 & 0.0359* & 0.435 \\
        Overall Average & 0.712 (0.088) & 0.747 (0.057) & -2.302 & 0.0236* & 0.470 \\
        \hline
        \\[-1.8ex]
        \multicolumn{6}{l}{$\textsuperscript{*}p < 0.05$, $\textsuperscript{**}p < 0.01$, $\textsuperscript{***}p < 0.001$}
    \end{tabular}
    \label{tab:performance_result}
\end{table}

% - Look at participant performance with AI vs. not cross with AI-hard questions (those AI are wrong) vs. AI-easy-questions : Is there
% - Overall participants are more correct when they have AI advice (M = XX) than when they do not (M = XX; T(X) = X, p < 0.0XX, effectsize).

The results comparing the performance of Tutors and Prolific participants across different task conditions are presented in Figure \ref{fig:correct_percentage} and Table \ref{tab:performance_result}.


We first conducted a 2-way ANOVA to compare how different groups of participants perform with and without AI. The analysis revealed a significant \textbf{main effect of AI assistance} (\( F(1, 471) = 6.83, p = .009, \eta_p^2 = .014 \)), indicating that participants using AI performed significantly better than those without AI. Moreover, there was a significant \textbf{main effect of group} (\( F(1, 471) = 5.34, p = .021, \eta_p^2 = .011 \)), indicating that participants from Prolific have better overall performance compared to experienced tutors. The interaction effect between AI assistance and group was not significant (\( F(1, 471) = 0.91, p = .341, \eta_p^2 = .002 \)). 
% The results indicate that the assistance of AI can enhance labeling performance, and participants performed better than the control condition without AI.% This is not necessary as we already discussed above.
We then conducted a comparison of repeated measures in terms of the presence of \textit{Text Reasoning} and \textit{Inline Highlighting}. The results showed no significant main effects or interactions. 

% A one-way ANOVA was conducted to further examine the differences in performance across the five conditions, revealing a marginally significant effect ($F(5, 570) = 1.959, p = .083$). 
% Although directly using a t-test comparing the No Label condition with the Highlighting and Reasoning conditions showed significant differences ($p < .05$), further post hoc analysis using Tukey’s Honestly Significant Difference (HSD) test indicated that the significant difference only existed between the No Label condition and the Highlighting + Reasoning condition ($p_{\text{adj}} = .045$).

% Ken, this is too long, may need to make it more clear.
% Eason: Maybe just use repeated measure, no ANOVA. At least no two ANOVA at once.
% Ken: repeated measure is not "ANOVA", it is MNOVA. And my result is contradict

% Eason: may need to check if I use "pair t test" or independent t test

% Need to said what is the 2 x 2 for repeated measures

% Another way is display 4 means

When further comparing performance between different groups of users in different conditions, as shown in Table \ref{tab:performance_result}, we found that under the conditions of \textbf{Inline Highlighting, Highlighting + Text Reasoning}, and for overall average, the Prolific group significantly outperformed the Tutor group. The Prolific group also marginally significantly outperformed the Tutor group in the AI Label Only case. However, under the \textbf{conditions without any AI suggestions} and with \textbf{only Text Reasoning} explanations, the two groups had no significant difference in performance.

\subsection{Time Spent Comparison}


\begin{figure}[b!]
    \centering
    \includegraphics[width=1\linewidth]{image/time_compare.png}
    \caption{Average Time Spent by Condition and Group. The data is obtained after excluding outliers using the 1.5 interquartile range (IQR) method. Participants spend more time on conditions that have AI explanations. The error bar is Standard Error.}
    \label{fig:time_compare}
\end{figure}



\begin{table}[b!]
\scriptsize
    \centering
    \caption{Comparison of average time spent in seconds after IQR filtering across conditions for Tutor and Prolific participants, including in-significant t-test results.}
    \begin{tabular}{c c c c c c}
        \hline
        Condition & Tutor Mean (SD) & Prolific Mean (SD) & t-statistic & p-value & Cohen's d \\
        \hline
        No Label & 6.65 (3.95) & 7.24 (3.43) & -0.600 & 0.551 & -0.160 \\
        AI Label Only & 5.88 (3.36) & 6.87 (3.30) & -1.122 & 0.267 & -0.297 \\
        Reasoning & 10.34 (6.40) & 11.88 (5.13) & -0.995 & 0.324 & -0.266 \\
        Highlight & 11.11 (7.58) & 12.56 (7.61) & -0.722 & 0.473 & -0.191 \\
        Highlight + Reasoning & 11.86 (7.12) & 12.10 (6.04) & -0.140 & 0.889 & -0.037 \\
        \hline
    \end{tabular}
    
    \label{tab:avg_time}
\end{table}

The results are presented in Figure \ref{fig:time_compare} and Table \ref{tab:avg_time}. We conducted a repeated measure 2-way ANOVA to examine the main effects of \textbf{Inline Highlighting} and \textbf{Text Reasoning}, as well as their interaction with the data after IQR filtering. The analysis revealed a significant main effect respectively for \textbf{Inline Highlighting} (\( F(1, 57) = 25.74, p < .001, \eta_p^2 = .311 \)) and \textbf{Text Reasoning} (\( F(1, 57) = 19.12, p < .001, \eta_p^2 = .251 \)). Additionally, there was a \textbf{significant interaction effect} between Highlighting and Reasoning (\( F(1, 57) = 18.26, p = .001, \eta_p^2 = .243 \)). This interaction suggests that the combined effect of \textbf{Inline Highlighting} and \textbf{Text Reasoning} does not lead to a substantial increase in time; rather, their effects partially offset each other.
Moreover, we found no significant difference in time spent between participants from Tutor and Prolific.

% The one-way ANOVA results show a significant effect (\textbf{$F(4, 285) = 14.76, p < 0.001$}). Post hoc analysis using Tukey's HSD indicates that participants in the control group (with no AI label) and those in the condition with only an AI label but no explanation spent a similar amount of time. Additionally, both of these conditions required significantly less time than those that included \textbf{Highlighting, Reasoning, or a combination of both Highlighting and Reasoning} (\textbf{$ p_{\text{adj}} < .05$}).


\subsection{Over-reliance across groups}




% The analysis of the over-reliance percentage between Tutor and Prolific participants is shown in Figure \ref{fig:over-reliance}.
The results comparing over-reliance percentages for Tutors and Prolific participants across different task conditions are presented in Figure \ref{fig:over-reliance} and Table \ref{tab:over_reliance_result}. We conducted a one-way ANOVA to investigate differences in over-reliance rates across conditions, revealing a highly significant effect \((F(5, 570) = 16.4074, p < .0001)\). Post-hoc analysis using Tukey’s HSD showed that, for questions where the AI was incorrect, the control group without any AI labels or suggestions exhibited significantly lower over-reliance rates than participants who received AI suggestions (all \(p_{\text{adj}} < .05\)). These findings indicate that the presence of wrong AI suggestions led participants to make wrong decisions, whether in the form of labels, Text Reasoning, or Inline Highlighting.



\begin{figure}[b!]
    \centering
    \includegraphics[width=1\linewidth]{image/over-reliance.png}
    \caption{Average Over-Reliance Percentage, where users were wrong when AI's suggestion was wrong, across different conditions and groups. It can be observed that participants from Prolific are more prone to over-reliance. Error bar is Standard Error.}
    \label{fig:over-reliance}
\end{figure}


\begin{table}[b!]
    \centering
    \scriptsize
    \caption{Over-Reliance T-test Comparison Results Between Tutor and Prolific Participants.}
    \begin{tabular}{c c c c l c}
        \hline
        Condition & Tutor Mean (SD) & Prolific Mean (SD) & t-statistic & p-value \\
        \hline
        Highlighting & 0.654 (0.279) & 0.792 (0.215) & -2.696 & 0.0083** \\
        Highlighting + Reasoning & 0.718 (0.253) & 0.839 (0.175) & -2.702 & 0.0082** \\
        AI Label Only & 0.660 (0.211) & 0.755 (0.222) & -2.151 & 0.0340* \\
        No Label & 0.516 (0.230) & 0.536 (0.225) & -0.440 & 0.6611 \\
        Reasoning & 0.750 (0.233) & 0.797 (0.245) & -0.954 & 0.3426 \\
        Overall Average & 0.660 (0.165) & 0.744 (0.114) & -2.896 & 0.0047** \\
        \hline
        \\[-1.8ex]
        {$\textsuperscript{*}p < 0.05$, $\textsuperscript{**}p < 0.01$, $\textsuperscript{***}p < 0.001$}
    \end{tabular}
    \label{tab:over_reliance_result}
\end{table}




Additionally, the repeated-measures comparison indicated a significant main effect of \textbf{Text Reasoning} on over-reliance, (\( F(1, 94) = 8.81, p = .004, \eta_p^2 = .086 \)), whereas \textbf{Inline Highlighting} did not exhibit a significant effect, (\( F(1, 94) = 0.49, p = .482, \eta_p^2 = .005 \)). Furthermore, there was no significant interaction effect between \textbf{Inline Highlighting} and \textbf{Text Reasoning} (\( F(1, 94) = 0.069, p = .793, \eta_p^2 = .001 \)). These results suggest that the presence of \textbf{Text Reasoning} significantly increases participants' tendency toward overreliance, while \textbf{Inline Highlighting} does not have a notable impact.




When further comparing over-reliance between different groups of users, as shown in Table \ref{tab:over_reliance_result} and Figure \ref{fig:over-reliance}, we found that under the conditions of Highlighting, Highlighting + Reasoning, AI Label Only, and for overall average, Prolific participants significantly exhibited more over-reliance compared to the Tutor. However, under the conditions without any AI suggestions and with only Reasoning explanations, the two groups had no significant difference in over-reliance.



\subsection{Under-reliance across groups}

The results comparing under-reliance percentages for Tutors and Prolific participants across different conditions are presented in Figure \ref{fig:under-reliance} and Table \ref{tab:under_reliance_result}.


\begin{table}[b!]
    \scriptsize
    \centering
    \caption{Under-Reliance Comparison T-test Results Between Tutor and Prolific Participants.}
    \begin{tabular}{c c c c l c}
        \hline
        Condition & Tutor Mean (SD) & Prolific Mean (SD) & t-statistic & p-value \\
        \hline
        Highlighting & 0.202 (0.155) & 0.099 (0.111) & 3.742 & 0.0003** \\
        Highlighting + Reasoning & 0.160 (0.161) & 0.083 (0.112) & 2.721 & 0.0078** \\
        AI Label Only & 0.206 (0.168) & 0.134 (0.137) & 2.314 & 0.0228* \\
        No Label & 0.251 (0.151) & 0.223 (0.127) & 0.989 & 0.3254 \\
        Reasoning & 0.156 (0.163) & 0.115 (0.149) & 1.280 & 0.2038 \\
        Overall Average & 0.195 (0.122) & 0.131 (0.083) & 3.022 & 0.0032** \\
        \hline
        \\[-1.8ex]
        {$\textsuperscript{*}p < 0.05$, $\textsuperscript{**}p < 0.01$, $\textsuperscript{***}p < 0.001$}
    \end{tabular}
    \label{tab:under_reliance_result}
\end{table}


\begin{figure}[b!]
    \centering
    \includegraphics[width=1\linewidth]{image/under-reliance.png}
    \caption{Average Under-Reliance Percentage, where users were wrong even if AI's suggestions are correct, across different Conditions and Groups. It can be observed that tutors are more prone to under-reliance. The error bar is Standard Error.}
    \label{fig:under-reliance}
\end{figure}


A one-way ANOVA examined differences in under-reliance percentages across conditions and revealed a highly significant effect (\(F(5, 570) = 7.9017, p < .001\)). Subsequent Tukey’s HSD post-hoc tests indicated that, for questions that AI was correct, the control group without labels exhibited significantly higher error (under-reliance) rates compared to conditions with AI suggestions (\(p_{\text{adj}} = .05\)). This finding suggests that participants tend to trust the AI, and thus, when the AI is correct, they tend to respond correctly, too.

Additionally, the repeated-measures comparison indicated a significant main effect of \textbf{Text Reasoning} on underreliance (\( F(1, 94) = 8.71, p = .004, \eta_p^2 = .085 \)), whereas \textbf{Inline Highlighting} did not exhibit a significant effect (\( F(1, 94) = 2.28, p = .134, \eta_p^2 = .024 \)). Furthermore, there was no significant interaction effect between \textbf{Inline Highlighting} and \textbf{Text Reasoning} (\( F(1, 94) = 0.05, p = .944, \eta_p^2 = .000 \)). These results suggest that \textbf{Text Reasoning} effectively reduces underreliance, while \textbf{Inline Highlighting} does not have a notable impact.


When further comparing under-reliance between different groups of users, we found that under the conditions of \textbf{Inline Highlighting, Highlighting + Reasoning, and AI Label Only}, the Tutor group significantly exhibited more under-reliance compared to the Prolific group. However, under the \textbf{conditions without any AI suggestions} and with \textbf{Text Reasoning} explanations, the two groups had no significant difference in under-reliance.

\section{Discussion}

% Our findings reveal three main insights about human-AI collaborative decision-making in educational grading and feedback contexts.

\subsubsection{\textbf{Human-AI Collaboration vs. ``AI-Only'' Approaches.}}
First, while participants do benefit from AI-generated explanations---particularly when both \emph{text reasoning} and \emph{highlighting} are provided---the improvement remains modest. Across most conditions, overall human-AI team accuracy lags behind the AI-only baseline 0.8. This result suggests that, even when collaborating with AI in judging the correctness of tutor responses, humans fail to achieve a complementary effect in decision-making tasks. That is, unlike some earlier studies \cite{morrison2023impact,morrison2023evaluating,bansal2021does}, we did not observe a complementary effect whereby experts collaborating with AI outperformed either the AI by itself or the human expert working alone.

From a practical standpoint, these results pose a question about the necessity of human intervention in educational decision-making tasks. It may be the case that an ensemble of multiple AIs (e.g., a majority vote from different Large Language Models) surpasses both humans alone and human-AI teams. This trend raises concerns about efficiency and accountability: if an AI ensemble makes the final decision better than a human, and humans will be biased by AI, who is held accountable for errors? Future work should thus explore reliable, ``responsible'' automation methods that can ensure accountability---whether through transparency measures, oversight processes, or clear guidelines for error remediation.


\subsubsection{\textbf{Reliance on AI: Novice Over-Reliance vs. Expert Under-Reliance}}

Our findings are consistent with previous research showing that, despite its potential advantages, human judgment often leads to errors through either under-reliance or over-reliance on AI \cite{bansal2021does,vereschak2021evaluate,si2023large}. 
Moreover, we found that in the context of evaluating tutor responses, \emph{text reasoning} exerts the more pronounced influence on both sides of the trust dynamic. Specifically, \textit{textual reasoning} reduces \emph{under-reliance} (users become more convinced when AI is correct) but increases \emph{over-reliance} (users are less likely to reject incorrect AI suggestions). In contrast, \textit{inline highlighting} on its own did not show a significant effect. In contrast, inline highlighting alone did not show a significant effect. These findings aligned with previous research as well that \textit{inline highlighting} can reduce reliance \cite{bansal2021does,si2023large,morrison2023impact,morrison2023evaluating}.


Yet, neither explanation strategy significantly improved performance nor reached a complementary effect, which differs from previous research~\cite{bansal2021does,vereschak2021evaluate,morrison2023impact,morrison2023evaluating}. This suggests that current approaches still have room for improvement. Future research in AI-assisted educational decision-making should thus explore more adaptive explanation styles and strategies that mitigate both over- and under-reliance while still leveraging the benefits of AI assistance.

Moreover, we observed notable differences between experienced tutors (``experts'') and non-tutors recruited from Prolific (``novices''). Novices were more inclined to trust the AI’s suggestions, leading to higher overall accuracy but greater \emph{over-reliance} when the AI was wrong. Conversely, they demonstrated fewer \emph{under-reliance} errors when the AI was correct. In other words, novices consistently “followed” the AI—an approach that performs well overall if the AI maintains a high baseline accuracy. Experts, by contrast, occasionally under-relied on correct AI suggestions, reflecting skepticism or a sense of confidence in their own expertise. These findings align with prior research indicating that novices tend to rely on AI more \cite{morrison2023impact}. Future efforts in designing human-AI collaboration systems should, therefore, account for users’ varying levels of expertise and adapt explanation strategies to balance trust and skepticism accordingly.


\subsubsection{\textbf{Limitations and Generalizability}}

Our study has two limitations; firstly, despite the potential advantages of AI assistance, our study revealed that participants actually spent \emph{more time} on AI-assisted tasks. The experimental context may encourage participants to try to be as accurate as possible. In real-world settings (e.g., busy teachers juggling many tasks late at night), decision-making could be more time-constrained or subject to different motivational factors. Further work is needed to investigate how time pressures influence human-AI interaction: for example, do educators just follow AI and skip carefully reading its explanations if they are overloaded in real-world decision-making scenarios?

Moreover, our task involved a single type of decision-making task (evaluating the appropriateness of tutor praise). Other educational tasks---like open-ended response evaluation or essay grading---may pose different challenges for AI explanation and user reliance. Our results, therefore, highlight the need for broader investigations into how AI-assisted decision-making scales across diverse educational contexts. Identifying consistent patterns in over-reliance and under-reliance can guide better interface design, explanation style, and training protocols, leading to improved collaboration between humans and AI in education.

\section{Conclusion}
Our findings reinforce a complex reality: although AI explanations can boost performance under certain conditions—particularly for novices—designers must carefully navigate the interplay between over-reliance, under-reliance, and practical constraints such as time. In addition, the style of explanations must be deliberately selected and tailored. To fully harness AI’s potential, future research should investigate more on adaptive explanation approaches in AI-assisted decision-making within educational settings, explore ensemble methods that enhance performance, and establish nuanced accountability frameworks to mitigate potential reliance.
% Our study highlights both the promise and the pitfalls of integrating LLM-based decision support into tutoring contexts. We found that although AI can effectively evaluate tutor praise responses with high accuracy—thereby offering valuable feedback to human tutors—human judgment still introduces additional complexity. Novices benefited the most from AI’s generally accurate guidance, yet they were also highly prone to over-reliance when the AI erred. Conversely, experienced tutors sometimes dismissed correct AI suggestions, revealing a propensity for under-reliance.
% Overall, these findings reinforce a complex reality: while AI explanations can boost performance under certain conditions---particularly for novices---designers must navigate the interplay between over-reliance, under-reliance, and practical constraints such as time. To maximize benefits for teaching and learning, future research should explore more adaptive explanation designs, ensembles of AI that refine accuracy, and nuanced accountability frameworks for AI-driven educational assessment.

\bibliographystyle{splncs04}
\bibliography{reference}

\end{document}
