\section{Introduction}
\label{sec:intro}


\begin{figure}
% \vspace{-0.5em}

  \centering
  \includegraphics[width=1.\columnwidth]{Fig/CIR-CoT_fig1.pdf}
  \vspace{-2em}
  \caption{Comparison of three retrieval approaches: 
(a) VLM-based method; 
(b) MLLM-based method (treating the MLLM as an encoder); 
(c) our CIR-CoT approach, enhanced with Chain-of-Thought reasoning for more accurate image retrieval.}
  \label{fig:intro}
\vspace{-1em}
  
\end{figure}

% CIR是什么
Composed Image Retrieval (CIR) builds on traditional image retrieval~\cite{gordo2016deep,liu2016deepfashion,liu2018attentive,yang2022video} by allowing users to provide a reference image along with a modification instruction. This flexibility makes CIR particularly useful for applications like e-commerce product search, where users often look for visually similar items with specific variations.
% CIR的挑战是什么
To retrieve the desired target image, the key challenge in CIR task lies in reasoning over the visual content of the reference image and the semantics of the modification instruction in a unified manner. As a challenging multimodal retrieval task, CIR has attracted increasing attention in both academia and industry.

%主流的做法可以分为两类（VLM_based, MLLM based）
To address the CIR task, current research primarily follows two mainstream approaches. 
% VLM_based
The first category builds on the success of Vision-Language Models (VLMs)~\cite{radford2021learning,jia2021scaling}, as shown in Fig.~\ref{fig:intro} (a). Specifically, some methods~\cite{anwaar2021compositional,chen2020image,liu2021image,levy2024data,liu2023candidate} encode the reference image and the modification text separately using VLM encoders, and perform feature fusion to retrieve the target image.
% VLM_based 
Other methods~\cite{saito2023pic2word,bai2023sentence} go beyond such straightforward fusion by first transforming the reference image into a textual embedding using mechanisms like textual inversion~\cite{gal2022image}, which is then combined with the modification instruction. While this strategy enhances the model’s ability to interpret complex user intent, the gains remain limited. This highlights the need for stronger semantic reasoning across modalities.
% MLLM based 
Recently, Multimodal Large Language Models (MLLMs), such as LLaVA~\cite{liu2023visual,liu2024llavanext} and Qwen-VL~\cite{wang2024qwen2,bai2025qwen2}, have gained popularity for their strong multimodal reasoning capabilities. Inspired by this progress, various studies~\cite{jiang2024e5,liu2025lamra,zhang2024gme} explore the use of MLLMs for universal retrieval tasks.
% 确实有很大优势 （图1b）
Finetuning MLLMs specifically for the CIR task has only recently been attempted, as shown in Fig.~\ref{fig:intro} (b). Specifically, CIR-LVLM~\cite{sun2025leveraging} pioneers this direction, achieving strong performance in understanding user intent and aggregating hybrid-modality query features, thereby demonstrating the effectiveness and promise of MLLMs for CIR. 

% 但是上述方法都是黑盒
Despite these advances, existing retrieval methods, including both VLM-based approaches and recent MLLM-based solutions, largely treat the model as a black box. In other words, users have little visibility into how the model reasons over hybrid-modality queries, which makes it difficult to verify the retrieved results.
% 带有CoT推理的模型有什么帮助
An interpretable reasoning process is therefore essential, since it not only enables users to understand the rationale behind retrieval decisions but also guides the model to perform structured reasoning over multimodal inputs. Such reasoning allows the model to capture critical cross-modal interactions that might otherwise be overlooked, ultimately improving retrieval performance. As illustrated in Fig.~\ref{fig:intro} (c), our approach successfully retrieves the correct target image under a complex instruction, whereas prior methods fail and provide no interpretable rationale.

% 我们的方法。
Therefore, we propose CIR-CoT, an end-to-end retrieval-oriented MLLM that performs explicit reasoning over interleaved multimodal inputs. The main challenge in training CIR-CoT is the lack of structured reasoning annotations in existing CIR datasets, such as FashionIQ~\cite{wu2021fashion} and CIRR~\cite{liu2021image}, which only provide basic image–instruction pairs. Inspired by LLaVA-CoT~\cite{xu2024llava}, we extend existing datasets with enriched annotations. Instead of generating a direct reasoning chain, we employ a multistage reasoning approach to structure the annotations. Specifically, we leverage the powerful open-source multimodal model Qwen2.5-VL-72B~\cite{bai2025qwen2} to produce three-stage annotations:
\begin{enumerate}
    \item \textbf{Caption}: Extracting detailed visual features from the reference image.
    \item \textbf{Reasoning}: Deliberating on how to integrate the reference image and the modification instruction.
    \item \textbf{Conclusion}: Deriving a description of the target image that should be retrieved, based on the reasoning process.
\end{enumerate}
To ensure the accuracy of the annotations, we extract the Conclusion from each sample and conduct a multi-expert review, comparing it against the correct target image in the dataset and filtering out any samples with inconsistent or incorrect annotations.

Based on the annotated dataset, we train CIR-CoT in two stages. In the first stage, the model is pretrained on the pure-text NLI dataset~\cite{gao2021simcse} to enhance its summarization ability, enabling it to effectively compress information into the newly introduced \texttt{<emb>} token. In the second stage, we finetune the model on the extended CIRR and FashionIQ datasets. The goal is to guide the model to first produce a structured Chain-of-Thought reasoning output, and then encode the retrieval intent into the \texttt{<emb>} token embedding, which acts as the semantic representation for retrieval. By enforcing a structured reasoning process, the model is encouraged to explicitly examine cross-modal interactions, which improves its ability to capture fine-grained details and better interpret complex user intent. Meanwhile, this process also makes the retrieval procedure more transparent to users, providing interpretable rationales and moving beyond the traditional black-box paradigm of retrieval.

% 性能
To evaluate the effectiveness of CIR-CoT, we conduct experiments on in-domain datasets, FashionIQ and CIRR, as well as the out-of-domain dataset CIRCO~\cite{baldrati2023zero}. The results demonstrate that CIR-CoT not only achieves strong performance on in-domain benchmarks but also exhibits remarkable generalization ability on out-of-domain data.
\begin{figure*}
% \vspace{-0.5em}

  \centering
  \includegraphics[width=2\columnwidth]{Fig/CIR-CoT_fig2.pdf}
  % \vspace{-1em}
  \caption{The pipeline for constructing CoT training data. A multimodal query is processed through automated annotation to produce reasoning-augmented descriptions, followed by MLLM-based evaluation for quality control.}
  \label{fig:data}
\vspace{-1.2em}
  
\end{figure*}

% 贡献
In summary, the contributions of this paper are threefold:
\begin{itemize}[leftmargin=2em]
    \item We construct \textbf{structured CoT-annotated datasets} by extending FashionIQ and CIRR with structured CoT annotations, providing valuable resources for reasoning-oriented CIR research.
    
    \item We propose \textbf{CIR-CoT}, the first end-to-end retrieval-oriented MLLM that incorporates explicit Chain-of-Thought reasoning, enabling interpretable and more accurate compositional image retrieval.
    
    \item We conduct comprehensive experiments on both in-domain datasets (FashionIQ, CIRR) and the out-of-domain dataset (CIRCO), demonstrating that CIR-CoT achieves competitive retrieval performance and strong generalization ability.

\end{itemize}

