\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Agarwal et~al.(2025{\natexlab{a}})Agarwal, Panda, Charles, Patel, Kumar, Pattnayak, Rafi, Kumar, Meghwani, Gupta, and Chae}]{agarwal-etal-2025-mvtamperbench}
Amit Agarwal, Srikant Panda, Angeline Charles, Hitesh~Laxmichand Patel, Bhargava Kumar, Priyaranjan Pattnayak, Taki~Hasan Rafi, Tejaswini Kumar, Hansa Meghwani, Karan Gupta, and Dong-Kyu Chae. 2025{\natexlab{a}}.
\newblock \href {https://doi.org/10.18653/v1/2025.findings-acl.963} {{MVT}amper{B}ench: Evaluating robustness of vision-language models}.
\newblock In \emph{Findings of the Association for Computational Linguistics: ACL 2025}, pages 18804--18828, Vienna, Austria. Association for Computational Linguistics.

\bibitem[{Agarwal et~al.(2024{\natexlab{a}})Agarwal, Panda, and Pachauri}]{agarwal2024synthetic}
Amit Agarwal, Srikant Panda, and Kulbhushan Pachauri. 2024{\natexlab{a}}.
\newblock Synthetic document generation pipeline for training artificial intelligence models.
\newblock US Patent App. 17/994,712.

\bibitem[{Agarwal et~al.(2025{\natexlab{b}})Agarwal, Panda, and Pachauri}]{agarwal-etal-2025-fs}
Amit Agarwal, Srikant Panda, and Kulbhushan Pachauri. 2025{\natexlab{b}}.
\newblock \href {https://aclanthology.org/2025.coling-industry.9/} {{FS}-{DAG}: Few shot domain adapting graph networks for visually rich document understanding}.
\newblock In \emph{Proceedings of the 31st International Conference on Computational Linguistics: Industry Track}, pages 100--114, Abu Dhabi, UAE. Association for Computational Linguistics.

\bibitem[{Agarwal et~al.(2024{\natexlab{b}})Agarwal, Patel, Pattnayak, Panda, Kumar, and Kumar}]{agarwal2024enhancing}
Amit Agarwal, Hitesh Patel, Priyaranjan Pattnayak, Srikant Panda, Bhargava Kumar, and Tejaswini Kumar. 2024{\natexlab{b}}.
\newblock Enhancing document ai data generation through graph-based synthetic layouts.
\newblock \emph{arXiv preprint arXiv:2412.03590}.

\bibitem[{Banerjee and Lavie(2005)}]{banerjee2005meteor}
Satanjeev Banerjee and Alon Lavie. 2005.
\newblock Meteor: An automatic metric for mt evaluation with improved correlation with human judgments.
\newblock In \emph{Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization}, pages 65--72.

\bibitem[{Chen et~al.(2024{\natexlab{a}})Chen, Li, Dong, Zhang, Zang, Chen, Duan, Wang, Qiao, Lin, and Zhao}]{chen2024rightwayevaluatinglarge}
Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu~Qiao, Dahua Lin, and Feng Zhao. 2024{\natexlab{a}}.
\newblock \href {https://arxiv.org/abs/2403.20330} {Are we on the right way for evaluating large vision-language models?}
\newblock \emph{Preprint}, arXiv:2403.20330.

\bibitem[{Chen et~al.(2015)Chen, Fang, Lin, Vedantam, Gupta, Doll{\'a}r, and Zitnick}]{chen2015microsoft}
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll{\'a}r, and C~Lawrence Zitnick. 2015.
\newblock Microsoft coco captions: Data collection and evaluation server.
\newblock \emph{arXiv preprint arXiv:1504.00325}.

\bibitem[{Chen et~al.(2024{\natexlab{b}})Chen, Wang, Cao, Liu, Gao, Cui, Zhu, Ye, Tian, Liu et~al.}]{chen2024expanding}
Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et~al. 2024{\natexlab{b}}.
\newblock Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling.
\newblock \emph{arXiv preprint arXiv:2412.05271}.

\bibitem[{Chen et~al.(2024{\natexlab{c}})Chen, Wang, Tian, Ye, Gao, Cui, Tong, Hu, Luo, Ma et~al.}]{chen2024far}
Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et~al. 2024{\natexlab{c}}.
\newblock How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites.
\newblock \emph{arXiv preprint arXiv:2404.16821}.

\bibitem[{Chen et~al.(2024{\natexlab{d}})Chen, Wu, Wang, Su, Chen, Xing, Zhong, Zhang, Zhu, Lu et~al.}]{chen2024internvl}
Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et~al. 2024{\natexlab{d}}.
\newblock Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 24185--24198.

\bibitem[{Deitke et~al.(2024)Deitke, Clark, Lee, Tripathi, Yang, Park, Salehi, Muennighoff, Lo, Soldaini, Lu, Anderson, Bransom, Ehsani, Ngo, Chen, Patel, Yatskar, Callison-Burch, Head, Hendrix, Bastani, VanderBilt, Lambert, Chou, Chheda, Sparks, Skjonsberg, Schmitz, Sarnat, Bischoff, Walsh, Newell, Wolters, Gupta, Zeng, Borchardt, Groeneveld, Nam, Lebrecht, Wittlif, Schoenick, Michel, Krishna, Weihs, Smith, Hajishirzi, Girshick, Farhadi, and Kembhavi}]{deitke2024molmopixmoopenweights}
Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae~Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah~A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, and Aniruddha Kembhavi. 2024.
\newblock \href {https://arxiv.org/abs/2409.17146} {Molmo and pixmo: Open weights and open data for state-of-the-art vision-language models}.
\newblock \emph{Preprint}, arXiv:2409.17146.

\bibitem[{Duan et~al.(2024)Duan, Yang, Qiao, Fang, Chen, Liu, Dong, Zang, Zhang, Wang et~al.}]{duan2024vlmevalkit}
Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et~al. 2024.
\newblock Vlmevalkit: An open-source toolkit for evaluating large multi-modality models.
\newblock In \emph{Proceedings of the 32nd ACM international conference on multimedia}, pages 11198--11201.

\bibitem[{Fu et~al.(2024{\natexlab{a}})Fu, Chen, Shen, Qin, Zhang, Lin, Yang, Zheng, Li, Sun, Wu, and Ji}]{fu2024mmecomprehensiveevaluationbenchmark}
Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu~Lin, Jinrui Yang, Xiawu Zheng, Ke~Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. 2024{\natexlab{a}}.
\newblock \href {https://arxiv.org/abs/2306.13394} {Mme: A comprehensive evaluation benchmark for multimodal large language models}.
\newblock \emph{Preprint}, arXiv:2306.13394.

\bibitem[{Fu et~al.(2024{\natexlab{b}})Fu, Hu, Li, Feng, Wang, Lin, Roth, Smith, Ma, and Krishna}]{fu2024blinkmultimodallargelanguage}
Xingyu Fu, Yushi Hu, Bangzheng Li, Yu~Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah~A. Smith, Wei-Chiu Ma, and Ranjay Krishna. 2024{\natexlab{b}}.
\newblock \href {https://arxiv.org/abs/2404.12390} {Blink: Multimodal large language models can see but not perceive}.
\newblock \emph{Preprint}, arXiv:2404.12390.

\bibitem[{Gao et~al.(2024)Gao, Chen, Cui, Ren, Wang, Zhu, Tian, Ye, He, Zhu et~al.}]{gao2024mini}
Zhangwei Gao, Zhe Chen, Erfei Cui, Yiming Ren, Weiyun Wang, Jinguo Zhu, Hao Tian, Shenglong Ye, Junjun He, Xizhou Zhu, et~al. 2024.
\newblock Mini-internvl: A flexible-transfer pocket multimodal model with 5\% parameters and 90\% performance.
\newblock \emph{arXiv preprint arXiv:2410.16261}.

\bibitem[{Geirhos et~al.(2020)Geirhos, Jacobsen, Michaelis, Zemel, Brendel, Bethge, and Wichmann}]{geirhos2020shortcut}
Robert Geirhos, J{\"o}rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix~A Wichmann. 2020.
\newblock Shortcut learning in deep neural networks.
\newblock \emph{Nature Machine Intelligence}, 2(11):665--673.

\bibitem[{Guan et~al.(2024)Guan, Liu, Wu, Xian, Li, Liu, Wang, Chen, Huang, Yacoob, Manocha, and Zhou}]{guan2024hallusionbenchadvanceddiagnosticsuite}
Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. 2024.
\newblock \href {https://arxiv.org/abs/2310.14566} {Hallusionbench: An advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models}.
\newblock \emph{Preprint}, arXiv:2310.14566.

\bibitem[{Gurari et~al.(2018)Gurari, Li, Stangl, Guo, Lin, Grauman, Luo, and Bigham}]{gurari2018vizwizgrandchallengeanswering}
Danna Gurari, Qing Li, Abigale~J. Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey~P. Bigham. 2018.
\newblock \href {https://arxiv.org/abs/1802.08218} {Vizwiz grand challenge: Answering visual questions from blind people}.
\newblock \emph{Preprint}, arXiv:1802.08218.

\bibitem[{Hessel et~al.(2021)Hessel, Holtzman, Forbes, Bras, and Choi}]{hessel2021clipscore}
Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan~Le Bras, and Yejin Choi. 2021.
\newblock Clipscore: A reference-free evaluation metric for image captioning.
\newblock \emph{arXiv preprint arXiv:2104.08718}.

\bibitem[{Huang et~al.(2024)Huang, Zhang, Zha, Lu, and Guo}]{paper4}
Zhipeng Huang, Zhizheng Zhang, Zheng-Jun Zha, Yan Lu, and Baining Guo. 2024.
\newblock \href {https://doi.org/10.48550/arXiv.2403.12801} {Relationvlm: Making large vision-language models understand visual relations}.

\bibitem[{Hudson and Manning(2019)}]{hudson2019gqanewdatasetrealworld}
Drew~A. Hudson and Christopher~D. Manning. 2019.
\newblock \href {https://arxiv.org/abs/1902.09506} {Gqa: A new dataset for real-world visual reasoning and compositional question answering}.
\newblock \emph{Preprint}, arXiv:1902.09506.

\bibitem[{Kamath et~al.(2023)Kamath, Hessel, and Chang}]{whatsup2024}
Amita Kamath, Jack Hessel, and Kai-Wei Chang. 2023.
\newblock What's" up" with vision-language models? investigating their struggle with spatial reasoning.
\newblock \emph{arXiv preprint arXiv:2310.19785}.

\bibitem[{Kembhavi et~al.(2016)Kembhavi, Salvato, Kolve, Seo, Hajishirzi, and Farhadi}]{kembhavi2016diagramworthdozenimages}
Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. 2016.
\newblock \href {https://arxiv.org/abs/1603.07396} {A diagram is worth a dozen images}.
\newblock \emph{Preprint}, arXiv:1603.07396.

\bibitem[{Li et~al.(2023)Li, Du, Zhou, Wang, Zhao, and Wen}]{li2023evaluatingobjecthallucinationlarge}
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne~Xin Zhao, and Ji-Rong Wen. 2023.
\newblock \href {https://arxiv.org/abs/2305.10355} {Evaluating object hallucination in large vision-language models}.
\newblock \emph{Preprint}, arXiv:2305.10355.

\bibitem[{Lu et~al.(2022)Lu, Mishra, Xia, Qiu, Chang, Zhu, Tafjord, Clark, and Kalyan}]{lu2022learnexplainmultimodalreasoning}
Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022.
\newblock \href {https://arxiv.org/abs/2209.09513} {Learn to explain: Multimodal reasoning via thought chains for science question answering}.
\newblock \emph{Preprint}, arXiv:2209.09513.

\bibitem[{Masry et~al.(2022)Masry, Long, Tan, Joty, and Hoque}]{masry2022chartqabenchmarkquestionanswering}
Ahmed Masry, Do~Xuan Long, Jia~Qing Tan, Shafiq Joty, and Enamul Hoque. 2022.
\newblock \href {https://arxiv.org/abs/2203.10244} {Chartqa: A benchmark for question answering about charts with visual and logical reasoning}.
\newblock \emph{Preprint}, arXiv:2203.10244.

\bibitem[{Meghwani et~al.(2025)Meghwani, Agarwal, Pattnayak, Patel, and Panda}]{meghwani-etal-2025-hard}
Hansa Meghwani, Amit Agarwal, Priyaranjan Pattnayak, Hitesh~Laxmichand Patel, and Srikant Panda. 2025.
\newblock \href {https://doi.org/10.18653/v1/2025.acl-industry.72} {Hard negative mining for domain-specific retrieval in enterprise systems}.
\newblock In \emph{Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 6: Industry Track)}, pages 1013--1026, Vienna, Austria. Association for Computational Linguistics.

\bibitem[{Papineni et~al.(2002)Papineni, Roukos, Ward, and Zhu}]{papineni2002bleu}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.
\newblock Bleu: a method for automatic evaluation of machine translation.
\newblock In \emph{Proceedings of the 40th annual meeting of the Association for Computational Linguistics}, pages 311--318.

\bibitem[{Patel et~al.(2025)Patel, Agarwal, Das, Kumar, Panda, Pattnayak, Rafi, Kumar, and Chae}]{patel2025sweeval}
Hitesh~Laxmichand Patel, Amit Agarwal, Arion Das, Bhargava Kumar, Srikant Panda, Priyaranjan Pattnayak, Taki~Hasan Rafi, Tejaswini Kumar, and Dong-Kyu Chae. 2025.
\newblock Sweeval: Do llms really swear? a safety benchmark for testing limits for enterprise use.
\newblock In \emph{Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: Industry Track)}, pages 558--582.

\bibitem[{Patel et~al.(2024)Patel, Agarwal, Kumar, Gupta, and Pattnayak}]{patel2024llm}
Hitesh~Laxmichand Patel, Amit Agarwal, Bhargava Kumar, Karan Gupta, and Priyaranjan Pattnayak. 2024.
\newblock Llm for barcodes: Generating diverse synthetic data for identity documents.
\newblock \emph{arXiv preprint arXiv:2411.14962}.

\bibitem[{Pattnayak et~al.(2025{\natexlab{a}})Pattnayak, Agarwal, Meghwani, Patel, and Panda}]{pattnayak2025hybrid}
Priyaranjan Pattnayak, Amit Agarwal, Hansa Meghwani, Hitesh~Laxmichand Patel, and Srikant Panda. 2025{\natexlab{a}}.
\newblock Hybrid ai for responsive multi-turn online conversations with novel dynamic routing and feedback adaptation.
\newblock In \emph{Proceedings of the 4th International Workshop on Knowledge-Augmented Methods for Natural Language Processing}, pages 215--229.

\bibitem[{Pattnayak et~al.(2025{\natexlab{b}})Pattnayak, Patel, Agarwal, Kumar, Panda, and Kumar}]{pattnayak2025clinicalqa20multitask}
Priyaranjan Pattnayak, Hitesh~Laxmichand Patel, Amit Agarwal, Bhargava Kumar, Srikant Panda, and Tejaswini Kumar. 2025{\natexlab{b}}.
\newblock \href {https://arxiv.org/abs/2502.13108} {Clinical qa 2.0: Multi-task learning for answer extraction and categorization}.
\newblock \emph{Preprint}, arXiv:2502.13108.

\bibitem[{Pattnayak et~al.(2024)Pattnayak, Patel, Kumar, Agarwal, Banerjee, Panda, and Kumar}]{pattnayak2024survey}
Priyaranjan Pattnayak, Hitesh~Laxmichand Patel, Bhargava Kumar, Amit Agarwal, Ishan Banerjee, Srikant Panda, and Tejaswini Kumar. 2024.
\newblock Survey of large multimodal model datasets, application categories and taxonomy.
\newblock \emph{arXiv preprint arXiv:2412.17759}.

\bibitem[{Peng et~al.(2024)Peng, Xie, You, Lan, and Wu}]{spec2024}
Wujian Peng, Sicheng Xie, Zuyao You, Shiyi Lan, and Zuxuan Wu. 2024.
\newblock Synthesize diagnose and optimize: Towards fine-grained vision-language understanding.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 13279--13288.

\bibitem[{Singh et~al.(2019)Singh, Natarajan, Shah, Jiang, Chen, Batra, Parikh, and Rohrbach}]{singh2019vqamodelsread}
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu~Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. 2019.
\newblock \href {https://arxiv.org/abs/1904.08920} {Towards vqa models that can read}.
\newblock \emph{Preprint}, arXiv:1904.08920.

\bibitem[{Tao et~al.(2024)Tao, Huang, Xu, Chen, Feng, and Zhao}]{tao2024probingmultimodallargelanguage}
Mingxu Tao, Quzhe Huang, Kun Xu, Liwei Chen, Yansong Feng, and Dongyan Zhao. 2024.
\newblock \href {https://arxiv.org/abs/2402.17304} {Probing multimodal large language models for global and local semantic representations}.
\newblock \emph{Preprint}, arXiv:2402.17304.

\bibitem[{Vedantam et~al.(2015)Vedantam, Zitnick, and Parikh}]{vedantam2015ciderconsensusbasedimagedescription}
Ramakrishna Vedantam, C.~Lawrence Zitnick, and Devi Parikh. 2015.
\newblock \href {https://arxiv.org/abs/1411.5726} {Cider: Consensus-based image description evaluation}.
\newblock \emph{Preprint}, arXiv:1411.5726.

\bibitem[{Wang et~al.(2024{\natexlab{a}})Wang, Wang, Xu, Zhang, Gu, Jia, Wang, Xu, Yan, Zhang, and Sang}]{wang2024amberllmfreemultidimensionalbenchmark}
Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Jiaqi Wang, Haiyang Xu, Ming Yan, Ji~Zhang, and Jitao Sang. 2024{\natexlab{a}}.
\newblock \href {https://arxiv.org/abs/2311.07397} {Amber: An llm-free multi-dimensional benchmark for mllms hallucination evaluation}.
\newblock \emph{Preprint}, arXiv:2311.07397.

\bibitem[{Wang et~al.(2024{\natexlab{b}})Wang, Bai, Tan, Wang, Fan, Bai, Chen, Liu, Wang, Ge, Fan, Dang, Du, Ren, Men, Liu, Zhou, Zhou, and Lin}]{wang2024qwen2}
Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. 2024{\natexlab{b}}.
\newblock \href {https://arxiv.org/abs/2409.12191} {Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution}.
\newblock \emph{Preprint}, arXiv:2409.12191.

\bibitem[{Woh et~al.(2022)Woh, Lee, joong Kim, and Lee}]{paper3}
Sangmyeong Woh, Jaemin Lee, Ho~joong Kim, and Jinsuk Lee. 2022.
\newblock \href {https://doi.org/10.48550/arXiv.2207.08333} {Towards the human global context: Does the vision-language model really judge like a human being?}

\bibitem[{Wu et~al.(2024)Wu, Zhao, Saxon, Bui, Wang, Zhang, and Chang}]{wu2024vspassessingdualchallenges}
Qiucheng Wu, Handong Zhao, Michael Saxon, Trung Bui, William~Yang Wang, Yang Zhang, and Shiyu Chang. 2024.
\newblock \href {https://arxiv.org/abs/2407.01863} {Vsp: Assessing the dual challenges of perception and reasoning in spatial planning tasks for vlms}.
\newblock \emph{Preprint}, arXiv:2407.01863.

\bibitem[{XAI-Org(2024)}]{RealWorldQA}
XAI-Org. 2024.
\newblock Realworldqa dataset.
\newblock \url{https://huggingface.co/datasets/xai-org/RealworldQA}.
\newblock Accessed: 2024-03-15.

\bibitem[{Yu et~al.(2024)Yu, Yu, and Wang}]{yu2024attention}
Runpeng Yu, Weihao Yu, and Xinchao Wang. 2024.
\newblock Attention prompting on image for large vision-language models.
\newblock In \emph{European Conference on Computer Vision}, pages 251--268. Springer.

\bibitem[{Yu et~al.(2021)Yu, Zhang, and Deng}]{fid_score}
Yu~Yu, Weibin Zhang, and Yun Deng. 2021.
\newblock Frechet inception distance (fid) for evaluating gans.
\newblock \emph{China University of Mining Technology Beijing Graduate School}, 3(11).

\bibitem[{Zhao et~al.(2023)Zhao, Xie, Zhuang, and Yu}]{zhaoarticle}
Ruibin Zhao, Zhiwei Xie, Yipeng Zhuang, and Philip Yu. 2023.
\newblock \href {https://doi.org/10.1142/S0129065724500096} {Automated quality evaluation of large-scale benchmark datasets for vision-language tasks}.
\newblock \emph{International Journal of Neural Systems}.

\end{thebibliography}
