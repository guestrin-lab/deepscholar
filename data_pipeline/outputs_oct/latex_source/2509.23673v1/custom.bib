% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@misc{hudson2019gqanewdatasetrealworld,
      title={GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering}, 
      author={Drew A. Hudson and Christopher D. Manning},
      year={2019},
      eprint={1902.09506},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1902.09506}, 
}

@misc{lu2022learnexplainmultimodalreasoning,
      title={Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering}, 
      author={Pan Lu and Swaroop Mishra and Tony Xia and Liang Qiu and Kai-Wei Chang and Song-Chun Zhu and Oyvind Tafjord and Peter Clark and Ashwin Kalyan},
      year={2022},
      eprint={2209.09513},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2209.09513}, 
}

@misc{gurari2018vizwizgrandchallengeanswering,
      title={VizWiz Grand Challenge: Answering Visual Questions from Blind People}, 
      author={Danna Gurari and Qing Li and Abigale J. Stangl and Anhong Guo and Chi Lin and Kristen Grauman and Jiebo Luo and Jeffrey P. Bigham},
      year={2018},
      eprint={1802.08218},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1802.08218}, 
}

@misc{singh2019vqamodelsread,
      title={Towards VQA Models That Can Read}, 
      author={Amanpreet Singh and Vivek Natarajan and Meet Shah and Yu Jiang and Xinlei Chen and Dhruv Batra and Devi Parikh and Marcus Rohrbach},
      year={2019},
      eprint={1904.08920},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1904.08920}, 
}

@misc{wang2024amberllmfreemultidimensionalbenchmark,
      title={AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation}, 
      author={Junyang Wang and Yuhang Wang and Guohai Xu and Jing Zhang and Yukai Gu and Haitao Jia and Jiaqi Wang and Haiyang Xu and Ming Yan and Ji Zhang and Jitao Sang},
      year={2024},
      eprint={2311.07397},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.07397}, 
}

@misc{fu2024blinkmultimodallargelanguage,
      title={BLINK: Multimodal Large Language Models Can See but Not Perceive}, 
      author={Xingyu Fu and Yushi Hu and Bangzheng Li and Yu Feng and Haoyu Wang and Xudong Lin and Dan Roth and Noah A. Smith and Wei-Chiu Ma and Ranjay Krishna},
      year={2024},
      eprint={2404.12390},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2404.12390}, 
}

@misc{guan2024hallusionbenchadvanceddiagnosticsuite,
      title={HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models}, 
      author={Tianrui Guan and Fuxiao Liu and Xiyang Wu and Ruiqi Xian and Zongxia Li and Xiaoyu Liu and Xijun Wang and Lichang Chen and Furong Huang and Yaser Yacoob and Dinesh Manocha and Tianyi Zhou},
      year={2024},
      eprint={2310.14566},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2310.14566}, 
}

@misc{mathew2021infographicvqa,
      title={InfographicVQA}, 
      author={Minesh Mathew and Viraj Bagal and Rubèn Pérez Tito and Dimosthenis Karatzas and Ernest Valveny and C. V Jawahar},
      year={2021},
      eprint={2104.12756},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2104.12756}, 
}

@misc{fu2024mmecomprehensiveevaluationbenchmark,
      title={MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models}, 
      author={Chaoyou Fu and Peixian Chen and Yunhang Shen and Yulei Qin and Mengdan Zhang and Xu Lin and Jinrui Yang and Xiawu Zheng and Ke Li and Xing Sun and Yunsheng Wu and Rongrong Ji},
      year={2024},
      eprint={2306.13394},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2306.13394}, 
}

@misc{li2023evaluatingobjecthallucinationlarge,
      title={Evaluating Object Hallucination in Large Vision-Language Models}, 
      author={Yifan Li and Yifan Du and Kun Zhou and Jinpeng Wang and Wayne Xin Zhao and Ji-Rong Wen},
      year={2023},
      eprint={2305.10355},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2305.10355}, 
}

@misc{kembhavi2016diagramworthdozenimages,
      title={A Diagram Is Worth A Dozen Images}, 
      author={Aniruddha Kembhavi and Mike Salvato and Eric Kolve and Minjoon Seo and Hannaneh Hajishirzi and Ali Farhadi},
      year={2016},
      eprint={1603.07396},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1603.07396}, 
}

@misc{masry2022chartqabenchmarkquestionanswering,
      title={ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning}, 
      author={Ahmed Masry and Do Xuan Long and Jia Qing Tan and Shafiq Joty and Enamul Hoque},
      year={2022},
      eprint={2203.10244},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.10244}, 
}

@misc{wang2024muirbenchcomprehensivebenchmarkrobust,
      title={MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding}, 
      author={Fei Wang and Xingyu Fu and James Y. Huang and Zekun Li and Qin Liu and Xiaogeng Liu and Mingyu Derek Ma and Nan Xu and Wenxuan Zhou and Kai Zhang and Tianyi Lorena Yan and Wenjie Jacky Mo and Hsiang-Hui Liu and Pan Lu and Chunyuan Li and Chaowei Xiao and Kai-Wei Chang and Dan Roth and Sheng Zhang and Hoifung Poon and Muhao Chen},
      year={2024},
      eprint={2406.09411},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.09411}, 
}

@misc{chen2024rightwayevaluatinglarge,
      title={Are We on the Right Way for Evaluating Large Vision-Language Models?}, 
      author={Lin Chen and Jinsong Li and Xiaoyi Dong and Pan Zhang and Yuhang Zang and Zehui Chen and Haodong Duan and Jiaqi Wang and Yu Qiao and Dahua Lin and Feng Zhao},
      year={2024},
      eprint={2403.20330},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2403.20330}, 
}

@misc{yue2024mmmumassivemultidisciplinemultimodal,
      title={MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI}, 
      author={Xiang Yue and Yuansheng Ni and Kai Zhang and Tianyu Zheng and Ruoqi Liu and Ge Zhang and Samuel Stevens and Dongfu Jiang and Weiming Ren and Yuxuan Sun and Cong Wei and Botao Yu and Ruibin Yuan and Renliang Sun and Ming Yin and Boyuan Zheng and Zhenzhu Yang and Yibo Liu and Wenhao Huang and Huan Sun and Yu Su and Wenhu Chen},
      year={2024},
      eprint={2311.16502},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.16502}, 
}

@misc{chen2015microsoftcococaptionsdata,
      title={Microsoft COCO Captions: Data Collection and Evaluation Server}, 
      author={Xinlei Chen and Hao Fang and Tsung-Yi Lin and Ramakrishna Vedantam and Saurabh Gupta and Piotr Dollar and C. Lawrence Zitnick},
      year={2015},
      eprint={1504.00325},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1504.00325}, 
}

@misc{wang2024qwen2vlenhancingvisionlanguagemodels,
      title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution}, 
      author={Peng Wang and Shuai Bai and Sinan Tan and Shijie Wang and Zhihao Fan and Jinze Bai and Keqin Chen and Xuejing Liu and Jialin Wang and Wenbin Ge and Yang Fan and Kai Dang and Mengfei Du and Xuancheng Ren and Rui Men and Dayiheng Liu and Chang Zhou and Jingren Zhou and Junyang Lin},
      year={2024},
      eprint={2409.12191},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.12191}, 
}

@article{deitke2024molmo,
  title={Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models},
  author={Deitke, Matt and Clark, Christopher and Lee, Sangho and Tripathi, Rohun and Yang, Yue and Park, Jae Sung and Salehi, Mohammadreza and Muennighoff, Niklas and Lo, Kyle and Soldaini, Luca and others},
  journal={arXiv preprint arXiv:2409.17146},
  year={2024}
}

@article{chen2024expanding,
  title={Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling},
  author={Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others},
  journal={arXiv preprint arXiv:2412.05271},
  year={2024}
}

@misc{chen2024vltpvisionlanguageguidedtoken,
      title={VLTP: Vision-Language Guided Token Pruning for Task-Oriented Segmentation}, 
      author={Hanning Chen and Yang Ni and Wenjun Huang and Yezi Liu and SungHeon Jeong and Fei Wen and Nathaniel Bastian and Hugo Latapie and Mohsen Imani},
      year={2024},
      eprint={2409.08464},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.08464}, 
}

@misc{yang2023finegrainedvisualprompting,
      title={Fine-Grained Visual Prompting}, 
      author={Lingfeng Yang and Yueze Wang and Xiang Li and Xinlong Wang and Jian Yang},
      year={2023},
      eprint={2306.04356},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2306.04356}, 
}

@misc{vedantam2015ciderconsensusbasedimagedescription,
      title={CIDEr: Consensus-based Image Description Evaluation}, 
      author={Ramakrishna Vedantam and C. Lawrence Zitnick and Devi Parikh},
      year={2015},
      eprint={1411.5726},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1411.5726}, 
}

@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@inproceedings{banerjee2005meteor,
  title={METEOR: An automatic metric for MT evaluation with improved correlation with human judgments},
  author={Banerjee, Satanjeev and Lavie, Alon},
  booktitle={Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization},
  pages={65--72},
  year={2005}
}

@misc{tao2024probingmultimodallargelanguage,
      title={Probing Multimodal Large Language Models for Global and Local Semantic Representations}, 
      author={Mingxu Tao and Quzhe Huang and Kun Xu and Liwei Chen and Yansong Feng and Dongyan Zhao},
      year={2024},
      eprint={2402.17304},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.17304}, 
}

@article{zhaoarticle,
author = {Zhao, Ruibin and Xie, Zhiwei and Zhuang, Yipeng and Yu, Philip},
year = {2023},
month = {11},
pages = {},
title = {Automated Quality Evaluation of Large-Scale Benchmark Datasets for Vision-Language Tasks},
journal = {International Journal of Neural Systems},
doi = {10.1142/S0129065724500096}
}

@misc{wu2024vspassessingdualchallenges,
      title={VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs}, 
      author={Qiucheng Wu and Handong Zhao and Michael Saxon and Trung Bui and William Yang Wang and Yang Zhang and Shiyu Chang},
      year={2024},
      eprint={2407.01863},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.01863}, 
}

@misc{woh2022humanglobalcontextdoes,
      title={Towards the Human Global Context: Does the Vision-Language Model Really Judge Like a Human Being?}, 
      author={Sangmyeong Woh and Jaemin Lee and Ho Joong Kim and Jinsuk Lee},
      year={2022},
      eprint={2207.08333},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2207.08333}, 
}

@misc{paper1,
  author = {Zhenlin Xu and Yi Zhu and Tiffany Deng and Abhay Mittal and Yanbei Chen and Manchen Wang and P. Favaro and Joseph Tighe and Davide Modolo},
  title = {Benchmarking Zero-Shot Recognition with Vision-Language Models: Challenges on Granularity and Specificity},
  journal = {2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  year = {2023},
  pages = {1827-1836},
  doi = {10.1109/CVPRW63382.2024.00189},
  
}

@misc{paper3,
  author = {Sangmyeong Woh and Jaemin Lee and Ho joong Kim and Jinsuk Lee},
  title = {Towards the Human Global Context: Does the Vision-Language Model Really Judge Like a Human Being?},
  journal = {ArXiv},
  year = {2022},
  volume = {abs/2207.08333},
  doi = {10.48550/arXiv.2207.08333},

}

@misc{paper4,
  author = {Zhipeng Huang and Zhizheng Zhang and Zheng-Jun Zha and Yan Lu and Baining Guo},
  title = {RelationVLM: Making Large Vision-Language Models Understand Visual Relations},
  journal = {ArXiv},
  year = {2024},
  volume = {abs/2403.12801},
  doi = {10.48550/arXiv.2403.12801},
 
}

@article{geirhos2020shortcut,
  title={Shortcut learning in deep neural networks},
  author={Geirhos, Robert and Jacobsen, J{\"o}rn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A},
  journal={Nature Machine Intelligence},
  volume={2},
  number={11},
  pages={665--673},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@article{risser2022survey,
  title={A survey of identification and mitigation of machine learning algorithmic biases in image analysis},
  author={Risser, Laurent and Picard, Agustin and Hervier, Lucas and Loubes, Jean-Michel},
  journal={arXiv preprint arXiv:2210.04491},
  year={2022}
}

@article{zhao2024automated,
  title={Automated quality evaluation of large-scale benchmark datasets for vision-language tasks},
  author={Zhao, Ruibin and Xie, Zhiwei and Zhuang, Yipeng and LH Yu, Philip},
  journal={International Journal of Neural Systems},
  volume={34},
  number={03},
  pages={2450009},
  year={2024},
  publisher={World Scientific}
}

@article{vsp2024,
  title={Vsp: Assessing the dual challenges of perception and reasoning in spatial planning tasks for vlms},
  author={Wu, Qiucheng and Zhao, Handong and Saxon, Michael and Bui, Trung and Wang, William Yang and Zhang, Yang and Chang, Shiyu},
  journal={arXiv preprint arXiv:2407.01863},
  year={2024}
}

@article{whatsup2024,
  title={What's" up" with vision-language models? Investigating their struggle with spatial reasoning},
  author={Kamath, Amita and Hessel, Jack and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:2310.19785},
  year={2023}
}

@inproceedings{spec2024,
  title={Synthesize diagnose and optimize: Towards fine-grained vision-language understanding},
  author={Peng, Wujian and Xie, Sicheng and You, Zuyao and Lan, Shiyi and Wu, Zuxuan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13279--13288},
  year={2024}
}

@article{humanglobal2024,
  title={Towards the Human Global Context: Does the Vision-Language Model Really Judge Like a Human Being?},
  author={Woh, Sangmyeong and Lee, Jaemin and Kim, Ho Joong and Lee, Jinsuk},
  journal={arXiv preprint arXiv:2207.08333},
  year={2022}
}

@article{fid_score,
  title={Frechet inception distance (fid) for evaluating gans},
  author={Yu, Yu and Zhang, Weibin and Deng, Yun},
  journal={China University of Mining Technology Beijing Graduate School},
  volume={3},
  number={11},
  year={2021}
}

@article{clip_score,
  title={An examination of the robustness of reference-free image captioning evaluation metrics},
  author={Ahmadi, Saba and Agrawal, Aishwarya},
  journal={arXiv preprint arXiv:2305.14998},
  year={2023}
}
@article{hessel2021clipscore,
  title={Clipscore: A reference-free evaluation metric for image captioning},
  author={Hessel, Jack and Holtzman, Ari and Forbes, Maxwell and Bras, Ronan Le and Choi, Yejin},
  journal={arXiv preprint arXiv:2104.08718},
  year={2021}
}

@inproceedings{gurari2018vizwiz,
  title={Vizwiz grand challenge: Answering visual questions from blind people},
  author={Gurari, Danna and Li, Qing and Stangl, Abigale J and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3608--3617},
  year={2018}
}


@inproceedings{singh2019towards,
  title={Towards vqa models that can read},
  author={Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={8317--8326},
  year={2019}
}

@article{chen2015microsoft,
  title={Microsoft coco captions: Data collection and evaluation server},
  author={Chen, Xinlei and Fang, Hao and Lin, Tsung-Yi and Vedantam, Ramakrishna and Gupta, Saurabh and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  journal={arXiv preprint arXiv:1504.00325},
  year={2015}
}

@inproceedings{yu2024attention,
  title={Attention prompting on image for large vision-language models},
  author={Yu, Runpeng and Yu, Weihao and Wang, Xinchao},
  booktitle={European Conference on Computer Vision},
  pages={251--268},
  year={2024},
  organization={Springer}
}

@inproceedings{duan2024vlmevalkit,
  title={Vlmevalkit: An open-source toolkit for evaluating large multi-modality models},
  author={Duan, Haodong and Yang, Junming and Qiao, Yuxuan and Fang, Xinyu and Chen, Lin and Liu, Yuan and Dong, Xiaoyi and Zang, Yuhang and Zhang, Pan and Wang, Jiaqi and others},
  booktitle={Proceedings of the 32nd ACM international conference on multimedia},
  pages={11198--11201},
  year={2024}
}

@misc{RealWorldQA,
  author = {XAI-Org},
  title = {RealWorldQA Dataset},
  howpublished = {\url{https://huggingface.co/datasets/xai-org/RealworldQA}},
  year = {2024},
  note = {Accessed: 2024-03-15}
}


 @article{chen2024far,
  title={How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites},
  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
  journal={arXiv preprint arXiv:2404.16821},
  year={2024}
 }

 @article{gao2024mini,
  title={Mini-internvl: A flexible-transfer pocket multimodal model with 5\% parameters and 90\% performance},
  author={Gao, Zhangwei and Chen, Zhe and Cui, Erfei and Ren, Yiming and Wang, Weiyun and Zhu, Jinguo and Tian, Hao and Ye, Shenglong and He, Junjun and Zhu, Xizhou and others},
  journal={arXiv preprint arXiv:2410.16261},
  year={2024}
 }
 @inproceedings{chen2024internvl,
  title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={24185--24198},
  year={2024}
 }

@misc{wang2024qwen2,
      title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution}, 
      author={Peng Wang and Shuai Bai and Sinan Tan and Shijie Wang and Zhihao Fan and Jinze Bai and Keqin Chen and Xuejing Liu and Jialin Wang and Wenbin Ge and Yang Fan and Kai Dang and Mengfei Du and Xuancheng Ren and Rui Men and Dayiheng Liu and Chang Zhou and Jingren Zhou and Junyang Lin},
      year={2024},
      eprint={2409.12191},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.12191}, 
}

@misc{deitke2024molmopixmoopenweights,
      title={Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models}, 
      author={Matt Deitke and Christopher Clark and Sangho Lee and Rohun Tripathi and Yue Yang and Jae Sung Park and Mohammadreza Salehi and Niklas Muennighoff and Kyle Lo and Luca Soldaini and Jiasen Lu and Taira Anderson and Erin Bransom and Kiana Ehsani and Huong Ngo and YenSung Chen and Ajay Patel and Mark Yatskar and Chris Callison-Burch and Andrew Head and Rose Hendrix and Favyen Bastani and Eli VanderBilt and Nathan Lambert and Yvonne Chou and Arnavi Chheda and Jenna Sparks and Sam Skjonsberg and Michael Schmitz and Aaron Sarnat and Byron Bischoff and Pete Walsh and Chris Newell and Piper Wolters and Tanmay Gupta and Kuo-Hao Zeng and Jon Borchardt and Dirk Groeneveld and Crystal Nam and Sophie Lebrecht and Caitlin Wittlif and Carissa Schoenick and Oscar Michel and Ranjay Krishna and Luca Weihs and Noah A. Smith and Hannaneh Hajishirzi and Ross Girshick and Ali Farhadi and Aniruddha Kembhavi},
      year={2024},
      eprint={2409.17146},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.17146}, 
}

@misc{pattnayak2025clinicalqa20multitask,
      title={Clinical QA 2.0: Multi-Task Learning for Answer Extraction and Categorization}, 
      author={Priyaranjan Pattnayak and Hitesh Laxmichand Patel and Amit Agarwal and Bhargava Kumar and Srikant Panda and Tejaswini Kumar},
      year={2025},
      eprint={2502.13108},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.13108}, 
}

@inproceedings{agarwal-etal-2025-mvtamperbench,
    title = "{MVT}amper{B}ench: Evaluating Robustness of Vision-Language Models",
    author = "Agarwal, Amit  and
      Panda, Srikant  and
      Charles, Angeline  and
      Patel, Hitesh Laxmichand  and
      Kumar, Bhargava  and
      Pattnayak, Priyaranjan  and
      Rafi, Taki Hasan  and
      Kumar, Tejaswini  and
      Meghwani, Hansa  and
      Gupta, Karan  and
      Chae, Dong-Kyu",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2025",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-acl.963/",
    doi = "10.18653/v1/2025.findings-acl.963",
    pages = "18804--18828",
    ISBN = "979-8-89176-256-5"
}

@article{pattnayak2024survey,
  title={Survey of Large Multimodal Model Datasets, Application Categories and Taxonomy},
  author={Pattnayak, Priyaranjan and Patel, Hitesh Laxmichand and Kumar, Bhargava and Agarwal, Amit and Banerjee, Ishan and Panda, Srikant and Kumar, Tejaswini},
  journal={arXiv preprint arXiv:2412.17759},
  year={2024}
}

@article{agarwal2024enhancing,
  title={Enhancing Document AI Data Generation Through Graph-Based Synthetic Layouts},
  author={Agarwal, Amit and Patel, Hitesh and Pattnayak, Priyaranjan and Panda, Srikant and Kumar, Bhargava and Kumar, Tejaswini},
  journal={arXiv preprint arXiv:2412.03590},
  year={2024}
}

@article{patel2024llm,
  title={LLM for Barcodes: Generating Diverse Synthetic Data for Identity Documents},
  author={Patel, Hitesh Laxmichand and Agarwal, Amit and Kumar, Bhargava and Gupta, Karan and Pattnayak, Priyaranjan},
  journal={arXiv preprint arXiv:2411.14962},
  year={2024}
}


@inproceedings{agarwal-etal-2025-fs,
    title = "{FS}-{DAG}: Few Shot Domain Adapting Graph Networks for Visually Rich Document Understanding",
    author = "Agarwal, Amit  and
      Panda, Srikant  and
      Pachauri, Kulbhushan",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven  and
      Darwish, Kareem  and
      Agarwal, Apoorv",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics: Industry Track",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-industry.9/",
    pages = "100--114"
}

@inproceedings{meghwani-etal-2025-hard,
    title = "Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems",
    author = "Meghwani, Hansa  and
      Agarwal, Amit  and
      Pattnayak, Priyaranjan  and
      Patel, Hitesh Laxmichand  and
      Panda, Srikant",
    editor = "Rehm, Georg  and
      Li, Yunyao",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 6: Industry Track)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-industry.72/",
    doi = "10.18653/v1/2025.acl-industry.72",
    pages = "1013--1026",
    ISBN = "979-8-89176-288-6"
}

@inproceedings{pattnayak2025hybrid,
  title={Hybrid AI for Responsive Multi-Turn Online Conversations with Novel Dynamic Routing and Feedback Adaptation},
  author={Pattnayak, Priyaranjan and Agarwal, Amit and Meghwani, Hansa and Patel, Hitesh Laxmichand and Panda, Srikant},
  booktitle={Proceedings of the 4th International Workshop on Knowledge-Augmented Methods for Natural Language Processing},
  pages={215--229},
  year={2025}
}

@inproceedings{patel2025sweeval,
  title={SweEval: Do LLMs Really Swear? A Safety Benchmark for Testing Limits for Enterprise Use},
  author={Patel, Hitesh Laxmichand and Agarwal, Amit and Das, Arion and Kumar, Bhargava and Panda, Srikant and Pattnayak, Priyaranjan and Rafi, Taki Hasan and Kumar, Tejaswini and Chae, Dong-Kyu},
  booktitle={Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: Industry Track)},
  pages={558--582},
  year={2025}
}

@misc{agarwal2024synthetic,
  title={Synthetic document generation pipeline for training artificial intelligence models},
  author={Agarwal, Amit and Panda, Srikant and Pachauri, Kulbhushan},
  year={2024},
  publisher={Google Patents},
  note={US Patent App. 17/994,712}
}

@misc{panda2025out,
  title={Out of distribution element detection for information extraction},
  author={Panda, Srikant and Agarwal, Amit and Nambirajan, Gouttham and Pachauri, Kulbhushan},
  year={2025},
  publisher={Google Patents},
  note={US Patent App. 18/347,983}
}