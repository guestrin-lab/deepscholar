\PassOptionsToPackage{table}{xcolor}
\documentclass{article} % For LaTeX2e
\usepackage{iclr2026_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{float}
\usepackage{makecell}       % colors
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{enumitem}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{enumitem}

\definecolor{customRed}{RGB}{255,0,0}
\definecolor{customYellow}{RGB}{255,204,0}
\definecolor{customBlue}{RGB}{0,102,204}
\definecolor{customGreen}{RGB}{0,153,0}
\definecolor{customPurple}{RGB}{153,51,204}


\title{\textcolor{customRed}{C}\textcolor{customYellow}{o}\textcolor{customBlue}{l}\textcolor{customGreen}{o}\textcolor{customPurple}{r}3D: Controllable and Consistent 3D Colorization with Personalized Colorizer}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Yecong Wan$^{1}$, Mingwen Shao$^{2}$, Renlong Wu$^{1}$, Wangmeng Zuo$^{1}$  \\
$^{1}$Harbin Institute of Technology, $^{2}$Shenzhen University of Advanced Technology\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


	\maketitle 
	\begin{center}
		\centering 
		\vspace{-30pt}
		\includegraphics[width=\textwidth]{figure1.pdf}
		\vspace{-20pt}
		\captionof{figure}{ \textbf{Exemplary Visual Results of Color3D}. Color3D is a unified controllable 3D colorization framework for both static and dynamic scenes, producing vivid and chromatically rich renderings with strong cross-view and cross-time consistency. Our method supports diverse colorization controls, including language-guided (left), automatic inference (middle), and reference-based (right), showcasing its versatility and practical value.}
		\vspace{-3pt}
		\label{figure1}
	\end{center}% 

\maketitle

\begin{abstract}
	In this work, we present \textbf{Color3D}, a highly adaptable framework for colorizing both static and dynamic 3D scenes from monochromatic inputs, delivering visually diverse and chromatically vibrant reconstructions with flexible user-guided control. In contrast to existing methods that focus solely on static scenarios and enforce multi-view consistency by averaging color variations which inevitably sacrifice both chromatic richness and controllability, our approach is able to preserve color diversity and steerability while ensuring cross-view and cross-time consistency. 
In particular, the core insight of our method is to colorize only a single key view and then fine-tune a personalized colorizer to propagate its color to novel views and time steps. Through personalization, the colorizer learns a scene-specific deterministic color mapping underlying the reference view, enabling it to consistently project corresponding colors to the content in novel views and video frames via its inherent inductive bias. Once trained, the personalized colorizer can be applied to infer consistent chrominance for all other images, enabling direct reconstruction of colorful 3D scenes with a dedicated Lab color space Gaussian splatting representation. The proposed framework ingeniously recasts complicated 3D colorization as a more tractable single image paradigm, allowing seamless integration of arbitrary image colorization models with enhanced flexibility and controllability.
Extensive experiments across diverse static and dynamic 3D colorization benchmarks substantiate that our method can deliver more consistent and chromatically rich renderings with precise user control. Project Page \url{https://yecongwan.github.io/Color3D/}
\end{abstract}
	\section{Introduction}
\vspace{-8pt}
The emerging 3D reconstruction techniques, such as Neural Radiance Fields (NeRF)~\cite{mildenhall2021nerf} and 3D Gaussian Splatting (3DGS)~\cite{kerbl20233d}, have catalyzed high-fidelity novel-view synthesis from the observed color images.
% 
These methods evolves rapidly~\cite{chen2022tensorf, fridovich2022plenoxels, garbin2021fastnerf, muller2022instant, yu2024mip, cheng2024gaussianpro, wu20244d, lu2024scaffold}, and are extended to dynamic scenes~\cite{liu2023robust, guo2023forward, shao2023tensor4d, duan20244d, li2024spacetime, yang2023real, wu20244d, yang2024deformable} with object motion modeling.
% 
Despite these advances, there is still a compelling challenge, i.e., reconstructing colorful 3D scenes from monochromatic inputs.
% 
This can significantly enhance the visual realism and expression of 3D reconstructions, unlocking new applications across digital art, artistic creation, and cultural heritage preservation.
% 
% Beyond enhancing visual realism, the recovery of vivid and controllable color information substantially , unlocking new applications across digital art, artistic creation, and cultural heritage preservation.

% 
% Their rapid evolution and variants  has sparked amount of researches to extend the representations to dynamic scenes \cite{liu2023robust, guo2023forward, shao2023tensor4d, duan20244d, li2024spacetime, yang2023real, wu20244d, yang2024deformable}, enabling  with impressive temporal coherence. 
% The emergence of radiance fields such as Neural Radiance Fields (NeRF) \cite{mildenhall2021nerf} and 3D Gaussian Splatting (3DGS) \cite{kerbl20233d} has revolutionized novel view synthesis, catalyzing high-fidelity 3D scene reconstruction directly from raw observations. 
% 
% Their rapid evolution and variants \cite{chen2022tensorf, fridovich2022plenoxels, garbin2021fastnerf, muller2022instant, yu2024mip, cheng2024gaussianpro, wu20244d, lu2024scaffold} has sparked a growing body of research that extends these representations to dynamic scenes \cite{liu2023robust, guo2023forward, shao2023tensor4d, duan20244d, li2024spacetime, yang2023real, wu20244d, yang2024deformable}, enabling high-fidelity modeling of deformation and motion with impressive temporal coherence. 
% 
% Capitalizing on these advances, a compelling yet underexplored challenge emerges: reconstructing chromatically rich 3D content from monochromatic inputs. 
% 
% Beyond enhancing visual realism, the recovery of vivid and controllable color information substantially augments the expressive potential of 3D reconstructions, unlocking new applications across digital art, artistic creation, and cultural heritage preservation.


In light of this, a straightforward way is to train a 3D colorization model on massive collections of 3D scene data.
% 
However, the formidable complexity of 3D geometry \cite{lu2024scaffold}, and the temporal intricacies in dynamic scenes \cite{yan20244d}, making it prohibitively infeasible in practice.
% 
An alternative solution is to first colorize multi-view monochromatic images with 2D colorization models and then reconstruct the 3D content. 
% 
Nevertheless, its naive implementation leads to severe cross-view color shift, due to latest 2D colorization models \cite{kang2023ddcolor, yang2024colormnet} struggling to colorize multi-view images consistently (See Fig. \ref{figure2}).
% 
% 

% 
% However, the formidable complexity of 3D geometry \cite{lu2024scaffold}, compounded by the temporal intricacies inherent in dynamic scenes \cite{yan20244d}, makes this approach prohibitively cumbersome and largely infeasible in practice due to its extreme data and computation demands.
% % Another intuitive and desirable solution is to first apply image or video colorization models to multi-view inputs and subsequently reconstruct the 3D scene with the colored results. Yet, this naive pipeline inevitably suffers from severe cross-view color inconsistency, since even the latest temporally coherent video colorization models \cite{yang2024bistnet, yang2024colormnet} struggle to maintain consistent color fidelity under large viewpoint shifts.
% An alternative and more feasible solution is to first colorize multi-view images with 2D colorization models and then reconstruct the 3D scene. 
% % 
% However, this naive pipeline inevitably suffers from severe cross-view color inconsistency. Even the latest temporally coherent video colorization models \cite{yang2024bistnet, yang2024colormnet} struggle to maintain 3D color consistency \cite{dhiman2023corf}, as the contextual shifts in 3D capturing far exceed those in consecutive video frames (See Fig. \ref{figure2}).

% In light of this challenge, a seemingly straightforward way would be to directly train a 3D colorization model on massive collections of 3D scene data. However, the formidable complexity of 3D geometry \cite{lu2024scaffold}, compounded by the temporal intricacies inherent in dynamic scenes \cite{yan20244d}, makes this approach prohibitively cumbersome and largely infeasible in practice due to its extreme data and computation demands.
% % Another intuitive and desirable solution is to first apply image or video colorization models to multi-view inputs and subsequently reconstruct the 3D scene with the colored results. Yet, this naive pipeline inevitably suffers from severe cross-view color inconsistency, since even the latest temporally coherent video colorization models \cite{yang2024bistnet, yang2024colormnet} struggle to maintain consistent color fidelity under large viewpoint shifts.
% An alternative and more feasible solution is to first colorize multi-view images with 2D colorization models and then reconstruct the 3D scene. 
% % 
% However, this naive pipeline inevitably suffers from severe cross-view color inconsistency. Even the latest temporally coherent video colorization models \cite{yang2024bistnet, yang2024colormnet} struggle to maintain 3D color consistency \cite{dhiman2023corf}, as the contextual shifts in 3D capturing far exceed those in consecutive video frames (See Fig. \ref{figure2}).

To mitigate the color inconsistency, a few attempts \cite{dhiman2023corf,cheng2024colorizing} aims to average multi-view color variations via distillation \cite{dhiman2023corf} or dynamic color injection \cite{cheng2024colorizing}.
% % 
While partially effective, they inevitably dilute the palette richness, producing desaturated and tone-flattened results. 
% 
Furthermore, smoothing color variations makes the colorized results unpredictable, sacrificing user-controlled colorization ability.
% 
Moreover, existing studies only focus on static scenes, while controllable colorization of dynamic ones remains an open and unexplored problem, where color consistency in spatial and temporal dimensions should be maintained.

% To mitigate the color inconsistency of the above paradigm, a few attempts \cite{dhiman2023corf,cheng2024colorizing} have been made on static scene colorization, which primarily aim to enforce multi-view consistency by averaging the color variations through distillation \cite{dhiman2023corf} or dynamic color injection \cite{cheng2024colorizing}.
% % % 
% While these ``averaging” strategies can partially alleviate color inconsistencies, such a manner inevitably dilutes the palette richness, leading to desaturated, tone-flattened results that diminish visual expressiveness and constrain practical usability. Furthermore, by smoothing color variations, these approaches fundamentally make the final color outcomes unpredictable, thereby sacrificing user-controlled colorization ability.
% % 
% Additionally, despite these prior efforts for static scenarios, controllable colorization of dynamic scenes while simultaneously maintaining spatial and temporal consistency remains an open and largely unexplored problem.

%To reduce the color inconsistency, existing works \cite{dhiman2023corf,cheng2024colorizing} typically average color variations by distillation or dynamic injection.
% 
%However, such manners dilute palette richness, producing desaturated, flat-toned results.
% 
%Besides, they lead to unpredictable results and lack user-controlled colorization ability.
% 
%Moreover, these studies focus on static scenes while failing in dynamic ones, due to the ignored temporal consistency.



In this work, we suggest a novel paradigm for 3D colorization, i.e., first colorizing a key image and then propagating the color information to novel views and time steps by fine-tuning a personalized colorizer.
% 
The benefits are three-fold.
% 
First, it transforms the complex 3D colorization into a simpler color information propagation task, enabling more consistent results.
%
Second, since only a key image needs to be controlled, it is easier to achieve controllable 3D colorization.
%
Third, it provides a chance for unifying 3D colorization of both static and dynamic scenes.
%

% 
Driven by the motivations, we propose \textbf{Color3D},
a unified controllable 3D colorization framework for both static and dynamic scenes (Fig. \ref{figure1}), where a personalized colorizer is optimized for each scene to enable consistent color propagation.
% 
% Unlike training a standard colorization model on massive samples with ambiguous and one-to-many color mappings, which results in inconsistent color predictions under minor contextual variations, our personalized colorizer is explicitly cultivated to learn the scene-specific deterministic color mapping underlying the reference view, thereby enabling consistent color predictions across novel views.
% 
Specifically, Color3D first selects the most informative view from the monochromatic images, and then colorizes it with a desirable off-the-shelf image colorization model.
% 
Next, to colorize other views or timesteps consistently, Color3D personalizes a scene-specific colorizer to learn the deterministic color mapping underlying this colorized view. In which a single view augmentation strategy is devised to expand the sample space, thereby enhancing the colorizer's generalization and its capacity to handle unseen visual content.
% 
Finally, the personalized colorizer is employed to infer consistent chromatic content of remaining views or frames, followed by direct reconstruction of colorful static or dynamic 3D scenes via a dedicated Lab color space Gaussian splatting representation, where luminance and chrominance components are optimized separately.
% 
Extensive experiments on public static and dynamic 3D datasets substantiate that our Color3D can produce colorful 3D content that aligns with user intentions, outperforming alternatives quantitatively and qualitatively.
% 
Additionally, Color3D also achieves visually promising results on real-world in-the-wild scenes, further demonstrating the practicality of our method in legacy restoration.
% 

% To tackle the aforementioned limitations, we advocate a novel paradigm: instead of colorizing all views inconsistently followed by post-hoc consistency refinements, we colorize only a single key view and then consistently propagate its color information to novel views and time steps for direct 3D reconstruction.
% %
% By recasting the inherently complex 3D colorization problem to a more tractable single image formulation, this paradigm significantly enhances flexibility, controllability, and fidelity.

% Based on this insight, we propose \textbf{Color3D},
% a novel controllable 3D colorization framework for both static and dynamic scenes (Fig. \ref{figure1}). For each scene, Color3D fine-tunes a personalized colorizer to propagate the color from a single reference view to novel views. Unlike training a standard colorization model on massive samples with ambiguous and one-to-many color mappings, which results in inconsistent color predictions under minor contextual variations, our personalized colorizer is explicitly cultivated to learn the scene-specific deterministic color mapping underlying the reference view, thereby enabling consistent color predictions across novel views.
% % 
% More specifically, we first identify the most informative key view from monochromatic images and apply an arbitrary off-the-shelf image colorization model with inherent controllability to generate a desired single colorized view. 
% % 
% Then, we personalize the scene-specific colorizer to learn the definite color mapping underlying in this particular view, enabling consistent colorization for novel views or video frames. 
% % 
% In which a single view augmentation strategy is devised to expand the sample space, enhancing the colorizer's generalization ability and its capacity to handle unseen visual content.
% % 
% Finally, the personalized colorizer is employed to infer consistent chromatic content for remaining views or frames, followed by direct reconstruction of colorful static or dynamic 3D scenes via a Lab color space Gaussian splatting representation, where luminance and chrominance components are optimized separately.

% 
% Comprehensive quantitative and qualitative evaluations on static and dynamic 3D scene colorization substantiate that our proposed Color3D can effectively maintain consistency across viewpoints and time while delivering vivid, color-rich renderings aligned with user intentions.
% Moreover, real-world legacy restoration and color manipulation applications consistently achieve high-quality and visually promising results, further demonstrating the robustness and versatility of our method.

% To tackle the aforementioned challenges, we propose a novel controllable 3D colorization framework Color3D, capable of colorizing both static and dynamic scenes from monochromatic inputs, yielding visually compelling and richly saturated renderings with flexible user control.
% 	Specifically, Color3D operates in a two-stage fashion: the personalized colorizer training stage and the 3D scene colorization stage. In the first stage, we identify the most informative key view from the given monochromatic images or video frames, and apply an arbitrary off-the-shelf image colorization model with inherent controllability to generate a desired single colorized view. Then, we personalize a scene-specific colorizer to learn the definite color mapping underlying in this particular view, enabling consistent colorization for novel views or video frames. In which a single view augmentation strategy is devised to expand the sample space, enhancing the colorizer's generalization ability and its capacity to handle unseen visual content.
% 	In the second stage, the personalized colorizer is leveraged to infer consistent chromatic content for the remaining views or frames, enabling direct reconstruction of colorful static and dynamic 3D scenes via a dedicated Lab color-space Gaussian splatting representation, wherein luminance and chromatic components are optimized separately.
% 	Comprehensive experimental results substantiate the superiority of the proposed Color3D for both static and dynamic 3D scene colorization. Furthermore, real-world applications on legacy restoration and ablation studies also demonstrate the robustness and effectiveness of our method.


In conclusion, the main contributions are summarized as follows:

\begin{itemize}[left=4pt, nosep]
	\item We propose a unified and versatile 3D colorization framework, termed Color3D, which enables user-guided colorization of both static and dynamic scenes by tuning a personalized colorizer for each scene, advancing the controllability and interactivity of 3D colorization.
	
	\item To facilitate personalized colorizer tuning, we propose a customized key view selection strategy along with a single view augmentation scheme to enhance coloring richness and generalization. Furthermore, we introduce a Lab Gaussian representation to improve color reconstruction fidelity and better preserve scene structures.
	
	\item Our Color3D delivers vibrant, controllable, consistent results that faithfully align with user intent, significantly outperforming alternatives on a variety of benchmarks.
\end{itemize}
\vspace{-8pt}

	\vspace{-4pt}
\section{Related Work}
\vspace{-6pt}
\noindent\textbf{Radiance Fields for Static and Dynamic Scenes.}
Radiance field representations \cite{mildenhall2021nerf,kerbl20233d} have driven a paradigm shift in novel view synthesis. Neural Radiance Fields (NeRF) \cite{mildenhall2021nerf} pioneered implicit volumetric modeling with coordinate-based neural networks, inspiring numerous extensions \cite{barron2021mip,barron2022mip,barron2023zip,verbin2022ref,chen2022tensorf,fridovich2022plenoxels,garbin2021fastnerf,muller2022instant} and subsequent work on dynamic scenes \cite{park2021nerfies,park2021hypernerf,wang2023masked,fang2022fast,liu2023robust,guo2023forward,shao2023tensor4d}. However, NeRF-based methods remain hindered by costly training and slow rendering. To overcome this, 3D Gaussian Splatting (3DGS) \cite{kerbl20233d} introduced an explicit Gaussian representation that enables real-time, photorealistic rendering. Recent advances further extend Gaussian splatting to dynamic reconstruction \cite{duan20244d,li2024spacetime,yang2023real,wu20244d,yang2024deformable}, combining efficiency and fidelity. Motivated by these developments, we employ 3DGS and 4DGS as canonical static and dynamic representations, building a unified framework for controllable 3D colorization with strong spatial-temporal consistency.

\noindent\textbf{From 2D Colorization to 3D.}
Image colorization aims to recover plausible chromatic content from grayscale inputs, with decades of progress improving realism and controllability \cite{zhang2017real,wu2021towards,kim2022bigcolor,kang2023ddcolor,ji2022colorformer}. Beyond automatic approaches, user-guided methods leverage reference images \cite{he2018deep,huang2022unicolor,zhao2021color2embed} or language prompts \cite{weng2023cad,zabari2023diffusing,weng2022code,chang2023coins,liang2024control}. Extending to videos introduces temporal consistency challenges, tackled by matching \cite{yang2024bistnet}, palette transfer \cite{wang2025consistent}, attention \cite{li2024towards}, and memory propagation \cite{yang2024colormnet}. Yet these remain limited to 2D domains, lacking cross-view consistency.
Efforts on 3D scene colorization are still nascent. GBC \cite{liao2024gbc} exploits video models for continuous inputs, while ChromaDistill \cite{dhiman2023corf} and ColorNeRF \cite{cheng2024colorizing} transfer knowledge from pretrained colorizers by averaging inconsistent predictions, often sacrificing controllability and vividness. Moreover, colorizing dynamic 3D scenes while ensuring spatial-temporal consistency remains unaddressed. Our work bridges this gap with a unified framework for both static and dynamic settings, achieving visually rich, consistent, and user-controllable 3D colorization.

\vspace{-6pt}
\vspace{-3pt}
\section{Methodology}
\vspace{-9pt}
Our core idea is to fine-tune a personalized colorizer for each scene using a single colorized key view and then employ it to propagate the color information to novel views and time steps. By specializing in the scene-specific deterministic color mapping derived from this reference view, the colorizer is able to project the same content in novel views to the corresponding colors in the reference view through the inherent inductive bias capability, thereby elegantly addressing the challenge of color consistency across both static and dynamic 3D scenes.
\vspace{-4pt}
\subsection{Overview of Color3D}
\vspace{-4pt}
The schematic illustration of the proposed Color3D is depicted in Fig. \ref{figure3}. Color3D is primarily composed of two phases, i.e., the personalized colorizer training stage and the 3D scene colorization stage. In the first stage, we begin by selecting a single key view from the inputs and apply an off-the-shelf image colorization model to generate a colorized reference view. With the augmented data from a customized single view augmentation scheme, we then fine-tune a personalized colorizer to learn the definite color mapping underlying this reference view.
In the second stage, the personalized colorizer is employed to infer consistent chromatic content for the remaining views or frames, which are directly leveraged to optimize the vibrant 3D scene with a tailored Lab Gaussian representation.



\begin{figure*}[t]
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
		\includegraphics[width=\linewidth]{figure3.pdf}
	\end{center}
	\vspace{-9pt}
	\caption{  The overall pipeline of Color3D. Our framework comprises two primary stages. In the first stage, we initially identify the most informative key view from the given monochromatic images and video frames, and employ an off-the-shelf image colorization model to generate a colorized single view. Then, a single view augmentation scheme is elaborated to amplify the data, and the augmented samples are subsequently used to fine-tune a per-scene personalized colorizer.
		In the second stage, this personalized colorizer is utilized to infer consistent chromatic content of the remaining views or frames, and directly reconstruct the colorful 3D scene with Lab color space 3DGS or 4DGS.   }
	\label{figure3}
	\vspace{-18pt}
\end{figure*}
\vspace{-4pt}
\subsection{Stage 1: Personalized Colorizer Training}
\vspace{-4pt}
In the initial stage, we aim to fine-tune a personalized colorizer using a single colorized reference view, which enables consistent color propagation to novel views and frames. In contrast to training a standard image colorization model on diverse samples with inherent one-to-many color mappings, which results in inconsistent color predictions under minor contextual changes, our personalized colorizer is explicitly trained to learn the scene-specific one-to-one color mapping derived from a single image. This allows for consistent chromatic outputs across both spatial and temporal variations.

\noindent\textbf{Key View Selection.} Since our goal is to fine-tune a personalized colorizer using a single view, this view should capture the widest range of visual information while minimizing redundancy, which is crucial to ensure that the colorizer can be more robust to handle other views of the scene. To achieve this, we propose a heuristic strategy that selects the most informative and representative view by comparing feature similarity entropies across all candidates.

Specifically, given a set of $N$ candidate monochromatic inputs, we first extract their single dimension feature representations using CLIP \cite{radford2021learning} and the concated feature matrix can be denoted as $F \in \mathbb{R}^{N \times D}$, where $D$ is the dimension of the feature space. To ensure that similarity comparisons reflect angular distances, we normalize each feature vector to unit length $\smash{\hat{F}_i = \frac{F_i}{\|F_i\|_2}}$, for $i = 1, 2, \dots, N$. Then, we compute the pairwise cosine similarity matrix to measure the relationships between different views $S=\hat{F}\hat{F}^T$,
where $S_{ij}$ represents the similarity between view $i$ and view $j$.
For each view $i$, we define a probability distribution $P_{ij}$ over its similarity with all other views, and use this distribution to compute the information entropy $H(I_i)$ associated with view $i$:
\begin{equation}
	\setlength{\abovedisplayskip}{1pt}
	\setlength{\belowdisplayskip}{1pt}
	H(I_i) = - \sum\nolimits_j P_{ij} \log P_{ij}, ~~~~~~  P_{ij} = \frac{\exp(S_{ij})}{\sum_{k} \exp(S_{ik})}
\end{equation}
A higher entropy value indicates that the view is more evenly related to all other views, meaning it encapsulates a broader and more diverse set of visual information.
We define the optimal key view as the one that maximizes the entropy:
\begin{equation}
	\setlength{\abovedisplayskip}{1pt}
	\setlength{\belowdisplayskip}{1pt}
	I^* = \arg\max_{I_i} H(I_i).
\end{equation}

\noindent\textbf{Key View Colorization.} After obtaining the key view, users can apply any off-the-shelf image colorization model to perform colorization on the single view. Such a single-view-based strategy allows for the seamless integration of various mature techniques, including language-guided \cite{weng2023cad,chang2023coins,liang2024control}, reference-based \cite{he2018deep,huang2022unicolor}, and automatic \cite{kang2023ddcolor,ji2022colorformer} image colorization methods without any additional tuning or modification, thereby offering more flexible and convince controllability over both static and dynamic 3D scene colorization.


\noindent\textbf{Single View Augmentation.} In order to avoid overfitting and more robustly generalize to novel views and video frames, we propose a single view augmentation scheme that combines generative augmentations and traditional augmentations to generate more samples in which the same content consistently retains its color across all instances.
As illustrated in Fig. \ref{figure4}(a), the proposed scheme first leverages generative models to produce plausible scene content beyond the provided single view, while ensuring chromatic coherence with the input to enrich the diversity of colorization supervision. Specifically, given a single colored view, we incorporate three complementary generative strategies:
(1) Outpainting: we divide the image into a $2\times2$ grid and apply Stable Diffusion \cite{rombach2022high} to each of the four regions individually to imagine plausible scene content beyond their outer boundaries. Meanwhile, we employ LLaVA \cite{liu2023visual} to generate a descriptive caption based on the image, which serves as the text prompt to guide the diffusion process.
(2) Image-to-Video: we generate continuous video frames through Stable Video Diffusion \cite{blattmann2023stable} conditioned on the single image to provide coherent object motion and gradual changes in viewpoint.
(3) Novel View: to simulate consistent novel viewpoints, we employ Stable Virtual Camera \cite{zhou2025stable}
to generate views along a predefined orbital trajectory with consistent content across perspectives. Notably, we do not require the generated content to be completely the same as the scene to be colored; instead, we emphasize preserving consistent chromatic styles with the input view, which is sufficient to improve coloring robustness and enhance coloring richness for scene content not included in the single view.

Afterward, we apply a series of traditional augmentations to further enhance the samples, including rotation, flip, grid shuffle that divides the image into grids and randomly disrupts them, and elastic transform \cite{simard2003best} that introduces smooth deformations to simulate realistic variations in object shape and structure. This can facilitate the learning of more robust and variation-agnostic color mapping.

Finally, these augmented images are converted from RGB to the CIE Lab color space \cite{iizuka2016let}, where the $L$ channel is used as the input to the colorizer and the $ab$ channels serve as the target for color prediction. The Lab color space not only decouples luminance from chromatic components but also offers a perceptually uniform representation that better aligns with human visual perception.


\begin{figure*}[t]
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
		\includegraphics[width=\linewidth]{figure4.pdf}
	\end{center}
	\vspace{-11pt}
	\caption{(a): Illustration of the proposed single view augmentation scheme that combines generative augmentations and traditional augmentations to enrich the single colored view with consistent color distribution. (b): Architecture of the colorizer consists of a frozen DDColor encoder alongside a trainable adapter and CNN decoder. (c): Lab Gaussian that first warms up with three $L$ channels and then switches to full $Lab$ channels for color optimization.}
	\label{figure4}
	\vspace{-18pt}
\end{figure*}

\noindent\textbf{Colorizer Tuning.} With the augmented samples, we can fine-tune a personalized colorizer to learn the scene-specific, variation-agnostic, deterministic color mapping underlying the single view. 
The structure of the colorizer is illustrated in Fig. \ref{figure4}(b). We adopt an encoder from the pre-trained image colorization model DDColor \cite{kang2023ddcolor} and freeze its parameters to preserve its strong capability in extracting high-level color-relevant features. This enables our model to learn semantic-level color mappings from limited samples, which are inherently more robust to viewpoint and motion variations than low-level luminance-to-color correspondences. On the other hand, we deliberately avoid using a pre-trained decoder, whose built-in color priors are a major source of multi-view color inconsistency. Instead, we employ a lightweight, learnable CNN-based decoder initialized from scratch to serve as a clean and unbiased slate for learning personalized color mappings. Furthermore, we equip the encoder with additional learnable adapters \cite{chen2024conv} to better adapt to the current scenario while preserving its original pre-trained priors. The personalized colorizer is fine-tuned using a simple L1 loss:
\begin{equation}
	\setlength{\abovedisplayskip}{1pt}
	\setlength{\belowdisplayskip}{1pt}
	\begin{aligned} 
		\mathcal{L}_{colorizer} = ~~ \parallel P^{ab} - G^{ab}\parallel_1
	\end{aligned}
\end{equation} 
where $P^{ab}$ and $G^{ab}$ denote the predicted $ab$ channels by the personalized colorizer and the ground-truth $ab$ channels, respectively.  Please refer to Appendix \ref{a.2} for additional network details and a more in-depth explanation of why the personalized colorizer can achieve consistent color propagation.


\vspace{-12pt}
\subsection{Stage 2: 3D Scene Colorization}
\vspace{-8pt}
With the personalized colorizer, in this stage, we aim to generate consistent colors for other views and
video frames, which are then directly utilized to reconstruct colorful static and dynamic 3D scenes. Considering that the luminance information of the scene is already known and it can serve as reliably scene structure constraint, we advocate optimizing in CIE Lab color space \cite{iizuka2016let}. By decoupling the known luminance ($L$ channel) from the predicted chrominance ($ab$ channels), our approach can constrain them separately to more stably optimize the 3D scene, attenuating the effect of perturbations in the predicted chromatic information. 
﻿To achieve this, we deploy 3DGS\cite{kerbl20233d} and 4DGS \cite{wu20244d} to model static and dynamic 3D scenes and propose a Lab Gaussian representation for rendering images in Lab color space and facilitating separate optimization of luminance and chrominance.


\noindent\textbf{Lab Gaussian.} The vanilla Gaussian attributes including position ($\mu$), rotation ($q$), scaling ($s$), opacity ($\alpha$), and Spherical
Harmonics (SH) coefficients ($SH_R,SH_G,SH_B$) for modeling RGB color channels. To better deouple luminance and chrominance, 
we retain the same architecture but reformulate the three sets of SH coefficients to the Lab color space as $\{SH_L, SH_a, SH_b\}$, which separately parameterize the luminance ($L$) and chrominance ($a$ and $b$) channels. Following the original splatting pipeline \cite{kerbl20233d}, we can render images with Lab color space representation directly from these SH coefficients. This strategy allows for fully leveraging the high-fidelity luminance information to provide more stable optimization signals for Gaussian primitives such as position and shape, thereby improving training stability and scene convergence. 


Due to the differing numerical ranges between $L$ and $ab$ channels in the Lab color space, directly modeling chrominance and luminance jointly with the same Gaussian primitives can lead to unstable optimization and convergence. To mitigate this issue, we normalize rendered Lab channels into a unified range of $[0, 1]$. Specifically, the luminance channel $L$, originally in $[0, 100]$, is scaled by a factor of $1/100$. The chrominance channels $a$ and $b$, originally spanning $[-128, 127]$, are first shifted by $128$ and then scaled by $1/255$ accordingly.
The normalization process can be formulated as:
\begin{equation}
	\setlength{\abovedisplayskip}{1pt}
	\setlength{\belowdisplayskip}{1pt}
	L' = \frac{L}{100}, \quad a' = \frac{a + 128}{255}, \quad b' = \frac{b + 128}{255}
\end{equation}
\noindent\textbf{Optimization objectives.} We deploy the same loss functions for both static and dynamic scenarios and otherwise remain identical with the original 3DGS and 4DGS. For the rendered image in the Lab color space, we optimize the $L$ and $ab$ channels separately. Since the $L$ channel contains the core structural and high-frequency detail information of the scene, we deploy an edge loss $\mathcal{L}_{\text{edge}}$ to encourage the Gaussian primitives to better capture fine textures and structural details.
\begin{equation}
	\setlength{\abovedisplayskip}{1pt}
	\setlength{\belowdisplayskip}{1pt}
	\resizebox{0.4\hsize}{!}{
		$\mathcal{L}_{\text{edge}} = \sum \sqrt{\left(\triangledown(R^L) - \triangledown(G^L)\right)^2 + \epsilon^2}$}
\end{equation} 
where $\triangledown$ denotes the Laplacian operator, and $R^L$, $G^L$ are the rendered and ground-truth luminance channels, respectively. In addition, we also deploy the $\mathcal{L}_{1}$ and $\mathcal{L}_{D\!-\!SSIM}$ from 3DGS
and the overall objective for the $L$ channel can be written as:
\begin{equation}
	\setlength{\abovedisplayskip}{1pt}
	\setlength{\belowdisplayskip}{1pt}
	\begin{aligned} 
		\mathcal{L}_{l} \!=\! (1\!-\! \beta) \mathcal{L}_{1} \!+\! \beta \mathcal{L}_{D\!-\!SSIM} + \mathcal{L}_{edge},
	\end{aligned}
\end{equation} 
where $\beta$ is set to 0.2 similar to the original 3DGS \cite{kerbl20233d}.
For the $ab$ channels, which primarily contain low-frequency chromatic information, we exclude the edge loss term and utilize only the following objective:
\begin{equation}
	\setlength{\abovedisplayskip}{1pt}
	\setlength{\belowdisplayskip}{1pt}
	\begin{aligned} 
		\mathcal{L}_{ab} \!=\! (1\!-\! \beta) \mathcal{L}_{1} \!+\! \beta \mathcal{L}_{D\!-\!SSIM}.
	\end{aligned}
\end{equation} 
\noindent\textbf{Warm-Up.} To facilitate more accurate 3D structural and temporal motion modeling, we propose to first optimize the structure and deformation of the scene with only luminance supervision for warm-up and then co-optimize the scene with color constraints. More concretely, as illustrated in Fig. \ref{figure4}(c), during the first half of training iterations in 3DGS or 4DGS, we allocate all three sets of SH coefficients to represent the $L$ channel and render a three-channel luminance image, supervised by the luminance loss $\mathcal{L}_{l}$. In the latter half of the training, we then reassign two of the SH coefficient sets to represent the $a$ and $b$ chrominance channels for color modeling, and optimize them following the above-presented objectives. 
This warm-up scheme stabilizes early-stage geometry learning while providing a strong initialization for color modeling, ultimately yielding more robust colorization.

\begin{table*}[t]
	\setlength{\tabcolsep}{2pt}	
	\caption{Quantitative comparisons on static (LLFF and Mip-NeRF 360) and dynamic (DyNeRF) 3D scene colorization benchmarks. The top-performing results are highlighted with color.}
	\footnotesize
	\vspace{-13pt}
	\begin{center}
		\resizebox{\textwidth}{!}{
		\begin{tabular}{l|ccc|ccc|ccc}
			\toprule[1pt]
			% \begin{minipage}{1cm}\vspace{1mm} \centering Type\vspace{1mm} \end{minipage} 
			\makecell[l]{\multirow{2}{*}{Method}} &\multicolumn{3}{c|}{\cellcolor{red!20}{Language}}&\multicolumn{3}{c|}{\cellcolor{blue!20}{Automatic}}&\multicolumn{3}{c}{\cellcolor{green!20}{Reference}}
			\\
			& FID$\downarrow$ & CLIP Score$\uparrow$& ME$\downarrow$&FID$\downarrow$ & Colorful$\uparrow$& ME$\downarrow$ & FID$\downarrow$ & Ref-LPIPS$\downarrow$& ME$\downarrow$\\
			\midrule[1pt]
			\multicolumn{10}{c}{LLFF (Static 3D Scenes)}\\
			\midrule
			GaussianEditor \cite{chen2024gaussianeditor}&136.49&0.6346&0.093&-&-&-&-&-&-\\
			GenN2N \cite{liu2024genn2n}&-&-&-&52.63&25.24&0.078&-&-&-\\
			Ref-NPR \cite{zhang2023ref}&-&-&-&-&-&-&111.93&0.7958&0.112\\
			ColorNeRF \cite{cheng2024colorizing}&138.62&0.6388&0.089&58.26&21.78&0.062&103.68&0.7891&0.086\\
			3DGS+2DColorizer&87.45&0.6415&0.178&40.25&\cellcolor{blue!20}\textbf{34.07}&0.084&67.48&0.7366&0.131\\
			\textbf{Color3D (Ours)}&\cellcolor{red!20}\textbf{62.84}&\cellcolor{red!20}\textbf{0.6544}&\cellcolor{red!20}\textbf{0.051}&\cellcolor{blue!20}\textbf{35.10}&33.99&\cellcolor{blue!20}\textbf{0.056}&\cellcolor{green!20}\textbf{41.36}&\cellcolor{green!20}\textbf{0.6823}&\cellcolor{green!20}\textbf{0.055}\\
			
			\midrule[1pt] 
			\multicolumn{10}{c}{Mip-NeRF 360 (Static 3D Scenes)}\\
			\midrule
			GaussianEditor \cite{chen2024gaussianeditor}&123.71&0.6045&0.088&-&-&-&-&-&-\\
			GenN2N \cite{liu2024genn2n} &-&-&-&83.69&23.44&0.137&-&-&-\\
			Ref-NPR \cite{zhang2023ref} &-&-&-&-&-&-&146.32&0.7928&0.143\\
			ColorNeRF \cite{cheng2024colorizing}&163.12&0.6075&0.093&87.42&18.90&0.093&121.57&0.7852&0.106\\
			3DGS+2DColorizer&112.33&0.6148&0.202&62.32&29.32&0.135&86.25&0.7521&0.178\\
			\textbf{Color3D (Ours)}&\cellcolor{red!20}\textbf{68.23}&\cellcolor{red!20}\textbf{0.6246}&\cellcolor{red!20}\textbf{0.058}&\cellcolor{blue!20}\textbf{39.03}&\cellcolor{blue!20}\textbf{33.36}&\cellcolor{blue!20}\textbf{0.082}&\cellcolor{green!20}\textbf{48.62}&\cellcolor{green!20}\textbf{0.7028}&\cellcolor{green!20}\textbf{0.079}\\
			
			\midrule[1pt]
			\multicolumn{10}{c}{DyNeRF (Dynamic 3D Scenes)}\\
			\midrule
			Instruct 4D-to-4D \cite{mou2024instruct}&112.35&0.6082&0.062&-&-&-&-&-&-\\
			4DGS+2DColorizer&89.39&0.6159&0.124&39.17&29.45&0.080&84.65&0.7446&0.128\\
			\textbf{Color3D (Ours)}&\cellcolor{red!20}\textbf{58.62}&\cellcolor{red!20}\textbf{0.6271}&\cellcolor{red!20}\textbf{0.041}&\cellcolor{blue!20}\textbf{37.28}&\cellcolor{blue!20}\textbf{32.75}&\cellcolor{blue!20}\textbf{0.062}&\cellcolor{green!20}\textbf{52.77}&\cellcolor{green!20}\textbf{0.6717}&\cellcolor{green!20}\textbf{0.052}\\
			\bottomrule[1pt]
		\end{tabular}}
	\end{center}
	\label{table1}
	\vspace{-23pt}
\end{table*}

\vspace{-3pt}
\section{Experiments and Analysis}
\vspace{-6pt}
\subsection{Experimental Settings}\label{4.1}
\vspace{-6pt}
\noindent\textbf{Implementation details.} 
All experiments are conducted using the PyTorch framework on NVIDIA RTX A6000 GPUs. We evaluate our method on two static datasets (LLFF \cite{mildenhall2019local} and Mip-NeRF 360 \cite{barron2022mip}) and one dynamic dataset (DyNeRF \cite{li2022neural}), all of which are converted to monochrome images for colorization. Structure-from-Motion (SfM) is employed to initialize Gaussian points from monochrome inputs in all experiments. Without loss of generality, both our method and competing approaches deploy the same set of image colorization models for fair comparison: ControlColor \cite{liang2024control} for language-guided colorization, DDColor \cite{kang2023ddcolor} for automatic colorization, and UniColor \cite{huang2022unicolor} for reference-based colorization.
Apart from our proposed Lab Gaussian representation and optimization objectives, all other settings are kept identical to the original 3DGS and 4DGS frameworks. Notably, our entir pipeline incurs only about eight additional minutes for personalized colorizer tuning, which is considered acceptable for chromatic 3D scene reconstruction.

\noindent\textbf{Metrics.} We adopt CLIP score \cite{radford2021learning} for language-guided colorization to measure text-image alignment, Colorful Score \cite{hasler2003measuring} for automatic colorization to assess color vividness, and Ref-LPIPS \cite{zhang2023ref} for reference-based colorization to evaluate perceptual similarity. 
Fréchet Inception Distance (FID) \cite{heusel2017gans} is employed as a standard metric for assessing the rendering quality of all colorization tasks. Additionally, to evaluate the consistency of colorization across views and time, we propose a metric termed Matching Error (ME). It utilizes a dense image matching model \cite{shen2024gim} to identify pixel correspondences between ground-truth color images from different views or time steps, and computes the average color difference at these matched locations in the rendered colorized results.

\vspace{-8pt}
\subsection{Results on Static 3D Scene Colorization}
\vspace{-6pt}
The experimental results for quantitative comparisons in static 3D scenes are shown in the middle and upper portions of Tab. \ref{table1}. 
It is observed that our Color3D delivers remarkable performance gains and outperforms all competitive methods significantly across diverse colorization tasks. A direct combination of 3DGS and 2D colorization models leads to significantly higher Matching Error (ME), indicating severe multi-view inconsistency. While ColorNeRF \cite{cheng2024colorizing} improves 3D color consistency by averaging inconsistent 2D colorizations, this strategy inevitably sacrifices color richness and controllability, resulting in noticeable degradation in FID scores and other task-specific evaluation metrics.
We also demonstrate visual comparisons in Fig. \ref{figure5}. As suggested, 
our method effectively accommodates various forms of user control and accurately generates the desired colorized scenes while preserving strong multi-view consistency. In contrast, direct integration of 2D colorizers introduces obvious view inconsistency and visual artifacts, while ColorNeRF produces overly uniform color tones with limited color diversity and controllability. 



\begin{figure*}[t]
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
		\includegraphics[width=\linewidth]{figure5.pdf}
	\end{center}
	\vspace{-12pt}
	\caption{Qualitative comparisons on static 3D scene colorization benchmarks. Our method produces more color-accurate and color-rich results while maintaining multi-view consistency.  }
	\label{figure5}
	\vspace{-12pt}
\end{figure*}


\begin{figure*}[t]
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
		\includegraphics[width=\linewidth]{figure6.pdf}
	\end{center}
	\vspace{-12pt}
	\caption{ Qualitative comparisons on dynamic 3D scene colorization benchmarks. Our method consistently yields spatial-temporal coherent results with vivid and perceptually realistic color. }
	\label{figure6}
	\vspace{-16pt}
\end{figure*}


\vspace{-6pt}
\subsection{Results on Dynamic 3D Scene Colorization}
\vspace{-6pt}
The quantitative and qualitative experimental results for the dynamic 3D scenarios are exhibited in the bottom portion of Tab. \ref{table1} and Fig. \ref{figure6}. It can be observed that our method significantly outperforms the combination of 4DGS and 2D colorizers, particularly under language-guided and reference-based settings, achieving better FID scores along with substantial reductions in Matching Error (ME) by 0.83 and 0.70, respectively. Moreover, the visual results further demonstrate that our method offers more precise controllability and superior consistency both spatially and temporally.


\vspace{-10pt}
\subsection{Results on Real-World Applications}
\vspace{-8pt}
To more rigorously validate the effectiveness of our proposed Color3D, we also conduct experiments on collected in-the-wild monochrome multi-view images and old movies. As illustrated in Fig. \ref{figure_real}, our method is capable of producing realistic and vivid colorizations while maintaining impressive color consistency across viewpoints and time, demonstrating the effectiveness of our approach in revitalizing historical or legacy visual content.





\vspace{-6pt}
\subsection{Ablation Studies}\label{abla}
\vspace{-6pt}
In Tab. \ref{table_as}, we conduct ablation experiments on the dedicated components introduced in Color3D. The effectiveness of each proposed component in Color3D is evaluated by gradually integrating them into the model, revealing their individual contributions to the overall performance. More detailed analyses and visual demonstrations are as below.

\begin{figure*}[t]
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
		\includegraphics[width=\linewidth]{figure_realworld.pdf}
	\end{center}
	\vspace{-9pt}
	\caption{ \textit{Left:} Novel views of static 3D scene colorization from in-the-wild monochrome multi-view images.
		\textit{Right:} Novel views of dynamic 3D scene colorization from a historical monochrome video.}
	\label{figure_real}
	\vspace{-9pt}
\end{figure*}

\begin{table}[t]	
	\footnotesize
	\caption{Ablation studies on language-guided colorization on the Mip-NeRF 360 dataset.}
	\tabcolsep=4.7pt
	\vspace{-12pt}
	\begin{center}
		\begin{tabular}{llll}
			\toprule[1pt]
			$~~~$ Variant&FID$\downarrow$&CLIP Score$\uparrow$&ME$\downarrow$\\
			\midrule
			$~~~$ Baseline &115.66&0.6145&0.078\\
			$+$ Key View Selection 	&100.28 (\textcolor{green}{+15.38}) &0.6175 (\textcolor{green}{+0.0030})&0.074 (\textcolor{green}{+0.004})\\
			$+$ Single View Augmentation&85.25~~ (\textcolor{green}{+15.03})&0.6196 (\textcolor{green}{+0.0021})&0.069 (\textcolor{green}{+0.005})\\
			$+$	Fine-Tuning-Based Colorizer&75.48~~ (\textcolor{green}{+9.77})&0.6225 (\textcolor{green}{+0.0029})&0.064 (\textcolor{green}{+0.005})\\
			$+$ Lab Gaussian&68.23~~ (\textcolor{green}{+7.25})&0.6246 (\textcolor{green}{+0.0021})&0.058 (\textcolor{green}{+0.006})\\
			\bottomrule[1pt]
		\end{tabular}
	\end{center}
	\label{table_as}
	\vspace{-17pt}
\end{table} 

\noindent\textbf{Effort of key view selection.} As shown in Fig. \ref{figure7}(a), randomly selected key view often fail to sufficiently capture the full content distribution of the scene, leading to fine-tuned colorizers exhibiting limited color richness when generalized to unseen regions of the scene. In contrast, our key view selection scheme identifies more informative and representative key view, thereby enabling the propagation of richer and more diverse colorizations across novel views.


\noindent\textbf{Effort of single view augmentation.} Fig. \ref{figure7}(b) demonstrates that the proposed single view augmentation strategy can effectively extrapolate potential visual content beyond the selected view and expand the sample space, thereby facilitating the colorizer to generate more plausible and enriched colorizations for objects not originally visible in the key view and better preserve scene saturation.


\noindent\textbf{Effort of personalized colorizer.} 
As illustrated in Fig. \ref{figure7}(c), a randomly initialized colorizer, due to its suboptimal feature extraction capabilities, tends to cause color drifting and miscolorization when generalized to novel views. On the contrary, our personalized colorizer fine-tunes a pre-trained DDColor encoder to learn a high-level semantically aware color mapping, ensuring more accurate and consistent colorization across novel views in the scene.


\noindent\textbf{Effort of Lab Gaussian.} 
As depicted in Fig. \ref{figure7}(d), the vanilla RGB Gaussian representation, which entangles luminance and chrominance, tends to introduce blurring artifacts and ghosting since color perturbations will simultaneously affect all three channels. In contrast, our proposed Lab Gaussian representation decouples luminance from predicted chrominance for independent optimization, resulting in significantly sharper textures and more faithful structural details.





\vspace{-5pt}
\begin{figure*}[h]
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
		\includegraphics[width=\linewidth]{figure7.pdf}
	\end{center}
	\vspace{-11pt}
	\caption{Visual ablation study illustrating the impact of (a) key view selection, (b) single view augmentation, (c) personalized colorizer design, and (d) Lab Gaussian representation.}
	\label{figure7}
	\vspace{-15pt}
\end{figure*}
\vspace{-6pt}


\section{Concluding Remarks}
\vspace{-8pt}
In this work, we propose Color3D, an innovative framework that can efficiently reconstruct colorful static and dynamic 3D scenes from monochromatic inputs with desirable controllability and consistency. In contrast to existing methods that rely on color averaging strategies to enforce multi-view consistency, which often leads to monotonous hues and uncontrollable results, our approach elegantly tackles cross-view and cross-time consistency by fine-tuning a personalized colorizer using a single colorized view. By specializing in the deterministic color mapping underlying this reference view, the colorizer can consistently propagate the intended colors to novel views and frames, preserving color consistency without sacrificing vividness and diversity. Such a design also empowers users to control the overall scene via controlling only one view, enabling flexible and concise controlled colorization.
Extensive experiments on various static and dynamic 3D scene colorization benchmarks manifest the effectiveness, superiority, and controllability of our method.

\bibliography{iclr2026_conference}
\bibliographystyle{iclr2026_conference}

\newpage
\appendix
\section{Appendix}
\subsection{3DGS \& 4DGS}
\noindent\textbf{3D Gaussian Splatting} (3DGS) is an emerging and highly effective representation for novel view synthesis, which models a 3D scene as a set of \(G\) anisotropic Gaussians. Each Gaussian \(g_i\) defines a spatial density function:
\begin{equation}
	\setlength{\abovedisplayskip}{1pt}
	\setlength{\belowdisplayskip}{1pt}
	g_i(x) = \exp\left(-\frac{1}{2} (x - \mu_i)^\top \Sigma_i^{-1} (x - \mu_i)\right),
\end{equation}
where \(1 \leq i \leq G\), \(\mu_i \in \mathbb{R}^3\) denotes the mean (i.e., the Gaussian center), and \(\Sigma_i \in \mathbb{R}^{3 \times 3}\) is the covariance matrix encoding its anisotropic shape. In practice, \(\Sigma_i\) is parameterized by a positive scaling vector \(s_i \in \mathbb{R}^3_+\) and a unit quaternion \(q_i \in \mathbb{R}^4\) representing its orientation. Each Gaussian is further associated with an opacity value \(\alpha_i \in \mathbb{R}_+\) and a set of spherical harmonics (SH) coefficients $c_i$ for view-dependent RGB color modeling.


\noindent\textbf{4D Gaussian Splatting} (4DGS) extends 3DGS by introducing a learnable deformation field, allowing dynamic scenes to be modeled via a canonical set of 3D Gaussians. Temporal dynamics are captured by predicting time-dependent offsets in position, rotation, and scale, formulated as:

\begin{equation}
	\setlength{\abovedisplayskip}{1pt}
	\setlength{\belowdisplayskip}{1pt}
	g_i = (\mu_i + \delta \mu_i, q_i + \delta q_i, s_i + \delta s_i, \alpha_i, c_i), ~~~
	(\delta \mu_i, \delta q_i, \delta s_i) = \mathcal{F}(\mu_i,t), 
\end{equation}
where $\mathcal{F}$ indicates deformation prediction module. 

\subsection{More Method Details and Analysis} \label{a.2}
\noindent\textbf{Structure details of pensorlized colorizer.}
As shown in Fig. \ref{figure_colorizer}(a), our personalized colorizer adopts an encoder-decoder structure, augmented with lightweight adapter modules to enable efficient per-scene fine-tuning. The encoder comprises several pre-trained ConvNeXt blocks, each equipped with an inserted adapter. During fine-tuning, the backbone ConvNeXt blocks from pre-trained DDColor \cite{kang2023ddcolor} are frozen, and only the adapters and decoder are updated. 
Each adapter (Fig. \ref{figure_colorizer}(b)) follows a bottleneck design consisting of a depth-wise convolution, nonlinearity, and point-wise convolution. Given an input feature map, the adapter first reduces its channel dimension, applies a nonlinear transformation (ReLU), and projects it back to the original size. A residual connection modulated by an attention weight further refines the adaptation. The decoder is composed of a stack of simple and lightweight CNN blocks, each consisting of two convolutional layers, one instance normalization layer, and LeakyReLU serves as activation function.
Our structure design effectively allows the colorizer to specialize in learning scene-specific color mappings from given views while maintaining the semantic representation capacity gained from the pre-trained image colorization model.

\begin{figure*}[h]
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
		\includegraphics[width=\linewidth]{figure_colorizer.pdf}
	\end{center}
	\vspace{-10pt}
	\caption{(a) Overall architecture of the proposed personalized colorizer. Only the adapters and decoder are updated during fine-tuning. (b) Structure of the adapter module \cite{chen2024conv}, which efficiently injects scene-specific colorization capabilities.}
	\label{figure_colorizer}
	\vspace{-7pt}
\end{figure*}


\noindent\textbf{Why can the personalized colorizer achieve consistent color propagation?}
Here, we intuitively explain why a colorizer trained on augmented samples from a single colorized view can maintain color consistency when generalized to novel viewpoints and video frames. We illustrate this by comparing a standard image colorization model with our per-scene personalized colorizer.

\begin{itemize}[left=4pt, nosep]
	\item \textbf{Standard image colorization model.} Typically, training a general-purpose image colorization model requires large-scale luminance-color image pairs. However, the same semantic object may appear in different colors across images, for example, a door might be white in one image but black in another. Consequently, the model learns an uncertain one-to-many color mapping and tend to infer colors based on contextual cues rather than object identity, making it prone to inconsistent color predictions when facing slight viewpoint or motion changes.
	
	\item \textbf{Our per-scene personalized colorizer.}  
	In contrast, our personalized colorizer is trained on augmented samples derived from a single key view, where the same objects maintain consistent colors across all variations. These augmentations simulate changes in deformation, context, viewpoint, motion, unseen content, etc. This setup encourages the model to learn a scene-specific, variation-agnostic, one-to-one color mapping.  
	Leveraging the inherent inductive bias of deep neural networks \cite{neyshabur2017exploring,battaglia2018relational}, the trained colorizer can predict consistent colors for novel views and video frames by mapping the same content to the corresponding colors it learned during training.
	
\end{itemize}







\subsection{More Experiments}\label{A.3}
\noindent\textbf{Dataset details.}

\textit{LLFF Dataset} \cite{mildenhall2019local} consists of 8 forward-facing real-world scenes. Following prior work, we select every eighth image for testing and use the remaining images for training. All images are processed at a resolution of $1008 \times 756$ for our experiments.


\textit{Mip-NeRF 360 Dataset} \cite{barron2022mip} comprises 9 real-world scenes, including 5 outdoor and 4 indoor environments. Following the same protocol as LLFF, we use one-eighth of the views for testing and the remaining views for training. All experiments are conducted on images downsampled by a factor of 4 (approximately 1K resolution).


\textit{DyNeRF Dataset} \cite{li2022neural} includes six 10-second video
sequences captured at 30 fps by 15 to 20 cameras with face
forward perspective, involving extended periods and intricate camera motions. 


\noindent\textbf{More implementation details of pensorlized colorizer tuning.}
For the single view augmentation strategy, we first generate nine augmented samples based on the original view. Specifically, four samples are produced using the outpainting model Stable Diffusion \cite{rombach2022high}, one sample is extracted from the final frame generated by the image-to-video model Stable Video Diffusion \cite{blattmann2023stable}, and three samples are obtained by sparsely sampling along an orbit using the novel view synthesis model Stable Virtual Camera \cite{zhou2025stable}, together with the original view itself. The nine samples were then randomized to apply traditional augmentation operations. During training, the samples input to the personalized colorizer are randomly cropped to
$320\times 320$. We train the personalized colorizer for 4K iterations using the Adam optimizer. During training, only one sample is used per iteration, i.e., $batch size = 1$, which allows the model to focus on capturing the most distinctive and rich color information from the given sample. The learning rate is initialized to $1\times10^4$, which is steadily decreased to $1\times10^6$ using the cosine annealing strategy. 



\noindent\textbf{Image and video colorizers vs. per-scene colorizer.} 
We evaluate the effectiveness of our per-scene colorizer design by comparing it with two representative baselines: the image colorization model DDColor \cite{kang2023ddcolor} and the video colorization method ColorMNet \cite{yang2024colormnet}, as illustrated in Fig.~\ref{figure2}.
It can be observed that DDColor suffer from severe cross-view color inconsistencies due to its per-image one-to-many color mapping nature. Treating multi-view images as a video sequence and applying video colorization similarly fails to resolve this issue, since the viewpoint changes inherent in 3D capture far exceed the temporal variations commonly seen in video data.
In contrast, our personalized colorizer learns a scene-specific, one-to-one color mapping from the reference view and leverages its inherent inductive bias to consistently project colors to corresponding content in novel views and time steps. This design effectively addresses the challenge of color consistency in both static and dynamic 3D scenes.

\begin{figure*}[h]
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
		\includegraphics[width=\linewidth]{figure2.pdf}
	\end{center}
	\vspace{-7pt}
	\caption{Visual ablation study illustrating the effectiveness of the per-scene colorizer design.}
	\label{figure2}
	\vspace{-5pt}
\end{figure*}



\begin{table*}[h]
	\setlength{\tabcolsep}{10pt}	
	\caption{Quantitative comparisons of language-guided 3D scene colorization on Mip-NeRF 360 and DyNeRF datasets.}
	\footnotesize
	\begin{center}
		\begin{tabular}{l|ccc}
			\toprule[1pt]
			% \begin{minipage}{1cm}\vspace{1mm} \centering Type\vspace{1mm} \end{minipage} 
			\makecell[l]{\multirow{2}{*}{Method}} &\multicolumn{3}{c}{\cellcolor{red!20}{Language-Guided Colorization}}
			\\
			& FID$\downarrow$ & CLIP Score$\uparrow$& ME$\downarrow$\\
			\midrule[1pt]
			
			\multicolumn{4}{c}{Mip-NeRF 360 (Static 3D Scenes)}\\
			\midrule
			NeRF+2DColorizer&104.85&0.6153&0.225\\
			3DGS+2DColorizer&112.33&0.6148&0.202\\
			Color3D-NeRF&76.68&0.6213&0.065\\
			\textbf{Color3D (Ours)}&\textbf{68.23}&\textbf{0.6246}&\textbf{0.058}\\
			
			\midrule[1pt]
			\multicolumn{4}{c}{DyNeRF (Dynamic 3D Scenes)}\\
			\midrule
			NeRFPlayer+2DColorizer&86.77&0.6148&0.133\\
			4DGS+2DColorizer&89.39&0.6159&0.124\\
			Color3D-NeRFPlayer&63.29&0.6243&0.046\\
			\textbf{Color3D (Ours)}&\textbf{58.62}&\textbf{0.6271}&\textbf{0.041}\\
			\bottomrule[1pt]
		\end{tabular} 
	\end{center}
	\label{table5}
	\vspace{-5pt}
\end{table*}

\noindent\textbf{Effort of 3D representations.} We further investigate how different 3D representations impact colorization performance. As shown in Tab.\ref{table5}, compared to 3DGS, combining NeRF\cite{mildenhall2021nerf} or NeRFPlayer~\cite{song2023nerfplayer} with 2D colorizers yields suboptimal consistency, reflected by an increase of 0.023 in ME. This is mainly because the explicit modeling of 3DGS gives it an inherent resistance and geometric priors, which can naturally alleviate color inconsistency to some extent.
In addition, we replace the Lab Gaussian representation in Color3D with NeRF and NeRFPlayer, respectively. The results underscore the effectiveness of our proposed Lab Gaussian representation while also demonstrating that the Color3D framework can be adapted to various 3D reconstruction backbones with reasonable performance.



\noindent\textbf{Can editing or stylization methods work for colorization?}
In Tab. \ref{table1} of the main text, we present quantitative comparisons with a representative 3D editing method (GaussianEditor \cite{chen2024gaussianeditor}) and a 3D stylization method (Ref-NPR \cite{zhang2023ref}), both of which yield suboptimal performance on colorization tasks. To demonstrate their visual limitations, we further provide qualitative comparisons in Fig. \ref{figure_editor}.


It is observed that GaussianEditor struggles to comply with color editing instructions when applied to monochromatic radiation fields, primarily prioritizing semantic fidelity over chromatic accuracy.
As a result, the edited outputs, although containing colors, often lack chromatic realism and diversity, and may also suffer from structural artifacts.
Similarly, Ref-NPR primarily focuses on transferring global appearance by relying on perceptual constraints, but fails to accurately capture local color values. In contrast, colorization aims to produce plausible, detailed, and spatially coherent colors that align with the underlying scene content. Consequently, editing and stylization methods are inherently ill-suited for faithful 3D scene colorization. Moreover, their controllability remains limited: editing is typically driven by text prompts, while stylization relies on reference images to guide appearance transfer.

\begin{figure*}[h]
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
		\includegraphics[width=\linewidth]{figure_editorsty.pdf}
	\end{center}
	\vspace{-7pt}
	\caption{Visual comparisons with the 3D editing method GaussianEditor \cite{chen2024gaussianeditor}, and the 3D stylization method Ref-NPR \cite{zhang2023ref}.}
	\label{figure_editor}
	\vspace{-6pt}
\end{figure*}

\noindent\textbf{Universal color manipulation framework.} Beyond colorizing 3D scenes from monochromatic inputs, our proposed scheme can also be naturally extended to 3D scene recoloring. Specifically, we apply InstructPix2Pix \cite{brooks2023instructpix2pix} to edit the colors of a single key view and leverage SAM \cite{kirillov2023segment} to constrain the editing within specified regions. For the personalized colorizer, the input is the original image, and the output is its recolored version. As illustrated in Fig. \ref{figure_recolor}, our method not only enables colorization from monochromatic inputs, but also supports high-fidelity and consistent color manipulations, demonstrating its robustness and versatility.


\begin{figure*}[h]
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
		\includegraphics[width=\linewidth]{figure_recolor.pdf}
	\end{center}
	\vspace{-7pt}
	\caption{Visual results of 3D recoloring.}
	\label{figure_recolor}
	\vspace{-10pt}
\end{figure*}

\noindent\textbf{User study.} To further assess the perceptual quality of the generated results, we conduct a comprehensive user study, acknowledging that subjective human perception remains a critical benchmark in the field of colorization. Specifically, we design 5 evaluation questions covering multiple aspects, including color richness, consistency, aesthetic preference, image quality, and the fidelity of alignment between the synthesized views and the provided conditions (text, exemplars). Participants are asked to rate each sample on a scale of 0 to 5 for each aspect. For a fair comparison, we recruit 30 participants, each evaluating 10 sets of comparisons across different methods. Aggregated results are shown in Fig. \ref{figureus}.
The naive combination of 3DGS with off-the-shelf 2D colorizers yields reasonably rich colors but suffers from significant inconsistency and reduced visual quality. While ColorNeRF demonstrates better consistency, it often lacks color diversity and visual appeal. In contrast, our method consistently achieves higher user ratings across all aspects, highlighting its advantage in terms of perceptual quality and user preference.

\begin{figure*}[h]
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
		\includegraphics[width=\linewidth]{figure_us.pdf}
	\end{center}
	\vspace{-7pt}
	\caption{User study results. Our method demonstrates superior performance across all evaluated aspects.}
	\label{figureus}
	\vspace{-10pt}
\end{figure*}

\noindent\textbf{More visual results.} We further present additional colorization renderings of our Color3D in Fig. \ref{figure_lang}, Fig. \ref{figure_aut}, and Fig. \ref{figure_exam}. Our approach exhibits strong consistency across both spatial and temporal dimensions, delivering vibrant and semantically coherent colorizations that faithfully adhere to user-specified intent.






\subsection{Limitations and Future Work}
One potential limitation, shared with other single-view methods, is that our approach tends to favor assigning plausible colors consistent with the observed input rather than hallucinating entirely novel chromatic content when confronted with highly out-of-domain views (e.g., reconstructing a large room with multiple unseen bedrooms). In future work, we plan to further explore the integration of generative priors to enrich color diversity under drastically unseen viewpoints while maintaining color consistency. Moreover, the per-scene personalization strategy introduced in this work is highly general and carries substantial potential beyond colorization. Extending this paradigm to other scene attributes—such as illumination enhancement, white balance adjustment, and style transfer—represents a promising avenue toward broader and more versatile scene-level editing capabilities.

\begin{figure*}[!p]
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
		\includegraphics[width=\linewidth]{figure_lang.pdf}
	\end{center}
	\caption{ Visual results of language-guided 3D scene colorization. From top to bottom are samples from the LLFF, Mip-NeRF 360, and DyNeRF datasets.}
	\label{figure_lang}
\end{figure*}

\begin{figure*}[!p]
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
		\includegraphics[width=\linewidth]{figure_aut.pdf}
	\end{center}
	\caption{ Visual results of automatic 3D scene colorization. From top to bottom are samples from the LLFF, Mip-NeRF 360, and DyNeRF datasets.}
	\label{figure_aut}
\end{figure*}

\begin{figure*}[!p]
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
		\includegraphics[width=\linewidth]{figure_exam.pdf}
	\end{center}
	\caption{ Visual results of reference-based 3D scene colorization. From top to bottom are samples from the LLFF, Mip-NeRF 360, and DyNeRF datasets.}
	\label{figure_exam}
\end{figure*}



\end{document}
