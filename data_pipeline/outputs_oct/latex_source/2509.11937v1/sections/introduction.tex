
\section{Introduction}
As of 2025, the public web is conservatively estimated to host more than 2.5\,trillion PDF documents, alongside petabytes of mixed-modality slide decks, spreadsheets, images, and audiovisual artefacts \citep{pdfscloudfiles}.  Yet fewer than one percent of these resources are represented in popular machine-learning corpora as they are remain locked behind brittle, heterogeneous formats that frustrate automated parsing at scale. Existing pipelines rely on ad hoc mosaics of format-specific utilities, limiting throughput, reproducibility, and long-term maintainability.

As data-supply forecasts estimate that the pool of high-quality human-generated text could be exhausted by prevailing scaling trends as early as 2026 \cite{villalobos2022,llmdata2024}, it has become essential to find more format-agnostic preprocessing workflows. Much of this data, particularly in specialized or institutional settings, is unavailable for training but remains crucial for improving the verifiability of LLM outputs through RAG. Hallucinations \citep{openai2025o3o4} and factual drift \citep{huang2025survey} remain significant challenges, and robust RAG pipelines are increasingly explored as a means to mitigate these issues, thereby reducing the burden of manual validation and better aligning model outputs with trustworthy source material.

To address these limitations, we introduce \mmore{} an open-source tool for \textbf{\texttt{M}}assive \textbf{\texttt{M}}ultimodal \textbf{\texttt{O}}pen \textbf{\texttt{R}}etrieval-Augmented Generation and \textbf{\texttt{E}}xtraction, a unified pipeline for scalable extraction, transformation, and retrieval of multimodal data. \mmore{} supports diverse formats such as documents, presentations, spreadsheets, and multimedia and integrates them into a structured knowledge base, enabling LLMs to access accurate, contextually grounded information via the RAG paradigm.

Designed for modularity and scalability, our pipeline natively supports parallelized processing across multi-node architectures and distributed environments such as Kubernetes clusters. Compared to Docling demonstrates more than 2-fold faster end-to-end processing, while achieving 40\% higher layout accuracy on scanned PDFs. In distributed mode, we show that our pipeline processes 720 pages in 185s using four nodes, resulting in 3.8-fold speedup over single-node mode. The results demonstrate \mmore{}'s effectiveness as a scalable, high-accuracy solution for multimodal document processing in real-world deployment.

% MMORE runs seamlessly on a single machine or scales up to large compute clusters, fully utilizing available resources. It can efficiently handle terabytes of data — think 8000 PDFs, 2000 videos, and 500 spreadsheets — and prepare them for RAG-based systems.

% Designed with software engineering best practices, MMORE prioritizes modularity, interoperability, and ease of extension. It builds on reliable, actively maintained tools such as Dask (distributed compute), FastAPI (API layer), and PyTorch (ML workflows), and integrates natively with LangChain, Hugging Face, and Milvus to support seamless RAG pipeline construction. This foundation ensures high performance and long-term sustainability.

% \begin{tcolorbox}[colback=blue!5!white]
% \small
% \textbf{MMORE Design Philosophy} \\
% 1. \textbf{Modularity} — Plug-and-play components \\
% 2. \textbf{Interoperability} — Integrates with LangChain, Hugging Face, Milvus \\
% 3. \textbf{Extensibility} — Add formats or models quickly \\
% 4. \textbf{Sustainability} — Built on trusted, maintained tools
% \end{tcolorbox}