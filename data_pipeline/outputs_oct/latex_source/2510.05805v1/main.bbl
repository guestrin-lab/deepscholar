\begin{thebibliography}{}

\bibitem[Bai et~al., 2018]{bai2018empirical}
Bai, S., Kolter, J.~Z., and Koltun, V. (2018).
\newblock An empirical evaluation of generic convolutional and recurrent networks for sequence modeling.
\newblock {\em arXiv preprint arXiv:1803.01271}.

\bibitem[Bohdal et~al., 2020]{bohdal2020flexible}
Bohdal, O., Yang, Y., and Hospedales, T. (2020).
\newblock Flexible dataset distillation: Learn labels instead of images.
\newblock {\em arXiv preprint arXiv:2006.08572}.

\bibitem[Carlini et~al., 2022]{carlini2022no}
Carlini, N., Feldman, V., and Nasr, M. (2022).
\newblock No free lunch in ``privacy for free: How does dataset condensation help privacy''.
\newblock {\em arXiv preprint arXiv:2209.14987}.

\bibitem[Cazenavette et~al., 2022]{cazenavette2022dataset}
Cazenavette, G., Wang, T., Torralba, A., Efros, A.~A., and Zhu, J.-Y. (2022).
\newblock Dataset distillation by matching training trajectories.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 10718--10727.

\bibitem[Draxler et~al., 2018]{draxler2018essentially}
Draxler, F., Veschgini, K., Salmhofer, M., and Hamprecht, F.~A. (2018).
\newblock Essentially no barriers in neural network energy landscape.
\newblock In {\em International Conference on Machine Learning}, pages 1309--1318. PMLR.

\bibitem[Du et~al., 2023]{du2023ftd}
Du, J., Jiang, Y., Tan, V.~Y., Zhou, J.~T., and Li, H. (2023).
\newblock Minimizing the accumulated trajectory error to improve dataset distillation.
\newblock {\em arXiv preprint arXiv:2211.11004}.

\bibitem[Dwork et~al., 2006]{dwork2006calibrating}
Dwork, C., McSherry, F., Nissim, K., and Smith, A. (2006).
\newblock Calibrating noise to sensitivity in private data analysis.
\newblock In {\em Theory of Cryptography Conference}, pages 265--284. Springer.

\bibitem[Garipov et~al., 2018]{garipov2018loss}
Garipov, T., Izmailov, P., Podoprikhin, D., Vetrov, D.~P., and Wilson, A.~G. (2018).
\newblock Loss surfaces, mode connectivity, and fast ensembling of dnns.
\newblock In {\em Advances in Neural Information Processing Systems}, volume~31.

\bibitem[Guo et~al., 2024]{guo2024datm}
Guo, Z., Wang, K., Cazenavette, G., Li, H., Zhang, K., and You, Y. (2024).
\newblock Towards lossless dataset distillation via difficulty-aligned trajectory matching.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Hochreiter and Schmidhuber, 1997]{hochreiter1997long}
Hochreiter, S. and Schmidhuber, J. (1997).
\newblock Long short-term memory.
\newblock {\em Neural Computation}, 9(8):1735--1780.

\bibitem[Izmailov et~al., 2018]{izmailov2018averaging}
Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., and Wilson, A.~G. (2018).
\newblock Averaging weights leads to wider optima and better generalization.
\newblock In {\em Conference on Uncertainty in Artificial Intelligence}, pages 876--885. PMLR.

\bibitem[Jacot et~al., 2018]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C. (2018).
\newblock Neural tangent kernel: Convergence and generalization in neural networks.
\newblock {\em Advances in Neural Information Processing Systems}, 31.

\bibitem[Johnson et~al., 2016]{johnson2016mimic}
Johnson, A. E.~W., Pollard, T.~J., Shen, L., Li-wei, L.~H., Feng, M., Ghassemi, M., Moody, B., Szolovits, P., Celi, A.~L., and Mark, R.~G. (2016).
\newblock Mimic-iii, a freely accessible critical care database.
\newblock {\em Scientific Data}, 3(1):1--9.

\bibitem[Li et~al., 2025]{li2025dataset}
Li, M., Cui, C., Liu, Q., Deng, R., Yao, T., Lionts, M., and Huo, Y. (2025).
\newblock Dataset distillation in medical imaging: A feasibility study.
\newblock In {\em Medical Imaging 2025: Image Perception, Observer Performance, and Technology Assessment}, volume 13409, pages 172--179. SPIE.

\bibitem[Liu et~al., 2023]{liu2023tesla}
Liu, X., Tapia, M., Mallya, A., Davis, L., and Shrivastava, A. (2023).
\newblock Scaling up dataset distillation to imagenet-1k with constant memory.
\newblock {\em arXiv preprint arXiv:2211.10586}.

\bibitem[Moulines and Bach, 2011]{moulines2011}
Moulines, {\'E}. and Bach, F. (2011).
\newblock Non-asymptotic analysis of stochastic approximation algorithms for machine learning.
\newblock In {\em Advances in Neural Information Processing Systems}, volume~24, pages 451--459.

\bibitem[Nguyen et~al., 2021]{nguyen2021dataset}
Nguyen, T., Chen, Z., and Lee, J. (2021).
\newblock Dataset meta-learning from kernel ridge regression.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Nguyen et~al., 2022]{nguyen2022dataset}
Nguyen, T., Novak, R., Xiao, L., and Lee, J. (2022).
\newblock Dataset distillation with infinitely wide convolutional networks.
\newblock In {\em Advances in Neural Information Processing Systems}, volume~35, pages 5186--5198.

\bibitem[Pollard et~al., 2018]{pollard2018eicu}
Pollard, T.~J., Johnson, A., Raffa, J.~D., Celi, L.~A., Mark, R.~G., and Badawi, O. (2018).
\newblock The eicu collaborative research database, a freely available multi-center database for critical care research.
\newblock {\em Scientific Data}, 5(1):1--13.

\bibitem[Pottmann and Wallner, 2001]{pottmann2001}
Pottmann, H. and Wallner, J. (2001).
\newblock {\em Computational Line Geometry}.
\newblock Springer.

\bibitem[Rajkomar et~al., 2019]{rajkomar2019machine}
Rajkomar, A., Dean, J., and Kohane, I. (2019).
\newblock Machine learning in medicine.
\newblock {\em New England Journal of Medicine}, 380(14):1347--1358.

\bibitem[Rumelhart et~al., 1986]{rumelhart1986learning}
Rumelhart, D.~E., Hinton, G.~E., and Williams, R.~J. (1986).
\newblock Learning representations by back-propagating errors.
\newblock {\em Nature}, 323(6088):533--536.

\bibitem[Shabani, 2019]{shabani2019gdpr}
Shabani, M. (2019).
\newblock The impact of the general data protection regulation (gdpr) on artificial intelligence.
\newblock {\em European Journal of Human Genetics}, 27(1):15--17.

\bibitem[Soltan et~al., 2024]{soltan2024scalable}
Soltan, A.~A., Thakur, A., Yang, J., Chauhan, A., D'Cruz, L.~G., Dickson, P., Soltan, M.~A., Thickett, D.~R., Eyre, D.~W., Zhu, T., et~al. (2024).
\newblock A scalable federated learning solution for secondary care using low-cost microcomputing: privacy-preserving development and evaluation of a covid-19 screening test in uk hospitals.
\newblock {\em The Lancet Digital Health}, 6(2):e93--e104.

\bibitem[Sucholutsky and Schonlau, 2019]{sucholutsky2019soft}
Sucholutsky, I. and Schonlau, M. (2019).
\newblock Soft-label dataset distillation and text dataset distillation.
\newblock {\em arXiv preprint arXiv:1910.02551}.

\bibitem[Sun et~al., 2023]{sun2023rded}
Sun, P., Shi, B., Yu, D., and Lin, T. (2023).
\newblock On the diversity and realism of distilled dataset: An efficient dataset distillation paradigm.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 3928--3937.

\bibitem[Thakur et~al., 2025]{thakur2025optimising}
Thakur, A., Molaei, S., Schwab, P., Belgrave, D., Branson, K., and Clifton, D.~A. (2025).
\newblock Optimising clinical federated learning through mode connectivity-based model aggregation.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}, pages 163--171. PMLR.

\bibitem[Thakur et~al., 2023]{thakur2023clinical}
Thakur, A., Wang, C., Ceritli, T., Clifton, D., and Eyre, D. (2023).
\newblock Mode connections for clinical incremental learning: Lessons from the covid-19 pandemic.
\newblock {\em medRxiv}.
\newblock ICML Workshop on Interpretable Machine Learning in Healthcare.

\bibitem[Thakur et~al., 2024]{thakur2024data}
Thakur, A., Zhu, T., Abrol, V., Armstrong, J., Wang, Y., and Clifton, D.~A. (2024).
\newblock Data encoding for healthcare data democratization and information leakage prevention.
\newblock {\em Nature Communications}, 15(1):1582.

\bibitem[Topol, 2019]{topol2019high}
Topol, E.~J. (2019).
\newblock High-performance medicine: the convergence of human and artificial intelligence.
\newblock {\em Nature Medicine}, 25(1):44--56.

\bibitem[Wang et~al., 2022]{wang2022cafe}
Wang, K., Zhao, B., Peng, X., Zhu, Z., Yang, S., Wang, S., Huang, G., Bilen, H., Wang, X., and You, Y. (2022).
\newblock Cafe: Learning to condense dataset by aligning features.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 12196--12205.

\bibitem[Wang et~al., 2018]{wang2018dataset}
Wang, T., Zhu, J.-Y., Torralba, A., and Efros, A.~A. (2018).
\newblock Dataset distillation.
\newblock {\em arXiv preprint arXiv:1811.10959}.

\bibitem[Wang et~al., 2023]{wang2023medical}
Wang, Y., Thakur, A., Dong, M., Ma, P., Petridis, S., Shang, L., Zhu, T., and Clifton, D.~A. (2023).
\newblock Medical records condensation: a roadmap towards healthcare data democratisation.
\newblock {\em arXiv preprint arXiv:2305.03711}.

\bibitem[Yin et~al., 2023]{yin2023sre2l}
Yin, Z., Xing, E., and Shen, Z. (2023).
\newblock Squeeze, recover and relabel: Dataset condensation at imagenet scale from a new perspective.
\newblock In {\em Advances in Neural Information Processing Systems}, volume~36.

\bibitem[Zhang et~al., 2024]{zhang2024m3d}
Zhang, H., Li, S., Wang, P., Zeng, D., and Ge, S. (2024).
\newblock M3d: Dataset condensation by minimizing maximum mean discrepancy.
\newblock {\em arXiv preprint arXiv:2312.15927}.

\bibitem[Zhao and Bilen, 2021]{zhao2021dataset}
Zhao, B. and Bilen, H. (2021).
\newblock Dataset condensation with gradient matching.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Zhao and Bilen, 2023]{zhao2023dataset}
Zhao, B. and Bilen, H. (2023).
\newblock Dataset condensation with distribution matching.
\newblock In {\em Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}, pages 6514--6523.

\bibitem[Zhao et~al., 2023]{zhao2023improved}
Zhao, G., Li, G., Qin, Y., and Yu, Y. (2023).
\newblock Improved distribution matching for dataset condensation.
\newblock {\em arXiv preprint arXiv:2307.09742}.

\end{thebibliography}
