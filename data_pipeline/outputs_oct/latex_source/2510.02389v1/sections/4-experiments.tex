\vspace{-6mm}
\section{Experiment Setup}
\vspace{-2mm}
% \textbf{Metrics.} We report two complementary scores tailored to project-level OSS vulnerability studies. \emph{Detection} captures coarse-grained capability: did the agent correctly flag the presence of a vulnerability within the relevant module or chunk and narrow the search space in a large codebase? \emph{Localization} then measures fine-grained precision by requiring exact line-level matches against the ground-truth patches in the repositories. Together, these metrics separate “finding the right neighborhood” from “pinpointing the faulty line,” which mirrors how real debugging proceeds in practice.

% \textbf{Data Preparation.} We evaluate our T2L-Agent on the full version of \textbf{T2L-ARVO} set of 50 verified and structured challenges designed for agentic task from \texttt{ARVO}. The cohort spans a broad range of project domains and codebases—from image processing libraries to network protocols—and is balanced across complexity levels, so results reflect real production patterns rather than a single project family or bug type.

% Because the original ARVO release does not provide chunking suitable for detection tasks, T2L-ARVO introduces AST-based code segmentation to support agentic evaluation. We partition each project into semantically meaningful units and score agent proposals against these units, then require exact line matches for patch-aligned ground truth. This preprocessing lets us measure coarse-grained \emph{detection}: finding the right region, and fine-grained \emph{localization}: pinpointing the faulty line within one consistent framework.

% \textbf{Model Selection.} Our evaluation covers eight state-of-the-art language models, including both open-source and commercial systems, to ensure a comprehensive assessment of the T2L-Agent framework. The open-source models include Qwen 3, DeepSeek V3.1, and LLaMA 4 Maverick. The commercial models evaluated are Claude 4 Sonnet, GPT-5, and Gemini 2.5 Pro. This diverse selection enables a thorough analysis of the T2L-Agent's performance across different architectural approaches and parameter scales, providing insights into its generalizability and robustness.

% \textbf{Implementation.} We build T2L-Agent from scratch without frameworks like LangChain, DSPy, or LlamaIndex to keep the core lightweight, retain fine-grained control over reasoning, and provide ample extensibility for researchers given the task's complexity and space for agentic advances in the future. The \emph{Agentic Trace Analyzer} compiles targets with AddressSanitizer (ASAN) and harvests crashes, stack traces, and allocation metadata to surface memory errors and produce actionable traces for subsequent narrowing. Following ARVO’s organization model, we preserve a registry-backed layout for per-project environments and provide both vulnerable and patched revisions in containers. Our evaluation harness runs dockerized T2L agents that talk to the full T2L-ARVO dataset via the Docker SDK for Python, orchestrating build, run, and reproduce cycles directly from the agent loop. This containerized design enforces consistent conditions across samples while keeping reproduction and measurement deterministic and auditable.

\textbf{Metrics.} We report two complementary scores for project-level OSS vulnerability studies. \emph{Detection} asks whether the agent flags a vulnerability within the correct module/chunk and materially shrinks the search space. \emph{Localization} requires exact line matches to ground-truth patches. Together, they separate “finding the neighborhood” from ``pinpointing the line'', mirroring real debugging.

\textbf{Data Preparation.} We evaluate on the full \texttt{T2L-ARVO} set: 50 verified, structured challenges derived from \texttt{ARVO}, spanning diverse domains (e.g., imaging, networking) and balanced complexity so results reflect production patterns rather than a single project or bug family. Because ARVO lacks detection-ready chunking, \texttt{T2L-ARVO} adds AST-based segmentation: projects are partitioned into semantically meaningful units for scoring coarse \emph{detection}, while exact line matches assess fine \emph{localization}—a single framework for both levels.

\textbf{Model Selection.} We assess a set of state-of-the-art language models—both open-source and commercial to probe generality and robustness of \texttt{T2L-Agent} across architectures and scales, including open models such as Qwen3 Next, Qwen3 235B, DeepSeek 3.1, LLaMA 4 and commercial models like Claude4 Sonnet, GPT-5, GPT-4.1, GPT-4o-mini, Gemini 2.5 Pro, Gemini 2.5 Flash with a maximum budget \$1.0. We use API keys from commercial model's official providers and Together.ai's inference service for open source models.

\textbf{Implementation.} We build \texttt{T2L-Agent} from scratch without LangChain, DSPy, and LlamaIndex to keep the core lightweight, retain fine-grained control over reasoning and tools, and maximize extensibility. The \emph{Agentic Trace Analyzer} compiles targets with ASAN and collects crashes, stack traces, and allocation metadata to yield actionable traces for narrowing. Following ARVO’s layout, we maintain registry-backed, per-project environments and provide both vulnerable and patched revisions in containers. Our harness runs dockerized \texttt{T2L-Agent}'s that interface with \texttt{T2L-ARVO} via the Docker SDK for Python, orchestrating build–run–reproduce cycles from within the agent loop—ensuring consistent conditions, deterministic reproduction, and auditable measurement.

