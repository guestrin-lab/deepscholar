\vspace{-2mm}
\section{Evaluation}
\vspace{-2mm}

\subsection{Baseline Benchmarking}

\begin{table}[!t]
\centering
\tiny
\caption{Localization and Detection Rate Performance Across Different Models.}
\label{tab:Benchmark_Performance_Comparison}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{l r r c c c c c c c c c c}
\toprule
& \multicolumn{2}{c}{\textbf{\% Avg.}} & \multicolumn{2}{c}{\textbf{Buffer}} & \multicolumn{2}{c}{\textbf{Initialize}} & \multicolumn{2}{c}{\textbf{Memory}} & \multicolumn{2}{c}{\textbf{Parameter}} & \multicolumn{2}{c}{\textbf{Runtime}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
& \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} \\
\midrule
GPT-5    & 44.3 & \textbf{41.7} & 57.5 & \textbf{53.8} & 35.6 & 35.5 & \textbf{60.8} & \textbf{55.9} & 36.5 & 39.4 & 11.2 & 10.0 \\
GPT-4.1            & \textbf{48.0} & 38.5 & \textbf{60.8} & 36.5 & 50.6 &\textbf{46.4} & \textbf{60.8} & 46.1 & 26.5 & 29.9 & 21.2 & \textbf{20.5} \\
GPT-4o-mini        & 44.3 & 22.6 & \textbf{60.8} & 20.2 & 48.1 & 12.5 & 55.8 & 24.7 & 28.7 & 26.7 & 11.2 & 10.0 \\
Claude 4 Sonnet & 45.9 & 30.5 & 57.5 & 50.6 & \textbf{60.8} & 36.1 & 1.3  & 31.6 & \textbf{37.8} & \textbf{46.1}& \textbf{24.8} & 0.5   \\
Gemini2.5 Pro &17.4 &10.5& 25.0 &	5.6 &	25.0 & 20.0 &	11.3 &	10.8 &	5.4 &	11.9 &	10.0 &	10.5 \\
Qwen3 235B         & 25.9 &  9.2 & 25.8 & 6.5 & 23.1 & 1.7 & 40.8 & 16.7 & 28.7 & 17.3 & 7.9 & 0.0 \\
Qwen3 Next 80B             & 37.4 &  5.9 & 54.2 & 3.7 & 33.1 & 0.4 & 55.8 & 14.5 & 29.1 & 6.8 & 1.2 & 0.0 \\
\bottomrule
\end{tabular}
\vspace{-2mm}
\end{table}

% \textcolor{blue}{Minghao: I need to go over the evaluation and discussion and reduce the length. Before that, Haoran once you finished other sections please check the merged table and modify the text.}

We evaluate \texttt{T2L-Agent} on \texttt{T2L-ARVO} end-to-end and report Detection Rate and Localization Rate. Table~\ref{tab:Benchmark_Performance_Comparison} covers five models—GPT-5, GPT-4.1, GPT-4o-mini, Qwen 3 Next 80B, and Qwen3 235B, running under identical per-case budgets, environments, and AST-based chunking. Overall, detection is higher than localization by design. Under this setting, GPT-5 leads localization at 41.7\%, followed by GPT-4.1 at 38.5\%; GPT-4o-mini lands at 22.6\%, and open-source models trail Qwen 3 235B 9.2\% and Qwen 3 Next 80B 5.9\%. For detection, GPT-4.1 is highest at 48.0\%, with GPT-5 and GPT-4o-mini both at 44.3\%; Qwen3 reaches 37.4\% and Qwen3~235B 25.9\%. While Gemini2.5 Pro shows limited effectiveness with 17.4\% detection and 10.5\% localization rates.

Family-wise patterns are consistent across metrics. \emph{Buffer} and \emph{Memory} are easier due to concrete runtime cues: for localization, GPT-5 reaches 53.8\% and 55.9\%, and most models cluster near the mid-50s for detection. \emph{Initialize} sits mid-range and benefits from multi-step reasoning (e.g., GPT-5 35.5\% vs.\ GPT-4.1 46.4\% in localization). \emph{Parameter} is often solvable from interface/call-site context (39.4\% for GPT-5; 29.9\% for GPT-4.1). \emph{Runtime} remains uniformly hardest: detection hovers around 11.1–21.2\% even for top configurations, and the best localization we observe is 20.5\% on GPT-4.1, reflecting sparse, unstable traces.

Taken together, equal budgets surface clear profiles. GPT-5 (low-think) converts evidence into the strongest line-level localization, while GPT-4.1 extracts slightly more coarse-grained signal at the chunk level. GPT-4o-mini’s mix—competitive detection (44.3\%) but weak localization (22.6\%)—suggests higher recall with looser ranking that does not always translate to precise line hits. Open-source models lag on both metrics under the same constraints, indicating gaps in code understanding and tool use rather than simple parameter tuning. Overall, improvements track the availability of concrete runtime evidence, and structured, tool-grounded reasoning appears more impactful than generation settings for end-to-end vulnerability localization.


% We evaluate \textsc{T2L-Agent} on \textbf{T2L-ARVO} under the end-to-end protocol using the Detection Rate and Localization Rate metrics. 

% Table~\ref{tab:Benchmark_Performance_Comparison} report results across five different models, GPT-5 (low-think), GPT-4.1, GPT-4o mini, Qwen3, and Qwen3 235B, ``\textbf{low-think}'' means a constrained reasoning configuration for GPT-5, encourages short justifications. And the \textbf{T2L-ARVO} dataset is classified by the five crash families. All models run with identical per-case budgets (tooling, rounds, and API spend), identical environments, and the same AST-based chunking used for detection.

% Focusing first on localization (Loc), GPT-5 (low-think) achieves the highest overall localization rate (41.7\%), followed by GPT-4.1 (38.5\%), and GPT-4o mini reaches 22.6\% as the lowest performance among GPT models. For the open source models, Qwen3 235B and Qwen3 are 9.2\% and 5.9\% respectively, indicating that robust line-level localization ability benefits from stronger code understanding and better tool-use behaviors under tight budgets. 

% Performance also varies by crash families. Buffer and Memory are relatively easy to handle, reaching GPT-5 at 53.8\% and 55.9\%, benefit from concrete runtime cues, Initialize is in the middle range, with 35.5\% for GPT-5 and 46.4\% for GPT-4.1. Parameter is often solvable from call-site information, showing 39.4\% for GPT-5 and 29.9 for GPT-4.1. For Runtime, it remains uniformly low across settings due to sparse, unstable traces, getting the best performance at 20.5\% on GPT-4.1



% %% for those paragraphs i commented, you can see if we need this
% \iffalse
% Performance also varies by crash families. Buffer Overflow vulnerabilities are relatively easy to handle. GPT-5 achieves localization rate at 100.0\% and GPT-4o mini at 87.5\%. Precise sanitizer cues and short, project-level stacks makes trace to line localization easier for our agent. Uninitialized Access \& Unknown States show moderate difficulty: 50.0\% on GPT-5 and 60.0\% on GPT-4.1. Here, sanitizer reports are often less specific and the root cause are input-conditioned states across frames, so cross frame reasoning and multi-round refinement are always required. Memory Lifecycle Errors, like heap-use-after-free, benefit from runtime evidence but still need temporal reasoning across memory and lifetime. We observe 88.9\% on GPT-5 and 77.8\% on GPT-4.1. For Type Safety \& Parameter Validation, most models are around 60-70\%, largely because these cases can often be solved from context cues like argument sizes, without heavy temporal reasoning. Lastly, System \& Runtime Errors are the hardest across all five crash families. Demonstrate 11.1\% for GPT-5 and 33.3\% for GPT-4.1. They are sensitive to environment settings and crashes often occur at entry points, so the available evidence is sparse and unstable, making it difficult to narrow down lines within the fixed budget.
% \fi


% Turning to chunk-level detection (Det), measuring whether the agent locates the correct chunk in the repository. Overall detection rates are higher than line-level localization rates as it has a broader target. GPT-4.1 achieves the highest dectection rate at 48.0\%, with GPT-5 (low-think) and GPT-4o mini slightly behind, both at 44.3\%. Open source models trail achieves 37.4\% for Qwen3 and 25.9\% for Qwen3 235B. 

% Family-wise patterns are consistent with the localization analysis: Buffer and Memory are reliably easier to detect across models, typically around 55\%. Initialize remain strong performance on the detection task. Parameter depends on interface signals, the 25\% - 36\% performance indicating that agents can still pick the right chunk even with noisier evidence. And Runtime remains the hardest, 11.1–21.2\% even for the top configurations, where shallow stacks and setup dependence give little to detection.

% \iffalse
% Breaking the performance down by crash family reveals consistent patterns. Buffer Overflow Vulnerabilities show nearly all models reach 100\%, Qwen3 235B is lower but still at 50\%. Uninitialized Access \& Unknown States remain strong performance on the detection task, reaching 50–70\% across all models, indicating that agents can usually pick the right chunk even with noisier evidence. Detection for Memory Lifecycle Errors is reliable among all models, showing 88.9\% for most models and 66.7\% for Qwen3 235B. For Type Safety \& Parameter Validation, it's detection rate are near 50–60\%, since simple call-site cues often point to the right chunk, makes the deep heap state signals less critical. System \& Runtime Errors remain the toughest to detect, only get 11.1–33.3\% detection rate, where stack traces carry limited, unstable cues, so detection of vulnerable chunks is difficult.
% \fi


% Under the shared budget, GPT-5 (low-think) yields the best localization, while GPT-4.1 leads detection; GPT-4o mini detects competitively but localizes poorly, and open models lag on both. Likely due to lightweight design that prioritizes efficiency over code understaning ability. 

% \begin{table*}[t]
% \centering
% \small
% \caption{Localization Rate Performance Across Different Models.}
% \label{tab:Localization_Rate}
% \setlength{\tabcolsep}{5pt}
% \begin{tabular}{l r c c c c c}
% \toprule
% & \textbf{\% solved} &
% \makecell[c]{\textbf{Buffer}} &
% \makecell[c]{\textbf{Initialize}} &
% \makecell[c]{\textbf{Memory}} &
% \makecell[c]{\textbf{Parameter}} &
% \makecell[c]{\textbf{Runtime}} \\
% \midrule
% GPT-5 low-think    & 41.7 & 53.8 & 35.5 & \textbf{55.9} & \textbf{39.4} & 10.0 \\
% GPT-4.1            & 38.5 & 36.5 & 46.4 & 46.1 & 29.9 & \textbf{20.5} \\
% GPT-4o mini        & 22.6 & 20.2 & 12.5 & 24.7 & 26.7 & 10.0 \\
% Qwen3 235B         &  9.2 & 6.5 & 1.7 & 16.7 & 17.3 & 0.0 \\
% Qwen3              &  5.9 & 3.7 & 0.4 & 14.5 & 6.8 & 0.0 \\
% \midrule
% GPT-4o mini refine        & 29.1 & 43.6 & 20.2 & 33.6 & 21.2 & 0.0  \\
% Qwen3 235B refine       &  26.7 & 32.0 & 34.5 & 38.8 & 13.3 & 8.2  \\
% Qwen3 refine             &  39.5 & 55.1 & 44.4 & 48.6 & 29.2 & 0.0  \\
% \midrule
% GPT-4o mini refine top10        & \textbf{43.3} & \textbf{55.6} & \textbf{48.8} & 47.0 & 32.4 & 0.5  \\

% \bottomrule
% \end{tabular}
% \end{table*}


% \begin{table*}[t]
% \centering
% \small
% \caption{Detection Rate Performance Across Different Models.}
% \label{tab:Detection_Rate}
% \setlength{\tabcolsep}{5pt}
% \begin{tabular}{l r c c c c c}
% \toprule
% & \textbf{\% solved} &
% \makecell[c]{\textbf{Buffer}} &
% \makecell[c]{\textbf{Initialize}} &
% \makecell[c]{\textbf{Memory}} &
% \makecell[c]{\textbf{Parameter}} &
% \makecell[c]{\textbf{Runtime}} \\

% \midrule
% GPT-5 low-think  & 44.3 & 57.5 & 35.6 & \textbf{60.8} & \textbf{36.5} & 11.2 \\
% GPT-4.1            & \textbf{48.0} &  60.8 & 50.6 & \textbf{60.8} & 26.5 & \textbf{21.2} \\
% GPT-4o mini        & 44.3 & 60.8 & 48.1 & 55.8 & 28.7 & 11.2 \\
% Qwen3 235B         & 25.9 &   25.8 & 23.1 & 40.8 & 28.7 & 7.9 \\
% Qwen3              & 37.4 & 54.2 & 33.1 & 55.8 & 29.1 & 1.2 \\

% \midrule

% GPT-4o mini refine        & 34.6 &  45.8 & 30.6 & 45.8 & 22.4 & 0.0 \\
% Qwen3 235B refine       &  34.1 & 34.2 & 30.6 & 57.5 & 23.3 & 5.0 \\
% Qwen3 refine             &  42.9 & 60.8 & 50.6 & \textbf{60.8} & 25.7 & 0.0 \\

% \midrule

% GPT-4o mini refine top10        & 47.2 & \textbf{64.2} & \textbf{55.6} & 52.5 & 33.9 & 1.2 \\
% \bottomrule
% \end{tabular}
% \end{table*}

% \begin{wrapfigure}{r}{0.47\textwidth}
%     \vspace{-4mm}
%     \centering
%     \includegraphics[width=\linewidth]{diagrams/failure_reasons.png}
%     \vspace{-6mm}
%     \caption{Failure reason distribution.}
%     \label{fig:placeholder}
%     \vspace{-12mm}
% \end{wrapfigure}

\vspace{-2mm}
\subsection{Discussion 1: Feature-wise Evaluation}
\vspace{-2mm}

\textbf{Agentic Trace Analyzer.}
This table ~\ref{tab:Combined_Performance_Comparison} demonstrates the critical effectiveness of our proposed Agentic Trace Analyzer (ATA) through ablation experiments. Without ATA, both GPT-5 and Claude 4 Sonnet achieve 0.0\% detection and localization rates across all vulnerability families. This complete performance breakdown validates that our ATA component successfully bridges the gap between crash symptoms and vulnerability locations, addresses the fundamental challenge of vulnerability localization in complex codebases. 

% \begin{table*}[t]
% \centering
% \small
% \caption{Localization and Detection Rate Performance with Refinement Across Different Models.}
% \label{tab:Refinement_Performance_Comparison}
% \setlength{\tabcolsep}{4pt}
% \begin{tabular}{l r r r r r r r r r r r r}
% \toprule
% & \multicolumn{2}{c}{\textbf{\% Avg.}} & \multicolumn{2}{c}{\textbf{Buffer}} & \multicolumn{2}{c}{\textbf{Initialize}} & \multicolumn{2}{c}{\textbf{Memory}} & \multicolumn{2}{c}{\textbf{Parameter}} & \multicolumn{2}{c}{\textbf{Runtime}} \\
% \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
% & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} \\
% \midrule

% Claude4s\_refine & 44.8 & 41.4 & 57.5 & 54.3 & 40.6 & 43.2 & 61.7 & 52.5 & 26.1 & 29.4 & 14.6 & 10.5 \\
% Gemini2.5 pro\_refine & 14.1 & 11.4 & 10.0 & 8.6 & 20.0 & 16.2 & 40.0 & 32.1 & 0.4 & 0.3 & 0.0 & 0.0 \\
% GPT-5 low-think \_refine & 52.4 & 44.5 & 57.5 & 55.0 & 55.6 & 43.1 & 60.8 & 41.3 & 43.5 & \textbf{48.2} & 21.2 & 20.5 \\
% GPT-4.1\_refine & 48.3 & 40.8 & 60.8 & 51.9 & 53.1 & 44.9 & 57.5 & 46.1 & 39.8 & 42.9 & 6.7 & 0.0 \\
% GPT-4o mini \_refine & 34.6 & 29.1 & 45.8 & 43.6 & 30.6 & 20.2 & 45.8 & 33.6 & 22.4 & 21.2 & 0.0 & 0.0 \\
% Qwen3\_refine & 42.9 & 39.5 & 60.8 & 55.1 & 50.6 & 44.4 & 60.8 & 48.6 & 25.7 & 29.2 & 0.0 & 0.0 \\
% Qwen3\_235B\_refine & 34.1 & 26.7 & 34.2 & 32.0 & 30.6 & 34.5 & 57.5 & 38.8 & 23.3 & 13.3 & 5.0 & 8.2 \\

% \bottomrule
% \end{tabular}
% \end{table*}

\textbf{Detection Refinement.} Compared with Tab ~\ref{tab:Benchmark_Performance_Comparison}, Tab.~\ref{tab:Combined_Performance_Comparison} shows broad, across-the-board gains after enabling refinement. Strong proprietary models improve steadily, while open-source models jump the most—Qwen3~235B’s localization rises by roughly sevenfold. Improvements vary by crash family: \emph{Initialize} bugs benefit most (they demand multi-step reasoning), whereas \emph{Buffer} and \emph{Memory} see smaller lifts because concrete runtime evidence already anchors the search. \emph{Runtime} cases remain hard—when traces are sparse, refinement offers limited benefit. Net effect: higher recall and more precise line-level hits with minimal tuning. Several additional models show promising performance. Deepseek V3.1 achieves the highest overall results with 53.9\% detection and 53.4\% localization rate. LLaMa 4 demonstrates balanced capabilities on both metrics, and Gemini 2.5 Flash shows variable performance across crash families.

% The refinement feature shows different impact among five vulnerability families. For Initialize, models achieve largest improvements in both detection and localization rate, the GPT-5 gains from 35.6\% to 55.6\% in detection and 35.5\% to 43.1\% in localization, while Qwen3 shows remarkable growth from 33.1\% to 50.6\% in detection and 0.4\% to 44.4\% in localization. The performance on Parameter also exhibits strong increase, with GPT-5 reaching 43.5\% detection and 48.2\% localization rate compared to baseline performance of 36.5\% and 39.4\% respectively. Buffer and Memory cases show more modest improvements, as these families already benefit from concrete runtime evidence that provide clear information. Runtime remains uniformly challenging with limited refinement benefits across all models, reflecting even through multiple analysis rounds, localizing using traces without much information is still difficult.

% Refinement improves the Qwen models, Qwen3 to 42.9\% and Qwen3 235B to 34.1\%, while lowers the GPT-4o mini’s detection to 34.6\%, indicating that its extra candidates are more spread out and push the correct chunk lower in the ranking. 

% \begin{table*}[t]
% \centering
% \small
% \caption{Localization and Detection Rate Performance with Divergence Tracing Across Different Models.}
% \label{tab:Divergence_Performance_Comparison}
% \setlength{\tabcolsep}{4pt}
% \begin{tabular}{l r r r r r r r r r r r r}
% \toprule
% & \multicolumn{2}{c}{\textbf{\% Avg.}} & \multicolumn{2}{c}{\textbf{Buffer}} & \multicolumn{2}{c}{\textbf{Initialize}} & \multicolumn{2}{c}{\textbf{Memory}} & \multicolumn{2}{c}{\textbf{Parameter}} & \multicolumn{2}{c}{\textbf{Runtime}} \\
% \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
% & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} \\

% \midrule

% GPT-4o mini\_diverse & 47.2 & 43.3 & \textbf{64.2} & 55.6 & 55.6 & 48.8 & 52.5 & 47.0 & 33.9 & 32.4 & 1.2 & 0.5 \\
% GPT-4.1\_diverse & 52.0 & 49.9 & 60.8 & 53.5 & \textbf{60.6} & \textbf{57.3} & 57.5 & 53.0 & 43.2 & 37.1 & 21.2 & 20.1 \\
% GPT-5 low-think\_diverse & \textbf{58.0} & \textbf{52.0} & 60.8 & \textbf{56.8} & \textbf{60.6} & 53.4 & \textbf{62.5} & 47.6 & \textbf{53.2} & 46.7 & \textbf{26.2} & \textbf{28.7} \\
% \bottomrule
% \end{tabular}
% \end{table*}

\begin{table}[t]
\centering
\tiny
\caption{Localization and Detection Rate Performance with Refinement and Divergence Tracing Across Different Models.}
\label{tab:Combined_Performance_Comparison}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{l r r r r r r r r r r r r r r}
\toprule
& \multicolumn{4}{c}{\textbf{\% Avg.}} & \multicolumn{2}{c}{\textbf{Buffer}} & \multicolumn{2}{c}{\textbf{Initialize}} & \multicolumn{2}{c}{\textbf{Memory}} & \multicolumn{2}{c}{\textbf{Parameter}} & \multicolumn{2}{c}{\textbf{Runtime}} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13} \cmidrule(lr){14-15}
& \textbf{Det} & \textbf{Loc} & \textbf{$\Delta$Det} & \textbf{$\Delta$Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} \\
\midrule
\multicolumn{15}{c}{\textbf{w/ Detection Refinement}} \\
\midrule
GPT-5            & 52.4 & 44.5 & +8.1\textcolor{ForestGreen}{$\uparrow$} & +2.8\textcolor{ForestGreen}{$\uparrow$} & 57.5 & 55.0 & 55.6 & 43.1 & 60.8 & 41.3 & 43.5 & 48.2 & 21.2 & 20.5 \\
GPT-4.1          & 48.3 & 40.8 & +0.3\textcolor{ForestGreen}{$\uparrow$} & +2.3\textcolor{ForestGreen}{$\uparrow$} & 60.8 & 51.9 & 53.1 & 44.9 & 57.5 & 46.1 & 39.8 & 42.9 & 6.7 & 0.0 \\
GPT-4o-mini      & 34.6 & 29.1 & -9.7\textcolor{red}{$\downarrow$} & +6.5\textcolor{ForestGreen}{$\uparrow$} & 45.8 & 43.6 & 30.6 & 20.2 & 45.8 & 33.6 & 22.4 & 21.2 & 0.0 & 0.0 \\

Claude 4 Sonnet         & 44.8 & 41.4 & -1.1\textcolor{red}{$\downarrow$} & +10.9\textcolor{ForestGreen}{$\uparrow$} & 57.5 & 54.3 & 40.6 & 43.2 & 61.7 & 52.5 & 26.1 & 29.4 & 14.6 & 10.5 \\

Gemini2.5 Pro    & 14.1 & 11.4 & -3.3\textcolor{red}{$\downarrow$} & -0.9\textcolor{red}{$\downarrow$} & 10.0 & 8.6  & 20.0 & 16.2 & 40.0 & 32.1 & 0.4  & 0.3  & 0.0  & 0.0 \\

Qwen3 Next 80B   & 42.9 & 39.5 & +5.5\textcolor{ForestGreen}{$\uparrow$} & +33.6\textcolor{ForestGreen}{$\uparrow$} & 60.8 & 55.1 & 50.6 & 44.4 & 60.8 & 48.6 & 25.7 & 29.2 & 0.0 & 0.0 \\
Qwen3 235B       & 34.1 & 26.7 & +8.2\textcolor{ForestGreen}{$\uparrow$} & +17.5\textcolor{ForestGreen}{$\uparrow$} & 34.2 & 32.0 & 30.6 & 34.5 & 57.5 & 38.8 & 23.3 & 13.3 & 5.0 & 8.2 \\
Gemini 2.5 Flash  & 22.5 & 18.4 & -- & -- & 34.2 & 0.6  & 40.8 & 25.7 & 7.9  & 27.6 & 0.4  & 33.4 & 24.6 & 0.6 \\
Llama4           & 28.3 & 28.1 & -- & -- & 30.8 & 25.6 & 35.8 & 26.1 & 0.0  & 27.9 & 24.1 & 32.0 & 30.2 & 0.0 \\
Deepseek V3.1     & 53.9 & 53.4 & -- & -- & 60.8 & 55.6 & \textbf{62.5} & 47.5 & 16.3 & 58.1 & 55.0 & \textbf{60.2} & 47.6 & 19.4 \\
\midrule
\multicolumn{15}{c}{\textbf{w/ Divergence Tracing}} \\
\midrule

GPT-5            & \textbf{58.0} & 52.0 & +13.7\textcolor{ForestGreen}{$\uparrow$} & +10.3\textcolor{ForestGreen}{$\uparrow$} & 60.8 & 56.8 & 60.6 & 53.4 & \textbf{62.5} & 47.6 & 53.2 & 46.7 & 26.2 & \textbf{28.7} \\

GPT-4.1          & 52.0 & 49.9 & +4.0\textcolor{ForestGreen}{$\uparrow$} & +11.4\textcolor{ForestGreen}{$\uparrow$} & 60.8 & 53.5 & 60.6 & \textbf{57.3} & 57.5 & 53.0 & 43.2 & 37.1 & 21.2 & 20.1 \\
GPT-4o-mini      & 47.2 & 43.3 & +2.9\textcolor{ForestGreen}{$\uparrow$} & +20.7\textcolor{ForestGreen}{$\uparrow$} & \textbf{64.2} & 55.6 & 55.6 & 48.8 & 52.5 & 47.0 & 33.9 & 32.4 & 1.2 & 0.5 \\

Claude 4 Sonnet         & 48.7 & 49.8 & +2.8\textcolor{ForestGreen}{$\uparrow$} & +19.3\textcolor{ForestGreen}{$\uparrow$} & 60.8 & 55.6 & \textbf{62.5} & 39.8 & 11.3 & 57.5 & 53.7 & 57.6 & 46.3 & 10.6 \\

Qwen 3 Next 80B   & 51.2 & \textbf{54.8} & +13.8\textcolor{ForestGreen}{$\uparrow$} & +48.9\textcolor{ForestGreen}{$\uparrow$} & \textbf{64.2} & \textbf{58.1} & \textbf{62.5} & 43.2 & 11.3 & \textbf{63.2} & \textbf{58.6} & 57.7 & \textbf{48.8} & 21.2 \\

Qwen 3 235B       & 42.7 & 42.1 & +16.8\textcolor{ForestGreen}{$\uparrow$} & +32.9\textcolor{ForestGreen}{$\uparrow$} & 50.8 & 50.6 & 47.5 & 40.2 & 1.0  & 45.4 & 46.9 & 53.5 & 33.7 & 11.2 \\

\midrule
\multicolumn{15}{c}{\textbf{w/o Agentic Trace Analyzer}} \\
\midrule
GPT-5            & 0.0 & 0.0 & -44.3\textcolor{red}{$\downarrow$} & -41.7\textcolor{red}{$\downarrow$} & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
Claude 4 Sonnet         & 0.0 & 0.0 & -45.9\textcolor{red}{$\downarrow$} & -30.5\textcolor{red}{$\downarrow$} & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
\bottomrule
\end{tabular}
\end{table}


% \begin{table}[t]
% \centering
% \footnotesize
% \caption{Localization and Detection Rate Performance with Refinement and Divergence Tracing Across Different Models.}
% \label{tab:Combined_Performance_Comparison}
% \setlength{\tabcolsep}{4pt}
% \begin{tabular}{l r r r r r r r r r r r r r r}
% \toprule
% & \multicolumn{4}{c}{\textbf{\% Avg.}} & \multicolumn{2}{c}{\textbf{Buffer}} & \multicolumn{2}{c}{\textbf{Initialize}} & \multicolumn{2}{c}{\textbf{Memory}} & \multicolumn{2}{c}{\textbf{Parameter}} & \multicolumn{2}{c}{\textbf{Runtime}} \\
% \cmidrule(lr){2-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13} \cmidrule(lr){14-15}
% & \textbf{Det} & \textbf{Loc} & \textbf{$\Delta$Det} & \textbf{$\Delta$Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} \\
% \midrule
% \multicolumn{15}{c}{\textbf{Detection Refinement}} \\
% \midrule
% Claude4s         & 44.8 & 41.4 & \ddown{-1.1} & \dup{+10.9} & 57.5 & 54.3 & 40.6 & 43.2 & 61.7 & 52.5 & 26.1 & 29.4 & 14.6 & 10.5 \\
% Gemini2.5 Pro    & 14.1 & 11.4 & \ddown{-3.3}        & \ddown{-0.9}       & 10.0 & 8.6  & 20.0 & 16.2 & 40.0 & 32.1 & 0.4  & 0.3  & 0.0  & 0.0 \\
% GPT-5 & 52.4 & 44.5& \dup{8.1}  & \dup{2.8} & 57.5 & 55.0 & 55.6 & 43.1 & 60.8 & 41.3 & 43.5 & \textbf{48.2} & 21.2 & 20.5 \\
% GPT-4.1 & 48.3 & 40.8& \textcolor{ForestGreen}{+0.3}\dup & \textcolor{ForestGreen}{+2.3}\dup & 60.8 & 51.9 & 53.1 & 44.9 & 57.5 & 46.1 & 39.8 & 42.9 & 6.7 & 0.0 \\
% GPT-4o-mini & 34.6 & 29.1& \ddown{-9.7}& \dup{+6.5} & 45.8 & 43.6 & 30.6 & 20.2 & 45.8 & 33.6 & 22.4 & 21.2 & 0.0 & 0.0 \\
% Qwen3 Next 80B & 42.9 & 39.5& \dup{+5.5}  & \dup{+33.6} & 60.8 & 55.1 & 50.6 & 44.4 & 60.8 & 48.6 & 25.7 & 29.2 & 0.0 & 0.0 \\
% Qwen3 235B & 34.1 & 26.7& \dup{+8.2}  & \dup{+17.5} & 34.2 & 32.0 & 30.6 & 34.5 & 57.5 & 38.8 & 23.3 & 13.3 & 5.0 & 8.2 \\
% Gemini2.5 Flash   & 22.5 & 18.4& \dna        & \dna   & 34.2 & 0.6  & 40.8 & 25.7 & 7.9  & 27.6 & 0.4  & 33.4 & 24.6 & 0.6  \\
% Llama4           & 28.3 & 28.1& \dna        & \dna  & 30.8 & 25.6 & 35.8 & 26.1 & 0.0  & 27.9 & 24.1 & 32.0 & 30.2 & 0.0  \\

% Deepseek 3.1 & 53.9 &	53.4 & \dna        & \dna & 60.8 &	55.6 &	62.5 &	47.5 &	16.3 &	58.1 &	55.0 &	60.2 &	47.6 &	19.4 \\

% \midrule
% \multicolumn{13}{c}{\textbf{Divergence Tracing}} \\
% \midrule
% GPT-4o-mini & 47.2 & 43.3& \dup{+2.9}  & \dup{+20.7} & \textbf{64.2} & 55.6 & 55.6 & 48.8 & 52.5 & 47.0 & 33.9 & 32.4 & 1.2 & 0.5 \\
% GPT-4.1 & 52.0 & 49.9& \dup{+4.0}  & \dup{+11.4} & 60.8 & 53.5 & \textbf{60.6} & \textbf{57.3} & 57.5 & 53.0 & 43.2 & 37.1 & 21.2 & 20.1 \\
% GPT-5 & \textbf{58.0} & \textbf{52.0}& \dup{+13.7} & \dup{+10.3} & 60.8 & \textbf{56.8} & \textbf{60.6} & 53.4 & \textbf{62.5} & 47.6 & \textbf{53.2} & 46.7 & \textbf{26.2} & \textbf{28.7} \\
% Claude4s          & 48.7 & 49.8& \dup{+2.8}  & \dup{+19.3} & 60.8 & 55.6 & 62.5 & 39.8 & 11.3 & 57.5 & 53.7 & 57.6 & 46.3 & 10.6 \\
% Qwen3 235B        & 42.7 & 42.1& \dup{+16.8} & \dup{+32.9} & 50.8 & 50.6 & 47.5 & 40.2 & 1.0  & 45.4 & 46.9 & 53.5 & 33.7 & 11.2 \\
% Qwen3 Next 80B              & 51.2 & 54.8& \dup{+13.8} & \dup{+48.9} & 64.2 & 58.1 & 62.5 & 43.2 & 11.3 & 63.2 & 58.6 & 57.7 & 48.8 & 21.2 \\

% \midrule
% \multicolumn{13}{c}{\textbf{Agentic Trace Analyzer}} \\
% \midrule

% GPT-5 & 0.0 & 0.0& \ddown{-44.3} & \ddown{-41.7} & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
% Claude4s          & 0.0 & 0.0&\ddown{-45.9} & \ddown{-30.5}  & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0\\
% \bottomrule
% \end{tabular}
% \end{table}

% \textcolor{blue}{Haoran: Minghao please take a look to if this is ok to present topn in this way}
\textbf{Divergence Tracing.} Tab. ~\ref{tab:Combined_Performance_Comparison} shows divergence tracing delivers the strongest overall gains over baseline. All models improve on both metrics: GPT-5 is up 13.7\% in detection and 10.3 in localization; GPT-4.1 gains 4.0\% and 11.4\% respectively. Qwen3 Next see the largest jumps with adding 13.8\% in detection and 48.9\% in localization, while Qwen3 235B adds 16.8\% and 32.9\%. These consistent lifts across architectures highlight divergence tracing as a core algorithmic upgrade for vulnerability localization.

% \textbf{Divergence Tracing.} This feature yields the strongest overall results. GPT-5 reaches 58.0\% detection and 52.0\% localization which is the best across all configurations. GPT-4o~mini also rises to 47.2\% detection and 43.3\% localization, approaching stronger baselines. By crash type, \emph{Initialize} benefits most; for example, GPT-4.1 attains 60.6\% detection and 57.3\% localization. These results indicate that divergence tracing recovers correct localizations that do not initially rank highest, especially for cases that require cross-module reasoning.

% Table~\ref{tab:Combined_Performance_Comparison} presents results for T2L-Agent's divergence tracing feature, which explores multiple paths in parallel from the same initial crash log. This feature generates a ranked set of localization candidates through the vulnerability search space. Helps the agent to capture correct localizations that might not rank highest. 

% This configuration demonstrates substantial improvements over previous approaches across all evaluated models. GPT-5 (low-think) achieves the strongest overall performance with 58.0\% detection and 52.0\% localization rate, representing the highest results observed across all experiment configurations. GPT-4.1 similarly improves, reaching 52.0\% detection and 49.9\% localization rate, while GPT-4o mini exhibits notable improvements achieving 47.2\% detection and 43.3\% localization rate, competitive with the strongest baseline models, and delivers the best or near-best performance in several crash type families. 

% The effectiveness of divergence tracing varies significantly across crash type families. For Initialize, models show large improvements, GPT-5 achieves 60.6\% detection and 53.4\% localization rate with divergence tracing, while GPT-4.1 reaches the best performance of 60.6\% detection and 57.3\% localization rates. Parameter cases demonstrate substantial benefits, with GPT-5 achieving 53.2\% detection and 46.7\% localization, and GPT-4.1 reaching 43.2\% detection and 37.1\% localization. These improvements suggest that divergence tracing is particularly valuable for crash types requiring complex reasoning and cross-module interactions, where the correct localization may not always rank highest but becomes accessible through divergence tracing.

\subsection{Discussion 2: Parameter Tuning}
\vspace{-1mm}

\textbf{Thinking Budget.} As shown in Tab. ~\ref{tab:parameter_tuning}, more thinking didn’t help. On GPT-5, the Medium budget outperforms High with 50.9\% detection vs 41.3\%, and 41.6\% localization vs 36.1\%. The Low setting trails Medium by a few points yet often matches or even exceeds High on key metrics, while sharply reducing compute and latency. This pattern suggests diminishing returns—and decision drag—at very high budgets: the model over-explores, delays commitment, and accumulates tool-use errors. In practice, Medium strikes the best accuracy–cost balance for vulnerability localization; Low is a strong option when throughput and responsiveness matter most.

% \begin{table*}[t]
% \centering
% \small
% \caption{Localization and Detection Rate Performance Across Different Models.}
% \label{tab:Thinking_Performance_Comparison}
% \setlength{\tabcolsep}{4pt}
% \begin{tabular}{l r r r r r r r r r r r r}
% \toprule
% & \multicolumn{2}{c}{\textbf{\% Avg.}} & \multicolumn{2}{c}{\textbf{Buffer}} & \multicolumn{2}{c}{\textbf{Initialize}} & \multicolumn{2}{c}{\textbf{Memory}} & \multicolumn{2}{c}{\textbf{Parameter}} & \multicolumn{2}{c}{\textbf{Runtime}} \\
% \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
% & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} \\
% \midrule

% GPT-5 high-think\_think & 41.3 & 36.1 & 54.2 & 47.8 & 40.6 & 32.4 & 55.8 & 39.3 & 32.8 & 36.7 & 10.0 & 10.0 \\

% \bottomrule
% \end{tabular}
% \end{table*}

% \begin{table*}[t]
% \centering
% \small
% \caption{Localization and Detection Rate Performance for Temperature Tuning Across Different Models.}
% \label{tab:Temperature_Performance_Comparison}
% \setlength{\tabcolsep}{4pt}
% \begin{tabular}{l r r r r r r r r r r r r}
% \toprule
% & \multicolumn{2}{c}{\textbf{\% solved}} & \multicolumn{2}{c}{\textbf{Buffer}} & \multicolumn{2}{c}{\textbf{Initialize}} & \multicolumn{2}{c}{\textbf{Memory}} & \multicolumn{2}{c}{\textbf{Parameter}} & \multicolumn{2}{c}{\textbf{Runtime}} \\
% \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
% & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} \\
% \midrule

% GPT-4.1 temp02\_temp & 51.0 & 43.6 & 57.5 & 44.4 & 55.6 & 51.1 & 55.8 & 45.7 & 43.2 & 35.4 & 21.2 & 20.2 \\
% GPT-4.1 temp06\_temp & 50.8 & 43.5 & 60.8 & 52.0 & 53.1 & 47.8 & 55.8 & 44.8 & 39.4 & 42.7 & 17.9 & 10.1 \\
% Claude4stemp02\_temp & 46.5 & 43.0 & 54.2 & 56.1 & 55.6 & 43.0 & 61.7 & 49.0 & 36.5 & 39.7 & 11.2 & 10.9 \\
% Claude4stemp06\_temp & 47.3 & 44.9 & 54.2 & 56.1 & 50.6 & 48.0 & 60.8 & \textbf{53.6} & 36.5 & 39.6 & 11.2 & 10.9 \\

% \bottomrule
% \end{tabular}
% \end{table*}

\begin{table}[htbp]
\centering
\tiny
\caption{Localization and Detection Rate Performance for Temperature Tuning and Chain of Thought Across Different Models.}
\label{tab:parameter_tuning}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{l c r r r r r r r r r r r r}
\toprule
& & \multicolumn{2}{c}{\textbf{\% Avg.}} & \multicolumn{2}{c}{\textbf{Buffer}} & \multicolumn{2}{c}{\textbf{Initialize}} & \multicolumn{2}{c}{\textbf{Memory}} & \multicolumn{2}{c}{\textbf{Parameter}} & \multicolumn{2}{c}{\textbf{Runtime}} \\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12} \cmidrule(lr){13-14}
& \textbf{Config} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} \\
\midrule
\multicolumn{14}{c}{\textbf{Temperature}} \\
\midrule
GPT-4.1 & 0.2 & \textbf{51.0} & 43.6 & 57.5 & 44.4 & 55.6 & \textbf{51.1} & 55.8 & 45.7 & 43.2 & 35.4 & 21.2 & \textbf{20.2}g \\
GPT-4.1 & 0.6 & 50.8 & 43.5 & \textbf{60.8} & 52.0 & 53.1 & 47.8 & 55.8 & 44.8 & 39.4 & \textbf{42.7} & 17.9 & 10.1 \\
Claude 4 Sonnet & 0.2 & 46.5 & 43.0 & 54.2 & \textbf{56.1} & 55.6 & 43.0 & \textbf{61.7} & 49.0 & 36.5 & 39.7 & 11.2 & 10.9 \\
Claude 4 Sonnet & 0.6 & 47.3 & \textbf{44.9} & 54.2 & \textbf{56.1} & 50.6 & 48.0 & 60.8 & \textbf{53.6} & 36.5 & 39.6 & 11.2 & 10.9 \\
\midrule
\multicolumn{14}{c}{\textbf{Reasoning Effect}} \\
\midrule
GPT-5 & High & 41.3 & 36.1 & 54.2 & 47.8 & 40.6 & 32.4 & 55.8 & 39.3 & 32.8 & 36.7 & 10.0 & 10.0 \\
GPT-5 & Medium   & 50.9 & 41.6 & \textbf{60.8} & 55.6 & \textbf{60.8} & 42.8 & 11.3 & 46.1 & \textbf{45.8} & 40.4 & \textbf{47.4} & 10.5 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Temperature.} Temperature changes barely matter from Tab. ~\ref{tab:parameter_tuning}. On GPT-4.1, detection is 51.0\% at 0.2 and 50.8\% at 0.6, with localization 43.6\% vs.\ 43.5\%. Claude 4 Sonnet shows the same pattern: 46.5\% vs.\ 47.3\% detection and 43.0\% vs.\ 44.9\% localization. Performance is stable in 0.2-0.6 range. The exception is \emph{Initialize} bugs, which are more temperature sensitive than \emph{Buffer} and \emph{Memory} cases that lean on concrete runtime evidence. Overall, precise localization benefits more from structured, tool-grounded reasoning than from extra sampling, making parameter choices simple.

% Table~\ref{tab:parameter_tuning} presents results for temperature tuning experiments with settings of 0.2 and 0.6. Overall, temperature adjustments have minimal impact on detection and localization performance, with most configurations showing stable results across the tested range. For GPT-4.1, both temperature configurations achieve similar performances, indicating that performance is quite stable around this temperature range. Claude4s demonstrates similar patterns with different temperature configurations both showing solid performance across the evaluation metrics.

% \begin{table*}[t]
% \centering
% \small
% \caption{Loc. and Det. Rate Performance for Temperature Tuning Across Different Models.}
% \label{tab:Temperature_Performance_Comparison}
% \setlength{\tabcolsep}{4pt}
% \begin{tabular}{l c r r r r r r r r r r r r}
% \toprule
% & & \multicolumn{2}{c}{\textbf{\% Avg.}} & \multicolumn{2}{c}{\textbf{Buffer}} & \multicolumn{2}{c}{\textbf{Initialize}} & \multicolumn{2}{c}{\textbf{Memory}} & \multicolumn{2}{c}{\textbf{Parameter}} & \multicolumn{2}{c}{\textbf{Runtime}} \\
% \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12} \cmidrule(lr){13-14}
% & \textbf{Temp} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} & \textbf{Det} & \textbf{Loc} \\
% \midrule
% GPT-4.1 & 0.2 & 51.0 & 43.6 & 57.5 & 44.4 & 55.6 & 51.1 & 55.8 & 45.7 & 43.2 & 35.4 & 21.2 & 20.2 \\
% GPT-4.1 & 0.6 & 50.8 & 43.5 & 60.8 & 52.0 & 53.1 & 47.8 & 55.8 & 44.8 & 39.4 & 42.7 & 17.9 & 10.1 \\
% Claude4s & 0.2 & 46.5 & 43.0 & 54.2 & 56.1 & 55.6 & 43.0 & 61.7 & 49.0 & 36.5 & 39.7 & 11.2 & 10.9 \\
% Claude4s & 0.6 & 47.3 & 44.9 & 54.2 & 56.1 & 50.6 & 48.0 & 60.8 & \textbf{53.6} & 36.5 & 39.6 & 11.2 & 10.9 \\
% \bottomrule
% \end{tabular}
% \end{table*}

% The temperature tuning impact differently among crash type families, with Initialize particularly sensitives to temperature settings. Claude4s with temperature 0.6 achieves 50.6\% detection and 48.0\% localization for initialization errors, while GPT-4.1 temperature 0.2 reaches 55.6\% detection and 51.1\% localization, indicating that temperature adjustments may influence the model's ability to explore diverse reasoning paths for complex Initialize vulnerabilities. Buffer and Memory cases demonstrate relatively stable performance across temperature settings, since they rely more on concrete runtime evidence and less sensitive to generation diversity. Parameter cases show mixed responses to temperature tuning, with different configurations yielding different performance levels, suggesting that the optimal temperature may depend on specific crash type and model architecture.

\subsection{Discussion 3: Case Study}
\vspace{-2mm}
% Figure ~\ref{fig:arvo-3pages} illustrates the complete T2L-Agent workflow through a real vulnerability localization scenario from our T2L-ARVO  dataset, demonstrating the iterative planner-executor architecture in practice. The workflow begins with the Planner orchestrating localization tasks, calling chunk\_case to divide the codebase into 5441 manageable chunks and diff\_index to establish ground truth. The Executor then implements multi-round refinement through runtime and static evidence collection, executing run\_san() to generate runtime traces and crash logs, followed by llm\_analyze() to generate ranked localization candidates. Through iterative calls to compare\_llm\_metrics, the agent evaluates and refines candidate locations against both static patterns and dynamic execution traces, ultimately achieving perfect localization performance with detection rate: 1.0 and localization rate: 1.0, demonstrating T2L-Agent's ability to systematically transform crash symptoms into precise vulnerability diagnostics.
Figure~\ref{fig:arvo-3pages} illustrates a full \texttt{T2L-Agent} workflow on a real case from \texttt{T2L-ARVO}, showcasing the iterative planner–executor architecture in action. The process starts with the Planner orchestrating code chunking (\texttt{chunk\_case}, 5441 chunks) and diff indexing (\texttt{diff\_index}), then running sanitized execution (\texttt{run\_san}) to collect crash logs before delegating reasoning to the Executor.

\vspace{-4mm}
\begin{figure}[htbp]
  \centering
  \begin{subfigure}[t]{0.32\linewidth}
    \centering
    \includegraphics[width=\linewidth]{diagrams/16614_figure_p01_anno.png}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.32\linewidth}
    \centering
    \includegraphics[width=\linewidth]{diagrams/16614_figure_p02_anno.png}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.32\linewidth}
    \centering
    \includegraphics[width=\linewidth]{diagrams/16614_figure_p03_anno.png}
  \end{subfigure}
  \caption{\texttt{T2L-Agent} pipeline visualization.}
  \label{fig:arvo-3pages}
  \vspace{-2mm}
\end{figure}

The Executor analyzes traces via \texttt{llm\_analyze}, extracts ranked candidates (\texttt{extract\_json}), and iteratively evaluates them against ground truth using \texttt{compare\_llm\_metrics}, combining static patterns and dynamic signals. This multi-round refinement achieves perfect localization (detection: 1.0, localization: 1.0), demonstrating \texttt{T2L-Agent}'s ability to convert crash symptoms into precise diagnostics. Each panel visualizes thought (function call) and observation (result) with hand-drawn borders and no hallucinated text. Together, they highlight data flow, role separation, and metric-driven validation across planner and executor components.

% The Planner orchestrates lightweight tooling on the repo: it builds case chunks chunk\_case, indexes the patch diff\_index, 
% marks changed lines (mark\_diff), and runs sanitized instrumentation to collect a crash log (run\_san). The Planner then delegates reasoning to the Executor. The Executor consumes the cached crash log via llm\_analyze, converts candidates with extract\_json, and reports quality with compare\_llm\_metrics against the diff-indexed ground truth. Each card shows the thought (call) and observation (result) with hand-drawn borders; no invented text is added. Together these panels illustrate data flow, responsibility split, and metric-driven verification.

