@misc{cve_details_2024,
  author = {{CVE Details}},
  title = {Number of common IT security vulnerabilities and exposures (CVEs) worldwide from 2009 to 2024 YTD},
  howpublished = {Statista},
  year = {2024},
  month = {August},
  url = {https://www.statista.com/statistics/500755/worldwide-common-vulnerabilities-and-exposures/},
  note = {Accessed: September 23, 2025}
}

@misc{indusface_2024,
  author = {{Indusface}},
  title = {181 Cybersecurity Statistics for 2025},
  howpublished = {Indusface Blog},
  year = {2024},
  url = {https://www.indusface.com/blog/key-cybersecurity-statistics/},
  note = {Accessed: September 23, 2025}
}

@misc{darkreading_supply_chain,
  author = {{Dark Reading}},
  title = {Juniper Research Study Reveals Staggering Cost of Vulnerable Software Supply Chains},
  howpublished = {Dark Reading},
  year = {2024},
  url = {https://www.darkreading.com/cybersecurity-operations/juniper-research-study-reveals-staggering-cost-of-vulnerable-software-supply-chains-},
  note = {Accessed: September 23, 2025}
}

@misc{liu_makridis_galinkin_2024,
  author = {Liu, Jingchen and Makridis, Christos A. and Galinkin, Evan},
  title = {Cybersecurity vulnerabilities and their financial impact},
  howpublished = {VoxEU},
  year = {2024},
  publisher = {Centre for Economic Policy Research},
  url = {https://cepr.org/voxeu/columns/cybersecurity-vulnerabilities-and-their-financial-impact},
  note = {Accessed: September 23, 2025}
}

@article{zhang_et_al_2025,
  author = {Zhang, Jie and Bu, Haoyu and Wen, Hui and Liu, Yongji and Fei, Haiqiang and Xi, Rongrong and Li, Lun and Yang, Yun and Zhu, Hongsong and Meng, Dan},
  title = {When LLMs meet cybersecurity: a systematic literature review},
  journal = {Cybersecurity},
  volume = {8},
  number = {1},
  pages = {1--41},
  year = {2025},
  publisher = {SpringerOpen},
  doi = {10.1186/s42400-025-00361-w},
  url = {https://cybersecurity.springeropen.com/articles/10.1186/s42400-025-00361-w}
}

@misc{awesome_llm4cybersecurity_2024,
  author = {{tmylla and contributors}},
  title = {Awesome-LLM4Cybersecurity: An overview of LLMs for cybersecurity},
  howpublished = {GitHub Repository},
  year = {2024},
  url = {https://github.com/tmylla/Awesome-LLM4Cybersecurity},
  note = {Accessed: September 23, 2025}
}

@article{zhang2024empirical,
  title={An empirical study of automated vulnerability localization with large language models},
  author={Zhang, Jian and Wang, Chong and Li, Anran and Sun, Weisong and Zhang, Cen and Ma, Wei and Liu, Yang},
  journal={arXiv preprint arXiv:2404.00287},
  year={2024}
}

@article{divakaran2024llms,
  title={LLMs for cyber security: New opportunities},
  author={Divakaran, Dinil Mon and Peddinti, Sai Teja},
  journal={arXiv preprint arXiv:2404.11338},
  year={2024}
}

@misc{github_copilot_2021,
  title = {{GitHub Copilot: Your AI pair programmer}},
  author = {{GitHub, Inc.}},
  year = {2021},
  url = {https://github.com/features/copilot},
  note = {Accessed: September 24 2025}
}

@article{nunez2024autosafecoder,
  title={Autosafecoder: A multi-agent framework for securing llm code generation through static analysis and fuzz testing},
  author={Nunez, Ana and Islam, Nafis Tanveer and Jha, Sumit Kumar and Najafirad, Peyman},
  journal={arXiv preprint arXiv:2409.10737},
  year={2024}
}

@misc{acm_3715758,
  title = {LLM-based Vulnerability Detection in Software Systems},
  howpublished = {ACM Digital Library},
  year = {2024},
  url = {https://dl.acm.org/doi/10.1145/3715758},
  note = {DOI: 10.1145/3715758}
}

@misc{springer_vulnerability_analysis,
  title = {Vulnerability Analysis and Detection using Machine Learning Approaches},
  author={Yuejun Guo and Seifeddine Bettaieb and Fran Casino},
  journal = {International Journal of Information Security},
  year = {2024},
  url = {https://link.springer.com/article/10.1007/s10207-024-00888-y},
  note = {DOI: 10.1007/s10207-024-00888-y}
}

@inproceedings{deng2024pentestgpt,
  title={$\{$PentestGPT$\}$: Evaluating and harnessing large language models for automated penetration testing},
  author={Deng, Gelei and Liu, Yi and Mayoral-Vilches, V{\'\i}ctor and Liu, Peng and Li, Yuekang and Xu, Yuan and Zhang, Tianwei and Liu, Yang and Pinzger, Martin and Rass, Stefan},
  booktitle={33rd USENIX Security Symposium (USENIX Security 24)},
  pages={847--864},
  year={2024}
}

@inproceedings{shen2025pentestagent,
  title={Pentestagent: Incorporating llm agents to automated penetration testing},
  author={Shen, Xiangmin and Wang, Lingzhi and Li, Zhenyuan and Chen, Yan and Zhao, Wencheng and Sun, Dawei and Wang, Jiashui and Ruan, Wei},
  booktitle={Proceedings of the 20th ACM Asia Conference on Computer and Communications Security},
  pages={375--391},
  year={2025}
}

@article{bran2023chemcrow,
  title={Chemcrow: Augmenting large-language models with chemistry tools},
  author={Bran, Andres M and Cox, Sam and Schilter, Oliver and Baldassari, Carlo and White, Andrew D and Schwaller, Philippe},
  journal={arXiv preprint arXiv:2304.05376},
  year={2023}
}

@article{jin2024prollm,
  title={ProLLM: protein chain-of-thoughts enhanced LLM for protein-protein interaction prediction},
  author={Jin, Mingyu and Xue, Haochen and Wang, Zhenting and Kang, Boming and Ye, Ruosong and Zhou, Kaixiong and Du, Mengnan and Zhang, Yongfeng},
  journal={arXiv preprint arXiv:2405.06649},
  year={2024}
}

@article{basit2025pennylang,
  title={Pennylang: Pioneering llm-based quantum code generation with a novel pennylane-centric dataset},
  author={Basit, Abdul and Innan, Nouhaila and Asif, Muhammad Haider and Shao, Minghao and Kashif, Muhammad and Marchisio, Alberto and Shafique, Muhammad},
  journal={arXiv preprint arXiv:2503.02497},
  year={2025}
}

@article{shao2024survey,
  title={Survey of different large language model architectures: Trends, benchmarks, and challenges},
  author={Shao, Minghao and Basit, Abdul and Karri, Ramesh and Shafique, Muhammad},
  journal={IEEE Access},
  year={2024},
  publisher={IEEE}
}

@article{pham2025chemgraph,
  title={ChemGraph: An Agentic Framework for Computational Chemistry Workflows},
  author={Pham, Thang D and Tanikanti, Aditya and Ke{\c{c}}eli, Murat},
  journal={arXiv preprint arXiv:2506.06363},
  year={2025}
}

@article{campbell2025mdcrow,
  title={Mdcrow: Automating molecular dynamics workflows with large language models},
  author={Campbell, Quintina and Cox, Sam and Medina, Jorge and Watterson, Brittany and White, Andrew D},
  journal={arXiv preprint arXiv:2502.09565},
  year={2025}
}

@article{ghafarollahi2024protagents,
  title={ProtAgents: protein discovery via large language model multi-agent collaborations combining physics and machine learning},
  author={Ghafarollahi, Alireza and Buehler, Markus J},
  journal={Digital Discovery},
  volume={3},
  number={7},
  pages={1389--1409},
  year={2024},
  publisher={Royal Society of Chemistry}
}

@article{ni2025agentic,
  title={Agentic End-to-End De Novo Protein Design for Tailored Dynamics Using a Language Diffusion Model},
  author={Ni, Bo and Buehler, Markus J},
  journal={arXiv preprint arXiv:2502.10173},
  year={2025}
}

@inproceedings{10.5555/800078.802557,
author = {Weiser, Mark},
title = {Program slicing},
year = {1981},
isbn = {0897911466},
publisher = {IEEE Press},
abstract = {Program slicing is a method used by experienced computer programmers for abstracting from programs. Starting from a subset of a program's behavior, slicing reduces that program to a minimal form which still produces that behavior. The reduced program, called a “slice”, is an independent program guaranteed to faithfully represent the original program within the domain of the specified subset of behavior.Finding a slice is in general unsolvable. A dataflow algorithm is presented for approximating slices when the behavior subset is specified as the values of a set of variables at a statement. Experimental evidence is presented that these slices are used by programmers during debugging. Experience with two automatic slicing tools is summarized. New measures of program complexity are suggested based on the organization of a program's slices.},
booktitle = {Proceedings of the 5th International Conference on Software Engineering},
pages = {439–449},
numpages = {11},
keywords = {Data flow analysis, Debugging, Human factors, Program maintenance, Program metrics, Software tools},
location = {San Diego, California, USA},
series = {ICSE '81}
}

@article{agrawal1990dynamic,
  title={Dynamic program slicing},
  author={Agrawal, Hiralal and Horgan, Joseph R},
  journal={ACM SIGPlan Notices},
  volume={25},
  number={6},
  pages={246--256},
  year={1990},
  publisher={ACM New York, NY, USA}
}

@inproceedings{zhou2012should,
  title={Where should the bugs be fixed? more accurate information retrieval-based bug localization based on bug reports},
  author={Zhou, Jian and Zhang, Hongyu and Lo, David},
  booktitle={2012 34th International conference on software engineering (ICSE)},
  pages={14--24},
  year={2012},
  organization={IEEE}
}

@INPROCEEDINGS{6693093,
  author={Saha, Ripon K. and Lease, Matthew and Khurshid, Sarfraz and Perry, Dewayne E.},
  booktitle={2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={Improving bug localization using structured information retrieval}, 
  year={2013},
  volume={},
  number={},
  pages={345-355},
  keywords={Computer bugs;Measurement;Accuracy;Information retrieval;Indexing;Java;Mathematical model;Bug localization;information retrieval;search},
  doi={10.1109/ASE.2013.6693093}}

@inproceedings{10.1145/2597008.2597148,
author = {Wang, Shaowei and Lo, David},
title = {Version history, similar report, and structure: putting them together for improved bug localization},
year = {2014},
isbn = {9781450328791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597008.2597148},
doi = {10.1145/2597008.2597148},
abstract = {During the evolution of a software system, a large number of bug reports are submitted. Locating the source code files that need to be fixed to resolve the bugs is a challenging problem. Thus, there is a need for a technique that can automatically figure out these buggy files. A number of bug localization solutions that take in a bug report and output a ranked list of files sorted based on their likelihood to be buggy have been proposed in the literature. However, the accuracy of these tools still need to be improved.  In this paper, to address this need, we propose AmaLgam, a new method for locating relevant buggy files that puts together version history, similar reports, and structure. To do this, AmaLgam integrates a bug prediction technique used in Google which analyzes version history, with a bug localization technique named BugLocator which analyzes similar reports from bug report system, and the state-of-the-art bug localization technique BLUiR which considers structure. We perform a large-scale experiment on four open source projects, namely AspectJ, Eclipse, SWT and ZXing to localize more than 3,000 bugs. Compared with a history-aware bug localization solution of Sisman and Kak, our approach achieves a 46.1\% improvement in terms of mean average precision (MAP). Compared with BugLocator, our approach achieves a 24.4\% improvement in terms of MAP. Compared with BLUiR, our approach achieves a 16.4\% improvement in terms of MAP.},
booktitle = {Proceedings of the 22nd International Conference on Program Comprehension},
pages = {53–63},
numpages = {11},
keywords = {Version History, Structure, Similar Report, Bug Localization},
location = {Hyderabad, India},
series = {ICPC 2014}
}

@inproceedings{yamaguchi2014modeling,
  title={Modeling and discovering vulnerabilities with code property graphs},
  author={Yamaguchi, Fabian and Golde, Nico and Arp, Daniel and Rieck, Konrad},
  booktitle={2014 IEEE symposium on security and privacy},
  pages={590--604},
  year={2014},
  organization={IEEE}
}

@inproceedings{yang2024large,
  title={Large language models for test-free fault localization},
  author={Yang, Aidan ZH and Le Goues, Claire and Martins, Ruben and Hellendoorn, Vincent},
  booktitle={Proceedings of the 46th IEEE/ACM International Conference on Software Engineering},
  pages={1--12},
  year={2024}
}

@article{stein2025s,
  title={Where's the Bug? Attention Probing for Scalable Fault Localization},
  author={Stein, Adam and Wayne, Arthur and Naik, Aaditya and Naik, Mayur and Wong, Eric},
  journal={arXiv preprint arXiv:2502.13966},
  year={2025}
}

@article{zhou2019devign,
  title={Devign: Effective vulnerability identification by learning comprehensive program semantics via graph neural networks},
  author={Zhou, Yaqin and Liu, Shangqing and Siow, Jingkai and Du, Xiaoning and Liu, Yang},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{croft2023data,
  title={Data quality for software vulnerability datasets},
  author={Croft, Roland and Babar, M Ali and Kholoosi, M Mehdi},
  booktitle={2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)},
  pages={121--133},
  year={2023},
  organization={IEEE}
}

@article{asad2025leveraging,
  title={Leveraging Large Language Model for Information Retrieval-based Bug Localization},
  author={Asad, Moumita and Yasir, Rafed Muhammad and Geramirad, Armin and Malek, Sam},
  journal={arXiv preprint arXiv:2508.00253},
  year={2025}
}

@article{qin2024agentfl,
  title={Agentfl: Scaling llm-based fault localization to project-level context},
  author={Qin, Yihao and Wang, Shangwen and Lou, Yiling and Dong, Jinhao and Wang, Kaixin and Li, Xiaoling and Mao, Xiaoguang},
  journal={arXiv preprint arXiv:2403.16362},
  year={2024}
}

@article{jiang2025cosil,
  title={CoSIL: Software Issue Localization via LLM-Driven Code Repository Graph Searching},
  author={Jiang, Zhonghao and Ren, Xiaoxue and Yan, Meng and Jiang, Wei and Li, Yong and Liu, Zhongxin},
  journal={arXiv preprint arXiv:2503.22424},
  year={2025}
}

@article{kang2024quantitative,
  title={A quantitative and qualitative evaluation of LLM-based explainable fault localization},
  author={Kang, Sungmin and An, Gabin and Yoo, Shin},
  journal={Proceedings of the ACM on Software Engineering},
  volume={1},
  number={FSE},
  pages={1424--1446},
  year={2024},
  publisher={ACM New York, NY, USA}
}

@INPROCEEDINGS{9796256,
  author={Fu, Michael and Tantithamthavorn, Chakkrit},
  booktitle={2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)}, 
  title={LineVul: A Transformer-based Line-Level Vulnerability Prediction}, 
  year={2022},
  volume={},
  number={},
  pages={608-620},
  keywords={Neural networks;System recovery;Transformers;Software systems;Security;Data mining;Vulnerability Prediction;AI for Software Engineering;Software Security},
  doi={10.1145/3524842.3528452}}

@article{li2024attention,
  title={Attention is all you need for llm-based code vulnerability localization},
  author={Li, Yue and Li, Xiao and Wu, Hao and Zhang, Yue and Cheng, Xiuzhen and Zhong, Sheng and Xu, Fengyuan},
  journal={arXiv e-prints},
  pages={arXiv--2410},
  year={2024}
}

@inproceedings{10.1145/3671016.3674807,
author = {Weng, Cheng and Qin, Yihao and Lin, Bo and Liu, Pei and Chen, Liqian},
title = {MatsVD: Boosting Statement-Level Vulnerability Detection via Dependency-Based Attention},
year = {2024},
isbn = {9798400707056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3671016.3674807},
doi = {10.1145/3671016.3674807},
abstract = {Software vulnerabilities inevitably arise during software development and may leave behind huge security risks. In order to detect and mitigate vulnerabilities before they can be exploited, various fine-grained deep learning (DL)-based vulnerablity detection (VD) approaches have been proposed to locate vulnerable statements, among which the Transformer-based methods have shown promising performances. However, existing Transformer-based statement-level approaches still suffer from a crucial limitation: they ignore the intrinsic data/control dependency relations between the statements. In this work, we propose a novel Transformer-based model MatsVD, which aims to address the above challenge from two aspects: Firstly, inspired by the hierarchical structure of code (i.e., tokens, statements, and functions), MatsVD comprises three different Transformer-based layers (i.e., statement embedding layer, statement representation layer, and function representation layer) to gradually aggregate the basic code tokens into meaningful statement/function representations; Secondly, to further exploit the data/control dependencies between statements, we replace the original attention mechanism of the Transformer with a novel dependency-based attention by masking irrelevant attention scores according to the program dependency graph. We comprehensively evaluate MatsVD on the widely used C/C++ vulnerability dataset Big-Vul. The results show that MatsVD significantly outperforms 6 other statement-level methods on both binary classification and ranking metrics. In particular, MatsVD obtains an F1 score of 86\% and a Top-1 Accuracy of 93\% on statement-le, which improves by respectively 22.97\% and 7.76\% compared to the state-of-the-art method VELVET.},
booktitle = {Proceedings of the 15th Asia-Pacific Symposium on Internetware},
pages = {115–124},
numpages = {10},
keywords = {Deep Learning, Software Vulnerability Detection, Transformer},
location = {Macau, China},
series = {Internetware '24}
}

@article{10.1145/3660804,
author = {Yang, Haoran and Nong, Yu and Zhang, Tao and Luo, Xiapu and Cai, Haipeng},
title = {Learning to Detect and Localize Multilingual Bugs},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660804},
doi = {10.1145/3660804},
abstract = {Increasing studies have shown bugs in multi-language software as a critical loophole in modern software quality assurance, especially those induced by language interactions (i.e., multilingual bugs). Yet existing tool support for bug detection/localization remains largely limited to single-language software, despite the long-standing prevalence of multi-language systems in various real-world software domains. Extant static/dynamic analysis and deep learning (DL) based approaches all face major challenges in addressing multilingual bugs. In this paper, we present xLoc, a DL-based technique/tool for detecting and localizing multilingual bugs. Motivated by results of our bug-characteristics study on top locations of multilingual bugs, xLoc first learns the general knowledge relevant to differentiating various multilingual control-flow structures. This is achieved by pre-training a Transformer model with customized position encoding against novel objectives. Then, xLoc learns task-specific knowledge for the task of multilingual bug detection/localization, through another new position encoding scheme (based on cross-language API vicinity) that allows for the model to attend particularly to control-flow constructs that bear most multilingual bugs during fine-tuning. We have implemented xLoc for Python-C software and curated a dataset of 3,770 buggy and 15,884 non-buggy Python-C samples, which enabled our extensive evaluation of xLoc against two state-of-the-art baselines: fine-tuned CodeT5 and zero-shot ChatGPT. Our results show that xLoc achieved 94.98\% F1 and 87.24\%@Top-1 accuracy, which are significantly (up to 162.88\% and 511.75\%) higher than the baselines. Ablation studies further confirmed significant contributions of each of the novel design elements in xLoc. With respective bug-location characteristics and labeled bug datasets for fine-tuning, our design may be applied to other language combinations beyond Python-C.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {97},
numpages = {24},
keywords = {Multi-language software, bug detection, fault localization, multilingual bugs}
}

@article{rafi2024multi,
  title={A Multi-Agent Approach to Fault Localization via Graph-Based Retrieval and Reflexion},
  author={Rafi, Md Nakhla and Kim, Dong Jae and Chen, Tse-Hsun and Wang, Shaowei},
  journal={arXiv preprint arXiv:2409.13642},
  year={2024}
}

@article{yeo2025improving,
  title={Improving LLM-Based Fault Localization with External Memory and Project Context},
  author={Yeo, Inseok and Ryu, Duksan and Baik, Jongmoon},
  journal={arXiv preprint arXiv:2506.03585},
  year={2025}
}

@article{shao2025craken,
  title={CRAKEN: Cybersecurity LLM Agent with Knowledge-Based Execution},
  author={Shao, Minghao and Xi, Haoran and Rani, Nanda and Udeshi, Meet and Putrevu, Venkata Sai Charan and Milner, Kimberly and Dolan-Gavitt, Brendan and Shukla, Sandeep Kumar and Krishnamurthy, Prashanth and Khorrami, Farshad and others},
  journal={arXiv preprint arXiv:2505.17107},
  year={2025}
}

@inproceedings{abramovichenigma,
  title={EnIGMA: Interactive Tools Substantially Assist LM Agents in Finding Security Vulnerabilities},
  year={2025},
  author={Abramovich, Talor and Udeshi, Meet and Shao, Minghao and Lieret, Kilian and Xi, Haoran and Milner, Kimberly and Jancheska, Sofija and Yang, John and Jimenez, Carlos E and Khorrami, Farshad and others},
  booktitle={Forty-second International Conference on Machine Learning}
}

@article{udeshi2025d,
  title={D-CIPHER: Dynamic Collaborative Intelligent Multi-Agent System with Planner and Heterogeneous Executors for Offensive Security},
  author={Udeshi, Meet and Shao, Minghao and Xi, Haoran and Rani, Nanda and Milner, Kimberly and Putrevu, Venkata Sai Charan and Dolan-Gavitt, Brendan and Shukla, Sandeep Kumar and Krishnamurthy, Prashanth and Khorrami, Farshad and others},
  journal={arXiv preprint arXiv:2502.10931},
  year={2025}
}

@article{shao2024nyu,
  title={Nyu ctf bench: A scalable open-source benchmark dataset for evaluating llms in offensive security},
  author={Shao, Minghao and Jancheska, Sofija and Udeshi, Meet and Dolan-Gavitt, Brendan and Milner, Kimberly and Chen, Boyuan and Yin, Max and Garg, Siddharth and Krishnamurthy, Prashanth and Khorrami, Farshad and others},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={57472--57498},
  year={2024}
}

@article{10.1145/3715758,
author = {Sovrano, Francesco and Bauer, Adam and Bacchelli, Alberto},
title = {Large Language Models for In-File Vulnerability Localization Can Be “Lost in the End”},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3715758},
doi = {10.1145/3715758},
abstract = {Traditionally, software vulnerability detection research has focused on individual small functions due to earlier language processing technologies’ limitations in handling larger inputs. However, this function-level approach may miss bugs that span multiple functions and code blocks. Recent advancements in artificial intelligence have enabled processing of larger inputs, leading everyday software developers to increasingly rely on chat-based large language models (LLMs) like GPT-3.5 and GPT-4 to detect vulnerabilities across entire files, not just within functions. This new development practice requires researchers to urgently investigate whether commonly used LLMs can effectively analyze large file-sized inputs, in order to provide timely insights for software developers and engineers about the pros and cons of this emerging technological trend. Hence, the goal of this paper is to evaluate the effectiveness of several state-of-the-art chat-based LLMs, including the GPT models, in detecting in-file vulnerabilities. We conducted a costly investigation into how the performance of LLMs varies based on vulnerability type, input size, and vulnerability location within the file. To give enough statistical power (β ≥ .8) to our study, we could only focus on the three most common (as well as dangerous) vulnerabilities: XSS, SQL injection, and path traversal. Our findings indicate that the effectiveness of LLMs in detecting these vulnerabilities is strongly influenced by both the location of the vulnerability and the overall size of the input. Specifically, regardless of the vulnerability type, LLMs tend to significantly (p < .05) underperform when detecting vulnerabilities located toward the end of larger files—a pattern we call the lost-in-the-end effect. Finally, to further support software developers and practitioners, we also explored the optimal input size for these LLMs and presented a simple strategy for identifying it, which can be applied to other models and vulnerability types. Eventually, we show how adjusting the input size can lead to significant improvements in LLM-based vulnerability detection, with an average recall increase of over 37\% across all models.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE041},
numpages = {23},
keywords = {Code Context, In-File Vulnerability Detection, Large Language Models, Path Traversal, SQL Injection, XSS, ‘Lost-in-the-End’ Issue}
}

@article{yang2023intercode,
  title={Intercode: Standardizing and benchmarking interactive coding with execution feedback},
  author={Yang, John and Prabhakar, Akshara and Narasimhan, Karthik and Yao, Shunyu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={23826--23854},
  year={2023}
}

@article{zhang2024cybench,
  title={Cybench: A framework for evaluating cybersecurity capabilities and risks of language models},
  author={Zhang, Andy K and Perry, Neil and Dulepet, Riya and Ji, Joey and Menders, Celeste and Lin, Justin W and Jones, Eliot and Hussein, Gashon and Liu, Samantha and Jasper, Donovan and others},
  journal={arXiv preprint arXiv:2408.08926},
  year={2024}
}