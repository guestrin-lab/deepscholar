@article{Peters2022,
   abstract = {Some artificial intelligence (AI) systems can display algorithmic bias, i.e. they may produce outputs that unfairly discriminate against people based on their social identity. Much research on this topic focuses on algorithmic bias that disadvantages people based on their gender or racial identity. The related ethical problems are significant and well known. Algorithmic bias against other aspects of people’s social identity, for instance, their political orientation, remains largely unexplored. This paper argues that algorithmic bias against people’s political orientation can arise in some of the same ways in which algorithmic gender and racial biases emerge. However, it differs importantly from them because there are (in a democratic society) strong social norms against gender and racial biases. This does not hold to the same extent for political biases. Political biases can thus more powerfully influence people, which increases the chances that these biases become embedded in algorithms and makes algorithmic political biases harder to detect and eradicate than gender and racial biases even though they all can produce similar harm. Since some algorithms can now also easily identify people’s political orientations against their will, these problems are exacerbated. Algorithmic political bias thus raises substantial and distinctive risks that the AI community should be aware of and examine.},
   author = {Uwe Peters},
   doi = {10.1007/s13347-022-00512-8},
   issn = {2210-5433},
   issue = {2},
   journal = {Philosophy & Technology},
   month = {6},
   pages = {25},
   title = {Algorithmic Political Bias in Artificial Intelligence Systems},
   volume = {35},
   year = {2022}
}
@inproceedings{Fisher2025,
   author = {Jillian Fisher and Shangbin Feng and Robert Aron and Thomas Richardson and Yejin Choi and Daniel W Fisher and Jennifer Pan and Yulia Tsvetkov and Katharina Reinecke},
   city = {Stroudsburg, PA, USA},
   doi = {10.18653/v1/2025.acl-long.328},
   booktitle = {Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
   pages = {6559-6607},
   publisher = {Association for Computational Linguistics},
   title = {Biased LLMs can Influence Political Decision-Making},
   year = {2025}
}
@article{Motoki2025,
   abstract = {A R T I C L E I N F O Dataset link: https://doi.org/10.7910/DVN/VZ RKWP JEL classification: C89 D83 L86 Z00 Keywords: Generative AI Societal values Large language models Multimodal AI governance A B S T R A C T Our analysis reveals a concerning misalignment of values between ChatGPT and the average American. We also show that ChatGPT displays political leanings when generating text and images, but the degree and direction of skew depend on the theme. Notably, ChatGPT repeatedly refused to generate content representing certain mainstream perspectives, citing concerns over misinformation and bias. As generative AI systems like ChatGPT become ubiquitous, such misalignment with societal norms poses risks of distorting public discourse. Without proper safeguards, these systems threaten to exacerbate societal divides and depart from principles that underpin free societies.},
   author = {Fabio Y S Motoki and Valdemar Pinho Neto and Victor Rangel},
   doi = {10.7910/DVN/VZ},
   isbn = {2025.106904},
   title = {Assessing political bias and value misalignment in generative artificial intelligence},
   url = {https://doi.org/10.7910/DVN/VZ},
   year = {2025}
}
@article{Messer2025,
   abstract = {Generative Artificial Intelligence (GAI) such as Large Language Models (LLMs) have a concerning tendency to generate politically biased content. This is a challenge, as the emergence of GAI meets politically polarized societies. Therefore, this research investigates how people react to biased GAI-content based on their pre-existing political beliefs and how this influences the acceptance of GAI. In three experiments (N = 513), it was found that perceived alignment between user's political orientation and bias in generated content (in text and images) increases acceptance and reliance on GAI. Participants who perceived alignment were more likely to grant GAI access to sensitive smartphone functions and to endorse the use in critical domains (e.g., loan approval; social media moderation). Because users see GAI as a social actor, they consider perceived alignment as a sign of greater objectivity, thus granting aligned GAI access to more sensitive areas.},
   author = {Uwe Messer},
   doi = {10.1016/j.chbah.2024.100108},
   issn = {29498821},
   journal = {Computers in Human Behavior: Artificial Humans},
   month = {3},
   pages = {100108},
   publisher = {Elsevier BV},
   title = {How do people react to political bias in generative artificial intelligence (AI)?},
   volume = {3},
   year = {2025}
}
@techReport{GoodmanFacultyAdvisor2024,
   abstract = {Although much research has explored the left-leaning bias of generative AI (Rozado2023, Suguri Motoki et al. 2023, Hartmann et al. 2022), less attention has been paid to its impact. Thus, I aim to investigate whether this political bias influences voter behavior, hypothesizing that the effect would vary based on users' political knowledge and confidence. High self-confidence and knowledge are expected to correlate with lower susceptibility to ChatGPT's influence, while lower levels suggest greater reliance on the AI model. I conduct two surveys, in which participants engage with political information from ChatGPT or SCOTUSblog before making judgments on court cases. The findings reveal that while political bias does not directly affect users' decisions, ChatGPT influences voting trends differently from SCOTUSblog. While the distribution of votes under SCOTUSblog is nearly identical; under ChatGPT, there's a noticeable deviation, particularly in the first court case where a predominantly left-leaning voter base exhibited a significant swing to the right. The anticipated relationship between reliance on information sources, self-confidence, and political knowledge, is confirmed, with individuals of high self-confidence and knowledge voting in line with their political party affiliation. Conversely, those with low self-confidence and knowledge are more susceptible to shifting their vote based on the model's recommendation.},
   author = {Neomi Goodman Faculty Advisor and Professor Susanne Lohmann},
   title = {How harmful is the political bias in ChatGPT?},
   year = {2024}
}
@article{Rotaru2024,
   abstract = {The rise of large language models (LLMs) such as ChatGPT and Gemini has raised concerns about their potential political biases and the implications for information dissemination and user influence. This study aims to measure the degree of political bias inherent in major LLMs by analyzing their responses to a standardized set of questions rating the quality and bias of popular news websites. Employing a systematic methodology, we queried both free and paid versions of ChatGPT and Gemini to rate news outlets on criteria such as authority, credibility, and objectivity. Results revealed that while all LLMs displayed a tendency to score left-leaning news sources higher, there was a notable difference between free and premium models in their assessment of subjectivity and bias. Furthermore, a comparison between the models indicated that premium versions offered more nuanced responses, suggesting a greater awareness of bias. The findings suggest that LLMs, despite their objective façade, are influenced by biases that can shape public opinion, underlining the necessity for efforts to mitigate these biases. This research highlights the importance of transparency and the potential impact of LLMs on the political landscape.},
   author = {George-Cristinel Rotaru and Sorin Anagnoste and Vasile-Marian Oancea},
   doi = {10.2478/picbe-2024-0158},
   issue = {1},
   journal = {Proceedings of the International Conference on Business Excellence},
   month = {6},
   pages = {1882-1891},
   publisher = {Walter de Gruyter GmbH},
   title = {How Artificial Intelligence Can Influence Elections: Analyzing the Large Language Models (LLMs) Political Bias},
   volume = {18},
   year = {2024}
}
@techReport{Peng2024,
   abstract = {As large language models (LLMs) become increasingly embedded in civic, educational, and political information environments, concerns about their potential political bias have grown. Prior research often evaluates such bias through simulated personas or predefined ideological typologies, which may introduce artificial framing effects or overlook how models behave in general-use scenarios. This study adopts a persona-free, topic-specific approach to evaluate political behavior in LLMs, reflecting how users typically interact with these systems-without ideological role-play or conditioning. We introduce a two-dimensional framework: one axis captures partisan orientation on highly polarized topics (e.g., abortion, immigration), and the other assesses sociopolitical engagement on less polarized issues (e.g., climate change, foreign policy). Using survey-style prompts drawn from the ANES and Pew Research Center, we analyze responses from 43 LLMs developed in the U.S., Europe, China, and the Middle East. We propose an entropy-weighted bias score to quantify both the direction and consistency of partisan alignment, and identify four behavioral clusters through engagement profiles. Findings show most models lean center-left or left ideologically and vary in their nonpartisan engagement patterns. Model scale and openness are not strong predictors of behavior, suggesting that alignment strategy and institutional context play a more decisive role in shaping political expression.},
   author = {Tai-Quan Peng and Kaiqi Yang and Sanguk Lee and Hang Li and Yucheng Chu and Yuping Lin and Hui Liu},
   keywords = {CCS CONCEPTS,Information Retrieval Additional Keywords and Phrases: Large language models, political bias, polarization,Information Systems},
   title = {Beyond Partisan Leaning: A Comparative Analysis of Political Bias in Large Language Models LLMs and Political Bias},
   year = {2024}
}
@techReport{Sullivan-Paul2023,
   author = {Michaela Sullivan-Paul and Yves Tiberghien},
   title = {How would ChatGPT vote in a federal election? A study exploring algorithmic political bias in artificial intelligence},
   year = {2023}
}
@article{Faulborn2025,
   abstract = {Prompt-based language models like GPT4 and LLaMa have been used for a wide variety of use cases such as simulating agents, searching for information, or for content analysis. For all of these applications and others, political biases in these models can affect their performance. Several researchers have attempted to study political bias in language models using evaluation suites based on surveys, such as the Political Compass Test (PCT), often finding a particular leaning favored by these models. However, there is some variation in the exact prompting techniques, leading to diverging findings, and most research relies on constrained-answer settings to extract model responses. Moreover, the Political Compass Test is not a scientifically valid survey instrument. In this work, we contribute a political bias measured informed by political science theory, building on survey design principles to test a wide variety of input prompts, while taking into account prompt sensitivity. We then prompt 11 different open and commercial models, differentiating between instruction-tuned and non-instruction-tuned models, and automatically classify their political stances from 88,110 responses. Leveraging this dataset, we compute political bias profiles across different prompt variations and find that while PCT exaggerates bias in certain models like GPT3.5, measures of political bias are often unstable, but generally more left-leaning for instruction-tuned models. Code and data are available on: https://github.com/MaFa211/theory_grounded_pol_bias},
   author = {Mats Faulborn and Indira Sen and Max Pellert and Andreas Spitz and David Garcia},
   month = {7},
   title = {Only a Little to the Left: A Theory-grounded Measure of Political Bias in Large Language Models},
   url = {http://arxiv.org/abs/2503.16148},
   year = {2025}
}

@inproceedings{wright-etal-2024-llm,
    title = "{LLM} Tropes: Revealing Fine-Grained Values and Opinions in Large Language Models",
    author = "Wright, Dustin  and
      Arora, Arnav  and
      Borenstein, Nadav  and
      Yadav, Srishti  and
      Belongie, Serge  and
      Augenstein, Isabelle",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.995/",
    doi = "10.18653/v1/2024.findings-emnlp.995",
    pages = "17085--17112",
    abstract = "Uncovering latent values and opinions embedded in large language models (LLMs) can help identify biases and mitigate potential harm. Recently, this has been approached by prompting LLMs with survey questions and quantifying the stances in the outputs towards morally and politically charged statements. However, the stances generated by LLMs can vary greatly depending on how they are prompted, and there are many ways to argue for or against a given position. In this work, we propose to address this by analysing a large and robust dataset of 156k LLM responses to the 62 propositions of the Political Compass Test (PCT) generated by 6 LLMs using 420 prompt variations. We perform coarse-grained analysis of their generated stances and fine-grained analysis of the plain text justifications for those stances. For fine-grained analysis, we propose to identify tropes in the responses: semantically similar phrases that are recurrent and consistent across different prompts, revealing natural patterns in the text that a given LLM is prone to produce. We find that demographic features added to prompts significantly affect outcomes on the PCT, reflecting bias, as well as disparities between the results of tests when eliciting closed-form vs. open domain responses. Additionally, patterns in the plain text rationales via tropes show that similar justifications are repeatedly generated across models and prompts even with disparate stances."
}

@inproceedings{feng-etal-2023-pretraining,
    title = "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair {NLP} Models",
    author = "Feng, Shangbin  and
      Park, Chan Young  and
      Liu, Yuhan  and
      Tsvetkov, Yulia",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.656/",
    doi = "10.18653/v1/2023.acl-long.656",
    pages = "11737--11762",
    abstract = "Language models (LMs) are pretrained on diverse data sources{---}news, discussion forums, books, online encyclopedias. A significant portion of this data includes facts and opinions which, on one hand, celebrate democracy and diversity of ideas, and on the other hand are inherently socially biased. Our work develops new methods to (1) measure media biases in LMs trained on such corpora, along social and economic axes, and (2) measure the fairness of downstream NLP models trained on top of politically biased LMs. We focus on hate speech and misinformation detection, aiming to empirically quantify the effects of political (social, economic) biases in pretraining data on the fairness of high-stakes social-oriented tasks. Our findings reveal that pretrained LMs do have political leanings which reinforce the polarization present in pretraining corpora, propagating social biases into hate speech predictions and media biases into misinformation detectors. We discuss the implications of our findings for NLP research and propose future directions to mitigate unfairness."
}

@misc{röttger2024politicalcompassspinningarrow,
      title={Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models}, 
      author={Paul Röttger and Valentin Hofmann and Valentina Pyatkin and Musashi Hinck and Hannah Rose Kirk and Hinrich Schütze and Dirk Hovy},
      year={2024},
      eprint={2402.16786},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.16786}, 
}

@misc{PoliticalCompassTest,
	title = {{The Political Compass}},
	note = {[https://www.politicalcompass.org/test]},
	url = {https://www.politicalcompass.org/test}
}

@misc{JointQuestionnaire,
	title = {{WVS Database}},
	note = {[https://www.worldvaluessurvey.org/WVSEVSjoint2017.jsp]},
	url = {https://www.worldvaluessurvey.org/WVSEVSjoint2017.jsp}
}
@inproceedings{Yangetal,
author = {Yang, Kai-Cheng and Menczer, Filippo},
title = {Accuracy and Political Bias of News Source Credibility Ratings by Large Language Models},
year = {2025},
isbn = {9798400714832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3717867.3717903},
doi = {10.1145/3717867.3717903},
abstract = {Search engines increasingly leverage large language models (LLMs) to generate direct answers, and AI chatbots now access the Internet for fresh data. As information curators for billions of users, LLMs must assess the accuracy and reliability of different sources. This paper audits nine widely used LLMs from three leading providers—OpenAI, Google, and Meta—to evaluate their ability to discern credible and high-quality information sources from low-credibility ones. We find that while LLMs can rate most tested news outlets, larger models more frequently refuse to provide ratings due to insufficient information, whereas smaller models are more prone to making errors in their ratings. For sources where ratings are provided, LLMs exhibit a high level of agreement among themselves (average Spearman’s ρ = 0.79), but their ratings align only moderately with human expert evaluations (average ρ = 0.50). Analyzing news sources with different political leanings in the US, we observe a liberal bias in credibility ratings yielded by all LLMs in default configurations. Additionally, assigning partisan roles to LLMs consistently induces strong politically congruent bias in their ratings. These findings have important implications for the use of LLMs in curating news and political information.},
booktitle = {Proceedings of the 17th ACM Web Science Conference 2025},
pages = {127–137},
numpages = {11},
keywords = {Large language models, AI search engines, news credibility, political bias},
location = {
},
series = {Websci '25}
}

@misc{rozado2025measuringpoliticalpreferencesai,
      title={Measuring Political Preferences in AI Systems: An Integrative Approach}, 
      author={David Rozado},
      year={2025},
      eprint={2503.10649},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2503.10649}, 
}

@article{Rettenberger2025,
   abstract = {Evaluating bias in Large Language Models (LLMs) has become a pivotal issue in current Artificial Intelligence (AI) research due to their significant impact on societal dynamics. Recognizing political bias in LLMs is particularly important as they approach performative prediction, influencing societal behavior and political events, such as the upcoming European Parliament elections. From a German voter’s perspective, we evaluate the political bias of the currently most popular open-source LLMs concerning political issues within the European Union. To do so, we use the "Wahl-O-Mat," a voting advice application used in Germany. We show that larger models, such as Llama3-70B, tend to align more closely with left-leaning political parties, while smaller models often remain neutral, particularly when prompted in English. The central finding is that LLMs are similarly biased, with low variances in the alignment concerning a specific party. Our findings offer crucial insights for developers and policymakers to understand and mitigate LLM biases, emphasizing the need for rigorous bias assessment to ensure the integrity and trustworthiness of AI applications.},
   author = {Luca Rettenberger and Markus Reischl and Mark Schutera},
   doi = {10.1007/s42001-025-00376-w},
   issn = {2432-2717},
   issue = {2},
   journal = {Journal of Computational Social Science},
   month = {5},
   pages = {42},
   title = {Assessing political bias in large language models},
   volume = {8},
   year = {2025}
}

@misc{bang2024measuringpoliticalbiaslarge,
      title={Measuring Political Bias in Large Language Models: What Is Said and How It Is Said}, 
      author={Yejin Bang and Delong Chen and Nayeon Lee and Pascale Fung},
      year={2024},
      eprint={2403.18932},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.18932}, 
}

@inbook{Ghose2025,
   author = {Anuttama Ghose and Pallav Pallav and S. M. Aamir Ali},
   doi = {10.4018/979-8-3693-7036-0.ch007},
   month = {1},
   pages = {135-160},
   title = {AI and Political Polarization},
   year = {2025}
}

@misc{nawale2025fairitalesevaluationfairness,
      title={FairI Tales: Evaluation of Fairness in Indian Contexts with a Focus on Bias and Stereotypes}, 
      author={Janki Atul Nawale and Mohammed Safi Ur Rahman Khan and Janani D and Mansi Gupta and Danish Pruthi and Mitesh M. Khapra},
      year={2025},
      eprint={2506.23111},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2506.23111}, 
}

@misc{saito2025answerinvestigatingpositionalbias,
      title={Where is the answer? Investigating Positional Bias in Language Model Knowledge Extraction}, 
      author={Kuniaki Saito and Kihyuk Sohn and Chen-Yu Lee and Yoshitaka Ushiku},
      year={2025},
      eprint={2402.12170},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.12170}, 
}

@misc{wang2025eliminatingpositionbiaslanguage,
      title={Eliminating Position Bias of Language Models: A Mechanistic Approach}, 
      author={Ziqi Wang and Hanlin Zhang and Xiner Li and Kuan-Hao Huang and Chi Han and Shuiwang Ji and Sham M. Kakade and Hao Peng and Heng Ji},
      year={2025},
      eprint={2407.01100},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.01100}, 
}

@misc{dong2023probingexplicitimplicitgender,
      title={Probing Explicit and Implicit Gender Bias through LLM Conditional Text Generation}, 
      author={Xiangjue Dong and Yibo Wang and Philip S. Yu and James Caverlee},
      year={2023},
      eprint={2311.00306},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.00306}, 
}

@misc{lim2024africanwomanrhythmicsoulful,
      title={The African Woman is Rhythmic and Soulful: An Investigation of Implicit Biases in LLM Open-ended Text Generation}, 
      author={Serene Lim and María Pérez-Ortiz},
      year={2024},
      eprint={2407.01270},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.01270}, 
}

@article{Huang10.1145/3724117,
author = {Huang, Dong and Zhang, Jie M. and Bu, Qingwen and Xie, Xiaofei and Chen, Junjie and Cui, Heming},
title = {Bias Testing and Mitigation in LLM-based Code Generation},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3724117},
doi = {10.1145/3724117},
abstract = {As the adoption of LLMs becomes more widespread in software coding ecosystems, a pressing issue has emerged: does the generated code contain social bias and unfairness, such as those related to age, gender, and race? This issue concerns the integrity, fairness, and ethical foundation of software applications that depend on the code generated by these models but are underexplored in the literature. This paper presents a novel bias testing framework that is specifically designed for code generation tasks. Based on this framework, we conduct an extensive empirical study on the biases in code generated by five widely studied LLMs (i.e., PALM-2-CodeChat-bison, Claude-instant-1, GPT-3.5-turbo, GPT-4-turbo, and GPT-4). Our findings reveal that biases are prevalent. For example, 13.47\% to 49.10\% of the codes generated by these LLMs have biased behaviors towards gender. Moreover, we study five bias mitigation prompt strategies that are commonly used in current code generation scenarios, i.e., zero-shot, one-shot, few-shot, and two Chain-of-Thought (CoT) prompts, with and without provided feedback-driven refinement. Our evaluation results illustrate that using direct prompt engineering strategies has limited effectiveness in mitigating bias, but our test execution feedback can help to reduce the ratio of code biases to a large extent (e.g., from 59.88\% to 4.79\% for GPT-4)1.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
keywords = {Fairness testing, code generation}
}

@inproceedings{soundararajan-delany-2024-investigating,
    title = "Investigating Gender Bias in Large Language Models Through Text Generation",
    author = "Soundararajan, Shweta  and
      Delany, Sarah Jane",
    editor = "Abbas, Mourad  and
      Freihat, Abed Alhakim",
    booktitle = "Proceedings of the 7th International Conference on Natural Language and Speech Processing (ICNLSP 2024)",
    month = oct,
    year = "2024",
    address = "Trento",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.icnlsp-1.42/",
    pages = "410--424"
}

@article{Fang2024,
   abstract = {Large language models (LLMs) have the potential to transform our lives and work through the content they generate, known as AI-Generated Content (AIGC). To harness this transformation, we need to understand the limitations of LLMs. Here, we investigate the bias of AIGC produced by seven representative LLMs, including ChatGPT and LLaMA. We collect news articles from The New York Times and Reuters, both known for their dedication to provide unbiased news. We then apply each examined LLM to generate news content with headlines of these news articles as prompts, and evaluate the gender and racial biases of the AIGC produced by the LLM by comparing the AIGC and the original news articles. We further analyze the gender bias of each LLM under biased prompts by adding gender-biased messages to prompts constructed from these news headlines. Our study reveals that the AIGC produced by each examined LLM demonstrates substantial gender and racial biases. Moreover, the AIGC generated by each LLM exhibits notable discrimination against females and individuals of the Black race. Among the LLMs, the AIGC generated by ChatGPT demonstrates the lowest level of bias, and ChatGPT is the sole model capable of declining content generation when provided with biased prompts.},
   author = {Xiao Fang and Shangkun Che and Minjia Mao and Hongzhe Zhang and Ming Zhao and Xiaohang Zhao},
   doi = {10.1038/s41598-024-55686-2},
   issn = {2045-2322},
   issue = {1},
   journal = {Scientific Reports},
   month = {3},
   pages = {5224},
   title = {Bias of AI-generated content: an examination of news produced by large language models},
   volume = {14},
   year = {2024}
}

@article{Rozado2023,
   abstract = {Recent advancements in Large Language Models (LLMs) suggest imminent commercial applications of such AI systems where they will serve as gateways to interact with technology and the accumulated body of human knowledge. The possibility of political biases embedded in these models raises concerns about their potential misusage. In this work, we report the results of administering 15 different political orientation tests (14 in English, 1 in Spanish) to a state-of-the-art Large Language Model, the popular ChatGPT from OpenAI. The results are consistent across tests; 14 of the 15 instruments diagnose ChatGPT answers to their questions as manifesting a preference for left-leaning viewpoints. When asked explicitly about its political preferences, ChatGPT often claims to hold no political opinions and to just strive to provide factual and neutral information. It is desirable that public facing artificial intelligence systems provide accurate and factual information about empirically verifiable issues, but such systems should strive for political neutrality on largely normative questions for which there is no straightforward way to empirically validate a viewpoint. Thus, ethical AI systems should present users with balanced arguments on the issue at hand and avoid claiming neutrality while displaying clear signs of political bias in their content.},
   author = {David Rozado},
   doi = {10.3390/socsci12030148},
   issn = {2076-0760},
   issue = {3},
   journal = {Social Sciences},
   title = {The Political Biases of ChatGPT},
   volume = {12},
   url = {https://www.mdpi.com/2076-0760/12/3/148},
   year = {2023}
}

@misc{yuksel2025languagedependentpoliticalbiasai,
      title={Language-Dependent Political Bias in AI: A Study of ChatGPT and Gemini}, 
      author={Dogus Yuksel and Mehmet Cem Catalbas and Bora Oc},
      year={2025},
      eprint={2504.06436},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.06436}, 
}

@article{Gallegos2024,
   abstract = {Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this article, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely, metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.},
   author = {Isabel O. Gallegos and Ryan A. Rossi and Joe Barrow and Md Mehrab Tanjim and Sungchul Kim and Franck Dernoncourt and Tong Yu and Ruiyi Zhang and Nesreen K. Ahmed},
   doi = {10.1162/coli_a_00524},
   issn = {0891-2017},
   issue = {3},
   journal = {Computational Linguistics},
   month = {9},
   pages = {1097-1179},
   title = {Bias and Fairness in Large Language Models: A Survey},
   volume = {50},
   year = {2024}
}

@misc{ranjan2024comprehensivesurveybiasllms,
      title={A Comprehensive Survey of Bias in LLMs: Current Landscape and Future Directions}, 
      author={Rajesh Ranjan and Shailja Gupta and Surya Narayan Singh},
      year={2024},
      eprint={2409.16430},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.16430}, 
}

