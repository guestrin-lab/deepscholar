\documentclass[journal]{IEEEtran}
\usepackage{amsmath,amsfonts,bm,mathtools,amssymb}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\usepackage{subcaption}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021
\newcommand{\cov}{\mathrm{cov}}
\begin{document}

\title{Bayesian Nonparametric Dynamical Clustering of Time Series
\author{Adrián~Pérez-Herrero,
        Paulo~Félix,
        Carl~Henrik~Ek
        and Jesús~Presedo% <-this % stops a space
\thanks{Adrián~Pérez-Herrero, Paulo~F\'elix and Jesús~Presedo are with the CiTIUS (Centro Singular de Investigaci\'on en Tecnolox\'ias Intelixentes), Universidade de Santiago de Compostela, Santiago de Compostela, 15782 Spain.\\
E-mail:\{adrian.perez.herrero,paulo.felix,jesus.presedo\}@usc.es.

Carl~Henrik~Ek is with the Department of Computer Science and Technology, University of Cambridge, 15 JJ Thomson Avenue, Cambridge, CB3 0FD, United Kingdom.\\
E-mail:che29@cam.ac.uk.
}}
}%<-this % stops a space

% The paper headers
\markboth{}%
{Pérez-Herrero \MakeLowercase{\textit{et al.}}: Bayesian nonparametric dynamical clustering of time series}

%\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
We present a method that models the evolution of an unbounded number of time series clusters by switching among an unknown number of regimes with linear dynamics. We develop a Bayesian non-parametric approach using a hierarchical Dirichlet process as a prior on the parameters of a Switching Linear Dynamical System and a Gaussian process prior to model the statistical variations in amplitude and temporal alignment within each cluster. By modeling the evolution of time series patterns, the method avoids unnecessary proliferation of clusters in a principled manner. We perform inference by formulating a variational lower bound for off-line and on-line scenarios, enabling efficient learning through optimization. We illustrate the versatility and effectiveness of the approach through several case studies of electrocardiogram analysis using publicly available databases.
\end{abstract}

\begin{IEEEkeywords}
Time series analysis, Bayesian methods, Gaussian processes, linear dynamical systems, Dirichlet processes, unsupervised learning, electrocardiogram, arrhythmia detection.
\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}
\IEEEPARstart{T}{ime} series data analysis has come to pervade all scientific and technological domains, driven by the need to understand change over time. With the growing availability of such data, machine learning has assumed an increasingly central role in a wide variety of tasks which fall under the category of pattern recognition. Particularly, there is growing interest in identifying similar behaviors in time series data as a preliminary step towards generating insights into the dynamics of the underlying processes. Some recent methodologies can be found for characterizing sea wave conditions \cite{hamilton2010}, transcriptome-wide gene expression profiling \cite{mcdowell2018}, selecting stocks with different share price performance \cite{nieto2014}, and discovering human motion primitives \cite{zhou2013}.

One of the main challenges in this setting is the dynamic nature of time series data, which often exhibits evolving trends, shifts over time and non-stationary patterns. While traditional clustering techniques effectively cope with sequence pattern analysis, they often focus on task-specific performance rather than the more fundamental question of describing the dynamics that generated the time series \cite{rabiner_1989}. Recovering the generative mechanism enables us to learn more about the data source (the real-world process) and to devise methods for solving prediction, recognition or identification tasks. 

Linear dynamical systems (LDS) have been successfully applied in describing dynamical phenomena, by considering continuous hidden state variables with Markovian dynamics given by Gaussian linear relationships between consecutive states. Furthermore, when time series data exhibit nonlinear or even non-stationary dynamics, switching linear dynamical systems (SLDS) are proposed as a natural generalization, where each state of a hidden Markov model (HMM) is associated with a linear dynamical process, so the discrete transition between states switches between different linear regimes \cite{harrison1976,hamilton1989}. Applications of SLDS can be found in econometrics \cite{kim1994}, human motion \cite{pavlovic2000}, or target tracking \cite{li2005}.

Model identification is a major issue in describing dynamics from a finite number of observations. Most existing methods for learning SLDS rely on either setting the number of different linear regimes \cite{ghahramani_variational_2000}, or including a change-point detection procedure for the addition of a new regime \cite{paoletti2007}. Fox {\em et al.} \cite{fox_2011} relaxed this assumption to allow an unbounded number of regimes by proposing a Bayesian nonparametric approach. They make use of the hierarchical Dirichlet process (HDP) of Teh {\em et al.} \cite{teh_2006} as a prior for the HMM. By introducing a prior over the addition of new regimes, Bayesian inference allows a natural trade-off between model complexity and data-fit in the posterior distribution, while leveraging previous knowledge of the dynamics. 

In this paper, we propose a method for clustering time series segments of different lengths, assuming they are realisations of the same underlying dynamic process. Underpinning our method is the use of Gaussian process (GP) priors to infer a model for representing the different patterns to be identified. Gaussian processes have the ability to capture a wide variety of behavior through the choice of a convenient covariance function, making it easier to compute robust confidence intervals and naturally include inference in a Bayesian framework \cite{mackay1992,rasmussen_gaussian_2006}. Our approach can discover an unknown number of clusters representing distinctive morphologies that change over time as new segments are processed. To this end, we provide a latent HDP-SLDS to describe the time series in terms of Gaussian processes that evolve according to a discrete set of linear dynamics. 

While GPs properly capture the statistical variations in amplitude, variations in time fall outside its scope. Temporal misalignment has been commonly addressed by means of the dynamic time warping (DTW) method \cite{sakoe1978}, and their numerous variants, based on the calculation of a pairwise similarity measure between time series segments with non-linear timing differences. Recent methods have been proposed for learning alignments from data by using GPs, adopting two different approaches: the first is based on learning a nonlinear transformation of the GP output \cite{snelson_2003, lazaro_2012}, while the second learns a monotonic function to warp the input space \cite{duncker_2018, kazlauskaite_2019}. Among the benefits of learning a warping function are the interpretability of misalignments and its ability to resample the data. Our proposal makes a step forward by assuming that misalignment is not just an incidental issue, resulting from noisy sampling the underlying dynamics, but it is an intrinsic part of the dynamics and, as such, should be included in the generative model, probably conveying its own specific physical meaning. From a practical point of view, by using GPs to learn a warping function we can simultaneously perform alignment and clustering in the same Bayesian framework, in a manner consistent with the predicted evolution of the set of linear dynamics.

Section \ref{sec:notation} introduces the basic notation used along the paper. In Section \ref{sec:problem}, we present the problem of identifying different beat morphologies in the electrocardiogram, which serves to illustrate our methodological framework and evaluate its effectiveness. In Section \ref{sec:background}, we provide background on the different formalisms that support our proposal. Our Bayesian nonparametric dynamical clustering is described in Section \ref{sec:generative_model}. Section \ref{sec:variational_inference} provides off-line and on-line variational inference methods for computing the posterior probabilities of the hidden variables of the model. In Section \ref{sec:hyperparameters} we study the impact of the different hyperparameters on the behavior of the model and on the interpretation of the results. In Section \ref{sec:applications} we present results on some real data sets and in Section \ref{sec:related_work} we analyze a set of alternative formulations for related problems.

\section{Notation}
\label{sec:notation}
Consider a dataset $\mathcal{Y}=\{(\mathbf{t}_n,\mathbf{y}_n)\}_{n=1}^{N}$ of time series segments, where $\mathbf{t}_n=(t_{ni})_{i=1}^q$ denotes an indexing time vector and $\mathbf{y}_n=(y_{ni})_{i=1}^q$ denotes a vector of real values. We are assuming that all the segments have the same number of samples to keep the notation uncluttered, although this is not a limitation of the present proposal. The result of the learning process is a generative model which captures the dynamics displayed by a number of evolving morphologies discovered within the dataset. These dynamics are summarized by a sequence of switch states $\{s_n\}_{n=1}^{N}$ evolving according to a discrete Markov transition structure, thus enabling to effectively approximate the underlying dynamics through transitions between multiple linear regimes. Each linear regime makes evolve a Gaussian process $\{f_n\}_{n=1}^{N}$ representing each distinctive morphology. Before observation, these morphologies are assumed to be distorted according to another Gaussian process model $\{g_n\}_{n=1}^{N}$. 

\section{An application scenario}
\label{sec:problem}
We illustrate the methodology proposed in this paper through applications in the domain of electrocardiography. The electrocardiogram (ECG) represents a recording of the electrical activity of the heart that is captured non-invasively from the surface of the body. An ECG signal comprises several significant waves and intervals, reflecting the anatomy and physiology of the conduction system of the heart. As such, it provides a rich source of information to analyze cardiovascular pathology, and even other pathologies that can affect electrocardiographic tracing.

ECG heartbeat clustering aims at dividing an ECG recording into a set of beat clusters, based on morphological similarity. Heartbeat clustering allows the identification of those beats that share a common activation origin and conduction pathway through the cardiac tissue, having proven to be an effective tool in diagnosing conduction disorders.

Figure \ref{fig:ecg_e1302} shows a great deal of variability among different realizations of the same conduction pattern through the cardiac tissue across time. There are two main sources of variability: on one hand, the stochastic nature of electrical propagation at the microscopic level along the cellular constituents of conduction pathways \cite{madison1995}; and on the other hand, the available evidence on a measurable impact of different mechanical, neurological, metabolic and hormonal processes, among others, on the electrocardiographic tracing \cite{malik_1996}. We hypothesized that this variability results in an observable evolution of the heartbeat morphology that can be properly modeled by the present proposal. 

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/ecg_examples/ecg_beats_e1302.png}
    \caption{Several heartbeat examples from record e1302 of European ST-T Database \cite{taddei_1992}. All of them were manually annotated as Normal except the fifth one which was annotated as Ventricular \cite{goldberger_2000}.}
    \label{fig:ecg_e1302}
\end{figure*}

\section{Background}
\label{sec:background}
\subsection{Gaussian Processes}
A {\em Gaussian process} (GP) is defined as a collection of random variables, any finite number of which have a joint Gaussian distribution \cite{rasmussen_gaussian_2006}. A GP can be indexed by some continuous set (e.g., time), which can be used to define a prior over an unknown function $f(t)$. A GP is fully specified by its mean function $m(t)$ and covariance function $k(t,t')$ and we will write $f(t) \sim \mathcal{GP}(m(t),k(t,t^{\prime}))$. Choosing the covariance function is a sort of model selection process that relies upon the objective of the modeling and the characteristics of the time series: by selecting a smooth covariance function we can model smooth functions, by selecting a periodic covariance function we can model periodic functions, etc. Ultimately, the covariance function determines almost all the properties of the resulting GP. We denote by $k_{\theta}(t,t^{\prime})$ the particular choice of the covariance function given by specific parameters $\theta$.

GPs are commonly used in regression tasks, consisting of learning from a dataset with data pairs $(t_i,y_i)_{i=1}^q$ where $\mathbf{t}=(t_1,...,t_q)$ denotes an input vector and $\mathbf{y}=(y_1,...,y_q)$ denotes a target vector of real values. We assume that each $y_i$ is a noisy version of the hidden $f(t_i)$, so that $\mathbf{y}=f(\mathbf{t})+\varepsilon$, with $\varepsilon \sim \mathcal{N}(0,\sigma^2)$, assuming additive identically distributed Gaussian noise. We denote by $f_i \triangleq f(t_i)$ the random variable representing the value of the function $f(t)$ at the input $t_i$. By the definition of GP, any finite number of random variables  $\mathbf{f}$ is distributed as $\mathbf{f}|\mathbf{t} \sim \mathcal{N}\left(\mathbf{f}|m(\mathbf{t}), K^{\theta}_{\mathbf{t},\mathbf{t}}\right)$, where $K^{\theta}_{\mathbf{t},\mathbf{t}}$ is a $q\times q$ matrix of components $k_{\theta}(t_i,t_j)$. Thus, prediction on new test points $\mathbf{t}^*$ can be computed from the joint distribution
\begin{equation}
 \left[\begin{array}{c} \mathbf{y}\\ \mathbf{f}^* \end{array}\right] \sim \mathcal{N}\left( \mathbf{0},\left[ \begin{array}{cc} K^{\theta}_{\mathbf{t},\mathbf{t}}+\sigma^2 I & K^{\theta}_{\mathbf{t},\mathbf{t}^*}\\ K^{\theta}_{\mathbf{t}^*,\mathbf{t}} & K^{\theta}_{\mathbf{t}^*,\mathbf{t}^*} \end{array}\right] \right).
\label{eq:gaussian_process}
\end{equation}
Deriving the conditional distribution, the predictive equations for GP regression are given by 
\begin{equation}
    \mathbf{f}^* | \mathbf{t}^*, \mathbf{t},  \mathbf{y} \sim \mathcal{N}(\mathbb{E}[\mathbf{f}^*], \text{cov}\left( \mathbf{f}^*\right)),
\end{equation}
where the predictive mean and covariance are
\begin{equation}
\begin{array}{rl}
    \mathbb{E}[\mathbf{f}^*] & = K^{\theta}_{\mathbf{t}^*,\mathbf{t}} \left[K^{\theta}_{\mathbf{t},\mathbf{t}}+\sigma^2 I\right]^{-1}\mathbf{y} \\
    \text{cov}( \mathbf{f}^*) & = K^{\theta}_{\mathbf{t}^*,\mathbf{t}^*} -K^{\theta}_{\mathbf{t}^*,\mathbf{t}} \left[K^{\theta}_{\mathbf{t},\mathbf{t}}+\sigma^2 I\right]^{-1}K^{\theta}_{\mathbf{t},\mathbf{t}^*}
\end{array}
\end{equation}
The computational complexity is dominated by the matrix inversion, which scales as $\mathcal{O}(q^3)$. To mitigate this, the information contained in the original $q$ input points can be projected onto a smaller set of $p<q$ inducing points (not necessarily a subset of the former), resulting in $\mathbf{f}_n^p\sim \mathcal{N}(m(\mathbf{t}^p),K^{\theta}_{\mathbf{t}^p,\mathbf{t}^p})$ latent variables, and thus benefiting computing efficiency. The linear Gaussian relation 
\begin{equation}
\mathbf{f}_n|\mathbf{f}_n^p \sim \mathcal{N}(\mathbf{f}_n|K^{\theta}_{\mathbf{t},\mathbf{t}^p}K^{\theta ~~-1}_{\mathbf{t}^{p},\mathbf{t}^p}\mathbf{f}_n^p,
K^{\theta}_{\mathbf{t},\mathbf{t}}-K^{\theta}_{\mathbf{t},\mathbf{t}^p}K^{\theta~~ -1}_{\mathbf{t}^p,\mathbf{t}^p}K^{\theta}_{\mathbf{t}^p,\mathbf{t}})
\label{eq:seeger_approach}
\end{equation}
where $\mathbb{E}[\mathbf{f}_n|\mathbf{f}^p_n]=K^{\theta}_{\mathbf{t},\mathbf{t}^p}K^{\theta ~~-1}_{\mathbf{t}^{p},\mathbf{t}^p}\mathbf{f}_n^p$, can be proven to be the KL-optimal projection, i.e., the approximating distribution $q(\mathbf{y}_n|\mathbf{f}_n^p)=\mathcal{N}(\mathbf{y}_n|\mathbb{E}[\mathbf{f}_n|\mathbf{f}^p_n],\sigma^2 I)$ allows us to minimize $\mathrm{KL}(q(\mathbf{f}_n|\mathbf{y}_n)||p(\mathbf{f}_n|\mathbf{y}_n))$  \cite{Seeger2005phd}. This approach also allows to make predictions on new different test points $\mathbf{t}^*$ from the joint distribution of $\mathbf{f}_n$ and the latent variables at the test locations $\mathbf{f}_n^*$ under the prior, by integrating over $\mathbf{f}_n^p$.

\subsection{GP-based temporal alignment}
Following Kazlauskaite {\em et al.} \cite{kazlauskaite_2019}, we consider each sequence to be generated from a latent function $f_n(\mathbf{t}_n)$ and a monotonic warping $g_n(\mathbf{t}^0_n)$ as $\mathbf{y}_{n}=f_n(g_n(\mathbf{t}^0_{n}))+\varepsilon_{n}$ where $\varepsilon_n \sim \mathcal{N}(0,\sigma_{n}^2)$. The latent function $f_n$ can be drawn from a GP prior with covariance function $k_{\vartheta}(t,t')$. The aligned sequence, which is unobserved, is given by the same function without the time warp $\mathbf{f}^0_n=f_n(\mathbf{t}^0_{n})$. The joint distribution of the random variables representing the misaligned sequence $\mathbf{f}_{n}$ and the aligned sequence $\mathbf{f}^0_{n}$ according to the prior is
\begin{equation}
 \left[\begin{array}{c} \mathbf{f}_n\\ \mathbf{f}^0_n \end{array}\right] \sim \mathcal{N}\left(\mathbf{0},\left[ \begin{array}{cc} K^{\vartheta}_{\mathbf{t}_n,\mathbf{t}_n} & K^{\vartheta}_{\mathbf{t}_n,\mathbf{t}^0_n}\\ K^{\vartheta}_{\mathbf{t}^0_n,\mathbf{t}_n} & K^{\vartheta}_{\mathbf{t}^0_n,\mathbf{t}^0_n} \end{array}\right] \right)
\end{equation}
The warping function can also be drawn from a GP prior with appropriate smoothness and monotonicity properties. Actually, since only $\mathbf{y}_n$ are observed in the time instants given by $\mathbf{t}_n$, the temporal support $\mathbf{t}^0_n$ of the aligned sequence could be inferred as $\mathbf{t}^0_{n}=g^{-1}_n(\mathbf{t}_{n})$. As the warping function is strictly monotonic it is bijective and its inverse is well-defined. 

\subsection{Hierarchical Dirichlet Processes and SLDS}
According to Ghahramani and Hinton \cite{ghahramani_variational_2000}, a {\em switching linear dynamical system} (SLDS) enables modeling of nonlinear dynamics by transitioning between multiple linear dynamical systems (LDS) in a discrete manner. Formally:
\begin{equation}
\begin{aligned}
s_{n}  &\sim  \pi ^{(s_{n-1})} \\
    \mathbf{f}_{n} &= A^{(s_{n})} \mathbf{f}_{n-1}  + \omega _n^{(s_{n})} \\
    \mathbf{y}_{n} &= C^{(s_{n})} \mathbf{f}_{n}  + \epsilon _n ^{(s_{n})}.
\label{eq:slds}
\end{aligned}
\end{equation}
Each latent switching state $s_n$ follows a multinomial distribution over $S$ discrete values. These states determine the probabilistic assignment of observations $\mathbf{y}_n$ to their corresponding LDS. It is assumed a first-order Markov dependency between states $s_{n}$ with transitions given by $\pi = \{\pi^{(j)}\}_{j=0}^S$, $\pi^{(j)}\in \mathbb{R}^S_{+}$. Specifically, $\pi^{(0)}$ denotes the initial state distribution and $\pi^{(j)}$ ($j=1,...,S$) denotes the {\em j}th row of the transition matrix. In Equation \ref{eq:slds} it is assumed that both the dynamics of the latent state process and the observation mechanism depend on the switching state, but a shared observation mechanism can also be assumed, depending on the applications \cite{fox_2011}.

A major limitation of this framework is assuming that the number of states $S$ is known in advance. Dirichlet processes (DP) are commonly used to build nonparametric Bayesian models, particularly HMMs such that the number of states is unbounded a priori. The {\em Dirichlet process} $\text{DP}(\gamma, H)$ is a measure on measures, where $\gamma >0$ is a concentration parameter and $H$ is a base probability measure \cite{ferguson_1973}. We can represent a draw from a Dirichlet process $G_0\sim \text{DP}(\gamma, H)$ as
\begin{equation}
    G_0=\sum_{k=1}^{\infty} \beta_k \delta_{\phi_k},
\end{equation}
where $\phi_k \overset{iid}{\sim} H$ and $\delta _{\phi_k}$ is a probability measure concentrated at $\phi_k$. The weights $\beta_k$ are sampled by means of a {\em stick-breaking process} \cite{sethuraman_1994}:
\begin{equation}
\beta_k= v_k \prod _{j=1} ^{k-1}(1-v_j), ~~~ v_k\sim \text{Beta}(1,\gamma).
\label{eq:stick_breaking}
\end{equation}
The sequence $\beta = (\beta_k)_{k=1} ^{\infty}$ satisfies $\sum _{k=1} ^{\infty}\beta_k=1$ and it is denoted as $\beta\sim \text{GEM}(\gamma)$. Intuitively, the stick-breaking process involves repeatedly breaking off and discarding a random fraction of a stick that is initially of length 1. This random fraction is in turn sampled from a beta distribution. Thus, each weight $v_k$ would define the probability of reaching state $k$, given that the first $k-1$ states were not reached, and each weight $\beta_k$ would define the marginal probability of reaching state $k$.

In a {\em hierarchical Dirichlet process} (HDP) the base measure of a DP is distributed as a DP. Thus, a HDP is defined as a collection of measures $\{G_j\}_{j=1}^\infty$ distributed as a DP with $G_j \overset{iid}{\sim} \text{DP}(\alpha, G_0)$, where $G_0$ is also distributed as a DP with $G_0\sim \text{DP}(\gamma, H)$ \cite{beal_2001,teh_2006}. According to the stick-breaking representation
\begin{equation}
    G_j=\sum_{k=1}^{\infty} \pi_{jk} \delta_{\phi_k},
    \label{eq:gj}
\end{equation}
where each $G_j$ has support at the same points $\phi=(\phi_{k})_{k=1}^{\infty}$ as $G_0$, providing a desirable sharing property for hierarchical clustering. Since each $G_j$ is drawn independently given $G_0$, the weights $(\pi_{jk})_{k=1}^{\infty}$ are independent given $\beta$.

A nonparametric HMM can be based on a set of DPs, one for each value of each state variable. The hierarchical Dirichlet process HMM (HDP-HMM) replaces the set of conditional finite mixture models of the classical HMM with an HDP mixture model. If a finite Bayesian HMM places a Dirichlet($\alpha$) prior on each row of the transition matrix $\pi$, the HDP-HMM considers each of the infinitely many rows of $\pi= \{\pi^{(j)}\}_{j=0}^{\infty}$ to be the stick-breaking pieces of some $G_{j} \sim \text{DP}(\alpha, G_{0})$, this allowing infinite distributions over infinite hidden states. Formally, the parameters of the HDP-HMM are distributed according to
%Importantly, its sharing property ensures that the same set of states can be reached from the current state.
\begin{align}
    \beta & \sim  \text{GEM}(\gamma)\\
    \pi^{(j)} & \overset{iid}{\sim}  \text{DP}(\alpha,\beta) \label{eq:hdp-hmm_pi} \\
    \phi^{(j)} & \overset{iid}{\sim}  H,
\end{align}
where $\beta$ is treated as a density concerning counting measure on $\mathbb{N}$. Each $\pi^{(j)}$ is a draw from a DP and $\mathbb{E}[\pi^{(j)}]=\beta$, thus making the transition distributions concentrate around a similar set of states, providing the desired property of re-using previous states. Both the latent and observed variables in the HDP-HMM are distributed according to
\begin{align}
    s_n  &\sim  \pi ^{(s_{n-1})} \label{eq:hmm_sn} \\
    \mathbf{y}_n | s_n & \sim F(\phi^{(s_n)}), \label{eq:hmm_yn}
\end{align}
where $F(\phi^{(n)})$ is the emission distribution and $\phi^{(n)}$ is the set of emission parameters for the $s_n$th state.

Fox {\em et al.} \cite{fox_2011} provide an extension of HDP-HMM for modeling SLDS by allowing to switch between an unknown number of linear dynamical systems (HDP-SLDS). Equation (\ref{eq:hmm_yn}) in their approach takes the form of a LDS, and model parameters are distributed as $(A^{(j)},\Sigma _{\omega}^{(j)}, \Sigma _{\epsilon}) \overset{iid}{\sim}  H$, proposing a shared measurement mechanism. Additionally, they provide a {\em sticky transition}, a construction that increases the expected probability of self-transition to prevent unrealistically fast transitions between linear dynamical systems. Authors place priors on the dynamics parameters $A^{(s_{n})}$, $\Sigma _{\omega} ^{(s_{n})}$, and on the measurement noise covariance $\Sigma _{\epsilon}$, and infer their posterior from the data.

\section{The generative model}
\label{sec:generative_model}
We explain the generative process of the HDP-GP clustering (HDP-GPC) by separating it into a sequence of stages (see Figure \ref{fig:gen_model}). The first one is a Bayesian nonparametric switching, modeled by a HDP as a prior on a set of HMM transitions, with concentration parameters $\gamma, \alpha >0$:
\begin{align}
    \beta & \sim  \text{GEM}(\gamma),\\
    \pi^{(j)} & \overset{iid}{\sim}  \text{DP}(\alpha,\beta), \label{eq:dp} \\
    \phi^{(j)} & \overset{iid}{\sim}  H_{\text{LDS}},\\
    s_{n}  &\sim  \pi ^{(s_{n-1})}, \label{eq:switching}
\end{align}
where $s_1\sim \pi^{(0)}$, and the parameters corresponding to the linear dynamical system are $(A^{(j)},\Sigma _{\omega}^{(j)}, C^{(j)}, \Sigma _{\epsilon} ^{(j)}) \overset{iid}{\sim}  H_{\text{LDS}}$.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/generative_model/gen_model_v12.pdf}
	\caption{Graphical model. A number of linear dynamics are generated to represent the different clusters that evolve across time. Conditioned on a specific setting of the switch variable $s_n=m$, an observation $\mathbf{y}_n$ is generated by LDS $m$ through a linear projection given by equations (\ref{eq:latent_trans}) and (\ref{eq:emission_trans}) from the last observation generated by this same LDS.}
	\label{fig:gen_model}
\end{figure}

Every time a new state is generated by expression (\ref{eq:switching}) a new LDS is generated by instantiating a GP $f^{(s_n)}(t) \sim \mathcal{GP}(m_{\theta ^{(s_{n})}}(t),k_{\theta ^{(s_{n})}}(t, t'))$, with hyperparameters $\theta ^{(s_{n})}$, on a certain time index set:
\begin{equation}
\mathbf{f} _1\sim \mathcal{N}(\mathbf{0},K^{\theta ^{(s_{n})}}_{\mathbf{t}_1,\mathbf{t}_1}),
\end{equation}
where $\mathbf{t}_1$ denotes the input vector from the first sequence. Since we have no prior knowledge about the mean we take it to be zero by symmetry. Then, each LDS can evolve a first realization of the corresponding GP several times as
\begin{align}
    \mathbf{f}^{(s_{n})}_{n} &= A^{(s_{n})} \mathbf{f}^{(s_{n})}_{n-1} + \omega _n^{(s_{n})}, \label{eq:latent_trans}\\
    \mathbf{x}^{(s_{n})}_{n} &= C^{(s_{n})} \mathbf{f}^{(s_{n})}_{n} + \epsilon ^{(s_{n})} _n. \label{eq:emission_trans}
\end{align}
Thus, each LDS represents a certain morphology as a GP which evolves across time, providing new instances that are continually being added to the chain of observations. Note that since (\ref{eq:latent_trans}) and (\ref{eq:emission_trans}) are composed of linear transformations, the GP structure is preserved. Actually, a pseudo-observation $\mathbf{x}^{(s_{n})}_n$ is generated at each step, which is then affected by an irregular time delay before it is finally observed. We model this by assuming $\mathbf{x}^{(s_{n})}_n$ as a finite representation at the input points of the GP resulting from the projection of $\mathbf{f}^{(s_{n})}_n$ by equations (\ref{eq:latent_trans}) and (\ref{eq:emission_trans}). We recall that equation (\ref{eq:gaussian_process}) allows us to predict $x^{(s_n)}(t)\sim \mathcal{GP}(m_{x^{(s_n)}}(t),k_{x^{(s_n)}}(t,t'))$ on new different test points, particularly, on those given by the time delay. The time delay produces a temporal misalignment that can be modeled by a monotonic warping given by a GP on the time index set $g^{(s_n)}(t) \sim  \mathcal{GP}(m_{\vartheta ^{(s_{n})}}(t),k_{\vartheta ^{(s_{n})}}(t,t'))$, with hyperparameters $\vartheta ^{(s_{n})}$, leading to a new temporal support of the observations that, in the absence of noise, is given by 
\begin{equation}
 \mathbf{t}_n^w \triangleq \mathbf{g}^{(s_{n})}_n\sim \mathcal{N}(m_{\vartheta ^{(s_{n})}}(\mathbf{t}_n), K^{\vartheta ^{(s_{n})}}_{\mathbf{t}_n,\mathbf{t}_n}).
\end{equation}
Both the target observations $\mathbf{y}_n$, inferred at the new locations $\mathbf{t}^w_n$, and $\mathbf{x}^{(s_{n})}_n$ have joint distribution under the prior
\begin{equation}
 \left[\!\!\begin{array}{c} \mathbf{x}^{(s_{n})}_n\\ \mathbf{y}_n \end{array}\!\!\right] \sim \mathcal{N}\left( \left[\!\begin{array}{c} m^{\varphi ^{(s_n)}}_{\mathbf{t}_n}\\  m^{\varphi ^{(s_n)}}_{\mathbf{t}^w_n} \end{array}\!\right],\left[ \begin{array}{cc} K^{\varphi ^{(s_n)}}_{\mathbf{t}_n,\mathbf{t}_n} & K^{\varphi ^{(s_n)}}_{\mathbf{t}_n,\mathbf{t}^w_n}\\ K^{\varphi ^{(s_n)}}_{\mathbf{t}^w_n,\mathbf{t}_n} & K^{\varphi ^{(s_n)}}_{\mathbf{t}^w_n,\mathbf{t}^w_n} \end{array}\right] \right) \label{eq:joint_dist_x}
\end{equation}
where 
\begin{align}
m^{\varphi ^{(s_n)}}_{\mathbf{t}} & =K^{\theta ^{(s_{n})}}_{\mathbf{t},\mathbf{t}_n}K^{\theta ^{(s_{n})}-1}_{\mathbf{t}_n,\mathbf{t}_n}\mathbb{E}[\mathbf{x}^{(s_{n})}_n], \nonumber \\
K^{\varphi ^{(s_n)}}_{\mathbf{t},\mathbf{t}'} &=K^{\theta ^{(s_{n})}}_{\mathbf{t},\mathbf{t}'}-K^{\theta ^{(s_{n})}}_{\mathbf{t},\mathbf{t}_n}K^{\theta ^{(s_{n})}-1}_{\mathbf{t}_n,\mathbf{t}_n}K^{\theta ^{(s_{n})}}_{\mathbf{t}_n,\mathbf{t}'} \nonumber \\
\hspace{0.5cm} &+ K^{\theta ^{(s_{n})}}_{\mathbf{t},\mathbf{t}_n}K^{\theta ^{(s_{n})}-1}_{\mathbf{t}_n,\mathbf{t}_n}\cov(\mathbf{x}_n^{(s_n)})K^{\theta ^{(s_{n})}-1}_{\mathbf{t}_n,\mathbf{t}_n}K^{\theta ^{(s_{n})}}_{\mathbf{t}_n,\mathbf{t}'}.
\end{align}
Ultimately, a vector of observations $\mathbf{y}_n$ is generated by marginalization
\begin{equation}
\mathbf{y}_n \sim \mathcal{N}(m^{\varphi ^{(s_n)}}_{\mathbf{t}^w_n}, K^{\varphi ^{(s_n)}}_{\mathbf{t}^w_n,\mathbf{t}^w_n})
\end{equation}

\section{Variational inference}
\label{sec:variational_inference}
We now focus on estimating the posterior probabilities of the hidden variables of the model $\mathcal{M}$ conditional on the set of observed variables given as a set of time sequences $\mathcal{Y}=\{\mathbf{y}_1,...,\mathbf{y}_N\}$. Our goal is therefore to infer the posterior distribution $p(\mathcal{M}|\mathcal{Y})$ where $\mathcal{M}$ denotes the collection of variables $\mathcal{M} = \{\beta, \pi^{j}, \phi^{j}, s_{n}, \mathbf{f}_{n},  \mathbf{x}_{n}, \mathbf{g}_{n}\}$ with $j=1,\dots,\infty$ and $n=1,\dots, N$. From Bayes' theorem
\begin{equation}
p(\mathcal{M}|\mathcal{Y}) \propto p(\mathcal{Y}|\mathcal{M}) p(\mathcal{M}),
\end{equation}
When conditioning on the observations, two main sources of intractability can be identified in computing the posterior: first, the switching variable induces a coupling between all linear dynamics, leading to an exponential growth of the Gaussian terms involved in the posterior distribution \cite{ghahramani_variational_2000}; second, as in many hierarchical Bayesian models, the posterior in HDP is not available in closed form \cite{teh_2006}. Previous approaches to infer the posterior for the HDP-HMM model have used Gibbs sampling \cite{fox_2011}, showing slow convergence and limited scalability, or stochastic variational inference \cite{johnson_stochastic_2014}, easily scaling to massive data but leading to poor accuracy as the latent space dimension increases \cite{dhaka_2020}. Following \cite{hughes_2015}, we adopt a scalable variational inference approach that allows us to exploit the sequential structure of the model and hence is easily applicable to streaming data. 

Variational inference turns the inference problem into an optimisation problem \cite{bishop_pattern_2006, blei_2017}, whereby we approximate the intractable true posterior with the distribution from a tractable family $\mathcal{Q}$ that minimises the Kullback-Leibler (KL) divergence to the true posterior $q^{*}=\mathrm{argmin}_{q\in\mathcal{Q}} \mathrm{KL}(q||p)$. Since $\log p(\mathcal{Y})=\mathcal{L}(q)+\mathrm{KL}(q||p)$, we can equivalently seek $q^*$ to maximize the lower bound of the log marginal evidence: 
\begin{equation}
\mathcal{L}(q) = \mathbb{E}_{q}\left[\log p(\mathcal{M},\mathcal{Y}) - \log q(\mathcal{M})\right]
\label{eq:elbo}
\end{equation}
As stated in \cite{saul_1995}, a natural approach is to consider a {\em mean-field variational family} that factors according to tractable substructures where exact inference algorithms can be used. We propose the factorized distribution $q(\mathcal{\cdot})=q(\beta)q(\pi)q(\phi)q(\mathcal{S})q(\mathcal{F})q(\mathcal{X})q(\mathcal{G})$, with global factors $q(\beta)$, $q(\pi)$, $q(\phi)$, and local factors $q(\mathcal{S})$, $q(\mathcal{F})$, $q(\mathcal{X})$, $q(\mathcal{G})$, where $\mathcal{S}=\{s_1,...,s_N\}$, $\mathcal{F}=\{\mathbf{f}_1,...,\mathbf{f}_N\}$, $\mathcal{X}=\{\mathbf{x}_1,...,\mathbf{x}_N\}$ and $\mathcal{G}=\{\mathbf{g}_1,...,\mathbf{g}_N\}$. While the objective of optimizing (\ref{eq:elbo}) is not convex, a local maximum can be achieved by a coordinate ascent procedure, where we iteratively optimize each factor while holding the others fixed \cite{bishop_pattern_2006}. For each of the factors $\mathbf{z}_i\in \mathcal{M}$, the optimal $q_i(\mathbf{z}_i)$ is then proportional to the exponentiated expected log of the joint distribution
\begin{equation}
q^{*}_j(\mathbf{z}_j) \propto \operatorname{exp}\{\mathbb{E}_{i\neq j} [\log p(\mathcal{M},\mathcal{Y})]\}
\end{equation}

A common approach to approximate the posterior in HDP models is to use a truncated variational distribution \cite{bryant_2012}, such that, for a truncation parameter $K$ representing the active clusters, we fix $q(s_n)=0$ for any $s_n>K$. Thus, we can initialise with small $K$ and let the inference discover new clusters as it progresses. Truncation only affects the form of local factors; global factors with indices greater than $K$ are conditionally independent of the observations and only differ from their prior under $p(\cdot)$ for the first $K$ elements.

{\em Factor} $q(\mathcal{S})$. We propose the usual factorisation for discrete Markov chains \cite{ghahramani_variational_2000,beal_2003}. We are interested in the functional dependence of the generative model on the variable $\mathcal{S}$, so the log of the optimized factor is given by
\begin{align}
\log q^*(\mathcal{S}) &=\mathbb{E}_{\pi}[\log p(\mathcal{S}|\pi)] + \mathbb{E}_{\mathcal{X},\mathcal{G}} [\log p(\mathcal{Y}|\mathcal{S},\mathcal{X},\mathcal{G})]+\mathrm{const.} \nonumber \\
& = \sum^{N}_{n=1}\sum _{j=1}^{K} \sum _{k=1}^{K} s_{n-1,j} s_{nk} \log \zeta_{njk}+ \mathrm{const.}
\end{align}
where $s_{nk}=1$ if the switching variable is in state $k$ at time $n$ and $0$ otherwise. The initial term for $n=0$ is defined as $s_{0k}\zeta_{00k}$ with $\zeta_{00k} = \mathbb{E}[\log \pi_{0k}]$ acting as an initial state distribution. For $n>0$, 
\begin{align}
\log \zeta_{njk} &= \mathbb{E}[\log \pi_{jk}] + \frac{1}{2} \mathbb{E}[\log |K^{x_n^k}_{\mathbf{g}^k_n,\mathbf{g}^k_n}|] - \frac{|\mathbf{y}_n|}{2}\log (2\pi) \nonumber \\
&\hspace{-0.5cm} - \frac{1}{2} \mathbb{E}_{{x_n^k},\mathbf{g}^k_n} [(\mathbf{y}_n-m_{x_n^k}(\mathbf{g}^k_n))^T  K^{x_n^k-1}_{\mathbf{g}^k_n,\mathbf{g}^k_n}(\mathbf{y}_n-m_{x_n^k}(\mathbf{g}^k_n))] \label{eq:zeta}
\end{align}
where, 
\begin{equation}
\begin{array}{ll}
     m_{x_n^k}(\mathbf{g}^k_n) & = K^{\theta ^{(k)}}_{\mathbf{g}^k_n,\mathbf{t}_n}K^{\theta ^{(k)}-1}_{\mathbf{t}_n,\mathbf{t}_n}\mathbf{x}^{(k)}_n\\
     K^{x_n^k}_{\mathbf{g}^k_n,\mathbf{g}^k_n} &  = K^{\theta ^{(k)}}_{\mathbf{g}^k_n,\mathbf{g}^k_n}-K^{\theta ^{(k)}}_{\mathbf{g}^k_n,\mathbf{t}_n}K^{\theta ^{(k)}-1}_{\mathbf{t}_n,\mathbf{t}_n}K^{\theta ^{(k)}}_{\mathbf{t}_n,\mathbf{g}^k_n}.
\end{array}
\end{equation}
The expectation over $\mathbf{g}_n^k$ results from a point estimate approximation of the warping function (see below).  We obtain
\begin{equation}
q^*(\mathcal{S})=\prod ^N_{n=1}\prod ^K_{j=1}\prod ^K_{k=1} \xi_{njk}^{s_{n-1,j} s_{nk}},
\end{equation}
where $\xi_{njk}=\zeta_{njk}/\sum _{k=1}^K \zeta_{njk}$ for ensuring normalization. The optimized factor adopts the same form as the prior distribution $p(\mathcal{S}|\pi)$, as expected. The quantities 
\begin{equation}
\mathbb{E}[s_{nk}]=\sum^K_{j=1} \xi_{njk}=r_{nk},
\end{equation}
play the usual role of responsibilities. Moreover, the quantities $\xi_{njk}$ can be interpreted as the probability of being in state $j$ at time $n-1$, and state $k$ at time $n$, and hence the quantities $r_{nk}$ can be interpreted as the expected number of times that state $k$ is visited at time $n$, as it is common in HMM literature.

Following \cite{bishop_pattern_2006}, we define some statistics weighted by the responsibilities: $N_k=\sum_{n=1}^N r_{nk}$, and $N_{jk}=\sum _{n=1}^N \xi_{njk}$.

{\em Factor} $q(\beta)$. The update to $q(\beta)$ is not conjugate given the other factors. The usual approach for $q(\beta)$, a simple point estimate $q(\beta)=\delta_{\beta^*}(\beta)$ \cite{johnson_stochastic_2014, fox_hdp-hmm_2008}, has proven not to penalize empty clusters \cite{hughes_2015}, and it is suggested instead a proper Beta distribution over each stick breaking weight $v_k$:
\begin{equation}
q^*(v|\lambda,\eta)=\prod ^{\infty}_{k=1} \operatorname{Beta}(v_k|\lambda _k\eta_k,(1-\lambda_k)\eta_k), \label{eq:param_stick_breaking}
\end{equation}
where the hyperparameter $\lambda_k \in (0,1)$ establishes the mean of $v_k$ and $\eta_k>0$ controls its variance.


{\em Factor} $q(\pi)$. It is convenient to rewrite $\pi^{(j)}$ as a finite partition $[\pi_{j1},...,\pi_{jK},\pi_{jK+1}]\sim \operatorname{Dir}(\pi_j|\alpha\beta_1,...,\alpha\beta_{K+1})$, where $\pi_{jK+1}\triangleq \sum^{\infty}_{h=K+1}\pi_{jh}$ represents the aggregate mass of all inactive clusters and $\beta_{K+1}\triangleq \sum^{\infty}_{h=K+1}\beta_{h}$. The log of the optimized factor is given by 
\begin{align}
\log q^*(\pi) &= \mathbb{E}_{\mathcal{S}}[\log p(\mathcal{S}|\pi)] + \mathbb{E}_{v}[\log p(\pi|v)] + \mathrm{const.} \nonumber \\
&= \sum_{j=1}^{K+1} \sum_{k=1}^{K+1}\left[\sum_{n=1}^N \xi_{njk}+\alpha \mathbb{E}_v[\beta_k] -1 \right] \log \pi_{jk} \nonumber \\
&+ \mathrm{const.}
\end{align}
We recognize a product of Dirichlet distributions with updated parameters 
\begin{equation}
q^*(\pi) = \prod_{j=1}^{K+1} \operatorname{Dir} (\pi_j|\kappa_{j1},...,\kappa_{jK+1}), 
\label{eq:kappa}
\end{equation}
where $\kappa_{jk}=\alpha \mathbb{E}_v[\beta_k]+N_{jk}$ and a term $N_{jk}$ is added to the prior providing the expected number of times that the observations are generated after a transition $\pi_{jk}$. Note that $N_{jK+1}=0$ since the inactive clusters have no elements. The expectations $\mathbb{E}_v[\beta_k]$ are computed from equation (\ref{eq:stick_breaking}). The parameterization of $q^*(\pi)$ allows the computation of the expectation involved in equation (\ref{eq:zeta}) as
\begin{equation}
    \mathbb{E}\left[\log \pi_{j k}\right] = \psi(\kappa_{j k}) - \psi\left(\sum_{k=1}^{K+1}\kappa_{j k}\right),
\label{eq:transition_parameterized}
\end{equation}
where $\psi(\cdot)$ is the digamma function.


{\em Factor} $q(\mathcal{G})$. The log of the optimized factor is given by
\begin{equation}
\log q^*(\mathcal{G}) = \log p(\mathcal{G}) + \mathbb{E}_{\mathcal{S},\mathcal{X}}[\log p(\mathcal{Y}|\mathcal{S},\mathcal{X},\mathcal{G})] + \mathrm{const.}
\label{eq:g}
\end{equation}
The second term at the right-hand side of (\ref{eq:g}) involves a composition of GPs $(x \circ g)$, which does not have a closed-form solution. Following \cite{lawrence_2007}, we use a {\em maximum a posteriori} approach, maximizing
\begin{align}
\log q^*(\mathbf{g}_n^k|\mathbf{y}_n,\mathbf{t}_n) &= \log p(\mathbf{y}_n|m_{x_n^k}(\mathbf{g}_n^k), K^{x_n^k}_{\mathbf{g}^k_n,\mathbf{g}^k_n}) \nonumber \\
&+  \log p(\mathbf{g}^k_n|\mathbf{t}_n) + \mathrm{const.}
\label{eq:maximizing_g}
\end{align}
where we have assumed an independent time warping affecting each time sequence for each different cluster. This time warping is a realization $\mathbf{g}^k_n\sim \mathcal{N}(m_{\vartheta}^k(\mathbf{t}_n),K^{\vartheta ^k}_{\mathbf{t}_n,\mathbf{t}_n})$ of the corresponding GP. Monotonicity is mandatory for a warping function, and we adopt a parametrisation of each $\mathbf{g}^k_n$ by using a set of auxiliary variables $\mathbf{a}^k_n\in \mathbb{R}^Q$ such that $g^k_{n,q}=Q\sum^q_{i=1}[\operatorname{softmax}(\mathbf{a}^k_n)]_i$, with $\mathbf{a}^k_n\sim \mathcal{N}(\mathbf{0}, I)$, and appropriate hyperparameters $\vartheta$ to ensure smoothness.


{\em Factor} $q(\phi)$. As stated in \cite{west_1997}, the matrix-normal inverse-Wishart prior provides conjugates to the likelihood equations  (\ref{eq:latent_trans}) and (\ref{eq:emission_trans}), which can be interpreted as linear regression problems that can be independently solved for each cluster. For convenience, we deal with the two components of the LDS separately, assuming the usual prior
\begin{align}
p(A^k,\Sigma^k_\omega) &=p(A^k|\Sigma^k_\omega)p(\Sigma^k_\omega) \nonumber \\
&= \mathcal{MN}(M^k_A,W^k_A,V^k_A)\mathcal{IW}(\nu_{0},S^k_\omega).
\end{align}
Thus, $\mathcal{MN}(M^k_A,W^k_A,V^k_A)$ denotes a matrix-normal prior with mean matrix $M^k_A$, left covariance $(V^k_A)^{-1}$ and right covariance $W^k_A$, and $\mathcal{IW}(\nu_0,S^k_\omega)$ denotes an inverse-Wishart prior with $\nu_0$ degrees of freedom and scale matrix $S^k_\omega$. The log of the optimized factor is given by
\begin{align}
\log q^*(A^k,\Sigma^k_\omega) &= \sum_{m=1}^{n_k}\mathbb{E}_{\mathbf{f}}[\log p(\mathbf{f}^k_{m}|\mathbf{f}^k_{m-1},A^k,\Sigma^k_\omega)] \nonumber \\
& + \log p(A^k,\Sigma^k_\omega) + \mathrm{const.}
\end{align}
where the index $m$ runs through the elements of the cluster $k$. The optimized factor can be derived as a posterior distribution \cite{fox_2011}
\begin{align}
q^*(A^k,\Sigma^k_{\omega}) &=\mathcal{MN}(\Psi_{m,m-1}\Psi^{-1}_{m-1,m-1},\Sigma^k_{\omega},\Psi_{m-1,m-1}) \nonumber \\
& \hspace{0.5cm} \mathcal{IW}(N_k+\nu_0,\Psi_{m|m-1}+S^k_\omega)
\end{align}
where
\begin{align}
\Psi_{m-1,m-1} &= \mathbb{E}[\mathbf{f}^k_{m-1}\mathbf{f}^{kT}_{m-1}]+V^k_A, \nonumber \\
\Psi_{m,m-1} &= \mathbb{E}[\mathbf{f}^k_{m}\mathbf{f}_{m-1}^{kT}]+M^k_AV^k_A, \nonumber \\
\Psi_{m,m} &= \mathbb{E}[\mathbf{f}^k_{m}\mathbf{f}_{m}^{kT}]+M^k_AV^k_AM_A^{kT}, \nonumber \\
\Psi_{m|m-1} &=\Psi_{m,m}-\Psi_{m,m-1}\Psi_{m-1,m-1}^{-1}\Psi_{m,m-1}^T \label{eq:dynamical_param_estimators}
\end{align}
We denote the new posteriors by $\mathcal{MN}(\widetilde{M}^k_A,\widetilde{W}^k_A,\widetilde{V}^k_A)$ and $\mathcal{IW}(\tilde{\nu}_{0},\widetilde{S}^k_\omega)$ where $\tilde{\nu}_0=N_k+\nu_0$. The optimized factor for the parameters of (\ref{eq:emission_trans}) can be obtained analogously, resulting in $\mathcal{MN}(\widetilde{M}^k_C,\widetilde{W}^k_C,\widetilde{V}^k_C)$ and $\mathcal{IW}(\tilde{\varsigma}_{0},\widetilde{S}^k_\epsilon)$ where $\tilde{\varsigma}_0=N_k+\varsigma_0$.

{\em Factor} $q(\mathcal{X},\mathcal{F})$. 
The variational posterior distribution $q^*(\mathcal{X},\mathcal{F})$ can be written in the form $q^*(\mathcal{X},\mathcal{F})=q^*(\mathcal{X}|\mathcal{F})q^*(\mathcal{F})$. The log of the optimized factor is given by
\begin{align}
\log q^*(\mathcal{X},\mathcal{F}) &= \mathbb{E}_{\phi}[\log p(\mathcal{X}|\mathcal{F},\phi)] + \mathbb{E}_{\phi}[\log p(\mathcal{F}|\phi)] \nonumber \\
&+ \mathbb{E}_{\mathcal{S},\mathcal{G}} [\log p(\mathcal{Y}|\mathcal{S},\mathcal{X},\mathcal{G})] + \mathrm{const.}
\end{align}
A standard message passing algorithm can be applied to each cluster, and we can identify the Kalman filtering and smoothing equations. As a result of the forward equations we obtain
\begin{equation}
q(\mathbf{f}^k_n)=\mathcal{N}(\mathbf{f}^k_n|\mu_{\mathbf{f}^k_n},\Sigma_{\mathbf{f}^k_n})
\end{equation}
where
\begin{align}
\mu_{\mathbf{f}^k_n} &\triangleq \mathbb{E}[\mathbf{f}_n^k]=\widetilde{M}_A^k \mu_{\mathbf{f}^k_{n-1}} +  H_n^k (\mathbf{y}_{n} - \hat{K}_n^k\widetilde{M}_C^k\widetilde{M}_A^k\mu_{\mathbf{f}^k_{n-1}}) \nonumber \\
\Sigma_{\mathbf{f}^k_n} &\triangleq \cov(\mathbf{f}_n^k) = (I-H_n^k\hat{K}_n^k\widetilde{M}_C^k) P_{n-1}^k, 
\label{eq:forward_message}
\end{align}
with
\begin{align}
H_n^k &=P_{n-1}^k (\hat{K}_n^k \widetilde{M}_C^k)^T (K^{\theta^k}_{\mathbf{g}_n^k,\mathbf{g}_n^k}-\hat{K}_n^k K^{\theta^k}_{\mathbf{t}_n,\mathbf{g}_n^k}+ \hat{K}_n^k \Phi_n^k \hat{K}_n^k)^{-1}, \nonumber \\
\Phi_n^k &= (\widetilde{M}_C^kP_{n-1}^k\widetilde{M}_C^{kT}+\widetilde{S}_{\epsilon}^k)\oslash r_{nk}, \nonumber \\
P_{n-1}^k &= \widetilde{M}_A^k \Sigma_{\mathbf{f}_{n-1}^k} \widetilde{M}_A^{kT} + \widetilde{S}_{\omega}^k, \nonumber \\
\hat{K}_n^k &= K^{\theta^k}_{\mathbf{g}_n^k,\mathbf{t}_n} K^{\theta^k -1}_{\mathbf{t}_n,\mathbf{t}_n}.
\end{align}
In the above equations, the index $n$ represents $n_k$ when it comes with some descriptor of cluster $k$. The initial condition for this message scheme is $q^*(\mathbf{f}_0^k)=\mathcal{N}(m_{\theta^k}(t_0),K^{\theta^k}_{\mathbf{t}_0,\mathbf{t}_0})$. The matrix $H_n^k$ is the well-known Kalman gain matrix. The symbol $\oslash$ denotes the element-wise division. The matrix $\hat{K}_n^k$ represents the KL-optimal projection introduced in equation (\ref{eq:seeger_approach}), to compute the posteriors in locations other than those of the observations, allowing us to efficiently reduce the computation of latent GPs to a small set of inducing points \cite{rasmussen_gaussian_2006}. Note that in the limit $r_{nk}\rightarrow 0$, the Kalman gain goes to zero, as it is expected. We finally obtain each posterior factor after applying the backward equations

\begin{equation}
q^*(\mathbf{f}_n^k)=\mathcal{N}(\mathbf{f}_n^k|\tilde{\mu}_{\mathbf{f}_n^k},\widetilde{\Sigma}_{\mathbf{f}_n^k})
\label{eq:forward_backward}
\end{equation}
where
\begin{align}
\tilde{\mu}_{\mathbf{f}_n^k} &\triangleq \mu_{\mathbf{f}^k_n} + J_n^k (\tilde{\mu}_{\mathbf{f}^k_{n+1}} - \widetilde{M}_A^k \mu_{\mathbf{f}^k_n}) \nonumber \\
\widetilde{\Sigma}_{\mathbf{f}_n^k} &\triangleq \Sigma_{\mathbf{f}^k_n} + J_n^k (\widetilde{\Sigma}_{\mathbf{f}^k_{n+1}} - P_n^k) J_n^{kT}
\end{align}
with
\begin{equation}
J_n^k= \Sigma_{\mathbf{f}^k_n}\widetilde{M}_A^{kT} (P_n^{k})^{-1}
\end{equation}
The $q^*(\mathcal{X}|\mathcal{F})$ factor can be obtained by projection under the respective dynamic equations of each cluster
\begin{equation}
q^*(\mathbf{x}_n^k|\tilde{\mathbf{f}}_n^k) = \mathcal{N}(\mathbf{x}_n^k|\widetilde{M}_C^k {\tilde{\mathbf{f}}_n^k},\widetilde{S}_{\epsilon}^k),
\end{equation}
and $q^*(\mathcal{X})$ can be computed by marginalization
\begin{equation}
q^*(\mathbf{x}_n^k) = \mathcal{N}(\mathbf{x}_n^k|\tilde{\mu}_{\mathbf{x}_n^k},\widetilde{\Sigma}_{\mathbf{x}_n^k}),
\end{equation}
where
\begin{align}
\tilde{\mu}_{\mathbf{x}_n^k} &\triangleq \widetilde{M}_C^k \tilde{\mu}_{\mathbf{f}^k_n} \nonumber \\
\widetilde{\Sigma}_{\mathbf{x}_n^k} &\triangleq \widetilde{S}_{\epsilon}^{k} + \widetilde{M}_C^{kT}\widetilde{\Sigma}_{\mathbf{f}^k_n}\widetilde{M}_C^k
\end{align}
Let us notice that the posterior $\mathbf{x}_n^k$ can show heteroscedastic behaviour, in so far as previous observations in the same LDS lead to non-diagonal covariance matrix, as will be seen later on with real examples.

\subsection{Variational lower bound}
The lower bound of the log marginal evidence can be decomposed as
\begin{equation}
\mathcal{L} =\mathcal{L}_{OBS} + \mathcal{L}_{HDP} + \mathbb{H}[q(\mathcal{S})],
\label{eq:lower_bound}
\end{equation}
which, due to the factorization of the variational family, is given by
\begin{align}
    \mathcal{L}_{OBS} &= \mathbb{E}_{q}\!\left[\log p\!\left(\mathcal{Y}| \mathcal{X},\mathcal{G}, \mathcal{S}\right) + \log \frac{p(\mathcal{X}|\mathcal{F},\phi)p(\mathcal{F}|\phi)}{q(\mathcal{X} |\mathcal{F})q(\mathcal{F})}\right. \nonumber\\
    &+ \left. \log \frac{p(\mathcal{\phi})}{q(\mathcal{\phi})} + \log \frac{p(\mathcal{G})}{q(\mathcal{G})}\right], \\
    \mathcal{L}_{HDP} &= \mathbb{E}_{q}\! \left[\log\frac{p(\mathcal{S}|\pi)p(\pi|v,\alpha)}{q\!\left(\mathcal{S}\right) q\!\left(\pi\right)} + \log \frac{p(v|\gamma)}{q(v|\lambda,\eta)} \right], \label{eq:L_HDP}
\end{align}
where we have omitted the $*$ superscript on the $q$ distributions. $\mathbb{H}[q(\mathcal{S})]$ is defined as the Shannon entropy of the distribution $q(\mathcal{S})$. We use equation (\ref{eq:lower_bound}) to check convergence in the coordinate ascent procedure. A full derivation of the lower bound can be found in the supplementary material. 

\subsection{Variational algorithm}
The optimization of the evidence lower bound involves an iterative re-estimation procedure following a bottom-up strategy from local to global variables. It is important to note that the initial model $p(\mathcal{M})$ can be a completely uninformed model or a pre-trained model. 


\subsection{Off-line variational algorithm}
The algorithm iterates through the entire dataset $\mathcal{Y}$, ensuring the convergence of the local step before updating the global variational factors. The truncation level $K$ varies during inference until variational lower bound convergence, as stated in the Algorithm \ref{alg:offline_algorithm}.
\begin{algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\caption{Off-line variational inference}\label{alg:offline_algorithm}
\begin{algorithmic}
\REQUIRE A dataset $\mathcal{Y}=\{(\mathbf{t}_n,\mathbf{y}_n)\}_{n=1}^N$ of time sequences
\REQUIRE A model $p(\mathcal{M},\mathcal{Y})$
\ENSURE The variational density $q(\mathcal{M}|\mathcal{Y})$
\WHILE{$\mathcal{L}$ has not converged}
\WHILE{$\mathcal{L}_{OBS}$ has not converged}
\FORALL {$k=1,2,...$}
\STATE // local update
\FORALL {$n=1,...,N$}
\STATE compute $q(s_{nk})$ using eq. \eqref{eq:zeta}
\ENDFOR
\FORALL{$m=1,...,n_k$}
\STATE compute $q(\mathbf{g}_m^k)$ numerically using eq. \eqref{eq:maximizing_g}
\STATE compute $q(\mathbf{f}_{m}^{k})$ and $q(\mathbf{x}_{m}^{k})$ using eq. \eqref{eq:forward_backward}
\ENDFOR
\STATE compute $q(\phi^{k})$ using eq. \eqref{eq:dynamical_param_estimators}
\ENDFOR
\ENDWHILE
\FORALL {$k=1,2,...$}
\STATE // global update
\STATE compute $q(\pi_{k})$ using eq. \eqref{eq:kappa}
\STATE compute $q(v_{k})$ numerically using eq. \eqref{eq:L_HDP}
\ENDFOR
\STATE compute $\mathcal{L} = \mathcal{L}_{OBS} + \mathcal{L}_{HDP} + \mathbb{H}[q(\mathcal{S})]$
\ENDWHILE
\RETURN $q(\mathcal{M}|\mathcal{Y})$
\end{algorithmic}
\end{algorithm}

\subsection{On-line variational algorithm}
The algorithm handles each instance of the data set $\mathcal{Y}$ in an ordered fashion. Using only one sample at a time limits the computation of the variational objective, resulting in unbiased noisy estimates, which do not guarantee an improvement over the variational bound evaluated using the entire dataset. In return, a single pass over all the data is needed, as stated in Algorithm \ref{alg:online_algorithm}.

\begin{algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\caption{On-line variational inference}\label{alg:online_algorithm}
\begin{algorithmic}
\REQUIRE A dataset $\mathcal{Y}=\{(\mathbf{t}_n,\mathbf{y}_n)\}_{n=1}^N$ of time sequences
\REQUIRE A model $p(\mathcal{M},\mathcal{Y})$
\ENSURE The variational density $q(\mathcal{M}|\mathcal{Y})$
\FORALL {$(\mathbf{t}_n,\mathbf{y}_n)\in \mathcal{Y}$}
\WHILE{$\mathcal{L}$ has not converged}
\FORALL {$k=1,2,...$}
\STATE // local update
\STATE compute $q(\mathbf{g}_n^k)$ numerically using eq. \eqref{eq:maximizing_g}
\STATE compute $q(s_{nk})$ using eq. \eqref{eq:zeta}
\FORALL {$m=1,...,n_k$}
\STATE compute $q(\mathbf{f}_{m}^{k})$ and $q(\mathbf{x}_{m}^{k})$ using eq. \eqref{eq:forward_backward}
\ENDFOR
\STATE // global update
\STATE compute $q(\pi_{k})$ using eq. \eqref{eq:kappa}
\STATE compute $q(v_{k})$ numerically using eq. \eqref{eq:L_HDP}
\STATE compute $q(\phi^{k})$ using eq. \eqref{eq:dynamical_param_estimators}
\ENDFOR
\STATE compute $\mathcal{L} = \mathcal{L}_{OBS} + \mathcal{L}_{HDP} + \mathbb{H}[q(\mathcal{S})]$
\ENDWHILE
\ENDFOR
\RETURN $q(\mathcal{M}|\mathcal{Y})$
\end{algorithmic}
\end{algorithm}

\section{Hyperparameter initialization}
\label{sec:hyperparameters}
Hyperparameters refer to variables that are not estimated in the variational inference process. These variables play a key role in terms of both the behavior and performance of the model. The main aim of this section is to explore the impact of these hyperparameters on cluster formation and evolution, ultimately conditioning the number of clusters. This analysis is intended to aid in fine-tuning the model to achieve the desired performance.

The HDP is controlled by two concentration parameters. The parameter $\gamma$ controls the number of effective clusters. A larger $\gamma$ implies a slower decay of stick-breaking weights $\beta_1, \beta_2, \beta_3,...$ and thus more clusters. The parameter $\alpha$ controls the similarity of the transition distribution from each state to $\beta$ distribution.

Each LDS is controlled by the parameters of the corresponding matrix-normal inverse-Wishart priors, both for the evolution of $\mathbf{f}^k_n$ and $\mathbf{x}^k_n$. Initialization of these priors takes place whenever a new cluster is produced, hence we set $M_A=I$ and $M_C=I$, so that each new LDS will be provided with loosely constraints at its onset. 

The covariance priors $S_\omega$ and $S_\epsilon$, and particularly, the magnitude of disparity between them, plays a key role in managing the stability-plasticity balance of the resulting model, namely the preference for enhancing the ability to adapt the current clusters to new data (plasticity) versus the preference for enhancing the capacity to retain previously learnt clusters (stability). As $S_{\omega} > S_{\epsilon}$ the model tends to explain a succession of sequences by making evolve each cluster according to the respective linear dynamics, hence enhancing plasticity (see Figure \ref{fig:plasticity_model}). Conversely, as $S_{\omega} < S_{\epsilon}$ the model tends to explain the same succession of sequences by considering their evolution as the result of some sort of variability around an average sequence (see Figure \ref{fig:stability_model}). So far we are assuming that all the clusters raised by the model share the same covariance priors, and therefore a single stability-plasticity criterion is enforced. But the availability of previous knowledge about the problem can enable us to specify different covariance priors for different categories, in a semisupervised fashion.

\begin{figure}[ht]
\centering
    \subfloat[]{
    \includegraphics[width=0.8\columnwidth]{figures/balance_stable_plast/Plasticity_model_r114_onlyGamma.png}
    }
\\
    \subfloat[]{
    \includegraphics[width=0.8\columnwidth]{figures/balance_stable_plast/Plasticity_model_r114_noGamma.png}
    }
\caption{Model enhancing plasticity. A total of 50 heartbeats are represented, manually annotated as Normal, from record 114 of the MIT-BIH Arrhythmia Database [11:30.000,12:20.000]. The resulting GP $f(t)$ (a) and $x(t)$ (b), after evolving along with these sequences, was sampled over a dense time axis. The mean of the posterior is shown as a black line. A 95\% confidence region for the posterior is shown in cyan blue. Kernel hyperparameters for $k_{\theta}(t,t')$ are $\sigma^{2}_{f} = 16.0^{2}$, $l = 2.5$, $\sigma^{2}_{n} = 5.0^{2}$. LDS priors: $S_{\omega} = 10.0^{2}I, S_{\epsilon} = 5.0^{2}I$, where $I$ is the identity matrix. For clarity purposes, no warping function has been used. This model embraces the totality of the heartbeats in a single cluster.}
\label{fig:plasticity_model}
\end{figure}

\begin{figure}[ht]
\centering
    \subfloat[]{  
    \includegraphics[width=0.8\columnwidth]{figures/balance_stable_plast/Stability_model_r114_onlyGamma.png}
    }
\\
    \subfloat[]{
    \includegraphics[width=0.8\columnwidth]{figures/balance_stable_plast/Stability_model_r114_noGamma.png}
    }
\caption{Model enhancing stability. The same 50 heartbeats as of Figure \ref{fig:plasticity_model} are represented, from record 114 of the MIT-BIH Arrhythmia Database. The resulting GP $f(t)$ (a) and $x(t)$ (b), after evolving along with these sequences, was sampled over a dense time axis. Kernel hyperparameters for $k_{\theta}(t,t')$ are $\sigma^{2}_{f} = 16.0^{2}$, $l = 2.7$, $\sigma^{2}_{n} = 5.0^{2}$. LDS priors: $S_{\omega} = 4.0^{2}I, S_{\epsilon} = 15.0^{2}I$. For clarity purposes no warping function has been used. Again, the model embraces the totality of the heartbeats in a single cluster.}
\label{fig:stability_model}
\end{figure}

We consistently set $W_A=S_\omega$ and $W_C=S_\epsilon$, and the inverse-Wishart portions are given $\nu_0=p+2$ and $\varsigma_0=q+2$ degrees of freedom, respectively.

Each LDS models a linear evolution of a GP with prior $f^{k}(t) \sim \mathcal{GP}(m_{\theta ^{k}}(t),k_{\theta ^{k}}(t,t'))$, where mean $m_{\theta ^{k}}(t)$ is usually chosen to be $\mathbf{0}$ due to symmetry. A reasonable choice for covariance function $k_{\theta ^{k}}(t,t')$ commonly used in the GP literature when no prior information is available is the squared exponential kernel, although any kernel could be used within our method \cite{rasmussen_gaussian_2006}. The squared exponential kernel has the form $k_{\theta ^{k}}(t, t') = \sigma^{2}_{f} \exp{(-\frac{1}{2 l^{2}}(t-t')^{2}}) + \sigma^{2}_{n}\delta_{t,t'}$ with hyperparameters $\theta=(\sigma_{f}, l, \sigma_{n})$. The length-scale $l$ controls the temporal variation necessary for the sequence values to appreciably change; the signal variance $\sigma^2_{f}$ controls the variation of function values from their mean; and the noise variance $\sigma^2_{n}$ controls the noise which is expected to be present in the observations. These hyperparameters are determined by maximum likelihood from the dataset. 

Each warping function models the misalignment between the observations and the predicted pseudo-observations, with prior $g^{k}(t) \sim  \mathcal{GP}(m_{\vartheta ^k}(t),k_{\vartheta ^k}(t,t'))$. The mean $m_{\vartheta ^k}(t)$ has been chosen to be $\mathbf{0}$ around the linear unit function. An squared exponential kernel has also been selected for covariance function $k_{\vartheta ^k}(t,t')$ since it yields smoothing properties. Reducing the length-scale hyperparameter or using a non-smooth kernel, like the Matérn kernel, can improve alignment functions and model time alignments with skipping events.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/evolution_model/Grid_Evolution_Models_e1302.png}
    \caption{The GP model for each one of the beats depicted in Figure \ref{fig:ecg_e1302} is shown. Our method correctly includes all the beats in the same evolving cluster, except for the fifth one that gives rise to a new cluster.}
    \label{fig:evolution_models_e1302}
\end{figure*}

\section{Applications}
\label{sec:applications}
\subsection{ECG heartbeat clustering}
Indeed, Figure \ref{fig:evolution_models_e1302} shows how a model representing a cluster generated from a first Normal beat evolves to adapt to the morphological changes that this morphology undergoeses over time, while a new different cluster is generated to represent Ventricular beats.

Still, a more thorough evaluation has been performed to test the present proposal. We have chosen the MIT-BIH Arrhythmia Database \cite{goldberger_2000}, the reference database for automatic ECG arrhythmia and heartbeat clustering and classification \cite{moody2001}. This database totals 48 excerpts of two-channel ambulatory ECG recordings, obtained from 47 subjects. Both channels have been configured by placing the electrodes on the chest. The recordings were sampled at 360 Hz for 30 minutes. All beats present in the database were annotated by at least two expert cardiologists, and assigned a class label using a 17 label set.

We adopt the common approach in ECG analysis by focusing on the QRS complex, representing the ventricular depolarization, to characterize each beat. We thus ignore the representation of the atrial activity, with low amplitude, since the noise level often precludes its detection, especially in ambulatory signals. As a consequence, beats with different atrial activity but sharing the same QRS morphology will be assigned to the same cluster. Rhythm information can be useful in some cases to circumvent this limitation, but we avoid it since we want to test the generality of our approach.

We compare our clustering performance with two previous proposals achieving state-of-the-art results. Both of them include rhythm information as part of the feature vector representing each beat for computing similarity. Authors in \cite{lagerholm2000} rely on a SOM with 25 clusters to represent the different morphologies identified in each ECG recording, obtaining a global purity of 98.49\%, a measure of the extent to which each cluster contains beats of a single class. This purity is attained by including in the representation of each beat some rhythm information. As it is expected, a fixed number of clusters leads to redundant clusters when faced with a smaller number of different morphologies and to mixed clusters with a greater number of different morphologies. Authors in \cite{castro2015} propose a context-based adaptive method, providing a different number of clusters according to the morphological variability of each record. The method exploits the available physiological knowledge in a  heuristics approach, trying to mimic the sort of criteria used by cardiologists. They obtain a global purity of 97.15\% without using rhythm information, and 98.56\% by including it. 

Table \ref{tab:clusters_per_record} shows the number of clusters generated for all the records of the MIT-BIH Arrhythmia Database by the method provided by \cite{castro2015} (excluding rhythm information) and by both the off-line and the on-line methods provided in the present proposal. A 46.3\% reduction in the number of clusters for the off-line method is observed, and a 14.3\% reduction for the on-line method. A global purity of 97.12\% is obtained by the off-line method and of 97.31\% by the on-line method. Figure \ref{fig:enter-label} illustrates the set of clusters obtained in an offline fashion for record 210.

\begin{table}[ht] 
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.1}
\caption{Number of clusters per record in MIT-BIH DB} 
\label{tab:clusters_per_record} 
\centering
\begin{tabular}{c c c c | c c c c} 
\hline \hline
Record & \cite{castro2015} & N$_{OFF}$ & N$_{ON}$ & Record & \cite{castro2015} & N$_{OFF}$ & N$_{ON}$\\ \hline
100 & 4 & 3 & 2 & 201 & 15 & 6 & 10 \\
101 & 4 & 3 & 2 & 202 & 9 & 8 & 6 \\
102 & 10 & 9 & 8 & 203 & 33 & 12 & 38 \\
103 & 10 & 5 & 2 & 205 & 14 & 7 & 9 \\
104 & 16 & 10 & 27 & 207 & 61 & 15 & 36 \\
105 & 10 & 10 & 50 & 208 & 28 & 11 & 30 \\
106 & 27 & 8 & 11 & 209 & 10 & 9 & 6 \\
107 & 11 & 3 & 11 & 210 & 27 & 9 & 11 \\
108 & 22 & 8 & 9 & 212 & 5 & 4 & 5 \\
109 & 13 & 9 & 11 & 213 & 17 & 12 & 23 \\
111 & 8 & 5 & 5 & 214 & 21 & 12 & 18 \\
112 & 4 & 2 & 2 & 215 & 16 & 7 & 10 \\
113 & 5 & 3 & 2 & 217 & 28 & 10 & 22 \\
114 & 8 & 4 & 5 & 219 & 14 & 10 & 10 \\
115 & 11 & 4 & 3 & 220 & 2 & 7 & 3 \\
116 & 10 & 7 & 9 & 221 & 14 & 9 & 6 \\
117 & 4 & 3 & 3 & 222 & 8 & 3 & 3 \\
118 & 3 & 6 & 12 & 223 & 23 & 10 & 20 \\
119 & 6 & 4 & 5 & 228 & 14 & 10 & 16 \\
121 & 5 & 5 & 6 & 230 & 3 & 3 & 6 \\
122 & 1 & 2 & 2 & 231 & 5 & 7 & 6 \\
123 & 3 & 3 & 2 & 232 & 4 & 3 & 3 \\
124 & 14 & 7 & 7 & 233 & 24 & 10 & 20 \\
200 & 20 & 13 & 22 & 234 & 5 & 5 & 3 \\\hline
&&&&TOTAL& 629 & 335 & 539 \\ \hline \hline
\\
\end{tabular}
 \centering
{Column N$_{OFF}$ stands for the number of clusters generated by the off-line strategy and N$_{ON}$ stands for the number of clusters generated by the on-line strategy}
\end{table}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/cluster_example/Rec_210_bv92_Models_Lead_1_strongerBands.png}
    \caption{Clusters identified from record 210 of the MIT-BIH Database along with the beats assigned to them. The black line denotes the mean of the posterior distribution, while the pink region represents the 95\% confidence interval for the latent process $f(\cdot)$, and the red region corresponds to the emission process $x(\cdot)$. Cluster 1 captures nearly all normal heartbeats, even in the presence of a morphological evolution. Cluster 7 represents an aberrated atrial premature morphology. The remaining clusters represent multiform premature ventricular contractions, exhibiting distinct morphologies caused by the multiplicity of activation sites within the ventricles.}
    \label{fig:enter-label}
\end{figure*}

Most of the beats in the database are annotated as normal (68.3\%) and most of the normal beats (99.7\% for the off-line method and 99.8\% for the on-line) are assigned to clusters in which they represent the majority. The most common type of error (52.5\% for the off-line method and 54.1\% for the on-line) is that abnormal beats are assigned to clusters dominated by the normal ones. The group of beats with supraventricular or nodal activation are the main source of errors, since they require to consider the atrial activity, leading to 49.5\% of errors for the off-line method and 52.9\% for the on-line method. Escape beats from recording 222 account for 6.7\% of errors for the off-line method and 7.2\% of errors for the on-line method. Those recordings performed on patients with pacemakers (102, 104 and 217) show fusion beats, resulting from the concurrent activation of the same part of the ventricle by the intrinsic beat and the impulse from the pacemaker. They account for the 7.9\% of errors for the off-line method and 9.6\% for the on-line method. In this sense, it can be argued that the smoothing properties of the squared exponential kernel yield blurred representations of pacing pulses, and hence they may be added as slight variations of previous models. Other different kernels might properly manage these signal discontinuities.

\emph{Hyperparameters.} MIT-BIH Arrhythmia Database is a compilation of a wide range of common and less common but clinically significant arrhythmias, hence demanding to cope with different morphologies in each recording. The HDP concentration parameters were therefore initialized as $\gamma=10.0$ and $\alpha=20.0$. We set the LDS covariance priors $S_{\omega} = \varrho \Sigma_{\mathbf{y}_{d-1}}$ and $S_{\epsilon} = \varrho \Sigma_{\mathbf{y}_{d}}$, where we made use of the diagonalized data variance $\Sigma_{\mathbf{y}_{d}} = \frac{1}{D} \sum_{d=0}^{D} \mathbf{y}_{d}\mathbf{y}_{d}^{T}$ and the 1-step rolling variance of the data $\Sigma_{\mathbf{y}_{d-1}} = \frac{1}{D} \sum_{d=1}^{D} (\mathbf{y}_{d}-\mathbf{y}_{d-1})(\mathbf{y}_{d}-\mathbf{y}_{d-1})^{T}$. For the on-line approach, a small calibration segment at the beginning of the recording was used ($D=20$ beats), whereas the entire data set was used for the off-line approach. The factor $\varrho$ modulates the scaling matrix, biasing towards the emergence of new clusters as $\varrho$ increases. As a rule of thumb, $\varrho=0.5$ can be advised for the on-line method, $\varrho=1.0$ for the off-line, and $\varrho=2.0$ for those recordings corresponding to patients with pacemakers (102, 104 and 217). Regarding the covariance function $k_{\theta}(t,t')$ we set the hyperparameters by maximizing the marginal likelihood on the first beat of each cluster, from a common initial $\theta=(\sigma_{f}=300.0, l=1.0, \sigma_{n}=\sqrt{S_{\epsilon}})$, where $\sigma_{f}=300.0$ corresponds to the maximum signal deviation in a common ECG recording. We also set the hyperparameters of the covariance function $k_{\vartheta}(t,t')$, by maximizing the marginal likelihood on the first beat of each cluster, from a common initial $\vartheta=(\sigma_{f}=1.0, l=4.0, \sigma_{n}=1.0)$.

\subsection{Breathing estimation}
The present approach allows us to pose new questions on the dynamics underlying the available observations. Let us focus, for instance, on the sequence of warping functions used to perform beat clustering. These warping functions capture the specific misalignment of each observed beat with respect to the predicted one, and we can wonder about whether they contain any physiological information. Indeed, the observation of some regularity in the sequence of warping functions may lead us to conjecture that the misalignments of a sequence of beats with the same morphology result to a large extent from the breathing mechanism, since the electrical activity of the heart has to pass through the lungs before it is measured by the electrodes placed on the chest. The inspiration and expiration cycle changes the volume of air, with strong dielectric properties, in the conduction pathway to the electrodes, cyclically delaying the observation of the different constituents of the heartbeat. As a consequence, the sequence of warping functions would show a correlation with the breathing cycle, allowing its estimation.

To test this hypothesis, a supervised reconstruction experiment has been devised on two public databases, the Fantasia Database \cite{Iyengar1996}, consisting of ECG and respiration recordings from 40 healthy subjects in sinus rhythm during a resting state; and the Apnea-ECG Database \cite{Penzel2000}, including 8 ECG recordings accompanied by different respiration signals and apnea annotations. We consider two time series, the respiration signal denoted by $\{(t_{s},r_{s})\}_{s=1}^S$ and the sequence of different warping segments obtained from the ECG signal, denoted by $\{(\mathbf{t}_{n},\mathbf{t}_{n}^{w})\}_{n=1}^N$. Let us note that this last sequence is not defined for the whole temporal range of the respiration signal, as the warping function is just computed for each QRS temporal interval. The sequence of warping segments is obtained by the Equation (\ref{eq:maximizing_g}). Thus, an initial GP model $\mathbf{f_1}$ is computed from the first normal beat $(\mathbf{t}_1,\mathbf{y}_{1})$ of the ECG recording and then an alignment function is computed for each one of the following beats. To emphasize the alignment differences between consecutive beats, the initial model has not been evolved with new beats, hence preventing the LDS from absorbing some of the alignment variability.

We focus on those respiration signal segments $\{(\mathbf{t}_n,\mathbf{r}_{n})\}_{n=1}^N$ for which there is a warping segment in the same time interval $(\mathbf{t}_n,\mathbf{r}_n)=\{(t_s,r_s):t_s\in\mathbf{t}_n\}$, so trying to find a relation between them can be approached as a usual supervised estimation problem. Assuming linearity as a naive approach, estimation can be posed as a least squares fitting problem, where $\mathbf{r}_{n} = M_{w} \mathbf{t}_{n}^{w}$. For testing the robustness of this approach we estimate the matrix $M_{w}$ from a training set of both respiration and warping segments at the beginning of the recording and reconstruct the respiration signal from then on with subsequent smoothing. Estimation results from some respiration recordings are shown in Figures \ref{fig:breathing_test_fy01}, \ref{fig:breathing_test_f1o03} and \ref{fig:breathing_test_a03}.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/breathing/breath_test_f1y01_nolegend.png}
    \caption{Respiration signal (in blue) and prediction (in red) for record f1y01, [6:30.000,8:30.000] of Fantasia Database is shown. Training set involves a total of 500 beats from [0:00.000,6:30.000].}
    \label{fig:breathing_test_fy01}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/breathing/breath_test_f1o03_nolegend.png}
    \caption{Respiration signal (in blue) and prediction (in red) for record f1o03, [10:42.000,12:42.000] of Fantasia Database is shown. Training set involves a total of 500 beats from the beginning of the recording.}
    \label{fig:breathing_test_f1o03}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/breathing/breath_test_a03_nolegend.png}
    \caption{Respiration signal (in blue) and prediction (in red) for record a03 (seconds 20640 to 21360), [5:44:00.000,5:56:00.000] of Apnea-ECG Database is shown. An apnea episode (in cyan) is annotated in the record. Prediction lacks accuracy due to a low sampling rate (100Hz) but the period of the respiration signal can be correctly estimated. Training set involves a total of 1500 beats from the beginning of the recording to include the occurrence of apnea episodes.}
    \label{fig:breathing_test_a03}
\end{figure*}


\section{Efficient approximations}
Optimizing the hyperparameters of both the initial GP of each cluster and the warping GP computed for each new segment involves a computational complexity driven by the inversion of the covariance matrices \( K^{\theta^{(s_{n})}}_{\mathbf{t}_1,\mathbf{t}_1} \) and \( K^{\vartheta^{(s_{n})}}_{\mathbf{t}_n,\mathbf{t}_n} \), respectively, each scaling with \(\mathcal{O}(q^3)\). To mitigate this, several efficient approximations have been proposed, including methods from \cite{quinonero-candela05, rasmussen_gaussian_2006, csato2000}. A notable approach by Seeger \cite{Seeger2005phd}, stated in Equation \eqref{eq:seeger_approach}, reduces complexity to \(\mathcal{O}(p^3)\) by using a reduced number \( p < q \) of inducing points.

The computation of the warp function remains the most demanding component of the proposed framework. In fact, the on-line strategy attained real-time performance for heartbeat clustering when warping computation was excluded, since the message-passing operations involved in variational inference are efficiently handled using a forward-backward approach. Optimizing the hyperparameters of the warping GP is currently solved by maximum a posteriori (MAP) estimation; however, convergence can be slow and computationally expensive. A promising approach is introduced in \cite{mikheeva2022}, where the alignment is achieved using monotonic processes defined as solutions to Ordinary Differential Equations (ODEs) with uncertain drift functions. These drift functions are modeled as GPs, providing smooth and monotonic warps while allowing for uncertainty quantification. To sample the monotonic processes, an efficient path-wise sampling technique is used, reducing complexity to \(\mathcal{O}(q)\).

\section{Related work}
\label{sec:related_work}
Probabilistic approaches to clustering time-series data have been widely explored, particularly through mixture models \cite{smyth1996, biernacki2000, zhong2003, bach2004}. These methods assume that the data is generated by a mixture of distributions with predefined forms, such as Gaussian, HMMs, or graphical models. With the aim of reducing the strong modeling assumptions, a nonparametric approach is proposed by \cite{kahlegi2016} in exchange for assuming stationary ergodicity over the data distribution. While effective for static data, these methods often fail to account for the evolving nature of time series, where distributional shifts can reduce precision, and where a fixed number of clusters is not flexible enough to deal with the emergence of new clusters. Trying to reduce this variability between observations, alignment based on GPs is developed in \cite{kazlauskaite_2019}, resulting in better clustering within the latent space. Also, works such as \cite{chiu2022, kolter2007} address this variability by detecting and reacting to concept drift. In contrast, the present approach integrates this dynamic transformation directly into the model, enabling clusters to adapt independently while parameterizing the evolving distributions over time.

Bayesian nonparametrics have provided a natural solution to overcome the limitation of fixed cardinality for the state space. In \cite{qi2007}, authors propose a HDP-HMM model for clustering fixed-length segments of a time series, with applications to music analysis. In \cite{chakraborty2023}, an infinite hidden Markov model (iHMM) on a
state space of multi-dimensional vector fields described by GPs is proposed to extract traffic patterns from multi-vehicle trajectories. In \cite{fox_2011}, HDP-SLDS has proven effective for modeling temporal dynamics in time-series data, with applications to segmentation of dancing honey bee trajectories and learning changes in the volatility of the IBOVESPA stock exchange. A sticky version of the original model increases the expected probability of self-transition, thus avoiding unrealistic fast changes between linear regimes caused by the intrinsic variability of raw data. The present proposal avoids this problem through pattern abstraction on the time-series data, and therefore the segmentation is performed using time-series segments as samples, increasing the stability of the required dynamics. 
    
Procedures for efficient learning of nonlinear state-space models and nonlinear mappings from latent states to observations have been proposed in \cite{damianou11, frigola14}, by placing proper GP priors. These methods exhibit good performance in applications such as reconstructing human motion capture data and high-dimensional video sequences, but they need more complex non-conjugate inference and learning algorithms. Notably, in \cite{alvarez09} the latent force model is introduced as a combination of mechanistic modeling principles with non-parametric data-driven components, using GPs as nonparametric models for unknown input functions in physical models, which are formulated as ordinary differential equations (ODEs). Applications in modeling human motion capture data, describing the time-dependent expression levels of genes, and predicting heavy metal concentrations in geostatistics show the versatility of the proposal.
\cite{hensman2015} proposed a Dirichlet Process Gaussian Process (DP-GP) model for clustering structured time-series data, effectively capturing both intra-group and inter-group variability. However, their approach focuses on clustering from a static perspective, without explicitly addressing the temporal dynamics underlying the time-series data.
Importantly, an extension to nonlinear (ODEs) and significant improvement in computational complexity for latent force modeling has been achieved in \cite{hartikainen2011, hartikainen2012}. However, their application to dynamic clustering with nonlinear mappings to observations given by GP priors faces identifiability and scalability issues due to the nested dependencies over complex GP structures.

\section{Conclusion}
In this paper, we have presented a Bayesian nonparametric approach to learning switching linear dynamical models for describing the evolution of an unbounded number of time series clusters. We used Gaussian process priors to model the variability both in amplitude and temporal alignment of the elements within each cluster. We have demonstrated the utility and adaptability of our approach in different real scenarios and its ability to properly unveil some of the mechanisms generating the data, thereby enabling their estimation. Hyperparameters of the model are easily interpretable, providing actionable criteria to guide the learning process and to qualify the meaning of the results.

Off-line and on-line variational inference methods, with good scalability properties, have been devised\footnote{An open-source implementation is available on-line from the site \url{https://github.com/AdrianPerezHerrero/HDP-GPC}} and applied in a real problem of ECG heartbeat clustering, showing better results than previous state-of-the-art proposals by making evolve each cluster over time, thus avoiding the proliferation of new clusters in static approaches to preserve a good performance. The on-line method has been proved as suitable for processing data streams, and the number of time series segments to be processed at a time can be increased according to resource availability. Still, the warping computation can be particularly costly and arises as the main barrier to real-time processing, demanding a more efficient approach like the path-wise sampling of the warp functions proposed in \cite{mikheeva2022}. A possible direction of future research is the extension of the present methods to the multidimensional setting, where it is assumed that multiple time series are simultaneously observed from the same real-world process and challenges include handling noise and missing data. A problem that can be approached under the multi-output learning paradigm \cite{alvarez2012}. Another interesting line to work would be to generalize the Markovian switching by including additional dependencies on observations or continuous latent states, endowing the model with more versatility and interpretability \cite{linderman2017}. 

Ultimately, the present proposal represents a flexible, Bayesian nonparametric approach for describing complex dynamical phenomena through some temporal substructures that evolve according to linear dynamics. 

\bibliographystyle{IEEEtran}
\bibliography{biblio}

\newpage

\end{document}
