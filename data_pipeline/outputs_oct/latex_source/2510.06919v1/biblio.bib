
@book{daprato_1992,
	title = {Stochastic equations in infinite dimensions},
	author = {G. da Prato and J. Zabczyk},
	year = {1992},
	publisher = {Cambridge University Press}
}

@book{grewal_2001,
	title = {Kalman filtering},
	author = {M.S. Grewal and A.P. Andrews},
	year = {2001},
	publisher = {Wiley Interscience}
}


@article{li_fei-fei_one-shot_2006,
	title = {One-shot learning of object categories},
	volume = {28},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2006.79},
	abstract = {Learning visual models of object categories notoriously requires hundreds or thousands of training examples. We show that it is possible to learn much information about a category from just one, or a handful, of images. The key insight is that, rather than learning from scratch, one can take advantage of knowledge coming from previously learned categories, no matter how different these categories might be. We explore a Bayesian implementation of this idea. Object categories are represented by probabilistic models. Prior knowledge is represented as a probability density function on the parameters of these models. The posterior model for an object category is obtained by updating the prior in the light of one or more observations. We test a simple implementation of our algorithm on a database of 101 diverse object categories. We compare category models learned by an implementation of our Bayesian approach to models learned from by maximum likelihood ({ML}) and maximum a posteriori ({MAP}) methods. We find that on a database of more than 100 categories, the Bayesian approach produces informative models when the number of training examples is too small for other methods to operate successfully.},
	pages = {594--611},
	number = {4},
	journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {{Fei-Fei}, L. and Fergus, R. and Perona, P.},
	year = {2006},
	date = {2006-04},
	keywords = {Algorithms, Artificial Intelligence, Automotive materials, Bayes methods, Bayes Theorem, Bayesian implementation, Bayesian methods, Cluster Analysis, Computer Simulation, few images, Image databases, Image Enhancement, Image Interpretation, Computer-Assisted, image recognition, Imaging, Three-Dimensional, Information Storage and Retrieval, Layout, learning, learning (artificial intelligence), Management training, maximum a posteriori method, maximum likelihood method, Models, Biological, Models, Statistical, object categories, object category posterior model, one-shot learning, Pattern Recognition, Automated, priors., probabilistic models, probability density function, Probability density function, Recognition, Reproducibility of Results, Rough surfaces, Sensitivity and Specificity, statistical analysis, Surface roughness, Taxonomy, Testing, unsupervised, variational inference},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\adria\\Zotero\\storage\\KPE5RTUY\\Li Fei-Fei et al. - 2006 - One-shot learning of object categories.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\adria\\Zotero\\storage\\CTNFKYHM\\1597116.html:text/html},
}

@article{mao_learning_2015,
	title = {Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images},
	url = {http://arxiv.org/abs/1504.06692},
	shorttitle = {Learning like a Child},
	abstract = {In this paper, we address the task of learning novel visual concepts, and their interactions with other concepts, from a few images with sentence descriptions. Using linguistic context and visual features, our method is able to efficiently hypothesize the semantic meaning of new words and add them to its word dictionary so that they can be used to describe images which contain these novel concepts. Our method has an image captioning module based on m-{RNN} with several improvements. In particular, we propose a transposed weight sharing scheme, which not only improves performance on image captioning, but also makes the model more suitable for the novel concept learning task. We propose methods to prevent overfitting the new concepts. In addition, three novel concept datasets are constructed for this new task. In the experiments, we show that our method effectively learns novel visual concepts from a few examples without disturbing the previously learned concepts. The project page is http://www.stat.ucla.edu/{\textasciitilde}junhua.mao/projects/child\_learning.html},
	journaltitle = {{arXiv}:1504.06692 [cs]},
	author = {Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Huang, Zhiheng and Yuille, Alan},
	urldate = {2021-02-20},
	date = {2015-10-01},
	eprinttype = {arxiv},
	eprint = {1504.06692},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, I.2.10, I.2.6, I.2.7},
	file = {arXiv Fulltext PDF:C\:\\Users\\adria\\Zotero\\storage\\ECFEIGJ7\\Mao et al. - 2015 - Learning like a Child Fast Novel Visual Concept L.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\adria\\Zotero\\storage\\RR899QXM\\1504.html:text/html},
}





@thesis{johnson_bayesian_2014,
	title = {Bayesian time series models and scalable inference},
	abstract = {With large and growing datasets and complex models, there is an increasing need for scalable Bayesian inference. We describe two lines of work to address this need. In the first part, we develop new algorithms for inference in hierarchical Bayesian time series models based on the hidden Markov model ({HMM}), hidden semi-Markov model ({HSMM}), and their Bayesian nonparametric extensions. The {HMM} is ubiquitous in Bayesian time series models, and it and its Bayesian nonparametric extension, the hierarchical Dirichlet process hidden Markov model ({HDP}-{HMM}), have been applied in many settings. {HSMMs} and {HDP}-{HSMMs} extend these dynamical models to provide state-specific duration modeling, but at the cost of increased computational complexity for inference, limiting their general applicability. A challenge with all such models is scaling inference to large datasets. We address these challenges in several ways. First, we develop classes of duration models for which {HSMM} message passing complexity scales only linearly in the observation sequence length. Second, we apply the stochastic variational inference ({SVI}) framework to develop scalable inference for the {HMM}, {HSMM}, and their nonparametric extensions. Third, we build on these ideas to define a new Bayesian nonparametric model that can capture dynamics at multiple timescales while still allowing efficient and scalable inference. In the second part of this thesis, we develop a theoretical framework to analyze a special case of a highly parallelizable sampling strategy we refer to as Hogwild Gibbs sampling. Thorough empirical work has shown that Hogwild Gibbs sampling works very well for inference in large latent Dirichlet allocation models ({LDA}), but there is little theory to understand when it may be effective in general. By studying Hogwild Gibbs applied to sampling from Gaussian distributions we develop analytical results as well as a deeper understanding of its behavior, including its convergence and correctness in some regimes. Thesis Supervisor: Alan S. Willsky Professor of Electrical Engineering and Computer Science},
	institution = {Massachusetts Institute of Technology},
	type = {Thesis},
	author = {Johnson, M.},
	date = {2014},
}

@book{bishop_pattern_2006,
	location = {New York},
	title = {Pattern Recognition and Machine Learning},
	isbn = {978-0-387-31073-2},
	abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year {PhD} students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. Christopher M. Bishop is Deputy Director of Microsoft Research Cambridge, and holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, a Fellow of the Royal Academy of Engineering, and a Fellow of the Royal Society of Edinburgh. His previous textbook "Neural Networks for Pattern Recognition" has been widely adopted. Coming soon: *For students, worked solutions to a subset of exercises available on a public web site (for exercises marked "www" in the text) *For instructors, worked solutions to remaining exercises from the Springer web site *Lecture slides to accompany each chapter *Data sets available for download},
	publisher = {Springer-Verlag},
	author = {Bishop, Christopher},
	urldate = {2021-02-21},
	year = {2006},
	langid = {english},
	file = {Snapshot:C\:\\Users\\adria\\Zotero\\storage\\38RFADA3\\9780387310732.html:text/html},
}

@article{perez-cruz_gaussian_2013,
	title = {Gaussian Processes for Nonlinear Signal Processing},
	volume = {30},
	issn = {1053-5888},
	url = {http://arxiv.org/abs/1303.2823},
	doi = {10.1109/MSP.2013.2250352},
	abstract = {Gaussian processes ({GPs}) are versatile tools that have been successfully employed to solve nonlinear estimation problems in machine learning, but that are rarely used in signal processing. In this tutorial, we present {GPs} for regression as a natural nonlinear extension to optimal Wiener filtering. After establishing their basic formulation, we discuss several important aspects and extensions, including recursive and adaptive algorithms for dealing with non-stationarity, low-complexity solutions, non-Gaussian noise models and classification scenarios. Furthermore, we provide a selection of relevant applications to wireless digital communications.},
	pages = {40--50},
	number = {4},
	journaltitle = {{IEEE} Signal Processing Magazine},
	shortjournal = {{IEEE} Signal Process. Mag.},
	author = {Pérez-Cruz, Fernando and Van Vaerenbergh, Steven and Murillo-Fuentes, Juan José and Lázaro-Gredilla, Miguel and Santamaria, Ignacio},
	urldate = {2021-02-21},
	date = {2013-07},
	eprinttype = {arxiv},
	eprint = {1303.2823},
	keywords = {Computer Science - Machine Learning, Computer Science - Information Theory, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\adria\\Zotero\\storage\\67CD769Z\\Pérez-Cruz et al. - 2013 - Gaussian Processes for Nonlinear Signal Processing.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\adria\\Zotero\\storage\\5PBAWMBS\\1303.html:text/html},
}






@online{frigyik_introduction_2010,
	title = {Introduction to the Dirichlet Distribution and Related Processes},
	url = {/paper/Introduction-to-the-Dirichlet-Distribution-and-Frigyik-Kapila/775e5727f5df0cb9bf834af2ea2548a696c27a38},
	abstract = {This tutorial covers the Dirichlet distribution, Dirichlet process, Pólya urn (and the associated Chinese restaurant process), hierarchical Dirichlet Process, and the Indian buffet process. Apart from basic properties, we describe and contrast three methods of generating samples: stick-breaking, the Pólya urn, and drawing gamma random variables. For the Dirichlet process we first present an informal introduction, and then a rigorous description for those more comfortable with probability theory.},
	author = {Frigyik, Andrew and Kapila, A. and Gupta, M.},
	urldate = {2021-02-22},
	date = {2010},
	langid = {english},
	file = {Snapshot:C\:\\Users\\adria\\Zotero\\storage\\RAURU859\\775e5727f5df0cb9bf834af2ea2548a696c27a38.html:text/html},
}

@online{cartella_hidden_2015,
	title = {Hidden Semi-Markov Models for Predictive Maintenance},
	url = {https://www.hindawi.com/journals/mpe/2015/278120/},
	abstract = {Realistic predictive maintenance approaches are essential for condition monitoring and predictive maintenance of industrial machines. In this work, we propose Hidden Semi-Markov Models ({HSMMs}) with (i) no constraints on the state duration density function and (ii) being applied to continuous or discrete observation. To deal with such a type of {HSMM}, we also propose modifications to the learning, inference, and prediction algorithms. Finally, automatic model selection has been made possible using the Akaike Information Criterion. This paper describes the theoretical formalization of the model as well as several experiments performed on simulated and real data with the aim of methodology validation. In all performed experiments, the model is able to correctly estimate the current state and to effectively predict the time to a predefined event with a low overall average absolute error. As a consequence, its applicability to real world settings can be beneficial, especially where in real time the Remaining Useful Lifetime ({RUL}) of the machine is calculated.},
	titleaddon = {Mathematical Problems in Engineering},
	type = {Research Article},
	author = {Cartella, Francesco and Lemeire, Jan and Dimiccoli, Luca and Sahli, Hichem},
	urldate = {2021-02-23},
	date = {2015-02-22},
	langid = {english},
	doi = {https://doi.org/10.1155/2015/278120},
	doi = {https://doi.org/10.1155/2015/278120},
	note = {{ISSN}: 1024-123X
Pages: e278120
Publisher: Hindawi
Volume: 2015},
	file = {Full Text PDF:C\:\\Users\\adria\\Zotero\\storage\\T34IIE3I\\Cartella et al. - 2015 - Hidden Semi-Markov Models for Predictive Maintenan.pdf:application/pdf;Snapshot:C\:\\Users\\adria\\Zotero\\storage\\CP2JQ6HN\\278120.html:text/html},
}

@article{melnyk_spectral_2016,
	title = {A Spectral Algorithm for Inference in Hidden Semi-Markov Models},
	url = {http://arxiv.org/abs/1407.3422},
	abstract = {Hidden semi-Markov models ({HSMMs}) are latent variable models which allow latent state persistence and can be viewed as a generalization of the popular hidden Markov models ({HMMs}). In this paper, we introduce a novel spectral algorithm to perform inference in {HSMMs}. Unlike expectation maximization ({EM}), our approach correctly estimates the probability of given observation sequence based on a set of training sequences. Our approach is based on estimating moments from the sample, whose number of dimensions depends only logarithmically on the maximum length of the hidden state persistence. Moreover, the algorithm requires only a few matrix inversions and is therefore computationally efficient. Empirical evaluations on synthetic and real data demonstrate the advantage of the algorithm over {EM} in terms of speed and accuracy, especially for large datasets.},
	journaltitle = {{arXiv}:1407.3422 [cs, stat]},
	author = {Melnyk, Igor and Banerjee, Arindam},
	urldate = {2021-02-23},
	date = {2016-02-28},
	eprinttype = {arxiv},
	eprint = {1407.3422},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\adria\\Zotero\\storage\\IF5847BV\\Melnyk y Banerjee - 2016 - A Spectral Algorithm for Inference in Hidden Semi-.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\adria\\Zotero\\storage\\CYVZLSPQ\\1407.html:text/html},
}

@thesis{le_song_learning_2008,
	title = {Learning via Hilbert Space Embedding of Distributions.},
	institution = {The University of Sydney},
	type = {Thesis},
	author = {{Le Song}},
	date = {2008},
}

@inproceedings{koch_siamese_2015,
	title = {Siamese Neural Networks for One-Shot Image Recognition},
	abstract = {The process of learning good features for machine learning applications can be very computationally expensive and may prove difficult in cases where little data is available. A prototypical example of this is the one-shot learning setting, in which we must correctly make predictions given only a single example of each new class. In this paper, we explore a method for learning siamese neural networks which employ a unique structure to naturally rank similarity between inputs. Once a network has been tuned, we can then capitalize on powerful discriminative features to generalize the predictive power of the network not just to new data, but to entirely new classes from unknown distributions. Using a convolutional architecture, we are able to achieve strong results which exceed those of other deep learning models with near state-of-the-art performance on one-shot classification tasks.},
	author = {Koch, G. and Zemel, R. and Salakhutdinov, R},
	booktitle = "{Proceedings of the 32nd International Conference on Machine Learning}",
	volume = "37",
	year={2015}
}

@article{wang_generalizing_2020,
	title = {Generalizing from a Few Examples: A Survey on Few-Shot Learning},
	abstract = {Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-Shot Learning ({FSL}) is proposed to tackle this problem. Using prior knowledge, {FSL} can rapidly generalize to new tasks containing only a few samples with supervised information. In this paper, we conduct a thorough survey to fully understand {FSL}. Starting from a formal definition of {FSL}, we distinguish {FSL} from several relevant machine learning problems. We then point out that the core issue in {FSL} is that the empirical risk minimized is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize {FSL} methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the {FSL} problem setups, techniques, applications and theories, are also proposed to provide insights for future research.},
	journal = {ACM Computing Surveys},
	author = {Wang, Yaqing and Yao, Quanming and Kwok, James and Ni, Lionel M.},
	year = {2020},
	volume = {1},
	number = {1},
	eprinttype = {arxiv},
	eprint = {1904.05046},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\adria\\Zotero\\storage\\MVR774UH\\Wang et al. - 2020 - Generalizing from a Few Examples A Survey on Few-.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\adria\\Zotero\\storage\\FMURVKIR\\1904.html:text/html},
}

@online{murphy_hidden_2002-1,
	title = {Hidden semi-Markov models ( {HSMMs} )},
	url = {/paper/Hidden-semi-Markov-models-(-HSMMs-)-Murphy/a2125e4fba6c69ff7a6d247bcf04312bb2b79606},
	abstract = {A semi-Markov {HMM} (more properly called a hidden semi-Markov model, or {HSMM}) is like an {HMM} except each state can emit a sequence of observations. Let Y (Gt) be the subsequence emitted by “generalized state” Gt. The “generalized state” usually contains both the automaton state, Qt, and the length (duration) of the segment, Lt. We will define Y (Gt) to be the subsequence yt−l+1:t. After emitting a segment, the next state is Gtn , where tn = t + Lt. Similarly, denote the previous state by Gtp . Let Y (G + t ) be all observations following Gt, and Y (G − t ) be all observations preceeding Gt, as in Figure 1. Each segment Ot(q, l) def =P (Y (Gt){\textbar}Qt = q, Lt = l) can be an arbitrary distribution. If P (Y (Gt){\textbar}q, l) = ∏t i=t−l+1 P (yi{\textbar}q), this is an explicit duration {HMM} [Fer80, Lev86, Rab89, {MJ}93, {MHJ}95]. If P (Y (Gt){\textbar}q, l) is modelled by an {HMM} or state-space model (linear-dynamical system), this is called a segment model [{GY}93, {ODK}96]. In computational biology, P (Y (Gt){\textbar}q, l) is often modelled by a weight matrix or higher-order Markov chain (see e.g., [{BK}97]). In this paper, we are agnostic about the form of P (Y (Gt){\textbar}q, l). It is possible to approximate a variable-duration {HMM} by adding extra states to a regular {HMM} (see [{DEKM}98, p69]), i.e., a mixture of geometric distributions. However, our main interest will be segment models, which are strict generalizations of variable-duration {HMMs}. For the relationship between semi-Markov {HMMs}, pseudo-2D {HMMs}, hierarchical {HMMs}, etc., please see [Mur02]. (Essentially, with a pseudo-2D {HMMs}, we know the size of each segment ahead of time; an {HHMM} is a generalization of a segment model where each segment can have subsegments inside of it, each modelled by an {HMM}.) We can represent a variable-duration {HMMs} as a {DBN} as shown in {FIgure} 2. We explicitly add Qt , the remaining duration of state Qt, to the state-space. (Even though Qt is constant for a long period, we copy its value across every},
	author = {Murphy, K.},
	urldate = {2021-02-24},
	date = {2002},
	langid = {english},
	file = {Snapshot:C\:\\Users\\adria\\Zotero\\storage\\BQ2KY287\\a2125e4fba6c69ff7a6d247bcf04312bb2b79606.html:text/html},
}

@thesis{murphy_dynamic_2002,
	title = {Dynamic bayesian networks: representation, inference and learning},
	url = {/paper/Dynamic-bayesian-networks%3A-representation%2C-and-Murphy-Russell/5e86e17d83c97dafa3413d1d0dae219bd527ed61},
	shorttitle = {Dynamic bayesian networks},
	abstract = {Dynamic Bayesian Networks: Representation, Inference and Learning by Kevin Patrick Murphy Doctor of Philosophy in Computer Science University of California, Berkeley Professor Stuart Russell, Chair Modelling sequential data is important in many areas of science and engineering. Hidden Markov models ({HMMs}) and Kalman filter models ({KFMs}) are popular for this because they are simple and flexible. For example, {HMMs} have been used for speech recognition and bio-sequence analysis, and {KFMs} have been used for problems ranging from tracking planes and missiles to predicting the economy. However, {HMMs} and {KFMs} are limited in their “expressive power”. Dynamic Bayesian Networks ({DBNs}) generalize {HMMs} by allowing the state space to be represented in factored form, instead of as a single discrete random variable. {DBNs} generalize {KFMs} by allowing arbitrary probability distributions, not just (unimodal) linear-Gaussian. In this thesis, I will discuss how to represent many different kinds of models as {DBNs}, how to perform exact and approximate inference in {DBNs}, and how to learn {DBN} models from sequential data. In particular, the main novel technical contributions of this thesis are as follows: a way of representing Hierarchical {HMMs} as {DBNs}, which enables inference to be done in O(T ) time instead of O(T ), where T is the length of the sequence; an exact smoothing algorithm that takes O(log T ) space instead of O(T ); a simple way of using the junction tree algorithm for online inference in {DBNs}; new complexity bounds on exact online inference in {DBNs}; a new deterministic approximate inference algorithm called factored frontier; an analysis of the relationship between the {BK} algorithm and loopy belief propagation; a way of applying Rao-Blackwellised particle filtering to {DBNs} in general, and the {SLAM} (simultaneous localization and mapping) problem in particular; a way of extending the structural {EM} algorithm to {DBNs}; and a variety of different applications of {DBNs}. However, perhaps the main value of the thesis is its catholic presentation of the field of sequential data modelling.},
	institution = {{UC} Berkeley},
	type = {Thesis},
	author = {Murphy, Kevin and Russell, S.},
	urldate = {2021-02-24},
	date = {2002},
	langid = {english},
	file = {Snapshot:C\:\\Users\\adria\\Zotero\\storage\\5AYMGRXR\\5e86e17d83c97dafa3413d1d0dae219bd527ed61.html:text/html},
}

@article{johnson_bayesian_2012,
	title = {Bayesian Nonparametric Hidden Semi-Markov Models},
	url = {http://arxiv.org/abs/1203.1365},
	abstract = {There is much interest in the Hierarchical Dirichlet Process Hidden Markov Model ({HDP}-{HMM}) as a natural Bayesian nonparametric extension of the ubiquitous Hidden Markov Model for learning from sequential and time-series data. However, in many settings the {HDP}-{HMM}'s strict Markovian constraints are undesirable, particularly if we wish to learn or encode non-geometric state durations. We can extend the {HDP}-{HMM} to capture such structure by drawing upon explicit-duration semi-Markovianity, which has been developed mainly in the parametric frequentist setting, to allow construction of highly interpretable models that admit natural prior information on state durations. In this paper we introduce the explicit-duration Hierarchical Dirichlet Process Hidden semi-Markov Model ({HDP}-{HSMM}) and develop sampling algorithms for efficient posterior inference. The methods we introduce also provide new methods for sampling inference in the finite Bayesian {HSMM}. Our modular Gibbs sampling methods can be embedded in samplers for larger hierarchical Bayesian models, adding semi-Markov chain modeling as another tool in the Bayesian inference toolbox. We demonstrate the utility of the {HDP}-{HSMM} and our inference methods on both synthetic and real experiments.},
	journaltitle = {{arXiv}:1203.1365 [stat]},
	author = {Johnson, Matthew J. and Willsky, Alan S.},
	urldate = {2021-02-24},
	date = {2012-09-07},
	eprinttype = {arxiv},
	eprint = {1203.1365},
	keywords = {Statistics - Machine Learning, Statistics - Applications, Statistics - Methodology},
	file = {arXiv Fulltext PDF:C\:\\Users\\adria\\Zotero\\storage\\77YI4W4H\\Johnson y Willsky - 2012 - Bayesian Nonparametric Hidden Semi-Markov Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\adria\\Zotero\\storage\\I2HJ4Q88\\1203.html:text/html},
}

@article{baum_statistical_1966,
	title = {Statistical Inference for Probabilistic Functions of Finite State Markov Chains},
	doi = {10.1214/AOMS/1177699147},
	abstract = {Semantic Scholar extracted view of "Statistical Inference for Probabilistic Functions of Finite State Markov Chains" by L. E. Baum et al.},
	author = {Baum, L. E. and Petrie, T.},
	date = {1966},
	file = {Texto completo:C\:\\Users\\adria\\Zotero\\storage\\6H9KBF8C\\Baum y Petrie - 1966 - Statistical Inference for Probabilistic Functions .pdf:application/pdf},
}

@article{guedon_estimating_2003,
	title = {Estimating Hidden Semi-Markov Chains From Discrete Sequences},
	volume = {12},
	doi = {10.1198/1061860032030},
	abstract = {This article addresses the estimation of hidden semi-Markov chains from nonstationary discrete sequences. Hidden semi-Markov chains are particularly useful to model the succession of homogeneous zones or segments along sequences. A discrete hidden semi-Markov chain is composed of a nonobservable state process, which is a semi-Markov chain, and a discrete output process. Hidden semi-Markov chains generalize hidden Markov chains and enable the modeling of various durational structures. From an algorithmic point of view, a new forward-backward algorithm is proposed whose complexity is similar to that of the Viterbi algorithm in terms of sequence length (quadratic in the worst case in time and linear in space). This opens the way to the maximum likelihood estimation of hidden semi-Markov chains from long sequences. This statistical modeling approach is illustrated by the analysis of branching and flowering patterns in plants.},
	pages = {604--639},
	journaltitle = {Journal of Computational and Graphical Statistics},
	shortjournal = {Journal of Computational and Graphical Statistics},
	author = {Guédon, Yann},
	date = {2003-09-01},
	file = {Versión enviada:C\:\\Users\\adria\\Zotero\\storage\\BT78DX3D\\Guédon - 2003 - Estimating Hidden Semi-Markov Chains From Discrete.pdf:application/pdf},
}

@article{du_biomvrhsmm_2014,
	title = {{biomvRhsmm}: Genomic Segmentation with Hidden Semi-Markov Model},
	volume = {2014},
	doi = {10.1155/2014/910390},
	shorttitle = {{biomvRhsmm}},
	abstract = {High-throughput technologies like tiling array and next-generation sequencing ({NGS}) generate continuous homogeneous segments or signal peaks in the genome that represent transcripts and transcript variants (transcript mapping and quantification), regions of deletion and amplification (copy number variation), or regions characterized by particular common features like chromatin state or {DNA} methylation ratio (epigenetic modifications). However, the volume and output of data produced by these technologies present challenges in analysis. Here, a hidden semi-Markov model ({HSMM}) is implemented and tailored to handle multiple genomic profile, to better facilitate genome annotation by assisting in the detection of transcripts, regulatory regions, and copy number variation by holistic microarray or {NGS}. With support for various data distributions, instead of limiting itself to one specific application, the proposed hidden semi-Markov model is designed to allow modeling options to accommodate different types of genomic data and to serve as a general segmentation engine. By incorporating genomic positions into the sojourn distribution of {HSMM}, with optional prior learning using annotation or previous studies, the modeling output is more biologically sensible. The proposed model has been compared with several other state-of-the-art segmentation models through simulation benchmarking, which shows that our efficient implementation achieves comparable or better sensitivity and specificity in genomic segmentation.},
	journaltitle = {{BioMed} Research International},
	shortjournal = {{BioMed} Research International},
	author = {Du, Yang and Murani, Eduard and Ponsuksili, Siriluck and Wimmers, Klaus},
	date = {2014-06-03},
	file = {Full Text PDF:C\:\\Users\\adria\\Zotero\\storage\\BD2JZWE2\\Du et al. - 2014 - biomvRhsmm Genomic Segmentation with Hidden Semi-.pdf:application/pdf},
}

@inproceedings{jd_ferguson_variable_1980,
	location = {Institute for Defense Analyses, Princeton, {NJ}},
	title = {Variable duration models for speech},
	eventtitle = {Symp. Application of Hidden Markov Models to Text and Speech},
	pages = {143--179},
	author = {{J.D. Ferguson}},
	date = {1980},
}

@article{yamagishi_average-voice-based_2007,
	title = {Average-Voice-Based Speech Synthesis Using {HSMM}-Based Speaker Adaptation and Adaptive Training},
	volume = {E90D},
	doi = {10.1093/ietisy/e90-d.2.533},
	abstract = {In speaker adaptation for speech synthesis, it is desirable to convert both voice characteristics and prosodic features such as F0 and phone duration. For simultaneous adaptation of spectrum, F0 and phone duration within the {HMM} framework, we need to transform not only the state output distributions corresponding to spectrum and F0 but also the duration distributions corresponding to phone duration. However, it is not straightforward to adapt the state duration because the original {HMM} does not have explicit duration distributions. Therefore, we utilize the framework of the hidden semi-Markov model ({HSMM}), which is an {HMM} having explicit state duration distributions, and we apply an {HSMM}-based model adaptation algorithm to simultaneously transform both the state output and state duration distributions. Furthermore, we propose an {HSMM}-based adaptive training algorithm to simultaneously normalize the state output and state duration distributions of the average voice model. We incorporate these techniques into our {HSMM}-based speech synthesis system, and show their effectiveness from the results of subjective and objective evaluation tests.},
	journaltitle = {{IEICE} Transactions on Information and Systems},
	shortjournal = {{IEICE} Transactions on Information and Systems},
	author = {Yamagishi, Junichi and Kobayashi, Takao},
	date = {2007-02-01},
	file = {Full Text PDF:C\:\\Users\\adria\\Zotero\\storage\\7MKZ2H85\\Yamagishi y Kobayashi - 2007 - Average-Voice-Based Speech Synthesis Using HSMM-Ba.pdf:application/pdf},
}

@inproceedings{faisan_hidden_2002,
	title = {Hidden semi-Markov event sequence models: application to brain functional {MRI} sequence analysis},
	volume = {1},
	doi = {10.1109/ICIP.2002.1038166},
	shorttitle = {Hidden semi-Markov event sequence models},
	abstract = {Due to the piecewise stationarity assumption required for the observable process of a hidden Markov chain, the application of hidden Markov models ({HMMs}) to the analysis of event-based random processes remains intricate. For such processes, a new class of {HMMs} is proposed: the hidden semi-Markov event sequence model ({HSMESM}). In a {HSMESM}, the observable process is no more considered as segmental in nature but issued from a detection-characterization preprocessing step. The standard markovian formalism is adapted accordingly. Results obtained in functional {MRI} sequence analysis validate this novel statistical modeling approach while opening new perspectives in detection-recognition of event-based random processes.},
	eventtitle = {Proceedings / {ICIP} ... International Conference on Image Processing},
	pages = {I--880},
	author = {Faisan, Sylvain and Thoraval, Laurent and Armspach, Jp and Heitz, Fabrice},
	date = {2002-02-01},
	file = {Full Text PDF:C\:\\Users\\adria\\Zotero\\storage\\TSVDGRT5\\Faisan et al. - 2002 - Hidden semi-Markov event sequence models applicat.pdf:application/pdf},
}

@inproceedings{faisan_hidden_2002-1,
	title = {Hidden semi-Markov event sequence models: application to brain functional {MRI} sequence analysis},
	volume = {1},
	doi = {10.1109/ICIP.2002.1038166},
	shorttitle = {Hidden semi-Markov event sequence models},
	abstract = {Due to the piecewise stationarity assumption required for the observable process of a hidden Markov chain, the application of hidden Markov models ({HMMs}) to the analysis of event-based random processes remains intricate. For such processes, a new class of {HMMs} is proposed: the hidden semi-Markov event sequence model ({HSMESM}). In a {HSMESM}, the observable process is no more considered as segmental in nature but issued from a detection-characterization preprocessing step. The standard markovian formalism is adapted accordingly. Results obtained in functional {MRI} sequence analysis validate this novel statistical modeling approach while opening new perspectives in detection-recognition of event-based random processes.},
	eventtitle = {Proceedings / {ICIP} ... International Conference on Image Processing},
	pages = {I--880},
	author = {Faisan, Sylvain and Thoraval, Laurent and Armspach, Jp and Heitz, Fabrice},
	date = {2002-02-01},
	file = {Full Text PDF:C\:\\Users\\adria\\Zotero\\storage\\AH6EQAEY\\Faisan et al. - 2002 - Hidden semi-Markov event sequence models applicat.pdf:application/pdf},
}

@inproceedings{salakhutdinov_one-shot_2012,
	title = {One-Shot Learning with a Hierarchical Nonparametric Bayesian Model},
	url = {http://proceedings.mlr.press/v27/salakhutdinov12a.html},
	abstract = {We develop a hierarchical Bayesian model that learns categories from single training examples. The model transfers acquired knowledge from previously learned categories to a novel category, in the ...},
	eventtitle = {Proceedings of {ICML} Workshop on Unsupervised and Transfer Learning},
	pages = {195--206},
	booktitle = {Proceedings of {ICML} Workshop on Unsupervised and Transfer Learning},
	publisher = {{JMLR} Workshop and Conference Proceedings},
	author = {Salakhutdinov, Ruslan and Tenenbaum, Joshua and Torralba, Antonio},
	urldate = {2021-02-24},
	date = {2012-06-27},
	langid = {english},
	note = {{ISSN}: 1938-7228},
	file = {Full Text PDF:C\:\\Users\\adria\\Zotero\\storage\\BPQPF4S8\\Salakhutdinov et al. - 2012 - One-Shot Learning with a Hierarchical Nonparametri.pdf:application/pdf;Snapshot:C\:\\Users\\adria\\Zotero\\storage\\BZYTRH3R\\salakhutdinov12a.html:text/html},
}


@thesis{fox_bayesian_2009,
	title = {Bayesian nonparametric learning of complex dynamical phenomena},
	rights = {M.I.T. theses are protected by  copyright. They may be viewed from this source for any purpose, but  reproduction or distribution in any format is prohibited without written  permission. See provided {URL} for inquiries about permission.},
	url = {https://dspace.mit.edu/handle/1721.1/55111},
	abstract = {The complexity of many dynamical phenomena precludes the use of linear models for which exact analytic techniques are available. However, inference on standard nonlinear models quickly becomes intractable. In some cases, Markov switching processes, with switches between a set of simpler models, are employed to describe the observed dynamics. Such models typically rely on pre-specifying the number of Markov modes. In this thesis, we instead take a Bayesian nonparametric approach in defining a prior on the model parameters that allows for flexibility in the complexity of the learned model and for development of efficient inference algorithms. We start by considering dynamical phenomena that can be well-modeled as a hidden discrete Markov process, but in which there is uncertainty about the cardinality of the state space. The standard finite state hidden Markov model ({HMM}) has been widely applied in speech recognition, digital communications, and bioinformatics, amongst other fields. Through the use of the hierarchical Dirichlet process ({HDP}), one can examine an {HMM} with an unbounded number of possible states. We revisit this {HDPHMM} and develop a generalization of the model, the sticky {HDP}-{HMM}, that allows more robust learning of smoothly varying state dynamics through a learned bias towards self-transitions. We show that this sticky {HDP}-{HMM} not only better segments data according to the underlying state sequence, but also improves the predictive performance of the learned model. Additionally, the sticky {HDP}-{HMM} enables learning more complex, multimodal emission distributions.},
	institution = {Massachusetts Institute of Technology},
	type = {Thesis},
	author = {Fox, Emily Beth},
	urldate = {2021-02-24},
	date = {2009},
	note = {Accepted: 2010-05-25T20:43:39Z},
	file = {Snapshot:C\:\\Users\\adria\\Zotero\\storage\\GG56WCVM\\55111.html:text/html;Full Text PDF:C\:\\Users\\adria\\Zotero\\storage\\T795G437\\Fox - 2009 - Bayesian nonparametric learning of complex dynamic.pdf:application/pdf},
}

@inproceedings{fox_hdp-hmm_2008,
	title = {An {HDP}-{HMM} for systems with state persistence},
	isbn = {978-1-60558-205-4},
	doi = {10.1145/1390156.1390196},
	abstract = {The hierarchical Dirichlet process hidden Markov model ({HDP}-{HMM}) is a flexible, nonparametric model which allows state spaces of unknown size to be learned from data. We demonstrate some limitations of the original {HDP}-{HMM} formulation (Teh et al., 2006), and propose a sticky extension which allows more robust learning of smoothly varying dynamics. Using {DP} mixtures, this formulation also allows learning of more complex, multimodal emission distributions. We further develop a sampling algorithm that employs a truncated approximation of the {DP} to jointly resample the full state sequence, greatly improving mixing rates. Via extensive experiments with synthetic data and the {NIST} speaker diarization database, we demonstrate the advantages of our sticky extension, and the utility of the {HDP}-{HMM} in real-world applications.},
	pages = {312--319},
	booktitle = {Proceedings of the 25th international conference on Machine learning},
	author = {Fox, Emily B. and Sudderth, Erik B. and Jordan, Michael I. and Willsky, Alan S.},
	year = {2008}
}

@article{fox_2011,
    author = {Fox, Emily B. and Sudderth, Erik B. and Jordan, Michael I. and Willsky, Alan S.},
    title = {Bayesian nonparametric inference of switching dynamic linear models},
    journal = {IEEE Transactions on Signal Processing},
    volume = {59},
    number = {4},
    pages = {1569-1585},
    year = {2011}
}

@article{ghahramani_variational_2000,
	title = {Variational Learning for Switching State-Space Models},
	volume = {12},
	number = {4},
	issn = {0899-7667},
	doi = {10.1162/089976600300015619},
	abstract = {We introduce a new statistical model for time series that iteratively segments data into regimes with approximately linear dynamics and learns the parameters of each of these linear regimes. This model combines and generalizes two of the most widely used stochastic time-series models—hidden Markov models and linear dynamical systems—and is closely related to models that are widely used in the control and econometrics literatures. It can also be derived by extending the mixture of experts neural network (Jacobs, Jordan, Nowlan, \& Hinton, 1991) to its fully dynamical version, in which both expert and gating networks are recurrent. Inferring the posterior probabilities of the hidden states of this model is computationally intractable, and therefore the exact expectation maximization ({EM}) algorithm cannot be applied. However, we present a variational approximation that maximizes a lower bound on the log-likelihood and makes use of both the forward and backward recursions for hidden Markov models and the Kalman filter recursions for linear dynamical systems. We tested the algorithm on artificial data sets and a natural data set of respiration force from a patient with sleep apnea. The results suggest that variational approximations are a viable method for inference and learning in switching state-space models.},
	pages = {831--864},
	journal = {Neural Computation},
	author = {Ghahramani, Zoubin and Hinton, Geoffrey E.},
	year = {2000}
}

@article{goldberger_2000,
	author = "Goldberger, A. L. and Amaral, L. A. N. and Glass, L. and Hausdorff, J. M. and Ivanov, P. Ch. and Mark, R. G. and Mietus, J. E. and Moody, G. B. and Peng, C-K. and Stanley, H. E.",
	doi = "10.1161/01.CIR.101.23.e215",
	issn = "0009-7322",
	journal = "Circulation",
	month = jun,
	number = "23",
	pages = "215--220",
	title = "{PhysioBank, PhysioToolkit, and PhysioNet: Components of a New Research Resource for Complex Physiologic Signals}",
	volume = "101",
	year = "2000"
}

@inproceedings{huber13,
	title = {Recursive Gaussian process regression},
	doi = {10.1109/ICASSP.2013.6638281},
	abstract = {For large data sets, performing Gaussian process regression is computationally demanding or even intractable. If data can be processed sequentially, the recursive regression method proposed in this paper allows incorporating new data with constant computation time. For this purpose two operations are performed alternating on a fixed set of so-called basis vectors used for estimating the latent function: First, inference of the latent function at the new inputs. Second, utilization of the new data for updating the estimate. Numerical simulations show that the proposed approach significantly reduces the computation time and at the same time provides more accurate estimates compared to existing on-line and/or sparse Gaussian process regression approaches.},
	eventtitle = {Acoustics, Speech, and Signal Processing, 1988. {ICASSP}-88., 1988 International Conference on},
	year = {2013},
    	month = {05},
    	journal = {Acoustics, Speech, and Signal Processing, 1988. ICASSP-88., 1988 International Conference on},
	pages = {3362--3366},
	author = {Huber, Marco},
	date = {2013-05-01},
	booktitle = {Acoustics, Speech, and Signal Processing, 1988. {ICASSP}-88., 1988 International Conference on},
	file = {Full Text PDF:C\:\\Users\\adria\\Zotero\\storage\\8VR4RAYA\\Huber - 2013 - Recursive Gaussian process regression.pdf:application/pdf},
}

@inproceedings{johnson_stochastic_2014,
	title = {Stochastic Variational Inference for {B}ayesian Time Series Models},
	pages = {1854--1862},
	booktitle = {International Conference on Machine Learning},
	author = {Johnson, Matthew and Willsky, Alan},
    year = {2014}
}

@collection{knill_perception_1996,
	location = {Cambridge},
	title = {Perception as Bayesian Inference},
	isbn = {978-0-521-46109-2},
	url = {https://www.cambridge.org/core/books/perception-as-bayesian-inference/0442F577F5E4CD874FA6819978574C8F},
	abstract = {Bayesian probability theory has emerged not only as a powerful tool for building computational theories of vision, but also as a general paradigm for studying human visual perception. This 1996 book provides an introduction to and critical analysis of the Bayesian paradigm. Leading researchers in computer vision and experimental vision science describe general theoretical frameworks for modelling vision, detailed applications to specific problems and implications for experimental studies of human perception. The book provides a dialogue between different perspectives both within chapters, which draw on insights from experimental and computational work, and between chapters, through commentaries written by the contributors on each others' work. Students and researchers in cognitive and visual science will find much to interest them in this thought-provoking collection.},
	publisher = {Cambridge University Press},
	editor = {Knill, David C. and Richards, Whitman},
	urldate = {2021-02-20},
	date = {1996},
	doi = {10.1017/CBO9780511984037},
	file = {Snapshot:C\:\\Users\\adria\\Zotero\\storage\\XATSRX2I\\0442F577F5E4CD874FA6819978574C8F.html:text/html},
}

@inproceedings{lake_one-shot_2011,
	author = "Lake, B.M. and Salakhutdinov, R. and Gross, J. and Tenenbaum, J.B.",
	booktitle = "{Proceedings of the 33rd Annual Conference of the Cognitive Science Society}",
	number = "",
	pages = "",
	title = "{One shot learning of simple visual concepts}",
	volume = "172",
	year = "2011"
}

@article{lake_one-shot_2015,
	title = {One-shot learning by inverting a compositional causal process},
	abstract = {People can learn a new visual class from just one example, yet machine learning algorithms typically require hundreds or thousands of examples to tackle the same problems. Here we present a Hierarchical Bayesian model based on compositionality and causality that can learn a wide range of natural (although simple) visual concepts, generalizing in human-like ways from just one image. We evaluated performance on a challenging one-shot classification task, where our model achieved a human-level error rate while substantially outperforming two deep learning models. We also used a visual Turing test "to show that our model produces human-like performance on other conceptual tasks, including generating new examples and parsing."},
	journaltitle = {Advances in Neural Information Processing Systems},
	shortjournal = {Advances in Neural Information Processing Systems},
	author = {Lake, Brenden and Salakhutdinov, Ruslan and Tenenbaum, Joshua},
	date = {2015-02-18},
}



@article{lake_one-shot_2014,
	title = {One-shot learning of generative speech concepts},
	abstract = {One-shot learning of generative speech concepts Brenden M. Lake* Chia-ying Lee* James R. Glass Joshua B. Tenenbaum Brain and Cognitive Sciences {MIT} {CSAIL} {MIT} {CSAIL} {MIT} Brain and Cognitive Sciences {MIT} Abstract 2007). Related computational work has investigated other factors that contribute to learning word meaning, including learning-to-learn which features are important (Colunga \& Smith, 2005; Kemp et al., 2007) and cross-situational word learning (Smith \& Yu, 2008; Frank, Goodman, \& Tenen- baum, 2009). But by any account, the acquisition of mean- ing is only possible because the child can also learn the spo- ken word as a category, mapping all instances (and exclud- ing non-instances) of a word like “elephant” to the same phonological representation, regardless of speaker identify and other sources of acoustic variability. This is the focus of the current paper. Previous work has shown that chil- dren can do one-shot spoken word learning (Carey \& Bartlett, 1978). When children (ages 3-4) were asked to bring over a “chromium” colored object, they seemed to flag the sound as a new word; some even later produced their own approxima- tion of the word “chromium.” Furthermore, acquiring new spoken words remains an important problem well into adult- hood whether its learning a second language, a new name, or a new vocabulary word. The goal of our work is twofold: to develop one-shot learn- ing tasks that can compare people and models side-by-side, and to develop a computational model that performs well on these tasks. Since the tasks must contain novel words for both people and algorithms, we tested English speakers on their ability to learn Japanese words. This language pairing also offers an interesting test case for learning-to-learn through the transfer of phonetic structure, since the Japanese analogs to English phonemes fall roughly within a subset of English phonemes (Ohata, 2004). Can the recent progress on models of one-shot learning be leveraged for learning new spoken words from raw speech? How could a generative model of a word be learned from just one example? Recent behavioral and computational work suggests that compositionality, combined with Hierarchical Bayesian modeling, can be a powerful way to build a “gen- erative model for generative models” that supports one-shot learning (Lake, Salakhutdinov, \& Tenenbaum, 2012; Lake et al., 2013). This idea was applied to the one-shot learning of handwritten characters, a similarly high-dimensional do- main of natural concepts, using an “analysis-by-synthesis” approach. Given a raw image of a novel character, the model learns to represent it by a latent dynamic causal process, com- posed of pen strokes and their spatial relations (Fig. 1a). The sharing of stochastic motor primitives across concepts (Fig. 1a-i) provides a means of synthesizing new generative mod- els out of pieces of existing ones (Fig. 1a-iii). Compositional generative models are well-suited for the problem of spoken word acquisition, as they relate to classic One-shot learning – the human ability to learn a new concept from just one or a few examples – poses a challenge to tradi- tional learning algorithms, although approaches based on Hi- erarchical Bayesian models and compositional representations have been making headway. This paper investigates how chil- dren and adults readily learn the spoken form of new words from one example – recognizing arbitrary instances of a novel phonological sequence, and excluding non-instances, regard- less of speaker identity and acoustic variability. This is an es- sential step on the way to learning a word’s meaning and learn- ing to use it, and we develop a Hierarchical Bayesian acoustic model that can learn spoken words from one example, utiliz- ing compositions of phoneme-like units that are the product of unsupervised learning. We compare people and computa- tional models on one-shot classification and generation tasks with novel Japanese words, finding that the learned units play an important role in achieving good performance. Keywords: one-shot learning; speech recognition; category learning; exemplar generation Introduction People can learn a new concept from just one or a few ex- amples, making meaningful generalizations that go far be- yond the observed data. Replicating this ability in machines has been challenging, since standard learning algorithms re- quire tens, hundreds, or thousands of examples before reach- ing a high level of classification performance. Nonetheless, recent interest from cognitive science and machine learning has advanced our computational understanding of “one-shot learning,” and several key themes have emerged. Proba- bilistic generative models can predict how people general- ize from just one or a few examples, as shown for data ly- ing in a low-dimensional space (Shepard, 1987; Tenenbaum \& Griffiths, 2001). Another theme has developed around learning-to-learn, the idea that one-shot learning itself de- velops from previous learning with related concepts, and Hi- erarchical Bayesian ({HB}) models can learn-to-learn by high- lighting the dimensions or features that are most important for generalization (Fei-Fei, Fergus, \& Perona, 2006; Kemp, Perfors, \& Tenenbaum, 2007; Salakhutdinov, Tenenbaum, \& Torralba, 2012). In this paper, we study the problem of learning new spoken words, an essential ingredient for language development. By one estimate, children learn an average of ten new words per day from the age of one to the end of high school (Bloom, 2000). For learning to proceed at such an astounding rate, children must be learning new words from very little data. Previous computational work has focused on the problem of learning the meaning of words from a few examples; for in- stance, upon hearing the word “elephant” paired with an ex- emplar, the child must decide which objects belong to the set of “elephants” and which do not (e.g., Xu \& Tenenbaum, * The first two authors contributed equally to this work.},
	journal = {Cognitive Science},
	author = {Lake, B. and Lee, C. and Glass, J. R. and Tenenbaum, J.},
	volume = {36},
	year = {2014},
}

@inproceedings{lee_nonparametric_2012,
	location = {{USA}},
	title = {A nonparametric Bayesian approach to acoustic model discovery},
	series = {{ACL} '12},
	abstract = {We investigate the problem of acoustic modeling in which prior language-specific knowledge and transcribed data are unavailable. We present an unsupervised model that simultaneously segments the speech, discovers a proper set of sub-word units (e.g., phones) and learns a Hidden Markov Model ({HMM}) for each induced acoustic unit. Our approach is formulated as a Dirichlet process mixture model in which each mixture is an {HMM} that represents a sub-word unit. We apply our model to the {TIMIT} corpus, and the results demonstrate that our model discovers sub-word units that are highly correlated with English phones and also produces better segmentation than the state-of-the-art unsupervised baseline. We test the quality of the learned acoustic models on a spoken term detection task. Compared to the baselines, our model improves the relative precision of top hits by at least 22.1\% and outperforms a language-mismatched acoustic model.},
	pages = {40--49},
	booktitle = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1},
	publisher = {Association for Computational Linguistics},
	author = {Lee, Chia-ying and Glass, James},
	urldate = {2021-02-24},
	date = {2012-07-08},
	file = {Full Text PDF:C\:\\Users\\adria\\Zotero\\storage\\JLM6BTHS\\Lee y Glass - 2012 - A nonparametric Bayesian approach to acoustic mode.pdf:application/pdf},
}

@article{Moody01,
	author = "Moody, G. B. and Mark, R. G.",
	doi = "10.1109/51.932724",
	issn = "0739-5175",
	journal = "IEEE Engineering in Medicine and Biology Magazine",
	month = "May",
	number = "3",
	pages = "45--50",
	title = "{The impact of the MIT-BIH Arrhythmia Database}",
	volume = "20",
	year = "2001"
}

@article{muandet_kernel_2017,
	title = {Kernel Mean Embedding of Distributions: A Review and Beyond},
	volume = {10},
	issn = {1935-8237, 1935-8245},
	url = {http://arxiv.org/abs/1605.09522},
	doi = {10.1561/2200000060},
	shorttitle = {Kernel Mean Embedding of Distributions},
	abstract = {A Hilbert space embedding of a distribution---in short, a kernel mean embedding---has recently emerged as a powerful tool for machine learning and inference. The basic idea behind this framework is to map distributions into a reproducing kernel Hilbert space ({RKHS}) in which the whole arsenal of kernel methods can be extended to probability measures. It can be viewed as a generalization of the original "feature map" common to support vector machines ({SVMs}) and other kernel methods. While initially closely associated with the latter, it has meanwhile found application in fields ranging from kernel machines and probabilistic modeling to statistical inference, causal discovery, and deep learning. The goal of this survey is to give a comprehensive review of existing work and recent advances in this research area, and to discuss the most challenging issues and open problems that could lead to new research directions. The survey begins with a brief introduction to the {RKHS} and positive definite kernels which forms the backbone of this survey, followed by a thorough discussion of the Hilbert space embedding of marginal distributions, theoretical guarantees, and a review of its applications. The embedding of distributions enables us to apply {RKHS} methods to probability measures which prompts a wide range of applications such as kernel two-sample testing, independent testing, and learning on distributional data. Next, we discuss the Hilbert space embedding for conditional distributions, give theoretical insights, and review some applications. The conditional mean embedding enables us to perform sum, product, and Bayes' rules---which are ubiquitous in graphical model, probabilistic inference, and reinforcement learning---in a non-parametric way. We then discuss relationships between this framework and other related areas. Lastly, we give some suggestions on future research directions.},
	pages = {1--141},
	number = {1},
	journaltitle = {Foundations and Trends® in Machine Learning},
	shortjournal = {{FNT} in Machine Learning},
	author = {Muandet, Krikamol and Fukumizu, Kenji and Sriperumbudur, Bharath and Schölkopf, Bernhard},
	urldate = {2021-02-24},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1605.09522},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\adria\\Zotero\\storage\\63Q82AWD\\Muandet et al. - 2017 - Kernel Mean Embedding of Distributions A Review a.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\adria\\Zotero\\storage\\5SX44SNV\\1605.html:text/html},
}

@online{murphy_hidden_2002,
	title = {Hidden semi-Markov models ( {HSMMs} )},
	url = {/paper/Hidden-semi-Markov-models-(-HSMMs-)-Murphy/a2125e4fba6c69ff7a6d247bcf04312bb2b79606},
	abstract = {A semi-Markov {HMM} (more properly called a hidden semi-Markov model, or {HSMM}) is like an {HMM} except each state can emit a sequence of observations. Let Y (Gt) be the subsequence emitted by “generalized state” Gt. The “generalized state” usually contains both the automaton state, Qt, and the length (duration) of the segment, Lt. We will define Y (Gt) to be the subsequence yt−l+1:t. After emitting a segment, the next state is Gtn , where tn = t + Lt. Similarly, denote the previous state by Gtp . Let Y (G + t ) be all observations following Gt, and Y (G − t ) be all observations preceeding Gt, as in Figure 1. Each segment Ot(q, l) def =P (Y (Gt){\textbar}Qt = q, Lt = l) can be an arbitrary distribution. If P (Y (Gt){\textbar}q, l) = ∏t i=t−l+1 P (yi{\textbar}q), this is an explicit duration {HMM} [Fer80, Lev86, Rab89, {MJ}93, {MHJ}95]. If P (Y (Gt){\textbar}q, l) is modelled by an {HMM} or state-space model (linear-dynamical system), this is called a segment model [{GY}93, {ODK}96]. In computational biology, P (Y (Gt){\textbar}q, l) is often modelled by a weight matrix or higher-order Markov chain (see e.g., [{BK}97]). In this paper, we are agnostic about the form of P (Y (Gt){\textbar}q, l). It is possible to approximate a variable-duration {HMM} by adding extra states to a regular {HMM} (see [{DEKM}98, p69]), i.e., a mixture of geometric distributions. However, our main interest will be segment models, which are strict generalizations of variable-duration {HMMs}. For the relationship between semi-Markov {HMMs}, pseudo-2D {HMMs}, hierarchical {HMMs}, etc., please see [Mur02]. (Essentially, with a pseudo-2D {HMMs}, we know the size of each segment ahead of time; an {HHMM} is a generalization of a segment model where each segment can have subsegments inside of it, each modelled by an {HMM}.) We can represent a variable-duration {HMMs} as a {DBN} as shown in {FIgure} 2. We explicitly add Qt , the remaining duration of state Qt, to the state-space. (Even though Qt is constant for a long period, we copy its value across every},
	author = {Murphy, K.},
	urldate = {2021-02-21},
	date = {2002},
	langid = {english},
	file = {Snapshot:C\:\\Users\\adria\\Zotero\\storage\\8739RIYU\\a2125e4fba6c69ff7a6d247bcf04312bb2b79606.html:text/html},
}




@misc{Petersen2008,
  	abstract = {Matrix identities, relations and approximations. A desktop reference for quick overview of mathematics of matrices.},
  	added-at = {2011-01-17T12:52:58.000+0100},
  	author = {Petersen, K. B. and Pedersen, M. S.},
  	biburl = {https://www.bibsonomy.org/bibtex/263c840382cc4b1efb8cefe447465b7ac/hkayabilisim},
  	file = {:home/hkaya/Projeler/diagnus/Screener/doc/literature/Petersen2008.pdf:PDF},
  	interhash = {6368b9b490c0225e22334ea0a0841a33},
  	intrahash = {63c840382cc4b1efb8cefe447465b7ac},
  	keywords = {matrixderivative inverse Matrixidentity matrixrelations},
  	month = oct,
  	note = {Version 20081110},
  	publisher = {Technical University of Denmark},
  	review = {Matrix Cookbook},
  	timestamp = {2011-01-17T12:52:58.000+0100},
  	title = {The Matrix Cookbook},
  	url = {http://www2.imm.dtu.dk/pubdb/p.php?3274},
  	year = 2008
}

@inproceedings{chang_2008,
    	title={Importance of semantic representation: dataless classification},
    	author={Chang, M.W. and Ratinov, L. and Roth, D. and Srikumar, V.},
	booktitle = {Proceedings of the 23rd National Conference on Artificial Intelligence},
	pages = {830-835},
    	year={2008}
}

@inproceedings{lampert_2009,
    	title={Learning to detect unseen object classes by betweenclass attribute transfer},
    	author={Lampert, C.H. and Nickisch, H. and Harmeling, S.},
	booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
	pages = {951-958},
    	year={2009}
}

@inproceedings{larochelle_2008,
    	title={Zero-data learning of new tasks},
    	author={Larochelle, H. and Erhan, D. and Bengio, Y.},
	booktitle = {Proceedings of the 23rd National Conference on Artificial Intelligence},
	pages = {646-651},
    	year={2008}
}

@article{rabiner_1989,
	author = "L. R. Rabiner",
	journal = "Proceedings of the IEEE",
	volume = "77",
	number = "2",
	pages = "257--285",
	title = "{A tutorial on hidden Markov models and selected applications in speech recognition}",
	year = "1989"
}

@book{rasmussen_gaussian_2006,
	location = {Cambridge, Mass},
	title = {Gaussian Processes for Machine Learning},
	isbn = {978-262-18253-9},
	abstract = {A comprehensive and self-contained introduction to Gaussian processes, which provide a principled, practical, probabilistic approach to learning in kernel machines.Gaussian processes ({GPs}) provide a principled, practical, probabilistic approach to learning in kernel machines. {GPs} have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of {GPs} in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the {PAC}-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.},
	author = {Carl Edward Rasmussen and Christopher K. I. Williams},
	year = {2006},
	publisher = {The MIT Press}
}

@article{sejdinovic_equivalence_2013,
	title = {Equivalence of distance-based and {RKHS}-based statistics in hypothesis testing},
	volume = {41},
	issn = {0090-5364},
	url = {http://arxiv.org/abs/1207.6076},
	doi = {10.1214/13-AOS1140},
	abstract = {We provide a unifying framework linking two classes of statistics used in two-sample and independence testing: on the one hand, the energy distances and distance covariances from the statistics literature; on the other, maximum mean discrepancies ({MMD}), that is, distances between embeddings of distributions to reproducing kernel Hilbert spaces ({RKHS}), as established in machine learning. In the case where the energy distance is computed with a semimetric of negative type, a positive definite kernel, termed distance kernel, may be defined such that the {MMD} corresponds exactly to the energy distance. Conversely, for any positive definite kernel, we can interpret the {MMD} as energy distance with respect to some negative-type semimetric. This equivalence readily extends to distance covariance using kernels on the product space. We determine the class of probability distributions for which the test statistics are consistent against all alternatives. Finally, we investigate the performance of the family of distance kernels in two-sample and independence tests: we show in particular that the energy distance most commonly employed in statistics is just one member of a parametric family of kernels, and that other choices from this family can yield more powerful tests.},
	pages = {2263--2291},
	number = {5},
	journaltitle = {The Annals of Statistics},
	shortjournal = {Ann. Statist.},
	author = {Sejdinovic, Dino and Sriperumbudur, Bharath and Gretton, Arthur and Fukumizu, Kenji},
	urldate = {2021-02-24},
	date = {2013-10},
	eprinttype = {arxiv},
	eprint = {1207.6076},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology, Mathematics - Statistics Theory},
	file = {arXiv Fulltext PDF:C\:\\Users\\adria\\Zotero\\storage\\3CNP3NEA\\Sejdinovic et al. - 2013 - Equivalence of distance-based and RKHS-based stati.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\adria\\Zotero\\storage\\DV9Z3K7R\\1207.html:text/html},
}


@inproceedings{sarkka_2012,
 author = {S. S\"{a}rk\"{a} and J. Hartikainen},
 booktitle = {15th International Conference on Artificial Intelligence and Statistics},
 pages = {993-1001},
 title = {Infinite-dimensional {K}alman filtering approach to spatio-temporal {G}aussian process regression},
 year = {2012}
}

@inproceedings{beal_2001,
 author = {Beal, Matthew and Ghahramani, Zoubin and Rasmussen, Carl},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {577--584},
 title = {The infinite hidden {M}arkov model},
 volume = {14},
 year = {2001}
}

@article{lawrence_2005,
	author = "Lawrence, N.D.",
	journal = "Journal of Machine Learning Research",
	number = "6",
	pages = "1783-1816",
	title = "{Probabilistic non-linear principal component analysis with {G}aussian process latent variable models}",
	year = "2005"
}

@inproceedings{lawrence_2007,
    title={Hierarchical {G}aussian {P}rocess latent variable models},
    author={Lawrence, N.D. and Moore, A.J.},
	booktitle = {Proceedings of the 24th {I}nternational {C}onference on {M}achine {L}earning},
	pages = {481-488},
    year={2007}
}


@inproceedings{vinyals_2016,
    	title={Matching Networks for One Shot Learning},
    	author={Vinyals, O. and Blundell, C. and Lillicrap, T. and Kavukcuoglu, K. and Wierstra, D.},
	booktitle = {Advances in Neural Information Processing Systems (NIPS)},
	pages = {3630-3638},
    	year={2016}
}

@inproceedings{snelson_2003,
 author = {Snelson, Edward and Ghahramani, Zoubin and Rasmussen, Carl},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 pages = {337-344},
 title = {Warped {G}aussian Processes},
 volume = {16},
 year = {2003}
}

@inproceedings{lazaro_2012,
 author = {L\'{a}zaro-Gredilla, Miguel},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {1619-1627},
 title = {Bayesian Warped {G}aussian Processes},
 volume = {25},
 year = {2012}
}




@InProceedings{kazlauskaite_2019,
  title = 	 {Gaussian Process Latent Variable Alignment Learning},
  author =       {Kazlauskaite, Ieva and Ek, Carl Henrik and Campbell, Neill},
  booktitle = 	 {Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics},
  pages = 	 {748--757},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89}
}

@inproceedings{duncker_2018,
 author = {Duncker, Lea and Sahani, Maneesh},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {10445-10455},
 title = {Temporal alignment and latent {G}aussian process factor inference in population spike trains},
 volume = {31},
 year = {2018}
}

@article{yu_hidden_2010,
	title = {Hidden semi-{M}arkov models},
	volume = {174},
	issn = {0004-3702},
	doi = {10.1016/j.artint.2009.11.011},
	series = {Special Review Issue},
	abstract = {As an extension to the popular hidden Markov model ({HMM}), a hidden semi-Markov model ({HSMM}) allows the underlying stochastic process to be a semi-Markov chain. Each state has variable duration and a number of observations being produced while in the state. This makes it suitable for use in a wider range of applications. Its forward–backward algorithms can be used to estimate/update the model parameters, determine the predicted, filtered and smoothed probabilities, evaluate goodness of an observation sequence fitting to the model, and find the best state sequence of the underlying stochastic process. Since the {HSMM} was initially introduced in 1980 for machine recognition of speech, it has been applied in thirty scientific and engineering areas, such as speech recognition/synthesis, human activity recognition/prediction, handwriting recognition, functional {MRI} brain mapping, and network anomaly detection. There are about three hundred papers published in the literature. An overview of {HSMMs} is presented in this paper, including modelling, inference, estimation, implementation and applications. It first provides a unified description of various {HSMMs} and discusses the general issues behind them. The boundary conditions of {HSMM} are extended. Then the conventional models, including the explicit duration, variable transition, and residential time of {HSMM}, are discussed. Various duration distributions and observation models are presented. Finally, the paper draws an outline of the applications.},
	pages = {215-243},
	number = {2},
	journal = {Artificial Intelligence},
	author = {Yu, S.Z.},
	urldate = {2021-02-21},
	year = {2010},
	langid = {english},
	keywords = {Explicit duration {HMM}, Forward–backward ({FB}) algorithm, Hidden Markov model ({HMM}), Hidden semi-Markov model ({HSMM}), Variable duration {HMM}, Viterbi algorithm},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\adria\\Zotero\\storage\\XUU8RU34\\Yu - 2010 - Hidden semi-Markov models.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\adria\\Zotero\\storage\\JFQ2WR2Y\\S0004370209001416.html:text/html},
}

@InProceedings{hughes_2015,
    title = {Reliable and Scalable Variational Inference for the hierarchical {D}irichlet Process},
    author = {Hughes, Michael and Kim, Dae Il and Sudderth, Erik},
    booktitle = {Proceedings of the 18th International Conference on Artificial Intelligence and Statistics},
    pages = {370--378},
    year = {2015},
}


@article{blei_2017,
    author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
    title = {Variational Inference: A Review for Statisticians},
    journal = {Journal of the American Statistical Association},
    volume = {112},
    number = {518},
    pages = {859--877},
    year = {2017}
}



@article{Kanagawa18,
    author = {Kanagawa, Motonobu and Hennig, Philipp and Sejdinovic, Dino and Sriperumbudur, Bharath},
    year = {2018},
    month = {07},
    pages = {},
    title = {Gaussian Processes and Kernel Methods: A Review on Connections and Equivalences}
}

@phdthesis{Seeger2005phd,
    author = {Seeger, Matthias},
    title = {Bayesian {G}aussian Process Models: {PAC-B}ayesian Generalisation Error Bounds and Sparse Approximations},
    year = {2005},
    school = {University of Edinburgh},
    keywords = {Bayes,Nonparametric,npbayes},
}


@inproceedings{dhaka_2020,
 author = {Dhaka, Akash Kumar and Catalina, Alejandro and Andersen, Michael R and Magnusson, M\aans and Huggins, Jonathan and Vehtari, Aki},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {10961--10973},
 title = {Robust, Accurate Stochastic Optimization for Variational Inference},
 volume = {33},
 year = {2020}
}

@inproceedings{saul_1995,
    author = {Saul, Lawrence and Jordan, Michael},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
    title = {Exploiting Tractable Substructures in Intractable Networks},
    volume = {8},
    year = {1995}
}

@inproceedings{sarkka10,
    author = {Hartikainen, Jouni and Särkkä, Simo},
    year = {2010},
    month = {10},
    pages = {379--384},
    title = {Kalman filtering and smoothing solutions to temporal Gaussian process regression models},
    doi = {10.1109/MLSP.2010.5589113}
}

@inproceedings{sarkka11,
author = {Särkkä, Simo},
year = {2011},
month = {06},
pages = {151-158},
title = {Linear Operators and Stochastic Partial Differential Equations in Gaussian Process Regression},
isbn = {978-3-642-21737-1},
doi = {10.1007/978-3-642-21738-8_20}
}

@inproceedings{HartikainenSarka11,
author="Hartikainen, Jouni and Riihim{\"a}ki, Jaakko
and S{\"a}rkk{\"a}, Simo",
editor="Honkela, Timo
and Duch, W{\l}odzis{\l}aw
and Girolami, Mark
and Kaski, Samuel",
title="Sparse Spatio-temporal Gaussian Processes with General Likelihoods",
booktitle="Artificial Neural Networks and Machine Learning -- ICANN 2011",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="193--200",
abstract="In this paper, we consider learning of spatio-temporal processes by formulating a Gaussian process model as a solution to an evolution type stochastic partial differential equation. Our approach is based on converting the stochastic infinite-dimensional differential equation into a finite dimensional linear time invariant (LTI) stochastic differential equation (SDE) by discretizing the process spatially. The LTI SDE is time-discretized analytically, resulting in a state space model with linear-Gaussian dynamics. We use expectation propagation to perform approximate inference on non-Gaussian data, and show how to incorporate sparse approximations to further reduce the computational complexity. We briefly illustrate the proposed methodology with a simulation study and with a real world modelling problem.",
isbn="978-3-642-21735-7"
}

@inproceedings{hartikainen2011,
  title={Sequential Inference for Latent Force Models},
  author={Hartikainen, Jouni and Särkkä, Simo},
  booktitle={Proceedings of the 27th Conf. Uncertainty in Artificial Intelligence},
  pages={311-318},
  year={2011}
}

@inproceedings{hartikainen2012,
  title={State-Space Inference for Nonlinear Latent Force Models with Application to Satellite Orbit Prediction},
  author={Hartikainen, Jouni and Seppänen, M. and Särkkä, Simo},
  booktitle={Proceedings of the 29th International Conference on Machine Learning},
  pages={903-910},
  year={2012}
}

@inproceedings{bryant_2012,
  title={Truly Nonparametric Online Variational Inference for Hierarchical {D}irichlet Processes},
  author={Bryant, Michael and Sudderth, Erik B.},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2708-2716},
  year={2012}
}

@book{west_1997,
	location = {New York},
	title = {Bayesian forecasting and dynamic models},
	author = {West, M. and Harrison, J.},
	year = {1997},
	publisher = {Springer}
}

@phdthesis{beal_2003,
    title        = {Variational algorithms for approximate {B}ayesian inference},
    author       = {Beal, Mathew James},
    year         = {2003},
    school       = {University of London},
    type         = {PhD thesis}
}

@article{taddei_1992,
    author = {Taddei, A. and Distante, G. and Emdin, M. and Pisani, P. and Moody, G.B. and Zeelenberg, C. and Marchesi, C.},
    year = {1992},
    journal = {European Heart Journal},
    volume = {13},
    pages = {1164-1172},
    title = {The {E}uropean {ST-T} {D}atabase: standard for evaluating systems for the analysis of {ST-T} changes in ambulatory electrocardiography}
}

@article{madison1995,
    author = {Madison S. Spach  and J. Francis Heidlage },
    title = {The Stochastic Nature of Cardiac Propagation at a Microscopic Level },
    journal = {Circulation Research},
    volume = {76},
    number = {3},
    pages = {366-380},
    year = {1995}
}

@book{malik_1996,
	location = {New York},
	title = {Heart rate variability},
	author = {Malik, Ml and Camm, A. J.},
	year = {1996},
	publisher = {Futura Publishing Company}
}

@article{moody2001,
    author = {Moody, G. and Mark, R. },
    title = {The impact of the {MIT-BIH} {A}rrhythmia {D}atabase},
    journal = {IEEE Engineering in Medicine and Biology Magazine},
    volume = {20},
    number = {3},
    pages = {45-50},
    year = {2001}
}

@article{lagerholm2000,
    author = {Lagerholm, M. and Peterson, C. and Braccini, G. and Edenbrandt, L. and S\"ornmo, L.},
    title = {Clustering {ECG} complexes using {H}ermite functions and self-organizing maps},
    journal = {IEEE Transactions on Biomedical Engineering},
    volume = {47},
    number = {7},
    pages = {838-848},
    year = {2000}
}

@article{castro2015,
    author = {Castro, D. and F\'elix, P. and Presedo, J.},
    title = {A method for context-based adaptive {QRS} clustering in real time},
    journal = {IEEE Journal of Biomedical and Health Informatics},
    volume = {19},
    number = {5},
    pages = {1660-1671},
    year = {2015}
}

@article{Iyengar1996,
    author = {Iyengar, N. and Peng, C. K. and Morin, R. and Goldberger, A. L. and Lipsitz, L. A.},
    title = {Age-related alterations in the fractal scaling of cardiac interbeat interval dynamics},
    journal = {The American Journal of Physiology},
    volume = {271},
    number = {4},
    pages = {R1078-R1084},
    year = {1996}
}

@inproceedings{Penzel2000,
    author = {Penzel, T. and Moody, G.B. and Mark, R.G. and Goldberger, A.L. and Peter, J.H.},
    title = {The Apnea-{ECG} Database},
    booktitle = {Computers in Cardiology},
    number = {27},
    pages = {255-258},
    year = {2000}
}


@article{nieto2014,
    author = {Nieto-Barajas, Luis E. and Contreras-Cristi{\'a}n, Alberto},
    year = {2014},
    month = {03},
    pages = {147-170},
    title = {A {B}ayesian Nonparametric Approach for Time Series Clustering},
    volume = {9},
    journal = {Bayesian Analysis},
    publisher={International Society for Bayesian Analysis},
    doi = {10.1214/13-BA852}
}

@InProceedings{chakraborty2023,
  title = {Scalable nonparametric {B}ayesian learning for dynamic velocity fields},
  author = {Chakraborty, Sunrit and Guha, Aritra and Lei, Rayleigh and Nguyen, XuanLong},
  booktitle = {Proceedings of the 39th Conference on Uncertainty in Artificial Intelligence},
  pages = {282--292},
  year = {2023},
  volume = {216}
}

@article{hamilton2010,
title = {Characterising spectral sea wave conditions with statistical clustering of actual spectra},
journal = {Applied Ocean Research},
volume = {32},
number = {3},
pages = {332-342},
year = {2010},
issn = {0141-1187},
doi = {https://doi.org/10.1016/j.apor.2009.12.003},
author = {L.J. Hamilton}
}

@ARTICLE{zhou2013,
  author={Zhou, Feng and De la Torre, Fernando and Hodgins, Jessica K.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Hierarchical Aligned Cluster Analysis for Temporal Clustering of Human Motion}, 
  year={2013},
  volume={35},
  number={3},
  pages={582-596},
  keywords={Kernel;Time series analysis;Humans;Motion segmentation;Clustering algorithms;Heuristic algorithms;Legged locomotion;Temporal segmentation;time series clustering;time series visualization;human motion analysis;kernel k-means;spectral clustering;dynamic programming},
  doi={10.1109/TPAMI.2012.137}
}


@inproceedings{huynh2015,
  title={Streaming Variational Inference for {D}irichlet Process Mixtures},
  author={Huynh, Lam and Bui, Thang and Turner, Richard E.},
  booktitle={Proceedings of the 7th Asian Conference on Machine Learning},
  pages={237--252},
  year={2015}
}

@article{mcdowell2018,
    doi = {10.1371/journal.pcbi.1005896},
    author = {McDowell, Ian C. AND Manandhar, Dinesh AND Vockley, Christopher M. AND Schmid, Amy K. AND Reddy, Timothy E. AND Engelhardt, Barbara E.},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Clustering gene expression time series data using an infinite {G}aussian process mixture model},
    year = {2018},
    month = {01},
    volume = {14},
    pages = {1-27},
    number = {1}
}

@inproceedings{sykacek2001,
 author = {Sykacek, Peter and Roberts, Stephen J},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {937-944},
 title = {Bayesian time series classification},
 volume = {14},
 year = {2001}
}

@article{wulsin2014,
title = {Modeling the complex dynamics and changing correlations of epileptic events},
journal = {Artificial Intelligence},
volume = {216},
pages = {55-75},
year = {2014},
doi = {https://doi.org/10.1016/j.artint.2014.05.006},
author = {Drausin F. Wulsin and Emily B. Fox and Brian Litt}
}

@inproceedings{broderick2013,
 author = {Broderick, Tamara and Boyd, Nicholas and Wibisono, Andre and Wilson, Ashia C and Jordan, Michael I},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 title = {Streaming Variational {B}ayes},
 volume = {26},
 year = {2013}
}

@article{beal04,
    author = {Beal, Matthew J. and Falciani, Francesco and Ghahramani, Zoubin and Rangel, Claudia and Wild, David L.},
    title = "{A Bayesian approach to reconstructing genetic regulatory networks with hidden factors}",
    journal = {Bioinformatics},
    volume = {21},
    number = {3},
    pages = {349--356},
    year = {2004},
    abstract = "{Motivation: We have used state-space models (SSMs) to reverse engineer transcriptional networks from highly replicated gene expression profiling time series data obtained from a well-established model of T cell activation. SSMs are a class of dynamic Bayesian networks in which the observed measurements depend on some hidden state variables that evolve according to Markovian dynamics. These hidden variables can capture effects that cannot be directly measured in a gene expression profiling experiment, for example: genes that have not been included in the microarray, levels of regulatory proteins, the effects of mRNA and protein degradation, etc.Results: We have approached the problem of inferring the model structure of these state-space models using both classical and Bayesian methods. In our previous work, a bootstrap procedure was used to derive classical confidence intervals for parameters representing ‘gene–gene’ interactions over time. In this article, variational approximations are used to perform the analogous model selection task in the Bayesian context. Certain interactions are present in both the classical and the Bayesian analyses of these regulatory networks. The resulting models place JunB and JunD at the centre of the mechanisms that control apoptosis and proliferation. These mechanisms are key for clonal expansion and for controlling the long term behavior (e.g. programmed cell death) of these cells.Availability: Supplementary data is available at http://public.kgi.edu/wild/index.htm and Matlab source code for variational Bayesian learning of SSMs is available at http://www.cse.buffalo.edu/faculty/mbeal/software.htmlContact: David\_Wild@kgi.edu}",
    issn = {1367-4803}
}


@InProceedings{alvarez09,
  title = 	 {Latent Force Models},
  author = 	 {Álvarez, Mauricio and Luengo, David and Lawrence, Neil D.},
  booktitle = 	 {Proceedings of the 12th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {9--16},
  year = 	 {2009},
  volume = 	 {5}
}

@article{harrison1976,
    author = {P. J. Harrison and C. F. Stevens},
    journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
    number = {3},
    pages = {205--247},
    title = {Bayesian Forecasting},
    volume = {38},
    year = {1976}
}

@article{hamilton1989,
    author = {James D. Hamilton},
    journal = {Econometrica},
    number = {2},
    pages = {357--384},
    title = {A New Approach to the Economic Analysis of Nonstationary Time Series and the Business Cycle},
    volume = {57},
    year = {1989}
}

@article{kim1994,
title = {Dynamic linear models with {M}arkov-switching},
journal = {Journal of Econometrics},
volume = {60},
number = {1},
pages = {1--22},
year = {1994},
author = {Chang-Jin Kim}
}

@inproceedings{pavlovic2000,
    author = {Pavlovic, Vladimir and Rehg, James M and MacCormick, John},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {T. Leen and T. Dietterich and V. Tresp},
    title = {Learning Switching Linear Models of Human Motion},
    volume = {13},
    year = {2000}
}

@inproceedings{damianou11,
    author = {Damianou, Andreas C. and Titsias, Michalis K. and Lawrence, Neil D.},
    title = {Variational {G}aussian process dynamical systems},
    year = {2011},
    abstract = {High dimensional time series are endemic in applications of machine learning such as robotics (sensor data), computational biology (gene expression data), vision (video sequences) and graphics (motion capture data). Practical nonlinear probabilistic approaches to this data are required. In this paper we introduce the variational Gaussian process dynamical system. Our work builds on recent variational approximations for Gaussian process latent variable models to allow for nonlinear dimensionality reduction simultaneously with learning a dynamical prior in the latent space. The approach also allows for the appropriate dimensionality of the latent space to be automatically determined. We demonstrate the model on a human motion capture data set and a series of high resolution video sequences.},
    booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
    pages = {2510--2518}
}

@ARTICLE{li2005,
    author={Rong Li, X. and Jilkov, V.P.},
    journal={IEEE Transactions on Aerospace and Electronic Systems}, 
    title={Survey of maneuvering target tracking. Part V. Multiple-model methods}, 
    year={2005},
    volume={41},
    number={4},
    pages={1255--1321}
}

@article{paoletti2007,
    title = {Identification of Hybrid Systems A Tutorial},
    journal = {European Journal of Control},
    volume = {13},
    number = {2},
    pages = {242--260},
    year = {2007},
    author = {Simone Paoletti and Aleksandar Lj. Juloski and Giancarlo Ferrari-Trecate and René Vidal}
}

@ARTICLE{qi2007,
  author={Qi, Yuting and Paisley, John William and Carin, Lawrence},
  journal={IEEE Transactions on Signal Processing}, 
  title={Music Analysis Using Hidden {M}arkov Mixture Models}, 
  year={2007},
  volume={55},
  number={11},
  pages={5209--5224},
  keywords={Hidden Markov models;Multiple signal classification;Statistical distributions;Bayesian methods;Monte Carlo methods;Libraries;Databases;Robustness;Music information retrieval;Machine learning;Dirichlet process;hidden Markov model (HMM) mixture;Markov chain Monte Carlo (MCMC);music;variational Bayes},
  doi={10.1109/TSP.2007.898782}
}

@article{mackay1992,
    author = {MacKay, David J. C.},
    title = {Bayesian Interpolation},
    journal = {Neural Computation},
    volume = {4},
    number = {3},
    pages = {415-447},
    year = {1992}
}

@ARTICLE{sakoe1978,
    author={Sakoe, H. and Chiba, S.},
    journal={IEEE Transactions on Acoustics, Speech, and Signal Processing}, 
    title={Dynamic programming algorithm optimization for spoken word recognition}, 
    year={1978},
    volume={26},
    number={1},
    pages={43-49}
}


@inproceedings{frigola14,
author = {Frigola, Roger and Chen, Yutian and Rasmussen, Carl E.},
title = {Variational {G}aussian process state-space models},
year = {2014},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3680–3688}
}

@article{ferguson_1973,
author = {Thomas S. Ferguson},
title = {A {B}ayesian Analysis of Some Nonparametric Problems},
volume = {1},
journal = {The Annals of Statistics},
number = {2},
publisher = {Institute of Mathematical Statistics},
pages = {209-230},
year = {1973}
}

@article{antoniak74,
author = {Charles E. Antoniak},
title = {{Mixtures of {D}irichlet Processes with Applications to {B}ayesian Nonparametric Problems}},
volume = {2},
journal = {The Annals of Statistics},
number = {6},
publisher = {Institute of Mathematical Statistics},
pages = {1152-1174},
year = {1974}
}


@article{sethuraman_1994,
 author = {Jayaram Sethuraman},
 journal = {Statistica Sinica},
 number = {2},
 pages = {639--650},
 publisher = {Institute of Statistical Science, Academia Sinica},
 title = {A constructive definition of {D}irichlet priors},
 urldate = {2024-12-05},
 volume = {4},
 year = {1994}
}

@article{teh_2006,
author = {Teh, Y. W. and Jordan, M. I. and Beal, M. J. and Blei, D. M.},
title = {Hierarchical {D}irichlet Processes},
journal = {Journal of the American Statistical Association},
volume = {101},
number = {476},
pages = {1566--1581},
year = {2006},
publisher = {ASA Website}
}

@inproceedings{teh04,
 author = {Teh, Yee W. and Jordan, Michael I. and Beal, Matthew J. and Blei, David M.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {L. Saul and Y. Weiss and L. Bottou},
 title = {Sharing Clusters among Related Groups: Hierarchical {D}irichlet Processes},
 volume = {17},
 year = {2004}
}


@inproceedings{rasmussen99,
 author = {Rasmussen, Carl},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 title = {The Infinite {G}aussian Mixture Model},
 volume = {12},
 year = {1999}
}

@InProceedings{mikheeva2022,
  title = {Aligned Multi-Task {G}aussian Process},
  author = {Mikheeva, O. and Kazlauskaite, I. and Hartshorne, A. and Kjellstr\"om, H. and Ek, C. H. and Campbell, N. D. F.},
  booktitle = {Proceedings of the 25th International Conference on Artificial Intelligence and Statisticd},
  year = {2022},
  volume = {151}
}

@article{quinonero-candela05,
  author  = {Joaquin Qui{{\~n}}onero-Candela and Carl Edward Rasmussen},
  title   = {A Unifying View of Sparse Approximate Gaussian Process Regression},
  journal = {Journal of Machine Learning Research},
  year    = {2005},
  volume  = {6},
  number  = {65},
  pages   = {1939--1959},
  url     = {http://jmlr.org/papers/v6/quinonero-candela05a.html}
}

@article{alvarez2012,
author = {\'{A}lvarez, Mauricio A. and Rosasco, Lorenzo and Lawrence, Neil D.},
title = {Kernels for Vector-Valued Functions: A Review},
year = {2012},
volume = {4},
number = {3},
journal = {Foundations and Trends in Machine Learning},
pages = {195–266}
}

@inproceedings{csato2000,
 author = {Csat\'{o}, Lehel and Opper, Manfred},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 pages = {},
 publisher = {MIT Press},
 title = {Sparse Representation for Gaussian Process Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/file/56f9f88906aebf4ad985aaec7fa01313-Paper.pdf},
 volume = {13},
 year = {2000}
}


@ARTICLE{bach2004,
  author={Bach, F.R. and Jordan, M.I.},
  journal={IEEE Transactions on Signal Processing}, 
  title={Learning graphical models for stationary time series}, 
  year={2004},
  volume={52},
  number={8},
  pages={2189-2199},
  keywords={Graphical models;Signal processing algorithms;Hidden Markov models;Frequency domain analysis;Machine learning algorithms;Machine learning;Time series analysis;Spectral analysis;Biomedical signal processing;Inference algorithms},
  doi={10.1109/TSP.2004.831032}
}

@ARTICLE{biernacki2000,
  author={Biernacki, C. and Celeux, G. and Govaert, G.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Assessing a mixture model for clustering with the integrated completed likelihood}, 
  year={2000},
  volume={22},
  number={7},
  pages={719-725},
  keywords={Bayesian methods;Numerical simulation;Context modeling;Probability distribution;Robustness;Gaussian distribution},
  doi={10.1109/34.865189}
}


@article{zhong2003,
author = {Zhong, Shi and Ghosh, Joydeep},
year = {2003},
month = {12},
pages = {1001-1037},
title = {A Unified Framework for Model-based Clustering},
volume = {4},
journal = {Journal of Machine Learning Research},
doi = {10.1162/1532443041827943}
}

@inproceedings{smyth1996,
 author = {Smyth, Padhraic},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 pages = {648-654},
 title = {Clustering Sequences with Hidden {M}arkov Models},
 volume = {9},
 year = {1996}
}

@ARTICLE{chiu2022,
  author={Chiu, Chun Wai and Minku, Leandro L.},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={A Diversity Framework for Dealing With Multiple Types of Concept Drift Based on Clustering in the Model Space}, 
  year={2022},
  volume={33},
  number={3},
  pages={1299-1309},
  keywords={Predictive models;Memory management;Data models;Monitoring;Prediction algorithms;Detectors;Training;Clustering in the model space;concept drift;diversity;online ensemble learning;recurring concepts},
  doi={10.1109/TNNLS.2020.3041684}
}

@article{kolter2007,
  author  = {J. Zico Kolter and Marcus A. Maloof},
  title   = {Dynamic Weighted Majority: An Ensemble Method for Drifting Concepts},
  journal = {Journal of Machine Learning Research},
  year    = {2007},
  volume  = {8},
  number  = {91},
  pages   = {2755-2790},
  url     = {http://jmlr.org/papers/v8/kolter07a.html}
}

@article{kahlegi2016,
  author  = {Azadeh Khaleghi and Daniil Ryabko and J{{\'e}}r{{\'e}}mie Mary and Philippe Preux},
  title   = {Consistent Algorithms for Clustering Time Series},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {3},
  pages   = {1--32},
  url     = {http://jmlr.org/papers/v17/khaleghi16a.html}
}

@article{hensman2015,
  author={Hensman, James and Rattray, Magnus and Lawrence, Neil D.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Fast Nonparametric Clustering of Structured Time-Series}, 
  year={2015},
  volume={37},
  number={2},
  pages={383-393},
  keywords={Gaussian processes;Data models;Time series analysis;Biological system modeling;Computational modeling;Optimization;Vectors;Variational Bayes;Gaussian processes;structured time series;gene expression},
  doi={10.1109/TPAMI.2014.2318711}
}


@InProceedings{linderman2017,
  title = 	 {{Bayesian Learning and Inference in Recurrent Switching Linear Dynamical Systems}},
  author = 	 {Linderman, Scott and Johnson, Matthew and Miller, Andrew and Adams, Ryan and Blei, David and Paninski, Liam},
  booktitle = 	 {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {914--922},
  year = 	 {2017},
  volume = 	 {54},
  series = 	 {Proceedings of Machine Learning Research}
}

