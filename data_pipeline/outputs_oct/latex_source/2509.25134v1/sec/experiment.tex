\section{Experiments}
\label{sec:experiments}

\begin{figure*}[t]
    \centering
    \begin{subfigure}{0.495\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/main-comparison-wo-text.pdf}
        \caption{Without text layers}
        \label{fig:main_comparison_wo_text}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.495\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/main-comparison.pdf}
        \caption{All layers}
        \label{fig:main_comparison_all}
    \end{subfigure}
    \caption{Baseline comparisons. We show visual quality metrics (RGB L1, Alpha IoU) as the maximum number of allowed edits increases. The left two are the results when we exclude text layers from the dataset, and the right two are the results when all layers are included. ``w/o text training'' indicates the case where text layers are not included during training.}
    \label{fig:main_comparison}
\end{figure*}



\subsection{Datasets}
We use the Crello dataset~\cite{yamaguchi2021canvasvae}, which is a collection of graphic design templates, for both training and evaluation.
We obtain pairs of input images and their ground-truth layer sequences from the layer structure in design templates.
We follow the data split of v5.1.0 %
to obtain 19,478 / 1,852 / 1,971 samples for train, validation, and test split, respectively.
We resize all images to maintain their aspect ratio, with the shorter side set to 512.
In this work, we exclude transparent layers from the evaluation since neither our method nor the baselines primarily focuses on accurate transparency estimation. %
For all methods, we conduct training and validation on the train and validation split, respectively, and report the results on the test split.
For training of \ours{} as described in \cref{sec:method:training}, we generate input / ground-truth pairs, and finally obtain 48,725 and 4,674 pairs for the train and validation, respectively.
As typography is one of the unique domain properties, we prepare the full dataset and the variant that excludes all text layers for evaluation.

\subsection{Baselines}
Although there are a few methods comparable to \ours{}, none of them have publicly available code or models.
We design the following baseline and implement an existing approach with minor modifications to fit our setting.
Additionally, we evaluate all methods using Hi-SAM~\cite{ye2024hi}, a state-of-the-art text segmentation, for initial layer extraction.


\paragraph{YOLO baseline}
We design a naive baseline that combines state-of-the-art object detection and pretrained segmentation models.
First, since most graphic designs contain text on the top, we segment text using Hi-SAM~\cite{ye2024hi} and complete the background using LaMa~\cite{lama}.
Next, we detect bounding boxes of layers from the remaining background.
To this end, we extract bounding boxes from the layer structure of Crello and fine-tune YOLO~\cite{wang2024yolov9} using them.
We determine the z-index of layers based on heuristics in graphic design, assuming that the smaller box is in front when the boxes overlap.
Then, we obtain the segmentation masks of the topmost boxes using a pretrained SAM2~\cite{ravi2024sam} and perform background completion.
We repeat this process, except for text segmentation, until the number of detections becomes zero.
We obtain the final layers by replacing the alpha of the input or completion image with the predicted segmentation mask and blacking out the color of pixels with an alpha close to zero.


\paragraph{VLM baseline}
We also consider a baseline that follows the approach of Accordion~\cite{accordion}.
Accordion generates layered graphic design by combining raster-based image generation~\cite{flux} and layer decomposition, where VLM first takes an image as input and generates a decomposition plan with a JSON-like description of bounding boxes and z-indices of layers, and then applies segmentation and background completion in a front-to-back order to obtain the layer sequence.
Since the model and training data are not publicly available, we reproduce this with minor modifications in our experiment.
We use PaliGemma2~\cite{steiner2024paligemma} as the backbone VLM and fine-tune on the Crello train set, Hi-SAM for text detection, and SAM2 for other elements.
For background completion, we use LaMa~\cite{lama} to ensure fairness with other methods.

\subsection{Implementation Details}
We use BiRefNet~\cite{birefnet} with Swin-L~\cite{liu2021swin} pre-trained on natural image object segmentation 
for the top-layers matting model, and train it on Crello for 60 epochs with a batch size of 12.
We set the maximum number of iterations for decomposition to 3 for \ours{} and the YOLO baseline.
The maximum number of colors for palette-based refinement is set to 10 for the foreground and 2 for the background.
For evaluation, we change the maximum number of allowed edits from 0 to 5, and report the visual quality for each case.

\begin{figure}[t]
    \centering
    \includegraphics[keepaspectratio, width=1.0\linewidth]{figures/refine-ablation.pdf}
    \caption{Ablation results of foreground color estimation and refinement.
    }
    \label{fig:refine_ablation}
\end{figure}

\begin{figure}[tb]
    \centering
        \includegraphics[width=\linewidth]{figures/baseline-comparison.pdf}
    \caption{Comparison of decomposition results by \ours{} and baselines. 
    The leftmost images are the input image (\ours{}), object detections (VLM), and text-removed input (YOLO).
    Red rectangles indicate text, and blue rectangles indicate other elements.
    }
    \label{fig:vis_main_comparison}
\end{figure}

\subsection{Quantitative Evaluation}

\paragraph{Baseline comparison}
We compare \ours{} with baselines with and without text layers in \cref{fig:main_comparison_all} and \cref{fig:main_comparison_wo_text}, respectively.
In all metrics, \ours{} generates layer sequences close to the ground truth with fewer edits.
Our simplified pipeline and training objective are effective in layer decomposition.
Moreover, in the results for all layers (\cref{fig:main_comparison_all}), \ours{} alone shows higher performance than \ours{} + Hi-SAM, which replaces the first iteration with Hi-SAM.
\ours{}, which is specifically trained for graphic design, is more effective than Hi-SAM, which is trained for text segmentation without being limited to graphic design.
More interestingly, in the case of decomposition without text layers, as shown in \cref{fig:main_comparison_wo_text}, \ours{} trained with text layers exhibits slightly better performance than \ours{} trained without text, even though the decomposition targets contain no text.
We suspect that text is essentially a variant of vector shapes, and training with text layers improves the decomposition performance of these elements.
Our method outperforms the BiRefNet without additional training in \cref{fig:main_comparison_all}, indicating the importance of training on top-layer matting.

\paragraph{Refinement ablation}
\cref{fig:refine_ablation} shows the ablation results of foreground color estimation and refinement.
First, the color estimation by the inverse blending (\cref{eq:unblend}) reduces the RGB L1 compared to the ``Naive'', which simply replaces the alpha with the predicted mask.
Background refinement significantly improves the RGB L1, resulting in better subsequent layer decomposition, as indicated by the improvement in Alpha IoU.
The foreground refinement slightly improves the quality of the alpha map. Although the quantitative improvement is slight, we observe that foreground refinement improves the quality of boundary regions.





\subsection{Qualitative Evaluation}

\paragraph{Baseline comparison}
\cref{fig:vis_main_comparison} shows the qualitative comparison of \ours{} with VLM and YOLO baseline.
The VLM baseline, which relies on bounding boxes, struggles with proper decomposition when detection fails or when the detected boxes overlap.
The bounding box of a layer with a large hole like a donut includes the elements of the entire image, making it difficult for the subsequent segmentation (\cref{fig:vis_main_comparison} top sample).
The YOLO baseline suffers from false negatives in detection, resulting in incomplete decomposition.
In contrast, \ours{} directly extracts layers without relying on bounding boxes, resulting in clean decomposition results in all cases.


\paragraph{Refinement effect}

\cref{fig:vis_fg_refine} shows an example of foreground refinement.
The foreground refinement recovers the large missing gold decoration with a large hole, which is a typical failure case of the top-layers matting model.
In \cref{fig:vis_bg_refine}, we show an example of background refinement.
Background refinement successfully completes the missing part of the background.
Since layer decomposition is recursive, failures in the previous iteration have a negative impact on subsequent iterations.
Our refinement contributes not only to the quality of the target layer itself but also to the quality of the subsequent layers, \ie, the overall quality of the layer decomposition.



\begin{figure}[tb]
    \centering
        \includegraphics[width=1\linewidth]{figures/fg-refine-ablation.pdf}
    \caption{Example with or without foreground refinement. Without refinement, layers tend to have a collapsed boundary.}
    \label{fig:vis_fg_refine}
\end{figure}

\begin{figure}[tb]
    \centering
        \includegraphics[width=1\linewidth]{figures/bg-refine-ablation.pdf}
        \caption{Example with or without background refinement. Our refinement prevents the background from being divided into segments or filled with unexpected colors.}
    \label{fig:vis_bg_refine}
\end{figure}


\begin{figure}[tb]
    \centering
    \includegraphics[keepaspectratio, width=1\linewidth]{figures/genai.pdf}
    \caption{Decomposition results of \ours{} on raster images generated by FLUX.1 [dev]~\cite{flux}.}
    \label{fig:genai}
\end{figure}


\subsection{Applications}

In this section, we demonstrate that \ours{} enables a few applications out of the box.

\paragraph{Decomposing generated images}
\cref{fig:genai} shows the decomposition results of \ours{} on graphic design generated by FLUX.1 [dev]~\cite{flux}, which is one of the state-of-the-art text-to-image generator, using prompts from DESIGNERINTENTION-v2 benchmark~\cite{jia2023cole}.
Note that quantitative evaluation is not possible in this setup as ground truth layers are not available.
The results suggest that \ours{} can generalize and successfully decompose generated images, enabling an editable workflow for raster image generators.

\paragraph{Image editing}
We show the image editing examples using the decomposed layers by \ours{} in \cref{fig:teaser} and \suppref{sup:application}.
Here, we only perform simple operations such as color conversion, translation, and resizing at the layer level.
Although the layers obtained by \ours{} are grouped by top-layers, we can easily divide them into connected components for more granular editing.
Our decomposition enables flexible and intuitive layer-based editing.
Note also that there is no significant artifact in the editing results.
