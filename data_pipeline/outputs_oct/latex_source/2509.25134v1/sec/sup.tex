\clearpage
\setcounter{figure}{0}
\renewcommand{\thefigure}{\Alph{figure}}
\renewcommand{\thetable}{\Alph{table}}
\renewcommand{\thealgorithm}{\Alph{algorithm}}
\maketitlesupplementary
\appendix

\section{Editing Examples}
\label{sup:application}
We show editing examples in \cref{fig:application-examples}.
Here, we use \ours{} to decompose the input image into layers, divide each layer into connected components, and group text components using CRAFT~\cite{baek2019character} to facilitate editing.
We import the layers into PowerPoint\footnote{\url{https://www.microsoft.com/powerpoint}} and perform various edits, from simple layout manipulation to applying built-in image effects, \emph{at the layer level}.
As the examples show, once the images are decomposed, users can intuitively edit them with precise control over each graphic element.

\section{Additional Results}
\label{sup:results}
We present additional examples of decomposed graphic design images using our method in \cref{fig:sup-qualitative-1,fig:sup-qualitative-2}.
These examples are selected from the Crello~\cite{yamaguchi2021canvasvae} test set and demonstrate the effectiveness of our method across diverse design styles.


\section{Failure Cases}
\label{sup:failure}
In \cref{fig:sup-failure-1,fig:sup-failure-2}, we show typical failure cases of our method.
The first set of failure cases (\cref{fig:sup-failure-1}) involves objects that are too small, such as detailed text descriptions, which are challenging to decompose due to their limited spatial extent.
We believe that these can be mitigated by increasing the resolution of the input images.
The second set of failure cases (\cref{fig:sup-failure-2}) is due to the ambiguity of the layer granularity.
For these samples, it is difficult even for humans to decompose them into the same layers consistently.
Although our evaluation metrics account for such ambiguity, we may need to improve training objectives or the post-refinement process to address these cases.

\section{User Study}
\label{sup:userstudy}
We conduct a user study in which 21 cloudworkers experienced in layer-based image editing rate the practical utility of 50 decomposition results---randomly ordered and anonymized---from \ours{} and our two baselines on the same images using a five-point scale.
\cref{tab:user-study} summarizes the results of the user study.
\ours{} achieves the highest average score, and a significant majority of the users (71.4\%) rate \ours{} the highest average score.
This result further emphasizes the practical superiority of our method.

\begin{table}[h]
    \centering
    \caption{
        Results of the user study. We report the average score, the number of users who rate each method as the best on average across all samples (\#Pref. users), and the number of samples for which each method is rated the best on average across all users (\#Win samples).
    }
    \begin{tabular}{lccc}
        \toprule
        & Score & \#Pref. users & \#Win samples\\
        \midrule
        LayerD & \bf{3.74} & \bf{15 (71.4\%)} & \bf{27 (54.0\%)} \\
        YOLO base & 3.52 & 2 (9.5\%) & 15 (30.0\%) \\
        VLM base & 3.31 & 4 (19.0\%) & 8 (16.0\%) \\
        \bottomrule
    \end{tabular}
    \label{tab:user-study}
\end{table}

\begin{figure*}[h]
    \centering
    \includegraphics[keepaspectratio, width=0.89\linewidth]{figures/application-examples-vis.pdf}
    \caption{Editing examples on Crello~\cite{yamaguchi2021canvasvae} test set.
    The leftmost images are the original images, and the remaining images are edited ones based on the decomposed layers. We use \ours{} to decompose the original images into layers, divide them into connected components, and group text components using CRAFT~\cite{baek2019character}.
    Then, we perform various \emph{layer-level} edits, from simple layout changes to applying built-in image effects, on PowerPoint.}
    \label{fig:application-examples}
\end{figure*}



\begin{figure*}[h]
    \centering
    \includegraphics[keepaspectratio, width=0.85\linewidth]{figures/additional-samples-1.pdf}
    \caption{Additional qualitative results of our method on Crello~\cite{yamaguchi2021canvasvae} test set.
    The leftmost column shows the input image, and the remaining columns show the decomposed layers from back to front.
    }
    \label{fig:sup-qualitative-1}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[keepaspectratio, width=0.85\linewidth]{figures/additional-samples-2.pdf}
    \caption{Additional qualitative results of our method on Crello~\cite{yamaguchi2021canvasvae} test set.
    The leftmost column shows the input image, and the remaining columns show the decomposed layers from back to front.
    }
    \label{fig:sup-qualitative-2}
\end{figure*}


\begin{figure*}[h]
    \centering
    \includegraphics[keepaspectratio, width=0.82\linewidth]{figures/failure-samples-small-object.pdf}
    \caption{Failure samples for too small objects on Crello~\cite{yamaguchi2021canvasvae} test set.
    The leftmost column shows the input image, and the remaining columns show the decomposed layers from back to front.
    }
    \label{fig:sup-failure-1}
\end{figure*}


\begin{figure*}[h]
    \centering
    \includegraphics[keepaspectratio, width=0.82\linewidth]{figures/failure-samples-granularity.pdf}
    \caption{Failure samples due to the ambiguity of the layer granularity on Crello~\cite{yamaguchi2021canvasvae} test set.
    The leftmost column shows the input image, and the remaining columns show the decomposed layers from back to front.
    }
    \label{fig:sup-failure-2}
\end{figure*}


\section{Influence of Matting and Inpainting Model Choices}
We vary the matting backbones (Swin-L/T~\cite{liu2021swin}, PVT-M/S~\cite{wang2021pyramid}) and replace the inpainting model with FLUX.1 Fill [dev]~\cite{blackforest2024fluxfill} and evaluate their influence.
The larger matting models improve performance while using FLUX.1 Fill [dev] shows significant degradation.
Generative inpainting often introduces unwanted objects, which interfere with subsequent decomposition steps.
This highlights the need for graphic design-specific inpainting as well as refinement.

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/matting-model-ablation.pdf}
    \caption{Results with different matting backbones, SwinTransformer~\cite{liu2021swin} and PVT~\cite{wang2021pyramid} variants. The inpainting model is fixed to LaMa~\cite{lama}.}
    \label{fig:matting_ablation}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/impaint-model-ablation.pdf}
    \caption{Results with different inpainting models, LaMa~\cite{lama} and FLUX~\cite{flux}.}
    \label{fig:inpaint_ablation}
  \end{subfigure}

  \caption{Evaluation results of \ours{} with different matting (a) and inpainting model (b) choices.}
  \label{fig:ablation}
\end{figure}





\section{Detail of Decomposition Metrics}
\subsection{Dynamic Time Warping}
\label{sup:dtw}
We implement the Dynamic Time Warping (DTW) as shown in \cref{alg:dtw}. 
Given decomposition results $\hat{Y}=(\hat{\bm{l}}_k)_{k=0}^{K}$ and ground truth $Y=(\bm{l}_q)_{q=0}^{Q}$, the output pairs must include $(0,0)$ and $(K,Q)$ as the start point and end point with a step size of 1, and every layer must be included in at least one pair. An average distance is then computed over all pairs as the final output.
\begin{algorithm}[]
\caption{Dynamic Time Warping (DTW)}
\label{alg:dtw}
\definecolor{codeblue}{rgb}{0.25,0.5,0.5}
\definecolor{codekw}{rgb}{0.85, 0.18, 0.50}
\lstset{
  breaklines=true,
  columns=fullflexible,
  basicstyle=\fontsize{7.2pt}{7.2pt}\ttfamily\selectfont,
  commentstyle=\fontsize{7.2pt}{7.2pt}\color{codeblue},
  keywordstyle=\fontsize{7.2pt}{7.2pt}\color{codekw},
}
\begin{lstlisting}[language=python]
# Inputs:
#  - ls: decomposition results of length K (from bottom to top)
#  - gts: ground truth of length Q (from bottom to top)
#  - dist: distance func bounded in [0, 1]
#
# Outputs:
#  - pairs: a list of (l_idx, gt_idx)
#  - D: distance

# Step 1: Compute Cost Matrix
C = np.zeros((len(ls), len(gts))) 
for i in range(len(ls)):
    for j in range(len(gts)):
        C[i,j] = dist(ls[i], ls[j])

# Step 2: Compute Accumulated Cost Matrix
D = np.zeros((len(ls), len(gts))) 
for i in range(1, len(ls)):
    D[i, 0] = D[i-1,0] + C[i,0]
for j in range(1, len(gts)):
    D[0, j] = D[0,j-1] + C[0,j]
for i in range(1, len(ls)):
    for j in range(1, len(gts)):
        D[i,j] = C[i, j] + min(D[i-1,j], D[i,j-1], D[i-1,j-1])

# Step 3: Backtrace to Find Optimal Alignment
i, j = len(ls)-1, len(gts)-1
pairs = [(i,j)]
while True:
    if i==0 and j==0:
        break
    elif i==0:
        pairs.append((i,j-1))
        j-=1
    elif j==0:
        pairs.append((i-1,j))
        i -= 1
    elif D[i-1,j-1]<=D[i-1,j] and D[i-1,j-1]<=D[i,j-1]:
        pairs.append((i-1,j-1))
        i -= 1
        j -= 1
    elif D[i-1,j]<=D[i-1,j-1] and D[i-1,j]<=D[i,j-1]:
        pairs.append((i-1,j))
        i -= 1
    else:
        pairs.append((i,j-1))
        j -= 1

D = sum([Dist(ls[i], gts[j]) for i,j in pairs])/len(pairs)

return pairs, D
\end{lstlisting}
\end{algorithm}



\subsection{Edits algorithm}
\label{sup:edits}
We employ an iterative refinement process with DTW to quantify the number of edits required to align the decomposition results with the given ground truth. At each iteration, we apply the edit (\texttt{Merge}) that yields the highest gain until either the maximum number of edits is reached or the number of layers is reduced to two, as shown in \cref{alg:merge_edit,alg:find_merge_gains}. 
To efficiently approximate the optimal edit, we adopt a greedy search strategy: at iteration $i$, we focus on changes in distances between consecutive layers---specifically, layers $i$, $i+1$, and $i+2$ (if present)---rather than evaluating all layers globally. The optimal edit is then selected from among all candidates at each iteration, ensuring a balance between computational efficiency and alignment accuracy. 
Although \cref{alg:merge_edit,alg:find_merge_gains} describe only the merging of predicted layers for simplicity, we apply the same merging procedure to both the predicted and ground truth layers to address both under- and over-decomposition.
See \cref{fig:sup-edit-process-1,fig:sup-edit-process-2} for visualization of the alignment and merging process.

\begin{algorithm}[h]
\caption{MergeEdit}
\label{alg:merge_edit}
\definecolor{codeblue}{rgb}{0.25,0.5,0.5}
\definecolor{codekw}{rgb}{0.85, 0.18, 0.50}
\lstset{
  breaklines=true,
  columns=fullflexible,
  basicstyle=\fontsize{7.2pt}{7.2pt}\ttfamily\selectfont,
  commentstyle=\fontsize{7.2pt}{7.2pt}\color{codeblue},
  keywordstyle=\fontsize{7.2pt}{7.2pt}\color{codekw},
}
\begin{lstlisting}[language=python]
# Inputs:
#  - ls: decomposition results of length K (bottom to top)
#  - gts: ground truth of length Q (bottom to top)
#  - emax: maximum number of edits 
#  - dist: distance function bounded in [0, 1]
#
# Outputs:
#  - pairs: a list of (l_idx, gt_idx)
#  - D: distance
#  - e: number of edits

e = 0
while e < emax and len(ls) > 2: 
    pairs, _ = dtw(ls, gts)
    merged_ids, gains = find_gains(ls, gts, 
                                pairs, dist)
    if len(gains) > 0:
        best_id = merged_ids[argmin(gains)]
        merged = merge(ls[best_id], ls[best_id+1])
        ls[best_id] = merged
        ls.pop(best_id+1)
    else:
        break
    e += 1
return dtw(ls, gts), e


def merge(x, y):  # Merge func by OpenCV 
    return Image.alpha_composite(x, y)
\end{lstlisting}
\end{algorithm}

\clearpage

\begin{algorithm}[h]
\caption{FindGains}
\label{alg:find_merge_gains}
\definecolor{codeblue}{rgb}{0.25,0.5,0.5}
\definecolor{codekw}{rgb}{0.85, 0.18, 0.50}
\lstset{
  breaklines=true,
  columns=fullflexible,
  basicstyle=\fontsize{7.2pt}{7.2pt}\ttfamily\selectfont,
  commentstyle=\fontsize{7.2pt}{7.2pt}\color{codeblue},
  keywordstyle=\fontsize{7.2pt}{7.2pt}\color{codekw},
}
\begin{lstlisting}[language=python]
# Inputs:
#  - ls: decomposition results of length K (bottom to top)
#  - gts: ground truth of length Q (bottom to top)
#  - pairs: list of (l_idx, gt_idx) obtained from DTW
#  - dist: distance function bounded in [0, 1]
#
# Outputs:
#  - merged_ids: list of indices where merging occurs
#  - gains: list of corresponding distance reductions

merged_ids, gains = [], []
for i in range(len(ls)-1):
    # Step 1: Compute merged layer candidates
    subls = [merge(ls[i], ls[i+1])] + ([ls[i+2]] if i+2 < len(ls) else [])
    
    # Step 2: Gather corresponding ground truth layers
    subgts = [
        [gts[p[1]] for p in pairs if p[0] == i],
        [gts[p[1]] for p in pairs if p[0] == i+1]
    ]

    # Step 3: Compute current distance sum
    curD = sum([dist(ls[i], subgt) for subgt in subgts[0]]) + \
            sum([dist(ls[i+1], subgt) for subgt in subgts[1]])

    # Step 4: Compute distance sum after merging
    Ds = []
    for j in range(len(subls)):
        for k in range(len(subgts)):
            Ds.append(sum([dist(subls[j], subgt) for subgt in subgts[k]]))      
    Ds = [d + Ds[0] for d in Ds[1:]]
    minD = min(Ds)        
    
    # Step 5: Check if merging reduces distance
    if minD < curD:
        merged_ids.append(i)
        gains.append(minD - curDs)
return merged_ids, gains


def merge(x, y):  # Merge func by OpenCV 
    return Image.alpha_composite(x, y)
\end{lstlisting}
\end{algorithm}

\newpage

\section{Loss functions}
\label{sup:loss}

We use binary cross-entropy loss $\mathcal{L}_{\text{BCE}}$, IoU loss $\mathcal{L}_{\text{IoU}}$, and SSIM loss $\mathcal{L}_{\text{SSIM}}$ in our training as BiRefNet~\citep{birefnet}. Definitions of each loss function are as follows.
\begin{align}
    \mathcal{L}_{\text{BCE}}(\hat{\bm{l}}^{\text{A}}, \bm{l}^{\text{A}}) &= \frac{1}{|\Omega|} \sum_{i,j\in\Omega} -\bm{l}^{\text{A}}_{i,j} \log \hat{\bm{l}}^{\text{A}}_{i,j} \notag \\
    &\quad - (1 - \bm{l}^{\text{A}}_{i,j}) \log (1 - \hat{\bm{l}}^{\text{A}}_{i,j}),
\end{align}
\begin{equation}
    \mathcal{L}_{\text{IoU}}(\hat{\bm{l}}^{\text{A}}, \bm{l}^{\text{A}}) = 1 -  \frac{\sum\limits_{i,j\in\Omega}\bm{l}^{\text{A}}_{i,j} \hat{\bm{l}}^{\text{A}}_{i,j}}{\sum\limits_{m,n\in\Omega}\bm{l}^{\text{A}}_{m,n} + \hat{\bm{l}}^{\text{A}}_{m,n} - \bm{l}^{\text{A}}_{m,n} \hat{\bm{l}}^{\text{A}}_{m,n}},
\end{equation}
{\scriptsize
\begin{equation}
    \mathcal{L}_{\text{SSIM}}(\hat{\bm{l}}^{\text{A}}, \bm{l}^{\text{A}}) = 1- \frac{1}{|\mathcal{P}|} \sum_{p\in \mathcal{P}} \frac{(2\mu_{\bm{l}^{\text{A}}_p} \mu_{\hat{\bm{l}}^{\text{A}}_p} + C_1)(2\sigma_{\bm{l}^{\text{A}}_p\hat{\bm{l}}^{\text{A}}_p} + C_2)}{(\mu_{\bm{l}^{\text{A}}_p}^2 + \mu_{\hat{\bm{l}}^{\text{A}}_p}^2 + C_1)(\sigma_{\bm{l}^{\text{A}}_p}^2 + \sigma_{\hat{\bm{l}}^{\text{A}}_p}^2 + C_2)},
\end{equation}
}
where $\Omega$ denotes the set of spatial indices, and $\mathcal{P}$ represents the set of overlapping patches. The local mean $\mu_{\hat{\bm{l}}^{\text{A}}_p}$ and variance $\sigma^2_{\hat{\bm{l}}^{\text{A}}_p}$, as well as the local mean $\mu_{\bm{l}^{\text{A}}_p}$ and variance $\sigma^2_{\bm{l}^{\text{A}}_p}$ of ground truth, are computed within corresponding patches indexed by $p \in \mathcal{P}$. The covariance $\sigma_{\bm{l}^{\text{A}}_p\hat{\bm{l}}^{\text{A}}_p}$ quantifies structural similarity between the prediction and ground truth patches. $C_1$ and $C_2$ are constants and the setting details follow \cite{birefnet}, except that both the predicted and ground-truth alpha maps $\hat{\bm{l}}^{\text{A}}$ and $ \bm{l}^{\text{A}}$,  are not binarized due to shading and smooth transitions commonly used in graphic design.



\begin{figure*}[p]
    \centering
    \includegraphics[keepaspectratio, width=0.9\linewidth]{figures/merge-edit-1.pdf}
    \caption{
        Visual example of the DTW-based layer alignment and editing process. Red lines connect matched layers between \ours{}'s prediction and the ground truth; their thickness represents the matching score (the inverse of the distance), \ie, the thicker the line, the higher the score. Green boxes indicate the layers that are merged during the editing process. All layers are sorted from back to front, with the backmost layer on the left and the frontmost on the right.
        Although the decomposition result appears useful for editing the input image, its quality is underestimated due to a mismatch in granularity with the ground truth. Layer merging resolves this mismatch, enabling a more faithful evaluation of the decomposition quality.
    }
    \label{fig:sup-edit-process-1}
\end{figure*}

\begin{figure*}[p]
    \centering
    \includegraphics[keepaspectratio, width=0.93\linewidth]{figures/merge-edit-2.pdf}
    \caption{
        Visual example of the DTW-based layer alignment and editing process. Red lines connect matched layers between \ours{}'s prediction and the ground truth; their thickness represents the matching score (the inverse of the distance), \ie, the thicker the line, the higher the score. Green boxes indicate the layers that are merged during the editing process. All layers are sorted from back to front, with the backmost layer on the left and the frontmost on the right.
        \ours{} overdecomposes the white background, but in practical scenarios, it is easy to merge these into a single layer.
        Our evaluation treats such cases as requiring a single edit operation, reflecting the actual editing workload for users.
    }
    \label{fig:sup-edit-process-2}
\end{figure*}
