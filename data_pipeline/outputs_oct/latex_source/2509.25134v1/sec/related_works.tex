\section{Related Work}
\label{sec:related_works}

\begin{figure*}[t]
    \centering
    \includegraphics[keepaspectratio, width=1\linewidth]{figures/method.pdf}
    \caption{\ours{} decopmoses raster graphic designs into layers by iteratively extracting the top-layer and completing the background.
    Our training target is the top-layer matting model.
    \cref{fig:bg-refine,fig:fg-refine} illustrate details of the top-layer extraction and background completion.}
    \label{fig:pipeline}
\end{figure*}


\subsection{Image Layer Decomposition}
Image layer decomposition is a task to decompose an image into a sequence of layers, which are composable with a specific compositing function (e.g., alpha compositing) to reproduce or approximate the original image~\cite{porter1984compositing}.
Color segmentation represents an image with semi-transparent color layers, targeting digital paintings~\cite{tan2016decomposing} and natural images~\cite{tan2018efficient,aksoy2017unmixing,akimoto2020fast}.
Koyama \etal~\cite{koyama2018decomposing} propose to handle non-linear color blending functions, followed by the efficient deep learning-based extension~\cite{horita2022fast}.

There have been many studies on decomposing natural scenes at the object level~\cite{isola2013scene,monnier2021unsupervised,zhan2020self,zheng2021visiting,mulan,liu2024object}. For instance, PCNet~\cite{zhan2020self} decomposes a scene image into object layers by estimating the order of objects and the RGB of occluded parts.
While PCNet assumes the object modal mask is given, Zhang \etal~\cite{zheng2021visiting} create layered data including occluded parts in indoor scenes and decompose the image by training instance segmentation, depth estimation, and background completion.
Text2Layer~\cite{zhang2023text2layer} extracts salient objects from natural images using matting and generates training data for layered image generation.
\TODO{違いを明確に。ただしpreprint}
Recently, MULAN~\cite{mulan} decomposes natural images, including outdoor scenes where obtaining ground truth data is difficult, by combining the latest off-the-shelf open vocabulary object detection models~\cite{yao2023detclipv2}, zero-shot segmentation~\cite{kirillov2023segment}, depth estimation~\cite{ranftl2020towards}, and instance ordering~\cite{lee2022instance} with heuristics.
While the above studies mainly focus on object decomposition, Yang \etal~\cite{yang2024generative} decompose physical object effects (\eg, shadows or reflections) as well.

Compared to the natural image decomposition, graphic design decomposition has to deal with different granularities of \emph{objects}; \eg, a corporate logo in a graphic design consists of an illustration and a text, and whether they should be decomposed into parts depends on the context.
Considering the nature of the task, we propose a simple and effective method and a new quantitative evaluation protocol for inconsistent ground-truth.
A concurrent work~\cite{accordion} tackles the same task as ours with a stacked pipeline approach using a VLM trained on closed data.
\ours{}'s pipeline is overwhelmingly simple and leverages domain knowledge to refine the final quality.
We compare \ours{} with a VLM-based pipeline in our experiment.


\subsection{Image Vectorization}
Related to layer decomposition, image vectorization converts an image or a part into a set of parameters of a specific drawing function, rather than layer images.
Our layer decomposition approach can be useful for vectorization as a pre-processing step to extract part-based raster images.
Du \etal~\cite{du2023image} and Favreau \etal~\cite{favreau2017photo2clipart} obtain a sequence of linear gradient layers that approximate the original image by optimization using alpha blending.
Several works attempt to generate SVG-based representation from raster images~\cite{shen2021clipgen,ma2022towards,song2023clipvg,carlier2020deepsvg,reddy2021im2vec,rodriguez2023starvector},
where they typically assume vector art, cleanly masked images, or clean segmented images as input.
A few specifically focus on typographic representation in graphic design, where they estimate text rendering parameters~\cite{accordion,shimoda2021rendering}.





\subsection{Image Matting and Foreground Extraction}
\TODO{segmentationも?}
Image matting is a task to estimate alpha mattes of objects in an image, and together with other tasks such as background inpainting, forms the layer decomposition task.
Matting approach often assumes user-specified trimap~\cite{chuang2001bayesian,sun2004poisson,xu2017deep,yao2024vitmatte}, and a few trimap-free methods have been reported recently~\cite{birefnet,li2023matting}.
\ours{} mainly uses network architectures used in matting~\cite{birefnet} to extract unoccluded top layers.

While matting estimates alpha mattes, foreground color estimation involves determining the color of the foreground that is mixed with the background.
There are energy-based methods~\cite{levin2007closed,chen2013knn,aksoy2017designing} and their efficient versions~\cite{germer2021fast,forte2021approximate}, as well as deep learning-based methods~\cite{Lutz2021foreground} that estimate the foreground color given the alpha.
Hou \etal and Li \etal simultaneously estimate the alpha map and foreground color given an image and a trimap~\cite{hou2019context,li2025drip}.
The foreground color is deterministic when the background color and foreground alpha are given.
In our setup, we obtain the foreground matte from our trained model and the background color from high-quality background inpainting~\cite{lama}, and then calculate the foreground color.

