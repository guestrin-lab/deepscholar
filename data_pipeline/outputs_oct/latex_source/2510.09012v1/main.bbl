\begin{thebibliography}{10}

\bibitem{sun2024llamagen}
Peize Sun, Yi~Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan.
\newblock Autoregressive model beats diffusion: Llama for scalable image generation.
\newblock {\em arXiv preprint arXiv:2406.06525}, 2024.

\bibitem{liu2024lumina_mgpt}
Dongyang Liu, Shitian Zhao, Le~Zhuo, Weifeng Lin, Yu~Qiao, Hongsheng Li, and Peng Gao.
\newblock Lumina-mgpt: Illuminate flexible photorealistic text-to-image generation with multimodal generative pretraining.
\newblock {\em arXiv preprint arXiv:2408.02657}, 2024.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock {\em arXiv preprint arXiv:2303.08774}, 2023.

\bibitem{touvron2023llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em arXiv preprint arXiv:2307.09288}, 2023.

\bibitem{yang2024qwen2technicalreport}
An~Yang, Baosong Yang, Binyuan Hui, Bo~Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na~Ni, Pei Zhang, Peng Wang, Ru~Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu~Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan.
\newblock Qwen2 technical report, 2024.

\bibitem{tian2024var}
Keyu Tian, Yi~Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang.
\newblock Visual autoregressive modeling: Scalable image generation via next-scale prediction.
\newblock {\em Advances in neural information processing systems}, 37:84839--84865, 2024.

\bibitem{wu2024janus}
Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, and Ping Luo.
\newblock Janus: Decoupling visual encoding for unified multimodal understanding and generation, 2024.

\bibitem{chen2025janus_pro}
Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan.
\newblock Janus-pro: Unified multimodal understanding and generation with data and model scaling.
\newblock {\em arXiv preprint arXiv:2501.17811}, 2025.

\bibitem{esser2021vqgan}
Patrick Esser, Robin Rombach, and Bjorn Ommer.
\newblock Taming transformers for high-resolution image synthesis.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 12873--12883, 2021.

\bibitem{yu2021vit_vqgan}
Jiahui Yu, Xin Li, Jing~Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu.
\newblock Vector-quantized image modeling with improved vqgan.
\newblock {\em arXiv preprint arXiv:2110.04627}, 2021.

\bibitem{chameleonteam2025chameleon}
Chameleon Team.
\newblock Chameleon: Mixed-modal early-fusion foundation models, 2025.

\bibitem{ma2024star}
Xiaoxiao Ma, Mohan Zhou, Tao Liang, Yalong Bai, Tiejun Zhao, Huaian Chen, and Yi~Jin.
\newblock Star: Scale-wise text-to-image generation via auto-regressive representations.
\newblock {\em arXiv preprint arXiv:2406.10797}, 2024.

\bibitem{chang2022maskgit}
Huiwen Chang, Han Zhang, Lu~Jiang, Ce~Liu, and William~T Freeman.
\newblock Maskgit: Masked generative image transformer.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 11315--11325, 2022.

\bibitem{rombach2022stablediffusion}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 10684--10695, 2022.

\bibitem{peebles2023dit}
William Peebles and Saining Xie.
\newblock Scalable diffusion models with transformers.
\newblock In {\em Proceedings of the IEEE/CVF international conference on computer vision}, pages 4195--4205, 2023.

\bibitem{esser2024sdv3}
Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M{\"u}ller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et~al.
\newblock Scaling rectified flow transformers for high-resolution image synthesis.
\newblock {\em arXiv preprint arXiv:2403.03206}, 2024.

\bibitem{holtzman2019top_p}
Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi.
\newblock The curious case of neural text degeneration.
\newblock {\em arXiv preprint arXiv:1904.09751}, 2019.

\bibitem{tang2024top_nsigma}
Chenxia Tang, Jianchun Liu, Hongli Xu, and Liusheng Huang.
\newblock Top-$n\sigma$: Not all logits are you need, 2024.

\bibitem{zhang2024edt}
Shimao Zhang, Yu~Bao, and Shujian Huang.
\newblock Edt: Improving large language models' generation by entropy-based dynamic temperature sampling.
\newblock {\em arXiv preprint arXiv:2403.14541}, 2024.

\bibitem{su2022contrastive}
Yixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Lingpeng Kong, and Nigel Collier.
\newblock A contrastive framework for neural text generation.
\newblock {\em Advances in Neural Information Processing Systems}, 35:21548--21561, 2022.

\bibitem{chuang2023dola}
Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He.
\newblock Dola: Decoding by contrasting layers improves factuality in large language models.
\newblock {\em arXiv preprint arXiv:2309.03883}, 2023.

\bibitem{huang2024opera}
Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu.
\newblock Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 13418--13427, 2024.

\bibitem{luo2024sed}
Ziqin Luo, Haixia Han, Haokun Zhao, Guochao Jiang, Chengyu Du, Tingyun Li, Jiaqing Liang, Deqing Yang, and Yanghua Xiao.
\newblock Sed: Self-evaluation decoding enhances large language models for better generation.
\newblock {\em arXiv preprint arXiv:2405.16552}, 2024.

\bibitem{guan2025rstar}
Xinyu Guan, Li~Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi~Zhu, Fan Yang, and Mao Yang.
\newblock rstar-math: Small llms can master math reasoning with self-evolved deep thinking.
\newblock {\em arXiv preprint arXiv:2501.04519}, 2025.

\bibitem{teng2024sjd}
Yao Teng, Han Shi, Xian Liu, Xuefei Ning, Guohao Dai, Yu~Wang, Zhenguo Li, and Xihui Liu.
\newblock Accelerating auto-regressive text-to-image generation with training-free speculative jacobi decoding.
\newblock {\em arXiv preprint arXiv:2410.01699}, 2024.

\bibitem{van2016pixelcnn}
Aaron Van~den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et~al.
\newblock Conditional image generation with pixelcnn decoders.
\newblock {\em Advances in neural information processing systems}, 29, 2016.

\bibitem{ding2021cogview}
Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da~Yin, Junyang Lin, Xu~Zou, Zhou Shao, Hongxia Yang, et~al.
\newblock Cogview: Mastering text-to-image generation via transformers.
\newblock {\em Advances in neural information processing systems}, 34:19822--19835, 2021.

\bibitem{ge2023seed}
Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan.
\newblock Planting a seed of vision in large language model.
\newblock {\em arXiv preprint arXiv:2307.08041}, 2023.

\bibitem{ramesh2021dalle}
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.
\newblock Zero-shot text-to-image generation.
\newblock In {\em International conference on machine learning}, pages 8821--8831. Pmlr, 2021.

\bibitem{yu2022parti}
Jiahui Yu, Yuanzhong Xu, Jing~Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu~Karagol Ayan, et~al.
\newblock Scaling autoregressive models for content-rich text-to-image generation.
\newblock {\em arXiv preprint arXiv:2206.10789}, 2(3):5, 2022.

\bibitem{he2024mars}
Wanggui He, Siming Fu, Mushui Liu, Xierui Wang, Wenyi Xiao, Fangxun Shu, Yi~Wang, Lei Zhang, Zhelun Yu, Haoyuan Li, Ziwei Huang, LeiLei Gan, and Hao Jiang.
\newblock Mars: Mixture of auto-regressive models for fine-grained text-to-image synthesis, 2024.

\bibitem{wang2024emu3}
Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et~al.
\newblock Emu3: Next-token prediction is all you need.
\newblock {\em arXiv preprint arXiv:2409.18869}, 2024.

\bibitem{chern2024anole}
Ethan Chern, Jiadi Su, Yan Ma, and Pengfei Liu.
\newblock Anole: An open, autoregressive, native large multimodal models for interleaved image-text generation.
\newblock {\em arXiv preprint arXiv:2407.06135}, 2024.

\bibitem{jiao2025unitoken}
Yang Jiao, Haibo Qiu, Zequn Jie, Shaoxiang Chen, Jingjing Chen, Lin Ma, and Yu-Gang Jiang.
\newblock Unitoken: Harmonizing multimodal understanding and generation through unified visual encoding, 2025.

\bibitem{unitok}
Chuofan Ma, Yi~Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, and Xiaojuan Qi.
\newblock Unitok: A unified tokenizer for visual generation and understanding.
\newblock {\em arXiv preprint arXiv:2502.20321}, 2025.

\bibitem{yu2024titok}
Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen.
\newblock An image is worth 32 tokens for reconstruction and generation.
\newblock {\em arXiv preprint arXiv:2406.07550}, 2024.

\bibitem{lee2022rqvae}
Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han.
\newblock Autoregressive image generation using residual quantization.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 11523--11532, 2022.

\bibitem{yu2023lfq}
Lijun Yu, Jos{\'e} Lezama, Nitesh~B Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et~al.
\newblock Language model beats diffusion--tokenizer is key to visual generation.
\newblock {\em arXiv preprint arXiv:2310.05737}, 2023.

\bibitem{zhao2024bsq}
Yue Zhao, Yuanjun Xiong, and Philipp Krähenbühl.
\newblock Image and video tokenization with binary spherical quantization, 2024.

\bibitem{zhang2025v2flow}
Guiwei Zhang, Tianyu Zhang, Mohan Zhou, Yalong Bai, and Biye Li.
\newblock V2flow: Unifying visual tokenization and large language model vocabularies for autoregressive image generation, 2025.

\bibitem{qu2024tokenflow}
Liao Qu, Huichao Zhang, Yiheng Liu, Xu~Wang, Yi~Jiang, Yiming Gao, Hu~Ye, Daniel~K Du, Zehuan Yuan, and Xinglong Wu.
\newblock Tokenflow: Unified image tokenizer for multimodal understanding and generation.
\newblock {\em arXiv preprint arXiv:2412.03069}, 2024.

\bibitem{bai2024meissonic}
Jinbin Bai, Tian Ye, Wei Chow, Enxin Song, Qing-Guo Chen, Xiangtai Li, Zhen Dong, Lei Zhu, and Shuicheng Yan.
\newblock Meissonic: Revitalizing masked generative transformers for efficient high-resolution text-to-image synthesis.
\newblock In {\em The Thirteenth International Conference on Learning Representations}, 2024.

\bibitem{xie2024show_o}
Jinheng Xie, Weijia Mao, Zechen Bai, David~Junhao Zhang, Weihao Wang, Kevin~Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike~Zheng Shou.
\newblock Show-o: One single transformer to unify multimodal understanding and generation.
\newblock {\em arXiv preprint arXiv:2408.12528}, 2024.

\bibitem{tang2024hart}
Haotian Tang, Yecheng Wu, Shang Yang, Enze Xie, Junsong Chen, Junyu Chen, Zhuoyang Zhang, Han Cai, Yao Lu, and Song Han.
\newblock Hart: Efficient visual generation with hybrid autoregressive transformer.
\newblock {\em arXiv preprint arXiv:2410.10812}, 2024.

\bibitem{han2024infinity}
Jian Han, Jinlai Liu, Yi~Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu.
\newblock Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis.
\newblock {\em arXiv preprint arXiv:2412.04431}, 2024.

\bibitem{he2025nar}
Yefei He, Yuanyu He, Shaoxuan He, Feng Chen, Hong Zhou, Kaipeng Zhang, and Bohan Zhuang.
\newblock Neighboring autoregressive modeling for efficient visual generation.
\newblock {\em arXiv preprint arXiv:2503.10696}, 2025.

\bibitem{yu2024rar}
Qihang Yu, Ju~He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen.
\newblock Randomized autoregressive visual generation.
\newblock {\em arXiv preprint arXiv:2411.00776}, 2024.

\bibitem{radford2019gpt2}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem{keskar2019repetition_penalty}
Nitish~Shirish Keskar, Bryan McCann, Lav~R Varshney, Caiming Xiong, and Richard Socher.
\newblock Ctrl: A conditional transformer language model for controllable generation.
\newblock {\em arXiv preprint arXiv:1909.05858}, 2019.

\bibitem{leviathan2023speculative_decoding}
Yaniv Leviathan, Matan Kalman, and Yossi Matias.
\newblock Fast inference from transformers via speculative decoding.
\newblock In {\em International Conference on Machine Learning}, pages 19274--19286. PMLR, 2023.

\bibitem{chen2023accelerating}
Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper.
\newblock Accelerating large language model decoding with speculative sampling.
\newblock {\em arXiv preprint arXiv:2302.01318}, 2023.

\bibitem{meister2020beam_search}
Clara Meister, Tim Vieira, and Ryan Cotterell.
\newblock If beam search is the answer, what was the question?
\newblock {\em arXiv preprint arXiv:2010.02650}, 2020.

\bibitem{snell2024lookahead}
Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar.
\newblock Scaling llm test-time compute optimally can be more effective than scaling model parameters.
\newblock {\em arXiv preprint arXiv:2408.03314}, 2024.

\bibitem{lightman2023letsverifystepstep}
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.
\newblock Let's verify step by step, 2023.

\bibitem{snell2024scalingllmtts}
Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar.
\newblock Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024.

\bibitem{jang2025lantern}
Doohyuk Jang, Sihwan Park, June~Yong Yang, Yeonsung Jung, Jihun Yun, Souvik Kundu, Sung-Yub Kim, and Eunho Yang.
\newblock Lantern: Accelerating visual autoregressive models with relaxed speculative decoding, 2025.

\bibitem{leviathan2023spec_decode}
Yaniv Leviathan, Matan Kalman, and Yossi Matias.
\newblock Fast inference from transformers via speculative decoding.
\newblock In {\em International Conference on Machine Learning}, pages 19274--19286. PMLR, 2023.

\bibitem{he2024zipar}
Yefei He, Feng Chen, Yuanyu He, Shaoxuan He, Hong Zhou, Kaipeng Zhang, and Bohan Zhuang.
\newblock Zipar: Accelerating autoregressive image generation through spatial locality.
\newblock {\em arXiv preprint arXiv:2412.04062}, 2024.

\bibitem{wang2024par}
Yuqing Wang, Shuhuai Ren, Zhijie Lin, Yujin Han, Haoyuan Guo, Zhenheng Yang, Difan Zou, Jiashi Feng, and Xihui Liu.
\newblock Parallelized autoregressive visual generation.
\newblock {\em arXiv preprint arXiv:2412.15119}, 2024.

\bibitem{chen2023pixart_alpha}
Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et~al.
\newblock Pixart-${\alpha}$: Fast training of diffusion transformer for photorealistic text-to-image synthesis.
\newblock {\em arXiv preprint arXiv:2310.00426}, 2023.

\bibitem{podell2023sdxl}
Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M{\"u}ller, Joe Penna, and Robin Rombach.
\newblock Sdxl: Improving latent diffusion models for high-resolution image synthesis.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2023.

\bibitem{lin2014coco}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em Computer vision--ECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part v 13}, pages 740--755. Springer, 2014.

\bibitem{hu2024ella}
Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu.
\newblock Ella: Equip diffusion models with llm for enhanced semantic alignment, 2024.

\bibitem{wu2023hps}
Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li.
\newblock Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis.
\newblock {\em arXiv preprint arXiv:2306.09341}, 2023.

\bibitem{wang2025varedit}
Yufei Wang, Lanqing Guo, Zhihao Li, Jiaxing Huang, Pichao Wang, Bihan Wen, and Jian Wang.
\newblock Training-free text-guided image editing with visual autoregressive model, 2025.

\end{thebibliography}
