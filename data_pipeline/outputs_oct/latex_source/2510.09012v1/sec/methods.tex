\section{Methods}
\label{sec_methods}

In this section, we first introduce basic of autoregressive image generation in Sec.~\ref{sec_motivation}. Starting from the difference between image and text generation, we present our method from an entropy-based perspective by adjusting token-level randomness during generation (Sec.~\ref{sec_entropy_temperature}, Sec.~\ref{sec_adaption_more_ar}), and further extend this view to acceleration (Sec.~\ref{sec_ar_acceleration}).
\input{figs/motivation}

\subsection{Preliminaries \& motivation}
\label{sec_motivation}
\noindent\textbf{Autoregressive image generation.}
In a typical autoregressive generation process, an image $I\in \mathbb{R}^{H\times W\times 3}$ is quantized into a set of discrete tokens $(x_1, x_2, ..., x_{h\times w})$, where each token $x_i \in [V]$, $V$ denotes the size of the VQ-VAE codebook. The image tokens are generated sequentially by a transformer, with the $i+1$-th token $x_{i+1}$ conditioned on the previously generated tokens. This process is modeled as $\prod_{i=1}^{hw-1} p(x_{i+1} \mid x_{1:i})$, where $x_{1:i} = (x_1, x_2, ..., x_{i})$, and $p(x \mid x_{1:i})$ represents a categorical distribution over token at position $i+1$.
In text-conditioned generation tasks, the full image sequence is generated conditioned on a prefix of text tokens.
At each step, a sampling method such as top-$K$ or top-$p$ is applied to select a token from $p(x \mid x_{1:i})$. And choice of sampling strategy can significantly affect the quality of generated image, as illustrated in Fig.~\ref{fig_motivation} (b).

\input{figs/entropy_analysis}

\noindent\textbf{Difference between image $\&$ text generation.}
Compared to text, \textit{the information density in images is lower and highly non-uniform}. For example, images often contain large regions of solid color or visually similar content, while nearby tokens in text are typically distinct. To illustrate the difference, we segment the embedding sequences of both images and texts into equal lengths, compute the average frequency spectrum per segment, and visualize the distributions in Fig.~\ref{fig_motivation} (a). The average frequency spectrum distribution of image segments is more dispersed, with a large amount of low-frequency areas; whereas textual segments demonstrate more compact and uniform distributions.

This discrepancy poses a challenge for sampling: fixed parameters like top-$K$ or top-$p$, though effective in language generation, perform suboptimally when simply applying them across all image tokens. They fail to account for spatial variability, resulting in regional artifacts: overly deterministic sampling may lose fine details, producing flat regions, excessive smoothness, or simplistic backgrounds; while excessively random sampling compromises semantic consistency and structural coherence, causing artifacts, distorted limbs, or chaotic textures.


\noindent\textbf{Relationship between entropy $\&$ image contents.}
We demonstrate that the \textit{entropy of predicted token distribution serves as an effective indicator of local information density} in an image.
Specifically, we compute the entropy $\epsilon$ of log-likelihood over all $V$ codebook entries at each generation step as:
\begin{equation}
\label{eq_entropy}
    \epsilon = - \sum_{k=1}^{V} p_k \log(p_k).
\end{equation}
As shown in Fig.~\ref{fig_entropy_analysis}, regions with simple content (\textit{e.g.,} solid colors) typically exhibit lower entropy, while more complex foreground (\textit{e.g.,} objects, structures, and textures) areas have higher entropy.
Low-entropy regions correspond to peaked distributions over a few tokens, indicating high model confidence. Conversely, high-entropy regions display more uniform distribution, reflecting greater uncertainty in token selection and higher information density. These observations validate entropy as a reliable proxy for measuring information density in images.

\subsection{Entropy-aware dynamic temperature}
\label{sec_entropy_temperature}
Building on the observation in Sec.~\ref{sec_motivation}, we further investigate how regions with different entropy levels affect image quality and the optimal sampling strategy.
Under a simple experimental setup, we adopt~\cite{sun2024llamagen} to analyze the entropy distribution of logits during generation, discretize it into intervals, and adjust token temperature within each interval to control sampling randomness. We then examine the relationship between image quality and text alignment via FID and CLIP-Score as indicators, results can be seen in Fig.~\ref{fig_entropy_analysis} (c). Our findings are as follows:
\begin{enumerate}
\item Across most entropy intervals, adjusting randomness leads to a trade-off between image quality and text alignment, especially for tokens in the entropy range of [2,8].
\item In high-entropy regions (\textgreater5), lower randomness helps improve text-image consistency.
\item In regions with extreme low entropy (\textless 2), increasing sampling randomness consistently improves visual quality, while having negligible impact on text alignment.
\end{enumerate}

These findings suggest that token-level sampling should be entropy-aware: during inference, tokens with \textit{lower entropy} should be assigned relatively \textit{higher randomness} to enhance the quality and visual richness of generated image, while \textit{high-entropy} tokens should be sampled \textit{more cautiously} to preserve clear structure, details and text alignment.

To better adapt to real-world inference, we introduce a dynamic temperature mechanism that adjusts sampling randomness on a per-token basis. Specifically, after computing the entropy of predicted distribution at each position, we determine a temperature value with predefined mapping function:
\begin{equation}
\label{eq_temperature}
    T=T_0 e^{-\frac{\epsilon}{\alpha}}+\theta,
\end{equation}
% \begin{equation}
% \label{eq_temperature}
%     T_{new}=T_{raw}\cdot clip(\frac{\epsilon}{\epsilon_0}, 1 +\delta,1-\delta)
% \end{equation}
where $\epsilon$ denotes the entropy at current token position, $T_0$ represents the maximum temperature, $\theta$ sets the lower bound, and $\alpha$ controls the decay rate of temperature with increasing entropy. See Sec.~\ref{sec_ablation} for further analysis and discussion.
Subsequently, the resulting temperature $T$ is then applied to rescale the predicted logits as follows:
\begin{equation}
\label{eq_temperature2}
    \tilde{p}_i=\frac{p_i}{T}.
\end{equation}
Then, by applying $softmax(\tilde{p}_i)$, the differences between logits of different tokens are amplified (when $T$\textless 1) or reduced (when $T$\textgreater 1), which makes the probability distribution more concentrated or spread out, achieving the dynamic adjustment of sampling based on the region's content distribution.

\subsection{Adaptation to more AR models}
\label{sec_adaption_more_ar}
Additionally, many recent methods deviate from strict next-token prediction and instead adopt paradigms such as \textit{mask-prediction} or \textit{scale-wise} generation. We show that the proposed entropy-based strategy remains effective in these settings. After obtaining multiple token-level logit distributions from the transformer, we directly apply Eq.~(\ref{eq_temperature}) to them. As shown in Table~\ref{tab_results}, this approach consistently improves performance across standard evaluation metrics. Moreover, for different paradigms, specific designs can be incorporated to further enhance performance:

\input{figs/visual_mask}

\noindent\textbf{Mask-prediction models.}
For mask-based models such as~\cite{bai2024meissonic,xie2024show_o}, a full probability distribution over all image tokens is obtained at each forward step. After sampling, a dynamic masking mechanism based on token confidence is applied. We observe that this masking strategy also has a significant impact on the quality of the generated results (see Fig.~\ref{fig_visual_mask}). Specifically, a soft categorical distribution is used to select $k$ tokens from all candidates to be accepted at the current timestep $t$:
\begin{equation}
\label{eq_mask_conf}
\text{conf} = \log p_t + T \cdot g,
\end{equation}
where $p$ is the predicted token probability, $T$ is the dynamic temperature defined in Eq. (\ref{eq_temperature}), $g \sim \text{Gumbel}(0, 1)$ is sampled from standard Gumbel distribution.
\begin{equation}
\label{eq_mask_conf2}
\mathcal{M}_t = \text{conf} < \text{TopK}(\text{conf}\odot \tilde{\mathcal{M}}_{t-1}, k).
\end{equation}
Here, $\text{TopK}(\text{conf}, k)$ returns the $k$-th highest confidence score, and tokens with lower confidence are masked out. $\mathcal{M}_t$ and $\mathcal{M}_{t-1}$ represent the accepted token masks at the current and previous timesteps, respectively, while ${\tilde{\mathcal{M}}_{t-1}}$ denotes the element-wise negation of $\mathcal{M}_{t-1}$. The number of accepted tokens $k$ at each timestep $t$ is determined by a predefined scheduler.
This design further encourages randomness in low-entropy regions while enhancing accuracy in high-entropy regions, leading to improved image quality.

\noindent\textbf{Scale-wise models}~\cite{tian2024var,ma2024star,tang2024hart,han2024infinity} generate tokens within each scale simultaneously. We find that assigning greater randomness to earlier scales while reducing randomness at later scales yields more accurate results without compromising image richness.
Specifically, we define a temperature term that decreases as the scale increases.
For the tokens at $s$-th scale, $T_s$ is calculated as:
\begin{subequations}
\begin{align}
T_s = T\cdot & [1-\beta \cdot (s-\left\lfloor S/2 \right\rfloor)];\\
& p_s = \frac{p_s}{T_s},
\end{align}
\end{subequations}
where $s$ denotes the scale index and $s \in \{1, 2, \dots, S\}$; $p_s$ is the logits of tokens at $s$-th scale, with shape of $h_s\times w_s \times V$. $T$ is dynamic temperature defined in Eq. (\ref{eq_temperature}). $\beta$ controls the decay rate of $T_s$ across scales and is set to 0.3 in experiments.

\input{figs/results_mgpt}
\input{tables/results}

\subsection{Autoregressive acceleration}
\label{sec_ar_acceleration}
We further explore the use of entropy to accelerate autoregressive generation.
Existing speculative decoding approaches~\cite{teng2024sjd,jang2025lantern} typically generate multiple candidate tokens via a draft model, followed by a verification step using a target model.
When the draft and target share the same model, the process reduces to comparing the confidence scores from two consecutive iterations. Specifically, the probability at the $(j{-}1)$-th step, $p(x \mid x_{1:i-1}^{(j-1)})$, and the $j$-th step, $p(x \mid x_{1:i-1}^{(j)})$, are compared. The acceptance probability of the token $x_i$ is then computed based on these two distributions:
\begin{equation}
p = \min \left( 1, \frac{p_{\theta}(x_{i}^{(j)} \mid x_{1:i-1}^{(j)})}{p_{\theta}(x_{i}^{(j-1)} \mid x_{1:i-1}^{(j-1)})} \right).
\end{equation}
In practice, a token is accepted if $p > r$, where $r$ is drawn from a uniform distribution $\mathcal{U}[0,1]$, which naturally balances randomness required for sampling diversity and accuracy during generation.

\input{figs/results_meissonic_star}

To make the process entropy-aware, we propose a simple modification to the acceptance rule. Since low-entropy regions are more predictable and allow higher randomness, while high-entropy regions require stricter verification, we scale the threshold $r$ by an entropy-based factor $\epsilon / e$, where $e$ is constant. This dynamic adjustment enables more efficient generation based on local uncertainty.

To improve stability, we rewrite $r$ as $0.5 + (r - 0.5)$, treating it as $0.5 + \mathcal{U}[-0.5, 0.5]$, where the noise term controls acceptance randomness. We scale this term with a decaying factor $(1 - \lambda \cdot \epsilon)$, where $\epsilon$ is the entropy, making high-entropy tokens more deterministically verified, while low-entropy areas retain near-uniform.
Combining both strategies above, the final acceptance rule can be formulated as:
\begin{equation}
p > \frac{\epsilon}{e} \left[ 0.5 + (r - 0.5)(1 - \lambda \cdot \epsilon) \right],
\end{equation}
where $\epsilon$ is entropy at the current token, $e$ and $\lambda$ are constants set to 8 and 16, respectively.

This dynamic acceptance criterion allocates inference budget more efficientlyâ€”being more permissive in confident regions and stricter in ambiguous ones, thereby reducing inference time with minimal performance loss. For detailed metrics and comparisons, please refer to Sec.~\ref{sec_exp_acc} and Sec.~\ref{sec_ablation}.