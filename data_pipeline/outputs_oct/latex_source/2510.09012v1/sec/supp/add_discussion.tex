\section{Additional discussion about entropy}
\label{sec_add_discussion}

\subsection{Entropy $\&$ generative models}
\subsubsection{Visualization of entropy \& images}
Due to space limitations in the main text, we did not provide extended entropy visualizations and analysis. Here, we include additional entropy maps for LlamaGen and Lumina-mGPT. See Fig.~\ref{fig_entropy_view_llamagen} and Fig.~\ref{fig_entropy_view_mgpt}.

\input{figs/supp/entropy_view_llamagen}
\input{figs/supp/entropy_view_mgpt}

\subsubsection{Mask-prediction models}
We provide additional entropy-based analysis of the mask model. Since the generation involves multiple timesteps, where a subset of tokens is accepted at each step based on previously generated content, we compute the entropy of accepted tokens at each timestep and aggregate them into a final entropy map. As shown in Fig.~\ref{fig_entropy_view_meissonic}, applying the proposed entropy-based temperature leads to a more spatially balanced entropy distribution and enables richer image content while maintaining generation stability.

\input{figs/supp/entropy_view_meissonic}
In addition, we further analyze the average entropy of tokens accepted at each timestep, as shown in Fig.~\ref{fig_supp_mask_entropy_mean}. As discussed in the main text, due to the confidence-based token selection strategy, tokens accepted in earlier steps tend to have lower entropy, since they are more likely to receive high confidence scores. In contrast, tokens accepted in later steps (\textgreater60) exhibit significantly higher entropy. Moreover, more tokens are accepted in these later stages, which increases the risk of violating the autoregressive assumption that spatially adjacent tokens should be sampled as independently as possible. This may lead to degraded image quality. Therefore, adopting a more conservative sampling strategy for these high-entropy tokens could help improve the overall generation quality.


\input{figs/supp/mask_entropy_mean}
\subsubsection{Scale-wise models}
For the scale-wise model, the generation process constructs a complete image by predicting logits maps at multiple scales. Each scale is conditioned on the residuals from the preceding scales, meaning that the sum of the feature maps generated at all scales is passed through the detokenizer to form the final output. In this generation paradigm, different scales exhibit distinct roles. Specifically, as described in~\cite{wang2025varedit}, the earlier scales are responsible for generating the main structure of the image, while the later scales refine the result with fine details such as texture. We visualize the entropy maps of each scale during generation, as shown in Fig.~\ref{fig_entropy_view_star}. From scale 8 to scale 12, the model tends to focus more on the foreground, with significantly higher entropy observed in the regions corresponding to the primary subject. In contrast, at scale 13 and 14, there is no clear bias between foreground and background, indicating a more uniform attention across the image.

\input{figs/supp/entropy_view_star}

In addition, we compute the average entropy for each scale, as shown in Fig.~\ref{fig_supp_star_entropy_mean}. The later scales exhibit relatively higher entropy, while the earlier scales tend to have lower average entropy (however a decreasing trend is observed in the final two scales). This further indicates that different scales carry varying amounts of information.

\input{figs/supp/star_entropy_mean}

\subsection{Entropy $\&$ generated contents}
In practice, the logits are not simply positively correlated with the complexity of image content. We observe that regions with clear, well-defined content do not always exhibit high entropy; instead, their entropy typically falls within a moderate range (e.g., between 2 and 8). The more deterministic the content, the lower the entropy tends to be. In contrast, regions with entropy lower than 2 or higher than 8 often correspond to simple backgrounds or overly complex, unfaithful details. Especially for regions with entropy above 8, the generated details are frequently meaningless. This also explains why adjusting the logits in these low- and high-entropy areas, as discussed in our motivation experiment, does not significantly harm text-image alignment.

\subsection{Is entropy the best indicator for information?}
From a theoretical perspective, the entropy of logits reflects the model's confidence in predicting the current token. When the model is sufficiently trained—or when its capacity is strong—it may produce low entropy even in semantically important regions. In practice, we observe cases where foreground objects (e.g., faces) yield lower entropy than complex backgrounds. This suggests that the model's confidence is not solely determined by information density, but also by the number of plausible token candidates in a region. For instance, highly structured areas like faces tend to have a unique correct token and thus low uncertainty, despite containing rich semantic information. In contrast, cluttered textures such as grass or foliage may allow for more varied token predictions, resulting in higher entropy.

% \input{figs/supp/abnorm_visualization}

Based on the above analysis, entropy may need to be combined with additional indicators to more accurately characterize the information distribution within an image.
Specifically, more precise token-wise handling can be achieved by incorporating the similarity among top-ranked tokens in the logits distribution. For instance, if the entropy is low but the top tokens are not similar, the prediction can be deemed accurate; however, if the top tokens are highly similar under low entropy, the randomness at that position may need to be further increased. Conversely, under high-entropy conditions, a set of similar top tokens may indicate the existence of genuinely diverse possibilities. Moreover, analyzing the similarity of logits between adjacent tokens could help identify tokens that require more precise predictions—for example, if a token’s probability distribution significantly differs from that of the previous token, it may warrant stricter sampling, regardless of its entropy level. We leave these directions for future exploration.

Moreover, since dynamic temperature only adjusts the randomness of the probability distribution (i.e., the variance of the logits) but not the location of its peak, further combining it with CFG may help achieve better performance.