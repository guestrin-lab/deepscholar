\section{Additional details of method}
\label{sec_add_detail}

\subsection{Hyperparameter settings in Sec. 3.2 in maintext}
In Sec. 3.2 in maintext, we propose to dynamically control the sampling temperature based on entropy. However, due to significant differences between base models, it is difficult to apply the same parameters across all settings. Therefore, we list the detailed parameters for each model in Table~\ref{tab_supp_hyper_param_setting}. For undertrained models such as LlamaGen stage1, higher randomness is required at low-entropy stages to avoid generating large areas of repetitive tokens. In contrast, well-trained models benefit from a smoother temperature schedule.

\input{tables/supp/hyper_param_setting}

\subsection{Detailed description of speculative decoding in images}
We accelerate inference based on existing speculative decoding schemes~\cite{teng2024sjd} in Sec. 3.4 in maintext, thereby further reducing inference cost without sacrificing output quality. Due to space constraints, we did not elaborate on the baseline speculative decoding methods in the main text. Here, we provide more details.

This method aims to accelerate auto-regressive text-to-image generation by allowing multiple tokens to be generated in parallel without training. Inspired by speculative decoding, SJD introduces a probabilistic acceptance criterion that compares the confidence of draft tokens from two consecutive iterations. In each iteration $j$, given a draft token $x_i^{(j)}$, SJD computes its acceptance probability based on the ratio between two conditional probabilities:
\begin{equation}
r < \min\left(1, \frac{p_{\theta}(x_i^{(j)} \mid x_{1:i-1}^{(j)})}{p_{\theta}(x_i^{(j)} \mid x_{1:i-1}^{(j-1)})}\right),
\end{equation}
where $r \sim \mathcal{U}[0,1]$. Accepted tokens are fixed, while the others are resampled from a calibrated distribution:
\begin{equation}
x_i^{(j+1)} \sim \frac{\max(0, p_{\theta}(x \mid x_{1:i-1}^{(j)}) - p_{\theta}(x \mid x_{1:i-1}^{(j-1)}))}{\sum_x \max(0, \cdot)}.
\end{equation}
This allows high-randomness sampling, crucial for image diversity, while significantly reducing decoding steps. SJD operates in a windowed, iterative manner and supports optional spatially-informed token initialization to further improve efficiency.
