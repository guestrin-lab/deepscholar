\section{Conclusion}

In this work, we first point out the need for different sampling strategies in autoregressive image and text generation, given their distinct information distributions. 
Starting from this perspective, we find entropy effectively represents image information density, offering new possibilities for improving and accelerating image generation.
As our method involve parameter adjustments without training, this approach could be further integrated into training or fine-tuning frameworks, potentially accelerating training, boosting inference speed, improving stability, and reducing hyperparameter dependence.