\section{Related works}
\label{sec_related_works}
\subsection{Autoregressive image generation}
Early work~\cite{van2016pixelcnn} generates images directly at pixel level. Later approaches adopt a two-stage pipeline: images are first quantized into discrete tokens~\cite{esser2021vqgan,yu2021vit_vqgan}, then generated with Transformers in raster order~\cite{ding2021cogview,ge2023seed,ramesh2021dalle,yu2022parti,he2024mars,wang2024emu3}.
Recent efforts scale this paradign with larger models and stronger conditioning. LlamaGen~\cite{sun2024llamagen} provides class and text-conditioned baselines; Lumina-mGPT~\cite{liu2024lumina_mgpt} and Anole~\cite{chern2024anole} fine-tune Chameleon~\cite{chameleonteam2025chameleon} for improved text-conditioned generation. Unified frameworks further bridge understanding and generation~\cite{wu2024janus,chen2025janus_pro,jiao2025unitoken,unitok} in a single Transformer.
Meanwhile, image tokenizers have evolved for better reconstruction~\cite{yu2024titok,lee2022rqvae,yu2023lfq,zhao2024bsq} or multimodal integration~\cite{zhang2025v2flow,qu2024tokenflow}.

While proven effective, the vanilla autoregressive paradigm suffers from slow and rigid next-token prediction. To improve efficiency, recent studies explore more strategies, including multi-token prediction via random masking~\cite{chang2022maskgit,bai2024meissonic,xie2024show_o}, coarse-to-fine modeling~\cite{tian2024var,ma2024star,tang2024hart,han2024infinity}or hybrid approaches~\cite{he2025nar,yu2024rar}. Nonetheless, vector-quantized models still rely on sampling from token distributions, making generation quality sensitive to the sampling strategy.

\subsection{Sampling strategies in autoregressive models}
Transformers model the probability distribution over tokens, requiring specific sampling strategies to obtain concrete outputs. Common approaches in language modeling include top-\emph{k}~\cite{radford2019gpt2} and top-\emph{p}~\cite{holtzman2019top_p} sampling, which truncate the candidate space by rank or cumulative probability. EDT~\cite{zhang2024edt} dynamically adjusts temperature based on entropy to balance diversity and precision. Other approaches explore repetition penalties~\cite{keskar2019repetition_penalty}, contrastive decoding~\cite{chuang2023dola}, speculative decoding~\cite{leviathan2023speculative_decoding,chen2023accelerating}, and search-based techniques~\cite{meister2020beam_search,snell2024lookahead,guan2025rstar,lightman2023letsverifystepstep,snell2024scalingllmtts} to reduce hallucination or speed up inference.

In visual generation, a higher degree of randomness is often needed to produce more realistic and detailed content. LlamaGen~\cite{sun2024llamagen} and Lumina-mGPT~\cite{liu2024lumina_mgpt} demonstrate that much larger top-\emph{k} values than those used in language models help avoid over-smoothed and low-detail outputs. Recent methods~\cite{teng2024sjd,jang2025lantern} apply speculative~\cite{leviathan2023spec_decode} or parallel decoding~\cite{he2024zipar,wang2024par} to accelerate image synthesis. 
However, they overlook the highly uneven spatial information distribution in images compared to text, and do not tailor decoding for autoregressive image generation.