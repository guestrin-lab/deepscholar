@article{zhao2022bapipe,
  title={Ba{P}ipe: Balanced pipeline parallelism for {DNN} training},
  author={Zhao, Letian and Xu, Rui and Wang, Tianqi and Tian, Teng and Wang, Xiaotian and Wu, Wei and Ieong, Chio-In and Jin, Xi},
  journal={Parallel Processing Letters},
  volume={32},
  number={03n04},
  pages={2250005},
  year={2022},
  publisher={World Scientific}
}
@inproceedings{narayanan2019pipedream,
author = {Narayanan, D and Harlap, A and Phanishayee, A and Seshadri, V and Devanur, N R. and Ganger, G R. and Gibbons, P B. and Zaharia, M},
title = {{PipeDream}: generalized pipeline parallelism for {DNN} training},
year = {2019},
isbn = {9781450368735},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341301.3359646},
doi = {10.1145/3341301.3359646},
booktitle = {Proceedings of the 27th ACM Symposium on Operating Systems Principles},
pages = {1â€“15},
numpages = {15},
location = {Huntsville, Ontario, Canada},
series = {SOSP'19}
}
@inproceedings{unnikrishnan2021layerpipe,
  title={{LayerPipe}: Accelerating deep neural network training by intra-layer and inter-layer gradient pipelining and multiprocessor scheduling},
  author={Unnikrishnan, Nanda K and Parhi, Keshab K},
  booktitle={Proceedings of the 2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD)},
  pages={1--8},
  year={2021},
  organization={IEEE}
}
@article{ben2019demystifying,
  title={Demystifying parallel and distributed deep learning: An in-depth concurrency analysis},
  author={Ben-Nun, Tal and Hoefler, Torsten},
  journal={ACM Computing Surveys (CSUR)},
  volume={52},
  number={4},
  pages={1--43},
  year={2019},
  publisher={ACM New York, NY, USA}
}
@inproceedings{raina2009large,
  title={Large-scale deep unsupervised learning using graphics processors},
  author={Raina, Rajat and Madhavan, Anand and Ng, Andrew Y},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={873--880},
  year={2009}
}
@article{shallue2019measuring,
  title={Measuring the effects of data parallelism on neural network training},
  author={Shallue, Christopher J and Lee, Jaehoon and Antognini, Joseph and Sohl-Dickstein, Jascha and Frostig, Roy and Dahl, George E},
  journal={Journal of Machine Learning Research},
  volume={20},
  number={112},
  pages={1--49},
  year={2019}
}
@article{dean2012large,
  title={Large scale distributed deep networks},
  author={Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc'aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and others},
  journal={Advances in Neural Information Processing Systems},
  volume={25},
  year={2012}
}
@inproceedings{li2013parameter,
  title={Parameter server for distributed machine learning},
  author={Li, Mu and Zhou, Li and Yang, Zichao and Li, Aaron and Xia, Fei and Andersen, David G and Smola, Alexander},
  booktitle={Big learning NIPS workshop},
  volume={6},
  number={2},
  year={2013},
  organization={Lake Tahoe, CA}
}
@article{huang2019gpipe,
  title={G{P}ipe: Efficient training of giant neural networks using pipeline parallelism},
  author={Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and others},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@inproceedings{narayanan2021memory,
  title={Memory-efficient pipeline-parallel {DNN} training},
  author={Narayanan, Deepak and Phanishayee, Amar and Shi, Kaiyu and Chen, Xie and Zaharia, Matei},
  booktitle={Proceedings of the International Conference on Machine Learning},
  pages={7937--7947},
  year={2021},
  organization={PMLR}
}
@inproceedings{chilimbi2014project,
  title={Project {A}dam: Building an efficient and scalable deep learning training system},
  author={Chilimbi, Trishul and Suzue, Yutaka and Apacible, Johnson and Kalyanaraman, Karthik},
  booktitle={11th USENIX symposium on operating systems design and implementation (OSDI 14)},
  pages={571--582},
  year={2014}
}
@inproceedings{fan2021dapple,
  title={{DAPPLE}: A pipelined data parallel approach for training large models},
  author={Fan, Shiqing and Rong, Yi and Meng, Chen and Cao, Zongyan and Wang, Siyu and Zheng, Zhen and Wu, Chuan and Long, Guoping and Yang, Jun and Xia, Lixue and others},
  booktitle={Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages={431--445},
  year={2021},
  address = {New York, NY, USA},
}
@article{yang2021pipemare,
  title={Pipe{M}are: Asynchronous pipeline parallel {DNN} training},
  author={Yang, Bowen and Zhang, Jian and Li, Jonathan and R{\'e}, Christopher and Aberger, Christopher and De Sa, Christopher},
  journal={Proceedings of Machine Learning and Systems},
  volume={3},
  pages={269--296},
  year={2021}
}
@article{shoeybi2019megatron,
  title={Megatron-{LM}: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}
@inproceedings{jiang2024megascale,
  title={{MegaScale}: Scaling Large Language Model Training to More Than 10,000 {GPUs}},
  author={Jiang, Ziheng and Lin, Haibin and Zhong, Yinmin and Huang, Qi and Chen, Yangrui and Zhang, Zhi and Peng, Yanghua and Li, Xiang and Xie, Cong and Nong, Shibiao and others},
  booktitle={Proceedings of the 21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24)},
  pages={745--760},
  year={2024}
}
@inproceedings{li2021chimera,
  title={Chimera: efficiently training large-scale neural networks with bidirectional pipelines},
  author={Li, Shigang and Hoefler, Torsten},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--14},
  year={2021},
  address = {New York, NY, USA}
}
@inproceedings {park2020hetpipe,
author = {Jay H. Park and Gyeongchan Yun and Chang M. Yi and Nguyen T. Nguyen and Seungmin Lee and Jaesik Choi and Sam H. Noh and Young-ri Choi},
title = {{HetPipe}: Enabling Large {DNN} Training on (Whimpy) Heterogeneous {GPU} Clusters through Integration of Pipelined Model Parallelism and Data Parallelism},
booktitle = {2020 USENIX Annual Technical Conference (USENIX ATC 20)},
year = {2020},
isbn = {978-1-939133-14-4},
pages = {307--321},
url = {https://www.usenix.org/conference/atc20/presentation/park},
publisher = {USENIX Association},
month = jul
}
@article{brakel2024model,
  title={Model Parallelism on Distributed Infrastructure: A Literature Review from Theory to {LLM} Case-Studies},
  author={Brakel, Felix and Odyurt, Uraz and Varbanescu, Ana-Lucia},
  journal={arXiv preprint arXiv:2403.03699},
  year={2024}
}
@article{yi2022optimizing,
  title={Optimizing {DNN} compilation for distributed training with joint OP and tensor fusion},
  author={Yi, Xiaodong and Zhang, Shiwei and Diao, Lansong and Wu, Chuan and Zheng, Zhen and Fan, Shiqing and Wang, Siyu and Yang, Jun and Lin, Wei},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={33},
  number={12},
  pages={4694--4706},
  year={2022},
  publisher={IEEE}
}
@inproceedings{li2023colossal,
  title={Colossal-{AI}: A unified deep learning system for large-scale parallel training},
  author={Li, Shenggui and Liu, Hongxin and Bian, Zhengda and Fang, Jiarui and Huang, Haichen and Liu, Yuliang and Wang, Boxiang and You, Yang},
  booktitle={Proceedings of the 52nd International Conference on Parallel Processing},
  pages={766--775},
  year={2023}
}
@article{li2020pytorch,
  title={Pytorch distributed: Experiences on accelerating data parallel training},
  author={Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and others},
  journal={arXiv preprint arXiv:2006.15704},
  year={2020}
}
@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{chen2016revisiting,
  title={Revisiting distributed synchronous SGD},
  author={Chen, Jianmin and Pan, Xinghao and Monga, Rajat and Bengio, Samy and Jozefowicz, Rafal},
  journal={arXiv preprint arXiv:1604.00981},
  year={2016}
}
@inproceedings{cui2016geeps,
  title={Gee{PS}: Scalable deep learning on distributed gpus with a gpu-specialized parameter server},
  author={Cui, Henggang and Zhang, Hao and Ganger, Gregory R and Gibbons, Phillip B and Xing, Eric P},
  booktitle={Proceedings of the eleventh european conference on computer systems},
  pages={1--16},
  year={2016}
}
@inproceedings{boral2023anomaly,
  title={Anomaly Detection in Streaming Environment by Evolving Neural Network with Interim Decision},
  author={Boral, Subhadip and Poddar, Sayan and Ghosh, Ashish},
  booktitle={2023 IEEE Region 10 Symposium (TENSYMP)},
  pages={1--6},
  year={2023},
  organization={IEEE}
}
@article{nassif2021machine,
  title={Machine learning for anomaly detection: A systematic review},
  author={Nassif, Ali Bou and Talib, Manar Abu and Nasir, Qassim and Dakalbab, Fatima Mohamad},
  journal={Ieee Access},
  volume={9},
  pages={78658--78700},
  year={2021},
  publisher={IEEE}
}
@article{guan2019xpipe,
  title={{XPipe}: Efficient pipeline model parallelism for multi-GPU {DNN} training},
  author={Guan, Lei and Yin, Wotao and Li, Dongsheng and Lu, Xicheng},
  journal={arXiv preprint arXiv:1911.04610},
  year={2019}
}
@article{chen2018efficient,
  title={Efficient and robust parallel {DNN} training through model parallelism on multi-gpu platform},
  author={Chen, Chi-Chung and Yang, Chia-Lin and Cheng, Hsiang-Yun},
  journal={arXiv preprint arXiv:1809.02839},
  year={2018}
}
@article{kingma2014adam,
  title={{Adam}: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@article{LIU2024317,
title = {{APapo}: An asynchronous parallel optimization method for {DNN} models},
journal = {Future Generation Computer Systems},
volume = {152},
pages = {317-330},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23004041},
author = {Shuai Liu and Tao Ju}
}
@article{LI2021206,
title = {Optimizing makespan and resource utilization for multi-{DNN} training in GPU cluster},
journal = {Future Generation Computer Systems},
volume = {125},
pages = {206-220},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.06.021},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21002168},
author = {Zhongjin Li and Victor Chang and Haiyang Hu and Maozhong Fu and Jidong Ge and Francesco Piccialli}
}
@article{ZHANG2023107,
title = {Experimental evaluation of the performance of {Gpipe} parallelism},
journal = {Future Generation Computer Systems},
volume = {147},
pages = {107-118},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.04.033},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23001735}
}
@article{AKINTOYE2023432,
title = {Layer-wise partitioning and merging for efficient and scalable deep learning},
journal = {Future Generation Computer Systems},
volume = {149},
pages = {432-444},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.07.043},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2300300X},
author = {S.B. Akintoye and L. Han and H. Lloyd and X. Zhang and D. Dancey and H. Chen and D. Zhang}
}
@article{besta2024parallel,
  title={Parallel and distributed graph neural networks: An in-depth concurrency analysis},
  author={Besta, Maciej and Hoefler, Torsten},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2024},
  publisher={IEEE}
}
@article{zhang2023pipepar,
  title={{PipePar}: Enabling fast {DNN} pipeline parallel training in heterogeneous GPU clusters},
  author={Zhang, Jinghui and Niu, Geng and Dai, Qiangsheng and Li, Haorui and Wu, Zhihua and Dong, Fang and Wu, Zhiang},
  journal={Neurocomputing},
  volume={555},
  pages={126661},
  year={2023},
  publisher={Elsevier}
}
@article{dutta2024timeprest,
  title={Ti{M}e{PR}e{S}t: Time and Memory Efficient Pipeline Parallel {DNN} Training with Removed Staleness},
  author={Dutta, Ankita and Chaki, Nabendu and De, Rajat K},
  journal={arXiv preprint arXiv:2410.14312},
  note ={Submitted to FGCS (Under Review)},
  year={2024}  
}
@article{al2024optimizing,
  title={Optimizing {DNN} training with pipeline model parallelism for enhanced performance in embedded systems},
  author={Al Maruf, Md and Azim, Akramul and Auluck, Nitin and Sahi, Mansi},
  journal={Journal of Parallel and Distributed Computing},
  volume={190},
  pages={104890},
  year={2024},
  publisher={Elsevier}
}
@inproceedings{hu2022pipeedge,
  title={Pipe{E}dge: Pipeline parallelism for large-scale model inference on heterogeneous edge devices},
  author={Hu, Yang and Imes, Connor and Zhao, Xuanang and Kundu, Souvik and Beerel, Peter A and Crago, Stephen P and Walters, John Paul},
  booktitle={Proceedings of the 2022 25th Euromicro Conference on Digital System Design (DSD)},
  pages={298--307},
  year={2022},
  organization={IEEE}
}
@inproceedings{li2021terapipe,
  title={Tera{P}ipe: Token-level pipeline parallelism for training large-scale language models},
  author={Li, Zhuohan and Zhuang, Siyuan and Guo, Shiyuan and Zhuo, Danyang and Zhang, Hao and Song, Dawn and Stoica, Ion},
  booktitle={Proceedings of the International Conference on Machine Learning},
  pages={6543--6552},
  year={2021},
  organization={PMLR}
}
@inproceedings{kim2023bpipe,
  title={{Bpipe: Memory-balanced pipeline parallelism for training large language models}},
  author={Kim, Taebum and Kim, Hyoungjoo and Yu, Gyeong-In and Chun, Byung-Gon},
  booktitle={International Conference on Machine Learning},
  pages={16639--16653},
  year={2023},
  organization={PMLR}
}
@article{zhao2021v,
  title={{v Pipe: A virtualized acceleration system for achieving efficient and scalable pipeline parallel {DNN} training}},
  author={Zhao, Shixiong and Li, Fanxin and Chen, Xusheng and Guan, Xiuxian and Jiang, Jianyu and Huang, Dong and Qing, Yuhao and Wang, Sen and Wang, Peng and Zhang, Gong and others},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={33},
  number={3},
  pages={489--506},
  year={2021},
  publisher={IEEE}
}
@inproceedings{jacobs2024system,
  title={{System optimizations for enabling training of extreme long sequence transformer models}},
  author={Jacobs, Sam Ade and Tanaka, Masahiro and Zhang, Chengming and Zhang, Minjia and Aminadabi, Reza Yazdani and Song, Shuaiwen Leon and Rajbhandari, Samyam and He, Yuxiong},
  booktitle={Proceedings of the 43rd ACM Symposium on Principles of Distributed Computing},
  pages={121--130},
  year={2024}
}
@article{10049507,
  author={Lai, Zhiquan and Li, Shengwei and Tang, Xudong and Ge, Keshi and Liu, Weijie and Duan, Yabo and Qiao, Linbo and Li, Dongsheng},
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={{Merak: An Efficient Distributed {DNN} Training Framework With Automated 3D Parallelism for Giant Foundation Models}}, 
  year={2023},
  volume={34},
  number={5},
  pages={1466-1478},
  keywords={Training;Parallel processing;Three-dimensional displays;Solid modeling;Computational modeling;Pipelines;Data models;Deep learning;distributed systems;foundation model training},
  doi={10.1109/TPDS.2023.3247001}
}
@inproceedings{10.1145/3472456.3472497,
author = {Ye, Xiangyu and Lai, Zhiquan and Li, Shengwei and Cai, Lei and Sun, Ding and Qiao, Linbo and Li, Dongsheng},
title = {{Hippie: A Data-Paralleled Pipeline Approach to Improve Memory-Efficiency and Scalability for Large {DNN} Training}},
year = {2021},
isbn = {9781450390682},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472456.3472497},
doi = {10.1145/3472456.3472497},
abstract = {With the increase of both data and parameter volume, it has become a big challenge to efficiently train large-scale DNN models on distributed platforms. Ordinary parallelism modes, i.e., data parallelism, model parallelism and pipeline parallelism, can no longer satisfy the efficient scaling of large DNN model training on multiple nodes. Meanwhile, the problem of too much memory consumption seriously restricts GPU computing efficiency and training throughput. In this paper, we propose Hippie, a hybrid parallel training framework that integrates pipeline parallelism and data parallelism to improve the memory efficiency and scalability of large DNN training. Hippie adopts a hybrid parallel method based on hiding gradient communication, which improves the throughput and scalability of training. Meanwhile, Hippie introduces the last-stage pipeline scheduling and recomputation for specific layers to effectively reduce the memory overhead and ease the difficulties of training large DNN models on memory-constrained devices. To achieve a more reasonable evaluation of the optimization effect, we propose an index of memory efficiency (ME) to represent the tradeoff between throughput and memory overhead. We implement Hippie based on PyTorch and NCCL. Experiments on various models show that Hippie achieves above 90\% scaling efficiency on a 16-GPU platform. Moreover, Hippie increases throughput by up to 80\% while saving 57\% of memory overhead, achieving 4.18 \texttimes{} memory efficiency.},
booktitle = {Proceedings of the 50th International Conference on Parallel Processing},
articleno = {71},
numpages = {10},
keywords = {memory efficiency, hybrid parallelism, distributed training, deep learning},
location = {Lemont, IL, USA},
series = {ICPP '21}
}
@article{chen2022sapipe,
  title={{SAPipe: Staleness-aware pipeline for data parallel DNN training}},
  author={Chen, Yangrui and Xie, Cong and Ma, Meng and Gu, Juncheng and Peng, Yanghua and Lin, Haibin and Wu, Chuan and Zhu, Yibo},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={17981--17993},
  year={2022}
}
@article{tangkoala,
  title={{Koala: Efficient Pipeline Training through Automated Schedule Searching on Domain-Specific Language}},
  author={Tang, Yu and Yin, Lujia and Li, Qiao and Zhu, Hongyu and Li, Hengjie and Zhang, Xingcheng and Qiao, Linbo and Li, Dongsheng and Li, Jiaxin},
  journal={ACM Transactions on Architecture and Code Optimization},
  publisher={ACM New York, NY}
}
@article{wan2025pipeoffload,
  title={{PipeOffload: Improving Scalability of Pipeline Parallelism with Memory Optimization}},
  author={Wan, Xinyi and Qi, Penghui and Huang, Guangxing and Li, Jialin and Lin, Min},
  journal={arXiv preprint arXiv:2503.01328},
  year={2025}
}
@article{lin2025enhancing,
  title={{Enhancing Memory Efficiency in Large Language Model Training Through Chronos-aware Pipeline Parallelism}},
  author={Lin, Xinyuan and Li, Chenlu and Huang, Zongle and Wang, Chunyu and Xiao, Bo and Yang, Huazhong and Duan, Shishi and Liu, Yongpan},
  journal={arXiv preprint arXiv:2503.03182},
  year={2025}
}
@inproceedings{liu2025mario,
  title={{Mario: Near Zero-cost Activation Checkpointing in Pipeline Parallelism}},
  author={Liu, Weijian and Li, Mingzhen and Tan, Guangming and Jia, Weile},
  booktitle={Proceedings of the 30th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming},
  pages={197--211},
  year={2025}
}
@inproceedings{lin2025weipipe,
  title={{WeiPipe: Weight Pipeline Parallelism for Communication-Effective Long-Context Large Model Training}},
  author={Lin, Junfeng and Liu, Ziming and You, Yang and Wang, Jun and Zhang, Weihao and Zhao, Rong},
  booktitle={Proceedings of the 30th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming},
  pages={225--238},
  year={2025}
}
@inproceedings{sun2025mepipe,
  title={MEPipe: Democratizing LLM Training with Memory-Efficient Slice-Level Pipeline Scheduling on Cost-Effective Accelerators},
  author={Sun, Zhenbo and Chen, Shengqi and Wang, Yuanwei and Sha, Jian and Feng, Guanyu and Chen, Wenguang},
  booktitle={Proceedings of the Twentieth European Conference on Computer Systems},
  pages={1263--1278},
  year={2025}
}
@article{liu2024deepseek,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}
@inproceedings{qi2024zero,
  title={Zero bubble (almost) pipeline parallelism},
  author={Qi, Penghui and Wan, Xinyi and Huang, Guangxing and Lin, Min},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}
@inproceedings{jeon2025graphpipe,
  title={Graph{P}ipe: Improving performance and scalability of dnn training with graph pipeline parallelism},
  author={Jeon, Byungsoo and Wu, Mengdi and Cao, Shiyi and Kim, Sunghyun and Park, Sunghyun and Aggarwal, Neeraj and Unger, Colin and Arfeen, Daiyaan and Liao, Peiyuan and Miao, Xupeng and others},
  booktitle={Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1},
  pages={557--571},
  year={2025}
}