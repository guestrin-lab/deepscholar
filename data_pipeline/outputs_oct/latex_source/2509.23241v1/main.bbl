\begin{thebibliography}{10}

\bibitem{10049507}
Z.~Lai, S.~Li, X.~Tang, K.~Ge, W.~Liu, Y.~Duan, L.~Qiao, and D.~Li, ``{Merak: An Efficient Distributed {DNN} Training Framework With Automated 3D Parallelism for Giant Foundation Models},'' {\em IEEE Transactions on Parallel and Distributed Systems}, vol.~34, no.~5, pp.~1466--1478, 2023.

\bibitem{LIU2024317}
S.~Liu and T.~Ju, ``{APapo}: An asynchronous parallel optimization method for {DNN} models,'' {\em Future Generation Computer Systems}, vol.~152, pp.~317--330, 2024.

\bibitem{kim2023bpipe}
T.~Kim, H.~Kim, G.-I. Yu, and B.-G. Chun, ``{Bpipe: Memory-balanced pipeline parallelism for training large language models},'' in {\em International Conference on Machine Learning}, pp.~16639--16653, PMLR, 2023.

\bibitem{chilimbi2014project}
T.~Chilimbi, Y.~Suzue, J.~Apacible, and K.~Kalyanaraman, ``Project {A}dam: Building an efficient and scalable deep learning training system,'' in {\em 11th USENIX symposium on operating systems design and implementation (OSDI 14)}, pp.~571--582, 2014.

\bibitem{unnikrishnan2021layerpipe}
N.~K. Unnikrishnan and K.~K. Parhi, ``{LayerPipe}: Accelerating deep neural network training by intra-layer and inter-layer gradient pipelining and multiprocessor scheduling,'' in {\em Proceedings of the 2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD)}, pp.~1--8, IEEE, 2021.

\bibitem{brakel2024model}
F.~Brakel, U.~Odyurt, and A.-L. Varbanescu, ``Model parallelism on distributed infrastructure: A literature review from theory to {LLM} case-studies,'' {\em arXiv preprint arXiv:2403.03699}, 2024.

\bibitem{shoeybi2019megatron}
M.~Shoeybi, M.~Patwary, R.~Puri, P.~LeGresley, J.~Casper, and B.~Catanzaro, ``Megatron-{LM}: Training multi-billion parameter language models using model parallelism,'' {\em arXiv preprint arXiv:1909.08053}, 2019.

\bibitem{yi2022optimizing}
X.~Yi, S.~Zhang, L.~Diao, C.~Wu, Z.~Zheng, S.~Fan, S.~Wang, J.~Yang, and W.~Lin, ``Optimizing {DNN} compilation for distributed training with joint op and tensor fusion,'' {\em IEEE Transactions on Parallel and Distributed Systems}, vol.~33, no.~12, pp.~4694--4706, 2022.

\bibitem{jiang2024megascale}
Z.~Jiang, H.~Lin, Y.~Zhong, Q.~Huang, Y.~Chen, Z.~Zhang, Y.~Peng, X.~Li, C.~Xie, S.~Nong, {\em et~al.}, ``{MegaScale}: Scaling large language model training to more than 10,000 {GPUs},'' in {\em Proceedings of the 21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24)}, pp.~745--760, 2024.

\bibitem{narayanan2019pipedream}
D.~Narayanan, A.~Harlap, A.~Phanishayee, V.~Seshadri, N.~R. Devanur, G.~R. Ganger, P.~B. Gibbons, and M.~Zaharia, ``{PipeDream}: generalized pipeline parallelism for {DNN} training,'' in {\em Proceedings of the 27th ACM Symposium on Operating Systems Principles}, SOSP'19, (New York, NY, USA), p.~1â€“15, 2019.

\bibitem{huang2019gpipe}
Y.~Huang, Y.~Cheng, A.~Bapna, O.~Firat, D.~Chen, M.~Chen, H.~Lee, J.~Ngiam, Q.~V. Le, Y.~Wu, {\em et~al.}, ``G{P}ipe: Efficient training of giant neural networks using pipeline parallelism,'' {\em Advances in Neural Information Processing Systems}, vol.~32, 2019.

\bibitem{boral2023anomaly}
S.~Boral, S.~Poddar, and A.~Ghosh, ``Anomaly detection in streaming environment by evolving neural network with interim decision,'' in {\em 2023 IEEE Region 10 Symposium (TENSYMP)}, pp.~1--6, IEEE, 2023.

\bibitem{zhao2021v}
S.~Zhao, F.~Li, X.~Chen, X.~Guan, J.~Jiang, D.~Huang, Y.~Qing, S.~Wang, P.~Wang, G.~Zhang, {\em et~al.}, ``{v Pipe: A virtualized acceleration system for achieving efficient and scalable pipeline parallel {DNN} training},'' {\em IEEE Transactions on Parallel and Distributed Systems}, vol.~33, no.~3, pp.~489--506, 2021.

\bibitem{ben2019demystifying}
T.~Ben-Nun and T.~Hoefler, ``Demystifying parallel and distributed deep learning: An in-depth concurrency analysis,'' {\em ACM Computing Surveys (CSUR)}, vol.~52, no.~4, pp.~1--43, 2019.

\bibitem{chen2022sapipe}
Y.~Chen, C.~Xie, M.~Ma, J.~Gu, Y.~Peng, H.~Lin, C.~Wu, and Y.~Zhu, ``{SAPipe: Staleness-aware pipeline for data parallel DNN training},'' {\em Advances in neural information processing systems}, vol.~35, pp.~17981--17993, 2022.

\bibitem{LI2021206}
Z.~Li, V.~Chang, H.~Hu, M.~Fu, J.~Ge, and F.~Piccialli, ``Optimizing makespan and resource utilization for multi-{DNN} training in gpu cluster,'' {\em Future Generation Computer Systems}, vol.~125, pp.~206--220, 2021.

\bibitem{raina2009large}
R.~Raina, A.~Madhavan, and A.~Y. Ng, ``Large-scale deep unsupervised learning using graphics processors,'' in {\em Proceedings of the 26th annual international conference on machine learning}, pp.~873--880, 2009.

\bibitem{jacobs2024system}
S.~A. Jacobs, M.~Tanaka, C.~Zhang, M.~Zhang, R.~Y. Aminadabi, S.~L. Song, S.~Rajbhandari, and Y.~He, ``{System optimizations for enabling training of extreme long sequence transformer models},'' in {\em Proceedings of the 43rd ACM Symposium on Principles of Distributed Computing}, pp.~121--130, 2024.

\bibitem{shallue2019measuring}
C.~J. Shallue, J.~Lee, J.~Antognini, J.~Sohl-Dickstein, R.~Frostig, and G.~E. Dahl, ``Measuring the effects of data parallelism on neural network training,'' {\em Journal of Machine Learning Research}, vol.~20, no.~112, pp.~1--49, 2019.

\bibitem{li2020pytorch}
S.~Li, Y.~Zhao, R.~Varma, O.~Salpekar, P.~Noordhuis, T.~Li, A.~Paszke, J.~Smith, B.~Vaughan, P.~Damania, {\em et~al.}, ``Pytorch distributed: Experiences on accelerating data parallel training,'' {\em arXiv preprint arXiv:2006.15704}, 2020.

\bibitem{cui2016geeps}
H.~Cui, H.~Zhang, G.~R. Ganger, P.~B. Gibbons, and E.~P. Xing, ``Gee{PS}: Scalable deep learning on distributed gpus with a gpu-specialized parameter server,'' in {\em Proceedings of the eleventh european conference on computer systems}, pp.~1--16, 2016.

\bibitem{dean2012large}
J.~Dean, G.~Corrado, R.~Monga, K.~Chen, M.~Devin, M.~Mao, M.~Ranzato, A.~Senior, P.~Tucker, K.~Yang, {\em et~al.}, ``Large scale distributed deep networks,'' {\em Advances in Neural Information Processing Systems}, vol.~25, 2012.

\bibitem{li2013parameter}
M.~Li, L.~Zhou, Z.~Yang, A.~Li, F.~Xia, D.~G. Andersen, and A.~Smola, ``Parameter server for distributed machine learning,'' in {\em Big learning NIPS workshop}, vol.~6, Lake Tahoe, CA, 2013.

\bibitem{paszke2019pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen, Z.~Lin, N.~Gimelshein, L.~Antiga, {\em et~al.}, ``Pytorch: An imperative style, high-performance deep learning library,'' {\em Advances in neural information processing systems}, vol.~32, 2019.

\bibitem{li2023colossal}
S.~Li, H.~Liu, Z.~Bian, J.~Fang, H.~Huang, Y.~Liu, B.~Wang, and Y.~You, ``Colossal-{AI}: A unified deep learning system for large-scale parallel training,'' in {\em Proceedings of the 52nd International Conference on Parallel Processing}, pp.~766--775, 2023.

\bibitem{dutta2024timeprest}
A.~Dutta, N.~Chaki, and R.~K. De, ``Ti{M}e{PR}e{S}t: Time and memory efficient pipeline parallel {DNN} training with removed staleness,'' {\em arXiv preprint arXiv:2410.14312}, 2024.
\newblock Submitted to FGCS (Under Review).

\bibitem{AKINTOYE2023432}
S.~Akintoye, L.~Han, H.~Lloyd, X.~Zhang, D.~Dancey, H.~Chen, and D.~Zhang, ``Layer-wise partitioning and merging for efficient and scalable deep learning,'' {\em Future Generation Computer Systems}, vol.~149, pp.~432--444, 2023.

\bibitem{chen2016revisiting}
J.~Chen, X.~Pan, R.~Monga, S.~Bengio, and R.~Jozefowicz, ``Revisiting distributed synchronous sgd,'' {\em arXiv preprint arXiv:1604.00981}, 2016.

\bibitem{wan2025pipeoffload}
X.~Wan, P.~Qi, G.~Huang, J.~Li, and M.~Lin, ``{PipeOffload: Improving Scalability of Pipeline Parallelism with Memory Optimization},'' {\em arXiv preprint arXiv:2503.01328}, 2025.

\bibitem{narayanan2021memory}
D.~Narayanan, A.~Phanishayee, K.~Shi, X.~Chen, and M.~Zaharia, ``Memory-efficient pipeline-parallel {DNN} training,'' in {\em Proceedings of the International Conference on Machine Learning}, pp.~7937--7947, PMLR, 2021.

\bibitem{10.1145/3472456.3472497}
X.~Ye, Z.~Lai, S.~Li, L.~Cai, D.~Sun, L.~Qiao, and D.~Li, ``{Hippie: A Data-Paralleled Pipeline Approach to Improve Memory-Efficiency and Scalability for Large {DNN} Training},'' in {\em Proceedings of the 50th International Conference on Parallel Processing}, ICPP '21, (New York, NY, USA), Association for Computing Machinery, 2021.

\bibitem{zhang2023pipepar}
J.~Zhang, G.~Niu, Q.~Dai, H.~Li, Z.~Wu, F.~Dong, and Z.~Wu, ``{PipePar}: Enabling fast {DNN} pipeline parallel training in heterogeneous gpu clusters,'' {\em Neurocomputing}, vol.~555, p.~126661, 2023.

\bibitem{tangkoala}
Y.~Tang, L.~Yin, Q.~Li, H.~Zhu, H.~Li, X.~Zhang, L.~Qiao, D.~Li, and J.~Li, ``{Koala: Efficient Pipeline Training through Automated Schedule Searching on Domain-Specific Language},'' {\em ACM Transactions on Architecture and Code Optimization}.

\bibitem{liu2025mario}
W.~Liu, M.~Li, G.~Tan, and W.~Jia, ``{Mario: Near Zero-cost Activation Checkpointing in Pipeline Parallelism},'' in {\em Proceedings of the 30th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming}, pp.~197--211, 2025.

\bibitem{lin2025weipipe}
J.~Lin, Z.~Liu, Y.~You, J.~Wang, W.~Zhang, and R.~Zhao, ``{WeiPipe: Weight Pipeline Parallelism for Communication-Effective Long-Context Large Model Training},'' in {\em Proceedings of the 30th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming}, pp.~225--238, 2025.

\bibitem{liu2024deepseek}
A.~Liu, B.~Feng, B.~Xue, B.~Wang, B.~Wu, C.~Lu, C.~Zhao, C.~Deng, C.~Zhang, C.~Ruan, {\em et~al.}, ``Deepseek-v3 technical report,'' {\em arXiv preprint arXiv:2412.19437}, 2024.

\bibitem{sun2025mepipe}
Z.~Sun, S.~Chen, Y.~Wang, J.~Sha, G.~Feng, and W.~Chen, ``Mepipe: Democratizing llm training with memory-efficient slice-level pipeline scheduling on cost-effective accelerators,'' in {\em Proceedings of the Twentieth European Conference on Computer Systems}, pp.~1263--1278, 2025.

\bibitem{qi2024zero}
P.~Qi, X.~Wan, G.~Huang, and M.~Lin, ``Zero bubble (almost) pipeline parallelism,'' in {\em The Twelfth International Conference on Learning Representations}, 2024.

\bibitem{jeon2025graphpipe}
B.~Jeon, M.~Wu, S.~Cao, S.~Kim, S.~Park, N.~Aggarwal, C.~Unger, D.~Arfeen, P.~Liao, X.~Miao, {\em et~al.}, ``Graph{P}ipe: Improving performance and scalability of dnn training with graph pipeline parallelism,'' in {\em Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1}, pp.~557--571, 2025.

\bibitem{lin2025enhancing}
X.~Lin, C.~Li, Z.~Huang, C.~Wang, B.~Xiao, H.~Yang, S.~Duan, and Y.~Liu, ``{Enhancing Memory Efficiency in Large Language Model Training Through Chronos-aware Pipeline Parallelism},'' {\em arXiv preprint arXiv:2503.03182}, 2025.

\end{thebibliography}
