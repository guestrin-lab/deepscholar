\section{Appendix}
We provide additional results and details that were not included in the main text due to space limitations.
\subsection{Correlation Heatmaps: Uncertainty versus Prediction Error}
Fig.~\ref{fig:heatmaps} shows correlation heatmaps discussed in Section \label{subsec:unc_analysis} in the main text. This heatmap visualizes the relationship between the Mean Absolute Error (MAE) of MoGU's predictions and its reported uncertainties (aleatoric, epistemic, and total) for a model using three iTransformer experts.
    \begin{figure}[h!]
        \centering
        \begin{subfigure}[t]{0.24\textwidth}
            \centering
            \includegraphics[width=\linewidth]{Chapters/Figures/itransformer_etth1_heatmap_elig.png}
             \subcaption{ETTh1}
            \label{fig:heatmap_a}
        \end{subfigure}
        \hfill % Adds horizontal space between subfigures
        \begin{subfigure}[t]{0.23\textwidth}
            \centering
            \includegraphics[width=\linewidth]{Chapters/Figures/itransformer_etth2_heatmap_elig.png}
             \subcaption{ETTh2}
            \label{fig:heatmap_b}
        \end{subfigure}
        \hfill % Adds horizontal space between subfigures
        \begin{subfigure}[t]{0.23\textwidth}
            \centering
            \includegraphics[width=\linewidth]{Chapters/Figures/itransformer_ettm1_heatmap_elig.png}
             \subcaption{ETTm1}
            \label{fig:heatmap_c}
        \end{subfigure}
        \hfill % Adds horizontal space between subfigures
        \begin{subfigure}[t]{0.23\textwidth}
            \centering
            \includegraphics[width=\linewidth]{Chapters/Figures/itransformer_ettm2_heatmap_elig.png}
            \subcaption{ETTm2}
            \label{fig:heatmap_d}
        \end{subfigure}
        
        \caption{Heatmaps of the Pearson correlation between MoGU's reported uncertainties (aleatoric, epistemic, and total) and the MAE of its predictions. The correlation is displayed per variable for the ETT datasets.}
        \label{fig:heatmaps}
    \end{figure}
\subsection{Additional Ablations}\label{subsec:app_ablations}
\textbf{Resolution of Uncertainty Estimation.} We provide Table \ref{tab:ablations_time}, discussed in the main text. This table explores an alternative where the expert estimates uncertainty at the variable level ('Time-Fixed'), rather than for each individual time point ('Time-Varying').

\textbf{Loss Function.} We note that the MoGU model can also be optimized through the following MoG loss:
\begin{equation}\label{eq:mog_loss_alt}
    \mathcal{L}_{}= - \log (\sum_i w_i(x)\mathcal{N}(y;y_i(x), \sigma_i^2(x))) 
\end{equation}
where $\mathcal{N}$ is the Normal density function and the loss
has the form of a Negative Log Likelihood (NLL) of a MoG distribution.
We compare the performance of our model when using the loss presented in Eq. \ref{eq:mog_loss} and when using the aforementioned alternative (Eq. \ref{eq:mog_loss_alt}). The results of this experiment, presented in Table \ref{tab:ablations_loss} in our Appendix, suggest that optimizing with our proposed loss (Eq. \ref{eq:mog_loss}) yields more effective learning and consistently better results by imposing a stricter constraint on expert learning compared to the MoG loss.

\input{Chapters/Tables/table_ablations_appendix}
\subsection{MoGU's Algorithm}
We provide the pseudo code for MoGU in Listing 1 to enhance clarity.
Furthermore, to ensure reproducibility, our code and the scripts needed to reproduce the main results are available at: \url{https://github.com/yolish/moe_unc_tsf}
We implemented MoGU to be highly configurable, so that users can  specify the number of experts, the expert architecture, the mixture type (MoE or MoG) and the gating mechanism.

\begin{algorithm}[t]
\label{listing:mogu}
\caption{Mixture-of-Gaussians with Uncertainty-based gating (MoGU)}
\begin{algorithmic}[1]  % line numbers
\Require Training set $X$, labels $y$
\Ensure Model parameters $\theta$

\For{each training epoch}
  \For{each mini-batch $\mathcal{B}$}
    \For{each sample $x \in \mathcal{B}$}
      \For{each expert $i=1,\dots,k$}
        \State Compute expert output $f_i(x) = \mathcal{N}(y; \mu_i(x,\theta),\sigma_i^2(x,\theta))$.
        \State Set $w_i(x)= \frac{\sigma_i^{-2}(x)}{\sum_j \sigma_j^{-2}(x)}$.
      \EndFor
     \EndFor
    %\State Compute loss $\mathcal{L} = -\sum_{x \in \mathcal{B}} \sum_{i=1}^k w_i(x) \log (f_i(x))$.
    \State Compute loss $\mathcal{L}= \sum w_i(x)\mathcal{L_{NLLG}}(y;y_i(x), \sigma_i^2(x)))$.
    \State Update model parameters.
  \EndFor
\EndFor 

\State Test time prediction is $\hat{y}(x) = \sum_i w_i(x) \mu_i(x)$.
\State Test time prediction uncertainty is: $\sum_i w_i(x) \sigma_i^2(x) +  \sum_i w_i(x) (\hat{y}(x)-\mu_i(x))^2$.
\end{algorithmic}
\end{algorithm}