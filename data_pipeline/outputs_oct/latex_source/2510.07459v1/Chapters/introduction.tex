\section{Introduction}\label{sec:introduction}
Mixture-of-Experts (MoE) is an architectural paradigm that adaptively combines predictions from multiple neural modules, known as "experts," via a learned gating mechanism. This concept has evolved from ensemble-based MoEs, where experts, jointly trained with a gating function, are often full, independent models whose outputs are combined to improve overall performance and robustness \citep{jacobs1991adaptive}. More recently, MoE layers have been integrated within larger neural architectures, with experts operating in a latent domain. These "latent MoEs" offer significant scalability benefits, especially in large language models (LLMs) \citep{shazeer2017outrageously,fedus2022switch}.
MoE makes it possible to train massive but efficient LLMs, where each token activates only a fraction of the model’s parameters, enabling specialization, better performance, and lower computational cost compared to equally sized dense models.

Regardless of their specific implementation, conventional MoE systems typically produce point estimates, lacking a direct quantification of their uncertainty. In critical applications, this absence of uncertainty information hinders interpretability, making it difficult for users to gauge the reliability of a prediction and limits informed decision-making, as the system cannot express its confidence or identify ambiguous cases. Importantly, the learned gating mechanism, which dictates the relative contribution of each expert, does not take into account expert confidence, potentially leading to suboptimal routing decisions.

In this work, we propose Mixture-of-Gaussians with Uncertainty-based Gating (MoGU), a framework for uncertainty-aware MoE architectures, which provides explicit uncertainty quantification for both individual experts and the overall MoE model. Our approach fundamentally reimagines the expert's output: instead of a point estimate, we model each expert's prediction as a random variable drawn from a normal distribution. In this setup, each expert simultaneously predicts both the mean (the label estimate) and variance of the distribution, representing its predictive uncertainty. This shift enables a more nuanced understanding of expert behavior and the derivation of the overall model's uncertainty. Furthermore, we introduce a novel gating mechanism where the estimated uncertainty of each expert directly informs its relative contribution to the overall MoE prediction, bypassing the need for a separate gating function typically found in traditional MoE setups. This creates a self-aware MoE where more confident experts naturally exert greater influence.

We evaluate MoGU on time series forecasting as our primary regression task. This choice is motivated by the inherent uncertainty in real-world time series data and the wide variety of expert architectures applicable to forecasting tasks across numerous domains \citep{time_series_survey, wang2024deep}. Our evaluation spans various expert types, forecasting benchmarks and forecasting horizon sizes, allowing for a comprehensive assessment of our method's efficacy. MoGU is shown to consistently yield more accurate forecasts compared to input-based gating MoE architectures, while simultaneously, providing uncertainty estimates that are positively correlated with prediction error. These estimates are available at both the individual expert and overall model levels. By further distinguishing between aleatoric (data-related) and epistemic (model-related) uncertainty, MoGU offers valuable insights into the source of a model's uncertainty. We also conducted a detailed ablation study to validate our key design choices.

In summary, our contributions are as follows: 
\begin{itemize}
\item \textbf{MoGU: A Novel Framework for Uncertainty-Aware MoE Architectures}: We introduce a novel framework that directly quantifies uncertainty for both individual experts and the overall model, moving beyond conventional point estimates. A key innovation is a routing mechanism that uses each expert’s estimated predictive uncertainty to dynamically determine its contribution to the final MoE output, replacing traditional input-based gating mechanisms.
\item \textbf{MoGU Improves Time Series Forecasting}: Our method effectively reduces forecasting error across various benchmarks, horizon lengths, and expert architectures.
\item \textbf{MoGU Provides Meaningful Uncertainty Estimates for Time Series Forecasting}: MoGU generates uncertainty estimates at the expert-level and overall. These estimates are positively correlated with prediction error, providing valuable insight into the model's confidence and the sources of its uncertainty.
\end{itemize}

By embedding uncertainty estimation into prediction and gating, MoGU moves beyond input-based gating  MoEs toward architectures that are more accurate, transparent, and reliable.

