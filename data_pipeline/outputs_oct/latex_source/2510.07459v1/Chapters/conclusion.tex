\section{Conclusion}
We introduced MoGU, a novel extension of MoE for time series forecasting. Instead of using traditional input-based gating, MoGU's gating mechanism aggregates expert predictions based on their individual uncertainty (variance) estimates. This approach led to superior performance over single-expert and conventional MoE models across various benchmarks, architectures, and time horizons. Our results suggest a promising new direction for MoEs: integrating probabilistic information directly into the gating process for more robust and reliable models.

\textbf{Limitations and Future Work:} While MoGU shows promise for time series forecasting, broadening its scope to other regression (and classification) tasks, will further validate its robustness and generalization. In addition, adapting its dense gating for sparse architectures like those in LLMs remains a challenge for future work.
\begin{comment}
While MoGU is shown to successfully quantify and leverage uncertainty for time series forecasting, we acknowledge that broadening its scope to other regression (and classification) tasks, will further validate its robustness and generalization. In addition,  
our uncertainty-derived gating mechanism is primarily designed for "ensemble" MoEs, which do not inherently require sparsity for scalability. Adapting this gating strategy to enable sparse activation, as required in "latent" MoE architectures prevalent in LLMs, is a challenging future research direction we plan to pursue. Finally, while our framework provides direct uncertainty estimates, we have not rigorously addressed the calibration of these uncertainties. Future research will focus on developing and integrating calibration techniques to enhance the trustworthiness of MoGU's uncertainty estimates.
We introduced MoGU, an extension to Mixture-of-Experts (MoE) architectures for regression tasks, where gating is based on the uncertainty estimates (variance) of each expert. Evaluated for time series forecasting, MoGU is shown to consistently outperform both single and traditional MoE setups across multiple forecasting benchmarks, expert architectures, and horizon lengths. Furthermore, its proposed uncertainty-based gating mechanism provides superior performance compared to the common input-based gating. This success highlights a promising new direction for enhancing MoE architectures by incorporating probabilistic information directly into the gating process, which could lead to more robust and reliable models in domains where uncertainty is a critical factor. 
\end{comment}