\begin{thebibliography}{28}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Cini et~al.(2025)Cini, Bogunovic, Pavez, and Cand{\`e}s]{cini2025corel}
Andrea Cini, Ilija Bogunovic, Eduardo Pavez, and Emmanuel~J. Cand{\`e}s.
\newblock Relational conformal prediction for correlated time series.
\newblock In \emph{Proceedings of the International Conference on Machine Learning (ICML)}, 2025.
\newblock arXiv:2502.09443.

\bibitem[Dai et~al.(2024)Dai, Deng, Zhao, Xu, Gao, Chen, Li, Zeng, Yu, Wu, et~al.]{dai2024deepseekmoe}
Damai Dai, Chengqi Deng, Chenggang Zhao, RX~Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Yu~Wu, et~al.
\newblock Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models.
\newblock \emph{arXiv preprint arXiv:2401.06066}, 2024.

\bibitem[Fedus et~al.(2022)Fedus, Zoph, and Shazeer]{fedus2022switch}
William Fedus, Barret Zoph, and Noam Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2022.

\bibitem[Gal \& Ghahramani(2016)Gal and Ghahramani]{gal2016dropout}
Yarin Gal and Zoubin Ghahramani.
\newblock Dropout as a bayesian approximation: Representing model uncertainty in deep learning.
\newblock In \emph{Proceedings of the 33rd International Conference on Machine Learning (ICML)}, pp.\  1050--1059, 2016.

\bibitem[Jacobs et~al.(1991)Jacobs, Jordan, Nowlan, and Hinton]{jacobs1991adaptive}
Robert~A Jacobs, Michael~I Jordan, Steven~J Nowlan, and Geoffrey~E Hinton.
\newblock Adaptive mixtures of local experts.
\newblock \emph{Neural computation}, 3\penalty0 (1):\penalty0 79--87, 1991.

\bibitem[Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, Bressand, et~al.]{jiang2024mixtral}
Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al.
\newblock Mixtral of experts.
\newblock \emph{arXiv preprint arXiv:2401.04088}, 2024.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{reformer}
Nikita Kitaev, {\L}ukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock \emph{arXiv preprint arXiv:2001.04451}, 2020.

\bibitem[Kuleshov et~al.(2018)Kuleshov, Fenner, and Ermon]{kuleshov2018accurate}
Volodymyr Kuleshov, Nathan Fenner, and Stefano Ermon.
\newblock Accurate uncertainties for deep learning using calibrated regression.
\newblock In \emph{Proceedings of the 35th International Conference on Machine Learning (ICML)}, pp.\  2796--2804, 2018.

\bibitem[Lai et~al.(2018)Lai, Chang, Yang, and Liu]{lai2018modeling}
Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu.
\newblock Modeling long-and short-term temporal patterns with deep neural networks.
\newblock In \emph{The 41st international ACM SIGIR conference on research \& development in information retrieval}, pp.\  95--104, 2018.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and Blundell]{lakshminarayanan2017simple}
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep ensembles.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, pp.\  6402--6413, 2017.

\bibitem[Lepikhin et~al.(2020)Lepikhin, Lee, Xu, Chen, Firat, Huang, Krikun, Shazeer, and Chen]{lepikhin2020gshard}
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen.
\newblock Gshard: Scaling giant models with conditional computation and automatic sharding.
\newblock \emph{arXiv preprint arXiv:2006.16668}, 2020.

\bibitem[Lim \& Zohren(2021)Lim and Zohren]{time_series_survey}
Bryan Lim and Stefan Zohren.
\newblock Time-series forecasting with deep learning: a survey.
\newblock \emph{Philosophical Transactions of the Royal Society A}, 379\penalty0 (2194):\penalty0 20200209, 2021.

\bibitem[Liu et~al.(2023)Liu, Hu, Zhang, Wu, Wang, Ma, and Long]{liu2023itransformer}
Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long.
\newblock itransformer: Inverted transformers are effective for time series forecasting.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2023.

\bibitem[Nie et~al.(2023)Nie, H.~Nguyen, Sinthong, and Kalagnanam]{Yuqietal-2023-PatchTST}
Yuqi Nie, Nam H.~Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam.
\newblock A time series is worth 64 words: Long-term forecasting with transformers.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Pavlitska et~al.(2025)Pavlitska, Maillard, Matejek, Lucic, and Houlsby]{pavlitska2025moeuncertainty}
Inna Pavlitska, Adrien Maillard, Brian Matejek, Mario Lucic, and Neil Houlsby.
\newblock Extracting uncertainty estimates from mixtures of experts for semantic segmentation.
\newblock \emph{arXiv preprint arXiv:2509.04816}, 2025.

\bibitem[Romano et~al.(2019)Romano, Patterson, and Candes]{romano2019conformalized}
Yaniv Romano, Evan Patterson, and Emmanuel Candes.
\newblock Conformalized quantile regression.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, pp.\  3543--3553, 2019.

\bibitem[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton, and Dean]{shazeer2017outrageously}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc~V. Le, Geoffrey Hinton, and Jeffrey Dean.
\newblock Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2017.

\bibitem[Vovk et~al.(2005)Vovk, Gammerman, and Shafer]{vovk2005algorithmic}
Vladimir Vovk, Alexander Gammerman, and Glenn Shafer.
\newblock \emph{Algorithmic Learning in a Random World}.
\newblock Springer, 2005.

\bibitem[Wang et~al.(2024{\natexlab{a}})Wang, Du, Cao, Zhang, Wang, Liang, and Wen]{wang2024deep}
Jun Wang, Wenjie Du, Wei Cao, Keli Zhang, Wenjia Wang, Yuxuan Liang, and Qingsong Wen.
\newblock Deep learning for multivariate time series imputation: A survey.
\newblock \emph{arXiv preprint arXiv:2402.04059}, 2024{\natexlab{a}}.

\bibitem[Wang et~al.(2024{\natexlab{b}})Wang, Wu, Shi, Hu, Luo, Ma, Zhang, and Zhou]{wang2024timemixer}
Shiyu Wang, Haixu Wu, Xiaoming Shi, Tengge Hu, Huakun Luo, Lintao Ma, James~Y Zhang, and Jun Zhou.
\newblock Timemixer: Decomposable multiscale mixing for time series forecasting.
\newblock \emph{arXiv preprint arXiv:2405.14616}, 2024{\natexlab{b}}.

\bibitem[Wang et~al.(2024{\natexlab{c}})Wang, Wu, Dong, Liu, Long, and Wang]{xwang2024deep}
Yuxuan Wang, Haixu Wu, Jiaxiang Dong, Yong Liu, Mingsheng Long, and Jianmin Wang.
\newblock Deep time series models: A comprehensive survey and benchmark.
\newblock \emph{arXiv preprint arXiv:2407.13278}, 2024{\natexlab{c}}.

\bibitem[Wu et~al.(2021)Wu, Xu, Wang, and Long]{autoformer}
Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long.
\newblock Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting.
\newblock \emph{Advances in neural information processing systems}, 2021.

\bibitem[Wu et~al.(2023)Wu, Hu, Liu, Zhou, Wang, and Long]{wu2023timesnet}
Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long.
\newblock Timesnet: Temporal 2d-variation modeling for general time series analysis.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Wu et~al.(2025)Wu, Lin, Vovk, and Chen]{wu2025eci}
Junxi Wu, Yilin Lin, Vladimir Vovk, and Changyou Chen.
\newblock Error-quantified conformal inference for time series.
\newblock In \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2025.
\newblock openreview.net/forum?id=RD9q5vEe1Q.

\bibitem[Zeng et~al.(2023)Zeng, Chen, Zhang, and Xu]{dlinear}
Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu.
\newblock Are transformers effective for time series forecasting?
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~37, pp.\  11121--11128, 2023.

\bibitem[Zhang et~al.(2023)Zhang, Chen, Li, Gao, and Zhang]{zhang2023mofme}
Yunbo Zhang, Yanhua Chen, Mingyang Li, Yujing Gao, and Zhiqiang Zhang.
\newblock Efficient deweather mixture-of-experts with uncertainty-aware feature-wise linear modulation.
\newblock \emph{arXiv preprint arXiv:2312.16610}, 2023.

\bibitem[Zhou et~al.(2021)Zhou, Zhang, Peng, Zhang, Li, Xiong, and Zhang]{informer}
Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.
\newblock Informer: Beyond efficient transformer for long sequence time-series forecasting.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~35, pp.\  11106--11115, 2021.

\end{thebibliography}
