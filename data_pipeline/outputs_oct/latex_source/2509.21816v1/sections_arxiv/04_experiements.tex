\section{Experiements}
% highlight 每一部分用了哪个模型。
% apply到了同一个agent+Generation是同一组setting
% rq 23 合并
% 把目前的metric分散到不同的rq中
% 简单介绍一下baseline


To comprehensively evaluate the performance of our automated tutorial generation framework, we designed a series of experiments to address three key Research Questions (RQs):

\begin{itemize}
    \item \textbf{RQ1:} How does ExeAgent perform compared to existing CUAs in terms of success rate?
    \item \textbf{RQ2:} Does our framework generate high-quality Excel tutorials? Do human evaluations and LLM-based evaluations yield consistent results? What are the quality differences among Excel tutorials generated by different LLMs? How do automatically generated tutorials compare with those authored by domain experts?
    \item \textbf{RQ3:} What are the costs (time, money, token, step) of automatically generating tutorials?
    % \cz{I feel RQ23 can be merged.}
\end{itemize}

\subsection{Experiment Setup} 

\subsubsection{Baseline and Backbone Models} 
To evaluate the performance of our framework, we apply it to three state-of-the-art LLMs: GPT-4.1~\cite{openai2025gpt41}, GPT-o3~\cite{openai2025o3o4} and GPT-5~\cite{openai2025gpt5}. Access to all models is provided via their respective official APIs. The specific model versions used are gpt-4.1-2025-04-14, gpt-o3-2025-04-16 and gpt-5-2025-08-07. We use the same LLM as the backbone model for both the ExeAgent and tutorial generation.

\subsubsection{Parameter Setting} 
For LLM model configurations, we set the temperature to 0.01, top\_p to 0.95, and the max\_tokens parameter to 4096 for each model, while other parameters are maintained at their default settings as specified in the corresponding API documentation~\cite{openai2024api}.


\subsection{RQ1: Success Rate and Performance Comparison}
\label{subsec:rq1,Success Rate and Performance Comparison}
We compare the success rate with state-of-arts GUI agents on the dataset in \autoref{subsec:dataset contruction}. As \autoref{tab:success rates comparison} shows, ExeAgent achieves a 39.58\% success rate across the benchmark, surpassing the best-performing baseline (UFO) by 8.47\%. Furthermore, compared to other CUAs, ExeAgent accomplishes tasks with fewer steps, thereby demonstrating higher efficiency. When using GPT-4.1 as the base model, ExeAgent completes tasks in an average of 4.57 steps, which is substantially lower than UFO (5.79 steps) and Operator (8.38 steps). 
% Furthermore, compared to other CUAs, ExeAgent completes tasks with fewer steps, showcasing its efficiency, whereas UFO and Operator tend to generate longer step sequences that are less effective. This difference arises because, when an operation fails, these frameworks often repeat the same action until it succeeds or alter the execution plan.
% In contrast, ExeAgent leverages APIs to perform operations with higher precision, avoiding ineffective steps and completing tasks with minimal actions.

\begin{table}[h!]
\centering
\caption{Comparison of success rates (\%) and step number across agents.}
\vspace{5pt}
\begin{tabular}{llccccc}
\toprule
Agent & Model & Search & Websites & In-App & All & \# Step\\
\midrule

UFO & GPT-4.1 & 30.00& 32.73& 28.26 & 31.11 &5.79\\
Operator & computer-use & 28.71& 24.83& 25.36& 26.62 & 8.38\\
% UFO\textsuperscript{2} & GPT-4.1 & 35.71 & 26.46& 32.61 & 31.17\\
ExeAgent & GPT-5 & 26.43 & 30.79 & 26.09 & 28.42 & 5.51\\
ExeAgent & GPT-o3 & 28.43 & 39.67 & 33.33 & 34.06 &5.72\\
ExeAgent & GPT-4.1 & \textbf{39.29} & \textbf{40.78} & \textbf{34.78} & \textbf{39.58} & \textbf{4.57}\\
\bottomrule
\end{tabular}
\label{tab:success rates comparison}
\vspace{-5pt}
\end{table}

These results underscore the effectiveness of ExeAgent in completing complex Excel-specific tasks through API-assisted execution. This approach allows accurate execution of Excel operations, reduces vulnerability caused by GUI variations, and significantly enhances the reliability of multi-step workflows. In comparison, the performance of baselines is primarily attributable to inconsistencies between planned and executed actions. For example, selecting the wrong control or performing unintended operations. Execution errors often stem from inaccurate visual reasoning, incorrect associations between GUI elements and actions, or erroneous inference by the LLM. Unlike tasks dominated by simple click-based operations on web pages or other desktop applications, Excel tasks involve precise selections, drag-and-drop actions, and other high-precision operations. The intricate and complex nature of Excel operations increases the possibilities of such errors. By integrating API calls in execution, ExeAgent can accurately execute specific actions via parameter passing, preventing errors during task execution.




Interestingly, although GPT-5 exhibits strong general capabilities, its success rate is not outstanding. We found that it's because GPT-5 tends to adopt an overly “comprehensive” approach when reasoning about a task, resulting in unnecessary operations, redundant execution steps, and an increased probability of errors and evaluation difficulty. For example, when selecting a table range, GPT-5 enforces explicit confirmation of the active spreadsheet, thereby introducing an additional step for sheet selection.


\subsection{RQ2: Quality of Generated Tutorials}


\subsubsection{Evaluation Metrics} 
% \cz{List them?}
\label{sec:eval_metrics}
% To comprehensively evaluate the quality of our generated Excel tutorials, we define evaluation metrics for documents and videos separately. 

To rigorously assess the quality of the generated Excel tutorials, we establish distinct evaluation metrics for tutorial documents and videos.
Building upon prior work ~\cite{zhong2021helpviz, liu2024having,morain2012yoututorial, swarts2012new, yousef2014drives} and tailoring it to the specific characteristics of our task, we established 11 evaluation metrics for the documents (\autoref{document_evaluation_metrics}) and 7 evaluation metrics for the videos (\autoref{video_evaluation_metrics}).

\begin{table}[t]
\centering
\caption{Evaluation metrics for document.}
\vspace{5pt}
\renewcommand{\arraystretch}{1.1} % 将行高设置为默认的1.05倍
\begin{tabularx}{\textwidth}{|l|X|}
\hline
\textbf{Metric} & \textbf{Description} \\
\hline
Clarity & Each step is described explicitly and unambiguously. \\
\hline
Conciseness & Redundant, repetitive, or irrelevant steps are avoided. \\
\hline
Correctness & All necessary steps are performed accurately and correctly. \\
\hline
Completeness & All essential operations are covered. \\
\hline
Sequential Order & Steps are presented in a logical and coherent sequence. \\
\hline
Text--Image Mapping & Text and images are accurately aligned. \\
\hline
Understandability & The content is clear, intuitive, and easy to follow. \\
\hline
Efficiency & The format enables faster task completion compared to alternatives. \\
\hline
Task Completion & Users can complete the intended tasks smoothly. \\
\hline
Satisfaction & Users are satisfied with the tutorial. \\
\hline
Preference & Users prefer this type of tutorial over other formats. \\
\hline
\end{tabularx}
\label{document_evaluation_metrics}
\end{table}

\begin{table}[t]
\centering
\caption{Evaluation metrics for video.}
\vspace{5pt}
\renewcommand{\arraystretch}{1.1}
\begin{tabularx}{\textwidth}{|l|X|} % 加竖线
\hline
\textbf{Metric} & \textbf{Description} \\
\hline
Usability & The operation process is clear, enabling users to follow it easily. \\
\hline
Correctness & All necessary steps are performed completely and accurately. \\
\hline
Interactivity & Key operations are highlighted, allowing users to easily notice them. \\
\hline
Design Quality & The video is well-structured and user-friendly. \\
\hline
Transferability & The methods can be effectively applied to similar tasks. \\
\hline
Comp \& Sat & Users can complete the task smoothly, and their overall satisfaction is high. \\
\hline
Eff \& Pref & The tutorial improves efficiency and is the preferred format for future learning. \\
\hline
\end{tabularx}
\label{video_evaluation_metrics}
\vspace{-8pt}
\end{table}



We design the evaluation metrics from both objective and subjective perspectives. On the objective side, the focus lies in assessing whether the tutorial description is expressed with clarity and accuracy~\cite{kay2014developing,yousef2014drives}, whether the tutorial content is complete and correct~\cite{yousef2014drives}, whether the structure is logically coherent~\cite{zhong2021helpviz,liu2024having}, and whether the alignment between text and visuals is consistent~\cite{brame2017effective,van2013eight}. These criteria ensure that the material provides reliable, understandable, and executable guidance for users. 
On the subjective side, the metrics emphasize user experience~\cite{sauro2016quantifying}, including content comprehensibility, execution efficiency, task completion, as well as user satisfaction and preference~\cite{zhong2021helpviz,liu2024having}. These criteria ensure that the tutorials can meet users’ practical needs and expectations. This dual design not only guarantees the scientific rigor and objectivity of the evaluation but also reflects the practical experiences of users.




\subsubsection{Human Evaluation}
\label{subsec:human_evaluation_details} 

% recruit volunteer xx， 有difference experience。 diversity比较重要。每个人打多少个样本，每个样本被几个人打分。选了哪些metric
\begin{table}[t]
\centering
\caption{Mean (variance) of human and LLM ratings across document metrics.}
\vspace{5pt}
\label{tab:doc_metrics_score}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{lcccccc}
\toprule
\textbf{Metric}&\multicolumn{2}{c}{\textbf{GPT-4.1}}&\multicolumn{2}{c}{\textbf{GPT-o3}}&\multicolumn{2}{c}{\textbf{GPT-5}}\\
\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
&\textbf{Human}&\textbf{LLM}&\textbf{Human}&\textbf{LLM}&\textbf{Human}&\textbf{LLM}\\
\midrule
Text-Image Mapping&4.75 (0.11)&4.98 (0.02)&4.73 (0.14)&4.98 (0.02)&4.63 (0.25)&4.98 (0.02)\\
Clarity&4.69 (0.28)&4.90 (0.21)&4.66 (0.32)&4.88 (0.23)&4.56 (0.32)&4.84 (0.25)\\
Completeness&4.68 (0.27)&4.98 (0.02)&4.59 (0.42)&4.96 (0.04)&4.73 (0.35)&4.92 (0.07)\\
Correctness&4.67 (0.31)&4.96 (0.08)&4.59 (0.43)&4.96 (0.08)&4.58 (0.47)&4.94 (0.10)\\
Sequential Order&4.63 (0.38)&4.92 (0.19)&4.62 (0.38)&4.92 (0.19)&4.76 (0.22)&4.90 (0.21)\\
Understandability&4.63 (0.23)&4.88 (0.36)&4.60 (0.25)&4.66 (0.34)&4.61 (0.26)&4.90 (0.21)\\
Conciseness&4.56 (0.49)&4.72 (0.64)&4.51 (0.57)&4.70 (0.69)&4.43 (0.61)&4.56 (0.89)\\
Completion&4.53 (0.46)&4.74 (0.71)&4.41 (0.70)&4.76 (0.42)&4.52 (0.71)&4.90 (0.25)\\
Task 
Efficiency&4.38 (0.41)&4.48 (0.88)&4.24 (0.51)&4.44 (0.65)&4.31 (0.52)&4.56 (0.61)\\
Satisfaction&4.29 (0.45)&4.58 (0.84)&4.12 (0.64)&4.56 (0.57)&4.32 (0.67)&4.80 (0.52)\\
Preference&4.24 (0.44)&4.58 (0.84)&4.09 (0.57)&4.46 (0.61)&4.27 (0.58)&4.62 (0.60)\\
\midrule
\textbf{Average}&4.55 (0.35)&4.79 (0.44)&4.47 (0.45)&4.75 (0.35)&4.52 (0.45)&4.81 (0.34)\\
\bottomrule
\end{tabular}
\end{table}



\begin{table}[t]
  \centering
  \caption{Mean (variance) of human and LLM ratings across video metrics.}
  \vspace{5pt}
\label{tab:video_metrics_score}
  \begin{tabular}{l c c c c c c}
  \toprule
  \textbf{Metric} & \multicolumn{2}{c}{\textbf{GPT-4.1}} & \multicolumn{2}{c}{\textbf{GPT-o3}} & \multicolumn{2}{c}{\textbf{GPT-5}} \\
  \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
   & \textbf{Human} & \textbf{LLM} & \textbf{Human} & \textbf{LLM} & \textbf{Human} & \textbf{LLM} \\
  \midrule
  Design Quality & 4.80 (0.11) & 4.40 (0.54) & 4.66 (0.14) & 4.28 (0.40) & 4.68 (0.18) & 4.44 (0.49) \\
  Interactivity & 4.80 (0.13) & 4.90 (0.19) & 4.62 (0.23) & 4.82 (0.23) & 4.64 (0.29) & 4.84 (0.21) \\
  Transferability & 4.70 (0.33) & 4.68 (0.33) & 4.51 (0.26) & 4.68 (0.26) & 4.55 (0.22) & 4.62 (0.40) \\

  Usability & 4.55 (0.40) & 4.74 (0.49) & 4.40 (0.52) & 4.82 (0.31) & 4.44 (0.48) & 4.82 (0.39) \\
  Comp \& Sat & 4.53 (0.76) & 4.74 (0.49) & 4.47 (0.41) & 4.86 (0.24) & 4.52 (0.49) & 4.86 (0.36) \\
  Correctness & 4.50 (0.40) & 4.90 (0.19) & 4.61 (0.27) & 4.94 (0.10) & 4.59 (0.30) & 4.92 (0.15) \\
  Eff \& Pref & 4.50 (0.78) & 4.70 (0.81) & 4.32 (0.41) & 4.82 (0.43) & 4.38 (0.50) & 4.78 (0.65) \\
 \midrule
 \textbf{Average} & 4.62 (0.42) & 4.73 (0.43) & 4.51 (0.32) & 4.75 (0.28) & 4.54 (0.35) & 4.75 (0.38) \\
 \bottomrule
 \end{tabular}
 \vspace{-8pt}
\end{table}

For the human evaluation, we recruited 10 people with different level of experience in Excel to independently assess the automatically generated tutorials. From the full set of generated tutorials, we selected a representative sample of 50 tasks. 
To ensure diversity among the selected cases, we ensure that the 50 tasks that covers all operation categories and target object categories by reject sampling. In total, we evaluate the Excel tutorial documents and videos generated by the three best-performing LLMs (GPT-4.1, GPT-o3, GPT-5), covering a total of 300 tutorials (150 tutorial documents and 150 videos).

We ensure each tutorial is rated by at least 2 evaluators independently. The result was calculated as the average of the two evaluators’ ratings. Participants were instructed to rate the assigned cases based on the criteria in ~\autoref{sec:eval_metrics}, using a 5-point Likert scale with explicit descriptions for each score. The average ratings for each case were used as the final evaluation result.

In addition, participants were asked to complete an open-ended questionnaire designed to capture  background information and subjective feedback, including: (1) basic demographic and background information, such as age, years of Excel usage, and self-reported proficiency level; (2) perceptions of the strengths and weaknesses of the automatically generated instructional documents and videos; (3) comparative evaluations of the automatically generated content against expert-authored materials. The responses to this questionnaire provide complementary qualitative insights into participants’ perceptions, thereby informing future improvements to the automatic content generation system.




The evaluation results are presented in \autoref{tab:doc_metrics_score} and \autoref{tab:video_metrics_score}. 
Overall, the average ratings for all metrics across the three base model settings exceeded 4. This suggests that participants gave positive evaluations for the quality of documents and video tutorials generated by different models across all 18 assessment metrics. In addition, the low variance indicates that participants’ ratings are consistently concentrated across all tutorials.

\textbf{Tutorial Document Rating Analysis.} Six metrics (Clarity, Completeness, Correctness, Sequential Order, Understandability, and Text-Image Mapping) got high average ratings, which exceeded 4.5 across all models. 
These results indicate that the generated document tutorials are accurate, well-structured, and complete. This further demonstrates that ExeAgent achieves high execution fidelity and produces correct and comprehensive operational sequences.
In addition, the strong performance on Understandability and Text-Image Mapping suggests that the tutorials are highly interpretable, featuring clear textual descriptions and precisely annotated images that effectively highlight key instructional steps.
Two additional metrics, Conciseness and Completion, also achieved average ratings above 4.5 for most models. However, for GPT-5, the Conciseness rating was slightly lower at 4.31, indicating that while the generated tutorials are generally concise, some redundancy remains. This observation aligns with the findings discussed in \autoref{subsec:rq1,Success Rate and Performance Comparison}. The redundancy may stem from ExeAgent (GPT-5) occasionally performing exploratory or repetitive actions during task execution, which can reduce conciseness and require users to exert additional effort to complete the tasks.
Scores for Task Efficiency, Satisfaction, and Preference ranged from 4.0 to 4.5 across all models. These results suggest that users were generally able to complete tasks efficiently and were satisfied with the tutorials, showing a preference for using them in most scenarios. Nonetheless, occasional redundancy and incorrect steps may have caused confusion for some participants.


\textbf{Tutorial Video Rating Analysis.} 
Four metrics (Design Quality, Interactivity, Transferability, and Correctness) achieved average ratings exceeding 4.5 across all models. These results suggest that the generated video tutorials were perceived by participants as accurate, engaging, well-structured, and broadly applicable. By integrating both API and GUI based approaches, ExeAgent ensures the correctness of operational sequences while enhancing user attention through visual cues, thereby improving interactivity. Furthermore, the video generation process emphasizes aesthetic presentation and the generalization of tutorial content.
Two additional metrics, Completion and Satisfaction, averaged around 4.5, indicating that most video tutorials effectively guided users through task completion and yielded a satisfactory experience. However, in some cases, insufficient clarity in step-level details may have required users to engage in self-reflection during execution.
Scores for Efficiency and Preference ranged between 4.0 and 4.5 across models, suggesting that the tutorials generally improved task efficiency and were favorably received by users. Nonetheless, compared to expert-produced tutorials, the generated videos may lack vividness and operational fluency due to occasional redundancy, which could negatively impact the overall user experience.

\subsubsection{LLM Evaluation} 
For the automated evaluation, we employed GPT-4.1 to rate the generated tutorials across all metrics using a 5-point Likert scale. The rating criteria for the LLM evaluation were identical to those used in the human evaluation. To facilitate comprehension by the model, each metric was accompanied by detailed explanations, and two concrete examples were provided to illustrate the expected ratings. For each metric, the model was instructed to output both the assigned score and a justification for that score.

The evaluation results are presented in \autoref{tab:doc_metrics_score} and \autoref{tab:video_metrics_score}. LLM ratings are predominantly above 4.5 for both documents and videos, indicating that LLMs express a high level of approval for the generated tutorials. 
Moreover, except for Design Quality, LLM ratings exceed the corresponding human ratings.
The relatively lower ratings in Design Quality may be attributed to the weakness of current LLMs to process audio information, which limits their capacity to evaluate the alignment between audio, textual, and visual elements.
Consequently, LLMs tend to assign a more conservative rating of 4 rather than 5 for this metric. 
Other metrics with relatively lower LLM ratings are concentrated in more subjective aspects, such as Task Efficiency, Satisfaction, and Preference. The reason might be compared with objective metrics, LLMs are less confident in evaluating subjective dimensions, resulting in a tendency to assign non-maximal ratings.



\subsubsection{Evaluation Summary}
Overall, the tutorials generated in this study received high ratings from both human assessors and LLM. The average rating from humans was approximately 4.5, while the mean LLM rating reached 4.7. This result indicates that the generated tutorials achieved a high standard in both objective accuracy and subjective user experience. Notably, both the average rating and the proportion of high ratings (see \autoref{fig:score_distribution}) from the LLM were higher than those from the human assessors. 
We hypothesize that this discrepancy may arise from the LLM's limitations in identifying subtle flaws within the content and in perceiving nuanced human subjective feedback, leading to a tendency to award higher ratings.

\begin{figure}[htbp]
    \vspace{-3pt}
    \centering
    \includegraphics[width=0.95\textwidth]{sections/figures/split_metrics_distribution.pdf}
    \vspace{-8pt}
    \caption{The distribution of human and LLM ratings on document metrics (left) and video metrics (right). The left bars represent human ratings, while the right bars represent LLM ratings.}
    \label{fig:score_distribution}
\vspace{-8pt}
\end{figure}

Furthermore, we investigate the correlations between LLM-based evaluations and human assessments using both Pearson and Kendall metrics, as shown in \autoref{tab:overall_correlation}. Overall, both document and video tutorials exhibit positive correlations. Notably, the correlations for documents are consistently higher than those for videos. This discrepancy may be attributed to the inherent limitations of current LLMs in comprehensively understanding multi-modal content—particularly tutorials that integrate textual descriptions, visual elements, and audio narration. As LLMs lack the capability to process audio and fully interpret visual context, their evaluations may not accurately reflect the holistic quality of video tutorials.


\begin{table}[h!] 
  \centering
  \caption{Correlation between human ratings and LLM ratings on document and video metrics.}
  \vspace{5pt}
  \label{tab:overall_correlation}
  \begin{tabular}{l c c c c c c}
  \toprule
  \textbf{Tutorial Type} & \multicolumn{2}{c}{\textbf{GPT-4.1}} & \multicolumn{2}{c}{\textbf{GPT-o3}} & \multicolumn{2}{c}{\textbf{GPT-5}} \\
  \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
   & \textbf{Pearson} & \textbf{Kendall} 
   & \textbf{Pearson} & \textbf{Kendall} 
   & \textbf{Pearson} & \textbf{Kendall} \\
  \midrule
  Document & 0.44 & 0.40 & 0.27 & 0.13 & 0.43 & 0.42 \\
  Video    & 0.21 & 0.21 & 0.17 & 0.11 & 0.29 & 0.21 \\
  \bottomrule
  \end{tabular}
  \vspace{-5pt}
\end{table}




\subsubsection{User Study on Expert-Authored Tutorials}
As described in \autoref{subsec:human_evaluation_details}, ten participants were asked to respond to four open-ended questions regarding the advantages and limitations of our tutorials compared to expert-authored ones.

\paragraph{Document Tutorials.}
Participants generally found our document tutorials to be more detailed and intuitive. They highlighted the step-by-step guidance supported by screenshots and annotated highlights as particularly helpful. For instance, Participant 1 (P1) noted that many expert-authored tutorials lacked visual illustrations, making them difficult to follow, whereas our tutorials consistently included images for each step. Participants P7 and P8 emphasized that our tutorials explained every step thoroughly, while expert-authored documents often assumed prior knowledge and used overly concise descriptions, which hindered sequential understanding. Additionally, P3 remarked that our tutorials provided explanatory content that not only supported task completion but also conveyed underlying principles of Excel, thereby enhancing knowledge transferability.

\paragraph{Video Tutorials.}
Participants appreciated the clearer segmentation of steps, moderate pacing, and the inclusion of subtitles in our video tutorials. P2 and P3 observed that many expert-authored videos were simple screen recordings without explicit step boundaries, whereas our videos followed a structured, step-by-step format. P9 noted that expert videos were often fast-paced and lacked subtitles, requiring frequent pauses for comprehension. In contrast, our videos maintained an appropriate pace and included subtitles, which facilitated both understanding and task replication.

\paragraph{Suggestions for Improvement.}
Participants also identified areas for enhancement. In addition to using highlights, they suggested incorporating magnification to better emphasize operational regions. Furthermore, they recommended enriching the tutorials with contextual background, such as application scenarios of the features, to improve content depth and relevance, drawing inspiration from expert-authored materials.


\subsubsection{Case Study}
We conduct a comparative analysis of online expert-authored tutorials and our automatically generated tutorials on the task “add the tool to the Quick Access Toolbar in Excel”. 

\begin{figure}[htbp]
    \vspace{-3pt}
    \centering
    \includegraphics[width=0.95\textwidth]{sections/figures/case_study_v4.pdf}
    \caption{Comparison of ours with an online tutorial for the task ‘Add the tool to the Quick Access Toolbar in Excel.’ Content highlighted in green and red rectangles corresponds to excerpts from the automatically generated tutorial document and the online tutorial. Circled numbers indicate the tutorial step order.}
    \label{fig:case_study}
    \vspace{-5pt}
\end{figure}

Our tutorials decompose the task into seven explicit atomic steps: 
(1) Open the quick access toolbar customization menu.
(2) Access more commands for customization.
(3) Open the quick access toolbar options.
(4) Switch to Commands Not in the Ribbon.
(5) Select the tool.
(6) Add the tool to the quick access toolbar.
(7) Confirm and apply the customization.
Each step is clearly delineated and accompanied by a step number, a title, a detailed description, a screenshot, and a highlighted marker indicating the exact operation location. In addition, both the initial and completed states of the file are provided as visual references. The tutorial video further integrates narration with introductory and concluding slides to enhance accessibility and comprehension. 

In contrast, expert-authored tutorials have several limitations (\autoref{fig:case_study}). Owing to the authors’ high familiarity with Excel, certain steps in exiting help documents are either omitted or merged. Furthermore, many documents lack visual aids and explicit highlighting of operation areas, creating comprehension difficulties for users. For example, Steps 1--2, 3--4, and 5--6 are often collapsed into a single step in expert-authored tutorials, while Step~7 is omitted entirely. In terms of illustrations, only a few steps are occasionally supported by a screenshot, while the remaining steps are left without visual guidance. Additionally, the highlighted markers for key operations are often omitted. For novice users without prior background knowledge, such omissions, merging of steps, and absence of annotated screenshots may result in confusion and hinder them from completing the task efficiently. Similarly, most expert-authored help videos are simple screen recordings that do not explicitly name or separate individual steps. The fast-paced presentation makes videos difficult for viewers to follow along effectively. Moreover, exiting videos typically lack subtitles or textual annotations, relying solely on narration, which reduces intuitiveness and limits accessibility.

% For manually authored video tutorials, most existing ones are presented as continuous screen recordings, lacking step-by-step segmentation and clear structural organization. 
% Moreover, such tutorials exhibit common deficiencies, including: (1) an absence of subtitles; (2) a lack of voice-over narration; and (3) a failure to highlight key operational areas on the screen.






\subsection{RQ3: Cost Analysis of Tutorial Generation}


As shown in \hyperref[tab:cost]{Table~\ref*{tab:cost}} and \autoref{fig:pipeline}, the three models exhibit differences in cost-efficiency when generating tutorials, primarily due to their underlying architectures and training paradigms. GPT-4.1, as a generative model, achieves the highest efficiency across all evaluated dimensions (step count, token usage, execution time, and monetary cost). This advantage stems from its optimization for direct instruction execution. For tasks such as Excel tutorial generation, which rely heavily on existing knowledge, GPT-4.1 can produce high-quality action sequences without redundant intermediate reasoning or reflective steps, thereby yielding the lowest overall cost. In contrast, GPT-o3, designed as a reasoning model, incurs the highest resource consumption. Its training paradigm requires the model to “think” explicitly before producing a final answer, generating multiple intermediate reasoning traces and performing reflective checks, which substantially increases cost. GPT-5 integrates both generative and reasoning paradigms, with cost metrics falling between those of GPT-4.1 and GPT-o3, reflecting a trade-off between generation efficiency and reasoning depth. Overall, for the specific application of Excel tutorial generation, GPT-4.1’s direct generation strategy offers clear advantages, making it the most cost-effective model.


Additionally, prior studies have emphasized the substantial cost of manually producing tutorials~\cite{kim2014crowdsourcing,hu2023smartrecorder}. To further quantify this cost, we asked three Excel experts to manually generate documentation and video tutorials while recording the time required. On average, manual tutorial creation required approximately 2.5 hours per task. In contrast, our automated approach completes the same process in only \textbf{1/20} of the time, yielding a dramatic reduction in human effort.

% This cost advantage directly translates into scalability. Whereas manual creation limits practitioners to producing only a small number of tutorials due to the prohibitive time investment, our automated pipeline enables the large-scale generation of hundreds or even thousands of tutorials with minimal additional overhead. Such scalability is critical for covering the wide variety of real-world Excel tasks, ensuring both breadth and timeliness in tutorial availability.
% The results demonstrate that the time for manual creation is above 2.5h, which is substantially higher than that of our automated generation method.



\begin{figure}[t]
\vspace{-6pt}
\centering
% ---- 左边表格 ----
\begin{minipage}{0.47\textwidth}
\centering
\captionof{table}{Tutorial Generation Cost}
\begin{tabular}{l c c c c}
\toprule
Model &\# Step & Token (k) & Money (\$) & time (s) \\
\midrule
GPT-4.1 & \textbf{4.57} & \textbf{132.15} & \textbf{0.28} & \textbf{270.58} \\
GPT-o3  & 5.72 & 183.24 & 2.00 & 431.15 \\
GPT-5   & 5.51 & 142.86 & 0.48 & 408.93 \\
% Human   & / & / & / & / \\
\bottomrule
\end{tabular}
\label{tab:cost}
\end{minipage}
\hfill
% ---- 右边图片 ----
\begin{minipage}{0.4\textwidth}
\centering
\includegraphics[width=\linewidth]{sections/figures/token_distribution_kde_plot_7.pdf} % 换成你的图
\caption{Token Cost Distribution}
\label{fig:pipeline}
\end{minipage}
\vspace{-5pt}
\end{figure}






% \begin{table}[h!]
% \centering
% \caption{Tutorial Generation Cost}
% \begin{tabular}{l l c c c}
% \toprule
% Model & Step & Token (k) & Money (\$) & time (s) \\
% \midrule
% GPT-4.1 & 4.57 & 132.15 & 0.28 & 270.58 \\
% GPT-o3  & 5.72 & 183.24 & 2.00 & 431.15 \\
% GPT-5  & 5.51 & 142.86 & 0.48 & 408.93 \\
% \bottomrule
% \end{tabular}
% \label{tab:cost}
% \end{table}






% \begin{table}[h!]
% \centering
% \begin{minipage}{0.47\textwidth}
% \centering
% \caption{Tutorial Generation Cost}
% \begin{tabular}{l l c c c}
% \toprule
% Model & Step & Token (k) & Money (\$) & time (s) \\
% \midrule
% GPT-4.1 & 4.57 & 132.15 & 0.28 & 270.58 \\
% GPT-o3  & 5.72 & 183.24 & 2.00 & 431.15 \\
% GPT-5   & 5.51 & 142.86 & 0.48 & 408.93 \\
% \bottomrule
% \end{tabular}
% \label{tab:cost}
% \end{minipage}
% \hfill
% \begin{minipage}{0.42\textwidth}
% \centering
% \includegraphics[width=\linewidth]{sections/figures/token_distribution_kde_plot_3.pdf}
% \caption{Token Cost Distribution}
% \label{fig:pipeline}
% \end{minipage}
% \end{table}
