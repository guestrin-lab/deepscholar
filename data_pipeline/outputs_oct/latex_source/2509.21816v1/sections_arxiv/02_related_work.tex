\section{Related Work}
\subsection{Spreadsheet Agent}

With the rapid advancement of large language models (LLMs), recent spreadsheet agents have increasingly leveraged LLM capabilities to address complex tasks involving spreadsheet data. SheetCopilot \cite{li2023sheetcopilot} enables stable interaction between LLMs and spreadsheets by defining atomic actions within a state machine-based task planning framework. SheetAgent \cite{chen2025sheetagent} introduces a modular Planner–Informer–Retriever architecture that effectively addresses long-horizon spreadsheet tasks through iterative reasoning. SheetMind \cite{zhu2025sheetmind} builds a multi-agent system with clearly defined roles, enhancing the robustness and reliability of task execution. Although existing spreadsheet agents have made progress in automating table-level tasks through code execution, their operations remain opaque to end users and fail to provide transparent, interface-based learning paths.

\subsection{Computer-Using Agents (CUAs)}
Recent advances in desktop automation have produced a range of LLM-powered agent systems. UFO \cite{zhang2024ufo} represents one of the earliest multi-agent automation frameworks for Windows, emphasizing GUI interaction through the integration of UI Automation and visual perception. NAVI \cite{bonatti2024windows}, a single-agent baseline from WAA, leverages both screenshots and accessibility metadata to facilitate GUI understanding. OmniAgent \cite{lu2024omniparser} combines OmniParser for visual grounding with GPT-based action planning, enabling robust multimodal reasoning. Agent S \cite{agashe2024agent} employs a multi-agent architecture with experience-driven hierarchical planning, optimized for executing complex, multi-step tasks. Operator \cite{openai2025computer}, a recent high-performance CUA from OpenAI, simulates human-like mouse and keyboard operations based on screenshot inputs. While these systems demonstrate the promise of LLM-driven desktop agents, they often suffer from shallow OS integration and heavy reliance on brittle visual inputs. 

% UFO\^2 \cite{zhang2025ufo2} advances this area by introducing a system-level framework with specialized agents, a hybrid GUI–API action layer, and deep OS awareness, thereby improving robustness and accuracy in task execution across diverse applications. Notably, UFO2 achieves state-of-the-art performance on multiple benchmark evaluations.




\subsection{Automated Tutorial Generation}
Prior work has explored semi-automated tutorial generation using pre-existing materials such as user demonstrations and instructional videos. For instance, MixT \cite{chi2012mixt} produces mixed-media tutorials by combining static instructions with video segments derived from user demonstrations.
In a similar vein, an approach~\cite{truong2021automatic} has been proposed to automatically construct hierarchical tutorials from makeup instructional videos.
Other research \cite{grabler2009generating} captures software operations along with screencast recordings and converts them into document-style tutorials enriched with text descriptions and annotated step images. In the domain of physical tasks, some approaches \cite{denning2011meshflow,grossman2010chronicle} produce  instructional videos through semi-automatic editing of creator-annotated single-take or multi-take demonstrations. 
% \lu{foot note format seems not correct}


In addition to leveraging demonstrations and videos, several studies have explored transforming static textual content (such as user manuals) into richer, multimedia-based tutorials.
HelpViz \cite{zhong2021helpviz} automatically converts text-based mobile instructions into contextual visual tutorials by parsing action sequences, simulating interactions on Android emulators, and synthesizing step-by-step visual assets, leading to higher user preference over plain text.
Other systems such as M2V \cite{liu2024having} and HowToCut \cite{chi2021automatic} focus on generating instructional videos from manuals or markdown tutorials, combining NLP, computer vision, and automatic video editing to produce engaging, easy-to-follow guidance preferred by most users over traditional documentation.

% Unlike these methods, our approach does not rely on pre-existing instructional steps or external resources such as text, images, or videos. Instead, it directly generates the corresponding tutorial in an end-to-end manner based solely on a given query, thereby achieving a higher degree of automation.


% \section{Background and Motivation}
% %%%%%%%%%%%%%%%%%%%%%%%
% % \fix{finish}\cz{This should define what is tutorial/video or give an example, and how they are generated by human previous.}

% In this work, we produce tutorials in two formats: textual documents and demonstration videos. 

% \subsection{Tutorial Formats}
% Textual tutorials present a structured sequence of steps, each accompanied by textual instructions and, in some cases, illustrative images~\cite{zhong2021helpviz}. This format allows learners to quickly scan procedures and reference specific steps efficiently. Moreover, textual tutorials can be delivered in various formats, such as HTML or Markdown, which can be tailored to different instructional contexts and user preferences. By providing clear, step-by-step guidance in a compact and accessible manner, textual tutorials facilitate rapid comprehension and flexible navigation through complex tasks.

% Demonstration videos, typically produced by professionals, consist of screen recordings synchronized with audio narration~\cite{torrey2007pages, tuncer2020pause}. These videos provide a dynamic presentation of individual operations while conveying a holistic view of the task workflow~\cite{chi2012mixt, truong2021automatic, zhu2021gif}. The inclusion of voice narration further benefits learners who prefer auditory guidance alongside visual observation~\cite{tuncer2020pause}. Consequently, instructional videos serve as an immersive and intuitive medium for procedural knowledge transfer, effectively complementing textual tutorials.


% \subsection{Problem Definition}

% This work addresses the task-to-tutorial problem. The input is a natural language description of an Excel task (e.g., “create a bar chart” or “apply the VLOOKUP function”). The goal is to automatically generate a tutorial set that effectively guides users in successfully accomplishing the task. The final output comprises two formats: a tutorial document and a video tutorial. Each tutorial is organized as an ordered sequence of operational steps. Each step includes a step index, a step title, natural language description of the actions, and any associated materials, such as images and audio. Structuring tutorials in this manner provides users with clear, step-by-step guidance that integrates textual instructions with visual demonstrations, thereby facilitating efficient and accurate task completion in Excel.
% Unlike automated tutorial generation methods that rely on existing solutions, this task only takes a task description as input and requires the pipeline to independently plan and execute a solution in order to obtain all the actions in textual, visual formats necessary for tutorial construction. The objective of this task is to perform end-to-end tutorial generation, thereby maximizing savings in time and resources while building a more comprehensive tutorial library at low cost.

\section{Task Definition}

In this work, we focus on the automatic generation of tutorials for Excel tasks, producing outputs in two complementary formats: textual documents and demonstration videos. We first introduce the characteristics of these tutorial formats, then formally define the problem setting.

\subsection{Tutorial Formats}

Textual tutorials present a structured sequence of steps, each accompanied by natural language instructions and, in some cases, illustrative images~\cite{zhong2021helpviz}. This format allows learners to quickly scan procedures and reference specific steps efficiently. Moreover, textual tutorials can be delivered in various formats, such as HTML or Markdown, which can be adapted to different instructional contexts and user preferences. By providing clear, step-by-step guidance in a compact and accessible manner, textual tutorials facilitate rapid comprehension and flexible navigation through complex tasks.

Demonstration videos, typically produced by professionals, consist of screen recordings synchronized with audio narration~\cite{torrey2007pages,tuncer2020pause}. These videos provide a dynamic presentation of individual operations while conveying a holistic view of the task workflow~\cite{chi2012mixt,truong2021automatic,zhu2021gif}. The inclusion of voice narration further benefits learners who prefer auditory guidance alongside visual observation~\cite{tuncer2020pause}. Consequently, instructional videos serve as an immersive and intuitive medium for procedural knowledge transfer, effectively complementing textual tutorials.

\subsection{Problem Definition}

We define the \emph{task-to-tutorial problem} as the process of transforming a natural language task description into a complete instructional tutorial for Excel. 
The \emph{input} is a plain-language description of a task (e.g., “create a bar chart” or “apply the VLOOKUP function”). 
The \emph{output} is a tutorial package that contains both a textual document and a video demonstration. 

Each tutorial is structured as an ordered sequence of steps. 
Every step includes four essential components:  
\begin{itemize}
    \item \emph{Step index} – indicates the position of the step in the procedure.  
    \item \emph{Step title} – a concise phrase summarizing the action.  
    \item \emph{Step description} – a natural language explanation of the operation in context.  
    \item \emph{Supporting materials} – artifacts such as screenshots or audio narration.  
\end{itemize}
This design ensures that tutorials are systematic, pedagogically clear, and easy to follow.  

A key distinction of this problem is that the system cannot rely on pre-existing expert demonstrations or manually curated operation sequences. 
Instead, it must \emph{autonomously plan and execute} the given task in Excel, capture the execution trace, and convert this trace into user-facing instructional materials. 
This end-to-end formulation is more challenging than semi-automated approaches because it integrates both \emph{task completion} and \emph{tutorial construction}.  

The overarching objective is to generate tutorials that are operationally accurate, instructionally effective, and scalable, while dramatically reducing the manual effort traditionally required for tutorial authoring.

