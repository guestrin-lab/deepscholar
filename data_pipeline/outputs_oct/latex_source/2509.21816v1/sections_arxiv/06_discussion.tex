\section{Discussion}
\subsection{Benefits and Limitations}
% In this section, we discuss several potential benefits and limitations of our framework. 

\textbf{ Benefits.} Existing approaches to automated tutorial generation often rely on predefined solutions, such as materials derived from manual demonstrations. In contrast, our approach directly generates executable solutions from task descriptions, enabling true end-to-end tutorial generation. This design significantly reduces manual effort, cutting the time cost to just 1/20 of the original, thereby achieving a higher degree of automation and production efficiency. Our approach not only achieves low costs but also produces tutorials that are of equal or higher quality than those created manually. This is made possible by the integration of our ExeAgent GUI and API, which ensures the generation of accurate solutions. Furthermore, the tutorial generation process adheres to principles such as stepwise organization and aesthetic design, resulting in a superior user experience.

\textbf{Limitations.} Complex tasks remain a challenge. Such tasks often involve fine-grained operations and multi-step reasoning, where our system may fail to produce the most concise or optimal solutions. Future work will explore strategies such as task decomposition to enhance robustness and efficiency in complex scenarios.

\subsection{Threats to Validity}
The first threat concerns the generalizability of our experimental results. To mitigate this issue, we exercised control in both model selection and data sources. Specifically, we carefully selected three representative state-of-the-art LLMs (GPT-4.1, GPT-5, and GPT-o3) as our base models. In addition, our evaluation queries were drawn from three different scenarios from real-world, ensuring authenticity and diversity in task types. Based on this foundation, we applied these models to our automated tutorial creation framework and systematically assessed their performance across different query platforms, thereby enhancing the generalizability of our findings.

The second threat relates to the completeness and reliability of evaluation metrics. To address this concern, we referenced established dimensions used in prior work on automated tutorial generation, and further extended the metric system to provide a more comprehensive characterization of system performance. Beyond relatively objective measures such as Correctness and Clarity, we also incorporated subjective criteria such as Satisfaction and Preference. This combination ensures that our results capture not only the technical quality of the generated tutorials but also their perceived value from the userâ€™s perspective.

The third threat involves the representativeness and accuracy of human evaluations. Since manual assessments were conducted on a relatively limited set of tasks, the representativeness of the results may be affected. This limitation primarily arises from evaluator time and coordination costs. To mitigate this risk, we ensured diversity in task selection, including operations and target objects. For evaluation accuracy, we recruited assessors with at least three years of Excel experience, and assigned two independent evaluators to each task to provide parallel ratings, reducing potential biases from individual variation. Through these measures, we sought to maximize the reliability of human evaluation and the robustness of our conclusions.