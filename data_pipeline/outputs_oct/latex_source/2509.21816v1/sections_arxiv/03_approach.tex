
\section{Approach}
% done: add datasets here.
% \fix{Dataset collection is described at the beginning of the paragraph, as it is not part of the tutorial generation process. In the task definition, the task itself serves as the input; therefore, task collection should not be considered part of the generation workflow?}\cz{Why dataset is not in the stage?}

% In this section, we introduce data collection process and the overall pipeline for Excel tutorial generation as \autoref{fig:overview}.


\begin{figure}[htbp]      
  \centering              
  \includegraphics[width=0.95\textwidth]{sections/figures/overview_pic_v4.pdf} 
  \vspace{-8pt}
  \caption{Automated Workflow for Excel Task Execution and Tutorial Generation.}  
  \label{fig:overview}   
  % \vspace{-8pt}
\end{figure}

We propose \emph{an automated GUI framework} for Excel tutorial generation that addresses the \emph{task-to-tutorial} problem, as illustrated in \autoref{fig:overview}. 
This framework enables the creation of both instructional documents and video tutorials directly from natural language task descriptions. 
It is supported by a curated dataset of 1,559 real-world Excel tasks, which provides the foundation for training and evaluation. 
The framework itself consists of three stages. 
\emph{Stage 1: Task Instantiation} transforms a raw task description into an executable Excel task by aligning it with appropriate templates and contextual structures. 
\emph{Stage 2: Automatic Trajectory Collection} employs the Execution Agent to plan and execute the task while recording a fine-grained sequence of user-like actions (e.g., selecting ranges, navigating menus, applying formulas). 
\emph{Stage 3: Tutorial Document and Video Generation} converts the collected trajectory into user-facing materials by filtering essential steps, enhancing visual clarity, and preparing step-level descriptions. 
The final outputs are both structured documents and narrated video tutorials, offering complementary modalities for effective learning.
In the following subsections, we first introduce the dataset construction process and then describe each of the three stages in detail.
 
% (1) \textbf{Task Instantiation} (\autoref{sec:task_instantiation}), aiming at generating executable task instances through template matching and task refinement;
% (2) \textbf{Solution Planning and Execution} (\autoref{sec:solution-planning-execution}), focused on planning and executing the operation sequence for the task via an Excel task executor; 
% (3) \textbf{Tutorial Document and Video Generation} (\autoref{sec:tutorial-generation}), which generates structured tutorial components based on operation step sequences and synthesizes them into video and document tutorials. 
% Below, we provide detailed information for each stage of the EGA.




\subsection{Dataset Construction} 
\label{subsec:dataset contruction}
%%chaoyun:我们需要什么样的query，是true query，不是生成的。还有高频但是目前的dataset不是很满足。所以这motivate我们collect新的q

Existing datasets for Excel are derived from  either manually crafted examples ~\cite{payan2023instructexcel} or Excel exam datasets~\cite{chen2025sheetagent}. These tasks are artificially generated and intended for examination testing. As such, they do not correspond to authentic user behavior patterns.
Other datasets collect queries from Excel forums where users seek help online ~\cite{li2023sheetcopilot, ma2024spreadsheetbench}. These queries are formulated under constrained and highly specific conditions, thereby limiting their generalization to broader user scenarios.
Moreover, these existing datasets ~\cite{payan2023instructexcel, chen2025sheetagent, li2023sheetcopilot, ma2024spreadsheetbench} focus exclusively on spreadsheet-level operations and omit those at the Excel application level. We define Excel-level tasks as operations that target the Excel application itself (e.g., adding the Camera tool, opening the navigation pane), rather than the specific spreadsheet content. In contrast, spreadsheet-level tasks directly affect the content, formatting, structure, or other elements within a particular spreadsheet (e.g., inserting a new row, merging cells).
% \fix{finish}\cz{Like what? These dataset are also not realistic. We need to first highlight that need to collect real-world and representative data for tutorial making but current dataset may not satisfied. Highlight they are real-world}
To obtain real-world Excel tasks that are pedagogically meaningful, broadly transferable, and diverse in scope, we collect task descriptions from three primary sources:
\begin{itemize}
    \item \textbf{Search Queries (Search)}: We extract task descriptions from user-issued search engine queries related to Excel operations. These queries capture real user needs and represent common challenges encountered in practice.
    \item \textbf{Community Websites (Websites)}: We crawl community-driven websites, forums, and technical Q\&A platforms to obtain task descriptions. Such sources provide a wide range of authentic and diverse real-world tasks.
    \item \textbf{In-App Help Content (In-APP)}: We mine task descriptions from Excel’s built-in help documentation and tutorials, which cover a broad set of standardized operations.
\end{itemize}
The diverse composition of sources ensures comprehensive coverage of practical user needs, authentic interaction scenarios, and standardized documentation practices. After filtering the collected data for semantic ambiguity using GPT-5 and removing duplicates based on embedding similarity, we obtained the original task set containing 1,559 entries. 

\begin{table}[t]
\centering
\caption{Dataset Statistic.}
\vspace{5pt}
\label{tab:dataset}
\begin{tabular}{lccc}
\toprule
\textbf{Dataset} & \textbf{Count} & \textbf{Spreadsheet-Level (\%)} & \textbf{Excel-Level (\%)} \\
\midrule
 Search & 700 & 87.71 & 12.29 \\
Websites & 721 & 75.59 & 24.41 \\
In-App & 138 & 82.61 & 17.39 \\
\midrule
All & 1559 & 81.65 & 18.35 \\
\bottomrule
\end{tabular}
\vspace{-5pt}
\end{table}


\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\linewidth]{sections/figures/operation_use.pdf}
        \caption{Hierarchical distribution of operations.}
        \label{fig:type_distribution}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\linewidth]{sections/figures/object_use.pdf}
        \caption{Hierarchical distribution of objects.}
        \label{fig:object_distribution}
    \end{subfigure}
    \caption{The distribution of operation categories and target object categories in the dataset.}
    \label{fig:type_object_distribution}
    \vspace{-8pt}
\end{figure}










% mugeng : 容易被argue
% \cz{Interpret the results}
To further investigate the coverage and representative of real-world conditions, we analyze the involving objects (e.g. charts, tables) and operations (e.g. add, delete, move) in our dataset. Specifically, we employed GPT-5 to extract the objects and operations in each task, and further identify the task is about Spreadsheet-level or Excel-level (see ~\autoref{tab:dataset}). The classification results (see ~\autoref{fig:type_object_distribution}) show that the dataset comprises 28 operation categories and 17 target object categories. Our dataset exhibits substantial diversity, with many distinct combinations of operation categories and target object categories, which is attributable to its broad and authentic data sources.


% \begin{table}[t]
% \centering
% \caption{Definitions and examples of two task levels. \cz{Do we need a table?}}
% \begin{tabular}{|p{2cm}|p{7cm}|p{4cm}|}
% \hline
% \textbf{Task level} & \textbf{Definition} & \textbf{Example} \\
% \hline
% Excel-level & Operations related to the Excel application itself, rather than the contents of a specific spreadsheet. & Add the Excel camera tool; Open the navigation pane \\
% \hline
% Spreadsheet-level & Operations that directly affect the content, formatting, structure, or other elements of a spreadsheet. & Add a sheet; Merge cells \\
% \hline
% \end{tabular}
% \label{tab:instruction_levels}
% \end{table}






% \begin{table}[t]
% \caption{Operations and Categories of Excel Tasks \cz{Do we need a table?}}
% \label{tab:excel_task_type}
% \centering
% \begin{tabularx}{\textwidth}{|l|X|}
% \hline
% \multicolumn{1}{|l|}{\textbf{Category}} & \multicolumn{1}{l|}{\textbf{Operation}} \\ \hline
% calculation & calculate, count, compare, categorize \\ \hline
% content editing & create, add, remove, fill, replace, modify, copy, paste, merge, unmerge, display, hide, unhide, freeze, expand \\ \hline
% data handling & sort, filter, select, access, find, format, validate \\ \hline
% file operations & save, open \\ \hline
% % State Operations & enable, disable \\ \hline
% \end{tabularx}
% \end{table}


% \begin{table}[t]
% \caption{Objects and Categories of Excel Tasks \cz{Do we need a table?}}
% \label{tab:excel_task_object}
% \centering
% \begin{tabularx}{\textwidth}{|l|X|}
% \hline
% \multicolumn{1}{|l|}{\textbf{Category}} & \multicolumn{1}{|l|}{\textbf{Object}} \\ \hline
% table \& data & cell, row, column, range, table, worksheet, external data, hyperlink \\ \hline
% visualization & chart, shape, symbol, layout, view, format, button \\ \hline
% calculate & VBA, formula \\ \hline
% \end{tabularx}
% \end{table}




\subsection{Stage 1: Task Instantiation}
\label{sec:task_instantiation}

% ambiguous，加一个图或者加一个例子。like 加一个symbol，需要做一个实例化。
% Template match 加一个图
% \cz{This section needs a figure, like Figure 4 in \url{https://arxiv.org/pdf/2412.10047}}

To produce Excel tutorials, we need real Excel environments to perform. 
In \autoref{subsec:dataset contruction}, real queries are collected without the matching Excel files. Furthermore, raw real-world queries are sometimes ambiguous. In this stage, we ensure that the task is clear and feasible. For example, the original collected query ``add a title'' is ambiguous, because it is unclear whether where the title should be added. In contrast, the instruction ``add the title `AI Agent' to the table in range A1:G3'' constitutes an actionable and clear task.
Therefore, we synthesize instantiated tasks from the original tasks through the following three-stage process.
% : create template set, select suitable template, and optimize task description.

% \fix{finish}\cz{Need to mention that these queries are prototyping (give an example) and not grounded in an environment.}

\textbf{Template Set Creation.} 
To provide an execution environment for each task, we constructed a template set consisting of sufficient Excel files with diverse domains and compositions. Each template includes an Excel file, screenshots, and a description of the file content and data structure.

\textbf{Template Matching.}  
The execution of an Excel task requires a compatible Excel environment. For example, task ``add a title for the chart''. should be executed on the Excel with charts. In detail, we leverage LLMs to analyze the essential elements for each task and automatically match with appropriate Excel file. 

\textbf{ Executable Task Construction.}  
In this step, we rewrite the ambiguous queries into clear, executable instructions aligned with the matched Excel template.
For example, a vague request like ``sort the data'' is rewritten as ``sort the data by age in ascending order''. We use LLMs to identify the specific objects in the template to which the operation applies and rewrites the instructions to be fully concrete.




Given the instantiated queries, the action plans, execution trajectories, and the execution results are produced in this stage. Traditionally, this process requires human effort for task execution and log collection, which is costly and challenging to scale-up \cite{wang2024large}.

To address this challenge, we design a specialized execution agent, ExeAgent, to automatically complete Excel queries in batch (\autoref{fig:data_collection}). Before execution, ExeAgent prepares the environment and performs  the task execution. The complete execution trajectory is recorded and subsequently evaluated by an LLM-based evaluator, which automatically determines task completion. Only successfully completed trajectories are retained for tutorial generation. After execution, ExeAgent closes the template files to reset the environment and prepare for the next task. Notably, the entire workflow is \emph{fully automated} and requires no human intervention.

\subsection{Stage 2: Automatic Trajectory Collection}
\label{sec:solution-planning-execution}

\begin{figure}[t]      
  \centering              
  \includegraphics[width=0.95\textwidth]{sections/figures/data_collection.pdf}
  \caption{An overview of the trajectories collection workflow with ExeAgent.}  
  \label{fig:data_collection}   
  \vspace{-5pt}
\end{figure}

\subsubsection{Executor Design in ExeAgent}
The core component of ExeAgent is its \emph{executor}, as the executor’s success rate and the quality of its execution traces directly determine the amount and usefulness of the generated tutorials. The executor is designed as a GUI-based agent because such agents mimic natural human interactions with the interface through mouse clicks and keyboard inputs \cite{zhang2024large}. This design ensures that the collected trajectories are intuitive and closely aligned with real user behavior, which makes them particularly suitable for tutorial generation.

However, existing pure-visual GUI-based Computer-Using Agents (CUAs) (e.g., \cite{anthropic2024, cua2025, zheng2025vem, wu2025gui}) are inadequate for handling complex Excel tasks for two reasons. First, Excel interfaces are highly intricate, with hundreds of cells arranged in diverse layouts and embedded diagrams. Relying solely on screenshot-based visual grounding is error-prone and often results in failed executions. Second, CUAs usually represent actions as screen coordinates $(x, y)$, which lack semantic meaning, making the trajectory difficult to interpret and unsuitable for generating tutorials. These limitations motivate the design of a specialized executor tailored to Excel automation and tutorial generation.

\subsubsection{Accessibility-Augmented Perception.}
To address the challenges of screenshot-only grounding, ExeAgent integrates accessibility information into its perception pipeline. Accessibility metadata provide a structured list of actionable GUI elements, each annotated with its name, type, and bounding box. For example: \{"name": "File Menu", "type": "MenuItem", "bbox": [10, 20, 80, 40]\}.

% \begin{verbatim}
% {"name": "File Menu", "type": "MenuItem", "bbox": [10, 20, 80, 40]}
% \end{verbatim}

Such information is overlaid on screenshots using the Set-of-Marks (SoM) mechanism \cite{yang2023set}, thereby enriching visual perception with semantic context. This augmentation improves execution in two ways: (1) it reduces reliance on brittle screenshot understanding, particularly in Excel’s cell-dense interface, and (2) it enables semantically meaningful action representation (e.g., ``click on \texttt{File Menu}’’) instead of opaque coordinate-based actions. As a result, the trajectories become both more robust and more interpretable for tutorial generation.




\subsubsection{Hybrid GUI–API Execution.}
Although GUI actions such as mouse clicks and keystrokes resemble human behavior, they remain error-prone and may reduce execution success rates \cite{zhang2025api}. To mitigate this, ExeAgent employs a hybrid execution model that combines GUI actions with dedicated Excel APIs (see~\autoref{tab:excel_functions}). API calls leverage well-defined programmatic endpoints to perform operations precisely and efficiently, substantially reducing execution errors. In this design, GUI actions handle generic interactions not covered by APIs, while complex or fragile operations are offloaded to the API layer. This hybrid approach ensures both generality and robustness in task execution.

\begin{table}[t]
\centering
\caption{Overview of Excel-related APIs incorporated in the ExeAgent.}
\vspace{5pt}
\renewcommand{\arraystretch}{1.1}
\begin{tabularx}{\textwidth}{l|X}
\hline
\textbf{API Name} & \textbf{Description} \\
\hline
table2markdown & Converts a table from a given Excel sheet into markdown format. \\
\hline
insert\_excel\_table & Inserts a set of data into a specified location in an Excel sheet as a table. \\
\hline
select\_table\_range & Select a specified range of cells in an Excel sheet. \\
\hline
set\_cell\_value & Sets the value or formula of a given cell in an Excel sheet. \\
\hline
auto\_fill & Automatically fills a range of cells based on recognized patterns. \\
\hline
reorder\_columns & Reorders the columns of an Excel sheet according to a specified order. \\
\hline
\end{tabularx}
\label{tab:excel_functions}
\vspace{-5pt}
\end{table}


\subsubsection{Execution Workflow.}
With accessibility-enhanced perception and a hybrid action set, ExeAgent follows a reactive execution framework. Upon receiving a query, it processes the screenshots and accessibility metadata, reasons through the task using a Chain-of-Thought approach~\cite{wei2022chain, dingeverything}, and selects the most appropriate action iteratively until it determines the task is complete. During execution, ExeAgent systematically records multi-modal artifacts for tutorial generation, organizing them into a step-by-step trajectory. Each step in the trajectory includes: (1) a screenshot of the interface, (2) the intermediate reasoning trace, (3) the concrete action executed and its result, (4) the interaction location or targeted GUI element. These rich multi-modal artifacts provide sufficient detail to generate high-quality, human-friendly tutorials.

\subsubsection{Automatic Evaluation}

Finally, before generating tutorials, it is necessary to filter out unsuccessful trajectories. Traditional approaches—such as using hand-crafted scripts to generate oracles or relying on human evaluators, are both costly and difficult to scale. To address this challenge, we adopt an \emph{LLM-as-a-judge} approach, leveraging GPT-4.1 as the evaluator. The evaluator consumes the multi-modal execution trajectory as input and automatically determines whether the task has been successfully completed. Only trajectories deemed successful are retained for tutorial generation. A preliminary experiment on 76 samples demonstrates that the LLM-based evaluator achieves an agreement rate of 86.8\% with human evaluation, indicating that it is both reliable and scalable.



% \cz{Please connect with the prior section, and always talk about the motivation first.}
% Current CUAs leverage the capabilities of vision-language models (VLMs) to identify and select available graphical user interface (GUI) controls within applications to accomplish tasks. 
% observe that existing CUAs often exhibit deviations when performing some Excel-specific operations, resulting in a mismatch between planned and actual actions, and consequently failing to complete tasks correctly. For example, current CUAs frequently fail to accurately click the bottom-right corner of a cell, causing all auto-fill tasks to fail.

% \cz{This deserves more words and a figure, since it is the core of the framework}
% To address the inherent limitations of GUI-based operations, we create a set of dedicated APIs (as shown in \autoref{tab:excel_functions}) to improve execution accuracy \cz{why these APIs? Mention these are hard to do by UI but can be easily achieved by API}. Compared with GUI operations, API calls rely on well-defined programmatic endpoints, offering higher reliability and efficiency. By providing appropriate parameters for specific operations, these APIs can execute the corresponding actions precisely and reliably in a single call, substantially reducing errors due to execution failures and enhancing the overall accuracy and robustness of Excel task completion.

% To leverage both the generality of GUI operations and the reliability of API execution, we design a unified GUI–API execution model -- Excel task executor. Upon receiving an executable task, the Excel task executor analyzes it to generate a sequence of steps and flexibly chooses between GUI and API operations during execution. Routine operations are performed via the GUI to take advantage of its ability to manipulate all visible UI elements and support diverse types of operations, whereas fine-grained operations that cannot be precisely executed through the GUI are handled via the corresponding APIs. By combining the broad accessibility of GUI operations with the high efficiency and reliability of API calls, this approach effectively prevents execution errors and improves the task completion success rate.

% % During the planning and execution process, we systematically collect the multimodal resources required for tutorial generation. , including the reasoning process, concrete operations, operation locations, screenshots captured before and after execution, and so on.
% During the planning and execution stages, we systematically collect the multimodal artifacts required for tutorial generation and organize them into a step sequence log. The log records: (1) an initial-state screenshot prior to task execution; (2) a final-state screenshot upon task completion; (3) the step-wise reasoning trace; (4) the concrete action executed at each step and its action type; (5) the interaction location for each action; and (6) a screenshot captured when each step completed. \fix{finish}\cz{What are they?}







% \textbf{Operation Sequence Evaluation.} We adopt a dual-assessment approach based on both the execution process and the completion of subtasks to determine whether an agent has successfully completed a task. The LLM first thoroughly interprets the original task requirements and then analyzes the intention behind each step in the execution trajectory, including the APIs or UI controls used and their parameters. The criteria include that the target control or API is successfully selected or invoked, the results meet expectations, and no unintended or erroneous actions occur. Additionally, tasks are decomposed into fine-grained subtasks, each independently scored as yes/no/unsure, and the overall task is only considered complete if all subtasks are explicitly verified and the final state meets the requirements.The LLM examines the before-and-after screenshots of each subtask to verify its completion. This method ensures the accuracy and reliability of Excel task completion assessment.






\subsection{Stage 3: Tutorial Document and Video Generation}
\label{sec:tutorial-generation}
% \subsubsection{Tutorial Design}
% \cz{Too abstractive, need a figure or example, or move it, or merge later.}

% To construct high-quality tutorials, we undertake the following work, drawing upon relevant literature~\cite{kay2014developing,yousef2014drives,brame2017effective,van2013eight}. In terms of content, we begin by framing a clear task description at the outset of the tutorial to establish a distinct learning objective for learners~\cite{kay2014developing,van2013eight,farkas1999logical,van2004four}. We then decompose the core instructional content into a series of explicit, coherent, and manageable steps to facilitate comprehension~\cite{brame2017effective,renkl2005worked,kirschner2006minimal}. To prevent cognitive leaps during the learning process, we accompany each step with a thorough explanation of its underlying rationale, linking the execution of individual steps organically to the overarching conceptual framework~\cite{kay2014developing,renkl2005worked}. Also, we maintain conciseness, focusing on conveying relevant and meaningful information while minimizing extraneous elements that may increase cognitive load~\cite{van2013eight,brame2017effective}. In terms of presentation, we actively incorporate visual materials to combine text with graphics, as research indicates this approach is more effective than text alone~\cite{kay2014developing,clark2011learning,atkinson2000learning}. We also ensure high readability for all text and highlight key information and core operations to effectively reduce learners' cognitive load~\cite{van2013eight,brame2017effective}. Furthermore, we carefully plan the overall layout of the tutorial to maintain a clean and organized structure, thereby avoiding the potential for learners to feel overwhelmed by redundant or densely packed information~\cite{kay2014developing,clark2011learning}. In addition, for video tutorials, we ensure synchronization across text, visuals, and audio to provide a coherent and engaging learning experience~\cite{brame2017effective,kay2014developing}.


% \cz{Better with a figure. What is the source of the generation}
% In the tutorial generation stage, we first filter the operation sequence produced in~\autoref{sec:task_instantiation}, discarding the analysis steps that do not correspond to actual operations by operation type\cz{How?}. Next, we apply visual enhancement to the collected multimodal resources in order to highlight the key aspects of each operation. Subsequently, we generate the components of the help document and video, such as titles and step descriptions, based on the solution plan and task description. Finally, we systematically assemble these components into complete tutorials.


\subsubsection{Step Filtering}
Execution trajectories in \autoref{sec:solution-planning-execution} could be sometimes redundant and too detailed even if the task is successfully executed. For example, "select range A1:B2" could sometimes repeated 2 times. To make the tutorials concise and clear, we remove repeated steps and those do not involve actual operations, as they primarily serve analytical purposes like "look at this chart". 
%Non-operational step examples include annotation (capturing the current window and marking control elements for subsequent analysis) and summary (providing a descriptive overview of the interface based on screenshots or control lists). 
By filtering the step sequences, we get a structured action flow that retains the effective interactions with the Excel interface.

\subsubsection{Visual Enhancement}
% Highlighting key information and core actions has been shown to improve tutorial readability and reduce cognitive load~\cite{van2013eight,brame2017effective}. 
Now we get the clear interactions, but it's hard for human to follow the steps only by screenshots and textual instructions because some key actions could be hard to follow when no obvious signs. Therefore, for each screenshot, we draw red bounding boxes over key regions to direct attention and enable rapid localization. To provide a realistic demonstration and avoid occlusion, we overlay a cursor icon at the bottom-right corner of the boxed region, aiding learner comprehension.

\subsubsection{Tutorial Content Preparation} 
% \fix{finish}\cz{Use different name,}
Based on the task description, the filtered step sequence log, and the enhanced visualization, we generate the following textual content of the tutorial:
\begin{itemize}
    \item Task title, provides an immediate and intuitive understanding of the tutorial’s purpose.
    %~\cite{farkas1999logical,van2004four}.
    \item Task description, establishes explicit learning objectives with environment settings. %~\cite{kay2014developing,van2013eight}.
    \item Step titles, concisely summarize the content of each operation. Decomposing core instructional content into a series of explicit and coherent steps facilitates comprehension. % ~\cite{brame2017effective,renkl2005worked,kirschner2006minimal}.
    \item Step descriptions, provide detailed guidance on how each step is executed and explain its purpose. This level of elaboration helps prevent cognitive leaps during the learning process by linking the execution of individual steps to the overarching conceptual framework. %~\cite{kay2014developing,renkl2005worked}.
\end{itemize}
We generate textual content for document and video tutorials separately, as their linguistic and structural requirements differ. Specifically, step descriptions in document tutorials adopt a formal written style, whereas those in video tutorials are phrased as spoken narration, which is subsequently synthesized into audio. To ensure the generation of high-quality instructional content, we design a structured prompt comprising four components:
\begin{itemize}
    \item An instruction, formulated to guide the LLM in generating textual content by role-playing ~\cite{sun2023principled,white2023prompt} as a tutorial authoring expert. 
    \item Description of each textual content, to specify the requirements for the content to be generated. 
    \item Few-shot  manually-crafted demonstrations~\cite{sahoo2024systematic}, which can assist LLMs in better understanding the task and requirements described in the instruction.
    \item Task description and the filtered step sequence log, which serve as the raw information for LLMs to generate textual content.
\end{itemize}


To ensure a well-structure format, we define a JSON template and specify a structured output format for LLM calls. We then match the generated step titles and descriptions with the corresponding visually enhanced screenshots by step index. Finally, we complete the preparation of textual and visual materials by integrating predefined introductions, conclusions, initial/final state screenshots, and background images into the JSON file. All images are referenced by their file paths.

\subsubsection{Tutorial Document Synthesis} 
We provides the document constructor with a JSON file containing textual content and image paths. The constructor then generates tutorials in both HTML and Markdown formats. Following a predefined format scheme, the document constructor places the title at the top center of the document as the heading. It subsequently traverses the JSON file to process the introduction, operation steps, and the output presentation module, preserving the logical order and hierarchical structure of the tutorial to ensure that users can quickly grasp the overall workflow. For each module, the corresponding data are formatted into a block containing a title, an image, and the description, thereby conveying information through both textual and visual cues. Finally, these blocks are sequentially integrated into the main body of the document.

\subsubsection{Tutorial Video Synthesis}
Upon receiving a JSON file containing multi-modal resources, the video constructor generates and integrates following segments:
\begin{enumerate}
\item Introduction. The video title, initial background, and task overview are combined to form the introductory segment, enabling users to quickly understand the tutorial’s theme.
\item Initial state presentation. The initial screenshots are displayed with predefined text explaining, which represent the starting point.
\item Step-by-step demonstration. For each operation step, the constructor iteratively processes the content, including the step index, step title, annotated action image, and step description.
\item Final state presentation. Upon task completion, the final state screenshots are displayed, indicating that they represent the completed outcome. 
\item Ending segment. Following common practices in instructional videos, the tutorial concludes with a message thanking users for watching.
\end{enumerate}

Besides, all video segments are accompanied by audio narration. Subtitles and audio are precisely synchronized to ensure audiovisual consistency, thereby providing an engaging learning experience ~\cite{brame2017effective,kay2014developing}. From a visual design perspective, we adopt adaptive typography to ensure that video titles of varying lengths are evenly distributed across line widths, producing a harmonious presentation that satisfies aesthetic constraints. In addition, we carefully design the overall tutorial layout to maintain a clean and organized structure, thereby preventing learners from becoming overwhelmed by redundant or densely packed information~\cite{kay2014developing,clark2011learning}.