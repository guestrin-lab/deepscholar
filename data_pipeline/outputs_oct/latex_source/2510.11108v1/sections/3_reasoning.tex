\section{The Core Engine: Reasoning for Access Control}

AAC is characterized by dynamic monitoring and comprehensive information flow governance. These features make it impractical to implement through simple software patches or a set of prompts. Realizing AAC requires a separate, dedicated component: a core reasoning engine for access control. Acting as the agent’s “cognitive conscience,” the engine operates independently of the primary LLM and is solely responsible for evaluating and distributing permissions. The necessity for such a dedicated unit is based on a critical security principle: \textbf{the separation of concerns.} 

The agent’s primary LLM focuses on task execution and user interaction, but is inherently vulnerable to manipulations such as prompt injection~\citep{greshake2023youvesignedforcompromising,liu2024formalizingbenchmarkingpromptinjection} or adversarial alignment~\cite{zou2023universaltransferableadversarialattacks,lu2024poex}. Entrusting this same LLM with responsibility for its own security would severely distract its attention, leaving it highly susceptible to attacks. This design flaw leaves the agent’s safety unprotected and simultaneously undermines its task performance.

A dedicated AC engine, operating with a degree of insulation from the main LLM, provides a crucial layer of robustness. It receives contextual input, but does not engage in the primary task, focusing solely on risk assessment and permission allocation. Based on this design, the reasoning engine can consistently enforce access control principles such as \textit{least privilege} and \textit{need to know}, while dynamically adjusting individual permissions in response to evolving interaction relationships. Such guarantees are difficult to achieve with existing models. Thus, the engine could be considered as an \textit{internal arbiter}, which mediates the flow of information between the core logic of agents and the external world~\citep{bai2022constitutionalaiharmlessnessai}.


The engine can be implemented in multiple ways: \textbf{(1) Independent Reasoning Module.} Acting as an external “advisor” or verifier, the engine offers modularity and ease of audit. It can intercept and evaluate requests and responses before they are processed or sent~\citep{lightman2023letsverifystepstep}. However, this approach may introduce latency and faces challenges in fully capturing the context maintained by the primary models. \textbf{(2) Deep Integration.} Embedding access control logic directly into the agent’s cognitive architecture, such as through fine-tuning or dedicated “neurons” ~\citep{askell2021generallanguageassistantlaboratory}. An independent module facilitates auditing, but only deep integration can achieve the low latency and high-fidelity reasoning required for real-time interaction.

Each approach carries its own trade-offs. Our envisioned core engine is not a general purpose LLM, but rather a compact, verifiable neuro-symbolic reasoner that integrates contextual embeddings with formal policy logic. Although more complex to develop, these methods enable access control to become an intrinsic part of the agent’s reasoning process, rather than a secondary check applied after policy generation or action execution. Regardless of the chosen architecture, the principle remains unchanged: effective agent access control requires a dedicated reasoning mechanism.
