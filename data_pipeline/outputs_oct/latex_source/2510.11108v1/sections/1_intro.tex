\section{Introduction}

Recently, agent systems based on Large Language Models (LLMs) have attracted significant research interest. With the rapid development of AI technologies, LLM-based agent systems dynamically handle complex tasks in various interactive environments, alleviating human burdens. Agent systems are increasingly applied across diverse fields~\citep{liu2025advances}, including clinical treatment~\citep{wang2025surveyllmbasedagentsmedicine}, scientific simulations~\citep{park2023generativeagentsinteractivesimulacra}, and software engineering~\citep{wang2025openhandsopenplatformai}.

Despite the considerable attention agent systems have received, their security remains an urgent area for improvement~\citep{wang2025comprehensivesurvey,yu2025asurveytrustagent}. One security topic previously discussed in cybersecurity yet still crucial in agent systems, is \textbf{access control (AC)}. To minimize the loss caused by privilege escalation or unexpected behavior, systems require optimal permission allocation strategies that are adapted to the generative behavior patterns of LLM-based agents. Traditional AC models, built on static rules and binary allow/deny logic~\citep{ferraiolo2009rolebasedaccesscontrols,8594462}. Early research proposed formal methods based on Information Flow Control (IFC)~\citep{myers1997decentralized} to allocate permissions, ensuring secure interactions among system entities. Subsequent advances extended these mechanisms to incorporate contextual factors such as user role, location, and time
~\citep{covington2001context}. Unfortunately, traditional IFC mechanisms face severe limitations in handling dynamic, implicit semantics or complex interactive behaviors, which renders agent access control vulnerable. Our vision paper underscores the importance of access control in agent systems, and argues that strategies for permission allocation must \textbf{move beyond the traditional focus on securing static data, to instead emphasize the governance of dynamic information flow.} Therefore, we envision \textbf{Agent Access Control (AAC)} as a novel framework which redefines access control not as an external security gate, but as an intrinsic cognitive capability of the agent itself. The core of AAC is to view information disclosure as a process of judging appropriateness based on reasoning and context, rather than relying only on fixed rules.

To realize this vision, our AAC framework is built upon two integrated modules, executed by a dedicated reasoning engine: (1) Multi-dimensional Contextual Evaluation, which analyzes the holistic context of an interaction, and (2) Adaptive Response Formulation, which crafts nuanced, appropriate information outputs. In the following content, we begin by detailing the framework, then explore the critical role of its core reasoning engine for effective access control. Finally, the future implications and challenges of this new paradigm will be presented to pave the way for agents that are not only capable, but trustworthy.

