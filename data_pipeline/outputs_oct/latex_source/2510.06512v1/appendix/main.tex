\newpage
\appendix

\section{More details on Algorithm 1}
\label{appendix:alg1_details}

Algorithm~\ref{alg:LogSTOP_complete} describes how LogSTOP is computed for a sequence \(X[t_s:t_e]\) with respect to temporal property \(\varphi\), given start and end timesteps \(t_s\) and \(t_e\), and smoothing window \(w\). Here we discuss the intuition behind the operators, along with some examples:

\noindent{\textbf{Downsampling and average smoothing of confidence scores of local properties.}} Firstly, LogSTOP downsamples the sequence of length \(T\) to contiguous blocks of length \(w\). The confidence scores for any local property in each block starting at \(t\), \(\hat{y}(c, t' \in [t, t+w])\) are first averaged after being normalized to the \([0, 1]\) range. The confidence score for each block is then the \(\log\) of this averaged \([0,1]\)-normalized score. This series of downsampling, normalizing and smoothing operations starting at timestep \(t\) for a window \(w\) can be seen on line~\ref{alg:smoothing_op}. For example, given a sequence of log scores for object "car",
\[\hat{y}({car}, t \in [0, 5]) = [\log(0.9), \log(0.1), \log(0.9), \log(0.9), \log(0.9), \log(0.9)]\] 
and \(w = 3\), the LogSTOP first downsamples to 2 blocks,
\[[[\log(0.9), \log(0.1), \log(0.9)], [\log(0.9), \log(0.9), \log(0.9)]]\] 
before averaging by block, 
\[[\log((0.9 + 0.1 + 0.9)/3), \log((0.9 + 0.9 + 0.9)/3)] = [\log(0.63), \log(0.9)]\]
In this example, note that the score \(\log(0.1)\) is likely incorrect since the car cannot momentarily disappear. Downsampling and then smoothing reduce the impact of this incorrect local prediction hence essentially capturing the property that confidence scores cannot drastically change in a local window. 
Note that this is done online for each successive block and the shifting for temporal operators is handled by the Next operator (line~\ref{alg:next_op}). 

\noindent{\textbf{Handling of logical operators (\(\neg, \land, \lor\))}.} 
The LogSTOP for \(\neg \varphi\) at timestep \(t\) is intuitively high when the score for \(\varphi\) is low (line~\ref{alg:not_op}). For example, given a high score \(\hat{y}({car}, t, w) = \log(0.9)\), the score for "not car" \(\hat{y}(\neg {car}, t, w) = \log(1 - 0.9) = \log(0.1)\) is low.

The LogSTOP for \(\varphi_1 \land \varphi_2\) at timestep \(t\) is high only when the scores for both \(\varphi_1\) and \(\varphi_2\) are high (line~\ref{alg:and_op}). For example, given high scores \(\hat{y}({car}, t, w) = \log(0.9)\) and \(\hat{y}({pedestrian}, t, w) = 0.9\), the score for "car and pedestrian" \(\hat{y}({car} \land {pedestrian}, t, w) = \log(0.9) + \log(0.9) = \log(0.81)\). However, if either of the scores are low, e.g., if \(\hat{y}({pedestrian}, t, w) = 0.1\), the score drops significantly to \(\hat{y}({car} \land {pedestrian}, t, w) = \log(0.9) + \log(0.1) = \log(0.09)\). Inspired by DeMorgan's law, the LogSTOP for \(\varphi_1 \lor \varphi_2\) is simply the score for equivalent property \(\neg (\neg\varphi_1 \land \neg\varphi_2)\) (line~\ref{alg:or_op}). This is intuitively only low when both the scores are low. 

Note that these operators are defined not only over local properties \(c\) as in the examples above, but over any temporal property \(\varphi, \varphi_1, \varphi_2\). Hence, for temporal property \(\varphi\) = "(car or truck) and (not pedestrian)", the score is high only if it is high for both \(\varphi_1\) = "(car or truck)" and \(\varphi_2\) = "(not pedestrian)". The scores for \(\varphi_1, \varphi_2\) can be recursively computed.

\noindent{\textbf{Handling of temporal operators (\(\bigcirc, \BoxOp{I}, \UntilOp{I}\)).} }
As discussed above, the property Next \(\varphi\) (\(\bigcirc\, \varphi\)) evaluates whether \(\varphi\) is expressed starting at the next block \(t + w\) (line~\ref{alg:next_op}). When \(w = 1\), this represents the standard Next operator.
The property Always \(\varphi\) (\(\BoxOp{I}\, \varphi\)) is interpreted here as a "temporal and" operator over the sequence (line~\ref{alg:always_op}). Hence, \(\BoxOp{I}\, \varphi\) can be equivalently written as \(\varphi \land \bigcirc\, \BoxOp{I}\, \varphi\): the property \(\varphi\) is expressed by \(x_t\) and always after by \([x_{t+w}, \ldots]\) (\(\bigcirc\, {\BoxOp{I}\,\varphi}\)). Similar to the logical \(\land\) operator, the score is high only if it is high for all timesteps of the sequence. The computation in \(\log\) space is beneficial to prevent any underflow here with fixed precision.

The property \(\varphi_1\) Until \(\varphi_2\) (\(\varphi = \varphi_1 \UntilOp{I} \varphi_2\)) can be equivalently written as \(\varphi = \varphi_2 \lor (\neg \varphi_2 \land \varphi_1 \land \bigcirc\,(\varphi_1 \UntilOp{I-1} \varphi_2))\) (line~\ref{alg:until_op}). This informally translates to evaluating whether either (1) \(\varphi_2\) is expressed by \(x_t\), or (2) \(\varphi_1\) is expressed instead and \(\varphi = \varphi_1 \UntilOp{I-1} \varphi_2\) is expressed  by \([x_{t+w}, \ldots]\). 

\section{More Details on Quantitative Semantics for Temporal Logic}
\label{appendix:semantics}

We now present more details on the standard quantitative semantics for Signal Temporal Logic (STL) discussed in Section~\ref{sec:method}. A formula in Signal Temporal Logic can be written as:

\[\varphi := \top \mid \mu \mid \neg \varphi \mid \varphi_1 \land \varphi_2 \mid \varphi_1 \UntilOpSTL{I} \varphi_2\]

where \(\mu = f(s(t)) \geq 0\) is a Lipschitz continuous function over the signal \(s\) and \(I = [t_1, t_2]\) is a time interval, \(t_2 \geq t_1 \geq 0\). The operators \textit{Eventually} and \textit{Always} can be defined as follows:

\[\DiaOpSTL{I} \varphi := \top \UntilOpSTL{I} \varphi\]
\[\BoxOpSTL{I} \varphi := \neg \DiaOpSTL{I} \neg\varphi\]

In the context of the STOP problem, \(s(t) = \hat{y}(X, \cdot, t)\) and \(f_c(s(t)) = \hat{y}(X, c, t) - \tau\). The query "car until pedestrian" can be written as \(\mu_{car}(s(t)) \UntilOpSTL{I} \mu_{pedestrian}(s(t))\) where \(I = [0, T]\) and \(\mu_{car}(s(t)) = \hat{y}(X, {car}, t) - \tau \geq 0\) (\(\mu_{pedestrian}\) is defined similarly).

The STL quantitative semantics, also called \textit{robustness} \(\rho\)~\citep{FAINEKOS20094262}, is defined as follows to indicate how much a signal satisfies or violates the formula:

\[\rho(\top, s, t) := \rho_\top\]
\[\rho(\mu, s, t) := f(s(t))\]
\[\rho(\neg \varphi, s, t) := -\rho(\varphi, s, t)\]
\[\rho(\varphi_1 \land \varphi_2, s, t) := \min(\rho(\varphi_1, s, t), \rho(\varphi_2, s, t))\]
\[\rho(\varphi_1 \lor \varphi_2, s, t) := \max(\rho(\varphi_1, s, t), \rho(\varphi_2, s, t))\]
\[\rho(\varphi_1 \UntilOpSTL{I} \varphi_2, s, t) := \sup_{t' \in\, t+I}(\min\{\rho(\varphi_2, s, t'), \inf_{t'' \in\, [t, t']}\rho(\varphi_1, s, t'')\})\]
\[\rho(\BoxOpSTL{I}\,\varphi, s, t) := \inf_{t' \in\,[t+I]} \rho(\varphi, s, t')\]
\[\rho(\DiaOpSTL{I}\,\varphi, s, t) := \sup_{t' \in\,[t+I]} \rho(\varphi, s, t')\]

where, \(\rho_\top\) is the maximum robustness, i.e., \(\rho_\top = b -\tau\) for the CSTOP problem where \(b = \max(\hat{y}(\cdot, \cdot, \cdot))\).

We argue that LogSTOP offers advantages over such semantics in the context of the STOP problem.
This is primarily because the traditional robustness measure is defined using \(\max\) and \(\min\) functions over temporal and logical formulae. 
The measure, hence, only reflects the most violating or most satisfying timestep in the sequence. For example, consider assigning confidence scores to the property "Always car" in two different scenarios:
\[\hat{y}_1({car}, t\in[0,2]) = [\log(0.9), \log(0.9), \log(0.1)]\]
% \[\hat{y}_2({car}, t\in[0,2]) = [\log(0.9), \log(0.1), \log(0.1)]\]
\[\hat{y}_2({car}, t\in[0,2]) = [\log(0.1), \log(0.1), \log(0.1)]\]
Ideally, the confidence score for "Always car" should follow the order: 
\(\hat{y}_1(\BoxOp{I}{car}, \cdot) > \hat{y}_2(\BoxOp{I}{car}, \cdot)\). 
The standard STL semantics, however, would assign the same robustness to both sequences for any \(\tau > 0.1\) since the most violating score is \(\log(0.1)\) in either case. This makes the robustness measure unsuitable for downstream applications that require such ordering: for example, ranking / search. 

\noindent{\textbf{An example with Boolean operators. }} For example, consider assigning confidence scores to the property "car and pedestrian" at \(t=0\) in two different scenarios:
\[\hat{y}_1({car}, t=0) = \log(0.9), \hat{y}_1({pedestrian}, t=0) = \log(0.6)\]
\[\hat{y}_2({car}, t=0) = \log(0.6), \hat{y}_2({pedestrian}, t=0) = \log(0.6)\]
% \[\hat{y}_2({car}, t\in[0,2]) = [\log(0.9), \log(0.1), \log(0.1)]\]
Ideally, the confidence scores for "car and pedestrian" should follow the order: 
\(\hat{y}_1({car} \land {pedestrian}, t=0) > \hat{y}_2({car} \land {pedestrian}, t=0)\) since \(\hat{y}_1({car}, t=0) > \hat{y}_2({car}, t=0)\). The robustness for both the cases is the same, i.e., \(\log(0.6) - \log(0.5)\), because of the \(\min\) semantics for the Boolean \textit{and} operator. The LogSTOP for the two cases are \(\log(0.9) + \log(0.6)\) and  \(\log(0.6) + \log(0.6)\) respectively, which reflect the expected order.

\noindent{\textbf{An example with the Until operator. }} Consider assigning confidence scores to the property "car Until pedestrian" in two different scenarios:
\[\hat{y}_1({car}, t\in[0,2]) = [\log(0.6), \log(0.6), \log(0.6)]\]
\[\hat{y}_1({pedestrian}, t\in[0,2]) = [\log(0.4), \log(0.4), \log(0.9)]\]
and,
\[\hat{y}_2({car}, t\in[0,2]) = [\log(0.6), \log(0.6), \log(0.6)]\]
\[\hat{y}_2({pedestrian}, t\in[0,2]) = [\log(0.4), \log(0.4), \log(0.6)]\]

Note that the only difference between the two scenarios is the score for "pedestrian" at \(t = 2\) (a high score of \(0.9\) for the first scenario and a lower score of \(0.6\) for the second scenario) . The robustness for the two cases is the same because of the \(\min\) semantics within the \textit{Until} operator. LogSTOP assigns a higher score for the first scenario because of the difference at \(t=2\).

\section{More Details on Datasets}
\label{appendix:datasets}

In Section~\ref{sec:experiments}, we briefly discuss the QMTP and TP2VR benchmarks for evaluation.
For
constructing these benchmarks
, we use 
three existing datasets with frame/segment-level annotations for local properties:
The RealTLV dataset~\citep{Choi2024TowardsNV}
consists of videos from NuScenes~\citep{nuscenes} and Waymo~\citep{waymo} driving datasets with frame-level annotations for 6 object classes. 
(for example, "car", "truck", etc). 
The IEMOCAP dataset~\citep{busso2008iemocap} provides speech segments from conversations 
between two speakers 
and each segment is labeled with one of the 4 major emotions expressed by the speaker. 
(for example, "happy", "sad", etc.). 
The AVA dataset~\citep{gu2018ava} consists of frame-level action annotations for 80 actions in 15-min clips from YouTube. We only consider the validation subset of this dataset and sample 5 frames per second.
These datasets can be used for evaluating
temporal properties over objects in videos 
("car until pedestrian", for example)
, emotions in speech 
("always happy")
, and actions in videos 
("a person sits until they stand up") 
respectively.

\noindent{\textbf{The QMTP dataset.}}
For any temporal property template (for example, "p1 Until p2") and samples from these datasets, we identify matching and non-matching sequences of desired length as follows: for every sample, we first identify candidates for local properties in the template (p1, p2, etc.) as the set of all ground-truth objects / emotions in the sequence. We then use the standard LTL semantics over the frame/segment-level ground-truth labels to collect matching and non-matching subsequences of the desired length. This creates a TP-query matching dataset for an arbitrary set of temporal properties as long as these properties are sufficiently expressed by sequences from the underlying dataset. Moreover, this pipeline is agnostic to the choice of the dataset since it only requires sequences of ground-truth labels for local properties. We use this pipeline to create  
the \textbf{QMTP-video dataset} with \(7468\) samples (\(3750\) matching and \(3718\) non-matching) with video sequences of lengths \(\{10, 20, 30, 40, 50\}\). For each target length, this dataset contains approximately \(100\) samples corresponding to each of the \(15\) property templates. Similarly, we create the \textbf{QMTP-speech dataset} with \(3300\) samples, including speech sequences of lengths in ranges \(\{5-10, 10-20, 20-30\}\). The QMTP-speech dataset only contains samples from \(11/15\) property templates. This is because there are no sequences matching \(4\) properties "Always p1 and Eventually p2", "Always (p1 and p2)", "(p1 and p2) Until p3", and "(p1 and p2) Until Eventually p3" since two emotions cannot be expressed at the same time. 

\noindent{\textbf{The TP2VR dataset.}}
We restrict queries to a maximum of 5 per temporal property template. For TP2VR objects, we aim to find \(25-50\) frame-long subsequences satisfying a temporal property; we only include a temporal property if less than \(250\) videos are retrieved using the standard LTL semantics with the ground truth labels. 
With all possible combinations of objects, this gives us a total of \(42\) queries, with an average of \(163\) videos relevant to a query.
Similarly, for TP2VR-actions, we aim to find 10-second long subsequences (\(50\) frames) satisfying a temporal property; we only include a temporal property if less than \(50\) videos are retrieved using the standard LTL semantics with the ground truth labels.  With all possible combinations of actions, this gives us a total of \(70\) queries, with an average of \(21\) videos relevant to a query.

\section{More details on Query Matching Methods}
\label{appendix:baselines_qm}

\noindent{\textbf{LogSTOP with simple trained neural predictors:}} We use YOLOv8~\citep{yolov8_ultralytics}\AKadds{, Grounding DINO~\citep{liu2024grounding}, and OWLv2~\citep{minderer2023scaling},} to get confidence scores for object detection in video frames, and HuBERT~\citep{superb} for speech emotion recognition in audio segments. Since the scores are in the \([0,1]\) range, we normalize them in the \([-\infty, 0]\) range using the \(\log\) operation, as required by LogSTOP.
A video matches query \(\varphi\) if LogSTOP \(\hat{y}(\varphi, 0, w) > \tau(\varphi, 0)\) (and vice versa for non-matching examples). The estimates are evaluated against ground-truth labels \(y(\varphi, 0)\). The window \(w\) is selected as follows: \(w = 2\) for \(T<20\) and \(w = 5\) otherwise. 


\noindent{\textbf{Large Vision Language Models (LVLMs)}.} We evaluate two popular LVLMs on query matching for videos: \texttt{Video-LLava-7B}~\citep{videollava} and \texttt{LongVA-7B}~\citep{zhang2024longva}. 
% These models can be prompted with text, images and videos, and can generate text as output. 
For the "always car" example, we provide the models with the video sequence and a text prompt "Is a car detected in all frames of this video?". The response is considered correct if the model responds with "Yes" or "No" for matching and non-matching samples respectively. \texttt{Video-LLava-7B} supports a context window of \(4096\) tokens while \texttt{LongVA-7B} can handle up to 2000 frames. We set the maximum tokens to generate to \(60\) and \(1024\) respectively and use a temperature of 0.1 and standard values for the other parameters.

\noindent{\textbf{Large Audio Language Models (LALMs)}.} Similarly, we evaluate two popular LALMs on query matching for speech: \texttt{Qwen-Audio-Chat}~\citep{Qwen-Audio} and \texttt{Qwen2-Audio-7B-Instruct}~\citep{Qwen2-Audio}. 
For the "eventually happy" example, we provide the models with the audio sequence and a text prompt "Does the speaker sound happy at some time in this audio clip?". We set the sampling rate to \(16000\) and generate a maximum of \(256\) new tokens, with standard values for other parameters.

\noindent{\textbf{NSVS-TL~\citep{Choi2024TowardsNV}.}} Proposed for event detection in videos, NSVS-TL~\citep{Choi2024TowardsNV} uses the PCTL-based model checker STORM~\citep{storm} to identify video frame subsequences where a certain event is detected. NSVS-TL reports state-of-the-art performance on detecting temporal events in videos, surpassing large language models such as GPT-4. 
For our task, we specify the target query in PCTL ("always car" is \(P > 0.5 [G \) "\({car}\)" \(]\)) and the response is considered correct if NSVS-TL returns / does not return the entire video sequence as output for a matching / non-matching query respectively.

We do not evaluate the method from ~\citet{yang2023specification} since the implementation is not publicly available and LTL model checking with STORM is not well-documented.

\section{More details on the Ranked Retrieval methods}
\label{appendix:baselines_vr}

\noindent{\textbf{LogSTOP.}} 
We use LogSTOP with Grounding DINO~\citep{liu2024grounding} for TP2VR-objects and with SlowR50~\citep{feichtenhofer2019slowfast} for TP2VR-actions respectively. 
We repurpose the script from \texttt{tutorials/video\_detection\_example} at \texttt{https://github.com/facebookresearch/pytorchvideo/}
to run SlowR50 on videos from the TP2VR-actions dataset.
We use a smoothing window \(w = 5\) for all retrieval experiments.


\noindent{\textbf{mPLUG.}} 
We use the implementation from \texttt{https://github.com/alibaba/AliceMind}. We repurpose the \texttt{mPLUG/retrieval\_vid\_mplug.py} script to run \texttt{mPLUG\_{large}\_v2} on videos and queries from the TP2VR datasets.

\noindent{\textbf{CaptionSim.}} 
Inspired by ELIOT~\citep{liu-etal-2025-eliot}, we also include embedding similarity between video captions and text queries as a baseline.
We refer to this as \texttt{CaptionSim} in the discussion, and use 
\texttt{LLaVA-NeXT-Video-7B}~\citep{zhang2024llavanextvideo} for generating video captions and \texttt{SentenceBERT/all-MiniLM-L6-v2}~\citep{reimers-2020-multilingual-sentence-bert} for embedding captions and queries. 
Due to the limited context window of \texttt{LLaVA-NeXT-Video-7B}, we divide videos in sections of 50 frames and generate captions for each before concatenating them together. We use the following prompt to generate the captions for the first 50 frames: "Describe this video in detail, listing objects in each frame. Keep the descriptions concise." for TP2VR-objects.
For any next sections, we use the prompt "Continue describing the video, listing objects in each frame. You are now at frame {i}, you have already described the previous {i} frames."
For TP2VR-actions, we use the prompt "Describe this video in detail, listing actions and objects in each frame. Keep the descriptions concise." We set the \texttt{max\_new\_tokens} to 1024.

We use \texttt{CaptionSim} because the implementation of ELIOT is not publicly available. Our local implementation of ELIOT did not report good results (the video captions generated by ~\citep{tewel2022zero} did not include mentions of objects or actions).


\section{Queries and Prompts}
\label{appendix:queries}

We choose \(15\) temporal property templates for the experiments in Section~\ref{sec:experiments}.

The LogSTOP queries for these templates are as follows:

\begin{enumerate}
    \item Eventually p1: \(\DiaOp{I}\, p1\)
    \item Always p1: \(\BoxOp{I}\,p1\)
    \item p1 Until p2: \(p1 \UntilOp{I} p2\)
    \item Always p1 and Eventually p2: \(\BoxOp{I}\,p1 \land \DiaOp{I}\,p2\)
    \item Always p1 or Eventually p2: \(\BoxOp{I}\,p1 \lor \DiaOp{I}\,p2\)
    \item (Not p1) Until p2: \(\neg p1 \UntilOp{I} p2\)
    \item p1 Until (Not p2): \(p1 \UntilOp{I} \neg p2\)
    \item Always (p1 and p2): \(\BoxOp{I} (p1 \land p2)\)
    \item (p1 and p2) Until p3: \((p1 \land p2) \UntilOp{I} p3\)
    \item p1 Until Always p2: \(p1 \UntilOp{I} \BoxOp{I}\, p2\)
    \item Eventually Always p1: \(\DiaOp{I}\,\BoxOp{I}\,p1\)
    \item Always Eventually p1: \(\BoxOp{I}\,\DiaOp{I}\,p1\)
    \item (Not p1) Until Eventually p2: \(\neg p1 \UntilOp{I} \DiaOp{I}\, p2\)
    \item (Not p1) Until Always p2: \(\neg p1 \UntilOp{I} \BoxOp{I}\, p2\)
    \item (p1 and p2) Until Eventually p3: \((p1 \land p2) \UntilOp{I} \DiaOp{I}\, p3\)
\end{enumerate}

NSVS-TL~\citep{Choi2024TowardsNV} uses the model checker STORM~\citep{storm} to verify if a given sequence satisfies a temporal property, where the temporal properties are represented in Probabilistic Computation Tree Logic (PCTL). In PCTL, the \(F\), \(G\) and \(U\) operators represent the \textit{Eventually}, \textit{Always} and \textit{Until} operators respectively. The \(\sim\), \(\&\) and \(\mid\) operators represent the Boolean \textit{negation}, \textit{and}, and \textit{or} operators respectively. The operator \(P\) is used to indicate the ranges of probability of a given property being satisfied: for example, \(P > 0.5 [F\, \varphi]\) translates to "the probability of \(\varphi\) eventually being satisfied is more than \(0.5\)".

The PCTL queries for the 15 temporal property templates are as follows:

\begin{enumerate}
    \item Eventually p1: \(P > 0.5\, [F \) "\({p1}\)" \(]\)
    \item Always p1: \(P > 0.5\, [G \) "\({p1}\)" \(]\)
    \item p1 Until p2: \(P > 0.5\, [ \) "\({p1}\)" \(U\) "\({p2}\)" \(]\)
    \item Always p1 and Eventually p2: \(P > 0.5\, [\,G \) "\({p1}\)" \(\&\, F\,\) "\({p2}\)" \(]\)
    \item Always p1 or Eventually p2: \(P > 0.5\, [\,G \) "\({p1}\)" \(\mid\, F\,\) "\({p2}\)" \(]\)
    \item (Not p1) Until p2:  \(P > 0.5\, [ \sim \) "\({p1}\)" \(U\) "\({p2}\)" \(]\)
    \item p1 Until (Not p2):  \(P > 0.5\, [\) "\({p1}\)" \(U \sim\) "\({p2}\)" \(]\)
    \item Always (p1 and p2): \(P > 0.5\, [\,G \) "\({p1}\)" \(\&\, G\,\) "\({p2}\)" \(]\)
    \item (p1 and p2) Until p3: \(P > 0.5\, [ \) "\({p1}\)" \(\&\) "\({p2}\)" \(U\) "\({p3}\)" \(]\)
    \item p1 Until Always p2:  \(P > 0.5\, [ \) "\({p1}\)" \(U\,G\,\) "\({p2}\)" \(]\)
    \item Eventually Always p1: \(P > 0.5\, [F\,G\, \) "\({p1}\)" \(]\)
    \item Always Eventually p1: \(P > 0.5\, [G\,F\, \) "\({p1}\)" \(]\)
    \item (Not p1) Until Eventually p2: \(P > 0.5\, [ \sim \) "\({p1}\)" \(U\,F\,\) "\({p2}\)" \(]\)
    \item (Not p1) Until Always p2: \(P > 0.5\, [ \sim \) "\({p1}\)" \(U\,G\,\) "\({p2}\)" \(]\)
    \item (p1 and p2) Until Eventually p3: \(P > 0.5\, [ \) "\({p1}\)" \(\&\) "\({p2}\)" \(U\,F\,\) "\({p3}\)" \(]\)
\end{enumerate}

The prompts for LVLMs for query matching on QMTP-video are as follows:

\begin{enumerate}
    \item Eventually p1: "Is a \({p1}\) present in any frame of this video?"
    \item Always p1: "Is a \({p1}\) present in all frames of this video?"
    \item p1 Until p2: "Is a \({p2}\) present in any frame of this video and \({p1}\) present in all previous frames?"
    \item Always p1 and Eventually p2: "Is a \({p1}\) present in all frames of this video and is a \({p2}\) present in any frame of this video?"
    \item Always p1 or Eventually p2: "Is a \({p1}\) present in all frames of this video or is a \({p2}\) present in any frame of this video?"
    \item (Not p1) Until p2: "Is a \({p2}\) present in any frame of this video and \({p1}\) absent in all previous frames?"
    \item p1 Until (Not p2): "Is a \({p2}\) absent in any frame of this video and \({p1}\) present in all previous frames?"
    \item Always (p1 and p2): "Are both \({p1}\) and \({p2}\) present in all frames of this video?"
    \item (p1 and p2) Until p3: "Is a \({p3}\) present in any frame of this video and both \({p1}\) and \({p2}\) present in all previous frames?"
    \item p1 Until Always p2: "Starting at some frame in this video, is a \({p2}\) present in all subsequent frames and \({p1}\) present in all previous frames?"
    \item Eventually Always p1: "Starting at some frame in this video, is a \({p1}\) present in all subsequent frames?"
    \item Always Eventually p1: "Starting at any frame in this video, is a \({p1}\) present in some subsequent frame?"
    \item (Not p1) Until Eventually p2: "Starting at some frame in this video, is a \({p2}\) present in some subsequent frame and \({p1}\) absent in all previous frames?"
    \item (Not p1) Until Always p2: "Starting at some frame in this video, is a \({p2}\) present in all subsequent frames and \({p1}\) absent in all previous frames?"
    \item (p1 and p2) Until Eventually p3: "Starting at some frame in this video, is a \({p3}\) present in some subsequent frame and both \({p1}\) and \({p2}\) present in all previous frames?"
\end{enumerate}

The prompts for LALMs for query matching on QMTP-speech are as follows:

\begin{enumerate}
    \item Eventually p1: "Does the speaker's emotion sound \({p1}\) at any time?"
    \item Always p1: "Does the speaker's emotion sound \({p1}\) at all times?"
    \item p1 Until p2: "Does the speaker's emotion sound \({p2}\) at any time and \({p1}\) at all times until then?"
    \item Always p1 and Eventually p2: "Does the speaker's emotion sound \({p1}\) at all times and \({p2}\) at any time?"
    \item Always p1 or Eventually p2: "Does the speaker's emotion sound \({p1}\) at all times or \({p2}\) at any time?"
    \item (Not p1) Until p2: "Does the speaker's emotion sound \({p2}\) at any time and not \({p1}\) at all times until then?"
    \item p1 Until (Not p2): "Does the speaker's emotion sound not \({p2}\) at any time and \({p1}\) at all times until then?"
    \item Always (p1 and p2): "Does the speaker's emotion sound both \({p1}\) and \({p2}\) at all times?"
    \item (p1 and p2) Until p3: "Does the speaker's emotion sound \({p3}\) at any time and both \({p1}\) and \({p2}\) at all times until then?"
    \item p1 Until Always p2: "Starting at some time in this audio clip, does the speaker's emotion sound \({p2}\) at all subsequent times and \({p1}\) at all previous times?"
    \item Eventually Always p1: "Starting at some time in this audio clip, does the speaker's emotion sound \({p1}\) at all subsequent times?"
    \item Always Eventually p1: "Starting at any time in this audio clip, does the speaker's emotion sound \({p1}\) at some subsequent time?"
    \item (Not p1) Until Eventually p2: "Starting at some time in this audio clip, does the speaker's emotion sound \({p2}\) at some subsequent time and not \({p1}\) at all previous times?"
    \item (Not p1) Until Always p2: "Starting at some time in this audio clip, does the speaker's emotion sound \({p2}\) at all subsequent times and not \({p1}\) at all previous times?"
    \item (p1 and p2) Until Eventually p3: "Starting at some time in this audio clip, does the speaker's emotion sound \({p3}\) at some subsequent time and both \({p1}\) and \({p2}\) at all previous times?"
\end{enumerate}

The queries for \texttt{mPLUG} and \texttt{CaptionSim} for retrieval on TP2VR-objects are as follows (\(t_{lo} = 25\) and \(t_{hi} = 50\) here):

\begin{enumerate}
    \item Eventually p1: "A sequence of \(t_{lo}\) to \(t_{hi}\) frames where a \({p1}\) appears at some point."
    \item Always p1: "A sequence of \(t_{lo}\) to \(t_{hi}\) frames where a \({p1}\) is always present."
    \item p1 Until p2: "A sequence of \(t_{lo}\) to \(t_{hi}\) frames where a \({p2}\) is present at some point and a \({p1}\) is present in all frames before that."
    \item Always p1 and Eventually p2: "A sequence of \(t_{lo}\) to \(t_{hi}\) frames where a \({p1}\) is always present and a \({p2}\) appears at some point."
    \item Always p1 or Eventually p2: "A sequence of \(t_{lo}\) to \(t_{hi}\) frames where either a \({p1}\) is always present or a \({p2}\) appears at some point."
    \item (Not p1) Until p2: "A sequence of \(t_{lo}\) to \(t_{hi}\) frames where a \({p2}\) is present at some point and a \({p1}\) is absent in all frames before that."
    \item p1 Until (Not p2): "A sequence of \(t_{lo}\) to \(t_{hi}\) frames where a \({p2}\) is absent at some point and a \({p1}\) is present in all frames before that."
    \item Always (p1 and p2): "A sequence of \(t_{lo}\) to \(t_{hi}\) frames where a \({p1}\) and a \({p2}\) are always present."
    \item (p1 and p2) Until p3: "A sequence of \(t_{lo}\) to \(t_{hi}\) frames where a \({p3}\) is present at some point and both a \({p1}\) and a \({p2}\) are present in all frames before that."
    \item p1 Until Always p2: "A sequence of \(t_{lo}\) to \(t_{hi}\) frames where starting at some point, a \({p2}\) is always present and a \({p1}\) is present in all frames before that."
    \item Eventually Always p1: "A sequence of \(t_{lo}\) to \(t_{hi}\) frames where starting at some point, a \({p1}\) starts being always present."
    \item Always Eventually p1: "A sequence of \(t_{lo}\) to \(t_{hi}\) frames where starting at any point, a \({p1}\) appears in some frames."
    \item (Not p1) Until Eventually p2: "A sequence of \(t_{lo}\) to \(t_{hi}\) frames where starting at some point, a \({p2}\) appears in some frames and a \({p1}\) is absent in all frames before that."
    \item (Not p1) Until Always p2: "A sequence of \(t_{lo}\) to \(t_{hi}\) frames where starting at some point, a \({p2}\) is always present and a \({p1}\) is absent in all frames before that."
    \item (p1 and p2) Until Eventually p3: "A sequence of \(t_{lo}\) to \(t_{hi}\) frames where starting at some point, a \({p3}\) appears at some point and both a \({p1}\) and a \({p2}\) are present in all frames before that."
\end{enumerate}

The queries for \texttt{mPLUG} and \texttt{CaptionSim} for retrieval on TP2VR-actions are as follows (note that \(t_{lo} = t_{hi} = 50\) here):

\begin{enumerate}
    \item Eventually p1: "A sequence of \(t_{lo}\) frames where the action '\({p1}\)' happens at some point."
    \item Always p1: "A sequence of \(t_{lo}\) frames where the action '\({p1}\)' is always happening."
    \item p1 Until p2: "A sequence of \(t_{lo}\) frames where the action '\({p2}\)' happens at some point and the action '\({p1}\)' is happening in all frames before that."
    \item Always p1 and Eventually p2: "A sequence of \(t_{lo}\) frames where the action '\({p1}\)' is always happening and the action '\({p2}\)' happens at some point."
    \item Always p1 or Eventually p2: "A sequence of \(t_{lo}\) frames where either the action '\({p1}\)' is always happening or the action '\({p2}\)' happens at some point."
    \item (Not p1) Until p2: "A sequence of \(t_{lo}\) frames where the action '\({p2}\)' happens at some point and the action '\({p1}\)' is not happening in all frames before that."
    \item p1 Until (Not p2): "A sequence of \(t_{lo}\) frames where the action '\({p2}\)' is not happening at some point and the action '\({p1}\)' is happening in all frames before that."
    \item Always (p1 and p2): "A sequence of \(t_{lo}\) frames where the actions '\({p1}\)' and '\({p2}\)' are always happening."
    \item (p1 and p2) Until p3: "A sequence of \(t_{lo}\) frames where the action '\({p3}\)' happens at some point and both the actions '\({p1}\)' and '\({p2}\)' are happening in all frames before that."
    \item p1 Until Always p2: "A sequence of \(t_{lo}\) frames where starting at some point, the action '\({p2}\)' is always happening and the action '\({p1}\)' is happening in all frames before that."
    \item Eventually Always p1: "A sequence of \(t_{lo}\) frames where starting at some point, the action '\({p1}\)' is always happening."
    \item Always Eventually p1: "A sequence of \(t_{lo}\) frames where starting at any point, the action '\({p1}\)' happens in some frames."
    \item (Not p1) Until Eventually p2: "A sequence of \(t_{lo}\) frames where starting at some point, the action '\({p2}\)' happens in some frames and the action '\({p1}\)' is not happening in all frames before that."
    \item (Not p1) Until Always p2: "A sequence of \(t_{lo}\) frames where starting at some point, the action '\({p2}\)' is always happening and the action '\({p1}\)' is not happening in all frames before that."
    \item (p1 and p2) Until Eventually p3: "A sequence of \(t_{lo}\) frames where starting at some point, the action '\({p3}\)' happens at some point and both the actions '\({p1}\)' and '\({p2}\)' are happening in all frames before that."
\end{enumerate}

\section{Other Experiment Details}
\label{appendix:experiments}

\noindent{\textbf{Compute Resources.}} 
All experiments were run on a shared cluster with the following GPUs:
eight NVIDIA A100 PCIe (80GB RAM each) and eight NVIDIA RTX A6000 (48GB RAM each).

\section{Examples}
\label{appendix:examples}

Figure~\ref{fig:retrieval_example} presents examples of ranking using the three methods. 
In the first example, the video captions used by \texttt{CaptionSim} do not include smaller objects that might be relevant to the query ("car" in this example). In the second example, the two actions are mentioned in the caption -- the video is ranked lower than other videos with more mentions of the actions ("stand" and "hand clap" in this case). 
This demonstrates that while caption-based methods outperform joint model embeddings (\texttt{mPLUG}), they rely on semantic similarity between captions and text to determine relevance, which might not be sufficient for effective retrieval with temporal queries.

\section{Adaptive threshold vs. constant threshold}
\label{appendix:thresholds}

Figure~\ref{fig:thresholds_all} presents a comparison of the adaptive threshold and the constant \(\log 0.5\) threshold for all temporal property templates, using LogSTOPs for matching and non-matching sequences from the QMTP-video (detections using YOLOv8).

\input{graphics/charts/threshold_comparison_all}

\section{Detailed Results}
\label{appendix:detailed_results}

\subsection{Ranked retrieval}
\label{appendix:vr_results}

In Section~\ref{sec:experiments}, we present the results for ranked retrieval with different methods. Table~\ref{tab:retrieval_results} reports these results (with additional metrics, including Precision@5 and Precision@10).

\input{graphics/tables/retrieval_results}

\subsection{Query matching}
\label{appendix:qm_results}

In Section~\ref{sec:experiments}, we present the query matching results for the QMTP datasets. 
Table~\ref{tab:results_qm} presents the results (balanced accuracies) aggregated by category.
Table~\ref{tab:results_tlv} Table~\ref{tab:results_iemocap} report the results for the 15 temporal property templates for QMTP-video and QMTP-speech respectively.

\input{graphics/tables/query_matching_main_results}

\input{graphics/tables/query_matching_nuscenes.tex}
\input{graphics/tables/query_matching_iemocap.tex}