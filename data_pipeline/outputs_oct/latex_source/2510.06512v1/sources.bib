@article{gmrob,
  author={Mehdipour, Noushin and Vasile, Cristian-Ioan and Belta, Calin},
  journal={IEEE Transactions on Automatic Control}, 
  title={Generalized Mean Robustness for Signal Temporal Logic}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  keywords={Robustness;Logic;Vectors;Semantics;Tuning;Automobiles;Trajectory;Noise;Encoding;Costs;Control synthesis;generalized mean;power mean;robustness;signal temporal logics},
  doi={10.1109/TAC.2024.3482104}}

@article{FAINEKOS20094262,
title = {Robustness of temporal logic specifications for continuous-time signals},
journal = {Theoretical Computer Science},
volume = {410},
number = {42},
pages = {4262-4291},
year = {2009},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2009.06.021},
url = {https://www.sciencedirect.com/science/article/pii/S0304397509004149},
author = {Georgios E. Fainekos and George J. Pappas},
keywords = {Linear and metric temporal logic, Robustness, Metric spaces, Testing},
abstract = {In this paper, we consider the robust interpretation of Metric Temporal Logic (MTL) formulas over signals that take values in metric spaces. For such signals, which are generated by systems whose states are equipped with non-trivial metrics, for example continuous or hybrid, robustness is not only natural, but also a critical measure of system performance. Thus, we propose multi-valued semantics for MTL formulas, which capture not only the usual Boolean satisfiability of the formula, but also topological information regarding the distance, ε, from unsatisfiability. We prove that any other signal that remains ε-close to the initial one also satisfies the same MTL specification under the usual Boolean semantics. Finally, our framework is applied to the problem of testing formulas of two fragments of MTL, namely Metric Interval Temporal Logic (MITL) and closed Metric Temporal Logic (clMTL), over continuous-time signals using only discrete-time analysis. The motivating idea behind our approach is that if the continuous-time signal fulfills certain conditions and the discrete-time signal robustly satisfies the temporal logic specification, then the corresponding continuous-time signal should also satisfy the same temporal logic specification.}
}

@inproceedings{Akazaki2015TimeRI,
  title={Time Robustness in MTL and Expressivity in Hybrid System Falsification},
  author={Takumi Akazaki and Ichiro Hasuo},
  booktitle={International Conference on Computer Aided Verification},
  year={2015},
  url={https://api.semanticscholar.org/CorpusID:14307127}
}

@InProceedings{timerob,
    author="Donz{\'e}, Alexandre
    and Maler, Oded",
    editor="Chatterjee, Krishnendu
    and Henzinger, Thomas A.",
    title="Robust Satisfaction of Temporal Logic over Real-Valued Signals",
    booktitle="Formal Modeling and Analysis of Timed Systems",
    year="2010",
    publisher="Springer Berlin Heidelberg",
    address="Berlin, Heidelberg",
    pages="92--106",
    abstract="We consider temporal logic formulae specifying constraints in continuous time and space on the behaviors of continuous and hybrid dynamical system admitting uncertain parameters. We present several variants of robustness measures that indicate how far a given trajectory stands, in space and time, from satisfying or violating a property. We present a method to compute these robustness measures as well as their sensitivity to the parameters of the system or parameters appearing in the formula. Combined with an appropriate strategy for exploring the parameter space, this technique can be used to guide simulation-based verification of complex nonlinear and hybrid systems against temporal properties. Our methodology can be used for other non-traditional applications of temporal logic such as characterizing subsets of the parameter space for which a system is guaranteed to satisfy a formula with a desired robustness degree.",
    isbn="978-3-642-15297-9"
}

@inproceedings{Choi2024TowardsNV,
  title={Towards Neuro-Symbolic Video Understanding},
  author={Minkyu Choi and Harsh Goel and Mohammad Omama and Yunhao Yang and Sahil Shah and Sandeep Chinchali},
  booktitle={European Conference on Computer Vision},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:268513042}
}

@article{Sabater2020RobustAE,
  title={Robust and efficient post-processing for video object detection},
  author={Alberto Sabater and Luis Montesano and Ana Cristina Murillo},
  journal={2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year={2020},
  pages={10536-10542},
  url={https://api.semanticscholar.org/CorpusID:221857748}
}

@article{videomme,
  title={Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis},
  author={Fu, Chaoyou and Dai, Yuhan and Luo, Yongdong and Li, Lei and Ren, Shuhuai and Zhang, Renrui and Wang, Zihan and Zhou, Chenyu and Shen, Yunhang and Zhang, Mengdan and others},
  journal={arXiv preprint arXiv:2405.21075},
  year={2024}
}

@article{videollava,
  title={Video-llava: Learning united visual representation by alignment before projection},
  author={Lin, Bin and Ye, Yang and Zhu, Bin and Cui, Jiaxi and Ning, Munan and Jin, Peng and Yuan, Li},
  journal={arXiv preprint arXiv:2311.10122},
  year={2023}
}

@article{xue2024longvila,
  title={Longvila: Scaling long-context visual language models for long videos},
  author={Xue, Fuzhao and Chen, Yukang and Li, Dacheng and Hu, Qinghao and Zhu, Ligeng and Li, Xiuyu and Fang, Yunhao and Tang, Haotian and Yang, Shang and Liu, Zhijian and others},
  journal={arXiv preprint arXiv:2408.10188},
  year={2024}
}

@inproceedings{yolo,
  title={You only look once: Unified, real-time object detection},
  author={Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={779--788},
  year={2016}
}

@article{Hansson1990ALF,
  title={A logic for reasoning about time and reliability},
  author={Hans A. Hansson and Bengt Jonsson},
  journal={Formal Aspects of Computing},
  year={1990},
  volume={6},
  pages={512-535},
  url={https://api.semanticscholar.org/CorpusID:16322562}
}

@article{storm,
  title={The probabilistic model checker Storm},
  author={Hensel, Christian and Junges, Sebastian and Katoen, Joost-Pieter and Quatmann, Tim and Volk, Matthias},
  journal={International Journal on Software Tools for Technology Transfer},
  pages={1--22},
  year={2022},
  publisher={Springer}
}

@inproceedings{carla,
  title = { {CARLA}: {An} Open Urban Driving Simulator},
  author = {Alexey Dosovitskiy and German Ros and Felipe Codevilla and Antonio Lopez and Vladlen Koltun},
  booktitle = {Proceedings of the 1st Annual Conference on Robot Learning},
  pages = {1--16},
  year = {2017}
}

@InProceedings{waymo, author = {Sun, Pei and Kretzschmar, Henrik and Dotiwalla, Xerxes and Chouard, Aurelien and Patnaik, Vijaysai and Tsui, Paul and Guo, James and Zhou, Yin and Chai, Yuning and Caine, Benjamin and Vasudevan, Vijay and Han, Wei and Ngiam, Jiquan and Zhao, Hang and Timofeev, Aleksei and Ettinger, Scott and Krivokon, Maxim and Gao, Amy and Joshi, Aditya and Zhang, Yu and Shlens, Jonathon and Chen, Zhifeng and Anguelov, Dragomir}, title = {Scalability in Perception for Autonomous Driving: Waymo Open Dataset}, booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, month = {June}, year = {2020} }

@InProceedings{nuscenes,
author = {Caesar, Holger and Bankiti, Varun and Lang, Alex H. and Vora, Sourabh and Liong, Venice Erin and Xu, Qiang and Krishnan, Anush and Pan, Yu and Baldan, Giancarlo and Beijbom, Oscar},
title = {nuScenes: A Multimodal Dataset for Autonomous Driving},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@article{busso2008iemocap,
  title={IEMOCAP: Interactive emotional dyadic motion capture database},
  author={Busso, Carlos and Bulut, Murtaza and Lee, Chi-Chun and Kazemzadeh, Abe and Mower, Emily and Kim, Samuel and Chang, Jeannette N and Lee, Sungbok and Narayanan, Shrikanth S},
  journal={Language resources and evaluation},
  volume={42},
  pages={335--359},
  year={2008},
  publisher={Springer}
}

@article{zhang2024longva,
  title={Long Context Transfer from Language to Vision},
  author={Peiyuan Zhang and Kaichen Zhang and Bo Li and Guangtao Zeng and Jingkang Yang and Yuanhan Zhang and Ziyue Wang and Haoran Tan and Chunyuan Li and Ziwei Liu},
  journal={arXiv preprint arXiv:2406.16852},
  year={2024},
  url = {https://arxiv.org/abs/2406.16852}
}

@article{Qwen-Audio,
  title={Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models},
  author={Chu, Yunfei and Xu, Jin and Zhou, Xiaohuan and Yang, Qian and Zhang, Shiliang and Yan, Zhijie  and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2311.07919},
  year={2023}
}

@article{Qwen2-Audio,
  title={Qwen2-Audio Technical Report},
  author={Chu, Yunfei and Xu, Jin and Yang, Qian and Wei, Haojie and Wei, Xipin and Guo,  Zhifang and Leng, Yichong and Lv, Yuanjun and He, Jinzheng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2407.10759},
  year={2024}
}


@software{yolov8_ultralytics,
  author = {Glenn Jocher and Ayush Chaurasia and Jing Qiu},
  title = {Ultralytics YOLOv8},
  version = {8.0.0},
  year = {2023},
  url = {https://github.com/ultralytics/ultralytics},
  orcid = {0000-0001-5950-6979, 0000-0002-7603-6750, 0000-0003-3783-7069},
  license = {AGPL-3.0}
}

@inproceedings{superb,
  author={Shu-wen Yang and Po-Han Chi and Yung-Sung Chuang and Cheng-I Jeff Lai and Kushal Lakhotia and Yist Y. Lin and Andy T. Liu and Jiatong Shi and Xuankai Chang and Guan-Ting Lin and Tzu-Hsien Huang and Wei-Cheng Tseng and Ko-tik Lee and Da-Rong Liu and Zili Huang and Shuyan Dong and Shang-Wen Li and Shinji Watanabe and Abdelrahman Mohamed and Hung-yi Lee},
  title={{SUPERB: Speech Processing Universal PERformance Benchmark}},
  year=2021,
  booktitle={Proc. Interspeech 2021},
  pages={1194--1198},
  doi={10.21437/Interspeech.2021-1775}
}

@article{hsu2021hubert,
  title={Hubert: Self-supervised speech representation learning by masked prediction of hidden units},
  author={Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
  journal={IEEE/ACM transactions on audio, speech, and language processing},
  volume={29},
  pages={3451--3460},
  year={2021},
  publisher={IEEE}
}

@inproceedings{ltl,
  title={The temporal logic of programs},
  author={Pnueli, Amir},
  booktitle={18th annual symposium on foundations of computer science (sfcs 1977)},
  pages={46--57},
  year={1977},
  organization={ieee}
}

@article{cai2024temporalbench,
  title={Temporalbench: Benchmarking fine-grained temporal understanding for multimodal video models},
  author={Cai, Mu and Tan, Reuben and Zhang, Jianrui and Zou, Bocheng and Zhang, Kai and Yao, Feng and Zhu, Fangrui and Gu, Jing and Zhong, Yiwu and Shang, Yuzhang and others},
  journal={arXiv preprint arXiv:2410.10818},
  year={2024}
}

@article{liu2024tempcompass,
  title={Tempcompass: Do video llms really understand videos?},
  author={Liu, Yuanxin and Li, Shicheng and Liu, Yi and Wang, Yuxiang and Ren, Shuhuai and Li, Lei and Chen, Sishuo and Sun, Xu and Hou, Lu},
  journal={arXiv preprint arXiv:2403.00476},
  year={2024}
}

@article{chen2024rextime,
  title={Rextime: A benchmark suite for reasoning-across-time in videos},
  author={Chen, Jr-Jen and Liao, Yu-Chien and Lin, Hsi-Che and Yu, Yu-Chu and Chen, Yen-Chun and Wang, Frank},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={28662--28673},
  year={2024}
}

@inproceedings{nextqa,
  title={Next-qa: Next phase of question-answering to explaining temporal actions},
  author={Xiao, Junbin and Shang, Xindi and Yao, Angela and Chua, Tat-Seng},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9777--9786},
  year={2021}
}

@article{qvhighlights,
  title={Detecting moments and highlights in videos via natural language queries},
  author={Lei, Jie and Berg, Tamara L and Bansal, Mohit},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={11846--11858},
  year={2021}
}

@article{sakshi2024mmau,
  title={Mmau: A massive multi-task audio understanding and reasoning benchmark},
  author={Sakshi, S and Tyagi, Utkarsh and Kumar, Sonal and Seth, Ashish and Selvakumar, Ramaneswaran and Nieto, Oriol and Duraiswami, Ramani and Ghosh, Sreyan and Manocha, Dinesh},
  journal={arXiv preprint arXiv:2410.19168},
  year={2024}
}

@article{ghosh2023compa,
  title={Compa: Addressing the gap in compositional reasoning in audio-language models},
  author={Ghosh, Sreyan and Seth, Ashish and Kumar, Sonal and Tyagi, Utkarsh and Evuru, Chandra Kiran and Ramaneswaran, S and Sakshi, S and Nieto, Oriol and Duraiswami, Ramani and Manocha, Dinesh},
  journal={arXiv preprint arXiv:2310.08753},
  year={2023}
}

@inproceedings{strem,
  title={Pattern matching for perception streams},
  author={Anderson, Jacob and Fainekos, Georgios and Hoxha, Bardh and Okamoto, Hideki and Prokhorov, Danil},
  booktitle={International Conference on Runtime Verification},
  pages={251--270},
  year={2023},
  organization={Springer}
}

@inproceedings{donze2010robust,
  title={Robust satisfaction of temporal logic over real-valued signals},
  author={Donz{\'e}, Alexandre and Maler, Oded},
  booktitle={International Conference on Formal Modeling and Analysis of Timed Systems},
  pages={92--106},
  year={2010},
  organization={Springer}
}

@article{yang2023specification,
  title={Specification-driven video search via foundation models and formal verification},
  author={Yang, Yunhao and Gaglione, Jean-Rapha{\"e}l and Chinchali, Sandeep and Topcu, Ufuk},
  journal={arXiv preprint arXiv:2309.10171},
  year={2023}
}

@inproceedings{liu2024grounding,
  title={Grounding dino: Marrying dino with grounded pre-training for open-set object detection},
  author={Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Jiang, Qing and Li, Chunyuan and Yang, Jianwei and Su, Hang and others},
  booktitle={European conference on computer vision},
  pages={38--55},
  year={2024},
  organization={Springer}
}

@article{minderer2023scaling,
  title={Scaling open-vocabulary object detection},
  author={Minderer, Matthias and Gritsenko, Alexey and Houlsby, Neil},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={72983--73007},
  year={2023}
}

@inproceedings{gu2018ava,
  title={Ava: A video dataset of spatio-temporally localized atomic visual actions},
  author={Gu, Chunhui and Sun, Chen and Ross, David A and Vondrick, Carl and Pantofaru, Caroline and Li, Yeqing and Vijayanarasimhan, Sudheendra and Toderici, George and Ricco, Susanna and Sukthankar, Rahul and others},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6047--6056},
  year={2018}
}

@inproceedings{feichtenhofer2019slowfast,
  title={Slowfast networks for video recognition},
  author={Feichtenhofer, Christoph and Fan, Haoqi and Malik, Jitendra and He, Kaiming},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={6202--6211},
  year={2019}
}

@inproceedings{xu2023mplug,
  title={mplug-2: A modularized multi-modal foundation model across text, image and video},
  author={Xu, Haiyang and Ye, Qinghao and Yan, Ming and Shi, Yaya and Ye, Jiabo and Xu, Yuanhong and Li, Chenliang and Bi, Bin and Qian, Qi and Wang, Wei and others},
  booktitle={International Conference on Machine Learning},
  pages={38728--38748},
  year={2023},
  organization={PMLR}
}

@article{li2022mplug,
  title={mplug: Effective and efficient vision-language learning by cross-modal skip-connections},
  author={Li, Chenliang and Xu, Haiyang and Tian, Junfeng and Wang, Wei and Yan, Ming and Bi, Bin and Ye, Jiabo and Chen, Hehong and Xu, Guohai and Cao, Zheng and others},
  journal={arXiv preprint arXiv:2205.12005},
  year={2022}
}

@article{pctl,
author = {Hansson, Hans and Jonsson, Bengt},
title = {A logic for reasoning about time and reliability},
year = {1994},
issue_date = {Sep 1994},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {6},
number = {5},
issn = {0934-5043},
url = {https://doi.org/10.1007/BF01211866},
doi = {10.1007/BF01211866},
abstract = {We present a logic for stating properties such as, “after a request for service there is at least a 98\% probability that the service will be carried out within 2 seconds”. The logic extends the temporal logic CTL by Emerson, Clarke and Sistla with time and probabilities. Formulas are interpreted over discrete time Markov chains. We give algorithms for checking that a given Markov chain satisfies a formula in the logic. The algorithms require a polynomial number of arithmetic operations, in size of both the formula and the Markov chain. A simple example is included to illustrate the algorithms.},
journal = {Form. Asp. Comput.},
month = sep,
pages = {512–535},
numpages = {24},
keywords = {Markov chains, Modal logic, CTL, Real time, Probability, Soft deadlines, Automatic verification, Model checking}
}

@inproceedings{xu2016msrvtt,
  title={Msr-vtt: A large video description dataset for bridging video and language},
  author={Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016}
}

@inproceedings{didemo,
  title={Localizing moments in video with natural language},
  author={Anne Hendricks, Lisa and Wang, Oliver and Shechtman, Eli and Sivic, Josef and Darrell, Trevor and Russell, Bryan},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={5803--5812},
  year={2017}
}

@inproceedings{liu-etal-2025-eliot,
    title = "{ELIOT}: Zero-Shot Video-Text Retrieval through Relevance-Boosted Captioning and Structural Information Extraction",
    author = "Liu, Xuye  and
      Wang, Yimu  and
      Zhao, Jian",
    editor = "Ebrahimi, Abteen  and
      Haider, Samar  and
      Liu, Emmy  and
      Haider, Sammar  and
      Leonor Pacheco, Maria  and
      Wein, Shira",
    booktitle = "Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 4: Student Research Workshop)",
    month = apr,
    year = "2025",
    address = "Albuquerque, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.naacl-srw.37/",
    doi = "10.18653/v1/2025.naacl-srw.37",
    pages = "381--391",
    ISBN = "979-8-89176-192-6"
}

@misc{zhang2024llavanextvideo,
  title={LLaVA-NeXT: A Strong Zero-shot Video Understanding Model},
  url={https://llava-vl.github.io/blog/2024-04-30-llava-next-video/},
  author={Zhang, Yuanhan and Li, Bo and Liu, haotian and Lee, Yong jae and Gui, Liangke and Fu, Di and Feng, Jiashi and Liu, Ziwei and Li, Chunyuan},
  month={April},
  year={2024}
}

@inproceedings{reimers-2020-multilingual-sentence-bert,
    title = "Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2020",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/2004.09813",
}

@misc{wu2019detectron2,
  author =       {Yuxin Wu and Alexander Kirillov and Francisco Massa and
                  Wan-Yen Lo and Ross Girshick},
  title =        {Detectron2},
  howpublished = {\url{https://github.com/facebookresearch/detectron2}},
  year =         {2019}
}

@article{huang2023laser,
  title={Laser: A neuro-symbolic framework for learning spatial-temporal scene graphs with weak supervision},
  author={Huang, Jiani and Li, Ziyang and Naik, Mayur and Lim, Ser-Nam},
  journal={arXiv preprint arXiv:2304.07647},
  year={2023}
}

@inproceedings{msvd,
  title={Collecting highly parallel data for paraphrase evaluation},
  author={Chen, David and Dolan, William B},
  booktitle={Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2011}
}

@inproceedings{vatex,
  title={Vatex: A large-scale, high-quality multilingual dataset for video-and-language research},
  author={Wang, Xin and Wu, Jiawei and Chen, Junkun and Li, Lei and Wang, Yuan-Fang and Wang, William Yang},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={4581--4591},
  year={2019}
}

@article{lsmdc,
  title={Movie description},
  author={Rohrbach, Anna and Torabi, Atousa and Rohrbach, Marcus and Tandon, Niket and Pal, Christopher and Larochelle, Hugo and Courville, Aaron and Schiele, Bernt},
  journal={International Journal of Computer Vision},
  volume={123},
  number={1},
  pages={94--120},
  year={2017},
  publisher={Springer}
}

@inproceedings{krishna2017dense,
    title={Dense-Captioning Events in Videos},
    author={Krishna, Ranjay and Hata, Kenji and Ren, Frederic and Fei-Fei, Li and Niebles, Juan Carlos},
    booktitle={International Conference on Computer Vision (ICCV)},
    year={2017}
}

@inproceedings{fainekos2012verification,
  title={Verification of automotive control applications using s-taliro},
  author={Fainekos, Georgios E and Sankaranarayanan, Sriram and Ueda, Koichi and Yazarel, Hakan},
  booktitle={2012 American Control Conference (ACC)},
  pages={3567--3572},
  year={2012},
  organization={IEEE}
}

@article{luo2021clip4clip,
  title={Clip4clip: An empirical study of clip for end to end video clip retrieval},
  author={Luo, Huaishao and Ji, Lei and Zhong, Ming and Chen, Yang and Lei, Wen and Duan, Nan and Li, Tianrui},
  journal={arXiv preprint arXiv:2104.08860},
  year={2021}
}

@InProceedings{Bain21,
  author       = "Max Bain and Arsha Nagrani and G{\"u}l Varol and Andrew Zisserman",
  title        = "Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval",
  booktitle    = "IEEE International Conference on Computer Vision",
  year         = "2021",
}

@inproceedings{liu2022ts2,
  title={Ts2-net: Token shift and selection transformer for text-video retrieval},
  author={Liu, Yuqi and Xiong, Pengfei and Xu, Luhui and Cao, Shengming and Jin, Qin},
  booktitle={European conference on computer vision},
  pages={319--335},
  year={2022},
  organization={Springer}
}

@article{tewel2022zero,
  title={Zero-shot video captioning with evolving pseudo-tokens},
  author={Tewel, Yoad and Shalev, Yoav and Nadler, Roy and Schwartz, Idan and Wolf, Lior},
  journal={arXiv preprint arXiv:2207.11100},
  year={2022}
}

@inproceedings{ltlf, author = {De Giacomo, Giuseppe and Vardi, Moshe Y.}, title = {Linear temporal logic and linear dynamic logic on finite traces}, year = {2013}, isbn = {9781577356332}, publisher = {AAAI Press}, abstract = {In this paper we look into the assumption of interpreting LTL over finite traces. In particular we show that LTLf, i.e., LTL under this assumption, is less expressive than what might appear at first sight, and that at essentially no computational cost one can make a significant increase in expressiveness while maintaining the same intuitiveness of LTLf. Indeed, we propose a logic, LDLf for Linear Dynamic Logic over finite traces, which borrows the syntax from Propositional Dynamic Logic (PDL), but is interpreted over finite traces. Satisfiability, validity and logical implication (as well as model checking) for LTLf. are PSPACE-complete as for LTLf. (and LTL).}, booktitle = {Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence}, pages = {854–860}, numpages = {7}, location = {Beijing, China}, series = {IJCAI '13} }