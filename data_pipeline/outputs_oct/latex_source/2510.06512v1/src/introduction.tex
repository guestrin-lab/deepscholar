\section{Introduction}

Detecting complex temporal events in unstructured data sequences such as videos and audio clips is important in several domains.
For instance, traffic surveillance systems 
need to
\textit{match} scenes perceived by autonomous vehicles 
against critical \textit{temporal} properties such as 
"the vehicle \textit{always} remains in a given lane". 
Similarly, search engines 
might need to
\textit{rank} videos or audio clips by relevance to temporal scenes ("a 10 second scene where a person \textit{eventually} starts running" or "a 20-30 second segment where speaker A sounds sad and B sounds frustrated \textit{until} both sound neutral").

\input{graphics/charts/example}

Recent work on temporal event detection in videos~\citep{yang2023specification,Choi2024TowardsNV}
has focused on using neural detection models such as YOLO~\citep{yolo}
to detect objects (for example, "car") in individual video frames,
and employing off-the-shelf model checkers such as STORM~\citep{storm} 
to verify if the sequence of detection scores satisfies 
a temporal property.
Inspired by these works, we introduce the problem 
of lifting scores for local properties to \textit{Scores for TempOral Properties} (STOPs) 
over sequences. Concretely, 

\begin{center}
\textit{Given a temporal property and (potentially noisy) predictors for local properties, how can we assign a score for a sequence expressing the temporal property?}    
\end{center}

The sequences and local properties could correspond to arbitrary modalities and classes of interest, including objects or actions in videos, and speakers or emotions in audio clips. These STOPs are useful for several downstream applications such as
\textbf{query matching} 
, i.e., checking if the scores 
are over a threshold to decide if the sequence expresses a temporal property, 
and
\textbf{ranked retrieval}, i.e.,
ranking sequences
against a temporal query by these scores to provide the top-k most relevant results. 

We argue that Linear Temporal Logic, with temporal operators such as "Always" (\(\BoxOp{I}\)) and "Until" (\(\UntilOp{I}\)), provides a suitable language
for expressing diverse
temporal properties of interest.
For example, 
the property "A and B sound happy until A sounds sad" can be written in LTL
as \((\mathsf{{happy}_A} \land \mathsf{{happy}_B}) \UntilOp{I} \mathsf{{sad}_A}\).
For temporal properties in LTL, the framework proposed by \citet{yang2023specification} can be used as a solution to the STOP problem with two major caveats. 
Firstly, this approach requires \(\mathcal{O}(T \cdot 2^{|C|})\) space and time for a sequence of length \(T\) and \(|C|\) local properties.
This exponential space and time complexity renders this approach inefficient for applications such as retrieval where sequences from a large database (with potentially many local properties) need to be checked.
Secondly, this approach has no provision to handle incorrectly low or high, i.e., \textit{potentially noisy}, scores for local properties.

We propose a novel scoring function called LogSTOP, inspired by quantitative semantics for LTL~\citep{FAINEKOS20094262},
that 
assigns a score for a sequence of length \(T\) satisfying temporal property \(\varphi\)
in \(\mathcal{O}(T \cdot |\varphi|)\) time and space.
LogSTOP employs a simple downsampling and smoothing strategy for handling locally incorrect
predictions by the local property predictors. 
This makes it robust to cases where, for example, an object detector detects a car with very low scores in some frames because it is occluded.
Moreover, the linear time computational complexity makes it an efficient solution for applications such as query matching and ranked retrieval (see example in Figure~\ref{fig:method_overview}). 
For query matching, we propose a length and query-adaptive threshold which is guaranteed to accept at least as many sequences as a fixed threshold. 
We also demonstrate how LogSTOP can be used to rank sequences based on subsequence relevance to temporal queries in \(\mathcal{O}(T^2 \cdot |\varphi|)\) time. 

We evaluate LogSTOP on query matching and ranked retrieval,
with sequence modalities including videos and speech, and local properties such as objects, actions, and emotions.
We focus on 15 diverse temporal property templates of varying complexity. 
Since no existing benchmarks support this breadth of temporal properties 
and sequence types, we propose two new benchmarks:
the QMTP (Query Matching for Temporal Properties) benchmark
for objects-in-videos from the RealTLV dataset~\citep{Choi2024TowardsNV},
and emotions-in-speech from the IEMOCAP dataset~\citep{busso2008iemocap};
and the TP2VR (Temporal Property to Video Retrieval) benchmark
for objects-in-videos from the RealTLV dataset,
and actions-in-videos from the AVA dataset~\citep{gu2018ava}.

We find that LogSTOP with simple detection models,
such as YOLO and HuBERT~\citep{hsu2021hubert,superb},
outperforms baselines including Large Vision / Audio Language Models (LVLMs / LALMs), and
NSVS-TL
, on query matching by more than \(16\%\) in terms of balanced accuracy.
Similarly, 
LogSTOP with Grounding DINO~\citep{liu2024grounding} 
and SlowR50~\citep{feichtenhofer2019slowfast} outperforms 
zero-shot text-to-video retrieval methods, such as mPLUG~\citep{li2022mplug}
and text-text similarity with video captions, 
by more than \(19\%\) and \(16\%\) in terms of mean average precision and recall respectively.