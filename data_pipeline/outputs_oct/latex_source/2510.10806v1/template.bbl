\begin{thebibliography}{22}

\bibitem{jin2024large}
Bowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji, and Jiawei Han.
\newblock Large language models on graphs: A comprehensive survey.
\newblock {\em IEEE Transactions on Knowledge and Data Engineering}, 2024.

\bibitem{xypolopoulos2024graph}
Christos Xypolopoulos, Guokan Shang, Xiao Fei, Giannis Nikolentzos, Hadi Abdine, Iakovos Evdaimon, Michail Chatzianastasis, Giorgos Stamou, and Michalis Vazirgiannis.
\newblock Graph linearization methods for reasoning on graphs with large language models.
\newblock {\em arXiv preprint arXiv:2410.19494}, 2024.

\bibitem{wang2023can}
Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and Yulia Tsvetkov.
\newblock Can language models solve graph problems in natural language?
\newblock In {\em Advances in Neural Information Processing Systems}, volume~36, pages 30840--30861, 2023.

\bibitem{sarthi2024raptor}
Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher~D. Manning.
\newblock Raptor: Recursive abstractive processing for tree-organized retrieval.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2024.

\bibitem{liu2024graphcoder}
Wei Liu, Ailun Yu, Daoguang Zan, Bo Shen, Wei Zhang, Haiyan Zhao, Zhi Jin, and Qianxiang Wang.
\newblock Graphcoder: Enhancing repository-level code completion via code context graph-based retrieval and language model.
\newblock {\em arXiv preprint arXiv:2406.07003}, 2024.

\bibitem{fatemi2023talk}
Bahare Fatemi, Jonathan Halcrow, and Bryan Perozzi.
\newblock Talk like a graph: Encoding graphs for large language models.
\newblock {\em arXiv preprint arXiv:2310.04560}, 2023.

\bibitem{li2024can}
Xin Li, Weize Chen, Qizhi Chu, Haopeng Li, Zhaojun Sun, Ran Li, Chen Qian, Yiwei Wei, Chuan Shi, Zhiyuan Liu, et~al.
\newblock Can large language models analyze graphs like professionals? a benchmark, datasets and models.
\newblock In {\em Advances in Neural Information Processing Systems}, volume~37, pages 141045--141070, 2024.

\bibitem{he2024g}
Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, and Bryan Hooi.
\newblock G-retriever: Retrieval-augmented generation for textual graph understanding and question answering.
\newblock In {\em Advances in Neural Information Processing Systems}, volume~37, pages 132876--132907, 2024.

\bibitem{li2024simple}
Mufei Li, Siqi Miao, and Pan Li.
\newblock Simple is effective: The roles of graphs and large language models in knowledge-graph-based retrieval-augmented generation.
\newblock {\em arXiv preprint arXiv:2410.20724}, 2024.

\bibitem{edge2024local}
Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert~Osazuwa Ness, and Jonathan Larson.
\newblock From local to global: A graph rag approach to query-focused summarization.
\newblock {\em arXiv preprint arXiv:2404.16130}, 2024.

\bibitem{levy2025more}
Shahar Levy, Nir Mazor, Lihi Shalmon, Michael Hassid, and Gabriel Stanovsky.
\newblock More documents, same length: Isolating the challenge of multiple documents in rag.
\newblock {\em arXiv preprint arXiv:2503.04388}, 2025.

\bibitem{warfield2024vector}
Daniel Warfield and Benjamin Fletcher.
\newblock Do vector databases lose accuracy at scale?
\newblock {\em EyeLevel.ai Blog}, October 2024.
\newblock URL: \url{https://www.eyelevel.ai/post/do-vector-databases-lose-accuracy-at-scale}.

\bibitem{jiang2024longrag}
Ziyan Jiang, Xueguang Ma, and Wenhu Chen.
\newblock Longrag: Enhancing retrieval-augmented generation with long-context llms.
\newblock {\em arXiv preprint arXiv:2406.15319}, 2024.

\bibitem{deng2023implicit}
Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart Shieber.
\newblock Implicit chain of thought reasoning via knowledge distillation.
\newblock {\em arXiv preprint arXiv:2311.01460}, 2023.

\bibitem{wang2023explicit}
Yuzheng Wang, Zuhao Ge, Zhaoyu Chen, Xian Liu, Chuangjia Ma, Yunquan Sun, and Lizhe Qi.
\newblock Explicit and implicit knowledge distillation via unlabeled data.
\newblock In {\em ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pages 1--5. IEEE, 2023.

\bibitem{li2024direct}
Yixing Li, Yuxian Gu, Li Dong, Dequan Wang, Yu Cheng, and Furu Wei.
\newblock Direct preference knowledge distillation for large language models.
\newblock {\em arXiv preprint arXiv:2406.19774}, 2024.

\bibitem{dong2022survey}
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Tianyu Liu, et~al.
\newblock A survey on in-context learning.
\newblock {\em arXiv preprint arXiv:2301.00234}, 2022.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock In {\em Advances in Neural Information Processing Systems}, volume~33, pages 1877--1901, 2020.

\bibitem{gao2023retrieval}
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, and Haofen Wang.
\newblock Retrieval-augmented generation for large language models: A survey.
\newblock {\em arXiv preprint arXiv:2312.10997}, 2023.

\bibitem{ren2024survey}
Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh Chawla, and Chao Huang.
\newblock A survey of large language models for graphs.
\newblock In {\em Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages 6616--6626, 2024.

\bibitem{shi2023large}
Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed~H. Chi, Nathanael Sch{\"a}rli, and Denny Zhou.
\newblock Large language models can be easily distracted by irrelevant context.
\newblock In {\em International Conference on Machine Learning}, pages 31210--31227. PMLR, 2023.

\end{thebibliography}
