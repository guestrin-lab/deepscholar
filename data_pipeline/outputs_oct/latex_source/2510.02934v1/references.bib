@misc{website, 
title = {Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection},
author={Vu, Trong Thanh and Bui, Tuan-Dung and Nguyen, Thu-Trang and Nguyen, Son and Vo, Dinh Hieu},
url={https://github.com/iSE-UET-VNU/AUTOPROBE}
}



@article{minaee2024large,
  title={Large language models: A survey},
  author={Minaee, Shervin and Mikolov, Tomas and Nikzad, Narjes and Chenaghlu, Meysam and Socher, Richard and Amatriain, Xavier and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2402.06196},
  year={2024}
}

@article{floridi2020gpt,
  title={GPT-3: Its nature, scope, limits, and consequences},
  author={Floridi, Luciano and Chiriatti, Massimo},
  journal={Minds and machines},
  volume={30},
  number={4},
  pages={681--694},
  year={2020},
  publisher={Springer}
}

@article{anil2023palm,
  title={Palm 2 technical report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:2305.10403},
  year={2023}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@misc{deepseek-coder,
  author = {DeepSeek},
  title = {DeepSeek Coder: Let the Code Write Itself},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/deepseek-ai/DeepSeek-Coder}},
}
@article{codellama,
  title={Code llama: Open foundation models for code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\'e}r{\'e}my and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@article{magicoder,
  title={Magicoder: Source code is all you need},
  author={Wei, Yuxiang and Wang, Zhe and Liu, Jiawei and Ding, Yifeng and Zhang, Lingming},
  journal={arXiv preprint arXiv:2312.02120},
  year={2023}
}


@article{ji2024zero,
  title={Zero-shot prompting for llm-based machine translation using in-domain target sentences},
  author={Ji, Baijun and Duan, Xiangyu and Zhang, Yue and Wu, Kaixin and Zhang, Min},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year={2024},
  publisher={IEEE}
}

@article{zhang2025systematic,
  title={A systematic survey of text summarization: From statistical methods to large language models},
  author={Zhang, Haopeng and Yu, Philip S and Zhang, Jiawei},
  journal={ACM Computing Surveys},
  volume={57},
  number={11},
  pages={1--41},
  year={2025},
  publisher={ACM New York, NY}
}

@inproceedings{li2024flexkbqa,
  title={Flexkbqa: A flexible llm-powered framework for few-shot knowledge base question answering},
  author={Li, Zhenyu and Fan, Sunqi and Gu, Yu and Li, Xiuxing and Duan, Zhichao and Dong, Bowen and Liu, Ning and Wang, Jianyong},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={38},
  number={17},
  pages={18608--18616},
  year={2024}
}

@article{zheng2025towards,
  title={Towards an understanding of large language models in software engineering tasks},
  author={Zheng, Zibin and Ning, Kaiwen and Zhong, Qingyuan and Chen, Jiachi and Chen, Wenqing and Guo, Lianghong and Wang, Weicheng and Wang, Yanlin},
  journal={Empirical Software Engineering},
  volume={30},
  number={2},
  pages={50},
  year={2025},
  publisher={Springer}
}

@article{wang2025can,
  title={Can llms replace human evaluators? an empirical study of llm-as-a-judge in software engineering},
  author={Wang, Ruiqi and Guo, Jiyu and Gao, Cuiyun and Fan, Guodong and Chong, Chun Yong and Xia, Xin},
  journal={Proceedings of the ACM on Software Engineering},
  volume={2},
  number={ISSTA},
  pages={1955--1977},
  year={2025},
  publisher={ACM New York, NY, USA}
}

@article{bui2024rambo,
  title={Rambo: Enhancing rag-based repository-level method body completion},
  author={Bui, Tuan-Dung and Luu-Van, Duc-Thieu and Nguyen, Thanh-Phat and Nguyen, Thu-Trang and Nguyen, Son and Vo, Hieu Dinh},
  journal={arXiv preprint arXiv:2409.15204},
  year={2024}
}

@inproceedings{chen2024chatunitest,
  title={Chatunitest: A framework for llm-based test generation},
  author={Chen, Yinghao and Hu, Zehao and Zhi, Chen and Han, Junxiao and Deng, Shuiguang and Yin, Jianwei},
  booktitle={Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
  pages={572--576},
  year={2024}
}

@article{openia,
title = {Correctness assessment of code generated by Large Language Models using internal representations},
journal = {Journal of Systems and Software},
volume = {230},
pages = {112570},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2025.112570},

}

@article{zhang2025llm,
  title={Llm hallucinations in practical code generation: Phenomena, mechanism, and mitigation},
  author={Zhang, Ziyao and Wang, Chong and Wang, Yanlin and Shi, Ensheng and Ma, Yuchi and Zhong, Wanjun and Chen, Jiachi and Mao, Mingzhi and Zheng, Zibin},
  journal={Proceedings of the ACM on Software Engineering},
  volume={2},
  number={ISSTA},
  pages={481--503},
  year={2025},
  publisher={ACM New York, NY, USA}
}

@article{llmcheck,
  title={Llm-check: Investigating detection of hallucinations in large language models},
  author={Sriramanan, Gaurang and Bharti, Siddhant and Sadasivan, Vinu Sankar and Saha, Shoumik and Kattakinda, Priyatham and Feizi, Soheil},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={34188--34216},
  year={2024}
}

@article{inside,
  title={INSIDE: LLMs' internal states retain the power of hallucination detection},
  author={Chen, Chao and Liu, Kai and Chen, Ze and Gu, Yi and Wu, Yue and Tao, Mingyuan and Fu, Zhihang and Ye, Jieping},
  journal={arXiv preprint arXiv:2402.03744},
  year={2024}
}

@article{zhang2025icr,
  title={ICR Probe: Tracking Hidden State Dynamics for Reliable Hallucination Detection in LLMs},
  author={Zhang, Zhenliang and Hu, Xinyu and Zhang, Huixuan and Zhang, Junzhe and Wan, Xiaojun},
  journal={arXiv preprint arXiv:2507.16488},
  year={2025}
}

@inproceedings{al2011software,
  title={Software quality models: A comparative study},
  author={Al-Badareen, Anas Bassam and Selamat, Mohd Hasan and A. Jabar, Marzanah and Din, Jamilah and Turaev, Sherzod},
  booktitle={International Conference on Software Engineering and Computer Systems},
  pages={46--55},
  year={2011},
  organization={Springer}
}

@article{HumanEval,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}
@article{MBPP,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{jin2024exploring,
  title={Exploring Concept Depth: How Large Language Models Acquire Knowledge at Different Layers?},
  author={Jin, Mingyu and Yu, Qinkai and Huang, Jingyuan and Zeng, Qingcheng and Wang, Zhenting and Hua, Wenyue and Zhao, Haiyan and Mei, Kai and Meng, Yanda and Ding, Kaize and others},
  journal={arXiv preprint arXiv:2404.07066},
  year={2024}
}

@article{song2024layer,
  title={Layer Importance and Hallucination Analysis in Large Language Models via Enhanced Activation Variance-Sparsity},
  author={Song, Zichen and Huang, Sitan and Wu, Yuxin and Kang, Zhongfeng},
  journal={arXiv preprint arXiv:2411.10069},
  year={2024}
}

@inproceedings{snyder2024early,
  title={On early detection of hallucinations in factual question answering},
  author={Snyder, Ben and Moisescu, Marius and Zafar, Muhammad Bilal},
  booktitle={Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={2721--2732},
  year={2024}
}

@inproceedings{slobodkin2023curious,
  title={The curious case of hallucinatory (un) answerability: Finding truths in the hidden states of over-confident large language models},
  author={Slobodkin, Aviv and Goldman, Omer and Caciularu, Avi and Dagan, Ido and Ravfogel, Shauli},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={3607--3625},
  year={2023}
}

@article{codegemma,
  title={Codegemma: Open code models based on gemma},
  author={Team, CodeGemma and Zhao, Heri and Hui, Jeffrey and Howland, Joshua and Nguyen, Nam and Zuo, Siqi and Hu, Andrea and Choquette-Choo, Christopher A and Shen, Jingyue and Kelley, Joe and others},
  journal={arXiv preprint arXiv:2406.11409},
  year={2024}
}

@inproceedings{li-etal-2024-deveval,
    title = "{D}ev{E}val: A Manually-Annotated Code Generation Benchmark Aligned with Real-World Code Repositories",
    author = "Li, Jia  and
      Li, Ge  and
      Zhao, Yunfei  and
      Li, Yongmin  and
      Liu, Huanyu  and
      Zhu, Hao  and
      Wang, Lecheng  and
      Liu, Kaibo  and
      Fang, Zheng  and
      Wang, Lanshen  and
      Ding, Jiazheng  and
      Zhang, Xuanming  and
      Zhu, Yuqi  and
      Dong, Yihong  and
      Jin, Zhi  and
      Li, Binhua  and
      Huang, Fei  and
      Li, Yongbin  and
      Gu, Bin  and
      Yang, Mengfei",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.214/",
    doi = "10.18653/v1/2024.findings-acl.214",
    pages = "3603--3614",
}

@inproceedings{SecurityEval,
  title={SecurityEval dataset: mining vulnerability examples to evaluate machine learning-based code generation techniques},
  author={Siddiq, Mohammed Latif and Santos, Joanna CS},
  booktitle={Proceedings of the 1st International Workshop on Mining Software Repositories Applications for Privacy and Security},
  pages={29--33},
  year={2022}
}

@inproceedings{Cweval,
  title={Cweval: Outcome-driven evaluation on functionality and security of llm code generation},
  author={Peng, Jinjun and Cui, Leyi and Huang, Kele and Yang, Junfeng and Ray, Baishakhi},
  booktitle={2025 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code)},
  pages={33--40},
  year={2025},
  organization={IEEE}
}

@article{CODEGUARD+,
  title={Constrained decoding for secure code generation},
  author={Fu, Yanjun and Baker, Ethan and Ding, Yu and Chen, Yizheng},
  journal={arXiv preprint arXiv:2405.00218},
  year={2024}
}

@inproceedings{Sallm,
  title={Sallm: Security assessment of generated code},
  author={Siddiq, Mohammed Latif and da Silva Santos, Joanna Cecilia and Devareddy, Sajith and Muller, Anna},
  booktitle={Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering Workshops},
  pages={54--65},
  year={2024}
}

@misc{bandit, 
title = {Bandit},
url={https://bandit.readthedocs.io/en/latest/}
}

@misc{codeql, 
title = {CodeQL},
url={https://codeql.github.com}
}

@inproceedings{lookback,
  title={Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps},
  author={Chuang, Yung-Sung and Qiu, Linlu and Hsieh, Cheng-Yu and Krishna, Ranjay and Kim, Yoon and Glass, James},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={1419--1436},
  year={2024}
}

@article{embedding_emse22,
  title={Can pre-trained code embeddings improve model performance? Revisiting the use of code embeddings in software engineering tasks},
  author={Ding, Zishuo and Li, Heng and Shang, Weiyi and Chen, Tse-Hsun Peter},
  journal={Empirical Software Engineering},
  volume={27},
  number={3},
  pages={1--38},
  year={2022},
  publisher={Springer}
}

@inproceedings{opt-pretrained-model,
  title={Coding-PTMs: How to Find Optimal Code Pre-trained Models for Code Embedding in Vulnerability Detection?},
  author={Zhao, Yu and Gong, Lina and Huang, Zhiqiu and Wang, Yongwei and Wei, Mingqiang and Wu, Fei},
  booktitle={Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
  pages={1732--1744},
  year={2024}
}
@inproceedings{pretrained-survey,
  title     = {Deep Learning Meets Software Engineering: A Survey on Pre-Trained Models of Source Code},
  author    = {Niu, Changan and Li, Chuanyi and Luo, Bin and Ng, Vincent},
  booktitle = {Proceedings of the Thirty-First International Joint Conference on
               Artificial Intelligence, {IJCAI-22}},
  pages     = {5546--5555},
  year      = {2022}
}

@inproceedings{cubert,
author = {Kanade, Aditya and Maniatis, Petros and Balakrishnan, Gogul and Shi, Kensen},
title = {Learning and Evaluating Contextual Embedding of Source Code},
year = {2020},
publisher = {JMLR.org},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {474},
numpages = {12},
series = {ICML'20}
}
@inproceedings{codebert,
    title = "{C}ode{BERT}: A Pre-Trained Model for Programming and Natural Languages",
    author = "Feng, Zhangyin  and
      Guo, Daya  and
      Tang, Duyu  and
      Duan, Nan  and
      Feng, Xiaocheng  and
      Gong, Ming  and
      Shou, Linjun  and
      Qin, Bing  and
      Liu, Ting  and
      Jiang, Daxin  and
      Zhou, Ming",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    pages = "1536--1547"
}

@inproceedings{codet5,
  title={CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation},
  author={Wang, Yue and Wang, Weishi and Joty, Shafiq and Hoi, Steven CH},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={8696--8708},
  year={2021}
}
@inproceedings{codet5+,
  title={CodeT5+: Open Code Large Language Models for Code Understanding and Generation},
  author={Wang, Yue and Le, Hung and Gotmare, Akhilesh and Bui, Nghi and Li, Junnan and Hoi, Steven},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={1069--1088},
  year={2023}
}

@inproceedings{internal-state-2,
  title={LLM Internal States Reveal Hallucination Risk Faced With a Query},
  author={Ji, Ziwei and Chen, Delong and Ishii, Etsuko and Cahyawijaya, Samuel and Bang, Yejin and Wilie, Bryan and Fung, Pascale},
  booktitle={Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP},
  pages={88--104},
  year={2024}
}

@article{rambo,
  title={RAMBO: Enhancing RAG-based Repository-Level Method Body Completion},
  author={Bui, Tuan-Dung and Luu-Van, Duc-Thieu and Nguyen, Thanh-Phat and Nguyen, Thu-Trang and Nguyen, Son and Vo, Hieu Dinh},
  journal={arXiv preprint arXiv:2409.15204},
  year={2024}
}

@inproceedings{zhang2023repocoder,
  title={RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation},
  author={Zhang, Fengji and Chen, Bei and Zhang, Yue and Keung, Jacky and Liu, Jin and Zan, Daoguang and Mao, Yi and Lou, Jian-Guang and Chen, Weizhu},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={2471--2484},
  year={2023}
}

@inproceedings{wei2023empirical,
  title={Empirical study of llm fine-tuning for text classification in legal document review},
  author={Wei, Fusheng and Keeling, Robert and Huber-Fliflet, Nathaniel and Zhang, Jianping and Dabrowski, Adam and Yang, Jingchao and Mao, Qiang and Qin, Han},
  booktitle={2023 IEEE international conference on big data (BigData)},
  pages={2786--2792},
  year={2023},
  organization={IEEE}
}

@inproceedings{zhang2025teleclass,
  title={Teleclass: Taxonomy enrichment and llm-enhanced hierarchical text classification with minimal supervision},
  author={Zhang, Yunyi and Yang, Ruozhen and Xu, Xueqiang and Li, Rui and Xiao, Jinfeng and Shen, Jiaming and Han, Jiawei},
  booktitle={Proceedings of the ACM on Web Conference 2025},
  pages={2032--2042},
  year={2025}
}

@article{zhang2024vision,
  title={Vision-language models for vision tasks: A survey},
  author={Zhang, Jingyi and Huang, Jiaxing and Jin, Sheng and Lu, Shijian},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={46},
  number={8},
  pages={5625--5644},
  year={2024},
  publisher={IEEE}
}

@inproceedings{chen2024vitamin,
  title={Vitamin: Designing scalable vision models in the vision-language era},
  author={Chen, Jieneng and Yu, Qihang and Shen, Xiaohui and Yuille, Alan and Chen, Liang-Chieh},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12954--12966},
  year={2024}
}

@article{mu2024clarifygpt,
  title={Clarifygpt: A framework for enhancing llm-based code generation via requirements clarification},
  author={Mu, Fangwen and Shi, Lin and Wang, Song and Yu, Zhuohao and Zhang, Binquan and Wang, ChenXue and Liu, Shichao and Wang, Qing},
  journal={Proceedings of the ACM on Software Engineering},
  volume={1},
  number={FSE},
  pages={2332--2354},
  year={2024},
  publisher={ACM New York, NY, USA}
}


@inproceedings{wang2024hits,
  title={Hits: High-coverage llm-based unit test generation via method slicing},
  author={Wang, Zejun and Liu, Kaibo and Li, Ge and Jin, Zhi},
  booktitle={Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
  pages={1258--1268},
  year={2024}
}

@inproceedings{sun2024source,
  title={Source Code Summarization in the Era of Large Language Models},
  author={Sun, Weisong and Miao, Yun and Li, Yuekang and Zhang, Hongyu and Fang, Chunrong and Liu, Yi and Deng, Gelei and Liu, Yang and Chen, Zhenyu},
  booktitle={2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)},
  pages={419--431},
  year={2024},
  organization={IEEE Computer Society}
}

@inproceedings{jin2023inferfix,
  title={Inferfix: End-to-end program repair with llms},
  author={Jin, Matthew and Shahriar, Syed and Tufano, Michele and Shi, Xin and Lu, Shuai and Sundaresan, Neel and Svyatkovskiy, Alexey},
  booktitle={Proceedings of the 31st ACM joint european software engineering conference and symposium on the foundations of software engineering},
  pages={1646--1656},
  year={2023}
}

@misc{github_copilot, 
title = {GitHub Copilot},
url={https://github.com/features/copilot}
}

@misc{codegeex2, 
title = {CodeGeeX2},
url={https://github.com/zai-org/CodeGeeX2}
}


@article{liu2024exploring,
  title={Exploring and evaluating hallucinations in llm-powered code generation},
  author={Liu, Fang and Liu, Yang and Shi, Lin and Huang, Houkun and Wang, Ruifeng and Yang, Zhen and Zhang, Li and Li, Zhongqi and Ma, Yuchi},
  journal={arXiv preprint arXiv:2404.00971},
  year={2024}
}

@article{liu2023your,
  title={Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation},
  author={Liu, Jiawei and Xia, Chunqiu Steven and Wang, Yuyao and Zhang, Lingming},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={21558--21572},
  year={2023}
}

@inproceedings{wang2025towards,
  title={Towards understanding the characteristics of code generation errors made by large language models},
  author={Wang, Zhijie and Zhou, Zijie and Song, Da and Huang, Yuheng and Chen, Shengmai and Ma, Lei and Zhang, Tianyi},
  booktitle={2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)},
  pages={717--717},
  year={2025},
  organization={IEEE Computer Society}
}

@inproceedings{manakul2023selfcheckgpt,
  title={SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models},
  author={Manakul, Potsawee and Liusie, Adian and Gales, Mark},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={9004--9017},
  year={2023}
}

@article{huang2025survey,
  title={A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions},
  author={Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and others},
  journal={ACM Transactions on Information Systems},
  volume={43},
  number={2},
  pages={1--55},
  year={2025},
  publisher={ACM New York, NY}
}

@article{farquhar2024detecting,
  title={Detecting hallucinations in large language models using semantic entropy},
  author={Farquhar, Sebastian and Kossen, Jannik and Kuhn, Lorenz and Gal, Yarin},
  journal={Nature},
  volume={630},
  number={8017},
  pages={625--630},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{augenstein2024factuality,
  title={Factuality challenges in the era of large language models and opportunities for fact-checking},
  author={Augenstein, Isabelle and Baldwin, Timothy and Cha, Meeyoung and Chakraborty, Tanmoy and Ciampaglia, Giovanni Luca and Corney, David and DiResta, Renee and Ferrara, Emilio and Hale, Scott and Halevy, Alon and others},
  journal={Nature Machine Intelligence},
  volume={6},
  number={8},
  pages={852--863},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{huang2023look,
  title={Look before you leap: An exploratory study of uncertainty measurement for large language models},
  author={Huang, Yuheng and Song, Jiayang and Wang, Zhijie and Zhao, Shengming and Chen, Huaming and Juefei-Xu, Felix and Ma, Lei},
  journal={arXiv preprint arXiv:2307.10236},
  year={2023}
}

@inproceedings{zhang2024self,
  title={Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation},
  author={Zhang, Xiaoying and Peng, Baolin and Tian, Ye and Zhou, Jingyan and Jin, Lifeng and Song, Linfeng and Mi, Haitao and Meng, Helen},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1946--1965},
  year={2024}
}

@article{zhang2024knowhalu,
  title={Knowhalu: Hallucination detection via multi-form knowledge based factual checking},
  author={Zhang, Jiawei and Xu, Chejian and Gai, Yu and Lecue, Freddy and Song, Dawn and Li, Bo},
  journal={arXiv preprint arXiv:2404.02935},
  year={2024}
}

@inproceedings{hu2024knowledge,
  title={Knowledge-centric hallucination detection},
  author={Hu, Xiangkun and Ru, Dongyu and Qiu, Lin and Guo, Qipeng and Zhang, Tianhang and Xu, Yang and Luo, Yun and Liu, Pengfei and Zhang, Yue and Zhang, Zheng},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={6953--6975},
  year={2024}
}

@inproceedings{zhou2021detecting,
  title={Detecting Hallucinated Content in Conditional Neural Sequence Generation},
  author={Zhou, Chunting and Neubig, Graham and Gu, Jiatao and Diab, Mona and Guzm{\'a}n, Francisco and Zettlemoyer, Luke and Ghazvininejad, Marjan},
  booktitle={Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  pages={1393--1404},
  year={2021}
}

@article{inner-working,
  title={A primer on the inner workings of transformer-based language models},
  author={Ferrando, Javier and Sarti, Gabriele and Bisazza, Arianna and Costa-juss{\`a}, Marta R},
  journal={arXiv preprint arXiv:2405.00208},
  year={2024}
}

@article{probing,
  title={Probing the decision boundaries of in-context learning in large language models},
  author={Zhao, Siyan and Nguyen, Tung and Grover, Aditya},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={130408--130432},
  year={2024}
}

@inproceedings{internal-state,
  title={The Internal State of an LLM Knows When It’s Lying},
  author={Azaria, Amos and Mitchell, Tom},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={967--976},
  year={2023}
}

@inproceedings{factoscope,
  title={Llm factoscope: Uncovering llms’ factual discernment through measuring inner states},
  author={He, Jinwen and Gong, Yujia and Lin, Zijin and Zhao, Yue and Chen, Kai and others},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={10218--10230},
  year={2024}
}

@inproceedings{croft2021empirical,
  title={An empirical study of rule-based and learning-based approaches for static application security testing},
  author={Croft, Roland and Newlands, Dominic and Chen, Ziyu and Babar, M Ali},
  booktitle={Proceedings of the 15th ACM/IEEE international symposium on empirical software engineering and measurement (ESEM)},
  pages={1--12},
  year={2021}
}

@inproceedings{ivdetect,
  title={Vulnerability detection with fine-grained interpretations},
  author={Li, Yi and Wang, Shaohua and Nguyen, Tien N},
  booktitle={Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages={292--303},
  year={2021}
}
@inproceedings{velvet,
  title={VELVET: a noVel Ensemble Learning approach to automatically locate VulnErable sTatements},
  author={Ding, Yangruibo and Suneja, Sahil and Zheng, Yunhui and Laredo, Jim and Morari, Alessandro and Kaiser, Gail and Ray, Baishakhi},
  booktitle={2022 IEEE International Conference on Software Analysis, Evolution and Reengineering},
  pages={959--970},
  year={2022},
  organization={IEEE}
}
@inproceedings{linevd,
  author    = {David Hin and
              Andrey Kan and
              Huaming Chen and
              Muhammad Ali Babar},
  title     = {LineVD: Statement-level Vulnerability Detection using Graph Neural
              Networks},
  booktitle = {{IEEE/ACM} 19th International Conference on Mining Software Repositories,
              {MSR} 2022, Pittsburgh, PA, USA, May 23-24, 2022},
  pages     = {596--607},
  publisher = {{IEEE}},
  year      = {2022}
}

@INPROCEEDINGS {linevul,
author = {M. Fu and C. Tantithamthavorn},
booktitle = {2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)},
title = {LineVul: A Transformer-based Line-Level Vulnerability Prediction},
year = {2022},
volume = {},
issn = {},
pages = {608-620},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {may}
}

@article{COSTA,
  title={Context-based statement-level vulnerability localization},
  author={Nguyen, Thu-Trang and Vo, Hieu Dinh},
  journal={Information and Software Technology},
  volume={169},
  pages={107406},
  year={2024},
  publisher={Elsevier}
}

@article{zhang2020graph,
  title={Graph-bert: Only attention is needed for learning graph representations},
  author={Zhang, Jiawei and Zhang, Haopeng and Xia, Congying and Sun, Li},
  journal={arXiv preprint arXiv:2001.05140},
  year={2020}
}

@inproceedings{lost-at-c,
  title={Lost at c: A user study on the security implications of large language model code assistants},
  author={Sandoval, Gustavo and Pearce, Hammond and Nys, Teo and Karri, Ramesh and Garg, Siddharth and Dolan-Gavitt, Brendan},
  booktitle={32nd USENIX Security Symposium (USENIX Security 23)},
  pages={2205--2222},
  year={2023}
}

@inproceedings{empirical-study-2,
  title={An empirical study of code generation errors made by large language models},
  author={Song, Da and Zhou, Zijie and Wang, Zhijie and Huang, Yuheng and Chen, Shengmai and Kou, Bonan and Ma, Lei and Zhang, Tianyi},
  booktitle={7th Annual Symposium on Machine Programming},
  year={2023}
}

@article{llm-gen-code-emp-study,
  title={Can We Trust Large Language Models Generated Code? A Framework for In-Context Learning, Security Patterns, and Code Evaluations Across Diverse LLMs},
  author={Mohsin, Ahmad and Janicke, Helge and Wood, Adrian and Sarker, Iqbal H and Maglaras, Leandros and Janjua, Naeem},
  journal={arXiv preprint arXiv:2406.12513},
  year={2024}
}

@article{calibration,
  title={Calibration and correctness of language models for code},
  author={Spiess, Claudio and Gros, David and Pai, Kunal Suresh and Pradel, Michael and Rabin, Md Rafiqul Islam and Alipour, Amin and Jha, Susmit and Devanbu, Prem and Ahmed, Toufique},
  journal={arXiv preprint arXiv:2402.02047},
  year={2024},
  publisher={Aug}
}

@article{vulnerabilities-copilot,
  title={Security weaknesses of copilot generated code in github},
  author={Fu, Yujia and Liang, Peng and Tahir, Amjed and Li, Zengyang and Shahin, Mojtaba and Yu, Jiaxin and Chen, Jinfu},
  journal={arXiv preprint arXiv:2310.02059},
  year={2023}
}

@article{llm-attention,
  title={Do large language models pay similar attention like human programmers when generating code?},
  author={Kou, Bonan and Chen, Shengmai and Wang, Zhijie and Ma, Lei and Zhang, Tianyi},
  journal={Proceedings of the ACM on Software Engineering},
  volume={1},
  number={FSE},
  pages={2261--2284},
  year={2024},
  publisher={ACM New York, NY, USA}
}

@article{autosafecoder,
  title={AutoSafeCoder: A Multi-Agent Framework for Securing LLM Code Generation through Static Analysis and Fuzz Testing},
  author={Nunez, Ana and Islam, Nafis Tanveer and Jha, Sumit Kumar and Najafirad, Peyman},
  journal={arXiv preprint arXiv:2409.10737},
  year={2024}
}

@inproceedings{llm-security-guard,
  title={LLM security guard for code},
  author={Kavian, Arya and Pourhashem Kallehbasti, Mohammad Mehdi and Kazemi, Sajjad and Firouzi, Ehsan and Ghafari, Mohammad},
  booktitle={Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
  pages={600--603},
  year={2024}
}

@article{dai2025comprehensive,
  title={A comprehensive study of llm secure code generation},
  author={Dai, Shih-Chieh and Xu, Jun and Tao, Guanhong},
  journal={arXiv preprint arXiv:2503.15554},
  year={2025}
}

@inproceedings{perry2023users,
  title={Do users write more insecure code with ai assistants?},
  author={Perry, Neil and Srivastava, Megha and Kumar, Deepak and Boneh, Dan},
  booktitle={Proceedings of the 2023 ACM SIGSAC conference on computer and communications security},
  pages={2785--2799},
  year={2023}
}

@article{liu2024no,
  title={No need to lift a finger anymore? assessing the quality of code generation by chatgpt},
  author={Liu, Zhijie and Tang, Yutian and Luo, Xiapu and Zhou, Yuming and Zhang, Liang Feng},
  journal={IEEE Transactions on Software Engineering},
  volume={50},
  number={6},
  pages={1548--1584},
  year={2024},
  publisher={IEEE}
}

@article{tihanyi2025secure,
  title={How secure is AI-generated code: A large-scale comparison of large language models},
  author={Tihanyi, Norbert and Bisztray, Tamas and Ferrag, Mohamed Amine and Jain, Ridhi and Cordeiro, Lucas C},
  journal={Empirical Software Engineering},
  volume={30},
  number={2},
  pages={47},
  year={2025},
  publisher={Springer}
}

@inproceedings{msr2023-prem-llm-code-bugs,
  title={Large language models and simple, stupid bugs},
  author={Jesse, Kevin and Ahmed, Toufique and Devanbu, Premkumar T and Morgan, Emily},
  booktitle={2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)},
  pages={563--575},
  year={2023},
  organization={IEEE}
}

@article{llm-code-bugs1,
  title={Bugs in large language models generated code},
  author={Tambon, Florian and Dakhel, Arghavan Moradi and Nikanjam, Amin and Khomh, Foutse and Desmarais, Michel C and Antoniol, Giuliano},
  journal={arXiv preprint arXiv:2403.08937},
  year={2024}
}

@article{code-quality-chagpt,
  title={No need to lift a finger anymore? assessing the quality of code generation by chatgpt},
  author={Liu, Zhijie and Tang, Yutian and Luo, Xiapu and Zhou, Yuming and Zhang, Liang Feng},
  journal={IEEE Transactions on Software Engineering},
  year={2024},
  publisher={IEEE}
}

@article{evaluating-chatgpt,
  title={Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation},
  author={Liu, Jiawei and Xia, Chunqiu Steven and Wang, Yuyao and Zhang, Lingming},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{survey_3,
  title={The cost of poor software quality in the US: a 2022 report},
  author={Krasner, Herb},
  journal={Consortium for Information and Software Quality (CISQ)},
  year={2022}
}

@inproceedings{expectation,
  title={Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models},
  author={Vaithilingam, Priyan and Zhang, Tianyi and Glassman, Elena L},
  booktitle={Chi conference on human factors in computing systems extended abstracts},
  pages={1--7},
  year={2022}
}

@inproceedings{survey-icse24,
  title={A large-scale survey on the usability of ai programming assistants: Successes and challenges},
  author={Liang, Jenny T and Yang, Chenyang and Myers, Brad A},
  booktitle={Proceedings of the 46th IEEE/ACM International Conference on Software Engineering},
  pages={1--13},
  year={2024}
}

@inproceedings{yin2024characterizing,
  title={Characterizing truthfulness in large language model generations with local intrinsic dimension},
  author={Yin, Fan and Srinivasa, Jayanth and Chang, Kai-Wei},
  booktitle={Proceedings of the 41st International Conference on Machine Learning},
  pages={57069--57084},
  year={2024}
}

@article{jiang2024tracing,
  title={Tracing representation progression: Analyzing and enhancing layer-wise similarity},
  author={Jiang, Jiachen and Zhou, Jinxin and Zhu, Zhihui},
  journal={arXiv preprint arXiv:2406.14479},
  year={2024}
}