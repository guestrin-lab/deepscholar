\section{Conclusion}

In this paper, we introduced \tool, a model-agnostic framework for assessing the correctness of LLM-generated code by dynamically selecting and leveraging informative internal representations. Specifically, \tool uses an attention-based mechanism to learn which internal states are most predictive of code correctness, enabling better adaptation across diverse LLM architectures and tasks.
%
Our experimental results demonstrate that \tool consistently outperforms SOTA black-box and white-box baselines across different correctness criteria, including compilability, functionality, and security. 
These findings highlight that dynamically selecting important internal signals enables \tool to serve as a robust and generalizable solution for assessing the correctness of code generated by various LLMs.