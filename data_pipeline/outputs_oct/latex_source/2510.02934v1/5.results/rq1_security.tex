\subsubsection{Security Assessment}

\input{tables/tab_performance_security}

The performance of \tool and the baseline approaches in code correctness assessment under the security criterion is shown in Table~\ref{tab:rq1_performance_security}. Overall, \textit{\tool consistently outperforms all baselines across all studied code LLMs. Notably, \tool's performance closely matches or even exceeds that of \oracle, indicating its effectiveness in capturing informative internal signals.}

%Openia
Compared to the white-box approach \openia, \tool yields improvement gains of up to 18\%. For example, when assessing code generated by \codellama-13B, \tool reaches an accuracy of 0.88, while this figure of \openia is only 0.75. This performance gap can be attributed to the limitations of \openia's design, which relies on fixed representations from the last generated token at the last layer of the LLM. Such a rigid strategy hinders its flexibility and generalizability across different model architectures.
In contrast, \tool dynamically selects the most informative internal states,  allowing it to tailor the selected representations to each model's characteristics. This flexible mechanism enables \tool to capture correctness-related signals more effectively, resulting in more robust performance across diverse models.

%LLM AJ
Both \inhouse and \external achieve considerably low performance. The accuracy of \inhouse lags behind \tool by margins ranging from 24\% to 133\%.
In the \inhouse setting, the same LLM that generates the code is also used to assess its own outputs, making it prone to overconfidence in the correctness of its responses. Similarly, \external falls short of \tool by 19\% to 82\%, even when using a powerful external model like \gpt.
This performance gap underscores the challenge of detecting security vulnerabilities, which often require deep semantic understanding and context-aware reasoning.
%
These results expose the limitations of using LLMs directly as correctness judges under the security criterion. 

%backbox
The back-box classification approaches based on CodeBERT and CodeT5+ show quite strong performance across all code LLMs in evaluating the security correctness. For instance, the accuracies achieved by CodeT5+ are comparable to \openia and better than \external by up to 55\%. However, their performance still lags behind \tool from 5\% to 14\%.

%Oracle
Furthermore, \tool exhibits strong performance, which is close to \oracle. 
Interestingly, on \deepseek-1.3B, \tool achieves an accuracy of 0.90, which slightly surpasses \oracle's performance. By aggregating the sampled and weighted internal representations, \tool is able to not only focus on the most informative states but also integrate complementary information across multiple tokens and layers. This mechanism enhances its ability to detect security vulnerabilities. As a result, \tool can outperform \oracle, which relies on a single representation in some cases.
These results highlight the effectiveness of \tool in capturing important signals for security correctness prediction.
