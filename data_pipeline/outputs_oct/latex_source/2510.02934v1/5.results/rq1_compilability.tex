\subsubsection{Compilability Assessment}


% \input{tables/tab_performance_compilability_standalone}

% \input{tables/tab_performance_compilability_repo}

\input{tables/tab_performance_compilability}

Table~\ref{tab:rq1_performance_compilability} shows the results of the compilability assessment under two levels of code generation granularity, \textit{independent-unit} code and \textit{repo-level} code. As seen, \textit{most correctness assessment approaches achieve strong performance in \textit{independent-unit} code, yet show considerable decline in the more complex \textit{repo-level} setting}. Among them, \textit{\tool demonstrates the highest robustness, maintaining the smallest performance drop between the two settings.}



Specifically, in the \textit{independent-unit} setting, \external achieves the highest performance, with an average accuracy of 0.92, which is 69\% higher than \inhouse and 11\% higher than black-box classification with CodeT5+. However, its performance drops by 1.5 times, reaching an average accuracy of 0.59 in the \textit{repo-level} setting. 
%
Indeed, determining whether a piece of code is compilable is mainly dependent on checking syntax and structural correctness, without the need for deep reasoning. 
In \textit{independent-unit} snippets, all relevant information, such as imports, declared variables, and function definitions, is self-contained, allowing LLMs like \gpt to simply scan the code to verify its syntactic validity. 
Meanwhile, \textit{repo-level} code often references variables, functions, or modules defined across multiple files. Without explicit access to the entire project context, the LLM struggles to accurately determine compilability, leading to a significant performance drop in this setting.

In the more complex setting of \textit{repo-level} code assessment, \tool achieves the highest performance, outperforming \external by 12\% on average. 
For example, when evaluating the compilability of code generated by \codegemma-7B, both \tool and \external reach the accuracy of 0.90 on \textit{independent-unit} code. 
However, for \textit{repo-level} code, \tool obtains an accuracy of 0.69, which is 27\% higher than that of \external. Interestingly, in this case, \tool even slightly surpasses the performance of \oracle.
%
These results demonstrate that \tool is less sensitive to the increased code complexity, making it a more reliable choice for practical correctness assessment scenarios.