

\subsubsection{Impact of Aggregation and Probing Classier}
\label{sec:impact_aggregation_classifier}

\input{tables/tab_intrinsic_aggregation}

Table~\ref{tab:intrinsic_aggregation} presents the performance of \tool with different \textit{\textbf{aggregation functions}}, which are applied to combine the weighted representations in $\mathcal{Z}^*$ before feeding them to a probing classifier (Sec.~\ref{sec:predictor}). 
Among the evaluated strategies, \textit{max pooling proves to be the most effective aggregation function, enabling \tool to achieve the highest prediction performance.}
As seen, with \textit{max pooling}, \tool achieves an accuracy of 0.79, which is about 10\% higher than the worst setting using \textit{min pooling}. 
This indicates that \textit{max pooling}, by selecting the most prominent features across representation dimensions, captures more discriminative signals for code correctness prediction, thereby enhancing \tool's overall performance.



%
%
%
\input{tables/tab_intrinsic_classifiers}

The performance of different \textit{\textbf{probing classifiers}} used in \tool is shown in Table~\ref{tab:intrinsic_classifiers}. \textit{Both Logistic Regression and MLP achieve comparable results, while the SVM classifier performs significantly worse}. Specifically, \tool obtains an average accuracy of about 0.77 with either Logistic Regression or MLP, whereas this figure drops to only 0.47 when using SVM.
SVM is not a reliable choice in this context due to its limited capability in handling high-dimensional and possibly noisy feature spaces, which is an inherent characteristic of the internal representations extracted from LLMs. 
Furthermore, SVM is not typically suitable for joint training with upstream components such as the attention module, as their standard optimization is not gradient-based.
Although SVM can be trained using gradient-based methods via hinge loss, the loss function itself is non-smooth, which can lead to unstable or inefficient gradient propagation.
In contrast, Logistic Regression and MLP are trained using gradient descent and are fully differentiable, making them naturally compatible with \tool's end-to-end learning framework.