\subsubsection{Model Generalization Across Code LLMs}

% \begin{figure}
%     \centering
%     \includegraphics[width=\columnwidth]{images/sensivity_models.png}
%     \caption{Impact of LLM generalization between training and inference on \tool's performance on F1-Score}
%     \label{fig:sensivity_model}
% \end{figure}

% Figure~\ref{fig:sensivity_model} shows the effectiveness of \tool when trained on internal states from either the same or different models as the test-time model. This experiment employs three code LLMs, \deepseek-6.7B, \codellama-7B, and \magiccoder-7B, which share a common architecture of 32 layers and 4,096 dimensional hidden states. 
% %
% Generally, \textit{training the classifier on representations extracted from the same model  yields significantly higher F1-Scores.}
% For example, when evaluating code generated by \codellama, the classifier trained on its own internal states achieves an F1-Score of 0.77, whereas training on representations from different models results in a drop to 0.52. 

% Although \tool is a model-agnostic approach which is designed to dynamically select informative internal signals, the results suggest that correctness-related representations remain model-specific to some extent. Due to differences in architectural design, training dynamics, or pretraining data, the patterns of correctness-related signals cannot directly transferable across models. These findings highlight the importance of aligning the training and inference models to achieve optimal performance in white-box correctness assessment.



\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{images/sensitiviy_model2.png}
    \caption{Impact of LLM generalization between training and inference on \tool's performance on F1-Score}
    \label{fig:sensivity_model}
\end{figure}

Figure~\ref{fig:sensivity_model} shows \tool's performance when trained on internal states from either the same or different models as the test-time model. This experiment employs three code LLMs, \deepseek-6.7B, \codellama-7B, and \magiccoder-7B, which share a common architecture of 32 layers and 4,096-dimensional hidden states. We evaluates three settings: \textit{intra-model}, \textit{cross-model}, and \textit{mixed-model}. In the \textit{intra-model} setting, the classifier is both trained and tested on the internal states of the same model. In the \textit{cross-model} setting, training is conducted on the internal states of two LLMs, while testing is performed on the remaining one. In the \textit{mixed-model} setting, the classifier is trained on the combined internal states of all three models and evaluated individually on each model. 


Across all three models, \tool performs consistently better on \textit{\textit{intra-model} setting compared to on \textit{cross-model} setting}. For example, \tool on \textit{intra-model} for \codellama achieves an F1-Score of 0.77, whereas its performance on \textit{cross-model} results in a drop to 0.52. For \magiccoder and \deepseek, \tool's performance on \textit{cross-model} is also worse, achieving only 0.68 and 0.66, compared to its performance on \textit{intra-model}, over 0.78 and 0.73, respectively. 

The \textit{mixed-model} setting is designed to leverage both the specific characteristics of the test-time model and the generalizable patterns learned from the other models. However, as shown in Figure~\ref{fig:sensivity_model}, its effectiveness is not consistent across different target LLMs. For example, in the case of \magiccoder, \tool's performance on \textit{mixed-model} achieves a slightly higher F1-score than that on \textit{intra-model}. In contrast, for \codellama on \textit{mixed-model}, \tool performs worse than when it performs on \textit{intra-model}, indicating that the inclusion of external representations may introduce noise. These results suggest that its effectiveness depends on the degree of representational similarity among the models and the classifier's ability to extract shared correctness features.


Although \tool is model-agnostic, designed to dynamically select informative internal signals, the results suggest that correctness-related representations remain model-specific to some extent. Due to differences in architectural design, training dynamics, and pretraining data, the patterns of correctness-related signals might not be directly transferable across models. These findings highlight the importance of aligning the training and inference models to achieve optimal performance in white-box correctness assessment.



