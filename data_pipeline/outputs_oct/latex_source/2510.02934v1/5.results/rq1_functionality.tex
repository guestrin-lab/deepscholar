\subsubsection{Functionality Assessment}

% \input{tables/tab_performance_functionality_standalone}

% \input{tables/tab_performance_functionality_repo}

\input{tables/tab_performance_functionality}


Table~\ref{tab:rq1_performance_functionality} shows that \textit{\external is the most effective approach for assessing the functionality correctness of \textit{independent-unit} code, while \tool performs best in the \textit{repo-level} code setting.} Specifically, in the \textit{independent-unit} setting, \external achieves an average accuracy of 0.94, exceeding that of the other approaches by 32--121\%.
In the \textit{repo-level} setting, \tool reaches an average accuracy of 0.82, which is comparable to the \oracle's performance and surpasses the other approaches by up to 111\%.
% , outperforming \external by 7\%.

% In addition, the approaches exhibit different performance trends when transitioning from \textit{independent-unit} setting to \textit{repo-level} setting, which is more challenging and practical.

The performance of LLM-AJ approaches, including both \inhouse and \external, \textit{declines} when transitioning from \textit{independent-unit} setting to \textit{repo-level} setting. For example, when evaluating the functional correctness of standalone programs generated by \codegemma-7B, \inhouse and \external achieve F1-Scores of 0.22 and 0.91, respectively. However, these scores significantly drop to 0.05 and 0.80 when assessing the correctness of \textit{repo-level} code.
%
%
This performance degradation can be attributed to the \textit{reasoning-based} nature of LLM-AJ approaches. Specifically, they judge functional correctness by analyzing and reasoning over the provided context. In the \textit{independent-unit} setting, the program is self-contained, allowing LLMs to access to the full context needed for reliable reasoning. Meanwhile, \textit{repo-level} code involves multi-file dependencies, preventing LLMs from explicitly accessing the entire project. As a result, they lack crucial information, leading to degraded performance in the \textit{repo-level} setting.

In contrast, the other approaches shows performance \textit{gains} when moving from \textit{independent-unit} to \textit{repo-level} functionality assessment. For instance, the black-box method with CodeBERT improves by 52\%, while \tool achieves a 21\% increase. 
These methods,
% Meanwhile,
including black-box classifiers, \openia, and \tool, are \textit{representation-based} approaches that distinguish correct and incorrect code by leveraging features encoded in embedding vectors or hidden states of the generated code. In the \textit{independent-unit} setting, generated code snippets are often short and simple, offering limited  discriminative signals. This makes it more challenging for the representation-based models to separate correct and incorrect cases. On the other hand, the \textit{repo-level} setting involves longer and more complex code, which introduces richer structural and semantic information. 
These characteristics lead to more distinctive feature patterns in the representations, thereby improving separability. 
As a result, representation-based approaches demonstrate improved performance of functionality assessment when transitioning from the \textit{independent-unit} to the \textit{repo-level} setting.

Overall, these differences highlight that reasoning-based methods are highly dependent on complete contextual information, whereas representation-based approaches can exploit the richer feature patterns of complex code to achieve better robustness in \textit{repo-level} functionality assessment.

% However OPENIA ...
% \input{5.results/example}

\begin{figure*}
    \centering
\includegraphics[width=\linewidth]{images/example_functionality.png}
    \caption{An example of code generated by \codellama-7B, \deepseek-6.7B, and \magiccoder-7B for Task ID\_72 in the MBPP benchmark. The task requires ``\texttt{Write a Python function to check whether the given number can be represented as difference of two squares or not}''. Due to space constraints, portions of the generated outputs are omitted. Full generations are available on our website~\cite{website}.
    % 
    }
    \label{fig:example}
\end{figure*}


Furthermore, compared to the SOTA white-box approach \openia, \tool exhibits greater robustness. 
Figure~\ref{fig:example} shows an example of code generated by \codellama-7B, \deepseek-6.7B, and \magiccoder-7B for the same task.
%
Although all the three models produced similar implementations, the generated functions are all incorrect. Specifically, instead of checking whether a number can be represented as the \textit{difference} of two squares, the generated code mistakenly checks for the \textit{sum} of two squares.

Despite the nearly identical functions, \openia yields inconsistent predictions about their functional correctness. It accurately assesses the correctness of the code generated by \codellama-7B, but  misclassified the outputs from \deepseek-6.7B and \magiccoder-7B. This inconsistency arises because \openia relies on the \textit{fixed} representation of only the last generated token (e.g., token \texttt{correctly} for \codellama and \texttt{False} for the other models) at the final layer of the models. Since different model architectures and generation dynamics assign varying semantic signals to these tokens, \openia's predictions become unstable and unreliable across models, even when the generated code is essentially the same.

Meanwhile, \tool overcomes this limitation through an adaptive mechanism that dynamically selects the most informative internal signals for each model. This flexibility allows \tool to remain robust to architectural differences and consistently produce correct predictions for the code generated by all three models. As a result, \tool achieves reliable correctness assessment across diverse code LLMs, demonstrating its superiority in both stability and effectiveness over \openia.
%
% For instance, in evaluating correctness for code  generated by \magiccoder, \tool surpasses \open 6\% in \textit{repo-level} setting and 24\% in \textit{independent-unit} setting. 

