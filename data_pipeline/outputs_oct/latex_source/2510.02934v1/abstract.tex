

\begin{abstract}
Large Language Models (LLMs) have demonstrated impressive capabilities in code generation and are increasingly integrated into the software development process. 
%
However, ensuring the correctness of LLM-generated code remains a critical concern. Prior work has shown that the internal representations of LLMs encode meaningful signals for assessing code correctness. Nevertheless, the existing methods rely on representations from pre-selected/fixed layers and token positions, which could limit its generalizability across diverse model architectures and tasks.
%
In this work, we introduce \tool, a novel model-agnostic approach that dynamically selects the most informative internal representations for code correctness assessment. \tool employs an attention-based mechanism to learn importance scores for hidden states, enabling it to focus on the most relevant features. These weighted representations are then aggregated and passed to a probing classifier to predict code correctness across multiple dimensions, including compilability, functionality, and security.
%
To evaluate the performance of \tool, we conduct extensive experiments across multiple benchmarks and code LLMs. 
Our experimental results show that \tool consistently outperforms the baselines. For security assessment, \tool surpasses the state-of-the-art white-box approach by 18\%. For compilability and functionality assessment, \tool demonstrates its highest robustness to code complexity, with the performance higher than the other approaches by up to 19\% and 111\%, respectively.   
These findings highlight that dynamically selecting important internal signals enables \tool to serve as a robust and generalizable solution for assessing the correctness of code generated by various LLMs.
\end{abstract}





\begin{keywords}
LLM-generated code, code LLMs, code quality assessment, internal representations, white-box, open-box approach
\end{keywords}