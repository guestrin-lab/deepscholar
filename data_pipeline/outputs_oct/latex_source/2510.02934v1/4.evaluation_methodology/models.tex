\subsection{Studied Code LLMs}

To enable a fair, reproducible, and practical evaluation, we selected the studied LLMs based on several key criteria. 
First, \textit{access to the model's  hidden states is essential}, as our goal is to evaluate code correctness by leveraging these internal representations. This requirement is typically met by open-source models, whereas closed-models or commercial models often restrict such access.
Second, \textit{we focus on code-specialized LLMs}, which are trained or fine-tuned on large-scale code corpora. These models generally outperform general-purpose LLMs on programming tasks, leading to a more realistic and meaningful evaluation.
Third, \textit{we consider the diversity of model architectures, model sizes, and training strategies} to broaden the scope of our analysis and improve the generalizability of our findings. 
Finally, \textit{we limit the model size to a maximum of 13 billion parameters} to accommodate hardware resource constraints and ensure that our evaluation is practical and reproducible.


Based on these criteria, we selected six representative LLMs from different model families to ensure both architectural diversity and broad generalizability of our evaluation, including \deepseek-1.3B~\cite{deepseek-coder}, \deepseek-6.7B~\cite{deepseek-coder}, \codellama-7B~\cite{codellama}, \codellama-13B~\cite{codellama},  \magiccoder-7B~\cite{magicoder}, and \codegemma-7B~\cite{codegemma}. 
All selected models are open-source, allowing full access to their internal representations during code generation. 
These models are widely adopted in both academic research and industrial applications, and have demonstrated strong performance on SE tasks~\cite{zheng2025towards, liu2023your, openia}.
Each employed model is either trained or fine-tuned on large-scale code corpora to capture the syntactic, semantic, and logical aspects of programming languages. 
For consistency, we use the instruction-tuned versions with officially released pre-trained weights from HuggingFace and do not perform further fine-tuning. This setup enables a fair assessment of each model's generalization ability and facilitates a controlled comparison of how their internal representations relate to code correctness.




