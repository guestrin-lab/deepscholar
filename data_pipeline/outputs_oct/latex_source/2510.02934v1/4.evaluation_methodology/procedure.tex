\subsection{Evaluation Procedure}

\textbf{RQ1. Performance analysis:}
%
We evaluate \tool and the baselines across three key dimensions: \textit{compilability}, \textit{functionality}, and \textit{security} under different settings: 
%


\begin{itemize}
    \item \textbf{\textit{Independent-unit code generation}:} This focuses on standalone programming tasks~\cite{HumanEval, MBPP, SecurityEval, Cweval, CODEGUARD+, Sallm}.  For each studied LLM, the classifier is trained on internal representations extracted from one benchmark (e.g., MBPP) and evaluated on another (e.g., HumanEval). This setting tests the generalizability of correctness assessment across different benchmarks.
  
   
    \item \textbf{\textit{Repo(sitory)-level code generation}:} This setting evaluates the code correctness in the context of real-world software projects~\cite{li-etal-2024-deveval}. Repo-level tasks require LLMs to maintain contextual consistency across multiple code units, significantly increasing the complexity of code generation~\cite{rambo, zhang2023repocoder}. This setting measures how robust the approaches are in handling context-dependent code generation scenarios. 

\end{itemize}
%
For a fair comparison, we use the same architecture for the classification models in the post-hoc methods, \openia, and \tool. Each classifier contains an input layer, two hidden layers with 128 and 64 neurons respectively, and an output layer. All models are trained for 50 epochs using a batch size of 32 and a learning rate of $10^{-3}$.


\textbf{RQ2. Intrinsic Analysis:} We investigate how different components of \tool contribute to its overall performance. To answer this question, we systematically evaluate \tool under different configurations.

For the \textit{Internal Representation Extractor}, we aim to examine the impact of different sampling strategies:
\begin{itemize}
    \item \textit{Token sampling}: We compare \tool's performance under three sampling strategies: (1) \textit{full token}, (2) \textit{random}, and (3) \textit{boundary-aware}. \textit{Full token} uses representations from all generated tokens. To accommodate varying sequence lengths, we fix the sequence size to 256 tokens and apply padding or truncation as needed. \textit{Random} randomly selects one interior token position. \textit{Boundary-aware} selects four key tokens at boundary positions (Sec.~\ref{sec:extractor}).
  
    \item \textit{Layer sampling}: We evaluate the impacts of sampling different subsets of layers by varying the sampling intervals,  $k \in \{1, 2, 3, 4, 5\}$, which defines the step size between selected layers.
\end{itemize}
%
%

For the \textit{Informative Representation Selector}, we conduct an ablation study to investigate its contribution to \tool's performance. Specifically, we compare \tool's performance under two configurations:
\begin{itemize}
    \item \textit{With Selector}: This module is \textit{enabled} to assign importance scores to each internal representation.
    % , allowing the model to emphasize more informative signals.
    \item \textit{Without Selector}: The selector module is \textit{disabled}. All internal representations are uniformly aggregated with equal importance.
\end{itemize}
%

For the \textit{Code Correctness Predictor}, we study how the choice of aggregation function and probing classifier affects the overall results. Regarding the \textit{aggregation function}, we evaluate several strategies for combining the selected internal representations $\mathcal{Z}^*$, including concatenation, summation, mean pooling, and max pooling. For the \textit{probing classifier}, we explore various classifier architectures, including Multi-Layer Perceptron (MLP), Support Vector Machine (SVM), and Logistic Regression. 

All intrinsic analysis experiments are conducted using \codegemma-7B on the MBPP benchmark. For each experiment, the MBPP tasks are randomly split into a 90:10 ratio for training and testing. 
Internal representations extracted during the code generation process for the tasks in the training set are used to train the probing classifier, while those from the test set are used to evaluate its performance.



\textbf{RQ3. Sensitivity analysis:}
%
This experiment examines \tool's generalization capability across programming languages and LLMs. 
Specifically, we assess whether \tool, when trained on internal representations extracted from code generation in certain programming languages, can accurately predict the correctness of code written in a completely different language. 
We also analyze how varying the size of the training data affects \tool's prediction performance. 
%
Additionally, we examine \tool's ability to transfer across models by training it on the internal states of several code LLMs and testing its performance on code generated by a different model. 


