\documentclass{article} % For LaTeX2e
\usepackage{iclr2026_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage{amssymb}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{url}
\usepackage[many]{tcolorbox}
\usepackage{graphicx}
\usepackage{amsmath} 
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage[table]{xcolor}
\usepackage{multirow}
\usepackage{needspace}
\usepackage{soul}

\definecolor{llamagreen}{HTML}{00bf63}
\definecolor{qwenpurple}{HTML}{5e17eb}
\definecolor{olmoorange}{HTML}{e76f51}
\definecolor{gemmapink}{HTML}{ff6f91}
\definecolor{phiblue}{HTML}{457b9d}
\tcbuselibrary{listings, breakable, skins, theorems, documentation, hooks}
\newcommand{\tianjian}[1]{{\color{blue}[TL: #1]}}
\newcommand{\todo}[1]{{\color{red}[Todo: #1]}}
\newcommand{\method}{{\textsc{Harmony}}}

%\title{Benchmark Harmony: A Distributional Perspective on Benchmark Quality}
%\title{The Flaw of Averages: A Distributional Perspective on Benchmark Harmony}
%\title{The Flaw of Averages: Measuring Uniformity of Model Performance Across Benchmark Subdomains}
%\title{When Aggregate Scores Can Mislead: Quantifying Uniformity of Model Performance on a Benchmark}
%\title{When Aggregate Scores Can Mislead: A Distributional Perspective on Uniformity of Model Performance}
%\title{When Aggregate Scores Can Mislead: Quantifying the Uniformity of Model Performance}
%\title{The Flaw of Averages: Quantifying Uniformity of Model Performance on a Benchmark}
%\title{The Flaw of Averages: Quantifying\\ Uniformity of Benchmark Performances}
\title{The Flaw of Averages: Quantifying\\ Uniformity of Performance on Benchmarks}



\newcommand{\daniel}[1]{{\color{cyan}[DK: #1]}}
% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Arda Uzunoğlu\begingroup\renewcommand\thefootnote{\fnsymbol{footnote}}\footnotemark[2]\endgroup \quad
  Tianjian Li \quad
  Daniel Khashabi \\
  Department of Computer Science, Johns Hopkins University \\
}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begingroup
\renewcommand\thefootnote{\fnsymbol{footnote}}
\footnotetext[2]{Corresponding author: \texttt{auzunog1@jhu.edu}}
\endgroup

\begin{abstract}
Benchmarks shape scientific conclusions about model capabilities and steer model development. This creates a feedback loop: stronger benchmarks drive better models, and better models demand more discriminative benchmarks. Ensuring benchmark reliability is therefore essential for trustworthy evaluation and meaningful progress. In this work, we study benchmark reliability from a \emph{distributional} perspective and introduce benchmark \method{}, which measures \textit{how uniformly a model's performance is distributed across the subdomains of a benchmark}. We posit that high \method{} is a desirable benchmark property, indicating that the aggregate metric reflects uniform competence across subdomains. Across 19 multiple-choice benchmarks and five model families, we map each benchmark onto a mean-variance plane of \method{} computed across models, where high mean and low variance signal more reliable evaluation. Our analysis shows that less harmonious benchmarks can give misleading results, since overall accuracy may be disproportionately influenced by specific subdomains. For instance, \emph{ARC-Easy} is overwhelmed by questions on \emph{Biological Concepts}, overshadowing other critical subdomains such as Geography, Physics, Chemistry, and Environmental Science. By recommending that \method{} should be reported alongside accuracy, we reframe evaluation from 
simple performance averages to a more robust, distributionally reliable measurement of performance.
\end{abstract}

\section{Introduction}
Benchmarks lie at the crux of measuring and shaping scientific progress in language models, forming a feedback loop with model development. Discriminative benchmarks refine learning signals and guide model design, while stronger models expose benchmark limitations and drive the creation of more rigorous evaluations. In this reciprocal process, benchmark reliability is essential to ensure that reported improvements reflect genuine capabilities rather than evaluation artifacts~\citep{Ott_2022}. Yet, despite its importance, benchmark auditing \citep{swayamdipta2020datasetcartographymappingdiagnosing, damour2020underspecificationpresentschallengescredibility, sainz2023nlpevaluationtroubleneed} has received far less attention than algorithmic advances \citep{brown2020languagemodelsfewshotlearners, ouyang2022traininglanguagemodelsfollow, deepseekai2025deepseekr1incentivizingreasoningcapability}.

Motivated by this gap, recent work identifies structural issues in popular benchmarks, such as redundancy \citep{polo2024tinybenchmarksevaluatingllmsfewer, perlitz2024efficientbenchmarkinglanguagemodels} and uneven data distributions \citep{huang2025mathperturbbenchmarkingllmsmath}, that can skew results and mislead interpretations of model capability~\citep{ruan2024observational, Ili2024}. In response, the research community \emph{interrogates the reliability of existing benchmarks}, in addition to proposing new ones. Rather than treating benchmark gains as definitive, recent work urges caution about what benchmarks measure and how these measurements are obtained \citep{singh2025leaderboardillusion, heineman2025signalnoiseframeworkreducing}. This reframes evaluation as an ongoing measurement challenge, highlighting the need for benchmarks whose properties and limitations are well understood.

\begin{figure}
    \centering
    \vspace{-3em}
    % \fbox{
\includegraphics[width=0.98\linewidth,trim=0cm 2.7cm 0cm 2.7cm,clip=true]{figs/teaser_harmony.pdf}
    % }
    \vspace{-0.1cm}
    \caption{\textbf{Pipeline of evaluating \method{} for a given benchmark.} Step 1: We partition the benchmark into semantic clusters (subdomains or skills). 
    % that correspo, where each cluster represents a subdomain. 
    Step 2: We gather each model's performance on every cluster. Step 3: We calculate the harmony --- the uniformity of the distribution of performance across subdomains. We posit that high \method{} implies that aggregate metrics capture broad competence, whereas low \method{} obscures strengths and weaknesses.}
    \vspace{-1.4em}
    \label{fig:teaser}
\end{figure}

In our work, we investigate benchmark reliability from a distributional perspective. Since benchmarks claim to assess competence over a stated domain, we ask whether their data evenly represents its subdomains and whether performance is uniform on these subdomains. We instantiate this idea with benchmark \method{}, a measurement of performance uniformity among subdomains of a benchmark (\S\ref{sec:benchmark_harmony}). \autoref{fig:teaser} illustrates our pipeline: Given a target benchmark, we first partition the datapoints in this benchmark into semantic clusters, where each cluster represents a subdomain (Step~1); we then gather the performances per subdomain for different models (Step~2); finally, we compute \method{} for each benchmark-model pair, where high \method{} suggests that aggregate metrics capture broad competence, while low \method{} obscures strengths and weaknesses of the model (Step~3).


Using \method{}, we conduct a range of analyses on a variety of benchmarks and models to assess the reliability of benchmark evaluations (\S\ref{sec:main:analyses}, \S\ref{sec:beyond}).
We posit that \textbf{high \method{}} is a \textbf{desirable} benchmark 
property, since it implies that the benchmark reflects \emph{overall competence} for a given model. 
This distinction matters because \emph{less harmonious benchmarks can yield skewed aggregate conclusions}. For example, if a benchmark is dominated by one subdomain (e.g., Biological Concepts in ARC-Easy), aggregate accuracy can obscure weaknesses or strengths in other subdomains (e.g., Geography, Physics, Chemistry, and Environmental Science). As a result, a model that masters the dominant subdomain can appear to outperform a more generalist model despite having poorer cross-domain generalization (e.g., biologist model vs. generalist model in \autoref{fig:teaser}).
We therefore recommend reporting \method{} alongside accuracy to move from simple averages to a distributionally reliable measurement of competence across subdomains.

In summary, our contributions are twofold:
\begin{itemize}[leftmargin=1.2em,itemsep=1pt,topsep=1pt]
    % \item 
    \item We introduce \method{}, an entropy-based metric that quantifies how uniformly performance is distributed across subdomains in a benchmark. 
    This provides a measure of how well the overall accuracy captures performance consistency across the benchmark’s skill domains.
    \item We provide a large-scale empirical mapping of 19 MCQA benchmarks across five model families in the \method{} mean-variance plane, revealing the spectrum of benchmark reliability.
\end{itemize}

\section{Benchmark \method{}}
\label{sec:benchmark_harmony}
\subsection{Preliminaries and Notation}
\label{subsec:preliminaries}
Let $\mathcal{B}=\{(x_i,y_i)\}_{i=1}^N$ be a benchmark consisting of input-output pairs $(x,y)$. Our goal is to understand the \emph{underlying distribution} of $\mathcal{B}$ by inducing a semantic partition $\mathcal{G}=\{A_1,\ldots,A_k\}$ of $\mathcal{B}$, where $A_i \subseteq \mathcal{B}$, $A_i \cap A_j = \emptyset$ for $i \neq j$, and $\bigcup_{i=1}^k A_i = \mathcal{B}$. The partition is guided by a similarity function $\mathcal{S}:\mathcal{X}\times\mathcal{X}\to(0,1]$ that measures the semantic similarity between data points $x_i,x_j\sim\mathcal{B}$. Lastly, let $f$ be a model and let $\Psi(f;A_i)$ denote a measure of performance (e.g., accuracy) for $f$ computed on a subset $A_i \subseteq \mathcal{B}$.

%Given a partition, we define a \emph{good} benchmark as one whose distribution is balanced enough that no coherent subset induces an outsized gain or drop in performance (i.e., no \emph{shortcuts}). Beyond this, we measure the entropy of accuracies across the subsets of a given partition to quantify how evenly performance is distributed. We begin with an informal intuition for this notion, then introduce precise definitions of a \emph{shortcut} and a \emph{good} benchmark, followed by the details of our entropy-based analysis and a discussion of the resulting implications.

%\paragraph{Shortcut.}
%Let $f$ be a model and let $\Psi(f;A)$ be a measure of success (e.g., accuracy) for $f$ computed on $A \subset \mathcal{B}$, where $|A|=m$ and $|\mathcal{B}|=n$. Then, the accuracy of $f$ on $A$ and $\mathcal{B}$ are defined as follows:
%\[
%a_A \;=\; \Psi(f;A) \;=\; \frac{1}{m}\sum_{(x_i,y_i)\in A}\mathbf{1}\{f(x_i)=y_i\},
%\qquad
%a_\mathcal{B} \;=\; \Psi(f;\mathcal{B}) \;=\; \frac{1}{n}\sum_{i=1}^n \mathbf{1}\{f(x_i)=y_i\}.
%\]
%Next, we define the performance deviation $\Delta_f(A)=a_A-a_{\mathcal{B}}$ and test
%\[
%H_0(A):\ \Delta_f(A)=0 \quad\text{vs.}\quad H_1(A):\ \Delta_f(A)\neq 0.
%\]
%Under $H_0$, the number of correct answers in $A$, $X=m \cdot a_A$, follows a hypergeometric distribution
%\[
%X \sim \mathrm{Hypergeometric}(N=n,\ a_\mathcal{B}\cdot n,\ m),
%\]
%with mean $\mathbb{E}[X]=ma_\mathcal{B}$. We report the two-sided \emph{exact} $p$-value
%\[
%p_{\mathrm{acc}}(A)\;=\;\Pr\!\big(\,|X-ma_\mathcal{B}| \,\ge\, |x-ma_\mathcal{B}|\,\big),\qquad x=ma_A.
%\]

%Finally, we declare $A$ a \emph{shortcut} for $f$ at level $\alpha$ if its two-sided $p$-value $p_{\mathrm{acc}}(A)$ satisfies $p_{\mathrm{acc}}(A)\le \alpha$.

\subsection{\method{}: A Measure of Balanced Coverage and Uniform Performance}

\paragraph{Intuition.} Consider a biology benchmark spanning microbiology, animal biology, and plant biology. If microbiology dominates and a model excels only there, the overall score may misleadingly suggest broad competence in biology. Conversely, if microbiology is underrepresented and the model is weak on it but strong elsewhere, the aggregate evaluation may conceal a critical weakness. Moreover, even when subdomains are equal in size, large accuracy gaps make the aggregate metric uninformative (e.g., 90\% accuracy in microbiology and 50\% accuracy in plant biology averages to a number that reflects neither). A \emph{harmonious} benchmark therefore mitigates these distortions by balancing coverage and promoting comparable performance across subdomains.

\paragraph{Formal definition of \method{}.}
Given a partition $\mathcal{G}_f=\{A_i\}_{i=1}^k$, \emph{\method{}} measures how uniformly performance is distributed across the subsets in this partition. For each $A_i$, let $w_i = \frac{|A_i|}{|\mathcal{B}|}$ be the size weight and let $\mu=\sum_{i=1}^k w_i \Psi(f;A_i)$ be the weighted mean.
We convert differences between $\mu$ and $\Psi(f;A_i)$ into smooth proximity scores via a Gaussian kernel:
\[
K_i \;=\; \exp\!\Big(-\big(\tfrac{\Psi(f;A_i)-\mu}{b}\big)^2\Big),
\]
where $b>0$ is a bandwidth parameter.\footnote{We set $b$ by a robust scale of $\{\Psi(f;A_i)\}$. Let $\tilde a=\mathrm{median}_i\,\Psi(f;A_i)$ and $\mathrm{MAD}=\mathrm{median}_i|\Psi(f;A_i)-\tilde a|$, then $b=\max\{0.02,\;1.4826\cdot\mathrm{MAD}\}$.} We then form \emph{performance masses}
\[
p_i \;=\; \frac{w_i K_i}{\sum_{j=1}^k w_j K_j}, \qquad \sum_{i=1}^k p_i = 1,
\]
and compute the \method{} (normalized Shannon entropy)
\[
H(\mathcal{G}_f) \;=\; -\frac{1}{\log k}\sum_{i=1}^k p_i \log\!\big(p_i+\varepsilon\big)\in[0,1],
\]
with a small $\varepsilon=10^{-12}$ for numerical stability. 

Subsets with accuracies far from $\mu$ receive exponentially smaller $p_i$, lowering entropy. Thus, higher \method{} $H(\mathcal{G}_f)$ indicates performance that is evenly distributed across subsets, while lower \method{} captures a more concentrated performance in a few subsets. Therefore, \method{} quantifies the uniformity of performance while considering the distributional balance.

%\paragraph{Alt1: \emph{Good} benchmark.}  We define a benchmark $\mathcal{B}$ to be \emph{good} (with respect to a model $f$ and significance level $\alpha$) if its partition $\mathcal{G}$ admits no subsets that constitute shortcuts and the distribution of accuracies across subsets attains sufficiently high entropy. In this sense, a good benchmark is one where performance remains stable across coherent subsets: no subset yields spurious deviations beyond random variation, and accuracy is not disproportionately concentrated in a small number of clusters.

\paragraph{Interpreting \emph{\method{}}.} Let $\Pi$ be a partitioning rule that maps a benchmark $\mathcal{B}$ and a model $f$ to a partition $\mathcal{G}_f(\mathcal{B})=\Pi(\mathcal{B};f)$ using $\mathcal{S}$. Then, define the per-model harmony of $\mathcal{B}$ as
\[
H_{\mathcal{B}}(f)\;:=\;H\!\big(\mathcal{G}_f(\mathcal{B})\big)\in[0,1].
\]
Given a model set $\mathcal{F}$, we evaluate $\mathcal{B}$ by the cross-model mean and variance
\begin{equation}\label{eq:mu_and_sigma}
\mu_H(\mathcal{B}) \;=\; \mathbb{E}_{f\sim\mathcal{F}}\!\left[ H_{\mathcal{B}}(f) \right], \qquad \sigma_H^2(\mathcal{B}) \;=\; \mathrm{Var}_{f\sim\mathcal{F}}\!\left( H_{\mathcal{B}}(f) \right).
\end{equation}
Higher $\mu_H(\mathcal{B})$ indicates that, on average across models, performance is more uniformly distributed across the subsets of $\mathcal{B}$, while lower $\sigma_H^2(\mathcal{B})$ indicates that this property is stable across models. Rather than dichotomizing benchmarks as \emph{good} or \emph{bad}, we adopt a comparative view, where $\mathcal{B}_1$ is preferred to $\mathcal{B}_2$ if it attains a higher expectation and a lower variance.

\paragraph{Implications.} We approach benchmarks as diagnostic tools rather than scoreboards. A benchmark with \emph{high mean \method{}} and \emph{low cross-model variance} indicates that aggregate metrics consistently capture broad competence rather than artifacts of data composition. In contrast, either \emph{low mean} or \emph{large variance} signals fragility, since the conclusions about model evaluation may depend excessively on a few subdomains and be less reliable. Notably, models with similar aggregate accuracy can differ in \method{}, implying different breadth of competence. In practice, benchmarks with favorable mean-variance \method{} profiles enable more trustworthy evaluation, fairer comparisons, and clearer measure of progress.

\subsection{Partition Induction}
\label{subsec:methodology}
To compute \method{}, we require a semantic partition of the benchmark. To this end, we introduce a novel similarity metric named \textbf{predictive similarity}, a model-aware similarity between data points based on the divergence of their probability distributions, and induce $\mathcal{G}_f$ via spectral clustering on the resulting affinity matrix $\mathcal{S}$.

\paragraph{Predictive Similarity.} We define predictive similarity as a model-aware similarity measure that quantifies how similarly a model $f$ distributes probability over the output space for two data points. For \(x_i, x_j \sim \mathcal{B}\), let \(\bar{p}_f(x)\) denote the model's length-normalized probability distribution over tokens. Then, predictive similarity is computed as
\begin{equation}
S(x_i, x_j) \;=\; \exp\!\Bigg(-\tfrac{\tau}{2} \Big[ 
D_{\mathrm{KL}}\big(\bar{p}_f(x_i) \,\|\, \bar{p}_f(x_j)\big) \;+\;
D_{\mathrm{KL}}\big(\bar{p}_f(x_j) \,\|\, \bar{p}_f(x_i)\big)
\Big] \Bigg), 
\label{eq:predictive:similarity}
\end{equation}

where $D_{\mathrm{KL}}(\cdot \,\|\, \cdot)$ denotes the Kullback–Leibler divergence, 
$\tau$ is a scaling factor chosen as the reciprocal of the median symmetric divergence, 
and the averaged predictive distribution is given by $\bar{p}_f(x_i) \;=\; \frac{1}{T} \sum_{t=1}^{T} p_f(x_i, y_{i}^{<t}),$ with $y_{i}^{<t}$ denoting the ground-truth prefix up to token $t-1$.\footnote{For $t>1$, we condition on the ground-truth answer tokens rather than on the model’s own autoregressive predictions, ensuring that accumulated model errors do not affect the similarity measure.}

Intuitively, predictive similarity \(S\) is large when the model treats \(x_i\) and \(x_j\) as interchangeable from a predictive standpoint and small when the model sharply distinguishes them. We defer in-depth discussion on different aspects of predictive similarity to Appendix \ref{sec:pred_sim_app}.

\paragraph{Clustering.} Given the predictive similarity matrix $S\in(0,1]^{N\times N}$, we induce the partition of a benchmark via spectral clustering \citep{NIPS2001_801272ee}. We treat $S$ as a precomputed affinity, form the symmetric normalized Laplacian
$L \;=\; D^{-1/2}(D-S)D^{-1/2}$ with $D \;=\; \mathrm{diag}(S\mathbf{1}),$
compute the $k$ eigenvectors of $L$ associated with its smallest eigenvalues, and apply $k$-means in this spectral embedding to obtain a partition $\mathcal{G}=\{A_1,\ldots,A_k\}$. To determine the optimal number of subsets, we sweep $2 \leq k \leq 20$ and select the value maximizing the silhouette score $s(k)\in[-1,1]$ as an intrinsic compactness diagnostic \citep{ROUSSEEUW198753}.

\subsection{Empirical Validation of Partition Induction}
\label{subsec:empirical_val}
% \daniel{Why is RedundantQA not introduced alongside the other benchmarks above? I am not saying that you should. My point is that the current organization is weird: I just read a bucnh of datasets in 3.3. And now I come down here and see that you introduce an experiment with a totally new benchmark.}

%\daniel{The current start of this section is sudden. You need to state the intuition behind how you're going to validate the approach. Then use this intuition to motivate your synthetic data. }

We need a controlled benchmark with a known partition to evaluate our method’s ability to induce well-defined semantic partitions. We therefore introduce RedundantQA, a synthetic, four-domain\footnote{Biology, History, Economics, Popular Culture.} MCQA benchmark where each item pairs a reference question with two \emph{true-similar} paraphrases (same underlying knowledge) and two \emph{false-similar} distractors (high lexical overlap, different answers). This structure cleanly separates semantic from lexical similarity and allows us to control underlying data distribution. See Appendix~\ref{sec:redundantqa} for construction and validation details of RedundantQA, along with representative examples.

\begin{wrapfigure}[18]{r}{0.66\textwidth}
  \centering
  \vspace{-\baselineskip} % optional: tighten top gap

  % First subfigure
  \begin{subfigure}[b]{0.48\linewidth}
    \centering
    \caption{RedundantQA}
    \label{fig:validating_methodology_red}
    \includegraphics[width=\linewidth]{figs/ps_validation_redundantqa.png}
  \end{subfigure}
  \hfill
  % Second subfigure
  \begin{subfigure}[b]{0.48\linewidth}
    \centering
    \caption{MMLU subtasks.}
    \label{fig:validating_methodology_mmlu}
    \includegraphics[width=\linewidth]{figs/ps_validation_mmlu.png}
    
  \end{subfigure}
    \vspace{-0.2cm}
  \caption{\textbf{Validation of our approach on (a) RedundantQA and (b) MMLU high school subtasks.} Estimated \method{} strongly correlates with the ground truth and clearly separates \textcolor{red}{low} from \textcolor{green!75!black}{high} \method{} variants. Each dot represents one variant averaged across five random seeds.}
  \label{fig:validating_methodology}
\end{wrapfigure}

We empirically validate our partitioning approach on controlled variants of RedundantQA and a compilation of MMLU high school subtasks.\footnote{Biology, Geography, European History, Computer Science.} In each variant, we designate a domain as dominant and assign it a proportion $r \in \{0.3, 0.4, 0.5, 0.6, 0.7\}$ of the benchmark, with the remaining domains sharing $1-r$ equally. This yields a spectrum of distributional imbalance with known ground-truth \method{}. We repeat every $(\text{dominant domain}, \text{ratio})$ variant with five random seeds.  

As shown in Fig. \ref{fig:validating_methodology}, \method{} estimated by our method exhibits a strong positive correlation with the ground-truth \method{}. This alignment demonstrates that our measure reliably distinguishes between high \method{} and low \method{} regimes across different degrees of imbalance. Importantly, the trend persists across different (dominant domain, ratio) variants, indicating that the signal is robust to variations in benchmark construction. 
%This indicates that our methodology offers a faithful lens for quantifying the distributional balance in benchmarks.



We further validate predictive similarity along three axes and defer all details and results to Appendix~\ref{sec:pred_sim_app}: (i) discrimination of semantic vs.\ lexical similarity (App.~\ref{subsec:sem_vs_lex}), (ii) recovery of ground-truth domains on RedundantQA and MMLU (App.~\ref{subsec:recover_dom}), and (iii) fidelity of \method{} estimates under controlled distributional shifts (App.~\ref{subsec:capture_quality}). %Across these validation experiments, predictive similarity consistently outperforms alternative similarity metrics.

\section{Main Analyses: How Harmonious Are the Benchmarks?}
\label{sec:main:analyses}

\begin{figure}[t]
  \centering

  \begin{subfigure}{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/non_mmlu_tasksv2.png}
    \caption{MCQA Benchmarks.}
    \label{fig:non_mmlu_tasks}
  \end{subfigure}
  \begin{subfigure}{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/mmlu_tasks.png}
    \caption{MMLU Subtasks} \label{fig:benchmarks_overview}
  \end{subfigure}

  \vspace{-0.2em}
  \caption{\textbf{Mean-variance plane for \method{} across (a) MCQA Benchmarks and (b) MMLU subtasks.}
    Each point represents a benchmark or subtask plotted by the \method{} mean ($\mu_H(\mathcal{B})$) and variance ($\sigma_H^2(\mathcal{B})$) over 36 models. Upper-left (high mean, low variance) indicates \textcolor{green!75!black}{higher} \emph{benchmark reliability}; rightward (higher variance) and downward (lower mean) shifts signal \textcolor{red}{diminished} reliability. The star at top-left represents an optimal benchmark. Harmony mean/variance are defined in Eq.~\ref{eq:mu_and_sigma}.}
  \label{fig:combined_benchmark_fig}
\end{figure}

We now examine how harmonious widely used MCQA benchmarks are. Accordingly, we compute \method{} for each model and benchmark pair (as detailed in \S\ref{sec:benchmark_harmony}) and then aggregate it across models to position each benchmark in the mean-variance plane given by $(\mu_H(\mathcal{B}), \sigma_H^2(\mathcal{B}))$ in Eq.~\ref{eq:mu_and_sigma}. This section first details the experimental setup (\S\ref{subsec:exp_setup}), then maps each benchmark to this plane and provides an interpretation of this mapping (\S\ref{subsec:mapping}).

\subsection{Experimental Setup}
\label{subsec:exp_setup}
We conduct evaluations using a modified version of \texttt{lm-evaluation-harness}\footnote{https://github.com/EleutherAI/lm-evaluation-harness}, covering a wide range of model sizes across five prominent model families: \texttt{Llama 3} \citep{grattafiori2024llama}, \texttt{Qwen3} \citep{yang2025qwen3}, \texttt{Gemma 3} \citep{gemmateam2025gemma3technicalreport}, \texttt{Phi-3} \citep{abdin2024phi3technicalreporthighly}, and \texttt{OLMo 2} \citep{olmo20252olmo2furious} (see App.~\ref{sec:model_list} for full model list). Our setup spans 19 MCQA benchmarks that assess a broad range of model capabilities: 

\vspace{-0.5em}
\begin{itemize}[left=0pt, itemsep=1pt]
  \item\textbf{Reasoning}: 
  ARC-Challenge \citep{clark2018think}, ARC-Easy \citep{clark2018think}, ART \citep{Bhagavatula2020Abductive}, BoolQ \citep{clark2019boolq}, CommonsenseQA \citep{talmor2018commonsenseqa}, COPA \citep{roemmele2011choice}, LogiQA \citep{liu2020logiqachallengedatasetmachine}, PIQA \citep{Bisk2020}, QUARTZ \citep{quartz}, SocialIQA \citep{sap-etal-2019-social}, StrategyQA \citep{geva2021didaristotleuselaptop}.
  \item \textbf{Mathematical Problem-Solving}: 
  AQUA-RAT \citep{ling2017program}, MathQA \citep{amini2019mathqainterpretablemathword}.
  \item \textbf{World Knowledge}: 
  GPQA \citep{rein2023gpqa}, MMLU \citep{hendrycks2020measuring}, OpenBookQA \citep{mihaylov2018suit}, PubMedQA \citep{jin2019pubmedqa}, SciQ \citep{SciQ}.
  \item {\textbf{Truthfulness}}: 
  TruthfulQA \citep{lin2021truthfulqa}.
\end{itemize}
\vspace{-0.5em}

For evaluation, we use average token-level log-likelihood scoring over answer options as implemented in the harness, selecting the option with the highest average log-probability. We follow each benchmark’s evaluation protocol as implemented in the harness, using zero-shot evaluation by default, and report accuracy. We focus on MCQA benchmarks because (i) the discrete label space $\mathcal{Y}$ yields unambiguous ground truth and exact accuracy,\footnote{In free-response settings, multiple plausible intermediate steps can lead to the same answer, complicating ground truth.} and (ii) evaluation is automatic and does not rely on a judge, avoiding grading variance that is common in free-form scoring. However, we note that our methodology extends to free-response benchmarks given an appropriate evaluation metric.

\subsection{Mapping Benchmark \method{}}
\label{subsec:mapping}

%\begin{figure}[ht]
% \begin{wrapfigure}[23]{r}{0.68\textwidth}
  %\vspace{5pt}
  %\centering
  %\vspace{-0.3cm}
  %\includegraphics[width=0.63\linewidth]{figs/non_mmlu_tasksv2.png}
  %\vspace{-0.3cm}
  %\caption{\textbf{Plot of the mean and variance of \method{} across 36 models on 18 MCQA benchmarks.} A higher mean indicates benchmark scores are more evenly distributed across subdomains, while a lower variance shows this uniformity is consistent across models. Benchmarks with high mean and low variance are thus more reliable and informative for evaluation.
  % Plot of the mean and variance of \method{} across 36 models on 18 MCQA datasets. A higher \method{} mean indicates that benchmark scores are more evenly distributed across subdomains, while a lower \method{} variance shows this uniformity is consistent across models. Benchmarks with both high mean and low variance are therefore more reliable and informative for evaluation.
  %}
  %\label{fig:non_mmlu_tasks}
  %\vspace{-100pt}
% \end{wrapfigure}
%\end{figure}

%\begin{figure}[th]
    %\centering
    %\includegraphics[width=0.95\linewidth]{figs/mmlu_tasks.png}
  %\caption{
  %\textbf{The mean-variance plane for \method{} across MMLU subtasks.} Each point represents an MMLU subtask, plotted by the \method{} mean ($\mu_H(\mathcal{B})$) and variance ($\sigma_H^2(\mathcal{B})$) over 36 different models. Upper-left (high mean, low variance) indicates \textcolor{green!75!black}{higher} \emph{benchmark reliability} (i.e., consistently balanced performance across subdomains), while rightward (higher variance) and downward (lower mean) shifts signal \textcolor{red}{diminished} reliability. The star on the top left represents an optimal benchmark. Harmony mean and variance are defined in Eq.~\ref{eq:mu_and_sigma}.}
%\vspace{-0.5em}
%\label{fig:benchmarks_overview}
%\end{figure}



% \begin{wrapfigure}[25]{r}{0.66\textwidth}
% \vspace{-0.2cm}
% % 
%   \centering
%   %--- (a) ----------------------------------------------------
%   % \begin{subfigure}{\linewidth}
%   %   \centering
%   %   \caption{MCQA Benchmarks.}
%   %   \label{fig:mmlu_tasks}
%   %   \includegraphics[width=\linewidth]{figs/non_mmlu_tasksv2.png}
%   % \end{subfigure}
%   % \hfill
%   % %--- (b) ----------------------------------------------------
%   \begin{subfigure}{\linewidth}
%     \centering
%     \caption{MMLU subtasks.}
%     \label{fig:non_mmlu}
%     \includegraphics[width=\linewidth]{figs/mmlu_tasks.png}
%   \end{subfigure}

%   \caption{
%   \textbf{The mean-variance plane for \method{} across various benchmarks.}
%       Each point represents a MCQA benchmark, plotted by the cross-model \method{} mean ($\mu_H(\mathcal{B})$) and variance ($\sigma_H^2(\mathcal{B})$). Upper-left (high mean, low variance) indicates higher \emph{benchmark reliability} (i.e., consistently balanced performance across subsets), while rightward (higher variance) and downward (lower mean) shifts signal diminished reliability.
%       \daniel{Include a reference to the top-left star. }
%       \daniel{Add reference to definition of the equations}
%       \daniel{make it clear that mean/var are computed over different models.}
%       \daniel{@Tianjian: articulate that message here. }
%       }
%   \label{fig:benchmarks_overview}
% \end{wrapfigure}
We analyze the widely used benchmarks from \S\ref{subsec:exp_setup} with \method{}, placing each benchmark in a two dimensional plane whose axes are the mean and variance of \method{} across our model suite.

Fig.~\ref{fig:non_mmlu_tasks} and ~\ref{fig:benchmarks_overview} respectively position benchmarks and MMLU subtasks in the cross-model mean-variance plane of \method{}. The vertical axis is \(\mu_H(\mathcal{B})\) (average uniformity of performance across subdomains) and
the horizontal axis is \(\sigma_H^2(\mathcal{B})\) (stability of that uniformity across models).
Moving \emph{upward} increases average distributional uniformity, while moving \emph{leftward} increases cross-model
stability. Consequently, the \emph{upper-left} region (high mean, low variance) identifies benchmarks
whose aggregate scores consistently reflect broad competence. In contrast, model performances on benchmarks with
\emph{low mean} are distributionally skewed on average. If accompanied by \emph{low variance}, this skew is consistent across models (i.e., \emph{consistently fragile}), whereas if accompanied by \emph{high variance}, reliability becomes model-dependent. Thus, \emph{upward} and \emph{leftward} trajectories
indicate more reliable evaluation, whereas \emph{downward} and \emph{rightward} shifts suggest more concentrated model performance on a few subdomains and conclusions that vary substantially across models.

\section{Controlled Analyses of Confounding Factors}
\label{sec:beyond} 

In this section, we (i) show how less harmonious benchmarks can distort model evaluations 
% (ii) use interpretable partitions to enable multi-dimensional evaluation and mitigate aggregate metric pitfalls, 
and (ii) examine whether low \method{} benchmarks warrant extra caution for larger models or those trained with more tokens.

% \subsection{What are the true performances of models?}
% \subsection{How do model performance change upon amplifying harmony?}
\subsection{How does model performance change with increased \method{}?}


\label{subsec:balancing_subsets}

We analyze the extent to which less harmonious benchmarks can distort model evaluations via unrepresentative aggregate metrics. To this end, we prune benchmarks using predictive similarity to eliminate overly similar items. The pruning ratio is set to be inversely proportional to benchmark \method{}, such that high \method{} benchmarks receive minimal pruning while low \method{} benchmarks are pruned more aggressively.\footnote{Specifically, we use the formula \(p=\mathrm{clip}_{[0.05,0.5]}\!\big(0.05+(0.5-0.05)\big(\tfrac{1-\mathrm{clip}(H;\,0.1,\,1)}{1-0.1}\big)^{1.5}\big)\).} By mitigating the skewness, this procedure reveals models' uniform performance on benchmarks.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figs/pruning.png}
    \caption{\textbf{Balancing benchmarks via pruning.} We remove overly similar items with a pruning rate inversely proportional to \method{}. \emph{Top row} shows more harmonious benchmarks, where accuracy remains stable as \method{} increases. \emph{Bottom row} shows less harmonious benchmarks, where \method{} rises and accuracy shifts significantly. Model-averaged \method{} values for the original and pruned benchmarks are reported in parentheses in the legends.}
    \label{fig:pruning}
\end{figure}

As shown in Figure~\ref{fig:pruning}, model accuracies on high \method{} benchmarks remain stable under pruning, with differences that are not statistically significant despite increased \method{}. In contrast, low \method{} benchmarks are fragile, where pruning notably improves \method{} and aligns with statistically significant accuracy changes. Details of our significance tests appear in Appendix~\ref{sec:details_of_stat_sig}. As \method{} increases, per-subdomain accuracies tighten around the benchmark mean, making the aggregate a more faithful representation of the underlying accuracy distribution. Overall, low \method{} benchmarks can be misleading as they skew aggregate scores, whereas high \method{} benchmarks provide more reliable and representative evaluations.

While Figure~\ref{fig:pruning} illustrates our findings for the Qwen3 family, we provide the comprehensive results for the full experimental setup in Appendix~\ref{sec:extended_balancing}.

\subsection{How does \method{} change across model sizes and token budgets?}
% \subsection{Scaling Trends for \method{}}
\label{subsec:subsec6.2}
Given that low \method{} signals fragility, we now ask whether this risk depends on model scale or training budget. We therefore seek to characterize how \emph{\method{}} scales with model parameters and pre-training budget. Rather than focusing on raw accuracy, our goal is to understand whether larger models or longer pre-training runs yield more uniform performance across subdomains. Concretely, we pose two questions. \emph{Model size:} As parameter count increases within a family, does \method{} steadily rise, indicating broader competence across subsets? \emph{Token budget:} Along a fixed architecture, does increasing pre-training token budget improve \method{}, suggesting a more even reallocation of accuracy on the benchmark?

\paragraph{Model parameters.} We observe that \textbf{the relationship between model parameter count and \method{} is \emph{family-specific} rather than universal}. As shown in Fig.~\ref{fig:parameter_sl}, within-family comparisons reveal a negative correlation for Qwen and Llama families, indicating that larger models in these families concentrate performance more on a few subdomains. In contrast, Gemma and OLMo families exhibit a positive correlation between model size and \method{}, with larger models distributing accuracy more evenly across the subdomains in the benchmark. This suggests that parameter count alone is not a sufficient indicator of uniformity of the performance.

\begin{figure}[ht]
    \centering    \includegraphics[width=\linewidth]{figs/parameter_sl.png}
    \caption{\textbf{Model size vs.\ \method{}.} Scaling trends are \emph{family-specific}: Qwen and Llama show negative correlations, while Gemma and OLMo show positive correlations (larger models perform more uniformly). Thus, parameter count alone is not predictive of performance uniformity. Y-axis shows each model’s average \method{} over all benchmarks (\S\ref{subsec:exp_setup}).}
    \label{fig:parameter_sl}
\end{figure}

\paragraph{Pre-training tokens.}
Under a fixed architecture, \textbf{increasing the pre-training token budget yields a rise in \method{}}, after an initial dip. We examine how \method{} evolves under a fixed architecture by tracking OLMo2 1B and OLMo2 7B across increased token budgets. As shown in Fig.~\ref{fig:token_sl}, \method{} dips early and then rises steadily, while aggregate accuracy increases minimally across checkpoints. Thus, we find that the distribution of performance improves (i.e., increased \method{}) even as aggregate accuracy remains nearly unchanged. In other words, additional pre-training reallocates competence from a few dominant subdomains toward a more uniform spread, yielding a strictly more favorable accuracy profile without changing the aggregate score drastically.

\begin{wrapfigure}[23]{r}{0.5\linewidth}
    \vspace{-0.6cm}
    \centering    
    \includegraphics[width=\linewidth]{figs/token.png}
    \vspace{-0.6cm}
    \caption{\textbf{Pre-training tokens vs.\ \method{}.} For OLMo 2 1B/7B, \method{} dips then steadily rises with more pre-training tokens while aggregate accuracy improves slightly, indicating competence shifts from dominant subsets toward greater uniformity. Y-axis shows each model’s average \method{} over all benchmarks (\S\ref{subsec:exp_setup}).}
    \label{fig:token_sl}
\end{wrapfigure}

%This pattern echoes grokking, where test generalization improves with little change in training loss. Following \citet{varma2023explaininggrokkingcircuitefficiency}, who view grokking as a switch from overfitting to generalization, we adopt this analogy.

In addition to \method, we also formulate the uniformity of improvements that come with scaling and share our findings in Appendix~\ref{sec:imp_harmony}. We emphasize that these findings are empirical rather than causal. We leave modeling the mechanisms underlying these trends as valuable future work.

\section{Related Work} 
% \tianjian{It is also a bit too long, consider moving one paragraph to the appendix.}
% \daniel{+1. "Efficient Eval" should be a good candidate to move. The other two paragraphs are long as well. You need to add some structure to make it easier to read. You also need to tie them to your contribution: how does your work distinguish with or improve upon these works? }

\paragraph{Assessing Benchmark Reliability.} Beyond proposing new tasks, a growing body of work interrogates the \emph{reliability} of benchmarks themselves. A line of work targets the robustness of the test sets, focusing on building dynamic benchmarks to replace static benchmarks \citep{kiela2021dynabenchrethinkingbenchmarkingnlp, chiang2024chatbot} and building adversarial perturbations to eliminate spurious cues present in static benchmarks \citep{nie2020adversarialnlinewbenchmark, croce2021robustbenchstandardizedadversarialrobustness}. Closely related are concerns about overfitting to public test sets and contamination from pre-training corpora, which can inflate reported gains \citep{deng2024investigatingdatacontaminationmodern, golchin2024timetravelllmstracing, roberts2023datacontaminationlenstime, dong-etal-2024-generalization}. \citet{dbevalgauntlet} analyzes a collection of benchmarks, showing that some benchmarks (e.g. Hellaswag \citep{zellers2019hellaswagmachinereallyfinish}) scale smoothly with increased scale and compute, while others (e.g. CommonsenseQA \citep{talmor2018commonsenseqa}) do not. 
Another branch of literature audits data reliability and distributional coverage by introducing shifted test sets to probe generalization \citep{recht2019imagenetclassifiersgeneralizeimagenet, taori2020measuringrobustnessnaturaldistribution, teney2020valueoutofdistributiontestingexample} and correcting pervasive label errors in widely used benchmarks \citep{northcutt2021pervasivelabelerrorstest, gema2025mmlu}. Beyond individual datasets, meta-evaluation work proposes frameworks and documentation practices to systematically assess benchmark design, provenance, and intended use \citep{reuel2024betterbenchassessingaibenchmarks, mazumder2023dataperfbenchmarksdatacentricai, gebru2021datasheetsdatasets}. Another important topic is the external validity of benchmarks, such as how well leaderboard gains translate to real-world performance \citep{Ott_2022} and what reported scores actually measure \citep{dehghani2021benchmarklottery, singh2025leaderboardillusion}. Finally, a complementary line of work separates signal from noise in benchmark results by quantifying variance and prescribing protocols that stabilize rankings in order to make comparative conclusions more reliable \citep{madaan2024quantifyingvarianceevaluationbenchmarks, evalarena, heineman2025signalnoiseframeworkreducing}. Advancing this field of work, we contribute a distributional perspective on benchmark reliability. Rather than treating a benchmark evaluation as a single score, we model the benchmark as a mixture over the subdomains of the stated benchmark domain. We then measure how \emph{performance mass} is distributed across these subdomains. This perspective diagnoses whether aggregate metrics reflect a broad competence over the benchmark or are dominated by certain subdomains.


% \textbf{Efficient Evaluation.} 
\textbf{Distributional Frameworks for Efficient Evaluation.} 
Scaling laws of neural language models suggest that performance improves with model size \citep{kaplan2020scalinglawsneurallanguage}, encouraging the development of increasingly larger and costlier models. Consequently, there has been growing interest in developing efficient evaluation methods that reduce computational and financial costs without compromising reliability. \citet{perlitz-etal-2024-efficient} introduce a reliability metric that dynamically adjusts compute by performance tier while preserving rank fidelity. \citet{rodriguez-etal-2021-evaluation} propose Item Response Theory \citep{Tatsuoka1971StatisticalTO} based leaderboards that jointly model difficulty and discrimination to identify examples that best differentiate model performance. Similarly, \citet{polo2024tinybenchmarksevaluatingllmsfewer} propose tinyBenchmarks, an efficient evaluation method that uses IRT to model the discriminative power of benchmark examples, allowing the selection of a small yet representative subset of items that can accurately estimate performance. \citet{vivek2024anchorpointsbenchmarkingmodels} propose anchor point selection to identify small, representative subsets by leveraging cross-model correlations in instance-level predictions. \citet{ethayarajh2022understandingdatasetdifficultymathcalvusable} identify informative data points via \emph{usable information} (how much input a model family can exploit) extending Shannon information to account for model constraints. Notably, these works introduce distinct metrics such as IRT item parameters, cross-model instance correlations, and information-theoretic usable information to characterize the benchmark distribution and guide principled compression of benchmarks. Ultimately, these metrics enable targeted downsampling (e.g., selecting maximally discriminative or most informative items) that preserves rankings and reduces evaluation cost while maintaining coverage. In contrast, we do not seek cheaper evaluations. We instead assess whether a benchmark reliably measures its stated domain and, where it does not, we question the original evaluation rather than preserve it.

Prior work mainly (i) proposes new or dynamic tests, (ii) stabilizes leaderboards through variance control and guidelines, and (iii) compresses evaluation via discriminative selection. We instead audit \emph{existing} benchmarks through a distributional lens, modeling a benchmark as a mixture over subdomains and measuring whether models spread accuracy uniformly. Unlike efficiency work that preserves overall scores while reducing cost, \method{} reveals where aggregate metrics fail to provide a representative understanding of model competency. Our method is post hoc and lightweight, complements robustness and contamination audits, and yields practical guidance: report \method{} with accuracy and rebalance low \method{} benchmarks.

We further discuss additional related work on language model evaluation in Appendix~\ref{sec:add_rel_work}.

\vspace{-0.5em}
\section{Conclusion}
\vspace{-0.5em}
We introduce \method{}, an entropy-based measure of how uniformly performance is distributed across a benchmark’s subdomains. Mapping 19 MCQA benchmarks across five model families on the \method{} mean-variance plane reveals a spectrum of reliability. High mean and low variance indicate that aggregate metrics consistently reflect broad competence across models. In contrast, low mean signals that performance concentrates on a few subdomains and high variance indicates model-dependent reliability. Therefore, benchmarks with high mean and low variance of \method{} enable more reliable evaluation.

Controlled pruning shows that increasing \method{} stabilizes aggregate accuracy by reducing overrepresented subdomains. Moreover, we find that scaling trends of performance uniformity are family specific, rendering the number of parameters as an unreliable indicator for the uniformity of model performance. Nevertheless, models perform more uniformly on average as the pre-training budget increases. \method{} complements aggregate accuracy by exposing when performance gains reflect uniform competence versus concentrated strengths and supports multi-dimensional evaluation that makes subdomain trade-offs explicit.

\section*{Ethics Statement} 
This work evaluates publicly available MCQA benchmarks and introduces \textit{RedundantQA}, a synthetic dataset generated from author-written seeds and LLM generations with no human subjects, personal data, or sensitive attributes collected. All examples were screened to avoid offensive content and verbatim copyrighted material. We respect the licenses of all benchmarks and models used. We will release our code and RedundantQA with documentation of construction, intended use, and limitations.

\method{} is intended to complement (and \emph{not replace}) standard accuracy and robustness analyses. Potential risks include misinterpretation of \method{} or benchmark rebalancing to mask undesired failure modes. We therefore report both harmony and accuracy and encourage transparent, multi-metric evaluation. Our experiments rely on inference with open-sourced models. The authors declare no conflicts of interest or external sponsorship that could bias this work. We declare use of Large Language Models in this work in Appendix~\ref{sec:use_of_llms}.

\section*{Reproducibility Statement}
We describe all experiments, datasets, models, and evaluation protocols openly and in detail in the main paper and appendix, including the construction of RedundantQA, the predictive similarity computation, partition induction, \method{} definition and computation, pruning procedures, and statistical significance tests. We report model and benchmark versions, inference settings, and random seeds, and we specify hyperparameters and implementation choices (e.g., bandwidth selection, similarity scaling, and clustering criteria) to enable reproducibility. We also provide clear references to where each component is defined (Sections \S\ref{sec:benchmark_harmony}–\S\ref{sec:beyond} and Appendices \ref{sec:redundantqa}–\ref{sec:hier_labeling}). We will open-source our codebase and findings, as well as RedundantQA, to facilitate exact replication.

\bibliography{iclr2026_conference}
\bibliographystyle{iclr2026_conference}

\appendix
\section{RedundantQA}
\label{sec:redundantqa}
To rigorously evaluate the discriminative power of similarity metrics, we construct RedundantQA, a controlled benchmark designed to disentangle genuine semantic similarity from superficial lexical overlap. Each set in RedundantQA consists of a reference question accompanied by two \textit{true-similar} and two \textit{false-similar} questions. The true-similar questions are paraphrases that evaluate the same underlying knowledge as the reference, while differing in surface form.\footnote{E.g., variations in vocabulary, syntax, or phrasing.} In contrast, the false-similar questions exhibit high lexical similarity to the reference but target distinct conceptual content. This design ensures that strong similarity metrics must go beyond surface-level cues, rewarding semantic alignment while ignoring spurious correlations.

In this section, we detail the construction (\ref{subsec:gen_redundantqa}) and validation (\ref{subsec:val_redundantqa}) of RedundantQA, as well as showcasing examples (\ref{subsec:examples_redundantqa}).

\subsection{Construction}
\label{subsec:gen_redundantqa}
We construct RedundantQA through a two‐phase pipeline followed by strict validation: \textbf{(i) Seed Set Selection.}  We begin by manually authoring three high‐quality reference questions across four domains (Biology, Economics, Popular Culture, History). For each reference question, we also craft two paraphrases that target the same underlying knowledge (\emph{true‐similar}) and two distractors that share surface tokens but probe different concepts (\emph{false‐similar}). \textbf{(ii) Generative Expansion.} Using the seed sets as in-context learning examples, we prompt \texttt{Gemini-2.0-flash} \citep{deepmind2023gemini} to generate 100 sets that consist of one reference question, two true-similar questions, and two false-similar questions for each domain. For different domains, we use a fixed template (Listing \ref{box:gen-prompt}) with domain‐specific examples. This pipeline yields a large, automatically generated candidate pool.

\begin{tcolorbox}[
    float=htb,
    title={Prompt for Generating RedundantQA (Biology)},
    colback=gray!5!white,
    colframe=black!75,
    fonttitle=\bfseries,
    label={box:gen-prompt}
]
\small
Come up with question sets. Each set must contain:
\begin{itemize}
    \item A reference question,
    \item Two same-meaning questions: These should require the same factual answer and test the same biological concept as the reference question, but they should use different wording, phrasing styles, and sentence structures.
    \item Two distractor questions: These should look superficially very similar to the reference question but evaluate a different knowledge or skill with different answers than the reference question.
\end{itemize}

\textbf{Notes:}
\begin{itemize}
    \item Focus on the domain of biological knowledge.
    \item Same-meaning questions should preserve deep semantic equivalence but vary stylistically. These must have the same answer.
    \item Distractor questions should maximize shallow textual similarity (e.g., shared nouns, verbs, syntactic patterns) while changing the underlying meaning. So, distractor questions should trick an incapable similarity measure into thinking they are similar.
\end{itemize}

\textbf{Examples:}

\textbf{Set 1}
\begin{itemize}
    \item \textit{Reference Question:} What organ pumps blood throughout the human body?
    \item \textit{Same-meaning Question 1:} Which organ circulates blood to deliver oxygen and nutrients?
    \item \textit{Same-meaning Question 2:} What body system structure maintains blood flow across the body?
    \item \textit{Distracting Question 1:} What organ removes carbon dioxide from the blood?
    \item \textit{Distracting Question 2:} What organ transports nutrients through the blood?
\end{itemize}

\textbf{Set 2}
\begin{itemize}
    \item \textit{Reference Question:} What process converts glucose into energy in cells?
    \item \textit{Same-meaning Question 1:} Which process produces ATP from sugar molecules?
    \item \textit{Same-meaning Question 2:} What pathway transforms glucose into usable cellular energy?
    \item \textit{Distracting Question 1:} What process stores glucose in cells?
    \item \textit{Distracting Question 2:} What process breaks down proteins for energy?
\end{itemize}

\textbf{Set 3}
\begin{itemize}
    \item \textit{Reference Question:} What type of blood vessel carries blood away from the heart?
    \item \textit{Same-meaning Question 1:} Which vessels transport oxygenated blood from the heart?
    \item \textit{Same-meaning Question 2:} What structures move blood outward from the heart?
    \item \textit{Distracting Question 1:} What type of blood vessel brings blood to the heart?
    \item \textit{Distracting Question 2:} What blood vessel type filters blood in the kidneys?
\end{itemize}
\end{tcolorbox}

\subsection{Validation}
\label{subsec:val_redundantqa}
We validate each set generated by \texttt{Gemini-2.0-flash} through a two‐stage pipeline: (a) an automated and simple consistency check using \texttt{Gemini-2.0-flash} to confirm that true‐similar paraphrases produce identical answers while false‐similar distractors yield divergent ones (using Listing \ref{box:valid-prompt}); and (b) a manual review by expert annotators to correct any misclassifications, formatting issues, or errors introduced during automated filtering. After the validation step, we obtain 71, 39, 72, and 73 sets from Biology, Economics, Culture, and History domains respectively, with each set consisting of one reference question, two true-similar questions, and two false-similar questions.

This procedure yields a benchmark in which effective similarity metrics must discriminate semantic equivalence from mere lexical coincidence.

\begin{tcolorbox}[
    float=htb,
    title={Prompt for Validating RedundantQA},
    colback=gray!5!white,
    colframe=black!75,
    fonttitle=\bfseries,
    label={box:valid-prompt}
]
\small
Do the following questions have the same answer? Output only yes or no. \\
Question 1: REFERENCE\_QUESTION \\
Question 2: TRUE\_SIM\_1
\end{tcolorbox}

\subsection{Examples}
\label{subsec:examples_redundantqa}
We provide examples from RedundantQA across all four domains in Table \ref{tab:examples_redundantqa}.
\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{
\scriptsize
\begin{tabular}{|p{2.7cm}|p{4.2cm}|p{2.9cm}|p{3.1cm}|}
\hline
\textbf{Biology} & \textbf{Economics} & \textbf{History} & \textbf{Popular Culture} \\
\hline
\begin{tabular}[t]{@{}l@{}}
\textbf{Reference} \\
What process converts\\ glucose into energy in cells? \\
A: Cellular respiration \\
B: Photosynthesis \\
C: Osmosis \\
D: Transcription \\[6pt]
\textbf{True Similar} \\
Which process produces\\ ATP from sugar molecules? \\
A: Cellular respiration \\
B: Photosynthesis \\
C: Osmosis \\
D: Transcription \\[6pt]
\textbf{False Similar} \\
What process stores\\ glucose in cells? \\
A: Glycogenolysis \\
B: Gluconeogenesis \\
C: Glycogenesis \\
D: Glycolysis
\end{tabular} &
\begin{tabular}[t]{@{}l@{}}
\textbf{Reference} \\
How does increased government\\ spending affect aggregate demand? \\
A: Increases it. \\
B: Decreases it. \\
C: Has no effect. \\
D: Only affects aggregate supply. \\[6pt]
\textbf{True Similar} \\
What happens to total demand in economy\\ when the government increase its spending? \\
A: Increases it. \\
B: Decreases it. \\
C: Has no effect. \\
D: Only affects aggregate supply. \\[6pt]
\textbf{False Similar} \\
How does increased government\\ spending affect government debt? \\
A: Increases it. \\
B: Decreases it. \\
C: Has no effect. \\
D: Only affects short-term debt.
\end{tabular} &
\begin{tabular}[t]{@{}l@{}}
\textbf{Reference} \\
Who was the first president\\ of the United States? \\
A: George Washington \\
B: Abraham Lincoln \\
C: Thomas Jefferson \\
D: John Adams \\[6pt]
\textbf{True Similar} \\
Who assumed leadership as\\ America's first head of state? \\
A: George Washington \\
B: Abraham Lincoln \\
C: Thomas Jefferson \\
D: John Adams \\[6pt]
\textbf{False Similar} \\
Who was the first vice\\ president of the United States? \\
A: John Adams\\
B: Thomas Jefferson\\
C: Alexander Hamilton\\
D: James Madison
\end{tabular} &
\begin{tabular}[t]{@{}l@{}}
\textbf{Reference} \\
Who played Iron Man in the\\ Marvel Cinematic Universe? \\
A: Robert Downey Jr.\\
B: Chris Evans\\
C: Hugh Jackman\\
D: Tobey Maguire\\[6pt]
\textbf{True Similar} \\
Which actor portrayed\\ Tony Stark in the MCU? \\
A: Robert Downey Jr.\\
B: Chris Evans\\
C: Hugh Jackman\\
D: Tobey Maguire\\[6pt]
\textbf{False Similar} \\
Who played Captain America in\\ the Marvel Cinematic Universe? \\
A: Chris Evans\\
B: Chris Pratt\\
C: John Krasinski\\
D: Matt Damon
\end{tabular} \\
\hline
\end{tabular}}
\vspace{0.5em}
\caption{Example sets across all domains in RedundantQA.}
\label{tab:examples_redundantqa}
\end{table}

\section{Predictive Similarity}
\label{sec:pred_sim_app}

\subsection{Alternative Baselines}
\label{subsec:alt_baselines}
In this section, we describe the alternative baselines compared against predictive similarity across a range of controlled settings.

\paragraph{Bigram.}
We compute an $n$-gram-overlap Jaccard similarity matrix. For each text $x_i$, we lowercase and split on whitespace, then form the set $G_i$ of contiguous bigrams. The pairwise similarity is $S_{ij} = \dfrac{|G_i \cap G_j|}{|G_i \cup G_j|}$
with $S_{ii}=1$ for all $i$.\footnote{Note that if a text has fewer than $n$ tokens, its $n$-gram set is empty. In such a case, the pairwise similarity is set to be 1. However, this is not observed in practice.} The resulting $S \in [0,1]^{N\times N}$ is symmetric and measures surface-form overlap.

\paragraph{BERTScore F1.} We use BERTScore \citep{zhang2020bertscoreevaluatingtextgeneration} to measure semantic similarity between pairs of texts by comparing their contextualized token embeddings. Tokens are greedily matched via cosine similarity to compute precision and recall, and the final sentence-sentence score is the F1 aggregate, where
$F_1 = \tfrac{2PR}{P+R}$. We treat this F1 value as the pairwise similarity, which yields a symmetric matrix $S \in [-1,1]^{N\times N}$. We employ \texttt{Roberta$_\text{Large}$} \citep{liu2019robertarobustlyoptimizedbert} for obtaining the contextualized token embeddings.

\paragraph{Input Embeddings Cosine Similarity.}
We map each input to a single vector and measure pairwise similarity via cosine in embedding space. We use two variants: (i) for each example, we take the last-token hidden state from the model under evaluation, $\ell_2$-normalize it, and set $S_{ij}=\hat{h}_i^\top \hat{h}_j$, (ii) we encode each input with a frozen sentence-embedding model, normalize the embeddings, and compute the same cosine-based matrix. In both cases, we obtain a symmetric matrix $S \in [-1,1]^{N\times N}$. For the sentence-embedding variant, we use \texttt{MiniLM}\footnote{https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2} \citep{wang2020minilmdeepselfattentiondistillation} and \texttt{gte-Qwen2-7B-instruct}\footnote{https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct} \citep{li2023towards}; for the model-under-evaluation variant, we use \texttt{microsoft/Phi-3-mini-4k-instruct}\footnote{https://huggingface.co/microsoft/Phi-3-mini-4k-instruct} \citep{abdin2024phi3technicalreporthighly}, which yields the best performance in Appendix \ref{subsec:sem_vs_lex}.

\paragraph{Input and Output Embeddings Cosine Similarity.}
We represent each input-output pair (e.g., question+answer) as a single vector by taking the last-token hidden state of the concatenated sequence from the model under evaluation. We $\ell_2$-normalize these vectors and define pairwise similarity via cosine with $S_{ij}=\hat{h}_i^\top \hat{h}_j$. The resulting $S \in [-1,1]^{N\times N}$ is symmetric and reflects similarity over both the question and its associated answer. We use \texttt{microsoft/Phi-3-mini-4k-instruct} \citep{abdin2024phi3technicalreporthighly} for obtaining the hidden states, as it yields the best performance in Appendix \ref{subsec:sem_vs_lex}.

\paragraph{G-Vendi.} Following \cite{jung2025prismaticsynthesisgradientbaseddata}, we quantify the diversity of per–example gradients via a sketch-based spectral entropy. For each example, we form a compact count-sketch of the gradient of the (negative) log-probability of the correct answer under a proxy LM, yielding a matrix $G\!\in\!\mathbb{R}^{N\times d}$. We compute $C=\frac{1}{N}G^\top G$ and its eigenvalues $\{\lambda_i\}$. Let $p_i=\lambda_i/\sum_j \lambda_j$; the \emph{G-Vendi} score is the exponential Shannon entropy of this spectrum:
\[
\mathrm{G\text{-}Vendi}\;=\;\exp\!\Big(-\sum_i p_i \log p_i\Big),
\]
which acts as an effective rank where higher values indicate gradients spread across more orthogonal directions and lower values indicate concentration in a low dimensional subspace. For pairwise similarity, we $\ell_1$-normalize the sketch rows of $G$ and take their dot products to obtain a symmetric similarity matrix with unit diagonal. Following the original implementation, we employ \texttt{Qwen2.5-0.5B-Instruct} \citep{qwen2025qwen25technicalreport} as the proxy model.

\begin{wraptable}{r}{0.45\linewidth}
\centering
\scriptsize
\sisetup{
  detect-weight=true,
  detect-family=true,
  round-mode=places,
  round-precision=1,
  table-format=2.1,
}
\begin{tabular}{l S}
\toprule
\textbf{Method} & {\textbf{Duplicate Catch Ratio} ($\uparrow$)} \\
\midrule
\multicolumn{2}{l}{\textit{N-gram \& Token}} \\
Bigram            &  96.3 \\
BERTScore F1      & 100.0 \\
\midrule
\multicolumn{2}{l}{\textit{Embedding-Based}} \\
Input Embeddings$_\text{MiniLM}$        & 100.0  \\
Input Embeddings$_\text{gte-Qwen2}$     & 100.0  \\
Input Embeddings$_\text{phi3}$          & 100.0  \\
Input+Output Embeddings$_\text{phi3}$   & 100.0  \\
\midrule
\multicolumn{2}{l}{\textit{Literature}} \\
G-Vendi  & 100.0  \\
CORRS$_\text{Llama}$ &  100.0  \\
CORRS$_\text{all}$   & 100.0 \\
IRT Representation &  100.0 \\
\midrule
Predictive Similarity & 100.0 \\
\bottomrule
\end{tabular}
\caption{\textbf{Validation of our metric implementations.} All metrics other than bigram similarity perfectly catch exact duplicate question, satisfying the minimum requirement to ensure implementation accuracy.}
\label{tab:nec_condition_wrap}
\end{wraptable}

\paragraph{CORRS.} Following \cite{vivek2024anchorpointsbenchmarkingmodels}, given a bank of source models, we map each input $i$ to a vector $v_i \in \mathbb{R}^M$ whose $m$-th entry is the logit of the probability that model $m$ assigns to the correct choice. Then, the similarity between two examples is defined as the Pearson correlation of these vectors with $S_{ij}=\mathrm{corr}(v_i, v_j)$. The resulting $S \in [-1,1]^{N\times N}$ is symmetric and represents the cross-model agreement in correct class confidence across inputs. We instantiate the source bank using the Llama model family \citep{grattafiori2024llama} and the full set of models used in our experiments.

\paragraph{IRT Representation.} Following \cite{polo2024tinybenchmarksevaluatingllmsfewer}, from a bank of source models, we form a binary response matrix $Y\in\{0,1\}^{L\times N}$ whose $(\ell,i)$ entry indicates whether model $\ell$ answered example $i$ correctly. We then fit a $d$-dimensional IRT model with per-example parameters $(\alpha_i\in\mathbb{R}^d,\ \beta_i\in\mathbb{R})$ and per-model ability vectors $\theta_\ell\in\mathbb{R}^d$, using
\[
\Pr(Y_{\ell i}=1)=\sigma\!\big(-\,\theta_\ell^\top \alpha_i + \beta_i\big).
\]
Here, optimization alternates between gradient updates for $\theta$ (with $\ell_2$ regularization and recentring) and logistic regressions to update $(\alpha_i,\beta_i)$. Finally, we obtain the embedding $E_i=\big[\alpha_i;\beta_i\big]\in\mathbb{R}^{d+1}$ and define pairwise similarity by cosine similarity as $S_{ij}=\tfrac{E_i^\top E_j}{\|E_i\|\,\|E_j\|}$, which yields a symmetric matrix $S \in [-1,1]^{N\times N}$. We use $d=200$ and instantiate the source bank using the full set of models used in our experiments.

As open-source implementations of efficient evaluation metrics are unavailable, we re-implement them following the specifications in prior work. We then validate our implementations via a sanity check in which each metric was tasked with detecting verbatim duplicate questions. As shown in Table~\ref{tab:nec_condition_wrap}, all metrics (with the exception of bigram similarity) achieve perfect performance, satisfying the minimum requirement to ensure implementation accuracy.

\subsection{Measuring Semantic Similarity}
\label{subsec:sem_vs_lex}
An effective similarity measure for uncovering underlying data distributions must exhibit strong discriminative power, reliably identifying semantically similar data points while rejecting distractors. We empirically validate that predictive similarity meets this criterion, as it consistently distinguishes true semantic matches from misleading surface-level overlaps in RedundantQA.

\begin{table}[ht]
\centering
\resizebox{\linewidth}{!}{%
\scriptsize
\sisetup{
  detect-weight=true,
  detect-family=true,
  round-mode=places,
  round-precision=1,
  table-format=2.1,
}
\begin{tabular}{
    l
    S S S S S
    S S S S S
}
\toprule
\multirow{2}{*}{\textbf{Method}}
  & \multicolumn{5}{c}{\textbf{True Similar} ($\uparrow$)}
  & \multicolumn{5}{c}{\textbf{False Similar} ($\downarrow$)} \\
\cmidrule(lr){2-6} \cmidrule(lr){7-11}
  & {\textbf{Biology}} & {\textbf{Economics}} & {\textbf{Culture}} & {\textbf{History}} & {\textbf{All}}
  & {\textbf{Biology}} & {\textbf{Economics}} & {\textbf{Culture}} & {\textbf{History}} & {\textbf{All}} \\
\midrule
\textit{N-gram \& Token} \\
Bigram
  &  1.4 &  0.0 &  6.9 &  7.0 &  4.5
  & 70.0 & 67.6 & 30.6 & 47.9 & 53.7 \\
BERTScore F1
  &  8.6 &  0.0 & 12.5 & 21.1 & 12.4
  & 74.3 & 83.8 & 33.3 & 54.9 & 60.3 \\
\midrule
\textit{Embedding-Based} \\
Input Embeddings$_\text{MiniLM}$
  & 42.9 & 24.3 & \underline{62.5} & \underline{66.2} & \underline{54.1}
  & 27.1 & 51.4 &  5.6 & 12.7 & 21.1 \\
Input Embeddings$_\text{gte-Qwen2}$
  & 18.6 & 18.9 & 23.6 & 39.4 & 26.9
  & 38.6 & 51.4 & 26.4 & 22.5 & 33.5 \\
Input Embeddings$_\text{phi3}$
  & 21.4 &  5.4 & 16.7 & 36.6 & 22.7
  & 47.1 & 75.7 & 37.5 & 31.0 & 45.5 \\
Input+Output Embeddings$_\text{phi3}$
  & 51.4 & \underline{62.2} & 40.3 & 56.3 & 52.9
  & 22.9 & 27.0 & 16.7 & 15.5 & 20.2 \\
\midrule
\textit{Literature} \\
G-Vendi
  & \underline{62.9} & 37.8 & 36.1 & 46.5 & 47.9
  &  5.7 &  2.7 &  6.9 &  5.6 &  5.8 \\
CORRS$_\text{Llama}$
  &  2.9 &  8.1 &  9.7 & 14.1 &  9.1
  &  1.4 &  0.0 &  1.4 &  1.4 &  1.2 \\
CORRS$_\text{all}$
  & 35.7 & 13.5 & 59.7 & 59.2 & 47.5
  &  1.4 &  5.4 &  0.0 &  1.4 &  1.7 \\
IRT Representation
  &  1.4 &  0.0 &  1.4 &  0.0 &  0.8
  &  1.4 &  0.0 &  0.0 &  2.8 &  1.2 \\
\midrule
Predictive Similarity
  & \textbf{80.0} & \textbf{86.5} & \textbf{66.6} & \textbf{73.2} & \textbf{77.7}
  & 1.4 &  2.7 &  0.0 &  4.2 &  2.1 \\
\bottomrule
\end{tabular}
}
\caption{Proportion of identified true-similar ($\uparrow$) and false-similar ($\downarrow$) pairs by method and domain.}
\label{tab:suf_condition_restyled}
\end{table}

As shown in Table~\ref{tab:suf_condition_restyled}, predictive similarity achieves the highest retrieval of true semantic matches across all domains, while maintaining one of the lowest rate of false matches. This indicates that it captures semantic equivalence without being misled by superficial lexical similarity. In contrast, embedding-based and bigram baselines suffer from high false positives, conflating surface-level resemblance with meaning. Metrics from efficient evaluation literature show stronger performance but still fall short of predictive similarity. Overall, these results highlight the unique discriminative advantage of predictive similarity in measuring semantic similarity.

\subsection{Inducing the Semantic Partition}
\label{subsec:recover_dom}

\begin{wraptable}{r}{0.55\linewidth}  % {l} for left; set width you want
  \vspace{-2em}
  \centering
  \scriptsize                         % or \small
  \setlength{\tabcolsep}{4pt}         % tighten horizontally (optional)
  \renewcommand{\arraystretch}{0.95}  % tighten 
  \resizebox{\linewidth}{!}{%
  \begin{tabular}{lccccccc}
    \toprule
    \textbf{Method} & \multicolumn{2}{c}{\textbf{\texttt{RedundantQA}}} &
                      \multicolumn{2}{c}{\textbf{\texttt{RedundantQA-Ref}}} &
                      \multicolumn{2}{c}{\textbf{\texttt{MMLU-HS}}} \\
     & ARI & NMI & ARI & NMI & ARI & NMI \\
    \midrule
    \textit{N-gram \& Token} \\
    Bigram & -0.3 & 2.5 & -0.4 & 6.7 & 0.1 & 6.2  \\
    BERTScore F1 & -0.2 & 1.6 & 2.2 & 8.6 & 1.4 & 7.9 \\
    \midrule
    \textit{Embedding-Based} \\
    Input Embeddings$_\text{MiniLM}$      & 55.8 & 64.4 & 59.4 & 68.4  & 47.4 & 56.8 \\
    Input Embeddings$_\text{gte-qwen2-7b-instruct}$ & 28.6 & 34.9 & 36.3 & 45.9 & 27.1 & 34.4 \\
    Input Embeddings$_\text{phi3}$        & 27.8 & 37.1 & 33.0 & 45.6 & 25.1 & 31.6 \\
    Input+Output Embeddings$_\text{phi3}$ & 57.1 & 67.0 & 58.9 & 70.3 & 51.3 & 59.6 \\
    \midrule
    \textit{Literature} \\
    G-Vendi & 6.5 & 11.3 & 4.7 & 10.9 & 3.2 & 9.2 \\
    CORRS$_\text{all}$ & 6.1 & 8.2 & 4.8 & 8.2 & 2.8 & 8.4 \\
    IRT Representation & 2.1 & 2.7 & 0.4 & 1.9 & 0.4 & 1.5 \\
    \midrule
    Predictive Similarity & 60.4 & 70.4 & 62.5 & 76.5 & 55.4 & 62.1 \\
    \bottomrule
  \end{tabular}%
  }

  \caption{Validation of our partition induction method on RedundantQA and MMLU. Adjusted Rand Index (ARI) and Normalized Mutual Information (NMI) (both are higher is better) are shown for different methods on \texttt{\textbf{RedundantQA}}, \texttt{\textbf{RedundantQA-Ref}}, and \texttt{\textbf{MMLU-HS}}. Predictive similarity consistently achieves the best domain recovery.}
  \label{tab:redundantqa_results}
\end{wraptable}

A core requirement for our work is that the similarity function should induce a semantic partition of the data. We evaluate this property by inducing cluster assignments from each metric and measuring agreement with the ground-truth domain labels in RedundantQA and its reference-only subset (RedundantQA-Ref) using Adjusted Rand Index (ARI) and Normalized Mutual Information (NMI). In addition, we apply the same protocol to MMLU high school subtasks, testing whether clusters recover canonical subject domains (e.g., computer science, biology, physics).

As shown in Table~\ref{tab:redundantqa_results}, predictive similarity achieves the highest agreement on all three sets, while strong embedding-based baselines are competitive yet consistently behind. By contrast, token/ngram measures and metrics from the efficient evaluation literature fail to recover domain structure, indicating that they are unreliable for semantic grouping. Taken together with the pairwise retrieval evidence, these results show that predictive similarity not only discriminates true semantic matches from distractors, but also organizes instances into compact and consistent clusters. This behavior is precisely what enables a cluster-centric analysis of benchmarks, yielding low within-cluster variance and high between-cluster separation.

\subsection{Capturing Benchmark \method{}}
\label{subsec:capture_quality}
We evaluate all alternative similarity baselines from Appendix \ref{subsec:alt_baselines} in the identical setting of \S\ref{subsec:empirical_val}. For each baseline, we induce clusters from its similarity matrix, compute \method{} $H(\mathcal{G})$, and report correlations between the ground-truth \method{} and its counterpart computed from the partition induced by each baseline.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figs/baselines_validation.png}
    \caption{\textbf{Validation of our partition induction method on RedundantQA}. Predictive similarity achieves the strongest correlation between the ground truth \method{} and estimated \method{}, while input embeddings derived from \texttt{MiniLM} is a close runner-up.}
    \label{fig:baseline_validations}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figs/baselines_validation_mmlu_hs.png}
    \caption{\textbf{Validation of our partition induction method on MMLU}. Similar to the results on RedundantQA, predictive similarity achieves the strongest correlation between the ground truth \method{} and estimated \method{}, while input embeddings derived from \texttt{MiniLM} is a close runner-up.}
    \label{fig:baseline_validations_mmlu}
\end{figure*}

As shown in Fig. \ref{fig:baseline_validations} and \ref{fig:baseline_validations_mmlu}, predictive similarity achieves the strongest correlation with ground-truth entropy. Similar to prior validation experiments (App. \ref{subsec:sem_vs_lex}, \ref{subsec:recover_dom}), embedding-based baselines are the next-best performers but consistently lag behind, whereas token-and-$n$-gram overlap measures perform substantially worse. These results establish predictive similarity as the most reliable similarity metric choice for capturing the benchmark dynamics.

\subsection{Consistency of Predictive Similarity Across Models}
\label{subsec:consistency_pred_sim}
As defined in \S\ref{subsec:methodology}, predictive similarity induces, for each benchmark $\mathcal{B}$ and model $f$, a symmetric similarity matrix $S^{(f,\mathcal{B})} \in (0,1]^{N_\mathcal{B} \times N_\mathcal{B}}$ over the $N_\mathcal{B}$ items of $\mathcal{B}$. In this section, we ask whether these model-specific neighborhoods are idiosyncratic. To quantify cross-model consistency, we correlate the upper-triangular entries of the corresponding similarity matrices using both rank-based (Spearman) and linear (Pearson) correlations:
\[
r_{\mathrm{S}}^{\mathcal{B}}(f_1,f_2)
=
\rho_{\mathrm{S}}\!\big(\mathrm{vec}(S^{(f_1,\mathcal{B})}),\ \mathrm{vec}(S^{(f_2,\mathcal{B})})\,\big),
\qquad
r_{\mathrm{P}}^{\mathcal{B}}(f_1,f_2)
=
\rho_{\mathrm{P}}\!\big(\mathrm{vec}(S^{(f_1,\mathcal{B})}),\ \mathrm{vec}(S^{(f_2,\mathcal{B})})\,\big),
\]
where $\mathrm{vec}(\cdot)$ stacks the upper triangle of a matrix into a vector, $\rho_{\mathrm{S}}$ is Spearman’s rank correlation, and $\rho_{\mathrm{P}}$ is Pearson’s correlation. We report both, as Spearman is invariant to monotone re-scalings and thus robust to calibration differences across models, while Pearson captures linear alignment in similarity magnitudes.

\begin{table*}[t]
\centering
\scriptsize
\setlength{\tabcolsep}{4pt}
\resizebox{\textwidth}{!}{
\begin{tabular}{l*{6}{cc}}
\toprule
& \multicolumn{2}{c}{\textbf{Gemma}} & \multicolumn{2}{c}{\textbf{Llama}} & \multicolumn{2}{c}{\textbf{OLMo}} & \multicolumn{2}{c}{\textbf{Phi}} & \multicolumn{2}{c}{\textbf{Qwen}} & \multicolumn{2}{c}{\textbf{Average}} \\
\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}\cmidrule(lr){12-13}
\textbf{Benchmark} & \textbf{Spearman} & \textbf{Pearson} & \textbf{Spearman} & \textbf{Pearson} & \textbf{Spearman} & \textbf{Pearson} & \textbf{Spearman} & \textbf{Pearson} & \textbf{Spearman} & \textbf{Pearson} & \textbf{Spearman} & \textbf{Pearson} \\
\midrule
AQUA-RAT      & 61.8 & 62.7 & 63.3 & 65.9 & 47.0 & 49.3 & 49.8 & 50.9 & 76.1 & 76.5 & 59.6 & 61.1 \\
ARC-Challenge & 41.9 & 42.7 & 76.3 & 76.9 & 72.9 & 73.9 & 71.9 & 73.9 & 74.5 & 76.1 & 67.5 & 68.7 \\
ARC-Easy      & 41.6 & 42.0 & 66.6 & 67.1 & 67.9 & 68.7 & 75.3 & 76.3 & 62.7 & 64.0 & 62.8 & 63.6 \\
ART           & 25.5 & 26.4 & 84.1 & 84.6 & 76.6 & 77.9 & 78.6 & 79.8 & 75.7 & 76.8 & 68.1 & 69.1 \\
BoolQ         & 25.5 & 25.9 & 58.3 & 59.8 & 35.3 & 36.3 & 18.2 & 18.9 & 27.5 & 28.9 & 33.0 & 34.0 \\
CommonsenseQA & 33.8 & 35.6 & 33.6 & 34.4 & 62.8 & 64.5 & 42.4 & 43.8 & 39.1 & 39.8 & 42.3 & 43.6 \\
COPA          & 27.9 & 29.5 & 76.7 & 78.4 & 68.7 & 70.2 & 64.8 & 67.2 & 51.4 & 52.8 & 57.9 & 59.6 \\
GPQA          & 77.8 & 78.0 & 53.4 & 51.9 & 64.1 & 65.2 & 82.1 & 83.0 & 85.5 & 86.1 & 72.6 & 72.8 \\
LogiQA        & 55.4 & 57.3 & 77.7 & 79.8 & 67.8 & 67.2 & 70.3 & 72.1 & 80.1 & 80.3 & 70.3 & 71.3 \\
MathQA        & 33.5 & 34.2 & 61.0 & 62.0 & 55.2 & 56.0 & 54.9 & 54.0 & 57.1 & 56.4 & 52.3 & 52.5 \\
OpenBookQA    & 33.2 & 34.0 & 73.9 & 75.0 & 70.1 & 72.3 & 74.0 & 75.5 & 71.0 & 72.5 & 64.4 & 65.9 \\
PIQA          & 38.6 & 39.6 & 78.6 & 80.2 & 77.3 & 79.4 & 67.0 & 68.2 & 74.6 & 76.4 & 67.2 & 68.8 \\
PubMedQA      & 50.5 & 50.0 & 60.8 & 60.8 & 37.8 & 36.8 & 50.6 & 50.6 & 63.0 & 61.1 & 52.5 & 51.9 \\
QUARTZ        & 37.9 & 39.0 & 81.4 & 80.3 & 65.9 & 63.9 & 76.0 & 74.9 & 70.0 & 68.1 & 66.2 & 65.2 \\
SciQ          & 22.0 & 22.5 & 55.7 & 58.3 & 60.3 & 61.4 & 56.0 & 56.0 & 58.0 & 60.3 & 50.4 & 51.7 \\
SocialIQA     & 20.7 & 21.3 & 72.9 & 73.5 & 64.6 & 65.6 & 63.5 & 64.9 & 61.2 & 62.0 & 56.6 & 57.5 \\
StrategyQA    &  3.6 &  3.7 & 48.2 & 49.6 & 20.5 & 20.9 & 21.6 & 21.4 & 27.9 & 28.9 & 24.4 & 24.9 \\
TruthfulQA    & 50.4 & 52.7 & 78.4 & 80.7 & 71.4 & 73.6 & 85.3 & 86.4 & 77.1 & 79.3 & 72.5 & 74.5 \\
\midrule
\textbf{Average} & 37.9 & 38.7 & 66.7 & 67.7 & 60.3 & 61.3 & 61.2 & 62.1 & 62.9 & 63.7 & 57.8 & 58.7 \\
\bottomrule
\end{tabular}}
\caption{\textbf{Cross-model consistency of predictive similarity}: Spearman and Pearson correlations (values $\times 100$) between the upper-triangle entries of affinity matrices, by benchmark (rows) and model family (columns). The rightmost block reports per-benchmark averages across families; the bottom row reports per-family averages across benchmarks; the bottom-right cell shows overall means.}
\label{tab:predsim_consistency_table}
\end{table*}

As shown in Table~\ref{tab:predsim_consistency_table}, predictive similarity neighborhoods exhibit substantial within-family consistency across many benchmarks. Averaging across families yields high per-benchmark means clustered in the mid-$60$s, with notable peaks for \emph{LogiQA} (70.3 Spearman / 71.3 Pearson), \emph{TruthfulQA} (72.5 / 74.5), \emph{PIQA} (67.2 / 68.8), and \emph{ART} (68.1 / 69.1). Family-wise averages further show broad stability for Llama (66.7 / 67.7), Qwen (62.9 / 63.7), and Phi (61.2 / 62.1), with OLMo close behind (60.3 / 61.3) and Gemma lower (37.9 / 38.7). In particular, \emph{ART}, \emph{COPA}, \emph{LogiQA}, \emph{PIQA}, and \emph{TruthfulQA} attain high agreement for Llama, Phi, and Qwen (typically $70$–$85$), indicating that the induced item-item structure is largely task-driven rather than model idiosyncratic. Knowledge-centric \emph{GPQA} is also strong for Phi and Qwen ($82$–$86$). By contrast, \emph{StrategyQA}, \emph{BoolQ}, and, to a lesser extent, \emph{SocialIQA} show weaker agreement particularly for Gemma and Phi, suggesting greater family-specific effects on these benchmarks.

\subsection{Probing the Determinants of Predictive Similarity}
\label{subsec:determinants_pred_sim}

\paragraph{Dependence on the Tail.}
We test sensitivity to the probability tail by constructing a truncated variant that re-normalizes mass over the union of the \texttt{top-50} tokens, yielding $S_{\text{KL-\texttt{top-50}}}^{(f,\mathcal{B})}$, and contrasting it with the full $S_{\text{KL}}^{(f,\mathcal{B})}$.

\paragraph{JS vs.\ KL-based Similarity.}
Equation~\ref{eq:predictive:similarity} defines \(S\) as an RBF of the Jeffreys divergence, producing \emph{sharp, tunable} neighborhoods that strongly penalize coverage errors (as near-zeros drive \(J\) higher and \(S\) lower). Jensen-Shannon (JS) instead compares to the mixture \(M=\tfrac12(\bar{p}_f(x_i)+\bar{p}_f(x_j))\), yielding a bounded, tail-robust divergence that is easier to compare across benchmarks. To place JS on the same similarity scale, we apply the same RBF transform from Equation~\ref{eq:predictive:similarity} entrywise, obtaining $S_{\text{JS}}^{(f,\mathcal{B})}$.

\begin{wraptable}{r}{0.5\linewidth}
  \vspace{-\baselineskip}
  \centering
  \scriptsize
  \setlength{\tabcolsep}{6pt}
  \renewcommand{\arraystretch}{1.0}
  \begin{tabular}{lcccc}
    \toprule
    & \multicolumn{2}{c}{\textbf{$S_\text{JS}$}} & \multicolumn{2}{c}{\textbf{$S_\text{KL\texttt{-top-50}}$}} \\
    \cmidrule(lr){2-3}\cmidrule(lr){4-5}
    \textbf{Benchmark}  & Pearson & Spearman & Pearson & Spearman \\
    \midrule
    AQUA-RAT & 96.1 & 95.5 & 97.4 & 97.4 \\
    ARC-Challenge & 95.7 & 95.2 & 98.5 & 98.5 \\
    ARC-Easy & 95.4 & 95.0 & 98.7 & 98.7 \\
    ART & 95.5 & 95.4 & 97.1 & 96.9 \\
    BoolQ & 93.5 & 92.5 & 98.4 & 98.4 \\
    CommonsenseQA & 97.0 & 96.7 & 99.5 & 99.5 \\
    COPA & 94.7 & 95.2 & 95.1 & 94.9 \\
    GPQA & 97.1 & 97.1 & 99.1 & 99.0 \\
    LogiQA & 98.7 & 98.5 & 98.5 & 98.5 \\
    MathQA & 99.1 & 99.4 & 96.4 & 96.3 \\
    OpenBookQA & 97.6 & 97.7 & 97.5 & 97.6 \\
    PIQA & 96.4 & 96.1 & 96.2 & 95.9 \\
    PubMedQA & 96.8 & 96.7 & 99.6 & 99.6 \\
    QUARTZ & 95.8 & 95.8 & 99.8 & 99.8 \\
    SciQ & 93.6 & 93.3 & 98.8 & 98.9 \\
    SocialIQA & 98.2 & 98.2 & 97.0 & 96.8 \\
    StrategyQA & 99.4 & 99.5 & 97.0 & 97.0 \\
    TruthfulQA & 94.6 & 94.2 & 98.1 & 97.7 \\
    \bottomrule
  \end{tabular}
  \caption{\textbf{Agreement between predictive similarity variants.} Per-benchmark Pearson/Spearman correlations between \(S_{\text{KL}}^{(f,\mathcal{B})}\) and (i) \(S_{\text{JS}}^{(f,\mathcal{B})}\) and (ii) \(S_{\text{KL-}\texttt{top-50}}^{(f,\mathcal{B})}\) shows that neighborhoods are driven by head probability mass and remain stable under truncation or mixture smoothing.}
  \label{tab:sim_corrs}
  \vspace{-1em}
\end{wraptable}

Following Appendix \ref{subsec:consistency_pred_sim}, we compute Spearman and Pearson correlations between the upper-triangular entries of $S_{\text{KL}}^{(f,\mathcal{B})}$ and, respectively, $S_{\text{KL-\texttt{top-50}}}^{(f,\mathcal{B})}$ and $S_{\text{JS}}^{(f,\mathcal{B})}$. We report per-benchmark scores where $f$ is $\text{OLMo 2 7B}$. \emph{High agreement} indicates that neighborhoods are driven by high probability mass and remain stable under truncation or mixture smoothing, while \emph{low agreement} indicates sensitivity to tail mismatches or calibration asymmetries that Jeffreys magnifies but JS attenuates.

Across benchmarks, correlations between \(S_{\text{KL}}^{(f,\mathcal{B})}\) and (i) $S_\text{KL\texttt{-top-50}}^{(f,\mathcal{B})}$ and (ii) $S_\text{JS}^{(f,\mathcal{B})}$ variants are uniformly high (typically \(95\text{-}99\%\)) (Table~\ref{tab:sim_corrs}). This indicates that the divergence of probability distributions generated by OLMo~2~7B are governed by head probability mass rather than the tail. Truncation preserves structure nearly perfectly as \(S_{\text{KL\texttt{-top-50}}}\) matches or exceeds \(S_{\text{JS}}\) on most tasks, while \(S_{\text{JS}}\) remains strongly aligned, reflecting robustness to calibration and coverage noise. Modest dips (e.g., COPA, PIQA) suggest settings where tail mismatches or asymmetries matter more, but overall the stability under truncation and mixture smoothing supports that \(S\) captures meaningful, head-driven divergence.

\subsection{Theoretical and Computational Discussions}
\paragraph{A Theoretical Perspective on Predictive Similarity.} We measure similarity via the (symmetrized) $D_{\text{KL}}$ between model predictive distributions because it aligns with how models differ operationally and geometrically. First, $D_{\text{KL}}$ has a clear testing meaning, as it governs optimal error exponents in distinguishing two distributions. Hence, larger $D_{\text{KL}}$ divergence implies that the model would more reliably tell the two inputs apart (by Stein/Chernoff asymptotics) \citep{elements_of_it}. Second, small $D_{\text{KL}}$ guarantees closeness in total variation by Pinsker’s inequality, implying high indistinguishability and hence high similarity for our purposes \citep{elements_of_it}. Third, $D_{\text{KL}}$ is information monotone under coarse-graining, making the measure stable to relabeling or merging answer tokens/options that preserve semantics \citep{it_and_stats}. Finally, locally $D_{\text{KL}}$ induces the Fisher-Rao geometry on the probability simplex, so $\exp(-\tau\,\mathrm{D_\text{KL}})$ behaves like a Gaussian kernel in the natural metric of the model’s predictive space, yielding compact clusters of similar predictive behavior \citep{info_geo}. We use the Jeffreys (symmetrized) form to remove directionality while retaining these properties.


\paragraph{Computational Overhead of Predictive Similarity.} Predictive similarity is a \emph{post hoc} computation, since we operate on the logits already cached from the benchmark evaluation. Hence, no additional model forward passes are required. Given these logits, we convert them to predictive distributions and evaluate the pairwise KL terms that define the similarity in Eq.~\ref{eq:predictive:similarity}. 

The principal cost arises from forming pairwise interactions across $N$ items, which is quadratic in $N$ and linear in the label-space size $D$ (i.e., $O(N^2 \cdot D)$ time). Memory is dominated by storing the evaluation logits ($O(N \cdot D)$) and the similarity matrix ($O(N^2)$). In practice, $D$ corresponds to the size of the vocabulary of a given model and can be large. We therefore view the cost as $O(N^2 D)$ and the memory requirement as $O(N \cdot D)$. When $N$ is large, standard remedies (e.g. blockwise evaluation) reduce peak memory without changing the definition of the metric. Overall, computing predictive simlarity adds negligible \emph{inference} overhead and modest \emph{analysis} overhead relative to running the benchmarks themselves.

Compared to alternative baselines discussed in Appendix \ref{subsec:alt_baselines}, predictive similarity is computationally frugal: it reuses cached logits and requires neither additional inference nor any backward passes. By contrast, embedding baselines, as well as BERTScore, invoke separate encoders (extra forward passes), G-Vendi relies on gradients (backward passes), and CORRS/IRT aggregate signals from a bank of models (multiple evaluations per item). While string-based methods such as Bigram are lightweight, they do not leverage model behavior. Thus, predictive similarity offers a favorable trade-off between compute and quality when benchmarks are already being run.

\subsection{Discussion on Model-Specific Similarity}

Our goal is to \emph{evaluate benchmarks}, not to define a single, task-agnostic neighborhood data points. A benchmark can be \emph{reliable for one model but unreliable for another}. Accordingly, the similarity function used to induce the partition $\mathcal{G}_f$ should be \emph{conditional on the model $f$}. We provide our rationale below.

\paragraph{Evaluation target is $H_{\mathcal{B}}(f)$.}
Section~\ref{sec:benchmark_harmony} defines harmony \emph{per model}, $H_{\mathcal{B}}(f)$, and then aggregates across $f$ via $(\mu_H,\sigma_H^2)$ in Eq.~\ref{eq:mu_and_sigma}. Using a \emph{global, model-agnostic} similarity collapses distinct predictive neighborhoods into a single partition, implicitly assuming that $\mathcal{G}_f$ is invariant across $f$. This undermines the very statistic we report: two models with the same accuracy profile but different predictive structure could receive the same $H$ under a fixed partition, obscuring model specialities.

\paragraph{Benchmarks are instruments relative to a model.}
A benchmark is a diagnostic instrument for a \emph{given} model: priors, tokenization, calibration, and pre-training exposure all change which items are \emph{similar} from the model’s perspective. Hence, a model can perform uniformly on a benchmark while another one overfits to certain subdomains. Model specific similarity preserves this relativity, letting reliability vary meaningfully across families.

%\section{Mapping Benchmark \method{} for MMLU}
%\label{sec:mapping_mmlu}
%Unlike single-purpose benchmarks, MMLU composes \emph{dozens} of subject-specific subtasks spanning STEM, social sciences, humanities, and professional domains. Yet this breadth does not guarantee that each subtask faithfully represents its stated domain. We therefore analyze each MMLU subtask in the \method{} mean-variance plane across models, asking \emph{which} subtasks provide stable measurement and \emph{where} apparent gains may be driven by skewed subsets.


%Fig.~\ref{fig:mmlu_tasks} plots each subtask in the \method{} mean-variance plane across models (with vertical axis being $\mathbb{E}_{f}[H(f;\mathcal{G}_f)]$ and horizontal axis being $\operatorname{Var}_{f}[H(f;\mathcal{G}_f)]$). As discussed in §\ref{sec:mapping}, upper-left region (high mean, low variance) indicate higher subtask reliability, while rightward or downward shifts signal model-sensitive or distributionally skewed subtasks.

\section{Model List}
\label{sec:model_list}
We list all evaluated models and provide links to their open-source weights.

\vspace{-0.5em}
\begin{itemize}[left=0pt, itemsep=1pt]
  \item Qwen3: \hyperlink{https://huggingface.co/Qwen/Qwen3-0.6B-Base}{Qwen3-0.6B-Base}, \hyperlink{https://huggingface.co/Qwen/Qwen3-1.7B-Base}{Qwen3-1.7B-Base}, \hyperlink{https://huggingface.co/Qwen/Qwen3-4B-Base}{Qwen3-4B-Base}, \hyperlink{https://huggingface.co/Qwen/Qwen3-8B-Base}{Qwen3-8B-Base}, \hyperlink{https://huggingface.co/Qwen/Qwen3-14B-Base}{Qwen3-14B-Base}, \hyperlink{https://huggingface.co/Qwen/Qwen3-0.6B}{Qwen3-0.6B}, \hyperlink{https://huggingface.co/Qwen/Qwen3-1.7B}{Qwen3-1.7B}, \hyperlink{https://huggingface.co/Qwen/Qwen3-4B}{Qwen3-4B}, \hyperlink{https://huggingface.co/Qwen/Qwen3-8B}{Qwen3-8B}, \hyperlink{https://huggingface.co/Qwen/Qwen3-14B}{Qwen3-14B}.
  \item Llama 3: \hyperlink{https://huggingface.co/meta-llama/Llama-3.2-1B}{Llama-3.2-1B}, \hyperlink{https://huggingface.co/meta-llama/Llama-3.2-3B}{Llama-3.2-3B}, \hyperlink{https://huggingface.co/meta-llama/Llama-3.1-8B}{Llama-3.1-8B}, \hyperlink{https://huggingface.co/meta-llama/Llama-3.1-70B}{Llama-3.1-70B}, \hyperlink{https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct}{Llama-3.2-1B-Instruct}, \hyperlink{https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct}{Llama-3.2-3B-Instruct}, \hyperlink{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}{Llama-3.1-8B-Instruct}, \hyperlink{https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct}{Llama-3.1-70B-Instruct}.
  \item Olmo 2: \hyperlink{https://huggingface.co/allenai/OLMo-2-0425-1B}{OLMo-2-0425-1B}, \hyperlink{https://huggingface.co/allenai/OLMo-2-1124-7B}{OLMo-2-1124-7B}, \hyperlink{https://huggingface.co/allenai/OLMo-2-1124-13B}{OLMo-2-1124-13B}, \hyperlink{https://huggingface.co/allenai/OLMo-2-0325-32B}{OLMo-2-0325-32B}, \hyperlink{https://huggingface.co/allenai/OLMo-2-0425-1B-Instruct}{OLMo-2-0425-1B-Instruct}, \hyperlink{https://huggingface.co/allenai/OLMo-2-1124-7B-Instruct}{OLMo-2-1124-7B-Instruct}, \hyperlink{https://huggingface.co/allenai/OLMo-2-1124-13B-Instruct}{OLMo-2-1124-13B-Instruct}, \hyperlink{https://huggingface.co/allenai/OLMo-2-0325-32B-Instruct}{OLMo-2-0325-32B-Instruct}.
  \item Gemma 3: \hyperlink{https://huggingface.co/google/gemma-3-1b-pt}{gemma-3-1b-pt}, \hyperlink{https://huggingface.co/google/gemma-3-4b-pt}{gemma-3-4b-pt}, \hyperlink{https://huggingface.co/google/gemma-3-12b-pt}{gemma-3-12b-pt}, \hyperlink{https://huggingface.co/google/gemma-3-27b-pt}{gemma-3-27b-pt}, \hyperlink{https://huggingface.co/google/gemma-3-1b-it}{gemma-3-1b-it}, \hyperlink{https://huggingface.co/google/gemma-3-4b-it}{gemma-3-4b-it}, \hyperlink{https://huggingface.co/google/gemma-3-12b-it}{gemma-3-12b-it}, \hyperlink{https://huggingface.co/google/gemma-3-27b-it}{gemma-3-27b-it}.
  \item Phi-3: \hyperlink{https://huggingface.co/microsoft/Phi-3-mini-4k-instruct}{Phi-3-mini-4k-instruct}, \hyperlink{https://huggingface.co/microsoft/Phi-3-medium-4k-instruct}{Phi-3-medium-4k-instruct}.
\end{itemize}
\vspace{-0.5em}

\section{Model-wise Decomposition of Benchmark \method{}}
\label{sec:dissected}
\begin{wrapfigure}{r}{0.45\linewidth} % 
  \vspace{-\intextsep}
  \centering
  \includegraphics[width=\linewidth]{figs/dissected_non_mmlu.png}
  \caption{Model-wise decomposition of \method{} for MCQA benchmarks.}
  \label{fig:dissected_non_mmlu}
  \vspace{-\intextsep} % tighten bottom gap (optional)
\end{wrapfigure}

\begin{figure}[t]
  \centering
  % Subplot A
  \begin{subfigure}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs/dissected_mmlu_1.png} % or .png/.jpg
    \caption{MMLU subtask group 1.}
    \label{fig:three-a}
  \end{subfigure}
  \hfill
  % Subplot B
  \begin{subfigure}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs/dissected_mmlu_2.png}
    \caption{MMLU subtask group 2.}
    \label{fig:three-b}
  \end{subfigure}
  \hfill
  % Subplot C
  \begin{subfigure}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs/dissected_mmlu_3.png}
    \caption{MMLU subtask group 3.}
    \label{fig:three-c}
  \end{subfigure}

  \caption{Model-wise decomposition of \method{} for MMLU subtasks.}
  \label{fig:dissected_mmlu}
\end{figure}

In \S\ref{subsec:mapping}, we position each benchmark \(\mathcal{B}\) using the cross-model mean \(\mu_H(\mathcal{B})\) and variance \(\sigma_H^2(\mathcal{B})\). We now resolve this view at the model level. For each benchmark, Fig.~\ref{fig:dissected_non_mmlu} plots the per-model vector \(\{H_{\mathcal{B}}(f)\}_{f\in\mathcal{F}}\), revealing structure that is obscured by aggregation. Similarly, Fig.~\ref{fig:dissected_mmlu} provides the analogous decomposition for MMLU subtasks, treating each subtask as a benchmark on its own.

Tight \emph{horizontal} groupings (small spread across models) indicate \emph{model-invariant} distributional balance, where different families assign similar \method{} to the same benchmark, suggesting that aggregate accuracy reflects uniform competence irrespective of architectural or training choices. Conversely, wide horizontal scatter exposes \emph{model-dependent reliability}, as some families concentrate performance on a few subsets (low \method{}), while others distribute performance more evenly (high \method{}).

We note that benchmarks with tight clusters are favorable for cross-family comparison, as accuracy rankings are less likely to be artifacts of benchmark composition. In contrast, wide scatter warns that leaderboard deltas may be driven by subsets that particular families exploit. In such cases, we suggest reporting accuracy alongside the \method{} profiles of the models under evaluation, \(\{H(\mathcal{G}_f)\}_{f}\).

\section{Improvement \method{}}
\label{sec:imp_harmony}
In \S\ref{subsec:subsec6.2}, we show that scaling behavior varies across model families as parameter count increases: some families (e.g., Qwen3) exhibit increasing \method{}, while others (e.g., Gemma 3) show the opposite. We now ask whether \emph{performance improvements} from scaling are distributed evenly across subsets. For two adjacent model sizes within a family, let the per-subset change be
\[
d_i \;=\; \Psi(f_{\text{large}}; A_i)\;-\;\Psi(f_{\text{small}}; A_i),
\]
with subset weights $w_i$ and partition $\mathcal{G}=\{A_i\}_{i=1}^k$ defined as in \S\ref{subsec:preliminaries}. Let $\bar d=\sum_i w_i d_i$ be the weighted mean and reuse the \method{} computation by replacing accuracies $\Psi(f;A_i)$ with changes $d_i$:
\[
K_i \;=\; \exp\!\Big(-\big(\tfrac{d_i-\bar d}{b}\big)^2\Big), 
\qquad
p_i \;=\; \frac{w_i K_i}{\sum_j w_j K_j},
\qquad
H_\Delta(\mathcal{G}) \;=\; -\frac{1}{\log k}\sum_{i=1}^k p_i \log\!\big(p_i+\varepsilon\big).
\]
High $H_\Delta$ indicates that scaling yields \emph{uniform} changes across subsets, while low $H_\Delta$ indicates \emph{spiky} changes concentrated in a few clusters. Similar to \S~\ref{sec:benchmark_harmony}, we adopt a comparative perspective, asking which models improve more uniformly and which benchmarks most facilitate uniform gains. To ensure within-family comparability, we fix the partition to that induced by the smallest model in each family and evaluate all larger models on these partitions.

\begin{figure}
  \centering
  \vspace{-\intextsep} % tighten top space if needed
  \begin{tabular}{@{}c@{}}
    \includegraphics[width=\linewidth]{figs/imp_harmony_fig.png}
  \end{tabular}
  \caption{\textbf{\boldmath$H_\Delta$ across benchmarks.} Higher performance \method{} modestly correlates with improvement \method{} ($r=0.226$; $r=0.387$ excluding the three lowest \method{}) benchmarks, indicating an outlier-sensitive correlation.}
  \label{fig:imp_harm_bench_fig}
\end{figure}

\paragraph{Improvement \method{} of Benchmarks (Fig.~\ref{fig:imp_harm_bench_fig}).} Due to the lack of a principled baseline for $H_\Delta$, we interpret results comparatively rather than absolutely. Benchmarks vary in improvement \method{}, and those with higher performance \method{} tend to exhibit higher $H_\Delta$. Using all benchmarks in our setup, the fitted-line correlation is $r=0.226$, which increases to $r=0.387$ after excluding the three lowest-\method{} benchmarks. Thus, higher \method{} benchmarks are associated with more uniform improvements in a comparative sense, though the effect size is modest and sensitive to less harmonious outliers.

\paragraph{Improvement \method{} of Models (Fig.~\ref{fig:imp_harm_model_fig}).}  We measure improvement \method{} $H_\Delta$ for adjacent sizes within each family. For \texttt{Qwen} and \texttt{Llama}, despite a \emph{decline} in performance \method{} with scale (\S\ref{subsec:subsec6.2}), $H_\Delta$ \emph{increases} with the model scale, as larger models distribute their gains more evenly across subsets, whereas smaller variants exhibit spikier changes. \texttt{Gemma} shows the complementary pattern, where its larger models, which had higher performance \method{}, display \emph{lower} $H_\Delta$, indicating that improvements concentrate on fewer subsets as scale grows. By contrast, in \texttt{OLMo} model family, both performance \method{} and improvement \method{} \emph{rise} with model size. Taken together, these results underscore that aggregate \method{} and improvement \method{} can decouple, since models may become less harmonious overall yet still scale their improvements uniformly, or vice versa.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/imp_harmony_model_fig.png}
    \caption{\textbf{Model size vs.\ \boldmath$H_\Delta$}. Improvement harmony scales differently by family: it increases with size for \texttt{Qwen}, \texttt{Llama}, and \texttt{OLMo}, while decreasing for \texttt{Gemma}.}
    \label{fig:imp_harm_model_fig}
\end{figure}

\section{Details of Statistical Significance Test}
\label{sec:details_of_stat_sig}

We assess whether the subset we keep after pruning has a higher mean than the full set using a nonparametric, \emph{coupled} bootstrap sign test. Let $a_1,\dots,a_N$ be per-example accuracy and $k_i\in\{0,1\}$ indicate membership in the keep subset $K=\{i:\,k_i=1\}$. For $b=1,\dots,B$, we draw a bootstrap sample of indices $S^{(b)}$ of size $N$ (with replacement), compute both means on the \emph{same} resample, and then take their difference:
\begin{align}
\bar a_{\mathrm{all}}^{(b)} &= \frac{1}{N}\sum_{i\in S^{(b)}} a_i, \\
n_{\mathrm{keep}}^{(b)} &= \sum_{i\in S^{(b)}} k_i, \\
\bar a_{\mathrm{keep}}^{(b)} &= \frac{1}{n_{\mathrm{keep}}^{(b)}} \sum_{i\in S^{(b)}} k_i\, a_i, \\
\Delta^{(b)} &= \bar a_{\mathrm{keep}}^{(b)} - \bar a_{\mathrm{all}}^{(b)}.
\end{align}
Resamples with $n_{\mathrm{keep}}^{(b)}=0$ are discarded (to avoid degenerate runs we cap total draws at $3B$); let $m\le B$ be the number of valid differences retained. We then form a two-sided $p$-value from the sign statistic with a plus-one small-sample correction:
\begin{align}
r &= \sum_{b=1}^{m} \mathbf{1}\!\left\{\Delta^{(b)} \ge 0\right\}, \\
p &= \min\!\left\{\,1,\, 2\min\!\left(\frac{r+1}{m+1},\, \frac{m-r+1}{m+1}\right)\right\}.
\end{align}
We fix the random seed for reproducibility and declare significance at level $\alpha$ when $p<\alpha$ (testing $H_0:\mathbb{E}[\Delta]=0$ vs.\ $H_1:\mathbb{E}[\Delta]\neq 0$). We use $B=10000$ and set $\alpha(N)$ as follows:

\[
\alpha(N)=
\begin{cases}
0.1, & N<500,\\
0.05, & 500 \leq N<1500,\\
0.01, & 1500 \leq N<3000
\end{cases}
\]


\section{Extended Results: How does model performance change with increased \method{}?}
\label{sec:extended_balancing}

In this section, we generalize the pruning experiments from \S\ref{subsec:balancing_subsets} beyond the illustrative cases to \emph{all} model families and benchmarks in our setup. Our aim is methodological: we examine how aggregate accuracy and per-subset dispersion evolve as we progressively rebalance a benchmark. Concretely, for each (model, benchmark) pair we sweep a pruning budget (scheduled inversely to baseline \method{}), recompute \method{} and accuracy at each budget, and compare the pruned-set accuracy to the full-set accuracy using the coupled bootstrap significance test detailed in App.~\ref{sec:details_of_stat_sig}. Family-wise plots in this section visualize these trajectories, allowing us to observe whether increased \method{} coincides with stable (or shifting) aggregate scores and tighter per-subset distribution of performance.

Across all model families (Fig.~\ref{fig:qwen_pruning_base_full},~\ref{fig:llama_pruning_base_full},~\ref{fig:olmo_pruning_base_full},~\ref{fig:gemma_pruing_base_full}), two patterns consistently hold.
\textbf{(i) Accuracy shifts with increased harmony.} As pruning raises \method{}, aggregate accuracy frequently changes in a statistically significant manner (App.~\ref{sec:details_of_stat_sig}), indicating that low \method{} composition can result in a misleading aggregate score.
\textbf{(ii) Low \method{} benchmarks are fragile.} Benchmarks starting with lower \method{} exhibit more instances of significant accuracy change under the pruning procedure than high \method{} benchmarks, underscoring their susceptibility to presenting misleading aggregate scores.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/qwen_pruning_base_full.png}
    \caption{\textbf{Full results of balancing benchmarks via pruning in Qwen3 model family.} We remove overly similar items with a pruning rate inversely proportional to \method{}, which consistently improves \method{}. We find that aggregate scores often change statistically significantly on less harmonious benchmarks, whereas they remain more stable on more harmonious benchmarks.}
    \label{fig:qwen_pruning_base_full}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/llama_pruning_base_full.png}
    \caption{\textbf{Full results of balancing benchmarks via pruning in Llama 3 model family.} We remove overly similar items with a pruning rate inversely proportional to \method{}, which consistently improves \method{}. We find that aggregate scores often change statistically significantly on less harmonious benchmarks, whereas they remain more stable on more harmonious benchmarks.}
    \label{fig:llama_pruning_base_full}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/olmo_pruning_base_full.png}
    \caption{\textbf{Full results of balancing benchmarks via pruning in Olmo 2 model family.} We remove overly similar items with a pruning rate inversely proportional to \method{}, which consistently improves \method{}. We find that aggregate scores often change statistically significantly on less harmonious benchmarks, whereas they remain more stable on more harmonious benchmarks.}
    \label{fig:olmo_pruning_base_full}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/gemma_pruing_base_full.png}
    \caption{\textbf{Full results of balancing benchmarks via pruning in Gemma 3 model family.} We remove overly similar items with a pruning rate inversely proportional to \method{}, which consistently improves \method{}. We find that aggregate scores often change statistically significantly on less harmonious benchmarks, whereas they remain more stable on more harmonious benchmarks.}
    \label{fig:gemma_pruing_base_full}
\end{figure}

\section{Multi-Dimensional Evaluation}
\label{sec:multi_dim_eval}
Motivated by the skewed aggregate scores in low \method{} benchmarks, we conduct model evaluation at finer granularity. Following recent work on fine-grained evaluation \citep{zeng2025evaltree}, we recursively induce partitions as described in \S\ref{subsec:methodology}. This procedure yields a \emph{labeled tree}, where the root is the full benchmark; each internal node is a subset from the partitioning of its parent node; and leaves are atomic subdomains that admit no further valid split (see App.~\ref{sec:hier_labeling} for details). The resulting hierarchy enables interpretable, multi-dimensional evaluation, where each dimension corresponds to a subdomain of the benchmark.

We illustrate this approach on two examples: \emph{MMLU College Biology} and \emph{MMLU College Physics}. The average \method{} across Qwen3 models is markedly higher for biology (0.8538) than for physics (0.7534). This suggests that the aggregate accuracy for biology is a more representative reflection of the performance across subdomains. Indeed, Table~\ref{tab:mmlu_bio_clusters} shows that rankings in biology subdomains mirror the overall ordering: models that achieve higher overall accuracy also achieve higher accuracy in every subdomain. In other words, no model with superior overall performance is ever surpassed by a model with lower overall performance in any biology subdomain.\footnote{Comparison of 1.7B and 4B in \emph{Molecular \& Cellular Biology} is the only exception.} This alignment underscores that the aggregate score is a consistent and reliable summary of subdomain performance in biology.

In contrast, physics exhibits lower \method{} and more notable divergences (Table~\ref{tab:mmlu_physics_clusters}). For example, Qwen3-4B lags behind Qwen3-14B in overall accuracy (58.8\% vs.\ 69.6\%), yet it surpasses it in \emph{Special Relativity} (71.4\% vs.\ 57.1\%) and \emph{Electromagnetism} (83.3\% vs.\ 66.7\%). Similarly, Qwen3-0.6B, despite its weak overall score (24.5\%), achieves competitive performance in \emph{Quantum Mechanics} (56.5\%), outperforming Qwen3-1.7B (34.8\%). These cases highlight how aggregate scores can obscure areas of relative strength, and how fine-grained, multi-dimensional evaluation reveals nuanced interpretation of model competence across subdomains.

We provide extended results for Qwen3 and Llama 3 model families across 2 MCQA benchmarks and 6 MMLU subtasks in Appendix~\ref{sec:extended_multi_dim}.

\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrrrr}
\toprule
& Qwen3-0.6B & Qwen3-1.7B & Qwen3-4B & Qwen3-8B & Qwen3-14B \\
\midrule
Overall & 47.2 & 68.1 & 82.6 & 86.8 & \textbf{91.0} \\
\midrule
Multicellular Biology (42.4\%) & 45.9 & 68.9 & 88.5 & 88.5 & \textbf{93.4} \\
Evolutionary \& Ecological Processes (25.7\%) & 51.4 & 59.5 & 86.5 & \textbf{91.9} & \textbf{91.9} \\
Molecular \& Cellular Biology (31.9\%) & 45.7 & 73.9 & 71.7 & 80.4 & \textbf{87.0} \\
\midrule
\method{} & 0.951 & 0.918 & 0.757 & 0.784 & 0.859 \\
\bottomrule
\end{tabular}%
}
\caption{Multi-dimensional evaluation results of Qwen3 model family in MMLU College Biology. \textbf{Bold} implies the best performance.}
\label{tab:mmlu_bio_clusters}
\end{table*}


\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrrrr}
\toprule
& Qwen3-0.6B & Qwen3-1.7B & Qwen3-4B & Qwen3-8B & Qwen3-14B \\
\midrule
Overall & 24.5 & 34.3 & 58.8 & 57.8 & \textbf{69.6} \\
\midrule
Quantum Mechanics Principles and Applications (22.5\%) & 56.5 & 34.8 & 65.2 & 69.6 & \textbf{73.9} \\
Thermodynamics (8.8\%) & 0.0 & 55.6 & 66.7 & 55.6 & \textbf{77.8} \\
Special Relativity Concepts (13.7\%) & 21.4 & 14.3 & \textbf{71.4} & 50.0 & 57.1 \\
Classical Physics Principles and Relationships (20.6\%) & 9.5 & 38.1 & 47.6 & 52.4 & \textbf{61.9} \\
Physics Phenomena and Application (16.7\%) & 0.0 & 11.8 & 11.8 & 17.6 & \textbf{58.8} \\
Electromagnetism (5.9\%) & 16.7 & 16.7 & \textbf{83.3} & \textbf{83.3} & 66.7 \\
Solid State Physics (11.8\%) & 50.0 & 75.0 & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} \\
\midrule
\method{} & 0.686 & 0.712 & 0.765 & 0.815 & 0.789  \\
\bottomrule
\end{tabular}%
}
\caption{Multi-dimensional evaluation results of Qwen3 model family in MMLU College Physics. \textbf{Bold} implies the best performance.}
\label{tab:mmlu_physics_clusters}
\end{table*}


\section{Extended Results: Multi-Dimensional Evaluation}
\label{sec:extended_multi_dim}

We provide the extended results of multi-dimensional evaluation conducted as described in Appendix~\ref{sec:multi_dim_eval}. Our setup consists of Qwen3 and Gemma 3 model families and ARC-Easy, BoolQ, MMLU Anatomy, MMLU College Biology, MMLU College Computer Science, MMLU College Mathematics, MMLU College Physics, MMLU High School US History.

\subsection{Qwen3 Family}
See Tables~\ref{tab:qwen3_arce}, \ref{tab:qwen3_boolq}, \ref{tab:qwen3_anatomy}, \ref{tab:qwen3_col_bio}, \ref{tab:qwen3_col_cs}, \ref{tab:qwen3_col_math}, \ref{tab:qwen3_col_phy}, and \ref{tab:qwen3_hs_us_hist} for the extended results of Qwen3 model family.

\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrrrr}
\toprule
 & \textbf{Qwen3-0.6B} & \textbf{Qwen3-1.7B} & \textbf{Qwen3-4B} & \textbf{Qwen3-8B} & \textbf{Qwen3-14B} \\
\midrule
Overall & 60.7 & 72.2 & 80.5 & 83.5 & 84.2 \\
\midrule
Geology and Earth Sciences (15.0\%) & 64.4 & 73.4 & 83.8 & 83.8 & 83.8 \\
Scientific Principles and Processes (32.6\%) & 56.3 & 70.1 & 79.9 & 83.5 & 84.3 \\
Biological Processes and Concepts (24.1\%) & 65.4 & 74.0 & 78.8 & 82.9 & 83.6 \\
Physics Principles in Engineering and Science (9.4\%) & 64.3 & 75.9 & 82.1 & 86.6 & 86.6 \\
Environmental and Energy Assessment (6.5\%) & 60.4 & 63.6 & 78.6 & 81.2 & 79.9 \\
Fundamental Concepts in Astronomy (7.6\%) & 56.1 & 76.7 & 80.6 & 83.3 & 87.2 \\
Fundamentals of Chemical and Material Properties (4.8\%) & 56.1 & 71.9 & 81.6 & 84.2 & 84.2 \\
\bottomrule
\end{tabular}}
\caption{Multi-dimensional evaluation results for the Qwen3 model family on ARC-Easy.}
\label{tab:qwen3_arce}
\end{table}

% BoolQ (Qwen3)
\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrrrr}
\toprule
 & \textbf{Qwen3-0.6B} & \textbf{Qwen3-1.7B} & \textbf{Qwen3-4B} & \textbf{Qwen3-8B} & \textbf{Qwen3-14B} \\
\midrule
Overall & 64.1 & 77.5 & 85.0 & 86.6 & 89.3 \\
\midrule
Product Composition, Properties, and Standards (8.8\%) & 66.8 & 80.6 & 83.7 & 86.9 & 88.9 \\
Geographic, Operational, and Temporal Analysis (11.0\%) & 69.3 & 77.3 & 85.3 & 88.4 & 91.7 \\
Media Standards and Analysis (24.8\%) & 61.3 & 77.7 & 86.8 & 88.7 & 91.0 \\
Scientific and Analytical Principles (9.0\%) & 67.9 & 81.2 & 83.3 & 84.6 & 89.1 \\
Sports History and Regulations (10.1\%) & 63.5 & 75.1 & 82.1 & 84.5 & 87.2 \\
Governmental Laws and Regulations (10.6\%) & 64.1 & 72.8 & 80.3 & 84.6 & 86.1 \\
Human Biology and Medical Science (6.5\%) & 60.6 & 81.2 & 87.8 & 85.4 & 87.8 \\
Economic Systems (8.3\%) & 61.6 & 75.3 & 85.2 & 85.6 & 87.5 \\
Sociocultural, Geopolitical, and Linguistic Analysis (7.1\%) & 63.9 & 76.8 & 88.0 & 86.7 & 89.7 \\
Fictional Narrative Analysis and Elements (3.8\%) & 64.8 & 80.8 & 89.6 & 88.0 & 93.6 \\
\bottomrule
\end{tabular}}
\caption{Evaluation results for the Qwen3 model family on BoolQ.}
\label{tab:qwen3_boolq}
\end{table}

% MMLU Anatomy (Qwen3)
\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrrrr}
\toprule
 & \textbf{Qwen3-0.6B} & \textbf{Qwen3-1.7B} & \textbf{Qwen3-4B} & \textbf{Qwen3-8B} & \textbf{Qwen3-14B} \\
\midrule
Overall & 37.8 & 56.3 & 62.2 & 71.1 & 80.7 \\
\midrule
General Human Anatomy, Physiology, and Terminology (31.9\%) & 51.2 & 67.4 & 79.1 & 76.7 & 86.0 \\
Head and Neck Anatomy (21.5\%) & 20.7 & 34.5 & 34.5 & 62.1 & 82.8 \\
Skeletal Development and Anatomy (14.1\%) & 26.3 & 42.1 & 47.4 & 52.6 & 57.9 \\
Neurological Disorders (10.4\%) & 21.4 & 50.0 & 50.0 & 71.4 & 78.6 \\
Bone Anatomy and Terminology (3.0\%) & 0.0 & 50.0 & 50.0 & 50.0 & 50.0 \\
Anatomy of Circulatory System (2.2\%) & 66.7 & 100.0 & 100.0 & 100.0 & 100.0 \\
Developmental Structures (6.7\%) & 22.2 & 44.4 & 55.6 & 77.8 & 88.9 \\
Nephrology (10.4\%) & 78.6 & 92.9 & 100.0 & 92.9 & 92.9 \\
\bottomrule
\end{tabular}}
\caption{Evaluation results for the Qwen3 model family on MMLU Anatomy.}
\label{tab:qwen3_anatomy}
\end{table}

% MMLU College Biology (Qwen3) — with \method{}
\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrrrr}
\toprule
 & \textbf{Qwen3-0.6B} & \textbf{Qwen3-1.7B} & \textbf{Qwen3-4B} & \textbf{Qwen3-8B} & \textbf{Qwen3-14B} \\
\midrule
Overall & 47.2 & 68.1 & 82.6 & 86.8 & 91.0 \\
\midrule
Multicellular Biology (42.4\%) & 45.9 & 68.9 & 88.5 & 88.5 & 93.4 \\
Evolutionary and Ecological Processes (25.7\%) & 51.4 & 59.5 & 86.5 & 91.9 & 91.9 \\
Molecular and Cellular Biology (31.9\%) & 45.7 & 73.9 & 71.7 & 80.4 & 87.0 \\
\bottomrule
\end{tabular}}
\caption{Evaluation results for the Qwen3 model family on MMLU College Biology.}
\label{tab:qwen3_col_bio}
\end{table}

% MMLU College Computer Science (Qwen3)
\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrrrr}
\toprule
 & \textbf{Qwen3-0.6B} & \textbf{Qwen3-1.7B} & \textbf{Qwen3-4B} & \textbf{Qwen3-8B} & \textbf{Qwen3-14B} \\
\midrule
Overall & 28.0 & 41.0 & 66.0 & 72.0 & 68.0 \\
\midrule
Theoretical Foundations of Computation (52.0\%) & 26.9 & 40.4 & 59.6 & 69.2 & 65.4 \\
Computer Architecture and Optimization (7.0\%) & 28.6 & 42.9 & 71.4 & 57.1 & 57.1 \\
Operating Systems (10.0\%) & 50.0 & 80.0 & 90.0 & 90.0 & 80.0 \\
Network Layer Protocols and Technologies (5.0\%) & 60.0 & 60.0 & 80.0 & 100.0 & 100.0 \\
Data Processing (12.0\%) & 8.3 & 8.3 & 66.7 & 50.0 & 66.7 \\
Sorting Algorithms (4.0\%) & 25.0 & 25.0 & 75.0 & 100.0 & 100.0 \\
Graph Algorithms and Data Structures (10.0\%) & 20.0 & 40.0 & 60.0 & 80.0 & 50.0 \\
\bottomrule
\end{tabular}}
\caption{Evaluation results for the Qwen3 model family on MMLU College Computer Science.}
\label{tab:qwen3_col_cs}
\end{table}

% MMLU College Mathematics (Qwen3)
\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrrrr}
\toprule
 & \textbf{Qwen3-0.6B} & \textbf{Qwen3-1.7B} & \textbf{Qwen3-4B} & \textbf{Qwen3-8B} & \textbf{Qwen3-14B} \\
\midrule
Overall & 31.0 & 39.0 & 55.0 & 59.0 & 68.0 \\
\midrule
Advanced Real Analysis (19.0\%) & 26.3 & 26.3 & 42.1 & 52.6 & 52.6 \\
Abstract Algebra (11.0\%) & 27.3 & 45.5 & 90.9 & 63.6 & 81.8 \\
Probability (7.0\%) & 28.6 & 71.4 & 28.6 & 57.1 & 57.1 \\
Properties of Mathematical Operations and Functions (5.0\%) & 20.0 & 20.0 & 40.0 & 80.0 & 80.0 \\
Advanced Mathematical Concepts and Applications (16.0\%) & 31.3 & 37.5 & 75.0 & 56.3 & 75.0 \\
Mathematical Modeling and Algorithms (10.0\%) & 30.0 & 50.0 & 60.0 & 60.0 & 70.0 \\
Multivariable Calculus (27.0\%) & 25.9 & 37.0 & 40.7 & 59.3 & 63.0 \\
Mathematical Optimization Methods (5.0\%) & 100.0 & 40.0 & 80.0 & 60.0 & 100.0 \\
\bottomrule
\end{tabular}}
\caption{Evaluation results for the Qwen3 model family on MMLU College Mathematics.}
\label{tab:qwen3_col_math}
\end{table}

% MMLU College Physics (Qwen3)
\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrrrr}
\toprule
 & \textbf{Qwen3-0.6B} & \textbf{Qwen3-1.7B} & \textbf{Qwen3-4B} & \textbf{Qwen3-8B} & \textbf{Qwen3-14B} \\
\midrule
Overall & 24.5 & 34.3 & 58.8 & 57.8 & 69.6 \\
\midrule
Quantum Mechanics Principles and Applications (22.5\%) & 56.5 & 34.8 & 65.2 & 69.6 & 73.9 \\
Thermodynamics (8.8\%) & 0.0 & 55.6 & 66.7 & 55.6 & 77.8 \\
Special Relativity Concepts (13.7\%) & 21.4 & 14.3 & 71.4 & 50.0 & 57.1 \\
Classical Physics Principles and Relationships (20.6\%) & 9.5 & 38.1 & 47.6 & 52.4 & 61.9 \\
Physics Phenomena and Applications (16.7\%) & 0.0 & 11.8 & 11.8 & 17.6 & 58.8 \\
Electromagnetism (5.9\%) & 16.7 & 16.7 & 83.3 & 83.3 & 66.7 \\
Solid State Physics Concepts (11.8\%) & 50.0 & 75.0 & 100.0 & 100.0 & 100.0 \\
\bottomrule
\end{tabular}}
\caption{Evaluation results for the Qwen3 model family on MMLU College Physics.}
\label{tab:qwen3_col_phy}
\end{table}

% MMLU High School U.S. History (Qwen3)
\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrrrr}
\toprule
 & \textbf{Qwen3-0.6B} & \textbf{Qwen3-1.7B} & \textbf{Qwen3-4B} & \textbf{Qwen3-8B} & \textbf{Qwen3-14B} \\
\midrule
Overall & 52.5 & 66.2 & 83.3 & 88.7 & 91.7 \\
\midrule
US Sociopolitical Ideologies, Movements, and Issues (46.1\%) & 52.1 & 64.9 & 83.0 & 90.4 & 95.7 \\
Progressive Era Economic and Social Initiatives (3.9\%) & 75.0 & 87.5 & 100.0 & 100.0 & 100.0 \\
United States Governance and Politics (37.3\%) & 46.1 & 61.8 & 81.6 & 85.5 & 88.2 \\
Ideological and Territorial Expansion in the Americas (9.8\%) & 70.0 & 80.0 & 90.0 & 95.0 & 85.0 \\
American History Eras (2.9\%) & 50.0 & 66.7 & 66.7 & 66.7 & 83.3 \\
\bottomrule
\end{tabular}}
\caption{Evaluation results for the Qwen3 model family on MMLU High School U.S. History.}
\label{tab:qwen3_hs_us_hist}
\end{table}

\subsection{Gemma 3 Family}
See Tables~\ref{tab:llama_arce}, \ref{tab:llama_boolq}, \ref{tab:llama_anat}, \ref{tab:llama_col_bio}, \ref{tab:llama_col_cs}, \ref{tab:llama_col_math}, \ref{tab:llama_col_phy}, and \ref{tab:llama_hs_us_hist} for the extended results of Gemma 3 model family.

\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrrr}
\toprule
& \textbf{Gemma 3 1B} & \textbf{Gemma 3 4B} & \textbf{Gemma 3 12B} & \textbf{Gemma 3 27B}\\
\midrule
Overall & 72.0 & 81.6 & 87.2 & 87.5\\
\midrule
Geology and Earth Sciences (15.0\%) & 74.5 & 84.6 & 87.7 & 89.4\\
Scientific Principles and Processes (32.6\%) & 71.2 & 79.7 & 86.5 & 87.0\\
Biological Processes and Concepts (24.1\%) & 74.3 & 83.0 & 88.5 & 87.8\\
Physics Principles in Engineering and Science (9.4\%) & 70.5 & 83.0 & 88.8 & 88.8\\
Environmental and Energy Assessment (6.5\%) & 68.2 & 80.5 & 83.1 & 84.4\\
Fundamental Concepts in Astronomy (7.6\%) & 72.8 & 79.4 & 86.7 & 87.2\\
Fundamentals of Chemical and Material Properties (4.8\%) & 64.0 & 79.8 & 86.8 & 86.8\\
\bottomrule
\end{tabular}}
\caption{Multi-dimensional evaluation results for the Gemma 3 model family on ARC-Easy.}
\label{tab:llama_arce}
\end{table}

\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrrr}
\toprule
& \textbf{Gemma 3 1B} & \textbf{Gemma 3 4B} & \textbf{Gemma 3 12B} & \textbf{Gemma 3 27B}\\
\midrule
Overall & 66.5 & 79.0 & 85.3 & 87.1\\
\midrule
Product Composition, Properties, and Standards (8.8\%) & 67.5 & 76.1 & 82.4 & 88.6\\
Geographic, Operational, and Temporal Analysis (11.0\%) & 67.6 & 83.7 & 84.8 & 87.3\\
Media Standards and Analysis (24.8\%) & 66.7 & 80.6 & 88.5 & 89.6\\
Scientific and Analytical Principles (9.0\%) & 65.2 & 75.1 & 82.9 & 86.0\\
Sports History and Regulations (10.1\%) & 63.2 & 78.7 & 83.0 & 86.0\\
Governmental Laws and Regulations (10.6\%) & 64.1 & 75.7 & 80.3 & 82.0\\
Human Biology and Medical Science (6.5\%) & 77.0 & 82.6 & 86.9 & 89.2\\
Economic Systems (8.3\%) & 66.1 & 77.1 & 85.6 & 85.6\\
Sociocultural, Geopolitical, and Linguistic Analysis (7.1\%) & 61.8 & 78.1 & 87.6 & 84.1\\
Fictional Narrative Analysis and Elements (3.8\%) & 71.2 & 79.2 & 91.2 & 90.4\\
\bottomrule
\end{tabular}}
\caption{Multi-dimensional evaluation results for the Gemma 3 model family on BoolQ.}
\label{tab:llama_boolq}
\end{table}

\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrrr}
\toprule
& \textbf{Gemma 3 1B} & \textbf{Gemma 3 4B} & \textbf{Gemma 3 12B} & \textbf{Gemma 3 27B}\\
\midrule
Overall & 25.9 & 61.5 & 70.4 & 70.4\\
\midrule
General Human Anatomy, Physiology, and Terminology (31.9\%) & 27.9 & 69.8 & 83.7 & 79.1\\
Head and Neck Anatomy (21.5\%) & 27.6 & 41.4 & 55.2 & 48.3\\
Skeletal Development and Anatomy (14.1\%) & 21.1 & 52.6 & 52.6 & 57.9\\
Neurological Disorders (10.4\%) & 42.9 & 64.3 & 57.1 & 71.4\\
Bone Anatomy and Terminology (3.0\%) & 50.0 & 50.0 & 50.0 & 50.0\\
Anatomy of Circulatory System (2.2\%) & 0.0 & 100.0 & 100.0 & 100.0\\
Developmental Structures (6.7\%) & 11.1 & 44.4 & 77.8 & 88.9\\
Nephrology (10.4\%) & 14.3 & 92.9 & 92.9 & 92.9\\
\bottomrule
\end{tabular}}
\caption{Multi-dimensional evaluation results for the Gemma 3 model family on MMLU Anatomy.}
\label{tab:llama_anat}
\end{table}

\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrrr}
\toprule
& \textbf{Gemma 3 1B} & \textbf{Gemma 3 4B} & \textbf{Gemma 3 12B} & \textbf{Gemma 3 27B}\\
\midrule
Multicellular Biology (42.4\%) & 27.9 & 68.9 & 98.4 & 93.4\\
Evolutionary and Ecological Processes (25.7\%) & 21.6 & 67.6 & 86.5 & 86.5\\
Molecular and Cellular Biology (31.9\%) & 21.7 & 65.2 & 87.0 & 91.3\\
\bottomrule
\end{tabular}}
\caption{Multi-dimensional evaluation results for the Gemma 3 model family on MMLU College Biology.}
\label{tab:llama_col_bio}
\end{table}

\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrrr}
\toprule
& \textbf{Gemma 3 1B} & \textbf{Gemma 3 4B} & \textbf{Gemma 3 12B} & \textbf{Gemma 3 27B}\\
\midrule
Overall & 30.0 & 48.0 & 57.0 & 63.0\\
\midrule
Theoretical Foundations of Computation (52.0\%) & 36.5 & 38.5 & 51.9 & 61.5\\
Computer Architecture and Optimization (7.0\%) & 28.6 & 57.1 & 71.4 & 57.1\\
Operating Systems (10.0\%) & 20.0 & 60.0 & 80.0 & 80.0\\
Network Layer Protocols and Technologies (5.0\%) & 20.0 & 60.0 & 100.0 & 100.0\\
Data Processing (12.0\%) & 25.0 & 33.3 & 41.7 & 41.7\\
Sorting Algorithms (4.0\%) & 0.0 & 100.0 & 100.0 & 75.0\\
Graph Algorithms and Data Structures (10.0\%) & 30.0 & 70.0 & 30.0 & 60.0\\
\bottomrule
\end{tabular}}
\caption{Multi-dimensional evaluation results for the Gemma 3 model family on MMLU College Computer Science.}
\label{tab:llama_col_cs}
\end{table}

\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrrr}
\toprule
& \textbf{Gemma 3 1B} & \textbf{Gemma 3 4B} & \textbf{Gemma 3 12B} & \textbf{Gemma 3 27B}\\
\midrule
Overall & 33.0 & 41.0 & 50.0 & 58.0\\
\midrule
Advanced Real Analysis (19.0\%) & 57.9 & 52.6 & 47.4 & 52.6\\
Abstract Algebra (11.0\%) & 27.3 & 72.7 & 63.6 & 54.5\\
Probability (7.0\%) & 14.3 & 57.1 & 42.9 & 57.1\\
Properties of Mathematical Operations and Functions (5.0\%) & 60.0 & 40.0 & 40.0 & 80.0\\
Advanced Mathematical Concepts and Applications (16.0\%) & 25.0 & 31.3 & 56.3 & 68.8\\
Mathematical Modeling and Algorithms (10.0\%) & 20.0 & 30.0 & 50.0 & 60.0\\
Multivariable Calculus (27.0\%) & 29.6 & 25.9 & 40.7 & 55.6\\
Mathematical Optimization Methods (5.0\%) & 20.0 & 40.0 & 80.0 & 40.0\\
\bottomrule
\end{tabular}}
\caption{Multi-dimensional evaluation results for the Gemma 3 model family on MMLU College Mathematics.}
\label{tab:llama_col_math}
\end{table}

\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrrr}
\toprule
& \textbf{Gemma 3 1B} & \textbf{Gemma 3 4B} & \textbf{Gemma 3 12B} & \textbf{Gemma 3 27B}\\
\midrule
Overall & 20.6 & 41.2 & 52.9 & 63.7\\
\midrule
Quantum Mechanics Principles and Applications (22.5\%) & 8.7 & 39.1 & 43.5 & 73.9\\
Thermodynamics (8.8\%) & 11.1 & 22.2 & 55.6 & 55.6\\
Special Relativity Concepts (13.7\%) & 28.6 & 35.7 & 42.9 & 50.0\\
Classical Physics Principles and Relationships (20.6\%) & 42.9 & 28.6 & 57.1 & 66.7\\
Physics Phenomena and Applications (16.7\%) & 5.9 & 41.2 & 29.4 & 41.2\\
Electromagnetism (5.9\%) & 0.0 & 66.7 & 83.3 & 50.0\\
Solid State Physics Concepts (11.8\%) & 33.3 & 75.0 & 91.7 & 100.0\\
\bottomrule
\end{tabular}}
\caption{Multi-dimensional evaluation results for the Gemma 3 model family on MMLU College Physics.}
\label{tab:llama_col_phy}
\end{table}

\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrrr}
\toprule
& \textbf{Gemma 3 1B} & \textbf{Gemma 3 4B} & \textbf{Gemma 3 12B} & \textbf{Gemma 3 27B}\\
\midrule
Overall & 26.0 & 75.5 & 88.2 & 91.2\\
\midrule
US Sociopolitical Ideologies, Movements, and Issues (46.1\%) & 26.6 & 80.9 & 87.2 & 92.6\\
Progressive Era Economic and Social Initiatives (3.9\%) & 25.0 & 100.0 & 100.0 & 87.5\\
United States Governance and Politics (37.3\%) & 26.3 & 67.1 & 89.5 & 90.8\\
Ideological and Territorial Expansion in the Americas (9.8\%) & 20.0 & 75.0 & 85.0 & 90.0\\
American History Eras (2.9\%) & 33.3 & 66.7 & 83.3 & 83.3\\
\bottomrule
\end{tabular}}
\caption{Multi-dimensional evaluation results for the Gemma 3 model family on MMLU High School U.S. History.}
\label{tab:llama_hs_us_hist}
\end{table}

\section{Additional Related Work}
\label{sec:add_rel_work}

\paragraph{Language Model Evaluation.} Reliable evaluation is essential for accurately assessing model capabilities and enabling fair comparisons, which in turn informs future developments. Hence, there has been a surge in the development of benchmarks designed to test various model capabilities, such as reasoning \citep{bisk2019piqareasoningphysicalcommonsense, sap2019socialiqacommonsensereasoningsocial, zellers2019hellaswagmachinereallyfinish, liu2020logiqachallengedatasetmachine}, world knowledge \citep{mihaylov2018suitarmorconductelectricity, hendrycks2020measuring, srivastava2023imitationgamequantifyingextrapolating}, and truthfulness \citep{lin2022truthfulqameasuringmodelsmimic, khatun2024truthevaldatasetevaluatellm}. Beyond individual benchmarks, holistic frameworks have emerged to offer a more comprehensive assessment of model performance \citep{liang2023holisticevaluationlanguagemodels, chiang2024chatbot, eval-harness, lighteval, srivastava2023imitationgamequantifyingextrapolating}. Reciprocally, understanding and improving current benchmarks have been equally important. MMLU Pro \citep{wang2024mmluprorobustchallengingmultitask} and Big-Bench-Hard \citep{suzgun2022challenging} address benchmark saturation by constructing more challenging variants of MMLU \citep{hendrycks2020measuring} and Big-Bench \citep{srivastava2023beyond} respectively. As top models approach ceiling effects on narrow probes, evaluation has shifted toward complex end-to-end tasks and composite suites. HLE and ARC-AGI assess multi-step reasoning, tool use, and robustness across domains \citep{hle, arcagi}. Execution-grounded tasks such as SWE-bench measure real-world software problems and end-to-end correctness \citep{swebench}. Competitive exams like AIME and IMO, and professional exams such as the bar, push systems toward expert-level competence. Another recent practice is evaluation with online leaderboards, which use hidden test sets, fixed prompts, and compute disclosures in order to support fair comparison and consistent progress tracking \citep{chiang2024chatbot}. Yet, these advances rest on a common premise that benchmarks reliably evaluate models on their stated domains. We audit this premise by testing whether benchmarks provide balanced coverage and promote comparable performance across subdomains.

\section{Hierarchical Labeling for Multi-Dimensional Evaluation}
\label{sec:hier_labeling}

We build a tree benchmark over questions, then assign concise, human-readable labels to every node. Leaves summarize the shared evaluation focus of their questions, while internal nodes summarize their children.

To build the tree, we recursively induce partitions as discussed in \S\ref{subsec:methodology}, starting from the root (i.e., the entire benchmark) and ending at leaves (i.e., the clusters that do not admit a valid partition). For labeling leaves, we gather brief question annotations within a leaf and ask a model for one specific noun-phrase label. For labeling internal nodes of the tree, we pass the child labels to the model and ask for a slightly more abstract label that still captures the shared theme. Therefore, this procedure yields a bottom-up label propagation from leaves to internal nodes then to the root. We use \texttt{Gemini-2.0-flash} to annotate individual questions, assign each leaf a label from its question annotations, and propagate labels upward by aggregating child labels.

\paragraph{Prompts.} We share the prompts we use for annotating the questions (Prompt~\ref{box:annot_q}), labeling the leaves (Prompt~\ref{box:label_leaves}), and labeling the internal nodes (Prompt~\ref{box:label_nodes}).

\begin{tcolorbox}[
    float=htb,
    title={Prompt for annotating questions.},
    colback=gray!5!white,
    colframe=black!75,
    fonttitle=\bfseries,
    label={box:annot_q}
]
\small
You are given a question from the BENCHMARK benchmark. \\
Given this question, generate a single, concise sentence that clearly describes the **specific evaluation focus** of the question.  \\
Question: QUESTION \\
Requirements: \\
- Do not have a prefix, simply provide a brief phrase or a gerund. \\
- Do not add commentary. \\
\end{tcolorbox}

\begin{tcolorbox}[
    float=htb,
    title={Prompt for labeling leaves.},
    colback=gray!5!white,
    colframe=black!75,
    fonttitle=\bfseries,
    label={box:label_leaves}
]
\small
You are a taxonomy assistant. Your task is to read short annotations that describe what each question evaluates and produce one concise but descriptive label that summarizes the shared knowledge or concept.  \\
Guidelines:\\
- The label must be highly specific, directly capturing the core idea, while still generalizable across closely related items.  \\
- Prioritize specificity: avoid vague or overly broad terms.  \\
- Use a clear noun phrase.  \\
- Return only the label text. \\
\end{tcolorbox}

\begin{tcolorbox}[
    float=htb,
    title={Prompt for labeling internal nodes.},
    colback=gray!5!white,
    colframe=black!75,
    fonttitle=\bfseries,
    label={box:label_nodes}
]
\small
You are a taxonomy assistant. Your task is to read the labels of child clusters and generate one concise but descriptive parent label that captures their common theme at a higher level of abstraction.  \\
Guidelines: \\
- The label must be specific and clearly meaningful, while still broad enough to encompass all children.  \\
- Prioritize specificity: avoid vague or generic terms that do not capture the essence of the group.  \\
- Use a clear noun phrase. \\  
- Return only the label text. \\
\end{tcolorbox}

\section{The Use of Large Language Models (LLMs)}
\label{sec:use_of_llms}

In this work, we used large language models (LLMs) only for light polishing (grammar, wording, and clarity) after the technical content was written. LLMs were not used for research ideation, experimental design or execution, analysis, figure or table generation, or drafting technical sections. All substantive content, results, and conclusions are authored by the listed authors, who take full responsibility for the paper’s contents, including any text edited with LLM assistance. LLMs are not eligible for authorship, and no LLM is listed as an author.

\end{document}