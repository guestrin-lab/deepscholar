@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{tsai2019transformer,
  title={Transformer dissection: a unified understanding of transformer's attention via the lens of kernel},
  author={Tsai, Yao-Hung Hubert and Bai, Shaojie and Yamada, Makoto and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1908.11775},
  year={2019}
}
@article{choromanski2020rethinking,
  title={Rethinking attention with performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  journal={arXiv preprint arXiv:2009.14794},
  year={2020}
}
@article{oren2024transformers,
  title={Transformers are multi-state rnns},
  author={Oren, Matanel and Hassid, Michael and Adi, Yossi and Schwartz, Roy},
  journal={arXiv preprint arXiv:2401.06104},
  year={2024}
}
@inproceedings{Gu2022,
  title={{Efficiently Modeling Long Sequences with Structured State Spaces}},
  author={Gu, Albert and Goel, Karan and R\'e, Christopher},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2022}
}
@article{shashua2009introduction,
  title={Introduction to machine learning: Class notes 67577},
  author={Shashua, Amnon},
  journal={arXiv preprint arXiv:0904.3664},
  year={2009}
}
@article{lopez2018data,
  title={Data-independent random projections from the feature-space of the homogeneous polynomial kernel},
  author={L{\'o}pez-S{\'a}nchez, Daniel and Arrieta, Ang{\'e}lica Gonz{\'a}lez and Corchado, Juan M},
  journal={Pattern Recognition},
  volume={82},
  pages={130--146},
  year={2018},
  publisher={Elsevier}
}


@article{nguyen2022s4nd,
      title={S4ND: Modeling Images and Videos as Multidimensional Signals Using State Spaces}, 
      author={Eric Nguyen and Karan Goel and Albert Gu and Gordon W. Downs and Preey Shah and Tri Dao and Stephen A. Baccus and Christopher Ré},
      year={2022},
      journal={Advances in Neural Information Processing Systems}
}

@misc{zucchet2023online,
      title={Online learning of long-range dependencies}, 
      author={Nicolas Zucchet and Robert Meier and Simon Schug and Asier Mujika and João Sacramento},
      year={2023},
      eprint={2305.15947},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{katsch2023gateloop,
      title={GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling}, 
      author={Tobias Katsch},
      year={2023},
      eprint={2311.01927},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{sun2023retentive,
      title={Retentive Network: A Successor to Transformer for Large Language Models}, 
      author={Yutao Sun and Li Dong and Shaohan Huang and Shuming Ma and Yuqing Xia and Jilong Xue and Jianyong Wang and Furu Wei},
      year={2023},
      eprint={2307.08621},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{qin2024hgrn2,
  title={HGRN2: Gated linear RNNs with state expansion},
  author={Qin, Zhen and Yang, Songlin and Sun, Weixuan and Shen, Xuyang and Li, Dong and Sun, Weigao and Zhong, Yiran},
  journal={arXiv preprint arXiv:2404.07904},
  year={2024}
}

@article{peng2023rwkv,
  title={RWKV: Reinventing RNNs for the Transformer Era},
  author={Peng, Bo and Alcaide, Eric and Anthony, Quentin and Albalak, Alon and Arcadinho, Samuel and Cao, Huanqi and Cheng, Xin and Chung, Michael and Grella, Matteo and GV, Kranthi Kiran and others},
  journal={arXiv preprint arXiv:2305.13048},
  year={2023}
}

@article{zucchet2023gated,
  title={Gated recurrent neural networks discover attention},
  author={Zucchet, Nicolas and Kobayashi, Seijin and Akram, Yassir and Von Oswald, Johannes and Larcher, Maxime and Steger, Angelika and Sacramento, Joao},
  journal={arXiv preprint arXiv:2309.01775},
  year={2023}
}

@article{merrill2024illusion,
  title={The illusion of state in state-space models},
  author={Merrill, William and Petty, Jackson and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:2404.08819},
  year={2024}
}

@article{yang2023gated,
  title={Gated Linear Attention Transformers with Hardware-Efficient Training},
  author={Yang, Songlin and Wang, Bailin and Shen, Yikang and Panda, Rameswar and Kim, Yoon},
  journal={arXiv preprint arXiv:2312.06635},
  year={2023}
}

@article{liu2024point,
  title={Point mamba: A novel point cloud backbone based on state space model with octree-based ordering strategy},
  author={Liu, Jiuming and Yu, Ruiji and Wang, Yian and Zheng, Yu and Deng, Tianchen and Ye, Weicai and Wang, Hesheng},
  journal={arXiv preprint arXiv:2403.06467},
  year={2024}
}

@article{wang2024mambabyte,
  title={Mambabyte: Token-free selective state space model},
  author={Wang, Junxiong and Gangavarapu, Tushaar and Yan, Jing Nathan and Rush, Alexander M},
  journal={arXiv preprint arXiv:2401.13660},
  year={2024}
}

@article{Touvron2022DeiTIR,
  title={DeiT III: Revenge of the ViT},
  author={Hugo Touvron and Matthieu Cord and Herve Jegou},
  journal={arXiv preprint arXiv:2204.07118},
  year={2022},
}

@article{ali2024hidden,
  title={The Hidden Attention of Mamba Models},
  author={Ali, Ameen and Zimerman, Itamar and Wolf, Lior},
  journal={arXiv preprint arXiv:2403.01590},
  year={2024}
}

@article{lu2023structured,
      title={Structured State Space Models for In-Context Reinforcement Learning}, 
      author={Chris Lu and Yannick Schroecker and Albert Gu and Emilio Parisotto and Jakob Foerster and Satinder Singh and Feryal Behbahani},
      year={2023},
      journal={Advances in Neural Information Processing Systems}
}

@article{cirone2024theoretical,
  title={Theoretical Foundations of Deep Selective State-Space Models},
  author={Cirone, Nicola Muca and Orvieto, Antonio and Walker, Benjamin and Salvi, Cristopher and Lyons, Terry},
  journal={arXiv preprint arXiv:2402.19047},
  year={2024}
}

@article{goel2022s,
  title={It's Raw! Audio Generation with State-Space Models},
  author={Goel, Karan and Gu, Albert and Donahue, Chris and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2202.09729},
  year={2022}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International Conference on Machine Learning},
  year={2015},
}

@article{gupta2022simplifying,
  title={Simplifying and Understanding State Space Models with Diagonal Linear RNNs},
  author={Gupta, Ankit and Mehta, Harsh and Berant, Jonathan},
  journal={arXiv preprint arXiv:2212.00768},
  year={2022}
}

@inproceedings{bender2021dangers,
  title={On the dangers of stochastic parrots: Can language models be too big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
  pages={610--623},
  year={2021}
}
@misc{Gu2022s4d,
      url = {https://arxiv.org/abs/2206.11893},
      title={{On the Parameterization and Initialization of Diagonal State Space Models}}, 
      author={Albert Gu and Ankit Gupta and Karan Goel and Christopher Ré},
      year={2022},
      publisher={arXiv}
}
@article{nauen2024taylorshift,
  title={TaylorShift: Shifting the Complexity of Self-Attention from Squared to Linear (and Back) using Taylor-Softmax},
  author={Nauen, Tobias Christian and Palacio, Sebastian and Dengel, Andreas},
  journal={arXiv preprint arXiv:2403.02920},
  year={2024}
}
@inproceedings{s5,
title={{Simplified State Space Layers for Sequence Modeling}},
author={Jimmy T.H. Smith and Andrew Warrington and Scott Linderman},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
}
@article{S4,
  title={Efficiently modeling long sequences with structured state spaces},
  author={Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2111.00396},
  year={2021}
}
@inproceedings{LRU,
  title={Resurrecting recurrent neural networks for long sequences},
  author={Orvieto, Antonio and Smith, Samuel L and Gu, Albert and Fernando, Anushan and Gulcehre, Caglar and Pascanu, Razvan and De, Soham},
  booktitle={International Conference on Machine Learning},
  pages={26670--26698},
  year={2023},
  organization={PMLR}
}
@inproceedings{Tay2021,
title={{Long Range Arena : A Benchmark for Efficient Transformers}},
author={Yi Tay and Mostafa Dehghani and Samira Abnar and Yikang Shen and Dara Bahri and Philip Pham and Jinfeng Rao and Liu Yang and Sebastian Ruder and Donald Metzler},
booktitle={International Conference on Learning Representations ({ICLR})},
year={2021},
}
@InProceedings{Orvieto2023,
  title = {{Resurrecting Recurrent Neural Networks for Long Sequences}},
  author = {Orvieto, Antonio and Smith, Samuel L and Gu, Albert and Fernando, Anushan and Gulcehre, Caglar and Pascanu, Razvan and De, Soham},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  pages = {26670--26698},
  year = {2023},
  volume = {202},
  month = {23--29 Jul},
  publisher = {PMLR},
}
@misc{mamba,
      url = {https://arxiv.org/abs/2312.00752},
      title={{Mamba: Linear-Time Sequence Modeling with Selective State Spaces}}, 
      author={Albert Gu and Tri Dao},
      year={2023},
      publisher={arXiv}
}
@inproceedings{mamba2,
      title={{Transformers are SSMs: Generalized Models and Efficient Algorithms with Structured State Space Duality}}, 
      author={Tri Dao and Albert Gu},
      booktitle={{ICML} 2024},
      year={2024},
}

@article{li2022makes,
  title={What makes convolutional models great on long sequence modeling?},
  author={Li, Yuhong and Cai, Tianle and Zhang, Yi and Chen, Deming and Dey, Debadeepta},
  journal={arXiv preprint arXiv:2210.09298},
  year={2022}
}

@misc{griffin,
      url = {https://arxiv.org/abs/2402.19427},
      title={{Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models}}, 
      author={Soham De and Samuel L. Smith and Anushan Fernando and Aleksandar Botev and George Cristian-Muraru and Albert Gu and Ruba Haroun and Leonard Berrada and Yutian Chen and Srivatsan Srinivasan and Guillaume Desjardins and Arnaud Doucet and David Budden and Yee Whye Teh and Razvan Pascanu and Nando De Freitas and Caglar Gulcehre},
      year={2024},
      publisher={arXiv}
}

@inproceedings{poli2023hyena,
  title={Hyena hierarchy: Towards larger convolutional language models},
  author={Poli, Michael and Massaroli, Stefano and Nguyen, Eric and Fu, Daniel Y and Dao, Tri and Baccus, Stephen and Bengio, Yoshua and Ermon, Stefano and R{\'e}, Christopher},
  booktitle={International Conference on Machine Learning},
  pages={28043--28078},
  year={2023},
  organization={PMLR}
}

@article{liu2024vmamba,
  title={VMamba: Visual State Space Model},
  author={Liu, Yue and Tian, Yunjie and Zhao, Yuzhong and Yu, Hongtian and Xie, Lingxi and Wang, Yaowei and Ye, Qixiang and Liu, Yunfan},
  journal={arXiv preprint arXiv:2401.10166},
  year={2024}
}

@article{zhu2024vision,
    title={Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model}, 
    author={Lianghui Zhu and Bencheng Liao and Qian Zhang and Xinlong Wang and Wenyu Liu and Xinggang Wang},
    year={2024},
    eprint={2401.09417},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@article{schiff2024caduceus,
  title={Caduceus: Bi-directional equivariant long-range dna sequence modeling},
  author={Schiff, Yair and Kao, Chia-Hsiang and Gokaslan, Aaron and Dao, Tri and Gu, Albert and Kuleshov, Volodymyr},
  journal={arXiv preprint arXiv:2403.03234},
  year={2024}
}

@misc{Goel2022,
      url = {https://arxiv.org/abs/2202.09729},
      title={{It's Raw! Audio Generation with State-Space Models}}, 
      author={Karan Goel and Albert Gu and Chris Donahue and Christopher Ré},
      year={2022},
      publisher={arXiv}
}
@article{Blelloch1990,
  title={Prefix sums and their applications},
  author={Blelloch, Guy E},
  year={1990},
  publisher={School of Computer Science, Carnegie Mellon University Pittsburgh, PA, USA}
}
@inproceedings{Gupta2022,
 author = {Gupta, Ankit and Gu, Albert and Berant, Jonathan},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {22982--22994},
 publisher = {Curran Associates, Inc.},
 title = {Diagonal State Spaces are as Effective as Structured State Spaces},
 volume = {35},
 year = {2022}
}
@article{Hochreiter1997,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT press}
}
@inproceedings{Glorot2010,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}
@article{Klambauer2017,
  title={Self-normalizing neural networks},
  author={Klambauer, G{\"u}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@book{Goodfellow2016,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT press}
}
@misc{Kingma2014,
      url = {https://arxiv.org/abs/1412.6980},
      title={{Adam: A method for stochastic optimization}},
      author={Kingma, Diederik P and Ba, Jimmy},
      year={2014},
      publisher={arXiv}
}
@inproceedings{Gu2020,
 author = {Gu, Albert and Dao, Tri and Ermon, Stefano and Rudra, Atri and R\'{e}, Christopher},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1474--1487},
 publisher = {Curran Associates, Inc.},
 title = {{HiPPO: Recurrent Memory with Optimal Polynomial Projections}},
 volume = {33},
 year = {2020}
}
@misc{Fu2023,
      url = {https://arxiv.org/abs/2212.14052},
      title={{Hungry Hungry Hippos: Towards Language Modeling with State Space Models}},
      author={Daniel Y. Fu and Tri Dao and Khaled K. Saab and Armin W. Thomas and Atri Rudra and Christopher Ré},
      year={2023},
      publisher={arXiv}
}
@article{Sherman1950,
author = {Jack Sherman and Winifred J. Morrison},
title = {{Adjustment of an Inverse Matrix Corresponding to a Change in One Element of a Given Matrix}},
volume = {21},
journal = {The Annals of Mathematical Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {124 -- 127},
year = {1950}
}
@inproceedings{Gu2021,
 author = {Gu, Albert and Johnson, Isys and Goel, Karan and Saab, Khaled and Dao, Tri and Rudra, Atri and R\'{e}, Christopher},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {572--585},
 publisher = {Curran Associates, Inc.},
 title = {{Combining Recurrent, Convolutional, and Continuous-time Models with Linear State Space Layers}},
 volume = {34},
 year = {2021}
}
@inproceedings{Transformer,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {{Attention is All you Need}},
 volume = {30},
 year = {2017}
}
@misc{Longformer,
      url = {https://arxiv.org/abs/2004.05150},
      title={{Longformer: The Long-Document Transformer}},
      author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
      year={2020},
      publisher={arXiv}
}
@article{Bigbird,
  title={{Big bird: Transformers for longer sequences}},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{wang2022pretraining,
  title={Pretraining without attention},
  author={Wang, Junxiong and Yan, Jing Nathan and Gu, Albert and Rush, Alexander M},
  journal={arXiv preprint arXiv:2212.10544},
  year={2022}
}

@inproceedings{Reformer,
title={{Reformer: The Efficient Transformer}},
author={Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},
booktitle={International Conference on Learning Representations},
year={2020},
}
@inproceedings{Performer,
title={{Rethinking Attention with Performers}},
author={Krzysztof Marcin Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Quincy Davis and Afroz Mohiuddin and Lukasz Kaiser and David Benjamin Belanger and Lucy J Colwell and Adrian Weller},
booktitle={International Conference on Learning Representations},
year={2021},
}
@misc{Axialattention,
      url = {https://arxiv.org/abs/1912.12180},
      title={{Axial Attention in Multidimensional Transformers}},
      author={Jonathan Ho and Nal Kalchbrenner and Dirk Weissenborn and Tim Salimans},
      year={2019},
      publisher={arXiv}
}
@inproceedings{MLP,
title={{Pay Attention to MLPs}},
author={Hanxiao Liu and Zihang Dai and David So and Quoc V Le},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
}
@misc{Ali2024,
      url = {https://arxiv.org/abs/2403.01590},
      title={{The Hidden Attention of Mamba Models}}, 
      author={Ameen Ali and Itamar Zimerman and Lior Wolf},
      year={2024},
      publisher={arXiv}
}
@article{zoology2023,
  title={{Zoology: Measuring and Improving Recall in Efficient Language Models}},
  author={Arora, Simran and Eyuboglu, Sabri and Timalsina, Aman and Johnson, Isys and Poli, Michael and Zou, James and Rudra, Atri and Ré, Christopher},
  journal={	arXiv:2312.04927},
  year={2023}
}
@misc{qlstm,
      title={{The Languini Kitchen: Enabling Language Modelling Research at Different Scales of Compute}}, 
      author={Aleksandar Stanić and Dylan Ashley and Oleg Serikov and Louis Kirsch and Francesco Faccio and Jürgen Schmidhuber and Thomas Hofmann and Imanol Schlag},
      year={2023},
      eprint={2309.11197},
}
@inproceedings{adamw,
title={{Decoupled Weight Decay Regularization}},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}
@inproceedings{Brown2020,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {{Language Models are Few-Shot Learners}},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}
@article{xlstm,
      title={{xLSTM: Extended Long Short-Term Memory}}, 
      author={Maximilian Beck and Korbinian Pöppel and Markus Spanring and Andreas Auer and Oleksandra Prudnikova and Michael Kopp and Günter Klambauer and Johannes Brandstetter and Sepp Hochreiter},
      year={2024},
      journal={arXiv preprint arXiv:2405.04517}
}
@article{hgrn2,
      title={{HGRN2: Gated Linear RNNs with State Expansion}}, 
      author={Zhen Qin and Songlin Yang and Weixuan Sun and Xuyang Shen and Dong Li and Weigao Sun and Yiran Zhong},
      year={2024},
      journal={arXiv preprint arXiv:2404.07904}
}
@software{accelerated-scan,
author =   {Volodymyr Kyrylov},
title =    {{Accelerated Scan}},
url = {https://github.com/proger/accelerated-scan},
month = jan,
year = {2024}
}
@article{flashattention2,
  title={Flash{A}ttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Dao, Tri},
  year={2023}
}
@software{flashlinattention,
  title  = {FLA: A Triton-Based Library for Hardware-Efficient Implementations of Linear Attention Mechanism},
  author = {Yang, Songlin and Zhang, Yu},
  url    = {https://github.com/sustcsonglin/flash-linear-attention},
  month  = jan,
  year   = {2024}
}
@inproceedings{Katharopoulos2020,
author = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran\c{c}ois},
title = {Transformers are {RNN}s: fast autoregressive transformers with linear attention},
year = {2020},
publisher = {JMLR.org},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {478},
numpages = {10},
series = {ICML'20}
}

@inproceedings{arorasimple2024,
  title={Simple linear attention language models balance the recall-throughput tradeoff},
  author={Arora, Simran and Eyuboglu, Sabri and Zhang, Michael and Timalsina, Aman and Alberti, Silas and Zou, James and Rudra, Atri and Re, Christopher},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{dorfler2024towards,
  title={Towards a systems theory of algorithms},
  author={D{\"o}rfler, Florian and He, Zhiyu and Belgioioso, Giuseppe and Bolognani, Saverio and Lygeros, John and Muehlebach, Michael},
  journal={IEEE Control Systems Letters},
  year={2024},
  publisher={IEEE}
}

@article{bhattamishra2024separations,
  title={Separations in the Representational Capabilities of Transformers and Recurrent Architectures},
  author={Bhattamishra, Satwik and Hahn, Michael and Blunsom, Phil and Kanade, Varun},
  journal={arXiv preprint arXiv:2406.09347},
  year={2024}
}

@inproceedings{merrillillusion2024,
  title={The Illusion of State in State-Space Models},
  author={Merrill, William and Petty, Jackson and Sabharwal, Ashish},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@inproceedings{schlag2021linear,
  title={Linear transformers are secretly fast weight programmers},
  author={Schlag, Imanol and Irie, Kazuki and Schmidhuber, J{\"u}rgen},
  booktitle={International Conference on Machine Learning},
  pages={9355--9366},
  year={2021},
  organization={PMLR}
}
@article{orvieto2024universality,
  title={Universality of Linear Recurrences Followed by Non-linear Projections: Finite-Width Guarantees and Benefits of Complex Eigenvalues}, 
  author={Orvieto, Antonio and De, Soham and Gulcehre, Caglar and Pascanu, Razvan and Smith, Samuel L},
  journal={International Conference on Machine Learning},
  year={2024}
}

@article{wang2023state,
  title={State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory},
  author={Wang, Shida and Xue, Beichen},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={74021--74038},
  year={2023}
}
@article{olsson2022context,
  title={In-context learning and induction heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
  journal={arXiv preprint arXiv:2209.11895},
  year={2022}
}
@misc{wikitext,
      title={Pointer Sentinel Mixture Models},
      author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
      year={2016},
      eprint={1609.07843},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{pythia,
  title={Pythia: A suite for analyzing large language models across training and scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O’Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  booktitle={International Conference on Machine Learning},
  pages={2397--2430},
  year={2023},
  organization={PMLR}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{amoalonso2024state,
  title={State Space Models as Foundation Models: A Control Theoretic Overview},
  author={Amo Alonso, Carmen and Sieber, Jerome and Zeilinger, Melanie N},
  journal={arXiv preprint arXiv:2403.16899},
  year={2024}
}

@inproceedings{
sieber2024understanding,
title={Understanding the Differences in Foundation Models: Attention, State  Space Models, and Recurrent Neural Networks},
author={Jerome Sieber and Carmen Amo Alonso and Alexandre Didier and Melanie Zeilinger and Antonio Orvieto},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=iF7MnXnxRw}
}

@book{kailath1980linear,
  title={Linear systems},
  author={Kailath, Thomas},
  volume={156},
  year={1980},
  publisher={Prentice-Hall Englewood Cliffs, NJ}
}

@misc{pascanu2013difficultytrainingrecurrentneural,
      title={On the difficulty of training Recurrent Neural Networks}, 
      author={Razvan Pascanu and Tomas Mikolov and Yoshua Bengio},
      year={2013},
      eprint={1211.5063},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1211.5063}, 
}

@article{yang2024gated,
  title={Gated delta networks: Improving mamba2 with delta rule},
  author={Yang, Songlin and Kautz, Jan and Hatamizadeh, Ali},
  journal={arXiv preprint arXiv:2412.06464},
  year={2024}
}

@book{oppenheim1997signals,
  title={Signals \& systems},
  author={Oppenheim, Alan V and Willsky, Alan S and Nawab, Syed Hamid},
  year={1997},
  publisher={Pearson Educaci{\'o}n}
}
@misc{grazzi2025unlockingstatetrackinglinearrnns,
      title={Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues}, 
      author={Riccardo Grazzi and Julien Siems and Arber Zela and Jörg K. H. Franke and Frank Hutter and Massimiliano Pontil},
      year={2025},
      eprint={2411.12537},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.12537}, 
}
@misc{wang2024stablessmalleviatingcursememory,
      title={StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization}, 
      author={Shida Wang and Qianxiao Li},
      year={2024},
      eprint={2311.14495},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2311.14495}, 
}
@misc{raffel2016feedforwardnetworksattentionsolve,
      title={Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems}, 
      author={Colin Raffel and Daniel P. W. Ellis},
      year={2016},
      eprint={1512.08756},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1512.08756}, 
}
@misc{sukhbaatar2019augmentingselfattentionpersistentmemory,
      title={Augmenting Self-attention with Persistent Memory}, 
      author={Sainbayar Sukhbaatar and Edouard Grave and Guillaume Lample and Herve Jegou and Armand Joulin},
      year={2019},
      eprint={1907.01470},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1907.01470}, 
}
@article{Jarne_2022,
   title={Different eigenvalue distributions encode the same temporal tasks in recurrent neural networks},
   volume={17},
   ISSN={1871-4099},
   url={http://dx.doi.org/10.1007/s11571-022-09802-5},
   DOI={10.1007/s11571-022-09802-5},
   number={1},
   journal={Cognitive Neurodynamics},
   publisher={Springer Science and Business Media LLC},
   author={Jarne, Cecilia},
   year={2022},
   month=apr, pages={257–275} }
@misc{helfrich2019eigenvaluenormalizedrecurrentneural,
      title={Eigenvalue Normalized Recurrent Neural Networks for Short Term Memory}, 
      author={Kyle Helfrich and Qiang Ye},
      year={2019},
      eprint={1911.07964},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1911.07964}, 
}
@article{Su_2019,
   title={On extended long short-term memory and dependent bidirectional recurrent neural network},
   volume={356},
   ISSN={0925-2312},
   url={http://dx.doi.org/10.1016/j.neucom.2019.04.044},
   DOI={10.1016/j.neucom.2019.04.044},
   journal={Neurocomputing},
   publisher={Elsevier BV},
   author={Su, Yuanhang and Kuo, C.-C. Jay},
   year={2019},
   month=sep, pages={151–161} }
@misc{zhong2025understandingtransformerperspectiveassociative,
      title={Understanding Transformer from the Perspective of Associative Memory}, 
      author={Shu Zhong and Mingyu Xu and Tenglong Ao and Guang Shi},
      year={2025},
      eprint={2505.19488},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2505.19488}, 
}


@misc{merity2016pointer,
      title={Pointer Sentinel Mixture Models},
      author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
      year={2016},
      eprint={1609.07843},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{bhojanapalli2021eigen,
  title={Eigen analysis of self-attention and its reconstruction from partial computation},
  author={Bhojanapalli, Srinadh and Chakrabarti, Ayan and Jain, Himanshu and Kumar, Sanjiv and Lukasik, Michal and Veit, Andreas},
  journal={arXiv preprint arXiv:2106.08823},
  year={2021}
}

@inproceedings{bao2024self,
  title={Self-attention networks localize when QK-eigenspectrum concentrates},
  author={Bao, Han and Hataya, Ryuichiro and Karakida, Ryo},
  booktitle={Proceedings of the 41st International Conference on Machine Learning},
  pages={2903--2922},
  year={2024}
}

@article{okpekpe2025recalling,
  title={When recalling in-context, Transformers are not SSMs},
  author={Okpekpe, Destiny and Orvieto, Antonio},
  journal={arXiv preprint arXiv:2508.19029},
  year={2025}
}

@article{sanford2024one,
  title={One-layer transformers fail to solve the induction heads task},
  author={Sanford, Clayton and Hsu, Daniel and Telgarsky, Matus},
  journal={arXiv preprint arXiv:2408.14332},
  year={2024}
}