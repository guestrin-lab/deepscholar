\begin{thebibliography}{34}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arora et~al.(2023)Arora, Eyuboglu, Timalsina, Johnson, Poli, Zou, Rudra, and Ré]{zoology2023}
Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher Ré.
\newblock {Zoology: Measuring and Improving Recall in Efficient Language Models}.
\newblock \emph{arXiv:2312.04927}, 2023.

\bibitem[Bao et~al.(2024)Bao, Hataya, and Karakida]{bao2024self}
Han Bao, Ryuichiro Hataya, and Ryo Karakida.
\newblock Self-attention networks localize when qk-eigenspectrum concentrates.
\newblock In \emph{Proceedings of the 41st International Conference on Machine Learning}, pp.\  2903--2922, 2024.

\bibitem[Beck et~al.(2024)Beck, Pöppel, Spanring, Auer, Prudnikova, Kopp, Klambauer, Brandstetter, and Hochreiter]{xlstm}
Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter.
\newblock {xLSTM: Extended Long Short-Term Memory}.
\newblock \emph{arXiv preprint arXiv:2405.04517}, 2024.

\bibitem[Bhojanapalli et~al.(2021)Bhojanapalli, Chakrabarti, Jain, Kumar, Lukasik, and Veit]{bhojanapalli2021eigen}
Srinadh Bhojanapalli, Ayan Chakrabarti, Himanshu Jain, Sanjiv Kumar, Michal Lukasik, and Andreas Veit.
\newblock Eigen analysis of self-attention and its reconstruction from partial computation.
\newblock \emph{arXiv preprint arXiv:2106.08823}, 2021.

\bibitem[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von Arx, Bernstein, Bohg, Bosselut, Brunskill, et~al.]{bommasani2021opportunities}
Rishi Bommasani, Drew~A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael~S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et~al.
\newblock On the opportunities and risks of foundation models.
\newblock \emph{arXiv preprint arXiv:2108.07258}, 2021.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{Brown2020}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock {Language Models are Few-Shot Learners}.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33, pp.\  1877--1901. Curran Associates, Inc., 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}.

\bibitem[Dao \& Gu(2024)Dao and Gu]{mamba2}
Tri Dao and Albert Gu.
\newblock {Transformers are SSMs: Generalized Models and Efficient Algorithms with Structured State Space Duality}.
\newblock In \emph{{ICML} 2024}, 2024.

\bibitem[Fu et~al.(2023)Fu, Dao, Saab, Thomas, Rudra, and Ré]{Fu2023}
Daniel~Y. Fu, Tri Dao, Khaled~K. Saab, Armin~W. Thomas, Atri Rudra, and Christopher Ré.
\newblock {Hungry Hungry Hippos: Towards Language Modeling with State Space Models}, 2023.
\newblock URL \url{https://arxiv.org/abs/2212.14052}.

\bibitem[Grazzi et~al.(2025)Grazzi, Siems, Zela, Franke, Hutter, and Pontil]{grazzi2025unlockingstatetrackinglinearrnns}
Riccardo Grazzi, Julien Siems, Arber Zela, Jörg K.~H. Franke, Frank Hutter, and Massimiliano Pontil.
\newblock Unlocking state-tracking in linear rnns through negative eigenvalues, 2025.
\newblock URL \url{https://arxiv.org/abs/2411.12537}.

\bibitem[Gu et~al.(2020)Gu, Dao, Ermon, Rudra, and R\'{e}]{Gu2020}
Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\'{e}.
\newblock {HiPPO: Recurrent Memory with Optimal Polynomial Projections}.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~33, pp.\  1474--1487. Curran Associates, Inc., 2020.

\bibitem[Gu et~al.(2021)Gu, Goel, and R{\'e}]{S4}
Albert Gu, Karan Goel, and Christopher R{\'e}.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock \emph{arXiv preprint arXiv:2111.00396}, 2021.

\bibitem[Gu et~al.(2022)Gu, Goel, and R\'e]{Gu2022}
Albert Gu, Karan Goel, and Christopher R\'e.
\newblock {Efficiently Modeling Long Sequences with Structured State Spaces}.
\newblock In \emph{The International Conference on Learning Representations ({ICLR})}, 2022.

\bibitem[Helfrich \& Ye(2019)Helfrich and Ye]{helfrich2019eigenvaluenormalizedrecurrentneural}
Kyle Helfrich and Qiang Ye.
\newblock Eigenvalue normalized recurrent neural networks for short term memory, 2019.
\newblock URL \url{https://arxiv.org/abs/1911.07964}.

\bibitem[Jarne(2022)]{Jarne_2022}
Cecilia Jarne.
\newblock Different eigenvalue distributions encode the same temporal tasks in recurrent neural networks.
\newblock \emph{Cognitive Neurodynamics}, 17\penalty0 (1):\penalty0 257–275, April 2022.
\newblock ISSN 1871-4099.
\newblock \doi{10.1007/s11571-022-09802-5}.
\newblock URL \url{http://dx.doi.org/10.1007/s11571-022-09802-5}.

\bibitem[Kailath(1980)]{kailath1980linear}
Thomas Kailath.
\newblock \emph{Linear systems}, volume 156.
\newblock Prentice-Hall Englewood Cliffs, NJ, 1980.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and Fleuret]{Katharopoulos2020}
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\c{c}ois Fleuret.
\newblock Transformers are {RNN}s: fast autoregressive transformers with linear attention.
\newblock In \emph{Proceedings of the 37th International Conference on Machine Learning}, ICML'20. JMLR.org, 2020.

\bibitem[Loshchilov \& Hutter(2019)Loshchilov and Hutter]{adamw}
Ilya Loshchilov and Frank Hutter.
\newblock {Decoupled Weight Decay Regularization}.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=Bkg6RiCqY7}.

\bibitem[Merity et~al.(2016)Merity, Xiong, Bradbury, and Socher]{merity2016pointer}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models, 2016.

\bibitem[Okpekpe \& Orvieto(2025)Okpekpe and Orvieto]{okpekpe2025recalling}
Destiny Okpekpe and Antonio Orvieto.
\newblock When recalling in-context, transformers are not ssms.
\newblock \emph{arXiv preprint arXiv:2508.19029}, 2025.

\bibitem[Oppenheim et~al.(1997)Oppenheim, Willsky, and Nawab]{oppenheim1997signals}
Alan~V Oppenheim, Alan~S Willsky, and Syed~Hamid Nawab.
\newblock \emph{Signals \& systems}.
\newblock Pearson Educaci{\'o}n, 1997.

\bibitem[Orvieto et~al.(2023{\natexlab{a}})Orvieto, Smith, Gu, Fernando, Gulcehre, Pascanu, and De]{LRU}
Antonio Orvieto, Samuel~L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De.
\newblock Resurrecting recurrent neural networks for long sequences.
\newblock In \emph{International Conference on Machine Learning}, pp.\  26670--26698. PMLR, 2023{\natexlab{a}}.

\bibitem[Orvieto et~al.(2023{\natexlab{b}})Orvieto, Smith, Gu, Fernando, Gulcehre, Pascanu, and De]{Orvieto2023}
Antonio Orvieto, Samuel~L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De.
\newblock {Resurrecting Recurrent Neural Networks for Long Sequences}.
\newblock In \emph{Proceedings of the 40th International Conference on Machine Learning}, volume 202, pp.\  26670--26698. PMLR, 23--29 Jul 2023{\natexlab{b}}.

\bibitem[Raffel \& Ellis(2016)Raffel and Ellis]{raffel2016feedforwardnetworksattentionsolve}
Colin Raffel and Daniel P.~W. Ellis.
\newblock Feed-forward networks with attention can solve some long-term memory problems, 2016.
\newblock URL \url{https://arxiv.org/abs/1512.08756}.

\bibitem[Sanford et~al.(2024)Sanford, Hsu, and Telgarsky]{sanford2024one}
Clayton Sanford, Daniel Hsu, and Matus Telgarsky.
\newblock One-layer transformers fail to solve the induction heads task.
\newblock \emph{arXiv preprint arXiv:2408.14332}, 2024.

\bibitem[Schlag et~al.(2021)Schlag, Irie, and Schmidhuber]{schlag2021linear}
Imanol Schlag, Kazuki Irie, and J{\"u}rgen Schmidhuber.
\newblock Linear transformers are secretly fast weight programmers.
\newblock In \emph{International Conference on Machine Learning}, pp.\  9355--9366. PMLR, 2021.

\bibitem[Sieber et~al.(2024)Sieber, Alonso, Didier, Zeilinger, and Orvieto]{sieber2024understanding}
Jerome Sieber, Carmen~Amo Alonso, Alexandre Didier, Melanie Zeilinger, and Antonio Orvieto.
\newblock Understanding the differences in foundation models: Attention, state space models, and recurrent neural networks.
\newblock In \emph{The Thirty-eighth Annual Conference on Neural Information Processing Systems}, 2024.
\newblock URL \url{https://openreview.net/forum?id=iF7MnXnxRw}.

\bibitem[Smith et~al.(2023)Smith, Warrington, and Linderman]{s5}
Jimmy~T.H. Smith, Andrew Warrington, and Scott Linderman.
\newblock {Simplified State Space Layers for Sequence Modeling}.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Su \& Kuo(2019)Su and Kuo]{Su_2019}
Yuanhang Su and C.-C.~Jay Kuo.
\newblock On extended long short-term memory and dependent bidirectional recurrent neural network.
\newblock \emph{Neurocomputing}, 356:\penalty0 151–161, September 2019.
\newblock ISSN 0925-2312.
\newblock \doi{10.1016/j.neucom.2019.04.044}.
\newblock URL \url{http://dx.doi.org/10.1016/j.neucom.2019.04.044}.

\bibitem[Sukhbaatar et~al.(2019)Sukhbaatar, Grave, Lample, Jegou, and Joulin]{sukhbaatar2019augmentingselfattentionpersistentmemory}
Sainbayar Sukhbaatar, Edouard Grave, Guillaume Lample, Herve Jegou, and Armand Joulin.
\newblock Augmenting self-attention with persistent memory, 2019.
\newblock URL \url{https://arxiv.org/abs/1907.01470}.

\bibitem[Tay et~al.(2021)Tay, Dehghani, Abnar, Shen, Bahri, Pham, Rao, Yang, Ruder, and Metzler]{Tay2021}
Yi~Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler.
\newblock {Long Range Arena : A Benchmark for Efficient Transformers}.
\newblock In \emph{International Conference on Learning Representations ({ICLR})}, 2021.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{Transformer}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, \L~ukasz Kaiser, and Illia Polosukhin.
\newblock {Attention is All you Need}.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem[Wang \& Li(2024)Wang and Li]{wang2024stablessmalleviatingcursememory}
Shida Wang and Qianxiao Li.
\newblock Stablessm: Alleviating the curse of memory in state-space models through stable reparameterization, 2024.
\newblock URL \url{https://arxiv.org/abs/2311.14495}.

\bibitem[Wang \& Xue(2023)Wang and Xue]{wang2023state}
Shida Wang and Beichen Xue.
\newblock State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 74021--74038, 2023.

\bibitem[Yang et~al.(2024)Yang, Kautz, and Hatamizadeh]{yang2024gated}
Songlin Yang, Jan Kautz, and Ali Hatamizadeh.
\newblock Gated delta networks: Improving mamba2 with delta rule.
\newblock \emph{arXiv preprint arXiv:2412.06464}, 2024.

\end{thebibliography}
