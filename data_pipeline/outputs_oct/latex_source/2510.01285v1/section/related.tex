\section{Related Work}

\paragraph{LLMs for Data Science:} Specialized benchmarks have emerged to evaluate LLMs in data science. DS-1000 \citep{10.5555/3618408.3619164}, ARCADE \citep{yin-etal-2023-natural}, DataSciBench \citep{zhang2025datascibenchllmagentbenchmark}, and DSEval \citep{zhang-etal-2024-benchmarking-data} assess the translation of natural language instructions into correct implementations, distinguishing them from broader programming benchmarks such as SWE-Bench \citep{jimenez2024swebench}, ML-Bench \citep{tang2025mlbench}, and BigCodeBench \citep{zhuo2025bigcodebench}. While most assume that the relevant data files are pre-specified, recent efforts address multi-step reasoning: DS-Bench \citep{jing2025dsbench} and BLADE \citep{gu-etal-2024-blade} evaluate implementation planning, and ScienceAgentBench \citep{chen2025scienceagentbench} and BixBench \citep{mitchener2025bixbenchcomprehensivebenchmarkllmbased} focus on integrating domain knowledge. These benchmarks, however, still overlook the practical challenge of discovering relevant data within large, heterogeneous repositoriesâ€”a gap addressed by KramaBench \citep{lai2025kramabenchbenchmarkaisystems}, which explicitly evaluates data discovery. Building on this, we study how agents can autonomously identify and leverage the correct data sources for end-to-end analysis.

Applications of LLMs in data science have evolved from single-turn code generation to interactive, tool-augmented agents that exploit models specialized for code, including GPT \citep{10.5555/3495724.3495883}, CodeGen \citep{rubavicius2025conversationalcodegenerationcase}, StarCoder \citep{li2023starcoder}, and Code Llama \citep{codellama}. While few-shot prompting \citep{10.5555/3495724.3495883} remains effective, state-of-the-art approaches adopt agentic or multi-agentic frameworks that combine iterative reasoning with external tool use. ReAct \citep{yao2023react} pioneered the interleaving of reasoning and action, later extended to execution environments \citep{chen2018executionguided}. Toolformer \citep{schick2023toolformer} and Gorilla \citep{patil2024gorilla} explicitly train LLMs to call APIs, a capability critical for tasks relying on specialized libraries. Self-correction is a another key feature: frameworks like Self-Debug \citep{chen2024teaching} and Reflexion \citep{shinn2023reflexion} refine generated code using execution feedback. To further enhance reliability, many systems integrate RAG \citep{10.5555/3495724.3496517, salemi2025planandrefinediversecomprehensiveretrievalaugmented, 10.1145/3731120.3744584, 10.1145/3626772.3657733} to retrieve documentation or code examples, reducing hallucinations and ensuring up-to-date library use. Additionally, multi-agent master-slave frameworks, such as AutoKaggle \citep{li2025autokaggle}, have demonstrated promising results in addressing these challenges.

\paragraph{Blackboard Systems:}

The blackboard system is a seminal architectural model from classical AI, developed for complex problems that require incremental and opportunistic reasoning. It was implemented in the Hearsay-II speech understanding \citep{10.1145/356810.356816} and is characterized by three components: (1) a global, hierarchical data structure (the blackboard) that maintains the current state of the solution; (2) independent specialist modules, known as knowledge sources, which monitor the blackboard and contribute partial solutions; and (3) a control mechanism that opportunistically determines which knowledge source to activate next \citep{Nii_1986}. Following successful applications in domains such as sonar interpretation with the HASP/SIAP system \citep{Nii_Feigenbaum_Anton_1982}, the architecture evolved to incorporate more sophisticated control strategies. Inspired by this paradigm, we adapt the blackboard architecture for multi-agent communication: rather than a central controller assigning tasks, all agents operate autonomously, responding to requests posted on the blackboard. A central main agent then leverages the information contributed by sub-agents to solve the problem.