\section{Datasets and Preprocessing}
\label{app:dataset}


To the best of our knowledge, KramaBench \citep{lai2025kramabenchbenchmarkaisystems} is the only publicly available dataset for data science problems that explicitly require data discovery to answer the questions. We adopt this dataset as one of our evaluation benchmarks in this paper.

To further investigate this problem, we repurpose two widely used datasets for data science tasks, DS-Bench \citep{jing2025dsbench} and DA-Code \citep{huang-etal-2024-da}, which were not originally designed to include a data discovery phase. In their original form, each question in these datasets is paired with the specific data files required to answer it. To adapt them to our setting, we remove this direct mapping: the model is provided only with the question, while all files from the dataset are aggregated into a single data lake. The model must therefore first identify the relevant files within the data lake and then use them to solve the question.

\paragraph{Filtering:} we observed that not all questions in these datasets are suitable for the data discovery setting. For instance, some questions provide no hints about the characteristics of the files needed to answer them, while others simply ask for computing a statistic on a column without specifying sufficient information to identify the relevant file. To address this issue, we manually filter out such questions and retain only those that include adequate cues for discovering the appropriate files. After this filtering process, the resulting dataset statistics are reported in Table~\ref{tab:stats}.

\begin{table}[h!]
    \centering
    \caption{Statistics of the datasets used in our evaluation setup.}
    \label{tab:stats}
    \begin{tabular}{l|ccc}
        \toprule
        \textbf{Dataset} & \textbf{\#Tasks} & \textbf{Size of data lake} & \textbf{\#Clusters created by Gemini 2.5 Pro} \\
        \midrule
        KramaBench & 104 & 1746\tablefootnote{Note that, in line with the original benchmark design, we construct a separate data lake for each subtask. However, the reported number of files corresponds to the total number of files aggregated across all subtasks in the benchmark.} & 27\tablefootnote{Note that, in line with the original benchmark design, we construct a separate data lake for each subtask. However, the reported number of clusters corresponds to the total number of clusters aggregated across all subtasks in the benchmark.} \\
        \quad\quad - Archeology & 12 & 5 & 3 \\
        \quad\quad - Astronomy & 12 & 1556 & 8 \\
        \quad\quad - Biomedical & 9 & 7 & 2 \\
        \quad\quad - Environment & 20 & 37 & 4 \\
        \quad\quad - Legal & 30 & 136 & 4 \\
        \quad\quad - Wildfire & 21 & 23 & 6 \\
        \midrule
        DS-Bench & 253 & 48 & 12 \\
        \midrule
        DA-Code & 91 & 145 & 26 \\
        \bottomrule
    \end{tabular}
\end{table}


\paragraph{Evaluation:}

To evaluate the programs generated by the system, we execute each program and assess its final output against the reference answer for the given question. For each dataset, we adopt its original evaluation methodology. Specifically, for KramaBench, we use the official evaluation script provided in their repository.\footnote{Available at: \url{https://github.com/mitdbg/KramaBench}} For DA-Code, we similarly rely on the official evaluation script released in their repository.\footnote{Available at: \url{https://github.com/yiyihum/da-code}} For DS-Bench, we follow the original evaluation protocol that uses LLM-based judging: the generated programs output is compared to the reference answer using Gemini 2.5 Pro as the judge LLM, with the prompt shown in Figure~\ref{fig:eval-ds-bench}.

\begin{figure}[h!]
    \centering
    \includegraphics{figs/eval-prompt-ds-bench.pdf}
    \caption{Evaluation prompt used for DS-Bench dataset using LLM as the judge.}
    \label{fig:eval-ds-bench}
\end{figure}

\newpage
\section{Agents' Prompts}
\label{app:prompts}


This section presents the prompts used by the agents and baselines in this paper. Figure~\ref{fig:clustering-example} shows the prompt for clustering the data lake into multiple partitions based on file names. Figure~\ref{fig:main-agent-blackboard-prompt} presents the prompt used by the main agent in the blackboard system. Figure~\ref{fig:file-agent-prompt} shows the prompt for the file agents. Figure~\ref{fig:search-agent-prompt} displays the prompt used by the search agent. Figure~\ref{fig:main-agent-master-slave-prompt} presents the prompt for the main agent in the master–slave system, and Figure~\ref{fig:main-agent-rag-prompt} shows the prompt used by the RAG agent.


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figs/clustering-prompt.pdf}
    \vspace{-0.6cm}
    \caption{Prompt used by for clustering the files in data lakes into partitions.}
    \label{fig:clustering-prompt}
\end{figure}

\newpage
\begin{figure}
    \centering
    \includegraphics[width=0.85\textwidth]{figs/figures-main-agent-blackboard.pdf}
    % \vspace{-1.2cm}
    \caption{Prompt used by the main agent for the blackboard system.}
    \label{fig:main-agent-blackboard-prompt}
\end{figure}

\newpage
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figs/figures-file-agent.pdf}
    \vspace{-1.2cm}
    \caption{Prompt used by the file agent for the both master-slave and blackboard system.}
    \label{fig:file-agent-prompt}
\end{figure}

\newpage
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figs/figures-search-agent.pdf}
    \vspace{-1.2cm}
    \caption{Prompt used by the search agent for the, master-slave, RAG, and blackboard system.}
    \label{fig:search-agent-prompt}
\end{figure}

\newpage
\begin{figure}
    \centering
    \includegraphics[width=0.85\textwidth]{figs/figures-main-agent-master-slave.pdf}
    \vspace{-0.6cm}
    \caption{Prompt used by the main agent for the master-slave system.}
    \label{fig:main-agent-master-slave-prompt}
\end{figure}

\newpage
\begin{figure}
    \centering
    \includegraphics[width=0.85\textwidth]{figs/figures-main-agent-RAG.pdf}
    \vspace{-0.6cm}
    \caption{Prompt used by the main agent for the RAG system.}
    \label{fig:main-agent-rag-prompt}
\end{figure}

% \newpage
\newpage
\section{Implementation Details}
\label{app:implementation}

\paragraph{Presenting Files to File Agents:} A file agent may request a file by name, in which case it is shown a subset of the files contents. For this case, we employ a controlled procedure for loading and presenting the data to the agent, as described below:
\begin{itemize}[leftmargin=*]

\item Files with \textit{.csv} format: In this case, we use the \textit{pandas}\footnote{Available at: \url{https://pandas.pydata.org/}} library to load the CSV files, presenting the column names, their data types, and the top 20 rows of the table to the agent.

\item Files with \textit{.gpkg} format: which provides a pandas-like interface for geospatial data. The agent is then presented with the column names, their data types, and the top 20 rows of the table.

\item Files with \textit{.xlsx} format: In this case, we use the \textit{pandas}\footnote{Available at: \url{https://pandas.pydata.org/}} library to handle this file format. For files containing multiple sheets, we provide the agent with all sheet names, the data types of columns in each sheet, and the top 20 rows from each sheet.

\item Files with \textit{.npz} format: In this case, we utilize the \textit{numpy}\footnote{Available at: \url{https://numpy.org/}} library to load the data. The agent is then presented with all keys and their corresponding values within this data structure.

\item Files with \textit{.cdf} format: In this case, we utilize the \textit{cdflib}\footnote{Available at: \url{https://cdflib.readthedocs.io/en/latest/}} library to load the data. For presentation, we call the \texttt{cdf\_info} and \texttt{globalattsget} functions on the loaded data structure, concatenate their outputs, and provide the result to the agent.

\item Any other data format: In this case, we open the files using Pythons \texttt{open} function and present the first 20 lines of the file to the agent.

\end{itemize}


\paragraph{Inference Setup.} We limit the maximum number of actions taken by the main agent to $T = 10$. For decoding, we use nucleus sampling \citep{Holtzman2020The} with a temperature of $\tau = 0.1$. Proprietary models are accessed through Vertex AI,\footnote{\url{https://cloud.google.com/vertex-ai?hl=en}} while open-source models are served using the vLLM library.\footnote{\url{https://docs.vllm.ai/en/latest/}} At each generation step, we cap the output length at 8,192 tokens. We evaluate three proprietary LLMs—Gemini 2.5 Pro, Gemini 2.5 Flash \citep{comanici2025gemini25pushingfrontier}, and Claude 4 Opus \citep{anthropic2025claude4}—alongside an open-source model specialized for code generation, Qwen3-Coder-30B-A3B-Instruct \citep{qwen3technicalreport}.\footnote{\url{https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct}} Experiments with open-source models are conducted on 2 NVIDIA A100 GPUs (80GB VRAM each) with 128GB RAM.

\newpage
\section{Examples and Case Studies}
\label{app:case-study}


This section presents several case studies highlighting different aspects of the Blackboard system.

Figure~\ref{fig:file-agent-analyze-example} illustrates an example where a file agent requests access to files and performs their analysis. Figures~\ref{fig:search-example-1} and~\ref{fig:search-example-2} illustrate scenarios where the main agent lacked domain-specific knowledge and therefore posted requests on the blackboard seeking relevant information. In these cases, the search agent contributed by retrieving the necessary knowledge from the web, enabling the system to proceed with problem solving, which shows the effectiveness of search agent in problem solving.

Another example of a blackboard request is shown in Figure~\ref{fig:request-example}. In this example, specifically file agents responded to the request. Here, the main agent, given a data science question, formulated a request specifying the likely column names and data formats required, along with guidance for interpretation. In response, three out of eight helper agents contributed. Although the relevant files were spread across different clusters managed by separate file agents, each responding agent independently provided file addresses, code snippets for loading the data, explanations of the structure, and suggested preprocessing steps. Together, these contributions encompassed all the ground-truth files needed to solve the problem. This case demonstrates how the main agent can effectively leverage the blackboard to coordinate decentralized knowledge and achieve accurate data discovery.

Figure~\ref{fig:program-example} compares programs generated by the Blackboard and Master–Slave systems. The Blackboard agent produced the correct solution by accurately interpreting the prompt and selecting the appropriate data files. Specifically, it identified that the patients \texttt{Age} was located in \texttt{mmc1.xlsx} and that the requested \texttt{APP-Z score} was in \texttt{mmc7.xlsx}. By contrast, the Master–Slave agent misinterpreted the request and instead used a general protein abundance score (\texttt{APP\_log2\_abundance}) from the wrong file, \texttt{mmc2.xlsx}. This misstep resulted in an incorrect answer of \texttt{74}, whereas the Blackboard agents precise data discovery and reasoning yielded the correct answer of \texttt{60}.

\newpage
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{figs/figures-case-study-file-agent.pdf}
    \vspace{-1.2cm}
    \caption{An example analyzing files by the file agent.}
    \label{fig:file-agent-analyze-example}
\end{figure}


\newpage
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{figs/clustering-example.pdf}
    \vspace{-1.2cm}
    \caption{An example of clustering data lake into partitions using Gemini 2.5 Pro.}
    \label{fig:clustering-example}
\end{figure}

\newpage
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{figs/figures-search-agent-example-1.pdf}
    \vspace{-1.2cm}
    \caption{An example of the request by the main agent that the search agent has provided a guideline based on search results.}
    \label{fig:search-example-1}
\end{figure}

\newpage
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{figs/figures-search-agent-example-2.pdf}
    \vspace{-1.2cm}
    \caption{An example of the request by the main agent that the search agent has provided a guideline based on search results.}
    \label{fig:search-example-2}
\end{figure}


\newpage
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{figs/figures-request-example.pdf}
    \vspace{-1cm}
    \caption{An example of the generated request by the blackboard system.}
    \label{fig:request-example}
\end{figure}

\newpage
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{figs/figures-program-example.pdf}
    \vspace{-1cm}
    \caption{An example of the generated program by the blackboard system and master-slave system. The green box highlights where the blackboard system correctly selected the relevant files from the data lake, while the red box indicates where the master–slave system made an incorrect selection.}
    \label{fig:program-example}
\end{figure}

\newpage
\section{Large Language Model Usage for Writing}

In this paper, we employ LLMs---specifically Gemini and ChatGPT---as general-purpose writing tools. Draft text is provided to these models, which are then asked to improve the writing by correcting grammatical errors and refining the structure. The edited text is then verified and edited if needed. The use of LLMs in this paper is limited strictly to text refinement. They were not employed for tasks such as generating any new content or references.
